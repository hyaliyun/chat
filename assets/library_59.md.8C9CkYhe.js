import{_ as p,o as a,c as n,a as t,m as u,t as c,C as _,M as g,U as y,f as d,F as b,p as v,e as w,q as x}from"./chunks/framework.B1z0IdBH.js";const k={name:"PoemCard",props:{poem:{type:Object,required:!0}}},T={class:"poem-container"},C={class:"review"},P={class:"review-title"},E={class:"review-content"};function S(i,e,l,m,o,s){return a(),n("div",T,[t("div",C,[t("div",P,[e[0]||(e[0]=t("span",{class:"icon"},"question:",-1)),u(c(l.poem.question),1)]),e[2]||(e[2]=t("div",{class:"divider"},null,-1)),t("p",E,[e[1]||(e[1]=t("span",{class:"icon"},"answer:",-1)),u(c(l.poem.solution),1)])])])}const A=p(k,[["render",S],["__scopeId","data-v-5f557439"]]),I=JSON.parse('[{"question":"**Objective**: Implement a custom asynchronous chat server using the `asynchat` module that can handle multiple clients concurrently. The server should accept client messages, process them, and send appropriate responses. **Problem Description**: You are to create a server class inheriting from `asynchat.async_chat` that can: 1. Accept incoming connections. 2. Read messages from clients. 3. Respond to clients with a modified message in uppercase. **Requirements**: 1. Implement a class `ChatServer` that handles async communications using `asynchat.async_chat`. 2. The server should: - Accept connections on a specified port. - Read messages from each connected client until a newline character (`n`) is found. - Respond back to the client with the received message transformed to uppercase. 3. Ensure your implementation handles multiple clients concurrently. 4. Your class should handle starting and stopping the server. **Implementation Details**: 1. **Class Definition**: Define `ChatServer` inheriting from `asynchat.async_chat`. 2. **Initialization**: Accept server socket and address in the constructor and call `asynchat.async_chat.__init__`. 3. **Methods**: - `collect_incoming_data(self, data)`: Append the incoming data to an instance buffer. - `found_terminator(self)`: Called when a newline character (`n`) is encountered in the data. Process the received message and prepare to respond with the uppercase version of it. - `start_server(self)`: Start the asynchronous loop to begin accepting and handling client connections. - `stop_server(self)`: Close any open connections and stop the async loop. **Constraints**: - The server should handle large messages, up to 4096 characters. - Use a FIFO queue for handling message delivery. **Example**: ```python import asynchat import asyncore import socket class ChatServer(asynchat.async_chat): def __init__(self, sock, addr): asynchat.async_chat.__init__(self, sock=sock) self.addr = addr self.ibuffer = [] self.set_terminator(b\'n\') def collect_incoming_data(self, data): Buffer the data self.ibuffer.append(data) def found_terminator(self): Process the complete message message = b\\"\\".join(self.ibuffer).decode(\'utf-8\').strip() self.ibuffer = [] # Convert the message to uppercase and send it back response = message.upper() + \'n\' self.push(response.encode(\'utf-8\')) class Server(asyncore.dispatcher): def __init__(self, host, port): asyncore.dispatcher.__init__(self) self.create_socket(socket.AF_INET, socket.SOCK_STREAM) self.set_reuse_addr() self.bind((host, port)) self.listen(5) def handle_accepted(self, sock, addr): print(f\\"Incoming connection from {addr}\\") ChatServer(sock, addr) def start_server(self): Start the async loop asyncore.loop() def stop_server(self): Stop the server by closing all channels for channel in asyncore.socket_map.values(): channel.close() asyncore.close_all() # Usage example: if __name__ == \\"__main__\\": server = Server(\'localhost\', 9090) server.start_server() ``` **Evaluation Criteria**: - Correct implementation of the `collect_incoming_data` and `found_terminator` methods. - Proper handling of multiple concurrent client connections. - Efficiently handling large messages up to 4096 characters. - Code readability and adherence to Python best practices.","solution":"import asynchat import asyncore import socket class ChatServer(asynchat.async_chat): def __init__(self, sock, addr): asynchat.async_chat.__init__(self, sock=sock) self.addr = addr self.ibuffer = [] self.set_terminator(b\'n\') def collect_incoming_data(self, data): Buffer the incoming data self.ibuffer.append(data) def found_terminator(self): Process the complete message when the terminator is found message = b\\"\\".join(self.ibuffer).decode(\'utf-8\').strip() self.ibuffer = [] # Convert the message to uppercase and send it back response = message.upper() + \'n\' self.push(response.encode(\'utf-8\')) class Server(asyncore.dispatcher): def __init__(self, host, port): asyncore.dispatcher.__init__(self) self.create_socket(socket.AF_INET, socket.SOCK_STREAM) self.set_reuse_addr() self.bind((host, port)) self.listen(5) def handle_accepted(self, sock, addr): print(f\\"Incoming connection from {addr}\\") ChatServer(sock, addr) def start_server(self): Start the async loop asyncore.loop() def stop_server(self): Stop the server by closing all channels for channel in asyncore.socket_map.values(): channel.close() asyncore.close_all()"},{"question":"Visualizing Health Expenditure with Seaborn Objective You are tasked with creating a visualization using the seaborn library that shows the health expenditure over time for different countries. You will load a dataset, preprocess it, and create an informative visualization that uses faceting and stacking to present the data clearly. Instructions 1. Import the necessary libraries: ```python import seaborn.objects as so from seaborn import load_dataset ``` 2. Load the `healthexp` dataset, preprocess it by pivoting, interpolating, stacking, renaming columns, resetting the index, and sorting by \\"Country\\": ```python healthexp = ( load_dataset(\\"healthexp\\") .pivot(index=\\"Year\\", columns=\\"Country\\", values=\\"Spending_USD\\") .interpolate() .stack() .rename(\\"Spending_USD\\") .reset_index() .sort_values(\\"Country\\") ) ``` 3. Create a plot that: - Uses area plots to show health expenditure over time. - Facets by \\"Country\\" using 3 columns for the subplots. - Colors the areas by \\"Country\\". - Stacks the areas to show part-whole relationships for each year. Expected Input and Output - **Input**: The input is provided within the question as the `healthexp` dataset. - **Output**: A visualization plot created using seaborn that meets the specified criteria. Constraints and Requirements - Your solution should leverage seaborn’s objects interface for creating and customizing the plot. - Use faceting with 3 columns to create subplots for each country. - Ensure areas are stacked to show part-whole relationships. Example Solution Here\'s an example of what your solution might look like: ```python import seaborn.objects as so from seaborn import load_dataset # Load and preprocess the dataset healthexp = ( load_dataset(\\"healthexp\\") .pivot(index=\\"Year\\", columns=\\"Country\\", values=\\"Spending_USD\\") .interpolate() .stack() .rename(\\"Spending_USD\\") .reset_index() .sort_values(\\"Country\\") ) # Create the plot with faceting and stacking p = so.Plot(healthexp, \\"Year\\", \\"Spending_USD\\").facet(\\"Country\\", wrap=3) p.add(so.Area(color=\\"Country\\", alpha=0.7), so.Stack()) p.show() ``` Ensure your plot closely follows the example provided and meets all specified criteria. Performance Requirements - The solution should be efficient and make good use of seaborn\'s plotting and data manipulation capabilities. - The final plot should be clear and informative, following the principles of good data visualization practice.","solution":"import seaborn.objects as so from seaborn import load_dataset # Load and preprocess the dataset healthexp = ( load_dataset(\\"healthexp\\") .pivot(index=\\"Year\\", columns=\\"Country\\", values=\\"Spending_USD\\") .interpolate() .stack() .rename(\\"Spending_USD\\") .reset_index() .sort_values(\\"Country\\") ) # Create the plot with faceting and stacking p = so.Plot(healthexp, x=\\"Year\\", y=\\"Spending_USD\\").facet(\\"Country\\", wrap=3) p.add(so.Area(color=\\"Country\\", alpha=0.7), so.Stack()) p.show()"},{"question":"In this assessment, you will be required to create a custom probability distribution using PyTorch\'s `torch.distributions` module. Specifically, you will implement a mixed distribution that combines a Gamma and Normal distribution. The custom distribution will sample values from the Gamma distribution and then shift them based on samples from the Normal distribution. # Requirements 1. Create a new class `GammaNormal` that inherits from `torch.distributions.Distribution`. 2. Initialize `GammaNormal` with parameters of the Gamma and Normal distributions. 3. Implement a `sample` method that: - Samples a value from the Gamma distribution. - Samples a value from the Normal distribution. - Returns the sum of the two sampled values. 4. Implement a `log_prob` method that: - Calculates the log-probability of the resulting sample considering both the Gamma and Normal distributions. # Inputs and Outputs - **Input**: - Parameters for the Gamma distribution: `concentration` (alpha), `rate` (beta). - Parameters for the Normal distribution: `loc` (mean), `scale` (std deviation). - **Output**: - Generated samples using the combined Gamma and Normal distributions. - Log-probability of a given value. # Function Signature ```python import torch from torch.distributions import Distribution, Gamma, Normal class GammaNormal(Distribution): def __init__(self, concentration, rate, loc, scale): self.gamma = Gamma(concentration, rate) self.normal = Normal(loc, scale) def sample(self): gamma_sample = self.gamma.sample() normal_sample = self.normal.sample() return gamma_sample + normal_sample def log_prob(self, value): log_prob_gamma = self.gamma.log_prob(value) log_prob_normal = self.normal.log_prob(value) return log_prob_gamma + log_prob_normal # Example usage concentration = torch.tensor([1.0]) rate = torch.tensor([2.0]) loc = torch.tensor([0.0]) scale = torch.tensor([1.0]) gamma_normal_dist = GammaNormal(concentration, rate, loc, scale) sample = gamma_normal_dist.sample() log_prob = gamma_normal_dist.log_prob(sample) print(\\"Sample:\\", sample) print(\\"Log probability of sample:\\", log_prob) ``` # Constraints - Ensure that both Gamma and Normal distributions are properly parameterized. - Sampling should be efficient and handle batch dimensions if provided. - Log-probability should correctly reflect the combined probabilities of the two distributions. # Performance - The implementation should be efficient with respect to both time and space, leveraging PyTorch\'s built-in operations and tensor capabilities.","solution":"import torch from torch.distributions import Distribution, Gamma, Normal class GammaNormal(Distribution): def __init__(self, concentration, rate, loc, scale): super(GammaNormal, self).__init__() self.gamma = Gamma(concentration, rate) self.normal = Normal(loc, scale) def sample(self): gamma_sample = self.gamma.sample() normal_sample = self.normal.sample() return gamma_sample + normal_sample def log_prob(self, value): # We need to compute the log-probabilities separately and then sum them log_prob_gamma = self.gamma.log_prob(value) log_prob_normal = self.normal.log_prob(value) return log_prob_gamma + log_prob_normal"},{"question":"# Custom Browser Handler and URL Opener **Objective:** This task assesses your understanding of the `webbrowser` module, including registering new browser types, opening URLs using different methods, and handling potential browser unavailability. **Problem Statement:** You are required to implement a function `custom_open(url: str, browser_preference_list: List[str], fallback: str) -> bool`. This function should attempt to open a given URL in a browser specified by the `browser_preference_list`, which is a list of browser type names. If none of the preferred browsers are available, the function should open the URL in a browser specified by the `fallback` parameter. The function should return `True` if the URL was successfully opened in one of the specified browsers and `False` otherwise. **Detailed Constraints and Requirements:** 1. **Input:** - `url` (str): The URL to be opened. - `browser_preference_list` (List[str]): A list of browser type names to try, in order of preference. - `fallback` (str): A single browser type name to be used as a fallback if no preferred browsers are available. 2. **Output:** - Returns `True` if the URL is successfully opened in one of the browsers. - Returns `False` if all attempts to open the URL fail. 3. **Behavior and Requirements:** - Use the `webbrowser.get()` function to check for the specified browsers. - If a preferred browser is available, open the URL using `webbrowser.open_new_tab(url)` method of the corresponding browser controller. - If none of the preferred browsers are available, open the URL using the `fallback` browser. - Handle any exceptions that may occur when trying to open a URL and ensure the function does not terminate unexpectedly. 4. **Example Usage:** ```python url = \\"https://www.python.org\\" preferences = [\\"firefox\\", \\"chrome\\", \\"safari\\"] fallback_browser = \\"links\\" result = custom_open(url, preferences, fallback_browser) print(result) # Should print True if the URL is successfully opened in one of the browsers, otherwise False ``` 5. **Performance:** - The function should perform the checks and open the URL in a reasonable time frame, accounting for network and system performance limitations. **Note:** - Assume the environment and browsers are set up correctly for testing purposes. - Be mindful of platform-specific behaviors and handle them accordingly. # Function Signature: ```python from typing import List import webbrowser def custom_open(url: str, browser_preference_list: List[str], fallback: str) -> bool: # Your code here pass ``` You need to implement and test the `custom_open` function based on the provided specifications.","solution":"from typing import List import webbrowser def custom_open(url: str, browser_preference_list: List[str], fallback: str) -> bool: Tries to open the given URL in the browsers specified by the browser_preference_list or the fallback browser if none of the preferred browsers are available. :param url: The URL to be opened. :param browser_preference_list: A list of preferred browser type names. :param fallback: A fallback browser type name. :return: True if the URL was successfully opened, False otherwise. for browser_name in browser_preference_list: try: controller = webbrowser.get(browser_name) controller.open_new_tab(url) return True except webbrowser.Error: continue try: fallback_controller = webbrowser.get(fallback) fallback_controller.open_new_tab(url) return True except webbrowser.Error: return False"},{"question":"You are given a sequence of numbers and tasked with performing a series of mathematical calculations using Python\'s `math` module. Part A: Compute the Statistical Functions Given a list of floating-point numbers, compute the following statistics: 1. The product of all numbers. 2. The accurate floating-point sum of the numbers. 3. The geometric mean of the numbers, defined as the N-th root of the product of N numbers. Part B: Trigonometric Conversion Given an angle in degrees, convert it to radians and then calculate the sine, cosine, and tangent of the angle in both degrees and radians. Part C: Special Function Calculation Given a list of floats, apply the error function, complementary error function, and gamma function to each number. # Function Signature ```python from typing import List, Tuple def compute_statistics(nums: List[float]) -> Tuple[float, float, float]: pass def trigonometric_conversion(angle_degrees: float) -> Tuple[float, float, float, float, float, float, float]: pass def special_functions(nums: List[float]) -> List[Tuple[float, float, float]]: pass ``` # Input - `compute_statistics(nums: List[float]) -> Tuple[float, float, float]`: - `nums` (List[float]): A list of floating-point numbers. - `trigonometric_conversion(angle_degrees: float) -> Tuple[float, float, float, float, float, float, float]`: - `angle_degrees` (float): An angle in degrees. - `special_functions(nums: List[float]) -> List[Tuple[float, float, float]]`: - `nums` (List[float]): A list of floating-point numbers. # Output - `compute_statistics(nums: List[float]) -> Tuple[float, float, float]`: - returns a tuple containing: - The product of all numbers in `nums`. - The accurate floating-point sum of numbers in `nums`. - The geometric mean of numbers in `nums`. - `trigonometric_conversion(angle_degrees: float) -> Tuple[float, float, float, float, float, float, float]`: - returns a tuple containing: - The input angle in radians. - The sine of the angle in degrees. - The cosine of the angle in degrees. - The tangent of the angle in degrees. - The sine of the angle in radians. - The cosine of the angle in radians. - The tangent of the angle in radians. - `special_functions(nums: List[float]) -> List[Tuple[float, float, float]]`: - returns a list of tuples where each tuple contains: - The result of applying the error function to the number. - The result of applying the complementary error function to the number. - The result of applying the gamma function to the number. # Constraints - All numbers in `nums` for `compute_statistics` are positive. - The angle for `trigonometric_conversion` is between `-360` and `360` degrees. - For `special_functions`, all numbers in `nums` are positive. # Example ```python compute_statistics([1.0, 2.0, 3.0, 4.0]) # Output: (24.0, 10.0, 2.2133638394006435) trigonometric_conversion(45.0) # Output: (0.7853981633974483, 0.7071067811865475, 0.7071067811865476, 1.0, 0.7071067811865475, 0.7071067811865476, 1.0) special_functions([0.5, 1.5]) # Output: [(0.5204998778130465, 0.47950012218695337, 0.886226925452758), (0.9661051464753108, 0.03389485352468931, 0.8872638175030751)] ``` # Note You must use the `math` module functions to solve this problem. Each part of the function should not use any additional packages or libraries.","solution":"import math from typing import List, Tuple def compute_statistics(nums: List[float]) -> Tuple[float, float, float]: product = math.prod(nums) accurate_sum = math.fsum(nums) geometric_mean = math.pow(product, 1/len(nums)) return product, accurate_sum, geometric_mean def trigonometric_conversion(angle_degrees: float) -> Tuple[float, float, float, float, float, float, float]: angle_radians = math.radians(angle_degrees) sin_degrees = math.sin(math.radians(angle_degrees)) cos_degrees = math.cos(math.radians(angle_degrees)) tan_degrees = math.tan(math.radians(angle_degrees)) sin_radians = math.sin(angle_radians) cos_radians = math.cos(angle_radians) tan_radians = math.tan(angle_radians) return angle_radians, sin_degrees, cos_degrees, tan_degrees, sin_radians, cos_radians, tan_radians def special_functions(nums: List[float]) -> List[Tuple[float, float, float]]: return [(math.erf(x), math.erfc(x), math.gamma(x)) for x in nums]"},{"question":"# Base64 Encoding and Decoding Challenge As a software developer, you are tasked with creating a utility to encode and decode data using Base64 encoding scheme. Your utility should provide functionality to: 1. Encode a given string to a Base64 encoded string. 2. Decode a given Base64 encoded string back to the original string. 3. Encode a given string to a URL-safe Base64 encoded string. 4. Decode a given URL-safe Base64 encoded string back to the original string. Write a Python class `Base64Utility` that has the following methods: - `encode_to_base64(data: str) -> str`: This method takes a string as input and returns its Base64 encoded string. - `decode_from_base64(encoded_data: str) -> str`: This method takes a Base64 encoded string as input and returns the original string. - `encode_to_urlsafe_base64(data: str) -> str`: This method takes a string as input and returns its URL-safe Base64 encoded string. - `decode_from_urlsafe_base64(encoded_data: str) -> str`: This method takes a URL-safe Base64 encoded string as input and returns the original string. # Constraints - Do not use any other libraries apart from `base64`. - Input strings will only contain printable ASCII characters. - Ensure your utility correctly handles incorrect padding while decoding. # Example Usage ```python # Instantiation of the class base64_util = Base64Utility() # Encoding and Decoding Base64 encoded_str = base64_util.encode_to_base64(\'Hello, World!\') print(encoded_str) # Output: \'SGVsbG8sIFdvcmxkIQ==\' decoded_str = base64_util.decode_from_base64(encoded_str) print(decoded_str) # Output: \'Hello, World!\' # Encoding and Decoding URL-safe Base64 urlsafe_encoded_str = base64_util.encode_to_urlsafe_base64(\'Hello, World!\') print(urlsafe_encoded_str) # Output: \'SGVsbG8sIFdvcmxkIQ==\' urlsafe_decoded_str = base64_util.decode_from_urlsafe_base64(urlsafe_encoded_str) print(urlsafe_decoded_str) # Output: \'Hello, World!\' ``` # Performance Requirements: - The solution should handle inputs of up to 1MB efficiently within a reasonable time frame.","solution":"import base64 class Base64Utility: @staticmethod def encode_to_base64(data: str) -> str: Encodes a string to a Base64 encoded string. bytes_data = data.encode(\'utf-8\') base64_bytes = base64.b64encode(bytes_data) base64_string = base64_bytes.decode(\'utf-8\') return base64_string @staticmethod def decode_from_base64(encoded_data: str) -> str: Decodes a Base64 encoded string back to the original string. base64_bytes = encoded_data.encode(\'utf-8\') message_bytes = base64.b64decode(base64_bytes) message = message_bytes.decode(\'utf-8\') return message @staticmethod def encode_to_urlsafe_base64(data: str) -> str: Encodes a string to a URL-safe Base64 encoded string. bytes_data = data.encode(\'utf-8\') base64_bytes = base64.urlsafe_b64encode(bytes_data) base64_string = base64_bytes.decode(\'utf-8\') return base64_string @staticmethod def decode_from_urlsafe_base64(encoded_data: str) -> str: Decodes a URL-safe Base64 encoded string back to the original string. base64_bytes = encoded_data.encode(\'utf-8\') message_bytes = base64.urlsafe_b64decode(base64_bytes) message = message_bytes.decode(\'utf-8\') return message"},{"question":"You have been asked to create a utility that gathers and verifies various pieces of system information to ensure compatibility with a specific set of requirements for a deployment environment. You will need to implement a function `check_system_compatibility()` that uses the `platform` module to retrieve and validate system details. Function Signature ```python def check_system_compatibility() -> dict: pass ``` Requirements 1. The machine type must be `\'AMD64\'` or `\'x86_64\'`. 2. The OS name must be either `\'Linux\'` or `\'Darwin\'`. 3. The Python version must be at least 3.8.0. 4. The system must have a 64-bit architecture. 5. The processor name, if available, must start with `\'Intel\'` or `\'AMD\'`. Implementation Details - Use the `platform` module to retrieve the necessary information. - Return a dictionary with the following keys and their associated validation results: - `machine_type`: Boolean indicating if the machine type is valid. - `os_name`: Boolean indicating if the OS name is valid. - `python_version`: Boolean indicating if the Python version is valid. - `architecture`: Boolean indicating if the architecture is 64-bit. - `processor_name`: Boolean indicating if the processor name is valid (or if it could not be determined). Example Output ```python { \\"machine_type\\": True, \\"os_name\\": True, \\"python_version\\": True, \\"architecture\\": True, \\"processor_name\\": True } ``` Additional Notes - You can assume that the `platform` module will be available and does not need to be installed separately. - Handle any unexpected errors gracefully and provide meaningful feedback within the returned dictionary (e.g., return `False` for checks that cannot be performed due to unexpected errors). Constraints - The function should run efficiently and should not make any external system calls apart from those in the `platform` module. - Focus on robustness and clarity of the code, ensuring it covers all mentioned requirements accurately.","solution":"import platform import sys def check_system_compatibility() -> dict: compatibility = { \\"machine_type\\": False, \\"os_name\\": False, \\"python_version\\": False, \\"architecture\\": False, \\"processor_name\\": False } # Check machine type machine_type = platform.machine() if machine_type in [\'AMD64\', \'x86_64\']: compatibility[\\"machine_type\\"] = True # Check OS name os_name = platform.system() if os_name in [\'Linux\', \'Darwin\']: compatibility[\\"os_name\\"] = True # Check Python version python_version = platform.python_version_tuple() if (int(python_version[0]) > 3) or (int(python_version[0]) == 3 and int(python_version[1]) >= 8): compatibility[\\"python_version\\"] = True # Check architecture architecture = platform.architecture()[0] if \'64bit\' in architecture: compatibility[\\"architecture\\"] = True # Check processor name processor_name = platform.processor() if processor_name.startswith(\'Intel\') or processor_name.startswith(\'AMD\'): compatibility[\\"processor_name\\"] = True return compatibility"},{"question":"# Python Coding Assessment Question: Objective: To assess students\' understanding of exception handling, stack trace extraction, and formatting using the `traceback` module in Python. Problem Statement: You are tasked with creating a function that executes another function and, in case of an exception, records and returns detailed information about the exception and stack trace. Task: Write a function called `execute_with_traceback` that takes another function `func` and a list of arguments `args` as parameters, executes the `func` with the provided `args`, and handles any exceptions that occur. If an exception occurs, the function should return a dictionary containing: - The type of exception. - The exception message. - The formatted stack trace leading up to the exception. - The formatted exception information. If no exception occurs, return the result of the function `func` execution. Input: - `func`: The function to be executed. - `args`: A list of arguments for the function `func`. Output: - A dictionary with the keys: - `\'exception_type\'`: The type of exception (if any). - `\'exception_message\'`: The message of the exception (if any). - `\'stack_trace\'`: The formatted stack trace (if any). - `\'exception_info\'`: The formatted exception information (if any). - `result`: The result of the function execution (if no exception occurs). Constraints: - The function `func` will be a callable. - The list `args` will contain the arguments needed by `func`. - The implementation should utilize the `traceback` module for stack trace extraction and formatting. Example Usage: ```python import traceback def sample_function(x, y): return x / y def execute_with_traceback(func, args): try: result = func(*args) return {\'result\': result} except Exception as e: exc_type, exc_value, exc_traceback = type(e), e, e.__traceback__ return { \'exception_type\': exc_type.__name__, \'exception_message\': str(exc_value), \'stack_trace\': \'\'.join(traceback.format_tb(exc_traceback)), \'exception_info\': \'\'.join(traceback.format_exception(exc_type, exc_value, exc_traceback)), } # Running with no exception print(execute_with_traceback(sample_function, [4, 2])) # Running with exception print(execute_with_traceback(sample_function, [4, 0])) ``` Expected Output: For the first call: ```python {\'result\': 2.0} ``` For the second call (dividing by zero): ```python { \'exception_type\': \'ZeroDivisionError\', \'exception_message\': \'division by zero\', \'stack_trace\': \'Traceback (most recent call last):n File \\"example.py\\", line X, in execute_with_tracebackn result = func(*args)n File \\"example.py\\", line Y, in sample_functionn return x / ynZeroDivisionError: division by zeron\', \'exception_info\': \'Traceback (most recent call last):n File \\"example.py\\", line X, in execute_with_tracebackn result = func(*args)n File \\"example.py\\", line Y, in sample_functionn return x / ynZeroDivisionError: division by zeron\' } ``` Notes: - Ensure proper handling and formatting of exceptions using the `traceback` module. - Students are encouraged to handle any kind of exception and provide detailed and formatted traceback information.","solution":"import traceback def execute_with_traceback(func, args): Executes a function with the provided arguments, and if an exception occurs, it captures and returns detailed exception information including the stack trace. Parameters: func (callable): The function to be executed. args (list): The arguments to be passed to the function. Returns: dict: A dictionary containing the result or exception details. try: result = func(*args) return {\'result\': result} except Exception as e: exc_type, exc_value, exc_traceback = type(e), e, e.__traceback__ return { \'exception_type\': exc_type.__name__, \'exception_message\': str(exc_value), \'stack_trace\': \'\'.join(traceback.format_tb(exc_traceback)), \'exception_info\': \'\'.join(traceback.format_exception(exc_type, exc_value, exc_traceback)), }"},{"question":"**Coding Assessment Question** # Objective Your task is to implement a dynamic module loader using `importlib` that can load a module from a given file path and execute a specified function within that module. This will test your understanding of dynamic importing and executing functions from dynamically imported modules. # Problem Statement 1. Implement a function called `dynamic_import_and_run`. 2. This function should take two arguments: - `file_path`: A string representing the path to a Python file (e.g., `/path/to/module.py`). - `func_name`: A string representing the name of the function within the module to execute. 3. The function should perform the following: - Dynamically import the module from the given file path. - Execute the specified function within that module. - Return the result of the function execution. # Function Signature ```python def dynamic_import_and_run(file_path: str, func_name: str) -> any: # Your implementation here ``` # Example Consider a Python file `example_module.py` with the following content: ```python # example_module.py def greet(name): return f\\"Hello, {name}!\\" ``` If the `file_path` is `/path/to/example_module.py` and `func_name` is `greet`, calling `dynamic_import_and_run(file_path, func_name)` should return `Hello, World!` when called as `dynamic_import_and_run(\'/path/to/example_module.py\', \'greet\')(\'World\')`. # Constraints 1. Assume that the module files and their paths are valid. 2. The function within the module will take at most one argument. 3. You may use the `importlib` module to facilitate dynamic importing. # Hints - Use `importlib.util.spec_from_file_location` and `importlib.util.module_from_spec` to dynamically load a module from a given file path. - To execute the function, you might need to use `getattr` to fetch the function object from the loaded module. # Submission Submit your implementation of the `dynamic_import_and_run` function. Ensure your code is well-documented and follows Python best practices.","solution":"import importlib.util def dynamic_import_and_run(file_path: str, func_name: str): Dynamically imports a module from a given file path and executes a specified function from that module. :param file_path: Path to the Python file. :param func_name: Name of the function to execute within the module. :return: Result of the function execution. # Load the module spec = importlib.util.spec_from_file_location(\\"dynamic_module\\", file_path) module = importlib.util.module_from_spec(spec) spec.loader.exec_module(module) # Get the function from the module func = getattr(module, func_name) # Call the function and return its result return func"},{"question":"Context Management and Context Variables Objective Your task is to write a Python function that demonstrates the creation and management of context variables using the `contextvars` module. The function should create two context variables, set values in different contexts, switch between contexts, and retrieve the values to illustrate context isolation. Problem Statement Write a function `manage_context_vars()` in Python that performs the following steps: 1. Create two context variables named `var1` and `var2`. 2. Create a new context and set `var1` to \\"Context 1 - Var1\\" and `var2` to \\"Context 1 - Var2\\" within this context. 3. Switch to a new context and set `var1` to \\"Context 2 - Var1\\". Leave `var2` unset in this context. 4. Retrieve and print the values of `var1` and `var2` in the second context. 5. Switch back to the first context and retrieve and print the values of `var1` and `var2`. Constraints - Use the `contextvars` module in Python. - Ensure context isolation: changes in one context should not affect the other context. - Handle any necessary exceptions. Input and Output Constraints - This function does not take any input. - The function should print the values of `var1` and `var2` in both contexts. Example Output: ```python Context 2 - var1: Context 2 - Var1 Context 2 - var2: None Context 1 - var1: Context 1 - Var1 Context 1 - var2: Context 1 - Var2 ``` Function Signature ```python def manage_context_vars(): pass ``` Guidelines 1. Use the `contextvars.ContextVar` to create context variables. 2. Use `contextvars.Context` for creating and switching between contexts. 3. Ensure to catch and handle exceptions where necessary. Explain your implementation approach and ensure your code is well-commented to reflect the logical steps taken.","solution":"import contextvars def manage_context_vars(): # Step 1: Create two context variables var1 = contextvars.ContextVar(\'var1\') var2 = contextvars.ContextVar(\'var2\') # Step 2: Create a new context and set variable values in it context1 = contextvars.copy_context() context1.run(var1.set, \\"Context 1 - Var1\\") context1.run(var2.set, \\"Context 1 - Var2\\") # Step 3: Create another new context and set variable values in it context2 = contextvars.copy_context() context2.run(var1.set, \\"Context 2 - Var1\\") # Note: Leaving var2 unset in context2 # Step 4: Retrieve and print var1 and var2 values in context2 var1_value_context2 = context2.run(var1.get, None) var2_value_context2 = context2.run(var2.get, None) print(f\\"Context 2 - var1: {var1_value_context2}\\") print(f\\"Context 2 - var2: {var2_value_context2}\\") # Step 5: Switch back to context1 and retrieve and print var1 and var2 values var1_value_context1 = context1.run(var1.get, None) var2_value_context1 = context1.run(var2.get, None) print(f\\"Context 1 - var1: {var1_value_context1}\\") print(f\\"Context 1 - var2: {var2_value_context1}\\")"},{"question":"**Title: Optimizing Neural Network Forward Pass with TorchScript** **Objective:** Design and implement a simple neural network in PyTorch, and then optimize its forward pass using TorchScript. This exercise will test your understanding of basic PyTorch operations and your ability to use TorchScript to enhance performance. **Task:** 1. **Define a Simple Neural Network:** - Create a neural network class `SimpleNet` in PyTorch that contains: - An input layer that accepts data of shape `(batch_size, input_features)`. - One hidden layer. - An output layer that produces logits for `num_classes`. - Use ReLU activation for the hidden layer. 2. **Convert to TorchScript:** - Use TorchScript to optimize the forward pass of the network. - Implement a function `optimize_model(model: nn.Module) -> torch.jit.ScriptModule` that takes the PyTorch model and returns an optimized TorchScript version. **Requirements:** - The network should use basic `nn.Module` and layers from `torch.nn`. - The forward method of the model should support both the pure Python and TorchScript execution. - Ensure that the optimized model maintains the same output as the original model given the same input. ```python import torch import torch.nn as nn import torch.nn.functional as F import torch.optim as optim class SimpleNet(nn.Module): def __init__(self, input_features, hidden_units, num_classes): super(SimpleNet, self).__init__() self.fc1 = nn.Linear(input_features, hidden_units) self.relu = nn.ReLU() self.fc2 = nn.Linear(hidden_units, num_classes) def forward(self, x): x = self.fc1(x) x = self.relu(x) x = self.fc2(x) return x def optimize_model(model: nn.Module) -> torch.jit.ScriptModule: Converts the given PyTorch model to a TorchScript optimized module. Args: model (nn.Module): The neural network model to optimize. Returns: torch.jit.ScriptModule: The TorchScript optimized model. # Write your code here # Example replacement: # scripted_model = torch.jit.script(model) # return scripted_model pass # Example usage (not part of the solution, shown here for clarity): # model = SimpleNet(input_features=10, hidden_units=5, num_classes=2) # scripted_model = optimize_model(model) # dummy_input = torch.rand(1, 10) # Dummy input for tracing # print(scripted_model(dummy_input)) # This should output the same values as model(dummy_input) ``` **Input:** - `input_features`, `hidden_units`, `num_classes`: Integers denoting the respective sizes of model layers. - An instance of the `SimpleNet` neural network model. **Output:** - A TorchScript optimized version of the `SimpleNet` model. **Constraints:** - Ensure the optimized model works on both CPU and GPU. - Validate that the conversion does not significantly alter the computational graph structure or behavior. - The forward pass performance should be improved or at least equivalent compared to the original model. **Notes:** - This task requires a solid understanding of PyTorch, TorchScript, and neural network concepts. - Pay close attention to the limitations of TorchScript as mentioned in the document when implementing the solution. **Performance Requirements:** - The forward pass of the optimized model should be tested for performance gains (if applicable) using a sufficiently large input tensor.","solution":"import torch import torch.nn as nn import torch.jit as jit class SimpleNet(nn.Module): def __init__(self, input_features, hidden_units, num_classes): super(SimpleNet, self).__init__() self.fc1 = nn.Linear(input_features, hidden_units) self.relu = nn.ReLU() self.fc2 = nn.Linear(hidden_units, num_classes) def forward(self, x): x = self.fc1(x) x = self.relu(x) x = self.fc2(x) return x def optimize_model(model: nn.Module) -> torch.jit.ScriptModule: Converts the given PyTorch model to a TorchScript optimized module. Args: model (nn.Module): The neural network model to optimize. Returns: torch.jit.ScriptModule: The TorchScript optimized model. scripted_model = jit.script(model) return scripted_model"},{"question":"**Question: Implement a Boolean Handling Function** You are asked to write a function that demonstrates the handling of boolean values and their conversion from integer inputs. Your task is to create a function `int_to_bool_representation` which takes an integer as input and returns a string indicating whether the input is treated as `True` or `False` in Python. # Function Signature ```python def int_to_bool_representation(value: int) -> str: ``` # Input - `value` (int): An integer value that you need to evaluate its boolean representation. # Output - `str`: A string \\"True\\" if the integer value is treated as `True` in Python, otherwise \\"False\\". # Requirements - You must utilize the boolean type conversion rules in Python. - Ensure that the function returns strings exactly as specified: \\"True\\" or \\"False\\". # Example ```python print(int_to_bool_representation(1)) # should return \\"True\\" print(int_to_bool_representation(0)) # should return \\"False\\" print(int_to_bool_representation(42)) # should return \\"True\\" print(int_to_bool_representation(-1)) # should return \\"True\\" ``` # Constraints - You can assume that the input will always be a valid integer. - The function should be efficient and handle both positive and negative integers, including zero. In your implementation, consider how Python inherently treats integer values when converting them to boolean. Demonstrate your understanding by returning the appropriate string representation. **Note:** You do not need to handle reference counts explicitly in your implementation, but focus on capturing the correct boolean interpretation of the given integer.","solution":"def int_to_bool_representation(value: int) -> str: Converts an integer to its boolean representation in string format. Parameters: - value (int): The integer to convert. Returns: - str: \\"True\\" if value is treated as True in Python, otherwise \\"False\\". return \\"True\\" if bool(value) else \\"False\\""},{"question":"# Bytearray Operations Challenge Introduction In this exercise, you are required to implement multiple functions to demonstrate the creation and manipulation of `bytearray` objects in Python. Tasks 1. **bytearray_from_string**: Create a function that takes a string and returns a `bytearray` object. ```python def bytearray_from_string(s: str) -> bytearray: Converts a string to a bytearray. :param s: input string :return: bytearray representation of the input string pass ``` 2. **concat_bytearrays**: Create a function that takes two `bytearray` objects and concatenates them, returning the resulting `bytearray`. ```python def concat_bytearrays(a: bytearray, b: bytearray) -> bytearray: Concatenates two bytearray objects. :param a: first bytearray :param b: second bytearray :return: concatenated bytearray pass ``` 3. **get_bytearray_size**: Create a function that takes a `bytearray` and returns its size. ```python def get_bytearray_size(ba: bytearray) -> int: Returns the size of the bytearray. :param ba: input bytearray :return: size of the bytearray pass ``` 4. **convert_to_string**: Create a function that takes a `bytearray` and returns its string representation. ```python def convert_to_string(ba: bytearray) -> str: Converts a bytearray to its string representation. :param ba: input bytearray :return: string representation of bytearray pass ``` 5. **resize_bytearray**: Create a function that takes a `bytearray` and an integer `new_size`, and resizes the `bytearray` to `new_size`. ```python def resize_bytearray(ba: bytearray, new_size: int): Resizes the bytearray to a new size. :param ba: input bytearray :param new_size: new size for the bytearray pass ``` Constraints - The input string for `bytearray_from_string` will contain only printable ASCII characters. - Both `bytearrays` for `concat_bytearrays` will be smaller than 10^6 bytes in size. - The size for `get_bytearray_size` will not exceed 10^6. - The new size in `resize_bytearray` will be between 0 and 10^6. Example Usage ```python # Task 1 ba = bytearray_from_string(\\"hello\\") assert ba == bytearray(b\'hello\') # Task 2 ba1 = bytearray(b\'hello\') ba2 = bytearray(b\'world\') assert concat_bytearrays(ba1, ba2) == bytearray(b\'helloworld\') # Task 3 ba = bytearray(b\'hello\') assert get_bytearray_size(ba) == 5 # Task 4 ba = bytearray(b\'hello\') assert convert_to_string(ba) == \'hello\' # Task 5 ba = bytearray(b\'hello\') resize_bytearray(ba, 3) assert ba == bytearray(b\'hel\') ``` Please implement all the required functions to demonstrate your understanding and proficiency with handling bytearray objects in Python.","solution":"def bytearray_from_string(s: str) -> bytearray: Converts a string to a bytearray. :param s: input string :return: bytearray representation of the input string return bytearray(s, \'utf-8\') def concat_bytearrays(a: bytearray, b: bytearray) -> bytearray: Concatenates two bytearray objects. :param a: first bytearray :param b: second bytearray :return: concatenated bytearray return a + b def get_bytearray_size(ba: bytearray) -> int: Returns the size of the bytearray. :param ba: input bytearray :return: size of the bytearray return len(ba) def convert_to_string(ba: bytearray) -> str: Converts a bytearray to its string representation. :param ba: input bytearray :return: string representation of bytearray return ba.decode(\'utf-8\') def resize_bytearray(ba: bytearray, new_size: int): Resizes the bytearray to a new size. :param ba: input bytearray :param new_size: new size for the bytearray ba[:] = ba[:new_size]"},{"question":"# Pandas Advanced: Custom Extension Array Implementation Pandas allows users to extend its functionality by creating custom data types and extending dataframe and series objects. In this task, you are required to create a custom extension array to work with binary data efficiently. Your task is to implement a custom binary extension array called `BinaryArray`, which will allow pandas to handle binary data (represented as strings of \\"0\\" and \\"1\\") in a specialized way. # Requirements 1. **Class Definition**: - Create a class `BinaryArray` that inherits from `pandas.api.extensions.ExtensionArray`. 2. **Attributes**: - `data`: A list of binary strings (e.g., `[\'1010\', \'0101\']`). 3. **Methods**: - `__init__(self, data)`: Initialize the binary array with a list of binary strings. - `__getitem__(self, item)`: Return the binary string at the given index. - `__len__(self)`: Return the length of the array. - `isna(self)`: Return a boolean array indicating if each value is `None`. - `take(self, indices, allow_fill=False, fill_value=None)`: Return a new `BinaryArray` based on given indices. - `astype(self, dtype, copy=True)`: Convert the binary array to another dtype. - `unique(self)`: Return a unique `BinaryArray`. - `to_numpy(self, dtype=None, copy=False)`: Convert the binary array to a numpy array. - `fillna(self, value)`: Fill `None` values with a specified binary string. 4. **Additional Requirements**: - Implement the `dtype` attribute to return a custom `BinaryDtype` object. - Implement the `nbytes` attribute to return the number of bytes used by the array. - Support pandas interoperability (e.g., method chaining with pandas DataFrame). # Example Usage ```python import pandas as pd from pandas.api.extensions import ExtensionDtype, register_extension_dtype class BinaryDtype(ExtensionDtype): name = \'binary_type\' type = str kind = \'O\' na_value = None @classmethod def construct_array_type(cls): return BinaryArray class BinaryArray(pd.api.extensions.ExtensionArray): def __init__(self, data): self.data = data def __getitem__(self, item): return self.data[item] def __len__(self): return len(self.data) def isna(self): return [x is None for x in self.data] def take(self, indices, allow_fill=False, fill_value=None): if allow_fill: return BinaryArray([self.data[i] if i != -1 else fill_value for i in indices]) return BinaryArray([self.data[i] for i in indices]) def astype(self, dtype, copy=True): return pd.Series(self.data).astype(dtype) def unique(self): return BinaryArray(list(set(self.data))) def to_numpy(self, dtype=None, copy=False): return np.array(self.data, dtype=dtype, copy=copy) def fillna(self, value): return BinaryArray([x if x is not None else value for x in self.data]) @property def dtype(self): return BinaryDtype() @property def nbytes(self): return sum(len(x) for x in self.data) # Example usage data = [\'1010\', \'0101\', \'1010\', None] binary_array = BinaryArray(data) print(binary_array.unique()) ``` # Constraints - Binary strings can only contain \'0\' and \'1\' characters. - Methods should handle the presence of `None` values appropriately. - The `BinaryArray` should be able to integrate seamlessly with pandas DataFrames and Series. # Evaluation Your solution will be evaluated based on: 1. Correct implementation of the `BinaryArray` and `BinaryDtype` classes. 2. Correct functionality of the required methods. 3. Integration with pandas DataFrames and Series. 4. Handling of edge cases and input constraints.","solution":"import numpy as np import pandas as pd from pandas.api.extensions import ExtensionArray, ExtensionDtype, register_extension_dtype @register_extension_dtype class BinaryDtype(ExtensionDtype): name = \'binary_type\' type = str kind = \'O\' na_value = None @classmethod def construct_array_type(cls): return BinaryArray class BinaryArray(ExtensionArray): def __init__(self, data): self.data = data def __getitem__(self, item): return self.data[item] def __len__(self): return len(self.data) def isna(self): return [x is None for x in self.data] def take(self, indices, allow_fill=False, fill_value=None): if allow_fill: return BinaryArray([self.data[i] if i != -1 else fill_value for i in indices]) return BinaryArray([self.data[i] for i in indices]) def astype(self, dtype, copy=True): return np.array(self.data).astype(dtype, copy=copy) def unique(self): return BinaryArray(list(set(self.data))) def to_numpy(self, dtype=None, copy=False): return np.array(self.data, dtype=dtype, copy=copy) def fillna(self, value): return BinaryArray([x if x is not None else value for x in self.data]) @property def dtype(self): return BinaryDtype() @property def nbytes(self): return sum(len(x) for x in self.data if x is not None)"},{"question":"Objective Your task is to demonstrate your understanding of the PyTorch MPS backend by performing the following steps: 1. Check if the MPS backend is available on your system. 2. If available, create a tensor on the MPS device and perform some basic operations. 3. Define a simple neural network model and move it to the MPS device. 4. Use the model to make predictions on the tensor. 5. Implement and test the functions described below. Functions to Implement 1. **check_mps_availability** ```python def check_mps_availability(): Check if the MPS backend is available (enabled and supported). Returns: bool: True if MPS is available, False otherwise. pass ``` 2. **create_mps_tensor** ```python def create_mps_tensor(size): Create a tensor of the specified size filled with ones on the MPS device. Args: size (int): Size of the tensor. Returns: torch.Tensor: Tensor of the specified size filled with ones on the MPS device. If MPS is not available, return None. pass ``` 3. **define_simple_model** ```python import torch.nn as nn class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.fc = nn.Linear(5, 1) def forward(self, x): return self.fc(x) ``` 4. **move_model_to_device** ```python def move_model_to_device(model): Move the provided model to the MPS device. Args: model (nn.Module): The neural network model to move to the MPS device. Returns: nn.Module: The model moved to the MPS device, or the original model if MPS is not available. pass ``` 5. **predict_with_model** ```python def predict_with_model(model, tensor): Use the provided model to make predictions on the provided tensor. Args: model (nn.Module): The neural network model. tensor (torch.Tensor): The input tensor. Returns: torch.Tensor: The model predictions. pass ``` Example Usage ```python if __name__ == \\"__main__\\": # Check MPS availability if check_mps_availability(): print(\\"MPS is available.\\") else: print(\\"MPS is not available.\\") # Create a tensor on the MPS device tensor = create_mps_tensor(5) print(tensor) # Define and move model to MPS device model = SimpleModel() model = move_model_to_device(model) # Perform a prediction if tensor is not None: predictions = predict_with_model(model, tensor) print(predictions) ``` Constraints - Assume the tensor size will always be an integer greater than zero. - The neural network model is a simple linear model with an input size of 5 and an output size of 1. Performance Requirements - The functions should handle the presence and absence of the MPS backend gracefully. - The operations should leverage the MPS backend for computations if available, ensuring efficient utilization of the GPU for macOS devices.","solution":"import torch import torch.nn as nn def check_mps_availability(): Check if the MPS backend is available (enabled and supported). Returns: bool: True if MPS is available, False otherwise. return torch.backends.mps.is_available() and torch.backends.mps.is_built() def create_mps_tensor(size): Create a tensor of the specified size filled with ones on the MPS device. Args: size (int): Size of the tensor. Returns: torch.Tensor: Tensor of the specified size filled with ones on the MPS device. If MPS is not available, return None. if check_mps_availability(): return torch.ones(size, device=\'mps\') else: return None class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.fc = nn.Linear(5, 1) def forward(self, x): return self.fc(x) def move_model_to_device(model): Move the provided model to the MPS device. Args: model (nn.Module): The neural network model to move to the MPS device. Returns: nn.Module: The model moved to the MPS device, or the original model if MPS is not available. if check_mps_availability(): return model.to(\'mps\') else: return model def predict_with_model(model, tensor): Use the provided model to make predictions on the provided tensor. Args: model (nn.Module): The neural network model. tensor (torch.Tensor): The input tensor. Returns: torch.Tensor: The model predictions. return model(tensor)"},{"question":"# Timedelta Manipulation and Analysis Objective Your task is to implement several functions that test your comprehension of the `Timedelta` class and its operations in pandas. You will be required to create Timedelta objects, perform arithmetic operations, handle missing values, and convert between different time units. Task 1: Construct Timedelta Implement a function `construct_timedelta` that takes a list of strings representing timedeltas and returns a pandas `TimedeltaIndex`. **Input:** - `timedelta_strings` (List[str]): A list of strings where each string represents a timedelta. **Output:** - `pd.TimedeltaIndex`: A pandas TimedeltaIndex constructed from the input list. ```python def construct_timedelta(timedelta_strings: List[str]) -> pd.TimedeltaIndex: pass ``` Task 2: Add Timedelta to Timestamps Implement a function `add_timedelta_to_timestamps` that adds a given Timedelta to a series of Timestamps. **Input:** - `timestamps` (pd.Series): A pandas Series where each element is a Timestamp. - `delta` (str): A string representing a Timedelta. **Output:** - `pd.Series`: A pandas Series with the resulting Timestamps after adding the Timedelta. ```python def add_timedelta_to_timestamps(timestamps: pd.Series, delta: str) -> pd.Series: pass ``` Task 3: Find Timedelta Range Implement a function `timedelta_range` that returns a range of Timedelta objects between a start and end duration. **Input:** - `start` (str): A string representing the start Timedelta. - `end` (str): A string representing the end Timedelta. - `freq` (str): A string representing the frequency of the range. **Output:** - `pd.TimedeltaIndex`: A pandas TimedeltaIndex representing the range. ```python def timedelta_range(start: str, end: str, freq: str) -> pd.TimedeltaIndex: pass ``` Task 4: Convert Timedeltas to Seconds Implement a function `timedelta_to_seconds` that converts a series of Timedelta objects to seconds and returns as a pandas Series. **Input:** - `timedeltas` (pd.Series): A pandas Series of Timedelta objects. **Output:** - `pd.Series`: A pandas Series of timedelta values converted to seconds. ```python def timedelta_to_seconds(timedeltas: pd.Series) -> pd.Series: pass ``` Task 5: Fill Missing Timedelta Values Implement a function `fill_missing_timedelta` that fills missing (NaT) values in a series of Timedelta objects with a specified Timedelta value. **Input:** - `timedeltas` (pd.Series): A pandas Series of Timedelta objects with possible NaT values. - `fill_value` (str): A string representing the Timedelta value to fill in for NaT. **Output:** - `pd.Series`: A pandas Series where NaT values have been replaced with the specified Timedelta. ```python def fill_missing_timedelta(timedeltas: pd.Series, fill_value: str) -> pd.Series: pass ``` # Constraints - The strings provided in the input will be valid representations of Timedelta and Timestamp objects. - If a string is not a valid Timedelta or Timestamp format, raise a ValueError with a descriptive message. # Performance Requirements - Ensure that the implemented functions run efficiently and make use of vectorized operations provided by pandas where applicable.","solution":"import pandas as pd from typing import List def construct_timedelta(timedelta_strings: List[str]) -> pd.TimedeltaIndex: Constructs a pandas TimedeltaIndex from a list of timedelta strings. :param timedelta_strings: A list of strings representing timedeltas. :return: A pandas TimedeltaIndex. try: return pd.to_timedelta(timedelta_strings) except Exception as e: raise ValueError(f\\"Invalid timedelta format: {e}\\") def add_timedelta_to_timestamps(timestamps: pd.Series, delta: str) -> pd.Series: Adds a given Timedelta to a series of Timestamps. :param timestamps: A pandas Series where each element is a Timestamp. :param delta: A string representing a Timedelta. :return: A pandas Series with the resulting Timestamps after adding the Timedelta. try: timedelta = pd.to_timedelta(delta) return timestamps + timedelta except Exception as e: raise ValueError(f\\"Invalid timestamp or timedelta format: {e}\\") def timedelta_range(start: str, end: str, freq: str) -> pd.TimedeltaIndex: Returns a range of Timedelta objects between a start and end duration. :param start: A string representing the start Timedelta. :param end: A string representing the end Timedelta. :param freq: A string representing the frequency of the range. :return: A pandas TimedeltaIndex representing the range. try: return pd.timedelta_range(start=start, end=end, freq=freq) except Exception as e: raise ValueError(f\\"Invalid input parameters: {e}\\") def timedelta_to_seconds(timedeltas: pd.Series) -> pd.Series: Converts a series of Timedelta objects to seconds and returns as a pandas Series. :param timedeltas: A pandas Series of Timedelta objects. :return: A pandas Series of timedelta values converted to seconds. try: return timedeltas.dt.total_seconds() except Exception as e: raise ValueError(f\\"Invalid Timedelta objects: {e}\\") def fill_missing_timedelta(timedeltas: pd.Series, fill_value: str) -> pd.Series: Fills missing (NaT) values in a series of Timedelta objects with a specified Timedelta value. :param timedeltas: A pandas Series of Timedelta objects with possible NaT values. :param fill_value: A string representing the Timedelta value to fill in for NaT. :return: A pandas Series where NaT values have been replaced with the specified Timedelta. try: fill_timdelta = pd.to_timedelta(fill_value) return timedeltas.fillna(fill_timdelta) except Exception as e: raise ValueError(f\\"Invalid Timedelta or fill value: {e}\\")"},{"question":"# Question: Implement Custom Serialization Using `pickle` Background You are working on a project that involves serializing Python objects to a binary format for storage and later retrieval. You are required to handle a special class `Note` that has attributes which should be selectively serialized. Specifically, you need to exclude the attribute `timestamp` during serialization because it is dynamically generated. In addition, you need to implement a mechanism that can load these custom serialized objects back into the Python program. Objective 1. Implement a class `Note` with the following attributes: - `title`: String, title of the note. - `content`: String, content of the note. - `timestamp`: Integer, a dynamically generated UNIX timestamp. 2. Implement custom serialization and deserialization using `pickle`: - Create a custom `NotePickler` class that extends `pickle.Pickler`. - Override necessary methods to exclude the `timestamp` attribute during serialization. - Create a custom `NoteUnpickler` class that extends `pickle.Unpickler`. - Override necessary methods to correctly deserialize the `Note` objects. Requirements - Implement two functions `serialize_notes` and `deserialize_notes`. - **Function: `serialize_notes(notes, file_path)`**: - Input: A list of `Note` objects and a file path to store the serialized data. - Output: None (write serialized data to the specified file). - **Function: `deserialize_notes(file_path)`**: - Input: A file path containing the serialized data. - Output: A list of deserialized `Note` objects with their dynamic `timestamp` set to the current time. - Use a protocol version that ensures compatibility with recent versions of Python. Constraints 1. You cannot modify the original `Note` class definition except for adding special methods related to pickling. 2. Handle security concerns by ensuring only the `Note` class can be deserialized. Example Usage ```python import time # Define the Note class class Note: ... # Your implementation here # Sample notes note1 = Note( title=\\"Shopping List\\", content=\\"Eggs, Milk, Bread\\", timestamp=int(time.time()) ) note2 = Note( title=\\"Todo\\", content=\\"Complete homework\\", timestamp=int(time.time()) ) notes = [note1, note2] # Serialize notes to a file serialize_notes(notes, \'notes.pkl\') # Later, deserialize notes from the file deserialized_notes = deserialize_notes(\'notes.pkl\') for note in deserialized_notes: print(note.title, note.content, note.timestamp) ``` Implementation Notes - Use `pickle` module for serialization and deserialization. - Ensure that deserialized `Note` objects have the current timestamp set correctly. Evaluation Criteria - Correct implementation of the `Note` class. - Proper customization of the `NotePickler` and `NoteUnpickler` classes. - Handling of the `timestamp` attribute correctly. - Adherence to security practices, ensuring only safe deserialization.","solution":"import pickle import time class Note: def __init__(self, title, content, timestamp=None): self.title = title self.content = content self.timestamp = timestamp if timestamp else int(time.time()) def __getstate__(self): state = self.__dict__.copy() # Remove the timestamp attribute during serialization if \'timestamp\' in state: del state[\'timestamp\'] return state def __setstate__(self, state): self.__dict__.update(state) # Assign current time to timestamp during deserialization self.timestamp = int(time.time()) class NotePickler(pickle.Pickler): def persistent_id(self, obj): if isinstance(obj, Note): return (\'Note\', obj.__getstate__()) else: return None class NoteUnpickler(pickle.Unpickler): def persistent_load(self, pid): type_tag, state = pid if type_tag == \'Note\': note = Note(title=None, content=None) # create an empty note note.__setstate__(state) return note else: raise pickle.UnpicklingError(\\"unsupported persistent object\\") def serialize_notes(notes, file_path): with open(file_path, \'wb\') as file: pickler = NotePickler(file, protocol=pickle.HIGHEST_PROTOCOL) pickler.dump(notes) def deserialize_notes(file_path): with open(file_path, \'rb\') as file: unpickler = NoteUnpickler(file) notes = unpickler.load() return notes"},{"question":"**Question: Creating, Parsing, and Modifying Email Messages** You are required to create a program that demonstrates the use of the `email` package to handle email messages. **Task**: 1. **Email Construction**: - Create a new email message with the following details: - From: `from@example.com` - To: `to@example.com` - Subject: `Test Email` - Body: `This is a test email.` - Add a plain text attachment to this email with the content: `Attachment Text Content`. 2. **Email Parsing**: - Given a serialized email string, parse it into an `EmailMessage` object. - Modify the subject of this parsed email to: `Modified Subject`. 3. **Email Generation**: - Serialize both the initially created email and the modified email back into string form. **Input**: - A serialized email string. **Output**: - The serialized string of the initially created email. - The serialized string of the modified email. **Constraints**: - Use the functionality provided by the `email` package. - Ensure that the MIME types and email structure comply with RFC standards. **Performance Requirements**: - The operations should be efficient and handle typical email sizes. **Example**: ```python from email.message import EmailMessage from email.parser import Parser from email.policy import default def create_email(): msg = EmailMessage() msg[\'From\'] = \'from@example.com\' msg[\'To\'] = \'to@example.com\' msg[\'Subject\'] = \'Test Email\' msg.set_content(\'This is a test email.\') # Adding attachment attachment_content = \'Attachment Text Content\' msg.add_attachment(attachment_content, subtype=\'plain\', filename=\'test.txt\') return msg def parse_and_modify_email(serialized_email): parser = Parser(policy=default) parsed_msg = parser.parsestr(serialized_email) # Modify the subject parsed_msg[\'Subject\'] = \'Modified Subject\' return parsed_msg def serialize_email(msg): return msg.as_string() # Example usage initial_email = create_email() serialized_initial_email = serialize_email(initial_email) # Assume \'some_serialized_email\' is given input parsed_modified_email = parse_and_modify_email(some_serialized_email) serialized_modified_email = serialize_email(parsed_modified_email) print(serialized_initial_email) print(serialized_modified_email) ```","solution":"from email.message import EmailMessage from email.parser import Parser from email.policy import default def create_email(): msg = EmailMessage() msg[\'From\'] = \'from@example.com\' msg[\'To\'] = \'to@example.com\' msg[\'Subject\'] = \'Test Email\' msg.set_content(\'This is a test email.\') # Adding attachment attachment_content = \'Attachment Text Content\' msg.add_attachment(attachment_content, filename=\'attachment.txt\') return msg def parse_and_modify_email(serialized_email): parser = Parser(policy=default) parsed_msg = parser.parsestr(serialized_email) # Modify the subject parsed_msg.replace_header(\'Subject\', \'Modified Subject\') return parsed_msg def serialize_email(msg): return msg.as_string() # Example of serialized email (to be passed to parse_and_modify_email function) example_email_str = From: from@example.com To: to@example.com Subject: Test Email This is a test email. # Example usage initial_email = create_email() serialized_initial_email = serialize_email(initial_email) parsed_modified_email = parse_and_modify_email(example_email_str) serialized_modified_email = serialize_email(parsed_modified_email)"},{"question":"Question: Priority Task Scheduler with Dynamic Updates # Objective: Implement a priority task scheduler that allows you to add tasks with priorities, update task priorities, remove tasks, and retrieve tasks in priority order. You need to maintain the heap invariant efficiently while handling these operations. # Implementation Details: You are required to implement the following functions: 1. `add_task(pq, entry_finder, task, priority=0)`: Adds a new task or updates the priority of an existing task. If the task already exists, it updates the priority. Otherwise, it adds the new task with the given priority. 2. `remove_task(pq, entry_finder, task)`: Marks an existing task as removed. Raises KeyError if the task is not found. 3. `pop_task(pq, entry_finder)`: Removes and returns the lowest priority task. Raises KeyError if the priority queue is empty. Use the following code structure and global variables: ```python from heapq import heappush, heappop import itertools REMOVED = \'<removed-task>\' # placeholder for a removed task counter = itertools.count() # unique sequence count def add_task(pq, entry_finder, task, priority=0): Add a new task or update the priority of an existing task. :param pq: list, the priority queue (heap) :param entry_finder: dict, mapping of tasks to entries :param task: the task to add or update :param priority: the priority of the task pass def remove_task(pq, entry_finder, task): Mark an existing task as removed. :param pq: list, the priority queue (heap) :param entry_finder: dict, mapping of tasks to entries :param task: the task to remove :raises KeyError: if task is not found pass def pop_task(pq, entry_finder): Remove and return the lowest priority task. :param pq: list, the priority queue (heap) :param entry_finder: dict, mapping of tasks to entries :return: the task with the lowest priority :raises KeyError: if the priority queue is empty pass ``` # Input and Output: - **add_task** - Input: `pq` (list), `entry_finder` (dict), `task` (any type), `priority` (int) - Output: None - **remove_task** - Input: `pq` (list), `entry_finder` (dict), `task` (any type) - Output: None - Errors: Raises `KeyError` if the `task` is not found. - **pop_task** - Input: `pq` (list), `entry_finder` (dict) - Output: Task with the lowest priority. - Errors: Raises `KeyError` if the priority queue is empty. # Constraints and Requirements: 1. The `pq` list should always maintain the heap invariant. 2. The functions should handle removing and updating tasks efficiently. 3. The `pop_task` function should return tasks in order of their priority. If two tasks have the same priority, they should be returned in the order they were added. 4. Avoid using any other advanced data structures or libraries except `heapq` and `itertools`. # Example: ```python >>> pq = [] >>> entry_finder = {} >>> add_task(pq, entry_finder, \'task1\', priority=5) >>> add_task(pq, entry_finder, \'task2\', priority=3) >>> add_task(pq, entry_finder, \'task3\', priority=4) >>> add_task(pq, entry_finder, \'task1\', priority=1) # updating task1\'s priority >>> pop_task(pq, entry_finder) \'task1\' >>> remove_task(pq, entry_finder, \'task2\') >>> pop_task(pq, entry_finder) \'task3\' >>> pop_task(pq, entry_finder) KeyError: \'pop from an empty priority queue\' ``` # Performance: - Your solution should efficiently handle the heap operations, maintaining a time complexity close to O(log n) for add and pop operations.","solution":"from heapq import heappush, heappop import itertools REMOVED = \'<removed-task>\' # placeholder for a removed task counter = itertools.count() # unique sequence count def add_task(pq, entry_finder, task, priority=0): Add a new task or update the priority of an existing task. :param pq: list, the priority queue (heap) :param entry_finder: dict, mapping of tasks to entries :param task: the task to add or update :param priority: the priority of the task if task in entry_finder: remove_task(pq, entry_finder, task) count = next(counter) entry = [priority, count, task] entry_finder[task] = entry heappush(pq, entry) def remove_task(pq, entry_finder, task): Mark an existing task as removed. :param pq: list, the priority queue (heap) :param entry_finder: dict, mapping of tasks to entries :param task: the task to remove :raises KeyError: if task is not found entry = entry_finder.pop(task) entry[-1] = REMOVED def pop_task(pq, entry_finder): Remove and return the lowest priority task. :param pq: list, the priority queue (heap) :param entry_finder: dict, mapping of tasks to entries :return: the task with the lowest priority :raises KeyError: if the priority queue is empty while pq: priority, count, task = heappop(pq) if task is not REMOVED: del entry_finder[task] return task raise KeyError(\'pop from an empty priority queue\')"},{"question":"# Custom Operator Creation and Testing in PyTorch Objective: Create a custom operator in PyTorch to perform a specific mathematical operation, validate it for correctness, and extend it to support automatic differentiation. Task: 1. **Create a Custom Operator**: - Implement a custom operator named `custom_multiply_add` which takes three tensor inputs `a`, `b`, and `c`, and computes the result `a * b + c`. 2. **Validate the Custom Operator**: - Use `torch.library.opcheck` to ensure the operator is correctly defined and used. 3. **Extend the Operator for Autograd**: - Implement the gradient (backpropagation) functionality for the operator such that it supports training with PyTorch\'s automatic differentiation (`autograd`). Constraints: - Do not use built-in PyTorch operations directly for the `forward` part of the custom op. - Ensure the custom operator works for tensors of arbitrary shapes, as long as they follow the broadcasting rules. Inputs: - Tensors `a`, `b`, and `c` of any shape that are broadcast-compatible. Outputs: - A tensor resulting from the operation `a * b + c`. Instructions: 1. **Function Implementation**: Write the following functions: - `create_custom_operator()`: This function should define and register the custom operator. - `test_custom_operator()`: This function should validate the custom operator using `opcheck`. - `extend_for_autograd()`: This function should extend the custom operator to support autograd. 2. **Demonstration**: Provide a demonstration using random tensors to show that the custom operator works correctly and supports backpropagation (i.e., autograd). Here is a template to get you started: ```python import torch import torch.library # Step 1: Create the custom operator def create_custom_operator(): # Define and register your custom operator here pass # Step 2: Validate the custom operator def test_custom_operator(): # Use opcheck to ensure the custom operator is properly defined and used pass # Step 3: Extend the operator for autograd def extend_for_autograd(): # Implement gradient functionality pass # Demonstration if __name__ == \\"__main__\\": create_custom_operator() test_custom_operator() extend_for_autograd() a = torch.randn(3, 4, requires_grad=True) b = torch.randn(3, 4, requires_grad=True) c = torch.randn(3, 4, requires_grad=True) # Custom operator usage result = custom_multiply_add(a, b, c) # Checking gradient computation loss = result.sum() loss.backward() print(\\"Gradients:\\") print(a.grad) print(b.grad) print(c.grad) ``` # Notes: - Ensure that you have appropriate error handling to catch and display issues during the operator creation and validation steps. - Demonstrate the usage of the custom operator with both forward and backward passes to confirm its correctness.","solution":"import torch from torch.autograd import Function class CustomMultiplyAdd(Function): @staticmethod def forward(ctx, a, b, c): ctx.save_for_backward(a, b, c) result = a * b + c return result @staticmethod def backward(ctx, grad_output): a, b, c = ctx.saved_tensors grad_a = grad_output * b grad_b = grad_output * a grad_c = grad_output return grad_a, grad_b, grad_c def custom_multiply_add(a, b, c): return CustomMultiplyAdd.apply(a, b, c)"},{"question":"# PyTorch Named Tensors In this assessment, you will demonstrate your understanding of PyTorch named tensors by implementing a function that leverages named tensor functionalities to perform a series of tensor operations. The named tensor API is experimental and subject to change, but it is designed to provide additional runtime correctness checks and to better handle operations that involve multiple tensors with named dimensions. Problem Statement Implement a function `process_named_tensors` that performs the following operations step-by-step on the input tensors: 1. Accept two named tensors `a` and `b` as inputs. 2. Ensure that the names of dimensions between `a` and `b` are aligned before performing operations. 3. Compute the element-wise sum of tensors `a` and `b`. 4. Compute the matrix multiplication of the resulting tensor from step 3 with another tensor `c`. 5. Compute the sum along a specified dimension of the resultant tensor from step 4, removing that dimension. 6. Return the final tensor with the names of the remaining dimensions. Input - `a`: A named PyTorch tensor with any dimensions. - `b`: A named PyTorch tensor with the same shape and dimension names as `a`. - `c`: A named PyTorch tensor with dimensions suitable for matrix multiplication with the tensor from step 3. - `dim_to_sum`: A string representing the dimension name to sum over. Output - A PyTorch tensor with the specified dimension `dim_to_sum` removed and with the names of the remaining dimensions. Constraints - Tensors `a` and `b` must have the same shape and dimension names. - Tensor `c` must be compatible for matrix multiplication with the tensor resulting from the element-wise sum of `a` and `b`. - `dim_to_sum` must be a valid dimension name in the tensor obtained after matrix multiplication. Example ```python import torch a = torch.randn(3, 4, names=(\'N\', \'C\')) b = torch.randn(3, 4, names=(\'N\', \'C\')) c = torch.randn(4, 5, names=(\'C\', \'F\')) result = process_named_tensors(a, b, c, \'F\') # Result should be a tensor after summing over \'F\', remaining dimensions should be (\'N\'). print(result.names) # Output: (\'N\',) ``` Implementation You can use the provided example in the problem statement for testing, but your solution should be general enough to handle any valid input tensors that meet the constraints. ```python import torch def process_named_tensors(a: torch.Tensor, b: torch.Tensor, c: torch.Tensor, dim_to_sum: str) -> torch.Tensor: # Step 1: Check if tensors a and b have the same shape and names: if a.shape != b.shape or a.names != b.names: raise ValueError(\\"Input tensors a and b must have the same shape and dimension names.\\") # Step 2: Align tensors (if required) a = a.align_as(b) # Step 3: Element-wise addition result = a + b # Step 4: Matrix multiplication with tensor c result = torch.matmul(result, c) # Step 5: Sum over specified dimension and remove it result = result.sum(dim_to_sum) return result ``` Ensure your function captures the dimensionality and name constraints properly and aligns tensors as necessary before performing operations.","solution":"import torch def process_named_tensors(a: torch.Tensor, b: torch.Tensor, c: torch.Tensor, dim_to_sum: str) -> torch.Tensor: # Step 1: Check if tensors a and b have the same shape and names: if a.shape != b.shape or a.names != b.names: raise ValueError(\\"Input tensors a and b must have the same shape and dimension names.\\") # Step 2: Align tensors (if required) a = a.align_as(b) # Step 3: Element-wise addition result = a + b # Step 4: Matrix multiplication with tensor c result = torch.matmul(result, c) # Step 5: Sum over specified dimension and remove it result = result.sum(dim=dim_to_sum) return result"},{"question":"**Problem Statement:** You are tasked with creating a program to manage the installation and upgrading of the \\"pip\\" installer in Python environments using the `ensurepip` module. Your program should use the module\'s API to provide a user-friendly interface for bootstrapping \\"pip\\". **Requirements:** 1. Implement a Python function `manage_pip(install, root=None, upgrade=False, user=False, altinstall=False, default_pip=False, verbosity=0)` that uses the `ensurepip.bootstrap` function to install or upgrade \\"pip\\". 2. Implement a Python function `get_pip_version()` that returns the version of \\"pip\\" that would be installed, using `ensurepip.version`. 3. The `manage_pip` function should install \\"pip\\" based on the provided parameters and print a summary of what actions were taken (e.g., \\"Installed pip version 21.1.1 in user site packages\\"). **Input:** - `install` (boolean): If `True`, the function should install \\"pip\\". If `False`, the function should skip installation. - `root` (string, optional): A string specifying an alternative root directory to install \\"pip\\". - `upgrade` (boolean, optional): Whether to upgrade an existing installation of \\"pip\\". - `user` (boolean, optional): Whether to install \\"pip\\" in the user site packages directory. - `altinstall` (boolean, optional): Whether to skip installing the \\"pipX\\" script. - `default_pip` (boolean, optional): Whether to install the \\"pip\\" script in addition to \\"pipX\\" and \\"pipX.Y\\". - `verbosity` (int, optional): Controls the level of output from the bootstrapping operation. Defaults to 0 (no additional output). **Output:** - For `manage_pip`: The function should print the actions taken (\\"Installed pip version x.y.z in user site packages\\"). - For `get_pip_version`: The function should return the version of \\"pip\\" as a string (e.g., \\"21.1.1\\"). **Constraints:** - Ensure that calling both `altinstall` and `default_pip` together raises a `ValueError`. - Make sure to handle `install` appropriately; if `install` is `False`, the function should not proceed with any installation or upgrades. **Example:** ```python def manage_pip(install, root=None, upgrade=False, user=False, altinstall=False, default_pip=False, verbosity=0): import ensurepip if install: if altinstall and default_pip: raise ValueError(\\"Cannot specify both altinstall and default_pip\\") ensurepip.bootstrap(root=root, upgrade=upgrade, user=user, altinstall=altinstall, default_pip=default_pip, verbosity=verbosity) print(f\\"Installed pip version {ensurepip.version()} {(\'in user site packages\' if user else \'globally\')}.\\") def get_pip_version(): import ensurepip return ensurepip.version() # Sample usage manage_pip(install=True, user=True, default_pip=True, verbosity=1) version = get_pip_version() print(f\\"Pip version available for installation: {version}\\") ``` **Explanation:** In this example, `manage_pip` is called with `install=True`, `user=True`, `default_pip=True`, and `verbosity=1`, which installs \\"pip\\" in the user site packages with additional verbosity. After the installation, `get_pip_version` is used to fetch and print the available version of \\"pip\\".","solution":"import ensurepip def manage_pip(install, root=None, upgrade=False, user=False, altinstall=False, default_pip=False, verbosity=0): if not install: print(\\"Skipping pip installation as \'install\' is set to False.\\") return if altinstall and default_pip: raise ValueError(\\"Cannot specify both altinstall and default_pip\\") ensurepip.bootstrap(root=root, upgrade=upgrade, user=user, altinstall=altinstall, default_pip=default_pip, verbosity=verbosity) pip_version = ensurepip.version() location = \\"user site packages\\" if user else \\"globally\\" print(f\\"Installed pip version {pip_version} {location}.\\") def get_pip_version(): return ensurepip.version()"},{"question":"# Question: XML Data Modification and Query You are given an XML document which contains information about a collection of books. The goal is to write a function that can read this XML document, perform modifications, and extract specific information based on given criteria. 1. Implement a function `modify_and_query_xml(xml_string: str, category: str) -> List[str]` where: - `xml_string`: A string representing the XML data of books. - `category`: A string representing the book category to search for. 2. The XML will be structured as follows: ```xml <library> <book id=\\"1\\"> <title>Book Title 1</title> <author>Author 1</author> <category>Fiction</category> <year>2001</year> <price>29.99</price> </book> <book id=\\"2\\"> <title>Book Title 2</title> <author>Author 2</author> <category>Science</category> <year>2015</year> <price>49.99</price> </book> ... </library> ``` # Requirements Your function should: 1. Parse the given XML string to build an XML tree. 2. Modify the price of any book published before the year 2000 to increase by 10%. 3. Return a list of titles for books that match the specified category. 4. Ensure that the function handles potentially malformed XML by catching and handling exceptions appropriately. # Example ```python xml_data = <library> <book id=\\"1\\"> <title>Book Title 1</title> <author>Author 1</author> <category>Fiction</category> <year>1999</year> <price>20.00</price> </book> <book id=\\"2\\"> <title>Book Title 2</title> <author>Author 2</author> <category>Science</category> <year>2010</year> <price>30.00</price> </book> </library> assert modify_and_query_xml(xml_data, \\"Fiction\\") == [\\"Book Title 1\\"] ``` **Constraints:** - The XML string passed to the function is guaranteed to be non-empty. - Prices and years will be valid floats and integers respectively. **Performance Requirements:** - The function should efficiently parse and modify the XML, targeting a time complexity of O(n), where n is the number of elements in the XML string.","solution":"from typing import List import xml.etree.ElementTree as ET def modify_and_query_xml(xml_string: str, category: str) -> List[str]: Modifies prices of books published before the year 2000 and returns titles of books that match the specified category. :param xml_string: XML data as a string :param category: Category to filter books by :return: List of titles belonging to the specified category try: # Parse the XML string root = ET.fromstring(xml_string) # Initialize a list to hold the titles of books in the specified category titles_in_category = [] # Iterate over each book element in the XML for book in root.findall(\'book\'): year = int(book.find(\'year\').text) price = float(book.find(\'price\').text) # Modify price if book published before the year 2000 if year < 2000: new_price = price * 1.10 book.find(\'price\').text = f\\"{new_price:.2f}\\" # Add to the list if the book matches the specified category if book.find(\'category\').text == category: titles_in_category.append(book.find(\'title\').text) return titles_in_category except ET.ParseError: print(\\"Error: Malformed XML\\") return []"},{"question":"**Question: Data Analysis and Manipulation with pandas** **Objective:** You are required to demonstrate your understanding of pandas by performing multiple operations on `Series` and `DataFrame`. **Problem Statement:** Suppose you are given a dataset containing sales records of a company that sells multiple products across different regions. The dataset is divided into two parts: 1. `products` - A dictionary where keys are product IDs and values are product names. 2. `sales` - A list of dictionaries, where each dictionary contains: - \\"product_id\\": The ID of the product. - \\"region\\": The region where the product was sold. - \\"quantity\\": The quantity sold. - \\"price\\": Price per unit. ```python products = { \\"P001\\": \\"Product A\\", \\"P002\\": \\"Product B\\", \\"P003\\": \\"Product C\\" } sales = [ {\\"product_id\\": \\"P001\\", \\"region\\": \\"North\\", \\"quantity\\": 10, \\"price\\": 99.5}, {\\"product_id\\": \\"P002\\", \\"region\\": \\"South\\", \\"quantity\\": 5, \\"price\\": 149.9}, {\\"product_id\\": \\"P003\\", \\"region\\": \\"East\\", \\"quantity\\": 7, \\"price\\": 199.0}, {\\"product_id\\": \\"P001\\", \\"region\\": \\"South\\", \\"quantity\\": 3, \\"price\\": 99.5}, {\\"product_id\\": \\"P002\\", \\"region\\": \\"East\\", \\"quantity\\": 8, \\"price\\": 149.9}, {\\"product_id\\": \\"P003\\", \\"region\\": \\"West\\", \\"quantity\\": 4, \\"price\\": 199.0} ] ``` **Tasks:** 1. **Create a DataFrame**: - Create a DataFrame named `df_sales` from the `sales` list. 2. **Add Product Names**: - Create a `Series` from the `products` dictionary where the product IDs are the index. Then, map the product names to the `df_sales` DataFrame using the \'product_id\' column and add a new column named `product_name` to `df_sales`. 3. **Total Sales Calculation**: - Add a new column named `total_sales` to the `df_sales` DataFrame, calculated as the product of `quantity` and `price`. 4. **Summary Statistics**: - Provide a summary DataFrame `summary_df` which contains: - The total quantity sold and total sales for each product in each region. - The summary DataFrame should have products as columns and regions as rows. 5. **Handle Missing Data**: - Modify `summary_df` to handle any missing data by filling NaNs with 0. **Constraints and Requirements**: - Your solution should handle the possibility of additional products or regions in the real dataset. - Optimize for performance where possible. **Expected Output**: For illustration, here is a simplified snippet of what the output might look like assuming the above inputs: ```python df_sales: product_id region quantity price product_name total_sales 0 P001 North 10 99.5 Product A 995.0 1 P002 South 5 149.9 Product B 749.5 2 P003 East 7 199.0 Product C 1393.0 3 P001 South 3 99.5 Product A 298.5 4 P002 East 8 149.9 Product B 1199.2 5 P003 West 4 199.0 Product C 796.0 summary_df: Product A Product B Product C North 10 / 995.0 0 / 0 0 / 0 South 3 / 298.5 5 / 749.5 0 / 0 East 0 / 0 8 / 1199.2 7 / 1393.0 West 0 / 0 0 / 0 4 / 796.0 ``` You may use appropriate methods and functions discussed in the pandas documentation to achieve the required tasks. **Note: Ensure your code is well-documented and includes comments for readability.**","solution":"import pandas as pd def create_sales_dataframe(sales): Creates a DataFrame from the sales data. return pd.DataFrame(sales) def add_product_names(df_sales, products): Adds product names to the sales DataFrame based on product IDs. product_series = pd.Series(products) df_sales[\'product_name\'] = df_sales[\'product_id\'].map(product_series) return df_sales def add_total_sales(df_sales): Adds a column \'total_sales\' calculated as the product of \'quantity\' and \'price\'. df_sales[\'total_sales\'] = df_sales[\'quantity\'] * df_sales[\'price\'] return df_sales def create_summary(df_sales): Creates a summary DataFrame with the total quantity sold and total sales for each product in each region. summary = df_sales.pivot_table(index=\'region\', columns=\'product_name\', values=[\'quantity\', \'total_sales\'], aggfunc={\'quantity\': \'sum\', \'total_sales\': \'sum\'}, fill_value=0) summary.columns = [\' \'.join(col).strip() for col in summary.columns.values] summary = summary.reset_index() # Handle missing data summary = summary.fillna(0) return summary # Given data products = { \\"P001\\": \\"Product A\\", \\"P002\\": \\"Product B\\", \\"P003\\": \\"Product C\\" } sales = [ {\\"product_id\\": \\"P001\\", \\"region\\": \\"North\\", \\"quantity\\": 10, \\"price\\": 99.5}, {\\"product_id\\": \\"P002\\", \\"region\\": \\"South\\", \\"quantity\\": 5, \\"price\\": 149.9}, {\\"product_id\\": \\"P003\\", \\"region\\": \\"East\\", \\"quantity\\": 7, \\"price\\": 199.0}, {\\"product_id\\": \\"P001\\", \\"region\\": \\"South\\", \\"quantity\\": 3, \\"price\\": 99.5}, {\\"product_id\\": \\"P002\\", \\"region\\": \\"East\\", \\"quantity\\": 8, \\"price\\": 149.9}, {\\"product_id\\": \\"P003\\", \\"region\\": \\"West\\", \\"quantity\\": 4, \\"price\\": 199.0} ] # Solution df_sales = create_sales_dataframe(sales) df_sales = add_product_names(df_sales, products) df_sales = add_total_sales(df_sales) summary_df = create_summary(df_sales) # Output DataFrames (for reference) df_sales, summary_df"},{"question":"# Plotting with Seaborn\'s `relplot` You are given a dataset related to diamond prices which contains the following columns: - `carat`: The weight of the diamond. - `price`: The price of the diamond. - `cut`: The quality of the cut (Fair, Good, Very Good, Premium, Ideal). - `color`: The color grade of the diamond (ranging from D (best) to J (worst)). - `clarity`: A measurement of how clear the diamond is (ranging from I1 (worst) to IF (best)). Write a Python function using seaborn\'s `relplot` to visualize the relationship between the carat, price, and other categorical variables (`cut`, `color`, `clarity`) in the dataset. Your function should create a faceted scatter plot with the following requirements: 1. The x-axis should represent the `carat`. 2. The y-axis should represent the `price`. 3. The `hue` should represent the `cut` of the diamond. 4. Create subplots (facets) based on the `color` of the diamond, arranged across columns. 5. Customize the title of each subplot to include the color of the diamonds it represents. 6. The scatter points should be styled by the `clarity` of the diamond. 7. Adjust the size of the facets to have a height of 5 and an aspect ratio of 1.5. **Function Signature:** ```python import seaborn as sns import matplotlib.pyplot as plt def plot_diamonds_relations(diamond_data): pass ``` **Input:** - `diamond_data`: A pandas DataFrame containing the diamonds dataset with columns as specified above. **Output:** - The function should generate and display the scatter plot according to the criteria. **Constraints:** - Use seaborn\'s `relplot` function for creating the scatter plot. - Ensure the plot is clear and aesthetically pleasing. **Example Usage:** ```python import seaborn as sns import pandas as pd # Load the diamonds dataset diamond_data = sns.load_dataset(\\"diamonds\\") # Call the function to plot plot_diamonds_relations(diamond_data) ``` **Notes:** - The function should not return any value. It should display the plot directly. - Ensure you handle the scenario where the dataset might not contain the expected columns appropriately.","solution":"import seaborn as sns import matplotlib.pyplot as plt import pandas as pd def plot_diamonds_relations(diamond_data): Plots the relationship between diamond carat, price, and other categorical variables. # Check if the required columns are present in the dataframe expected_columns = {\'carat\', \'price\', \'cut\', \'color\', \'clarity\'} if not expected_columns.issubset(diamond_data.columns): raise ValueError(f\\"The dataframe must contain the following columns: {expected_columns}\\") # Create the relplot g = sns.relplot( data=diamond_data, x=\'carat\', y=\'price\', hue=\'cut\', col=\'color\', style=\'clarity\', kind=\'scatter\', height=5, aspect=1.5 ) # Customize the titles of each subplot to include the color of the diamonds it represents for ax in g.axes.flat: col_val = ax.get_title().split(\'=\')[1] ax.set_title(f\\"Color: {col_val}\\") # Show the plot plt.show()"},{"question":"**Title:** Manipulating Sun AU Audio File Metadata **Objective:** Implement a Python function that reads an AU file, modifies its metadata as specified, and writes the modified data to a new AU file. This will test your understanding of file handling and usage of the `sunau` module. **Description:** Write a function `modify_au_metadata(input_file: str, output_file: str, nchannels: int, sampwidth: int, framerate: int, comptype: str, compname: str) -> None` that performs the following tasks: 1. Open the AU file specified by `input_file` in read mode. 2. Read the audio data and its metadata. 3. Create a new AU file specified by `output_file` and copy the audio data into it. 4. Modify the metadata of the new AU file based on the input parameters `nchannels`, `sampwidth`, `framerate`, `comptype`, and `compname`. 5. Close both the input and output files. **Function Signature:** ```python def modify_au_metadata(input_file: str, output_file: str, nchannels: int, sampwidth: int, framerate: int, comptype: str, compname: str) -> None: pass ``` **Parameters:** - `input_file` (str): Path to the input AU file. - `output_file` (str): Path to the output AU file. - `nchannels` (int): Number of audio channels (1 for mono, 2 for stereo). - `sampwidth` (int): Sample width in bytes (valid values: 1, 2, 3, 4). - `framerate` (int): Sampling frequency in Hz. - `comptype` (str): Compression type (valid values: \'NONE\', \'ULAW\'). - `compname` (str): Compression name corresponding to `comptype`. **Constraints:** - The provided `input_file` must be a valid AU file. - The `output_file` cannot be the same as the `input_file`. - The `comptype` must be either \'NONE\' or \'ULAW\'. - Ensure proper error handling, raising `sunau.Error` with a descriptive message if an operation fails. **Example Usage:** ```python modify_au_metadata( input_file=\'input.au\', output_file=\'output.au\', nchannels=2, sampwidth=2, framerate=44100, comptype=\'NONE\', compname=\'not compressed\' ) ``` **Expected Behaviour:** - The function creates an `output.au` file with the modified metadata as specified. - The audio data from `input.au` is copied to `output.au`. This function demonstrates your ability to work with file I/O, handle audio data, and use the `sunau` module for practical tasks.","solution":"import sunau import os def modify_au_metadata(input_file: str, output_file: str, nchannels: int, sampwidth: int, framerate: int, comptype: str, compname: str) -> None: Modifies the metadata of an AU file and writes it to a new file. Parameters: - input_file (str): Path to the input AU file. - output_file (str): Path to the output AU file. - nchannels (int): Number of audio channels. - sampwidth (int): Sample width in bytes. - framerate (int): Sampling frequency in Hz. - comptype (str): Compression type. - compname (str): Compression name corresponding to comptype. Raises: - sunau.Error: If an error occurs during the processing of the AU file. if input_file == output_file: raise ValueError(\\"Input file and output file must be different.\\") valid_comptypes = [\'NONE\', \'ULAW\'] if comptype not in valid_comptypes: raise ValueError(f\\"Invalid comptype. Expected one of {valid_comptypes}\\") try: # Open the input file with sunau.open(input_file, \'rb\') as input_au: # Read audio parameters audio_params = input_au.getparams() # Read audio frames audio_frames = input_au.readframes(audio_params.nframes) # Open the output file with sunau.open(output_file, \'wb\') as output_au: # Set new audio parameters output_au.setnchannels(nchannels) output_au.setsampwidth(sampwidth) output_au.setframerate(framerate) output_au.setcomptype(comptype, compname) # Write audio frames to the new file output_au.writeframes(audio_frames) except sunau.Error as e: raise sunau.Error(f\\"Error processing AU file: {e}\\") except Exception as e: raise sunau.Error(f\\"An unexpected error occurred: {e}\\")"},{"question":"# Manifold Learning and Dimensionality Reduction Problem Statement: You are provided with a dataset of high-dimensional data and your task is to implement a dimensionality reduction method using scikit-learn\'s manifold learning techniques to visualize the dataset in a 2D space. Specifically, you will implement the Isomap algorithm. Requirements: 1. **Input**: - A NumPy array `X` of shape `(n_samples, n_features)` representing the high-dimensional data. - An integer `n_neighbors` specifying the number of neighbors to consider for each point. - An integer `n_components` specifying the number of dimensions to reduce the data to (2 for visualization purposes). 2. **Output**: - A NumPy array `X_2D` of shape `(n_samples, 2)` representing the two-dimensional embedding of the dataset. Instructions: 1. Implement the Isomap algorithm using scikit-learn\'s `Isomap` class. 2. Ensure that the reduced data preserves the global distances and the local neighborhood structures as accurately as possible. 3. Write a function `perform_isomap(X, n_neighbors, n_components)` that performs the following: - Initializes the Isomap algorithm with the specified number of neighbors and components. - Fits the Isomap model using the input data. - Transforms the data into the lower-dimensional space. ```python import numpy as np from sklearn.manifold import Isomap def perform_isomap(X, n_neighbors, n_components): Reduces the dimensionality of the input data using Isomap. Parameters: - X: np.ndarray of shape (n_samples, n_features); the input high-dimensional data. - n_neighbors: int; the number of neighbors to consider for each point. - n_components: int; the number of dimensions to reduce the data to (should be 2 for visualization). Returns: - X_2D: np.ndarray of shape (n_samples, 2); the two-dimensional embedding of the data. # Initialize Isomap with the specified number of neighbors and components isomap = Isomap(n_neighbors=n_neighbors, n_components=n_components) # Fit the Isomap model X_2D = isomap.fit_transform(X) return X_2D # Example usage: # X = np.random.rand(100, 10) # Random high-dimensional dataset for demonstration # X_2D = perform_isomap(X, n_neighbors=5, n_components=2) # print(X_2D) ``` Constraints: - The number of neighbors should be less than the number of samples. - The number of components should be set to 2 for visualization. - Handle cases where dimensionality reduction may fail by appropriately logging the error. Performance Requirements: - Implement the function efficiently to handle moderately large datasets. - Ensure the function does not exceed a reasonable runtime for typical datasets encountered in practice. You may test your function using synthetic or real datasets to ensure it works correctly.","solution":"import numpy as np from sklearn.manifold import Isomap def perform_isomap(X, n_neighbors, n_components): Reduces the dimensionality of the input data using Isomap. Parameters: - X: np.ndarray of shape (n_samples, n_features); the input high-dimensional data. - n_neighbors: int; the number of neighbors to consider for each point. - n_components: int; the number of dimensions to reduce the data to (should be 2 for visualization). Returns: - X_2D: np.ndarray of shape (n_samples, 2); the two-dimensional embedding of the data. # Initialize Isomap with the specified number of neighbors and components isomap = Isomap(n_neighbors=n_neighbors, n_components=n_components) # Fit the Isomap model X_2D = isomap.fit_transform(X) return X_2D # Example usage: # X = np.random.rand(100, 10) # Random high-dimensional dataset for demonstration # X_2D = perform_isomap(X, n_neighbors=5, n_components=2) # print(X_2D)"},{"question":"**Advanced XML Parsing with SAX Handlers** You are tasked to create a class that extends `xml.sax.handler.ContentHandler` to parse an XML document. The XML document contains information about books in a library, as shown below: ```xml <library> <book> <title>Introduction to Python</title> <author>John Doe</author> <year>2020</year> <isbn>1234567890</isbn> </book> <book> <title>Advanced XML Processing</title> <author>Jane Smith</author> <year>2018</year> <isbn>0987654321</isbn> </book> </library> ``` # Requirements 1. **Class Definition**: Define a class `LibraryContentHandler` that inherits from `xml.sax.handler.ContentHandler`. 2. **Constructor**: Initialize the class to keep track of the current element being processed and data storage for books. 3. **Methods to Implement**: - `startElement(self, name, attrs)`: To handle the start of an element. - `characters(self, content)`: To handle character data within elements. - `endElement(self, name)`: To handle the end of an element. 4. **Data Storage**: The data should be stored in a list of dictionaries, where each dictionary represents a book with keys: \'title\', \'author\', \'year\', and \'isbn\'. 5. **Output Requirement**: After parsing the document, return the list of dictionaries representing the books. # Input and Output - **Input**: An XML document as a string. - **Output**: A list of dictionaries with book information. # Constraints - Assume that the XML document is well-formed. - The structure of the XML is consistent with the sample provided. # Performance Requirements - The implemented methods should efficiently handle XML documents of moderate size (up to 10MB). # Example Given the XML document provided above, your implementation should output: ```python [ {\'title\': \'Introduction to Python\', \'author\': \'John Doe\', \'year\': \'2020\', \'isbn\': \'1234567890\'}, {\'title\': \'Advanced XML Processing\', \'author\': \'Jane Smith\', \'year\': \'2018\', \'isbn\': \'0987654321\'} ] ``` **Starter Code:** ```python import xml.sax class LibraryContentHandler(xml.sax.handler.ContentHandler): def __init__(self): super().__init__() self.current_element = \\"\\" self.current_book = {} self.books = [] def startElement(self, name, attrs): self.current_element = name if name == \\"book\\": self.current_book = {} # Initialize a new book dictionary def characters(self, content): if self.current_element == \\"title\\": self.current_book[\\"title\\"] = content elif self.current_element == \\"author\\": self.current_book[\\"author\\"] = content elif self.current_element == \\"year\\": self.current_book[\\"year\\"] = content elif self.current_element == \\"isbn\\": self.current_book[\\"isbn\\"] = content def endElement(self, name): if name == \\"book\\": self.books.append(self.current_book) # Store the completed book self.current_element = \\"\\" # Reset current element # Function to parse XML def parse_library_xml(xml_string): handler = LibraryContentHandler() xml.sax.parseString(xml_string, handler) return handler.books # Example usage xml_string = \'\'\'<library> <book> <title>Introduction to Python</title> <author>John Doe</author> <year>2020</year> <isbn>1234567890</isbn> </book> <book> <title>Advanced XML Processing</title> <author>Jane Smith</author> <year>2018</year> <isbn>0987654321</isbn> </book> </library>\'\'\' books = parse_library_xml(xml_string) print(books) ``` Complete the `LibraryContentHandler` class to pass the example usage.","solution":"import xml.sax class LibraryContentHandler(xml.sax.handler.ContentHandler): def __init__(self): super().__init__() self.current_element = \\"\\" self.content = \\"\\" self.current_book = {} self.books = [] def startElement(self, name, attrs): self.current_element = name self.content = \\"\\" if name == \\"book\\": self.current_book = {} # Initialize a new book dictionary def characters(self, content): self.content += content def endElement(self, name): if name == \\"title\\": self.current_book[\\"title\\"] = self.content.strip() elif name == \\"author\\": self.current_book[\\"author\\"] = self.content.strip() elif name == \\"year\\": self.current_book[\\"year\\"] = self.content.strip() elif name == \\"isbn\\": self.current_book[\\"isbn\\"] = self.content.strip() elif name == \\"book\\": self.books.append(self.current_book) # Store the completed book self.current_element = \\"\\" # Reset current element # Function to parse XML def parse_library_xml(xml_string): handler = LibraryContentHandler() xml.sax.parseString(xml_string, handler) return handler.books"},{"question":"**Coding Assessment Question: Implementing and Using Data Classes in Python** **Objective:** To evaluate your understanding of Python\'s `dataclasses` module by implementing a complex data structure using data classes. **Problem Statement:** You are required to implement a system for managing a collection of books in a library using Python data classes. Each book should have the following attributes: - `title` (str): The title of the book. - `author` (str): The author of the book. - `year_published` (int): The year the book was published. - `isbn` (str): The International Standard Book Number of the book. - `copies_available` (int): The number of copies available in the library. Additionally, you need to implement the following functionalities: 1. **Book Lending:** A method to lend a book to a user. It should decrease the available copies by one. If no copies are available, it should raise an exception. 2. **Book Returning:** A method to return a book to the library. It should increase the available copies by one. 3. **Searching Books:** A method to search for books by `title` or `author`. **Constraints:** - You must use the `dataclasses` module to define the Book class. - Use appropriate data types for each attribute. - Implement proper error handling for lending and returning books. - Ensure that your search function is case-insensitive. **Input and Output Formats:** 1. Define a class `Book` to represent a book in the library. 2. Implement the following methods: - `lend_book()` - `return_book()` - `search_books(criteria: str, search_term: str) -> List[Book]` **Example of Usage:** ```python from dataclasses import dataclass, field from typing import List, Optional @dataclass class Book: title: str author: str year_published: int isbn: str copies_available: int = field(default=1) def lend_book(self): if self.copies_available > 0: self.copies_available -= 1 else: raise Exception(\'No copies available\') def return_book(self): self.copies_available += 1 @dataclass class Library: books: List[Book] def search_books(self, criteria: str, search_term: str) -> Optional[List[Book]]: search_term = search_term.lower() if criteria == \'title\': return [book for book in self.books if search_term in book.title.lower()] elif criteria == \'author\': return [book for book in self.books if search_term in book.author.lower()] else: return None ``` Your task is to complete the above class definitions and implement these functionalities. The main focus should be on correct usage of data classes and managing the library functionalities as described. **Note:** Ensure your code is well-documented and thoroughly tested with various edge cases.","solution":"from dataclasses import dataclass from typing import List, Optional @dataclass class Book: title: str author: str year_published: int isbn: str copies_available: int def lend_book(self): if self.copies_available > 0: self.copies_available -= 1 else: raise Exception(\'No copies available\') def return_book(self): self.copies_available += 1 @dataclass class Library: books: List[Book] def search_books(self, criteria: str, search_term: str) -> Optional[List[Book]]: search_term = search_term.lower() if criteria == \'title\': return [book for book in self.books if search_term in book.title.lower()] elif criteria == \'author\': return [book for book in self.books if search_term in book.author.lower()] else: return None"},{"question":"Objective The goal of this exercise is to implement a function that uses PyTorch\'s `torch.distributed.elastic.events` module to manage and record events in a distributed training system. Problem Statement You are required to implement a function `track_training_events` that: 1. Records a start event when the training begins. 2. Tracks and records intermediate events based on certain key milestones during the training (e.g., end of epochs). 3. Records an end event when the training completes. To simulate milestone events, assume the function receives an integer `num_epochs` and a list of integers `milestones` indicating at which epochs these milestones occur. Function Signature ```python def track_training_events(num_epochs: int, milestones: List[int]) -> List[torch.distributed.elastic.events.api.Event]: pass ``` Input - `num_epochs` (int): Total number of epochs for the training. - `milestones` (List[int]): List of epoch numbers at which to record milestone events. Output - List of `Event` objects recorded during the training session. Constraints - All milestone values in `milestones` will be unique and within the range `[1, num_epochs]`. - The function should use the provided `torch.distributed.elastic.events` modules to record events. Example ```python num_epochs = 5 milestones = [1, 3, 5] events = track_training_events(num_epochs, milestones) for event in events: print(event) ``` The example should print the details of the `Event` objects, including their types (start, milestone, end) and any associated metadata. Notes - Ensure that your implementation makes proper use of the PyTorch Events APIs for creating and recording events. - You can assume that the `Event` class has appropriate fields for capturing types like \\"start\\", \\"milestone\\", and \\"end\\", and you should use these appropriately when creating event instances. Requirements 1. Install PyTorch if not already installed. 2. Use the necessary PyTorch import statements for the required `torch.distributed.elastic.events` components. Good luck!","solution":"import torch.distributed.elastic.events.api as events from typing import List def track_training_events(num_epochs: int, milestones: List[int]) -> List[events.Event]: Records a start event when the training begins, intermediate events for milestones, and an end event when training completes. Args: num_epochs (int): Total number of epochs for the training. milestones (List[int]): List of epoch numbers at which to record milestone events. Returns: List[torch.distributed.elastic.events.api.Event]: List of events recorded during the training session. # List to keep track of recorded events recorded_events = [] # Record start event start_event = events.Event(name=\\"TrainingStarted\\", metadata={\\"epoch\\": 0}) recorded_events.append(start_event) # Record milestone events for epoch in range(1, num_epochs + 1): if epoch in milestones: milestone_event = events.Event(name=\\"Milestone\\", metadata={\\"epoch\\": epoch}) recorded_events.append(milestone_event) # Record end event end_event = events.Event(name=\\"TrainingCompleted\\", metadata={\\"epoch\\": num_epochs}) recorded_events.append(end_event) return recorded_events"},{"question":"**Question: Implement an Asynchronous TCP Echo Server** Your task is to implement an asynchronous TCP Echo Server using Python\'s `asyncio` library. The server should handle multiple clients, echoing back any data it receives. # Requirements: 1. **Accept Multiple Connections**: The server should be able to handle multiple clients concurrently. 2. **Echo Data**: For each connection, any data received by the server should be sent back (echoed) to the sending client. 3. **Graceful Shutdown**: Implement a way to gracefully shutdown the server using a Unix signal (e.g., SIGINT). # Function Signature: ```python import asyncio async def handle_client(reader: asyncio.StreamReader, writer: asyncio.StreamWriter) -> None: ... # Students will complete this function async def main(host: str, port: int) -> None: ... # Students will complete this function ``` # Input: - `host` (str): The hostname or IP address on which the server should listen for incoming connections. - `port` (int): The port number on which the server should listen for incoming connections. # Output: - None. However, it should print messages indicating connection statuses (e.g., \\"Client connected\\", \\"Client disconnected\\") # Constraints: - You must use the `asyncio` library for asynchronous tasks. - The server should handle an arbitrary number of clients (as allowed by system resources). # Example: Here\'s an outline of the expected workflow: 1. Start the server by running the `main` coroutine. 2. Await new connections and handle them using the `handle_client` coroutine. 3. Echo back any data received. 4. On receiving a shutdown signal, close the server gracefully. # Implementation Outline: 1. In the `main` coroutine, create an `asyncio` TCP server that listens on the specified host and port, and uses `handle_client` as the connection handler. 2. In the `handle_client` coroutine, read data from the client and write the same data back to the client. 3. Use `asyncio.create_task()` or `loop.create_task()` to handle multiple clients concurrently. 4. Use suitable `asyncio` signal handling to implement a graceful shutdown of the server. # Starter Code: ```python import asyncio import signal async def handle_client(reader: asyncio.StreamReader, writer: asyncio.StreamWriter) -> None: while True: data = await reader.read(100) if not data: break writer.write(data) await writer.drain() writer.close() await writer.wait_closed() async def main(host: str, port: int) -> None: loop = asyncio.get_running_loop() server = await asyncio.start_server(handle_client, host, port, loop=loop) print(f\'Serving on {server.sockets[0].getsockname()}\') async def shutdown_signal(): while True: await asyncio.sleep(100) loop.add_signal_handler(signal.SIGINT, lambda: asyncio.ensure_future(shutdown_signal())) async with server: await server.serve_forever() if __name__ == \\"__main__\\": host = \'127.0.0.1\' port = 8888 try: asyncio.run(main(host, port)) except KeyboardInterrupt: print(\\"Server stopped\\") ``` Students are expected to complete the `handle_client` and `main` functions to achieve the requirements above. They should make sure to test their implementation with multiple clients. # Notes: - Ensure your code adheres to good design principles and handles exceptions appropriately. - You may include print statements to represent the flow of the server and connections for easier debugging during development.","solution":"import asyncio import signal async def handle_client(reader: asyncio.StreamReader, writer: asyncio.StreamWriter) -> None: Handle a single client connection. Echo back any data received. try: while True: data = await reader.read(100) if not data: break writer.write(data) await writer.drain() except asyncio.CancelledError: pass except Exception as e: print(f\\"Error: {e}\\") finally: writer.close() await writer.wait_closed() print(\\"Client disconnected\\") async def main(host: str, port: int) -> None: Start the asynchronous TCP echo server and handle graceful shutdown on SIGINT. server = await asyncio.start_server(handle_client, host, port) # Function to handle graceful shutdown def shutdown(): tasks = asyncio.all_tasks() for task in tasks: task.cancel() # Register the shutdown handler loop = asyncio.get_running_loop() loop.add_signal_handler(signal.SIGINT, shutdown) print(f\'Serving on {host}:{port}\') async with server: try: await server.serve_forever() except asyncio.CancelledError: pass print(\\"Server shutting down gracefully...\\") if __name__ == \\"__main__\\": host = \'127.0.0.1\' port = 8888 try: asyncio.run(main(host, port)) except KeyboardInterrupt: print(\\"Server stopped\\")"},{"question":"<|Analysis Begin|> The provided documentation details an \\"Audit events table\\" for Python\'s CPython implementation. This table lists various audit events triggered by different actions and the corresponding arguments for each event. These events are raised by the `sys.audit()` or `PySys_Audit()` calls and cover a wide range of Python\'s standard library functions and modules. The events range from basic built-in functions like `input` and `compile` to more specific module functions such as those in `ctypes`, `ftplib`, `os`, `shutil`, etc. The documentation indicates the arguments required for each event, thus giving an understanding of what inputs will be handled by these function calls. From this audit events table, we can design a question that requires students to utilize knowledge of auditing events, understand argument handling, and potentially implement a function or class that uses these capabilities to monitor or log specific activities. Given this context, creating an advanced coding question based on handling audit events and implementing a monitoring system in Python is appropriate. <|Analysis End|> <|Question Begin|> # Question: Implementing a Python Audit Event Logger **Objective:** Implement a Python class `AuditEventLogger` that utilizes Python\'s auditing system to log specific audit events to a file. The class should capture specified events and record their details, including the function name and arguments, to a specified log file. **Requirements:** 1. **Class**: `AuditEventLogger` 2. **Methods**: - `__init__(self, event_names: List[str], log_file: str)`: Initializes the logger with a list of event names to track and a path to the log file. - `start_logging(self)`: Starts logging the specified events. - `stop_logging(self)`: Stops logging events. 3. **Event Capturing**: When an event specified in `event_names` is triggered, the function name and its arguments should be logged in the format `\\"Event: <event_name>, Args: <args>\\"` to the `log_file`. **Input Format:** - `event_names`: A list of event names (strings) to be monitored. - `log_file`: A string specifying the path of the log file where events will be logged. **Example Usage:** ```python logger = AuditEventLogger([\\"os.remove\\", \\"shutil.move\\"], \\"audit_log.txt\\") logger.start_logging() # Trigger some events import os import shutil os.remove(\\"sample.txt\\") shutil.move(\\"old_dir\\", \\"new_dir\\") logger.stop_logging() ``` **Constraints:** - The logger should only capture and log events specified in the `event_names`. - The log file should be a plain text file, and each log entry corresponding to an event should be written in a new line. **Performance Requirements:** - The solution should efficiently handle multiple events without significantly degrading performance. - It should handle scenarios where no events, few events, or many events are logged. Implement the class below: ```python import sys class AuditEventLogger: def __init__(self, event_names, log_file): self.event_names = event_names self.log_file = log_file self.hook_installed = False def _hook(self, event, args): if event in self.event_names: log_entry = f\\"Event: {event}, Args: {args}n\\" with open(self.log_file, \'a\') as f: f.write(log_entry) def start_logging(self): if not self.hook_installed: sys.addaudithook(self._hook) self.hook_installed = True def stop_logging(self): if self.hook_installed: self.hook_installed = False # CPython does not have a direct way to remove audit hooks, # so the hook remains but self.event_names can be set to an empty list # Example usage logger = AuditEventLogger([\\"os.remove\\", \\"shutil.move\\"], \\"audit_log.txt\\") logger.start_logging() # Trigger some events import os import shutil try: os.remove(\\"sample.txt\\") except FileNotFoundError: pass shutil.move(\\"old_dir\\", \\"new_dir\\") logger.stop_logging() ``` **Note:** - Ensure to handle possible exceptions, such as when an event might not be triggered due to missing files (like in `os.remove`).","solution":"import sys class AuditEventLogger: def __init__(self, event_names, log_file): Initializes the logger with a list of event names to track and a path to the log file. Parameters: - event_names (List[str]): The names of the audit events to be logged. - log_file (str): The file where the log entries will be written. self.event_names = event_names self.log_file = log_file self.hook_installed = False def _hook(self, event, args): Internal hook method that logs the specified events and their arguments. Parameters: - event (str): The name of the audit event. - args (tuple): The arguments of the audit event. if event in self.event_names: log_entry = f\\"Event: {event}, Args: {args}n\\" with open(self.log_file, \'a\') as f: f.write(log_entry) def start_logging(self): Starts logging the specified audit events. if not self.hook_installed: sys.addaudithook(self._hook) self.hook_installed = True def stop_logging(self): Stops logging the audit events. if self.hook_installed: self.hook_installed = False # CPython does not have a direct way to remove audit hooks, # so the hook remains but self.event_names can be set to an empty list self.event_names = [] # Note: Since audit events are a feature of the Python interpreter and the standard library, # unit tests should be run in an environment where these events can be triggered."},{"question":"Objective: The objective of this exercise is to test your ability to use the seaborn library to visualize data, customize plots, and interpret the results. Problem Statement: You are given a dataset containing information about penguins, available via seaborn\'s `load_dataset(\\"penguins\\")` function. Your task is to create and customize various histograms to gain insights into the dataset. Function Signature: ```python def visualize_penguins_data(): pass ``` # Requirements: 1. **Loading the Dataset:** - Use the `seaborn` library to load the `penguins` dataset. 2. **Univariate Histograms:** - Plot a univariate histogram for the `flipper_length_mm` column. - Set the number of bins to 20. - Add a kernel density estimate to the histogram. 3. **Bivariate Histograms:** - Plot a bivariate histogram for `bill_length_mm` vs `bill_depth_mm`. - Use a color bar to annotate the colormap. 4. **Hue-Based Histograms:** - Plot a histogram for `flipper_length_mm` with different colors for each penguin `species`. - Display the histograms in a \\"stacked\\" manner. 5. **Advanced Customization:** - Plot a histogram for `body_mass_g` but show unfilled bars. - Modify the histogram to use log-scale for the x-axis. 6. **Subplots:** - Create a grid of subplots (2x2), each featuring one of the histograms described above, with clear annotations for each subplot. # Expected Output: The function should generate and display all the plots as described. No value is returned. # Constraints: - You may only use seaborn and matplotlib for this task. # Additional Notes: - Ensure proper labeling and titles for each plot for clarity. - Handle any missing values in the dataset appropriately. - Use consistent styling and color schemes for better readability and presentation. Example Usage: ```python visualize_penguins_data() ``` Ensure your solution leverages seaborn\'s API effectively to produce informative and well-styled visualizations.","solution":"import seaborn as sns import matplotlib.pyplot as plt def visualize_penguins_data(): # Load the dataset penguins = sns.load_dataset(\\"penguins\\") # Plot configuration plt.figure(figsize=(10, 10)) # 1. Univariate histogram for flipper_length_mm plt.subplot(2, 2, 1) sns.histplot(penguins[\'flipper_length_mm\'], bins=20, kde=True) plt.title(\\"Flipper Length Distribution\\") plt.xlabel(\\"Flipper Length (mm)\\") plt.ylabel(\\"Frequency\\") # 2. Bivariate histogram for bill_length_mm vs bill_depth_mm plt.subplot(2, 2, 2) sns.histplot(penguins, x=\'bill_length_mm\', y=\'bill_depth_mm\', cbar=True) plt.title(\\"Bill Length vs Bill Depth\\") plt.xlabel(\\"Bill Length (mm)\\") plt.ylabel(\\"Bill Depth (mm)\\") # 3. Histogram for flipper_length_mm by species plt.subplot(2, 2, 3) sns.histplot(penguins, x=\'flipper_length_mm\', hue=\'species\', multiple=\\"stack\\") plt.title(\\"Flipper Length by Species\\") plt.xlabel(\\"Flipper Length (mm)\\") plt.ylabel(\\"Count\\") # 4. Unfilled histogram for body_mass_g with log-scale x-axis plt.subplot(2, 2, 4) sns.histplot(penguins[\'body_mass_g\'], bins=20, fill=False) plt.xscale(\'log\') plt.title(\\"Body Mass Distribution\\") plt.xlabel(\\"Body Mass (g)\\") plt.ylabel(\\"Frequency\\") # Display plots plt.tight_layout() plt.show()"},{"question":"**Problem Statement: Directory Synchronization Utility** You are tasked with implementing a utility function that synchronizes two directories. The function should ensure that both directories contain identical files with the same contents. To achieve this, you will use the `filecmp` module for file and directory comparisons. # Requirements: 1. **Function Name**: `synchronize_directories` 2. **Input Parameters**: - `dir1` (str): The path to the first directory. - `dir2` (str): The path to the second directory. 3. **Output**: - A list of actions performed to synchronize the directories. Each action should be represented as a string in the following format: - `\\"Copied {filename} from {source} to {destination}\\"` - `\\"Deleted {filename} from {directory}\\"` - The list should be in chronological order of the performed actions. # Constraints: - You cannot assume that the directories are of the same size or contain similar files. - The synchronization should be one-way: make `dir2` identical to `dir1`. - If a file exists in `dir2` but not in `dir1`, it should be deleted from `dir2`. - If a file exists in `dir1` but not in `dir2`, or if the contents differ, it should be copied from `dir1` to `dir2`. # Example: ```python import os # Example directory setup os.makedirs(\\"dir1\\", exist_ok=True) os.makedirs(\\"dir2\\", exist_ok=True) with open(\\"dir1/file1.txt\\", \\"w\\") as f: f.write(\\"This is file 1 in dir1.\\") with open(\\"dir1/file2.txt\\", \\"w\\") as f: f.write(\\"This is file 2 in dir1.\\") with open(\\"dir2/file2.txt\\", \\"w\\") as f: f.write(\\"This is file 2 in dir2.\\") with open(\\"dir2/file3.txt\\", \\"w\\") as f: f.write(\\"This is file 3 in dir2.\\") # Expected output: [\'Copied file1.txt from dir1 to dir2\', # \'Copied file2.txt from dir1 to dir2\', # \'Deleted file3.txt from dir2\'] print(synchronize_directories(\\"dir1\\", \\"dir2\\")) ``` # Notes: - Use `filecmp.dircmp` and related functions to compare the directories. - Create necessary subdirectories in `dir2` if they exist in `dir1` but not in `dir2`. - Handle file comparison using `filecmp.cmp`. - Ensure to account for dealing with potential exceptions (like permission issues). Implement the function `synchronize_directories` to achieve the described functionality.","solution":"import os import shutil import filecmp def synchronize_directories(dir1, dir2): actions = [] def sync_helper(dcmp): # Compare files that are in both directories for name in dcmp.diff_files: src_file = os.path.join(dcmp.left, name) dst_file = os.path.join(dcmp.right, name) shutil.copy2(src_file, dst_file) actions.append(f\\"Copied {name} from {dcmp.left} to {dcmp.right}\\") # Copy files that are only present in dir1 for name in dcmp.left_only: src_file = os.path.join(dcmp.left, name) dst_file = os.path.join(dcmp.right, name) if os.path.isdir(src_file): shutil.copytree(src_file, dst_file) actions.append(f\\"Copied {name} from {dcmp.left} to {dcmp.right}\\") else: shutil.copy2(src_file, dst_file) actions.append(f\\"Copied {name} from {dcmp.left} to {dcmp.right}\\") # Delete files that are only present in dir2 for name in dcmp.right_only: del_file = os.path.join(dcmp.right, name) if os.path.isdir(del_file): shutil.rmtree(del_file) actions.append(f\\"Deleted {name} from {dcmp.right}\\") else: os.remove(del_file) actions.append(f\\"Deleted {name} from {dcmp.right}\\") # Recursively compare subdirectories for sub_dcmp in dcmp.subdirs.values(): sync_helper(sub_dcmp) # Start the synchronization process dcmp = filecmp.dircmp(dir1, dir2) sync_helper(dcmp) return actions"},{"question":"**ZIP File Operations Using Python\'s `zipfile` Module** **Objective**: This task assesses your ability to work with ZIP files using Python\'s `zipfile` module. # Problem Statement You are given a directory containing several files and a ZIP archive file that can be used to store these files. Your task is to create a Python function `create_zip_and_verify()` that takes two arguments: 1. **source_dir** (str): Path to the directory containing the files to be archived. 2. **zip_file_path** (str): Path where the resultant ZIP archive will be created. The function should: 1. Create a ZIP archive at `zip_file_path` containing all the files from `source_dir`. 2. Use the `ZIP_DEFLATED` compression method for compressing the files. 3. After creating the ZIP archive, verify that all files have been correctly written to the ZIP archive by: - Checking the filenames and sizes of the files in the ZIP archive against the original files in `source_dir`. - Reading each file from the ZIP archive and comparing its content with the original file in `source_dir`. If any discrepancy is found (name, size, or content), raise a `ValueError`. The function should return `True` if all files are correctly archived and verified without any discrepancies. # Constraints: - All files in the `source_dir` will have unique names. - The total number of files in `source_dir` will not exceed 100. - The total size of all files in `source_dir` will not exceed 2 GiB. # Function Signature: ```python def create_zip_and_verify(source_dir: str, zip_file_path: str) -> bool: pass ``` # Example Usage: ```python source_directory = \\"/path/to/source_dir\\" zip_path = \\"/path/to/archive.zip\\" try: result = create_zip_and_verify(source_directory, zip_path) if result: print(\\"ZIP archive created and verified successfully.\\") except ValueError as e: print(f\\"Verification failed: {str(e)}\\") ``` # Additional Information: - You may assume that the `os` module and `zipfile` module are available for use. - Utilize context managers (`with` statement) while working with files to ensure they are properly closed after operations. - Ensure proper error handling, e.g., when files are not accessible or the ZIP file cannot be created. # Relevant Documentation: Refer to the official Python documentation for more details on the `zipfile` module: https://docs.python.org/3/library/zipfile.html","solution":"import os import zipfile def create_zip_and_verify(source_dir: str, zip_file_path: str) -> bool: with zipfile.ZipFile(zip_file_path, \'w\', zipfile.ZIP_DEFLATED) as zipf: for root, _, files in os.walk(source_dir): for file in files: file_path = os.path.join(root, file) zipf.write(file_path, os.path.relpath(file_path, source_dir)) with zipfile.ZipFile(zip_file_path, \'r\') as zipf: zip_info = zipf.infolist() for zip_file_info in zip_info: original_file_path = os.path.join(source_dir, zip_file_info.filename) if not os.path.exists(original_file_path): raise ValueError(f\\"File {zip_file_info.filename} does not exist in the source directory.\\") if os.path.getsize(original_file_path) != zip_file_info.file_size: raise ValueError(f\\"File {zip_file_info.filename} size mismatch.\\") with open(original_file_path, \'rb\') as orig_file, zipf.open(zip_file_info) as zip_file: if orig_file.read() != zip_file.read(): raise ValueError(f\\"File {zip_file_info.filename} content mismatch.\\") return True"},{"question":"# Advanced Python Programming Assessment You are required to implement a `TaskManager` class that simulates a basic task management system. The class should support adding tasks, removing tasks, and listing all tasks. Each task has a name, description, and priority level. Implement appropriate methods and error handling to ensure robust functionality. Specifications 1. **TaskManager Class:** - `add_task(name: str, description: str, priority: int) -> None`: Adds a new task with the given name, description, and priority. - Raise a `ValueError` if a task with the same name already exists. - `remove_task(name: str) -> None`: Removes the task with the given name. - Raise a `KeyError` if the task does not exist. - `list_tasks(sorted_by: str = \\"name\\") -> List[Dict[str, Any]]`: Lists all tasks. The `sorted_by` parameter specifies the sorting criterion and can be either `\\"name\\"` or `\\"priority\\"` (default is `\\"name\\"`). 2. **Task Representation:** - Each task should be represented as a dictionary with the following keys: `name`, `description`, and `priority`. 3. **Additional Requirements:** - Implement function annotations and ensure PEP8 coding style. - Use list comprehensions where appropriate. - Handle any other edge cases gracefully. Example Usage ```python tm = TaskManager() tm.add_task(\\"Task1\\", \\"Description1\\", 1) tm.add_task(\\"Task2\\", \\"Description2\\", 2) # This should raise a ValueError tm.add_task(\\"Task1\\", \\"Description3\\", 3) tm.remove_task(\\"Task1\\") # This should raise a KeyError tm.remove_task(\\"Task1\\") tasks = tm.list_tasks(sorted_by=\\"priority\\") print(tasks) # Expected Output: [{\'name\': \'Task2\', \'description\': \'Description2\', \'priority\': 2}] ``` Constraints - All task names are unique strings. - Descriptions are strings. - Priority is an integer between 1 and 10. - The `TaskManager` class should handle up to 1000 tasks efficiently.","solution":"from typing import List, Dict, Any class TaskManager: def __init__(self): self.tasks = {} def add_task(self, name: str, description: str, priority: int) -> None: if name in self.tasks: raise ValueError(f\\"Task with the name \'{name}\' already exists.\\") self.tasks[name] = { \'name\': name, \'description\': description, \'priority\': priority } def remove_task(self, name: str) -> None: if name not in self.tasks: raise KeyError(f\\"Task with the name \'{name}\' does not exist.\\") del self.tasks[name] def list_tasks(self, sorted_by: str = \\"name\\") -> List[Dict[str, Any]]: if sorted_by == \\"name\\": return sorted(self.tasks.values(), key=lambda x: x[\'name\']) elif sorted_by == \\"priority\\": return sorted(self.tasks.values(), key=lambda x: x[\'priority\']) else: raise ValueError(f\\"Unknown sorted_by criterion \'{sorted_by}\'\\")"},{"question":"**Objective**: To assess the student\'s ability to work with PyTorch tensors and understand the `torch.Size` class to manipulate tensor dimensions. **Question**: You are given a multi-dimensional tensor `x` created using PyTorch. Your task is to write a function `tensor_operations` that performs the following operations: 1. Determines the size of the tensor `x`. 2. Splits the tensor into two tensors along a specified dimension. 3. Optimizes the memory usage by converting the tensors into a specified data type. 4. Returns the sizes of the new tensors as a tuple. **Function Signature**: ```python def tensor_operations(x: torch.Tensor, split_dim: int, dtype: torch.dtype) -> (torch.Size, torch.Size): pass ``` **Input**: - `x` (torch.Tensor): The original multi-dimensional tensor. - `split_dim` (int): The dimension along which to split the tensor. - `dtype` (torch.dtype): The target data type to convert the new tensors to. **Output**: - A tuple of `torch.Size` objects representing the sizes of the two new tensors. **Constraints**: - `split_dim` must be a valid dimension for the tensor `x`. - The data type conversion should be valid for the tensor `x`. **Example**: ```python import torch # Example tensor x = torch.randn(10, 20, 30) # Convert tensor to int and split along dimension 1 split_dim = 1 dtype = torch.int # Function call result_sizes = tensor_operations(x, split_dim, dtype) # Sample output (sizes might vary based on the random tensor) # (torch.Size([10, 10, 30]), torch.Size([10, 10, 30])) print(result_sizes) ``` **Notes**: - Ensure that you handle edge cases such as invalid dimensions and invalid data types gracefully. - Utilize the functionalities provided by the `torch.Size` class to manipulate and obtain sizes of the new tensors. - Consider performance implications of splitting tensors and converting data types.","solution":"import torch def tensor_operations(x: torch.Tensor, split_dim: int, dtype: torch.dtype) -> (torch.Size, torch.Size): Perform operations on the tensor: 1. Determine the size of the tensor x. 2. Split the tensor along the specified dimension. 3. Convert the tensors to the specified data type. 4. Return the sizes of the new tensors. Args: x: torch.Tensor - The original multi-dimensional tensor. split_dim: int - The dimension along which to split the tensor. dtype: torch.dtype - The target data type to convert the new tensors to. Returns: (torch.Size, torch.Size) - Tuple containing the sizes of the two new tensors. # Determine the size of the tensor original_size = x.size() # Split the tensor along the specified dimension split_tensors = torch.split(x, original_size[split_dim] // 2, dim=split_dim) # Convert the split tensors to the specified data type tensor1_converted = split_tensors[0].to(dtype) tensor2_converted = split_tensors[1].to(dtype) # Return the sizes of the new tensors return tensor1_converted.size(), tensor2_converted.size()"},{"question":"You are required to demonstrate your proficiency in using Seaborn by creating a custom heatmap from the provided dataset. Follow these steps to complete the task: 1. Load the \\"flights\\" dataset from Seaborn. 2. Pivot the dataset to have years as rows, months as columns, and the number of passengers as values. 3. Create a heatmap from this pivoted table with the following customizations: - Annotate each cell with the passenger count. - Use a formatting string to show the annotation up to one decimal place. - Add lines between the cells with a linewidth of 0.5. - Use the \\"YlGnBu\\" colormap to visualize the heatmap. 4. Set the colormap to only show values that fall between 100 and 600 passengers. 5. Customize the plotting area by: - Removing the xlabel and ylabel. - Moving the x-axis ticks to the top of the plot. # Specifications - **Input**: The code will not take any input. Your task is purely to write a function that accomplishes the above steps. - **Output**: The function should display the customized heatmap. - **Constraints**: - You must use the Seaborn and Pandas libraries for data manipulation and plotting. - The code should load the dataset from `seaborn.load_dataset`. # Function Signature ```python import seaborn as sns import matplotlib.pyplot as plt def create_custom_heatmap(): # Your code starts here pass ``` # Example When you run the function `create_custom_heatmap()`, it should display a heatmap as per the specifications above. # Notes - Ensure your heatmap is clearly annotated and visually appealing according to the specifications. - Make sure the colormap properly represents values within the range of interest (100–600). - Label removal and tick relocation are critical visual customizations to verify your understanding of the Seaborn and Matplotlib integration.","solution":"import seaborn as sns import matplotlib.pyplot as plt import pandas as pd def create_custom_heatmap(): # Load the flights dataset from Seaborn flights = sns.load_dataset(\'flights\') # Pivot the dataset to get years as rows, months as columns, and number of passengers as values pivot_table = flights.pivot_table(index=\'year\', columns=\'month\', values=\'passengers\') # Create the heatmap with customizations plt.figure(figsize=(10, 8)) ax = sns.heatmap( pivot_table, annot=True, fmt=\\".1f\\", linewidths=0.5, cmap=\\"YlGnBu\\", vmin=100, vmax=600 ) # Customize the plot ax.set_xlabel(\'\') ax.set_ylabel(\'\') ax.xaxis.tick_top() # Display the heatmap plt.show()"},{"question":"**Objective:** You are required to implement a Python function using the `os` module which performs the following tasks: 1. Creates a new directory. 2. Inside this directory, creates a specified number of empty files. 3. Writes a given string into each file. 4. Lists all the files in the directory and returns their names along with their sizes. **Function Signature:** ```python def manage_files(dir_name: str, num_files: int, content: str) -> list: Create a directory, create a specified number of files within it, write content into each file, and list all files with their sizes. Parameters: dir_name (str): The name of the directory to be created. num_files (int): The number of files to be created inside the directory. content (str): The content to be written into each file. Returns: list: A list of tuples where each tuple contains the file name and its size. ``` **Constraints:** - `dir_name` is a non-empty string. - `num_files` is a non-negative integer. - `content` is a non-empty string. **Example:** ```python manage_files(\\"test_dir\\", 3, \\"Sample content\\") ``` **Expected Output:** ```python [(\\"file_1.txt\\", 14), (\\"file_2.txt\\", 14), (\\"file_3.txt\\", 14)] ``` **Task Guidelines:** 1. Use `os` module functions to create the directory, files, and handle file operations. 2. Ensure to handle potential exceptions, such as the directory already existing. 3. Return a list of tuples where each tuple contains the filename and the size of the file in bytes. **Note:** The function must create a directory and files in a way that it does not leave residual files upon multiple runs if the directory already exists.","solution":"import os def manage_files(dir_name: str, num_files: int, content: str) -> list: Create a directory, create a specified number of files within it, write content into each file, and list all files with their sizes. Parameters: dir_name (str): The name of the directory to be created. num_files (int): The number of files to be created inside the directory. content (str): The content to be written into each file. Returns: list: A list of tuples where each tuple contains the file name and its size. if not os.path.exists(dir_name): os.makedirs(dir_name) files_info = [] for i in range(1, num_files + 1): file_name = f\\"file_{i}.txt\\" file_path = os.path.join(dir_name, file_name) with open(file_path, \'w\') as file: file.write(content) file_size = os.path.getsize(file_path) files_info.append((file_name, file_size)) return files_info"},{"question":"**Objective:** Demonstrate your understanding of the seaborn library by creating and applying custom color palettes to a dataset. **Dataset:** You are provided with a dataset containing information about different species of flowers. The dataset has the following columns: - `sepal_length`: Length of the sepal (in cm). - `sepal_width`: Width of the sepal (in cm). - `petal_length`: Length of the petal (in cm). - `petal_width`: Width of the petal (in cm). - `species`: Species of the flower. **Instructions:** 1. Load the given dataset into a pandas DataFrame. 2. Create a custom sequential color palette using seaborn\'s light_palette function. Define your palette using a named color, a hex code, and the HUSL system. Use different color palettes for each unique species of flower. 3. Increase the number of colors in one of the palettes to 10. 4. Return a continuous colormap for one of the palettes. 5. Create a scatter plot of `sepal_length` vs. `sepal_width`, color-coded by species. Apply your custom color palettes to the scatter plot. 6. Save the plot as `flower_scatter_plot.png`. **Constraints:** - Use seaborn version >= 0.11 and matplotlib for plotting. - Ensure that your code is well-commented and follows best practices. **Input Format:** A CSV file named `flower_data.csv` containing the dataset. **Output Format:** A saved plot image file named `flower_scatter_plot.png`. **Example Code Snippet:** ```python import seaborn as sns import pandas as pd import matplotlib.pyplot as plt # Load the dataset df = pd.read_csv(\'flower_data.csv\') # Create custom color palettes palette_named = sns.light_palette(\\"seagreen\\", 8) palette_hex = sns.light_palette(\\"#79C\\", 8) palette_husl = sns.light_palette((20, 60, 50), input=\\"husl\\") # Increase number of colors palette_named_extended = sns.light_palette(\\"xkcd:copper\\", 10) # Return continuous colormap cmap_husl = sns.light_palette(\\"#a275ac\\", as_cmap=True) # Create scatter plot plt.figure(figsize=(10, 6)) sns.scatterplot(data=df, x=\'sepal_length\', y=\'sepal_width\', hue=\'species\', palette=palette_named_extended) plt.title(\'Sepal Length vs. Sepal Width\') plt.xlabel(\'Sepal Length (cm)\') plt.ylabel(\'Sepal Width (cm)\') # Save the plot plt.savefig(\'flower_scatter_plot.png\') plt.show() ```","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt def create_and_apply_color_palettes(csv_file): Function to load dataset, create custom color palettes, generate a scatter plot and save it. # Load the dataset df = pd.read_csv(csv_file) # Create custom color palettes using different methods palette_named = sns.light_palette(\\"seagreen\\", 8) palette_hex = sns.light_palette(\\"#79C\\", 8) palette_husl = sns.light_palette((20, 60, 50), input=\\"husl\\") # Increase number of colors in one of the palettes palette_named_extended = sns.light_palette(\\"xkcd:copper\\", 10) # Return a continuous colormap for one of the palettes cmap_husl = sns.light_palette(\\"#a275ac\\", as_cmap=True) # Create scatter plot of sepal_length vs. sepal_width, color-coded by species plt.figure(figsize=(10, 6)) sns.scatterplot(data=df, x=\'sepal_length\', y=\'sepal_width\', hue=\'species\', palette=palette_named_extended) plt.title(\'Sepal Length vs. Sepal Width\') plt.xlabel(\'Sepal Length (cm)\') plt.ylabel(\'Sepal Width (cm)\') # Save the plot as \'flower_scatter_plot.png\' plot_filename = \'flower_scatter_plot.png\' plt.savefig(plot_filename) plt.show() # Return the name of the saved plot for verification return plot_filename"},{"question":"Complex Number Operations You are required to implement functions to perform various operations on complex numbers using the given C API for complex numbers (`Py_complex`) and their corresponding Python object (`PyComplexObject`). Using this API, write functions in Python that: 1. **Convert a complex number from Python to the C structure and back**. 2. **Perform arithmetic operations** (addition, subtraction, multiplication, division, and exponentiation) using the C API functions, and return the result as a Python complex number. 3. **Negate a given complex number** using the C API and return the result as a Python complex number. You need to implement the following functions: 1. `convert_to_c(py_num: complex) -> Py_complex` 2. `convert_to_py(c_num: Py_complex) -> complex` 3. `c_addition(left: complex, right: complex) -> complex` 4. `c_subtraction(left: complex, right: complex) -> complex` 5. `c_multiplication(left: complex, right: complex) -> complex` 6. `c_division(left: complex, right: complex) -> complex` 7. `c_exponentiation(base: complex, exp: complex) -> complex` 8. `c_negation(num: complex) -> complex` # Input and Output Formats: 1. **convert_to_c(py_num: complex) -> Py_complex** - **Input**: A Python complex number - **Output**: A `Py_complex` C structure representing the complex number 2. **convert_to_py(c_num: Py_complex) -> complex** - **Input**: A `Py_complex` C structure - **Output**: A Python complex number representing the C structure 3. **c_addition(left: complex, right: complex) -> complex** - **Input**: Two Python complex numbers `left` and `right` - **Output**: A Python complex number which is the result of `left + right` 4. **c_subtraction(left: complex, right: complex) -> complex** - **Input**: Two Python complex numbers `left` and `right` - **Output**: A Python complex number which is the result of `left - right` 5. **c_multiplication(left: complex, right: complex) -> complex** - **Input**: Two Python complex numbers `left` and `right` - **Output**: A Python complex number which is the result of `left * right` 6. **c_division(left: complex, right: complex) -> complex** - **Input**: Two Python complex numbers `left` and `right` - **Output**: A Python complex number which is the result of `left / right`. If `right` is `0`, it raises a `ZeroDivisionError`. 7. **c_exponentiation(base: complex, exp: complex) -> complex** - **Input**: Two Python complex numbers `base` and `exp` - **Output**: A Python complex number which is the result of `base ** exp` 8. **c_negation(num: complex) -> complex** - **Input**: A Python complex number `num` - **Output**: A Python complex number which is the result of `-num` Constraints - Make sure to handle the cases where complex number division by zero or invalid exponentiation cases as mentioned in the documentation occur. - Use the provided C API functions to perform arithmetic operations on complex numbers and handle conversion between Python and C complex number types. - Your implementation should use the C API functions `_Py_c_sum`, `_Py_c_diff`, `_Py_c_neg`, `_Py_c_prod`, `_Py_c_quot`, `_Py_c_pow`. Example ```python # Example of how to use the functions implemented # Convert Python complex number to C structure and back py_num = 3 + 4j c_num = convert_to_c(py_num) py_result = convert_to_py(c_num) print(py_result) # Output: (3+4j) # Perform addition using C API result = c_addition(3+4j, 1+2j) print(result) # Output: (4+6j) # Perform subtraction using C API result = c_subtraction(3+4j, 1+2j) print(result) # Output: (2+2j) # Perform multiplication using C API result = c_multiplication(3+4j, 1+2j) print(result) # Output: (-5+10j) # Perform division using C API result = c_division(3+4j, 1+2j) print(result) # Output: (2.2-0.4j) # Perform exponentiation using C API result = c_exponentiation(3+4j, 1+2j) print(result) # Example Output: (0.129009594074467+0.0339240929051706j) # Perform negation using C API result = c_negation(3+4j) print(result) # Output: (-3-4j) ```","solution":"import ctypes class Py_complex(ctypes.Structure): _fields_ = [(\\"real\\", ctypes.c_double), (\\"imag\\", ctypes.c_double)] def convert_to_c(py_num: complex) -> Py_complex: return Py_complex(py_num.real, py_num.imag) def convert_to_py(c_num: Py_complex) -> complex: return complex(c_num.real, c_num.imag) def c_addition(left: complex, right: complex) -> complex: left_c = convert_to_c(left) right_c = convert_to_c(right) result_c = Py_complex( left_c.real + right_c.real, left_c.imag + right_c.imag ) return convert_to_py(result_c) def c_subtraction(left: complex, right: complex) -> complex: left_c = convert_to_c(left) right_c = convert_to_c(right) result_c = Py_complex( left_c.real - right_c.real, left_c.imag - right_c.imag ) return convert_to_py(result_c) def c_multiplication(left: complex, right: complex) -> complex: left_c = convert_to_c(left) right_c = convert_to_c(right) real_part = left_c.real * right_c.real - left_c.imag * right_c.imag imag_part = left_c.real * right_c.imag + left_c.imag * right_c.real result_c = Py_complex(real_part, imag_part) return convert_to_py(result_c) def c_division(left: complex, right: complex) -> complex: left_c = convert_to_c(left) right_c = convert_to_c(right) if right_c.real == 0 and right_c.imag == 0: raise ZeroDivisionError(\\"division by zero\\") denom = right_c.real * right_c.real + right_c.imag * right_c.imag real_part = (left_c.real * right_c.real + left_c.imag * right_c.imag) / denom imag_part = (left_c.imag * right_c.real - left_c.real * right_c.imag) / denom result_c = Py_complex(real_part, imag_part) return convert_to_py(result_c) def c_exponentiation(base: complex, exp: complex) -> complex: import cmath result = cmath.exp(exp * cmath.log(base)) return result def c_negation(num: complex) -> complex: num_c = convert_to_c(num) result_c = Py_complex(-num_c.real, -num_c.imag) return convert_to_py(result_c)"},{"question":"# Custom Exception Handling and Chaining in Python **Objective:** Create a Python program that demonstrates understanding and proper usage of various built-in exceptions along with custom exception handling and chaining. **Problem Statement:** You are tasked with simulating a banking system where users can perform various operations such as depositing and withdrawing money, as well as transferring funds to other users. Your job is to implement these functionalities while ensuring that any faulty operation raises appropriate exceptions. Additionally, you need to create custom exceptions to handle specific scenarios in the banking context. Make sure to implement exception chaining to provide detailed error messages. **Specifications:** 1. **Classes and Methods:** - `class InsufficientFundsError(Exception)`: Custom exception indicating insufficient funds. - `class UnauthorizedAccessError(Exception)`: Custom exception raised when an unauthorized action is attempted. - `class BankingSystem`: - `__init__(self)`: Initializes the banking system with an empty user database. - `create_user(self, user_id, initial_balance)`: Creates a new user with an initial balance. - `deposit(self, user_id, amount)`: Deposits the given amount into the user\'s account. - `withdraw(self, user_id, amount)`: Withdraws the given amount from the user\'s account, raising `InsufficientFundsError` if the balance is insufficient. - `transfer(self, from_user_id, to_user_id, amount)`: Transfers the given amount from one user to another. Raises `InsufficientFundsError` if the balance of the sending user is insufficient and `UnauthorizedAccessError` if either user does not exist. - `display_balance(self, user_id)`: Prints the balance of the specified user. 2. **Exception Chaining:** - When `InsufficientFundsError` or `UnauthorizedAccessError` is raised, capture the exception and chain it with a `RuntimeError` to provide additional context. 3. **Example Usage:** ```python try: bank_system = BankingSystem() bank_system.create_user(\\"user1\\", 100) bank_system.create_user(\\"user2\\", 500) bank_system.deposit(\\"user1\\", 50) bank_system.withdraw(\\"user1\\", 200) # Should raise InsufficientFundsError bank_system.transfer(\\"user1\\", \\"user3\\", 50) # Should raise UnauthorizedAccessError except Exception as ex: print(\\"An error occurred:\\", ex) if ex.__context__: print(\\"Context:\\", ex.__context__) ``` **Constraints:** - User IDs are strings. - All monetary amounts are non-negative integers. - You may assume that the user IDs provided for any operation are unique when creating users. **Expected Output:** - Properly handled exceptions with chained context. - User-friendly error messages indicating what went wrong. Implementing the above specifications should demonstrate a deep understanding of exception handling, custom exceptions, and exception chaining in Python.","solution":"class InsufficientFundsError(Exception): pass class UnauthorizedAccessError(Exception): pass class BankingSystem: def __init__(self): self.users = {} def create_user(self, user_id, initial_balance): if user_id in self.users: raise ValueError(f\\"User {user_id} already exists.\\") self.users[user_id] = initial_balance def deposit(self, user_id, amount): if user_id not in self.users: raise UnauthorizedAccessError(f\\"User {user_id} does not exist.\\") self.users[user_id] += amount def withdraw(self, user_id, amount): if user_id not in self.users: raise UnauthorizedAccessError(f\\"User {user_id} does not exist.\\") if self.users[user_id] < amount: raise InsufficientFundsError(f\\"User {user_id} has insufficient funds.\\") self.users[user_id] -= amount def transfer(self, from_user_id, to_user_id, amount): if from_user_id not in self.users or to_user_id not in self.users: raise UnauthorizedAccessError(\\"One or both users do not exist.\\") if self.users[from_user_id] < amount: raise InsufficientFundsError(f\\"User {from_user_id} has insufficient funds.\\") self.users[from_user_id] -= amount self.users[to_user_id] += amount def display_balance(self, user_id): if user_id not in self.users: raise UnauthorizedAccessError(f\\"User {user_id} does not exist.\\") return self.users[user_id]"},{"question":"Implement a Custom TCP Chat Server and Client using asyncio Transports and Protocols Objective: The objective of this task is to evaluate students\' understanding of asyncio transports and protocols. You will implement a simple TCP chat server and client where multiple clients can connect to the server, send messages, and receive broadcast messages from other clients. Task: 1. Implement a custom TCP chat server using asyncio\'s low-level transport and protocol APIs. 2. Implement a custom TCP chat client that can connect to the server and participate in the chat. Requirements: 1. **Chat Server**: - It should accept multiple client connections. - Broadcast received messages to all connected clients except the sender. - Handle client disconnections gracefully. 2. **Chat Client**: - Connect to the server and allow the user to send and receive messages. - Display incoming messages from other users while still allowing the user to type and send their own messages. Specifications: 1. **Server Implementation** (`ChatServerProtocol`): - Use `Protocol` base class. - Methods to implement: - `connection_made`: Called when a new client connects. - `data_received`: Handle incoming messages and broadcast to other clients. - `connection_lost`: Handle client disconnections. 2. **Client Implementation** (`ChatClientProtocol`): - Use `Protocol` base class. - Methods to implement: - `connection_made`: Handle the establishment of a connection to the server. - `data_received`: Display incoming messages to the user. - `connection_lost`: Handle the server closing the connection. 3. **Input/Output**: - **Server**: No specific input, runs indefinitely to handle multiple client connections. - **Client**: Accepts user input for messages and displays messages from other clients. 4. **Constraints**: - Implement both server and client using asyncio transports and protocols. - Ensure proper cleanup of connections. Example: ```python import asyncio # Chat Server Implementation class ChatServerProtocol(asyncio.Protocol): clients = [] def connection_made(self, transport): self.transport = transport self.peername = transport.get_extra_info(\'peername\') self.clients.append(self) print(f\\"Connection from {self.peername}\\") def data_received(self, data): message = data.decode() print(f\\"Message from {self.peername}: {message}\\") for client in self.clients: if client is not self: client.transport.write(data) def connection_lost(self, exc): print(f\\"Connection lost with {self.peername}\\") self.clients.remove(self) # Chat Client Implementation class ChatClientProtocol(asyncio.Protocol): def connection_made(self, transport): self.transport = transport print(\\"Connected to the server\\") def data_received(self, data): message = data.decode() print(f\\"Message from server: {message}\\") def connection_lost(self, exc): print(\\"The server closed the connection\\") async def start_server(): loop = asyncio.get_running_loop() server = await loop.create_server(lambda: ChatServerProtocol(), \'127.0.0.1\', 8888) async with server: await server.serve_forever() async def start_client(): loop = asyncio.get_running_loop() transport, protocol = await loop.create_connection(lambda: ChatClientProtocol(), \'127.0.0.1\', 8888) while True: message = input(\\"Enter message: \\") transport.write(message.encode()) await asyncio.sleep(1) if __name__ == \\"__main__\\": choice = input(\\"Run as server or client? (s/c): \\") if choice == \'s\': asyncio.run(start_server()) elif choice == \'c\': asyncio.run(start_client()) else: print(\\"Invalid choice\\") ``` Deliverables: - Complete implementation of the `ChatServerProtocol` and `ChatClientProtocol`. - Code should be clean, well-commented, and should follow Python standards. - The server should handle multiple clients and broadcast messages appropriately. - The client should seamlessly send and receive messages from the server.","solution":"import asyncio class ChatServerProtocol(asyncio.Protocol): clients = [] def connection_made(self, transport): self.transport = transport self.peername = transport.get_extra_info(\'peername\') self.clients.append(self) print(f\\"Connection from {self.peername}\\") def data_received(self, data): message = data.decode() print(f\\"Message from {self.peername}: {message}\\") for client in self.clients: if client is not self: client.transport.write(data) def connection_lost(self, exc): print(f\\"Connection lost with {self.peername}\\") self.clients.remove(self) class ChatClientProtocol(asyncio.Protocol): def connection_made(self, transport): self.transport = transport print(\\"Connected to the server\\") def data_received(self, data): message = data.decode() print(f\\"Message from server: {message}\\") def connection_lost(self, exc): print(\\"The server closed the connection\\") async def start_server(): loop = asyncio.get_running_loop() server = await loop.create_server(lambda: ChatServerProtocol(), \'127.0.0.1\', 8888) async with server: await server.serve_forever() async def start_client(): loop = asyncio.get_running_loop() transport, protocol = await loop.create_connection(lambda: ChatClientProtocol(), \'127.0.0.1\', 8888) while True: message = input(\\"Enter message: \\") transport.write(message.encode()) await asyncio.sleep(1) if __name__ == \\"__main__\\": choice = input(\\"Run as server or client? (s/c): \\") if choice == \'s\': asyncio.run(start_server()) elif choice == \'c\': asyncio.run(start_client()) else: print(\\"Invalid choice\\")"},{"question":"Imagine you are developing a small utility program to manage and update plist configuration files used on macOS. Your task is to write a function `modify_plist` that will perform the following actions: 1. **Read**: Read an existing plist file from a given file path. The file may be in either XML or binary format. 2. **Modify**: Modify the content of the plist by: - Adding a new key-value pair to the top-level dictionary. The key and value will be provided as arguments to the function. - Updating an existing key with a new value if the key already exists in the dictionary. 3. **Write**: Write the modified dictionary back to a new plist file in the specified format (XML or binary). # Function Signature ```python import plistlib def modify_plist(input_file: str, output_file: str, new_key: str, new_value, output_format: str) -> None: Modify a plist file by adding or updating a key-value pair and saving it to a new file. Args: input_file (str): The file path to the existing plist file to be read. output_file (str): The file path to save the modified plist file. new_key (str): The key to add or update in the plist. new_value: The value to be associated with the new_key. output_format (str): The format to write the output file (\'xml\' or \'binary\'). Raises: ValueError: If output_format is not \'xml\' or \'binary\'. pass ``` # Constraints and Assumptions: - `input_file` and `output_file` are valid file paths. - `new_key` is a string. - `new_value` can be any of the supported plist data types (string, integer, float, boolean, tuple, list, dictionary with string keys, bytes, bytearray, or datetime). - `output_format` is either `\'xml\'` or `\'binary\'`. If it\'s not, the function should raise a `ValueError`. - The function should handle any exceptions that occur during reading or writing the plist file and output an appropriate error message. # Examples ```python # Example 1: Modifying an XML plist file modify_plist(\'config.plist\', \'updated_config.plist\', \'newSetting\', True, \'xml\') # Example 2: Modifying a binary plist file modify_plist(\'config_binary.plist\', \'updated_config_binary.plist\', \'maxUsers\', 150, \'binary\') ``` In these examples, `modify_plist` should read the contents of \'config.plist\', add or update the key-value pair `(\'newSetting\', True)`, and then write the modified content to \'updated_config.plist\' in XML format. Similarly, it should handle the binary plist in the second example.","solution":"import plistlib def modify_plist(input_file: str, output_file: str, new_key: str, new_value, output_format: str) -> None: Modify a plist file by adding or updating a key-value pair and saving it to a new file. Args: input_file (str): The file path to the existing plist file to be read. output_file (str): The file path to save the modified plist file. new_key (str): The key to add or update in the plist. new_value: The value to be associated with the new_key. output_format (str): The format to write the output file (\'xml\' or \'binary\'). Raises: ValueError: If output_format is not \'xml\' or \'binary\'. Exception: If there are issues reading or writing the plist file. if output_format not in [\'xml\', \'binary\']: raise ValueError(\\"Invalid output format. Must be \'xml\' or \'binary\'.\\") try: # Read the input plist file with open(input_file, \'rb\') as f: plist_content = plistlib.load(f) # Modify the plist content plist_content[new_key] = new_value # Write the modified content to the output plist file with open(output_file, \'wb\') as f: if output_format == \'xml\': plistlib.dump(plist_content, f, fmt=plistlib.FMT_XML) else: plistlib.dump(plist_content, f, fmt=plistlib.FMT_BINARY) except Exception as e: raise Exception(f\\"An error occurred while modifying the plist file: {e}\\")"},{"question":"You are given a complex data structure representing an employee directory. Each employee record contains the following information: - Employee ID (unsigned integer) - Name (string) - Age (unsigned integer) - Gender (enum: 0 for Male, 1 for Female, 2 for Other) - Salary (double) - Active status (boolean) You are required to: 1. Pack a list of such employee records into XDR format using the `xdrlib.Packer` class. 2. Unpack the XDR formatted data back into the original list of employee records using the `xdrlib.Unpacker` class. # Input - A list of employee records, where each record is a dictionary containing the keys: `employee_id`, `name`, `age`, `gender`, `salary`, `is_active`. # Output - The original list of employee records after packing and unpacking the data using `xdrlib`. # Constraints - You can assume that the employee list is non-empty. - The name of each employee is non-empty and can be up to 50 characters long. # Function Signature ```python def pack_and_unpack_employees(employee_list: list) -> list: pass ``` # Example Input ```python employee_list = [ {\'employee_id\': 1, \'name\': \'Alice\', \'age\': 30, \'gender\': 1, \'salary\': 70000.50, \'is_active\': True}, {\'employee_id\': 2, \'name\': \'Bob\', \'age\': 25, \'gender\': 0, \'salary\': 50000.75, \'is_active\': False} ] ``` Output ```python [ {\'employee_id\': 1, \'name\': \'Alice\', \'age\': 30, \'gender\': 1, \'salary\': 70000.50, \'is_active\': True}, {\'employee_id\': 2, \'name\': \'Bob\', \'age\': 25, \'gender\': 0, \'salary\': 50000.75, \'is_active\': False} ] ``` # Implementation Steps 1. Use the `xdrlib.Packer` to pack the data into an XDR format. 2. Use the `xdrlib.Unpacker` to unpack the data back into the original list format. 3. Ensure the unpacked data matches the original data structure. Implement the function `pack_and_unpack_employees` which performs the described operations.","solution":"import xdrlib def pack_and_unpack_employees(employee_list: list) -> list: packer = xdrlib.Packer() # Packing the list of employee records packer.pack_uint(len(employee_list)) for employee in employee_list: packer.pack_uint(employee[\'employee_id\']) packer.pack_string(employee[\'name\'].encode()) packer.pack_uint(employee[\'age\']) packer.pack_uint(employee[\'gender\']) packer.pack_double(employee[\'salary\']) packer.pack_bool(employee[\'is_active\']) packed_data = packer.get_buffer() # Unpacking the packed data unpacker = xdrlib.Unpacker(packed_data) num_employees = unpacker.unpack_uint() result = [] for _ in range(num_employees): employee_id = unpacker.unpack_uint() name = unpacker.unpack_string().decode() age = unpacker.unpack_uint() gender = unpacker.unpack_uint() salary = unpacker.unpack_double() is_active = unpacker.unpack_bool() result.append({ \'employee_id\': employee_id, \'name\': name, \'age\': age, \'gender\': gender, \'salary\': salary, \'is_active\': is_active }) return result"},{"question":"Objective To assess your understanding of the `urllib` package in Python, specifically focusing on URL requests, parsing URLs, and handling errors. Problem Statement Create a function `fetch_and_parse_urls` that takes a list of URLs as input, fetches the content of each URL, and returns a dictionary containing the parsed components of each URL and the length of the fetched content. Your function should: 1. Take a list of URLs `url_list` as its parameter. 2. For each URL in the list, fetch the content using the `urllib.request` module and handle any possible exceptions that may arise (e.g., HTTPError, URLError). 3. Parse the URL using the `urllib.parse` module to break it down into its component parts (scheme, netloc, path, params, query, fragment). 4. Store the parsed components and the length of the fetched content in a dictionary format, where the key is the URL itself and the value is another dictionary containing the parsed components and fetched content length. 5. Return the final dictionary containing the information for all URLs. Input - `url_list`: A list of strings where each string is a URL. Example: `[\\"https://www.example.com\\", \\"https://www.python.org\\"]` Output - A dictionary where each key is a URL from the input list, and the value is a dictionary with the following structure: ```python { \'scheme\': scheme, \'netloc\': netloc, \'path\': path, \'params\': params, \'query\': query, \'fragment\': fragment, \'content_length\': length_of_fetched_content } ``` Constraints - URLs in `url_list` will be valid URLs, but note that they may not always be reachable (handle exceptions appropriately). - The function must handle network-related exceptions gracefully. Example ```python def fetch_and_parse_urls(url_list): # Your implementation here urls = [\\"https://www.example.com\\", \\"https://www.python.org\\"] output = fetch_and_parse_urls(urls) print(output) ``` Expected Output: ```python { \\"https://www.example.com\\": { \'scheme\': \'https\', \'netloc\': \'www.example.com\', \'path\': \'/\', \'params\': \'\', \'query\': \'\', \'fragment\': \'\', \'content_length\': 1256 # Assuming content length is 1256 bytes }, \\"https://www.python.org\\": { \'scheme\': \'https\', \'netloc\': \'www.python.org\', \'path\': \'/\', \'params\': \'\', \'query\': \'\', \'fragment\': \'\', \'content_length\': 8754 # Assuming content length is 8754 bytes } } ``` Use the `urllib.request.urlopen` method to fetch URL content, `urllib.parse.urlparse` to parse the URL, and handle exceptions from `urllib.error`. Performance Requirements - The solution should be able to handle up to 100 URLs promptly, assuming that each URL request might take some time to fetch the data.","solution":"import urllib.request import urllib.parse import urllib.error def fetch_and_parse_urls(url_list): results = {} for url in url_list: try: response = urllib.request.urlopen(url) content = response.read() content_length = len(content) parsed_url = urllib.parse.urlparse(url) url_info = { \'scheme\': parsed_url.scheme, \'netloc\': parsed_url.netloc, \'path\': parsed_url.path, \'params\': parsed_url.params, \'query\': parsed_url.query, \'fragment\': parsed_url.fragment, \'content_length\': content_length } results[url] = url_info except (urllib.error.HTTPError, urllib.error.URLError) as e: results[url] = { \'error\': str(e) } return results"},{"question":"Objective: To test your understanding of TorchScript in PyTorch, your task is to implement and serialize a simple neural network model using `torch.jit.script`. Instructions: 1. **Define a Simple Neural Network Model**: - Define a PyTorch neural network class `SimpleNet` using `torch.nn.Module`. - This network should have: - One hidden layer with 128 units and ReLU activation. - An output layer suitable for a classification task with 10 classes. 2. **Script the Model Using TorchScript**: - Convert the defined model to TorchScript using `torch.jit.script`. 3. **Serialization**: - Serialize the scripted model to a file named `simple_net.pt`. Function Signature: ```python def create_and_serialize_model() -> None: # Define and serialize the model pass ``` Guidelines: - **Model Definition**: - The input layer should take tensors of shape `(batch_size, input_features)` where `input_features = 784` (for example, an unrolled 28x28 image). - Use `torch.nn.Linear` for defining layers. - **TorchScript Conversion**: - Use `torch.jit.script` to convert the network into TorchScript format. - **Serialization**: - Serialize the TorchScript model using the `save` method of the scripted module. - The file must be named `simple_net.pt`. Example: ```python import torch import torch.nn as nn class SimpleNet(nn.Module): def __init__(self): super(SimpleNet, self).__init__() self.fc1 = nn.Linear(784, 128) self.relu = nn.ReLU() self.fc2 = nn.Linear(128, 10) def forward(self, x): x = self.fc1(x) x = self.relu(x) x = self.fc2(x) return x def create_and_serialize_model(): model = SimpleNet() scripted_model = torch.jit.script(model) scripted_model.save(\\"simple_net.pt\\") ``` Constraints: - The solution must be contained within the function `create_and_serialize_model`. - You are allowed to use additional helper functions if required. - Ensure you handle any potential errors in the scripting process. Good luck!","solution":"import torch import torch.nn as nn class SimpleNet(nn.Module): def __init__(self): super(SimpleNet, self).__init__() self.fc1 = nn.Linear(784, 128) self.relu = nn.ReLU() self.fc2 = nn.Linear(128, 10) def forward(self, x): x = self.fc1(x) x = self.relu(x) x = self.fc2(x) return x def create_and_serialize_model(): model = SimpleNet() scripted_model = torch.jit.script(model) scripted_model.save(\\"simple_net.pt\\")"},{"question":"**Coding Assessment Question** # Function Implementation: Custom Sorting with Operator Functions Objective Implement a function `custom_sort(data, key=None, reverse=False)` that sorts a list of tuples based on a specified key using the `operator` module functions. Description - Given a `data` list of tuples, your task is to sort this list based on a specified element within the tuple. - The `key` parameter will be an integer that specifies which element of the tuple should be used as the sort key (0-based index). - The `reverse` parameter indicates the sort order. If `reverse` is `True`, the list should be sorted in descending order; otherwise, it should be sorted in ascending order. **Constraints**: 1. The `data` list is non-empty and contains at least one tuple. 2. Each tuple contains the same number of elements. 3. The `key` parameter is always a valid index for the tuples within `data`. **Input**: - `data`: A list of tuples `[ (v1, v2, ..., vn), (v1, v2, ..., vn), ... ]`. - `key`: An integer representing the index to sort by. - `reverse`: A boolean indicating if the sort should be in descending order. **Output**: - Returns a new list of tuples sorted based on the specified key. Example ```python data = [(1, \'a\', 100), (2, \'b\', 50), (3, \'c\', 75)] key = 2 reverse = False assert custom_sort(data, key, reverse) == [(2, \'b\', 50), (3, \'c\', 75), (1, \'a\', 100)] ``` ```python data = [(1, \'a\', 100), (2, \'b\', 50), (3, \'c\', 75)] key = 0 reverse = True assert custom_sort(data, key, reverse) == [(3, \'c\', 75), (2, \'b\', 50), (1, \'a\', 100)] ``` Implementation Specifications Use the `operator.itemgetter` for retrieving elements from tuples for sorting. Implement the function as follows: ```python from operator import itemgetter def custom_sort(data, key=None, reverse=False): Sorts a list of tuples based on a specified key. Args: data (list of tuple): List of tuples to be sorted. key (int): The index to sort by. reverse (bool): Whether to sort in descending order. Returns: list of tuple: Sorted list of tuples. # Your code here ``` Make sure to: - Use `itemgetter` from the `operator` module to retrieve elements. - Use the `sorted` function with the appropriate `key` and `reverse` parameters. Write comprehensive test cases to validate your solution.","solution":"from operator import itemgetter def custom_sort(data, key=None, reverse=False): Sorts a list of tuples based on a specified key. Args: data (list of tuple): List of tuples to be sorted. key (int): The index to sort by. reverse (bool): Whether to sort in descending order. Returns: list of tuple: Sorted list of tuples. return sorted(data, key=itemgetter(key), reverse=reverse)"},{"question":"**Question: Python `dataclasses` Advanced Exercises** You are given a task to implement a small library management system using Python\'s `dataclasses`. The system should have the following functionalities: 1. Define three dataclasses: `Book`, `Member`, and `Library`. 2. Implement methods to add books and members to the library. 3. Implement methods to lend books to members and return them. 4. Handle cases where books are already lent out. 5. Serialize and deserialize the library’s data to and from dictionaries. Your implementation should cover the creation and manipulation of these dataclasses while ensuring the usage of various `dataclasses` features discussed. # Specifications: 1. **Define the `Book` dataclass:** - `title` (string) - `author` (string) - `isbn` (string) - `available` (boolean, default to `True`) 2. **Define the `Member` dataclass:** - `name` (string) - `member_id` (string) 3. **Define the `Library` dataclass:** - `books` (list of `Book`, default to empty list) - `members` (list of `Member`, default to empty list) 4. **Library Class Methods:** - `add_book(self, book: Book) -> None`: Adds a book to the library. - `add_member(self, member: Member) -> None`: Adds a member to the library. - `lend_book(self, isbn: str, member_id: str) -> bool`: Lends a book to a member if it is available. Returns `True` if successful, `False` otherwise. - `return_book(self, isbn: str) -> bool`: Returns a book to the library. Returns `True` if successful, `False` otherwise. - `to_dict(self) -> dict`: Serializes the library into a dictionary. - `from_dict(cls, data: dict) -> Library`: Deserializes a dictionary into a `Library` object (class method). # Example Usage: ```python from dataclasses import dataclass, field @dataclass class Book: title: str author: str isbn: str available: bool = True @dataclass class Member: name: str member_id: str @dataclass class Library: books: list[Book] = field(default_factory=list) members: list[Member] = field(default_factory=list) def add_book(self, book: Book) -> None: self.books.append(book) def add_member(self, member: Member) -> None: self.members.append(member) def lend_book(self, isbn: str, member_id: str) -> bool: for book in self.books: if book.isbn == isbn and book.available: book.available = False return True return False def return_book(self, isbn: str) -> bool: for book in self.books: if book.isbn == isbn and not book.available: book.available = True return True return False def to_dict(self) -> dict: return { \'books\': [asdict(book) for book in self.books], \'members\': [asdict(member) for member in self.members], } @classmethod def from_dict(cls, data: dict) -> \'Library\': books = [Book(**book_data) for book_data in data.get(\'books\', [])] members = [Member(**member_data) for member_data in data.get(\'members\', [])] return cls(books=books, members=members) # Test the implementation library = Library() book1 = Book(\\"Python Programming\\", \\"Author A\\", \\"ISBN001\\") book2 = Book(\\"Data Science\\", \\"Author B\\", \\"ISBN002\\") member1 = Member(\\"Alice\\", \\"MEM001\\") member2 = Member(\\"Bob\\", \\"MEM002\\") library.add_book(book1) library.add_book(book2) library.add_member(member1) library.add_member(member2) assert library.lend_book(\\"ISBN001\\", \\"MEM001\\") == True assert library.lend_book(\\"ISBN001\\", \\"MEM002\\") == False assert library.return_book(\\"ISBN001\\") == True serialized_library = library.to_dict() new_library = Library.from_dict(serialized_library) assert len(new_library.books) == 2 assert len(new_library.members) == 2 ``` # Constraints: - You must use the `dataclass` decorator and related functionality from the `dataclasses` module. - Ensure proper handling of default mutable types. - Provide meaningful docstrings for your classes and methods.","solution":"from dataclasses import dataclass, field, asdict from typing import List, Dict, ClassVar @dataclass class Book: title: str author: str isbn: str available: bool = True @dataclass class Member: name: str member_id: str @dataclass class Library: books: List[Book] = field(default_factory=list) members: List[Member] = field(default_factory=list) def add_book(self, book: Book) -> None: Add a book to the library. self.books.append(book) def add_member(self, member: Member) -> None: Add a member to the library. self.members.append(member) def lend_book(self, isbn: str, member_id: str) -> bool: Lend a book to a member if it is available. for book in self.books: if book.isbn == isbn and book.available: book.available = False return True return False def return_book(self, isbn: str) -> bool: Return a book to the library. for book in self.books: if book.isbn == isbn and not book.available: book.available = True return True return False def to_dict(self) -> Dict: Serialize the library to a dictionary. return { \'books\': [asdict(book) for book in self.books], \'members\': [asdict(member) for member in self.members], } @classmethod def from_dict(cls, data: Dict) -> \'Library\': Deserialize a dictionary into a Library object. books = [Book(**book_data) for book_data in data.get(\'books\', [])] members = [Member(**member_data) for member_data in data.get(\'members\', [])] return cls(books=books, members=members)"},{"question":"# Question: Permutation Feature Importance for Regression Models You are given the California housing dataset and need to apply permutation feature importance to understand the importance of each feature with respect to a trained regression model. Follow the instructions below to complete the task. Instructions: 1. **Data Preparation**: - Load the California housing dataset using `sklearn.datasets.fetch_california_housing`. - Split the dataset into training and validation sets using `train_test_split` from `sklearn.model_selection`. 2. **Model Training**: - Train a Ridge regression model using the training set. 3. **Permutation Feature Importance**: - Use `permutation_importance` from `sklearn.inspection` to compute the feature importances based on the model\'s performance on the validation set. - Use at least 30 repeats for the permutation. 4. **Output**: - Print the mean and standard deviation of importances for each feature. - Identify the top 3 most important features based on the computed importances. Input and Output Format: 1. **Function Signature**: ```python def compute_permutation_importance() -> List[Tuple[str, float, float]]: pass ``` 2. **Expected Output**: - The function should return a list of tuples where each tuple contains the feature name, mean importance, and standard deviation of importance. - Example: `[(\\"MedInc\\", 0.15, 0.02), (\\"AveRooms\\", 0.10, 0.01), (\\"HouseAge\\", 0.08, 0.02)]`. Constraints: - Use a fixed random state (e.g., `random_state=0`) for reproducibility. - You may refer to the scikit-learn documentation for detailed usage of functions. Performance Requirements: - The implementation should be efficient and compute importances within a reasonable time frame for a dataset of this size. Example Solution Workflow: 1. Load the dataset and split it. 2. Train the Ridge regression model. 3. Compute permutation importances with the specified number of repeats. 4. Print and return the feature importances. Implement the function `compute_permutation_importance` to demonstrate your understanding of permutation feature importance using scikit-learn.","solution":"from typing import List, Tuple from sklearn.datasets import fetch_california_housing from sklearn.model_selection import train_test_split from sklearn.linear_model import Ridge from sklearn.inspection import permutation_importance import numpy as np def compute_permutation_importance() -> List[Tuple[str, float, float]]: # Load the dataset data = fetch_california_housing() X, y = data.data, data.target feature_names = data.feature_names # Split the dataset into training and validation sets X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=0) # Train the Ridge regression model model = Ridge(random_state=0) model.fit(X_train, y_train) # Compute permutation feature importances perm_importance = permutation_importance(model, X_val, y_val, n_repeats=30, random_state=0) importances = perm_importance.importances_mean importances_std = perm_importance.importances_std # Create a list of tuples containing feature name, mean importance, and standard deviation of importance result = [(feature_names[i], importances[i], importances_std[i]) for i in range(len(feature_names))] # Print the mean and standard deviation of importances for each feature for name, mean, std in result: print(f\\"Feature: {name}, Mean importance: {mean}, Std: {std}\\") # Identify the top 3 most important features top_3_features = sorted(result, key=lambda x: x[1], reverse=True)[:3] return top_3_features"},{"question":"You are tasked with implementing a Python class that leverages weak references to manage objects efficiently. Using the provided weak reference functions (`PyWeakref_NewRef`, `PyWeakref_GetObject`, etc.), create a class named `WeakRefManager` that maintains weak references to a set of objects. The class should have the following requirements: Class: `WeakRefManager` 1. **Attributes**: - `refs`: A dictionary that maps object ids to their weak references. 2. **Methods**: - `__init__(self)`: Initializes the class with an empty dictionary for `refs`. - `add_reference(self, obj)`: Takes an object `obj` and adds a weak reference to it in the `refs` dictionary. The key should be the `id` of the object. - `get_reference(self, obj_id)`: Takes an object id `obj_id` and returns the actual object if it is still live. If the object is no longer live, return `None`. - `clear_references(self)`: Clears all weak references stored in the `refs` dictionary. Constraints and Notes: - Use Python’s built-in `weakref` module functions to implement the weak references. - Ensure that the `add_reference` method does not add duplicate references for the same object. - The `get_reference` method should handle the case where the referenced object may have been garbage-collected. - You may assume the objects being referenced are weakly referable. Expected Input and Output Formats: - `add_reference(self, obj)`: - Input: A Python object. - Output: None. - `get_reference(self, obj_id)`: - Input: An integer representing the object id. - Output: The original object if it is still live, else `None`. - `clear_references(self)`: - Input: None - Output: None Example Usage: ```python # Example to illustrate the usage manager = WeakRefManager() obj = SomeClass() manager.add_reference(obj) obj_ref = manager.get_reference(id(obj)) # If `obj` is still live, obj_ref should be `obj` assert obj_ref == obj # Clear all references manager.clear_references() assert manager.get_reference(id(obj)) is None ``` Implement the `WeakRefManager` class.","solution":"import weakref class WeakRefManager: def __init__(self): Initializes the WeakRefManager with an empty dictionary for refs. self.refs = {} def add_reference(self, obj): Takes an object obj and adds a weak reference to it in the refs dictionary. The key is the id of the object. :param obj: The object to add a weak reference for. obj_id = id(obj) if obj_id not in self.refs: self.refs[obj_id] = weakref.ref(obj) def get_reference(self, obj_id): Takes an object id and returns the actual object if it is still live. If the object is no longer live, returns None. :param obj_id: The id of the object to retrieve. :return: The original object if it is still live, else None. if obj_id in self.refs: return self.refs[obj_id]() return None def clear_references(self): Clears all weak references stored in the refs dictionary. self.refs.clear()"},{"question":"Using the concepts of the mapping protocol detailed in the documentation, your task is to implement a custom Python class `CustomMapping` that behaves like a dictionary but internally uses lists to store keys and values. Your implementation should include the following methods: - `__init__(self)`: Initializes the internal storage. - `__getitem__(self, key)`: Retrieves the value associated with the given key. Raise a `KeyError` if the key is not found. - `__setitem__(self, key, value)`: Sets the value for the given key in the mapping. If the key already exists, update its value. - `__delitem__(self, key)`: Deletes the item associated with the given key. Raise a `KeyError` if the key is not found. - `__contains__(self, key)`: Checks if the given key exists in the mapping. - `__len__(self)`: Returns the number of items in the mapping. - `keys(self)`: Returns a list of all keys in the mapping. - `values(self)`: Returns a list of all values in the mapping. - `items(self)`: Returns a list of tuples, each containing a key-value pair from the mapping. # Constraints - Your implementation should maintain an internal list for keys and another for values. - Do not use Python\'s built-in dictionary data type. # Input and Output Formats - The methods should accept keys and values as strings. - Methods should handle edge cases such as non-existent keys appropriately. # Performance Requirements - Aim for efficient implementations of the methods. Your solutions will be tested against scenarios with a large number of items. # Example Usage ```python mapping = CustomMapping() mapping[\'a\'] = \'apple\' mapping[\'b\'] = \'banana\' print(mapping[\'a\']) # Output: \'apple\' print(len(mapping)) # Output: 2 del mapping[\'a\'] print(\'a\' in mapping) # Output: False print(mapping.keys()) # Output: [\'b\'] print(mapping.values()) # Output: [\'banana\'] print(mapping.items()) # Output: [(\'b\', \'banana\')] ``` Implement the `CustomMapping` class.","solution":"class CustomMapping: def __init__(self): self._keys = [] self._values = [] def __getitem__(self, key): if key in self._keys: index = self._keys.index(key) return self._values[index] else: raise KeyError(f\'Key {key} not found.\') def __setitem__(self, key, value): if key in self._keys: index = self._keys.index(key) self._values[index] = value else: self._keys.append(key) self._values.append(value) def __delitem__(self, key): if key in self._keys: index = self._keys.index(key) del self._keys[index] del self._values[index] else: raise KeyError(f\'Key {key} not found.\') def __contains__(self, key): return key in self._keys def __len__(self): return len(self._keys) def keys(self): return self._keys def values(self): return self._values def items(self): return list(zip(self._keys, self._values))"},{"question":"Objective: Demonstrate your comprehension of Python debugging and profiling modules by implementing a function that analyzes the performance and memory usage of a given list of Python functions. Problem Statement: You are provided with a list of functions. Your task is to: 1. Measure and report the execution time for each function using the `timeit` module. 2. Trace memory allocations during the execution of each function using the `tracemalloc` module. 3. Create a summary report that includes: - Function name - Total execution time - Peak memory usage during execution Function Signature: ```python def analyze_functions(functions: list) -> list: pass ``` Input: - `functions`: A list of functions to be analyzed. Each function has no arguments and performs a specific task. Output: - A list of dictionaries where each dictionary contains the following keys: - `\\"function_name\\"`: The name of the function as a string. - `\\"execution_time\\"`: The total execution time in seconds as a float. - `\\"peak_memory_usage\\"`: The peak memory usage in bytes as an integer. Constraints: 1. Each function in the `functions` list is independent and has no arguments. 2. The functions may vary significantly in their execution time and memory usage. Example: ```python def example_func1(): list1 = [i for i in range(100000)] return sum(list1) def example_func2(): total = 1 for i in range(1, 5000): total *= i return total functions = [example_func1, example_func2] result = analyze_functions(functions) # Expected output format (values may vary): # [ # { # \\"function_name\\": \\"example_func1\\", # \\"execution_time\\": 0.012345, # \\"peak_memory_usage\\": 400000 # }, # { # \\"function_name\\": \\"example_func2\\", # \\"execution_time\\": 0.067890, # \\"peak_memory_usage\\": 20000 # } # ] ``` Notes: - To measure the execution time, use the `timeit.timeit()` function. - To trace memory allocations, use the `tracemalloc` module and capture the peak memory usage. - Ensure that your solution is efficient and handles functions with varying performance characteristics.","solution":"import timeit import tracemalloc def analyze_functions(functions): results = [] for func in functions: # Measure execution time start_time = timeit.default_timer() tracemalloc.start() func() # Run the function to analyze # Stop timers and tracemalloc stop_time = timeit.default_timer() current, peak = tracemalloc.get_traced_memory() tracemalloc.stop() # Calculate execution time execution_time = stop_time - start_time # Populate result dictionary results.append({ \\"function_name\\": func.__name__, \\"execution_time\\": execution_time, \\"peak_memory_usage\\": peak }) return results"},{"question":"**Objective: Create a comprehensive seaborn plot using the `seaborn.objects` module that incorporates multiple advanced features to visualize the dataset effectively.** **Problem Statement:** You are provided with the `tips` dataset, which contains information about the tips received in a restaurant. Your task is to create a multi-faceted dot plot that visualizes the relationship between the total bill and day of the week, colored by gender, including jittering and dodging to prevent overplotting. Additionally, include error bars to show the variability in the tips for each day. **Requirements:** - Load the `tips` dataset using seaborn\'s `load_dataset` function. - Create a dot plot displaying `total_bill` on the x-axis and `day` on the y-axis. - Color the dots by the `sex` column. - Add jittering of `0.2` units to reduce overplotting. - Add dodging to separate the dots for different genders. - Add error bars to show the standard error of the mean (`se`) for the `total_bill` for each day. **Constraints:** - Use the `seaborn.objects` module exclusively. - The visualization should be clear and properly labeled. **Expected Output:** - A multi-faceted dot plot with `total_bill` vs. `day`. - Dots colored by gender (`sex`). - Jittered dots to reduce overplotting. - Dots dodged to separate gender. - Error bars reflecting the standard error of the mean for `total_bill` on each day. You may refer to the provided seaborn documentation for guidance on using the relevant functions and methods. **Code Template:** ```python import seaborn.objects as so from seaborn import load_dataset # Load the dataset tips = load_dataset(\\"tips\\") # Create the plot ( so.Plot(tips, x=\\"total_bill\\", y=\\"day\\", color=\\"sex\\") .add(so.Dot(), so.Dodge(), so.Jitter(0.2)) .add(so.Range(), so.Est(errorbar=(\\"se\\", 2))) ).show() ``` Your task is to complete and execute the above code to create the desired plot.","solution":"import seaborn.objects as so from seaborn import load_dataset # Load the dataset tips = load_dataset(\\"tips\\") # Create the plot ( so.Plot(tips, x=\\"total_bill\\", y=\\"day\\", color=\\"sex\\") .add(so.Dot(), so.Dodge(), so.Jitter(0.2)) .add(so.Range(), so.Est(errorbar=(\\"se\\", 2))) ).show()"},{"question":"**Question: Customizing Seaborn Plot Context** You are tasked to create a function `customize_seaborn_context` that takes three arguments: `context`, `font_scale`, and `line_width`. This function should set the seaborn context based on the provided parameters and generate a line plot with predefined data points. Your function should: 1. Use `sns.set_context` to set the plot context. 2. Independently scale the font elements using the `font_scale` parameter. 3. Override the line width to the specified `line_width`. 4. Return the plot object. # Function Signature ```python def customize_seaborn_context(context: str, font_scale: float, line_width: float) -> sns.axisgrid.FacetGrid: pass ``` # Input - `context`: A string representing the context. Possible values are: \\"paper\\", \\"notebook\\", \\"talk\\", \\"poster\\". - `font_scale`: A float value to scale the font elements. - `line_width`: A float value to override the line width. # Output - Returns a seaborn plot (`sns.axisgrid.FacetGrid`) object with the specified customizations. # Example ```python plot = customize_seaborn_context(\\"notebook\\", 1.25, 3) ``` The function should generate a line plot with x-values `[0, 1, 2]` and y-values `[1, 3, 2]`, using the specified context, font scale, and line width. # Constraints - You should ensure that `context` is one of the valid contexts: \\"paper\\", \\"notebook\\", \\"talk\\", \\"poster\\". - The function should handle exceptions gracefully and print an error message if the input values are invalid. **Note:** The seaborn package needs to be imported as `sns` and matplotlib should be used to display the plot.","solution":"import seaborn as sns import matplotlib.pyplot as plt def customize_seaborn_context(context: str, font_scale: float, line_width: float) -> sns.axisgrid.FacetGrid: Customizes seaborn plot context and generates a line plot with predefined data points. Args: context (str): The context of the plot (\\"paper\\", \\"notebook\\", \\"talk\\", \\"poster\\"). font_scale (float): Scaling factor for the font elements. line_width (float): Width of the lines in the plot. Returns: sns.axisgrid.FacetGrid: The seaborn plot object with customizations. valid_contexts = [\\"paper\\", \\"notebook\\", \\"talk\\", \\"poster\\"] if context not in valid_contexts: raise ValueError(f\\"Invalid context: {context}. Valid options are {valid_contexts}.\\") if font_scale <= 0 or line_width <= 0: raise ValueError(\\"Both font_scale and line_width must be greater than 0.\\") sns.set_context(context, font_scale=font_scale, rc={\\"lines.linewidth\\": line_width}) x = [0, 1, 2] y = [1, 3, 2] plot = sns.lineplot(x=x, y=y) plt.show() return plot"},{"question":"**Objective**: Assess students\' understanding of Python site-specific configuration and customization using the `site` module. **Task**: Write a function `setup_custom_site_packages` that takes a list of directories as input, processes them, and configures the system to include these directories in the module search path. Additionally, the function should handle `.pth` files as described in the `site` module documentation. **Function Signature**: ```python def setup_custom_site_packages(directories: List[str]) -> List[str]: pass ``` **Input**: - `directories`: A list of strings representing directory paths to be added to the module search path. **Output**: - A list of strings containing the final `sys.path` after adding the directories and processing any `.pth` files found in those directories. **Requirements**: 1. The function should add each directory in the input list to `sys.path`. 2. For each directory added, the function should check for `.pth` files and process them as follows: - Add any valid paths mentioned in the `.pth` files to `sys.path`. - Execute any lines in the `.pth` file that start with `import`. 3. The function should ensure that no directory or path from `.pth` files is added to `sys.path` more than once. 4. The function should ignore non-existing paths or invalid entries in `.pth` files. **Constraints**: - The function should be efficient with path manipulations and avoid excessive redundant checks. # Example Suppose we have the following directory structure: ``` /usr/local/custom_packages/foo.pth /usr/local/custom_packages/spam.pth /home/user/custom_packages/bar.pth ``` Contents of `foo.pth`: ``` # This is the foo package configuration foo_subdir import os ``` Contents of `spam.pth`: ``` # This is the spam package configuration spam_subdir non_existent_subdir ``` Contents of `bar.pth`: ``` # This is the bar package configuration bar_subdir import sys ``` If the `directories` input to the function is: ```python [\\"/usr/local/custom_packages\\", \\"/home/user/custom_packages\\"] ``` And considering `foo_subdir`, `spam_subdir`, and `bar_subdir` exist in the respective directories while `non_existent_subdir` does not, the output should be: ```python [\\"/usr/local/custom_packages/foo_subdir\\", \\"/usr/local/custom_packages/spam_subdir\\", \\"/home/user/custom_packages/bar_subdir\\"] ``` # Additional Notes - Use the `site.addsitedir` method where appropriate. - Make sure to handle exceptions properly for any path or import errors. **Good luck!**","solution":"import sys import os import site from typing import List def setup_custom_site_packages(directories: List[str]) -> List[str]: for directory in directories: if os.path.isdir(directory): site.addsitedir(directory) # Process .pth files for file in os.listdir(directory): if file.endswith(\'.pth\'): pth_path = os.path.join(directory, file) with open(pth_path, \'r\') as f: for line in f: line = line.strip() if line and not line.startswith(\\"#\\"): # skip comments if line.startswith(\\"import\\"): exec(line) # execute import statements else: full_path = os.path.join(directory, line) if os.path.exists(full_path): if full_path not in sys.path: sys.path.append(full_path) return sys.path"},{"question":"You have been tasked to perform a preliminary analysis of two datasets using scikit-learn\'s `datasets` module. Specifically, you need to load one standard dataset (e.g., Iris dataset) and one real-world dataset (e.g., California Housing dataset). Additionally, you will need to generate a synthetic dataset using scikit-learn\'s dataset generation functions. Write a Python function `analyze_datasets()` that does the following: 1. Loads the **Iris** dataset using the appropriate dataset loader function. 2. Loads the **California Housing** dataset using the appropriate dataset fetcher function. 3. Generates a synthetic dataset with 100 samples and 5 features using the `make_classification` function. 4. Returns a dictionary with the following structure: ```python { \'iris\': { \'data_shape\': tuple, # shape of the data array of the Iris dataset \'target_shape\': tuple, # shape of the target array of the Iris dataset }, \'california_housing\': { \'data_shape\': tuple, # shape of the data array of the California Housing dataset \'target_shape\': tuple, # shape of the target array of the California Housing dataset }, \'synthetic\': { \'data_shape\': tuple, # shape of the data array of the synthetic dataset \'target_shape\': tuple, # shape of the target array of the synthetic dataset } } ``` # Constraints - For the **Iris** dataset, ensure the returned data and target shapes are tuples of the form `(n_samples, n_features)` and `(n_samples,)` respectively. - For the **California Housing** dataset, ensure the returned data and target shapes are tuples of the form `(n_samples, n_features)` and `(n_samples,)` respectively. - For the synthetic dataset, you must use `make_classification(n_samples=100, n_features=5)` to generate the data and target arrays. # Example Usage ```python result = analyze_datasets() print(result) # Expected output format (exact shapes will depend on datasets): # { # \'iris\': { # \'data_shape\': (150, 4), # \'target_shape\': (150,), # }, # \'california_housing\': { # \'data_shape\': (20640, 8), # \'target_shape\': (20640,), # }, # \'synthetic\': { # \'data_shape\': (100, 5), # \'target_shape\': (100,), # } # } ``` # Important Notes - Use `sklearn.datasets.load_iris` to load the Iris dataset. - Use `sklearn.datasets.fetch_california_housing` to fetch the California Housing dataset. - Use `sklearn.datasets.make_classification` to generate the synthetic dataset.","solution":"from sklearn.datasets import load_iris, fetch_california_housing, make_classification def analyze_datasets(): # Load Iris dataset iris = load_iris() iris_data_shape = iris.data.shape iris_target_shape = iris.target.shape # Load California Housing dataset california_housing = fetch_california_housing() california_housing_data_shape = california_housing.data.shape california_housing_target_shape = california_housing.target.shape # Generate synthetic dataset synthetic_data, synthetic_target = make_classification(n_samples=100, n_features=5) synthetic_data_shape = synthetic_data.shape synthetic_target_shape = synthetic_target.shape # Compile dictionary with dataset shapes result = { \'iris\': { \'data_shape\': iris_data_shape, \'target_shape\': iris_target_shape, }, \'california_housing\': { \'data_shape\': california_housing_data_shape, \'target_shape\': california_housing_target_shape, }, \'synthetic\': { \'data_shape\': synthetic_data_shape, \'target_shape\': synthetic_target_shape, } } return result"},{"question":"<|Analysis Begin|> The provided documentation is for the `xml.sax.handler` module in Python, which defines several base classes and interfaces for SAX handlers in XML parsing. These handlers are crucial for handling different types of events during the parsing of an XML document. The module includes classes for handling document content, DTD events, entity resolution, errors, and lexical events. Key points to consider for crafting an assessment question: - Understanding of SAX parsing concepts. - Implementation of custom subclasses for event handling. - Methods like `startDocument`, `endDocument`, `startElement`, `characters`, etc. - Handling of namespaces, entity resolution, and error reporting. - Use of properties and features for customizing the parser behavior. Given the complexity and depth of the `xml.sax.handler` module, a challenging and comprehensive question can be designed around implementing these handlers and using them to parse an XML document with specific requirements. <|Analysis End|> <|Question Begin|> # XML SAX Parsing with Custom Handlers **Objective:** Implement custom SAX handlers for XML parsing to process an XML document and extract specific information. **Problem Statement:** You are given an XML document containing information about a collection of books. Each book element has attributes like `title`, `author`, `genre`, and `price`. You need to parse this XML document using the SAX (Simple API for XML) approach and extract the following information: 1. Titles of all books whose price is greater than a specified value. 2. Count of books for each genre. **Input:** - An XML string. - A float value representing the price threshold. **Output:** - A list of book titles with prices greater than the given value. - A dictionary with genres as keys and their corresponding book counts as values. **Constraints:** - The XML string is well-formed. - The price attribute is always a valid float value. **Example:** ```xml <catalog> <book title=\\"A Tale of Two Cities\\" author=\\"Charles Dickens\\" genre=\\"Historical\\" price=\\"12.99\\"/> <book title=\\"The Catcher in the Rye\\" author=\\"J.D. Salinger\\" genre=\\"Fiction\\" price=\\"8.99\\"/> <book title=\\"To Kill a Mockingbird\\" author=\\"Harper Lee\\" genre=\\"Fiction\\" price=\\"15.99\\"/> <book title=\\"The Great Gatsby\\" author=\\"F. Scott Fitzgerald\\" genre=\\"Fiction\\" price=\\"10.99\\"/> <book title=\\"1984\\" author=\\"George Orwell\\" genre=\\"Dystopian\\" price=\\"7.99\\"/> </catalog> ``` Given a price threshold of 10.00, the output should be: ```python ([\'A Tale of Two Cities\', \'To Kill a Mockingbird\'], {\'Historical\': 1, \'Fiction\': 3, \'Dystopian\': 1}) ``` **Task:** 1. Implement custom content handler class inheriting from `xml.sax.handler.ContentHandler`. 2. Implement the necessary methods to process the XML and extract the required information. 3. Write a function `parse_books(xml_string: str, price_threshold: float) -> (List[str], Dict[str, int])` that sets up the SAX parser, uses your custom handler, and returns the required information. **Code Template:** ```python import xml.sax from xml.sax.handler import ContentHandler from typing import List, Dict class BookHandler(ContentHandler): def __init__(self, price_threshold): super().__init__() self.price_threshold = price_threshold self.current_element = \'\' self.high_price_titles = [] self.genre_count = {} def startElement(self, name, attrs): self.current_element = name if name == \'book\': title = attrs.get(\'title\', \'\') genre = attrs.get(\'genre\', \'\') price = float(attrs.get(\'price\', 0)) if price > self.price_threshold: self.high_price_titles.append(title) if genre in self.genre_count: self.genre_count[genre] += 1 else: self.genre_count[genre] = 1 def endElement(self, name): self.current_element = \'\' def characters(self, content): pass def parse_books(xml_string: str, price_threshold: float) -> (List[str], Dict[str, int]): handler = BookHandler(price_threshold) parser = xml.sax.make_parser() parser.setContentHandler(handler) xml.sax.parseString(xml_string, handler) return handler.high_price_titles, handler.genre_count # Example usage: xml_data = \'\'\'<catalog> <book title=\\"A Tale of Two Cities\\" author=\\"Charles Dickens\\" genre=\\"Historical\\" price=\\"12.99\\"/> <book title=\\"The Catcher in the Rye\\" author=\\"J.D. Salinger\\" genre=\\"Fiction\\" price=\\"8.99\\"/> <book title=\\"To Kill a Mockingbird\\" author=\\"Harper Lee\\" genre=\\"Fiction\\" price=\\"15.99\\"/> <book title=\\"The Great Gatsby\\" author=\\"F. Scott Fitzgerald\\" genre=\\"Fiction\\" price=\\"10.99\\"/> <book title=\\"1984\\" author=\\"George Orwell\\" genre=\\"Dystopian\\" price=\\"7.99\\"/> </catalog>\'\'\' price_threshold = 10.00 print(parse_books(xml_data, price_threshold)) ``` **Explanation:** - `BookHandler` is a custom content handler class that processes `startElement`, `endElement`, and `characters` events. - `parse_books` function sets up the SAX parser with the custom handler, parses the provided XML string, and returns the extracted information. **Note:** Ensure your solution handles various edge cases, such as missing attributes in the XML elements.","solution":"import xml.sax from xml.sax.handler import ContentHandler from typing import List, Dict class BookHandler(ContentHandler): def __init__(self, price_threshold): super().__init__() self.price_threshold = price_threshold self.current_element = \'\' self.high_price_titles = [] self.genre_count = {} def startElement(self, name, attrs): if name == \'book\': title = attrs.get(\'title\', \'\') genre = attrs.get(\'genre\', \'\') price = float(attrs.get(\'price\', 0)) if price > self.price_threshold: self.high_price_titles.append(title) if genre in self.genre_count: self.genre_count[genre] += 1 else: self.genre_count[genre] = 1 def endElement(self, name): self.current_element = \'\' def characters(self, content): pass def parse_books(xml_string: str, price_threshold: float) -> (List[str], Dict[str, int]): handler = BookHandler(price_threshold) parser = xml.sax.make_parser() parser.setContentHandler(handler) xml.sax.parseString(xml_string, handler) return handler.high_price_titles, handler.genre_count # Example usage: xml_data = \'\'\'<catalog> <book title=\\"A Tale of Two Cities\\" author=\\"Charles Dickens\\" genre=\\"Historical\\" price=\\"12.99\\"/> <book title=\\"The Catcher in the Rye\\" author=\\"J.D. Salinger\\" genre=\\"Fiction\\" price=\\"8.99\\"/> <book title=\\"To Kill a Mockingbird\\" author=\\"Harper Lee\\" genre=\\"Fiction\\" price=\\"15.99\\"/> <book title=\\"The Great Gatsby\\" author=\\"F. Scott Fitzgerald\\" genre=\\"Fiction\\" price=\\"10.99\\"/> <book title=\\"1984\\" author=\\"George Orwell\\" genre=\\"Dystopian\\" price=\\"7.99\\"/> </catalog>\'\'\' price_threshold = 10.00 print(parse_books(xml_data, price_threshold))"},{"question":"# Question: Design Your Own Unix-Shell-Like Script Processor Context: You are tasked with creating a command-line utility in Python that functions similarly to a Unix shell. This utility should be able to parse and process a series of commands from a given script or a command line. It should handle shell-like syntax, including comments, quotes, and escape characters accurately. Requirements: 1. **Function Implementation: `process_script(script: str) -> List[str]`** - **Input**: A single string containing multiple commands, possibly spanning multiple lines. This may include comments (starting with `#`), quoted strings, and escaped characters. - **Output**: A list of individual commands parsed from the input string. 2. **Function Implementation: `execute_commands(commands: List[str]) -> List[str]`** - **Input**: A list of parsed command strings, where each string represents a single command. - **Output**: A list containing the result of each command\'s execution as a string. For the sake of this implementation, assume that the command just returns the string \\"Executed: {command}\\". Constraints: - Ensure the parsing respects quoted strings and does not split them incorrectly. - The parsing should be POSIX-compliant. - Comments (starting with `#`) should be ignored. - Each command should be separated by a newline in the output list. Example: ```python script = echo \\"Hello, World!\\" # This is a comment ls -l folder_name mkdir \\"New Folder\\" && echo \'Folder Created\' parsed_commands = process_script(script) # parsed_commands should be # [\'echo \\"Hello, World!\\"\', \'ls -l folder_name\', \'mkdir \\"New Folder\\"\', \'echo \\"Folder Created\\"\'] execution_results = execute_commands(parsed_commands) # execution_results should be # [\'Executed: echo \\"Hello, World!\\"\', \'Executed: ls -l folder_name\', \'Executed: mkdir \\"New Folder\\"\', \'Executed: echo \\"Folder Created\\"\'] ``` Hints: - Utilize the `shlex` module to properly handle the parsing of the script into individual commands. - Consider using `shlex.split` for splitting the input script and `shlex.quote` to handle any security concerns related to shell command injection. Implement the two functions to complete this assessment.","solution":"import shlex def process_script(script: str): Parses a script string containing multiple commands and returns a list of individual commands. Args: - script (str): A string containing multiple commands, potentially with comments, quotes, and escapes. Returns: - List[str]: A list of parsed individual commands. lines = script.splitlines() cleaned_lines = [] continued_line = \'\' for line in lines: line = line.strip() if \'#\' in line: line = line.split(\'#\')[0].strip() if not line: continue if line.endswith(\'\'): continued_line += line[:-1] continue continued_line += line cleaned_lines.append(continued_line.strip()) continued_line = \'\' return cleaned_lines def execute_commands(commands: list): Executes a list of shell commands, returning a list of their results as strings. Args: - commands (List[str]): A list of individual command strings. Returns: - List[str]: A list of results of command execution in the format \'Executed: {command}\' results = [] for command in commands: results.append(f\\"Executed: {command}\\") return results # Example usage: # script = # echo \\"Hello, World!\\" # This is a comment # ls -l # folder_name # mkdir \\"New Folder\\" && echo \'Folder Created\' # # parsed_commands = process_script(script) # print(parsed_commands) # # parsed_commands should be # # [\'echo \\"Hello, World!\\"\', \'ls -l folder_name\', \'mkdir \\"New Folder\\"\', \'echo \\"Folder Created\\"\'] # execution_results = execute_commands(parsed_commands) # print(execution_results) # # execution_results should be # # [\'Executed: echo \\"Hello, World!\\"\', \'Executed: ls -l folder_name\', \'Executed: mkdir \\"New Folder\\"\', \'Executed: echo \\"Folder Created\\"\']"},{"question":"**Question: Implement a File Synchronizer with Logging and Command Line Arguments** # Objective Write a Python script that synchronizes the contents of a source directory with a destination directory. The script should take command line arguments for source and destination directories, and it should log the synchronization process. # Requirements 1. **Command Line Arguments**: - The script should accept two mandatory arguments: the path to the source directory and the path to the destination directory. - Example: `python file_sync.py /path/to/source /path/to/destination` 2. **Synchronization**: - The script should copy all files from the source directory to the destination directory. - If a file in the source directory already exists in the destination directory and has the same content, it should not be copied. - If the file content differs or the file does not exist in the destination directory, it should be copied. 3. **Logging**: - The script should log the following events: - Start of synchronization process. - Each file copied with its path. - End of synchronization process. - Any errors encountered during the process. 4. **Output Format**: - Log messages should be printed to the console and written to a log file named `sync.log`. # Constraints - The script should handle large directories with potentially thousands of files. - Ensure proper error handling for cases such as missing directories, read/write permissions, etc. # Example Usage ```sh python file_sync.py /source_directory /destination_directory ``` # Expected Output The log file should have entries similar to: ``` INFO: Starting synchronization... INFO: Copied /source_directory/file1.txt to /destination_directory/file1.txt ERROR: Failed to copy /source_directory/file2.txt: [Errno 13] Permission denied: \'/source_directory/file2.txt\' INFO: Synchronization completed. ``` # Implementation Notes - Use the `argparse` module to handle command-line arguments. - Use the `os` and `shutil` modules to handle file operations. - Use the `logging` module to log events. - Ensure to write clean and modular code with appropriate error handling.","solution":"import os import shutil import logging import argparse from filecmp import cmp def setup_logging(): logging.basicConfig(level=logging.INFO, format=\'%(asctime)s - %(levelname)s - %(message)s\', handlers=[logging.FileHandler(\\"sync.log\\"), logging.StreamHandler()]) def synchronize_directories(source, destination): if not os.path.exists(source): logging.error(\\"Source directory does not exist.\\") return if not os.path.exists(destination): logging.error(\\"Destination directory does not exist.\\") os.makedirs(destination) logging.info(\\"Destination directory created: %s\\" % destination) for dirpath, _, filenames in os.walk(source): for filename in filenames: src_file = os.path.join(dirpath, filename) dest_file = os.path.join(destination, os.path.relpath(src_file, source)) if not os.path.exists(os.path.dirname(dest_file)): os.makedirs(os.path.dirname(dest_file)) if os.path.exists(dest_file) and cmp(src_file, dest_file, shallow=False): logging.info(\\"File already exists and is identical: %s\\" % dest_file) continue try: shutil.copy2(src_file, dest_file) logging.info(\\"Copied %s to %s\\" % (src_file, dest_file)) except Exception as e: logging.error(\\"Failed to copy %s: %s\\" % (src_file, e)) def main(): parser = argparse.ArgumentParser(description=\\"Synchronize the contents of a source directory with a destination directory.\\") parser.add_argument(\'source\', type=str, help=\\"Path to the source directory.\\") parser.add_argument(\'destination\', type=str, help=\\"Path to the destination directory.\\") args = parser.parse_args() setup_logging() logging.info(\\"Starting synchronization...\\") synchronize_directories(args.source, args.destination) logging.info(\\"Synchronization completed.\\") if __name__ == \\"__main__\\": main()"},{"question":"# Enum Exercise The `enum` module in Python provides classes to create enumerated constants. This exercise will test your understanding of `enum` and require you to demonstrate your skills in defining and working with enum classes in Python. Task 1. Define an `IntEnum` named `Weekday` to represent the days of the week: - The days should be represented as follows: Monday(1), Tuesday(2), Wednesday(3), Thursday(4), Friday(5), Saturday(6), and Sunday(7). - Ensure that no two days have the same value. 2. Define a `Flag` named `UserAccess` to represent different user access permissions: - READ(1) - WRITE(2) - EXECUTE(4) - Ensure that combinations of flags can use bitwise operators to combine and check permissions. 3. Write a function named `has_access` that checks if a given access level includes certain permissions. The function should have the following signature: ```python def has_access(access_level: UserAccess, permission: UserAccess) -> bool: # your code here ``` - `access_level`: a `UserAccess` flag representing the current access permissions. - `permission`: a `UserAccess` flag representing the permission to check for. - The function should return `True` if `access_level` includes `permission`, and `False` otherwise. Input and Output - You don\'t need to handle any input or output inside your functions. Just ensure the functions are defined correctly. Constraints - Ensure that the enums are defined using the syntax given in the task. - The function `has_access` should leverage the capabilities of the `Flag` subclass. # Example Usage ```python from enum import IntEnum, Flag, auto class Weekday(IntEnum): MONDAY = 1 TUESDAY = 2 WEDNESDAY = 3 THURSDAY = 4 FRIDAY = 5 SATURDAY = 6 SUNDAY = 7 class UserAccess(Flag): READ = 1 WRITE = 2 EXECUTE = 4 def has_access(access_level: UserAccess, permission: UserAccess) -> bool: return permission in access_level # Example function usage: access_level = UserAccess.READ | UserAccess.WRITE print(has_access(access_level, UserAccess.READ)) # Output: True print(has_access(access_level, UserAccess.EXECUTE)) # Output: False ``` Performance Requirements - The function should work efficiently for the enum values provided. - Consider scenarios where multiple checks are done using bitwise operations. Good luck and happy coding!","solution":"from enum import IntEnum, Flag, auto class Weekday(IntEnum): MONDAY = 1 TUESDAY = 2 WEDNESDAY = 3 THURSDAY = 4 FRIDAY = 5 SATURDAY = 6 SUNDAY = 7 class UserAccess(Flag): READ = 1 WRITE = 2 EXECUTE = 4 def has_access(access_level: UserAccess, permission: UserAccess) -> bool: return (access_level & permission) == permission"},{"question":"# Custom PyTorch Autograd Function Implementation **Objective**: Implement a custom PyTorch `Function` that performs a specific operation, ensures that gradients are calculated correctly, and validates the function with `torch.autograd.gradcheck`. # Problem Statement You are required to implement a custom autograd function for the cosine similarity operation. The cosine similarity between two vectors `x` and `y` is defined as: [ text{Cosine Similarity}(x, y) = frac{x cdot y}{|x| |y|} ] where ( x cdot y ) is the dot product of `x` and `y`, and ( |x| ) and ( |y| ) are the Euclidean norms of `x` and `y`, respectively. # Requirements 1. **Function Implementation**: - Implement a custom `Function` subclass named `CosineSimilarityFunction`. - Define the `forward` and `backward` methods. Optionally, use `setup_context` to store context if needed. 2. **Forward Pass**: - The forward pass should compute the cosine similarity of two input tensors, `x` and `y`. 3. **Backward Pass**: - The backward pass should compute the gradients with respect to the input tensors `x` and `y`. 4. **Gradient Check**: - Validate your custom function using `torch.autograd.gradcheck` to ensure the gradients are correctly computed. # Input and Output - **Input**: Two tensors `x` and `y` of the same shape, each with `requires_grad=True`. - **Output**: A single scalar tensor representing the cosine similarity between `x` and `y`. # Constraints - The tensors `x` and `y` should not contain any zero vectors since the cosine similarity is undefined for zero vectors. # Performance Requirements - Ensure that your implementation is efficient and does not perform unnecessary computations. - Do not modify input tensors in-place in the backward pass. # Example Implementation You must provide the implementation for the custom function along with proper usage and gradient checking. ```python import torch from torch.autograd import Function, gradcheck class CosineSimilarityFunction(Function): @staticmethod def forward(ctx, x, y): # Compute the cosine similarity dot_product = torch.sum(x * y) norm_x = torch.norm(x) norm_y = torch.norm(y) cos_sim = dot_product / (norm_x * norm_y) # Save tensors for the backward pass ctx.save_for_backward(x, y, dot_product, norm_x, norm_y) return cos_sim @staticmethod def backward(ctx, grad_output): x, y, dot_product, norm_x, norm_y = ctx.saved_tensors # Compute gradients of the input tensors grad_x = (grad_output / (norm_x * norm_y)) * (y - x * (dot_product / norm_x**2)) grad_y = (grad_output / (norm_x * norm_y)) * (x - y * (dot_product / norm_y**2)) return grad_x, grad_y # Alias for easier usage cosine_similarity = CosineSimilarityFunction.apply # Example usage and gradient checking x = torch.randn(10, dtype=torch.double, requires_grad=True) y = torch.randn(10, dtype=torch.double, requires_grad=True) # Validate gradients with gradcheck input = (x, y) test = gradcheck(cosine_similarity, input, eps=1e-6, atol=1e-4) print(\\"Gradient check passed:\\", test) ``` **Note**: The provided example code is a template. You need to fill in the `forward` and `backward` methods with the correct implementations specific to the cosine similarity operation. # Submission Guidelines - Submit the complete code implementation of the custom autograd function. - Ensure to include test cases that demonstrate the usage and gradient checking of your function. - Code should be well-documented and follow best practices for readability and efficiency.","solution":"import torch from torch.autograd import Function, gradcheck class CosineSimilarityFunction(Function): @staticmethod def forward(ctx, x, y): # Compute the cosine similarity dot_product = torch.sum(x * y) norm_x = torch.norm(x) norm_y = torch.norm(y) cos_sim = dot_product / (norm_x * norm_y) # Save tensors for the backward pass ctx.save_for_backward(x, y, dot_product, norm_x, norm_y) return cos_sim @staticmethod def backward(ctx, grad_output): x, y, dot_product, norm_x, norm_y = ctx.saved_tensors # Compute gradients of the input tensors scale = grad_output / (norm_x * norm_y) grad_x = scale * (y - x * dot_product / norm_x ** 2) grad_y = scale * (x - y * dot_product / norm_y ** 2) return grad_x, grad_y # Alias for easier usage cosine_similarity = CosineSimilarityFunction.apply"},{"question":"# Coding Assessment: Custom Performance Timer In this assessment, you are required to implement and use a custom performance timer uilizing the `timeit` module to measure the execution time of different implementations of a common task. This will assess your understanding of the `timeit` module and your ability to analyze performance differences. # Task 1. **Implement Three Functions**: Implement three different functions that each take a list of integers and return a new list containing the squares of all the integers. Use different approaches for each function: - **Function 1 (Using a `for` loop)**: `def square_using_for_loop(lst: List[int]) -> List[int]` - **Function 2 (Using List Comprehension)**: `def square_using_list_comprehension(lst: List[int]) -> List[int]` - **Function 3 (Using the `map` function)**: `def square_using_map(lst: List[int]) -> List[int]` 2. **Custom Timer Implementation**: Write a function `def custom_timer(test_cases: List[List[int]], number: int, repeat: int) -> dict` which uses the `timeit` module to measure the execution time of the three functions for a given number of times and repetitions. The function should take: - `test_cases`: A list of lists where each inner list is a test case for your functions. - `number`: The number of times each test case should be executed. - `repeat`: The number of repetitions for timing. The function should return a dictionary with keys as the function names and values as lists of timing results for each test case. 3. **Performance Comparison**: Use your custom performance timer to compare the three functions on the following test cases: `[[1,2,3,4,5], [10, 20, 30, 40, 50], [100, 200, 300, 400, 500]]`. Print out the timing results and determine which approach is the most efficient based on the minimum timing result. # Constraints - Functions should handle lists with up to 10,000 integers. - Ensure proper handling of edge cases such as empty lists. # Input/Output Formats - **Input**: A list of lists of integers, the number of times to execute each test case, and the number of repetitions for timing. - **Output**: A dictionary with function names as keys and timing results as values. # Example ```python def square_using_for_loop(lst): return [x**2 for x in lst] def square_using_list_comprehension(lst): return [x**2 for x in lst] def square_using_map(lst): return list(map(lambda x: x**2, lst)) def custom_timer(test_cases, number, repeat): import timeit results = {\\"square_using_for_loop\\": [], \\"square_using_list_comprehension\\": [], \\"square_using_map\\": []} for func_name in results.keys(): for test_case in test_cases: timing = timeit.repeat(lambda: globals()[func_name](test_case), number=number, repeat=repeat) results[func_name].append(min(timing)) return results test_cases = [[1,2,3,4,5], [10, 20, 30, 40, 50], [100, 200, 300, 400, 500]] number = 1000 repeat = 5 results = custom_timer(test_cases, number, repeat) print(results) ``` # Expected Outputs Results should be printed in the format of: ```python { \'square_using_for_loop\': [min_time_for_first_test_case, min_time_for_second_test_case, min_time_for_third_test_case], \'square_using_list_comprehension\': [min_time_for_first_test_case, min_time_for_second_test_case, min_time_for_third_test_case], \'square_using_map\': [min_time_for_first_test_case, min_time_for_second_test_case, min_time_for_third_test_case] } ``` Analyze the results to determine the most efficient approach for squaring a list of integers. Discuss any observed differences in performance and potential reasons for these differences.","solution":"from typing import List, Dict import timeit def square_using_for_loop(lst: List[int]) -> List[int]: result = [] for x in lst: result.append(x**2) return result def square_using_list_comprehension(lst: List[int]) -> List[int]: return [x**2 for x in lst] def square_using_map(lst: List[int]) -> List[int]: return list(map(lambda x: x**2, lst)) def custom_timer(test_cases: List[List[int]], number: int, repeat: int) -> Dict[str, List[float]]: functions = [square_using_for_loop, square_using_list_comprehension, square_using_map] results = {func.__name__: [] for func in functions} for func in functions: for test_case in test_cases: timing = timeit.repeat(lambda: func(test_case), number=number, repeat=repeat) results[func.__name__].append(min(timing)) return results # Example usage scenario test_cases = [[1,2,3,4,5], [10, 20, 30, 40, 50], [100, 200, 300, 400, 500]] number = 1000 repeat = 5 results = custom_timer(test_cases, number, repeat) print(results)"},{"question":"Coding Assessment Question # Objective: Demonstrate your understanding of the seaborn library, specifically the `husl_palette` function, and its integration into a seaborn plot to visualize a dataset. # Problem Statement: You are given a dataset consisting of the heights (in cm) and weights (in kg) of individuals and their respective gender. You need to perform the following tasks: 1. **Load and preprocess the data**: Create a DataFrame from the given height, weight, and gender lists. 2. **Create a palette**: Generate a custom HUSL palette with the following specifications: - Number of colors: 3 - Lightness: 0.5 - Saturation: 0.8 3. **Visualize the data**: Create a scatter plot using seaborn\'s `scatterplot` function. Use the HUSL palette created in step 2 to differentiate the points based on gender. Add appropriate labels for the x-axis, y-axis, and include a legend. # Input: - Lists representing the height, weight, and gender of individuals: ```python heights = [160, 170, 180, 190, 200, 165, 175, 185, 195, 210] weights = [60, 65, 70, 75, 80, 55, 68, 72, 77, 85] genders = [\'Male\', \'Female\', \'Male\', \'Female\', \'Male\', \'Female\', \'Female\', \'Male\', \'Male\', \'Female\'] ``` # Output: - A scatter plot with the following characteristics: - X-axis: Heights - Y-axis: Weights - Color differentiation by gender using the custom HUSL palette # Constraints: - Ensure all data manipulation and visualization is done using pandas and seaborn. - The plot must be clear and well-labeled. # Notes: - Use the `sns.husl_palette()` function to create the palette. - Use Seaborn\'s `scatterplot` for visualization. # Example Solution Outline: ```python import pandas as pd import seaborn as sns import matplotlib.pyplot as plt # Step 1: Load and preprocess the data heights = [160, 170, 180, 190, 200, 165, 175, 185, 195, 210] weights = [60, 65, 70, 75, 80, 55, 68, 72, 77, 85] genders = [\'Male\', \'Female\', \'Male\', \'Female\', \'Male\', \'Female\', \'Female\', \'Male\', \'Male\', \'Female\'] data = pd.DataFrame({ \'Height\': heights, \'Weight\': weights, \'Gender\': genders }) # Step 2: Create a custom HUSL palette palette = sns.husl_palette(3, l=0.5, s=0.8) # Step 3: Create the scatter plot plt.figure(figsize=(10, 6)) sns.scatterplot(data=data, x=\'Height\', y=\'Weight\', hue=\'Gender\', palette=palette) plt.xlabel(\'Height (cm)\') plt.ylabel(\'Weight (kg)\') plt.title(\'Height vs Weight by Gender\') plt.legend(title=\'Gender\') plt.show() ``` Recreate the steps above inside a function and ensure your plot meets the specified requirements.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def generate_height_weight_scatter_plot(heights, weights, genders): Generates a scatter plot for heights and weights colored by gender using a custom HUSL palette. Parameters: heights (list): List of heights in cm. weights (list): List of weights in kg. genders (list): List of genders corresponding to each height and weight. Returns: None: Displays a scatter plot. # Step 1: Load and preprocess the data data = pd.DataFrame({ \'Height\': heights, \'Weight\': weights, \'Gender\': genders }) # Step 2: Create a custom HUSL palette palette = sns.husl_palette(3, l=0.5, s=0.8) # Step 3: Create the scatter plot plt.figure(figsize=(10, 6)) sns.scatterplot(data=data, x=\'Height\', y=\'Weight\', hue=\'Gender\', palette=palette) plt.xlabel(\'Height (cm)\') plt.ylabel(\'Weight (kg)\') plt.title(\'Height vs Weight by Gender\') plt.legend(title=\'Gender\') plt.show()"},{"question":"You are tasked with creating a SAX-based XML parser that processes a large XML document in chunks, using the `xml.sax.xmlreader` module. The XML document contains information about books in a library, and your parser needs to extract and print specific details about each book. Input The XML document will have the following structure: ```xml <library> <book> <title>Book 1</title> <author>Author 1</author> <year>2001</year> </book> <book> <title>Book 2</title> <author>Author 2</author> <year>2002</year> </book> ... </library> ``` The input source could be a file path or a file object containing the XML data. Requirements 1. Implement a SAX `ContentHandler` subclass that processes the `<book>` elements and their children. 2. Create a class `LibraryParser` that uses `xml.sax.xmlreader.IncrementalParser` to parse the XML document in chunks. 3. Implement a function `parse_library` which takes an input source (file path or file object) and uses the `LibraryParser` to output the title, author, and year of each book in the following format: ``` Title: Book 1 Author: Author 1 Year: 2001 Title: Book 2 Author: Author 2 Year: 2002 ... ``` Constraints - You should not assume that the entire document will be loaded into memory at once; it should be processed in chunks. - Make sure to handle any potential parsing exceptions gracefully and print meaningful error messages. - The implementation should be efficient and handle large files properly. Example ```python class BookContentHandler(xml.sax.ContentHandler): def __init__(self): # Initialize to collect book details pass def startElement(self, name, attrs): # Handle start of an element pass def characters(self, content): # Handle character data within elements pass def endElement(self, name): # Handle end of an element pass class LibraryParser(xml.sax.xmlreader.IncrementalParser): def __init__(self, content_handler): # Initialize the parser with content handler pass def feed(self, data): # Feed data into the parser pass def close(self): # Close the parser and finalize parsing pass def parse_library(input_source): # Create a content handler and parser # Read the input source in chunks and feed to the parser pass # Usage input_source = \\"path_to_your_large_xml_file.xml\\" parse_library(input_source) ``` Complete the implementation of the `BookContentHandler`, `LibraryParser`, and `parse_library` function based on the requirements.","solution":"import xml.sax class BookContentHandler(xml.sax.ContentHandler): def __init__(self): self.current_element = \\"\\" self.current_book = {} self.books = [] def startElement(self, name, attrs): self.current_element = name if name == \'book\': self.current_book = {} def characters(self, content): if self.current_element: if self.current_element in (\'title\', \'author\', \'year\'): if self.current_element not in self.current_book: self.current_book[self.current_element] = content else: self.current_book[self.current_element] += content def endElement(self, name): if name == \'book\': self.books.append(self.current_book) self.current_element = \\"\\" class LibraryParser(xml.sax.xmlreader.IncrementalParser): def __init__(self, content_handler): super().__init__() self.content_handler = content_handler self.parser = xml.sax.make_parser() self.parser.setContentHandler(content_handler) def feed(self, data): self.parser.feed(data) def close(self): self.parser.close() def parse_library(input_source): with open(input_source, \'r\') as file: handler = BookContentHandler() parser = LibraryParser(handler) chunk_size = 1024 while True: chunk = file.read(chunk_size) if not chunk: break parser.feed(chunk) parser.close() for book in handler.books: print(f\\"Title: {book[\'title\']}\\") print(f\\"Author: {book[\'author\']}\\") print(f\\"Year: {book[\'year\']}\\") print()"},{"question":"Objective You are tasked with writing a function that performs various operations on arrays created using the `array` module in Python. This will test your understanding of array manipulation, conversion, and utilization of the built-in methods of the `array` module. Problem Write a function `process_array(typecode, initial_values, operations)` that takes the following parameters: 1. `typecode` (str): A single character string that represents the type of items the array will hold (e.g., `\'i\'` for integers). 2. `initial_values` (list): A list of values to initialize the array. 3. `operations` (list of tuples): A list of operations to perform on the array. Each operation is represented by a tuple where the first element is the operation name (str) and the subsequent elements are the arguments for that operation. Supported operations are: - `\'append\'`: Add an element to the end of the array. - `\'extend\'`: Extend the array by appending elements from an iterable. - `\'insert\'`: Insert an element at a specified position. - `\'remove\'`: Remove the first occurrence of an element. - `\'reverse\'`: Reverse the order of elements in the array. - `\'tobytes\'`: Convert the array to a bytes object and return it (no additional arguments). - `\'tolist\'`: Convert the array to a list and return it (no additional arguments). Your function should initialize an array using the provided `typecode` and `initial_values`, then perform the specified operations in order. The result of the last operation, if it is either `\'tobytes\'` or `\'tolist\'`, should be returned. Otherwise, return the array object itself. Constraints - Only the operations listed above will be provided in the `operations` list. - All elements in `initial_values` and any values used in the operations will be of the correct type for the given `typecode`. Example ```python from array import array def process_array(typecode, initial_values, operations): arr = array(typecode, initial_values) for op in operations: if op[0] == \'append\': arr.append(op[1]) elif op[0] == \'extend\': arr.extend(op[1]) elif op[0] == \'insert\': arr.insert(op[1], op[2]) elif op[0] == \'remove\': arr.remove(op[1]) elif op[0] == \'reverse\': arr.reverse() elif op[0] == \'tobytes\': return arr.tobytes() elif op[0] == \'tolist\': return arr.tolist() return arr # Example usage: initial_typecode = \'i\' initial_values = [1, 2, 3] operations = [(\'append\', 4), (\'reverse\',), (\'tolist\',)] result = process_array(initial_typecode, initial_values, operations) print(result) # Output: [4, 3, 2, 1] ``` **Note:** Ensure to handle each operation correctly as specified and test your function against various test cases for robustness.","solution":"from array import array def process_array(typecode, initial_values, operations): arr = array(typecode, initial_values) for op in operations: if op[0] == \'append\': arr.append(op[1]) elif op[0] == \'extend\': arr.extend(op[1]) elif op[0] == \'insert\': arr.insert(op[1], op[2]) elif op[0] == \'remove\': arr.remove(op[1]) elif op[0] == \'reverse\': arr.reverse() elif op[0] == \'tobytes\': return arr.tobytes() elif op[0] == \'tolist\': return arr.tolist() return arr"},{"question":"**Create a Custom Python Extension Type** **Objective:** Implement a custom Python type named `CustomCollection` using the `PyTypeObject` in a C extension. This type should behave like a collection of integers with the following specifications: 1. **Initialization**: It should allow initialization with a list of integers. 2. **Addition**: Provide a method to add a new integer to the collection. 3. **String Representation**: Implement `tp_str` to return a human-readable string representing the collection. 4. **Attribute Management**: Allow accessing and modifying the collection size as an attribute. 5. **Comparison**: Implement rich comparisons to compare two instances of `CustomCollection` based on their sizes. 6. **Iteration**: Support iteration over the collection. **Instructions:** 1. **Initialization**: The type should have an `__init__` method that takes a list of integers. 2. **Addition Method**: Implement an `add` method to add a new integer to the collection. 3. **String Representation (`tp_str`)**: Implement a string representation method that returns a string like `\\"CustomCollection([1, 2, 3])\\"`. 4. **Attribute Management**: Implement attribute management for a `size` attribute that returns the number of elements in the collection and allows setting a new size (truncates or extends the collection). 5. **Comparison**: Implement rich comparisons (`__lt__`, `__le__`, `__eq__`, `__ne__`, `__gt__`, `__ge__`) to compare the sizes of two `CustomCollection` instances. 6. **Iteration**: Implement iterator protocol allowing iteration over the integers in the collection. **Requirements:** 1. Create the necessary C structures and functions to define the `CustomCollection` type. 2. Register the type with the Python interpreter. 3. Ensure proper memory management, including allocation and deallocation. 4. Provide methods to handle attribute access and modification. 5. Implement the rich comparison methods and string representation. 6. Support iteration correctly with `tp_iter` and `tp_iternext`. **Constraints:** - Code should handle error cases, such as invalid initialization parameters or type mismatches. - Performance should be considered, especially for large collections. - Your solution should follow best practices for memory management and error handling. **Deliverables:** 1. A C file defining the `CustomCollection` type. 2. A Python script to build the extension module. 3. Example Python usage demonstrating initialization, adding elements, attribute access, comparison, and iteration. Example usage in Python: ```python from custom_extension import CustomCollection # Initialize the collection col = CustomCollection([1, 2, 3]) # Add an element col.add(4) # Print string representation print(col) # Output: CustomCollection([1, 2, 3, 4]) # Access and modify the size attribute print(col.size) # Output: 4 col.size = 2 print(col) # Output: CustomCollection([1, 2]) # Compare collections col2 = CustomCollection([5, 6, 7]) print(col < col2) # Output: True # Iterate over the collection for item in col: print(item) # Output: 1 2 ``` Feel free to extend and document your code comprehensively.","solution":"def add(a, b): Returns the sum of a and b. return a + b"},{"question":"Objective To assess your understanding of Python\'s `multiprocessing` module, you will implement a system that spawns multiple processes to perform various computational tasks. The tasks and data will be distributed among the processes using shared memory and synchronization mechanisms. The results of these computations will be collected and aggregated in the main process. Problem Statement Implement a function `distributed_computation` that performs the following operations: 1. Creates a pool of worker processes to execute a given computational function concurrently. 2. Uses shared memory constructs to store the results of computations. 3. Employs synchronization mechanisms to ensure safe access to shared resources. 4. Aggregates the results from all workers and returns the final combined result. # Computational Function Your function will receive a computational function `compute` with the following signature: ```python def compute(data: Any) -> Any: pass ``` This function performs some computation on the input `data` and returns the result. # Requirements 1. The `distributed_computation` function should accept the following parameters: - `compute`: The computational function to be executed by worker processes. - `data_list`: A list of data items to be processed by the workers. - `num_workers`: The number of worker processes to create. - `aggregator`: A function to aggregate the results from all workers. Signature: ```python def aggregator(results: List[Any]) -> Any: pass ``` 2. Use `multiprocessing`\'s shared memory constructs to store the results safely. 3. Ensure proper synchronization using locks or other synchronization mechanisms. 4. Return the aggregated result after all computations are complete. # Constraints - Each worker process should work on a single data item from `data_list`. - Properly handle the starting and joining of all processes. - Ensure thread safety when accessing shared resources. # Example ```python import multiprocessing from multiprocessing import Value, Array, Lock from typing import Any, List def compute(data: int) -> int: return data * data def aggregator(results: List[int]) -> int: return sum(results) def distributed_computation(compute, data_list: List[int], num_workers: int, aggregator): # Create a result array using shared memory results = Array(\'i\', len(data_list)) lock = Lock() # Define a worker function def worker(data, index): result = compute(data) with lock: results[index] = result # Create and start worker processes processes = [] for i, data in enumerate(data_list): p = multiprocessing.Process(target=worker, args=(data, i)) processes.append(p) p.start() # Ensure all processes complete for p in processes: p.join() # Aggregate results final_result = aggregator(results[:]) return final_result # Example usage if __name__ == \\"__main__\\": data_list = [1, 2, 3, 4, 5] num_workers = 3 print(distributed_computation(compute, data_list, num_workers, aggregator)) # Output: 55 ``` # Notes - Pay attention to synchronization to avoid race conditions. - Ensure that the shared memory constructs are properly used. - Handle potential edge cases where the number of workers exceeds the number of data items.","solution":"import multiprocessing from multiprocessing import Array, Lock from typing import Any, List def distributed_computation(compute, data_list: List[int], num_workers: int, aggregator): # Create a result array using shared memory results = Array(\'i\', len(data_list)) lock = Lock() # Define a worker function def worker(data, index): result = compute(data) with lock: results[index] = result # Create and start worker processes processes = [] for i, data in enumerate(data_list): p = multiprocessing.Process(target=worker, args=(data, i)) processes.append(p) p.start() # Ensure all processes complete for p in processes: p.join() # Aggregate results final_result = aggregator(results[:]) return final_result def compute(data: int) -> int: return data * data def aggregator(results: List[int]) -> int: return sum(results) # Example usage if __name__ == \\"__main__\\": data_list = [1, 2, 3, 4, 5] num_workers = 3 print(distributed_computation(compute, data_list, num_workers, aggregator)) # Output: 55"},{"question":"As a Python developer, it\'s crucial to understand and correctly utilize the `__main__` namespace in your Python scripts and packages. This ensures that certain code is executed only when the script is run directly and not when imported as a module. It also helps maintain code clarity and reusability. Problem Statement You are required to create a Python script that processes command-line arguments and performs specific actions based on those arguments. If the script is run directly, it should greet the user with a welcome message and display the provided name and age. If the script is imported as a module, it should not perform any actions immediately but should allow the importing file to use the provided functions to access the command-line arguments. Implement the following functions in your script: 1. `parse_arguments()`: This function should parse the command-line arguments `name` and `age`. 2. `greet_user(name: str, age: int) -> str`: This function should return a greeting message that includes the given `name` and `age`. 3. `main()`: This function should call `parse_arguments()` to get the command-line arguments, then call `greet_user()` with the parsed arguments and display the returned message. Use the following command-line syntax to run your script: ``` python3 script.py --name <your_name> --age <your_age> ``` # Function Specifications 1. `parse_arguments() -> tuple[str, int]`: * Parses `--name` and `--age` from the command line. * Returns a tuple `(name, age)`. 2. `greet_user(name: str, age: int) -> str`: * Takes `name` and `age` as input. * Returns a string: `\\"Hello, <name>! You are <age> years old.\\"` 3. `main()`: * Calls `parse_arguments()` to get the `name` and `age`. * Calls `greet_user()` with the parsed `name` and `age`. * Prints the result of `greet_user()`. # Example: If the script is run with the following command-line arguments: ``` python3 script.py --name Alice --age 30 ``` The output should be: ``` Hello, Alice! You are 30 years old. ``` Ensure that your script includes the following check to only run the `main()` function if the script is executed directly: ```python if __name__ == \\"__main__\\": main() ``` # Constraints: * You may assume that the `--name` argument will always be a string and the `--age` argument will always be an integer. * You should handle the case where arguments are missing by displaying an appropriate error message. # Evaluation Criteria: * Correctness: Ensure that your script correctly parses arguments and generates the correct greeting message. * Usage of `if __name__ == \\"__main__\\"` block. * Code clarity and reusability.","solution":"import argparse def parse_arguments(): Parses command-line arguments for name and age. Returns a tuple (name, age) where name is a string and age is an integer. parser = argparse.ArgumentParser(description=\'Process user name and age.\') parser.add_argument(\'--name\', type=str, required=True, help=\'Name of the user\') parser.add_argument(\'--age\', type=int, required=True, help=\'Age of the user\') args = parser.parse_args() return args.name, args.age def greet_user(name: str, age: int) -> str: Returns a greeting message that includes the given name and age. return f\\"Hello, {name}! You are {age} years old.\\" def main(): Main function that calls parse_arguments and greet_user, and prints the resulting greeting message. name, age = parse_arguments() message = greet_user(name, age) print(message) if __name__ == \\"__main__\\": main()"},{"question":"# Complex Number Operations using `cmath` Module **Objective:** Write a Python class `ComplexNumberOps` that leverages the `cmath` module to perform and validate complex number operations. The class should offer methods for conversion between rectangular and polar coordinates, calculation of various mathematical functions, and classification of complex numbers based on their properties. **Requirements:** 1. **Class Definition:** - Define a class `ComplexNumberOps`. 2. **Initialization:** - The class should initialize with a complex number `z`. 3. **Methods:** Implement the following methods in the `ComplexNumberOps` class: ```python class ComplexNumberOps: def __init__(self, z: complex): self.z = z def to_polar(self) -> tuple: Convert the stored complex number to polar coordinates. Returns: tuple: A tuple containing the modulus (r) and phase angle (phi) in radians. def from_polar(self, r: float, phi: float) -> complex: Convert polar coordinates back to a complex number. Args: r (float): The modulus of the complex number. phi (float): The phase angle in radians. Returns: complex: The complex number corresponding to given polar coordinates. def exponential(self) -> complex: Calculate the exponential e**z of the stored complex number. Returns: complex: Result of e**z. def logarithm(self, base: float = None) -> complex: Calculate the logarithm of the stored complex number, optionally to a specified base. Args: base (float, optional): The base of the logarithm. Defaults to None (natural logarithm). Returns: complex: The logarithm of the complex number to the given base. def classify(self) -> dict: Classify the stored complex number as finite, infinite, or NaN. Returns: dict: A dictionary with boolean values for keys \'finite\', \'infinite\', and \'nan\'. ``` **Input and Output:** - The class is initialized with a complex number. - Each method operates on this complex number and produces the specified results. **Constraints:** - The complex number can be any valid complex number in Python. - Handle edge cases such as branch cuts and special values (NaN and infinity) appropriately. - Use the `cmath` module only for mathematical operations. **Example Usage:** ```python # Initialize with a complex number cn_ops = ComplexNumberOps(complex(-2.0, 0.0)) # Convert to polar coordinates print(cn_ops.to_polar()) # Output: (2.0, 3.141592653589793) # Convert from polar coordinates print(cn_ops.from_polar(2.0, cmath.pi)) # Output: (-2+2.4492935982947064e-16j) # Calculate exponential print(cn_ops.exponential()) # Output: (-0.1353352832366127+0j) # Calculate natural logarithm print(cn_ops.logarithm()) # Output: (0.6931471805599453+3.141592653589793j) # Classify the complex number print(cn_ops.classify()) # Output: {\'finite\': True, \'infinite\': False, \'nan\': False} ``` Ensure your implementations are efficient and handle all edge cases as described in the documentation.","solution":"import cmath class ComplexNumberOps: def __init__(self, z: complex): self.z = z def to_polar(self) -> tuple: Convert the stored complex number to polar coordinates. Returns: tuple: A tuple containing the modulus (r) and phase angle (phi) in radians. return cmath.polar(self.z) def from_polar(self, r: float, phi: float) -> complex: Convert polar coordinates back to a complex number. Args: r (float): The modulus of the complex number. phi (float): The phase angle in radians. Returns: complex: The complex number corresponding to given polar coordinates. return cmath.rect(r, phi) def exponential(self) -> complex: Calculate the exponential e**z of the stored complex number. Returns: complex: Result of e**z. return cmath.exp(self.z) def logarithm(self, base: float = None) -> complex: Calculate the logarithm of the stored complex number, optionally to a specified base. Args: base (float, optional): The base of the logarithm. Defaults to None (natural logarithm). Returns: complex: The logarithm of the complex number to the given base. if base is None: return cmath.log(self.z) else: return cmath.log(self.z) / cmath.log(base) def classify(self) -> dict: Classify the stored complex number as finite, infinite, or NaN. Returns: dict: A dictionary with boolean values for keys \'finite\', \'infinite\', and \'nan\'. return { \'finite\': cmath.isfinite(self.z), \'infinite\': cmath.isinf(self.z), \'nan\': cmath.isnan(self.z) }"},{"question":"# Question: Converting a Python Function Using Argument Clinic As an experienced Python developer, you are asked to demonstrate your understanding of Argument Clinic by converting a basic Python function into an equivalent Argument Clinic block, ensuring that the generated C code correctly parses arguments. Objective: You need to write a Python function that emulates the behavior of a built-in CPython function using Argument Clinic principles. The function will demonstrate defining the input parameters, handling default values, and generating the necessary C code for argument parsing. Task: 1. Implement a Python function named `example_function` that takes the following parameters: - `a` (an integer, required). - `b` (an optional string, default value is \\"default\\"). - `c` (an optional boolean, default value is `True`). 2. Create an Argument Clinic block that specifies this function, including proper input parameter definitions and default values, and generates the necessary C code for parsing these arguments. 3. Include per-parameter docstrings and ensure the generated C code reflects the correct parsing logic. Input and Output Formats: - `input`: - An integer `a` that is required. - An optional string `b` (default value is \\"default\\"). - An optional boolean `c` (default value is `True`). - `output`: - Return a tuple `(a, b, c)`. Constraints: - The `a` parameter is required and must be an integer. - The `b` parameter is optional and must be a string if provided. - The `c` parameter is optional and must be a boolean if provided. Example: ```python def example_function(a, b=\\"default\\", c=True): Function that returns a tuple of input parameters. Parameters: a (int): Required integer parameter. b (str): Optional string parameter. Default is \\"default\\". c (bool): Optional boolean parameter. Default is True. Returns: tuple: A tuple containing the input parameters (a, b, c). return (a, b, c) ``` Convert this function into an Argument Clinic block to generate the necessary C code. Notes: - Make sure to include the necessary boilerplate code for defining the module and function in the Argument Clinic block. - Ensure the Argument Clinic block is correctly formatted and follows the guidelines as described in the provided documentation.","solution":"def example_function(a, b=\\"default\\", c=True): Function that returns a tuple of input parameters. Parameters: a (int): Required integer parameter. b (str): Optional string parameter. Default is \\"default\\". c (bool): Optional boolean parameter. Default is True. Returns: tuple: A tuple containing the input parameters (a, b, c). return (a, b, c)"},{"question":"# LDA and QDA Application Objective Design a function using scikit-learn\'s LDA and QDA that evaluates their performance on a given dataset with specified parameters. Problem Statement You are given a labeled dataset and need to: 1. Perform Linear Discriminant Analysis (LDA) for dimensionality reduction and classification. 2. Perform Quadratic Discriminant Analysis (QDA) for classification. 3. Evaluate and compare the classification accuracy of both methods. Implement the following function: ```python def evaluate_lda_qda(X_train, y_train, X_test, y_test, n_components=None, shrinkage=None, solver=\'svd\'): Evaluates LDA and QDA on the provided training and testing data. Parameters: - X_train: numpy.ndarray of shape (n_samples_train, n_features), the training features. - y_train: numpy.ndarray of shape (n_samples_train,), the training labels. - X_test: numpy.ndarray of shape (n_samples_test, n_features), the testing features. - y_test: numpy.ndarray of shape (n_samples_test,), the testing labels. - n_components: int, the number of components for LDA dimensionality reduction. Default is None. - shrinkage: str or float, shrinkage parameter for LDA (\'auto\', None, 0 <= shrinkage <= 1). Default is None. - solver: str, the solver type for LDA (\'svd\', \'lsqr\', \'eigen\'). Default is \'svd\'. Returns: - result: dict, contains accuracy scores of LDA and QDA. Example: {\'LDA_accuracy\': 0.85, \'QDA_accuracy\': 0.80} pass # Implement the function here ``` Constraints 1. Ensure the implementation uses scikit-learn\'s `LinearDiscriminantAnalysis` and `QuadraticDiscriminantAnalysis` classes. 2. `n_components` should be set only for LDA if it is not None. 3. The `shrinkage` parameter should only be considered if the LDA solver is set to \'lsqr\' or \'eigen\'. 4. Include error handling for invalid parameters or incompatible inputs. 5. Aim for efficient computation suited for datasets with a moderate number of features (up to 1000). Example Usage ```python from sklearn.model_selection import train_test_split from sklearn.datasets import load_iris # Load example data data = load_iris() X = data.data y = data.target # Split the data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Evaluate LDA and QDA result = evaluate_lda_qda(X_train, y_train, X_test, y_test, n_components=2, shrinkage=\'auto\', solver=\'eigen\') print(result) # Expected: {\'LDA_accuracy\': <some_value>, \'QDA_accuracy\': <some_value>} ``` Notes - Use suitable performance metrics and validation techniques to ensure robust evaluation. - Explain how the dimensionality reduction in LDA affects its performance compared to QDA, especially considering the separation boundaries. - Discuss the impact of different shrinkage methods and solvers on the results.","solution":"from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis from sklearn.metrics import accuracy_score def evaluate_lda_qda(X_train, y_train, X_test, y_test, n_components=None, shrinkage=None, solver=\'svd\'): Evaluates LDA and QDA on the provided training and testing data. Parameters: - X_train: numpy.ndarray of shape (n_samples_train, n_features), the training features. - y_train: numpy.ndarray of shape (n_samples_train,), the training labels. - X_test: numpy.ndarray of shape (n_samples_test, n_features), the testing features. - y_test: numpy.ndarray of shape (n_samples_test,), the testing labels. - n_components: int, the number of components for LDA dimensionality reduction. Default is None. - shrinkage: str or float, shrinkage parameter for LDA (\'auto\', None, 0 <= shrinkage <= 1). Default is None. - solver: str, the solver type for LDA (\'svd\', \'lsqr\', \'eigen\'). Default is \'svd\'. Returns: - result: dict, contains accuracy scores of LDA and QDA. Example: {\'LDA_accuracy\': 0.85, \'QDA_accuracy\': 0.80} # Initialize dictionaries to store results result = {} # Perform LDA lda = LinearDiscriminantAnalysis(n_components=n_components, shrinkage=shrinkage, solver=solver) lda.fit(X_train, y_train) y_pred_lda = lda.predict(X_test) lda_accuracy = accuracy_score(y_test, y_pred_lda) result[\'LDA_accuracy\'] = lda_accuracy # Perform QDA qda = QuadraticDiscriminantAnalysis() qda.fit(X_train, y_train) y_pred_qda = qda.predict(X_test) qda_accuracy = accuracy_score(y_test, y_pred_qda) result[\'QDA_accuracy\'] = qda_accuracy return result"},{"question":"# Shared Memory Inter-Process Communication **Objective**: You are tasked with creating a shared memory system where multiple processes can read from and write to a shared list of integers. You will use the `multiprocessing.shared_memory` package to achieve this. Your implementation will involve creating a `SharedList` class that wraps around `multiprocessing.shared_memory.ShareableList` and provides methods for adding, removing, and updating elements within a fixed size list. **Instructions**: 1. **SharedList Class**: - Initialize with a fixed size and optionally a list of integers. - Provide methods to: - **add**: Add an integer to the first available (None) position. - **remove**: Remove integer(s) from the list if they exist. - **update**: Update the integer at a specific position. - **fetch**: Fetch the current list as a Python list. 2. **Multi-process Usage**: - A method to demonstrate your `SharedList` class where two processes perform different operations on the shared list. - Ensure that changes made by one process are reflected in the other. **Function Signature**: ```python class SharedList: def __init__(self, size: int, initial_list: list = None) -> None: pass def add(self, value: int) -> None: pass def remove(self, value: int) -> None: pass def update(self, index: int, value: int) -> None: pass def fetch(self) -> list: pass # Function to demonstrate usage in a multi-process scenario def demonstrate_shared_list() -> None: pass ``` **Requirements**: - The `SharedList` should handle shared memory cleanup appropriately. - The `demonstrate_shared_list` should create two processes that perform operations on the `SharedList` and print the results before and after to display synchronization. - If the list is full when trying to add a new element or an index is out of bounds for update, raise a suitable exception. - Make sure there are no race conditions. Example: ```python # Create a shared list with size 5 shared_list = SharedList(5, [1, 2, 3]) # Add an element shared_list.add(4) # Fetch the list: [1, 2, 3, 4, None] # Remove an element shared_list.remove(2) # Fetch the list: [1, None, 3, 4, None] # Update an element shared_list.update(1, 5) # Fetch the list: [1, 5, 3, 4, None] # Demonstrate multi-process usage demonstrate_shared_list() # Expected console output: # Before changes by Process 1: [1, 2, 3, None, None] # Before changes by Process 2: [1, 2, 3, None, None] # After changes by Process 1: [1, 2, 3, 4, None] # After changes by Process 2: [5, 2, 3, 4, None] ``` **Notes**: - The `SharedList` class should ensure thread-safe access if concurrent modifications are made. - Use the provided examples and ensure proper cleanup by closing and unlinking shared memory.","solution":"from multiprocessing import shared_memory, Process import multiprocessing from multiprocessing.shared_memory import ShareableList class SharedList: def __init__(self, size: int, initial_list = None) -> None: if initial_list is None: initial_list = [None] * size elif len(initial_list) > size: raise ValueError(\\"Initial list size exceeds specified size.\\") elif len(initial_list) < size: initial_list.extend([None] * (size - len(initial_list))) self.size = size self.shm = ShareableList(initial_list) def add(self, value: int) -> None: for i in range(self.size): if self.shm[i] is None: self.shm[i] = value return raise ValueError(\\"No available position to add the value\\") def remove(self, value: int) -> None: for i in range(self.size): if self.shm[i] == value: self.shm[i] = None return raise ValueError(\\"Value not found in the list\\") def update(self, index: int, value: int) -> None: if index < 0 or index >= self.size: raise IndexError(\\"Index is out of bounds\\") self.shm[index] = value def fetch(self) -> list: return list(self.shm) def cleanup(self) -> None: self.shm.shm.close() self.shm.shm.unlink() def demonstrate_shared_list() -> None: size = 5 initial_list = [1, 2, 3] shared_list = SharedList(size, initial_list) def process_1(): shared_list.add(4) print(f\\"After changes by Process 1: {shared_list.fetch()}\\") def process_2(): shared_list.update(0, 5) print(f\\"After changes by Process 2: {shared_list.fetch()}\\") p1 = Process(target=process_1) p2 = Process(target=process_2) print(f\\"Before changes: {shared_list.fetch()}\\") p1.start() p2.start() p1.join() p2.join() print(f\\"Final state: {shared_list.fetch()}\\") shared_list.cleanup() # If you want to test demonstrating multi-process example if __name__ == \\"__main__\\": demonstrate_shared_list()"},{"question":"**Question: Secure Password Management and Verification** As an experienced software developer, you have been tasked to develop a secure password management system for a new application. You need to implement three key functions using the `crypt` module: 1. **Function to hash a password using a specific method** 2. **Function to check if a provided password matches the hashed password** 3. **Function to generate a random salt using a specified method** **Details**: 1. **Function 1: `hash_password`** - **Input**: - `password`: A string representing the plaintext password. - `method`: A cryptographic method (e.g., `crypt.METHOD_SHA512`). - **Output**: - A string representing the hashed password. 2. **Function 2: `check_password`** - **Input**: - `password`: A string representing the plaintext password. - `hashed`: The string of the previously hashed password. - **Output**: - A boolean value `True` if the password matches the hash, otherwise `False`. 3. **Function 3: `generate_salt`** - **Input**: - `method`: A cryptographic method (e.g., `crypt.METHOD_SHA512`). - **Output**: - A string representing a randomly generated salt suitable for the given method. **Constraints and Requirements**: - The `hash_password` function should use the provided method for hashing. - The `check_password` function should use the `crypt.crypt` function to verify that the hashed password matches. - The `generate_salt` function should generate a salt based on the method provided. - Use `secure` techniques to prevent timing attacks while comparing passwords. - Performance is essential; make the functions efficient and avoid unnecessary computations. **Example Usage**: ```python import crypt from hmac import compare_digest as compare_hash def hash_password(password: str, method) -> str: # Your implementation here def check_password(password: str, hashed: str) -> bool: # Your implementation here def generate_salt(method) -> str: # Your implementation here # Sample usage: method = crypt.METHOD_SHA512 salt = generate_salt(method) hashed_password = hash_password(\\"my_secure_password\\", method) is_match = check_password(\\"my_secure_password\\", hashed_password) print(is_match) # Should print: True ``` Your task is to implement these three functions (`hash_password`, `check_password`, and `generate_salt`) according to the details provided.","solution":"import crypt from hmac import compare_digest as compare_hash def hash_password(password: str, method) -> str: Hashes the given password with the specified method. salt = crypt.mksalt(method) return crypt.crypt(password, salt) def check_password(password: str, hashed: str) -> bool: Checks if the provided password matches the hashed password. return compare_hash(crypt.crypt(password, hashed), hashed) def generate_salt(method) -> str: Generates a random salt suitable for the given method. return crypt.mksalt(method)"},{"question":"**Objective:** Write a Python function that validates and extends a given `setup.cfg` file based on specific rules and user inputs. **Problem Statement:** You are provided with a `setup.cfg` file containing configuration for building Python distributions. Your task is to write a function `update_setup_config(file_path: str, user_inputs: dict) -> None` that validates the existing configuration and updates it based on the following rules: 1. The function should read the existing `setup.cfg` file at the specified `file_path`. 2. It should validate that each section (e.g., `[command]`) and option (e.g., `option=value`) follows the correct format. 3. For each command specified in the `user_inputs` dictionary: - If the command already exists in the configuration file, update the specified options with the given values. - If the command does not exist, add the command and its options to the configuration file. 4. Write the updated configuration back to the `setup.cfg` file while preserving the original comments and formatting where possible. **Function Signature:** ```python def update_setup_config(file_path: str, user_inputs: dict) -> None: pass ``` **Input:** - `file_path` (str): The path to the `setup.cfg` file. - `user_inputs` (dict): A dictionary where keys are commands (str) and values are dictionaries of options (dict). For example: ```python { \\"build_ext\\": { \\"inplace\\": \\"1\\", \\"include_dirs\\": \\"/usr/local/include\\" }, \\"bdist_rpm\\": { \\"release\\": \\"2\\", \\"packager\\": \\"Jane Doe <jane.doe@example.com>\\" } } ``` **Output:** - The function does not return any value. It updates the `setup.cfg` file in-place. **Constraints:** - The configuration file might not have all commands present initially. - All user-provided commands and options should be added or updated in the configuration file. - Preserve existing comments and blank lines. **Example:** Suppose the initial `setup.cfg` file is: ``` [build_ext] inplace=1 # RPM build options [bdist_rpm] release=1 packager=Greg Ward <gward@python.net> ``` Function call: ```python update_setup_config(\\"path/to/setup.cfg\\", { \\"build_ext\\": { \\"include_dirs\\": \\"/usr/local/include\\" }, \\"bdist_rpm\\": { \\"release\\": \\"2\\", \\"packager\\": \\"Jane Doe <jane.doe@example.com>\\" } }) ``` Updated `setup.cfg` should be: ``` [build_ext] inplace=1 include_dirs=/usr/local/include # RPM build options [bdist_rpm] release=2 packager=Jane Doe <jane.doe@example.com> ``` **Notes:** - This exercise tests your ability to work with configuration files, dynamic updates, and handling hierarchical data structures in Python.","solution":"import configparser def update_setup_config(file_path: str, user_inputs: dict) -> None: Updates the setup.cfg file based on user inputs. Args: file_path (str): Path to the setup.cfg file. user_inputs (dict): A dictionary with update instructions. config = configparser.ConfigParser(allow_no_value=True) config.read(file_path) # Update or add sections and their options for section, options in user_inputs.items(): if not config.has_section(section): config.add_section(section) for option, value in options.items(): config.set(section, option, value) # Write the updated configuration back to the setup.cfg file with open(file_path, \'w\') as configfile: config.write(configfile)"},{"question":"# Advanced Python Coding Assessment Question **Objective:** Demonstrate your understanding of Python\'s `email.generator` module by creating an email message with mixed content, serializing it using different generators, and ensuring compliance with encoding standards and RFC requirements. # Problem Statement You are required to create a function `create_serialized_email` that generates an email message with text and binary attachments, serializes it using both `BytesGenerator` and `DecodedGenerator`, and returns the serialized outputs. # Requirements: 1. **Function Signature:** ```python def create_serialized_email() -> tuple: ``` 2. **Function Description:** - The function should create an `EmailMessage` object with the following content: - A plain text body. - A binary file attachment (you can simulate this with some dummy binary data). - Serialize the email message using `BytesGenerator` and `DecodedGenerator`. - Ensure that the serialization takes into account appropriate MIME types and content-transfer encodings. - Return the serialized outputs for both generators as a tuple of strings. 3. **Serialization Details:** - Use the `BytesGenerator` to serialize the email into a binary format. - Use the `DecodedGenerator` to create a simplified readable format where non-text parts are represented by informative strings. 4. **Policy Settings:** - Ensure the use of appropriate policy settings to guarantee RFC conformity and proper header wrapping. - Handle any necessary encoding transformations to maintain the integrity of the message content. # Expected Input and Output 1. **Input:** - No input parameters are required for the function. 2. **Output:** - A tuple containing two serialized representations of the email message. - The first element should be the output from `BytesGenerator`. - The second element should be the output from `DecodedGenerator`. # Example: ```python result = create_serialized_email() print(result[0]) # Serialized output from BytesGenerator (should be in bytes) print(result[1]) # Serialized output from DecodedGenerator (should be a readable string) ``` # Constraints: - You should use the `BytesGenerator` and `DecodedGenerator` classes from the `email.generator` module. - Ensure that the generated email is standards-compliant with respect to MIME types and content-transfer encodings. - Take care of any necessary encoding and wrapping of headers to fit specified limits (if applicable). # Additional Notes: - You may use hardcoded binary data for the attachment (e.g., b\'x00x01x02\'). - Ensure proper handling of Unicode and binary data to avoid encoding issues. - You are allowed to import necessary modules from Python’s standard library, but avoid using third-party packages that are not part of the email module. Happy coding!","solution":"from email.message import EmailMessage from email.generator import BytesGenerator, DecodedGenerator from email.policy import default def create_serialized_email() -> tuple: # Create a basic EmailMessage object msg = EmailMessage() msg[\'Subject\'] = \'Test Email\' msg[\'From\'] = \'sender@example.com\' msg[\'To\'] = \'recipient@example.com\' # Set the plain text body msg.set_content(\\"This is a plain text body of the email.\\") # Add a binary attachment binary_data = b\'x00x01x02x03x04x05x06x07x08x09\' msg.add_attachment(binary_data, maintype=\'application\', subtype=\'octet-stream\', filename=\'example.bin\') # Serialize the email using BytesGenerator from io import BytesIO, StringIO bytes_buffer = BytesIO() bytes_gen = BytesGenerator(bytes_buffer, policy=default) bytes_gen.flatten(msg) bytes_output = bytes_buffer.getvalue() # Serialize the email using DecodedGenerator string_buffer = StringIO() decoded_gen = DecodedGenerator(string_buffer, policy=default) decoded_gen.flatten(msg) decoded_output = string_buffer.getvalue() return bytes_output, decoded_output"},{"question":"# Seaborn and Matplotlib Integration Assessment **Objective:** Create a multi-faceted visualization using Seaborn\'s `Plot` objects and Matplotlib\'s customization features to demonstrate proficiency with Seaborn\'s advanced plotting capabilities and seamless integration with Matplotlib. **Instructions:** 1. **Dataset Loading:** Load the `diamonds` dataset using Seaborn\'s `load_dataset` function. 2. **Primary Plot:** Create a scatter plot showing the relationship between `carat` and `price` of diamonds using Seaborn\'s `Plot` class and `Dots` function. 3. **Advanced Customization:** Customize the resulting plot by embedding it in a Matplotlib figure and adding the following custom features: - A rectangular patch with 40% width and 10% height of the plot area, positioned in the top-right corner (outside the plotting area), with transparency level set to 20%. - A text annotation within the rectangular patch stating \\"Diamonds: very sparkly!\\" centered both horizontally and vertically. 4. **Subfigures:** Create a secondary plot within the same Matplotlib figure: - Divide the figure into two subfigures arranged horizontally. - In the second subfigure, create a faceted histogram of diamond prices, divided by the diamond\'s cut. Apply a logarithmic scale on the x-axis and ensure y-axes are not shared across facets. 5. **Display:** Ensure the figure is displayed correctly with all specified customizations and subfigures in place. **Constraints:** - The solution should be implemented in Python using Seaborn and Matplotlib. - The plot customizations should be applied using Matplotlib\'s transformation and artist mechanisms as demonstrated. - The figure layout should be managed to ensure no overlaps or visual inconsistencies. **Expected Input:** No specific input from the user, except for the required packages and dataset loading. **Expected Output:** A Matplotlib figure containing: - One primary scatter plot with the specified customizations. - One secondary subfigure with a faceted histogram as described. ```python import seaborn as sns import seaborn.objects as so import matplotlib as mpl import matplotlib.pyplot as plt # Load the dataset diamonds = sns.load_dataset(\\"diamonds\\") # Create the primary plot using seaborn.objects.Plot and Dots p = so.Plot(diamonds, \\"carat\\", \\"price\\").add(so.Dots()) # Create a matplotlib figure and subfigure layout f = mpl.figure.Figure(figsize=(14, 7), dpi=100, layout=\\"constrained\\") sf1, sf2 = f.subfigures(1, 2) # Render the primary plot on the first subfigure p.on(sf1).plot() # Add custom rectangular patch and annotation ax1 = sf1.axes[0] rect = mpl.patches.Rectangle( xy=(0.6, 1), width=.4, height=.1, color=\\"C1\\", alpha=.2, transform=ax1.transAxes, clip_on=False, ) ax1.add_artist(rect) ax1.text( x=0.8, y=1.05, s=\\"Diamonds: very sparkly!\\", size=12, ha=\\"center\\", va=\\"center\\", transform=ax1.transAxes, ) # Create the faceted histogram in the second subfigure ( so.Plot(diamonds, x=\\"price\\") .add(so.Bars(), so.Hist()) .facet(row=\\"cut\\") .scale(x=\\"log\\") .share(y=False) .on(sf2) ) # Display the figure plt.show() ```","solution":"import seaborn as sns import seaborn.objects as so import matplotlib as mpl import matplotlib.pyplot as plt def create_diamond_scatter_and_hist(): # Load the dataset diamonds = sns.load_dataset(\\"diamonds\\") # Create the primary plot using seaborn.objects.Plot and Dots p = so.Plot(diamonds, \\"carat\\", \\"price\\").add(so.Dots()) # Create a matplotlib figure and subfigure layout f = mpl.figure.Figure(figsize=(14, 7), dpi=100, layout=\\"constrained\\") sf1, sf2 = f.subfigures(1, 2) # Render the primary plot on the first subfigure p.on(sf1).plot() # Add custom rectangular patch and annotation ax1 = sf1.axes[0] rect = mpl.patches.Rectangle( xy=(0.6, 1), width=.4, height=.1, color=\\"C1\\", alpha=.2, transform=ax1.transAxes, clip_on=False, ) ax1.add_artist(rect) ax1.text( x=0.8, y=1.05, s=\\"Diamonds: very sparkly!\\", size=12, ha=\\"center\\", va=\\"center\\", transform=ax1.transAxes, ) # Create the faceted histogram in the second subfigure ( so.Plot(diamonds, x=\\"price\\") .add(so.Bars(), so.Hist()) .facet(row=\\"cut\\") .scale(x=\\"log\\") .share(y=False) .on(sf2) ) # Display the figure plt.show()"},{"question":"# Question: Custom Exception Handling and Manipulation Objective: Write a Python program that demonstrates a deep understanding of custom exception creation and handling, including setting custom messages, associating additional context, and correctly managing error states. Requirements: 1. **Define a Custom Exception Class:** Create a custom exception class called `CustomError` which inherits from the base `Exception` class. 2. **Exception Initialization:** This class should accept an additional parameter `context` during initialization, which will store extra information about the error. 3. **Raise Exception with Context:** Write a function `raise_custom_exception` which raises this `CustomError` with a specific message and context. 4. **Handle the Exception:** Write a function `handle_exception` that: - Catches the `CustomError`. - Prints the error message, context, traceback, and the custom stack level at which the error occurred. 5. **Issue a Warning:** Use the `warnings` module to issue a custom warning if a specific condition (e.g., context is empty) is met when handling the exception. 6. **Document All Functions:** Provide docstrings for all functions explaining their purpose and behavior. Constraints: - You must use Python\'s built-in `warnings`, `traceback`, and `sys` modules where applicable. - The `CustomError` class must correctly propagate the context information. Example: ```python class CustomError(Exception): Custom exception class with additional context. def __init__(self, message, context): super().__init__(message) self.context = context def raise_custom_exception(): Function to raise a custom exception with context. # Provide logic pass def handle_exception(): Function to handle custom exception and print error details. try: raise_custom_exception() except CustomError as ce: # Handle exception and issue warning if context is empty # Provide logic pass ``` Expected Output: - Raising and handling the `CustomError` should print the error message, context, traceback, and stack level. - If the context is empty, a custom warning should be issued using the `warnings` module. By completing this task, you will demonstrate your ability to create custom exceptions, utilize Python\'s error handling and warnings systems, and manage exception contexts effectively.","solution":"import warnings import traceback import sys class CustomError(Exception): Custom exception class with additional context. def __init__(self, message, context): super().__init__(message) self.context = context def raise_custom_exception(message, context): Function to raise a custom exception with a specific message and context. :param message: The error message to be raised. :param context: Additional context information for the error. raise CustomError(message, context) def handle_exception(message, context): Function to handle CustomError exception and print error details. :param message: The error message to be raised. :param context: Additional context information for the error. try: raise_custom_exception(message, context) except CustomError as ce: print(f\\"Error Message: {ce}\\") print(f\\"Context: {ce.context}\\") print(\\"Traceback:\\") traceback.print_exc(file=sys.stdout) if not ce.context: warnings.warn(\\"Warning: The context is empty.\\")"},{"question":"Coding Assessment Question # Objective: Create a Python class and write corresponding unit tests using the `unittest.mock` library to demonstrate your understanding of mocking, patching, handling side effects, and making assertions. # Problem Statement: You are required to implement a `DataProcessor` class that performs data fetching and processing operations. Additionally, write unit tests using the `unittest` framework and the `unittest.mock` library to verify the behavior of the `DataProcessor` class. # `DataProcessor` Class: 1. `__init__(self, fetcher)`: Initializes with a data `fetcher` dependency. 2. `fetch_and_process(self, endpoint)`: Uses the `fetcher` to get data from an endpoint and processes the data. - If fetching data raises an `Exception`, it should handle the exception and return `None`. - If data is successfully fetched, it processes the data by transforming each element in the list to its uppercase form. 3. `process_data(self, data)`: Processes the data by transforming each element in the list to its uppercase form. # Requirements: - Implement the `DataProcessor` class. - Write unit tests for the `DataProcessor` class methods using `unittest` and `unittest.mock`. # Constraints: - The `fetcher` argument in the `DataProcessor` class is an object with a `fetch` method that takes an `endpoint` as its argument and returns a list of strings. - The `fetch` method can raise an `Exception`. # Example: ```python class DataProcessor: def __init__(self, fetcher): self.fetcher = fetcher def fetch_and_process(self, endpoint): try: data = self.fetcher.fetch(endpoint) return self.process_data(data) except Exception: return None def process_data(self, data): return [element.upper() for element in data] ``` # Tasks: 1. Implement the `DataProcessor` class. 2. Write unit tests for the following scenarios using `unittest.mock`: - Verify that `fetch` is called with the correct endpoint. - Test that `fetch_and_process` handles exceptions correctly and returns `None`. - Test the processing of data to ensure elements are correctly transformed to uppercase. - Test the behavior of `process_data` method separately. # Input and Output Format: - No direct input from the user is required. - Test methods should use the `unittest` framework to validate the correctness of the `DataProcessor` methods. # Example Unit Test Skeleton: ```python import unittest from unittest.mock import Mock, patch class TestDataProcessor(unittest.TestCase): def test_fetch_and_process_success(self): fetcher_mock = Mock() fetcher_mock.fetch.return_value = [\'data1\', \'data2\'] processor = DataProcessor(fetcher_mock) result = processor.fetch_and_process(\'endpoint\') fetcher_mock.fetch.assert_called_once_with(\'endpoint\') self.assertEqual(result, [\'DATA1\', \'DATA2\']) def test_fetch_and_process_exception(self): fetcher_mock = Mock() fetcher_mock.fetch.side_effect = Exception(\'fetch error\') processor = DataProcessor(fetcher_mock) result = processor.fetch_and_process(\'endpoint\') self.assertIsNone(result) def test_process_data(self): processor = DataProcessor(Mock()) data = [\'data1\', \'data2\'] result = processor.process_data(data) self.assertEqual(result, [\'DATA1\', \'DATA2\']) if __name__ == \'__main__\': unittest.main() ```","solution":"class DataProcessor: def __init__(self, fetcher): self.fetcher = fetcher def fetch_and_process(self, endpoint): try: data = self.fetcher.fetch(endpoint) return self.process_data(data) except Exception: return None def process_data(self, data): return [element.upper() for element in data]"},{"question":"**Question: Time Zone Aware Event Scheduler** You are tasked with implementing a function to schedule events across different time zones while keeping track of their local times. Implement the following class: # Class: `EventScheduler` Methods: 1. **`__init__(self)`**: - Initializes an empty dictionary to store events by their IDs. 2. **`add_event(self, event_id: str, dt: str, tz: str)`**: - Adds a new event with a given `event_id`, scheduled at a given time `dt` (ISO format string), in a specified timezone `tz`. - `dt` is an ISO formatted datetime string (e.g., `\\"2023-02-15T18:00:00\\"`). - `tz` is a string specifying the IANA timezone (e.g., `\\"America/Los_Angeles\\"`). - If an event with the same `event_id` already exists, it should update the existing event. 3. **`list_events(self)`**: - Returns a list of all stored events. Each event should be a dictionary containing: - `event_id`: ID of the event. - `datetime`: The datetime of the event in ISO format. - `timezone`: The timezone of the event. 4. **`get_event_in_utc(self, event_id: str) -> dict`**: - Returns the details of the specified event, but with its datetime converted to UTC. The returned dictionary should contain: - `event_id`: ID of the event. - `datetime`: The datetime of the event in ISO format, converted to UTC. - `timezone`: The string `\'UTC\'`. Constraints: - Use the `ZoneInfo` class for handling time zones. - Raise a `KeyError` if trying to get an event that doesn\'t exist. - Raise a `ValueError` if the provided datetime or timezone string is invalid. Example Usage: ```python scheduler = EventScheduler() scheduler.add_event(\\"event1\\", \\"2022-11-01T14:00:00\\", \\"America/Los_Angeles\\") scheduler.add_event(\\"event2\\", \\"2023-03-10T09:00:00\\", \\"Asia/Tokyo\\") print(scheduler.list_events()) # [{\'event_id\': \'event1\', \'datetime\': \'2022-11-01T14:00:00\', \'timezone\': \'America/Los_Angeles\'}, # {\'event_id\': \'event2\', \'datetime\': \'2023-03-10T09:00:00\', \'timezone\': \'Asia/Tokyo\'}] print(scheduler.get_event_in_utc(\\"event1\\")) # {\'event_id\': \'event1\', \'datetime\': \'2022-11-01T21:00:00+00:00\', \'timezone\': \'UTC\'} ``` Implement the `EventScheduler` class as described.","solution":"from datetime import datetime from zoneinfo import ZoneInfo class EventScheduler: def __init__(self): self.events = {} def add_event(self, event_id: str, dt: str, tz: str): try: event_dt = datetime.fromisoformat(dt) event_tz = ZoneInfo(tz) event_dt = event_dt.replace(tzinfo=event_tz) except Exception: raise ValueError(\\"Invalid datetime or timezone string\\") self.events[event_id] = { \\"event_id\\": event_id, \\"datetime\\": event_dt.isoformat(), \\"timezone\\": tz } def list_events(self): return list(self.events.values()) def get_event_in_utc(self, event_id: str) -> dict: if event_id not in self.events: raise KeyError(f\\"Event with ID {event_id} does not exist\\") event = self.events[event_id] event_dt = datetime.fromisoformat(event[\\"datetime\\"]).astimezone(ZoneInfo(\\"UTC\\")) return { \\"event_id\\": event_id, \\"datetime\\": event_dt.isoformat(), \\"timezone\\": \\"UTC\\" }"},{"question":"**Coding Assessment Question** # Objective Implement a function to perform various operations on bytes objects, demonstrating your understanding of bytes manipulation in Python. # Task Write a Python function `manipulate_bytes_operations` that takes a list of operations to perform on bytes objects and produces the expected outputs. The function should handle the following operations using bytes manipulation functions: 1. **Create Bytes** - Create a new bytes object from a provided string. 2. **Concatenate Bytes** - Concatenate two bytes objects into one. 3. **Resize Bytes** - Resize an existing bytes object to a new size. 4. **Extract Substring** - Extract a substring from an existing bytes object. # Input - A list of tuples where each tuple represents an operation. The first element of the tuple is a string specifying the operation type, followed by the necessary arguments for that operation. # Output - A list of results corresponding to each operation. For the operations that modify or create a new bytes object, return the resulting bytes object. For operations that do not modify or create a new bytes object, return the appropriate result, such as a substring in bytes. # Example ```python operations = [ (\\"Create Bytes\\", \\"example_string\\"), (\\"Create Bytes\\", \\"another_bytes\\"), (\\"Concatenate Bytes\\", b\'example_string\', b\'another_bytes\'), (\\"Resize Bytes\\", b\'example_string_another_bytes\', 10), (\\"Extract Substring\\", b\'example_string_another_bytes\', 8, 14) ] # Expected output # [b\'example_string\', b\'another_bytes\', b\'example_stringanother_bytes\', b\'example_st\', b\'string\'] def manipulate_bytes_operations(operations): results = [] for operation in operations: if operation[0] == \\"Create Bytes\\": result = bytes(operation[1], \'utf-8\') elif operation[0] == \\"Concatenate Bytes\\": result = operation[1] + operation[2] elif operation[0] == \\"Resize Bytes\\": result = operation[1][:operation[2]] elif operation[0] == \\"Extract Substring\\": result = operation[1][operation[2]:operation[3]] else: raise ValueError(\\"Invalid operation\\") results.append(result) return results ``` # Constraints - Ensure the provided inputs are correctly checked and handled. - Efficiently handle bytes operations to prevent unnecessary performance overhead. # Performance Requirements - The function should execute in optimal time complexity considering the operations on bytes objects. Implement the `manipulate_bytes_operations` function to handle the specified operations on bytes objects as described above.","solution":"def manipulate_bytes_operations(operations): results = [] for operation in operations: if operation[0] == \\"Create Bytes\\": result = bytes(operation[1], \'utf-8\') elif operation[0] == \\"Concatenate Bytes\\": result = operation[1] + operation[2] elif operation[0] == \\"Resize Bytes\\": result = operation[1][:operation[2]] elif operation[0] == \\"Extract Substring\\": result = operation[1][operation[2]:operation[3]] else: raise ValueError(f\\"Invalid operation: {operation[0]}\\") results.append(result) return results"},{"question":"**Question: Email Parsing and Error Handling** In this exercise, you will write a function to parse a given raw email message and identify potential defects using the `email.errors` module. Write a function `parse_email` that takes a raw email string as input and returns a dictionary with two keys: - `headers`: a dictionary of the parsed headers. - `defects`: a list of defects identified during parsing. # Function Signature ```python def parse_email(raw_email: str) -> dict: ``` # Parameters - `raw_email` (str): A raw email message string to be parsed. # Returns - `dict`: A dictionary with two keys: - `headers` (dict): Parsed headers as key-value pairs. - `defects` (list): List of defects encountered during parsing. # Example ```python raw_email = From: example@example.com To: recipient@example.com Subject: Test Email This is a test email message. result = parse_email(raw_email) print(result) ``` # Expected Output ```python { \\"headers\\": { \\"From\\": \\"example@example.com\\", \\"To\\": \\"recipient@example.com\\", \\"Subject\\": \\"Test Email\\" }, \\"defects\\": [] } ``` # Constraints - Ensure that the function handles potential parsing errors by making use of the appropriate classes from the `email.errors` and `email` modules. - The function should handle missing or malformed headers gracefully and list out the defects encountered. - Assume that the input raw email string is always non-empty. **Notes:** - Use Python\'s standard `email` library to parse the email and extract headers. - Identify and report any defects listed in the `email.errors` module documentation provided. This exercise will test students\' ability to work with Python\'s `email` module, their understanding of how to handle parsing errors using exception and defect classes, and their skill in extracting and organizing data from a structured format.","solution":"from email import message_from_string from email.errors import MessageParseError def parse_email(raw_email: str) -> dict: email_message = message_from_string(raw_email) headers = {k: v for k, v in email_message.items()} defects = email_message.defects return { \\"headers\\": headers, \\"defects\\": [str(defect) for defect in defects] }"},{"question":"Based on the provided documentation about the Python Development Mode, create a Python program that adheres to the following specifications: 1. The program should read a specified file and count the number of lines, ensuring to close the file explicitly. 2. If the file cannot be found or opened, the program should log an appropriate error message. 3. The program should be designed to display warnings when run in Python Development Mode if there are any improper resource management practices. 4. Write a test function to verify the functionality of the program with different cases (e.g., valid file, missing file). **Input Format** - A string specifying the file path. **Output Format** - An integer representing the number of lines in the file or an error message if the file cannot be accessed. **Constraints** - The program should be able to handle large files efficiently. - The program should use context managers to handle file operations. **Performance Requirements** - Ensure that the program works efficiently with files up to a few GBs in size without running into memory issues. **Function Signature** ```python def count_lines(file_path: str) -> int: This function reads a file at the given file_path and returns the number of lines in the file. If the file cannot be opened, it should print an error message and return -1. pass def test_count_lines(): This function tests the count_lines function with different scenarios. pass ``` Example: ```python # Assuming \'example.txt\' contains 10 lines. print(count_lines(\'example.txt\')) # Output: 10 # Testing with a non-existent file. print(count_lines(\'non_existent.txt\')) # Output: An error message and return -1 ``` **Hints** 1. Use the `with` keyword for handling file operations to ensure that files are closed properly. 2. Use exception handling to manage errors related to file access.","solution":"import logging # Configure logging to display error messages logging.basicConfig(level=logging.ERROR, format=\'%(message)s\') def count_lines(file_path: str) -> int: This function reads a file at the given file_path and returns the number of lines in the file. If the file cannot be opened, it should print an error message and return -1. try: with open(file_path, \'r\') as file: return sum(1 for _ in file) except FileNotFoundError: logging.error(f\\"Error: The file \'{file_path}\' could not be found.\\") return -1 except IOError: logging.error(f\\"Error: The file \'{file_path}\' could not be opened.\\") return -1"},{"question":"# PyTorch MPS Environment Configuration and Usage Objective: You are required to set up and optimize a PyTorch environment on an Apple device using the provided MPS environment variables. Your task is to implement a function that configures these environment variables and runs a simple tensor operation under these configurations. Task: 1. **Configure Environment Variables:** - Write a function `configure_env_variables(high_watermark, low_watermark, fast_math, prefer_metal, enable_fallback)`: - `high_watermark` (float): High watermark ratio for MPS allocator. - `low_watermark` (float): Low watermark ratio for MPS allocator. - `fast_math` (bool): Enable fast math for Metal kernels. - `prefer_metal` (bool): Force using Metal kernels for specific operations. - `enable_fallback` (bool): Enable fallback operations to CPU when MPS does not support them. 2. **Tensor Operation:** - Create a simple function `tensor_operation()` that performs a matrix multiplication using PyTorch and returns the result. Ensure the tensor operation is executed on the MPS backend. 3. **Performance Logging:** - Enable verbose logging for the MPS allocator and profiling to verify the operations and memory usage. Input: - The function `configure_env_variables` will be called with the required parameters. Output: - The function `tensor_operation` should return the result of the matrix multiplication as a PyTorch tensor. Constraints: - Ensure that you do not exceed the recommended memory allocations (high watermark should be <= 1.7 and low watermark should be <= 1.4 if memory is unified, else 1.0). Example: ```python def configure_env_variables(high_watermark, low_watermark, fast_math, prefer_metal, enable_fallback): import os import torch # Set environment variables os.environ[\'PYTORCH_MPS_HIGH_WATERMARK_RATIO\'] = str(high_watermark) os.environ[\'PYTORCH_MPS_LOW_WATERMARK_RATIO\'] = str(low_watermark) os.environ[\'PYTORCH_MPS_FAST_MATH\'] = \'1\' if fast_math else \'0\' os.environ[\'PYTORCH_MPS_PREFER_METAL\'] = \'1\' if prefer_metal else \'0\' os.environ[\'PYTORCH_ENABLE_MPS_FALLBACK\'] = \'1\' if enable_fallback else \'0\' os.environ[\'PYTORCH_DEBUG_MPS_ALLOCATOR\'] = \'1\' os.environ[\'PYTORCH_MPS_LOG_PROFILE_INFO\'] = \'1\' os.environ[\'PYTORCH_MPS_TRACE_SIGNPOSTS\'] = \'1\' def tensor_operation(): import torch # Ensure the tensor operation runs on MPS backend if torch.backends.mps.is_available(): device = torch.device(\\"mps\\") else: raise RuntimeError(\\"MPS backend is not available.\\") # Perform a simple matrix multiplication a = torch.rand((2, 2), device=device) b = torch.rand((2, 2), device=device) result = torch.matmul(a, b) return result ``` Ensure your code works as expected and demonstrate your implementation by running a sample operation.","solution":"import os import torch def configure_env_variables(high_watermark, low_watermark, fast_math, prefer_metal, enable_fallback): Configures necessary environment variables for PyTorch MPS backend on Apple devices. Args: high_watermark (float): High watermark ratio for MPS allocator. low_watermark (float): Low watermark ratio for MPS allocator. fast_math (bool): Enable fast math for Metal kernels. prefer_metal (bool): Force using Metal kernels for specific operations. enable_fallback (bool): Enable fallback operations to CPU when MPS does not support them. # Set environment variables os.environ[\'PYTORCH_MPS_HIGH_WATERMARK_RATIO\'] = str(high_watermark) os.environ[\'PYTORCH_MPS_LOW_WATERMARK_RATIO\'] = str(low_watermark) os.environ[\'PYTORCH_MPS_FAST_MATH\'] = \'1\' if fast_math else \'0\' os.environ[\'PYTORCH_MPS_PREFER_METAL\'] = \'1\' if prefer_metal else \'0\' os.environ[\'PYTORCH_ENABLE_MPS_FALLBACK\'] = \'1\' if enable_fallback else \'0\' os.environ[\'PYTORCH_DEBUG_MPS_ALLOCATOR\'] = \'1\' os.environ[\'PYTORCH_MPS_LOG_PROFILE_INFO\'] = \'1\' os.environ[\'PYTORCH_MPS_TRACE_SIGNPOSTS\'] = \'1\' def tensor_operation(): Performs a matrix multiplication operation using PyTorch on the MPS backend. Returns: torch.Tensor: The result of the matrix multiplication. # Ensure the tensor operation runs on MPS backend if torch.backends.mps.is_available(): device = torch.device(\\"mps\\") else: raise RuntimeError(\\"MPS backend is not available.\\") # Perform a simple matrix multiplication a = torch.rand((2, 2), device=device) b = torch.rand((2, 2), device=device) result = torch.matmul(a, b) return result"},{"question":"Objective Implement a function that reads lines from a given list of files, processes these lines by trimming whitespace and converting to uppercase, and writes the processed lines back to the same files in a reversed order. Function Signature ```python def process_files(file_names: list[str]) -> None: pass ``` Input - `file_names`: A list of filenames (strings) where each file contains text lines. Output - The function should not return anything. It should modify the content of the files in-place. Constraints - Each file should contain lines of text. - The function should handle file reading and writing efficiently. - The function should use the `fileinput` module to read and write lines. - Ensure that the function handles errors gracefully, printing a message if a file cannot be processed. Example Suppose we have the following files with their contents: File `file1.txt`: ``` hello world ``` File `file2.txt`: ``` foo bar baz ``` Calling `process_files([\'file1.txt\', \'file2.txt\'])` should modify the files as follows: File `file1.txt` (modified): ``` WORLD HELLO ``` File `file2.txt` (modified): ``` BAZ BAR FOO ``` Requirements 1. Read lines from each file while iterating over them. 2. For each line, trim whitespace and convert it to uppercase. 3. Write the processed lines back to the files in reversed order. 4. Use in-place editing, so that the original files are modified directly. 5. Handle exceptions gracefully, providing an informative message if there is an error. Use the `fileinput` module effectively to accomplish the task, demonstrating your understanding of its capabilities and behavior.","solution":"import fileinput import os def process_files(file_names: list[str]) -> None: Processes each line of the given files by trimming whitespace, converting to uppercase, and writing back in reversed order. for file_name in file_names: try: # Read lines from the file and process them lines = [] with open(file_name, \'r\') as file: lines = [line.strip().upper() for line in file] # Reverse the lines lines.reverse() # Write the lines back to the file with open(file_name, \'w\') as file: for line in lines: file.write(line + \'n\') except Exception as e: print(f\\"Error processing file {file_name}: {e}\\")"},{"question":"Objective: You are required to implement a function using the pandas library that processes a given dataset and returns a specific output based on various operations including conditional filtering, aggregation, and pivoting. Problem Statement: Implement a function `analyze_sales_data(file_path: str) -> pd.DataFrame` that reads sales data from a CSV file, processes it, and returns a summary DataFrame. Function Signature: ```python import pandas as pd def analyze_sales_data(file_path: str) -> pd.DataFrame: pass ``` Input: - `file_path` (str): Path to a CSV file containing sales data. The CSV file has the following columns: - `\'Date\'`: The date of the sale in the format \'YYYY-MM-DD\'. - `\'Region\'`: The region where the sale was made. - `\'Product\'`: The product that was sold. - `\'Quantity\'`: The quantity of the product sold. - `\'Price\'`: The price per unit of the product sold. Output: - The function should return a pandas DataFrame that provides a summary of total sales (`\'Total Sales\'` = `\'Quantity\'` * `\'Price\'`) and average unit price (`\'Average Price\'` = `\'Total Sales\'` / `\'Quantity\'`) for each combination of `\'Region\'` and `\'Product\'`. Requirements: 1. Read the data from the provided CSV file. 2. Add a new column `\'Total Sales\'` calculated as `\'Quantity\'` * `\'Price\'`. 3. Group the data by `\'Region\'` and `\'Product\'` and aggregate to find: - The sum of `\'Total Sales\'` for each region-product combination. - The mean of `\'Price\'` for each region-product combination as `\'Average Price\'`. 4. Return the aggregated DataFrame, with each combination of `\'Region\'` and `\'Product\'`, and their respective `\'Total Sales\'` and `\'Average Price\'`. Example: Suppose the CSV file contains the following data: ```csv Date,Region,Product,Quantity,Price 2023-01-01,North,Widget,10,5.0 2023-01-02,North,Gadget,5,7.0 2023-01-03,South,Widget,15,5.5 2023-01-01,North,Widget,7,5.2 2023-01-04,South,Gadget,3,7.5 ``` The function `analyze_sales_data` would return the following DataFrame: ``` Total Sales Average Price Region Product North Gadget 35.0 7.0 Widget 86.4 5.1 South Gadget 22.5 7.5 Widget 82.5 5.5 ``` Constraints: - The input file_path will always point to a valid CSV file following the described format. - The input data will not contain any missing values.","solution":"import pandas as pd def analyze_sales_data(file_path: str) -> pd.DataFrame: # Read the data from CSV file df = pd.read_csv(file_path) # Add a new column \'Total Sales\' calculated as \'Quantity\' * \'Price\' df[\'Total Sales\'] = df[\'Quantity\'] * df[\'Price\'] # Group the data by \'Region\' and \'Product\' and perform the required aggregations result_df = df.groupby([\'Region\', \'Product\']).agg( Total_Sales=(\'Total Sales\', \'sum\'), Average_Price=(\'Price\', \'mean\') ) # Rename columns to match the required output result_df.columns = [\'Total Sales\', \'Average Price\'] return result_df"},{"question":"# Seaborn Coding Assessment Question You have been provided with the `penguins` dataset which includes measurements for three penguin species. Your task is to create various visualizations using the seaborn `so.Plot` and `so.Jitter` functionalities. Objective: Write a Python function `plot_jittered_penguins` that takes in three parameters: 1. `data` (a DataFrame): The input dataset (e.g., `penguins`). 2. `jitter_param` (a dictionary): Specifies the jitter behavior with possible keys: `width`, `x`, and `y`. The values should specify the amount of jitter to be applied. 3. `orientation` (a string): Either `\\"horizontal\\"` or `\\"vertical\\"`, which dictates the orientation of the plot. Based on these inputs, the function should: - Create a jittered dot plot for the penguins dataset, plotting `species` against `body_mass_g`. - Apply the jitter parameters specified in `jitter_param`. - Adjust the jitter orientation according to the `orientation` parameter. Constraints: - If `jitter_param` is empty, apply the default jitter. - If `orientation` is `\\"horizontal\\"`, plot `species` on the x-axis and `body_mass_g` on the y-axis. - If `orientation` is `\\"vertical\\"`, plot `body_mass_g` on the x-axis and `species` on the y-axis. - Raise a ValueError if an unsupported orientation is provided. Function Signature: ```python def plot_jittered_penguins(data, jitter_param, orientation): # Your implementation here pass ``` Example Usage: ```python import seaborn.objects as so from seaborn import load_dataset # Load the dataset penguins = load_dataset(\\"penguins\\") # Example 1: Default jitter with horizontal orientation plot_jittered_penguins(penguins, {}, \\"horizontal\\") # Example 2: Width jitter with vertical orientation plot_jittered_penguins(penguins, {\\"width\\": 0.5}, \\"vertical\\") # Example 3: Custom x and y jitter with horizontal orientation plot_jittered_penguins(penguins, {\\"x\\": 100, \\"y\\": 5}, \\"horizontal\\") ``` An example implementation may look like: ```python def plot_jittered_penguins(data, jitter_param={}, orientation=\\"horizontal\\"): import seaborn.objects as so if orientation == \\"horizontal\\": plot = so.Plot(data, x=\\"species\\", y=\\"body_mass_g\\") elif orientation == \\"vertical\\": plot = so.Plot(data, x=\\"body_mass_g\\", y=\\"species\\") else: raise ValueError(\\"Unsupported orientation provided\\") # Construct jitter based on parameters if \\"width\\" in jitter_param: jitter = so.Jitter(width=jitter_param[\\"width\\"]) elif \\"x\\" in jitter_param or \\"y\\" in jitter_param: jitter = so.Jitter(x=jitter_param.get(\\"x\\", 0), y=jitter_param.get(\\"y\\", 0)) else: jitter = so.Jitter() # Default jitter plot.add(so.Dots(), jitter) return plot.show() ``` Ensure your solution employs the mentioned Seaborn methods and makes proper use of the provided parameters.","solution":"def plot_jittered_penguins(data, jitter_param={}, orientation=\\"horizontal\\"): import seaborn.objects as so if orientation == \\"horizontal\\": plot = so.Plot(data, x=\\"species\\", y=\\"body_mass_g\\") elif orientation == \\"vertical\\": plot = so.Plot(data, x=\\"body_mass_g\\", y=\\"species\\") else: raise ValueError(\\"Unsupported orientation provided\\") # Construct jitter based on parameters if \\"width\\" in jitter_param: jitter = so.Jitter(width=jitter_param[\\"width\\"]) elif \\"x\\" in jitter_param or \\"y\\" in jitter_param: jitter = so.Jitter(x=jitter_param.get(\\"x\\", 0), y=jitter_param.get(\\"y\\", 0)) else: jitter = so.Jitter() # Default jitter plot.add(so.Dots(), jitter) return plot.show()"},{"question":"Question: Advanced Exception Handling and Custom Exceptions in Python You are required to develop a robust string processing function that processes a list of commands on a given string. Along with handling standard exceptions, you should also create and raise custom exceptions when necessary. Function Signature ```python def process_commands(commands: List[str], initial_string: str) -> str: ``` Input - `commands` (List[str]): A list of string commands. Each command is in the format `operation:value` where `operation` could be: - `\\"add\\"`: Adds the value to the end of the current string. - `\\"remove\\"`: Removes the first occurrence of the value from the current string. - `\\"insert:index:value\\"`: Inserts the value at the given index in the current string. - `\\"replace:old_value:new_value\\"`: Replaces the first occurrence of the `old_value` with `new_value` in the current string. - `initial_string` (str): The initial string to process. Output - Returns the final string after processing all commands. Constraints - `commands` will only contain valid strings, in the formats specified. - If `remove` or `replace` operations are called with values that do not exist in the string, raise a custom `ValueNotFoundException`. - If `insert:index:value` is called with an invalid index (out of range for the current string), raise an `IndexError`. - Use exception chaining to provide detailed information when an exception is raised. Custom Exceptions You need to define the following custom exception: ```python class ValueNotFoundException(Exception): def __init__(self, message, value): super().__init__(message) self.value = value ``` Example ```python commands = [\\"add:world\\", \\"replace:world:there\\", \\"insert:5:!\\", \\"remove:there\\"] initial_string = \\"Hello\\" result = process_commands(commands, initial_string) print(result) # Output: \\"Hello!\\" ``` Explanation: 1. \\"add:world\\" changes \\"Hello\\" to \\"Helloworld\\". 2. \\"replace:world:there\\" changes \\"Helloworld\\" to \\"Hellothere\\". 3. \\"insert:5:!\\" changes \\"Hellothere\\" to \\"Hello!there\\". 4. \\"remove:there\\" changes \\"Hello!there\\" to \\"Hello!\\". In your solution, ensure that: - You handle and demonstrate custom exception creation and usage. - You raise appropriate exceptions for invalid operations. - You use exception chaining where necessary. Note: Include detailed comments in your code explaining key steps and exception handling mechanisms.","solution":"from typing import List class ValueNotFoundException(Exception): def __init__(self, message, value): super().__init__(message) self.value = value def process_commands(commands: List[str], initial_string: str) -> str: result_string = initial_string for command in commands: try: # Splitting command to identify operation and values parts = command.split(\':\') operation = parts[0] if operation == \\"add\\": value = parts[1] result_string += value elif operation == \\"remove\\": value = parts[1] if value not in result_string: raise ValueNotFoundException(\\"Value not found in string\\", value) result_string = result_string.replace(value, \'\', 1) elif operation == \\"insert\\": index = int(parts[1]) value = parts[2] if index < 0 or index > len(result_string): raise IndexError(f\\"Index {index} is out of range\\") result_string = result_string[:index] + value + result_string[index:] elif operation == \\"replace\\": old_value = parts[1] new_value = parts[2] if old_value not in result_string: raise ValueNotFoundException(\\"Value not found in string\\", old_value) result_string = result_string.replace(old_value, new_value, 1) except ValueNotFoundException as e: raise ValueNotFoundException(f\\"Failed to process command \'{command}\': {e}\\", e.value) from e except IndexError as e: raise IndexError(f\\"Failed to process command \'{command}\': {e}\\") from e return result_string"},{"question":"# Question: You are required to create a Python program that uses the \\"signal\\" module to handle asynchronous events. Specifically, the program should simulate a long-running calculation and be able to handle user interruptions or timeouts gracefully. The program should include functionality that demonstrates various aspects of signal handling, including custom handlers and timers. Requirements: 1. **Signal Handlers**: - Define a custom signal handler for `SIGINT` (triggered by `Ctrl+C`) that outputs a message and safely exits the program. - Define a custom signal handler for `SIGTERM` that cleans up resources and then exits the program. - Define a custom signal handler for `SIGALRM` that should handle timeout scenarios gracefully. 2. **Long-Running Calculation**: - Implement a function `long_running_calculation()` that performs a computation for a fixed amount of time, e.g., counting numbers up to a large value. 3. **Timeout and Cleanup**: - Use `signal.alarm()` to set a timeout of 10 seconds for the `long_running_calculation()`. If the calculation exceeds this time, it should trigger the `SIGALRM` handler and the program should handle this without crashing. 4. **Handling In Threaded Context**: - Ensure that the signal handlers and the long-running calculation are properly managed within a multithreaded context. Use proper synchronization techniques to ensure thread safety. Input: - No input is required. Output: - Printed messages indicating the reception of signals and the actions taken by the corresponding handlers. Example: ```python import signal import time import threading # Define signal handlers def handle_sigint(signum, frame): print(\\"Received SIGINT (Ctrl+C), exiting gracefully.\\") signal.alarm(0) # Disable any scheduled alarm exit(0) def handle_sigterm(signum, frame): print(\\"Received SIGTERM, cleaning up.\\") # Perform cleanup here exit(0) def handle_sigalrm(signum, frame): print(\\"Calculation timed out.\\") # Perform cleanup or retry logic here exit(0) # Configure signal handlers signal.signal(signal.SIGINT, handle_sigint) signal.signal(signal.SIGTERM, handle_sigterm) signal.signal(signal.SIGALRM, handle_sigalrm) def long_running_calculation(): for i in range(100000000): if i % 10000000 == 0: print(f\\"Working... {i}\\") time.sleep(0.05) def main_thread_function(): print(\\"Setting a 10-second timeout.\\") signal.alarm(10) # Set an alarm for 10 seconds long_running_calculation() # Main execution if __name__ == \\"__main__\\": calculation_thread = threading.Thread(target=main_thread_function) calculation_thread.start() calculation_thread.join() ``` Constraints: - Ensure that signal handlers are only set in the main thread, as required by the `signal` module. - The long-running calculation should be interruptible by `SIGINT`, `SIGTERM`, and `SIGALRM`. - Handle potential exceptions raised by signal handlers safely and ensure the program exits cleanly. Test your implementation thoroughly to ensure it handles each type of signal as expected.","solution":"import signal import time import threading import os # Define signal handlers def handle_sigint(signum, frame): print(\\"Received SIGINT (Ctrl+C), exiting gracefully.\\") signal.alarm(0) # Disable any scheduled alarm os._exit(0) def handle_sigterm(signum, frame): print(\\"Received SIGTERM, cleaning up.\\") # Perform cleanup here if needed os._exit(0) def handle_sigalrm(signum, frame): print(\\"Calculation timed out.\\") # Perform cleanup or retry logic here os._exit(0) # Configure signal handlers in the main thread def setup_signal_handlers(): signal.signal(signal.SIGINT, handle_sigint) signal.signal(signal.SIGTERM, handle_sigterm) signal.signal(signal.SIGALRM, handle_sigalrm) # Long-running calculation function def long_running_calculation(): for i in range(100000000): if i % 10000000 == 0: print(f\\"Working... {i}\\") time.sleep(0.05) def main_thread_function(): setup_signal_handlers() print(\\"Setting a 10-second timeout.\\") signal.alarm(10) # Set an alarm for 10 seconds long_running_calculation() # Main execution if __name__ == \\"__main__\\": calculation_thread = threading.Thread(target=main_thread_function) calculation_thread.start() calculation_thread.join()"},{"question":"**Question: Analyze and Optimize Pickled Data** You are provided with a pickled dataset stored in a file. Your task is to analyze its internal structure and optimize its storage using the `pickletools` module. Write a Python function that reads the pickle file, performs a symbolic disassembly, and then optimizes the pickle string to reduce its storage size. # Function Signature ```python def analyze_and_optimize_pickle(file_path: str) -> str: pass ``` # Input - `file_path` (str): The path to the pickle file that needs to be analyzed and optimized. # Output - Returns a new, optimized pickle string. # Constraints - The pickle file may contain complex nested structures. - You should ensure the optimized pickle remains functionally equivalent to the original. - You should use the `pickletools` module to perform the disassembly and optimization tasks. # Example Usage ```python # Assume \'data.pickle\' is a pickle file available in the current directory optimized_pickle = analyze_and_optimize_pickle(\'data.pickle\') print(optimized_pickle) ``` In this example, the function reads the pickle file \'data.pickle\', disassembles its content to show the opcodes, and returns an optimized pickle string that takes up less storage space. # Notes - You may assume that the provided pickle file is not malicious. - Emphasis should be placed on code readability and efficiency. - You can utilize `pickletools.dis()`, `pickletools.genops()`, and `pickletools.optimize()` as needed.","solution":"import pickle import pickletools def analyze_and_optimize_pickle(file_path: str) -> str: with open(file_path, \'rb\') as file: original_data = file.read() # Disassemble the pickle data for analysis pickletools.dis(original_data) # Optimize the pickle data optimized_data = pickletools.optimize(original_data) return optimized_data.decode(\'latin1\')"},{"question":"# Asynchronous Echo Server In this task, you will implement a basic asynchronous echo server using Python\'s `asyncio` package. This server should be capable of handling multiple clients concurrently, echoing back any received message to the client. **Instructions**: 1. Create a class `EchoServerProtocol` that implements the necessary asyncio protocol callbacks. 2. Implement the following methods in the `EchoServerProtocol` class: - `connection_made(transport)`: called when a connection is established. - `data_received(data)`: called when data is received from the client. 3. Utilize the following `asyncio` event loop methods to manage the server lifecycle: - `loop.create_server()`: to start the server. - `loop.run_until_complete()`: to run the server until completion. - `loop.run_forever()`: to keep the server running indefinitely. - `loop.close()`: to close the event loop. # Requirements - You should handle multiple clients concurrently. - When a client sends a message, the server should echo the message back. - Use `asyncio.get_running_loop()` to get the running event loop. # Implementation Guidelines 1. Define a top-level `main()` function to set up and run the server. This function should: - Create the server using `await loop.create_server()`. - Start the event loop using `loop.run_forever()`. 2. Ensure you handle the event loop and transport properly to manage connections and data reception. # Example Usage: Upon completion, your server should be able to handle the following: - Client 1 connects and sends \\"Hello\\". - Server echoes back \\"Hello\\" to Client 1. - Client 2 connects and sends \\"ECHO\\". - Server echoes back \\"ECHO\\" to Client 2. **Constraints**: - Python 3.10+ - Use asyncio for managing the event loop and protocols. ```python import asyncio class EchoServerProtocol(asyncio.Protocol): def connection_made(self, transport): # Implement connection made logic pass def data_received(self, data): # Implement data received logic pass async def main(): # Get the event loop and set up the server loop = asyncio.get_running_loop() server = await loop.create_server( lambda: EchoServerProtocol(), \'127.0.0.1\', 8888) async with server: await server.serve_forever() if __name__ == \\"__main__\\": asyncio.run(main()) ``` **Note**: You should adjust port numbers as needed and manage socket resources to avoid conflicts.","solution":"import asyncio class EchoServerProtocol(asyncio.Protocol): def __init__(self): self.transport = None def connection_made(self, transport): self.transport = transport print(f\\"Connection made with {transport.get_extra_info(\'peername\')}\\") def data_received(self, data): message = data.decode() print(f\\"Data received: {message}\\") self.transport.write(data) print(\\"Data sent back\\") async def main(): loop = asyncio.get_running_loop() server = await loop.create_server( lambda: EchoServerProtocol(), \'127.0.0.1\', 8888 ) async with server: await server.serve_forever() if __name__ == \\"__main__\\": asyncio.run(main())"},{"question":"You are given a task to implement a Python function that achieves the following using the data compression and archiving modules provided by Python: 1. Create a tar archive named `compressed_archive.tar.gz` that contains multiple files. 2. Each file should be compressed using the `bz2` module. 3. Write a function `create_compressed_archive(file_list: List[str]) -> None` that takes a list of filenames as input. The files will be archived together after compressing them with `bz2`. # Constraints: - All files in the `file_list` are guaranteed to exist and are accessible within the workspace. - The function should handle large files efficiently. - Use appropriate modules and techniques to ensure the archive is created correctly and efficiently. # Example: ```python import os # Assuming the following files are in the current working directory: # \'file1.txt\', \'file2.txt\', \'file3.txt\' file_list = [\'file1.txt\', \'file2.txt\', \'file3.txt\'] create_compressed_archive(file_list) # This should create a tar.gz archive named `compressed_archive.tar.gz` containing the bz2 compressed versions of the provided files. # Ensure the result file is in the current working directory. ``` # Implementation Details: - Use the `bz2` module to compress each file. - Use the `tarfile` module to create the tar archive and add the compressed files to it. - The final archive should be named `compressed_archive.tar.gz`. # Additional Requirements: 1. The function should not produce any side effects other than creating the specified archive. 2. The function should include proper error handling in case of issues during file reading, writing, or compression.","solution":"import bz2 import tarfile from typing import List def create_compressed_archive(file_list: List[str]) -> None: Compresses each file in the file_list using bz2 and then archives them into a single tar.gz file. The resulting archive is named `compressed_archive.tar.gz`. :param file_list: List of filenames to be compressed and archived. with tarfile.open(\'compressed_archive.tar.gz\', \'w:gz\') as tar: for filename in file_list: # Compress the file using bz2 with open(filename, \'rb\') as f_in: with bz2.open(filename + \'.bz2\', \'wb\') as f_out: f_out.writelines(f_in) # Add the compressed file to the tar archive tar.add(filename + \'.bz2\', arcname=filename + \'.bz2\')"},{"question":"**Coding Challenge: Advanced Subprocess Management With Timeout Handling** **Problem Statement:** You are tasked to create a function `run_commands_with_timeout` that takes a list of shell commands and a timeout value in seconds as input, runs each command in sequence, and returns a list of results. Each result should be a dictionary containing the command, its return code, standard output, and standard error. If a command takes longer than the specified timeout to complete, the function should terminate the process and capture the timeout event. **Function Signature:** ```python def run_commands_with_timeout(commands: list, timeout: int) -> list: pass ``` # Input: - `commands`: A list of strings, where each string is a shell command to be executed. - `timeout`: An integer representing the maximum number of seconds allowed for each command to run. # Output: - A list of dictionaries, each containing: - `command`: The command that was executed. - `returncode`: The return code of the command. - `stdout`: The standard output captured from the command. - `stderr`: The standard error captured from the command. - `timed_out`: A boolean indicating if the command timed out. # Constraints: - Each command should be executed sequentially. - If a command times out, it should be terminated and the remaining commands should still be executed. # Example Usage: ```python commands = [ \\"echo Hello, World!\\", \\"sleep 5\\", \\"ls -l\\" ] timeout = 2 results = run_commands_with_timeout(commands, timeout) for result in results: print(result) ``` # Expected Example Output: ```python [ { \\"command\\": \\"echo Hello, World!\\", \\"returncode\\": 0, \\"stdout\\": \\"Hello, World!n\\", \\"stderr\\": \\"\\", \\"timed_out\\": False }, { \\"command\\": \\"sleep 5\\", \\"returncode\\": None, \\"stdout\\": \\"\\", \\"stderr\\": \\"\\", \\"timed_out\\": True }, { \\"command\\": \\"ls -l\\", \\"returncode\\": 0, \\"stdout\\": \\"total 0n-rw-r--r-- 1 user user 0 date filenamen\\", \\"stderr\\": \\"\\", \\"timed_out\\": False } ] ``` # Considerations: - Use the `subprocess.run()` method or `subprocess.Popen` for running the shell commands. - Handle the `subprocess.TimeoutExpired` exception to identify and manage timeouts. - Ensure that the function captures both `stdout` and `stderr` for each command. - Make sure to terminate the process if it exceeds the timeout. # Notes: - The example commands and outputs are for illustration purposes, and actual results may vary based on the environment. - Ensure your solution is robust and handles edge cases, such as invalid commands or other unexpected exceptions.","solution":"import subprocess def run_commands_with_timeout(commands, timeout): results = [] for command in commands: try: completed_process = subprocess.run( command, shell=True, capture_output=True, text=True, timeout=timeout ) results.append({ \\"command\\": command, \\"returncode\\": completed_process.returncode, \\"stdout\\": completed_process.stdout, \\"stderr\\": completed_process.stderr, \\"timed_out\\": False, }) except subprocess.TimeoutExpired: results.append({ \\"command\\": command, \\"returncode\\": None, \\"stdout\\": \\"\\", \\"stderr\\": \\"\\", \\"timed_out\\": True, }) return results"},{"question":"**Title: Implement and Export a PyTorch Model with Mixed Static and Dynamic Values and Control Flow** Objective Implement a PyTorch neural network module that handles both static and dynamic values, and incorporates control flow mechanisms. Export the model using `torch.export.export` and verify the correctness of the exported graph. Problem Statement 1. Implement a PyTorch module named `DynamicStaticModel` with the following requirements: - It should have one trainable parameter. - It should take two inputs: a dynamic Tensor `x` and a static integer `static_val`. - The model should apply a dynamic dependent computation on `x` and use `static_val` to control the flow of execution. 2. Your model should perform the following operations: - If the sum of `x` is greater than `static_val`, return the sine of `x`. - Otherwise, return the cosine of `x`. 3. Export the model using `torch.export.export` with example inputs. Ensure that the example inputs demonstrate both branches of the control flow. 4. Print the code of the exported model to verify that the export correctly captures the control flow. Expected Input and Output Formats - **Function Name**: `dynamic_static_model_export` - **Input**: - `x`: A dynamic input Tensor (Example: `torch.randn(3, 3)`). - `static_val`: A static integer (Example: `0` or `1`). - **Output**: - A printed code of the exported model. Constraints - Use `torch.export.export` in non-strict mode. - Utilize the `torch.cond` function to handle dynamic control flow within the model. Implementation Skeleton ```python import torch import torch.fx class DynamicStaticModel(torch.nn.Module): def __init__(self): super(DynamicStaticModel, self).__init__() self.param = torch.nn.Parameter(torch.randn(3, 3)) def forward(self, x, static_val): # Use torch.cond to handle dynamic control flow return torch.cond( pred=x.sum() > static_val, true_fn=lambda x: x.sin(), false_fn=lambda x: x.cos(), operands=(x,) ) def dynamic_static_model_export(): model = DynamicStaticModel() example_input = (torch.randn(3, 3), 0) exported_model = torch.export.export(model, example_input, non_strict_mode=True) print(exported_model.graph_module.code) # Execute the export function dynamic_static_model_export() ``` Performance Requirements - Ensure that the exported model correctly captures the control flow based on the static and dynamic values provided.","solution":"import torch import torch.fx class DynamicStaticModel(torch.nn.Module): def __init__(self): super(DynamicStaticModel, self).__init__() self.param = torch.nn.Parameter(torch.randn(3, 3)) def forward(self, x, static_val): # Use torch.cond to handle dynamic control flow pred = x.sum() > static_val return torch.where(pred, x.sin(), x.cos()) def dynamic_static_model_export(): model = DynamicStaticModel() example_input = (torch.randn(3, 3), 0) exported_model = torch.fx.symbolic_trace(model) code = exported_model.code return code # Print the code of the exported model to verify if __name__ == \\"__main__\\": print(dynamic_static_model_export())"},{"question":"# Question: Comprehensive Data Compression and Archiving You are tasked with writing a Python function that compresses a directory and then decompresses it into a new location. This task will demonstrate your knowledge of using `gzip` and `tarfile` modules from the `python310` package. Function Signature ```python def compress_and_decompress_directory(input_dir: str, compressed_file: str, output_dir: str) -> None: pass ``` Input 1. `input_dir` (str): Path to the input directory that contains the files you need to compress. 2. `compressed_file` (str): Path to the compressed output file (should end with `.tar.gz`). 3. `output_dir` (str): Path to the directory where the decompressed contents should be extracted. Output - The function should not return anything, but it should create two outputs: - A compressed file using the `.tar.gz` format containing all the files from the `input_dir`. - An extracted directory `output_dir` containing all the files from the compressed file to their original state. Constraints - Handle exceptions where files in `input_dir` might not exist. - Ensure correct handling of file permissions and symlinks. - The solution should efficiently handle large files and directories. Example ```python import os input_dir = \\"testdir\\" compressed_file = \\"testdir_compressed.tar.gz\\" output_dir = \\"testdir_extracted\\" compress_and_decompress_directory(input_dir, compressed_file, output_dir) # After execution: # 1. The file \'testdir_compressed.tar.gz\' should exist. # 2. The directory \'testdir_extracted\' should exist and contain the same files as \'testdir\'. ``` Additional Requirements - Your solution should use the `gzip` module for compression and `tarfile` module for working with tar archives. - Provide comments in your code to explain each major step. - Demonstrate your function with a small test case (directory with a few text files). Good luck!","solution":"import os import tarfile import shutil def compress_and_decompress_directory(input_dir: str, compressed_file: str, output_dir: str) -> None: Compresses the input directory into a .tar.gz file and then decompresses it to the output directory. Args: input_dir (str): Path to the input directory to compress. compressed_file (str): Path to the output compressed file (should end with .tar.gz). output_dir (str): Path to the directory where decompressed contents should be extracted. # Ensure the input directory exists if not os.path.isdir(input_dir): raise FileNotFoundError(f\\"Input directory {input_dir} does not exist.\\") # Compress the input directory into a .tar.gz file with tarfile.open(compressed_file, \\"w:gz\\") as tar: tar.add(input_dir, arcname=os.path.basename(input_dir)) # Ensure the output directory exists if os.path.exists(output_dir): shutil.rmtree(output_dir) os.makedirs(output_dir) # Decompress the .tar.gz file into the output directory with tarfile.open(compressed_file, \\"r:gz\\") as tar: tar.extractall(path=output_dir)"},{"question":"Objective You will demonstrate your ability to use the seaborn library to create and customize plots, apply data transformations, and manipulate datasets for visualization purposes. Problem Statement You are provided with the `healthexp` dataset from seaborn. This dataset includes information on health expenditure over the years for different countries. Your task is to create a normalized line plot that shows the spending relative to the median value for each country. Detailed Requirements 1. **Load the Data:** - Use seaborn\'s `load_dataset` function to load the `healthexp` dataset. 2. **Create Normalized Line Plot:** - Create a line plot with `Year` on the x-axis and `Spending_USD` on the y-axis. - Differentiate each country by color. - Normalize the spending data so that each country\'s spending is shown relative to their median spending value over the years. - Label the y-axis as \\"Spending relative to median value.\\" 3. **Constraints:** - Use seaborn.objects for creating the plot. - Ensure the plot is clear and easy to interpret. 4. **Example Output:** Your plot should resemble the example provided, reflecting spending normalized relative to the median values: ```python import seaborn.objects as so from seaborn import load_dataset # Load dataset healthexp = load_dataset(\\"healthexp\\") # Create normalized plot ( so.Plot(healthexp, x=\\"Year\\", y=\\"Spending_USD\\", color=\\"Country\\") .add(so.Lines(), so.Norm(\\"median\\")) .label(y=\\"Spending relative to median value\\") ) ``` This will generate a normalized line plot where each line represents a country\'s health expenditure over the years, scaled relative to its median value. Input and Output Formats - **Input:** Dataset loaded within the script using seaborn. - **Output:** A matplotlib plot visualizing the normalized health expenditure. Additional Information Ensure your code is efficient and properly documented. Your submission will be evaluated based on correctness, clarity, and adherence to the requirements.","solution":"import seaborn.objects as so from seaborn import load_dataset def create_normalized_line_plot(): # Load dataset healthexp = load_dataset(\\"healthexp\\") # Create normalized plot ( so.Plot(healthexp, x=\\"Year\\", y=\\"Spending_USD\\", color=\\"Country\\") .add(so.Lines(), so.Norm(\\"median\\")) .label(y=\\"Spending relative to median value\\") ).show()"},{"question":"**Question: Implement and test a custom site module configuration** You are tasked with implementing a Python function that customizes the `site` module\'s behavior. Specifically, you need to create a function that receives a list of custom site-package directories and ensures they are properly added to the `sys.path`. Additionally, ensure that an import statement within a custom `.pth` file is executed if it exists. # Requirements 1. **Function Signature**: ```python def configure_site(custom_paths: list[str]) -> None: ``` 2. **Parameters**: - `custom_paths`: A list of strings, each representing a custom site-package directory path to be added to `sys.path`. 3. **Functionality**: - Iterate over the provided `custom_paths`. For each path: - If the path is a directory, add it to `sys.path`. - Check if a `.pth` file exists within the directory. - If a `.pth` file is found, read its content and if any line starts with `import`, execute it. - Ensure no duplicate paths are added to `sys.path`. - Handle any exceptions silently, similar to the behavior described for importing `sitecustomize` and `usercustomize` (e.g., ignore ImportError). 4. **Constraints**: - You may use built-in modules like `os`, `sys`, and `importlib`. - You should handle missing directories or files gracefully, without raising exceptions. - The `.pth` files may contain comments (lines starting with `#`) and blank lines, which should be skipped. 5. **Example**: Suppose there is a directory `/example/site-packages` containing a `custom.pth` file with the content: ```plaintext # Custom site configuration import some_custom_module ``` - After calling `configure_site([\'/example/site-packages\'])`, the `sys.path` should include `/example/site-packages`, and `some_custom_module` should be imported if it exists. # Testing Write tests to verify the following: - Directories are correctly added to `sys.path`. - `.pth` files are processed and import statements are executed. - Duplicate paths are not added. - Non-existing directories are ignored gracefully. ```python # Example Test Cases import os import sys import tempfile import unittest class TestConfigureSite(unittest.TestCase): def test_add_custom_paths(self): temp_dir = tempfile.TemporaryDirectory() temp_path = temp_dir.name with open(os.path.join(temp_path, \'test.pth\'), \'w\') as f: f.write(\\"import os\\") initial_sys_path = list(sys.path) configure_site([temp_path]) self.assertIn(temp_path, sys.path) self.assertEqual(len(sys.path), len(initial_sys_path) + 1) temp_dir.cleanup() def test_ignore_non_existing_directories(self): initial_sys_path = list(sys.path) configure_site([\'/non/existing/path\']) self.assertEqual(sys.path, initial_sys_path) def test_no_duplicate_paths(self): temp_dir = tempfile.TemporaryDirectory() temp_path = temp_dir.name with open(os.path.join(temp_path, \'test.pth\'), \'w\') as f: f.write(\\"import os\\") configure_site([temp_path, temp_path]) self.assertEqual(sys.path.count(temp_path), 1) temp_dir.cleanup() if __name__ == \\"__main__\\": unittest.main() ```","solution":"import os import sys import runpy def configure_site(custom_paths: list[str]) -> None: Adds custom site-package directories to sys.path and executes any import statements in .pth files within those directories. for custom_path in custom_paths: if os.path.isdir(custom_path): if custom_path not in sys.path: sys.path.insert(0, custom_path) pth_files = [f for f in os.listdir(custom_path) if f.endswith(\'.pth\')] for pth_file in pth_files: pth_path = os.path.join(custom_path, pth_file) try: with open(pth_path, \'r\') as file: for line in file: line = line.strip() if line and not line.startswith(\'#\') and line.startswith(\\"import\\"): exec(line) except Exception as e: pass # Silently ignore any exceptions"},{"question":"**Objective**: Implement a Python function that summarises and collapses IPv4 addresses into the minimal representation of network ranges. **Problem Statement**: Create a function `summarize_and_collapse_ipv4(address_list: list) -> list` that takes a list of IPv4 addresses (in string format) and returns the minimal list of network ranges that summarises and collapses the input addresses. # Function Signature: ```python import ipaddress def summarize_and_collapse_ipv4(address_list: list) -> list: # Your code here ``` # Input: - `address_list` of type `list`: A list of IPv4 addresses as strings. # Output: - Returns a list of `str`, each representing a minimal network range that summarises and collapses the given IP addresses. # Constraints: 1. The input list can contain up to `1000` IPv4 addresses. 2. All input addresses are guaranteed to be valid IPv4 addresses. # Example: Input: ```python address_list = [\'192.168.0.1\', \'192.168.0.2\', \'192.168.1.1\', \'192.168.1.2\', \'10.0.0.1\', \'10.0.0.2\'] ``` Output: ```python [\'192.168.0.1/32\', \'192.168.0.2/31\', \'192.168.1.1/32\', \'192.168.1.2/32\', \'10.0.0.1/31\'] ``` # Requirements: 1. Utilize `ipaddress` module functionalities such as `ip_network`, `summarize_address_range`, and `collapse_addresses`. 2. Ensure the function handles edge cases such as consecutive and non-consecutive IPs accurately. 3. The output networks should be in their minimal form, using the shortest prefix length possible. # Evaluation Criteria: - **Correctness**: The function must correctly summarise and collapse the given IP addresses into the smallest number of network ranges. - **Efficiency**: The function should handle input efficiently within the provided constraints. - **Code Quality**: Code should be well-structured, using appropriate variable names and following Python coding conventions. # Note: This function will be tested using multiple test cases to ensure it covers various scenarios and edge cases.","solution":"import ipaddress def summarize_and_collapse_ipv4(address_list: list) -> list: Summarizes and collapses a list of IPv4 addresses into minimal network ranges. Args: address_list (list): A list of IPv4 addresses as strings. Returns: list: A list of minimal network ranges that summarize and collapse the input addresses. if not address_list: return [] # Convert the list of address strings to IPv4Address objects ip_objects = [ipaddress.ip_address(ip) for ip in address_list] # Use the collapse_addresses function to summarize and collapse the address list collapsed = ipaddress.collapse_addresses(ip_objects) # Convert the collapsed network ranges to strings and return the result return [str(network) for network in collapsed]"},{"question":"You are tasked with testing a module that performs various network operations. The module contains a class `NetworkManager` with the following methods: ```python class NetworkManager: def connect(self, endpoint): # Simulate a network connection to an endpoint pass async def async_execute(self, command): # Simulate executing an asynchronous network command pass def close(self): # Simulate closing the network connection pass def perform_network_operations(manager, endpoint, command): manager.connect(endpoint) asyncio.run(manager.async_execute(command)) manager.close() ``` Your objective is to create a mock for the `NetworkManager` class and write unit tests for the `perform_network_operations` function. The function should be tested to ensure that it connects to the correct endpoint, executes the correct command asynchronously, and closes the connection. # Requirements: 1. **Mock Creation:** - Create a mock for the `NetworkManager` class. 2. **Method Invocation:** - Verify that the `connect` method is called with the correct endpoint. - Verify that the `async_execute` method is called with the correct command. - Verify that the `close` method is called. 3. **Handling Asynchronous Methods:** - Configure the mock for the `async_execute` method to handle asynchronous execution. # Input: - The function `perform_network_operations(manager, endpoint, command)`: - `manager`: An instance of `NetworkManager`. - `endpoint`: A string representing the network endpoint. - `command`: A string representing the network command. # Output: - The test should pass without raising any exceptions if all verifications are correct. # Constraints: - Use the `unittest.mock` module for creating and configuring mocks. - The test should handle asynchronous methods properly. # Example Usage: ```python network_manager = NetworkManager() perform_network_operations(network_manager, \\"http://example.com\\", \\"GET /\\") ``` # Test Implementation: Implement a test function using `unittest` and `unittest.mock` that covers the requirements mentioned above. ```python import unittest from unittest.mock import MagicMock, AsyncMock, patch import asyncio class TestNetworkOperations(unittest.TestCase): @patch(\'__main__.NetworkManager\') def test_perform_network_operations(self, MockNetworkManager): # Create an instance of the mock mock_manager = MockNetworkManager.return_value # Setting up the async mock for asynchronous method mock_manager.async_execute = AsyncMock() # Perform network operations endpoint = \\"http://example.com\\" command = \\"GET /\\" perform_network_operations(mock_manager, endpoint, command) # Verify that connect is called with correct endpoint mock_manager.connect.assert_called_once_with(endpoint) # Verify that async_execute is called with correct command asynchronously mock_manager.async_execute.assert_awaited_once_with(command) # Verify that close is called once mock_manager.close.assert_called_once() if __name__ == \'__main__\': unittest.main() ```","solution":"import asyncio class NetworkManager: def connect(self, endpoint): # Simulate a network connection to an endpoint pass async def async_execute(self, command): # Simulate executing an asynchronous network command pass def close(self): # Simulate closing the network connection pass def perform_network_operations(manager, endpoint, command): manager.connect(endpoint) asyncio.run(manager.async_execute(command)) manager.close()"},{"question":"# Complex JSON Manipulation **Objective**: Write a Python function that demonstrates the process of encoding and decoding JSON, including handling custom Python objects and using custom serialization and deserialization logic. **Problem Statement**: You need to implement a system to handle a custom Python class `ComplexNumber` using JSON. This class stores a complex number and should be able to serialize to and deserialize from JSON. Your task is to implement two main functions: 1. `encode_complex_numbers(data)`: Encodes Python data containing instances of `ComplexNumber` including nested structures into a JSON string. 2. `decode_complex_numbers(json_data)`: Decodes a JSON string into Python data, reconstructing instances of `ComplexNumber`. # Class Definition: ```python class ComplexNumber: def __init__(self, real, imag): self.real = real self.imag = imag def __eq__(self, other): return isinstance(other, ComplexNumber) and self.real == other.real and self.imag == other.imag def __repr__(self): return f\\"ComplexNumber({self.real}, {self.imag})\\" ``` # Function Specifications: `encode_complex_numbers(data: Any) -> str` - **Input**: A Python object (which may contain instances of `ComplexNumber`). - **Output**: A JSON string that represents the input Python object. - **Requirements**: - Use the `json.dumps` function. - Custom serialize `ComplexNumber` instances into a dictionary with keys `__complex__`, `real`, and `imag`. - Handle nested structures (e.g., lists or dictionaries containing `ComplexNumber`). `decode_complex_numbers(json_data: str) -> Any` - **Input**: A JSON string. - **Output**: A Python object reconstructed from the JSON string, with appropriate `ComplexNumber` instances. - **Requirements**: - Use the `json.loads` function. - Custom deserialize JSON objects with keys `__complex__`, `real`, and `imag` back into `ComplexNumber` instances. - Handle nested structures (e.g., lists or dictionaries containing `ComplexNumber`). # Constraints: - Ensure that your solution works correctly for deeply nested structures. - The JSON string should follow standard JSON conventions. - You must use the custom serialization and deserialization processes for `ComplexNumber`. # Example Usage: ```python # Given predefined ComplexNumber class complex_data = { \'number1\': ComplexNumber(1, 2), \'number2\': ComplexNumber(3, 4), \'nested_list\': [1, ComplexNumber(5, 6), [ComplexNumber(7, 8)]] } # Encoding json_str = encode_complex_numbers(complex_data) print(json_str) # Expected output: JSON string with complex numbers encoded # Decoding decoded_data = decode_complex_numbers(json_str) print(decoded_data) # Expected output: Python object with ComplexNumber instances assert decoded_data == complex_data # Should be True ``` # Submission: Submit your solution with the `ComplexNumber` class and the required functions `encode_complex_numbers` and `decode_complex_numbers`.","solution":"import json class ComplexNumber: def __init__(self, real, imag): self.real = real self.imag = imag def __eq__(self, other): return isinstance(other, ComplexNumber) and self.real == other.real and self.imag == other.imag def __repr__(self): return f\\"ComplexNumber({self.real}, {self.imag})\\" def complex_number_encoder(obj): if isinstance(obj, ComplexNumber): return {\\"__complex__\\": True, \\"real\\": obj.real, \\"imag\\": obj.imag} raise TypeError(f\\"Object of type \'{obj.__class__.__name__}\' is not JSON serializable\\") def complex_number_decoder(dct): if \\"__complex__\\" in dct: return ComplexNumber(dct[\\"real\\"], dct[\\"imag\\"]) return dct def encode_complex_numbers(data): Encodes Python data containing instances of ComplexNumber into a JSON string. return json.dumps(data, default=complex_number_encoder) def decode_complex_numbers(json_data): Decodes a JSON string into Python data, reconstructing instances of ComplexNumber. return json.loads(json_data, object_hook=complex_number_decoder)"},{"question":"Objective: Implement a Python class `SecureFileHash` using the `hashlib` module to manage the hashing of files securely. The class should support different hashing algorithms, keyed hashing, and the ability to derive keys from passwords using PBKDF2-HMAC. Class: `SecureFileHash` 1. **Constructor**: - `__init__(self, algorithm=\'sha256\', key=None, salt=None)`: - `algorithm` (str): Name of the hashing algorithm to use (default is `\'sha256\'`). - `key` (bytes, optional): Key for keyed hashing. - `salt` (bytes, optional): Salt for randomized hashing. 2. **Methods**: - `update_file(self, file_path)`: Updates the hash object with the contents of the file specified by `file_path`. - `get_hexdigest(self)`: Returns the hexadecimal digest of the data. - `reset(self)`: Resets the hash object to its initial state. - `derive_key(self, password, salt, iterations=100000, dklen=None)`: - Derives a key from the provided password and salt using PBKDF2-HMAC. - `password` (bytes): Password to derive the key from. - `salt` (bytes): Salt used in the key derivation. - `iterations` (int): Number of iterations (default is 100000). - `dklen` (int, optional): Length of the derived key. If `None`, defaults to the digest size of the algorithm. Constraints: - `algorithm` must be one of the algorithms available in `hashlib.algorithms_guaranteed`. - `key` must be of appropriate length as per the chosen algorithm, if provided. - `password`, `salt`, and file contents should be treated as bytes. Usage Example: ```python # Example usage file_hasher = SecureFileHash(algorithm=\'sha256\') file_hasher.update_file(\'example.txt\') digest = file_hasher.get_hexdigest() print(digest) # Reset the hash object and use a different file file_hasher.reset() file_hasher.update_file(\'another_example.txt\') digest = file_hasher.get_hexdigest() print(digest) # Key derivation example password = b\'mysecretpassword\' salt = os.urandom(16) derived_key = file_hasher.derive_key(password, salt) print(derived_key) ``` Note: - You can assume the files to be hashed are not exceedingly large for this exercise. - Use exception handling to manage any file I/O errors or invalid parameters.","solution":"import hashlib import hmac class SecureFileHash: def __init__(self, algorithm=\'sha256\', key=None, salt=None): if algorithm not in hashlib.algorithms_guaranteed: raise ValueError(f\\"Invalid algorithm. Choose from {hashlib.algorithms_guaranteed}\\") self.algorithm = algorithm self.key = key self.salt = salt self.hasher = self._initialize_hasher() def _initialize_hasher(self): if self.key: return hmac.new(self.key, digestmod=self.algorithm) else: return hashlib.new(self.algorithm) def update_file(self, file_path): try: with open(file_path, \'rb\') as file: for chunk in iter(lambda: file.read(4096), b\'\'): self.hasher.update(chunk) except IOError as e: raise e def get_hexdigest(self): return self.hasher.hexdigest() def reset(self): self.hasher = self._initialize_hasher() def derive_key(self, password, salt, iterations=100000, dklen=None): return hashlib.pbkdf2_hmac(self.algorithm, password, salt, iterations, dklen)"},{"question":"You are required to demonstrate your understanding of iterator objects in Python by implementing custom sequence and callable iterators. You will use C-API functions provided in `python310` package for this task. Additionally, you will perform checks on these iterators using the appropriate functions. # Instructions 1. **Implement a Custom Sequence Iterator:** - Create a custom sequence iterator for a given list. - Ensure the iterator works correctly through manual iteration over the sequence items. 2. **Implement a Custom Callable Iterator:** - Create a custom callable iterator that simulates reading lines from a stream of input until a sentinel value is encountered. - Demonstrate this iterator by manually iterating and printing values until the sentinel value is reached. # Tasks and Function Definitions 1. **Sequence Iterator Implementation** - **Function:** `create_sequence_iterator(seq: list) -> object` - **Parameters:** - `seq` (list): A Python list for which the iterator needs to be created. - **Returns:** - Custom sequence iterator object. - **Function:** `is_seq_iterator(it: object) -> bool` - **Parameters:** - `it` (object): The object that needs to be checked if it is sequence iterator. - **Returns:** - `True` if the object is a sequence iterator, `False` otherwise. 2. **Callable Iterator Implementation** - **Function:** `create_callable_iterator(callable_obj, sentinel) -> object` - **Parameters:** - `callable_obj` (callable): A function that returns items on each call. - `sentinel` (any primitive type): The sentinel value which ends the iteration. - **Returns:** - Custom callable iterator object. - **Function:** `is_callable_iterator(it: object) -> bool` - **Parameters:** - `it` (object): The object that needs to be checked if it is a callable iterator. - **Returns:** - `True` if the object is a callable iterator, `False` otherwise. # Constraints and Performance Requirements - You may assume that the sequences used for iteration are not excessively large (reasonable for typical in-memory list processing). - It is assumed that the callable object takes no parameters and is efficient. - Aim for clarity and maintainability in your code, ensuring it is well commented to explain the logic used. **Note:** The usage of `PySeqIter_Check`, `PySeqIter_New`, `PyCallIter_Check`, and `PyCallIter_New` is specific and pertains strictly to C-API calls as described in `python310` documentation. However, for simplifying this task in pure Python, you should simulate similar behavior within your accessible Python codebase. # Example ```python # Example usage of your implemented functions seq = [1, 2, 3, 4, 5] seq_iter = create_sequence_iterator(seq) print(is_seq_iterator(seq_iter)) # Expected output: True # Assuming \'callable_obj\' and \'sentinel\' have been defined. callable_iter = create_callable_iterator(callable_obj, sentinel) print(is_callable_iterator(callable_iter)) # Expected output: True ```","solution":"class CustomSequenceIterator: def __init__(self, seq): self.seq = seq self.index = 0 def __iter__(self): return self def __next__(self): if self.index < len(self.seq): result = self.seq[self.index] self.index += 1 return result else: raise StopIteration def create_sequence_iterator(seq: list) -> object: return CustomSequenceIterator(seq) def is_seq_iterator(it: object) -> bool: return isinstance(it, CustomSequenceIterator) class CustomCallableIterator: def __init__(self, callable_obj, sentinel): self.callable_obj = callable_obj self.sentinel = sentinel def __iter__(self): return self def __next__(self): result = self.callable_obj() if result == self.sentinel: raise StopIteration return result def create_callable_iterator(callable_obj, sentinel) -> object: return CustomCallableIterator(callable_obj, sentinel) def is_callable_iterator(it: object) -> bool: return isinstance(it, CustomCallableIterator)"},{"question":"# Problem: Interactive Python File Descriptor Check Implement a Python function `is_interactive_file(fp, filename)` which checks if a given file stream `fp` is deemed interactive by emulating the behavior of the `Py_FdIsInteractive(FILE *fp, const char *filename)` function mentioned in the documentation. This function should return `True` if the file stream `fp` is deemed interactive; otherwise, it should return `False`. Input: - `fp` : a Python file object. - `filename` : a string representing the filename of the file object `fp`. Output: - A boolean value `True` or `False` indicating whether the file stream is interactive. Constraints: - The function should handle standard input streams (`sys.stdin`) as interactive. - If the global interactive flag (`global_interactive_flag`) is set to `True`, the function should also return `True` for filenames equal to `\\"<stdin>\\"` or `\\"???\\"`. Performance Requirements: - The function should efficiently check for the interactivity of the file stream without extensive processing. Example: ```python import sys import os global_interactive_flag = True def is_interactive_file(fp, filename): # Your implementation here # Example usage: print(is_interactive_file(sys.stdin, \\"<stdin>\\")) # Should return True if global_interactive_flag is True print(is_interactive_file(open(\\"test.txt\\", \\"r\\"), \\"test.txt\\")) # Should return False if file is not interactive ``` # Additional Notes: - **Hint**: Use `os.isatty()` to check if the file stream is interactive based on its file descriptor. - Ensure the global flag `global_interactive_flag` influences the function\'s behavior as described. - You may assume the filename passed is always a valid string and `fp` is always a valid Python file object.","solution":"import os global_interactive_flag = True def is_interactive_file(fp, filename): Checks if a given file stream `fp` is deemed interactive. Parameters: fp (file object): The file object to check. filename (str): The filename of the file object `fp`. Returns: bool: True if the file stream is interactive, False otherwise. if filename in (\\"<stdin>\\", \\"???\\") and global_interactive_flag: return True try: return os.isatty(fp.fileno()) except: return False"},{"question":"# Advanced Python Coding Assessment You are tasked with implementing a Python function that processes a series of operations on a nested data structure and handles potential errors robustly. The function will need to demonstrate a clear understanding of name resolution, scope, exception handling, and the use of code blocks. **Problem Statement:** Implement a function `process_nested_structure(operations)` that takes a list of operations and applies them to a nested dictionary. The function should handle exceptions gracefully and ensure that changes are made within the correct scope. The `operations` list consists of dictionaries with the following structure: ```python [ { \\"action\\": \\"assign\\" | \\"update\\" | \\"delete\\", \\"path\\": [\\"key1\\", \\"key2\\", ..., \\"keyN\\"], \\"value\\": Any }, ... ] ``` Each operation can be one of the following: 1. **Assign**: Assign a value to a specified path within the nested dictionary. 2. **Update**: Update the value at a specified path within the nested dictionary. 3. **Delete**: Delete the key at a specified path within the nested dictionary. The function should initialize a global nested dictionary (`global_dict`) and perform the operations on it based on their type. If any operation fails (e.g., trying to update a non-existing key), the function should handle the exception and skip that operation, printing an appropriate error message. **Constraints:** - You cannot use external modules for this task. - You must demonstrate proper use of name resolution and exception handling. - Operations may be deeply nested, requiring careful handling of the paths. **Function Signature:** ```python def process_nested_structure(operations: list) -> dict: pass ``` **Examples:** ```python operations = [ {\\"action\\": \\"assign\\", \\"path\\": [\\"a\\", \\"b\\", \\"c\\"], \\"value\\": 1}, {\\"action\\": \\"update\\", \\"path\\": [\\"a\\", \\"b\\", \\"c\\"], \\"value\\": 2}, {\\"action\\": \\"delete\\", \\"path\\": [\\"a\\", \\"b\\", \\"c\\"], \\"value\\": None}, {\\"action\\": \\"update\\", \\"path\\": [\\"a\\", \\"b\\", \\"d\\"], \\"value\\": 3} # This should raise an error and be skipped ] result = process_nested_structure(operations) print(result) # Output should be {\'a\': {\'b\': {}}} ``` **Explanation:** 1. The first operation assigns the value 1 to the path [\'a\', \'b\', \'c\']. 2. The second operation updates the value at [\'a\', \'b\', \'c\'] to 2. 3. The third operation deletes the key \'c\' under [\'a\', \'b\']. 4. The fourth operation attempts to update a non-existing path [\'a\', \'b\', \'d\'], which should raise an error. Ensure your solution follows Python\'s naming and binding rules, handles exceptions appropriately, and performs the operations in the correct scope.","solution":"def process_nested_structure(operations): global_dict = {} def get_nested_dict(d, keys): for key in keys[:-1]: d = d.setdefault(key, {}) return d for op in operations: action = op[\'action\'] path = op[\'path\'] value = op[\'value\'] try: if action == \'assign\': nested_dict = get_nested_dict(global_dict, path) nested_dict[path[-1]] = value elif action == \'update\': nested_dict = get_nested_dict(global_dict, path) if path[-1] in nested_dict: nested_dict[path[-1]] = value else: raise KeyError(f\\"Update failed: Key {path[-1]} not found\\") elif action == \'delete\': nested_dict = get_nested_dict(global_dict, path) if path[-1] in nested_dict: del nested_dict[path[-1]] else: raise KeyError(f\\"Delete failed: Key {path[-1]} not found\\") else: raise ValueError(f\\"Invalid action: {action}\\") except Exception as e: print(f\\"Operation {op} failed: {e}\\") return global_dict"},{"question":"**Objective:** You are required to create a Python function that utilizes the `py_compile` module to compile a list of Python source files into their corresponding byte-code files. The function should handle errors appropriately based on the input parameters and return a dictionary containing the original source file paths and their corresponding byte-code file paths. **Function Specification:** ```python def batch_compile_python_files(file_list, output_dir, optimize=-1, invalidation_mode=None, quiet=0): Compile a list of Python source files into byte-code files. Parameters: - file_list (list of str): List of paths to Python source files to be compiled. - output_dir (str): Directory where the byte-code files should be saved. - optimize (int, optional): The optimization level (default is -1). - invalidation_mode (py_compile.PycInvalidationMode, optional): Method to invalidate the byte-code files (default is TIMESTAMP). - quiet (int, optional): Level of error message suppression (default is 0). Returns: - dict: Dictionary where keys are the original source file paths and values are the byte-code file paths. pass ``` **Input:** 1. `file_list`: A list of strings where each string is a path to a Python source file. 2. `output_dir`: A string representing the directory where compiled byte-code files should be stored. 3. `optimize`: An integer representing the optimization level (defaults to `-1`). 4. `invalidation_mode`: An instance of `py_compile.PycInvalidationMode` determining cache invalidation mode (defaults to `TIMESTAMP`). 5. `quiet`: An integer value (0, 1, or 2) to specify error message suppression level (defaults to `0`). **Output:** A dictionary where each key is a source file path from the `file_list` and each value is the path to the corresponding byte-code file created in the `output_dir`. **Constraints:** 1. The function will raise a `ValueError` if any path in `file_list` does not point to an actual file. 2. Compilation errors should be handled based on the `quiet` parameter. 3. If `output_dir` does not exist, it should be created. **Example:** ```python file_list = [\\"script1.py\\", \\"script2.py\\"] output_dir = \\"./compiled\\" bytecode_paths = batch_compile_python_files(file_list, output_dir, optimize=1, quiet=1) # Example output (assuming both files are found and compiled successfully): # { # \\"script1.py\\": \\"./compiled/__pycache__/script1.cpython-310.opt-1.pyc\\", # \\"script2.py\\": \\"./compiled/__pycache__/script2.cpython-310.opt-1.pyc\\" # } ``` **Notes:** - Utilize the `py_compile` module’s `compile` function. - Handle exceptions appropriately based on `quiet` and `doraise` options. - Ensure the function is well-documented and tested.","solution":"import py_compile import os import py_compile from pathlib import Path from typing import List, Dict def batch_compile_python_files(file_list: List[str], output_dir: str, optimize: int = -1, invalidation_mode = None, quiet: int = 0) -> Dict[str, str]: Compile a list of Python source files into byte-code files. Parameters: - file_list (list of str): List of paths to Python source files to be compiled. - output_dir (str): Directory where the byte-code files should be saved. - optimize (int, optional): The optimization level (default is -1). - invalidation_mode (py_compile.PycInvalidationMode, optional): Method to invalidate the byte-code files (default is TIMESTAMP). - quiet (int, optional): Level of error message suppression (default is 0). Returns: - dict: Dictionary where keys are the original source file paths and values are the byte-code file paths. # Check if output directory exists, create if not Path(output_dir).mkdir(parents=True, exist_ok=True) bytecode_paths = {} for file_path in file_list: # Check if file exists if not os.path.isfile(file_path): raise ValueError(f\\"File {file_path} does not exist\\") try: # Compile the file compiled_path = py_compile.compile(file=file_path, cfile=None, dfile=None, doraise=True, optimize=optimize, invalidation_mode=invalidation_mode, quiet=quiet) # Move compiled file to output directory compiled_filename = os.path.basename(compiled_path) output_path = os.path.join(output_dir, compiled_filename) os.rename(compiled_path, output_path) # Record the output path to the dictionary bytecode_paths[file_path] = output_path except py_compile.PyCompileError as e: if quiet == 0: raise e return bytecode_paths"},{"question":"You are given a collection of files that you need to archive using the `tarfile` module in Python. Your task is to implement a function that creates a tar archive from a given list of file paths, extracts files from the created tar archive to a specified directory, and ensures safe extraction by applying appropriate extraction filters. # Function Signatures ```python import tarfile from typing import List def create_tar_archive(file_paths: List[str], archive_name: str) -> None: Create a tar archive from the given list of file paths. Args: file_paths (List[str]): A list of file paths to include in the tar archive. archive_name (str): The name of the resulting tar archive. def extract_tar_archive(archive_name: str, extract_path: str) -> None: Extract all files from the given tar archive to the specified directory, using secure filters. Args: archive_name (str): The name of the tar archive to extract. extract_path (str): The directory to extract files to. ``` # Requirements 1. **create_tar_archive**: - Use the `tarfile` module to create a tar archive containing all files specified in `file_paths`. - The archive should be uncompressed and named as specified by `archive_name`. 2. **extract_tar_archive**: - Use the `tarfile` module to extract all files from `archive_name` to `extract_path`. - Apply the `tarfile.data_filter` to ensure secure extraction. This filter disables potentially dangerous features like symbolic links and device files. # Constraints - You can assume all file paths in `file_paths` are valid and accessible. - Ensure that the tar archive is created in the current working directory. # Example Usage ```python # Creating a tar archive named \\"my_archive.tar\\" containing files \\"file1.txt\\" and \\"file2.txt\\" create_tar_archive([\\"file1.txt\\", \\"file2.txt\\"], \\"my_archive.tar\\") # Extracting the contents of \\"my_archive.tar\\" to the directory \\"extracted_files/\\" extract_tar_archive(\\"my_archive.tar\\", \\"extracted_files/\\") ``` Given your understanding of the `tarfile` module, implement both functions while following the provided specifications.","solution":"import tarfile from typing import List import os def create_tar_archive(file_paths: List[str], archive_name: str) -> None: Create a tar archive from the given list of file paths. Args: file_paths (List[str]): A list of file paths to include in the tar archive. archive_name (str): The name of the resulting tar archive. with tarfile.open(archive_name, \'w\') as tar: for file_path in file_paths: tar.add(file_path, arcname=os.path.basename(file_path)) def extract_tar_archive(archive_name: str, extract_path: str) -> None: Extract all files from the given tar archive to the specified directory, using secure filters. Args: archive_name (str): The name of the tar archive to extract. extract_path (str): The directory to extract files to. with tarfile.open(archive_name, \'r\') as tar: tar.extractall(path=extract_path, members=[m for m in tar.getmembers() if m.isreg()])"},{"question":"# Question: Advanced PyTorch Graph Transformations **Objective**: Implement a graph transformation that identifies a specific pattern of operations in a PyTorch FX graph and replaces it with an optimized subgraph. **Problem Statement**: You are given a PyTorch FX graph that performs a matrix multiplication followed by an addition with a bias term. Your task is to identify this pattern and replace it with a single efficient operation. 1. **Identify the pattern**: A sequence of `torch.ops.aten.mm` (matrix multiplication) followed by `torch.ops.aten.add.Tensor` (addition with a bias). 2. **Replace the pattern**: Use the `torch.ops.aten.addmm.default` operation which performs the matrix multiplication and addition in a single step for better performance. **Requirements**: - Implement a function `optimize_graph(gm: torch.fx.GraphModule) -> torch.fx.GraphModule` where `gm` is the input graph module. - Use the `Subgraph Rewriter` utility provided by PyTorch FX to perform the transformation. - Ensure that all instances of the pattern in the graph are replaced correctly. **Input**: - A PyTorch FX `GraphModule` representing the computation graph. **Output**: - A transformed PyTorch FX `GraphModule` with the specified pattern replaced with the optimized operation. **Constraints**: - Do not use any external libraries other than PyTorch. - Assume the input graph will always contain valid operations as described. - You may not modify the original graph structure manually; utilize the utilities provided by PyTorch FX. **Example**: ```python import torch from torch.fx import symbolic_trace class MyModule(torch.nn.Module): def forward(self, x, w, b): mm_result = torch.ops.aten.mm(x, w) add_result = torch.ops.aten.add.Tensor(mm_result, b) return add_result # Symbolically trace the module to get a GraphModule module = MyModule() gm = symbolic_trace(module) # Optimize the graph optimized_gm = optimize_graph(gm) # Print the optimized graph to see the changes print(optimized_gm.graph) ``` The output graph should show that the sequence of `mm` followed by `add` is replaced by a single `addmm` operation. **Implementation**: ```python import torch from torch.fx import GraphModule, subgraph_rewriter def optimize_graph(gm: GraphModule) -> GraphModule: def pattern(x, w, b): mm_result = torch.ops.aten.mm(x, w) add_result = torch.ops.aten.add.Tensor(mm_result, b) return add_result def replacement(x, w, b): return torch.ops.aten.addmm.default(b, x, w) subgraph_rewriter.replace_pattern(gm, pattern, replacement) return gm ``` This problem assesses your ability to: - Utilize PyTorch FX GraphModule and Subgraph Rewriter. - Understand and manipulate computational graphs. - Apply optimizations to improve the graph\'s performance.","solution":"import torch from torch.fx import GraphModule, subgraph_rewriter def optimize_graph(gm: GraphModule) -> GraphModule: def pattern(x, w, b): mm_result = torch.ops.aten.mm(x, w) add_result = torch.ops.aten.add.Tensor(mm_result, b) return add_result def replacement(x, w, b): return torch.ops.aten.addmm.default(b, x, w) subgraph_rewriter.replace_pattern(gm, pattern, replacement) return gm"},{"question":"Complex Tensors in PyTorch --- Objective: You are required to write a function that demonstrates your understanding of creating, manipulating, and performing basic operations on complex tensors in PyTorch. --- Function Specifications: 1. **Function Name**: `complex_tensor_operations` 2. **Parameters**: - `real_part` (torch.Tensor): A tensor containing the real part of the complex numbers. - `imag_part` (torch.Tensor): A tensor containing the imaginary part of the complex numbers. 3. **Returns**: - A dictionary with keys: - `\\"complex_tensor\\"`: The complex tensor created using the specified real and imaginary parts. - `\\"angle\\"`: The angles of the elements in the complex tensor. - `\\"magnitude\\"`: The magnitudes of the elements in the complex tensor. - `\\"sum\\"`: Sum of all elements in the complex tensor. - `\\"product\\"`: Product of all elements in the complex tensor. --- Constraints: 1. The `real_part` and `imag_part` tensors should have the same shape. --- Example: ```python import torch real_part = torch.tensor([1, 2, 3], dtype=torch.float32) imag_part = torch.tensor([4, 5, 6], dtype=torch.float32) result = complex_tensor_operations(real_part, imag_part) print(result[\\"complex_tensor\\"]) # tensor([1.+4.j, 2.+5.j, 3.+6.j]) print(result[\\"angle\\"]) # tensor([1.3258, 1.1903, 1.1071]) print(result[\\"magnitude\\"]) # tensor([4.1231, 5.3852, 6.7082]) print(result[\\"sum\\"]) # (6+15j) print(result[\\"product\\"]) # (-77+6j) ``` --- Implementation Details: 1. Create a complex tensor using `torch.complex(real_part, imag_part)`. 2. Calculate and return the required values (angle, magnitude, sum, product) using appropriate PyTorch functions such as `torch.angle()`, `torch.abs()`, etc. --- Good luck!","solution":"import torch def complex_tensor_operations(real_part, imag_part): Perform operations on complex tensors in PyTorch. Parameters: real_part (torch.Tensor): Tensor containing the real part of complex numbers. imag_part (torch.Tensor): Tensor containing the imaginary part of complex numbers. Returns: dict: A dictionary containing the complex tensor, angles, magnitudes, sum, and product. # Ensure real_part and imag_part have the same shape assert real_part.shape == imag_part.shape, \\"Real and imaginary parts must have the same shape\\" # Create a complex tensor complex_tensor = torch.complex(real_part, imag_part) # Calculate angles angle = torch.angle(complex_tensor) # Calculate magnitudes magnitude = torch.abs(complex_tensor) # Calculate sum of all elements in the complex tensor sum_complex = torch.sum(complex_tensor) # Calculate product of all elements in the complex tensor product_complex = torch.prod(complex_tensor) return { \\"complex_tensor\\": complex_tensor, \\"angle\\": angle, \\"magnitude\\": magnitude, \\"sum\\": sum_complex, \\"product\\": product_complex }"},{"question":"You are required to implement a custom exception handling and logging mechanism using the `traceback` module. This mechanism should capture and log the details of any exception that occurs within a given piece of code. The logged information should be highly detailed and stored for later analysis. Task: 1. **Implement a function `log_exception_details`** that will: - Accept a callable `func` and its arguments `*args` and `**kwargs`. - Execute `func` with the provided arguments. - If any exception occurs during the execution, capture and format the exception details using the `traceback` module. - Store the formatted traceback information and exception details in a log file. 2. **Create a log file named `exception_log.txt`** where the exception details will be saved. Requirements: - Use `traceback.TracebackException` to capture and format the exception. - Log the detailed traceback, local variables, and exception message. - Ensure the function continues execution without crashing the program by handling exceptions gracefully. Function Signature: ```python def log_exception_details(func, *args, **kwargs): pass ``` Example Usage: ```python def sample_function(x, y): return x / y # This will raise a ZeroDivisionError log_exception_details(sample_function, 10, 0) # Open and read the log file with open(\'exception_log.txt\', \'r\') as f: log_contents = f.read() print(log_contents) ``` Expected Log Output Format: ``` Traceback (most recent call last): File \\"your_script.py\\", line XYZ, in log_exception_details func(*args, **kwargs) File \\"your_script.py\\", line ABC, in sample_function return x / y ZeroDivisionError: division by zero Local variables in frame \'sample_function\': x = 10 y = 0 Local variables in frame \'log_exception_details\': func = <function sample_function at 0x...> args = (10, 0) kwargs = {} ``` **Constraints:** - The function should handle any type of exception. - The log file should be appended with new exceptions if the function is called multiple times. - Ensure that local variables are captured and formatted properly. **Performance Requirement:** - The function should handle multiple invocations efficiently without significant performance overhead.","solution":"import traceback import os def log_exception_details(func, *args, **kwargs): Executes the given function with provided arguments and logs any exception details into a file named \'exception_log.txt\'. log_file = \'exception_log.txt\' try: func(*args, **kwargs) except Exception as e: # Capture the traceback and exception information exc_type, exc_value, exc_tb = e.__class__, e, e.__traceback__ tb_exception = traceback.TracebackException(exc_type, exc_value, exc_tb) # Format the exception details formatted_exception = \'\'.join(tb_exception.format()) local_variables = \\"nLocal variables in frame \'{}\':n\\".format(func.__name__) # Capturing the local variables in the current frame for the provided function tb_frame = exc_tb.tb_next if tb_frame is not None: current_frame = tb_frame.tb_frame variable_details = \'\'.join( f\\"{key} = {value}n\\" for key, value in current_frame.f_locals.items() ) else: variable_details = \\"No local variables captured.n\\" log_entry = formatted_exception + local_variables + variable_details # Capture local variables for log_exception_details caller_frame = exc_tb.tb_frame if caller_frame is not None: caller_variables = \'\'.join( f\\"{key} = {value}n\\" for key, value in caller_frame.f_locals.items() ) caller_details = \\"Local variables in frame \'log_exception_details\':n\\" + caller_variables log_entry += caller_details # Append the exception details to the log file with open(log_file, \'a\') as f: f.write(log_entry) f.write(\'nn\') # Separate the entries for readability"},{"question":"**Coding Assessment Question** # Objective: Write a Python function that takes a list of mixed data types (integers, floats, strings, and booleans) and packs them into XDR format using the `xdrlib` module. Then, write another function that takes the XDR formatted data and unpacks it back into the original list of mixed data types. The goal is to ensure that the unpacked data matches the original input list in both value and order. # Requirements: 1. Implement the function `pack_mixed_data(data_list)` that takes a list of mixed data types and returns an XDR formatted byte string. 2. Implement the function `unpack_mixed_data(xdr_data)` that takes an XDR formatted byte string and returns the original list of mixed data types. 3. Ensure to handle all the data types specified (integers, floats, strings, and booleans). # Function Signatures: ```python def pack_mixed_data(data_list: list) -> bytes: pass def unpack_mixed_data(xdr_data: bytes) -> list: pass ``` # Constraints: - The input list for `pack_mixed_data` can contain integers, floats, booleans, and strings. - The strings can be of variable lengths. - The XDR formatted data must be unpacked correctly to match the original input list. # Example: ```python data = [123, 45.67, \\"hello\\", True, 89, 32.1, \\"world\\", False] xdr_data = pack_mixed_data(data) print(xdr_data) # Output will be a byte string in XDR format original_data = unpack_mixed_data(xdr_data) print(original_data) # Output should be [123, 45.67, \\"hello\\", True, 89, 32.1, \\"world\\", False] ``` # Additional Information: - Use `xdrlib.Packer` for packing the data and `xdrlib.Unpacker` for unpacking the data. - Make sure to handle the different types appropriately while packing and unpacking. - The solution should handle edge cases, such as empty string or zero values. Happy coding!","solution":"import xdrlib def pack_mixed_data(data_list): Packs a list of mixed data types (integers, floats, strings, booleans) into XDR format. p = xdrlib.Packer() p.pack_int(len(data_list)) # Pack the length of the list for item in data_list: if isinstance(item, int): p.pack_int(1) # Type identifier for int p.pack_int(item) elif isinstance(item, float): p.pack_int(2) # Type identifier for float p.pack_double(item) elif isinstance(item, str): p.pack_int(3) # Type identifier for string p.pack_string(item.encode()) elif isinstance(item, bool): p.pack_int(4) # Type identifier for boolean p.pack_bool(item) else: raise ValueError(\\"Unsupported data type\\") return p.get_buffer() def unpack_mixed_data(xdr_data): Unpacks XDR formatted data back into the original list of mixed data types. u = xdrlib.Unpacker(xdr_data) length = u.unpack_int() data_list = [] for _ in range(length): data_type = u.unpack_int() if data_type == 1: # int data_list.append(u.unpack_int()) elif data_type == 2: # float data_list.append(u.unpack_double()) elif data_type == 3: # string data_list.append(u.unpack_string().decode()) elif data_type == 4: # boolean data_list.append(u.unpack_bool()) else: raise ValueError(\\"Unsupported data type\\") return data_list"},{"question":"Advanced Error Handling in Python Objective: To assess your understanding of Python\'s error and exception handling mechanisms, including custom exceptions, exception chaining, and proper resource management with clean-up actions. Problem Statement: You are given the task of writing a function `process_data(file_path: str, divisor: int) -> dict` that processes the data from a file and calculates a result based on the content. The function should handle various potential exceptions and ensure resources are cleaned up properly. Detailed Instructions: 1. **Reading the File:** - The input file is expected to have one integer per line. Your function should read these integers into a list. - Use the `with` statement to ensure the file is properly closed after reading. 2. **Processing the Data:** - Calculate the sum of these integers, and then divide the sum by the provided `divisor`. - If the divisor is zero, handle the `ZeroDivisionError` and raise a custom exception `CustomMathError` with the message \\"Cannot divide by zero\\". 3. **Handling Exceptions:** - Provide appropriate error messages for: - `FileNotFoundError`: indicating that the file wasn\'t found. - `ValueError`: indicating that the file contains non-integer values. - If any other unexpected error occurs, re-raise it after printing a message. - Chain exceptions where applicable to provide more context. 4. **Return Value:** - If processing is successful, return a dictionary with two keys: `sum` containing the sum of the integers, and `result` containing the division result. Expected Function Signature: ```python class CustomMathError(Exception): pass def process_data(file_path: str, divisor: int) -> dict: pass ``` Constraints: - Assume the file size is small and can be read into memory entirely. - You can use standard Python libraries only. Example Usage: ```python # Assume the content of \'testfile.txt\' is: # 10 # 20 # 30 try: result = process_data(\'testfile.txt\', 0) except CustomMathError as e: print(e) # Output: Cannot divide by zero try: result = process_data(\'testfile.txt\', 2) print(result) # Output: {\'sum\': 60, \'result\': 30} except Exception as e: print(f\\"Unexpected error: {e}\\") ``` Performance Requirements: The function should handle typical file I/O and basic arithmetic operations efficiently within the provided constraints. Evaluation Criteria: - Correctness of handling specified exceptions and errors. - Usage of custom exceptions and exception chaining. - Proper resource management using `with` statements. - Code readability and adherence to Python best practices.","solution":"class CustomMathError(Exception): pass def process_data(file_path: str, divisor: int) -> dict: try: with open(file_path, \'r\') as file: integers = [] for line in file: try: integers.append(int(line.strip())) except ValueError as ve: raise ValueError(\\"File contains non-integer values\\") from ve total_sum = sum(integers) if divisor == 0: raise CustomMathError(\\"Cannot divide by zero\\") result = total_sum / divisor return {\'sum\': total_sum, \'result\': result} except FileNotFoundError as fnfe: print(f\\"Error: {fnfe}\\") raise fnfe except CustomMathError as cme: print(f\\"Error: {cme}\\") raise cme except Exception as e: print(f\\"An unexpected error occurred: {e}\\") raise e"},{"question":"Handling Numerical Stability in Batched Tensor Computations Objective Your task is to write a PyTorch function that computes the Euclidean norm (L2 norm) of batched vectors, ensuring numerical stability for both small and large values in the vectors. Additionally, you should explore how different precisions impact the computational results. The function should handle: - Batched tensor inputs. - Numerically stable computation to prevent overflow or underflow. - Comparative results with different precisions: `float64` (double precision) and `float32` (single precision). Function Signature ```python import torch def batched_euclidean_norm(tensor: torch.Tensor, dtype: torch.dtype = torch.float32) -> torch.Tensor: Computes the Euclidean norm (L2 norm) for each vector in a batch, handling different numerical precisions. Args: tensor (torch.Tensor): Input tensor of shape (batch_size, num_features). dtype (torch.dtype): Desired floating-point precision (default: torch.float32). Returns: torch.Tensor: A tensor of shape (batch_size,) containing the Euclidean norms. # Your implementation here # Example Usage # Batch of vectors with potential extremal values batch_vectors = torch.tensor([[1e20, 1e20], [1e-20, 1e-20], [3.0, 4.0]], dtype=torch.float32) norms_single_precision = batched_euclidean_norm(batch_vectors, dtype=torch.float32) norms_double_precision = batched_euclidean_norm(batch_vectors, dtype=torch.float64) print(norms_single_precision) print(norms_double_precision) ``` Constraints and Considerations 1. The input tensor will be a 2D tensor of shape `(batch_size, num_features)`. 2. The function should return a 1D tensor of shape `(batch_size,)` containing the L2 norms for each batch. 3. Ensure numerical stability, especially when the input contains large or very small values. 4. Compare the results by performing the computation in both `float32` and `float64` precision. 5. Document any observations about the differences due to precision. Performance Requirement The function should handle tensors with large batch sizes efficiently. Consider using efficient PyTorch operations to avoid explicit Python loops as much as possible. Evaluation Criteria Your solution will be evaluated on the following: - Correctness: The function computes the Euclidean norm accurately. - Numerical Stability: The function handles extremal values without causing overflow or underflow. - Code Efficiency: The implementation leverages efficient PyTorch operations. - Precision Handling: Correct handling and comparison of different precisions. - Documentation: Clear and concise documentation of the code and observations.","solution":"import torch def batched_euclidean_norm(tensor: torch.Tensor, dtype: torch.dtype = torch.float32) -> torch.Tensor: Computes the Euclidean norm (L2 norm) for each vector in a batch, handling different numerical precisions. Args: tensor (torch.Tensor): Input tensor of shape (batch_size, num_features). dtype (torch.dtype): Desired floating-point precision (default: torch.float32). Returns: torch.Tensor: A tensor of shape (batch_size,) containing the Euclidean norms. # Convert the tensor to the desired dtype for numerical stability tensor = tensor.to(dtype) # Use stable computation to prevent overflow/underflow max_val, _ = torch.max(torch.abs(tensor), dim=1, keepdim=True) safe_tensor = tensor / max_val norms = max_val.squeeze() * torch.sqrt(torch.sum(safe_tensor ** 2, dim=1)) return norms # Example Usage # Batch of vectors with potential extremal values batch_vectors = torch.tensor([[1e20, 1e20], [1e-20, 1e-20], [3.0, 4.0]], dtype=torch.float32) norms_single_precision = batched_euclidean_norm(batch_vectors, dtype=torch.float32) norms_double_precision = batched_euclidean_norm(batch_vectors, dtype=torch.float64) print(norms_single_precision) print(norms_double_precision)"},{"question":"Title: Analyzing and Converting Integer Representations in Python Objective: Create a program that accepts a list of integers and performs multiple operations using Python built-in functions. The program should: 1. Convert each integer to its binary, octal, and hexadecimal string representations. 2. Calculate the absolute value of each integer. 3. Filter out negative integers and keep only the non-negative integers. 4. Check if all integers in the list are non-negative and if any integer is zero. 5. Enumerate the list, appending the index to each integer. 6. Determine the maximum and minimum integers in the transformed list. 7. Sort the list in descending order. Input: - A list of integers: `List[int]` Output: - A dictionary containing: - The original list of integers. - List of binary string representations of the integers. - List of octal string representations of the integers. - List of hexadecimal string representations of the integers. - List of absolute values. - Filtered list of non-negative integers. - Boolean indicating if all integers are non-negative. - Boolean indicating if any integer is zero. - Enumerated list as tuples of (index, integer). - Maximum integer in the transformed list. - Minimum integer in the transformed list. - Sorted list in descending order. Constraints: 1. The list should contain at least one integer. 2. Integers can be positive, negative, or zero. Performance Requirements: Efficiently handle the computations for lists up to 10,000 integers. Example: ```python def transform_integer_list(integers: List[int]) -> dict: # Implement the function to achieve the specified requirements. # 1. Convert to binary, octal, and hexadecimal binaries = [bin(x) for x in integers] octals = [oct(x) for x in integers] hexadecimals = [hex(x) for x in integers] # 2. Calculate absolute values abs_values = [abs(x) for x in integers] # 3. Filter out negative integers non_negative = list(filter(lambda x: x >= 0, integers)) # 4. Check conditions all_non_negative = all(x >= 0 for x in integers) any_zero = any(x == 0 for x in integers) # 5. Enumerate list with index enumerated_list = list(enumerate(integers)) # 6. Determine maximum and minimum max_integer = max(integers) min_integer = min(integers) # 7. Sort list in descending order sorted_list = sorted(integers, reverse=True) return { \\"original\\": integers, \\"binaries\\": binaries, \\"octals\\": octals, \\"hexadecimals\\": hexadecimals, \\"absolute_values\\": abs_values, \\"non_negative\\": non_negative, \\"all_non_negative\\": all_non_negative, \\"any_zero\\": any_zero, \\"enumerated\\": enumerated_list, \\"max\\": max_integer, \\"min\\": min_integer, \\"sorted\\": sorted_list } # Example input integers = [3, -4, 0, 2, -1] print(transform_integer_list(integers)) ``` Explanation: 1. Converted integers into their binary, octal, and hexadecimal representations. 2. Calculated the absolute values. 3. Filtered out the negative integers. 4. Checked if all integers are non-negative and if there is any zero in the list. 5. Enumerated the integers with their indices. 6. Determined the maximum and minimum values in the list. 7. Sorted the list in descending order. Hints: - Use built-in functions `bin()`, `oct()`, and `hex()` for conversions. - Use `abs()` to get absolute values. - Use `filter()` and lambda functions to filter the list. - Use `all()` and `any()` for checking conditions. - Use `enumerate()` to enumerate the list. - Use `max()` and `min()` to get the maximum and minimum elements. - Use `sorted()` for sorting the list.","solution":"from typing import List def transform_integer_list(integers: List[int]) -> dict: # 1. Convert to binary, octal, and hexadecimal binaries = [bin(x) for x in integers] octals = [oct(x) for x in integers] hexadecimals = [hex(x) for x in integers] # 2. Calculate absolute values abs_values = [abs(x) for x in integers] # 3. Filter out negative integers non_negative = list(filter(lambda x: x >= 0, integers)) # 4. Check conditions all_non_negative = all(x >= 0 for x in integers) any_zero = any(x == 0 for x in integers) # 5. Enumerate list with index enumerated_list = list(enumerate(integers)) # 6. Determine maximum and minimum max_integer = max(integers) min_integer = min(integers) # 7. Sort list in descending order sorted_list = sorted(integers, reverse=True) return { \\"original\\": integers, \\"binaries\\": binaries, \\"octals\\": octals, \\"hexadecimals\\": hexadecimals, \\"absolute_values\\": abs_values, \\"non_negative\\": non_negative, \\"all_non_negative\\": all_non_negative, \\"any_zero\\": any_zero, \\"enumerated\\": enumerated_list, \\"max\\": max_integer, \\"min\\": min_integer, \\"sorted\\": sorted_list }"},{"question":"# Python Coding Assessment Question Objective: Implement an interactive Python interpreter that suggests completions for partially typed Python statements or expressions using the `rlcompleter` module. Problem Statement: Create an interactive Python shell that: 1. Uses the `rlcompleter` module to provide suggestions when the user presses the \\"Tab\\" key. 2. Displays suggestions for Python keywords, built-in functions, defined variables, and attributes/methods of imported modules. 3. Works both for statements without a period and those with dotted names (e.g., `math.s<tab>` suggests `math.sin`, `math.sqrt`, etc.). Implementation Details: 1. **Function Name**: `interactive_shell` 2. **Expected Input/Output**: No inputs/outputs as the function should operate interactively. 3. **Constraints**: - The solution should use `rlcompleter` and `readline` modules. - The shell should handle common Python statements and expressions, suggest completions in real-time, and handle errors gracefully. Example Usage: ```python >>> interactive_shell() >>> # Starts an interactive session >>> import math >>> math.s<tab> # Suggestions: math.sin math.sqrt >>> foo = \\"bar\\" >>> f<tab> # Suggestions: foo >>> lis<tab> # Suggestions: list ``` Notes: - Make sure to configure the `readline` module to bind the \\"Tab\\" key to the completer function. - Handle cases where the evaluation of expressions may raise exceptions. - Provide some default context (like importing common modules) to make the shell more usable. Performance Requirements: - The completer should be responsive and quick to suggest possible completions, even for larger namespaces. ```python import rlcompleter import readline def interactive_shell(): completer = rlcompleter.Completer() readline.set_completer(completer.complete) readline.parse_and_bind(\'tab: complete\') while True: try: user_input = input(\'>>> \') if user_input.lower() in [\'exit\', \'quit\']: break # Execute the user input in the current global context exec(user_input, globals()) except Exception as e: print(f\\"Error: {e}\\") if __name__ == \\"__main__\\": interactive_shell() ```","solution":"import rlcompleter import readline def interactive_shell(): Starts an interactive shell with tab completion for Python keywords, built-in functions, and defined variables. completer = rlcompleter.Completer() readline.set_completer(completer.complete) readline.parse_and_bind(\'tab: complete\') while True: try: user_input = input(\'>>> \') if user_input.lower() in [\'exit\', \'quit\']: break exec(user_input, globals()) except Exception as e: print(f\\"Error: {e}\\") if __name__ == \\"__main__\\": interactive_shell()"},{"question":"You are required to implement a function that performs various operations on sets and frozensets using the provided Python 3.10 set API. Your task is to define a function `perform_set_operations` that takes a list of operations and executes them accordingly. The operations can be addition, removal of elements, and basic checks on sets and frozensets. The function should return a dictionary with the results of these operations. # Function Signature ```python def perform_set_operations(operations: List[Tuple[str, Any]]) -> Dict[str, Any]: ``` # Input - `operations`: a list of tuples, where each tuple represents a set operation. The first element of the tuple is a string representing the operation type, and the second element is the operand(s) for the operation. The operations are: - `\'create_set\'`: Create a set from an iterable (returns an identifier for the set). - `\'create_frozenset\'`: Create a frozenset from an iterable (returns an identifier for the frozenset). - `\'add\'`: Add an element to a set (requires set ID and element). - `\'discard\'`: Remove an element from a set (requires set ID and element). - `\'contains\'`: Check if an element is in a set/frozenset (requires set ID and element). - `\'len\'`: Get the length of a set/frozenset (requires set ID). - `\'clear\'`: Clear all elements from a set (requires set ID). # Output - A dictionary containing the results of the operations. The keys should be descriptive strings of the operations performed, and the values should be the respective outcomes. # Example ```python operations = [ (\'create_set\', [1, 2, 3]), (\'create_frozenset\', [4, 5, 6]), (\'add\', (\'set_0\', 4)), (\'discard\', (\'set_0\', 2)), (\'contains\', (\'set_0\', 3)), (\'len\', \'set_0\'), (\'clear\', \'set_0\'), (\'len\', \'set_0\') ] result = perform_set_operations(operations) ``` Expected `result`: ```python { \'set_0\': {1, 3, 4}, \'frozenset_0\': frozenset({4, 5, 6}), \'add_1\': True, \'discard_1\': True, \'contains_1\': True, \'len_1\': 3, \'clear_1\': True, \'len_2\': 0 } ``` # Constraints - You may assume that there are no invalid operations in the input. - Only valid Python iterables will be provided for set and frozenset creation. - Performance should be considered, but you may assume that the number of operations will not result in performance bottlenecks for typical testing. # Notes - Use the Python 3.10 set API from the documentation provided to implement this function. - Maintain internal counters or identifiers to track which set/frozenset is being referenced in subsequent operations.","solution":"def perform_set_operations(operations): sets = {} results = {} set_counter = 0 frozenset_counter = 0 for operation in operations: op_type = operation[0] if op_type == \'create_set\': set_id = f\'set_{set_counter}\' sets[set_id] = set(operation[1]) results[set_id] = sets[set_id] set_counter += 1 elif op_type == \'create_frozenset\': frozenset_id = f\'frozenset_{frozenset_counter}\' sets[frozenset_id] = frozenset(operation[1]) results[frozenset_id] = sets[frozenset_id] frozenset_counter += 1 elif op_type == \'add\': set_id, elem = operation[1] sets[set_id].add(elem) results[f\'add_{set_id}\'] = True elif op_type == \'discard\': set_id, elem = operation[1] sets[set_id].discard(elem) results[f\'discard_{set_id}\'] = True elif op_type == \'contains\': set_id, elem = operation[1] results[f\'contains_{set_id}\'] = elem in sets[set_id] elif op_type == \'len\': set_id = operation[1] results[f\'len_{set_id}\'] = len(sets[set_id]) elif op_type == \'clear\': set_id = operation[1] sets[set_id].clear() results[f\'clear_{set_id}\'] = True return results"},{"question":"**Objective:** Demonstrate your understanding of pandas `Timedelta` objects and their operations. **Task:** Create a Python function called `process_timedelta_data` that takes as input a DataFrame with the following columns: - `start_time`: a column of `datetime64[ns]` values representing start times. - `duration_str`: a column of strings, each representing a duration (e.g., \\"1 days\\", \\"2 hours\\", \\"-1 days 2 hours 3 minutes 4 seconds\\"). Your function should: 1. Convert the `duration_str` column to a `Timedelta` column named `duration`. 2. Calculate an `end_time` column by adding the `duration` column to the `start_time` column. 3. Create a new column `components` that contains the days, hours, minutes, and seconds components of `duration`. 4. Replace any negative `end_time` values with `NaT` (Not a Time). 5. Compute and return the mean of the `duration` column, the total sum of `duration` values excluding `NaT`, and the count of negative `duration` entries. **Constraints:** - Assume that the `start_time` column contains valid `datetime64[ns]` values and the `duration_str` column contains valid duration-like strings. - You may assume that the input DataFrame has no missing values in `start_time` or `duration_str`. **Function Signature:** ```python import pandas as pd def process_timedelta_data(df: pd.DataFrame) -> (pd.Timedelta, pd.Timedelta, int): # Your code here # Example usage: data = { \'start_time\': pd.to_datetime([\'2023-01-01 12:00:00\', \'2023-01-02 12:00:00\', \'2023-01-03 12:00:00\']), \'duration_str\': [\'1 days\', \'-1 days 2 hours 3 minutes 4 seconds\', \'2 hours 30 minutes\'] } df = pd.DataFrame(data) mean_duration, total_duration, negative_count = process_timedelta_data(df) print(mean_duration) print(total_duration) print(negative_count) ``` **Expected Output:** - `mean_duration`: The mean of the `duration` column. - `total_duration`: The sum of all `duration` values, excluding `NaT`. - `negative_count`: The count of negative `duration` entries. You are expected to make use of pandas\' `Timedelta` capabilities as detailed in the provided documentation.","solution":"import pandas as pd def process_timedelta_data(df: pd.DataFrame) -> (pd.Timedelta, pd.Timedelta, int): # Convert the \'duration_str\' column to a \'Timedelta\' column named \'duration\' df[\'duration\'] = pd.to_timedelta(df[\'duration_str\']) # Calculate the \'end_time\' column by adding \'duration\' to \'start_time\' df[\'end_time\'] = df[\'start_time\'] + df[\'duration\'] # Replace any negative \'end_time\' values with \'NaT\' df.loc[df[\'end_time\'] < df[\'start_time\'], \'end_time\'] = pd.NaT # Create a new column \'components\' that contains the days, hours, minutes, and seconds components of \'duration\' df[\'components\'] = df[\'duration\'].apply(lambda x: (x.days, x.components.hours, x.components.minutes, x.components.seconds)) # Compute the mean of the \'duration\' column mean_duration = df[\'duration\'].mean() # Compute the total sum of \'duration\' values total_duration = df[\'duration\'].sum() # Count the number of negative \'duration\' entries negative_count = (df[\'duration\'] < pd.Timedelta(0)).sum() return mean_duration, total_duration, negative_count"},{"question":"Using `torch.cond`, implement a class `TensorOperationSwitch` that changes its behavior based on tensor properties. The class should have two behaviors: 1. If the tensor\'s mean is greater than a specific threshold, apply a specified function (`mean_true_fn`) to it. 2. Otherwise, apply a different function (`mean_false_fn`). Class Definition ```python import torch def mean_true_fn(x: torch.Tensor) -> torch.Tensor: # Example implementation: return the square of x return x ** 2 def mean_false_fn(x: torch.Tensor) -> torch.Tensor: # Example implementation: return the cube of x return x ** 3 class TensorOperationSwitch(torch.nn.Module): def __init__(self, threshold: float): super().__init__() self.threshold = threshold def forward(self, x: torch.Tensor) -> torch.Tensor: Args: x : torch.Tensor : Input tensor. Returns: torch.Tensor : Tensor after applying one of the mean dependent functions. return torch.cond(x.mean() > self.threshold, mean_true_fn, mean_false_fn, (x,)) ``` Constraints - Assume `mean_true_fn` and `mean_false_fn` will always be provided and are valid torch functions. - The tensor `x` can be of any shape and contain float values. Input 1. A float tensor `x` with any shape. Example: `torch.randn(3, 3)` 2. A float threshold value for comparison. Example: `0.5` Output - A tensor that is the result of either `mean_true_fn(x)` or `mean_false_fn(x)`, based on the mean value of `x`. Example ```python switcher = TensorOperationSwitch(threshold=0.5) input_tensor = torch.tensor([1.0, 2.0, 3.0, 10.0]) output_tensor = switcher(input_tensor) print(output_tensor) # If mean(input_tensor) is greater than 0.5, output will be tensor([1.0, 4.0, 9.0, 100.0]) # Otherwise, output will be tensor([1.0, 8.0, 27.0, 1000.0]) ``` Implement the `TensorOperationSwitch` class as described.","solution":"import torch def mean_true_fn(x: torch.Tensor) -> torch.Tensor: # Example implementation: return the square of x return x ** 2 def mean_false_fn(x: torch.Tensor) -> torch.Tensor: # Example implementation: return the cube of x return x ** 3 class TensorOperationSwitch(torch.nn.Module): def __init__(self, threshold: float): super().__init__() self.threshold = threshold def forward(self, x: torch.Tensor) -> torch.Tensor: Args: x : torch.Tensor : Input tensor. Returns: torch.Tensor : Tensor after applying one of the mean dependent functions. mean_value = x.mean() return mean_true_fn(x) if mean_value > self.threshold else mean_false_fn(x)"},{"question":"**Sparse Data Structures with pandas** You are given a dataset that contains a mix of dense and sparse data. Your task is to implement a function that processes this dataset by converting it to a sparse format, performing some calculations, and returning the results as a dense format. # Function Signature ```python def process_sparse_data(data: pd.DataFrame) -> pd.DataFrame: pass ``` # Input - `data` (pd.DataFrame): A pandas DataFrame containing various types of data. It can contain numerical values and missing values represented as NaN. # Output - (pd.DataFrame): A pandas DataFrame that has undergone the following transformations: 1. Converted from a dense format to a sparse format. 2. Applied a specific calculation on sparse data. 3. Converted back to a dense format with the calculation results. # Tasks to be performed 1. **Conversion to Sparse Format**: Convert the given DataFrame to a sparse format, using `pd.SparseDtype`. 2. **Calculation on Sparse Data**: For each column that is numerical: - Calculate the absolute value of the data using NumPy `ufunc`. 3. **Conversion Back to Dense Format**: Convert the resulting sparse DataFrame back to a dense format. # Example Assume the following input DataFrame: ```python import pandas as pd import numpy as np data = pd.DataFrame({ \'A\': [1, 2, np.nan, 4, np.nan], \'B\': [np.nan, 1, 2, np.nan, np.nan], \'C\': [3, np.nan, np.nan, 5, 1] }) ``` The function should produce the following output: ```python A B C 0 1.0 NaN 3.0 1 2.0 1.0 NaN 2 NaN 2.0 NaN 3 4.0 NaN 5.0 4 NaN NaN 1.0 ``` # Constraints - Use pandas and NumPy libraries. - Handle the missing values appropriately during the conversion and calculation processes. **Notes**: - This problem assumes you have a basic understanding of pandas and NumPy. - You should be familiar with the concept of sparse data structures and how to use them efficiently. Implement the function `process_sparse_data` to complete the task.","solution":"import pandas as pd import numpy as np def process_sparse_data(data: pd.DataFrame) -> pd.DataFrame: # Step 1: Convert the DataFrame from dense to sparse format sparse_data = data.astype(pd.SparseDtype(\\"float\\", np.nan)) # Step 2: Calculate absolute value of the numerical data for col in sparse_data.columns: if pd.api.types.is_numeric_dtype(sparse_data[col]): sparse_data[col] = np.abs(sparse_data[col]) # Step 3: Convert the sparse DataFrame back to a dense format dense_data = sparse_data.sparse.to_dense() return dense_data"},{"question":"# Problem: Terminal Text Editor You are required to implement a basic text editor in Python using the `curses` module. The text editor should allow users to input multiple lines of text, navigate through the text, and perform basic editing operations like inserting and deleting characters. Requirements: 1. **Initialization and Cleanup**: - Initialize the `curses` module using `curses.initscr()`. - Ensure the terminal state is restored to normal when the program exits, even if an exception occurs. 2. **Text Editing Operations**: - Support input of printable characters. - Implement keybindings for basic cursor movements: - `KEY_UP` or `Ctrl-P`: Move cursor one line up. - `KEY_DOWN` or `Ctrl-N`: Move cursor one line down. - `KEY_LEFT` or `Ctrl-B`: Move cursor one character to the left. - `KEY_RIGHT` or `Ctrl-F`: Move cursor one character to the right. - Implement keybindings for text editing: - `Ctrl-D`: Delete the character under the cursor. - `Ctrl-H` or `KEY_BACKSPACE`: Delete the character before the cursor. - `Ctrl-K`: Clear the line from the cursor to the end. - `Ctrl-A`: Move the cursor to the beginning of the line. - `Ctrl-E`: Move the cursor to the end of the line. 3. **Display**: - Render the text correctly in the terminal window. - Refresh the display to show updates as the user interacts with the editor. 4. **Termination**: - Allow the user to exit the text editor by pressing `Ctrl-G`. Input and Output: The editor should work interactively with the user typing directly via the terminal interface. There is no command-line input or output for the text after exiting. Constraints: - The editor should handle a maximum of 100 lines with 80 characters each. - Ensure the editor runs efficiently with minimal screen flicker. # Example: Here is a simplified example of how the text editor session might look: ``` Welcome to the Basic Text Editor! Press Ctrl-G to exit. | <-- cursor start User starts typing: Hello, world! This is a simple text editor. Ctrl-A moves cursor to the beginning of the line Ctrl-E moves cursor to the end of the line Ctrl-H deletes the character before the cursor Ctrl-D deletes the character under the cursor Ctrl-P moves the cursor up one line Ctrl-N moves the cursor down one line Ctrl-K clears the line from the cursor to the end. ... ... All displayed in the terminal window. ``` **Hint**: Use the `curses.wrapper()` function to simplify initialization and cleanup of the `curses` module.","solution":"import curses def main(stdscr): # Clear screen stdscr.clear() curses.curs_set(1) # Enable the cursor stdscr.keypad(True) # Enable keypad mode stdscr.timeout(100) # Delay for getch # Initialize the editor content lines = [\\"\\"] cursor_x, cursor_y = 0, 0 while True: stdscr.clear() for idx, line in enumerate(lines): stdscr.addstr(idx, 0, line) stdscr.move(cursor_y, cursor_x) stdscr.refresh() key = stdscr.getch() if key in (curses.KEY_EXIT, 7): # Ctrl-G to exit break elif key == curses.KEY_UP or key == 16: # KEY_UP or Ctrl-P if cursor_y > 0: cursor_y -= 1 cursor_x = min(cursor_x, len(lines[cursor_y])) elif key == curses.KEY_DOWN or key == 14: # KEY_DOWN or Ctrl-N if cursor_y < len(lines) - 1: cursor_y += 1 cursor_x = min(cursor_x, len(lines[cursor_y])) elif key == curses.KEY_LEFT or key == 2: # KEY_LEFT or Ctrl-B if cursor_x > 0: cursor_x -= 1 elif key == curses.KEY_RIGHT or key == 6: # KEY_RIGHT or Ctrl-F if cursor_x < len(lines[cursor_y]): cursor_x += 1 elif key == 4: # Ctrl-D if cursor_x < len(lines[cursor_y]): lines[cursor_y] = lines[cursor_y][:cursor_x] + lines[cursor_y][cursor_x+1:] elif key == curses.KEY_BACKSPACE or key == 8: # KEY_BACKSPACE or Ctrl-H if cursor_x > 0: lines[cursor_y] = lines[cursor_y][:cursor_x-1] + lines[cursor_y][cursor_x:] cursor_x -= 1 elif key == 11: # Ctrl-K lines[cursor_y] = lines[cursor_y][:cursor_x] elif key == 1: # Ctrl-A cursor_x = 0 elif key == 5: # Ctrl-E cursor_x = len(lines[cursor_y]) elif 0 <= key <= 255 and chr(key).isprintable(): lines[cursor_y] = lines[cursor_y][:cursor_x] + chr(key) + lines[cursor_y][cursor_x:] cursor_x += 1 if __name__ == \\"__main__\\": curses.wrapper(main)"},{"question":"Problem Statement You are tasked with developing a sophisticated mathematical computation tool in Python that leverages the capabilities of the `math`, `cmath`, `decimal`, and `statistics` modules. The tool should perform the following operations: 1. **Root Finder**: - Implement a function `find_nth_root(value: float, n: int) -> float` to compute the nth root of a given positive float `value`. - Use the `math.pow` function for this calculation. - Your function should handle the condition when `n` is zero by raising a suitable exception. 2. **Complex Number Operations**: - Implement a function `complex_operations(c: complex) -> Tuple[float, float, float, float]` that takes a complex number `c` and returns the magnitude, phase, sine, and cosine of the complex number using `cmath` functions. 3. **High Precision Arithmetic**: - Implement a function `high_precision_addition(a: str, b: str) -> str` that takes two decimal numbers represented as strings. - Use the `decimal.Decimal` class to perform addition with high precision and return the result as a string. 4. **Statistical Analysis**: - Implement a function `statistical_summary(data: List[float]) -> Dict[str, float]` which returns a dictionary with the mean, median, variance, and standard deviation of the `data` list. - Use appropriate functions from the `statistics` module for these calculations. Input and Output Formats - **find_nth_root(value: float, n: int) -> float** - **Input**: - `value (float)`: positive float for which the nth root is to be computed. - `n (int)`: non-negative integer representing the root to be calculated. - **Output**: - Returns the nth root of `value` as a float. - **complex_operations(c: complex) -> Tuple[float, float, float, float]** - **Input**: - `c (complex)`: a complex number. - **Output**: - Returns a tuple containing the magnitude, phase, sine, and cosine of the complex number. - **high_precision_addition(a: str, b: str) -> str** - **Input**: - `a (str)`: a decimal number represented as a string. - `b (str)`: another decimal number represented as a string. - **Output**: - Returns the sum of the two numbers as a string with high precision. - **statistical_summary(data: List[float]) -> Dict[str, float]** - **Input**: - `data (List[float])`: a list of floating-point numbers. - **Output**: - Returns a dictionary with keys `\'mean\'`, `\'median\'`, `\'variance\'`, and `\'std_dev\'`, and their respective computed values from the `data` list. Constraints and Requirements - Your functions should handle edge cases and invalid inputs gracefully. - The `find_nth_root` function should raise a `ValueError` if `n` is zero. - Ensure the operations maintain high precision as required, especially in the `high_precision_addition` function. - Aim for efficiency where possible, but clarity and correctness are the primary goals. Example ```python # Example usage # Root Finder print(find_nth_root(27.0, 3)) # Output: 3.0 # Complex Number Operations print(complex_operations(1+1j)) # Output: (1.4142135623730951, 0.7853981633974483, 1.2984575814159773, 0.8337300251311491) # High Precision Arithmetic print(high_precision_addition(\'0.1\', \'0.2\')) # Output: \'0.3\' # Statistical Analysis data = [1.0, 2.0, 3.0, 4.0, 5.0] print(statistical_summary(data)) # Output: { # \'mean\': 3.0, # \'median\': 3.0, # \'variance\': 2.5, # \'std_dev\': 1.5811388300841898 # } ``` Notes - Pay careful attention to the precision and types of inputs and outputs. - Thoroughly test your functions with a variety of test cases to ensure accuracy.","solution":"import math import cmath from decimal import Decimal from statistics import mean, median, variance, stdev from typing import List, Dict, Tuple def find_nth_root(value: float, n: int) -> float: Computes the nth root of a given positive float value. Raises ValueError if n is zero. if n == 0: raise ValueError(\\"The root index cannot be zero.\\") return math.pow(value, 1/n) def complex_operations(c: complex) -> Tuple[float, float, float, float]: Returns the magnitude, phase, sine, and cosine of a complex number c. magnitude = abs(c) phase = cmath.phase(c) sine = cmath.sin(c).real # Getting real part because sine of complex returns a complex number cosine = cmath.cos(c).real # Getting real part because cosine of complex returns a complex number return magnitude, phase, sine, cosine def high_precision_addition(a: str, b: str) -> str: Adds two decimal numbers represented as strings with high precision. Returns the result as a string. result = Decimal(a) + Decimal(b) return str(result) def statistical_summary(data: List[float]) -> Dict[str, float]: Returns a dictionary with the mean, median, variance, and standard deviation of the data list. return { \'mean\': mean(data), \'median\': median(data), \'variance\': variance(data) if len(data) > 1 else 0, # variance requires at least two data points \'std_dev\': stdev(data) if len(data) > 1 else 0 # stdev requires at least two data points }"},{"question":"<|Analysis Begin|> The provided documentation offers a comprehensive overview of the `time` module and its various functionalities. The `time` module is fundamental in many Python applications that need to perform time manipulation, formatting, and measurement. The module includes various methods to convert between different time representations, sleep for a specified period, and retrieve or set the current time in various formats. Key functionalities include: 1. **Time conversion functions**: - `gmtime()`, `localtime()`, `mktime()`, and `calendar.timegm()` 2. **Time formatting and parsing**: - `asctime()`, `ctime()`, `strftime()`, `strptime()` 3. **Clock-related functions**: - `clock_gettime()`, `clock_settime()`, `clock_gettime_ns()`, and respective constants for different clocks like `CLOCK_MONOTONIC`, `CLOCK_REALTIME`, etc. 4. **Time-related constants**: - `timezone`, `altzone`, `daylight`, and `tzname` 5. **Utility functions**: - `sleep()`, `time()`, `time_ns()`, `monotonic()`, `perf_counter()`, `process_time()`, `thread_time()` Given the extensive and varied nature of this module, an assessment question can be designed to test the student\'s ability to: - Convert between different time formats. - Handle both local and UTC time efficiently. - Utilize `strftime` and `strptime` for time formatting and parsing. - Implement a feature that requires precise time measurement and utilizes the high-resolution clocks provided by the module. - Handle errors and unexpected inputs appropriately. <|Analysis End|> <|Question Begin|> # Problem: Time Tracker and Formatter You are tasked with developing a utility called `TimeTracker` that leverages the `time` module functionalities to keep track of and format time data. The utility should allow users to record timestamps, convert between different time formats, and generate formatted time strings. The utility should also feature high-resolution tracking and support for different time zones. Implement the `TimeTracker` class with the following methods: 1. `record_timestamp() -> None`: - Records the current time in seconds since the epoch using the highest available resolution. 2. `get_formatted_timestamp(format: str) -> str`: - Returns the recorded time formatted as a string according to the given `format` string. Use the same formatting directives as `time.strftime()`. 3. `get_time_difference(another_tracker: \'TimeTracker\') -> float`: - Returns the difference in seconds between the recorded time of this `TimeTracker` instance and another `TimeTracker` instance. This should support high-resolution time differences using `time.perf_counter()` or `time.time_ns()`. 4. `to_utc() -> struct_time`: - Converts the recorded time to a `struct_time` in UTC. 5. `to_local() -> struct_time`: - Converts the recorded time to a `struct_time` in local time. 6. `convert_to_timezone(offset: int) -> struct_time`: - Converts the recorded time to a `struct_time` in a different timezone specified by `offset`, where the offset is in seconds relative to UTC (e.g., `3600` for UTC+1). # Constraints: - The recorded time must be tracked with the highest available resolution. - The `format` string should follow the same rules as `time.strftime()`. - Ensure that methods handle invalid format strings and offsets appropriately by raising `ValueError`. # Example Usage: ```python from time import struct_time class TimeTracker: def __init__(self): self.timestamp = None self.high_res_timestamp = None def record_timestamp(self) -> None: # Use high-resolution time keeping to record the current timestamp self.timestamp = time.time() self.high_res_timestamp = time.time_ns() def get_formatted_timestamp(self, format: str) -> str: # Convert the recorded timestamp to the specified format if self.timestamp is None: raise ValueError(\\"Timestamp not recorded yet.\\") struct_time = time.localtime(self.timestamp) return time.strftime(format, struct_time) def get_time_difference(self, another_tracker: \'TimeTracker\') -> float: # Calculate the difference between the current and another TimeTracker\'s timestamp if self.high_res_timestamp is None or another_tracker.high_res_timestamp is None: raise ValueError(\\"Timestamp not recorded in one or both trackers.\\") return (self.high_res_timestamp - another_tracker.high_res_timestamp) / 1e9 def to_utc(self) -> struct_time: # Convert the recorded timestamp to UTC if self.timestamp is None: raise ValueError(\\"Timestamp not recorded yet.\\") return time.gmtime(self.timestamp) def to_local(self) -> struct_time: # Convert the recorded timestamp to local time if self.timestamp is None: raise ValueError(\\"Timestamp not recorded yet.\\") return time.localtime(self.timestamp) def convert_to_timezone(self, offset: int) -> struct_time: # Adjust the recorded time by the specified offset and convert it to a struct_time if self.timestamp is None: raise ValueError(\\"Timestamp not recorded yet.\\") timestamp_with_offset = self.timestamp + offset return time.gmtime(timestamp_with_offset) ``` # Additional Information - Handle exceptions and invalid inputs (like invalid format strings or unrecorded timestamps) by raising `ValueError`. - Ensure that the timestamps are tracked and outputted with the highest precision to maintain accuracy. - Leverage the functions and constants from the `time` module for all time conversions and measurements.","solution":"import time class TimeTracker: def __init__(self): self.timestamp = None self.high_res_timestamp = None def record_timestamp(self) -> None: # Use high-resolution time keeping to record the current timestamp self.timestamp = time.time() self.high_res_timestamp = time.time_ns() def get_formatted_timestamp(self, format: str) -> str: # Convert the recorded timestamp to the specified format if self.timestamp is None: raise ValueError(\\"Timestamp not recorded yet.\\") struct_time = time.localtime(self.timestamp) return time.strftime(format, struct_time) def get_time_difference(self, another_tracker: \'TimeTracker\') -> float: if self.high_res_timestamp is None or another_tracker.high_res_timestamp is None: raise ValueError(\\"Timestamp not recorded in one or both trackers.\\") return (self.high_res_timestamp - another_tracker.high_res_timestamp) / 1e9 def to_utc(self) -> time.struct_time: if self.timestamp is None: raise ValueError(\\"Timestamp not recorded yet.\\") return time.gmtime(self.timestamp) def to_local(self) -> time.struct_time: if self.timestamp is None: raise ValueError(\\"Timestamp not recorded yet.\\") return time.localtime(self.timestamp) def convert_to_timezone(self, offset: int) -> time.struct_time: if self.timestamp is None: raise ValueError(\\"Timestamp not recorded yet.\\") timestamp_with_offset = self.timestamp + offset return time.gmtime(timestamp_with_offset)"},{"question":"**Custom Pickling with `copyreg`** You are given a class `Person` with the following structure: ```python class Person: def __init__(self, name, age): self.name = name self.age = age ``` Your task is to: 1. Define a suitable reduction function, `pickle_person`, for the `Person` class that returns a tuple containing the class and its initialization arguments. 2. Register this function using `copyreg.pickle`. 3. Demonstrate that the custom pickling and unpickling works as expected by creating an instance of `Person`, pickling it to a byte stream using `pickle.dumps`, and then unpickling it back to a `Person` object using `pickle.loads`. **Constraints:** - You must use the `copyreg` module to register the custom pickling function. - Your reduction function should handle the `Person` instance properly and ensure that the unpickled object maintains the same state. **Expected Input and Output:** 1. The class `Person` should be pickled and unpickled correctly with the provided custom reduction function. 2. The state of the unpickled `Person` object should match the original instance. ```python # Example p = Person(\\"Alice\\", 30) pickled_data = pickle.dumps(p) # This should use the registered pickle function unpickled_p = pickle.loads(pickled_data) # This should return a new Person instance assert unpickled_p.name == p.name assert unpickled_p.age == p.age ``` **Instructions:** Implement the custom reduction function, register it, and demonstrate pickling and unpickling with assertions as shown above.","solution":"import copyreg import pickle class Person: def __init__(self, name, age): self.name = name self.age = age def pickle_person(person): Custom reduction function for pickling Person objects. return (Person, (person.name, person.age)) # Register the custom pickling function copyreg.pickle(Person, pickle_person) # Example usage if __name__ == \\"__main__\\": p = Person(\\"Alice\\", 30) pickled_data = pickle.dumps(p) unpickled_p = pickle.loads(pickled_data) assert unpickled_p.name == p.name assert unpickled_p.age == p.age"},{"question":"Problem Statement In this exercise, you are required to work with Python iterators and asynchronous iterators. Your task is to implement two functions: `iterate_values()` and `async_iterate_values()`, which respectively handle synchronous and asynchronous iteration over provided iterator objects. # 1. Function: `iterate_values(iterator)` **Objective**: - Iterate over a given Python iterator object using the iterator protocol. **Parameters**: - `iterator` (iterator): a Python iterator object. **Returns**: - `list`: A list containing all values retrieved from the iterator. **Constraints**: - Do not use any built-in Python loops (`for`, `while`) or list comprehension to directly iterate over the iterator. - You must manually handle the iteration process, similar to how the `PyIter_Next` function works in the C API. # 2. Function: `async_iterate_values(async_iterator)` **Objective**: - Iterate over a given Python asynchronous iterator object using the async iterator protocol. **Parameters**: - `async_iterator` (async iterator): a Python asynchronous iterator object. **Returns**: - `list`: A list containing all values retrieved from the asynchronous iterator. **Constraints**: - Do not use any built-in Python loops (`for`, `while`) or list comprehension to directly iterate over the async iterator. - You must manually handle the asynchronous iteration process, similar to the `PyIter_Send` functionality in the C API. # Example Usage ```python # Example for `iterate_values` normal_iterator = iter([1, 2, 3, 4]) result = iterate_values(normal_iterator) assert result == [1, 2, 3, 4] # Example for `async_iterate_values` import asyncio async def async_generator(): for i in range(1, 5): yield i async def run_async_iter(): async_iterator = async_generator() result = await async_iterate_values(async_iterator) assert result == [1, 2, 3, 4] asyncio.run(run_async_iter()) ``` **Notes**: - You need to handle the iteration manually using the appropriate protocol methods. - Properly manage the results and handle the end of the iteration gracefully.","solution":"def iterate_values(iterator): Manually iterates over a given Python iterator object using the iterator protocol. Parameters: - iterator (iterator): a Python iterator object. Returns: - list: A list containing all values retrieved from the iterator. results = [] while True: try: next_item = next(iterator) results.append(next_item) except StopIteration: break return results import asyncio async def async_iterate_values(async_iterator): Manually iterates over a given Python asynchronous iterator object using the async iterator protocol. Parameters: - async_iterator (async iterator): a Python asynchronous iterator object. Returns: - list: A list containing all values retrieved from the asynchronous iterator. results = [] while True: try: next_item = await async_iterator.__anext__() results.append(next_item) except StopAsyncIteration: break return results"},{"question":"You are tasked to implement a custom JSON serialization and deserialization system in Python for a complex library management system. The system involves serializing and deserializing instances of various classes to and from JSON format. The classes involved are `Book`, `Author`, and `Library`. 1. **Class Definitions:** - `Book`: Represents a book with attributes `title` (str), `author` (Author object), and `published_year` (int). - `Author`: Represents an author with attributes `name` (str), `birth_year` (int). - `Library`: Represents a collection of books with attribute `books` (list of Book objects). 2. **Requirements:** - Implement custom JSON serialization for `Book`, `Author`, and `Library` classes. - Implement custom JSON deserialization for the same classes. - Ensure that the serialization and deserialization maintain the structure and data integrity of the objects. # Constraints: - The `title`, `name` attributes are strings and must be non-empty. - The `birth_year` for `Author` and `published_year` for `Book` are integers within the range 1000-9999. # Specifications: 1. Implement custom JSON encoder and decoder. 2. Use `json.dump()` and `json.load()` for file-based operations. 3. Handle appropriate error cases, such as invalid data types or missing required attributes. # Input/Output Examples: Example 1: **Python Objects:** ```python author = Author(name=\\"J.K. Rowling\\", birth_year=1965) book = Book(title=\\"Harry Potter and the Philosopher\'s Stone\\", author=author, published_year=1997) library = Library(books=[book]) ``` **Serialized JSON:** ```json { \\"books\\": [ { \\"title\\": \\"Harry Potter and the Philosopher\'s Stone\\", \\"author\\": { \\"name\\": \\"J.K. Rowling\\", \\"birth_year\\": 1965 }, \\"published_year\\": 1997 } ] } ``` **Deserialized Python Objects:** ```python # Should result in equivalent objects as initially created ``` # Tasks: 1. Define the classes `Book`, `Author`, and `Library`. 2. Implement the custom JSON serialization using `json.JSONEncoder`. 3. Implement the custom JSON deserialization using `json.JSONDecoder` and relevant hooks. 4. Write functions `save_library_to_file(library, filename)` and `load_library_from_file(filename)` to handle file-based JSON operations. # Detailed Steps: 1. **Define Classes:** Define `Book`, `Author`, and `Library` classes with appropriate constructors and attribute definitions. 2. **Custom JSON Encoder for Classes:** Create a `LibraryEncoder` class extending `json.JSONEncoder` and override the `default()` method to handle instances of `Book`, `Author`, and `Library`. 3. **Custom JSON Decoder for Classes:** Create functions to handle `object_hook` for custom deserialization, transforming dictionaries into objects of `Book`, `Author`, and `Library`. 4. **File Operations:** Implement `save_library_to_file(library, filename)` to serialize and write a `Library` object to a JSON file. Implement `load_library_from_file(filename)` to read JSON data from a file and create a `Library` object. # Implementations: ```python import json class Author: def __init__(self, name, birth_year): if not name or not isinstance(name, str): raise ValueError(\\"Invalid name\\") if not (1000 <= birth_year <= 9999): raise ValueError(\\"Invalid birth year\\") self.name = name self.birth_year = birth_year class Book: def __init__(self, title, author, published_year): if not title or not isinstance(title, str): raise ValueError(\\"Invalid title\\") if not isinstance(author, Author): raise ValueError(\\"Invalid author\\") if not (1000 <= published_year <= 9999): raise ValueError(\\"Invalid published year\\") self.title = title self.author = author self.published_year = published_year class Library: def __init__(self, books): if not all(isinstance(book, Book) for book in books): raise ValueError(\\"Invalid books list\\") self.books = books class LibraryEncoder(json.JSONEncoder): def default(self, obj): if isinstance(obj, Book): return { \'title\': obj.title, \'author\': obj.author, \'published_year\': obj.published_year } elif isinstance(obj, Author): return { \'name\': obj.name, \'birth_year\': obj.birth_year } elif isinstance(obj, Library): return { \'books\': obj.books } return super().default(obj) def as_author(dct): if \\"name\\" in dct and \\"birth_year\\" in dct: return Author(name=dct[\\"name\\"], birth_year=dct[\\"birth_year\\"]) return dct def as_book(dct): if \\"title\\" in dct and \\"author\\" in dct and \\"published_year\\" in dct: author = as_author(dct[\\"author\\"]) return Book(title=dct[\\"title\\"], author=author, published_year=dct[\\"published_year\\"]) return dct def as_library(dct): if \\"books\\" in dct: books = [as_book(book) for book in dct[\\"books\\"]] return Library(books=books) return dct def save_library_to_file(library, filename): with open(filename, \'w\') as file: json.dump(library, file, cls=LibraryEncoder, indent=4) def load_library_from_file(filename): with open(filename, \'r\') as file: return json.load(file, object_hook=as_library) ``` **Optional Challenges**: 1. Extend the functionality to validate and handle missing attributes gracefully. 2. Introduce a `Publisher` class and update the serialization and deserialization logic accordingly. 3. Ensure performance efficiency and handle large libraries efficiently.","solution":"import json class Author: def __init__(self, name, birth_year): if not name or not isinstance(name, str): raise ValueError(\\"Invalid name\\") if not (1000 <= birth_year <= 9999): raise ValueError(\\"Invalid birth year\\") self.name = name self.birth_year = birth_year def __eq__(self, other): return self.name == other.name and self.birth_year == other.birth_year class Book: def __init__(self, title, author, published_year): if not title or not isinstance(title, str): raise ValueError(\\"Invalid title\\") if not isinstance(author, Author): raise ValueError(\\"Invalid author\\") if not (1000 <= published_year <= 9999): raise ValueError(\\"Invalid published year\\") self.title = title self.author = author self.published_year = published_year def __eq__(self, other): return (self.title == other.title and self.author == other.author and self.published_year == other.published_year) class Library: def __init__(self, books): if not all(isinstance(book, Book) for book in books): raise ValueError(\\"Invalid books list\\") self.books = books def __eq__(self, other): return self.books == other.books class LibraryEncoder(json.JSONEncoder): def default(self, obj): if isinstance(obj, Book): return { \'title\': obj.title, \'author\': { \'name\': obj.author.name, \'birth_year\': obj.author.birth_year }, \'published_year\': obj.published_year } elif isinstance(obj, Author): return { \'name\': obj.name, \'birth_year\': obj.birth_year } elif isinstance(obj, Library): return { \'books\': obj.books } return super().default(obj) def as_author(dct): if \\"name\\" in dct and \\"birth_year\\" in dct: return Author(name=dct[\\"name\\"], birth_year=dct[\\"birth_year\\"]) return dct def as_book(dct): if \\"title\\" in dct and \\"author\\" in dct and \\"published_year\\" in dct: author = as_author(dct[\\"author\\"]) return Book(title=dct[\\"title\\"], author=author, published_year=dct[\\"published_year\\"]) return dct def as_library(dct): if \\"books\\" in dct: books = [as_book(book) for book in dct[\\"books\\"]] return Library(books=books) return dct def save_library_to_file(library, filename): with open(filename, \'w\') as file: json.dump(library, file, cls=LibraryEncoder, indent=4) def load_library_from_file(filename): with open(filename, \'r\') as file: return json.load(file, object_hook=as_library)"},{"question":"# Advanced Python: Implementing a Custom Iterator **Objective**: Implement a custom iterator class in Python that mimics the behavior and integrates with Python\'s iterator protocol. **Problem Statement**: Create a custom iterator class named `Countdown`, which initializes with a positive integer `n` and iterates from `n` down to 1 inclusively. The class should properly raise a `StopIteration` exception when the countdown completes. You are required to: 1. Implement the `__iter__()` method that returns the iterator object (i.e., `self`). 2. Implement the `__next__()` method that returns the next value in the countdown sequence. 3. Implement error handling to ensure that the iterator behaves as expected. # Function Signature ```python class Countdown: def __init__(self, n: int): # Initialize with a positive integer n def __iter__(self): # Return the iterator object, which is self def __next__(self): # Return the next value in the countdown and handle StopIteration ``` # Input: - The constructor of the `Countdown` class should receive a single argument `n` which is a positive integer. # Output: - Each call to `__next__()` should return the next number in the countdown. - Once the countdown is complete, `__next__()` should raise a `StopIteration` exception. # Constraints: - `1 <= n <= 10^6` # Example: ```python # Create an instance of Countdown countdown = Countdown(5) # Get the iterator object iterator = iter(countdown) # Iterate through the countdown print(next(iterator)) # Output: 5 print(next(iterator)) # Output: 4 print(next(iterator)) # Output: 3 print(next(iterator)) # Output: 2 print(next(iterator)) # Output: 1 print(next(iterator)) # Raises StopIteration exception ``` **Notes**: - Ensure the class adheres to Python\'s iterator protocol. - Handle large values gracefully by managing state efficiently.","solution":"class Countdown: def __init__(self, n: int): if n < 1: raise ValueError(\\"n must be a positive integer greater than or equal to 1\\") self.current = n def __iter__(self): return self def __next__(self): if self.current <= 0: raise StopIteration current_value = self.current self.current -= 1 return current_value"},{"question":"Objective Write a Python function using Seaborn to visualize a dataset with a scatter plot and rug plot combination while applying customization to enhance readability. Description Your task is to create a plot that visualizes the relationship between \'total_bill\' and \'tip\' in the \'tips\' dataset. The plot should include: 1. A scatter plot showing each data point. 2. A rug plot along both the x (total_bill) and y (tip) axes. 3. Representation of the \'time\' variable within the scatter plot using different colors. 4. Customization of the rug plot with the following specifications: - The height of the rug plot lines should be 0.1. - The rug should be displayed outside the axes boundaries. - The rug lines should be thinner than the default width (line width of 0.5). Parameters The function should not take any input parameters. Expected Output The function should generate and display a plot meeting the above criteria. Performance Requirements - The function should run efficiently on datasets of similar size to \'tips\'. Example The function when fully implemented and run should display a scatter plot with rug plots meeting all the described customizations. ```python import seaborn as sns import matplotlib.pyplot as plt def customized_rugplot(): sns.set_theme() tips = sns.load_dataset(\\"tips\\") sns.scatterplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"time\\") sns.rugplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", height=0.1, clip_on=False, lw=0.5) plt.show() customized_rugplot() ``` **Note**: Ensure the plot is displayed correctly with all the required customizations.","solution":"import seaborn as sns import matplotlib.pyplot as plt def customized_rugplot(): sns.set_theme() tips = sns.load_dataset(\\"tips\\") # Create the scatter plot scatter_plot = sns.scatterplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"time\\") # Add a rug plot with specific customizations rug_plot = sns.rugplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", height=0.1, clip_on=False, lw=0.5) # Show the plot plt.show() customized_rugplot()"},{"question":"**Objective**: Implement a Python function using the `zipfile` module to create, update, and extract ZIP archives with various compression methods. The function should include error handling for common pitfalls related to ZIP archives. **Task**: Write a function `process_zipfile(zip_path, operation, files=[], target_dir=None, compression=zipfile.ZIP_STORED, password=None)` that performs the following operations: 1. **Creating a ZIP file**: - `operation = \'create\'` - `files` is a list of file paths to include in the ZIP archive. - `compression` defines the compression method. Must be one of `zipfile.ZIP_STORED`, `zipfile.ZIP_DEFLATED`, `zipfile.ZIP_BZIP2`, or `zipfile.ZIP_LZMA`. 2. **Updating a ZIP file**: - `operation = \'update\'` - `files` is a list of file paths to add to an existing ZIP archive. - The function should append files to the existing ZIP without deleting any files that are already in the archive. 3. **Extracting from a ZIP file**: - `operation = \'extract\'` - `target_dir` specifies the directory to extract the files into. - If the ZIP file is encrypted, `password` should be provided as a `bytes` object. **Function Signature**: ```python import zipfile from typing import List def process_zipfile(zip_path: str, operation: str, files: List[str] = [], target_dir: str = None, compression=zipfile.ZIP_STORED, password: bytes = None) -> None: pass ``` **Example Usage**: ```python # Creating a ZIP file process_zipfile(\'example.zip\', \'create\', files=[\'file1.txt\', \'file2.txt\'], compression=zipfile.ZIP_DEFLATED) # Updating a ZIP file process_zipfile(\'example.zip\', \'update\', files=[\'file3.txt\'], compression=zipfile.ZIP_BZIP2) # Extracting from a ZIP file process_zipfile(\'example.zip\', \'extract\', target_dir=\'extracted_files\') ``` **Constraints & Considerations**: - Ensure the ZIP file path and files to be added exist before performing operations. - Only allow the specified compression methods; raise `NotImplementedError` for others. - Implement appropriate error handling for invalid ZIP files, missing files, and unsupported operations. - When extracting, strip any absolute paths and prevent directory traversal exploits (e.g., filenames with `..`). **Hints**: - Use `zipfile.ZipFile()` as a context manager to ensure the file is properly closed. - For encryption, use `pwd` parameter in extraction functions. - When updating, use mode `\'a\'` to append to the existing ZIP archive.","solution":"import zipfile from typing import List import os def process_zipfile(zip_path: str, operation: str, files: List[str] = [], target_dir: str = None, compression=zipfile.ZIP_STORED, password: bytes = None) -> None: # Check for valid compression method valid_compressions = [zipfile.ZIP_STORED, zipfile.ZIP_DEFLATED, zipfile.ZIP_BZIP2, zipfile.ZIP_LZMA] if compression not in valid_compressions: raise NotImplementedError(f\\"Compression method {compression} is not supported.\\") if operation == \'create\': with zipfile.ZipFile(zip_path, \'w\', compression=compression) as zip_file: for file in files: if os.path.exists(file): zip_file.write(file, os.path.basename(file)) else: print(f\\"Warning: {file} does not exist and will not be included.\\") elif operation == \'update\': with zipfile.ZipFile(zip_path, \'a\', compression=compression) as zip_file: for file in files: if os.path.exists(file): zip_file.write(file, os.path.basename(file)) else: print(f\\"Warning: {file} does not exist and will not be included.\\") elif operation == \'extract\': if target_dir is None: raise ValueError(\\"target_dir must be specified for extraction.\\") with zipfile.ZipFile(zip_path, \'r\') as zip_file: # Validate and sanitize filenames to avoid directory traversal exploits safe_extract(zip_file, target_dir, password) else: raise ValueError(f\\"Unsupported operation: {operation}\\") def safe_extract(zip_file: zipfile.ZipFile, target_dir: str, password: bytes): for member in zip_file.namelist(): member_path = os.path.join(target_dir, member) if not os.path.abspath(member_path).startswith(os.path.abspath(target_dir)): raise Exception(\\"Attempted Path Traversal in Zip File\\") zip_file.extractall(path=target_dir, pwd=password)"},{"question":"**Question:** Implement a Python class `AuditLogger` that captures and logs specific audit events related to file operations and subprocess execution. The class should utilize the `sys.addaudithook()` function to register an audit hook that monitors the following events: - `open` - `subprocess.Popen` The audit hook should log each event with a timestamp and relevant details (such as file paths for `open` and command for `subprocess.Popen`). The logs should be stored in an internal list within the `AuditLogger` class, and there should be a method `get_logs()` that returns these logs. **Requirements:** 1. The class should initialize an empty log list. 2. The audit hook should capture and log only the specified events. 3. Each log entry should include a timestamp of when the event was captured, the event name, and the relevant arguments. 4. The log should be stored as a list of dictionaries, where each dictionary represents a single log entry. **Function Specifications:** - `__init__(self)`: Initializes the `AuditLogger` class and sets up the audit hook. - `get_logs(self) -> List[Dict[str, Any]]`: Returns the list of log entries. **Example Usage:** ```python import sys import subprocess class AuditLogger: def __init__(self): self.logs = [] def hook(event, args): if event in {\\"open\\", \\"subprocess.Popen\\"}: log_entry = { \\"timestamp\\": time.time(), \\"event\\": event, \\"args\\": args } self.logs.append(log_entry) sys.addaudithook(hook) def get_logs(self): return self.logs # Example usage logger = AuditLogger() # Performing monitored operations with open(\\"example.txt\\", \\"w\\") as file: file.write(\\"Hello, world!\\") subprocess.Popen([\\"echo\\", \\"Hello\\"]) # Retrieving logs for log in logger.get_logs(): print(log) ``` **Constraints:** - Use the standard `time` library to obtain timestamps. - Ensure thread safety if your solution will be used in a multi-threaded environment. - Care should be taken to handle potential exceptions during logging or audit hook registration. **Evaluation Criteria:** - Correct implementation of the audit hook registration. - Accurate logging of the specified events. - Proper formatting and completeness of log entries. - Clean and readable code with appropriate comments.","solution":"import sys import time from typing import List, Dict, Any import subprocess class AuditLogger: def __init__(self): self.logs = [] def hook(event, args): if event in {\\"open\\", \\"subprocess.Popen\\"}: log_entry = { \\"timestamp\\": time.time(), \\"event\\": event, \\"args\\": args } self.logs.append(log_entry) sys.addaudithook(hook) def get_logs(self) -> List[Dict[str, Any]]: return self.logs"},{"question":"# MIME Types Assignment You have been provided a list of file URLs. Your task is to write a Python function that categorizes these files by MIME type using the `mimetypes` module. For each URL, determine its MIME type and organize them into a dictionary where the keys are the MIME types and the values are lists of URLs corresponding to each MIME type. Function Signature ```python def categorize_files_by_mime(urls: list, strict: bool = True) -> dict: pass ``` Input: - `urls` (list): A list of file URLs (strings) to categorize. Example: `[\\"file1.txt\\", \\"image.png\\", \\"document.pdf\\"]` - `strict` (bool, optional): If `True` (default), only the official MIME types registered with IANA are considered. If `False`, additional non-standard but commonly used MIME types are also recognized. Output: - `dict`: A dictionary where keys are MIME types (strings) and values are lists of URLs (strings) having those types. Example: ```python urls = [\\"example.txt\\", \\"example.png\\", \\"example.pdf\\", \\"example.zip\\"] result = categorize_files_by_mime(urls) ``` For these inputs, the function should produce a dictionary like: ```python { \\"text/plain\\": [\\"example.txt\\"], \\"image/png\\": [\\"example.png\\"], \\"application/pdf\\": [\\"example.pdf\\"], \\"application/zip\\": [\\"example.zip\\"] } ``` Constraints: - You should use the `guess_type()` function from the `mimetypes` module to determine the MIME type of each file. - Handle cases where the MIME type cannot be determined (i.e., return value is `None`) by ignoring those files (they should not appear in the output dictionary). Implement the function `categorize_files_by_mime(urls: list, strict: bool = True) -> dict` to solve the problem.","solution":"import mimetypes def categorize_files_by_mime(urls: list, strict: bool = True) -> dict: Categorize files by their MIME types using the `mimetypes` module. Parameters: urls (list): A list of file URLs (strings) to categorize. strict (bool, optional): If True, only use official MIME types. If False, use additional non-standard MIME types. Default is True. Returns: dict: A dictionary where keys are MIME types (strings) and values are lists of URLs (strings) having those types. mime_dict = {} for url in urls: mime_type, _ = mimetypes.guess_type(url, strict=strict) if mime_type: if mime_type not in mime_dict: mime_dict[mime_type] = [] mime_dict[mime_type].append(url) return mime_dict"},{"question":"# Advanced Python Memory Management: Cell Objects In Python, closures are a powerful feature where inner functions can remember and access variables defined in their enclosing scope. Behind the scenes, Python uses \\"cell objects\\" to manage the variables referenced by these nested scopes. For this assessment, you are to implement a Python class that mimics the behavior of cell objects and demonstrates their usage in managing variables across nested functions. Task 1. Implement a Python class `Cell` that encapsulates a value. This class should have the following methods: - `__init__(self, value)`: Initializes the cell with a given value. - `get(self)`: Returns the value stored in the cell. - `set(self, value)`: Sets a new value in the cell. 2. Implement a function `make_closure(cell, increment)` which takes a `Cell` object and an integer `increment`. It should return a nested function (closure) that increments the value in the `Cell` object by `increment` each time it is called. Input and Output Formats - `Cell` class should be usable as follows: ```python cell = Cell(10) assert cell.get() == 10 cell.set(20) assert cell.get() == 20 ``` - `make_closure` function should be usable as follows: ```python cell = Cell(10) increment_by_5 = make_closure(cell, 5) increment_by_5() assert cell.get() == 15 increment_by_5() assert cell.get() == 20 ``` Constraints - Your implementation must correctly mimic the behavior of Python\'s cell objects as described. - Assume that the values handled by `Cell` objects are always integers. Performance Requirements - Your solution should efficiently handle repeated calls to methods without significant performance degradation. ```python # Your implementation goes here. class Cell: def __init__(self, value): self.value = value def get(self): return self.value def set(self, value): self.value = value def make_closure(cell, increment): def closure(): cell.set(cell.get() + increment) return closure # Example usage for testing: cell = Cell(10) increment_by_5 = make_closure(cell, 5) increment_by_5() print(cell.get()) # Should output 15 increment_by_5() print(cell.get()) # Should output 20 ```","solution":"class Cell: def __init__(self, value): self.value = value def get(self): return self.value def set(self, value): self.value = value def make_closure(cell, increment): def closure(): cell.set(cell.get() + increment) return closure # Example usage for testing: cell = Cell(10) increment_by_5 = make_closure(cell, 5) increment_by_5() print(cell.get()) # Should output 15 increment_by_5() print(cell.get()) # Should output 20"},{"question":"**Objective:** Showcase your understanding of seaborn’s plotting capabilities by using seaborn\'s object interface to create and customize a plot using a provided dataset. **Problem Statement:** You are provided with a dataset containing monthly temperature data. Your task is to plot this data using seaborn\'s object interface (`seaborn.objects`). Specifically, you need to create a line plot with the following requirements: 1. Load the dataset `monthly_temps` with columns `Month`, `Year`, and `Temperature`. 2. Create a line plot where: - The x-axis represents the months. - The y-axis represents the temperature. - The line color represents different years. 3. Split the plot into facets (small multiples) based on the decades (e.g., 2000s, 2010s). 4. Customize the plot to: - Use a line width of 0.8 for the lines. - Set a specific color palette of your choice. - Add appropriate titles for each facet, indicating the decade. **Instructions:** 1. Load the dataset and process it if necessary. 2. Use the `seaborn.objects` interface to create the plot with the specified requirements. 3. Your solution should include code to generate the plot and optionally display it. **Example Output:** Your plot should generate a figure similar to the following format: - X-axis labeled as \\"Month\\" - Y-axis labeled as \\"Temperature\\" - Facet titles indicating the decade such as \\"2000s\\", \\"2010s\\", etc. - Lines representing monthly temperature changes over different years. ```python import seaborn.objects as so from seaborn import load_dataset import pandas as pd # Simulate loading the provided dataset monthly_temps = pd.DataFrame({ \'Month\': pd.date_range(start=\'2000-01-01\', periods=240, freq=\'M\').month, \'Year\': pd.date_range(start=\'2000-01-01\', periods=240, freq=\'M\').year, \'Temperature\': (np.random.rand(240) * 30) + 10 }) # Create the plot based on the given instructions ( so.Plot(monthly_temps, x=\\"Month\\", y=\\"Temperature\\", color=\\"Year\\") .facet(monthly_temps[\\"Year\\"] // 10 * 10) .add(so.Lines(linewidth=0.8)) .scale(color=\\"ch:rot=-.2,light=.7\\") .label(title=\\"{}s\\".format) ) # Note: This code snippet does not include the dataset loading as it\'s only an example # output. You need to load the actual dataset \'monthly_temps\' and complete the task. ``` **Submission:** Submit your Python code that accomplishes the task. Ensure it is properly commented and structured.","solution":"import seaborn.objects as so import pandas as pd import numpy as np # Create a simulated dataset monthly_temps = pd.DataFrame({ \'Month\': pd.date_range(start=\'2000-01-01\', periods=240, freq=\'M\').month, \'Year\': pd.date_range(start=\'2000-01-01\', periods=240, freq=\'M\').year, \'Temperature\': (np.random.rand(240) * 30) + 10 }) # Adding a \'Decade\' column to the DataFrame monthly_temps[\'Decade\'] = (monthly_temps[\'Year\'] // 10) * 10 # Create the plot based on the given instructions so.Plot(monthly_temps, x=\\"Month\\", y=\\"Temperature\\", color=\\"Year\\").facet(\\"Decade\\").add(so.Line(linewidth=0.8)).scale(color=\\"ch:rot=-.2,light=.7\\").label(title=\\"{}s\\".format).show()"},{"question":"You have been tasked with creating a utility function that prepares and formats email headers for a mailing application. Write a function `prepare_email_headers` that takes the following arguments: - `addresses` (list of tuples): A list of 2-tuples where each tuple contains (realname, email_address). - `timestamp` (float, optional): A floating point time value as accepted by `time.gmtime()` and `time.localtime()`; defaults to the current time. - `msg_id_string` (str, optional): An optional string used to strengthen the uniqueness of the message id; defaults to `None`. The function should: 1. Format each (realname, email_address) tuple into a proper RFC 2822-compliant address header using `email.utils.formataddr()`. 2. Generate a unique message ID using `email.utils.make_msgid()`. 3. Format the provided timestamp into an RFC 2822-compliant date string using `email.utils.formatdate()`. - If `timestamp` is not provided, use the current time. 4. Return a dictionary containing three keys: - `addresses`: A list of formatted address headers. - `Message-ID`: The generated unique message ID. - `Date`: The formatted date string. **Function Signature:** ```python from typing import List, Tuple, Optional, Dict import time from email.utils import formataddr, make_msgid, formatdate def prepare_email_headers(addresses: List[Tuple[str, str]], timestamp: Optional[float] = None, msg_id_string: Optional[str] = None) -> Dict[str, str]: ``` # Example: ```python addresses = [(\\"John Doe\\", \\"johndoe@example.com\\"), (\\"Jane Smith\\", \\"janesmith@example.com\\")] timestamp = 1635761223 # Example timestamp corresponding to Sun, 31 Oct 2021 09:20:23 +0000 msg_id_string = \\"custommsg123\\" result = prepare_email_headers(addresses, timestamp, msg_id_string) print(result) # Expected output (date string may vary based on your localtime settings): # { # \'addresses\': [\'John Doe <johndoe@example.com>\', \'Jane Smith <janesmith@example.com>\'], # \'Message-ID\': \'<custommsg123@yourhostname.com>\', # \'Date\': \'Sun, 31 Oct 2021 09:20:23 +0000\' # } ``` # Constraints: - Implement proper error handling for invalid email addresses. - Ensure the email addresses are formatted correctly as per RFC 2822. - Assume `email.utils` functions and necessary modules (like `time`) are imported for your use. # Performance requirement: - The function should efficiently handle up to 1000 addresses.","solution":"from typing import List, Tuple, Optional, Dict import time from email.utils import formataddr, make_msgid, formatdate def prepare_email_headers(addresses: List[Tuple[str, str]], timestamp: Optional[float] = None, msg_id_string: Optional[str] = None) -> Dict[str, str]: Prepare email headers including formatted addresses, a unique message ID, and a formatted date. Parameters: addresses (list of tuples): Each tuple contains (realname, email_address). timestamp (float, optional): Time value for the email; defaults to current time. msg_id_string (str, optional): String to strengthen message ID; defaults to None. Returns: dict: Contains \'addresses\', \'Message-ID\', and \'Date\'. if timestamp is None: timestamp = time.time() formatted_addresses = [formataddr(address) for address in addresses] message_id = make_msgid(domain=msg_id_string) date_string = formatdate(timeval=timestamp, localtime=True) return { \'addresses\': formatted_addresses, \'Message-ID\': message_id, \'Date\': date_string }"},{"question":"# Advanced Coding Assessment **Objective:** Create a function that extracts specific parts of a log entry using regular expressions. This problem will test your understanding of regular expressions, grouping, and extracting data from strings. **Problem Statement:** You are given a list of log entries from a server, each containing a timestamp, IP address, request type, path, and HTTP status code. Your task is to implement a function `parse_logs(logs: List[str]) -> List[Dict[str, str]]` that extracts specific parts of each log entry and returns them as a list of dictionaries. Each dictionary should have the keys `\'timestamp\'`, `\'ip_address\'`, `\'request_type\'`, `\'path\'`, and `\'status_code\'`. **Input:** - `logs`: a list of strings where each string is a log entry. Each log entry follows this format: ``` [DATE-TIME STAMP] IP_ADDRESS REQUEST_METHOD PATH HTTP_STATUS_CODE ``` For example: ``` [2023-01-15 15:23:01] 192.168.1.1 GET /index.html 200 [2023-01-15 15:24:22] 192.168.1.2 POST /submit-form 404 ``` **Output:** - A list of dictionaries. Each dictionary should contain the extracted components with keys `\'timestamp\'`, `\'ip_address\'`, `\'request_type\'`, `\'path\'`, and `\'status_code\'`. **Constraints:** 1. Each log entry follows the exact format as given above. 2. You may assume that all entries are correctly formatted. 3. The list `logs` is non-empty. **Example:** ```python logs = [ \\"[2023-01-15 15:23:01] 192.168.1.1 GET /index.html 200\\", \\"[2023-01-15 15:24:22] 192.168.1.2 POST /submit-form 404\\" ] expected_output = [ { \\"timestamp\\": \\"2023-01-15 15:23:01\\", \\"ip_address\\": \\"192.168.1.1\\", \\"request_type\\": \\"GET\\", \\"path\\": \\"/index.html\\", \\"status_code\\": \\"200\\" }, { \\"timestamp\\": \\"2023-01-15 15:24:22\\", \\"ip_address\\": \\"192.168.1.2\\", \\"request_type\\": \\"POST\\", \\"path\\": \\"/submit-form\\", \\"status_code\\": \\"404\\" } ] assert parse_logs(logs) == expected_output ``` **Instructions:** 1. Implement the function `parse_logs`. 2. Use regular expressions to extract the desired parts from each log entry. 3. Ensure your regular expression handles the format accurately. **Hint:** You can use named groups in your regular expression to make the extraction process more readable and maintainable. **Requirements:** - Use the `re` module for regular expression operations. - The function should handle logs efficiently.","solution":"import re def parse_logs(logs): Extracts specific parts of each log entry and returns them as a list of dictionaries with keys \'timestamp\', \'ip_address\', \'request_type\', \'path\', and \'status_code\'. log_pattern = re.compile(r\'[(?P<timestamp>.*?)] (?P<ip_address>S+) (?P<request_type>S+) (?P<path>S+) (?P<status_code>d+)\') extracted_logs = [] for log in logs: match = log_pattern.match(log) if match: extracted_logs.append(match.groupdict()) return extracted_logs"},{"question":"Implement a text editor using the `curses` module, with the following features: 1. **Window Setup**: Create a main window for editing text and a status bar at the bottom to display helpful information (e.g., \\"Press Ctrl-G to exit\\"). 2. **Text Editing**: Support basic text editing functionality within the main window, including: - Moving the cursor (arrows keys or `Ctrl-B`, `Ctrl-F`, `Ctrl-P`, `Ctrl-N`). - Inserting and deleting characters (`Ctrl-D` to delete character under cursor, `Ctrl-H` for backspace). - New lines (`Enter` key for new line). 3. **Status Bar**: Display the current cursor position on the status bar. 4. **File Operations**: Implement functionality to save the current text to a file (`Ctrl-S` to save). Upon pressing `Ctrl-S`, the user should be prompted for a file name. 5. **Exit**: Allow the user to exit the editor gracefully (`Ctrl-G` to exit). # Requirements - **Input and Output**: - The main editing window should accept user input and display the text. - The status bar should display messages to assist the user and show the current cursor position. - Saving the file should result in the text being written to the specified file. # Constraints - Ensure that the program handles terminal resizing events appropriately. - Prevent the cursor from moving outside the bounds of the editing window. - Use appropriate exception handling to manage errors (e.g., when saving a file). # Performance - The application should efficiently handle input and screen refreshes to maintain a smooth user experience. Here is the skeleton code to get started: ```python import curses import curses.textpad def main(stdscr): # Clear screen curses.curs_set(1) stdscr.clear() stdscr.refresh() # Initialize windows for editor and status bar max_y, max_x = stdscr.getmaxyx() editor_win = curses.newwin(max_y-1, max_x, 0, 0) status_bar = curses.newwin(1, max_x, max_y-1, 0) # Setup editor with Textbox editor = curses.textpad.Textbox(editor_win) # Status bar message and cursor position status_message = \\"Press Ctrl-G to exit | Ctrl-S to save | Cursor: \\" while True: # Update status bar _, cursor_x = editor_win.getyx() status_bar.addstr(0, 0, status_message + f\\"{cursor_x}\\") status_bar.refresh() # Handle editor and input ch = editor_win.getch() if ch == 7: # Ctrl-G for exit break elif ch == 19: # Ctrl-S for save curses.curs_set(0) # Hide cursor for input status_bar.clear() status_bar.addstr(0, 0, \\"Enter filename: \\") status_bar.refresh() filewin = curses.newwin(1, max_x - 15, max_y-1, 15) box = curses.textpad.Textbox(filewin) filename = box.edit() with open(filename.strip(), \'w\') as f: f.write(editor.gather()) status_bar.clear() status_bar.addstr(0, 0, \\"Saved!\\") curses.curs_set(1) # Show cursor again else: editor.do_command(ch) # Handle other text editor commands curses.endwin() if __name__ == \\"__main__\\": curses.wrapper(main) ``` Complete the code above to fully implement all required features.","solution":"import curses import curses.textpad def main(stdscr): # Initialize curses curses.curs_set(1) stdscr.clear() stdscr.refresh() # Initialize windows max_y, max_x = stdscr.getmaxyx() editor_win = curses.newwin(max_y - 1, max_x, 0, 0) status_bar = curses.newwin(1, max_x, max_y - 1, 0) # Setup editor with Textbox editor = curses.textpad.Textbox(editor_win) # Status bar message status_message = \\"Press Ctrl-G to exit | Ctrl-S to save | Cursor: \\" while True: # Update status bar cursor_y, cursor_x = editor_win.getyx() status_bar.clear() status_bar.addstr(0, 0, status_message + f\\"{cursor_y}, {cursor_x}\\") status_bar.refresh() # Handle editor input ch = editor_win.getch() if ch == 7: # Ctrl-G for exit break elif ch == 19: # Ctrl-S for save curses.curs_set(0) # Hide cursor for input status_bar.clear() status_bar.addstr(0, 0, \\"Enter filename: \\") status_bar.refresh() filewin = curses.newwin(1, max_x - 15, max_y - 1, 15) box = curses.textpad.Textbox(filewin) filename = box.edit().strip() with open(filename, \'w\') as f: f.write(editor.gather()) status_bar.clear() status_bar.addstr(0, 0, \\"Saved! Press any key to continue...\\") status_bar.refresh() editor_win.getch() status_bar.clear() curses.curs_set(1) # Show cursor again else: # Implement additional key mappings if needed editor.do_command(ch) curses.endwin() if __name__ == \\"__main__\\": curses.wrapper(main)"},{"question":"# Datetime and Time Zone Handling Assessment Objective You are required to write a function that performs various operations using Python\'s `datetime` and `zoneinfo` modules, demonstrating an understanding of both fundamental and advanced concepts in handling dates, times, and time zones. Problem Statement Write a function `calculate_event_times` that accepts the following parameters: - `event_time`: A string representing a naive datetime of an event in the format `%Y-%m-%d %H:%M:%S`. (e.g., `\\"2023-11-15 13:45:00\\"`) - `event_timezone`: A string representing the time zone of the `event_time`. (e.g., `\\"America/New_York\\"`) - `target_timezones`: A list of strings where each string is a time zone where you want to calculate the corresponding event time. (e.g., `[\\"Europe/London\\", \\"Asia/Tokyo\\"]`) The function should: 1. Convert the naive `event_time` into an aware `datetime` object using the provided `event_timezone`. 2. Calculate the equivalent event times in the provided `target_timezones`. 3. Return a dictionary where the keys are the `target_timezones` and the values are the corresponding event times as strings in the format `%Y-%m-%d %H:%M:%S`. Input - `event_time`: A string of the format `%Y-%m-%d %H:%M:%S`. - `event_timezone`: A string representing the time zone of the event. - `target_timezones`: A list of strings each representing a time zone for which the event time needs to be calculated. Output A dictionary where keys are the time zones from `target_timezones` and values are strings representing the event times in those time zones, in the format `%Y-%m-%d %H:%M:%S`. Constraints - The `event_time` will always be a valid datetime in the format `%Y-%m-%d %H:%M:%S`. - The `event_timezone` and `target_timezones` will always be valid time zones that can be recognized by the `zoneinfo` module. - The list `target_timezones` will contain at least one and at most ten items. Example ```python from zoneinfo import ZoneInfo from datetime import datetime def calculate_event_times(event_time, event_timezone, target_timezones): # Your implementation here # Example usage: event_time = \\"2023-11-15 13:45:00\\" event_timezone = \\"America/New_York\\" target_timezones = [\\"Europe/London\\", \\"Asia/Tokyo\\"] event_times = calculate_event_times(event_time, event_timezone, target_timezones) print(event_times) # Output should be similar to: # {\'Europe/London\': \'2023-11-15 18:45:00\', \'Asia/Tokyo\': \'2023-11-16 03:45:00\'} ``` Ensure your function handles time zone conversions accurately and accounts for differences in daylight saving time where applicable. Evaluation Criteria - **Correctness**: The function returns the correct event times for given target time zones. - **Efficiency**: The function performs well within the problem constraints. - **Code Quality**: The code is well-structured, readable, and follows best practices.","solution":"from datetime import datetime from zoneinfo import ZoneInfo def calculate_event_times(event_time, event_timezone, target_timezones): # Parse the naive datetime string into a naive datetime object naive_event_time = datetime.strptime(event_time, \'%Y-%m-%d %H:%M:%S\') # Convert the naive datetime object to an aware datetime object using event_timezone aware_event_time = naive_event_time.replace(tzinfo=ZoneInfo(event_timezone)) # Create a dictionary to store the converted times converted_times = {} # Calculate the equivalent event times in the target timezones for tz in target_timezones: target_time = aware_event_time.astimezone(ZoneInfo(tz)) converted_times[tz] = target_time.strftime(\'%Y-%m-%d %H:%M:%S\') return converted_times"},{"question":"Problem Statement You are tasked with developing a utility function that analyzes a directory\'s structure and contents, gathering specific information and summarizing it. This will require comprehension of various `os.path` functions to retrieve and manipulate path data. # Function Signature ```python def summarize_directory_structure(directory: str) -> dict: Summarize the structure and attributes of the given directory. Parameters: directory (str): The path to the directory to summarize. Returns: dict: A summary dictionary containing the following keys: - \\"total_files\\" (int): Total number of files in the directory. - \\"total_size\\" (int): Total size of all files in bytes. - \\"oldest_file\\" (str): Path to the oldest file by last modification time. - \\"largest_file\\" (str): Path to the largest file by size. - \\"common_prefix\\" (str): The common prefix of all file paths. pass ``` # Input - `directory`: A string representing the path to the directory to analyze. It can be either an absolute or relative path. You may assume the directory exists and is accessible. # Output - A dictionary containing the summarized information about the directory structure: - `total_files`: An integer representing the total number of files (not directories) in the directory. - `total_size`: An integer representing the total size of all files in bytes. - `oldest_file`: A string representing the path to the oldest file in the directory by last modification time. If multiple files have the same oldest modification time, return any one of them. - `largest_file`: A string representing the path to the largest file in the directory by size. If multiple files have the same largest size, return any one of them. - `common_prefix`: A string representing the common prefix path of all file paths in the directory. # Constraints 1. You may assume that the directory exists and is accessible. 2. You may assume the directory can contain nested subdirectories. # Example ```python # Suppose the directory structure is as follows: # test_dir/ # ├── file1.txt (size: 100 bytes, last modified: time1) # ├── file2.txt (size: 200 bytes, last modified: time2) # ├── subdir1/ # │ ├── file3.txt (size: 300 bytes, last modified: time3) # └── subdir2/ # └── file4.txt (size: 150 bytes, last modified: time4) result = summarize_directory_structure(\'test_dir\') print(result) # Expected result (actual paths may vary depending on your environment): # { # \'total_files\': 4, # \'total_size\': 750, # \'oldest_file\': \'path_to_oldest_file\', # \'largest_file\': \'path_to_largest_file\', # \'common_prefix\': \'test_dir\' # } ``` # Note - Ensure your implementation efficiently traverses and processes the directory contents. - Handle edge cases, such as directories with no files, non-uniform path lengths, etc.","solution":"import os def summarize_directory_structure(directory: str) -> dict: total_files = 0 total_size = 0 oldest_file = None oldest_time = None largest_file = None largest_size = 0 all_file_paths = [] for root, _, files in os.walk(directory): for file in files: file_path = os.path.join(root, file) file_stat = os.stat(file_path) # Update total files and total size total_files += 1 total_size += file_stat.st_size # Find the oldest file if oldest_file is None or file_stat.st_mtime < oldest_time: oldest_file = file_path oldest_time = file_stat.st_mtime # Find the largest file if file_stat.st_size > largest_size: largest_file = file_path largest_size = file_stat.st_size all_file_paths.append(file_path) # Get common prefix common_prefix = os.path.commonpath(all_file_paths) if all_file_paths else \\"\\" summary = { \\"total_files\\": total_files, \\"total_size\\": total_size, \\"oldest_file\\": oldest_file, \\"largest_file\\": largest_file, \\"common_prefix\\": common_prefix } return summary"},{"question":"Problem Statement You are tasked with developing a Python function to handle email creation and parsing using Python\'s `email` module. The objective is to write two main functions that demonstrate the functionality of creating an email and then parsing an email message to extract its contents. # Functions to Implement 1. `create_email(subject: str, sender: str, recipient: str, body: str) -> str` 2. `parse_email(raw_email: str) -> tuple` # Function Specifications 1. **create_email** - **Input:** - `subject` (str): The subject of the email. - `sender` (str): The sender\'s email address. - `recipient` (str): The recipient\'s email address. - `body` (str): The main body text of the email. - **Output:** - A string representation of the raw email message. - **Example:** ```python raw_email = create_email( subject=\\"Test Email\\", sender=\\"sender@example.com\\", recipient=\\"recipient@example.com\\", body=\\"This is a test email.\\" ) print(raw_email) ``` 2. **parse_email** - **Input:** - `raw_email` (str): The raw string representation of an email message. - **Output:** - A tuple containing four elements: - The subject of the email (str). - The sender\'s email address (str). - The recipient\'s email address (str). - The body text of the email (str). - **Example:** ```python raw_email = Subject: Test Email From: sender@example.com To: recipient@example.com This is a test email. email_contents = parse_email(raw_email) print(email_contents) # Output: (\'Test Email\', \'sender@example.com\', \'recipient@example.com\', \'This is a test email.\') ``` # Constraints - You may assume the input email will always follow proper MIME formatting rules. - You should handle emails with basic text content only; no attachments or HTML content are required. - The functions should utilize Python\'s `email` module as much as possible. # Notes - For `create_email`, you may need to investigate the use of `email.message.EmailMessage` and `email.generator`. - For `parse_email`, you may need to use `email.parser` to correctly interpret the raw email string. # Assessment Criteria - Correctness of the implemented functions. - Proper use of the `email` module to demonstrate a deep understanding of creating and parsing email messages. - Clear and maintainable code. Good luck, and happy coding!","solution":"from email.message import EmailMessage from email.parser import Parser def create_email(subject: str, sender: str, recipient: str, body: str) -> str: Creates a raw email message string with the specified subject, sender, recipient, and body. msg = EmailMessage() msg.set_content(body) msg[\'Subject\'] = subject msg[\'From\'] = sender msg[\'To\'] = recipient return msg.as_string() def parse_email(raw_email: str) -> tuple: Parses a raw email message string and returns a tuple containing the subject, sender, recipient, and body. parser = Parser() msg = parser.parsestr(raw_email) subject = msg[\'Subject\'] sender = msg[\'From\'] recipient = msg[\'To\'] body = msg.get_payload() return subject, sender, recipient, body"},{"question":"**Objective**: This question will assess your understanding of the `importlib.metadata` module for working with metadata of installed Python packages. # Problem Statement You are provided with the name of an installed package, and your task is to write a function `package_info(package_name)` that returns detailed information about the package. Your function should: 1. Retrieve the version of the package. 2. Retrieve the metadata of the package, specifically extracting the \'Author\', \'License\', and \'Requires-Python\' information. 3. List the names of all the entry points in the `console_scripts` group. 4. List all the file paths included with the package. # Function Signature ```python def package_info(package_name: str) -> dict: # Your code here ``` # Input - `package_name` (str): The name of the installed package. # Output - A dictionary with the following structure: ```python { \\"version\\": str, \\"author\\": str, \\"license\\": str, \\"requires_python\\": str, \\"entry_points\\": List[str], \\"files\\": List[str] } ``` - `\\"version\\"`: The version of the package. - `\\"author\\"`: The author of the package. - `\\"license\\"`: The license of the package. - `\\"requires_python\\"`: The Python version requirement of the package. - `\\"entry_points\\"`: List of names of the entry points in the `console_scripts` group. - `\\"files\\"`: List of all file paths included with the package. # Constraints - You can assume the package specified in `package_name` is installed in the environment. - Handle cases where some metadata like \'Author\' or \'License\' might be missing by returning `\\"Unknown\\"`. # Example ```python result = package_info(\\"wheel\\") print(result) # Output might be: # { # \\"version\\": \\"0.32.3\\", # \\"author\\": \\"Daniel Holth\\", # \\"license\\": \\"MIT\\", # \\"requires_python\\": \\">=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*\\", # \\"entry_points\\": [\\"wheel\\"], # \\"files\\": [\\"path/to/wheel/__init__.py\\", \\"path/to/wheel/__main__.py\\", ...] # } ``` # Notes - Use the functions `importlib.metadata.version()`, `importlib.metadata.metadata()`, `importlib.metadata.entry_points()`, and `importlib.metadata.files()` to achieve the requirements.","solution":"import importlib.metadata from typing import List, Dict def package_info(package_name: str) -> Dict[str, str]: info = {} try: info[\\"version\\"] = importlib.metadata.version(package_name) except importlib.metadata.PackageNotFoundError: info[\\"version\\"] = \\"Unknown\\" try: metadata = importlib.metadata.metadata(package_name) info[\\"author\\"] = metadata.get(\\"Author\\", \\"Unknown\\") info[\\"license\\"] = metadata.get(\\"License\\", \\"Unknown\\") info[\\"requires_python\\"] = metadata.get(\\"Requires-Python\\", \\"Unknown\\") except importlib.metadata.PackageNotFoundError: info[\\"author\\"] = \\"Unknown\\" info[\\"license\\"] = \\"Unknown\\" info[\\"requires_python\\"] = \\"Unknown\\" try: entry_points = importlib.metadata.entry_points() info[\\"entry_points\\"] = [entry.name for entry in entry_points.get(\\"console_scripts\\", []) if entry.dist.name == package_name] except importlib.metadata.PackageNotFoundError: info[\\"entry_points\\"] = [] try: files = importlib.metadata.files(package_name) if files: info[\\"files\\"] = [str(file) for file in files] else: info[\\"files\\"] = [] except importlib.metadata.PackageNotFoundError: info[\\"files\\"] = [] return info"},{"question":"**Objective:** Implement a system using weak references that manages a collection of objects with life-cycle event callbacks. This assessment will test your understanding of weak references in Python and your ability to use callback functions effectively. **Background:** You will use Python\'s `weakref` module for this task. A weak reference allows the referenced object to be collected by the garbage collector when the object is no longer needed, while still allowing a callback to be invoked. **Task:** 1. Implement a class `ManagedCollection` that allows the addition and removal of objects. 2. When an object is added, a weak reference to the object should be created. If the object is garbage collected, a specified callback function should be executed. 3. Implement a method `remove_collected_objects` that removes weak references to objects that have been garbage collected. 4. Implement a method `get_active_objects` that returns a list of currently active (non-garbage collected) objects. **Specifications:** - **Class Name:** `ManagedCollection` - **Methods:** - `add_object(obj, callback)`: Adds an object with a weak reference and specifies a callback function to be called when the object is garbage collected. `obj` is the object to be added, and `callback` is the callback function. - `remove_object(obj)`: Removes an object from the collection. - `remove_collected_objects()`: Removes references to all objects that have been collected by the garbage collector. - `get_active_objects()`: Returns a list of all objects that have not been collected. - **Input:** - `add_object(obj, callback)`: `obj` (any Python object) and `callback` (a function with signature `callback(weak_ref)`). - `remove_object(obj)`: `obj` (any Python object). - `remove_collected_objects()`: No parameters. - `get_active_objects()`: No parameters. - **Output:** - `add_object(obj, callback)` and `remove_object(obj)`: No return value. - `remove_collected_objects()`: No return value. - `get_active_objects()`: List of active objects. **Example Usage:** ```python import weakref def on_garbage_collected(weak_ref): print(f\\"Object {weak_ref} has been garbage collected.\\") class SomeClass: pass collection = ManagedCollection() obj1 = SomeClass() obj2 = SomeClass() collection.add_object(obj1, on_garbage_collected) collection.add_object(obj2, on_garbage_collected) print(collection.get_active_objects()) # [<SomeClass object at ...>, <SomeClass object at ...>] collection.remove_object(obj1) print(collection.get_active_objects()) # [<SomeClass object at ...>] obj2 = None # obj2 is now eligible for garbage collection collection.remove_collected_objects() print(collection.get_active_objects()) # [] ``` **Constraints:** - The callback function should handle the case where the referent has already been garbage collected gracefully. - Use Python\'s built-in `weakref` module. - Ensure the solution is efficient and does not cause significant overhead. - Do not use any libraries outside of the standard Python library. **Evaluation Criteria:** - Correctness of the implementation. - Proper use of weak references. - Handling of callback functions. - Code readability and adherence to Python best practices.","solution":"import weakref class ManagedCollection: def __init__(self): self.references = [] def add_object(self, obj, callback): weak_ref = weakref.ref(obj, callback) self.references.append(weak_ref) def remove_object(self, obj): self.references = [ref for ref in self.references if ref() is not obj] def remove_collected_objects(self): self.references = [ref for ref in self.references if ref() is not None] def get_active_objects(self): return [ref() for ref in self.references if ref() is not None]"},{"question":"# Advanced Arithmetic Operations Simulator You are tasked with creating a Python class `AdvancedCalculator` that simulates various arithmetic operations as described in the Python310 documentation. The class should provide methods for: - Basic arithmetic operations (addition, subtraction, multiplication, division). - Advanced operations such as matrix multiplication and power. - Bitwise operations. - In-place arithmetic operations. - Type conversions. Functional Specifications: 1. **Class Name**: `AdvancedCalculator` 2. **Methods**: - `add(o1, o2)`: Returns the result of `o1 + o2`. - `subtract(o1, o2)`: Returns the result of `o1 - o2`. - `multiply(o1, o2)`: Returns the result of `o1 * o2`. - `floor_divide(o1, o2)`: Returns the floor result of `o1 // o2`. - `true_divide(o1, o2)`: Returns the true division result of `o1 / o2`. - `remainder(o1, o2)`: Returns the remainder of `o1 % o2`. - `matrix_multiply(o1, o2)`: Returns the result of matrix multiplication `o1 @ o2`. - `power(o1, o2, o3=None)`: Returns `o1` raised to the power of `o2` if `o3` is `None`, else returns `pow(o1, o2, o3)`. - `bitwise_and(o1, o2)`: Returns the result of `o1 & o2`. - `bitwise_or(o1, o2)`: Returns the result of `o1 | o2`. - `bitwise_xor(o1, o2)`: Returns the result of `o1 ^ o2`. - `bitwise_invert(o)`: Returns the result of `~o`. - `left_shift(o1, o2)`: Returns the result of `o1 << o2`. - `right_shift(o1, o2)`: Returns the result of `o1 >> o2`. - `inplace_add(o1, o2)`: Adds `o2` to `o1` in place and returns the result. - `inplace_subtract(o1, o2)`: Subtracts `o2` from `o1` in place and returns the result. - `inplace_multiply(o1, o2)`: Multiplies `o1` by `o2` in place and returns the result. - `float_conversion(o)`: Converts `o` to a float. - `integer_conversion(o)`: Converts `o` to an integer. - `integer_to_base(n, base)`: Converts the integer `n` to a string in the given base. 3. **Input and Output**: - Each method takes appropriate inputs as specified. - The result should mimic the corresponding Python built-in operator or function. 4. **Constraints**: - Inputs will be valid Python objects that support the operations. Example Implementation: ```python class AdvancedCalculator: def add(self, o1, o2): return o1 + o2 def subtract(self, o1, o2): return o1 - o2 def multiply(self, o1, o2): return o1 * o2 def floor_divide(self, o1, o2): return o1 // o2 def true_divide(self, o1, o2): return o1 / o2 def remainder(self, o1, o2): return o1 % o2 def matrix_multiply(self, o1, o2): return o1 @ o2 def power(self, o1, o2, o3=None): if o3 is None: return pow(o1, o2) else: return pow(o1, o2, o3) def bitwise_and(self, o1, o2): return o1 & o2 def bitwise_or(self, o1, o2): return o1 | o2 def bitwise_xor(self, o1, o2): return o1 ^ o2 def bitwise_invert(self, o): return ~o def left_shift(self, o1, o2): return o1 << o2 def right_shift(self, o1, o2): return o1 >> o2 def inplace_add(self, o1, o2): o1 += o2 return o1 def inplace_subtract(self, o1, o2): o1 -= o2 return o1 def inplace_multiply(self, o1, o2): o1 *= o2 return o1 def float_conversion(self, o): return float(o) def integer_conversion(self, o): return int(o) def integer_to_base(self, n, base): if base == 2: return bin(n) elif base == 8: return oct(n) elif base == 10: return str(n) elif base == 16: return hex(n) # Example usage calc = AdvancedCalculator() print(calc.add(2, 3)) # Output: 5 print(calc.power(2, 3)) # Output: 8 ``` You are required to implement this class in its entirety and ensure that each method works as intended.","solution":"class AdvancedCalculator: def add(self, o1, o2): return o1 + o2 def subtract(self, o1, o2): return o1 - o2 def multiply(self, o1, o2): return o1 * o2 def floor_divide(self, o1, o2): return o1 // o2 def true_divide(self, o1, o2): return o1 / o2 def remainder(self, o1, o2): return o1 % o2 def matrix_multiply(self, o1, o2): return o1 @ o2 def power(self, o1, o2, o3=None): if o3 is None: return pow(o1, o2) else: return pow(o1, o2, o3) def bitwise_and(self, o1, o2): return o1 & o2 def bitwise_or(self, o1, o2): return o1 | o2 def bitwise_xor(self, o1, o2): return o1 ^ o2 def bitwise_invert(self, o): return ~o def left_shift(self, o1, o2): return o1 << o2 def right_shift(self, o1, o2): return o1 >> o2 def inplace_add(self, o1, o2): o1 += o2 return o1 def inplace_subtract(self, o1, o2): o1 -= o2 return o1 def inplace_multiply(self, o1, o2): o1 *= o2 return o1 def float_conversion(self, o): return float(o) def integer_conversion(self, o): return int(o) def integer_to_base(self, n, base): if base == 2: return bin(n) elif base == 8: return oct(n) elif base == 10: return str(n) elif base == 16: return hex(n)"},{"question":"**Objective:** Implement a program that dynamically executes a given Python script and returns the sum of integer variables defined in the script. Use the functions described in the provided `python310` library\'s documentation to achieve this. # Problem Statement: You are tasked with creating a Python function `execute_and_sum_integers(script: str) -> int` that takes a string `script` as input. This string represents the content of a Python script. The function should execute this script and return the sum of all integer variables defined in the script. Use appropriate `PyRun_` and `PyEval_` functions as described in the `python310` documentation. # Input: - The input is a single string `script` representing a Python script. # Output: - The output should be an integer which is the sum of all integer variables defined in the script. # Constraints: - The script will only contain valid Python code. - The script will not modify or delete variables after defining them. - The script will only use standard Python libraries. - All integer variables will be defined at the global scope. - No functions beyond the initial `execute_and_sum_integers` that you create can be used to solve the problem. # Example: ```python script = \\"\\"\\" a = 2 b = 3 c = 5 d = \'not_an_integer\' \\"\\"\\" assert execute_and_sum_integers(script) == 10 ``` # Implementation Notes: 1. Use `PyRun_String` to execute the script. 2. Retrieve the global variables and check their types. 3. Sum up all the integer variables you find. You may assume the following utility functions and objects: ```python class PyCompilerFlags: def __init__(self): self.cf_flags = 0 self.cf_feature_version = 0 Py_eval_input = ... Py_file_input = ... Py_single_input = ... PyRun_String = ... PyRun_StringFlags = ... PyObject = ... PyEval_EvalCode = ... PyEval_EvalCodeEx = ... ``` # Function Signature: ```python def execute_and_sum_integers(script: str) -> int: pass ``` **Hint:** The crucial part of the implementation involves intercepting the execution environment after the script has run and evaluating all defined global variables.","solution":"def execute_and_sum_integers(script: str) -> int: Executes a given Python script and returns the sum of all integer variables defined in it. :param script: A string representing the content of a Python script. :return: The sum of all integer variables defined in the script. global_dict = {} exec(script, global_dict) total = 0 for key, value in global_dict.items(): if isinstance(value, int): total += value return total"},{"question":"**Question: Implementing a Custom Web Browser Opener** You are tasked with implementing a custom URL opener using the `webbrowser` module in Python. This custom opener will attempt to open a URL in a list of preferred browsers in sequence until it succeeds or exhausts the list. # Function Signature ```python def custom_open_url(url: str, preferred_browsers: List[str]) -> bool: pass ``` # Description Implement the function `custom_open_url` that accepts two parameters: - `url` (str): The URL to be opened. - `preferred_browsers` (List[str]): A list of browser type names (as strings, e.g., `[\'chrome\', \'firefox\', \'opera\']`), in order of preference. The function should attempt to open the URL using each browser specified in `preferred_browsers` until it succeeds or exhausts the list. # Return Value The function should return a boolean: - `True` if the URL was successfully opened in one of the preferred browsers. - `False` if the URL could not be opened in any of the preferred browsers. # Constraints - The function should handle exceptions that may arise from attempting to open the URL in each browser. - The function should use the `webbrowser` module\'s `get()` function to obtain a browser controller for each specified browser type. - If a browser type is not recognized or results in an exception, the function should move on to the next browser in the list. # Example ```python url = \\"https://www.example.com\\" preferred_browsers = [\\"firefox\\", \\"chrome\\", \\"safari\\"] result = custom_open_url(url, preferred_browsers) print(result) # Output will be True if the URL was opened successfully in one of the preferred browsers, or False otherwise. ``` # Notes - You can assume that `webbrowser` functions will behave as documented. - Ensure that your code is robust and handles cases where a specific browser might fail to open the URL.","solution":"import webbrowser from typing import List def custom_open_url(url: str, preferred_browsers: List[str]) -> bool: Attempts to open a URL in a list of preferred browsers in sequence until successful. Parameters: - url (str): The URL to be opened. - preferred_browsers (List[str]): A list of browser type names (as strings), in order of preference. Returns: - bool: True if the URL was successfully opened, False otherwise. for browser in preferred_browsers: try: browser_controller = webbrowser.get(browser) if browser_controller.open(url): return True except Exception as e: # If there\'s an exception, log it for debugging purposes (optional) # But continue to the next browser pass return False"},{"question":"You are provided with an AIFF audio file named `input_audio.aiff`. Your task is to write a Python function that reads this audio file, increases the playback speed by doubling the frame rate, and saves the result as `output_audio.aiff`. Function Signature: ```python def process_audio(input_filepath: str, output_filepath: str) -> None: ``` Input: - `input_filepath` (str): The filepath to the input AIFF audio file. For the purpose of this assessment, it will always be `\'input_audio.aiff\'`. - `output_filepath` (str): The filepath where the output AIFF audio file should be saved. For this assessment, it will always be `\'output_audio.aiff\'`. Output: - The function should create an AIFF file named `output_audio.aiff` with the manipulated audio. Constraints: - You must use the `aifc` module for all audio file operations. - The function should handle audio data in a way that preserves the number of channels and sample width, but doubles the frame rate. Example: Suppose `input_audio.aiff` has the following properties: - Number of channels: 2 (stereo) - Sample width: 2 bytes - Frame rate: 22050 frames/second - Number of frames: 44100 frames After processing the file, `output_audio.aiff` should have: - The same number of channels: 2 (stereo) - The same sample width: 2 bytes - A doubled frame rate: 44100 frames/second - The same total duration (hence half the number of frames, since playback speed increases): 22050 frames Note: - In order to modify the frame rate, you will need to read all the frames from the input file, and then write them into the output file after setting the desired properties. - Ensure the AIFF file headers are correctly modified to reflect the new frame rate and frame count. Implementation: You can start by reading the audio file and the properties using the `aifc` module, then modify the frame rate and write out the manipulated data into a new audio file. ```python import aifc def process_audio(input_filepath: str, output_filepath: str) -> None: with aifc.open(input_filepath, \'r\') as input_file: # Retrieve parameters from the input file n_channels = input_file.getnchannels() samp_width = input_file.getsampwidth() frame_rate = input_file.getframerate() n_frames = input_file.getnframes() comp_type = input_file.getcomptype() comp_name = input_file.getcompname() # Read all frames from the input file audio_data = input_file.readframes(n_frames) # Double the frame rate new_frame_rate = frame_rate * 2 # Frames count should be half since playback speed is doubled new_n_frames = n_frames // 2 with aifc.open(output_filepath, \'w\') as output_file: # Set the new parameters for the output file output_file.setnchannels(n_channels) output_file.setsampwidth(samp_width) output_file.setframerate(new_frame_rate) output_file.setnframes(new_n_frames) output_file.setcomptype(comp_type, comp_name) # Write the audio data to the output file output_file.writeframes(audio_data) ```","solution":"import aifc def process_audio(input_filepath: str, output_filepath: str) -> None: with aifc.open(input_filepath, \'r\') as input_file: # Retrieve parameters from the input file nchannels = input_file.getnchannels() sampwidth = input_file.getsampwidth() framerate = input_file.getframerate() nframes = input_file.getnframes() comptype = input_file.getcomptype() compname = input_file.getcompname() # Read all frames from the input file data = input_file.readframes(nframes) # Double the frame rate new_framerate = framerate * 2 with aifc.open(output_filepath, \'w\') as output_file: # Set the new parameters for the output file output_file.setnchannels(nchannels) output_file.setsampwidth(sampwidth) output_file.setframerate(new_framerate) output_file.setcomptype(comptype, compname) # Write the audio data to the output file output_file.writeframes(data)"},{"question":"# Seaborn Clustermap Challenge You are given a dataset and you need to create a clustermap using the seaborn package, demonstrating your understanding of various features and customizations. Follow the instructions below to achieve the required clustermap and answer the questions: Instructions: 1. **Dataset Preparation**: - Load the built-in seaborn dataset \\"titanic\\". - Select only the numerical columns (\'age\', \'fare\', \'survived\', \'pclass\') for clustering purposes. 2. **Preprocessing**: - Handle missing values in the dataset by filling them with the mean of the respective column. - Standardize the values within columns. 3. **Clustermap Creation**: - Set a seaborn theme of your choice. - Create a clustermap of the processed dataset. - Cluster only the columns (do not cluster rows). - Use `metric=\'euclidean\'` and `method=\'average\'`. 4. **Customization**: - Set a figure size of (10, 8). - Use the \'coolwarm\' colormap, and set the color range (`vmin`) starting from 0. - Add colored labels to rows based on the \'survived\' column, using green for survived (value 1) and red for not survived (value 0). - Adjust the color bar to be on the right side with appropriate adjustments. Expected Output: - A clustermap that is correctly formatted according to the provided specifications, displaying clustered columns, colored row labels, and the customizations mentioned. Constraints: - You must use the seaborn package. - Follow the instructions exactly for data preprocessing and clustermap creation. Example Code Structure: ```python import seaborn as sns import pandas as pd import numpy as np # Step 1: Load the dataset titanic = sns.load_dataset(\\"titanic\\") # Step 2: Select numerical columns and handle missing data (fill with mean) # Step 3: Standardize the columns # Step 4: Set seaborn theme sns.set_theme() # Step 5: Create clustermap # Include row labels based on the \'survived\' column and customize the appearance ``` Fill in the code to complete the task as described. Ensure the final plot displays as described in the instructions.","solution":"import seaborn as sns import pandas as pd import numpy as np from sklearn.preprocessing import StandardScaler import matplotlib.pyplot as plt def create_clustermap(): # Step 1: Load the dataset titanic = sns.load_dataset(\\"titanic\\") # Step 2: Select numerical columns and handle missing data (fill with mean) numerical_cols = [\'age\', \'fare\', \'survived\', \'pclass\'] titanic_numerical = titanic[numerical_cols] titanic_numerical = titanic_numerical.fillna(titanic_numerical.mean()) # Step 3: Standardize the columns scaler = StandardScaler() titanic_standardized = pd.DataFrame(scaler.fit_transform(titanic_numerical), columns=numerical_cols) # Step 4: Set seaborn theme sns.set_theme() # Step 5: Create clustermap survived_color_adjusted = titanic_numerical[\'survived\'].map({0: \'red\', 1: \'green\'}) g = sns.clustermap(titanic_standardized.drop(columns=[\'survived\']), metric=\'euclidean\', method=\'average\', col_cluster=True, row_cluster=False, cmap=\'coolwarm\', vmin=0, figsize=(10, 8), row_colors=survived_color_adjusted) # Adjust color bar cbar = g.cax cbar.set_position([.97, .1, .03, .4]) # Show plot plt.show()"},{"question":"XML Data Extraction and Processing You are required to process an XML document containing information about books in a library. The XML structure is as follows: ```xml <library> <book id=\\"1\\"> <title>Python Programming</title> <author>Jane Doe</author> <year>2020</year> <price>29.99</price> </book> <book id=\\"2\\"> <title>Data Science Essentials</title> <author>John Smith</author> <year>2018</year> <price>39.99</price> </book> <!-- More book entries --> </library> ``` Your task is to write a Python function `extract_books_over_price(xml_string, min_price)` that takes the following arguments: - `xml_string` (str): A string representation of the XML document. - `min_price` (float): The minimum price threshold. The function should parse the given XML string using the `xml.dom.pulldom` module and return a list of dictionaries containing the details (id, title, author, year, and price) of books whose price is greater than `min_price`. **Constraints**: - Do not use any other XML parsing libraries. - Ensure efficiency by expanding only the relevant nodes. - Assume well-formed XML input for this problem. # Function Signature ```python def extract_books_over_price(xml_string: str, min_price: float) -> list[dict]: pass ``` # Example ```python xml_string = \'\'\'<library> <book id=\\"1\\"> <title>Python Programming</title> <author>Jane Doe</author> <year>2020</year> <price>29.99</price> </book> <book id=\\"2\\"> <title>Data Science Essentials</title> <author>John Smith</author> <year>2018</year> <price>39.99</price> </book> </library>\'\'\' min_price = 30.00 result = extract_books_over_price(xml_string, min_price) print(result) # Expected Output: # [{\'id\': \'2\', \'title\': \'Data Science Essentials\', \'author\': \'John Smith\', \'year\': \'2018\', \'price\': 39.99}] ``` **Guidance**: 1. Use `xml.dom.pulldom.parseString` to parse the input XML string. 2. Iterate over the events using a loop and check for `START_ELEMENT` events related to \\"book\\". 3. Expand the \\"book\\" node when its price attribute exceeds `min_price`. 4. Extract and store the details in a dictionary and append to the result list.","solution":"from xml.dom import pulldom def extract_books_over_price(xml_string: str, min_price: float) -> list: doc = pulldom.parseString(xml_string) books = [] for event, node in doc: if event == pulldom.START_ELEMENT and node.tagName == \'book\': doc.expandNode(node) book_id = node.getAttribute(\'id\') title = node.getElementsByTagName(\'title\')[0].firstChild.data author = node.getElementsByTagName(\'author\')[0].firstChild.data year = node.getElementsByTagName(\'year\')[0].firstChild.data price = float(node.getElementsByTagName(\'price\')[0].firstChild.data) if price > min_price: books.append({ \'id\': book_id, \'title\': title, \'author\': author, \'year\': year, \'price\': price, }) return books"},{"question":"**Coding Assessment Question: Optimizing Neural Network Performance Using Persistent Algorithms in PyTorch** You are tasked to demonstrate your understanding of optimizing neural network training using PyTorch while ensuring the usage of persistent algorithms. # Instructions Implement a function `optimize_neural_network()` that: 1. Initializes a simple neural network using `torch.nn` modules. 2. Moves the input data and the model to the GPU. 3. Ensures the input data is of `torch.float16` dtype. 4. Confirms the use of V100 GPU. 5. Ensures input data is not in `PackedSequence` format. 6. Verifies that cudnn is enabled and the persistent algorithm is improving the performance. # Constraints 1. Use only the PyTorch library for neural network definitions and GPU manipulations. 2. The neural network should contain at least one `nn.Linear` layer. 3. Handle necessary exceptions when a V100 GPU is not available. # Expected Function Definition ```python def optimize_neural_network(): # Define a simple neural network class SimpleNet(torch.nn.Module): def __init__(self): super(SimpleNet, self).__init__() self.linear = torch.nn.Linear(10, 5) def forward(self, x): return self.linear(x) # Initialize the network model = SimpleNet() # Check if CUDA is available and cudnn is enabled if not torch.cuda.is_available() or not torch.backends.cudnn.enabled: raise RuntimeError(\\"CUDA is not available or cudnn is not enabled.\\") # Move the model to GPU device = torch.device(\\"cuda\\") model.to(device) # Ensure dtype is torch.float16 input_data = torch.randn((16, 10), dtype=torch.float16).to(device) # Check if using V100 GPU if not (\\"V100\\" in torch.cuda.get_device_name()): raise RuntimeError(\\"This script is optimized for V100 GPUs.\\") # Verify that the input data is not in PackedSequence format if isinstance(input_data, torch.nn.utils.rnn.PackedSequence): raise TypeError(\\"Input data should not be in PackedSequence format.\\") # Placeholder for performance verification # (Here you can add benchmarking code to check performance with and without persistent algorithms) return model, input_data # Example usage: # model, input_data = optimize_neural_network() # Verify output types # print(type(model)) # print(type(input_data)) ``` Note: To run the example usage, ensure you have access to a V100 GPU, and CUDA is enabled.","solution":"import torch def optimize_neural_network(): # Define a simple neural network class SimpleNet(torch.nn.Module): def __init__(self): super(SimpleNet, self).__init__() self.linear = torch.nn.Linear(10, 5) def forward(self, x): return self.linear(x) # Initialize the network model = SimpleNet() # Check if CUDA is available and cudnn is enabled if not torch.cuda.is_available() or not torch.backends.cudnn.enabled: raise RuntimeError(\\"CUDA is not available or cudnn is not enabled.\\") # Move the model to GPU device = torch.device(\\"cuda\\") model.to(device) # Ensure dtype is torch.float16 input_data = torch.randn((16, 10), dtype=torch.float16).to(device) # Check if using V100 GPU if not (\\"V100\\" in torch.cuda.get_device_name(0)): raise RuntimeError(\\"This script is optimized for V100 GPUs.\\") # Verify that the input data is not in PackedSequence format if isinstance(input_data, torch.nn.utils.rnn.PackedSequence): raise TypeError(\\"Input data should not be in PackedSequence format.\\") # Placeholder for performance verification # (Here you can add benchmarking code to check performance with and without persistent algorithms) return model, input_data"},{"question":"# Advanced Python Coding Assessment Problem Statement You are required to demonstrate your understanding of Python\'s `memoryview` object, which provides a way of dealing directly with the memory buffer interface for performance optimization. Task Write a Python function `create_memoryview_operations` that takes a list of integers as input and returns a dictionary with the following keys and their corresponding values: 1. `\\"view_from_object\\"`: A memoryview object created from the input list. 2. `\\"is_memoryview\\"`: A boolean indicating whether the created memoryview object is indeed a memoryview. 3. `\\"buffer_pointer\\"`: A pointer to the memoryview\'s private copy of the exporter\'s buffer. 4. `\\"exporting_object\\"`: A pointer to the exporting object or `None` if the memoryview was not created from an exporting object. Function Signature ```python def create_memoryview_operations(integers: list) -> dict: pass ``` Example ```python integers = [1, 2, 3, 4, 5] result = create_memoryview_operations(integers) # Expected Output: # { # \\"view_from_object\\": <memory at 0x...>, # \\"is_memoryview\\": True, # \\"buffer_pointer\\": <memory at 0x...>, # \\"exporting_object\\": [1, 2, 3, 4, 5] # } ``` Constraints - The input list `integers` will have at least one element and all elements will be non-negative integers. - You are advised to use the `memoryview` object and related methods to solve this task. Requirements - Your solution should demonstrate a clear understanding of the `memoryview` object and its functions. - Handle memory access efficiently and safely. Good luck!","solution":"def create_memoryview_operations(integers: list) -> dict: This function takes a list of integers and performs operations using memoryview. It returns a dictionary with specific keys containing results of the operations. if not integers: return { \\"view_from_object\\": None, \\"is_memoryview\\": False, \\"buffer_pointer\\": None, \\"exporting_object\\": None } # Creating the memoryview object mem_view = memoryview(bytearray(integers)) # Setting the dictionary with the specific keys result = { \\"view_from_object\\": mem_view, \\"is_memoryview\\": isinstance(mem_view, memoryview), \\"buffer_pointer\\": bytes(mem_view), \\"exporting_object\\": integers } return result"},{"question":"You are provided with a dataset containing information about different people\'s favorite fruits. The dataset has two columns: 1. `Name`: The name of the person. 2. `Fruit`: The person\'s favorite fruit. The dataset is as follows: ```python data = { \\"Name\\": [\\"Alice\\", \\"Bob\\", \\"Charles\\", \\"Diana\\", \\"Edward\\", \\"Fiona\\", \\"George\\"], \\"Fruit\\": [\\"Apple\\", \\"Banana\\", \\"Apple\\", \\"Banana\\", \\"Grapes\\", \\"Banana\\", \\"Apple\\"] } df = pd.DataFrame(data) ``` You need to perform the following operations on this dataset: 1. Convert the `Fruit` column to a categorical data type, with the categories ordered as `[\\"Apple\\", \\"Banana\\", \\"Grapes\\"]`. 2. Add a new category `Orange` to the `Fruit` column\'s categories. 3. Rename the categories to be more descriptive: `Apple` -> `Fruit_Apple`, `Banana` -> `Fruit_Banana`, `Grapes` -> `Fruit_Grapes`, `Orange` -> `Fruit_Orange`. 4. Sort the DataFrame based on the `Fruit` column considering the order of the categories. 5. Provide summary statistics of the categorical column. 6. Remove any unused categories from the `Fruit` column. # Function Signature ```python def process_fruit_dataframe(df: pd.DataFrame) -> pd.DataFrame: Processes the given DataFrame to handle categorical data as specified. Parameters: df (pd.DataFrame): DataFrame containing \'Name\' and \'Fruit\' columns. Returns: pd.DataFrame: The processed DataFrame with categorical handling done. ``` # Expected Output The function `process_fruit_dataframe` should return the processed DataFrame where the `Fruit` column is handled as per the given steps and sorted accordingly. # Constraints - The DataFrame will always contain the `Name` and `Fruit` columns. - The `Fruit` column will only contain values from the specified categories (`Apple`, `Banana`, `Grapes`), and the additional category (`Orange`). - Ensure that the function handles the categorical dtype efficiently and utilizes appropriate pandas methods for manipulating categorical data. Here is the main function you need to implement. The function should return the modified DataFrame after all the specified operations have been applied.","solution":"import pandas as pd def process_fruit_dataframe(df: pd.DataFrame) -> pd.DataFrame: Processes the given DataFrame to handle categorical data as specified. Parameters: df (pd.DataFrame): DataFrame containing \'Name\' and \'Fruit\' columns. Returns: pd.DataFrame: The processed DataFrame with categorical handling done. # 1. Convert the `Fruit` column to a categorical data type df[\'Fruit\'] = pd.Categorical(df[\'Fruit\'], categories=[\\"Apple\\", \\"Banana\\", \\"Grapes\\"], ordered=True) # 2. Add a new category `Orange` to the `Fruit` column\'s categories df[\'Fruit\'] = df[\'Fruit\'].cat.add_categories(\\"Orange\\") # 3. Rename the categories to be more descriptive df[\'Fruit\'] = df[\'Fruit\'].cat.rename_categories({ \\"Apple\\": \\"Fruit_Apple\\", \\"Banana\\": \\"Fruit_Banana\\", \\"Grapes\\": \\"Fruit_Grapes\\", \\"Orange\\": \\"Fruit_Orange\\" }) # 4. Sort the DataFrame based on the `Fruit` column considering the order of the categories df = df.sort_values(by=\'Fruit\') # 5. Provide summary statistics of the categorical column summary_statistics = df[\'Fruit\'].describe() print(summary_statistics) # 6. Remove any unused categories from the `Fruit` column df[\'Fruit\'] = df[\'Fruit\'].cat.remove_unused_categories() return df # Example call to the function data = { \\"Name\\": [\\"Alice\\", \\"Bob\\", \\"Charles\\", \\"Diana\\", \\"Edward\\", \\"Fiona\\", \\"George\\"], \\"Fruit\\": [\\"Apple\\", \\"Banana\\", \\"Apple\\", \\"Banana\\", \\"Grapes\\", \\"Banana\\", \\"Apple\\"] } df = pd.DataFrame(data) processed_df = process_fruit_dataframe(df) print(processed_df)"},{"question":"**Objective**: Your task is to write a Python function that takes in a string containing Python source code and generates a summary report of the code\'s structure. The summary should include the following elements: - The total number of functions defined. - The total number of classes defined. - The total number of import statements. - The number of each type of literal (e.g., strings, numbers). # Input - A single string containing valid Python source code. For example: ```python source_code = import os import sys class MyClass: def method1(self): pass def method2(self): x = 10 y = \\"Hello\\" def my_function(): a = 5 b = \'World\' ``` # Output - A dictionary summarizing the structure of the code. For example: ```python { \'functions\': 3, \'classes\': 1, \'imports\': 2, \'literals\': { \'numbers\': 2, \'strings\': 2 } } ``` # Constraints - You must use the `ast` module to parse and analyze the provided source code. - Do not use any third-party libraries. - Assume the source code will always be valid Python code. # Implementation Write a function `analyze_code_structure(source_code: str) -> dict` that takes the source code string and returns the summary dictionary. Example ```python def analyze_code_structure(source_code): # Your implementation here # Example usage source_code = import os import sys class MyClass: def method1(self): pass def method2(self): x = 10 y = \\"Hello\\" def my_function(): a = 5 b = \'World\' result = analyze_code_structure(source_code) print(result) # Output should be: # { # \'functions\': 3, # \'classes\': 1, # \'imports\': 2, # \'literals\': { # \'numbers\': 2, # \'strings\': 2 # } # } ```","solution":"import ast def analyze_code_structure(source_code): tree = ast.parse(source_code) result = { \'functions\': 0, \'classes\': 0, \'imports\': 0, \'literals\': { \'numbers\': 0, \'strings\': 0 } } class CodeAnalyzer(ast.NodeVisitor): def visit_FunctionDef(self, node): result[\'functions\'] += 1 self.generic_visit(node) def visit_ClassDef(self, node): result[\'classes\'] += 1 self.generic_visit(node) def visit_Import(self, node): result[\'imports\'] += 1 def visit_ImportFrom(self, node): result[\'imports\'] += 1 def visit_Constant(self, node): if isinstance(node.value, (int, float, complex)): result[\'literals\'][\'numbers\'] += 1 elif isinstance(node.value, str): result[\'literals\'][\'strings\'] += 1 analyzer = CodeAnalyzer() analyzer.visit(tree) return result"},{"question":"Custom Importer with `importlib` Introduction In this assessment, you will apply your knowledge of Python\'s import system to create a custom importer. Specifically, you will implement a custom module importer that can import modules from a specified directory at runtime, using the `importlib` module. Problem Statement Your task is to implement a custom importer class `CustomDirectoryImporter` which can dynamically import Python modules from a given directory that is not necessarily in the current list of system paths. **Requirements:** 1. Implement the class `CustomDirectoryImporter` with the following methods: - `__init__(self, directory: str)`: Initialize the importer with the directory path containing the Python modules. - `find_spec(self, fullname: str, path: Optional[str], target: Optional[Type[ModuleType]]) -> Optional[ModuleSpec]`: Locate a module spec for the given module, if found in the specified directory. - `create_module(self, spec: ModuleSpec) -> Optional[ModuleType]`: Create a new module object, if necessary. - `exec_module(self, module: ModuleType) -> None`: Execute the module\'s code within its context. 2. Usage of `importlib.util`, `importlib.machinery`, and other relevant submodules should be considered for implementing the methods. 3. Demonstrate the usage of this custom importer to import a module named `example_module` from the given directory and execute a function `say_hello()` within it. Input and Output Formats - The `directory` in `__init__` will be a string representing the path to the directory containing the modules. - The module to be imported will be called `example_module.py` and contain a function `def say_hello():` which prints `\\"Hello, Custom Importer!\\"`. Example Suppose the `example_module.py` in the directory `/path/to/custom_modules` contains the following code: ```python # example_module.py def say_hello(): print(\\"Hello, Custom Importer!\\") ``` Your implementation should allow importing as follows: ```python import sys from custom_importer import CustomDirectoryImporter # Assuming your code for the class is in custom_importer.py # Create the importer and add it to the system\'s meta path importer = CustomDirectoryImporter(\'/path/to/custom_modules\') sys.meta_path.insert(0, importer) # Now, the example_module should be imported successfully import example_module # Call the function from the imported module example_module.say_hello() ``` **Expected Output:** ``` Hello, Custom Importer! ``` Constraints - The solution should handle cases where the module is not found in the specified directory. - Follow best practices for error handling and resource management. - Avoid modifying global import state permanently, ensure that changes to `sys.meta_path` are reversible upon import completion.","solution":"import importlib.util import importlib.machinery import importlib.abc import sys import os class CustomDirectoryImporter(importlib.abc.MetaPathFinder, importlib.abc.Loader): def __init__(self, directory: str): self.directory = directory def find_spec(self, fullname: str, path: None, target: None): module_name = fullname.split(\'.\')[-1] module_path = os.path.join(self.directory, module_name + \'.py\') if not os.path.exists(module_path): return None loader = self return importlib.util.spec_from_file_location(fullname, module_path, loader=loader) def create_module(self, spec): return None def exec_module(self, module): with open(module.__spec__.origin, \'r\') as file: code = file.read() exec(code, module.__dict__) # To demonstrate the usage if __name__ == \\"__main__\\": # Assuming the example_module.py is at the directory path \'/path/to/custom_modules\' directory = \'/path/to/custom_modules\' # Create the custom importer importer = CustomDirectoryImporter(directory) # Add the custom importer to the system meta path sys.meta_path.insert(0, importer) # Import the custom module import example_module # Call the function from the imported module example_module.say_hello() # Clean up by removing the custom importer from the system\'s meta path sys.meta_path.remove(importer)"},{"question":"# Advanced Python Integer Conversion Challenge In this task, you will use Python\'s C API for integer objects to implement several utility functions for converting integers between Python and C types. Your goal is to implement these functions in a way that handles all edge cases and errors robustly. Ensure all necessary error handling using `PyErr_Occurred()` is included to distinguish between valid results and errors. Requirements 1. **Function: `create_pylong_from_long`** Create a `PyLongObject` from a C `long`. **Input:** - An integer of type `long`. **Output:** - A new `PyLongObject` instance representing the input integer, or `None` on failure. 2. **Function: `get_long_from_pylong`** Convert a `PyLongObject` to a C `long`. **Input:** - A `PyLongObject` instance. **Output:** - The corresponding C `long` value of the input `PyLongObject`, or `None` on failure. 3. **Function: `create_pylong_from_string`** Create a `PyLongObject` from a string representation of an integer. **Input:** - A string representing an integer. - An integer base for the string representation (between 2 and 36 inclusive). **Output:** - A new `PyLongObject` instance representing the integer value of the string, or `None` on failure. 4. **Function: `create_pylong_from_double`** Create a `PyLongObject` from the integer part of a double. **Input:** - A double. **Output:** - A new `PyLongObject` instance representing the integer part of the input double, or `None` on failure. Constraints - Functions should check for necessary error conditions using appropriate API functions. - Handle cases where the value is out of range for the target type. - Ensure that memory allocation for new `PyLongObject` instances is properly managed. - Performance should be considered, particularly with respect to the conversions. Below is a template to help you get started: ```c #include <Python.h> // Function to create a PyLongObject from a C long PyObject* create_pylong_from_long(long value) { PyObject* py_long_object = PyLong_FromLong(value); if (py_long_object == NULL) { PyErr_Print(); // Handle the error occurring return NULL; } return py_long_object; } // Function to get a C long from a PyLongObject long get_long_from_pylong(PyObject* pylong) { if (!PyLong_Check(pylong)) { PyErr_SetString(PyExc_TypeError, \\"Expected a PyLongObject\\"); PyErr_Print(); return -1; } long result = PyLong_AsLong(pylong); if (PyErr_Occurred()) { PyErr_Print(); // Handle the possible errors return -1; } return result; } // Function to create a PyLongObject from a string PyObject* create_pylong_from_string(const char* str, int base) { if (base < 2 || base > 36) { PyErr_SetString(PyExc_ValueError, \\"Base must be between 2 and 36\\"); return NULL; } char *endptr; PyObject* py_long_object = PyLong_FromString(str, &endptr, base); if (py_long_object == NULL) { PyErr_Print(); // Handle the error occurring return NULL; } return py_long_object; } // Function to create a PyLongObject from a double (integer part only) PyObject* create_pylong_from_double(double value) { PyObject* py_long_object = PyLong_FromDouble(value); if (py_long_object == NULL) { PyErr_Print(); // Handle the error occurring return NULL; } return py_long_object; } ``` Solve the above coding challenge by correctly implementing the provided functions, ensuring the expected input and output formats, adherence to constraints, and proper performance and error handling. Good luck!","solution":"def create_pylong_from_long(value): Create a PyLongObject from a C long. Args: value (int): A C long value. Returns: int: A PyLongObject representing the given C long. try: py_long_object = int(value) return py_long_object except Exception as e: print(f\\"Error: {e}\\") return None def get_long_from_pylong(pylong): Convert a PyLongObject to a C long. Args: pylong (int): A PyLongObject instance. Returns: int: The C long value of the PyLongObject. try: if not isinstance(pylong, int): raise TypeError(\\"Expected a PyLongObject\\") return int(pylong) except Exception as e: print(f\\"Error: {e}\\") return None def create_pylong_from_string(integer_string, base): Create a PyLongObject from a string representation of an integer. Args: integer_string (str): A string representing an integer. base (int): The base of the integer representation (between 2 and 36 inclusive). Returns: int: A PyLongObject representing the integer value of the string. try: if base < 2 or base > 36: raise ValueError(\\"Base must be between 2 and 36\\") return int(integer_string, base) except Exception as e: print(f\\"Error: {e}\\") return None def create_pylong_from_double(value): Create a PyLongObject from the integer part of a double. Args: value (float): A double value. Returns: int: A PyLongObject representing the integer part of the given double. try: return int(value) except Exception as e: print(f\\"Error: {e}\\") return None"},{"question":"Objective Implement a function that uses the `pkgutil.walk_packages` function to find and return all submodules of a given module recursively. Problem Statement Write a function `list_submodules(package_name: str) -> list` that takes a package name as input and returns a list of submodules recursively for that package. The function should handle errors gracefully and skip any submodule that cannot be imported, logging the error instead. Input - A string `package_name`: The name of the package to list submodules for. Output - A list of strings: Each string is the name of a submodule, recursively discovered within the specified package. Example ```python import logging import pkgutil def list_submodules(package_name: str) -> list: submodules = [] def onerror(name): logging.error(f\\"Error importing package: {name}\\") try: package = __import__(package_name) package_path = package.__path__ prefix = package.__name__ + \\".\\" for module_info in pkgutil.walk_packages(path=package_path, prefix=prefix, onerror=onerror): submodules.append(module_info.name) except ImportError as e: logging.error(f\\"Package {package_name} could not be imported: {e}\\") return submodules # Example usage: # List all submodules in the \'xml\' package print(list_submodules(\'xml\')) # Output might include: # [\'xml.dom\', \'xml.dom.minidom\', \'xml.dom.pulldom\', \'xml.etree\', \'xml.etree.ElementTree\', ...] ``` Constraints 1. If the specified package does not exist or cannot be imported, log an appropriate error message and return an empty list. 2. Use the `logging` library to record any errors encountered during submodule import. 3. Ensure the function uses the `pkgutil.walk_packages` method as instructed. Performance - Assume the package structure is not extremely large, and efficiency is not a primary concern for this task. Notes - You can assume `package_name` is a valid Python package name. - The returned list should not contain any duplicates.","solution":"import logging import pkgutil def list_submodules(package_name: str) -> list: submodules = [] def onerror(name): logging.error(f\\"Error importing package: {name}\\") try: package = __import__(package_name) package_path = package.__path__ prefix = package.__name__ + \\".\\" for module_info in pkgutil.walk_packages(path=package_path, prefix=prefix, onerror=onerror): submodules.append(module_info.name) except ImportError as e: logging.error(f\\"Package {package_name} could not be imported: {e}\\") return submodules # Example usage: # List all submodules in the \'xml\' package print(list_submodules(\'xml\')) # Output might include: # [\'xml.dom\', \'xml.dom.minidom\', \'xml.dom.pulldom\', \'xml.etree\', \'xml.etree.ElementTree\', ...]"},{"question":"You are given a dataset of daily temperatures recorded in two different cities over one month. Your task is to visualize the central tendency of the temperatures using seaborn and to represent the spread and uncertainty using error bars. # Dataset The dataset is a CSV file with the following columns: - **date**: The date of recording (format: YYYY-MM-DD) - **city**: The name of the city - **temperature**: The recorded temperature (in °C) # Requirements 1. **Read the dataset** and preprocess it if necessary. 2. **Plot a point plot** for the average daily temperature in each city over the month. Add error bars that show the *standard deviation* of the temperatures. 3. **Create another plot** that shows the same point plot but with error bars representing the *percentile interval* (e.g., the inter-quartile range). 4. **Create a third plot** that shows the point plot with error bars representing a *bootstrap confidence interval* for the mean temperature. 5. **Use subplots** to display all three plots in a single figure for comparison. 6. **Include a brief explanation** (as comments in your code) for each type of error bar and why it might be useful. # Input - CSV file path, provided as the variable `file_path`. # Output A single figure containing three subplots, each depicting the average daily temperature in each city with different types of error bars. # Constraints - Use seaborn for all plots. - Assume the dataset is not empty and the temperature values are valid numerical entries. # Example ```python import pandas as pd import seaborn as sns import matplotlib.pyplot as plt # Your code here ``` # Explanation Your solution will be evaluated based on: - Correct reading and preprocessing of the dataset. - Accurate plotting of the average daily temperature with appropriate error bars. - Clear and concise explanations for the chosen error bar types.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt import numpy as np def read_and_preprocess(file_path): # Read the dataset from the CSV file data = pd.read_csv(file_path) # No additional preprocessing steps required return data def plot_central_tendency_with_error_bars(file_path): data = read_and_preprocess(file_path) # Extract the required data for seaborn plot data_grouped = data.groupby([\'date\', \'city\']).agg({\'temperature\': [\'mean\', \'std\']}) data_grouped.columns = [\'mean_temperature\', \'std_temperature\'] data_grouped.reset_index(inplace=True) # Setup the subplot grid fig, axes = plt.subplots(3, 1, figsize=(12, 18)) Plot 1: Point plot with Standard Deviation error bars # Create a point plot sns.pointplot( data=data_grouped, x=\'date\', y=\'mean_temperature\', hue=\'city\', dodge=True, join=False, ci=None, ax=axes[0], ) # Add the error bars manually for standard deviation for city in data_grouped[\'city\'].unique(): city_data = data_grouped[data_grouped[\'city\'] == city] axes[0].errorbar( x=city_data.index, y=city_data[\'mean_temperature\'], yerr=city_data[\'std_temperature\'], fmt=\'none\', c=\'b\', capsize=5 ) axes[0].set_title(\'Average Daily Temperature with Standard Deviation Error Bars\') Plot 2: Point plot with Interquartile Range (IQR) error bars data_grouped_iqr = data.groupby([\'date\', \'city\'])[\'temperature\'].agg( temperature_mean=np.mean, temperature_q1=lambda x: np.percentile(x, 25), temperature_q3=lambda x: np.percentile(x, 75) ).reset_index() data_grouped_iqr[\'iqr\'] = data_grouped_iqr[\'temperature_q3\'] - data_grouped_iqr[\'temperature_q1\'] sns.pointplot( x=\'date\', y=\'temperature_mean\', hue=\'city\', data=data_grouped_iqr, ax=axes[1] ) for city in data_grouped_iqr[\'city\'].unique(): city_data = data_grouped_iqr[data_grouped_iqr[\'city\'] == city] axes[1].errorbar( x=city_data.index, y=city_data[\'temperature_mean\'], yerr=(city_data[\'iqr\'] / 2), fmt=\'none\', c=\'r\', capsize=5 ) axes[1].set_title(\'Average Daily Temperature with IQR Error Bars\') Plot 3: Point plot with Bootstrap Confidence Interval error bars def bootstrap_mean_ci(data, n_boot=1000, ci=95): boot_means = np.array([np.mean(np.random.choice(data, size=len(data), replace=True)) for _ in range(n_boot)]) lower_p = ((100-ci)/2) upper_p = 100 - lower_p ci_bounds = np.percentile(boot_means, [lower_p, upper_p]) return ci_bounds data_grouped_bs = data.groupby([\'date\', \'city\'])[\'temperature\'].agg( temperature_mean=np.mean, temperature_ci_lower=lambda x: bootstrap_mean_ci(x)[0], temperature_ci_upper=lambda x: bootstrap_mean_ci(x)[1] ).reset_index() data_grouped_bs[\'ci_range\'] = (data_grouped_bs[\'temperature_ci_upper\'] - data_grouped_bs[\'temperature_ci_lower\']) / 2 sns.pointplot( x=\'date\', y=\'temperature_mean\', hue=\'city\', data=data_grouped_bs, ax=axes[2], ) for city in data_grouped_bs[\'city\'].unique(): city_data = data_grouped_bs[data_grouped_bs[\'city\'] == city] axes[2].errorbar( x=city_data.index, y=city_data[\'temperature_mean\'], yerr=city_data[\'ci_range\'], fmt=\'none\', c=\'g\', capsize=5 ) axes[2].set_title(\'Average Daily Temperature with Bootstrap CI Error Bars\') plt.tight_layout() plt.show()"},{"question":"# Question: Create and Customize an Area Chart using Seaborn You are required to demonstrate your understanding of Seaborn\'s `seaborn.objects` module by loading a dataset, preprocessing it, and creating a customized area chart. The final visualization should include multiple facets and custom coloring. **Instructions:** 1. **Load and Preprocess the Data:** - Load the `diamonds` dataset using `seaborn.load_dataset()`. - Reshape the data to show the average price of diamonds over different cuts and clarity levels, grouped by `cut` and `clarity`. 2. **Create a Faceted Area Chart:** - Create an area chart where the x-axis represents the `cut`, and the y-axis represents the average `price`. - Use `clarity` to create multiple facets. 3. **Customization:** - Color the area chart based on the `cut`. - Modify the edge color of the area chart to be a shade of gray (`.5`). - Add a line on the border of the area chart to enhance visibility. 4. **Combine Area and Line Charts:** - Combine the area chart with a line chart to show the trend clearly. 5. **Bonus:** - Create a stacked area chart to show the relationship between different cuts and their individual contributions to the total average price. **Function Signature:** ```python def create_area_chart() -> None: pass ``` **Expected Output:** - Your function should produce a faceted area chart with specified customizations when called. - The plots should show the average price of diamonds per cut and clarity. **Constraints and Notes:** - Ensure proper handling of the data before plotting. - Make use of Seaborn\'s `seaborn.objects` and appropriate customization methods as covered in the documentation. - You do not need to return any values; just ensure the plot is displayed when the function is called. - Performance is not a primary concern, but the code should be efficient and follow best practices for data handling and visualization. Good luck and happy plotting!","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt def create_area_chart(): # Load the diamonds dataset diamonds = sns.load_dataset(\'diamonds\') # Preprocess the data avg_price = diamonds.groupby([\'cut\', \'clarity\']).price.mean().reset_index() # Create a Faceted Area Chart g = sns.FacetGrid(avg_price, col=\\"clarity\\", col_wrap=4, height=4) def area_and_line_plot(data, **kwargs): sns.lineplot(data=data, x=\\"cut\\", y=\\"price\\", hue=\\"cut\\", linewidth=2.5, **kwargs) plt.fill_between(data[\'cut\'], data[\'price\'], alpha=0.3) g.map_dataframe(area_and_line_plot) g.add_legend() # Customize the plot colors = sns.color_palette(\'pastel\') for ax, color in zip(g.axes.flatten(), colors): for art in ax.collections: art.set_facecolor(color) art.set_edgecolor(\'.5\') art.set_linewidth(1.5) plt.show()"},{"question":"# Question Data Classes and Context Managers in Python Given the documentation of Python\'s runtime services, your task is to implement a library system that tracks borrowed and returned books using **dataclasses** and **contextlib** utilities. 1. **Define a `dataclass` named `Book`** with the following attributes: - `title` (str): The title of the book. - `author` (str): The author of the book. - `year` (int): The year the book was published. - `borrowed` (bool): A flag indicating whether the book is currently borrowed or not. 2. **Implement a context manager named `BookHandler`** that manages the borrowing and returning of books: - The context manager should ensure that a book cannot be borrowed if it is already borrowed. - When leaving the context, whether normally or due to an exception, the book\'s borrowed status should be returned to its previous state (not borrowed). Detailed Steps: - Define the `Book` dataclass to include the attributes described above with appropriate types. - Implement the `BookHandler` context manager that modifies the `borrowed` attribute of a `Book` instance during the context. # Constraints: - A borrowed book cannot be borrowed again. - Your context manager should handle exceptions gracefully and ensure the book\'s status is reverted correctly. # Input and Output: - **Input**: Code interactions with the `BookHandler` context manager and `Book` instances. The context manager should be tested in various scenarios as described below. - **Output**: String messages or returned values indicating the status of the book borrowing transaction. # Example: ```python from dataclasses import dataclass from contextlib import contextmanager @dataclass class Book: title: str author: str year: int borrowed: bool = False class BookHandler: def __init__(self, book: Book): self.book = book def __enter__(self): if self.book.borrowed: raise ValueError(\\"Book is already borrowed.\\") self.book.borrowed = True return self.book def __exit__(self, exc_type, exc_value, traceback): self.book.borrowed = False return False # Example Usage book = Book(title=\\"The Catcher in the Rye\\", author=\\"J.D. Salinger\\", year=1951) try: with BookHandler(book) as b: print(f\\"Borrowing book: {b.title}\\") # Simulate some processing assert book.borrowed == True except ValueError as e: print(e) # Check the book status after the context assert book.borrowed == False print(\\"Book returned to library\\") ``` In this example: - If the book is already borrowed, a `ValueError` should be raised. - Upon successful borrowing, the book\'s status should be updated to `borrowed`. - Regardless of success or exceptions within the context, the book\'s status should be reverted to not borrowed upon exiting the context. This question assesses students\' understanding of `dataclasses` for structured data representation and `contextlib` for managing resource context with proper exception handling.","solution":"from dataclasses import dataclass from contextlib import contextmanager @dataclass class Book: title: str author: str year: int borrowed: bool = False class BookHandler: def __init__(self, book: Book): self.book = book def __enter__(self): if self.book.borrowed: raise ValueError(\\"Book is already borrowed.\\") self.book.borrowed = True return self.book def __exit__(self, exc_type, exc_value, traceback): self.book.borrowed = False return False"},{"question":"**Secure Password Input and User Validation** In this assessment, you are required to implement two functions that utilize the `getpass` module to enhance security during password input and to validate the logged-in user against a predefined list of authorized users. # Function 1: `secure_password_input()` This function prompts the user for a password securely, without echoing the input. It should display a custom prompt message and handle the case where echo-free input is not available by printing a warning. The function should return the entered password. **Input:** - None **Output:** - A string representing the password entered by the user. # Function 2: `validate_user()` This function checks if the current logged-in user is in a predefined list of authorized users. Use the `getpass.getuser()` function to retrieve the logged-in user\'s name. **Input:** - A list of strings representing authorized user names. **Output:** - A string output that indicates whether the logged-in user is authorized or not. # Constraints: - In `validate_user()`, the list of authorized user names will have a maximum length of 100. - Each authorized user name in the list will be a non-empty string and will contain only alphanumeric characters. # Example Usage: ```python # Example authorized users list authorized_users = [\\"alice\\", \\"bob\\", \\"charlie\\"] # Securely input a password password = secure_password_input() # Validate if the logged-in user is authorized validation_message = validate_user(authorized_users) print(validation_message) ``` **Expected Output:** 1. When `secure_password_input()` is called, the user should be prompted to enter a password with the custom message \\"Enter your secure password: \\". 2. `validate_user(authorized_users)` should return \\"User [username] is authorized.\\" if the user\'s login name is in the authorized users list, or \\"User [username] is not authorized.\\" if it is not. **Note:** - While testing, ensure to run the code in an environment where `getpass` and `getuser` functions can function correctly, as they might behave differently in a Jupyter notebook or certain IDEs.","solution":"import getpass def secure_password_input(): Prompts the user for a password securely without echoing input. Returns the entered password. try: password = getpass.getpass(prompt=\\"Enter your secure password: \\") except getpass.GetPassWarning: print(\\"Warning: Secure password input not available. Input will be visible.\\") password = input(\\"Enter your secure password: \\") return password def validate_user(authorized_users): Validates the current logged-in user against a list of authorized users. Returns a message indicating whether the user is authorized or not. current_user = getpass.getuser() if current_user in authorized_users: return f\\"User {current_user} is authorized.\\" else: return f\\"User {current_user} is not authorized.\\""},{"question":"# XML Parsing with `xml.parsers.expat` Your task is to write a function `parse_and_extract_data` that parses an XML string and extracts data using the `xml.parsers.expat` module. The function should process the XML document and return a dictionary containing the following information: - A list of all the element names in the order they appear. - A list of all the character data (text content) outside tags in the order they appear. # Function Signature: ```python def parse_and_extract_data(xml_string: str) -> dict: pass ``` # Input: - `xml_string` (str): A string containing the XML document to be parsed. # Output: - A dictionary with two keys: - `elements` (List[str]): A list of all element names in the order they appear. - `character_data` (List[str]): A list of all character data outside tags in the order they appear. # Constraints: - The XML document is well-formed. - The document does not contain external entities. # Example: Input: ```python xml_string = \'\'\'<?xml version=\\"1.0\\"?> <root> <child1 name=\\"paul\\">Text1</child1> <child2 name=\\"fred\\">Text2</child2> </root>\'\'\' ``` Output: ```python { \'elements\': [\'root\', \'child1\', \'child2\'], \'character_data\': [\'Text1\', \'Text2\'] } ``` # Hints: 1. Use the `ParserCreate` function from the `xml.parsers.expat` module to create an XML parser object. 2. Set the appropriate handlers (`StartElementHandler` and `CharacterDataHandler`) to collect the required data. 3. Use the `Parse` method of the parser object to process the XML string.","solution":"import xml.parsers.expat def parse_and_extract_data(xml_string: str) -> dict: elements = [] character_data = [] def start_element(name, attrs): elements.append(name) def char_data(data): if data.strip(): # Ignore whitespace-only data character_data.append(data) parser = xml.parsers.expat.ParserCreate() parser.StartElementHandler = start_element parser.CharacterDataHandler = char_data parser.Parse(xml_string) return { \'elements\': elements, \'character_data\': character_data }"},{"question":"# Python Coding Assessment Question Task Write a Python function `modify_setup_config` that takes a dictionary of configuration options and appends these options to an existing `setup.cfg` file. If the file does not exist, your function should create it. Function Signature ```python def modify_setup_config(config_options: dict) -> None: ``` Input - `config_options`: A dictionary where keys are command names (strings) and values are dictionaries of option-value pairs (strings). Output - None (the function should simply modify or create the `setup.cfg` file). Constraints - You need to handle multiline options correctly (options whose values span multiple lines). - Maintain readability of the config file. - Ensure there are no duplicate command sections in the file. If a command section already exists, update it with new configurations from the input dictionary. Example Usage ```python config_options = { \\"build_ext\\": {\\"inplace\\": \\"1\\"}, \\"bdist_rpm\\": { \\"release\\": \\"1\\", \\"packager\\": \\"Greg Ward <gward@python.net>\\", \\"doc_files\\": \\"CHANGES.txt README.txt USAGE.txt doc/ examples/\\" } } modify_setup_config(config_options) ``` After running `modify_setup_config(config_options)`, the `setup.cfg` should look like this: ``` [build_ext] inplace=1 [bdist_rpm] release=1 packager=Greg Ward <gward@python.net> doc_files=CHANGES.txt README.txt USAGE.txt doc/ examples/ ``` Requirements - You may use the `configparser` module to help manage configuration files. - Properly handle file read/write operations and ensure the existing content of the `setup.cfg` file is preserved and updated accordingly.","solution":"import configparser import os def modify_setup_config(config_options: dict) -> None: Appends or updates the given configuration options to the existing setup.cfg file. Creates the file if it does not exist. Parameters: config_options (dict): A dictionary where keys are command names (strings) and values are dictionaries of option-value pairs (strings). config = configparser.ConfigParser() # Read the existing setup.cfg file if it exists config_path = \'setup.cfg\' if os.path.exists(config_path): config.read(config_path) # Update the config object with new options for section, options in config_options.items(): if not config.has_section(section): config.add_section(section) for option, value in options.items(): config.set(section, option, value) # Write the updated config to the setup.cfg file with open(config_path, \'w\') as configfile: config.write(configfile)"},{"question":"**Objective**: Test your understanding of the `queue` module in Python and your ability to implement thread-safe queue operations. Problem Statement You are tasked with designing a multi-threaded application that processes data items by simulating a manufacturing production line. The application needs to use different types of queues to manage the workflow. Here is the scenario: 1. Data items arrive at the production line and are placed in a `Queue` (FIFO order) to represent the raw material queue. 2. Workers take items from the raw material queue and partially process them. The partially processed items are then placed in a `LifoQueue` (LIFO order) to represent a temporary storage stack. 3. Finally, another set of workers takes items from the temporary storage stack and finishes the processing. The finished items are placed in a `PriorityQueue` where the items are prioritized based on some criteria (e.g., production urgency) for final delivery. You need to write a multi-threaded Python program to simulate this production line. Requirements: 1. Implement a `ProductionLine` class with the following methods: - `__init__(self)`: Initialize the queues and any necessary attributes. - `add_raw_item(self, item)`: Add a new raw item to the raw material queue. - `process_raw_items(self)`: Simulate workers processing raw items and placing them into the temporary storage stack. - `finalize_processing(self)`: Simulate workers taking items from the temporary storage stack, finishing the processing, and placing them into the priority queue. - `get_final_product(self)`: Retrieve items from the priority queue based on their priority for delivery. 2. Ensure that the `process_raw_items` and `finalize_processing` methods are thread-safe and can handle concurrent execution by multiple worker threads. 3. Handle exception scenarios such as trying to get from an empty queue or put into a full queue. 4. Use appropriate synchronization mechanisms to ensure thread safety. Function Signature ```python import queue import threading from dataclasses import dataclass, field from typing import Any # Define the PrioritizedItem class for the priority queue @dataclass(order=True) class PrioritizedItem: priority: int item: Any=field(compare=False) class ProductionLine: def __init__(self, max_raw_size=50, max_temp_size=50, max_final_size=50): # Queue for raw items (FIFO) self.raw_queue = queue.Queue(maxsize=max_raw_size) # LifoQueue for temporary storage (LIFO) self.temp_stack = queue.LifoQueue(maxsize=max_temp_size) # PriorityQueue for final products self.final_queue = queue.PriorityQueue(maxsize=max_final_size) def add_raw_item(self, item): try: self.raw_queue.put(item, block=True, timeout=5) except queue.Full: print(\\"Raw queue is full. Unable to add item.\\") def process_raw_items(self): while True: try: item = self.raw_queue.get(block=True, timeout=5) # Simulate processing processed_item = f\\"processed_{item}\\" self.temp_stack.put(processed_item, block=True, timeout=5) self.raw_queue.task_done() except queue.Empty: break except queue.Full: print(\\"Temporary stack is full. Unable to add processed item.\\") break def finalize_processing(self): while True: try: processed_item = self.temp_stack.get(block=True, timeout=5) # Simulate final processing priority = len(processed_item) # Example priority based on item length final_item = PrioritizedItem(priority=priority, item=processed_item) self.final_queue.put(final_item, block=True, timeout=5) self.temp_stack.task_done() except queue.Empty: break except queue.Full: print(\\"Final queue is full. Unable to add final item.\\") break def get_final_product(self): try: final_item = self.final_queue.get(block=True, timeout=5) self.final_queue.task_done() return final_item.item except queue.Empty: print(\\"Final queue is empty. No item to retrieve.\\") return None # Example usage def main(): pl = ProductionLine() # Add raw items for i in range(30): pl.add_raw_item(f\\"raw_item_{i}\\") # Start raw item processing threads raw_threads = [threading.Thread(target=pl.process_raw_items) for _ in range(3)] for t in raw_threads: t.start() # Ensure all raw items are processed before moving to final processing for t in raw_threads: t.join() # Start final processing threads final_threads = [threading.Thread(target=pl.finalize_processing) for _ in range(3)] for t in final_threads: t.start() # Ensure all final items are processed for t in final_threads: t.join() # Retrieve final products for _ in range(10): print(pl.get_final_product()) if __name__ == \\"__main__\\": main() ``` # Constraints - Ensure thread safety and proper synchronization. - Assume max sizes for each queue if not specified. - Ensure that all raw items are processed and all processed items are moved to the final queue. - Properly handle exceptions and edge cases. Evaluate the efficiency and correctness of your implementation. Use appropriate test cases to verify its functionality.","solution":"import queue import threading from dataclasses import dataclass, field from typing import Any @dataclass(order=True) class PrioritizedItem: priority: int item: Any = field(compare=False) class ProductionLine: def __init__(self, max_raw_size=50, max_temp_size=50, max_final_size=50): self.raw_queue = queue.Queue(maxsize=max_raw_size) self.temp_stack = queue.LifoQueue(maxsize=max_temp_size) self.final_queue = queue.PriorityQueue(maxsize=max_final_size) def add_raw_item(self, item): try: self.raw_queue.put(item, block=True, timeout=5) except queue.Full: print(\\"Raw queue is full. Unable to add item.\\") def process_raw_items(self): while True: try: item = self.raw_queue.get(block=True, timeout=5) # Simulate processing processed_item = f\\"processed_{item}\\" self.temp_stack.put(processed_item, block=True, timeout=5) self.raw_queue.task_done() except queue.Empty: break except queue.Full: print(\\"Temporary stack is full. Unable to add processed item.\\") break def finalize_processing(self): while True: try: processed_item = self.temp_stack.get(block=True, timeout=5) # Simulate final processing priority = len(processed_item) # Example priority based on item length final_item = PrioritizedItem(priority=priority, item=processed_item) self.final_queue.put(final_item, block=True, timeout=5) self.temp_stack.task_done() except queue.Empty: break except queue.Full: print(\\"Final queue is full. Unable to add final item.\\") break def get_final_product(self): try: final_item = self.final_queue.get(block=True, timeout=5) self.final_queue.task_done() return final_item.item except queue.Empty: print(\\"Final queue is empty. No item to retrieve.\\") return None"},{"question":"# Coding Question # Objective Create a bar plot using the seaborn `objects` interface to visualize the total bill amount per day for each time (Lunch/Dinner), with the bars dodged to handle overlapping. # Requirements 1. Use the `tips` dataset available in seaborn. 2. Create a `Plot` object that: - Sets the `day` variable on the x-axis. - Uses the `time` variable to separate the data by color. 3. Add a `Bar` element to the plot to visualize the total bill amount per day. 4. Apply the `Dodge` transformation to handle overlapping, ensuring the empty spaces are filled appropriately. 5. Add a slight gap between dodged bars for better clarity. 6. Use seaborn\'s `objects` interface to achieve the above requirements. The final plot should clearly show separate bars for Lunch and Dinner on each day, with any empty spaces handled appropriately to make the plot neat and informative. # Code Template ```python import seaborn.objects as so from seaborn import load_dataset # Load the tips dataset tips = load_dataset(\\"tips\\") # Create a Plot object p = so.Plot(tips, \\"day\\", \\"total_bill\\", color=\\"time\\") # Add a Bar element with Dodge transformation # - Use so.Agg(\\"sum\\") to sum total bills per day # - Fill empty spaces between dodged bars # - Add a slight gap between bars # Show the plot p.show() ``` # Expected Output A bar plot that shows the total bill amount per day, separated by time (Lunch/Dinner), with no overlapping and neatly organized bars. # Input Example No input required, the dataset is pre-loaded. # Output Example A bar plot as described. # Constraints - Ensure that the plot is using the seaborn `objects` interface. - Handle overlapping and empty spaces appropriately.","solution":"import seaborn.objects as so from seaborn import load_dataset # Load the tips dataset tips = load_dataset(\\"tips\\") # Create a Plot object with dodging to separate Lunch and Dinner bars per day p = ( so.Plot(tips, x=\\"day\\", y=\\"total_bill\\", color=\\"time\\") .add(so.Bar(), so.Dodge()) .scale(color=[\\"#1f77b4\\", \\"#ff7f0e\\"]) .layout(size=(10, 5)) .theme({ \\"axes.spines.right\\": False, \\"axes.spines.top\\": False }) ) # Show the plot p.show()"},{"question":"Objective: Assess students\' understanding of loading datasets, creating bar plots using various aggregation functions, and customizing plots with different mappings and transformations using seaborn.objects. Question: You are given a dataset \'diamonds\' from seaborn, which includes data about diamonds\' characteristics such as carat, cut, clarity, depth, table, price, and x, y, z dimensions. Your task is to create multiple bar plots to visualize certain aspects of this dataset using seaborn.objects. Steps: 1. Load the \'diamonds\' dataset from seaborn. 2. Create a bar plot to show the average carat for each clarity category. 3. Create a bar plot to show the median carat for each clarity category. 4. Create a bar plot to show the interquartile range (IQR) of the carat for each clarity category. 5. Create a bar plot to show the average carat for each clarity category, with bars dodged by the cut variable and colored according to the cut variable. Requirements: - Please write a function `create_plots()` that performs the following tasks: - Loads the \'diamonds\' dataset. - Creates and displays the four bar plots as specified above. ```python def create_plots(): import seaborn.objects as so from seaborn import load_dataset import matplotlib.pyplot as plt # Load the \'diamonds\' dataset diamonds = load_dataset(\\"diamonds\\") # Plot 1: Average carat for each clarity p1 = so.Plot(diamonds, \\"clarity\\", \\"carat\\").add(so.Bar(), so.Agg()) p1.show() # Plot 2: Median carat for each clarity p2 = so.Plot(diamonds, \\"clarity\\", \\"carat\\").add(so.Bar(), so.Agg(\\"median\\")) p2.show() # Plot 3: IQR of carat for each clarity p3 = so.Plot(diamonds, \\"clarity\\", \\"carat\\").add(so.Bar(), so.Agg(lambda x: x.quantile(0.75) - x.quantile(0.25))) p3.show() # Plot 4: Average carat for each clarity, dodged by cut p4 = so.Plot(diamonds, \\"clarity\\", \\"carat\\").add(so.Bar(), so.Agg(), so.Dodge(), color=\\"cut\\") p4.show() # Call the function to create and display the plots create_plots() ``` Constraints: - Ensure that your plots maintain a clear distinction between different categories and aggregations. - Use appropriate titles and labels for each plot to make them informative and readable. Notes: - Explain each step briefly in comments to demonstrate your understanding of how seaborn.objects and its methods work. - Ensure your plots are well-annotated for easy interpretation.","solution":"def create_plots(): import seaborn.objects as so from seaborn import load_dataset import matplotlib.pyplot as plt # Load the \'diamonds\' dataset diamonds = load_dataset(\\"diamonds\\") # Plot 1: Average carat for each clarity p1 = ( so.Plot(diamonds, x=\\"clarity\\", y=\\"carat\\") .add(so.Bar(), so.Agg()) .label(title=\\"Average Carat for Each Clarity\\") ) p1.show() # Plot 2: Median carat for each clarity p2 = ( so.Plot(diamonds, x=\\"clarity\\", y=\\"carat\\") .add(so.Bar(), so.Agg(\\"median\\")) .label(title=\\"Median Carat for Each Clarity\\") ) p2.show() # Plot 3: IQR of carat for each clarity p3 = ( so.Plot(diamonds, x=\\"clarity\\", y=\\"carat\\") .add(so.Bar(), so.Agg(lambda x: x.quantile(0.75) - x.quantile(0.25))) .label(title=\\"IQR of Carat for Each Clarity\\") ) p3.show() # Plot 4: Average carat for each clarity, dodged by cut p4 = ( so.Plot(diamonds, x=\\"clarity\\", y=\\"carat\\", color=\\"cut\\") .add(so.Bar(), so.Agg(), so.Dodge()) .label(title=\\"Average Carat for Each Clarity, Dodged by Cut\\") ) p4.show() # Call the function to create and display the plots create_plots()"},{"question":"Coding Assessment Question # Objective Implement and compare different PCA variants to understand their efficiencies, strengths, and use cases. # Question You are given a dataset `data.csv` which has a large number of features and samples. Your task is to perform dimensionality reduction using the following methods: 1. **PCA**: Standard Principal Component Analysis. 2. **IncrementalPCA**: An incremental version of PCA for handling large datasets that do not fit in memory. 3. **KernelPCA**: A kernelized version of PCA for non-linear dimensionality reduction. # Input * A CSV file `data.csv` with `n_samples` rows and `n_features` columns. # Output * Save the reduced dimensions for each method in three separate CSV files: - `pca_reduced.csv` - `incremental_pca_reduced.csv` - `kernel_pca_reduced.csv` # Instructions 1. **PCA**: - Load the CSV file into a DataFrame. - Apply PCA to reduce the dimensionality to 50 principal components. - Save the transformed data into `pca_reduced.csv`. 2. **IncrementalPCA**: - Load the CSV file into a DataFrame. - Use `IncrementalPCA` to reduce the dimensionality to 50 principal components. Assume the dataset is too large to fit in memory, so use mini-batches of 1000 samples. - Save the transformed data into `incremental_pca_reduced.csv`. 3. **KernelPCA**: - Load the CSV file into a DataFrame. - Apply `KernelPCA` with RBF kernel to reduce the dimensionality to 50 principal components. - Save the transformed data into `kernel_pca_reduced.csv`. # Constraints * The input data will have at least 10,000 samples and 1,000 features. * Ensure that each method\'s output is deterministic by setting a random state where applicable. * Use efficient computation to handle memory constraints for large datasets. # Example A basic structure for your implementation in Python: ```python import pandas as pd from sklearn.decomposition import PCA, IncrementalPCA, KernelPCA # Load dataset df = pd.read_csv(\'data.csv\') # Standard PCA pca = PCA(n_components=50, random_state=42) pca_reduced = pca.fit_transform(df) pd.DataFrame(pca_reduced).to_csv(\'pca_reduced.csv\', index=False) # Incremental PCA incremental_pca = IncrementalPCA(n_components=50) for batch in range(0, len(df), 1000): incremental_pca.partial_fit(df[batch:batch+1000]) incremental_pca_reduced = incremental_pca.transform(df) pd.DataFrame(incremental_pca_reduced).to_csv(\'incremental_pca_reduced.csv\', index=False) # Kernel PCA kernel_pca = KernelPCA(n_components=50, kernel=\'rbf\', random_state=42) kernel_pca_reduced = kernel_pca.fit_transform(df) pd.DataFrame(kernel_pca_reduced).to_csv(\'kernel_pca_reduced.csv\', index=False) ``` # Performance Requirements * Ensure that the solution can handle the large dataset efficiently within reasonable time and memory limits.","solution":"import pandas as pd from sklearn.decomposition import PCA, IncrementalPCA, KernelPCA def load_data(file_path): Load the dataset from a CSV file. return pd.read_csv(file_path) def standard_pca(df, n_components=50): Apply standard PCA to the dataframe to reduce to the given number of components. pca = PCA(n_components=n_components, random_state=42) pca_reduced = pca.fit_transform(df) return pd.DataFrame(pca_reduced) def incremental_pca(df, n_components=50, batch_size=1000): Apply Incremental PCA to the dataframe to reduce to the given number of components. ipca = IncrementalPCA(n_components=n_components) for batch_start in range(0, len(df), batch_size): batch_end = min(batch_start + batch_size, len(df)) ipca.partial_fit(df.iloc[batch_start:batch_end]) ipca_reduced = ipca.transform(df) return pd.DataFrame(ipca_reduced) def kernel_pca(df, n_components=50, kernel=\'rbf\'): Apply Kernel PCA with RBF kernel to the dataframe to reduce to the given number of components. kpca = KernelPCA(n_components=n_components, kernel=kernel, random_state=42) kpca_reduced = kpca.fit_transform(df) return pd.DataFrame(kpca_reduced) def save_data(df, file_path): Save the dataframe to a CSV file. df.to_csv(file_path, index=False) def main(): # Load the dataset df = load_data(\'data.csv\') # Standard PCA pca_reduced_df = standard_pca(df) save_data(pca_reduced_df, \'pca_reduced.csv\') # Incremental PCA incremental_pca_reduced_df = incremental_pca(df) save_data(incremental_pca_reduced_df, \'incremental_pca_reduced.csv\') # Kernel PCA kernel_pca_reduced_df = kernel_pca(df) save_data(kernel_pca_reduced_df, \'kernel_pca_reduced.csv\') if __name__ == \\"__main__\\": main()"},{"question":"# URL Validation and Construction Utility **Objective:** Write a Python class `URLUtility` that provides the following functionalities using the `urllib.parse` module: 1. **validate_url**: Validate a given URL and ensure it has a specific scheme (\'http\' or \'https\'). 2. **get_query_params**: Parse the query parameters from a given URL and return them as a dictionary. 3. **construct_url**: Construct a URL from its components. **Requirements:** 1. `validate_url(url: str) -> bool`: - Input: A URL string. - Output: Return `True` if the URL is valid and uses \'http\' or \'https\' schemes; otherwise `False`. - Constraints: Ensure the URL is not empty and has a valid scheme. 2. `get_query_params(url: str) -> dict`: - Input: A URL string. - Output: A dictionary containing the query parameters. - Constraints: The URL should be valid and contain query parameters. 3. `construct_url(scheme: str, netloc: str, path: str = \'\', params: str = \'\', query: str = \'\', fragment: str = \'\') -> str`: - Input: Components of a URL. - Output: A complete URL string constructed from the provided components. - Constraints: Handle empty components appropriately and ensure no ambiguous URL parts. **Format:** ```python from urllib.parse import urlparse, parse_qs, urlunparse class URLUtility: @staticmethod def validate_url(url: str) -> bool: # Your implementation here @staticmethod def get_query_params(url: str) -> dict: # Your implementation here @staticmethod def construct_url(scheme: str, netloc: str, path: str = \'\', params: str = \'\', query: str = \'\', fragment: str = \'\') -> str: # Your implementation here # Example usage: url = \\"http://www.example.com/path/to/page?name=ferret&color=purple\\" print(URLUtility.validate_url(url)) # Expected output: True print(URLUtility.get_query_params(url)) # Expected output: {\'name\': [\'ferret\'], \'color\': [\'purple\']} print(URLUtility.construct_url(\'http\', \'www.example.com\', \'/path/to/page\', \'\', \'name=ferret&color=purple\')) # Expected output: \'http://www.example.com/path/to/page?name=ferret&color=purple\' ``` **Notes:** - Use the `urllib.parse` module for URL parsing and construction. - Handle edge cases such as URLs without query parameters or with invalid schemes appropriately.","solution":"from urllib.parse import urlparse, parse_qs, urlunparse class URLUtility: @staticmethod def validate_url(url: str) -> bool: if not url: return False parsed_url = urlparse(url) return parsed_url.scheme in [\'http\', \'https\'] @staticmethod def get_query_params(url: str) -> dict: if not URLUtility.validate_url(url): return {} parsed_url = urlparse(url) return parse_qs(parsed_url.query) @staticmethod def construct_url(scheme: str, netloc: str, path: str = \'\', params: str = \'\', query: str = \'\', fragment: str = \'\') -> str: return urlunparse((scheme, netloc, path, params, query, fragment))"},{"question":"**Question: Date Offset Manipulation** You are working as a data analyst, and you need to create a calendar that highlights important dates for your organization. You decide to use the `pandas.tseries.offsets` module to help with this. Your task is to implement a function that takes a start date and a number of periods and returns a DataFrame of various date offsets for each period. # Function Signature ```python import pandas as pd from pandas.tseries.offsets import * def generate_date_offsets(start_date: str, periods: int) -> pd.DataFrame: Generates a DataFrame of various date offsets starting from the given start date for the specified number of periods. Parameters: start_date (str): The start date in \'YYYY-MM-DD\' format. periods (int): The number of periods to generate. Returns: pd.DataFrame: A DataFrame containing various date offsets. ``` # Input - `start_date` (str): A string representing the start date in the \'YYYY-MM-DD\' format. - `periods` (int): An integer representing the number of periods for which date offsets need to be generated. # Output - A pandas DataFrame with the following columns: - \'Original Date\': The original date for each period. - \'BusinessDay Added\': The date after adding a BusinessDay offset. - \'MonthEnd Added\': The date after adding a MonthEnd offset. - \'QuarterBegin Added\': The date after adding a QuarterBegin offset. - \'CustomBusinessDay Added\': The date after adding a CustomBusinessDay offset with default settings. # Constraints - The start date will always be a valid date in the \'YYYY-MM-DD\' format. - The periods will always be a positive integer. # Example ```python # Example usage: start_date = \'2023-01-01\' periods = 5 result = generate_date_offsets(start_date, periods) print(result) ``` Expected output: ``` Original Date BusinessDay Added MonthEnd Added QuarterBegin Added CustomBusinessDay Added 0 2023-01-01 2023-01-02 2023-01-31 2023-01-01 2023-01-02 1 2023-01-02 2023-01-03 2023-01-31 2023-04-01 2023-01-03 2 2023-01-03 2023-01-04 2023-01-31 2023-04-01 2023-01-04 3 2023-01-04 2023-01-05 2023-01-31 2023-04-01 2023-01-05 4 2023-01-05 2023-01-06 2023-01-31 2023-04-01 2023-01-06 ``` # Notes - Use the `pd.date_range` function to generate a range of dates starting from the `start_date`. - Apply various date offset classes to generate new dates. - Ensure each column in the DataFrame correctly represents the date manipulations specified. - Utilize the `pandas.tseries.offsets` module effectively to solve the problem.","solution":"import pandas as pd from pandas.tseries.offsets import BusinessDay, MonthEnd, QuarterBegin, CustomBusinessDay def generate_date_offsets(start_date: str, periods: int) -> pd.DataFrame: # Generate a date range starting from \'start_date\' for the specified number of periods date_range = pd.date_range(start=start_date, periods=periods, freq=\'D\') # Create a DataFrame to hold the original dates and their corresponding offsets data = { \'Original Date\': date_range, \'BusinessDay Added\': date_range + BusinessDay(), \'MonthEnd Added\': date_range + MonthEnd(), \'QuarterBegin Added\': date_range + QuarterBegin(startingMonth=1), \'CustomBusinessDay Added\': date_range + CustomBusinessDay() } # Creating the DataFrame df = pd.DataFrame(data) return df"},{"question":"**Objective**: You are required to implement a function in Python that compresses a directory of text files using the `bz2` module. Your function should handle both one-shot and incremental compression based on file size. **Function Specification**: ```python def compress_directory(input_dir, output_dir, mode=\'one-shot\', compresslevel=9): Compresses all text files in a given input directory using bzip2 compression. Parameters: input_dir (str): The directory containing text files to be compressed. output_dir (str): The directory where compressed files will be saved. mode (str): The compression mode to use (\'one-shot\' or \'incremental\'). - \'one-shot\': Uses bz2.compress() for one-shot compression. - \'incremental\': Uses BZ2Compressor for incremental compression. compresslevel (int): The compression level to use (1-9). Default is 9. Returns: None pass ``` **Requirements**: 1. **Input and Output**: - `input_dir`: This is the directory path containing the input text files. - `output_dir`: This is the directory path where compressed files will be saved. - `mode`: Specifies the compression mode (\'one-shot\' or \'incremental\'). Default is \'one-shot\'. - `compresslevel`: Specifies the compression level (1-9). Default is 9. 2. **Functionality**: - The function should read each text file from `input_dir`, compress the content, and write the compressed content to a correspondingly named file in `output_dir`. - If `mode` is \'one-shot\', use the `bz2.compress()` function. - If `mode` is \'incremental\', use the `BZ2Compressor` class. - Ensure that the structure and names of the files remain consistent in the `output_dir`. 3. **Constraints**: - Assume that the `input_dir` contains only text files and no subdirectories. - Ensure that the function handles potential exceptions, such as file read/write errors. 4. **Performance**: - Ensure efficient file handling and minimize memory usage where possible. - Make use of buffered reading and writing for both one-shot and incremental modes. **Example Usage**: ```python # Example directory compression compress_directory(\'path/to/input_dir\', \'path/to/output_dir\', mode=\'one-shot\', compresslevel=9) # Example directory compression using incremental mode compress_directory(\'path/to/input_dir\', \'path/to/output_dir\', mode=\'incremental\', compresslevel=9) ``` **Notes**: - Make sure to test your function with various sizes of text files to ensure it performs correctly and efficiently in both one-shot and incremental modes. - The implementation should consider edge cases such as empty files and very large files.","solution":"import os import bz2 def compress_directory(input_dir, output_dir, mode=\'one-shot\', compresslevel=9): Compresses all text files in a given input directory using bzip2 compression. Parameters: input_dir (str): The directory containing text files to be compressed. output_dir (str): The directory where compressed files will be saved. mode (str): The compression mode to use (\'one-shot\' or \'incremental\'). - \'one-shot\': Uses bz2.compress() for one-shot compression. - \'incremental\': Uses BZ2Compressor for incremental compression. compresslevel (int): The compression level to use (1-9). Default is 9. Returns: None if not os.path.exists(output_dir): os.makedirs(output_dir) for filename in os.listdir(input_dir): if filename.endswith(\'.txt\'): input_file_path = os.path.join(input_dir, filename) output_file_path = os.path.join(output_dir, filename + \'.bz2\') with open(input_file_path, \'rb\') as input_file: data = input_file.read() if mode == \'one-shot\': compressed_data = bz2.compress(data, compresslevel=compresslevel) with open(output_file_path, \'wb\') as output_file: output_file.write(compressed_data) elif mode == \'incremental\': compressor = bz2.BZ2Compressor(compresslevel) with open(output_file_path, \'wb\') as output_file: compressed_data = compressor.compress(data) output_file.write(compressed_data) output_file.write(compressor.flush()) else: raise ValueError(\\"Invalid mode. Choose either \'one-shot\' or \'incremental\'.\\")"},{"question":"# Python Coding Assessment Question: Formatted Numerical Conversion Objective You are required to write a Python function that converts a list of string representations of numbers into a list of formatted strings. The formatting should include converting each number string to a double (floating-point), performing necessary error checking, and formatting the number into a specific string format. Additionally, ensure the implementation obeys the constraints and handles edge cases as described. Function Signature ```python from typing import List, Union def convert_and_format_number_strings(strings: List[str], format_code: str, precision: int) -> Union[List[str], str]: pass ``` Input - `strings`: A list of strings, where each string represents a number similar to those acceptable by Python\'s `float()` constructor, except without leading or trailing whitespace. - `format_code`: A character indicating the format code to be used for the conversion. Valid options are `\'e\'`, `\'E\'`, `\'f\'`, `\'F\'`, `\'g\'`, `\'G\'`, or `\'r\'`. - `precision`: An integer indicating the precision to be used in the formatted string. If format code is `\'r\'`, precision should be `0` and ignored. Output - Returns a list of formatted strings corresponding to the input number strings, adhering to the provided format code and precision. - If an error occurs during conversion (e.g., an invalid number string), return a string \\"Conversion Error\\". Constraints 1. The function should handle the conversion and formatting using proper error checking. 2. Ensure that no more than the necessary buffer size is used for formatting. 3. Do not use any third-party libraries for the conversion and formatting. 4. Follow the rules indicated in the provided documentation. 5. The formatted output should include locale-independent representation. 6. Handle large values and edge cases as described (e.g., too large to store in a float or invalid input strings). # Example Input: ```python strings = [\\"3.14159\\", \\"2.71828\\", \\"1e10\\", \\"1e500\\"] format_code = \\"f\\" precision = 3 ``` Output: ```python [\\"3.142\\", \\"2.718\\", \\"10000000000.000\\", \\"Py_HUGE_VAL\\"] ``` Input: ```python strings = [\\"abc\\", \\"123\\"] format_code = \\"g\\" precision = 4 ``` Output: ```python \\"Conversion Error\\" ``` # Notes Consider edge cases like: - Extremely large numbers. - Invalid number strings that cannot be converted. - Proper memory management as indicated in the documentation. Hint You may find it helpful to define helper functions to manage the conversion of the string to double and then formatting the double to the desired string format.","solution":"from typing import List, Union def convert_and_format_number_strings(strings: List[str], format_code: str, precision: int) -> Union[List[str], str]: formatted_list = [] for string in strings: try: number = float(string) if number == float(\'inf\') or number == -float(\'inf\'): formatted_list.append(\\"Py_HUGE_VAL\\") else: if format_code == \'r\': formatted_list.append(str(number)) else: formatted_list.append(f\\"{number:.{precision}{format_code}}\\") except ValueError: return \\"Conversion Error\\" return formatted_list"},{"question":"**Objective:** Write a Python function that implements a complete machine learning pipeline using scikit-learn. Your task is to create a pipeline that preprocesses the data, trains a model, and evaluates its performance. **Task Description:** You need to implement a function `train_and_evaluate_pipeline(data: pd.DataFrame, target_column: str) -> float` that performs the following steps: 1. **Preprocessing:** - Separate the features and the target variable from the `data` DataFrame using the `target_column` parameter. - Handle missing values by imputing them with the median value for numerical columns and the most frequent value for categorical columns. - Normalize numerical features using `StandardScaler`. 2. **Model Training:** - Train a `RandomForestClassifier` using the preprocessed data. - Use a train-test split with 80% of data for training and 20% for testing. 3. **Evaluation:** - Evaluate the trained model using accuracy score on the test set. **Input:** - `data`: A pandas DataFrame containing the dataset with both features and the target variable. - `target_column`: The name of the column in the DataFrame representing the target variable to predict. **Output:** - Return the accuracy score as a `float`. **Constraints:** - You must use scikit-learn for implementing the pipeline. - You need to handle both numerical and categorical features. - Your function should be efficient and handle datasets of moderate size (tens of thousands of rows). **Performance Requirements:** Your function will be evaluated on its correctness and efficiency. Ensure your solution can handle data of moderate size efficiently. **Example:** ```python import pandas as pd # Example DataFrame data = pd.DataFrame({ \'feature1\': [1, 2, 3, None, 5], \'feature2\': [\'A\', \'B\', \'A\', \'B\', \'A\'], \'target\': [0, 1, 0, 1, 0] }) # Target column target_column = \'target\' # Function call accuracy = train_and_evaluate_pipeline(data, target_column) print(f\\"Accuracy: {accuracy:.2f}\\") ``` This function should handle various potential issues in the given dataset and provide a robust evaluation of the model\'s performance. Ensure your implementation follows scikit-learn\'s best practices for building machine learning pipelines.","solution":"import pandas as pd from sklearn.model_selection import train_test_split from sklearn.pipeline import Pipeline from sklearn.compose import ColumnTransformer from sklearn.impute import SimpleImputer from sklearn.preprocessing import StandardScaler, OneHotEncoder from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import accuracy_score def train_and_evaluate_pipeline(data: pd.DataFrame, target_column: str) -> float: # Separate features and target variable X = data.drop(columns=[target_column]) y = data[target_column] # Identify numerical and categorical columns numerical_features = X.select_dtypes(include=[\'int64\', \'float64\']).columns.tolist() categorical_features = X.select_dtypes(include=[\'object\']).columns.tolist() # Preprocessing for numerical data numerical_transformer = Pipeline(steps=[ (\'imputer\', SimpleImputer(strategy=\'median\')), (\'scaler\', StandardScaler()) ]) # Preprocessing for categorical data categorical_transformer = Pipeline(steps=[ (\'imputer\', SimpleImputer(strategy=\'most_frequent\')), (\'onehot\', OneHotEncoder(handle_unknown=\'ignore\')) ]) # Combine preprocessing steps preprocessor = ColumnTransformer( transformers=[ (\'num\', numerical_transformer, numerical_features), (\'cat\', categorical_transformer, categorical_features) ] ) # Create the pipeline pipeline = Pipeline(steps=[ (\'preprocessor\', preprocessor), (\'classifier\', RandomForestClassifier()) ]) # Split the data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Fit the pipeline on the training data pipeline.fit(X_train, y_train) # Predict on the test data y_pred = pipeline.predict(X_test) # Calculate the accuracy score accuracy = accuracy_score(y_test, y_pred) return accuracy"},{"question":"# Python Coding Assessment - Advanced I/O Operations **Objective**: Implement a Python class to handle advanced file operations combining text and binary I/O with optional buffering and encoding mechanisms. Problem Statement Create a class `AdvancedFileHandler` that provides methods to work with both text and binary files efficiently. The class should support the following operations: 1. **Initialization**: - Initialize with a file path, mode (`\'r\'`, `\'w\'`, `\'rb\'`, `\'wb\'`), encoding (default to `\'utf-8\'`), and buffering (default to -1 for default buffer size). 2. **Reading**: - `read_data(size=-1)`: Read up to `size` bytes/characters from the file. - `read_line()`: Read one line from the file. 3. **Writing**: - `write_data(data)`: Write the given data (`str` for text mode, `bytes` for binary mode) to the file. 4. **Seek and Tell**: - `seek_position(offset, whence=0)`: Change the stream position to the given `offset`. - `current_position()`: Return the current stream position. 5. **Context Management**: - Ensure the class can be used with a `with` statement for proper resource management. Input and Output - **Initialization Arguments**: - `path` (str): The file path. - `mode` (str): Mode in which to open the file, such as `\'r\'`, `\'w\'`, `\'rb\'`, `\'wb\'`. - `encoding` (str, optional): Encoding used for text files. Default: `\'utf-8\'`. - `buffering` (int, optional): Buffering policy. Default: `-1`. - **Methods**: - `read_data(self, size=-1) -> str/bytes`: Returns read bytes/characters. - `read_line(self) -> str`: Returns a single line read. - `write_data(self, data: str/bytes) -> None`: Writes data to file. - `seek_position(self, offset: int, whence: int = 0) -> None`: Changes stream position. - `current_position(self) -> int`: Returns current position. Constraints - The class must handle appropriate exceptions for unsupported operations based on file modes. - Utilize Python\'s `io` module for all file handling operations. Example Usage ```python # Example: Writing and Reading from a text file with AdvancedFileHandler(\'example.txt\', \'w\') as handler: handler.write_data(\'Hello, World!n\') handler.write_data(\'This is a test.\') with AdvancedFileHandler(\'example.txt\', \'r\') as handler: print(handler.read_line()) # Should output: \'Hello, World!n\' print(handler.read_data()) # Should output: \'This is a test.\' # Example: Writing and Reading from a binary file with AdvancedFileHandler(\'example.bin\', \'wb\') as handler: handler.write_data(b\'x00x01x02x03\') with AdvancedFileHandler(\'example.bin\', \'rb\') as handler: print(handler.read_data(2)) # Should output: b\'x00x01\' print(handler.current_position()) # Should output: 2 handler.seek_position(0) print(handler.read_data()) # Should output: b\'x00x01x02x03\' ``` Implement the `AdvancedFileHandler` class below. Class Implementation ```python import io class AdvancedFileHandler: def __init__(self, path, mode, encoding=\'utf-8\', buffering=-1): self.path = path self.mode = mode self.encoding = encoding self.buffering = buffering self.file = None def __enter__(self): if \'b\' in self.mode: self.file = open(self.path, self.mode, buffering=self.buffering) else: self.file = open(self.path, self.mode, buffering=self.buffering, encoding=self.encoding) return self def __exit__(self, exc_type, exc_value, traceback): if self.file: self.file.close() def read_data(self, size=-1): if self.file.mode in [\'r\', \'rb\']: return self.file.read(size) raise io.UnsupportedOperation(\\"File not open for reading.\\") def read_line(self): if \'r\' in self.file.mode: return self.file.readline() raise io.UnsupportedOperation(\\"File not open for reading.\\") def write_data(self, data): if \'w\' in self.file.mode or \'a\' in self.file.mode: self.file.write(data) else: raise io.UnsupportedOperation(\\"File not open for writing.\\") def seek_position(self, offset, whence=io.SEEK_SET): self.file.seek(offset, whence) def current_position(self): return self.file.tell() ```","solution":"import io class AdvancedFileHandler: def __init__(self, path, mode, encoding=\'utf-8\', buffering=-1): self.path = path self.mode = mode self.encoding = encoding self.buffering = buffering self.file = None def __enter__(self): if \'b\' in self.mode: self.file = open(self.path, self.mode, buffering=self.buffering) else: self.file = open(self.path, self.mode, buffering=self.buffering, encoding=self.encoding) return self def __exit__(self, exc_type, exc_value, traceback): if self.file: self.file.close() def read_data(self, size=-1): if \'r\' in self.mode: return self.file.read(size) raise io.UnsupportedOperation(\\"File not open for reading.\\") def read_line(self): if \'r\' in self.mode: return self.file.readline() raise io.UnsupportedOperation(\\"File not open for reading.\\") def write_data(self, data): if \'w\' in self.mode or \'a\' in self.mode: self.file.write(data) else: raise io.UnsupportedOperation(\\"File not open for writing.\\") def seek_position(self, offset, whence=io.SEEK_SET): self.file.seek(offset, whence) def current_position(self): return self.file.tell()"},{"question":"Objective: Design a function that demonstrates your ability to load and handle datasets from various sources using scikit-learn\'s dataset loading utilities. Task: You are required to implement a function `load_and_process_data(source: str) -> dict` that performs the following operations based on the specified source type: 1. **Sample Image:** When the source is \\"sample_image\\", load the sample image \\"china.jpg\\" using `sklearn.datasets.load_sample_image`, convert the image to a floating point representation scaled between 0-1, and return its shape. 2. **SVMLight/LibSVM File:** When the source is \\"svmlight\\", load the dataset from a sample SVMlight or LibSVM file provided in the current directory named `sample_svm_light.txt`, and return the number of features and the shape of the output arrays X and y. 3. **OpenML Dataset:** When the source is \\"openml\\", fetch the \\"miceprotein\\" dataset (data_id=40966) using `sklearn.datasets.fetch_openml`, and return the number of samples, number of features, and unique classes in the target. 4. **External CSV File:** When the source is \\"external_csv\\", load the dataset from a CSV file named `external_data.csv` in the current directory using pandas, and return the shape of the dataframe. The function should return a dictionary containing relevant information as specified above. Implementation Details: 1. The function should handle all loading operations and any necessary preprocessing steps. 2. Ensure all data fetching and loading methods are correctly used from the `sklearn.datasets` module or external libraries like pandas for CSV files. 3. Provide proper error handling for file not found or incorrect source type. Constraints: - Assume that all necessary external files (`sample_svm_light.txt`, `external_data.csv`) are correctly placed in the current directory and formatted correctly for their respective loaders. - The function should gracefully handle scenarios where files are missing or the source provided does not match any of the specified types. Expected Input and Output: ```python def load_and_process_data(source: str) -> dict: # Implement function here # Example Usage: # result = load_and_process_data(\\"sample_image\\") # Output: {\\"shape\\": (427, 640, 3)} # # result = load_and_process_data(\\"svmlight\\") # Output: {\\"n_features\\": 20, \\"X_shape\\": (100, 20), \\"y_shape\\": (100,)} # # result = load_and_process_data(\\"openml\\") # Output: {\\"n_samples\\": 1080, \\"n_features\\": 77, \\"n_classes\\": 8} # # result = load_and_process_data(\\"external_csv\\") # Output: {\\"shape\\": (200, 10)} ``` Note: - Carefully read the documentation on how to use respective dataset loading functions. - Ensure efficient processing and handling of datasets as required.","solution":"import numpy as np import pandas as pd from sklearn.datasets import load_sample_image, load_svmlight_file, fetch_openml def load_and_process_data(source: str) -> dict: result = {} if source == \\"sample_image\\": # Load sample image \'china.jpg\' image = load_sample_image(\'china.jpg\') # Convert to a floating point representation scaled between 0-1 image = image.astype(np.float32) / 255.0 # Return the shape of the image result[\\"shape\\"] = image.shape elif source == \\"svmlight\\": # Load SVMlight/LibSVM file \'sample_svm_light.txt\' try: X, y = load_svmlight_file(\'sample_svm_light.txt\') result[\\"n_features\\"] = X.shape[1] result[\\"X_shape\\"] = X.shape result[\\"y_shape\\"] = y.shape except FileNotFoundError: result[\\"error\\"] = \\"File not found: sample_svm_light.txt\\" elif source == \\"openml\\": # Fetch dataset from OpenML data = fetch_openml(data_id=40966, as_frame=True) X, y = data.data, data.target result[\\"n_samples\\"] = X.shape[0] result[\\"n_features\\"] = X.shape[1] result[\\"n_classes\\"] = len(np.unique(y)) elif source == \\"external_csv\\": # Load dataset from CSV file \'external_data.csv\' try: df = pd.read_csv(\'external_data.csv\') result[\\"shape\\"] = df.shape except FileNotFoundError: result[\\"error\\"] = \\"File not found: external_data.csv\\" else: result[\\"error\\"] = \\"Unknown source type\\" return result"},{"question":"You are tasked to implement a function that initializes the parameters of a neural network using various initialization schemes provided by the `torch.nn.init` module. Your function will take a model and an initialization configuration as inputs and apply the respective initialization to each layer of the model. Function Signature ```python import torch import torch.nn as nn import torch.nn.init as init def initialize_network(model: nn.Module, init_config: dict): Initialize the parameters of the given neural network model. Args: - model (nn.Module): The neural network model to initialize. - init_config (dict): A dictionary where keys are layer types (e.g., \'Linear\', \'Conv2d\') and values are the initialization method names (e.g., \'xavier_uniform_\', \'kaiming_normal_\') and their corresponding arguments in a tuple. Returns: - None: The function should modify the model in-place. # Your code here ``` # Input - `model`: An instance of a neural network model (`nn.Module`). - `init_config`: A dictionary with keys being layer type names as strings (e.g., \'Linear\', \'Conv2d\') and values being tuples containing the initialization method name (as strings) and additional arguments required for that method (if any). # Output - The function should initialize the parameters of the model in-place as per the initialization configuration. # Requirements - Use the `torch.nn.init` functions to initialize the parameters. - The function should handle any additional arguments required by the initialization methods, if specified. - Ensure all parameters of the type specified in the configuration are initialized using the correct method. # Example ```python import torch.nn as nn # Example model class ExampleModel(nn.Module): def __init__(self): super(ExampleModel, self).__init__() self.fc1 = nn.Linear(784, 256) self.fc2 = nn.Linear(256, 10) def forward(self, x): x = torch.relu(self.fc1(x)) x = self.fc2(x) return x # Example initialization configuration init_config = { \'Linear\': (\'xavier_uniform_\',) } model = ExampleModel() initialize_network(model, init_config) # Check the initialization print(model.fc1.weight) print(model.fc2.weight) ``` In this example, the `initialize_network` function should initialize the weights of `Linear` layers in the model using the `xavier_uniform_` initialization method. # Constraints - Assume the methods specified in the `init_config` dictionary are valid methods from the `torch.nn.init` module. - Handle cases where no additional arguments are needed for initialization. - Refer to the PyTorch documentation for the exact function signatures and usage of methods in `torch.nn.init`.","solution":"import torch import torch.nn as nn import torch.nn.init as init def initialize_network(model: nn.Module, init_config: dict): Initialize the parameters of the given neural network model. Args: - model (nn.Module): The neural network model to initialize. - init_config (dict): A dictionary where keys are layer types (e.g., \'Linear\', \'Conv2d\') and values are the initialization method names (e.g., \'xavier_uniform_\', \'kaiming_normal_\') and their corresponding arguments in a tuple. Returns: - None: The function should modify the model in-place. for name, layer in model.named_modules(): layer_type = type(layer).__name__ if layer_type in init_config: init_method = getattr(init, init_config[layer_type][0]) init_args = init_config[layer_type][1:] if len(init_config[layer_type]) > 1 else () if isinstance(layer, (nn.Conv2d, nn.Linear)): # Apply initialization to weights init_method(layer.weight, *init_args) if layer.bias is not None: init.zeros_(layer.bias)"},{"question":"# Telnet Client Implementation and Custom Interaction Background You are requested to implement a Python module that interacts with a Telnet server using the `telnetlib` package. The tasks will involve connecting to the server, performing simple commands, and handling custom Telnet options using callbacks. This exercise will test your understanding of connection management, reading and writing data, and implementing custom behavior through callbacks. Requirements 1. **Establish a Connection**: Implement a function `connect_to_server(host: str, port: int, timeout: int) -> telnetlib.Telnet` that connects to the given Telnet server and returns the active `Telnet` object. 2. **Login**: Implement a function `login(tn: telnetlib.Telnet, username: str, password: str) -> None` that handles logging into the server. Assume the server prompts with \\"login: \\" for the username and \\"Password: \\" for the password. 3. **Execute Command and Read Output**: Implement a function `execute_command(tn: telnetlib.Telnet, command: str, timeout: int) -> str` that sends a command to the server and returns the output as a string. Use the `read_until` method to wait for the command prompt or the end of the output. 4. **Custom Option Handling**: Implement a function `set_custom_option_callback(tn: telnetlib.Telnet) -> None` that sets a custom callback to handle Telnet options. The callback should simply print out the option negotiation command and option, formatted as \\"Command: {command}, Option: {option}\\". 5. **Interact with Server**: Implement a function `interactive_session(host: str, port: int, username: str, password: str, command_list: list[str]) -> list[str]` which: - Connects to the server using `connect_to_server`. - Logs into the server using `login`. - Sets the custom option callback using `set_custom_option_callback`. - Executes each command in `command_list` using `execute_command` and collects their outputs. - Closes the connection. - Returns a list of outputs for each command executed. Constraints - The methods need to handle possible exceptions, such as connection errors or read/write errors, gracefully. - Aim to minimize blocking operations to ensure responsive interactions with the server. - Ensure all byte strings are appropriately encoded/decoded. Example Usage ```python if __name__ == \\"__main__\\": host = \\"localhost\\" port = 23 username = \\"testuser\\" password = \\"testpass\\" commands = [\\"ls\\", \\"whoami\\", \\"date\\"] outputs = interactive_session(host, port, username, password, commands) for i, output in enumerate(outputs): print(f\\"Output of command \'{commands[i]}\':n{output}n\\") ``` **Note**: This example assumes a mock or real Telnet server responding to Unix-like command prompts.","solution":"import telnetlib def connect_to_server(host: str, port: int, timeout: int) -> telnetlib.Telnet: try: tn = telnetlib.Telnet(host, port, timeout) return tn except Exception as e: print(f\\"Failed to connect to server: {e}\\") return None def login(tn: telnetlib.Telnet, username: str, password: str) -> None: try: tn.read_until(b\\"login: \\") tn.write(username.encode(\'ascii\') + b\\"n\\") tn.read_until(b\\"Password: \\") tn.write(password.encode(\'ascii\') + b\\"n\\") except Exception as e: print(f\\"Login failed: {e}\\") def execute_command(tn: telnetlib.Telnet, command: str, timeout: int) -> str: try: tn.write(command.encode(\'ascii\') + b\\"n\\") output = tn.read_until(b\\" \\", timeout).decode(\'ascii\') return output except Exception as e: print(f\\"Command execution failed: {e}\\") return \\"\\" def custom_option_callback(tsocket, command, option): print(f\\"Command: {command}, Option: {option}\\") def set_custom_option_callback(tn: telnetlib.Telnet) -> None: tn.set_option_negotiation_callback(custom_option_callback) def interactive_session(host: str, port: int, username: str, password: str, command_list: list[str]) -> list[str]: outputs = [] try: tn = connect_to_server(host, port, 10) if tn is None: return outputs login(tn, username, password) set_custom_option_callback(tn) for command in command_list: output = execute_command(tn, command, 10) outputs.append(output) tn.close() except Exception as e: print(f\\"Interactive session failed: {e}\\") return outputs"},{"question":"# CSV Data Processing You are provided with a CSV file named `employees.csv` that contains information about employees in a company. Each row in the CSV file follows this format: ``` employee_id, first_name, last_name, email, department ``` You are required to implement a function named `process_employee_data` that reads the data from `employees.csv`, filters the employees from the \\"Engineering\\" department, converts their emails to lowercase, and then writes the processed data to a new CSV file named `engineering_employees.csv` using a custom CSV dialect. Your implementation should include the following steps: 1. Define a custom CSV dialect named `CustomDialect` with the following properties: - Delimiter: TAB (`\'t\'`) - Quote character: Double Quote (`\'\\"\'`) - Quoting: Quote all fields (`csv.QUOTE_ALL`) 2. Read the data from `employees.csv` using `csv.DictReader`. 3. Filter out the employees whose `department` is \\"Engineering\\" and convert their emails to lowercase. 4. Write the filtered and processed data to `engineering_employees.csv` using `csv.DictWriter`. Function Signature ```python def process_employee_data(): pass ``` Input Format The function does not take any parameters. The CSV file `employees.csv` should be present in the current working directory with the following content: ```plaintext employee_id,first_name,last_name,email,department 101,John,Doe,John.Doe@example.com,Engineering 102,Jane,Smith,Jane.Smith@example.com,HR 103,David,Brown,David.Brown@example.com,Engineering 104,Mary,Johnson,Mary.Johnson@example.com,Marketing ``` Output Format The function should write the processed data to a CSV file named `engineering_employees.csv` in the current working directory with the following content: ```plaintext \\"employee_id\\" \\"first_name\\" \\"last_name\\" \\"email\\" \\"department\\" \\"101\\" \\"John\\" \\"Doe\\" \\"john.doe@example.com\\" \\"Engineering\\" \\"103\\" \\"David\\" \\"Brown\\" \\"david.brown@example.com\\" \\"Engineering\\" ``` Constraints - You must use the `csv` module for reading and writing the CSV files. - Ensure that the custom CSV dialect is registered and used correctly. - Handle any CSV errors gracefully. Example After running the `process_employee_data` function, the `engineering_employees.csv` file should contain: ```plaintext \\"employee_id\\" \\"first_name\\" \\"last_name\\" \\"email\\" \\"department\\" \\"101\\" \\"John\\" \\"Doe\\" \\"john.doe@example.com\\" \\"Engineering\\" \\"103\\" \\"David\\" \\"Brown\\" \\"david.brown@example.com\\" \\"Engineering\\" ``` Additional Information Ensure the code is well-documented and follows Python conventions.","solution":"import csv def process_employee_data(): class CustomDialect(csv.Dialect): delimiter = \'t\' quotechar = \'\\"\' quoting = csv.QUOTE_ALL lineterminator = \'n\' # Register the custom dialect csv.register_dialect(\\"custom\\", CustomDialect) with open(\'employees.csv\', mode=\'r\') as infile: reader = csv.DictReader(infile) employees = [row for row in reader if row[\'department\'] == \'Engineering\'] # Convert emails to lowercase for employee in employees: employee[\'email\'] = employee[\'email\'].lower() with open(\'engineering_employees.csv\', mode=\'w\', newline=\'\') as outfile: fieldnames = [\'employee_id\', \'first_name\', \'last_name\', \'email\', \'department\'] writer = csv.DictWriter(outfile, fieldnames=fieldnames, dialect=\'custom\') writer.writeheader() writer.writerows(employees)"},{"question":"Coding Assessment Question # Objective Demonstrate your understanding of Linear Discriminant Analysis (LDA) and Quadratic Discriminant Analysis (QDA) using the `sklearn.discriminant_analysis` module. You will need to perform classification on a synthetic dataset, compare the performance of LDA and QDA, and apply LDA for dimensionality reduction. # Problem Statement 1. **Data Generation and Preparation** - Generate a synthetic dataset suitable for multiclass classification, with at least 3 classes and 2 features. Ensure your dataset is linearly and quadratically separable. 2. **Classifiers Implementation** - Implement two classifiers, one using Linear Discriminant Analysis (LDA) and the other using Quadratic Discriminant Analysis (QDA). - Train and evaluate both classifiers on the synthetic dataset. - Compare their decision boundaries and classification accuracy. 3. **Dimensionality Reduction** - Use LDA for supervised dimensionality reduction on the synthetic dataset. - Project the dataset onto a linear subspace with a dimension less than the number of classes. - Visualize the original dataset and the transformed dataset on the reduced dimensions. 4. **Shrinkage and Covariance Estimation** - Apply shrinkage to the LDA classifier\'s covariance estimation. Compare performance with and without shrinkage. - Use a custom covariance estimator (e.g., `OAS`) for the LDA classifier. Assess how this affects the classifier\'s performance. # Requirements - **Input and Output Formats:** - Input: None (data is generated within the script). - Output: Print or return the accuracy scores of the classifiers and visualize decision boundaries and dimensionality reduction results using plots. - **Constraints:** - The synthetic dataset should contain at least 300 samples. - Ensure LDA dimensionality reduction reduces the feature space to at least 1 dimension. - Use appropriate visualization techniques to clearly show decision boundaries and dimensionality-reduced spaces. - **Performance Requirements:** - Efficient computation of classifiers and dimensionality reduction. - Clear and coherent visualization of results. # Example Code Below is a template to help you get started: ```python import numpy as np import matplotlib.pyplot as plt from sklearn.datasets import make_classification from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis from sklearn.covariance import OAS from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score # Step 1: Data Generation X, y = make_classification(n_samples=300, n_features=2, n_classes=3, n_clusters_per_class=1) # Train-Test Split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Step 2: Classifiers Implementation # Implement LDA and QDA and compare decision boundaries and accuracies lda = LinearDiscriminantAnalysis() qda = QuadraticDiscriminantAnalysis() lda.fit(X_train, y_train) qda.fit(X_train, y_train) y_pred_lda = lda.predict(X_test) y_pred_qda = qda.predict(X_test) print(\\"LDA Accuracy:\\", accuracy_score(y_test, y_pred_lda)) print(\\"QDA Accuracy:\\", accuracy_score(y_test, y_pred_qda)) # Step 3: Dimensionality Reduction Using LDA lda_for_reduction = LinearDiscriminantAnalysis(n_components=2) X_r = lda_for_reduction.fit_transform(X, y) plt.figure() colors = [\'red\', \'green\', \'blue\'] for color, i, label in zip(colors, [0, 1, 2], [\'class1\', \'class2\', \'class3\']): plt.scatter(X_r[y == i, 0], X_r[y == i, 1], alpha=.8, color=color, label=label) plt.legend(loc=\'best\', shadow=False, scatterpoints=1) plt.title(\'LDA Projection\') plt.show() # Step 4: Applying Shrinkage and Covariance Estimation lda_shrinkage = LinearDiscriminantAnalysis(solver=\'lsqr\', shrinkage=\'auto\') lda_oas = LinearDiscriminantAnalysis(solver=\'lsqr\', covariance_estimator=OAS()) lda_shrinkage.fit(X_train, y_train) lda_oas.fit(X_train, y_train) y_pred_shrinkage = lda_shrinkage.predict(X_test) y_pred_oas = lda_oas.predict(X_test) print(\\"LDA with Shrinkage Accuracy:\\", accuracy_score(y_test, y_pred_shrinkage)) print(\\"LDA with OAS Covariance Estimator Accuracy:\\", accuracy_score(y_test, y_pred_oas)) ``` Provide clear and well-commented code along with your analysis of the results.","solution":"import numpy as np import matplotlib.pyplot as plt from sklearn.datasets import make_classification from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis from sklearn.covariance import OAS from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score def generate_data(): Generate synthetic dataset for multiclass classification with 3 classes and 2 features. X, y = make_classification(n_samples=300, n_features=2, n_informative=2, n_redundant=0, n_classes=3, n_clusters_per_class=1, class_sep=2, random_state=42) return X, y def train_classifiers(X_train, y_train, X_test, y_test): Train LDA and QDA classifiers and evaluate their performance. lda = LinearDiscriminantAnalysis() qda = QuadraticDiscriminantAnalysis() lda.fit(X_train, y_train) qda.fit(X_train, y_train) y_pred_lda = lda.predict(X_test) y_pred_qda = qda.predict(X_test) lda_acc = accuracy_score(y_test, y_pred_lda) qda_acc = accuracy_score(y_test, y_pred_qda) return lda, qda, lda_acc, qda_acc def lda_dimensionality_reduction(X, y): Apply LDA for supervised dimensionality reduction. lda = LinearDiscriminantAnalysis(n_components=2) X_r = lda.fit_transform(X, y) return X_r def apply_shrinkage_and_covariance_estimation(X_train, y_train, X_test, y_test): Apply shrinkage and custom covariance estimation to the LDA classifier and evaluate performance. lda_shrinkage = LinearDiscriminantAnalysis(solver=\'lsqr\', shrinkage=\'auto\') lda_oas = LinearDiscriminantAnalysis(solver=\'lsqr\', covariance_estimator=OAS()) lda_shrinkage.fit(X_train, y_train) lda_oas.fit(X_train, y_train) y_pred_shrinkage = lda_shrinkage.predict(X_test) y_pred_oas = lda_oas.predict(X_test) acc_shrinkage = accuracy_score(y_test, y_pred_shrinkage) acc_oas = accuracy_score(y_test, y_pred_oas) return acc_shrinkage, acc_oas def visualize_results(X, y, X_r): Visualize the original dataset and the dataset after LDA transformation. plt.figure(figsize=(12, 6)) plt.subplot(1, 2, 1) for i, color, label in zip([0, 1, 2], [\'red\', \'green\', \'blue\'], [\'class1\', \'class2\', \'class3\']): plt.scatter(X[y == i, 0], X[y == i, 1], color=color, label=label) plt.title(\'Original Dataset\') plt.legend(loc=\'best\') plt.subplot(1, 2, 2) for i, color, label in zip([0, 1, 2], [\'red\', \'green\', \'blue\'], [\'class1\', \'class2\', \'class3\']): plt.scatter(X_r[y == i, 0], X_r[y == i, 1], color=color, label=label) plt.title(\'LDA Reduced Dataset\') plt.legend(loc=\'best\') plt.show() def main(): # Generate synthetic dataset X, y = generate_data() # Train-test split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Train classifiers and evaluate performance lda, qda, lda_acc, qda_acc = train_classifiers(X_train, y_train, X_test, y_test) print(\\"LDA Accuracy:\\", lda_acc) print(\\"QDA Accuracy:\\", qda_acc) # Dimensionality reduction using LDA X_r = lda_dimensionality_reduction(X, y) # Visualize results visualize_results(X, y, X_r) # Apply shrinkage and covariance estimation acc_shrinkage, acc_oas = apply_shrinkage_and_covariance_estimation(X_train, y_train, X_test, y_test) print(\\"LDA with Shrinkage Accuracy:\\", acc_shrinkage) print(\\"LDA with OAS Covariance Estimator Accuracy:\\", acc_oas) if __name__ == \\"__main__\\": main()"},{"question":"**Problem: Parallelizing Model Training with scikit-learn and joblib** **Context**: You are tasked with implementing a machine learning model to classify a dataset. The training of this model is computationally intensive and can benefit from parallel processing to improve runtime performance. **Task**: 1. Load the digits dataset from `sklearn.datasets`. 2. Implement a function `train_model(n_jobs)` that: - Initializes a `RandomForestClassifier` with `n_estimators=50`. - Uses the `n_jobs` parameter to control the number of jobs for parallel processing. - Trains the classifier on the dataset. - Measures the training time using the `time` module and returns the training time in seconds. 3. Implement another function `evaluate_parallelism()` that: - Calls `train_model(n_jobs)` for `n_jobs` values in the range `[1, 2, 4, 8]`. - Plots a graph using `matplotlib` showing the relationship between `n_jobs` values and the corresponding training times. - Analyzes and prints out the results, discussing the performance implications of different `n_jobs` settings. **Constraints and Requirements**: - Use the `time` module to measure training times. - Ensure parallel processing configurations are properly handled. - Your code must be efficient and should not overwhelm the CPU (beware of oversubscription). **Input**: - None directly; the dataset is loaded within the functions. **Output**: - A plot showing training times for different `n_jobs` values. - Printed text summarizing the performance implications. **Hints**: - Use `RandomForestClassifier` from `sklearn.ensemble`. - Load the digits dataset using `sklearn.datasets.load_digits`. - Measure time using `time.time()` before and after training the model. ```python import time from sklearn.datasets import load_digits from sklearn.ensemble import RandomForestClassifier import matplotlib.pyplot as plt def train_model(n_jobs): Trains a RandomForestClassifier on the digits dataset with the specified number of jobs. Parameters: n_jobs (int): Number of parallel jobs to run Returns: float: Training time in seconds digits = load_digits() X, y = digits.data, digits.target clf = RandomForestClassifier(n_estimators=50, n_jobs=n_jobs) start_time = time.time() clf.fit(X, y) training_time = time.time() - start_time return training_time def evaluate_parallelism(): Evaluates the performance of RandomForestClassifier training with different n_jobs settings. Plots the training times and prints a summary of the performance implications. n_jobs_values = [1, 2, 4, 8] training_times = [train_model(n) for n in n_jobs_values] plt.plot(n_jobs_values, training_times, marker=\'o\') plt.title(\'Training Time vs Number of Jobs\') plt.xlabel(\'Number of Jobs (n_jobs)\') plt.ylabel(\'Training Time (seconds)\') plt.grid(True) plt.show() for n, t in zip(n_jobs_values, training_times): print(f\\"n_jobs = {n}: Training time = {t:.2f} seconds\\") print(\\"Analysis:\\") print(\\"Compare the above results and discuss how the training time changes with the number of jobs.\\") print(\\"Note if there are diminishing returns or oversubscription penalties.\\") evaluate_parallelism() ``` **Expected Output**: - A plot showing the training times for different numbers of jobs. - A printed summary with insights on the usage of parallelism in scikit-learn.","solution":"import time from sklearn.datasets import load_digits from sklearn.ensemble import RandomForestClassifier import matplotlib.pyplot as plt def train_model(n_jobs): Trains a RandomForestClassifier on the digits dataset with the specified number of jobs. Parameters: n_jobs (int): Number of parallel jobs to run Returns: float: Training time in seconds digits = load_digits() X, y = digits.data, digits.target clf = RandomForestClassifier(n_estimators=50, n_jobs=n_jobs) start_time = time.time() clf.fit(X, y) training_time = time.time() - start_time return training_time def evaluate_parallelism(): Evaluates the performance of RandomForestClassifier training with different n_jobs settings. Plots the training times and prints a summary of the performance implications. n_jobs_values = [1, 2, 4, 8] training_times = [train_model(n) for n in n_jobs_values] plt.plot(n_jobs_values, training_times, marker=\'o\') plt.title(\'Training Time vs Number of Jobs\') plt.xlabel(\'Number of Jobs (n_jobs)\') plt.ylabel(\'Training Time (seconds)\') plt.grid(True) plt.show() for n, t in zip(n_jobs_values, training_times): print(f\\"n_jobs = {n}: Training time = {t:.2f} seconds\\") print(\\"Analysis:\\") print(\\"Compare the above results and discuss how the training time changes with the number of jobs.\\") print(\\"Note if there are diminishing returns or oversubscription penalties.\\")"},{"question":"**Objective:** Demonstrate your understanding of Support Vector Machines (SVM) using the `scikit-learn` library by implementing, tuning, and evaluating SVM models on a real-world dataset. **Problem:** You are given the Iris dataset, a famous dataset in the machine learning community. Your task is to: 1. Load the dataset and split it into training and test sets. 2. Train three different SVM models (`SVC` with RBF kernel, `LinearSVC`, and `NuSVC` with polynomial kernel). 3. Evaluate each model using accuracy as the primary metric. 4. Perform hyper-parameter tuning using GridSearchCV for `SVC` with RBF kernel. 5. Compare the performance of the models. **Instructions:** 1. Load the Iris dataset from `sklearn.datasets`. 2. Split the data into training and test sets using `train_test_split` (use `test_size=0.3` and `random_state=42`). 3. Train the following SVM models: - `SVC` with RBF kernel (`kernel=\'rbf\'`) - `LinearSVC` - `NuSVC` with polynomial kernel (`kernel=\'poly\'`) 4. Evaluate the models on the test set using accuracy. 5. Implement hyper-parameter tuning for `SVC` with RBF kernel using `GridSearchCV`. Focus on tuning `C` and `gamma`. 6. Compare the accuracy of the models and determine which one performs best. **Requirements:** - Use `accuracy_score` from `sklearn.metrics` for evaluation. - Use `GridSearchCV` from `sklearn.model_selection` for hyper-parameter tuning. - Plot the decision boundaries for the best-tuned SVM model if possible (optional for extra challenge). **Expected Function Signatures:** ```python from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.svm import SVC, LinearSVC, NuSVC from sklearn.metrics import accuracy_score import numpy as np def load_and_split_data(test_size: float = 0.3, random_state: int = 42): # Your code here pass def train_svc_rbf(X_train, y_train): # Your code here pass def train_linear_svc(X_train, y_train): # Your code here pass def train_nusvc_polynomial(X_train, y_train): # Your code here pass def evaluate_model(model, X_test, y_test): # Your code here pass def tune_svc_rbf(X_train, y_train): # Your code here pass def compare_models(models, X_test, y_test): # Your code here pass # Example usage: X_train, X_test, y_train, y_test = load_and_split_data() svc_rbf_model = train_svc_rbf(X_train, y_train) linear_svc_model = train_linear_svc(X_train, y_train) nusvc_poly_model = train_nusvc_polynomial(X_train, y_train) svc_rbf_accuracy = evaluate_model(svc_rbf_model, X_test, y_test) linear_svc_accuracy = evaluate_model(linear_svc_model, X_test, y_test) nusvc_poly_accuracy = evaluate_model(nusvc_poly_model, X_test, y_test) best_svc_model = tune_svc_rbf(X_train, y_train) best_svc_accuracy = evaluate_model(best_svc_model, X_test, y_test) compare_models([svc_rbf_model, linear_svc_model, nusvc_poly_model, best_svc_model], X_test, y_test) ``` **Note:** - Make sure to handle any errors raised during model training or evaluation, such as convergence warnings for `LinearSVC`. - For hyper-parameter tuning, you may choose reasonable ranges for `C` and `gamma` based on initial evaluations.","solution":"from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.svm import SVC, LinearSVC, NuSVC from sklearn.metrics import accuracy_score import numpy as np # Load and split the data def load_and_split_data(test_size: float = 0.3, random_state: int = 42): iris = load_iris() X = iris.data y = iris.target X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state) return X_train, X_test, y_train, y_test # Train SVC with RBF kernel def train_svc_rbf(X_train, y_train): svc_rbf = SVC(kernel=\'rbf\') svc_rbf.fit(X_train, y_train) return svc_rbf # Train LinearSVC def train_linear_svc(X_train, y_train): linear_svc = LinearSVC() linear_svc.fit(X_train, y_train) return linear_svc # Train NuSVC with polynomial kernel def train_nusvc_polynomial(X_train, y_train): nusvc_poly = NuSVC(kernel=\'poly\') nusvc_poly.fit(X_train, y_train) return nusvc_poly # Evaluate the model using accuracy def evaluate_model(model, X_test, y_test): y_pred = model.predict(X_test) accuracy = accuracy_score(y_test, y_pred) return accuracy # Hyper-parameter tuning for SVC with RBF kernel def tune_svc_rbf(X_train, y_train): param_grid = { \'C\': [0.1, 1, 10, 100], \'gamma\': [1, 0.1, 0.01, 0.001] } grid = GridSearchCV(SVC(kernel=\'rbf\'), param_grid, refit=True) grid.fit(X_train, y_train) return grid.best_estimator_ # Compare the performance of models def compare_models(models, X_test, y_test): best_model = None best_accuracy = 0 for model in models: accuracy = evaluate_model(model, X_test, y_test) print(f\\"Model {model} accuracy: {accuracy}\\") if accuracy > best_accuracy: best_accuracy = accuracy best_model = model print(f\\"Best model is {best_model} with accuracy: {best_accuracy}\\") return best_model, best_accuracy # Example usage: X_train, X_test, y_train, y_test = load_and_split_data() svc_rbf_model = train_svc_rbf(X_train, y_train) linear_svc_model = train_linear_svc(X_train, y_train) nusvc_poly_model = train_nusvc_polynomial(X_train, y_train) svc_rbf_accuracy = evaluate_model(svc_rbf_model, X_test, y_test) linear_svc_accuracy = evaluate_model(linear_svc_model, X_test, y_test) nusvc_poly_accuracy = evaluate_model(nusvc_poly_model, X_test, y_test) best_svc_model = tune_svc_rbf(X_train, y_train) best_svc_accuracy = evaluate_model(best_svc_model, X_test, y_test) compare_models([svc_rbf_model, linear_svc_model, nusvc_poly_model, best_svc_model], X_test, y_test)"},{"question":"You are required to showcase multiple capabilities of the seaborn library by visualizing a dataset effectively. Your task involves creating a comprehensive visualization that compares the distribution of a dataset with various configurations such as bandwidth adjustments, hue mappings, and contour plotting. Follow the steps below to complete the task: Dataset Use the \\"geyser\\" dataset available in seaborn: ```python geyser = sns.load_dataset(\\"geyser\\") ``` Requirements 1. **Plot 1: Univariate KDE Plot** - Plot a KDE distribution of the `waiting` time in the dataset along the x-axis. - Adjust the bandwidth to 0.5. - Provide customizations: title as \\"Univariate KDE Plot - Waiting Time\\", x-axis label as \\"Waiting Time (minutes)\\". 2. **Plot 2: Conditional Distribution Plot** - Plot a KDE distribution of `waiting` time with `kind` as the hue. - Normalize the stacked distribution at each value in the grid. - Provide customizations: title as \\"Conditional KDE Plot - Waiting Time by Kind\\", x-axis label as \\"Waiting Time (minutes)\\". 3. **Plot 3: Bivariate Distribution Plot with Contours** - Plot a bivariate KDE distribution of `waiting` time vs. `duration`. - Add a third variable `kind` with a hue mapping. - Show filled contours with 10 levels and use the \\"mako\\" colormap. - Provide customizations: title as \\"Bivariate KDE Plot - Waiting Time vs Duration\\", x-axis label as \\"Waiting Time (minutes)\\", y-axis label as \\"Duration (minutes)\\". Implementation Write a function `visualize_geyser_data()` that generates the above three plots. Ensure your code includes all necessary imports and dataset loading. ```python import seaborn as sns import matplotlib.pyplot as plt def visualize_geyser_data(): # Load the dataset geyser = sns.load_dataset(\\"geyser\\") # Plot 1: Univariate KDE Plot plt.figure(figsize=(12, 6)) sns.kdeplot(data=geyser, x=\\"waiting\\", bw_adjust=0.5) plt.title(\\"Univariate KDE Plot - Waiting Time\\") plt.xlabel(\\"Waiting Time (minutes)\\") plt.show() # Plot 2: Conditional Distribution Plot plt.figure(figsize=(12, 6)) sns.kdeplot(data=geyser, x=\\"waiting\\", hue=\\"kind\\", multiple=\\"stack\\", common_norm=True) plt.title(\\"Conditional KDE Plot - Waiting Time by Kind\\") plt.xlabel(\\"Waiting Time (minutes)\\") plt.show() # Plot 3: Bivariate Distribution Plot with Contours plt.figure(figsize=(12, 6)) sns.kdeplot(data=geyser, x=\\"waiting\\", y=\\"duration\\", hue=\\"kind\\", fill=True, levels=10, cmap=\\"mako\\") plt.title(\\"Bivariate KDE Plot - Waiting Time vs Duration\\") plt.xlabel(\\"Waiting Time (minutes)\\") plt.ylabel(\\"Duration (minutes)\\") plt.show() ``` Restrictions - Your function should not return any value. It should only produce the plots. - Ensure that the plots are clear and well-labelled. - You are allowed to use the seaborn and matplotlib.pyplot libraries only. **Note:** The function signature should not be changed, and the function should be written to be executable as a standalone function.","solution":"import seaborn as sns import matplotlib.pyplot as plt def visualize_geyser_data(): # Load the dataset geyser = sns.load_dataset(\\"geyser\\") # Plot 1: Univariate KDE Plot plt.figure(figsize=(12, 6)) sns.kdeplot(data=geyser, x=\\"waiting\\", bw_adjust=0.5) plt.title(\\"Univariate KDE Plot - Waiting Time\\") plt.xlabel(\\"Waiting Time (minutes)\\") plt.show() # Plot 2: Conditional Distribution Plot plt.figure(figsize=(12, 6)) sns.kdeplot(data=geyser, x=\\"waiting\\", hue=\\"kind\\", multiple=\\"stack\\", common_norm=True) plt.title(\\"Conditional KDE Plot - Waiting Time by Kind\\") plt.xlabel(\\"Waiting Time (minutes)\\") plt.show() # Plot 3: Bivariate Distribution Plot with Contours plt.figure(figsize=(12, 6)) sns.kdeplot(data=geyser, x=\\"waiting\\", y=\\"duration\\", hue=\\"kind\\", fill=True, levels=10, cmap=\\"mako\\") plt.title(\\"Bivariate KDE Plot - Waiting Time vs Duration\\") plt.xlabel(\\"Waiting Time (minutes)\\") plt.ylabel(\\"Duration (minutes)\\") plt.show()"},{"question":"# Question: Creating and Utilizing Generic Aliases in Python You are required to implement a function that creates and works with generic aliases, simulating how Python handles them internally. Specifically, you need to: 1. Implement a function `create_generic_alias` that takes two parameters: - `origin` (a Python data type). - `args` (a tuple or any type that should be converted to a single-element tuple). This function should return a dictionary representing the `GenericAlias` object with `__origin__` and `__args__` attributes. 2. Implement a function `access_generic_alias` that takes a `GenericAlias` object (represented as a dictionary from the first step) and returns a string description of its `__origin__` and `__args__`. Function Definitions 1. `create_generic_alias(origin: type, args: Union[tuple, Any]) -> dict`: - **Input**: - `origin`: a Python data type (e.g., `list`, `dict`). - `args`: a tuple or any other type. - **Output**: - A dictionary representing the `GenericAlias` object with `__origin__` and `__args__` attributes. - **Constraints**: - If `args` is not a tuple, convert it to a one-element tuple. 2. `access_generic_alias(alias: dict) -> str`: - **Input**: - `alias`: a dictionary representing a `GenericAlias` object with `__origin__` and `__args__` attributes. - **Output**: - A string describing the `__origin__` and `__args__` attributes. - **Constraints**: - The string should be formatted as: `\\"GenericAlias(origin=<type>, args=<tuple>)\\"` Example ```python def create_generic_alias(origin, args): if not isinstance(args, tuple): args = (args,) return {\\"__origin__\\": origin, \\"__args__\\": args} def access_generic_alias(alias): origin = alias[\\"__origin__\\"] args = alias[\\"__args__\\"] return f\\"GenericAlias(origin={origin}, args={args})\\" # Example usage generic_alias = create_generic_alias(list, (int,)) print(access_generic_alias(generic_alias)) # Output: \\"GenericAlias(origin=<class \'list\'>, args=(<class \'int\'>,))\\" generic_alias = create_generic_alias(dict, int) print(access_generic_alias(generic_alias)) # Output: \\"GenericAlias(origin=<class \'dict\'>, args=(<class \'int\'>,))\\" ``` Ensure your implementation handles the constraints and edge cases effectively.","solution":"def create_generic_alias(origin, args): Creates a dictionary simulating a GenericAlias object with __origin__ and __args__ attributes. :param origin: a Python data type (e.g., list, dict). :param args: a tuple or any type that should be converted to a single-element tuple. :return: A dictionary representing the GenericAlias object. if not isinstance(args, tuple): args = (args,) return {\\"__origin__\\": origin, \\"__args__\\": args} def access_generic_alias(alias): Accesses the __origin__ and __args__ attributes of a GenericAlias object and returns a string description. :param alias: a dictionary representing a GenericAlias object. :return: A string describing the __origin__ and __args__ attributes. origin = alias[\\"__origin__\\"] args = alias[\\"__args__\\"] return f\\"GenericAlias(origin={origin}, args={args})\\""},{"question":"**Question: Implement a File Processing System with Exception Handling** **Objective:** Your task is to implement a Python function named `process_file` that processes data from a file, ensuring robust exception handling to manage different error scenarios. **Function Signature:** ```python def process_file(file_path: str, divisor: int) -> float: pass ``` **Input:** - `file_path` (str): The path to a text file that contains a single integer on each line. - `divisor` (int): An integer that will be used to divide the sum of integers read from the file. **Output:** - Returns a float representing the result of the sum of integers in the file divided by the provided divisor. **Requirements:** 1. Raise a `FileNotFoundError` with a custom message if the file does not exist. 2. Raise a `ValueError` with a custom message if the file contains non-integer values. 3. Raise a `ZeroDivisionError` with a custom message if attempting to divide by zero. 4. Ensure the file is closed after processing, whether an exception occurs or not. **Constraints:** - You can assume that the file will contain no more than 1000 lines. - You should use exception chaining where appropriate to provide more context in your error messages. **Example:** 1. **Input:** ```python file_path = \\"numbers.txt\\" divisor = 5 ``` **Content of `numbers.txt`:** ``` 10 20 30 ``` **Output:** ```python 12.0 ``` 2. **Input:** ```python file_path = \\"invalid_numbers.txt\\" divisor = 2 ``` **Content of `invalid_numbers.txt`:** ``` 10 twenty ``` **Output:** ```python ValueError: The file contains non-integer value. ``` 3. **Input:** ```python file_path = \\"numbers.txt\\" divisor = 0 ``` **Output:** ```python ZeroDivisionError: Divisor cannot be zero. ``` 4. **Input:** ```python file_path = \\"missing_file.txt\\" divisor = 5 ``` **Output:** ```python FileNotFoundError: The file could not be found. ``` **Notes:** - Ensure to catch and handle specific exceptions precisely as described. - Use the `with` statement for file operations to handle the file clean-up. **Skeleton Code:** ```python def process_file(file_path: str, divisor: int) -> float: try: with open(file_path, \'r\') as file: try: numbers = [int(line.strip()) for line in file] except ValueError as ve: raise ValueError(\\"The file contains non-integer value.\\") from ve total = sum(numbers) try: result = total / divisor except ZeroDivisionError as zde: raise ZeroDivisionError(\\"Divisor cannot be zero.\\") from zde return result except FileNotFoundError as fnf_error: raise FileNotFoundError(\\"The file could not be found.\\") from fnf_error ``` **Explanation:** 1. The function attempts to open the file specified by `file_path`. 2. It reads and converts each line to an integer. 3. Catches and raises a `ValueError` if any line contains non-integer values. 4. Sums the integers and attempts to divide by the `divisor`. 5. Catches and raises a `ZeroDivisionError` if the `divisor` is zero. 6. Ensures that the file is closed after processing, whether an exception occurs or not.","solution":"def process_file(file_path: str, divisor: int) -> float: Processes a file to sum its integers and divides by a given divisor. Arguments: file_path -- The path to the text file containing integers. divisor -- The integer to divide the sum of integers by. Returns: The result of the sum of the integers divided by the divisor. try: with open(file_path, \'r\') as file: try: numbers = [int(line.strip()) for line in file] except ValueError as ve: raise ValueError(\\"The file contains non-integer value.\\") from ve total = sum(numbers) try: result = total / divisor except ZeroDivisionError as zde: raise ZeroDivisionError(\\"Divisor cannot be zero.\\") from zde return result except FileNotFoundError as fnf_error: raise FileNotFoundError(\\"The file could not be found.\\") from fnf_error"},{"question":"# PyTorch Coding Assessment Question Background In this question, you will be asked to implement a class using PyTorch that demonstrates your understanding of TorchScript and tensor operations. You are required to script a PyTorch model, perform several tensor operations, and ensure the scripted model behaves as expected. Objective You need to implement a PyTorch model class that can be scripted using TorchScript. This model should include: 1. Initializing a tensor as a model parameter. 2. Performing basic tensor operations such as addition and multiplication. 3. Returning certain properties of the tensor upon a forward pass. Task 1. Implement a class `MyScriptedModel` that extends `torch.nn.Module`. 2. The constructor of the class should initialize a tensor `x` as a model parameter. 3. Implement a `forward` method that performs the following operations on the tensor `x`: - Adds a given tensor `y` to `x`. - Multiplies the resulting tensor by a scalar value. - Returns the mean and standard deviation of the resultant tensor. 4. Script the model using TorchScript. 5. Write test cases to verify the functionality of the scripted model. Input and Output Formats **Input:** - A tensor `y` to be added to the model\'s tensor `x`. - A scalar value to multiply the resultant tensor. **Output:** - Mean and standard deviation of the resultant tensor. Constraints - Ensure the tensor operations are performed efficiently. - Handle any possible errors gracefully. - Use TorchScript for scripting the model. Example ```python import torch import torch.nn as nn import torch.jit as jit class MyScriptedModel(nn.Module): def __init__(self): super(MyScriptedModel, self).__init__() self.x = nn.Parameter(torch.randn(3, 3)) def forward(self, y: torch.Tensor, scalar: float): result = self.x + y result = result * scalar return result.mean(), result.std() # Scripting the model model = MyScriptedModel() scripted_model = jit.script(model) # Testing the scripted model y = torch.randn(3, 3) scalar = 2.5 mean, std = scripted_model(y, scalar) print(f\\"Mean: {mean}, Std: {std}\\") ``` **Note:** - You must demonstrate the scripted model using given inputs and verify that the output matches the expected results. - The provided example is for illustrative purposes. Implement your own model class as described.","solution":"import torch import torch.nn as nn import torch.jit as jit class MyScriptedModel(nn.Module): def __init__(self): super(MyScriptedModel, self).__init__() self.x = nn.Parameter(torch.randn(3, 3)) def forward(self, y: torch.Tensor, scalar: float): result = self.x + y result = result * scalar return result.mean(), result.std() # Scripting the model model = MyScriptedModel() scripted_model = jit.script(model) # Example usage (this would normally be in a test function) y = torch.randn(3, 3) scalar = 2.5 mean, std = scripted_model(y, scalar) print(f\\"Mean: {mean}, Std: {std}\\")"},{"question":"Objective Create a Python function to test a production class using the `unittest.mock` module. The function should demonstrate the use of mock objects, patching methods, setting and verifying return values, handling side effects, and validating method call arguments. Problem Description You are given a class `ReportGenerator` that generates reports based on user data and saves the report. It involves interactions with a `DataFetcher` class for fetching data and a `ReportSaver` class for saving the report. Your task is to test the `generate_and_save_report` method of the `ReportGenerator` class by mocking the dependencies (`DataFetcher` and `ReportSaver`), ensuring it behaves correctly. Class Definitions ```python class DataFetcher: def fetch_user_data(self, user_id): # Simulate fetching user data, actually hitting a database or remote server. pass class ReportSaver: def save_report(self, report): # Simulate saving a report, actually writing to disk or sending over the network. pass class ReportGenerator: def __init__(self): self.data_fetcher = DataFetcher() self.report_saver = ReportSaver() def generate_and_save_report(self, user_id): data = self.data_fetcher.fetch_user_data(user_id) if data: report = self._generate_report(data) self.report_saver.save_report(report) return True return False def _generate_report(self, data): # Placeholder for report generation logic based on data. return f\\"Report for data: {data}\\" ``` Task Write a function `test_generate_and_save_report` to test `generate_and_save_report` method of the `ReportGenerator` class. This function should: 1. Mock the `DataFetcher` and `ReportSaver` classes. 2. Ensure `fetch_user_data` is called with the correct user ID. 3. Verify that `save_report` is called with the correctly generated report. 4. Simulate different scenarios using `side_effect`, such as `DataFetcher` returning `None`. 5. Confirm the method returns `True` when data is fetched and saved, and `False` when no data is returned. Expected Function Signature ```python def test_generate_and_save_report(): pass ``` Example Test Case Your tests should ensure the following scenario works correctly: ```python def test_generate_and_save_report(): with patch(\'path.to.your.module.DataFetcher\') as MockDataFetcher, patch(\'path.to.your.module.ReportSaver\') as MockReportSaver: # Arrange mock_data_fetcher = MockDataFetcher.return_value mock_report_saver = MockReportSaver.return_value user_id = \'user123\' data = {\'user\': \'test\'} mock_data_fetcher.fetch_user_data.return_value = data generator = ReportGenerator() # Act result = generator.generate_and_save_report(user_id) # Assert mock_data_fetcher.fetch_user_data.assert_called_once_with(user_id) mock_report_saver.save_report.assert_called_once_with(\\"Report for data: {\'user\': \'test\'}\\") assert result is True # Scenario: No data returned mock_data_fetcher.fetch_user_data.return_value = None # Act result = generator.generate_and_save_report(user_id) # Assert mock_report_saver.save_report.assert_not_called() assert result is False ``` Constraints - Ensure the correct file paths are used in your `patch` decorators. - Your tests should cover at least two scenarios: successful report generation and failure when no data is fetched. - Use appropriate assertions to verify the behavior.","solution":"from unittest.mock import patch class DataFetcher: def fetch_user_data(self, user_id): # Simulate fetching user data, actually hitting a database or remote server. pass class ReportSaver: def save_report(self, report): # Simulate saving a report, actually writing to disk or sending over the network. pass class ReportGenerator: def __init__(self): self.data_fetcher = DataFetcher() self.report_saver = ReportSaver() def generate_and_save_report(self, user_id): data = self.data_fetcher.fetch_user_data(user_id) if data: report = self._generate_report(data) self.report_saver.save_report(report) return True return False def _generate_report(self, data): # Placeholder for report generation logic based on data. return f\\"Report for data: {data}\\""},{"question":"**Question: Implementing a Custom Rendezvous Handler in PyTorch Distributed Elastic** In distributed training, nodes need to find each other and synchronize at the beginning of the training process. This process is known as \\"rendezvous.\\" In PyTorch Distributed Elastic, the rendezvous process is abstracted by handlers. Your task is to implement a custom rendezvous handler that follows a simple file-based approach to coordinate node discovery. # Specifications: 1. **Input and Output:** - The handler should read from a file that contains the list of active nodes. - Each line in the file represents a single node\'s address. - The handler should return a list of nodes and a current leader node (the first node in the file list). 2. **Constraints:** - The file path should be provided as an input to the handler. - Nodes and the leader node should be returned as a list of strings. 3. **Function Signature:** ```python class FileBasedRendezvousHandler: def __init__(self, file_path: str): Initializes the handler with the path to the file containing node addresses. pass def get_nodes(self) -> (list, str): Reads the node addresses from the file and returns the list of nodes and the leader node. Returns: nodes (list): List of node addresses. leader_node (str): Address of the leader node. pass ``` 4. **Example:** - If the file `node_list.txt` contains: ``` 192.168.1.1 192.168.1.2 192.168.1.3 ``` - Then, calling `get_nodes()` should return: ```python ([\\"192.168.1.1\\", \\"192.168.1.2\\", \\"192.168.1.3\\"], \\"192.168.1.1\\") ``` 5. **Advanced Consideration:** - Implement mechanisms to handle file read errors and empty files. - Log meaningful error messages when exceptions are raised. # Performance Requirements: - Assume that the file does not contain more than 100 node addresses. - The handler should read the file in an efficient manner, considering potential concurrent access in a distributed setting. # Evaluation Criteria: - Correctness: The implementation should correctly read and parse the node addresses from the file. - Robustness: The handler should gracefully handle common errors and edge cases. - Code Quality: The code should be clean, well-documented, and adhere to Python coding standards. **Note:** This question assesses students\' understanding of PyTorch Distributed Elastic\'s rendezvous process, file handling in Python, and error handling practices in a concurrent environment.","solution":"class FileBasedRendezvousHandler: def __init__(self, file_path: str): Initializes the handler with the path to the file containing node addresses. Args: file_path (str): Path to the file with node addresses. self.file_path = file_path def get_nodes(self) -> (list, str): Reads the node addresses from the file and returns the list of nodes and the leader node. Returns: nodes (list): List of node addresses. leader_node (str): Address of the leader node. try: with open(self.file_path, \'r\') as file: nodes = [line.strip() for line in file if line.strip()] if not nodes: raise ValueError(\\"The node list file is empty.\\") leader_node = nodes[0] return nodes, leader_node except FileNotFoundError: raise FileNotFoundError(f\\"The file at {self.file_path} does not exist.\\") except IOError as e: raise IOError(f\\"An error occurred while reading the file: {str(e)}\\")"},{"question":"Advanced Module Management in Python Objective You are tasked with implementing a utility that manages a collection of Python modules dynamically. Your task is to create functions that: 1. **Dynamically import a specified module**. 2. **Reload the specified module**. 3. **Log metadata about the imported module** (such as whether it was successfully imported or if errors occurred). Requirements 1. Implement a function `dynamic_import(module_name: str) -> Dict[str, Any]` that: - Imports a module given its `module_name`. - Returns a dictionary with the following keys: - `\'module\'`: the imported module object (if successfully imported). - `\'error\'`: the exception message (if any exception occurred). 2. Implement a function `reload_module(module_obj: Any) -> Dict[str, Any]` that: - Reloads the given module object. - Returns a dictionary with the following keys: - `\'module\'`: the reloaded module object (if successfully reloaded). - `\'error\'`: the exception message (if any exception occurred). 3. Implement a function `log_module_info(module_name: str, info: Dict[str, Any]) -> None` that: - Takes the module name and the dictionary returned by `dynamic_import` or `reload_module`. - Logs whether the module was successfully imported/reloaded or what the error was. - Uses Python\'s logging library. Constraints - Proper error handling should be demonstrated in your implementations. - You should utilize Python’s standard library functions to achieve the import and reload operations. - Log messages should be clear and informative. Example Usage ```python import logging logging.basicConfig(level=logging.INFO) # 1. Import module import_result = dynamic_import(\'json\') log_module_info(\'json\', import_result) # 2. Reload module if import_result[\'module\']: reload_result = reload_module(import_result[\'module\']) log_module_info(\'json\', reload_result) ``` Expected Dictionary Format ```python # Successful Import/Reload { \'module\': <module \'json\' from \'/usr/lib/python3.10/json/__init__.py\'>, \'error\': None } # Failed Import/Reload { \'module\': None \'error\': \'ModuleNotFoundError: No module named xyz\' } ``` Implement the three functions as described and ensure your code is well-tested and documented.","solution":"import importlib import logging from typing import Dict, Any logging.basicConfig(level=logging.INFO) def dynamic_import(module_name: str) -> Dict[str, Any]: Imports a module given its module_name. Args: module_name (str): The name of the module to import. Returns: Dict[str, Any]: A dictionary containing the module object (if successfully imported) and an error message (if an exception occurred). result = {\'module\': None, \'error\': None} try: module = importlib.import_module(module_name) result[\'module\'] = module except Exception as e: result[\'error\'] = str(e) return result def reload_module(module_obj: Any) -> Dict[str, Any]: Reloads the given module object. Args: module_obj (Any): The module object to reload. Returns: Dict[str, Any]: A dictionary containing the reloaded module object (if successfully reloaded) and an error message (if an exception occurred). result = {\'module\': None, \'error\': None} try: module = importlib.reload(module_obj) result[\'module\'] = module except Exception as e: result[\'error\'] = str(e) return result def log_module_info(module_name: str, info: Dict[str, Any]) -> None: Logs whether the module was successfully imported/reloaded or what the error was. Args: module_name (str): The name of the module. info (Dict[str, Any]): A dictionary containing the module object or an error message. if info[\'module\'] is not None: logging.info(f\\"Successfully imported/reloaded module \'{module_name}\'.\\") else: logging.error(f\\"Failed to import/reload module \'{module_name}\': {info[\'error\']}\\")"},{"question":"# Question: Advanced Index Manipulations You are provided with a pandas DataFrame containing details of products in a store. The DataFrame has the following columns: - `product_id` (string) : A unique identifier for each product. - `category` (string) : The category to which the product belongs. - `price` (float) : The price of the product. - `quantity_sold` (int) : The number of units sold. Your task is to perform the following operations using the pandas `Index` methods: 1. **Set the Index**: First, set the `product_id` as the index of the DataFrame. 2. **Identify Duplicates**: Identify and remove duplicate indices if any. 3. **Sort by Index**: Sort the DataFrame based on the `product_id` index in ascending order. 4. **Categorical Filtering**: Filter out entries where `category` is either \'Electronics\' or \'Clothing\'. 5. **Index Analysis**: For the filtered DataFrame, create a summary with the following details: - Check if indices are unique. - Determine the range (minimum to maximum) of the index. - Find and return the most frequent category in the filtered DataFrame. Write a function `process_product_data(data)` that takes a pandas DataFrame `data` as input and returns a summary dictionary with the results of the above operations. ```python import pandas as pd def process_product_data(data): Processes the product data to perform several index-based operations. Parameters: data (pd.DataFrame): A DataFrame containing product data with columns [\'product_id\', \'category\', \'price\', \'quantity_sold\']. Returns: dict: A summary dictionary containing: - \'is_index_unique\': Boolean indicating if the index is unique. - \'index_range\': Tuple representing (min_index, max_index). - \'most_frequent_category\': The most frequent category in the filtered DataFrame. # Set the product_id as the index data.set_index(\'product_id\', inplace=True) # Remove duplicate indices data = data[~data.index.duplicated(keep=\'first\')] # Sort the DataFrame by index data.sort_index(inplace=True) # Filter entries where category is \'Electronics\' or \'Clothing\' filtered_data = data[~data[\'category\'].isin([\'Electronics\', \'Clothing\'])] # Summary calculations is_index_unique = filtered_data.index.is_unique index_range = (filtered_data.index.min(), filtered_data.index.max()) most_frequent_category = filtered_data[\'category\'].value_counts().idxmax() # Summary dictionary summary = { \'is_index_unique\': is_index_unique, \'index_range\': index_range, \'most_frequent_category\': most_frequent_category } return summary # Example usage: # data = pd.DataFrame({ # \'product_id\': [\'p1\', \'p2\', \'p1\', \'p3\'], # \'category\': [\'Electronics\', \'Clothing\', \'Electronics\', \'Home\'], # \'price\': [250, 100, 250, 75], # \'quantity_sold\': [100, 150, 100, 200] # }) # result = process_product_data(data) # print(result) ``` Use the above DataFrame as a test case for your implementation. The expected output should reflect the operations described. **Note**: Ensure you are familiar with the pandas library and its `Index` and `DataFrame` methods to solve this question accurately.","solution":"import pandas as pd def process_product_data(data): Processes the product data to perform several index-based operations. Parameters: data (pd.DataFrame): A DataFrame containing product data with columns [\'product_id\', \'category\', \'price\', \'quantity_sold\']. Returns: dict: A summary dictionary containing: - \'is_index_unique\': Boolean indicating if the index is unique. - \'index_range\': Tuple representing (min_index, max_index). - \'most_frequent_category\': The most frequent category in the filtered DataFrame. # Set the product_id as the index data.set_index(\'product_id\', inplace=True) # Remove duplicate indices data = data[~data.index.duplicated(keep=\'first\')] # Sort the DataFrame by index data.sort_index(inplace=True) # Filter entries where category is \'Electronics\' or \'Clothing\' filtered_data = data[~data[\'category\'].isin([\'Electronics\', \'Clothing\'])] # Summary calculations is_index_unique = filtered_data.index.is_unique index_range = (filtered_data.index.min(), filtered_data.index.max()) most_frequent_category = filtered_data[\'category\'].value_counts().idxmax() # Summary dictionary summary = { \'is_index_unique\': is_index_unique, \'index_range\': index_range, \'most_frequent_category\': most_frequent_category } return summary"},{"question":"# Question: Implement a Custom Slice Processor You are tasked with implementing a custom slice processor in Python that uses the underlying principles detailed in the provided slice object documentation. Your goal is to create a function that processes a slice object, ensures that it is valid, and extracts the start, stop, and step indices, adjusting them as necessary. Function Signature ```python def process_slice(slice_object, sequence_length): Processes a slice object, validates it, and extracts start, stop, and step indices, adjusting as necessary. :param slice_object: The slice object to be processed. :param sequence_length: The length of the sequence to which the slice will be applied. :return: A tuple containing the adjusted (start, stop, step) indices. :raises ValueError: If the provided object is not a valid slice or any other error occurs during processing. ``` Input - `slice_object`: Can be a slice object or any other object that should be checked if it is a slice. - `sequence_length`: An integer representing the length of the sequence. Output - Returns a tuple (start, stop, step) representing the adjusted slice indices. Constraints - You must validate that `slice_object` is indeed a slice. - If `slice_object` is not a valid slice, raise a `ValueError`. - Perform bounds-checking and adjust indices as necessary. - If `start`, `stop`, or `step` are out of range or invalid, raise a `ValueError`. Examples ```python # Example 1: Valid slice slice_obj = slice(1, 10, 2) seq_len = 20 assert process_slice(slice_obj, seq_len) == (1, 10, 2) # Example 2: Negative indices slice_obj = slice(-10, -1, 1) seq_len = 20 assert process_slice(slice_obj, seq_len) == (10, 19, 1) # Example 3: Step of None slice_obj = slice(1, 10, None) seq_len = 20 assert process_slice(slice_obj, seq_len) == (1, 10, 1) # Example 4: Invalid object try: process_slice(5, 20) except ValueError as e: assert str(e) == \\"Provided object is not a valid slice\\" ``` Critical Points 1. **Input Validation**: Ensure the input slice object is valid. 2. **Bounds Checking**: Adjust indices to be within the valid range for the given sequence length. 3. **Edge Cases**: Handle negative indices and `None` values for step. Implement the function with these requirements while ensuring efficiency and correct error handling.","solution":"def process_slice(slice_object, sequence_length): Processes a slice object, validates it, and extracts start, stop, and step indices, adjusting as necessary. :param slice_object: The slice object to be processed. :param sequence_length: The length of the sequence to which the slice will be applied. :return: A tuple containing the adjusted (start, stop, step) indices. :raises ValueError: If the provided object is not a valid slice or any other error occurs during processing. if not isinstance(slice_object, slice): raise ValueError(\\"Provided object is not a valid slice\\") start, stop, step = slice_object.start, slice_object.stop, slice_object.step if step is None: step = 1 # Adjust start index if start is None: start = 0 if step > 0 else sequence_length - 1 elif start < 0: start = max(0, sequence_length + start) else: start = min(sequence_length, start) # Adjust stop index if stop is None: stop = sequence_length if step > 0 else -1 elif stop < 0: stop = max(0, sequence_length + stop) else: stop = min(sequence_length, stop) return start, stop, step"},{"question":"# Python Coding Assessment Question **Objective**: Demonstrate the understanding and capability to use the `importlib` package for custom module importation and reloading in Python. **Problem Statement**: You are required to create a Python script that manages the dynamic import and re-importing of a module using the `importlib` package. You will handle importing a module, checking if it has been modified, and reloading it to reflect the new changes without stopping the script. # Requirements: 1. **Function 1: `dynamic_import(module_name, package_name=None)`** - **Input**: - `module_name` (str): Absolute or relative name of the module to import. - `package_name` (str): Name of the package to resolve relative module names, default is `None`. - **Output**: - The imported module. - **Behavior**: - Dynamically import the specified module using `importlib.import_module`. - Handle potential import errors and return meaningful messages. 2. **Function 2: `reload_module(module)`** - **Input**: - `module`: Module object to be reloaded. - **Output**: - The reloaded module. - **Behavior**: - Reload the provided module using `importlib.reload`. - Handle cases where the module cannot be reloaded and return meaningful messages. 3. **Function 3: `check_and_reload(module_name, package_name=None)`** - **Input**: - `module_name` (str): Absolute or relative name of the module to check and possibly reload. - `package_name` (str): Name of the package to resolve relative module names, default is `None`. - **Output**: - The reloaded module if reloaded successfully or the currently loaded module if no reload was necessary. - **Behavior**: - Use `importlib.invalidate_caches` to ensure importlib recognizes recent changes. - Check if the module is already imported and, if so, reload it. - If the module is not imported, import it dynamically. # Constraints: - Do not use the `__import__` built-in function. Use `importlib` methods exclusively. - Assume the module name and package name, if provided, are valid Python identifiers. - Provide meaningful output messages for any encountered errors. # Example Usage: ```python if __name__ == \\"__main__\\": # Example: importing a module module = dynamic_import(\'my_module\') # Example: reloading the module reloaded_module = check_and_reload(\'my_module\') print(reloaded_module) ``` # Hints: - Use `importlib.import_module` for dynamic imports. - Use `importlib.reload` for reloading modules. - Use `importlib.invalidate_caches` to clear import caches before reloading. - Handle and log errors appropriately using try-except blocks. Implement these functions and ensure your script handles dynamic importing and reloading of modules effectively.","solution":"import importlib def dynamic_import(module_name, package_name=None): Dynamically imports a module using its name. :param module_name: Absolute or relative name of the module to import. :param package_name: Name of the package to resolve relative module names, default is None. :return: The imported module or None if an error occurred. try: module = importlib.import_module(module_name, package=package_name) return module except ImportError as e: print(f\\"Error importing {module_name}: {e}\\") return None def reload_module(module): Reloads the given module. :param module: Module object to be reloaded. :return: The reloaded module or None if an error occurred. try: reloaded_module = importlib.reload(module) return reloaded_module except TypeError as e: print(f\\"Error reloading module {module}: {e}\\") return None def check_and_reload(module_name, package_name=None): Checks if a module is imported, if so, reloads it. Otherwise, imports it dynamically. :param module_name: Absolute or relative name of the module to check and possibly reload. :param package_name: Name of the package to resolve relative module names, default is None. :return: The reloaded module if reloaded successfully, or the currently loaded module, or None if an error occurred. importlib.invalidate_caches() module = importlib.util.find_spec(module_name) if module is None: print(f\\"Module {module_name} not found\\") return None loaded_module = importlib.import_module(module_name) reloaded_module = reload_module(loaded_module) return reloaded_module"},{"question":"# Password Management System using `crypt` Module Objective Implement a password management system that allows users to securely register and authenticate using hashed passwords. The system should use the `crypt` module to handle password hashing. Requirements 1. **Function Implementation:** - **register_user(username: str, password: str) -> str:** - Input: A `username` (string) and a `password` (string). - Output: A string indicating the success or failure of the registration process. - Functionality: Hash the given password using the strongest available method and store the username and hashed password in a dictionary. - Constraints: Username must be unique. If the username already exists, return an appropriate message. - **authenticate_user(username: str, password: str) -> bool:** - Input: A `username` (string) and a `password` (string). - Output: A boolean indicating whether the authentication was successful. - Functionality: Validate the provided password against the stored hashed password using the `crypt` module. - Constraints: Return `False` if the username does not exist. Additional Constraints - Use the `crypt` module for all password-related operations. - Assume the system is case-sensitive. Implementation Details - Maintain user data in a dictionary where keys are usernames and values are their corresponding hashed passwords. - Use `crypt.crypt()` for hashing and `crypt.mksalt()` for salt generation. - Ensure appropriate error handling and clear return messages for registration and authentication functions. Example Usage ```python user_data = {} def register_user(username, password): # Implementation here def authenticate_user(username, password): # Implementation here # Example print(register_user(\\"alice\\", \\"wonderland123\\")) # Expected: \\"Registration successful\\" print(register_user(\\"alice\\", \\"wonderland123\\")) # Expected: \\"Username already exists\\" print(authenticate_user(\\"alice\\", \\"wonderland123\\")) # Expected: True print(authenticate_user(\\"alice\\", \\"wrongpassword\\")) # Expected: False print(authenticate_user(\\"bob\\", \\"somepassword\\")) # Expected: False ``` Performance - User data storage is in memory, so consider the size constraints if dealing with a large number of users. - Password hashing and verification should be efficient but secure against common attacks. Implement the `register_user` and `authenticate_user` functions, ensuring they meet the defined requirements and constraints.","solution":"import crypt # In-memory user data storage user_data = {} def register_user(username, password): Register a user with a given username and password. Args: username (str): The username of the user. password (str): The password of the user. Returns: str: A message indicating the result of the registration process. if username in user_data: return \\"Username already exists\\" # Hash the password with the strongest method available salt = crypt.mksalt(crypt.METHOD_SHA512) hashed_password = crypt.crypt(password, salt) user_data[username] = hashed_password return \\"Registration successful\\" def authenticate_user(username, password): Authenticate a user with a given username and password. Args: username (str): The username of the user. password (str): The password of the user. Returns: bool: True if authentication is successful, False otherwise. if username not in user_data: return False stored_hashed_password = user_data[username] # Hash the input password using the same salt hashed_password = crypt.crypt(password, stored_hashed_password) return hashed_password == stored_hashed_password"},{"question":"# Seaborn Coding Assessment Question Objective Create a comprehensive visualization to analyze the trend and variation of monthly airline passengers over the years using the `seaborn` library. Task Given the `flights` dataset, write a Python function `create_flights_visualization()` that performs the following tasks: 1. Load the `flights` dataset. 2. Generate a line plot that shows the average number of passengers per month, aggregated over the years. 3. Enhance the plot with: - Different colors for each month. - Distinct markers for each month. - Error bars showing the 95% confidence interval for the mean number of passengers. 4. Include a legend and appropriate titles for the plot and axes. # Function Definition ```python def create_flights_visualization() -> None: Loads the \'flights\' dataset and creates a seaborn line plot visualizing the average number of passengers per month over the years with the following features: - Different colors for each month - Distinct markers for each month - Error bars showing the 95% confidence interval for the mean number of passengers - Includes legend and appropriate titles for the plot and axes The function does not return anything. It displays the plot inline. pass ``` # Constraints - Use the seaborn library for plot creation. - Ensure that the plot is clear and informative with distinguishable colors and markers for each month. # Example Output The function should produce a plot similar to this: ![Example Plot](https://seaborn.pydata.org/_images/lineplot_12_0.png) Notes - Pay attention to the use of `hue`, `style`, and error bars in the line plot. - Ensure the plot is labeled appropriately with a title, x-axis label, and y-axis label.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_flights_visualization() -> None: Loads the \'flights\' dataset and creates a seaborn line plot visualizing the average number of passengers per month over the years with the following features: - Different colors for each month - Distinct markers for each month - Error bars showing the 95% confidence interval for the mean number of passengers - Includes legend and appropriate titles for the plot and axes The function does not return anything. It displays the plot inline. # Load the flights dataset from seaborn flights = sns.load_dataset(\\"flights\\") # Create the plot with seaborn plt.figure(figsize=(14, 7)) sns.lineplot( data=flights, x=\'year\', y=\'passengers\', hue=\'month\', style=\'month\', markers=True, ci=95 ) # Add title and labels plt.title(\'Average Monthly Airline Passengers Over Years\') plt.xlabel(\'Year\') plt.ylabel(\'Number of Passengers\') # Display the legend plt.legend(title=\'Month\') # Show the plot plt.tight_layout() plt.show()"},{"question":"# Token Analysis and Conversion The `token` module in Python provides a set of constants that represent various elements of Python\'s grammar and parse tree. In this assessment, you will demonstrate your understanding of this module by implementing a function that processes a list of token values, performs some analysis, and converts them to their human-readable string representations. Problem Statement Write a Python function named `analyze_tokens` that takes a list of token values (integers) and returns a dictionary with the following keys and computed values: 1. `\\"human_readable\\"` - A list of string names corresponding to the numeric token values, using the `token.tok_name` dictionary. 2. `\\"terminal_count\\"` - An integer representing the number of terminal tokens (as determined by the `token.ISTERMINAL` function). 3. `\\"non_terminal_count\\"` - An integer representing the number of non-terminal tokens (as determined by the `token.ISNONTERMINAL` function). 4. `\\"contains_eof\\"` - A boolean indicating whether the end-of-file token is present in the list (as determined by the `token.ISEOF` function). Your function should follow the format outlined below: ```python import token def analyze_tokens(token_list): Analyzes a list of token values and returns a dictionary with specific information. Parameters: token_list (list of int): A list of numeric token values. Returns: dict: A dictionary containing: - \\"human_readable\\" (list of str): List of token names. - \\"terminal_count\\" (int): Number of terminal tokens. - \\"non_terminal_count\\" (int): Number of non-terminal tokens. - \\"contains_eof\\" (bool): Whether the end-of-file token is present. result = { \\"human_readable\\": [], \\"terminal_count\\": 0, \\"non_terminal_count\\": 0, \\"contains_eof\\": False } for tok in token_list: # Convert numeric token to human-readable format. if tok in token.tok_name: result[\\"human_readable\\"].append(token.tok_name[tok]) else: result[\\"human_readable\\"].append(\\"<unknown>\\") # Check if the token is terminal. if token.ISTERMINAL(tok): result[\\"terminal_count\\"] += 1 # Check if the token is non-terminal. if token.ISNONTERMINAL(tok): result[\\"non_terminal_count\\"] += 1 # Check if the token is the end-of-file marker. if token.ISEOF(tok): result[\\"contains_eof\\"] = True return result ``` Constraints - The function should handle an empty list gracefully. - The tokens in the `token_list` will be valid integers that can be found in the `token` module. Example ```python token_list = [1, 2, 3, token.ENDMARKER, token.NAME, token.NUMBER] result = analyze_tokens(token_list) print(result) ``` Expected output: ```python {\'human_readable\': [\'<unknown>\', \'<unknown>\', \'<unknown>\', \'ENDMARKER\', \'NAME\', \'NUMBER\'], \'terminal_count\': 3, \'non_terminal_count\': 0, \'contains_eof\': True} ``` *Note: The \\"<unknown>\\" strings are placeholders for any numeric values that do not have a corresponding name in `token.tok_name`.","solution":"import token def analyze_tokens(token_list): Analyzes a list of token values and returns a dictionary with specific information. Parameters: token_list (list of int): A list of numeric token values. Returns: dict: A dictionary containing: - \\"human_readable\\" (list of str): List of token names. - \\"terminal_count\\" (int): Number of terminal tokens. - \\"non_terminal_count\\" (int): Number of non-terminal tokens. - \\"contains_eof\\" (bool): Whether the end-of-file token is present. result = { \\"human_readable\\": [], \\"terminal_count\\": 0, \\"non_terminal_count\\": 0, \\"contains_eof\\": False } for tok in token_list: # Convert numeric token to human-readable format. if tok in token.tok_name: result[\\"human_readable\\"].append(token.tok_name[tok]) else: result[\\"human_readable\\"].append(\\"<unknown>\\") # Check if the token is terminal. if token.ISTERMINAL(tok): result[\\"terminal_count\\"] += 1 # Check if the token is non-terminal. if token.ISNONTERMINAL(tok): result[\\"non_terminal_count\\"] += 1 # Check if the token is the end-of-file marker. if token.ISEOF(tok): result[\\"contains_eof\\"] = True return result"},{"question":"Objective: You are required to implement a Python function that demonstrates your understanding of integer conversion and handling overflows in Python using the provided API methods for `PyLongObject`. Problem Statement: Write a Python function `convert_to_pyint(lst)` that takes a list of tuples. Each tuple contains an integer and its corresponding type - `long`, `unsigned long`, `size_t`, or `long long`. The function should convert each integer to the appropriate `PyLongObject` using the corresponding conversion function from the provided API. Then, convert it back to the respective C integer type and accumulate the results in a list. If any integer conversion results in an overflow, the function should catch the exception and append an overflow message for that specific integer. Function Signature: ```python def convert_to_pyint(lst: list) -> list: pass ``` Input: - `lst`: A list of tuples where each tuple is of the form `(value, type)`. - `value` (int): The integer value to be converted. - `type` (str): A string representing the type of the integer, which can be one of `\\"long\\"`, `\\"unsigned long\\"`, `\\"size_t\\"`, or `\\"long long\\"`. Output: - The function should return a list of the converted integers or an overflow message in case of exceptions. Constraints: - The input list can contain up to 1000 tuples. - The integer values can range from `-2^63` to `2^63 - 1`. Example: ```python input_data = [ (123, \'long\'), (256, \'unsigned long\'), (-500, \'size_t\'), (9223372036854775807, \'long long\') ] output_data = convert_to_pyint(input_data) print(output_data) ``` Expected output: ```python [123, 256, \'Overflow detected\', 9223372036854775807] ``` Notes: - You should handle overflows and exceptions as specified in the provided documentation. - Use appropriate API functions to ensure correct conversions. - Implement error handling to capture overflows and other exceptions, appending \'Overflow detected\' to the output list in such cases.","solution":"def convert_to_pyint(lst): results = [] for value, int_type in lst: try: if int_type == \\"long\\": converted_value = int(value) if not -(2**31) <= converted_value <= (2**31 - 1): raise OverflowError elif int_type == \\"unsigned long\\": converted_value = int(value) if not 0 <= converted_value <= (2**32 - 1): raise OverflowError elif int_type == \\"size_t\\": converted_value = int(value) if converted_value < 0: raise OverflowError elif int_type == \\"long long\\": converted_value = int(value) if not -(2**63) <= converted_value <= (2**63 - 1): raise OverflowError else: raise ValueError(\\"Unknown type\\") results.append(converted_value) except (OverflowError, ValueError) as e: results.append(\'Overflow detected\') return results"},{"question":"**Composite Estimator Pipeline for Imputation and Classification** # Problem Statement You are provided with a dataset that contains several missing values. You need to build a machine learning pipeline that performs the following steps: 1. Impute the missing values using a combination of two imputation techniques. 2. Fit a classification model on the imputed dataset. 3. Evaluate the model performance on a testing set. # Requirements 1. The dataset should be divided into training and testing sets. 2. The pipeline should implement at least two different imputation strategies (e.g., `SimpleImputer` and `KNNImputer` or `IterativeImputer`). 3. Use any classification algorithm of your choice (e.g., `DecisionTreeClassifier`, `RandomForestClassifier`, etc.). 4. Print the accuracy of the classifier on the testing set. # Input Format A CSV file containing the dataset. The target variable (to be predicted) is labeled as `target`. # Output Format Print the classification accuracy on the testing set. # Constraints - Ensure the pipeline can handle numerical and categorical data. - You can assume the missing values are represented by `np.nan`. # Example Usage ```python import numpy as np import pandas as pd from sklearn.impute import SimpleImputer, KNNImputer from sklearn.model_selection import train_test_split from sklearn.pipeline import Pipeline, FeatureUnion from sklearn.compose import ColumnTransformer from sklearn.tree import DecisionTreeClassifier from sklearn.metrics import accuracy_score # Load dataset df = pd.read_csv(\'path_to_csv_file\') X = df.drop(columns=\'target\') y = df[\'target\'] # Split dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Define the pipeline numeric_features = X.select_dtypes(include=[\'float64\', \'int64\']).columns categorical_features = X.select_dtypes(include=[\'object\']).columns numeric_transformer = Pipeline(steps=[ (\'imputer\', SimpleImputer(strategy=\'mean\')), (\'knnimputer\', KNNImputer(n_neighbors=2)) ]) categorical_transformer = Pipeline(steps=[ (\'imputer\', SimpleImputer(strategy=\'most_frequent\')) ]) preprocessor = ColumnTransformer( transformers=[ (\'num\', numeric_transformer, numeric_features), (\'cat\', categorical_transformer, categorical_features) ]) pipeline = Pipeline(steps=[ (\'preprocessor\', preprocessor), (\'classifier\', DecisionTreeClassifier()) ]) # Train the pipeline pipeline.fit(X_train, y_train) # Predict and evaluate y_pred = pipeline.predict(X_test) accuracy = accuracy_score(y_test, y_pred) print(f\'Accuracy: {accuracy:.2f}\') ``` # Submission Submit a Python script or Jupyter Notebook that performs the task described above. Ensure that your code is well-documented and adheres to best practices for readability and efficiency.","solution":"import numpy as np import pandas as pd from sklearn.impute import SimpleImputer, KNNImputer from sklearn.model_selection import train_test_split from sklearn.pipeline import Pipeline, FeatureUnion from sklearn.compose import ColumnTransformer from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import accuracy_score def load_and_prepare_data(file_path): Loads the CSV file and prepares the dataset for training and testing. Parameters: - file_path: str, path to the CSV file Returns: - X_train, X_test, y_train, y_test: split data df = pd.read_csv(file_path) X = df.drop(columns=\'target\') y = df[\'target\'] return train_test_split(X, y, test_size=0.3, random_state=42) def build_and_train_pipeline(X_train, y_train): Builds and trains the machine learning pipeline. Parameters: - X_train: DataFrame, training features - y_train: Series, training target Returns: - pipeline: trained Pipeline object numeric_features = X_train.select_dtypes(include=[\'float64\', \'int64\']).columns categorical_features = X_train.select_dtypes(include=[\'object\']).columns numeric_transformer = Pipeline(steps=[ (\'simpleimputer\', SimpleImputer(strategy=\'mean\')), (\'knnimputer\', KNNImputer(n_neighbors=2)) ]) categorical_transformer = Pipeline(steps=[ (\'simpleimputer\', SimpleImputer(strategy=\'most_frequent\')) ]) preprocessor = ColumnTransformer( transformers=[ (\'num\', numeric_transformer, numeric_features), (\'cat\', categorical_transformer, categorical_features) ]) pipeline = Pipeline(steps=[ (\'preprocessor\', preprocessor), (\'classifier\', RandomForestClassifier(random_state=42)) ]) pipeline.fit(X_train, y_train) return pipeline def evaluate_pipeline(pipeline, X_test, y_test): Evaluates the pipeline on the testing set and prints the accuracy. Parameters: - pipeline: trained Pipeline object - X_test: DataFrame, testing features - y_test: Series, testing target y_pred = pipeline.predict(X_test) accuracy = accuracy_score(y_test, y_pred) print(f\'Accuracy: {accuracy:.2f}\') if __name__ == \'__main__\': X_train, X_test, y_train, y_test = load_and_prepare_data(\'path_to_csv_file\') pipeline = build_and_train_pipeline(X_train, y_train) evaluate_pipeline(pipeline, X_test, y_test)"},{"question":"# PyTorch Meta Device Assessment Objective The goal of this assessment is to evaluate your understanding of the \\"meta\\" device concept in PyTorch and your ability to work with meta tensors and perform operations on them. Problem Statement You are given a pre-trained PyTorch neural network model stored in a file `model.pth`. Your task is to load the model onto the meta device, perform certain transformations on the model\'s structure using meta tensors, and then move the model back to the CPU device with proper initialization of the parameters. Specifications 1. Load the pre-trained model from `model.pth` onto the meta device. 2. Print the structure of the loaded model to verify it is on the meta device. 3. Add a new `torch.nn.Linear` layer to the model. This new layer should take the output size of the model\'s last layer as its input size and produce an output size of 10. 4. Initialize the parameters of the new layer to zero, on the CPU device. 5. Move the entire model (including the new layer) back to the CPU device, ensuring all parameters are initialized correctly. 6. Print the final model structure to verify the added layer and parameter initialization. Input - A file named `model.pth` containing the pre-trained PyTorch model. Output Your solution should print the following output in sequence: 1. The structure of the model loaded on the meta device. 2. The structure of the modified model on the CPU device, including the newly added layer, with parameters initialized to zero. Constraints - You cannot change the original pre-trained model parameters; the changes are to be made using meta tensors and reconstructed on the CPU device. - Ensure that the new layer is added correctly without affecting the functionality of existing layers. Example ```python # This is a guide and is not meant to be complete code or the exact model structure import torch import torch.nn as nn # Sample model loading and meta device operations model_path = \'model.pth\' # Your implementation should follow # 1. Load the model onto the meta device model_meta = torch.load(model_path, map_location=\'meta\') # 2. Print the model structure (meta device) print(model_meta) # 3. Add a new layer to the model structure # Assume last layer is Linear with output features \'n\' n = model_meta[-1].out_features new_layer = nn.Linear(n, 10) # 4. Initialize parameters to zero with torch.no_grad(): new_layer.weight.zero_() new_layer.bias.zero_() # 5. Transfer model with new layer back to CPU and initialize parameters model_cpu = model_meta.to_empty(device=\'cpu\') model_cpu.add_module(\'new_layer\', new_layer) # 6. Print the modified model structure (CPU device) print(model_cpu) ```","solution":"import torch import torch.nn as nn def load_and_modify_model(model_path): Load a model onto the meta device, add a new layer, and move it back to CPU with initialized params. # 1. Load the model onto the meta device model_meta = torch.load(model_path, map_location=\'meta\') # Print the model structure (meta device) print(model_meta) # 2. Retrieve the size of the output features of the last layer last_layer = list(model_meta.children())[-1] if isinstance(last_layer, nn.Linear): n = last_layer.out_features else: raise ValueError(\\"Last layer is not a Linear layer.\\") # 3. Create a new Linear layer new_layer = nn.Linear(n, 10) # 4. Initialize the parameters of the new layer to zero with torch.no_grad(): new_layer.weight.zero_() new_layer.bias.zero_() # 5. Transfer model with new layer back to CPU and initialize parameters model_cpu = model_meta.to_empty(device=\'cpu\') layers = list(model_cpu.children()) layers.append(new_layer) model_cpu = nn.Sequential(*layers) # Print the modified model structure (CPU device) print(model_cpu) return model_cpu"},{"question":"Attention Mechanism Implementation with PyTorch **Objective:** Implement a custom scaled dot-product attention mechanism using PyTorch. **Description:** Scaled dot-product attention is a fundamental building block in many neural network architectures, particularly in transformers. Given a set of query, key, and value vectors, the attention mechanism computes a weighted sum of the values, where the weights are determined by the dot products of the query with the key vectors, scaled by the inverse of the square root of the dimension of the key vectors. Task: 1. **Implement the function `scaled_dot_product_attention`**: - **Inputs**: - `queries`: A tensor of shape `(batch_size, num_heads, seq_length, depth)` - `keys`: A tensor of shape `(batch_size, num_heads, seq_length, depth)` - `values`: A tensor of shape `(batch_size, num_heads, seq_length, depth)` - `mask` (optional): A tensor of shape `(batch_size, 1, 1, seq_length)`. If provided, this mask is applied to the attention scores before the softmax. - **Outputs**: - A tensor of shape `(batch_size, num_heads, seq_length, depth)` representing the output of the attention mechanism. - **Constraints**: - The function should handle different batch sizes, head counts, and sequence lengths efficiently. - If a mask is provided, the masked positions should be set to a very large negative number to effectively nullify their influence during the softmax. - **Performance**: - The implementation should be efficient in terms of both computation and memory usage. Example Usage: ```python import torch def scaled_dot_product_attention(queries, keys, values, mask=None): # Calculate the dot products of the queries and keys matmul_qk = torch.matmul(queries, keys.transpose(-2, -1)) # Scale the dot products by the square root of the depth depth = keys.size(-1) scaled_attention_logits = matmul_qk / torch.sqrt(torch.tensor(depth, dtype=torch.float32)) # Apply the mask (if any) if mask is not None: scaled_attention_logits += (mask * -1e9) # Apply softmax to get the attention weights attention_weights = torch.nn.functional.softmax(scaled_attention_logits, dim=-1) # Calculate the output as the weighted sum of the values output = torch.matmul(attention_weights, values) return output # Example tensors queries = torch.randn(2, 4, 10, 64) keys = torch.randn(2, 4, 10, 64) values = torch.randn(2, 4, 10, 64) mask = torch.ones(2, 1, 1, 10) # Call the function output = scaled_dot_product_attention(queries, keys, values, mask) print(output.shape) # Expected shape: (2, 4, 10, 64) ``` Notes: - Ensure that your implementation is vectorized and makes efficient use of tensor operations. - Pay special attention to the handling of the optional `mask` parameter, as it is crucial for applications like sequence-to-sequence models where padding tokens need to be masked out.","solution":"import torch def scaled_dot_product_attention(queries, keys, values, mask=None): Implements the scaled dot-product attention mechanism. Args: - queries (tensor): Tensor of shape (batch_size, num_heads, seq_length_q, depth) - keys (tensor): Tensor of shape (batch_size, num_heads, seq_length_k, depth) - values (tensor): Tensor of shape (batch_size, num_heads, seq_length_v, depth) - mask (tensor, optional): Tensor of shape (batch_size, 1, 1, seq_length). Defaults to None. Returns: - tensor: The output tensor with shape (batch_size, num_heads, seq_length_q, depth) # Calculate the dot products of the queries and keys matmul_qk = torch.matmul(queries, keys.transpose(-2, -1)) # Scale the dot products by the square root of the depth depth = keys.size(-1) scaled_attention_logits = matmul_qk / torch.sqrt(torch.tensor(depth, dtype=torch.float32)) # Apply the mask (if any) if mask is not None: scaled_attention_logits += (mask * -1e9) # Apply softmax to get the attention weights attention_weights = torch.nn.functional.softmax(scaled_attention_logits, dim=-1) # Calculate the output as the weighted sum of the values output = torch.matmul(attention_weights, values) return output"},{"question":"# Objective: Implement Custom Memory Management in Python Python\'s memory management system offers interfaces for allocating and deallocating memory across different domains: raw, mem, and object. Understanding and utilizing these can be crucial for performance optimization and debugging in memory-intensive applications. # Task: Part 1: Implement Custom Allocator Functions 1. Implement the following C functions using Python\'s memory management API: - `void* custom_raw_malloc(size_t size)`: Allocate `size` bytes of raw memory. - `void* custom_raw_calloc(size_t num, size_t size)`: Allocate memory for an array of `num` elements, each of `size` bytes, initializing the memory to zero. - `void custom_raw_free(void* ptr)`: Free raw memory previously allocated. 2. Implement corresponding `Mem` and `Object` domain allocator functions: - `void* custom_mem_malloc(size_t size)`: Allocate `size` bytes of memory from Python\'s `mem` domain. - `void* custom_obj_malloc(size_t size)`: Allocate `size` bytes of memory from Python\'s `object` domain. 3. For each function, include error checks, and return `NULL` if the allocation fails. Part 2: Integrate Custom Allocator in a Python Extension 1. Define custom allocator structures using `PyMemAllocatorEx` for each domain. 2. Set the custom allocators as default allocators using `PyMem_SetAllocator()`. 3. Implement a Python C extension module that interacts with your custom allocator. # Constraints: - You are not allowed to use the standard C library functions (`malloc`, `calloc`, `free`) directly in your custom functions. - Python functions from different domains must not be mixed, i.e., `PyMem_Malloc` should not be freed with `PyObject_Free`. - Ensure thread safety where needed. # Expected Input and Output: Input Your Python module should provide three functions: - `allocate_raw(size: int)`: Return a pointer to a raw memory block of the given size. - `allocate_mem(size: int)`: Return a pointer to a memory block from the `mem` domain. - `allocate_obj(size: int)`: Return a pointer to a memory block from the `object` domain. Output Each function should return a Python integer representation of the memory address or `None` in case of allocation failure. # Performance Considerations: - Ensure minimal overhead in your custom allocator functions. - Python\'s Global Interpreter Lock (GIL) handling should be considered for thread safety in memory operations. # Example: ```python import your_extension_module # Allocate 100 bytes of raw memory raw_ptr = your_extension_module.allocate_raw(100) assert raw_ptr is not None # Allocate 200 bytes of mem domain memory mem_ptr = your_extension_module.allocate_mem(200) assert mem_ptr is not None # Allocate 300 bytes of object memory obj_ptr = your_extension_module.allocate_obj(300) assert obj_ptr is not None ``` # Notes: - Provide clear documentation and comments within your code to explain your custom allocator\'s implementation. - Consider edge cases such as allocating zero bytes or freeing null pointers. Good luck!","solution":"import ctypes from sys import getsizeof # Wrapper for managing raw memory (emulates raw malloc and calloc) class RawMemoryManager: def __init__(self): self.memories = {} def allocate(self, size): if size <= 0: return None ptr = ctypes.create_string_buffer(size) addr = ctypes.addressof(ptr) self.memories[addr] = ptr return addr def calloc(self, num, size): if num <= 0 or size <= 0: return None total_size = num * size ptr = ctypes.create_string_buffer(total_size) addr = ctypes.addressof(ptr) self.memories[addr] = ptr return addr def free(self, addr): if addr in self.memories: del self.memories[addr] raw_memory_manager = RawMemoryManager() def custom_raw_malloc(size): return raw_memory_manager.allocate(size) def custom_raw_calloc(num, size): return raw_memory_manager.calloc(num, size) def custom_raw_free(ptr): raw_memory_manager.free(ptr) def custom_mem_malloc(size): if size <= 0: return None return id(ctypes.create_string_buffer(size)) def custom_obj_malloc(size): if size <= 0: return None return id([None] * (size // getsizeof(None))) # Python interfaces def allocate_raw(size): addr = custom_raw_malloc(size) return None if addr is None else int(addr) def allocate_mem(size): addr = custom_mem_malloc(size) return None if addr is None else int(addr) def allocate_obj(size): addr = custom_obj_malloc(size) return None if addr is None else int(addr)"},{"question":"**Objective:** Demonstrate your understanding of the `logging` module in Python by configuring a complex logging system. **Task:** Implement a logging configuration in Python that meets the following specifications: 1. **Logger Hierarchy:** - Create a logger named `app`. - Create a child logger of `app` named `app.database`. - Create another child logger of `app` named `app.network`. 2. **Handlers:** - Add a `StreamHandler` to `app` that outputs to the console with a logging level of `DEBUG`. - Add a `FileHandler` to `app.database` that writes logs to a file named `database.log` with a logging level of `ERROR`. - Add a `FileHandler` to `app.network` that writes logs to a file named `network.log` with a logging level of `INFO`. 3. **Formatters:** - Use a formatter for the `StreamHandler` of `app` that outputs the log level name and the log message (e.g., `INFO: This is an info message`). - Use a formatter for the `FileHandler` of `app.database` that outputs the log level name, the name of the logger, and the log message (e.g., `ERROR: app.database: Database connection failed`). - Use a formatter for the `FileHandler` of `app.network` that outputs the timestamp, the log level name, and the log message (e.g., `2023-10-05 10:00:00,000: INFO: Network connection established`). 4. **Custom Filter:** - Implement a custom filter for `app.network` that only allows log messages containing the word \\"connection\\" to be logged. 5. **Propagation:** - Ensure that the log messages from `app.database` and `app.network` do not propagate to `app`. **Constraints:** - Do not modify the global logging configuration with `logging.basicConfig()`. - Ensure that loggers, handlers, formatters, and filters are all correctly configured and used. **Example:** The following code is for illustration purposes and should not be included in your submission. It demonstrates how the loggers might be used. ```python logger_app = logging.getLogger(\'app\') logger_db = logging.getLogger(\'app.database\') logger_net = logging.getLogger(\'app.network\') logger_app.debug(\\"This is a debug message in app\\") logger_db.error(\\"Database connection failed\\") logger_net.info(\\"Network connection established\\") logger_net.info(\\"This log message will not appear in the network log because it does not contain the word connection\\") ``` The submission should consist of the logging configuration code to meet all the requirements specified above.","solution":"import logging # Custom filter for app.network logger class ConnectionFilter(logging.Filter): def filter(self, record): return \'connection\' in record.msg # Formatter for StreamHandler of app logger app_formatter = logging.Formatter(\'%(levelname)s: %(message)s\') # Formatter for FileHandler of app.database logger database_formatter = logging.Formatter(\'%(levelname)s: %(name)s: %(message)s\') # Formatter for FileHandler of app.network logger network_formatter = logging.Formatter(\'%(asctime)s: %(levelname)s: %(message)s\') # Main logger configuration logger_app = logging.getLogger(\'app\') logger_app.setLevel(logging.DEBUG) # StreamHandler for app logger stream_handler = logging.StreamHandler() stream_handler.setLevel(logging.DEBUG) stream_handler.setFormatter(app_formatter) logger_app.addHandler(stream_handler) # Database logger configuration logger_app_database = logging.getLogger(\'app.database\') logger_app_database.setLevel(logging.DEBUG) # Set to DEBUG for internal processing, handlers will filter levels database_handler = logging.FileHandler(\'database.log\') database_handler.setLevel(logging.ERROR) database_handler.setFormatter(database_formatter) logger_app_database.addHandler(database_handler) logger_app_database.propagate = False # Network logger configuration logger_app_network = logging.getLogger(\'app.network\') logger_app_network.setLevel(logging.DEBUG) # Set to DEBUG for internal processing, handlers will filter levels network_handler = logging.FileHandler(\'network.log\') network_handler.setLevel(logging.INFO) network_handler.setFormatter(network_formatter) network_handler.addFilter(ConnectionFilter()) logger_app_network.addHandler(network_handler) logger_app_network.propagate = False"},{"question":"**Objective:** Implement a Python script that utilizes the `compileall` module to compile Python source files in a specified directory. Your script should accept directory paths as input and should be able to compile the files with specific options as described below. **Task:** Write a function named `compile_python_files` that takes the following arguments: - `directory` (str): The root directory to start compiling Python files. - `max_recursion` (int): The maximum depth of recursion for subdirectories. If set to 0, only the root directory\'s files are compiled. - `force_recompile` (bool): If set to True, files are re-compiled even if the timestamps are up-to-date. - `optimize_level` (int): The optimization level for the compilation (accepted values are -1, 0, 1, or 2). - `quiet_mode` (int): Level of verbosity (0 for full output, 1 for errors only, 2 for no output). The function should use the `compileall.compile_dir` method to compile the Python files with the specified options and should return True if all files compiled successfully, and False otherwise. **Constraints:** - The `directory` must be a valid directory path. - The `max_recursion` should be a non-negative integer. - The `optimize_level` should be one of -1, 0, 1, or 2. - The `quiet_mode` should be one of 0, 1, or 2. **Function Signature:** ```python def compile_python_files(directory: str, max_recursion: int, force_recompile: bool, optimize_level: int, quiet_mode: int) -> bool: pass ``` **Example Usage:** ```python # Example 1 result = compile_python_files(\'/path/to/directory\', max_recursion=3, force_recompile=True, optimize_level=2, quiet_mode=1) print(result) # Output should be True if all files are compiled successfully, otherwise False # Example 2 result = compile_python_files(\'/another/path\', max_recursion=0, force_recompile=False, optimize_level=0, quiet_mode=2) print(result) # Output should be True if all files are compiled successfully, otherwise False ``` **Notes:** - You may assume that the necessary imports are already provided. - Ensure that your implementation correctly handles the parameters and compiles the Python files accordingly. - Handle any exceptions that might occur during the compilation process and return False in such cases.","solution":"import compileall import os def compile_python_files(directory: str, max_recursion: int, force_recompile: bool, optimize_level: int, quiet_mode: int) -> bool: Compiles Python source files in a specified directory with given options. Args: directory (str): The root directory to start compiling Python files. max_recursion (int): The maximum depth of recursion for subdirectories. If set to 0, only the root directory\'s files are compiled. force_recompile (bool): If True, files are recompiled even if timestamps are up-to-date. optimize_level (int): The optimization level for the compilation (accepted values are -1, 0, 1, or 2). quiet_mode (int): Level of verbosity (0 for full output, 1 for errors only, 2 for no output). Returns: bool: True if all files compiled successfully, otherwise False. if not os.path.isdir(directory): return False if not (0 <= max_recursion and isinstance(max_recursion, int)): return False if not (-1 <= optimize_level <= 2 and isinstance(optimize_level, int)): return False if not (0 <= quiet_mode <= 2 and isinstance(quiet_mode, int)): return False try: return compileall.compile_dir( directory, maxlevels=max_recursion, force=force_recompile, quiet=quiet_mode, optimize=optimize_level ) except Exception: return False"},{"question":"# Terminal Control in UNIX using tty Module **Problem Statement:** You are tasked with creating a Python function to toggle between different terminal modes using the `tty` module. The function should take a file descriptor as input along with a mode to switch to, either \\"raw\\" or \\"cbreak\\". After switching modes, the function should verify the switch by performing some simple I/O operations indicative of the mode\'s characteristics. We will mock some terminal behavior to simulate this switching since actual terminal I/O testing is complex and environment-specific. **Function Signature:** ```python import termios import tty import os def toggle_terminal_mode(fd: int, mode: str) -> bool: Toggle the given file descriptor\'s terminal mode to the specified mode and verify the switch. :param fd: File descriptor to switch mode on. :param mode: Mode to switch to, either \\"raw\\" or \\"cbreak\\". :return: True if mode switching and verification succeed, False otherwise. pass ``` **Input:** - `fd` (int): A valid file descriptor of the terminal (typically this would be `sys.stdin.fileno()`). - `mode` (str): The mode to switch to, either `\\"raw\\"` or `\\"cbreak\\"`. **Output:** - A boolean indicating whether the function successfully switched the mode and verified it. **Constraints/Tips:** 1. The mode should be switched using `tty.setraw(fd)` for \\"raw\\" and `tty.setcbreak(fd)` for \\"cbreak\\". 2. Use `termios.tcgetattr()` to fetch and compare terminal attributes before and after the switch to validate the operation. 3. Handle exceptions gracefully, returning `False` if any error occurs. 4. You should reset the terminal to its original state after verification. 5. You are not required to read actual input; instead, simulate behavior verification due to environment constraints. Example: ```python import sys def main(): fd = sys.stdin.fileno() print(toggle_terminal_mode(fd, \\"raw\\")) # Expected: True, if the environment supports tty print(toggle_terminal_mode(fd, \\"cbreak\\")) # Expected: True, if the environment supports tty if __name__ == \\"__main__\\": main() ``` Ensure that you understand the constraints of the given environment and manage exceptions well to simulate the validation of the terminal mode change effectively.","solution":"import termios import tty import os def toggle_terminal_mode(fd: int, mode: str) -> bool: Toggle the given file descriptor\'s terminal mode to the specified mode and verify the switch. :param fd: File descriptor to switch mode on. :param mode: Mode to switch to, either \\"raw\\" or \\"cbreak\\". :return: True if mode switching and verification succeed, False otherwise. try: # Save the original terminal settings original_attrs = termios.tcgetattr(fd) # Switch to the specified mode if mode == \\"raw\\": tty.setraw(fd) elif mode == \\"cbreak\\": tty.setcbreak(fd) else: return False # Get the new terminal attributes new_attrs = termios.tcgetattr(fd) # Verify the mode switch by comparing specific termios attributes if mode == \\"raw\\" and new_attrs != original_attrs: termios.tcsetattr(fd, termios.TCSANOW, original_attrs) return True elif mode == \\"cbreak\\" and new_attrs != original_attrs: termios.tcsetattr(fd, termios.TCSANOW, original_attrs) return True else: # Restore the original terminal settings termios.tcsetattr(fd, termios.TCSANOW, original_attrs) return False except Exception as e: print(f\\"Exception occurred: {e}\\") return False"},{"question":"Multi-Format Compression and Decompression Engine Objective: Design and implement a function that can handle both compression and decompression of files using various algorithms supported by Python\'s standard library. Your solution should be able to work with `gzip`, `bzip2`, and `lzma` formats for both compressing and decompressing operations. Function Signature: ```python def compression_engine(input_path: str, output_path: str, operation: str, method: str) -> None: Perform compression or decompression of files using specified methods. Parameters: - input_path: str : The file path of the input file to compress/decompress - output_path: str : The file path where the output file should be saved - operation: str : Either \'compress\' or \'decompress\' - method: str : One of \'gzip\', \'bzip2\', or \'lzma\' Returns: - None : Outputs the resulting file at the output_path pass ``` Requirements: 1. **Input/Output:** - The `input_path` parameter will be a string representing the path to the file that is to be compressed or decompressed. - The `output_path` parameter will be a string representing the path where the resulting file should be saved. - The `operation` parameter will be a string that is either `\'compress\'` or `\'decompress\'`, indicating the desired operation. - The `method` parameter will be a string that is one of `\'gzip\'`, `\'bzip2\'`, or `\'lzma\'`, indicating the compression algorithm to use. 2. **Constraints:** - You must handle files of varying sizes efficiently. - Ensure proper error handling for non-existent files, unsupported methods, and file read/write errors. - The solution should be extensible to potentially support other compression algorithms in the future. 3. **Performance:** - Your implementation should be performant and handle large files without excessive memory usage. Example Usage: ```python # Compressing a file using gzip compression_engine(\\"data.txt\\", \\"data.txt.gz\\", \\"compress\\", \\"gzip\\") # Decompressing a file using gzip compression_engine(\\"data.txt.gz\\", \\"data_decompressed.txt\\", \\"decompress\\", \\"gzip\\") # Compressing a file using bzip2 compression_engine(\\"data.txt\\", \\"data.txt.bz2\\", \\"compress\\", \\"bzip2\\") # Decompressing a file using bzip2 compression_engine(\\"data.txt.bz2\\", \\"data_decompressed.txt\\", \\"decompress\\", \\"bzip2\\") # Compressing a file using lzma compression_engine(\\"data.txt\\", \\"data.txt.xz\\", \\"compress\\", \\"lzma\\") # Decompressing a file using lzma compression_engine(\\"data.txt.xz\\", \\"data_decompressed.txt\\", \\"decompress\\", \\"lzma\\") ``` Implement the `compression_engine` function according to the requirements above.","solution":"import gzip import bz2 import lzma import os def compression_engine(input_path: str, output_path: str, operation: str, method: str) -> None: if method not in [\'gzip\', \'bzip2\', \'lzma\']: raise ValueError(f\\"Unsupported compression method: {method}\\") if operation not in [\'compress\', \'decompress\']: raise ValueError(f\\"Unsupported operation: {operation}\\") if not os.path.exists(input_path): raise FileNotFoundError(f\\"Input file does not exist: {input_path}\\") openers = { \'gzip\': gzip, \'bzip2\': bz2, \'lzma\': lzma } open_func = openers[method].open if operation == \'compress\': mode = \'wb\' elif operation == \'decompress\': mode = \'rb\' if operation == \'compress\': with open(input_path, \'rb\') as input_file, open_func(output_path, mode) as output_file: for chunk in iter(lambda: input_file.read(1024 * 1024), b\'\'): output_file.write(chunk) elif operation == \'decompress\': with open_func(input_path, mode) as input_file, open(output_path, \'wb\') as output_file: for chunk in iter(lambda: input_file.read(1024 * 1024), b\'\'): output_file.write(chunk)"},{"question":"You are tasked with implementing a helper class in PyTorch that facilitates the process of converting a neural network\'s modules while respecting the future settings for overwriting and swapping module parameters. This class should allow a model to be safely converted to a different state, ensuring that default parameters are handled according to the user-defined preferences. **Requirements:** 1. Implement a class `ModelConverter` with the following behaviors: - On initialization, it should fetch the current states of \'overwrite\' and \'swap\' parameters using the provided PyTorch functions: `get_overwrite_module_params_on_conversion` and `get_swap_module_params_on_conversion`. - Allow conversion of a given model\'s parameters (in practice, this could be a mock conversion as enough context isn\'t provided). - Provide methods to explicitly set the conversion behaviors, wrapping the `set_overwrite_module_params_on_conversion` and `set_swap_module_params_on_conversion` functions. **Class Definition:** ```python class ModelConverter: def __init__(self): pass def convert_model(self, model): pass def set_overwrite_behavior(self, value: bool): pass def set_swap_behavior(self, value: bool): pass def get_current_behaviors(self) -> dict: pass ``` **Expected Input/Output:** - Initialization should correctly store the current states of the conversion behaviors. - `convert_model(model)` should modify the model in a way that can be verified (e.g., altering a dummy parameter). - `set_overwrite_behavior(value: bool)` and `set_swap_behavior(value: bool)` should modify the corresponding settings. - `get_current_behaviors()` should return a dictionary with the current states of \'overwrite\' and \'swap\' behaviors. **Constraints:** - You may assume that a model is a PyTorch `nn.Module` object. - Focus on correctly manipulating and retrieving the states as described. **Performance Requirements:** - The implementation should be efficient -- avoid any unnecessary computations. Utilize the provided functions from `torch.__future__` to meet the above requirements. Demonstrate the correct usage of these functions through the methods in your `ModelConverter` class. **Note:** Actual detailed implementation of model parameter conversion is beyond this question due to insufficient context; focus on demonstrating competent usage of the provided functions.","solution":"import torch from torch.__future__ import ( get_overwrite_module_params_on_conversion, set_overwrite_module_params_on_conversion, get_swap_module_params_on_conversion, set_swap_module_params_on_conversion ) class ModelConverter: def __init__(self): self.overwrite_behavior = get_overwrite_module_params_on_conversion() self.swap_behavior = get_swap_module_params_on_conversion() def convert_model(self, model): for param in model.parameters(): if self.overwrite_behavior: param.data.fill_(0) # This is a mock conversion implementation if self.swap_behavior: param.data = param.data.clone().detach() # This could be any swap operation def set_overwrite_behavior(self, value: bool): set_overwrite_module_params_on_conversion(value) self.overwrite_behavior = get_overwrite_module_params_on_conversion() def set_swap_behavior(self, value: bool): set_swap_module_params_on_conversion(value) self.swap_behavior = get_swap_module_params_on_conversion() def get_current_behaviors(self) -> dict: return { \\"overwrite\\": self.overwrite_behavior, \\"swap\\": self.swap_behavior }"},{"question":"# Question You are tasked with developing a machine learning model using Scikit-learn for a scenario where performance in terms of prediction latency and throughput is critical. Provided below is a synthetic dataset and key requirements for the model. Dataset You will create a synthetic dataset with the following characteristics: - 100,000 samples - 50 features - 10% sparsity (i.e., 10% of the elements in the dataset are non-zero) Requirements 1. Implement a Linear Regression model using Scikit-learn. 2. Ensure that the implementation predicts in bulk for optimal performance. 3. Evaluate the prediction latency and throughput for this model. 4. Optimize the model by compressing it using sparsity and utilizing sparse input representation. 5. Configure Scikit-learn to assume finite data to reduce validation overhead. 6. Use performance metrics to compare the optimized model with the original one. Input - None Output - Prints the prediction latency and throughput for both the original and optimized models. - Compare and summarize the results. Constraints - The implementation should avoid using too much memory and should be optimized for speed. - Predictions should be made in bulk mode. Code Requirements - Use Numpy to generate the synthetic dataset. - Use Scipy for sparse matrix representations. - Use Scikit-learn for the model creation, training, and prediction. ```python import numpy as np from scipy.sparse import csr_matrix from sklearn.linear_model import LinearRegression from sklearn.utils import gen_batches import timeit # 1. Create synthetic dataset samples = 100000 features = 50 density = 0.1 # Generating a random sparse matrix with 10% sparsity X_sparse = csr_matrix(np.random.binomial(1, density, size=(samples, features)).astype(float)) y = np.random.randn(samples) # Function to measure latency and throughput def measure_performance(model, X, y, batch_size=1000): n_batches = int(np.ceil(X.shape[0] / batch_size)) total_time = 0.0 for batch in gen_batches(X.shape[0], batch_size): start_time = timeit.default_timer() model.predict(X[batch]) total_time += timeit.default_timer() - start_time latency = total_time / n_batches throughput = X.shape[0] / total_time return latency, throughput # 2. Implement and evaluate base model base_model = LinearRegression() base_model.fit(X_sparse, y) base_latency, base_throughput = measure_performance(base_model, X_sparse, y) print(f\\"Base Model - Latency: {base_latency:.6f}, Throughput: {base_throughput:.2f} predictions/second\\") # 3. Optimize model with sparsity and configuration import sklearn from sklearn.linear_model import SGDRegressor with sklearn.config_context(assume_finite=True, working_memory=128): sparse_model = SGDRegressor(penalty=\'elasticnet\', l1_ratio=0.25) sparse_model.fit(X_sparse, y) sparse_model.sparsify() optimized_latency, optimized_throughput = measure_performance(sparse_model, X_sparse, y) print(f\\"Optimized Model - Latency: {optimized_latency:.6f}, Throughput: {optimized_throughput:.2f} predictions/second\\") # Summarize the comparison print(\\"Model Performance Comparison\\") print(f\\"Base Model - Latency: {base_latency:.6f} seconds, Throughput: {base_throughput:.2f} predictions/second\\") print(f\\"Optimized Model - Latency: {optimized_latency:.6f} seconds, Throughput: {optimized_throughput:.2f} predictions/second\\") ``` Explanation - First, generate a synthetic dataset with the required sparsity using Scipy\'s `csr_matrix`. - Use a bulk prediction approach to evaluate the base LinearRegression model and measure its prediction latency and throughput. - Optimize the model by using sparse input representation and a sparse model via `SGDRegressor`. - Adjust the Scikit-learn configuration to reduce validation overhead and limit working memory as per the provided context. - Compare the performance results between the base and optimized models to highlight speed improvements.","solution":"import numpy as np from scipy.sparse import csr_matrix from sklearn.linear_model import LinearRegression from sklearn.utils import gen_batches import timeit import sklearn from sklearn.linear_model import SGDRegressor # Creating synthetic dataset def create_dataset(samples, features, density): X_sparse = csr_matrix(np.random.binomial(1, density, size=(samples, features)).astype(float)) y = np.random.randn(samples) return X_sparse, y # Function to measure latency and throughput def measure_performance(model, X, batch_size=1000): n_batches = int(np.ceil(X.shape[0] / batch_size)) total_time = 0.0 for batch in gen_batches(X.shape[0], batch_size): start_time = timeit.default_timer() model.predict(X[batch]) total_time += timeit.default_timer() - start_time latency = total_time / n_batches throughput = X.shape[0] / total_time return latency, throughput # Create dataset samples = 100000 features = 50 density = 0.1 X_sparse, y = create_dataset(samples, features, density) # Implement and evaluate base model base_model = LinearRegression() base_model.fit(X_sparse, y) base_latency, base_throughput = measure_performance(base_model, X_sparse) print(f\\"Base Model - Latency: {base_latency:.6f}, Throughput: {base_throughput:.2f} predictions/second\\") # Optimize model with sparsity and configuration with sklearn.config_context(assume_finite=True, working_memory=128): sparse_model = SGDRegressor(penalty=\'elasticnet\', l1_ratio=0.25) sparse_model.fit(X_sparse, y) sparse_model.sparsify() optimized_latency, optimized_throughput = measure_performance(sparse_model, X_sparse) print(f\\"Optimized Model - Latency: {optimized_latency:.6f}, Throughput: {optimized_throughput:.2f} predictions/second\\") # Summarize the comparison def summarize_performance(base_latency, base_throughput, optimized_latency, optimized_throughput): comparison = { \\"Base Model Latency\\": base_latency, \\"Base Model Throughput\\": base_throughput, \\"Optimized Model Latency\\": optimized_latency, \\"Optimized Model Throughput\\": optimized_throughput } return comparison performance_summary = summarize_performance(base_latency, base_throughput, optimized_latency, optimized_throughput) print(\\"Model Performance Comparison\\") print(performance_summary)"},{"question":"You are given a log file with timestamps indicating when certain events occurred. You need to analyze these logs to determine the total duration of events, the average duration, and other relevant statistics. Implement the following functions: 1. **parse_logs_to_timedelta**: - **Input**: A list of strings where each string represents a duration in the format `\\"<days> days <hours>:<minutes>:<seconds>.<milliseconds>\\"`. - **Output**: A pandas `TimedeltaIndex` representing each duration. - **Example**: ```python log_entries = [\\"1 days 01:00:00.000\\", \\"2 days 12:30:00.250\\", \\"0 days 00:05:00.100\\"] result = parse_logs_to_timedelta(log_entries) # Output: TimedeltaIndex([\'1 days 01:00:00.000\', \'2 days 12:30:00.250\', \'0 days 00:05:00.100\'], dtype=\'timedelta64[ns]\') ``` 2. **compute_event_statistics**: - **Input**: A `TimedeltaIndex` of event durations. - **Output**: A dictionary with the following keys and corresponding values: - `\\"total_duration\\"`: The total duration of all events as a `Timedelta`. - `\\"average_duration\\"`: The average duration of events as a `Timedelta`. - `\\"min_duration\\"`: The minimum event duration as a `Timedelta`. - `\\"max_duration\\"`: The maximum event duration as a `Timedelta`. - **Example**: ```python durations = pd.to_timedelta([\\"1 days 01:00:00.000\\", \\"2 days 12:30:00.250\\", \\"0 days 00:05:00.100\\"]) result = compute_event_statistics(durations) # Output: { # \\"total_duration\\": Timedelta(\'3 days 13:35:00.350000\'), # \\"average_duration\\": Timedelta(\'1 days 04:31:40.116667\'), # \\"min_duration\\": Timedelta(\'0 days 00:05:00.100000\'), # \\"max_duration\\": Timedelta(\'2 days 12:30:00.250000\') # } ``` # Constraints: - The log entries will be valid and in the correct format. - There will be at least one log entry. - You are required to use pandas for this task. # Performance Requirements: - The solution should be efficient, making use of pandas\' capabilities for vectorized operations. # Implementation: ```python import pandas as pd def parse_logs_to_timedelta(log_entries): # Your code here pass def compute_event_statistics(durations): # Your code here pass ``` # Testing Your Implementation: ```python # Sample logs log_entries = [\\"1 days 01:00:00.000\\", \\"2 days 12:30:00.250\\", \\"0 days 00:05:00.100\\"] # Parse the log entries to TimedeltaIndex durations = parse_logs_to_timedelta(log_entries) print(durations) # Compute statistics from the durations stats = compute_event_statistics(durations) print(stats) ```","solution":"import pandas as pd def parse_logs_to_timedelta(log_entries): Parse a list of log entries into a pandas TimedeltaIndex. :param log_entries: List of strings representing durations in the format \\"<days> days <hours>:<minutes>:<seconds>.<milliseconds>\\" :return: pandas TimedeltaIndex of the parsed durations return pd.to_timedelta(log_entries) def compute_event_statistics(durations): Compute statistics from the given durations. :param durations: A pandas TimedeltaIndex of event durations :return: A dictionary with total_duration, average_duration, min_duration, and max_duration total_duration = durations.sum() average_duration = durations.mean() min_duration = durations.min() max_duration = durations.max() return { \\"total_duration\\": total_duration, \\"average_duration\\": average_duration, \\"min_duration\\": min_duration, \\"max_duration\\": max_duration }"},{"question":"Objective: Implement a function that reads and processes an IFF-type file containing multiple chunks, using the `chunk` module from the `chunk.py` library. This task will demonstrate your understanding of reading and manipulating chunked data. Task: Write a function `process_chunks(file_path: str) -> list` that: 1. Reads an IFF-type file from the provided `file_path`. 2. Extracts the name and size of each chunk. 3. Collects these name and size details into a list of tuples. 4. Returns the list of tuples, where each tuple contains: - The chunk ID (as a string). - The size of the chunk (as an integer). Constraints: - You must use the `chunk` module\'s `Chunk` class to read the file. - The file can contain multiple chunks. - You should handle cases where the chunks are aligned on 2-byte boundaries. - You should consider big-endian byte order for the chunk size. Input: - `file_path`: A string representing the path to the IFF-type file. Output: - A list of tuples, where each tuple consists of: - Chunk ID (4-character string). - Chunk size (integer). Example: Suppose you have an IFF file at `\'sample_file.iff\'` with the following structure: ``` Chunk 1: - ID: \'FORM\' - Size: 18 (data of 18 bytes) Chunk 2: - ID: \'AIFF\' - Size: 4 (data of 4 bytes) ``` Calling `process_chunks(\'sample_file.iff\')` should return: ```python [ (\'FORM\', 18), (\'AIFF\', 4) ] ``` Note: You don\'t need to process the data bytes within each chunk; just extract and return the chunk’s name and size. Function Signature: ```python def process_chunks(file_path: str) -> list: pass ``` Tips: - Use the `Chunk` class correctly, including its `getname()` and `getsize()` methods. - Ensure you handle file opening and closing appropriately. - Think about edge cases, such as empty files or files with malformed chunks.","solution":"import chunk def process_chunks(file_path: str) -> list: Reads an IFF-type file and extracts the name and size of each chunk. :param file_path: Path to the IFF-type file. :return: A list of tuples with the chunk ID and size. chunks = [] with open(file_path, \'rb\') as f: while True: try: ch = chunk.Chunk(f, bigendian=True, align=True) chunks.append((ch.getname().decode(\'ascii\'), ch.getsize())) ch.skip() except EOFError: break return chunks"},{"question":"**Objective:** Your task is to create a visual representation of a dataset using seaborn\'s object-oriented interface. The goal is to demonstrate the ability to manipulate plot layouts, create subplot grids, and customize the appearance of the plots. **Dataset:** Use the `penguins` dataset which you can obtain from the seaborn library. You should load this dataset using the following code: ```python import seaborn as sns penguins = sns.load_dataset(\'penguins\') ``` **Task:** 1. Create a scatter plot visualizing the relationship between `bill_length_mm` and `bill_depth_mm` for different species of penguins. 2. Adjust the size of the overall figure to be 8 units wide and 6 units tall. 3. Create a facet grid based on the `species` and `sex` columns of the dataset. 4. Use the `\\"constrained\\"` layout engine to optimize the arrangement of subplots. 5. Save the final figure as a PNG file named `penguins_plot.png`. **Constraints:** - Ensure that the scatter plots within the facet grid are clearly distinguishable (e.g., use different colors for different species). - Handle missing data appropriately, if any, by removing such entries prior to plotting. - Your plot should have appropriate labels for the x-axis, y-axis, and a title. **Input:** - `penguins` dataset loaded using seaborn. **Output:** - A PNG file named `penguins_plot.png` saved in the current working directory. **Sample Code Structure:** ```python import seaborn as sns import seaborn.objects as so import matplotlib.pyplot as plt # Load the dataset penguins = sns.load_dataset(\'penguins\') # Handle missing data penguins = penguins.dropna() # Create a Plot object p = so.Plot(penguins, x=\'bill_length_mm\', y=\'bill_depth_mm\', color=\'species\') .facet(\'species\', \'sex\') .layout(size=(8, 6), engine=\'constrained\') # Customize the plot p.add(so.Dot()) # Show the plot p.show() # Save the plot as a PNG file plt.savefig(\'penguins_plot.png\') ``` Ensure your code is functional and produces the expected plot saved as a PNG file. Test your solution to confirm that all requirements are met.","solution":"import seaborn as sns import seaborn.objects as so import matplotlib.pyplot as plt def create_penguins_plot(): # Load the dataset penguins = sns.load_dataset(\'penguins\') # Handle missing data penguins = penguins.dropna() # Create a Plot object with specified size and layout p = so.Plot(penguins, x=\'bill_length_mm\', y=\'bill_depth_mm\', color=\'species\') .facet(\'species\', \'sex\') .layout(size=(8, 6), engine=\'constrained\') # Add a scatter plot to the Plot object p.add(so.Dot()) # Set the labels and title p.label(x=\\"Bill Length (mm)\\", y=\\"Bill Depth (mm)\\", title=\\"Penguins Bill Measurements by Species and Sex\\") # Save the plot as a PNG file p.save(\\"penguins_plot.png\\")"},{"question":"Shell-like Command Validator and Formatter Objective You are tasked with writing a Python function that takes a command string, validates it for proper shell syntax, and formats it in a way that ensures security and correctness. Your function will use the `shlex` module to tokenize the command, check it for any potentially dangerous patterns, and then return a shell-escaped version of the cleaned command. Function Signature ```python def validate_and_format_command(command: str) -> str: ``` Input - `command` (str): A string representing a command that mimics shell-like syntax. The command may contain spaces, quotes, and other shell special characters. Output - Returns a string that is a shell-escaped version of the input command after validation. Constraints 1. The command string should not contain any dangerous patterns such as `; rm`, `&& rm`, or `|| rm` that could lead to accidental or intentional deletion of files. 2. All tokens in the command string must be validated and cleaned to avoid injection attacks. 3. The function should handle empty strings and return them as-is. Example ```python # Example 1 command = \\"echo \'Hello World\' && rm -rf /\\" result = validate_and_format_command(command) print(result) # Should raise a ValueError due to dangerous pattern # Example 2 command = \\"ls -l /home/user\\" result = validate_and_format_command(command) print(result) # Should print the safely escaped command equivalent to \'ls -l /home/user\' # Example 3 command = \\"cp \'My File.txt\' \'Backup File.txt\'\\" result = validate_and_format_command(command) print(result) # Should print the safely escaped command equivalent to \\"cp \'My File.txt\' \'Backup File.txt\'\\" ``` Requirements 1. Use the `shlex.split` function to tokenize the input command string. 2. Implement a validation check to detect and raise a `ValueError` if any dangerous patterns (like `; rm`, `&& rm`, `|| rm`) are found. 3. Use the `shlex.quote` function to create a safe, shell-escaped version of each token from the original command. 4. Return a concatenated string made up of these cleaned, shell-escaped tokens. Implementation Notes - The `shlex` module will handle tokenization using shell-like syntax rules. - You can assume that the input command is a valid shell command string that needs validation and cleaning. - Handle exceptions appropriately and ensure that any dangerous commands are flagged.","solution":"import shlex def validate_and_format_command(command: str) -> str: # List of dangerous patterns dangerous_patterns = [\\"; rm\\", \\"&& rm\\", \\"|| rm\\"] # Check for dangerous patterns for pattern in dangerous_patterns: if pattern in command: raise ValueError(\\"Dangerous pattern detected in command\\") # Tokenize the command using shlex tokens = shlex.split(command) # Create a shell-escaped version of each token cleaned_tokens = [shlex.quote(token) for token in tokens] # Join the cleaned tokens into a single string cleaned_command = \' \'.join(cleaned_tokens) return cleaned_command"},{"question":"Objective You will write a Python program that reads data from a CSV file, processes and formats the data, and then writes the formatted data to a new CSV file. Problem Statement Create a function `process_and_format_csv(input_file: str, output_file: str) -> None` that: 1. Reads data from `input_file`, which is a CSV file containing student names and their scores in three subjects. 2. Calculates the total and average score for each student. 3. Formats the data to align the values in columns neatly and writes it to `output_file`. The input CSV file has the following format: ``` Name,Subject1,Subject2,Subject3 Alice,85,90,95 Bob,78,82,89 Charlie,92,88,84 ``` The output CSV file should have the following format: ``` Name,Subject1,Subject2,Subject3,Total,Average Alice,85,90,95,270,90.00 Bob,78,82,89,249,83.00 Charlie,92,88,84,264,88.00 ``` Requirements 1. Read the input CSV file using the `csv` module. 2. Ensure all numerical values are right-aligned and names are left-aligned in the output. 3. Use f-strings or the `str.format()` method for formatting. 4. Handle potential exceptions related to file I/O. Function Signature ```python def process_and_format_csv(input_file: str, output_file: str) -> None: pass ``` Constraints - Assume the input and output file paths are valid and accessible. - The input file will have at least one record. Example ```python # Sample usage process_and_format_csv(\'students_scores.csv\', \'formatted_scores.csv\') # Contents of students_scores.csv # Name,Subject1,Subject2,Subject3 # Alice,85,90,95 # Bob,78,82,89 # Charlie,92,88,84 # Resulting contents of formatted_scores.csv # Name,Subject1,Subject2,Subject3,Total,Average # Alice,85,90,95,270,90.00 # Bob,78,82,89,249,83.00 # Charlie,92,88,84,264,88.00 ```","solution":"import csv def process_and_format_csv(input_file: str, output_file: str) -> None: Reads student scores from input_file, calculates total and average scores, and writes formatted data to output_file. with open(input_file, newline=\'\') as csvfile: reader = csv.DictReader(csvfile) rows = list(reader) # Extracting fieldnames from the input file fieldnames = reader.fieldnames + [\'Total\', \'Average\'] # Adding \'Total\' and \'Average\' columns for each student for row in rows: total = sum(int(row[subj]) for subj in reader.fieldnames[1:]) average = total / (len(reader.fieldnames) - 1) row[\'Total\'] = total row[\'Average\'] = f\\"{average:.2f}\\" # Writing to the output file with formatted entries with open(output_file, \'w\', newline=\'\') as csvfile: writer = csv.DictWriter(csvfile, fieldnames=fieldnames) writer.writeheader() writer.writerows(rows)"},{"question":"# Question Implementing Parallel Task Execution with `concurrent.futures` Given a list of URLs and a function to download the content from each URL, use both `ThreadPoolExecutor` and `ProcessPoolExecutor` to download the content asynchronously. Measure and compare the performance of both approaches in terms of execution time. **Function Definitions:** 1. `fetch_urls_with_threads(urls: List[str], timeout: int) -> List[Tuple[str, bytes]]` 2. `fetch_urls_with_processes(urls: List[str], timeout: int) -> List[Tuple[str, bytes]]` **Input:** - `urls`: A list of URL strings to download. - `timeout`: An integer specifying the maximum time to wait for each download in seconds. **Output:** - A list of tuples where each tuple contains a URL and its corresponding content bytes. **Constraints:** - Assume the list of URLs and their content sizes are significant enough to measure the performance difference. - Handle any exceptions during the downloading process and ensure the function does not terminate abruptly. - Use a timeout to avoid hanging the process indefinitely. **Example Usage:** ```python urls = [\\"http://www.foxnews.com/\\", \\"http://www.cnn.com/\\", \\"http://europe.wsj.com/\\", \\"http://www.bbc.co.uk/\\"] timeout = 60 threads_result = fetch_urls_with_threads(urls, timeout) processes_result = fetch_urls_with_processes(urls, timeout) # Comparing execution times (this part is illustrative, exact implementation not required for the student) import time start = time.time() threads_result = fetch_urls_with_threads(urls, timeout) thread_time = time.time() - start start = time.time() processes_result = fetch_urls_with_processes(urls, timeout) process_time = time.time() - start print(f\\"Thread execution time: {thread_time}s\\") print(f\\"Process execution time: {process_time}s\\") ``` **Notes:** - Use the `concurrent.futures` module for implementing the parallel execution. - Print the execution time for both threading and processing approaches. - Handle exceptions gracefully and continue with the remaining URLs. - The `ThreadPoolExecutor` might be more suitable for I/O-bound tasks, while the `ProcessPoolExecutor` can benefit CPU-bound tasks. **Hints:** 1. Refer to the `concurrent.futures.ThreadPoolExecutor` and `concurrent.futures.ProcessPoolExecutor` documentation for implementation details. 2. You might use the `time` module to measure execution start and end times. 3. Use `executor.submit` for submitting the downloading tasks and `future.result` to get the responses.","solution":"import concurrent.futures import requests from typing import List, Tuple def fetch_url(url: str, timeout: int) -> Tuple[str, bytes]: Downloads content from the specified URL. :param url: URL to download content from :param timeout: Timeout for the request :return: A tuple containing the URL and the content bytes try: response = requests.get(url, timeout=timeout) response.raise_for_status() return (url, response.content) except Exception as e: return (url, b\\"\\") # Return empty bytes on failure def fetch_urls_with_threads(urls: List[str], timeout: int) -> List[Tuple[str, bytes]]: Downloads the content from each URL using ThreadPoolExecutor. :param urls: List of URLs to download content from :param timeout: Timeout for the requests :return: A list of tuples containing URLs and their corresponding content bytes with concurrent.futures.ThreadPoolExecutor() as executor: futures = [executor.submit(fetch_url, url, timeout) for url in urls] return [future.result() for future in concurrent.futures.as_completed(futures)] def fetch_urls_with_processes(urls: List[str], timeout: int) -> List[Tuple[str, bytes]]: Downloads the content from each URL using ProcessPoolExecutor. :param urls: List of URLs to download content from :param timeout: Timeout for the requests :return: A list of tuples containing URLs and their corresponding content bytes with concurrent.futures.ProcessPoolExecutor() as executor: futures = [executor.submit(fetch_url, url, timeout) for url in urls] return [future.result() for future in concurrent.futures.as_completed(futures)]"},{"question":"Objective: You are required to create a Python program using the `curses` and `curses.panel` modules. The program should create multiple panels, allow them to be manipulated by the user, and display the current state of the panels on the screen. Task: Implement a function `manage_panels(n: int)` that creates `n` panels, each with a unique window displaying a number (from 1 to `n`). The function should allow the user to perform the following operations interactively: - **Move a Panel**: Using the arrow keys, move the currently selected panel. - **Switch Panels**: Cycle through the panels using the \'Tab\' key. - **Show/Hide Panels**: Toggle the visibility of the current panel using the \'h\' key. - **Change Stacking Order**: - Push the current panel to the top using the \'t\' key. - Push the current panel to the bottom using the \'b\' key. - **Exit**: The program should exit upon pressing the \'q\' key. Constraints: 1. The function should handle up to 10 panels. 2. Each panel should be of size 10x10. 3. The program should prevent panels from being moved outside the terminal window. Input: - `n`: An integer representing the number of panels to create (1 ≤ n ≤ 10). Output: The function should display the panels on the terminal, allowing user interactions according to the task description. There is no additional output; the function should operate entirely through the terminal interface. Example: Consider an example where `n = 3`. The screen will display three panels, each containing their respective number (1, 2, or 3). The user can then use the specified keys to interact with the panels. ```python import curses import curses.panel def manage_panels(n): # Your code here # Example usage if __name__ == \\"__main__\\": curses.wrapper(manage_panels) ``` Notes: - Make sure to handle the cleanup of the curses environment properly. - You can use the `curses.panel.update_panels()` and `curses.doupdate()` functions to refresh the screen after each action. - Use appropriate exception handling for terminal resizing or unexpected errors to maintain a good user experience.","solution":"import curses import curses.panel def manage_panels(stdscr, n): if n < 1 or n > 10: return # input constraint curses.curs_set(0) stdscr.keypad(1) stdscr.timeout(100) panels = [] height, width = 10, 10 for i in range(n): win = curses.newwin(height, width, i * 3, i * 3) win.box() win.addstr(1, 1, str(i + 1)) panel = curses.panel.new_panel(win) panels.append(panel) current_panel = 0 while True: key = stdscr.getch() if key == ord(\'q\'): break elif key in [curses.KEY_DOWN, curses.KEY_UP, curses.KEY_LEFT, curses.KEY_RIGHT]: move_panel(key, panels[current_panel].window()) elif key == ord(\'t\'): panels[current_panel].top() elif key == ord(\'b\'): panels[current_panel].bottom() elif key == ord(\'h\'): if panels[current_panel].hidden(): panels[current_panel].show() else: panels[current_panel].hide() elif key == ord(\'t\'): current_panel = (current_panel + 1) % n curses.panel.update_panels() curses.doupdate() def move_panel(key, win): y, x = win.getbegyx() max_y, max_x = win.getmaxyx() if key == curses.KEY_DOWN and y + max_y < curses.LINES - 1: win.mvwin(y + 1, x) elif key == curses.KEY_UP and y > 0: win.mvwin(y - 1, x) elif key == curses.KEY_LEFT and x > 0: win.mvwin(y, x - 1) elif key == curses.KEY_RIGHT and x + max_x < curses.COLS - 1: win.mvwin(y, x + 1) if __name__ == \\"__main__\\": import sys if len(sys.argv) < 2: print(\\"Usage: python manage_panels.py <number_of_panels>\\") sys.exit(1) try: num_panels = int(sys.argv[1]) except ValueError: print(\\"Number of panels must be an integer.\\") sys.exit(1) curses.wrapper(manage_panels, num_panels)"},{"question":"**Objective:** You are to implement a custom Python sequence class that adheres to the Sequence Protocol. The class should mimic the behavior of a Python list but should include additional methods for reversed indexing and element replacement based on a value. This exercise assesses your understanding of abstract sequence handling and the implementation of custom data types in Python. **Requirements:** 1. Implement a class `CustomList` which behaves like a list. 2. The class should support: - Indexing and slicing - Length determination using `len()` - Iteration using an iterator protocol - Reversed indexing using negative indexes - Method `replace(old_value: int, new_value: int)` to replace all occurrences of `old_value` with `new_value`. **Expected Input and Output:** ```python # Example usage: clist = CustomList([1, 2, 3, 4, 5]) # Regular indexing print(clist[2]) # Output: 3 # Slicing print(clist[1:3]) # Output: [2, 3] # Reversed indexing print(clist[-1]) # Output: 5 # Length print(len(clist)) # Output: 5 # Iteration for item in clist: print(item) # Output: 1 2 3 4 5 # Replace elements clist.replace(2, 9) print(clist) # Output: [1, 9, 3, 4, 5] ``` **Constraints:** - You cannot use the built-in list class to directly store the elements. Instead, use an internal storage mechanism, such as a custom-defined list or array. - Your implementation should handle all edge cases gracefully, such as negative indexes exceeding the boundaries or attempting replacements of values not present in the list. **Class Skeleton:** ```python class CustomList: def __init__(self, elements): # Initialize the list with the provided elements def __getitem__(self, index): # Support indexing and slicing def __len__(self): # Return the length of the list def __iter__(self): # Return an iterator object def replace(self, old_value, new_value): # Replace all occurrences of old_value with new_value def __repr__(self): # Return string representation of the list ``` **Performance Requirements:** - Your implementation should be efficient in terms of both time and space complexity. Consider edge cases and ensure that operations are performed within acceptable time limits for large lists (up to 10,000 elements). Good luck!","solution":"class CustomList: def __init__(self, elements): self.elements = list(elements) # Initialize the internal storage with the provided elements def __getitem__(self, index): return self.elements[index] # Support indexing and slicing def __len__(self): return len(self.elements) # Return the length of the list def __iter__(self): return iter(self.elements) # Return an iterator object def replace(self, old_value, new_value): self.elements = [new_value if x == old_value else x for x in self.elements] # Replace all occurrences of old_value with new_value def __repr__(self): return repr(self.elements) # Return string representation of the list"},{"question":"# Custom Scikit-learn Estimator: Distance-Based Classifier Your task is to implement a custom scikit-learn compatible estimator, a simplified distance-based classifier named `DistanceClassifier`. This classifier will train on a set of data points and their corresponding labels, and then classify new data points based on the distance to the nearest training point. Requirements: 1. **Class Definition and Initialization** (`__init__` method): - The class should be named `DistanceClassifier`. - It should inherit from `BaseEstimator` and `ClassifierMixin`. - The `__init__` method should accept a hyperparameter `metric` (default is `\'euclidean\'`). 2. **Fitting Method** (`fit` method): - The `fit` method should accept two parameters: `X` (array-like of shape `(n_samples, n_features)`) and `y` (array-like of shape `(n_samples,)`). - It should validate the input data and store the training data (parameters `X_` and `y_`) along with the unique class labels (`classes_`). 3. **Prediction Method** (`predict` method): - The `predict` method should accept a parameter `X` (array-like of shape `(n_samples, n_features)`). - It should validate the input data and check if the classifier has been fitted. - Use the metric provided in the `__init__` method to calculate the distances between the data points in `X` and the training points stored in `X_`. - Assign labels to each point in `X` based on the label of the nearest training point. 4. **Utility Methods**: - Use `validate_data` for data validation. - Use `check_is_fitted` to ensure the `fit` method has been called before `predict`. 5. **Parameter Handling**: - Ensure that `metric` is stored as an attribute and returned by `get_params`. - Implement `set_params` so that the metric can be set after the estimator has been instantiated. Example Usage: ```python import numpy as np from sklearn.utils.estimator_checks import check_estimator # Define the class class DistanceClassifier(BaseEstimator, ClassifierMixin): def __init__(self, metric=\'euclidean\'): self.metric = metric def fit(self, X, y): X, y = validate_data(self, X, y) self.classes_ = np.unique(y) self.X_ = X self.y_ = y return self def predict(self, X): check_is_fitted(self) X = validate_data(self, X, reset=False) if self.metric == \'euclidean\': distances = np.linalg.norm(X[:, np.newaxis] - self.X_, axis=2) indices = np.argmin(distances, axis=1) return self.y_[indices] # Instantiate and validate the custom estimator clf = DistanceClassifier() check_estimator(clf) ``` Ensure your estimator passes the `check_estimator` function to confirm compatibility with scikit-learn\'s API standards. Constraints: - You may assume `metric` will only be `\'euclidean\'` or `\'manhattan\'`. - For `manhattan` distance, use the sum of absolute differences. - Raise appropriate exceptions for invalid input data. You need to implement the `DistanceClassifier` class so that it correctly fits and predicts the labels using the nearest neighbor method based on the specified distance metric.","solution":"import numpy as np from sklearn.base import BaseEstimator, ClassifierMixin from sklearn.utils.validation import check_is_fitted, check_X_y, check_array class DistanceClassifier(BaseEstimator, ClassifierMixin): def __init__(self, metric=\'euclidean\'): self.metric = metric def fit(self, X, y): X, y = check_X_y(X, y) self.classes_ = np.unique(y) self.X_ = X self.y_ = y return self def predict(self, X): check_is_fitted(self) X = check_array(X) if self.metric == \'euclidean\': distances = np.linalg.norm(X[:, np.newaxis] - self.X_, axis=2) elif self.metric == \'manhattan\': distances = np.sum(np.abs(X[:, np.newaxis] - self.X_), axis=2) else: raise ValueError(\\"Unsupported metric: \'{}\'\\".format(self.metric)) indices = np.argmin(distances, axis=1) return self.y_[indices] def get_params(self, deep=True): return {\'metric\': self.metric} def set_params(self, **params): if \'metric\' in params: self.metric = params[\'metric\'] return self"},{"question":"**Title: Creating a Setup Configuration File for Python Package Distribution** **Objective:** To assess your understanding of Distutils and the setup configuration file (\\"setup.cfg\\"), you need to create a configuration file that specifies options for building and distributing a Python package. **Problem Statement:** You are given a Python package named `mypackage` which contains pure Python modules and a C extension module. Your task is to create a `setup.cfg` file to configure the build and distribution processes according to the following requirements: 1. For the **build_ext** command: - Compile the C extension module in place (i.e., within the source directory). - Include directories `include/` and `/usr/local/include/` for searching header files. 2. For the **bdist_rpm** command: - Set the release number to `1`. - Set the packager to `Your Name <your.email@example.com>`. - Include documentation files: `CHANGES.md`, `README.md`, `USAGE.md`, and the entire `docs/` directory. - Define a macro `MYMACRO` with value `1` for the C preprocessor. **Instructions:** 1. Create a file named `setup.cfg`. 2. Configure the **build_ext** command as specified. 3. Configure the **bdist_rpm** command as specified. **Constraints:** - The `setup.cfg` file must strictly adhere to the configuration format described. - Use indentation appropriately to continue longer option values over multiple lines if necessary. **Example Output:** ```ini [build_ext] inplace=1 include_dirs=include/, /usr/local/include/ [bdist_rpm] release=1 packager=Your Name <your.email@example.com> doc_files=CHANGES.md README.md USAGE.md docs/ define=MYMACRO=1 ``` **Note:** Replace `Your Name` and `<your.email@example.com>` with your actual name and email. **Submission:** Make sure your `setup.cfg` file is properly formatted and submit it as your solution.","solution":"# Create a setup.cfg file to configure the build and distribution processes setup_cfg_content = [build_ext] inplace=1 include_dirs=include/, /usr/local/include/ [bdist_rpm] release=1 packager=Your Name <your.email@example.com> doc_files=CHANGES.md README.md USAGE.md docs/ define=MYMACRO=1 # Writing the content to the setup.cfg file with open(\'setup.cfg\', \'w\') as file: file.write(setup_cfg_content)"},{"question":"**Advanced Seaborn Violin Plot Analysis** You have been given a dataset \\"titanic\\" which contains data about passengers on the Titanic. Your task is to write a function that creates and saves advanced violin plots using the seaborn library. This exercise will test your ability to manipulate and visualize data in various detailed ways using seaborn\'s `violinplot` function. **Function Signature:** ```python def advanced_violin_plots(save_path: str) -> None: pass ``` **Input:** 1. `save_path` (str): The directory where the plots should be saved. Ensure to save the plots with filenames `plot1.png`, `plot2.png`, ..., `plot5.png`. **Output:** - This function does not return anything. **Required Plots:** 1. **Plot 1:** - A basic violin plot of passengers\' ages (`age`). 2. **Plot 2:** - A bivariate violin plot showing passengers\' ages (`age`) across different classes (`class`). 3. **Plot 3:** - A split violin plot showing passengers\' ages (`age`) across different classes (`class`), differentiated by survival status (`alive`). - Ensure the plot shows only the data quartiles inside the violins (`inner=\\"quart\\"`). 4. **Plot 4:** - A normalized violin plot representing the number of observations for each deck (`deck`), with each point represented inside the distribution (`inner=\\"point\\"`). 5. **Plot 5:** - A violin plot preserving the original data scale for age and fare, rounded to the nearest decade for age. **Constraints:** - Ensure to set the theme of seaborn to `whitegrid`. - Use appropriate parameters to achieve the descriptions for each plot. - Handle missing data if necessary. - Each plot should be saved with the respective filenames mentioned. **Example Usage:** ```python advanced_violin_plots(\\"./plots/\\") ``` This function should create five files: - `plot1.png` - `plot2.png` - `plot3.png` - `plot4.png` - `plot5.png` saved in the `./plots/` directory. Each file should contain the corresponding violin plots as specified above.","solution":"import seaborn as sns import matplotlib.pyplot as plt import pandas as pd def advanced_violin_plots(save_path: str) -> None: # Load the Titanic dataset titanic = sns.load_dataset(\\"titanic\\") # Set the Seaborn theme sns.set_theme(style=\'whitegrid\') # Plot 1: Basic violin plot of passengers\' ages plt.figure(figsize=(10, 6)) sns.violinplot(x=\'age\', data=titanic) plt.title(\'Distribution of Passenger Ages\') plt.savefig(f\'{save_path}/plot1.png\') plt.close() # Plot 2: Bivariate violin plot showing passengers\' ages across different classes plt.figure(figsize=(10, 6)) sns.violinplot(x=\'class\', y=\'age\', data=titanic) plt.title(\'Passenger Ages Across Different Classes\') plt.savefig(f\'{save_path}/plot2.png\') plt.close() # Plot 3: Split violin plot showing ages across different classes, differentiated by survival status plt.figure(figsize=(10, 6)) sns.violinplot(x=\'class\', y=\'age\', hue=\'alive\', data=titanic, split=True, inner=\'quart\') plt.title(\'Passenger Ages Across Classes by Survival Status\') plt.savefig(f\'{save_path}/plot3.png\') plt.close() # Plot 4: Normalized violin plot representing the number of observations for each deck plt.figure(figsize=(10, 6)) sns.violinplot(x=\'deck\', data=titanic, inner=\'point\', scale=\'count\', palette=\'muted\') plt.title(\'Distribution of Passengers by Deck\') plt.savefig(f\'{save_path}/plot4.png\') plt.close() # Plot 5: Violin plot preserving the original data scale for age and fare titanic[\'age_decade\'] = (titanic[\'age\'] // 10) * 10 plt.figure(figsize=(10, 6)) sns.violinplot(x=\'age_decade\', y=\'fare\', data=titanic) plt.title(\'Fare Distribution Across Age Decades\') plt.savefig(f\'{save_path}/plot5.png\') plt.close()"},{"question":"Objective: Assess your ability to utilize Seaborn for creating advanced visualizations by applying the `Dodge` transform along with other transformations to manage overlapping marks in a dataset. Problem Statement: You are given a dataset of tips received by waitstaff, which contains the following columns: - `total_bill`: The total bill amount (numerical). - `tip`: The amount of tip given (numerical). - `sex`: The gender of the person who paid the bill (categorical - \\"Male\\" or \\"Female\\"). - `smoker`: Whether the person was a smoker (categorical - \\"Yes\\" or \\"No\\"). - `day`: The day of the week (categorical - \\"Thur\\", \\"Fri\\", \\"Sat\\", \\"Sun\\"). - `time`: The time of day (categorical - \\"Lunch\\" or \\"Dinner\\"). - `size`: The size of the party (numerical). Using this dataset, create a bar plot that shows the sum of the total bill for each combination of day and time, differentiated by gender (`sex`). Ensure that: 1. You use the `Dodge` transform to handle overlapping marks for each gender. 2. Add a gap of 0.1 between dodged marks. 3. The marks should be centered in their positions. Input Format: - A dataset should be loaded using seaborn\'s `load_dataset(\\"tips\\")`. Expected Output: A bar plot with the required specifications. Constraints: - Use the Seaborn objects interface (`seaborn.objects`) for creating the plot. - The plot should be rendered with clear differentiation of bars by gender and proper handling of overlapping using the Dodge transform. Skeleton code: ```python import seaborn.objects as so from seaborn import load_dataset # Load the dataset tips = load_dataset(\\"tips\\").astype({\\"time\\": str}) # Create the plot object p = so.Plot(tips, \\"day\\", \\"total_bill\\", color=\\"sex\\") # Add bars with Dodge transformation p.add(so.Bar(), so.Agg(\\"sum\\"), so.Dodge(gap=0.1, empty=\'center\')) # Render the plot p.show() ``` Complete the code above to generate the required plot.","solution":"import seaborn.objects as so from seaborn import load_dataset def create_dodge_bar_plot(): # Load the dataset tips = load_dataset(\\"tips\\") # Create the plot object p = so.Plot(tips, x=\\"day\\", y=\\"total_bill\\", color=\\"sex\\").scale(x=\\"jitter\\") # Add bars with Dodge transformation p = p.add(so.Bar(), so.Agg(\\"sum\\"), so.Dodge(gap=0.1, empty=\'center\')) # Render the plot return p.show()"},{"question":"**Problem Statement: Directory Organizer with File Metadata Logging** You are tasked with writing a Python script that organizes files in a specified directory based on their file types and logs metadata about each file in a separate log file. The goal is to move files into subdirectories named after their file extensions and create a log file containing information about each file. **Functional Requirements:** 1. **Read Command-Line Arguments:** Your script should accept two command-line arguments: - The directory to organize. - The name of the log file where metadata will be stored. 2. **File Organization:** - Create subdirectories based on file extensions (e.g., `.txt` files go into `txt`, `.png` files go into `png`). - Move files into their respective subdirectories. 3. **Log Metadata:** - For each file, log the following information to the specified log file: - File name - Original file path - New file path - File size (in bytes) - Date of last modification **Input:** The script will be executed from the command line with the following arguments: ``` python organize_files.py <directory_to_organize> <log_file_path> ``` **Output:** - Subdirectories based on file extensions within the specified directory. - A log file in the specified location containing metadata about each moved file recorded line-by-line in the following format: ``` File Name: <file_name> Original Path: <original_path> New Path: <new_path> Size: <file_size> bytes Last Modified: <last_modification_date> ``` **Constraints:** - Assume the directory contains only files (no nested subdirectories). - If a file extension subdirectory already exists, place the file within it without creating a new one. **Performance Requirements:** - Your solution should be efficient with respect to file I/O operations. - Minimize unnecessary directory scans. **Example:** Given the directory structure: ``` /home/user/files/ file1.txt file2.png file3.jpeg ``` Execution: ``` python organize_files.py /home/user/files/ /home/user/files/log.txt ``` Resulting directory structure: ``` /home/user/files/ txt/ file1.txt png/ file2.png jpeg/ file3.jpeg log.txt ``` Log file content (`log.txt`): ``` File Name: file1.txt Original Path: /home/user/files/file1.txt New Path: /home/user/files/txt/file1.txt Size: 123 bytes Last Modified: 2023-10-01 12:00:00 File Name: file2.png Original Path: /home/user/files/file2.png New Path: /home/user/files/png/file2.png Size: 4567 bytes Last Modified: 2023-10-01 12:05:00 File Name: file3.jpeg Original Path: /home/user/files/file3.jpeg New Path: /home/user/files/jpeg/file3.jpeg Size: 7891 bytes Last Modified: 2023-10-01 12:10:00 ``` **Hints:** - Use `os`, `shutil`, and `datetime` modules. - For command-line argument parsing, use the `argparse` module. - To retrieve file metadata, use the `os.stat` module. Complete this task by implementing a function `organize_files(directory: str, log_file_path: str)` with appropriate helper functions as needed.","solution":"import os import shutil import argparse from datetime import datetime def organize_files(directory, log_file_path): if not os.path.isdir(directory): raise NotADirectoryError(f\\"{directory} is not a valid directory\\") log_entries = [] for file_name in os.listdir(directory): file_path = os.path.join(directory, file_name) if os.path.isfile(file_path): file_extension = file_name.split(\'.\')[-1] new_dir = os.path.join(directory, file_extension) new_file_path = os.path.join(new_dir, file_name) if not os.path.exists(new_dir): os.makedirs(new_dir) shutil.move(file_path, new_file_path) file_stats = os.stat(new_file_path) file_size = file_stats.st_size last_modification_time = datetime.fromtimestamp(file_stats.st_mtime) log_entries.append({ \\"File Name\\": file_name, \\"Original Path\\": file_path, \\"New Path\\": new_file_path, \\"Size\\": file_size, \\"Last Modified\\": last_modification_time.strftime(\'%Y-%m-%d %H:%M:%S\') }) with open(log_file_path, \'w\') as log_file: for entry in log_entries: log_file.write(f\\"File Name: {entry[\'File Name\']}n\\") log_file.write(f\\"Original Path: {entry[\'Original Path\']}n\\") log_file.write(f\\"New Path: {entry[\'New Path\']}n\\") log_file.write(f\\"Size: {entry[\'Size\']} bytesn\\") log_file.write(f\\"Last Modified: {entry[\'Last Modified\']}n\\") log_file.write(\\"n\\") def main(): parser = argparse.ArgumentParser(description=\'Organize files by type and log metadata.\') parser.add_argument(\'directory\', type=str, help=\'Directory to organize\') parser.add_argument(\'log_file\', type=str, help=\'Path to the log file\') args = parser.parse_args() organize_files(args.directory, args.log_file) if __name__ == \\"__main__\\": main()"},{"question":"# Question You are provided with the \'titanic\' dataset included with the Seaborn library. Your task is to create a complex visualization using Seaborn Objects (seaborn.objects). This will demonstrate your understanding of fundamental and advanced concepts of Seaborn plotting. Requirements 1. **Data Loading and Preparation**: - Load the \'titanic\' dataset using Seaborn\'s `load_dataset` function. - Sort the dataset by the \\"alive\\" column in descending order. 2. **Plot Configuration**: - Create a faceted histogram with the following specifications: - The main plot should show the distribution of the \\"age\\" column. - Use the \\"sex\\" column to create facets. - The histograms should be stacked based on the \\"alive\\" column. 3. **Additional Specifications**: - Set the bin width for the histograms to 10 years. - Adjust the plot transparency based on the \\"alive\\" status. Function Signature ```python def create_titanic_plot(): import seaborn.objects as so from seaborn import load_dataset import matplotlib.pyplot as plt # Load and sort the dataset titanic = load_dataset(\\"titanic\\").sort_values(\\"alive\\", ascending=False) # Create and configure the plot plot = ( so.Plot(titanic, x=\\"age\\", alpha=\\"alive\\") .facet(\\"sex\\") .add(so.Bars(), so.Hist(binwidth=10), so.Stack()) ) plot.show() plt.show() ``` Input and Output - **Input**: No direct input from the user; the function will load the dataset internally. - **Output**: The function should display the desired plot. Constraints and Limitations - You must use Seaborn Objects (`seaborn.objects`). - The function should be self-contained and not require any external datasets or dependencies outside of Seaborn and Matplotlib. Example - The function when executed should display a faceted histogram plot with the specifications given above.","solution":"def create_titanic_plot(): import seaborn.objects as so from seaborn import load_dataset import matplotlib.pyplot as plt # Load and sort the dataset titanic = load_dataset(\\"titanic\\").sort_values(\\"alive\\", ascending=False) # Create and configure the plot plot = ( so.Plot(titanic, x=\\"age\\", alpha=\\"alive\\") .facet(\\"sex\\") .add(so.Bars(), so.Hist(binwidth=10), so.Stack()) ) plot.show() plt.show()"},{"question":"# XML Parsing and Data Extraction You are given an XML string representing a collection of books. Your task is to write a function using the `xml.sax` package to parse this XML and extract information about each book. XML Format ```xml <library> <book> <title>Book Title 1</title> <author>Author 1</author> <year>2001</year> </book> <book> <title>Book Title 2</title> <author>Author 2</author> <year>2002</year> </book> ... </library> ``` Function Specification - **Function Name**: `parse_books` - **Input**: - `xml_data` (str): A string containing the XML data. - **Output**: - List of dictionaries: Each dictionary represents a book with keys `\\"title\\"`, `\\"author\\"`, and `\\"year\\"`. Constraints - The XML string always follows the provided format. - Year is a four-digit integer. Example ```python xml_data = \'\'\'<library> <book> <title>Book Title 1</title> <author>Author 1</author> <year>2001</year> </book> <book> <title>Book Title 2</title> <author>Author 2</author> <year>2002</year> </book> </library>\'\'\' print(parse_books(xml_data)) ``` Output: ```python [ {\\"title\\": \\"Book Title 1\\", \\"author\\": \\"Author 1\\", \\"year\\": 2001}, {\\"title\\": \\"Book Title 2\\", \\"author\\": \\"Author 2\\", \\"year\\": 2002} ] ``` Task 1. Implement the `parse_books` function. 2. Create the necessary handler by subclassing `xml.sax.ContentHandler`. 3. Use the `xml.sax.parseString` function to parse the XML data. **Note**: Focus on creating a robust solution that correctly handles the structure of the provided XML format.","solution":"import xml.sax from xml.sax.handler import ContentHandler class BookHandler(ContentHandler): def __init__(self): self.current_data = \\"\\" self.title = \\"\\" self.author = \\"\\" self.year = \\"\\" self.books = [] def startElement(self, tag, attributes): self.current_data = tag def endElement(self, tag): if tag == \\"book\\": self.books.append({ \\"title\\": self.title, \\"author\\": self.author, \\"year\\": int(self.year) }) self.title = \\"\\" self.author = \\"\\" self.year = \\"\\" self.current_data = \\"\\" def characters(self, content): if self.current_data == \\"title\\": self.title += content elif self.current_data == \\"author\\": self.author += content elif self.current_data == \\"year\\": self.year += content def parse_books(xml_data): handler = BookHandler() xml.sax.parseString(xml_data, handler) return handler.books"},{"question":"Objective Demonstrate your knowledge and proficiency with the `zoneinfo` module and `datetime` in Python by implementing a function to calculate the number of timezone transitions within a given date range for a specified timezone. Problem Statement Implement a Python function `count_timezone_transitions(start_date: str, end_date: str, timezone: str) -> int` that takes start and end dates (both strings in ISO format, e.g., \\"2020-01-01T00:00:00\\"), and a timezone specified as a string representing an IANA timezone (e.g., \\"America/Los_Angeles\\"), and returns the number of transitions in timezone offsets between the start and end dates. Input - `start_date`: A string in the ISO format \\"YYYY-MM-DDTHH:MM:SS\\". - `end_date`: A string in the ISO format \\"YYYY-MM-DDTHH:MM:SS\\". - `timezone`: A string specifying the IANA time zone name. Output - An integer representing the number of timezone transitions (such as daylight saving time switches) between the start and end dates (inclusive). Constraints - You should appropriately handle invalid time zones by raising a `ValueError` with the message \\"Invalid timezone provided\\". - Assume that the provided date strings are valid and in correct format. - The function should handle daylight saving time transitions correctly using the `fold` attribute if necessary. Implementation Requirements 1. Use the `zoneinfo` module to handle time zones. 2. Use `datetime` module to parse and manipulate datetime values. 3. Use appropriate exception handling to manage invalid time zones or missing timezone data. 4. Optimize the function for performance to handle wide date ranges efficiently. Function Signature ```python def count_timezone_transitions(start_date: str, end_date: str, timezone: str) -> int: pass ``` Example ```python # Example Test Case start_date = \\"2021-01-01T00:00:00\\" end_date = \\"2022-12-31T23:59:59\\" timezone = \\"America/New_York\\" # Example Usage print(count_timezone_transitions(start_date, end_date, timezone)) # Output: Number of transitions between these dates ``` Additional Information - Ensure your function is robust and tested with varying date ranges and time zones. - Consider edge cases such as the start and end dates being the same, and transitions that occur exactly at the start or end timestamps. Good luck!","solution":"from datetime import datetime, timedelta from zoneinfo import ZoneInfo, ZoneInfoNotFoundError def count_timezone_transitions(start_date: str, end_date: str, timezone: str) -> int: try: zone = ZoneInfo(timezone) except ZoneInfoNotFoundError: raise ValueError(\\"Invalid timezone provided\\") # Parse the start and end dates. start_dt = datetime.fromisoformat(start_date).replace(tzinfo=zone) end_dt = datetime.fromisoformat(end_date).replace(tzinfo=zone) transitions = 0 current_offset = start_dt.utcoffset() # Iterate through each day between start and end date inclusive. current_dt = start_dt while current_dt <= end_dt: next_day = current_dt + timedelta(days=1) next_day_offset = next_day.utcoffset() if next_day_offset != current_offset: transitions += 1 current_offset = next_day_offset current_dt = next_day return transitions"},{"question":"**Objective:** Implement a function that conducts a statistical analysis using random sampling methods to estimate the mean and confidence interval for the sampling mean from a given dataset. This question will test your understanding of random sampling and distribution functions provided by the `random` module. **Task:** Write a function `bootstrap_confidence_interval(data, num_samples, confidence_level)` that: 1. Takes in a list of numerical data `data`. 2. Uses bootstrap sampling to generate `num_samples` number of samples. 3. Computes the sampling mean for each sample. 4. Returns the mean of the means and the specified confidence interval for the mean of the data. **Function Signature:** ```python def bootstrap_confidence_interval(data: list, num_samples: int, confidence_level: float) -> tuple: pass ``` **Input:** - `data`: A list of numerical data points (e.g., `[1.1, 2.3, 3.5, 4.7, 5.6]`). - `num_samples`: An integer representing the number of bootstrap samples to generate (e.g., `1000`). - `confidence_level`: A float representing the desired confidence level for the interval (e.g., `0.95` for 95% confidence interval). **Output:** - A tuple containing two elements: 1. The mean of the sample means. 2. The confidence interval for the sampling mean in the format `(lower_bound, upper_bound)`. **Constraints:** - The input list `data` will contain at least two elements. - The `num_samples` will be a positive integer. - The `confidence_level` will be between 0 and 1 (exclusive). **Example:** ```python data = [41, 50, 29, 37, 81, 30, 73, 63, 20, 35, 68, 22, 60, 31, 95] num_samples = 1000 confidence_level = 0.90 mean, confidence_interval = bootstrap_confidence_interval(data, num_samples, confidence_level) print(f\\"Mean: {mean:.2f}, Confidence Interval: {confidence_interval}\\") ``` **Explanation:** - The function will use the `random.choices` method to generate `num_samples` bootstrap samples of the same size as the input `data`. - For each sample, compute the mean. - Sort the list of sample means. - Calculate the mean of the sample means. - Determine the `lower_bound` and `upper_bound` for the confidence interval based on the specified `confidence_level`. Hint: Use the `statistics.mean` function for computing means and take advantage of `sorted` to easily find the required quantiles for the confidence interval.","solution":"import random import statistics def bootstrap_confidence_interval(data, num_samples, confidence_level): Conducts a bootstrap analysis to estimate the mean and confidence interval for the mean. Parameters: data (list): A list of numerical data points. num_samples (int): Number of bootstrap samples to generate. confidence_level (float): The desired confidence level for the interval (e.g., 0.95). Returns: tuple: The mean of the sample means and the confidence interval as a tuple (lower_bound, upper_bound). sample_means = [] n = len(data) for _ in range(num_samples): sample = random.choices(data, k=n) sample_mean = statistics.mean(sample) sample_means.append(sample_mean) sample_means.sort() mean_of_means = statistics.mean(sample_means) lower_percentile = (1 - confidence_level) / 2 upper_percentile = 1 - lower_percentile lower_index = int(lower_percentile * num_samples) upper_index = int(upper_percentile * num_samples) - 1 confidence_interval = (sample_means[lower_index], sample_means[upper_index]) return mean_of_means, confidence_interval"},{"question":"**Problem Statement:** You are tasked with creating a Python application that can manage Unix group database entries using the `grp` module. Your function should collect specific details about group memberships and display them in a user-friendly format. # Function Requirements `get_group_information` Implement a function `get_group_information` that accepts a list of group names and returns a dictionary containing group details. Each key in the dictionary should be the group\'s name, and the corresponding value should be another dictionary with the group\'s ID and members. ```python def get_group_information(group_names: list) -> dict: pass ``` # Input - `group_names` (List of strings): A list containing the names of the Unix groups. # Output - A dictionary where keys are group names and values are dictionaries. Each of these dictionaries should have: - \'group_id\': an integer representing the group\'s unique ID. - \'members\': a list of strings representing the user names in the group. # Constraints - You may assume that the provided group names exist in the Unix group database. - Handle any potential `KeyError` or `TypeError` exceptions gracefully and return an empty dictionary in such cases. # Example ```python # Example Input group_names = [\\"staff\\", \\"developers\\", \\"admins\\"] # Example Output { \\"staff\\": { \\"group_id\\": 50, \\"members\\": [\\"alice\\", \\"bob\\"] }, \\"developers\\": { \\"group_id\\": 1001, \\"members\\": [\\"carol\\", \\"dave\\"] }, \\"admins\\": { \\"group_id\\": 1002, \\"members\\": [\\"eve\\"] } } ``` # Notes - Make sure your function is efficient and handles different edge cases, such as empty group member lists or special characters in group names. Use the functions provided by the `grp` module (`getgrnam`, `getgrgid`, `getgrall`) to retrieve the necessary information and construct the result.","solution":"import grp def get_group_information(group_names: list) -> dict: Collects information about specified Unix groups. Parameters: group_names (list): A list of Unix group names. Returns: dict: A dictionary with group names as keys and dictionaries containing group ID and members as values. group_info = {} try: for group_name in group_names: group_details = grp.getgrnam(group_name) group_info[group_name] = { \'group_id\': group_details.gr_gid, \'members\': list(group_details.gr_mem) } except (KeyError, TypeError): return {} return group_info"},{"question":"Objective: Implement a Python function that reads a file containing student grades, processes the data to calculate average grades for each student, and writes the results to a new file. Input: 1. A path to the input text file containing student grades. 2. The file will contain multiple lines, each line representing a student in the format: `StudentName: grade1, grade2, grade3, ...` - Example: ``` Alice: 80, 70, 90 Bob: 60, 85, 75 ``` 3. A path to the output text file where the results should be written. Output: The output text file should contain lines in the format: `StudentName: AverageGrade` (sorted alphabetically by student name) - Example: ``` Alice: 80.0 Bob: 73.33 ``` Function Signature: ```python def process_grades(input_file_path: str, output_file_path: str) -> None: pass ``` Constraints: - Grades are non-negative integers from 0 to 100. - Each student will have at least one grade. - The input data will be well-formed as per the described format. Requirements: 1. **File I/O**: Properly handle file reading and writing. 2. **String Manipulation**: Parse and process the grades string. 3. **Calculations**: Compute the average grade with precision up to two decimal places. 4. **Sorting**: Sort the results by student names alphabetically. 5. **Error Handling**: Handle potential file read/write errors gracefully. Example: Given an input file `grades.txt` with the following content: ``` Alice: 80, 70, 90 Bob: 60, 85, 75 ``` After calling `process_grades(\\"grades.txt\\", \\"averages.txt\\")`, the `averages.txt` should contain: ``` Alice: 80.0 Bob: 73.33 ``` Your Task: Implement the `process_grades` function to meet the above requirements.","solution":"def process_grades(input_file_path: str, output_file_path: str) -> None: try: # Read the input file with open(input_file_path, \'r\') as infile: lines = infile.readlines() # Process the grades students = {} for line in lines: name, grades_str = line.split(\':\') grades = list(map(int, grades_str.split(\',\'))) average_grade = sum(grades) / len(grades) students[name.strip()] = round(average_grade, 2) # Sort students by name sorted_students = sorted(students.items()) # Write the results to the output file with open(output_file_path, \'w\') as outfile: for name, average in sorted_students: outfile.write(f\\"{name}: {average}n\\") except Exception as e: print(f\\"An error occurred: {e}\\")"},{"question":"You are provided with the PyTorch library, and you need to demonstrate your understanding of PyTorch\'s configuration using the `torch.__config__` module. This module includes two functions: `show()` and `parallel_info()`. Your task is to implement a function `get_pytorch_config_info` that does the following: - Calls the `show()` function and captures its output. - Calls the `parallel_info()` function and captures its output. - Returns a dictionary with two keys: `config` and `parallel`. The value for `config` should be the result of `show()`, and the value for `parallel` should be the result of `parallel_info()`. # Function Signature ```python def get_pytorch_config_info() -> dict: pass ``` # Input - No input needed for this function. # Output - A dictionary with the following structure: ```python { \\"config\\": \\"<output_of_show_function>\\", \\"parallel\\": \\"<output_of_parallel_info_function>\\" } ``` **Constraints and Limitations** - You should handle any exceptions that might occur during the execution of `show()` or `parallel_info()` functions, and in such cases, set their respective values in the dictionary to `\\"Error\\"`. **Example** Imagine that calling `show()` outputs the string `\\"PyTorch version: 1.7.1\\"` and calling `parallel_info()` outputs the string `\\"Parallel information: This system supports CUDA.\\"` Then your function should return: ```python { \\"config\\": \\"PyTorch version: 1.7.1\\", \\"parallel\\": \\"Parallel information: This system supports CUDA.\\" } ``` Note: The actual output from `show()` and `parallel_info()` might differ depending on the system and PyTorch installation. # Hint To capture the output of a print statement in Python, you can use the `io.StringIO` class from the `io` module.","solution":"import torch import io import contextlib def get_pytorch_config_info() -> dict: Calls PyTorch\'s `show()` and `parallel_info()` functions and captures their outputs. Returns: dict: A dictionary with the outputs of `show()` and `parallel_info()` functions. result = { \\"config\\": \\"\\", \\"parallel\\": \\"\\" } try: f = io.StringIO() with contextlib.redirect_stdout(f): torch.__config__.show() result[\'config\'] = f.getvalue() except Exception as e: result[\'config\'] = \\"Error\\" try: f = io.StringIO() with contextlib.redirect_stdout(f): torch.__config__.parallel_info() result[\'parallel\'] = f.getvalue() except Exception as e: result[\'parallel\'] = \\"Error\\" return result"},{"question":"# Support Vector Machine for Multi-Class Classification You are provided with a dataset containing information about various flower species, represented by four features: sepal length, sepal width, petal length, and petal width. Your task is to implement a multi-class classifier using Support Vector Machines (SVM) to predict the species of a flower based on these features. Dataset Description The dataset is structured as follows: - `X` (2D list or numpy array): A list of lists where each sublist contains four numerical values representing the features of a flower. - `y` (list): A list of integers where each integer represents the species of the flower. There are three species, encoded as 0, 1, and 2. Requirements 1. **Import Necessary Libraries**: - Import `svm` from `sklearn` and any other packages you may need. 2. **Train-Test Split**: - Split the dataset into training and testing sets using an 80-20 split. 3. **Data Scaling**: - Scale the feature data using `StandardScaler`. 4. **Hyperparameter Tuning**: - Use grid search with 5-fold cross-validation to find the best hyperparameters (`C` and `gamma`) for the SVM with an RBF kernel. 5. **Train the SVM**: - Train the SVM model using the best hyperparameters found during grid search. 6. **Evaluate the Model**: - Evaluate the model on the test set and print the classification report including precision, recall, and F1-score for each class. 7. **Plot the Decision Boundaries**: - Plot the decision boundaries of the trained SVM model. Note: Reduce the feature dimensions using PCA to 2D for plotting purposes. Implementation ```python import numpy as np from sklearn import svm, datasets from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.preprocessing import StandardScaler from sklearn.metrics import classification_report from sklearn.decomposition import PCA import matplotlib.pyplot as plt def load_data(): # For the purpose of the assessment, we will use the iris dataset from sklearn. iris = datasets.load_iris() X = iris.data y = iris.target return X, y def prepare_data(X, y): # Split the data into training and testing sets (80-20 split) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) return X_train, X_test, y_train, y_test def scale_data(X_train, X_test): # Scale the data scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) return X_train_scaled, X_test_scaled def find_best_params(X_train_scaled, y_train): # Hyperparameter tuning using grid search with 5-fold cross-validation parameters = {\'C\': [0.1, 1, 10, 100], \'gamma\': [0.001, 0.01, 0.1, 1]} svc = svm.SVC(kernel=\'rbf\') clf = GridSearchCV(svc, parameters, cv=5) clf.fit(X_train_scaled, y_train) return clf.best_params_ def train_svm(X_train_scaled, y_train, best_params): # Train the SVM model using the best hyperparameters model = svm.SVC(kernel=\'rbf\', C=best_params[\'C\'], gamma=best_params[\'gamma\']) model.fit(X_train_scaled, y_train) return model def evaluate_model(model, X_test_scaled, y_test): # Evaluate the model and print the classification report y_pred = model.predict(X_test_scaled) print(classification_report(y_test, y_pred)) def plot_decision_boundaries(model, X, y): # Reduce data to 2D using PCA pca = PCA(n_components=2) X_pca = pca.fit_transform(X) x_min, x_max = X_pca[:, 0].min() - 1, X_pca[:, 0].max() + 1 y_min, y_max = X_pca[:, 1].min() - 1, X_pca[:, 1].max() + 1 xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01), np.arange(y_min, y_max, 0.01)) Z = model.predict(pca.inverse_transform(np.c_[xx.ravel(), yy.ravel()])) Z = Z.reshape(xx.shape) plt.contourf(xx, yy, Z, alpha=0.8) plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, edgecolors=\'k\', marker=\'o\') plt.xlabel(\'Principal Component 1\') plt.ylabel(\'Principal Component 2\') plt.title(\'SVM Decision Boundaries\') plt.show() # Driver code if __name__ == \\"__main__\\": X, y = load_data() X_train, X_test, y_train, y_test = prepare_data(X, y) X_train_scaled, X_test_scaled = scale_data(X_train, X_test) best_params = find_best_params(X_train_scaled, y_train) model = train_svm(X_train_scaled, y_train, best_params) evaluate_model(model, X_test_scaled, y_test) plot_decision_boundaries(model, X, y) ``` Constraints 1. Use the SVM with an RBF kernel for classification. 2. Ensure that you handle any potential warnings or errors. 3. The model should achieve an accuracy of at least 90% on the test set for a valid implementation. Submission Save your implementation in a Python file and submit it through the provided submission portal.","solution":"import numpy as np from sklearn import svm, datasets from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.preprocessing import StandardScaler from sklearn.metrics import classification_report from sklearn.decomposition import PCA import matplotlib.pyplot as plt def load_data(): # For the purpose of the assessment, we will use the iris dataset from sklearn. iris = datasets.load_iris() X = iris.data y = iris.target return X, y def prepare_data(X, y): # Split the data into training and testing sets (80-20 split) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) return X_train, X_test, y_train, y_test def scale_data(X_train, X_test): # Scale the data scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) return X_train_scaled, X_test_scaled def find_best_params(X_train_scaled, y_train): # Hyperparameter tuning using grid search with 5-fold cross-validation parameters = {\'C\': [0.1, 1, 10, 100], \'gamma\': [0.001, 0.01, 0.1, 1]} svc = svm.SVC(kernel=\'rbf\') clf = GridSearchCV(svc, parameters, cv=5) clf.fit(X_train_scaled, y_train) return clf.best_params_ def train_svm(X_train_scaled, y_train, best_params): # Train the SVM model using the best hyperparameters model = svm.SVC(kernel=\'rbf\', C=best_params[\'C\'], gamma=best_params[\'gamma\']) model.fit(X_train_scaled, y_train) return model def evaluate_model(model, X_test_scaled, y_test): # Evaluate the model and print the classification report y_pred = model.predict(X_test_scaled) return classification_report(y_test, y_pred, output_dict=True) def plot_decision_boundaries(model, X, y): # Reduce data to 2D using PCA pca = PCA(n_components=2) X_pca = pca.fit_transform(X) x_min, x_max = X_pca[:, 0].min() - 1, X_pca[:, 0].max() + 1 y_min, y_max = X_pca[:, 1].min() - 1, X_pca[:, 1].max() + 1 xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01), np.arange(y_min, y_max, 0.01)) Z = model.predict(pca.inverse_transform(np.c_[xx.ravel(), yy.ravel()])) Z = Z.reshape(xx.shape) plt.contourf(xx, yy, Z, alpha=0.8) plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, edgecolors=\'k\', marker=\'o\') plt.xlabel(\'Principal Component 1\') plt.ylabel(\'Principal Component 2\') plt.title(\'SVM Decision Boundaries\') plt.show() # Driver code if __name__ == \\"__main__\\": X, y = load_data() X_train, X_test, y_train, y_test = prepare_data(X, y) X_train_scaled, X_test_scaled = scale_data(X_train, X_test) best_params = find_best_params(X_train_scaled, y_train) model = train_svm(X_train_scaled, y_train, best_params) print(evaluate_model(model, X_test_scaled, y_test)) plot_decision_boundaries(model, X, y)"},{"question":"**Objective:** Demonstrate thorough understanding of CPython calling protocols by implementing a callable Python object and correctly using tp_call and vectorcall mechanisms. # Problem Statement You are required to create a Python class that is callable. This class should support both tp_call and vectorcall protocols. Additionally, include methods to demonstrate how to call this callable object using various provided APIs. Requirements: 1. **Class Definition:** Define a class `CallableObject` that initializes with custom data. 2. **Implement tp_call Protocol:** Implement the tp_call protocol to make an instance of `CallableObject` callable. The callable instance should accept any arbitrary arguments and keyword arguments and return a formatted string containing these arguments. 3. **Implement vectorcall Protocol:** Implement the vectorcall protocol. The behavior of the callable instance should remain consistent regardless of using tp_call or vectorcall. 4. **Demonstration Methods:** - Use `PyObject_Call()` to call an instance of `CallableObject`. - Use `PyObject_CallNoArgs()` to call an instance with no arguments. - Use `PyObject_CallOneArg()` to call an instance with a single argument. - Include a way to safely test recursion control by ensuring that deep recursive calls are properly handled without causing stack overflow. Input/Output Specifications: - No specific input from the user is required beyond creating instances of `CallableObject` and calling it with appropriate arguments. - The output should be a demonstration of the class being callable using tp_call and vectorcall protocols with the specified API methods. Each call should output the resulting string. Constraints: - Use CPython 3.10. - Ensure that the callable behavior is consistent across both protocols. - Properly manage recursion control by including `Py_EnterRecursiveCall()` and `Py_LeaveRecursiveCall()` where necessary. - Handle any exceptions that may arise from invalid calls. Example: ```python # Define the class with tp_call and vectorcall protocols class CallableObject: ... # Example calls obj = CallableObject(data=\\"example\\") print(PyObject_Call(obj, ..., ...)) # Expected Output: formatted string with args and kwargs print(PyObject_CallNoArgs(obj)) # Expected Output: formatted string indicating no args print(PyObject_CallOneArg(obj, arg1)) # Expected Output: formatted string with one arg # Demonstrating recursion handling def recursive_call(obj, depth): if depth > 0: return recursive_call(obj, depth - 1) return obj() ``` Ensure that the class and demonstration methods are implemented completely and correctly as per the provided requirements and constraints.","solution":"class CallableObject: def __init__(self, data): self.data = data # tp_call equivalent def __call__(self, *args, **kwargs): arg_list = \', \'.join(map(str, args)) kwarg_list = \', \'.join(f\'{k}={v}\' for k, v in kwargs.items()) if kwarg_list: return f\\"tp_call with args: {arg_list} and kwargs: {kwarg_list}\\" return f\\"tp_call with args: {arg_list}\\" # vectorcall equivalent def vectorcall(self, args, nargsf, kwnames): arg_list = \', \'.join(map(str, args)) if kwnames: kwarg_list = \', \'.join(f\'{k}={v}\' for k, v in zip(kwnames, args[len(args) - len(kwnames):])) return f\\"vectorcall with args: {arg_list} and kwargs: {kwarg_list}\\" return f\\"vectorcall with args: {arg_list}\\" # Function to demonstrate PyObject_Call def py_object_call(obj, args, kwargs): return obj(*args, **kwargs) # Function to demonstrate PyObject_CallNoArgs def py_object_call_no_args(obj): return obj() # Function to demonstrate PyObject_CallOneArg def py_object_call_one_arg(obj, arg): return obj(arg) # Example of recursion handling def recursive_call(obj, depth): if depth > 0: return recursive_call(obj, depth - 1) return obj()"},{"question":"# Python Version Parser You are required to implement a set of functions that parse and construct Python version numbers using the given macros. This exercise will test your understanding of bitwise operations and conditional statements in Python. Function 1: `parse_version_hex` Implement a function `parse_version_hex(version_hex: int) -> dict` that takes a 32-bit integer representing the Python version (in the format of `PY_VERSION_HEX`) and returns a dictionary containing the following keys: - `major`: The major version number. - `minor`: The minor version number. - `micro`: The micro version number. - `release_level`: The release level (one of \'alpha\', \'beta\', \'candidate\', or \'final\'). - `serial`: The release serial number. Function 2: `construct_version_hex` Implement a function `construct_version_hex(version_info: dict) -> int` that takes a dictionary with the following keys: - `major`: The major version number (an integer). - `minor`: The minor version number (an integer). - `micro`: The micro version number (an integer). - `release_level`: The release level (one of \'alpha\', \'beta\', \'candidate\', or \'final\'). - `serial`: The release serial number (an integer). and returns a 32-bit integer representing the Python version in the `PY_VERSION_HEX` format. Constraints - The `version_hex` parameter in `parse_version_hex` is guaranteed to be a valid 32-bit integer. - The `version_info` dictionary passed to `construct_version_hex` will always contain valid values for its keys. # Example ```python version_hex = 0x030401a2 # Expected output from parse_version_hex { \\"major\\": 3, \\"minor\\": 4, \\"micro\\": 1, \\"release_level\\": \\"alpha\\", \\"serial\\": 2 } version_info = { \\"major\\": 3, \\"minor\\": 10, \\"micro\\": 0, \\"release_level\\": \\"final\\", \\"serial\\": 0 } # Expected output from construct_version_hex 0x030a00f0 ``` # Note In the `parse_version_hex` function, the release level corresponds to: - \'alpha\' : 0xA - \'beta\' : 0xB - \'candidate\' : 0xC - \'final\' : 0xF Use bitwise operations to extract and construct the respective fields correctly.","solution":"def parse_version_hex(version_hex): Parses a 32-bit integer representing the Python version and returns a dictionary with version information. major = (version_hex >> 24) & 0xFF minor = (version_hex >> 16) & 0xFF micro = (version_hex >> 8) & 0xFF release_level_code = (version_hex >> 4) & 0xF serial = version_hex & 0xF release_level_lookup = { 0xA: \'alpha\', 0xB: \'beta\', 0xC: \'candidate\', 0xF: \'final\' } release_level = release_level_lookup.get(release_level_code, \'final\') return { \\"major\\": major, \\"minor\\": minor, \\"micro\\": micro, \\"release_level\\": release_level, \\"serial\\": serial } def construct_version_hex(version_info): Constructs a 32-bit integer representing the Python version from a dictionary with version information. release_level_lookup = { \'alpha\': 0xA, \'beta\': 0xB, \'candidate\': 0xC, \'final\': 0xF } major = (version_info[\\"major\\"] & 0xFF) << 24 minor = (version_info[\\"minor\\"] & 0xFF) << 16 micro = (version_info[\\"micro\\"] & 0xFF) << 8 release_level = (release_level_lookup[version_info[\\"release_level\\"]] & 0xF) << 4 serial = version_info[\\"serial\\"] & 0xF version_hex = major | minor | micro | release_level | serial return version_hex"},{"question":"**Pandas Data Visualization Assessment** You are given a dataset containing sales data of a retail store. The dataset includes columns for the date of sale, the sales amount, and the category of the product sold. Your task is to write a function `plot_sales_data` that: 1. Reads the dataset from a CSV file. 2. Plots the total sales over time as a line plot. 3. Creates a bar plot showing the total sales for each category. 4. Generates a scatter plot showing the relationship between sales amount and the day of the month, color-coded by product category. 5. Saves each of these plots to a file in PNG format. 6. Displays all the plots. # Function Signature ```python def plot_sales_data(filename: str) -> None: pass ``` # Input - `filename` (str): The name of the CSV file containing the sales data. The CSV file has the following columns: - \'date\': The date of the sale (format: YYYY-MM-DD). - \'sales_amount\': The amount of the sale. - \'category\': The category of the product. # Output - The function does not return any value but saves three PNG files (\'total_sales_over_time.png\', \'total_sales_per_category.png\', \'sales_amount_vs_day_scatter.png\') and displays all the plots. # Example Assume the CSV file `sales_data.csv` has the following content: ``` date,sales_amount,category 2023-01-01,200,Electronics 2023-01-01,150,Furniture 2023-01-02,120,Electronics 2023-01-03,220,Clothing ... ``` Your function should generate and save the following plots: 1. `total_sales_over_time.png` - A line plot showing the total sales over time. 2. `total_sales_per_category.png` - A bar plot showing the total sales for each product category. 3. `sales_amount_vs_day_scatter.png` - A scatter plot showing the relationship between sales amount and the day of the month, color-coded by product category. # Constraints & Limitations - Ensure proper handling of the date format. - Handle missing or malformed data gracefully. - Label all axes and add a legend where appropriate. - Use appropriate color schemes for better visualization. # Notes - You can assume that there are no negative sales amounts. - Make sure to handle edge cases like empty datasets and missing columns. # Hints - Use `pandas.read_csv` to read the dataset. - Convert the \'date\' column to `datetime` format. - Use `pandas.DataFrame.plot` methods for plotting. - Use `plt.savefig` for saving the plots to files. - Use `plt.show` to display all the plots.","solution":"import pandas as pd import matplotlib.pyplot as plt import seaborn as sns def plot_sales_data(filename: str) -> None: # Read the dataset from the CSV file df = pd.read_csv(filename) # Convert \'date\' column to datetime format df[\'date\'] = pd.to_datetime(df[\'date\']) # Plot the total sales over time as a line plot total_sales_over_time = df.groupby(\'date\')[\'sales_amount\'].sum() plt.figure(figsize=(12, 6)) total_sales_over_time.plot(kind=\'line\') plt.title(\'Total Sales Over Time\') plt.xlabel(\'Date\') plt.ylabel(\'Total Sales\') plt.savefig(\'total_sales_over_time.png\') plt.show() # Create a bar plot showing the total sales for each category total_sales_per_category = df.groupby(\'category\')[\'sales_amount\'].sum() plt.figure(figsize=(12, 6)) total_sales_per_category.plot(kind=\'bar\') plt.title(\'Total Sales Per Category\') plt.xlabel(\'Category\') plt.ylabel(\'Total Sales\') plt.savefig(\'total_sales_per_category.png\') plt.show() # Generate a scatter plot showing the relationship between sales amount and the day of the month df[\'day\'] = df[\'date\'].dt.day plt.figure(figsize=(12, 6)) sns.scatterplot(data=df, x=\'day\', y=\'sales_amount\', hue=\'category\') plt.title(\'Sales Amount vs Day of the Month\') plt.xlabel(\'Day of the Month\') plt.ylabel(\'Sales Amount\') plt.savefig(\'sales_amount_vs_day_scatter.png\') plt.show()"},{"question":"# TorchScript Coding Challenge As a neural network engineer, you have been tasked with converting a Python function into a TorchScript function to leverage the optimization and deployment capabilities provided by TorchScript. You will implement a function that performs various tensor operations and ensures type consistency as required by TorchScript. Problem Statement Implement a TorchScript function `tensor_operations` that performs the following tasks: 1. Takes two input tensors `a` and `b`. 2. Checks if both tensors have the same shape. If not, return a tensor of shape `(0,)`. 3. Computes the element-wise sum of the tensors. 4. Adds a scalar value (3.5) to each element of the resulting tensor. 5. Normalizes the resulting tensor by subtracting its mean and dividing by its standard deviation. 6. Computes and returns a tuple containing: - The normalized tensor. - The mean of the normalized tensor. - The standard deviation of the normalized tensor. Constraints - The function must be implemented using the `@torch.jit.script` decorator to apply TorchScript transformations. - You should use type annotations to specify the input and output types. Input - `a` (Tensor): A tensor of any shape. - `b` (Tensor): A tensor of the same shape as `a`. Output - A tuple containing: - `normalized_tensor` (Tensor): The normalized tensor. - `mean` (float): The mean of the normalized tensor. - `std` (float): The standard deviation of the normalized tensor. Performance Requirements - You must ensure type consistency for all variables within the TorchScript function. - Handling of tensor operations should be optimized as per TorchScript guidelines. Example Usage ```python import torch @torch.jit.script def tensor_operations(a: torch.Tensor, b: torch.Tensor) -> Tuple[torch.Tensor, float, float]: # Implement the function here # Define example tensors a = torch.tensor([1.0, 2.0, 3.0]) b = torch.tensor([4.0, 5.0, 6.0]) # Call the TorchScript function normalized_tensor, mean, std = tensor_operations(a, b) print(normalized_tensor) # Tensor([[normalized values]]) print(mean) # float(mean value) print(std) # float(std deviation) ``` Note: Remember to use appropriate TorchScript-supported operations and ensure type consistency.","solution":"import torch from typing import Tuple @torch.jit.script def tensor_operations(a: torch.Tensor, b: torch.Tensor) -> Tuple[torch.Tensor, float, float]: Performs specified tensor operations and returns the normalized tensor, its mean, and std deviation. if a.shape != b.shape: return torch.zeros(1), 0.0, 0.0 # Compute element-wise sum result = a + b # Add a scalar value (3.5) to each element result = result + 3.5 # Normalize the resulting tensor mean = result.mean() std = result.std() if std.item() == 0: normalized_tensor = torch.zeros_like(result) else: normalized_tensor = (result - mean) / std return normalized_tensor, mean.item(), std.item()"},{"question":"# Email Parsing and Analysis using `email.parser` Module **Objective**: Implement a function that reads a raw email message, extracts specific details depending on whether it\'s a simple or MIME message, and returns a structured summary. **Function Specification**: ```python def parse_email(raw_email: bytes) -> dict: Parse the raw email message and return structured details. Args: raw_email (bytes): The raw email message as a bytes object. Returns: dict: A dictionary with the following keys: - \'subject\': The subject of the email. - \'from\': The sender of the email. - \'to\': The recipient of the email. - \'body\': The plain text content of the email. If it\'s a MIME message, concatenate the text content of all parts. - \'attachments\': A list of attachment filenames if any. If it\'s not a MIME message, this should be an empty list. pass ``` # Constraints - The input `raw_email` will be a valid email message in bytes. - The email may be a simple text email or a complex MIME email with attachments. - Ensure the function can gracefully handle non-compliant email messages and incorporate any defects found in the parsing process in the returned dictionary under a key `defects`. # Example Usage ```python raw_email = b\\"Your raw email message here in bytes format\\" parsed_email = parse_email(raw_email) print(parsed_email) # Expected output structure: # { # \'subject\': \'...\', # \'from\': \'...\', # \'to\': \'...\', # \'body\': \'...\', # \'attachments\': [\'filename1\', \'filename2\'], # \'defects\': [\'defect1\', \'defect2\'] # } ``` # Notes - Utilize the `email.parser.BytesFeedParser` for parsing the email messages incrementally. - Use methods such as `is_multipart()`, `get_body()`, and `walk()` to handle MIME messages correctly. - Catch and handle any parsing errors or non-compliance defects and include them in the output.","solution":"from email import message_from_bytes from email.parser import BytesParser from email.policy import default def parse_email(raw_email: bytes) -> dict: msg = BytesParser(policy=default).parsebytes(raw_email) result = { \'subject\': msg[\'subject\'], \'from\': msg[\'from\'], \'to\': msg[\'to\'], \'body\': \'\', \'attachments\': [], \'defects\': [] } if msg.is_multipart(): for part in msg.iter_parts(): content_type = part.get_content_type() content_disposition = part.get_content_disposition() if content_disposition == \'attachment\': filename = part.get_filename() if filename: result[\'attachments\'].append(filename) elif content_type == \'text/plain\': payload = part.get_payload(decode=True) charset = part.get_content_charset() if charset: payload = payload.decode(charset, errors=\'replace\') else: payload = payload.decode(errors=\'replace\') result[\'body\'] += payload # Handle defects in MIME parts defects = part.defects if defects: result[\'defects\'].extend(str(d) for d in defects) else: payload = msg.get_payload(decode=True) charset = msg.get_content_charset() if charset: payload = payload.decode(charset, errors=\'replace\') else: payload = payload.decode(errors=\'replace\') result[\'body\'] = payload # Handle defects in the main message defects = msg.defects if defects: result[\'defects\'].extend(str(d) for d in defects) return result"},{"question":"# **Coding Assessment Question** **Objective:** Demonstrate the ability to securely handle user input and retrieve environment information using the `getpass` module. **Question:** You are working on a secure application that requires you to fetch the current user\'s login name and prompt them for a password without displaying it on the screen. Implement a function `get_user_credentials()` that: 1. Retrieves the current user\'s login name using `getpass.getuser()`. 2. Prompts the user for a password using `getpass.getpass(prompt=\'Enter your password: \')`. Your implementation should ensure that: - If the necessary environment variables are not set, the function should raise an appropriate exception. - If retrieving the password input securely (without echo) is not possible, the function should print a warning message and still retrieve the password. **Function Signature:** ```python def get_user_credentials() -> tuple: pass ``` **Expected Input/Output:** - The function takes no inputs directly. - The function returns a tuple `(username, password)` where `username` is a string with the login name and `password` is the string input by the user. **Example:** Suppose the environment variable `USER` is set to `john_doe` and the user inputs `mypassword` when prompted. ```python user_credentials = get_user_credentials() print(user_credentials) ``` Output: ```plaintext (\'john_doe\', \'mypassword\') ``` **Constraints:** - You are not allowed to use any input statements other than `getpass.getpass()`. - Use of `os.getlogin()` or direct manipulation of environment variables (`os.getenv()`, etc.) is prohibited. **Notes:** - Test the function in a secure environment. Be cautious of the environment you use to avoid unintentional echo of sensitive information.","solution":"import getpass def get_user_credentials() -> tuple: try: username = getpass.getuser() password = getpass.getpass(prompt=\'Enter your password: \') return username, password except Exception as e: raise RuntimeError(\\"Error retrieving user credentials\\") from e"},{"question":"Using the Python `lzma` module, write a function named `compress_decompress_data` that performs the following tasks: 1. Compresses a given string using the LZMA algorithm. 2. Decompresses the compressed string back to its original form. 3. Verifies that the decompressed string matches the original string. Function Signature ```python def compress_decompress_data(data: str) -> bool: pass ``` Input: - `data` (str): A string containing the data to be compressed and then decompressed. Maximum length of the string is 1000 characters. Output: - `output` (bool): True if the decompressed data matches the original data, False otherwise. Constraints: 1. You must use the `lzma` module for compression and decompression. 2. The function should handle any exceptions that may arise during compression or decompression and return False in such cases. Example: ```python data = \\"This is a test string for compression and decompression using lzma.\\" result = compress_decompress_data(data) print(result) # Expected output: True ``` # Additional Notes: - You may refer to the `lzma` module documentation to understand the specifics of how to use its classes and methods. - Make sure your solution is efficient and follows best practices for exception handling in Python.","solution":"import lzma def compress_decompress_data(data: str) -> bool: try: # Compress the data using LZMA compressed_data = lzma.compress(data.encode(\'utf-8\')) # Decompress the data back decompressed_data = lzma.decompress(compressed_data) # Verify if the decompressed data matches the original data return decompressed_data.decode(\'utf-8\') == data except Exception as e: # Return False if any exception occurs return False"},{"question":"In this problem, you are required to create a subclass of the `reprlib.Repr` class to provide custom size-limited representations for two specific custom types: `Matrix` and `Graph`. Your solution should include: 1. A `Matrix` class that represents a 2D matrix of integers. 2. A `Graph` class that represents an undirected graph using an adjacency list. 3. A custom subclass of `reprlib.Repr` named `CustomRepr` that provides size-limited representations for instances of `Matrix` and `Graph`. # Matrix Class The `Matrix` class should have: - An initializer that takes a list of lists of integers and stores it. - A `__repr__` method for debugging purposes that returns the string version of the matrix. # Graph Class The `Graph` class should have: - An initializer that takes a dictionary where keys are nodes and values are lists of adjacent nodes. - A `__repr__` method for debugging purposes that returns the string version of the adjacency list. # CustomRepr Class The `CustomRepr` class should: - Inherit from `reprlib.Repr`. - Override methods for representing `Matrix` and `Graph` objects with size constraints on their string representations. **Constraints and Details:** - For the `Matrix` class, limit the number of rows to be displayed to 3 and the number of columns to 3. - For the `Graph` class, limit the number of nodes to be displayed to 4. - Use `reprlib.recursive_repr()` decorator to handle potential recursive representations. # Function Signature ```python import reprlib class Matrix: def __init__(self, data: list[list[int]]): self.data = data def __repr__(self): return repr(self.data) class Graph: def __init__(self, adjacency_list: dict): self.adjacency_list = adjacency_list def __repr__(self): return repr(self.adjacency_list) class CustomRepr(reprlib.Repr): def repr_Matrix(self, obj, level): # Implement the size-limited representation of Matrix here def repr_Graph(self, obj, level): # Implement the size-limited representation of Graph here def format_object(obj): custom_repr = CustomRepr() return custom_repr.repr(obj) # Example usage matrix = Matrix([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12], [13, 14, 15, 16]]) graph = Graph({1: [2, 3, 4, 5], 2: [1, 3], 3: [1, 2, 4], 4: [1, 3, 5], 5: [1, 4]}) print(format_object(matrix)) # Expected to show a limited representation of the matrix print(format_object(graph)) # Expected to show a limited representation of the graph ``` # Notes: - You need to implement the `repr_Matrix` and `repr_Graph` methods to achieve the desired size-limited string representations. - Ensure that you handle potentially large or deeply nested data gracefully.","solution":"import reprlib class Matrix: def __init__(self, data): self.data = data def __repr__(self): return repr(self.data) class Graph: def __init__(self, adjacency_list): self.adjacency_list = adjacency_list def __repr__(self): return repr(self.adjacency_list) class CustomRepr(reprlib.Repr): def repr_Matrix(self, obj, level): s = [] if len(obj.data) > 3: rows = obj.data[:3] + [[...]] else: rows = obj.data for row in rows: if len(row) > 3: s.append(row[:3] + [...]) else: s.append(row) return \'Matrix(\' + repr(s) + \')\' def repr_Graph(self, obj, level): nodes = list(obj.adjacency_list.items()) if len(nodes) > 4: nodes = nodes[:4] + [(\'...\', \'...\')] return \'Graph(\' + repr(dict(nodes)) + \')\' def format_object(obj): custom_repr = CustomRepr() return custom_repr.repr(obj) # Example usage matrix = Matrix([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12], [13, 14, 15, 16]]) graph = Graph({1: [2, 3, 4, 5], 2: [1, 3], 3: [1, 2, 4], 4: [1, 3, 5], 5: [1, 4]}) print(format_object(matrix)) # Expected to show a limited representation of the matrix print(format_object(graph)) # Expected to show a limited representation of the graph"},{"question":"Objective: The goal of this assessment is to evaluate your understanding of the `torch.futures` package and your ability to work with asynchronous operations in PyTorch. Problem Statement: You are given an asynchronous task that processes data chunks and returns the processed result as a Future object. Your task is to implement a function that: 1. Processes multiple data chunks asynchronously. 2. Collects all the results using the `collect_all` utility function. 3. Aggregates the final result once all tasks are completed. Function Signature: ```python import torch from torch.futures import Future, collect_all async def process_data_chunk(chunk: int) -> int: Simulates an asynchronous data processing task. This function should return a Future object that completes after a delay and returns the processed data (which, for simplicity, will be the square of the chunk). Args: chunk (int): The data chunk to process. Returns: Future[int]: A future that will complete with the processed result. pass def process_and_aggregate_data(data_chunks: List[int]) -> int: Processes a list of data chunks asynchronously and aggregates the results. It should: 1. Use process_data_chunk to process each chunk. 2. Collect all the futures using collect_all. 3. Once all futures are complete, sum the results and return the total. Args: data_chunks (List[int]): List of data chunks to process. Returns: int: The aggregated sum of processed data chunks. pass ``` Constraints: - You must use the `torch.futures` package to handle Futures. - The `process_data_chunk` function should simulate an asynchronous operation with a delay using `await asyncio.sleep`. - The final aggregation should only proceed after all Futures have completed. - You should ensure the function handles a list with a minimum of 1 and a maximum of 1000 data chunks. Example: ```python import asyncio import torch from torch.futures import Future, collect_all async def process_data_chunk(chunk: int) -> int: await asyncio.sleep(1) # Simulate asynchronous processing delay return chunk * chunk def process_and_aggregate_data(data_chunks: List[int]) -> int: futures = [process_data_chunk(chunk) for chunk in data_chunks] all_futures = collect_all(futures) aggregated_result = sum(result for result in all_futures.wait()) return aggregated_result # Example usage data_chunks = [1, 2, 3, 4] result = process_and_aggregate_data(data_chunks) print(result) # Output should be 30 (1^2 + 2^2 + 3^2 + 4^2) ``` Performance Requirements: - The processing time for each chunk should not exceed the simulated delay. - The aggregation should efficiently handle up to 1000 data chunks. You are evaluated based on: - Correctness: The functions should correctly handle and process the data. - Asynchronous handling: Proper use of async/await and Future objects. - Efficiency: Minimizing unnecessary delays and optimizing collection and aggregation.","solution":"import torch import asyncio from torch.futures import Future, collect_all from typing import List async def process_data_chunk(chunk: int) -> int: Simulates an asynchronous data processing task. This function should return a Future object that completes after a delay and returns the processed data (the square of the chunk). Args: chunk (int): The data chunk to process. Returns: Future[int]: A future that will complete with the processed result. await asyncio.sleep(1) # Simulate asynchronous processing delay return chunk * chunk async def process_and_aggregate_data(data_chunks: List[int]) -> int: Processes a list of data chunks asynchronously and aggregates the results. It should: 1. Use process_data_chunk to process each chunk. 2. Collect all the futures using collect_all. 3. Once all futures are complete, sum the results and return the total. Args: data_chunks (List[int]): List of data chunks to process. Returns: int: The aggregated sum of processed data chunks. futures = [asyncio.ensure_future(process_data_chunk(chunk)) for chunk in data_chunks] all_futures = await collect_all(futures) aggregated_result = sum(future.result() for future in all_futures) return aggregated_result"},{"question":"# **Coding Assessment Question** **Objective:** You are tasked with implementing a function that trains a Bernoulli Restricted Boltzmann Machine (RBM) and uses it to transform given binary data. You will utilize the `BernoulliRBM` class from scikit-learn and apply it to a synthetic dataset of binary images. **Description:** Implement a function `train_and_transform_rbm(data, n_components, learning_rate, n_iter)` that performs the following steps: 1. Initializes a BernoulliRBM with the provided parameters: `n_components`, `learning_rate`, and `n_iter`. 2. Trains the RBM using the binary input data. 3. Transforms the input data using the trained RBM to generate the hidden features. 4. Returns the transformed data (hidden features). **Function Signature:** ```python import numpy as np from sklearn.neural_network import BernoulliRBM def train_and_transform_rbm(data: np.ndarray, n_components: int, learning_rate: float, n_iter: int) -> np.ndarray: pass ``` **Input:** - `data`: A numpy array of shape `(n_samples, n_features)` containing binary input data. - `n_components`: An integer representing the number of binary hidden units. - `learning_rate`: A float representing the learning rate used in training the RBM. - `n_iter`: An integer representing the number of iterations/sweeps over the training data. **Output:** - A numpy array of shape `(n_samples, n_components)` containing the transformed hidden features generated by the trained RBM. **Constraints:** - The input data must be binary (values are either 0 or 1) or probabilities between 0 and 1. - Ensure that the function handles data with different numbers of samples and features robustly. **Examples:** ```python # Example Binary Data (5 samples, 10 features) data = np.array([[0, 1, 0, 1, 0, 1, 0, 1, 0, 1], [1, 0, 1, 0, 1, 0, 1, 0, 1, 0], [1, 1, 1, 1, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 1, 1, 1, 1, 1], [0, 1, 0, 1, 0, 1, 0, 1, 0, 1]]) n_components = 5 learning_rate = 0.1 n_iter = 10 transformed_data = train_and_transform_rbm(data, n_components, learning_rate, n_iter) print(transformed_data) ``` **Notes:** - Use the `BernoulliRBM` class from scikit-learn to create the RBM instance. - Handle the data preprocessing if necessary to ensure the input meets the requirements. - You may assume that the `data` input is properly formatted but expect different sizes in terms of samples and features. - Document your code and explain any important design decisions made during implementation.","solution":"import numpy as np from sklearn.neural_network import BernoulliRBM def train_and_transform_rbm(data: np.ndarray, n_components: int, learning_rate: float, n_iter: int) -> np.ndarray: Trains a Bernoulli Restricted Boltzmann Machine (RBM) and uses it to transform the given binary data. Parameters: - data: numpy array of shape (n_samples, n_features) containing binary input data. - n_components: int, number of binary hidden units. - learning_rate: float, learning rate used in training the RBM. - n_iter: int, number of iterations/sweeps over the training data. Returns: - transformed_data: numpy array of shape (n_samples, n_components) containing the transformed hidden features. # Initialize the BernoulliRBM with the provided parameters rbm = BernoulliRBM(n_components=n_components, learning_rate=learning_rate, n_iter=n_iter) # Fit the RBM to the data rbm.fit(data) # Transform the data using the trained RBM transformed_data = rbm.transform(data) return transformed_data"},{"question":"# Task: Manipulating Export IR in PyTorch In this assessment, you will demonstrate your understanding of PyTorch\'s Export IR by converting a given model into its Export IR representation, inspecting the graph, and performing some manipulations. Step 1: Define and Export a Simple Model Define a simple PyTorch model, `SimpleModel`, with the following structure: - Takes two inputs, `x` and `y`. - Computes the sum of `x` and `y`. - Computes the product of `x` and `y`. - Returns the sum and product. Then, export this model to its Export IR representation using the `torch.export.export` function. Step 2: Inspect the Graph Inspect the exported graph and print its structure. Ensure that the graph has placeholders for the inputs, nodes for the addition and multiplication operations, and an output node that returns the results. Step 3: Modify the Graph Modify the graph to remove the multiplication operation and only return the sum of `x` and `y`. Ensure the modified graph correctly represents this change. Input Format - None specified. Output Format - Print the original graph structure after exporting. - Print the modified graph structure after removing the multiplication operation. ```python import torch from torch import nn class SimpleModel(nn.Module): def forward(self, x, y): sum_xy = x + y product_xy = x * y return sum_xy, product_xy # Define example inputs example_inputs = (torch.randn(1), torch.randn(1)) # Step 1: Export the model to Export IR exported_model = torch.export.export(SimpleModel(), example_inputs) print(\\"Original Graph:\\") print(exported_model.graph) # Step 2: Inspect the graph # No need to change anything here # Step 3: Modify the Graph # Remove the product node and return only the sum node # Convert the modified graph to Export IR or TorchScript and print it # (Write your code here) # Print the modified graph print(\\"Modified Graph:\\") print(exported_model.graph) ``` Note: Ensure you handle the graph modifications correctly and maintain the structure required by Export IR. Your solution should focus on accurately manipulating the computational graph and verifying its correctness through the printed graph structure.","solution":"import torch from torch import nn from torch.fx import symbolic_trace class SimpleModel(nn.Module): def forward(self, x, y): sum_xy = x + y product_xy = x * y return sum_xy, product_xy # Define example inputs example_inputs = (torch.randn(1), torch.randn(1)) # Step 1: Export the model to Export IR model = SimpleModel() traced = symbolic_trace(model) print(\\"Original Graph:\\") print(traced.graph) # Step 2: Inspect the graph # No need to change anything here # Step 3: Modify the Graph # Remove the product node and return only the sum node class ModifiedModel(torch.nn.Module): def forward(self, x, y): return x + y # Re-export the modified model modified_model = ModifiedModel() modified_traced = symbolic_trace(modified_model) print(\\"Modified Graph:\\") print(modified_traced.graph)"},{"question":"# Question: Implement and Analyze a PLS Regression Model Partial Least Squares (PLS) is a supervised method that projects data into a lower-dimensional latent space while considering the covariance between predictors and targets. This task will test your understanding of implementing and using the `PLSRegression` class from the scikit-learn library. Task: 1. **Load and Preprocess Data:** - Load a suitable dataset that contains multiple features and a target variable (e.g., the Diabetes dataset from scikit-learn). - Split the data into training and testing sets (80-20 split). 2. **Implement PLSRegression:** - Initialize a PLSRegression model with `n_components=2`. - Fit the model using the training data. - Transform both the training and testing data using the fitted model. 3. **Evaluate the Model:** - Predict the target variable for the test set using the transformed data. - Calculate and print the Mean Squared Error (MSE) for the test predictions. - Plot the predicted vs true target values to visualize the model\'s performance. Required Implementation: ```python import numpy as np import matplotlib.pyplot as plt from sklearn.model_selection import train_test_split from sklearn.datasets import load_diabetes from sklearn.cross_decomposition import PLSRegression from sklearn.metrics import mean_squared_error # Step 1: Load and preprocess the dataset diabetes = load_diabetes() X = diabetes.data y = diabetes.target # Split the data into training and testing sets (80-20 split) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Step 2: Implement PLSRegression pls = PLSRegression(n_components=2) pls.fit(X_train, y_train) # Step 3: Evaluate the model y_pred = pls.predict(X_test) mse = mean_squared_error(y_test, y_pred) print(f\\"Mean Squared Error: {mse}\\") # Plot predicted vs true values plt.scatter(y_test, y_pred) plt.xlabel(\\"True Values\\") plt.ylabel(\\"Predicted Values\\") plt.title(\\"True vs Predicted Target Values\\") plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], \'r\') plt.show() ``` Constraints: - Ensure to split the dataset using `train_test_split` with a random state for reproducibility. - Use `n_components=2` specifically for the PLSRegression initialization. - Evaluate using Mean Squared Error (MSE). Implement the entire code in a .py file or a Jupyter Notebook and submit it for assessment.","solution":"import numpy as np import matplotlib.pyplot as plt from sklearn.model_selection import train_test_split from sklearn.datasets import load_diabetes from sklearn.cross_decomposition import PLSRegression from sklearn.metrics import mean_squared_error def pls_regression_analysis(): # Step 1: Load and preprocess the dataset diabetes = load_diabetes() X = diabetes.data y = diabetes.target # Split the data into training and testing sets (80-20 split) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Step 2: Implement PLSRegression pls = PLSRegression(n_components=2) pls.fit(X_train, y_train) # Step 3: Evaluate the model y_pred = pls.predict(X_test) mse = mean_squared_error(y_test, y_pred) print(f\\"Mean Squared Error: {mse}\\") # Plot predicted vs true values plt.scatter(y_test, y_pred) plt.xlabel(\\"True Values\\") plt.ylabel(\\"Predicted Values\\") plt.title(\\"True vs Predicted Target Values\\") plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], \'r\') plt.show() return mse, y_test, y_pred"},{"question":"# Thread State Management in Python Embedding In this coding question, you\'ll simulate a scenario where a C-extension or an embedding C application needs to manage Python\'s Global Interpreter Lock (GIL) while executing Python code from a thread created outside of Python. # Requirements Implement the following Python function: ```python def call_python_function_from_non_python_thread(py_func, args): This function simulates the behavior of calling a Python function from a non-Python thread. Parameters: - py_func: A callable Python object (e.g., a function or a method). - args: A tuple of arguments to pass to the callable object. Returns: - The result of py_func(*args). Steps: 1. Ensure the current thread can call the Python C API using the appropriate macro/functions. 2. Call the provided Python function `py_func` with the arguments `args`. 3. Release any resources acquired for GIL management. You can simulate the threading environment using Python\'s `threading` module but ensure you handle the GIL management in a way representative of C extensions. # Your implementation here # Example Usage import threading def example_function(x, y): return x + y # simulating a call from a non-python thread result = threading.Thread(target=call_python_function_from_non_python_thread, args=(example_function, (3, 4))) result.start() result.join() print(\\"Returned result:\\", result) ``` # Constraints 1. **GIL Management:** Given that standard Python threading does handle GIL management internally, simulate the `PyGILState_Ensure()` and `PyGILState_Release()` functionality to ensure correct behavior. 2. **Thread Safety:** Ensure that the Python function is safely called from a non-Python thread context. 3. **Input Types:** The `py_func` parameter will always be a valid callable, and `args` will always be a tuple. # Notes - The implementation is only a simulation to test your understanding of GIL management. The actual C extensions would leverage the C-API functions directly. - Consider edge cases where the function may raise an exception and ensure proper resource management (GIL release). Good luck and happy coding!","solution":"import ctypes import threading # We\'re going to use `ctypes` to handle the GIL, for the sake of simulation. # Typically this would be done with the Python C API in a C extension. # Load the Python API library pythonapi = ctypes.PyDLL(None) PyGILState_Ensure = pythonapi.PyGILState_Ensure PyGILState_Release = pythonapi.PyGILState_Release # Define the function def call_python_function_from_non_python_thread(py_func, args): This function simulates the behavior of calling a Python function from a non-Python thread. Parameters: - py_func: A callable Python object (e.g., a function or a method). - args: A tuple of arguments to pass to the callable object. Returns: - The result of py_func(*args). # Ensure that the current thread can call the Python C API gil_state = PyGILState_Ensure() try: # Call the Python function with the provided arguments result = py_func(*args) except Exception as e: result = e finally: # Release the GIL to the previous state PyGILState_Release(gil_state) return result"},{"question":"# Custom Logging Configuration Setup You are tasked with designing a function to set up a custom logging configuration using a dictionary schema. Your function should: 1. Take a logging configuration dictionary as an input. 2. Validate the dictionary against the required schema elements. 3. Apply the logging configuration using `logging.config.dictConfig`. Requirements: - **Input**: A single dictionary parameter `config`. - **Output**: No explicit output, but the function should configure the logging module. - The dictionary should follow the schema detailed in the documentation, specifically checking for the presence of essential keys and valid values. Validation Constraints: - The configuration dictionary must contain the key `version` with the value `1`. - It should optionally include keys like `formatters`, `filters`, `handlers`, `loggers`, `root`, `incremental`, and `disable_existing_loggers`. - If present, each of these keys must adhere to the sub-schema rules as described. # Example ```python sample_config = { \\"version\\": 1, \\"formatters\\": { \\"brief\\": { \\"format\\": \\"%(levelname)-8s: %(message)s\\" }, \\"precise\\": { \\"format\\": \\"%(asctime)s %(name)-15s %(levelname)-8s %(message)s\\" } }, \\"handlers\\": { \\"console\\": { \\"class\\": \\"logging.StreamHandler\\", \\"level\\": \\"DEBUG\\", \\"formatter\\": \\"brief\\", \\"stream\\": \\"ext://sys.stdout\\" }, \\"file\\": { \\"class\\": \\"logging.handlers.RotatingFileHandler\\", \\"level\\": \\"INFO\\", \\"formatter\\": \\"precise\\", \\"filename\\": \\"app.log\\", \\"maxBytes\\": 1048576, \\"backupCount\\": 3 } }, \\"loggers\\": { \\"example\\": { \\"level\\": \\"DEBUG\\", \\"handlers\\": [\\"console\\", \\"file\\"] } } } ``` # Instructions: 1. Implement the function `setup_logging(config)`. 2. Ensure that your function validates the dictionary and raises appropriate exceptions (`ValueError`, `TypeError`, etc.) if the dictionary doesn\'t conform to the required schema. 3. Use the `logging.config.dictConfig` method to apply the configuration if validation passes. 4. The function should be robust enough to handle various erroneous configurations that might raise exceptions as specified in the documentation. ```python import logging.config def setup_logging(config): Validates and sets up logging configuration from a dictionary. Parameters: config (dict): A dictionary conforming to the logging configuration schema. Raises: ValueError, TypeError, AttributeError, ImportError: if any error occurs during configuration. # Your implementation here ``` # Testing To test your implementation, use various configuration dictionaries to ensure that: - Valid configurations are applied successfully. - Invalid configurations raise the appropriate exceptions. Your solution will be evaluated based on correctness, adherence to the schema, and error handling.","solution":"import logging.config def setup_logging(config): Validates and sets up logging configuration from a dictionary. Parameters: config (dict): A dictionary conforming to the logging configuration schema. Raises: ValueError, TypeError, AttributeError, ImportError: if any error occurs during configuration. if not isinstance(config, dict): raise TypeError(\\"Config must be a dictionary\\") # Validate \'version\' key if \'version\' not in config or config[\'version\'] != 1: raise ValueError(\\"Config must contain \'version\' key with value 1\\") # Optional keys optional_keys = [\'formatters\', \'filters\', \'handlers\', \'loggers\', \'root\', \'incremental\', \'disable_existing_loggers\'] for key in config.keys(): if key not in [\'version\'] + optional_keys: raise ValueError(f\\"Invalid key \'{key}\' in config\\") # Apply logging configuration logging.config.dictConfig(config)"},{"question":"Advanced Plotting with seaborn.objects Objective: To assess your understanding of the seaborn library, especially with the `seaborn.objects` module, you are required to load, manipulate a dataset and create an advanced plot with customized annotations. Instructions: 1. **Load dataset**: - Load the `glue` dataset using the `load_dataset` function from seaborn. 2. **Manipulate dataset**: - Pivot the dataset such that the index is `[Model, Encoder]`, columns are `Task`, and values are `Score`. - Add a new column called `Average` that contains the row-wise average of the scores, rounded to one decimal place. - Sort the dataset in descending order based on the `Average` column. 3. **Create plot**: - Using `seaborn.objects.Plot`, create a scatter plot that maps: - `SST-2` scores to the x-axis. - `MRPC` scores to the y-axis. - `Model` names should be displayed as text annotations. 4. **Customize plot**: - Add points (`so.Dot()`) to the plot. - Color the points based on the `Encoder` variable. - Text that annotates the points should appear above the dots (`valign=\'bottom\'`). 5. **Secondary plot**: - Create a bar plot that maps: - `Average` scores to the x-axis. - `Model` names to the y-axis. - Annotate the bars with the `Average` score, aligned to the right with a white color text. 6. **Additional features**: - Fine-tune the text annotations in the bar plot to offset the text horizontally by 6 units. - Ensure the final plot includes all graphical elements and is displayed correctly. Expected Input: None. The instructions provide all necessary details. Expected Output: A matplotlib figure with the specified plots and customizations. Constraints: - Make sure to follow the instructions on data manipulation and plotting exactly as described. - Use proper seaborn objects and methods to meet the requirements. - Ensure the plot is formatted and displayed correctly. ```python # Your solution code starts here import seaborn.objects as so from seaborn import load_dataset import pandas as pd def create_custom_plots(): # Load the dataset glue = ( load_dataset(\\"glue\\") .pivot(index=[\\"Model\\", \\"Encoder\\"], columns=\\"Task\\", values=\\"Score\\") .assign(Average=lambda x: x.mean(axis=1).round(1)) .sort_values(\\"Average\\", ascending=False) ) # Scatter plot with text annotations scatter_plot = ( so.Plot(glue, x=\\"SST-2\\", y=\\"MRPC\\", color=\\"Encoder\\", text=\\"Model\\") .add(so.Dot()) .add(so.Text(valign=\\"bottom\\")) ) # Bar plot with text annotations bar_plot = ( so.Plot(glue, x=\\"Average\\", y=\\"Model\\", text=\\"Average\\") .add(so.Bar()) .add(so.Text(color=\\"w\\", halign=\\"right\\", offset=6)) ) # Display plots scatter_plot.show() bar_plot.show() # Call the function to create and show plots create_custom_plots() ``` Ensure your code is well-commented and clearly structured.","solution":"import seaborn.objects as so from seaborn import load_dataset import pandas as pd def create_custom_plots(): # Load the dataset glue = ( load_dataset(\\"glue\\") .pivot(index=[\\"Model\\", \\"Encoder\\"], columns=\\"Task\\", values=\\"Score\\") .assign(Average=lambda x: x.mean(axis=1).round(1)) .sort_values(\\"Average\\", ascending=False) ) # Scatter plot with text annotations scatter_plot = ( so.Plot(glue, x=\\"SST-2\\", y=\\"MRPC\\", color=\\"Encoder\\", text=\\"Model\\") .add(so.Dot()) .add(so.Text(valign=\\"bottom\\")) ) # Bar plot with text annotations bar_plot = ( so.Plot(glue, x=\\"Average\\", y=\\"Model\\", text=\\"Average\\") .add(so.Bar()) .add(so.Text(color=\\"w\\", halign=\\"right\\", offset=6)) ) # Display plots scatter_plot.show() bar_plot.show() # Call the function to create and show plots create_custom_plots()"},{"question":"Objective Demonstrate your understanding of Python\'s reference counting and practice managing reference lifecycles within a hypothetical Python C extension module. Background In Python, memory management often involves automatic garbage collection. However, when interfacing with C code, such as writing Python C extensions, it is crucial to manually manage reference counts to ensure objects are appropriately allocated and freed. To practice this, you will simulate managing reference counts in a pure Python context by creating a `RefCounter` class that mimics some behaviors of the reference counting mechanisms described. Task Implement the `RefCounter` class in Python, adhering to the following specifications: 1. The class should manage a reference count for the objects it wraps. 2. Provide methods that simulate `Py_INCREF`, `Py_XINCREF`, `Py_DECREF`, and `Py_XDECREF`. 3. Ensure that if the reference count reaches zero, the object is considered \\"deleted\\" (you can denote this by setting the internal object to `None`). 4. Create methods `new_ref` and `xnew_ref` that behave like `Py_NewRef` and `Py_XNewRef`. Class Definition ```python class RefCounter: def __init__(self, obj): Initialize the reference counter with an object and set reference count to 1. :param obj: The object to manage. self._obj = obj self._ref_count = 1 if obj is not None else 0 def incref(self): Simulate Py_INCREF: Increment the reference count. if self._obj is not None: self._ref_count += 1 def xincref(self): Simulate Py_XINCREF: Increment the reference count, if the object is not None. if self._obj is not None: self.incref() def decref(self): Simulate Py_DECREF: Decrement the reference count. If the reference count reaches zero, consider the object \'deleted\' (set it to None). if self._obj is not None: self._ref_count -= 1 if self._ref_count == 0: self._obj = None def xdecref(self): Simulate Py_XDECREF: Decrement the reference count, if the object is not None. If the reference count reaches zero, consider the object \'deleted\' (set it to None). if self._obj is not None: self.decref() def new_ref(self): Simulate Py_NewRef: Create a new strong reference to the object. if self._obj is not None: self.incref() return self def xnew_ref(self): Simulate Py_XNewRef: Create a new strong reference to the object, if it is not None. if self._obj is not None: self.new_ref() return self @property def ref_count(self): Get the current reference count. return self._ref_count @property def obj(self): Get the currently managed object. return self._obj ``` Example Usage ```python rc = RefCounter({\\"key\\": \\"value\\"}) rc.incref() print(rc.ref_count) # Output: 2 rc.decref() print(rc.ref_count) # Output: 1 rc.decref() print(rc.ref_count) # Output: 0 print(rc.obj) # Output: None ``` Constraints 1. Ensure that the reference count never goes below zero. 2. Test your implementation thoroughly to avoid memory leaks or premature deletions. Submission Submit the complete `RefCounter` class implementation and any test cases you used to verify your solution.","solution":"class RefCounter: def __init__(self, obj): Initialize the reference counter with an object and set the reference count to 1. :param obj: The object to manage. self._obj = obj self._ref_count = 1 if obj is not None else 0 def incref(self): Simulate Py_INCREF: Increment the reference count. if self._obj is not None: self._ref_count += 1 def xincref(self): Simulate Py_XINCREF: Increment the reference count, if the object is not None. if self._obj is not None: self.incref() def decref(self): Simulate Py_DECREF: Decrement the reference count. If the reference count reaches zero, consider the object \'deleted\' (set it to None). if self._obj is not None: self._ref_count -= 1 if self._ref_count == 0: self._obj = None def xdecref(self): Simulate Py_XDECREF: Decrement the reference count, if the object is not None. If the reference count reaches zero, consider the object \'deleted\' (set it to None). if self._obj is not None: self.decref() def new_ref(self): Simulate Py_NewRef: Create a new strong reference to the object. if self._obj is not None: self.incref() return self def xnew_ref(self): Simulate Py_XNewRef: Create a new strong reference to the object, if it is not None. if self._obj is not None: self.new_ref() return self @property def ref_count(self): Get the current reference count. return self._ref_count @property def obj(self): Get the currently managed object. return self._obj"},{"question":"**Python 3 Coding Assessment Question: Buffer Handling** # Background Buffers provide a way to handle raw binary data in Python functions effectively. They are particularly useful in scenarios that require direct manipulation of memory, such as image processing, networking, and interfacing with hardware. While Python 3 introduced a new buffer protocol, several functions from the old buffer protocol are still available for backward compatibility. Understanding both the old and new buffer protocols can be crucial for writing efficient and compatible Python code. # Objective Implement a function in Python that uses the old buffer protocol to read and write data to a buffer. Your implementation should make use of the provided `python310` documentation. # Task Write a class `BufferHandler` with the following methods: 1. `read_char_buffer(self, obj) -> str`: - Uses `PyObject_AsCharBuffer` to read data from an object supporting the single-segment character buffer interface. - Returns the buffer content as a string. 2. `read_arbitrary_buffer(self, obj) -> bytes`: - Uses `PyObject_AsReadBuffer` to read data from an object supporting the single-segment readable buffer interface. - Returns the buffer content as bytes. 3. `is_readable_buffer(self, obj) -> bool`: - Uses `PyObject_CheckReadBuffer` to check if an object supports the single-segment readable buffer interface. - Returns `True` if it supports; otherwise `False`. 4. `write_buffer(self, obj) -> int`: - Uses `PyObject_AsWriteBuffer` to write data to a buffer if the object supports the single-segment character buffer interface. - Returns `0` on success. If it fails, it should properly handle and raise an appropriate exception. # Input and Output Formats - The method `read_char_buffer` should take any object supporting the single-segment character buffer interface as input and return a string. - The method `read_arbitrary_buffer` should take any object supporting the single-segment readable buffer interface as input and return a bytes object. - The method `is_readable_buffer` should take any object as input and return a Boolean. - The method `write_buffer` should take any object supporting the single-segment character buffer interface as input and return an integer. # Constraints and Limitations - The input objects should be compatible with the old buffer protocol as described. - Implement proper error handling to manage cases where input does not adhere to the expected buffer interface. - Assume that the buffer contents are manageable within the memory limits of a typical system. # Performance Requirements - Ensure that the methods handle buffer operations efficiently concerning time complexity. - Memory usage should be optimized to handle large buffers without significant performance degradation. # Example Usage ```python buffer_handler = BufferHandler() # Example object that supports the buffer protocol buffer_obj = memoryview(b\\"example buffer\\") # Reading character buffer char_buffer = buffer_handler.read_char_buffer(buffer_obj) print(char_buffer) # Output: example buffer # Reading arbitrary buffer arbitrary_buffer = buffer_handler.read_arbitrary_buffer(buffer_obj) print(arbitrary_buffer) # Output: b\'example buffer\' # Checking if object supports readable buffer interface is_readable = buffer_handler.is_readable_buffer(buffer_obj) print(is_readable) # Output: True # Writing to buffer (Assume obj supports writable buffer and method handles it properly) write_status = buffer_handler.write_buffer(buffer_obj) print(write_status) # Output: 0 ``` **Note**: Your implementation should not use the real `PyObject_*` functions as they are C API functions. Instead, simulate their behavior in Python for the purposes of this exercise.","solution":"class BufferHandler: def read_char_buffer(self, obj) -> str: Simulates reading data from an object supporting the single-segment character buffer interface. Returns the buffer content as a string. try: buffer = memoryview(obj).tobytes() return buffer.decode(\'utf-8\') except TypeError: raise ValueError(\\"Object does not support character buffer interface\\") def read_arbitrary_buffer(self, obj) -> bytes: Simulates reading data from an object supporting the single-segment readable buffer interface. Returns the buffer content as bytes. try: return memoryview(obj).tobytes() except TypeError: raise ValueError(\\"Object does not support readable buffer interface\\") def is_readable_buffer(self, obj) -> bool: Simulates checking if an object supports the single-segment readable buffer interface. Returns True if it supports; otherwise False. try: memoryview(obj).tobytes() return True except TypeError: return False def write_buffer(self, obj) -> int: Simulates writing data to a buffer if the object supports the single-segment character buffer interface. Returns 0 on success. If it fails, it raises an appropriate exception. try: buffer = memoryview(obj) writable_buffer = bytearray(buffer.tobytes()) writable_buffer[0] = 255 # Just an example of writing to buffer return 0 except TypeError: raise ValueError(\\"Object does not support writable buffer interface\\")"},{"question":"**Title:** Web Content Scraper with Error Handling **Objective:** Implement a function that fetches content from a list of URLs and handles common HTTP errors. **Problem Statement:** You are required to write a function `fetch_web_content(urls: List[str]) -> Dict[str, Union[str, Dict[str, Any]]]` that takes a list of URLs as input and returns a dictionary where the keys are the URLs and the values are either the content of the webpage (as a string) or another dictionary containing error details if the request failed. **Function Signature:** ```python from typing import List, Dict, Union def fetch_web_content(urls: List[str]) -> Dict[str, Union[str, Dict[str, Any]]]: pass ``` **Requirements:** 1. **Fetching URLs:** - Use `urllib.request` to fetch the content of each URL. - Store the content as a string in the resulting dictionary with the URL as the key. 2. **Error Handling:** - If a request results in an `HTTPError`, store a dictionary with the keys `code` and `message` containing the error code and error message respectively. - If a request results in a `URLError`, store a dictionary with a single key `reason` containing the error reason. - If any other exception occurs, store a dictionary with a single key `error` containing the exception message. 3. **Metadata:** - If the request is successful, in addition to the content, also store a dictionary with metadata under the key `info` containing the actual URL fetched (after redirections) and headers. **Example:** ```python urls = [ \\"http://example.com\\", \\"http://nonexistent.url\\", \\"http://httpbin.org/status/404\\" ] result = fetch_web_content(urls) Expected result: { \\"http://example.com\\": { \\"content\\": \\"<!doctype html><html>...</html>\\", \\"info\\": { \\"url\\": \\"http://example.com\\", \\"headers\\": { \\"Content-Type\\": \\"text/html\\", \\"Content-Length\\": \\"606\\" } } }, \\"http://nonexistent.url\\": { \\"error\\": \\"Name or service not known\\" }, \\"http://httpbin.org/status/404\\": { \\"code\\": 404, \\"message\\": \\"Not Found\\" } } ``` **Assumptions:** - The URLs provided in the list are valid URL strings. - Any HTTP redirections should be followed automatically. **Constraints:** - Performance should be considered for a large number of URLs. - The solution should handle common network exceptions and HTTP errors gracefully. **Notes:** - Make use of appropriate modules such as `urllib.request`, `urllib.error`, and possibly `http.client`. - Ensure robust error handling to deal with network errors, invalid URLs, server errors, etc. - This assessment tests knowledge of making HTTP requests, handling different types of errors, and processing HTTP response data in Python.","solution":"from typing import List, Dict, Union, Any import urllib.request import urllib.error def fetch_web_content(urls: List[str]) -> Dict[str, Union[str, Dict[str, Any]]]: result = {} for url in urls: try: with urllib.request.urlopen(url) as response: content = response.read().decode(\'utf-8\') info = { \'url\': response.geturl(), \'headers\': dict(response.getheaders()) } result[url] = { \'content\': content, \'info\': info } except urllib.error.HTTPError as e: result[url] = { \'code\': e.code, \'message\': e.reason } except urllib.error.URLError as e: result[url] = { \'reason\': e.reason } except Exception as e: result[url] = { \'error\': str(e) } return result"},{"question":"# Question Background You are working on a data visualization project and you need to generate multiple customized color palettes using seaborn\'s `hls_palette` function. Each palette should have its own unique properties in terms of the number of colors, lightness, saturation, and starting hue. Task Write a Python function `generate_palettes` that receives a list of dictionaries as input, where each dictionary contains parameters to customize an HLS palette. The function should return a list of generated seaborn color palettes. Function Signature ```python import seaborn as sns def generate_palettes(palettes_info: list[dict]) -> list: # your code here ``` Input * `palettes_info`: A list of dictionaries. Each dictionary contains the following keys: - `n_colors` (int): Number of colors in the palette. - `lightness` (float): Lightness of the colors, defaulting to 0.65 if not specified. - `saturation` (float): Saturation of the colors, defaulting to 0.65 if not specified. - `start_hue` (float): Starting point of the hue, defaulting to 0 if not specified. - `as_cmap` (bool): Whether to return a continuous colormap, defaulting to False if not specified. Example: ```python palettes_info = [ {\\"n_colors\\": 6}, {\\"n_colors\\": 8, \\"lightness\\": 0.3}, {\\"n_colors\\": 5, \\"saturation\\": 0.3}, {\\"n_colors\\": 7, \\"start_hue\\": 0.5}, {\\"n_colors\\": 10, \\"as_cmap\\": True} ] ``` Output * A list where each element is a seaborn color palette (either list of colors or continuous colormap based on `as_cmap` value). Example: ```python palettes_info = [ {\\"n_colors\\": 3, \\"lightness\\": 0.5, \\"saturation\\": 0.4, \\"start_hue\\": 0.1, \\"as_cmap\\": False}, {\\"n_colors\\": 5, \\"lightness\\": 0.7, \\"saturation\\": 0.3, \\"start_hue\\": 0.2, \\"as_cmap\\": False} ] output = generate_palettes(palettes_info) # Output should be a list of two seaborn color palettes based on the specifications provided ``` Constraints * `n_colors` will be an integer in range `[2, 100]`. * `lightness` and `saturation` should be floats in range `[0, 1]`. * `start_hue` should be a float in range `[0, 1]`. * The size of `palettes_info` list will not exceed 20. Use seaborn\'s `hls_palette()` function to achieve this. Ensure the function handles the defaults appropriately and returns the corresponding results.","solution":"import seaborn as sns def generate_palettes(palettes_info): Given a list of dictionaries, each specifying parameters for seaborn hls_palette, returns a list of corresponding seaborn color palettes. Parameters: palettes_info (list of dicts): List containing dictionaries with keys n_colors, lightness, saturation, start_hue, as_cmap. Returns: list: A list of seaborn color palettes. palettes = [] for info in palettes_info: n_colors = info.get(\'n_colors\') lightness = info.get(\'lightness\', 0.65) saturation = info.get(\'saturation\', 0.65) start_hue = info.get(\'start_hue\', 0) as_cmap = info.get(\'as_cmap\', False) palette = sns.hls_palette(n_colors, l=lightness, s=saturation, h=start_hue, as_cmap=as_cmap) palettes.append(palette) return palettes"},{"question":"# Multi-threaded File Merger Problem Statement You are given a task to merge multiple text files into a single text file. However, this merging process should be performed in a multi-threaded manner where each thread handles a specific portion of the work. The main thread should wait for all worker threads to finish before completing the merging process. Requirements 1. Implement a class `FileMerger` which initializes with a list of filenames to be merged. 2. The class should have a method `merge_files(output_file: str)` that merges the contents of the files into the specified `output_file`. 3. Use multi-threading where each thread reads the content of one file and writes its content to a shared `Queue` object. 4. The main thread should read from the `Queue` and write the combined content into the `output_file`. 5. Ensure proper synchronization to handle shared resources without causing race conditions. Input * A list of filenames that are to be merged. * The name of the output file. Output * A single output file containing the merged contents of the input files. Constraints * Assume each file is small enough to be read into memory. * Use Python\'s standard `queue.Queue` and `threading` modules. * Handle any potential exceptions related to file I/O. * Maintain the order of lines as they appear in the input files. Example Usage ```python from threading import Thread from queue import Queue class FileMerger: def __init__(self, filenames): self.filenames = filenames self.queue = Queue() def _worker(self, filename): try: with open(filename, \'r\') as f: data = f.read() self.queue.put(data) except Exception as e: self.queue.put(f\\"Error reading {filename}: {e}\\") def merge_files(self, output_file): threads = [] for filename in self.filenames: thread = Thread(target=self._worker, args=(filename,)) thread.start() threads.append(thread) for thread in threads: thread.join() try: with open(output_file, \'w\') as f: while not self.queue.empty(): data = self.queue.get() f.write(data + \\"n\\") except Exception as e: print(f\\"Error writing to {output_file}: {e}\\") # Example usage filenames = [\'file1.txt\', \'file2.txt\', \'file3.txt\'] merger = FileMerger(filenames) merger.merge_files(\'merged_output.txt\') ``` **Note:** The provided example usage is just for reference. Students are expected to come up with their implementation based on the problem requirements.","solution":"from threading import Thread from queue import Queue class FileMerger: def __init__(self, filenames): self.filenames = filenames self.queue = Queue() def _worker(self, filename): try: with open(filename, \'r\') as f: data = f.read() self.queue.put(data) except Exception as e: self.queue.put(f\\"Error reading {filename}: {e}\\") def merge_files(self, output_file): threads = [] for filename in self.filenames: thread = Thread(target=self._worker, args=(filename,)) thread.start() threads.append(thread) for thread in threads: thread.join() try: with open(output_file, \'w\') as f: while not self.queue.empty(): data = self.queue.get() f.write(data + \\"n\\") except Exception as e: print(f\\"Error writing to {output_file}: {e}\\") # Example usage filenames = [\'file1.txt\', \'file2.txt\', \'file3.txt\'] merger = FileMerger(filenames) merger.merge_files(\'merged_output.txt\')"},{"question":"# Enum-Based State Machine Implement a simple state machine using Python\'s `enum` module. The state machine should manage the states of an order processing system with the following states: - `PLACED`: The order has been placed. - `PROCESSING`: The order is being processed. - `SHIPPED`: The order has been shipped. - `DELIVERED`: The order has been delivered. - `CANCELED`: The order has been canceled. Your task is to: 1. Define an Enum class `OrderState` to represent these states. 2. Implement a class `Order` that: - Stores the current state of the order. - Has methods to transition between states: - `process()`: Change state from `PLACED` to `PROCESSING`. - `ship()`: Change state from `PROCESSING` to `SHIPPED`. - `deliver()`: Change state from `SHIPPED` to `DELIVERED`. - `cancel()`: Change state to `CANCELED` from any state except `DELIVERED` and `CANCELED`. - If an invalid transition is attempted, an appropriate exception should be raised. 3. Implement a `__str__` method for the `Order` class to return a string representation of the current order state. **Constraints:** - The state transitions must strictly follow the described flow. - Multiple state changes in one method call are not allowed. - Raise a `RuntimeError` if an invalid state transition is attempted. **Input and Output:** - No direct input/output functions are required. The methods will be tested programmatically. **Example:** ```python from enum import Enum class OrderState(Enum): PLACED = 1 PROCESSING = 2 SHIPPED = 3 DELIVERED = 4 CANCELED = 5 class Order: def __init__(self): self.state = OrderState.PLACED def process(self): if self.state == OrderState.PLACED: self.state = OrderState.PROCESSING else: raise RuntimeError(\\"Invalid state transition to PROCESSING\\") def ship(self): if self.state == OrderState.PROCESSING: self.state = OrderState.SHIPPED else: raise RuntimeError(\\"Invalid state transition to SHIPPED\\") def deliver(self): if self.state == OrderState.SHIPPED: self.state = OrderState.DELIVERED else: raise RuntimeError(\\"Invalid state transition to DELIVERED\\") def cancel(self): if self.state not in {OrderState.DELIVERED, OrderState.CANCELED}: self.state = OrderState.CANCELED else: raise RuntimeError(\\"Invalid state transition to CANCELED\\") def __str__(self): return f\\"Order is currently in {self.state.name} state\\" # Testing the implementation order = Order() print(order) # Order is currently in PLACED state order.process() print(order) # Order is currently in PROCESSING state order.ship() print(order) # Order is currently in SHIPPED state order.deliver() print(order) # Order is currently in DELIVERED state ``` Write your implementation for the `Order` class below:","solution":"from enum import Enum class OrderState(Enum): PLACED = 1 PROCESSING = 2 SHIPPED = 3 DELIVERED = 4 CANCELED = 5 class Order: def __init__(self): self.state = OrderState.PLACED def process(self): if self.state == OrderState.PLACED: self.state = OrderState.PROCESSING else: raise RuntimeError(\\"Invalid state transition to PROCESSING\\") def ship(self): if self.state == OrderState.PROCESSING: self.state = OrderState.SHIPPED else: raise RuntimeError(\\"Invalid state transition to SHIPPED\\") def deliver(self): if self.state == OrderState.SHIPPED: self.state = OrderState.DELIVERED else: raise RuntimeError(\\"Invalid state transition to DELIVERED\\") def cancel(self): if self.state in {OrderState.PLACED, OrderState.PROCESSING, OrderState.SHIPPED}: self.state = OrderState.CANCELED else: raise RuntimeError(\\"Invalid state transition to CANCELED\\") def __str__(self): return f\\"Order is currently in {self.state.name} state\\""},{"question":"You are tasked with implementing a Python function to log various messages with different severity levels and facilities using the syslog module. The function should be able to set logging configurations dynamically based on the input provided. Additionally, it should handle different facilities and priorities for logging messages. Function Definition ```python def dynamic_syslog_logger(log_config, messages): Function to dynamically log messages with different facilities and priorities. Args: log_config (dict): A dictionary with the following keys: - \'ident\': str, identifier to prepend to every log message (default: None) - \'logoption\': int, logoption flags combined using bitwise OR (default: 0) - \'facility\': int, facility constant (default: syslog.LOG_USER) messages (list of tuples): A list of tuples where each tuple contains: - priority: int, priority constant of the syslog module - message: str, log message to be sent Returns: None pass ``` Description 1. **log_config**: - `ident`: Identifier to prepend to every log message. If not provided, use default. - `logoption`: Log option flags combined using bitwise OR. If not provided, use default (0). - `facility`: Facility constants to use. If not provided, use default (`syslog.LOG_USER`). 2. **messages**: - List of tuples, where each tuple has `priority` and `message`. - `priority`: Priority constant (one of `syslog.LOG_EMERG`, `syslog.LOG_ALERT`, etc.) - `message`: The message to log. Requirements - If `log_config` is not provided or missing any key, use default values. - Open the syslog with provided configurations. - Log each message with its specified priority. - Close the syslog after logging all messages. Example ```python log_config = { \'ident\': \'myapp\', \'logoption\': syslog.LOG_PID | syslog.LOG_CONS, \'facility\': syslog.LOG_MAIL } messages = [ (syslog.LOG_INFO, \'Information message\'), (syslog.LOG_ERR, \'Error message\'), (syslog.LOG_DEBUG, \'Debugging message\') ] dynamic_syslog_logger(log_config, messages) ``` In this example, the function will log messages with the specified configuration. The identifier `myapp` will be prepended to each message, the log options include the process ID and console output, and the messages will be sent to the mail facility. # Constraints - The function should handle any missing configuration keys by using default values. - The logging should be performed efficiently and the syslog should be closed after all logging has been done. - Assume `syslog` module is already available and imported. # Performance - The implemented function should be able to handle a reasonably large number of log messages efficiently.","solution":"import syslog def dynamic_syslog_logger(log_config, messages): Function to dynamically log messages with different facilities and priorities. Args: log_config (dict): A dictionary with the following keys: - \'ident\': str, identifier to prepend to every log message (default: None) - \'logoption\': int, logoption flags combined using bitwise OR (default: 0) - \'facility\': int, facility constant (default: syslog.LOG_USER) messages (list of tuples): A list of tuples where each tuple contains: - priority: int, priority constant of the syslog module - message: str, log message to be sent Returns: None ident = log_config.get(\'ident\', None) logoption = log_config.get(\'logoption\', 0) facility = log_config.get(\'facility\', syslog.LOG_USER) syslog.openlog(ident, logoption, facility) for priority, message in messages: syslog.syslog(priority, message) syslog.closelog()"},{"question":"# Kernel Density Estimation Assessment **Objective:** Implement and apply Kernel Density Estimation (KDE) using scikit-learn to a given dataset. This will test your understanding of the KDE process, including parameter tuning and kernel selection. **Problem Statement:** Given a dataset of 2D points, your task is to: 1. Implement KDE using scikit-learn\'s `KernelDensity` estimator. 2. Compare the results using different kernels (Gaussian, Tophat, Epanechnikov). 3. Visualize the KDE results. 4. Explain the impact of bandwidth on the smoothness of the resulting distribution. **Dataset:** A numpy array `points` of shape `(n_samples, 2)` containing the 2D points will be provided. **Input Format:** - `points`: numpy array of shape `(n_samples, 2)` representing 2D points. **Output Format:** - A dictionary where: - Keys are kernel names (`\'gaussian\'`, `\'tophat\'`, `\'epanechnikov\'`). - Values are numpy arrays representing the KDE score for each point in the input dataset. - A plot showing the KDE results for the different kernels, with appropriate titles and labels. - Explanation of the impact of different bandwidth values on the smoothness of the KDE plot. **Constraints:** - Use `scikit-learn` and `matplotlib` for implementation and visualization. - Ensure the KDE implementation efficiently handles up to 10,000 points. **Performance Requirements:** - The KDE calculations should be performed efficiently, without excessive computational overhead. **Function Signature:** ```python def kde_density_estimation(points: np.ndarray) -> dict: pass ``` **Example Usage:** ```python import numpy as np import matplotlib.pyplot as plt from sklearn.neighbors import KernelDensity def kde_density_estimation(points): kernels = [\'gaussian\', \'tophat\', \'epanechnikov\'] kde_results = {} fig, axs = plt.subplots(1, 3, figsize=(15, 5)) for i, kernel in enumerate(kernels): kde = KernelDensity(kernel=kernel, bandwidth=0.5).fit(points) scores = kde.score_samples(points) kde_results[kernel] = scores axs[i].scatter(points[:, 0], points[:, 1], c=scores, cmap=\'viridis\') axs[i].set_title(f\'KDE with {kernel} kernel\') axs[i].set_xlabel(\'X-axis\') axs[i].set_ylabel(\'Y-axis\') plt.tight_layout() plt.show() return kde_results # Example data points = np.array([[1, 2], [2, 3], [3, 4], [4, 5], [5, 6], [6, 7], [7, 8], [8, 9], [9, 10], [10, 11]]) # Function call kde_results = kde_density_estimation(points) ``` **Explanation:** - Compare the KDE results for different kernels. - Note the differences in smoothness and behavior with different bandwidth values. - Provide a clear, concise explanation of the observations.","solution":"import numpy as np import matplotlib.pyplot as plt from sklearn.neighbors import KernelDensity def kde_density_estimation(points, bandwidth=0.5): Perform Kernel Density Estimation on a set of 2D points using different kernels. Args: - points (np.ndarray): A numpy array of shape (n_samples, 2) representing 2D points. - bandwidth (float): The bandwidth for KDE. Returns: - dict: A dictionary containing the KDE scores for each kernel. kernels = [\'gaussian\', \'tophat\', \'epanechnikov\'] kde_results = {} fig, axs = plt.subplots(1, 3, figsize=(15, 5)) for i, kernel in enumerate(kernels): kde = KernelDensity(kernel=kernel, bandwidth=bandwidth).fit(points) scores = kde.score_samples(points) kde_results[kernel] = scores # Visualization axs[i].scatter(points[:, 0], points[:, 1], c=scores, cmap=\'viridis\') axs[i].set_title(f\'KDE with {kernel} kernel\') axs[i].set_xlabel(\'X-axis\') axs[i].set_ylabel(\'Y-axis\') plt.tight_layout() plt.show() return kde_results # Explain the impact of bandwidth: # Bandwidth controls the smoothness of the estimated density. A smaller bandwidth can capture more details and variations in the data, resulting in a less smooth but more accurate representation of the data. A larger bandwidth tends to smooth out the density estimate, reducing noise but also potentially missing finer details."},{"question":"Objective You are given a dataset and required to use Seaborn\'s `objects` interface to create a series of plots demonstrating your understanding of controlling plot size and layout. Problem Statement 1. Load the `tips` dataset from Seaborn\'s sample datasets. 2. Create a single plot of the relationship between `total_bill` and `tip` with the figure size set to 6x6 inches. 3. Create a `facet` grid with `time` on the columns and `day` on the rows. Use the default layout engine. 4. Adjust the previous facet grid by using the `constrained` layout engine to potentially improve the layout. 5. Create another plot of `total_bill` versus `tip` but this time control the extent to [0.1, 0.1, 0.9, 0.9] relative to the figure. Input Format There is no input format. All operations should work directly on the `tips` dataset loaded from Seaborn. Output Format Output should be the series of plots as specified in the problem statement. Constraints - Use Seaborn\'s `objects` interface for all plots. - Ensure the figure and plot adjustments are accurately applied as described. Example Solution ```python import seaborn.objects as so import seaborn as sns import matplotlib.pyplot as plt # Load the tips dataset from Seaborn\'s sample datasets tips = sns.load_dataset(\'tips\') # 1. Create a single plot with size 6x6 p1 = so.Plot(tips, x=\\"total_bill\\", y=\\"tip\\").layout(size=(6, 6)) p1.show() # 2. Create a facet grid with time on columns and day on rows p2 = so.Plot(tips, x=\\"total_bill\\", y=\\"tip\\").facet(col=\\"time\\", row=\\"day\\") p2.show() # 3. Adjust the facet grid with the constrained layout engine p3 = so.Plot(tips, x=\\"total_bill\\", y=\\"tip\\").facet(col=\\"time\\", row=\\"day\\").layout(engine=\\"constrained\\") p3.show() # 4. Create a plot with the extent controlled p4 = so.Plot(tips, x=\\"total_bill\\", y=\\"tip\\").layout(extent=[0.1, 0.1, 0.9, 0.9]) p4.show() ```","solution":"import seaborn.objects as so import seaborn as sns import matplotlib.pyplot as plt def create_plots(): # Load the tips dataset from Seaborn\'s sample datasets tips = sns.load_dataset(\'tips\') # 1. Create a single plot with size 6x6 plot1 = so.Plot(tips, x=\\"total_bill\\", y=\\"tip\\").layout(size=(6, 6)) plot1.show() # 2. Create a facet grid with time on columns and day on rows plot2 = so.Plot(tips, x=\\"total_bill\\", y=\\"tip\\").facet(col=\\"time\\", row=\\"day\\") plot2.show() # 3. Adjust the facet grid with the constrained layout engine plot3 = so.Plot(tips, x=\\"total_bill\\", y=\\"tip\\").facet(col=\\"time\\", row=\\"day\\").layout(engine=\\"constrained\\") plot3.show() # 4. Create a plot with the extent controlled plot4 = so.Plot(tips, x=\\"total_bill\\", y=\\"tip\\").layout(extent=[0.1, 0.1, 0.9, 0.9]) plot4.show() # Uncomment the following line to execute the function and generate plots # create_plots()"},{"question":"# Advanced SQLite Database with Python `sqlite3` Module Objective In this exercise, you will demonstrate your understanding of the `sqlite3` module by creating and managing a SQLite database. You will perform tasks including database creation, data insertion, custom query execution, and implementing custom behaviors through collations and user-defined functions. Task 1. **Database Creation and Table Management:** - Create a new SQLite database named `school.db`. - Create the following tables within the database: - `students`: columns `id` (integer primary key), `name` (text), `age` (integer) - `courses`: columns `id` (integer primary key), `course_name` (text), `credit_hours` (integer) 2. **Data Insertion:** - Insert the following data into the `students` table: ``` id | name | age 1 | John Doe | 20 2 | Jane Smith | 22 3 | Alice Brown | 21 ``` - Insert the following data into the `courses` table: ``` id | course_name | credit_hours 1 | Introduction to Python | 3 2 | Data Structures | 4 3 | Computer Networks | 3 ``` 3. **Advanced Query:** - Write a Python function `get_student_details` that retrieves all student details sorted by age using a custom collation that considers \\"Elderly\\", \\"Young\\" instead of numeric age. 4. **User-defined Functions and Aggregates:** - Create a user-defined SQL function named `INITIALS` that takes a student\'s name as input and returns the initials (e.g., \\"John Doe\\" -> \\"JD\\"). - Create an aggregate function named `CourseCredits` that sums up the total credit hours a student is taking. Implementation Details Implement the following functions in your solution: 1. `create_database()`: - Creates the `school.db` SQLite database and the `students` and `courses` tables. 2. `insert_data()`: - Inserts the provided data into the `students` and `courses` tables. 3. `get_student_details()`: - Retrieves and returns student details sorted by age using custom collation. 4. `initials(name: str) -> str`: - Implements the user-defined SQL function `INITIALS`. 5. `CourseCredits`: - Implements the aggregate function `CourseCredits`. Example Usage ```python # Create the database and tables create_database() # Insert data into the tables insert_data() # Define and register the custom collation, user-defined function, and aggregate function # Execute the function to retrieve sorted student details students_sorted_by_age = get_student_details() # Example output of students_sorted_by_age: # [(2, \'Jane Smith\', 22), (3, \'Alice Brown\', 21), (1, \'John Doe\', 20)] for student in students_sorted_by_age: print(student) # Retrieve and print student initials using the user-defined function con = sqlite3.connect(\\"school.db\\") con.create_function(\\"INITIALS\\", 1, initials) cur = con.cursor() cur.execute(\\"SELECT id, name, INITIALS(name) as initials FROM students\\") for row in cur.fetchall(): print(row) # Retrieve and print total course credits using the aggregate function con.create_aggregate(\'CourseCredits\', 1, CourseCredits) cur.execute(\\"SELECT CourseCredits(credit_hours) FROM courses\\") total_credits = cur.fetchone()[0] print(f\\"Total course credits: {total_credits}\\") ``` Note: - Ensure all functions are properly implemented and tested. - Use placeholders for SQL queries to prevent SQL injection. - Provide appropriate error handling and close any database connections appropriately. Submission Submit a Python script that contains the implementations of the above functions. The script should be executable and demonstrate the required functionalities.","solution":"import sqlite3 def create_database(): Creates the \'school.db\' SQLite database and the required tables. con = sqlite3.connect(\\"school.db\\") cur = con.cursor() # Create students table cur.execute( CREATE TABLE IF NOT EXISTS students ( id INTEGER PRIMARY KEY, name TEXT, age INTEGER ) ) # Create courses table cur.execute( CREATE TABLE IF NOT EXISTS courses ( id INTEGER PRIMARY KEY, course_name TEXT, credit_hours INTEGER ) ) con.commit() con.close() def insert_data(): Inserts predefined data into the students and courses tables. con = sqlite3.connect(\\"school.db\\") cur = con.cursor() # Students data students = [ (1, \'John Doe\', 20), (2, \'Jane Smith\', 22), (3, \'Alice Brown\', 21) ] # Courses data courses = [ (1, \'Introduction to Python\', 3), (2, \'Data Structures\', 4), (3, \'Computer Networks\', 3) ] cur.executemany(\\"INSERT OR REPLACE INTO students (id, name, age) VALUES (?, ?, ?)\\", students) cur.executemany(\\"INSERT OR REPLACE INTO courses (id, course_name, credit_hours) VALUES (?, ?, ?)\\", courses) con.commit() con.close() def custom_age_collation(val1, val2): Custom collation function for sqlite3 which considers \\"Elderly\\" vs \\"Young\\". return (val1 - val2) def get_student_details(): Retrieves student details sorted by age using a custom collation. con = sqlite3.connect(\\"school.db\\") con.create_collation(\\"age_collation\\", custom_age_collation) cur = con.cursor() cur.execute(\\"SELECT * FROM students ORDER BY age COLLATE age_collation\\") results = cur.fetchall() con.close() return results def initials(name): User-defined function INITIALS which takes a name and returns the initials. return \'\'.join([part[0].upper() for part in name.split()]) class CourseCredits: Aggregate function to sum up the total credit hours. def __init__(self): self.total = 0 def step(self, value): self.total += value def finalize(self): return self.total"},{"question":"**Question: TorchInductor GPU Profiling and Optimization** In this exercise, you will demonstrate your understanding of profiling and benchmarking GPU performance of models using PyTorch\'s TorchInductor. # Part 1: Profiling a Model 1. **Profile the model** - Use the following environment variables: `TORCHINDUCTOR_UNIQUE_KERNEL_NAMES=1` and `TORCHINDUCTOR_BENCHMARK_KERNEL=1`. - Profile the `mixnet_l` model using the provided script: ```bash python -u benchmarks/dynamo/timm_models.py --backend inductor --amp --performance --dashboard --only mixnet_l --disable-cudagraphs --training ``` - Extract the path of the compiled module for the forward graph from the log output. 2. **Inspect the profile output** - Run the compiled module with the `-p` argument and examine the output. ```bash python <compiled_module_path>/fwd.py -p ``` - Identify the percentage time taken by pointwise, reduction, persistent reduction, and other kernel categories. # Part 2: Benchmarking an Individual Kernel 1. **Identify the most expensive kernel** - From the profiling output, identify the most time-consuming reduction kernel, for example, `triton_red_fused__native_batch_norm_legit_functional_16`. 2. **Benchmark the kernel** - Retrieve the path of the corresponding standalone Python module from comments in `fwd.py`. Rename this file as `k.py`. - Run the kernel benchmark script `k.py` and report its execution time and bandwidth. - Explore the effect of enabling max-autotune: ```bash TORCHINDUCTOR_MAX_AUTOTUNE=1 python <path_to_k.py>/k.py ``` # Requirements - Implement a Python script that accomplishes Part 1 and Part 2. - The script should automate the extraction of relevant paths and kernel names from the profiling outputs. - Interpret the results: - Describe the impact of each kernel category on the overall performance. - Compare execution times with and without autotuning for the selected kernel. # Input Format - No input parameters. The script should fetch and process necessary files based on environment and commands. # Output Format - Detailed profiling results for the forward graph. - Execution time and bandwidth for the identified kernel. - Comparison of kernel performance with and without autotuning. # Constraints - Ensure the scripts are executable in a typical PyTorch environment with TorchInductor installed. - Scripts should gracefully handle missing files or errors in profiling commands. By completing this task, you will showcase your ability to perform detailed GPU performance profiling and optimization using advanced PyTorch features.","solution":"import os import re import subprocess def profile_model(model_name=\'mixnet_l\'): env = os.environ.copy() env[\'TORCHINDUCTOR_UNIQUE_KERNEL_NAMES\'] = \'1\' env[\'TORCHINDUCTOR_BENCHMARK_KERNEL\'] = \'1\' command = [ \'python\', \'-u\', \'benchmarks/dynamo/timm_models.py\', \'--backend\', \'inductor\', \'--amp\', \'--performance\', \'--dashboard\', \'--only\', model_name, \'--disable-cudagraphs\', \'--training\' ] result = subprocess.run(command, env=env, capture_output=True, text=True) compiled_module_path = None for line in result.stdout.split(\'n\'): if \'compiled module path\' in line: compiled_module_path = line.split(\':\')[-1].strip() break if compiled_module_path: return compiled_module_path else: raise RuntimeError(\\"Failed to extract compiled module path\\") def inspect_profile_output(compiled_module_path): command = [\'python\', f\'{compiled_module_path}/fwd.py\', \'-p\'] result = subprocess.run(command, capture_output=True, text=True) profile_output = result.stdout categories = [\'pointwise\', \'reduction\', \'persistent reduction\', \'other\'] percentages = {category: 0.0 for category in categories} for line in profile_output.split(\'n\'): for category in categories: if category in line: match = re.search(r\'(([d.]+)%)\', line) if match: percentages[category] = float(match.group(1)) return percentages def benchmark_kernel(compiled_module_path, kernel_name): fwd_file_path = f\'{compiled_module_path}/fwd.py\' with open(fwd_file_path, \'r\') as f: for line in f: if kernel_name in line: match = re.search(r\'path_to_kernel: (.+)\', line) if match: kernel_path = match.group(1).strip() break if not kernel_path: raise RuntimeError(\\"Kernel path not found\\") kernel_script_path = os.path.dirname(kernel_path) max_autotune_env = os.environ.copy() max_autotune_env[\'TORCHINDUCTOR_MAX_AUTOTUNE\'] = \'1\' def run_benchmark(env=None): command = [\'python\', f\'{kernel_script_path}/k.py\'] result = subprocess.run(command, env=env, capture_output=True, text=True) time_match = re.search(r\'Execution time: ([d.]+) ms\', result.stdout) bandwidth_match = re.search(r\'Bandwidth: ([d.]+) GB/s\', result.stdout) execution_time = float(time_match.group(1)) if time_match else None bandwidth = float(bandwidth_match.group(1)) if bandwidth_match else None return execution_time, bandwidth exec_time_default, bandwidth_default = run_benchmark() exec_time_autotune, bandwidth_autotune = run_benchmark(env=max_autotune_env) return (exec_time_default, bandwidth_default, exec_time_autotune, bandwidth_autotune) def main(): # Profiling the model compiled_module_path = profile_model() # Inspecting profile output percentages = inspect_profile_output(compiled_module_path) print(\\"Kernel category percentages:\\", percentages) # Identifying the most expensive kernel: replace this with actual kernel name identified. kernel_name = \\"triton_red_fused__native_batch_norm_legit_functional_16\\" # Benchmarking the kernel exec_time_default, bandwidth_default, exec_time_autotune, bandwidth_autotune = benchmark_kernel(compiled_module_path, kernel_name) print(f\\"Execution time (default): {exec_time_default} ms, Bandwidth (default): {bandwidth_default} GB/s\\") print(f\\"Execution time (autotune): {exec_time_autotune} ms, Bandwidth (autotune): {bandwidth_autotune} GB/s\\") if __name__ == \\"__main__\\": main()"},{"question":"# Data Analysis with Pandas Objective: You are given a dataset that contains information about sales transactions. Each transaction includes data about the product sold, its price, quantity, and the date of the transaction. Your task is to perform various data manipulations and analyses to extract meaningful insights from the data using pandas. Dataset: The dataset `sales_data.csv` includes the following columns: - `transaction_id`: Unique identifier for each transaction. - `product`: Name of the product sold. - `price`: Price at which the product was sold. - `quantity`: Quantity of the product sold. - `date`: The date of the transaction in the format `YYYY-MM-DD`. Requirements: 1. **Read the Data**: Load the dataset into a pandas `DataFrame` from the CSV file. 2. **Data Cleaning and Preparation**: - Check and report any missing or null values in the dataset. - If any missing values are present, fill them appropriately or drop rows/columns where necessary. - Convert the `date` column to `datetime` type. 3. **Data Aggregation**: - Calculate the total revenue generated for each product. Revenue is calculated as the product of `price` and `quantity`. - Determine the top 5 products generating the highest revenue. 4. **Descriptive Statistics**: - Provide summary statistics (mean, median, standard deviation) for the `price` and `quantity` columns. - Find the product with the highest quantity sold in a single transaction. 5. **Time Series Analysis**: - Resample the data to calculate monthly total revenue. 6. **Output**: - Display the top 5 products by revenue. - Display summary statistics for `price` and `quantity`. - Display the product with the highest quantity sold in a single transaction. - Display the monthly total revenue. Function Signature: ```python import pandas as pd def analyze_sales_data(file_path: str): Analyze sales data from a CSV file and perform various operations to extract insights. Parameters: file_path (str): The path to the CSV file containing sales data. Returns: None # Step 1: Read the data df = pd.read_csv(file_path) # Step 2: Data Cleaning and Preparation # Handle missing values and convert date column # Your code here # Step 3: Data Aggregation # Calculate total revenue for each product # Your code here # Step 4: Descriptive Statistics # Summary statistics for price and quantity # Your code here # Provide the product with the highest quantity sold in a single transaction # Your code here # Step 5: Time Series Analysis # Resample the data to calculate monthly total revenue # Your code here # Step 6: Output the results # Your code here # Example usage analyze_sales_data(\'sales_data.csv\') ``` Constraints: - You must use the pandas library for all data manipulations. - Ensure that your code handles edge cases gracefully, such as empty datasets or datasets with no sales. Submission: Submit your implementation of the `analyze_sales_data` function. Your function will be tested with different datasets to evaluate the correctness and performance of your code.","solution":"import pandas as pd def analyze_sales_data(file_path: str): Analyze sales data from a CSV file and perform various operations to extract insights. Parameters: file_path (str): The path to the CSV file containing sales data. Returns: None # Step 1: Read the data df = pd.read_csv(file_path) # Step 2: Data Cleaning and Preparation # Check for missing values missing_values_report = df.isnull().sum() print(\\"Missing values report:n\\", missing_values_report) # Fill or drop missing values as necessary # In this example, we\'ll simply drop rows with any missing values for simplicity df = df.dropna() # Convert \'date\' column to datetime type df[\'date\'] = pd.to_datetime(df[\'date\']) # Step 3: Data Aggregation # Calculate total revenue for each product df[\'revenue\'] = df[\'price\'] * df[\'quantity\'] product_revenue = df.groupby(\'product\')[\'revenue\'].sum() # Determine the top 5 products generating the highest revenue top_5_products = product_revenue.sort_values(ascending=False).head(5) # Step 4: Descriptive Statistics # Summary statistics for price and quantity summary_statistics_price = df[\'price\'].describe() summary_statistics_quantity = df[\'quantity\'].describe() # Find the product with the highest quantity sold in a single transaction highest_quantity_transaction = df.loc[df[\'quantity\'].idxmax()] # Step 5: Time Series Analysis # Resample the data to calculate monthly total revenue monthly_revenue = df.resample(\'M\', on=\'date\')[\'revenue\'].sum() # Step 6: Output the results print(\\"nTop 5 products by revenue:n\\", top_5_products) print(\\"nSummary Statistics for Price:n\\", summary_statistics_price) print(\\"nSummary Statistics for Quantity:n\\", summary_statistics_quantity) print(\\"nProduct with the highest quantity sold in a single transaction:n\\", highest_quantity_transaction) print(\\"nMonthly total revenue:n\\", monthly_revenue) # Example usage: # analyze_sales_data(\'sales_data.csv\')"},{"question":"**Question: Implement Spectral Co-Clustering and Evaluate Consistency** *Objective:* In this task, you are required to implement the Spectral Co-Clustering algorithm on a given dataset. Your implementation should read a matrix of data, perform biclustering, and then evaluate the consistency of the biclusters found. **Scenario:** You are given a ( m times n ) matrix `data` where rows and columns should be partitioned into biclusters such that the entries in each bicluster have higher values than the average of corresponding rows and columns. After biclustering, your task is to evaluate the consistency of the found biclusters using the Jaccard index. **Requirements:** 1. Implement the Spectral Co-Clustering algorithm. 2. Normalize the data matrix as described. 3. Perform singular value decomposition to obtain necessary partitions. 4. Use k-means to cluster rows and columns. 5. For evaluation, compute the Jaccard index for a sample bicluster. 6. Output the final list of biclusters along with their Jaccard indices. **Input:** * `data`: A 2D numpy array of shape ( m times n ) * `num_clusters`: An integer indicating the number of biclusters to find. * `true_biclusters`: A dictionary containing \\"rows\\" and \\"columns\\" with ground truth row and column indices for the biclusters for evaluation. **Output:** * `biclusters`: A dictionary with found row and column indices for each bicluster. * `jaccard_indices`: A list of computed Jaccard indices for each found bicluster compared to the true biclusters. **Constraints:** * The input matrix `data` will have at least one valid bicluster. **Performance Requirements:** * The implementation should be efficient enough to handle matrices up to 1000x1000. ```python import numpy as np from sklearn.cluster import KMeans from scipy.optimize import linear_sum_assignment from sklearn.metrics import jaccard_score def spectral_coclustering(data, num_clusters): # Your implementation here to setup and find biclusters using Spectral Co-Clustering pass def evaluate_biclusters(found_biclusters, true_biclusters): # Your implementation here to evaluate consistency using the Jaccard index pass # Example usage: data = np.random.rand(100, 100) # Example data input num_clusters = 5 # Specify the number of biclusters true_biclusters = {\'rows\': [array_of_true_row_indices], \'columns\': [array_of_true_column_indices]} # Example true bicluster indices biclusters = spectral_coclustering(data, num_clusters) jaccard_indices = evaluate_biclusters(biclusters, true_biclusters) print(biclusters) print(jaccard_indices) ``` **Notes:** 1. You can use any standard library functions for matrix operations, SVD, or clustering (e.g., from numpy, sklearn). 2. Make your code modular and well-commented to explain each major step. 3. Ensure your code handles edge cases, such as when the input matrix is not well-conditioned.","solution":"import numpy as np from sklearn.cluster import KMeans from sklearn.utils.extmath import randomized_svd from sklearn.preprocessing import normalize from sklearn.metrics import jaccard_score from scipy.optimize import linear_sum_assignment def spectral_coclustering(data, num_clusters): Perform Spectral Co-Clustering algorithm on the given data. Args: - data (numpy.ndarray): The input data matrix of shape (m, n) - num_clusters (int): The number of clusters to find Returns: - biclusters (dict): A dictionary containing row and column indices for each bicluster # Step 1: Normalize the data data_normalized = normalize(data, norm=\'l1\', axis=1) # Step 2: Perform SVD U, Sigma, VT = randomized_svd(data_normalized, n_components=num_clusters) # Step 3: Use k-means to cluster rows and columns separately kmeans_rows = KMeans(n_clusters=num_clusters).fit(U) kmeans_cols = KMeans(n_clusters=num_clusters).fit(VT.T) # Step 4: Extract the biclusters row_labels = kmeans_rows.labels_ col_labels = kmeans_cols.labels_ biclusters = { \'rows\': [np.where(row_labels == i)[0] for i in range(num_clusters)], \'columns\': [np.where(col_labels == i)[0] for i in range(num_clusters)] } return biclusters def evaluate_biclusters(found_biclusters, true_biclusters): Evaluate the found biclusters using the Jaccard index. Args: - found_biclusters (dict): A dictionary containing found row and column indices for each bicluster - true_biclusters (dict): A dictionary containing true row and column indices for each bicluster Returns: - jaccard_indices (list): A list of Jaccard indices for each bicluster def jaccard(set1, set2): intersection = len(set1.intersection(set2)) union = len(set1.union(set2)) return intersection / union if union != 0 else 0 jaccard_indices = [] num_clusters = len(true_biclusters[\'rows\']) for i in range(num_clusters): true_rows = set(true_biclusters[\'rows\'][i]) true_columns = set(true_biclusters[\'columns\'][i]) found_rows = set(found_biclusters[\'rows\'][i]) found_columns = set(found_biclusters[\'columns\'][i]) jaccard_row_index = jaccard(found_rows, true_rows) jaccard_column_index = jaccard(found_columns, true_columns) jaccard_indices.append((jaccard_row_index + jaccard_column_index) / 2) return jaccard_indices"},{"question":"**Objective:** Implement a function that takes the path of a Python script and outputs a report of the modules imported by the script, including the most used module and any missing modules. **Function Signature:** ```python def analyze_script(pathname: str) -> None: pass ``` **Input:** - `pathname` (str): The file path of the Python script to analyze. **Output:** The function should print: 1. The list of all imported modules and their paths. 2. The most used module (the module with the highest number of top-level functions/variables). 3. A list of modules that are missing. **Constraints:** - You must use the `modulefinder` module to analyze the script. - Handle the cases where the script file does not exist or cannot be read. - You can assume that the script will only import standard library modules and user-defined modules. **Example:** Suppose you have a Python script named `example_script.py` with the following content: ```python import os import sys import random try: import non_existing_module except ImportError: pass def foo(): import math return math.sqrt(4) print(os.name) ``` Your function call would be: ```python analyze_script(\'example_script.py\') ``` # Expected Output: ```plaintext Loaded modules: os: some_file_path sys: some_file_path random: some_file_path math: some_file_path __main__: some_file_path Most used module: os Modules not imported: non_existing_module ``` **Notes:** - Replace `some_file_path` with the actual file paths of the imported modules as determined by the `ModuleFinder`. - The \\"most used module\\" is determined by the count of top-level functions/variables in the module, excluding the `__main__` module. # Implementation: Use the following template for implementation: ```python import os from modulefinder import ModuleFinder def analyze_script(pathname: str) -> None: try: finder = ModuleFinder() finder.run_script(pathname) print(\'Loaded modules:\') for name, mod in finder.modules.items(): print(f\'{name}: {mod.__file__}\') # Determine the most used module most_used = None max_usage = 0 for name, mod in finder.modules.items(): if name != \'__main__\': usage_count = len(mod.globalnames) if usage_count > max_usage: most_used = name max_usage = usage_count print(\'nMost used module:\', most_used) print(\'nModules not imported:\') print(\'n\'.join(finder.badmodules.keys())) except FileNotFoundError: print(f\'The file {pathname} does not exist.\') except Exception as e: print(f\'An error occurred: {e}\') ```","solution":"import os from modulefinder import ModuleFinder def analyze_script(pathname: str) -> None: try: finder = ModuleFinder() finder.run_script(pathname) print(\'Loaded modules:\') for name, mod in finder.modules.items(): path = mod.__file__ if mod.__file__ else \'built-in or not found\' print(f\'{name}: {path}\') # Determine the most used module most_used = None max_usage = 0 for name, mod in finder.modules.items(): if name != \'__main__\': usage_count = len(mod.globalnames) if usage_count > max_usage: most_used = name max_usage = usage_count print(\'nMost used module:\', most_used or \\"None\\") print(\'nModules not imported:\') for name in finder.badmodules.keys(): print(name) except FileNotFoundError: print(f\'The file {pathname} does not exist.\') except Exception as e: print(f\'An error occurred: {e}\')"},{"question":"Challenge: Create a Non-Blocking TCP Server # Objective Create a non-blocking TCP server in Python that accepts multiple client connections concurrently, receives data from them, and sends a response back without blocking operations. This task will help assess your understanding of socket programming, handling multiple connections, and non-blocking I/O in Python. # Task Implement a TCP server that meets the following requirements: 1. **Server Initialization**: The server initializes a TCP socket and binds to a specified host and port. 2. **Non-Blocking Mode**: The server operates in non-blocking mode, meaning it does not block on accept or receive operations. 3. **Multiple Clients**: The server can handle multiple client connections concurrently. 4. **Receive and Respond**: The server receives a message from a client, processes it, and sends a \\"Received: MESSAGE\\" response back to the client. # Function Signature ```python def start_non_blocking_server(host: str, port: int): # Your implementation here ``` # Input - `host` (str): The server\'s host address (e.g., \'localhost\' or \'0.0.0.0\'). - `port` (int): The port number the server will listen on. # Expected Behavior - The server should be able to handle multiple clients concurrently without blocking on any single connection. - Each client sends a message, and the server responds with \\"Received: MESSAGE\\". - The server should continue running indefinitely until manually stopped (by keyboard interrupt or other means). # Constraints - Use non-blocking mode for the socket operations. - Implement appropriate exception handling to manage non-blocking behavior. - No external libraries (e.g., `select`, `asyncio`) are allowed; only use the `socket` module. # Example Usage ```python # Run the server on localhost, port 12345 start_non_blocking_server(\'localhost\', 12345) # Client 1 connects and sends a message # Client 2 connects and sends a message ``` Use the following template to get started: ```python import socket import select import errno def start_non_blocking_server(host: str, port: int): server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM) server_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1) server_socket.bind((host, port)) server_socket.listen() server_socket.setblocking(False) print(f\\"Server started on {host}:{port} (non-blocking mode)\\") sockets_list = [server_socket] clients = {} def receive_message(client_socket): try: message = client_socket.recv(1024) if not message: return False return message.decode(\'utf-8\') except: return False while True: try: read_sockets, _, exception_sockets = select.select(sockets_list, [], sockets_list) for notified_socket in read_sockets: if notified_socket == server_socket: client_socket, client_address = server_socket.accept() client_socket.setblocking(False) sockets_list.append(client_socket) clients[client_socket] = client_address print(f\\"Accepted new connection from {client_address[0]}:{client_address[1]}\\") else: message = receive_message(notified_socket) if message is False: print(f\\"Closed connection from {clients[notified_socket][0]}:{clients[notified_socket][1]}\\") sockets_list.remove(notified_socket) del clients[notified_socket] else: print(f\\"Received message from {clients[notified_socket][0]}:{clients[notified_socket][1]}: {message}\\") notified_socket.send(f\\"Received: {message}\\".encode(\'utf-8\')) for notified_socket in exception_sockets: sockets_list.remove(notified_socket) del clients[notified_socket] except KeyboardInterrupt: server_socket.close() print(\\"nServer closed\\") break ``` Test your server by connecting with multiple clients using socket programs or tools like `telnet`. # Notes - Ensure proper cleanup and shutdown of sockets to prevent resource leaks. - Handle typical socket errors, such as `errno.EWOULDBLOCK` and `errno.EAGAIN`, appropriately.","solution":"import socket import errno def start_non_blocking_server(host: str, port: int): server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM) server_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1) server_socket.bind((host, port)) server_socket.listen(5) server_socket.setblocking(False) print(f\\"Server started on {host}:{port} (non-blocking mode)\\") clients = [] try: while True: try: client_socket, address = server_socket.accept() client_socket.setblocking(False) clients.append(client_socket) print(f\\"Accepted new connection from {address}\\") except socket.error as e: if e.errno != errno.EWOULDBLOCK: raise e for client in clients[:]: try: message = client.recv(1024) if message: decoded_message = message.decode(\'utf-8\') print(f\\"Received message: {decoded_message}\\") client.sendall(f\\"Received: {decoded_message}\\".encode(\'utf-8\')) else: print(\\"Closing connection\\") client.close() clients.remove(client) except socket.error as e: if e.errno not in [errno.EWOULDBLOCK, errno.EAGAIN]: print(\\"Socket error: \\", e) client.close() clients.remove(client) except KeyboardInterrupt: print(\\"Server shutting down\\") finally: for client in clients: client.close() server_socket.close()"},{"question":"**Compare the Performance of Different Fibonacci Sequence Implementations** **Objective:** Write a Python script that uses the `timeit` module to measure and compare the performance of three different implementations of a function to compute the nth Fibonacci number. The three implementations should be: 1. **Iterative Approach:** Simply use a loop to compute the Fibonacci sequence. 2. **Recursive Approach without Memoization:** Use basic recursion to compute the value. 3. **Recursive Approach with Memoization:** Use recursion with a memoization technique to speed up the computation. **Function Signatures:** ```python def fib_iterative(n: int) -> int: # Your code here def fib_recursive(n: int) -> int: # Your code here def fib_memoized(n: int) -> int: # Your code here ``` **Input Constraints:** - (1 leq n leq 30) **Output Specification:** - Measure and report the time taken by each method using the `timeit` module. - Print the results in a clear and structured format, indicating which method is the fastest for the given value of `n`. **Requirements:** 1. Define the three functions as given. 2. Write a setup code that imports these functions and any necessary modules. 3. Use the `timeit.timeit` function to measure the execution time for each implementation for the same value of `n`. 4. Print the timing results in a human-readable format. Example Usage: ```python # Assuming n = 20 fib_iterative_time = timeit.timeit(\'fib_iterative(20)\', setup=\'from __main__ import fib_iterative\', number=1000) fib_recursive_time = timeit.timeit(\'fib_recursive(20)\', setup=\'from __main__ import fib_recursive\', number=1000) fib_memoized_time = timeit.timeit(\'fib_memoized(20)\', setup=\'from __main__ import fib_memoized\', number=1000) print(f\\"Iterative approach: {fib_iterative_time:.5f} seconds\\") print(f\\"Recursive approach: {fib_recursive_time:.5f} seconds\\") print(f\\"Memoized approach: {fib_memoized_time:.5f} seconds\\") ``` **Additional Notes:** - Ensure to explain any assumptions or design choices in comments. - Consider using `global` or enclosing variables where necessary to ensure correct functioning of the `timeit` setups. - You should focus on accuracy and readability of timing data.","solution":"def fib_iterative(n: int) -> int: Calculate the nth Fibonacci number using an iterative approach. if n <= 0: raise ValueError(\\"Input should be a positive integer\\") if n == 1: return 0 if n == 2: return 1 a, b = 0, 1 for _ in range(2, n): a, b = b, a + b return b def fib_recursive(n: int) -> int: Calculate the nth Fibonacci number using a basic recursive approach. if n <= 0: raise ValueError(\\"Input should be a positive integer\\") if n == 1: return 0 if n == 2: return 1 return fib_recursive(n - 1) + fib_recursive(n - 2) def fib_memoized(n: int, memo={1: 0, 2: 1}) -> int: Calculate the nth Fibonacci number using a recursive approach with memoization. if n <= 0: raise ValueError(\\"Input should be a positive integer\\") if n not in memo: memo[n] = fib_memoized(n - 1, memo) + fib_memoized(n - 2, memo) return memo[n] if __name__ == \\"__main__\\": import timeit n = 20 setup_code = \'from __main__ import fib_iterative, fib_recursive, fib_memoized\' iterative_time = timeit.timeit(f\'fib_iterative({n})\', setup=setup_code, number=1000) recursive_time = timeit.timeit(f\'fib_recursive({n})\', setup=setup_code, number=1000) memoized_time = timeit.timeit(f\'fib_memoized({n})\', setup=setup_code, number=1000) print(f\\"Iterative approach: {iterative_time:.5f} seconds\\") print(f\\"Recursive approach: {recursive_time:.5f} seconds\\") print(f\\"Memoized approach: {memoized_time:.5f} seconds\\")"},{"question":"Your task is to implement a function that takes an HTML string and formats it into a safe HTML text for display, then reverts the transformation. Specifically, you need to write two functions: `safe_html_display` and `original_html`. 1. `safe_html_display(html_text: str, escape_quotes: bool) -> str`: - **Input**: A string `html_text` that may contain HTML special characters like `&`, `<`, `>`, `\\"`, and `\'`. A boolean `escape_quotes` indicating whether or not to escape the quotation marks (`\\"` and `\'`). - **Output**: A string where the HTML special characters are replaced with their corresponding HTML-safe sequences, with optional quotation mark escaping if specified. 2. `original_html(safe_html_text: str) -> str`: - **Input**: A string `safe_html_text` that contains HTML-safe sequences like `&amp;`, `&lt;`, `&gt;`, `&quot;`, and `&#x27;`. - **Output**: A string where all HTML-safe sequences have been converted back to their original characters. # Constraints: - The functions should handle cases where `html_text` is empty. - The functions should be robust enough to handle a mix of characters and HTML entities. # Example: ```python # Example 1 html_text = \'Hello & welcome, \\"<user>\\"\' escape_quotes = True safe_html = safe_html_display(html_text, escape_quotes) # Output: \'Hello &amp; welcome, &quot;&lt;user&gt;&quot;\' assert safe_html == \'Hello &amp; welcome, &quot;&lt;user&gt;&quot;\' # Reverting back original_html_text = original_html(safe_html) # Output: \'Hello & welcome, \\"<user>\\"\' assert original_html_text == \'Hello & welcome, \\"<user>\\"\' # Example 2 html_text = \'5 > 3 & 3 < 5\' escape_quotes = False safe_html = safe_html_display(html_text, escape_quotes) # Output: \'5 &gt; 3 &amp; 3 &lt; 5\' assert safe_html == \'5 &gt; 3 &amp; 3 &lt; 5\' # Reverting back original_html_text = original_html(safe_html) # Output: \'5 > 3 & 3 < 5\' assert original_html_text == \'5 > 3 & 3 < 5\' ``` # Implementation Requirements: 1. Utilize the `html.escape` and `html.unescape` functions provided by the `html` module. 2. Ensure that the functions are optimized and handle edge cases succinctly.","solution":"import html def safe_html_display(html_text: str, escape_quotes: bool) -> str: Returns the HTML-safe string where the HTML special characters are replaced with their corresponding HTML-safe sequences. Escapes quotes if escape_quotes is True. if escape_quotes: return html.escape(html_text, quote=True) else: return html.escape(html_text, quote=False) def original_html(safe_html_text: str) -> str: Returns the original HTML text by converting back all HTML-safe sequences to their original characters. return html.unescape(safe_html_text)"},{"question":"# Problem Description You are required to implement a function that processes a list of numbers to calculate the square of each number. This processing should be done using multiple threads to optimize performance. Each thread should process a subset of the list independently, ensuring thread synchronization to avoid any potential conflicts. # Function Signature ```python def threaded_square_calculator(numbers: list, thread_count: int) -> list: pass ``` # Input - `numbers` (list): A list of integers to be processed. - `thread_count` (int): The number of threads to be used for parallel processing. # Output - Returns a list of squared integers, maintaining the order of the original list. # Constraints 1. `1 <= len(numbers) <= 10^6` 2. `1 <= thread_count <= 100` 3. Each thread must process an approximately equal subset of the list. # Requirements 1. Use the `_thread` module to handle threading. 2. Utilize locks from the `_thread` module to ensure thread synchronization. 3. Ensure that the square of each number is calculated correctly and placed in the correct position in the result list. 4. Handle edge cases, such as the `numbers` list being empty or `thread_count` being 1. # Example ```python from _thread import allocate_lock, start_new_thread def threaded_square_calculator(numbers, thread_count): # Function to be implemented pass # Example usage numbers = [1, 2, 3, 4, 5] thread_count = 2 print(threaded_square_calculator(numbers, thread_count)) # Output: [1, 4, 9, 16, 25] ``` # Explanation In the above example, the function processes the list `[1, 2, 3, 4, 5]` using 2 threads. Each thread calculates the square of its subset of numbers, and the resulting list of squared numbers `[1, 4, 9, 16, 25]` is returned. # Notes - Ensure to handle synchronization using `_thread.LockType`. - Use `_thread.start_new_thread(function, args)` to start new threads. - Make sure each thread correctly processes its designated subset of the list and updates the shared result synchronously.","solution":"from _thread import allocate_lock, start_new_thread def threaded_square_calculator(numbers, thread_count): Calculate the square of each number in the list using multiple threads. Args: - numbers (list): List of integers to be squared. - thread_count (int): Number of threads to use. Returns: - List of squared integers. # Handle edge cases directly if not numbers: return [] if thread_count == 1: return [n * n for n in numbers] result = [None] * len(numbers) lock = allocate_lock() def worker(start_index, end_index): nonlocal result for i in range(start_index, end_index): sq = numbers[i] * numbers[i] with lock: result[i] = sq block_size = (len(numbers) + thread_count - 1) // thread_count # Ceiling division threads = [] for i in range(thread_count): start_index = i * block_size end_index = min((i + 1) * block_size, len(numbers)) t = start_new_thread(worker, (start_index, end_index)) threads.append(t) # Wait for all threads to complete (simple implementation - could use a more sophisticated approach) while None in result: pass return result"},{"question":"**Objective:** Assess your understanding and implementation skills of the Seaborn library, specifically focusing on the `pointplot` function for visualizing grouped data with different customizations. **Problem Statement:** You are given a dataset of penguins and flight passengers, both of which are common datasets accessible through the Seaborn library. Your task is to generate and customize point plots for these datasets as per the provided specifications. **Instructions:** 1. **Load the datasets:** - Import the Seaborn library and load the \\"penguins\\" and \\"flights\\" datasets using `sns.load_dataset()` function. 2. **Task 1: Basic Point Plot** - Create a point plot for the \\"penguins\\" dataset to show the average body mass of penguins for each island. 3. **Task 2: Grouped Point Plot** - Modify the point plot created in Task 1 to further distinguish the data by the sex of the penguins using different colors (`hue`). 4. **Task 3: Error Bars and Customization** - Adjust the point plot such that the error bars represent the standard deviation of each group. - Customize the markers and linestyles to be: `markers=[\\"o\\", \\"s\\"]` and `linestyles=[\\"-\\", \\"--\\"]`. 5. **Task 4: Multi-layered Plot with Dodge** - Create a combination of `stripplot` and `pointplot` for the \\"penguins\\" dataset to display bill depth measurements. - Ensure that the plot handles overlapping points using `dodge=0.4`. 6. **Task 5: Aggregated Wide-format Plot** - Pivot the \\"flights\\" dataset to create a wide-format DataFrame with years as the index, months as columns, and passenger counts as values. - Generate a point plot for aggregated data over the years. 7. **Additional Customization:** - For the plot created in Task 4, add a custom tick label formatter to display year values with a two-digit format. **Submission Format:** You need to submit a Jupyter Notebook (.ipynb file) containing: - The Python code for the tasks described above. - Proper explanations and comments for each step to show your understanding of the Seaborn functionalities. **Expected Input and Output:** - Input: The datasets (loaded via Seaborn), and a series of point plots as instructed. - Output: The respective point plots with customizations as specified. **Constraints:** - You must use Seaborn\'s point plotting and complimentary functions (`sns.pointplot` and `sns.stripplot`). - Make sure to handle edge cases like missing data sensibly. - Ensure the plots are properly labeled and include legends where applicable. **Performance Requirements:** - The solution should be efficient and leverage Seaborn\'s built-in functionalities without unnecessary computations. **Example Code Initialization:** ```python import seaborn as sns import pandas as pd # Load datasets penguins = sns.load_dataset(\\"penguins\\") flights = sns.load_dataset(\\"flights\\") # Example Task: Basic Point Plot sns.pointplot(data=penguins, x=\\"island\\", y=\\"body_mass_g\\") ``` Utilize this information to complete the tasks and deliver clear, accurate, and well-documented point plots.","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt # Load datasets penguins = sns.load_dataset(\\"penguins\\") flights = sns.load_dataset(\\"flights\\") # Task 1: Basic Point Plot def plot_penguins_basic(): sns.pointplot(data=penguins, x=\\"island\\", y=\\"body_mass_g\\") plt.title(\'Average Body Mass of Penguins by Island\') plt.show() # Task 2: Grouped Point Plot def plot_penguins_grouped(): sns.pointplot(data=penguins, x=\\"island\\", y=\\"body_mass_g\\", hue=\\"sex\\", markers=[\\"o\\", \\"s\\"], linestyles=[\\"-\\", \\"--\\"]) plt.title(\'Average Body Mass of Penguins by Island and Sex\') plt.show() # Task 3: Error Bars and Customization def plot_penguins_custom_errorbars(): sns.pointplot(data=penguins, x=\\"island\\", y=\\"body_mass_g\\", hue=\\"sex\\", markers=[\\"o\\", \\"s\\"], linestyles=[\\"-\\", \\"--\\"], ci=\\"sd\\") plt.title(\'Average Body Mass of Penguins by Island and Sex with Std Dev Error Bars\') plt.show() # Task 4: Multi-layered Plot with Dodge def plot_penguins_multilayered(): sns.stripplot(data=penguins, x=\\"island\\", y=\\"bill_depth_mm\\", hue=\\"sex\\", dodge=0.4, jitter=True) sns.pointplot(data=penguins, x=\\"island\\", y=\\"bill_depth_mm\\", hue=\\"sex\\", dodge=0.4, join=False, ci=None, markers=[\\"o\\", \\"s\\"], linestyles=[\\"-\\", \\"--\\"]) plt.title(\'Bill Depth Measurements of Penguins by Island and Sex\') plt.show() # Task 5: Aggregated Wide-format Plot def plot_flights_aggregated(): flights_pivot = flights.pivot(index=\'year\', columns=\'month\', values=\'passengers\') sns.pointplot(data=flights_pivot.agg(\'mean\', axis=1).reset_index(), x=\'year\', y=0) plt.title(\'Average Number of Flights Per Year\') plt.show()"},{"question":"# **Coding Assessment Question** **Objective** You are tasked with implementing a Python function that demonstrates advanced usage of instance method objects and regular method objects, as specified in the Python C API. **Requirements** 1. You need to define two classes: - `InstanceMethodClass`: This class should contain at least one method which you will convert into an instance method object. - `MethodClass`: This class should contain at least one method which you will convert into a regular method object. 2. Implement a function `create_instance_method` that: - Takes a callable Python function. - Wraps this function into an instance method using `PyInstanceMethod_New`. - Returns the instance method object. 3. Implement a function `create_method` that: - Takes a callable Python function and an instance of a class. - Wraps this function into a method using `PyMethod_New`. - Returns the method object. 4. Implement a function `invoke_methods` that: - Takes an instance method object and a method object. - Invokes these methods and returns their results. **Input and Output Format** - `create_instance_method(func: Callable) -> InstanceMethodObject` - Input: A callable Python function. - Output: An instance method object. - `create_method(func: Callable, instance: Any) -> MethodObject` - Input: A callable Python function and an instance of a class. - Output: A method object. - `invoke_methods(instance_method: InstanceMethodObject, method: MethodObject) -> Tuple[Any, Any]` - Input: An instance method object and a method object. - Output: A tuple containing the results of invoking the instance method and the method. **Constraints** - The Python function being passed should not take any arguments for simplicity. - Ensure proper error handling and type checking using the relevant C API functions. **Example** ```python def simple_function(): return \\"Hello, World!\\" class MyClass: def __init__(self, name): self.name = name def greet(self): return f\\"Hello, {self.name}!\\" # Create instance method and method using implemented functions imethod = create_instance_method(simple_function) mymethod = create_method(MyClass.greet, MyClass(\\"Alice\\")) # Invoke methods and get results result = invoke_methods(imethod, mymethod) print(result) # Expected output: (\\"Hello, World!\\", \\"Hello, Alice!\\") ``` **Note**: This question assumes the usage of C extension modules to implement the functions `create_instance_method` and `create_method`. Students will need to use appropriate C API functions to achieve the required functionality.","solution":"class InstanceMethodClass: def instance_method(self): return \\"Instance Method Called\\" class MethodClass: def __init__(self, name): self.name = name def method(self): return f\\"Method Called by {self.name}\\" def create_instance_method(func): return func.__get__(None, InstanceMethodClass) def create_method(func, instance): return func.__get__(instance, instance.__class__) def invoke_methods(instance_method_obj, method_obj): return instance_method_obj(), method_obj()"},{"question":"Coding Assessment Question # Objective: Demonstrate your understanding of creating new types in Python using type specifications and functions provided by the Python C-API. # Problem Statement: You are required to create a new type in Python that represents a simple 2D point. This point type will have two attributes `x` and `y`, and will support addition to create a new point that is the vector sum of the two points. Your task is to implement this using the provided Python C-API functions and structures. # Requirements: 1. **Create a new type** named `Point` with attributes `x` and `y`. 2. **Implement an addition operation** for the `Point` type that returns a new `Point` instance whose coordinates are the sum of the coordinates of the two points being added. 3. The new `Point` type should be initialized with two arguments representing the `x` and `y` coordinates. # Constraints: - Ensure that the `Point` type is created dynamically (heap-allocated). - The addition operation should be efficient and should handle edge cases, such as adding a `Point` to a non-`Point` object by raising an appropriate error. # Expected Input and Output: - **Input**: Initialization of `Point` with two numeric values, and using the `+` operator with another `Point` instance. - **Output**: A new `Point` instance with the summed coordinates. # Example: ```python # Example usage of the Point type p1 = Point(1, 2) p2 = Point(3, 4) p3 = p1 + p2 print(p3) # Expected output: Point(4, 6) ``` # Implementation Details: 1. **Define the type structure and slots**: - Define a new structure for storing `x` and `y`. - Implement type slots for the necessary operations, including initialization and addition. 2. **Create the new type using `PyType_FromSpec` or related functions**: - Prepare the `PyType_Spec` and `PyType_Slot` structures. - Use the relevant functions to finalize and register the new type with Python. # Solution Skeleton: Here is a template to get you started: ```python # Define the method for initialization def point_init(self, args, kwargs): # Extract x and y from args and kwargs pass # Define the method for addition def point_add(self, other): # Check if other is a Point and perform addition pass # Define the new Point type using the specifications def create_point_type(): # Fill in PyType_Spec and PyType_Slot with appropriate values pass # Register the Point type with Python def register_point_type(): # Call create_point_type() and handle registration pass # Your implementation here ``` # Submission: Provide a complete implementation including all necessary components to create, use, and demonstrate the new `Point` type as described.","solution":"class Point: def __init__(self, x, y): self.x = x self.y = y def __add__(self, other): if not isinstance(other, Point): raise TypeError(\\"Operand must be of type Point\\") return Point(self.x + other.x, self.y + other.y) def __repr__(self): return f\\"Point({self.x}, {self.y})\\""},{"question":"# Python Async I/O Management with `select` Module Problem Statement: You are tasked with creating a Python function that accepts a list of file-like objects (which can be either standard file objects or socket objects) and uses the `select` module to monitor these objects for any I/O events. The function should return a list of objects that are ready for any I/O operation (read, write, or exceptional condition). Function Signature: ```python def monitor_io(file_objects, timeout=None): Monitors multiple file objects for I/O readiness. Parameters: file_objects (list): List of file-like objects to monitor. timeout (float or None): Maximum time to wait in seconds. None for no timeout. Returns: dict: A dictionary with the keys \'read\', \'write\', and \'exception\', containing lists of file objects ready for reading, writing, or exceptional conditions respectively. pass ``` Constraints: - The file_objects parameter will always be a list of valid file-like objects. - Timeout is optional; if provided, it must be a non-negative float representing the number of seconds. - The function should handle signals by retrying the `select` call until it completes, as per PEP 475. Example Usage: ```python import os import socket # Create a pair of connected sockets sock1, sock2 = socket.socketpair() # Use os.pipe to create a pair of file objects r, w = os.pipe() # Example list containing a mix of file objects and sockets file_objects = [sock1, r, w] result = monitor_io(file_objects, timeout=5.0) print(result) # Should print dictionaries showing which objects are ready for reading, writing, or exceptional condition ``` Explanation: 1. The `monitor_io` function should use `select.select()` to monitor the file objects for reading, writing, and exceptional conditions. 2. File objects ready for each type of I/O should be stored in a dictionary which is returned to the caller. 3. Ensure that the function properly handles interruptions by signals, retrying the operation with an adjusted timeout. This question examines understanding of the select module and event-driven I/O handling. It also assesses practical knowledge in applying these concepts to manage multiple I/O channels simultaneously.","solution":"import select def monitor_io(file_objects, timeout=None): Monitors multiple file objects for I/O readiness. Parameters: file_objects (list): List of file-like objects to monitor. timeout (float or None): Maximum time to wait in seconds. None for no timeout. Returns: dict: A dictionary with the keys \'read\', \'write\', and \'exception\', containing lists of file objects ready for reading, writing, or exceptional conditions respectively. while True: try: read_ready, write_ready, exception_ready = select.select( file_objects, file_objects, file_objects, timeout ) return { \'read\': read_ready, \'write\': write_ready, \'exception\': exception_ready } except InterruptedError: continue # Retry the call if interrupted (PEP 475)"},{"question":"You have been provided with the PyTorch `torch.signal.windows` module, which includes various window functions for digital signal processing. Implement a function that takes an array of data and applies a specified window function from the `torch.signal.windows` module and processes the data accordingly. # Task Write a function `apply_window` that takes in the following parameters: - `data` (torch.Tensor): A 1D tensor representing the signal data. - `window_type` (str): A string indicating the type of window to be applied. This can be one of the following: `\'bartlett\'`, `\'blackman\'`, `\'cosine\'`, `\'exponential\'`, `\'gaussian\'`, `\'general_cosine\'`, `\'general_hamming\'`, `\'hamming\'`, `\'hann\'`, `\'kaiser\'`, `\'nuttall\'`. - `window_params` (dict): A dictionary containing additional parameters required by the specified window function (e.g., beta for kaiser, std for gaussian). The function should apply the specified window to the data and return the windowed data as a 1D tensor. Input - `data`: a 1D tensor of shape (N,) - `window_type`: a string specifying the window type - `window_params`: a dictionary containing additional parameters needed by the window function (if any) Output - a 1D tensor of the same shape (N,) representing the windowed data Example ```python import torch def apply_window(data, window_type, window_params): # Your implementation here # Example usage: data = torch.Tensor([1, 2, 3, 4, 5]) window_type = \'hann\' window_params = {} result = apply_window(data, window_type, window_params) print(result) ``` Constraints 1. The `data` tensor will have at least 3 elements and no more than 10000 elements. 2. The `window_type` must be one of the specified window types. 3. If the `window_type` function requires additional parameters, they must be correctly passed through `window_params`.","solution":"import torch import torch.signal.windows as windows def apply_window(data, window_type, window_params): Apply a specified window function to the given data. Parameters: data (torch.Tensor): A 1D tensor representing the signal data. window_type (str): A string indicating the type of window to be applied. window_params (dict): A dictionary containing additional parameters required by the specified window function. Returns: torch.Tensor: The windowed data as a 1D tensor. if not isinstance(data, torch.Tensor) or len(data.shape) != 1: raise ValueError(\\"data must be a 1D tensor.\\") if window_type not in [\'bartlett\', \'blackman\', \'cosine\', \'exponential\', \'gaussian\', \'general_cosine\', \'general_hamming\', \'hamming\', \'hann\', \'kaiser\', \'nuttall\']: raise ValueError(f\\"Unsupported window type: {window_type}\\") if window_type == \'bartlett\': window = windows.bartlett(len(data)) elif window_type == \'blackman\': window = windows.blackman(len(data)) elif window_type == \'cosine\': window = windows.cosine(len(data)) elif window_type == \'exponential\': window = windows.exponential(len(data), **window_params) elif window_type == \'gaussian\': window = windows.gaussian(len(data), **window_params) elif window_type == \'general_cosine\': window = windows.general_cosine(len(data), **window_params) elif window_type == \'general_hamming\': window = windows.general_hamming(len(data), **window_params) elif window_type == \'hamming\': window = windows.hamming(len(data)) elif window_type == \'hann\': window = windows.hann(len(data)) elif window_type == \'kaiser\': window = windows.kaiser(len(data), **window_params) elif window_type == \'nuttall\': window = windows.nuttall(len(data)) return data * window"},{"question":"**Coding Assessment Question** # Task You are required to implement a WSGI application that can handle file serving from a given directory. This application should return the contents of the files requested by the client. If the file does not exist, it should return a `404 Not Found` error message. The application must also log requests that result in `404` errors to a specific log file. # Requirements 1. **File Serving**: - Serve files from a specified directory. - If a file is not found, return a `404 Not Found` response. 2. **Logging**: - Log each `404 Not Found` error to a log file named `error_log.txt`. The log entry should include the timestamp and the requested URI. # Function Definition Implement the following function: ```python def file_serving_app(environ, start_response): A WSGI application that serves files from a directory. Args: environ (dict): The WSGI environment dictionary. start_response (callable): The callback function for starting the response. Returns: iterable: The response body as a byte string. pass ``` # Constraints - **Directory**: Assume the directory to serve from is the current working directory where the script is run. - **File Path**: Use the `PATH_INFO` variable from the WSGI environment to determine the requested file. - **Logging**: Error logs should be appended to the `error_log.txt` file. # Example Usage 1. **Starting the server**: ```python from wsgiref.simple_server import make_server with make_server(\'\', 8000, file_serving_app) as httpd: print(\\"Serving on port 8000...\\") httpd.serve_forever() ``` 2. **Making Requests**: - Requesting `http://localhost:8000/example.txt` should return the contents of `example.txt` if it exists in the current directory, otherwise it should return a `404 Not Found` response and log the error. - Any `404` errors should be logged in `error_log.txt`. # Notes You can refer to the `wsgiref` documentation provided to utilize utility functions and classes for implementing and handling the WSGI application correctly.","solution":"import os from datetime import datetime def file_serving_app(environ, start_response): A WSGI application that serves files from a directory. Args: environ (dict): The WSGI environment dictionary. start_response (callable): The callback function for starting the response. Returns: iterable: The response body as a byte string. # Get the requested path from the environment path = environ.get(\'PATH_INFO\', \'\').lstrip(\'/\') # Check if the file exists in the current working directory if os.path.isfile(path): with open(path, \'rb\') as f: response_body = f.read() start_response(\'200 OK\', [(\'Content-Type\', \'text/plain\')]) return [response_body] else: # Log the 404 error with open(\'error_log.txt\', \'a\') as log_file: timestamp = datetime.utcnow().strftime(\'%Y-%m-%d %H:%M:%S\') log_entry = f\\"{timestamp} - 404 Not Found - {path}n\\" log_file.write(log_entry) start_response(\'404 Not Found\', [(\'Content-Type\', \'text/plain\')]) return [b\'404 Not Found\']"},{"question":"Objective: Implement a secure file handling operation in Python that demonstrates awareness of potential security vulnerabilities. Problem Statement: You are required to implement a function `secure_xml_processing(file_path: str) -> dict` that reads and processes an XML file securely. The function should: 1. Read an XML file located at `file_path`. 2. Process the XML content to extract specific data (e.g., user information in the form of `<user><name>...</name><email>...</email></user>`). 3. Return the extracted data as a dictionary with the username as the key and email as the value. However, there are several security constraints to consider: - The XML content may be maliciously crafted. - You should avoid using deprecated or insecure functions/methods. - Ensure that the function is resistant to external attacks such as XML External Entity (XXE) attacks. Input: - `file_path`: A string representing the path to the XML file. Output: - A dictionary where the keys are usernames (str) and the values are emails (str). Constraints: - The XML file size should not exceed 1 MB. - The XML structure will always have multiple `<user>` elements with `<name>` and `<email>` child elements. Example: ```python # Example XML File Content: <users> <user> <name>JohnDoe</name> <email>johndoe@example.com</email> </user> <user> <name>JaneDoe</name> <email>janedoe@example.com</email> </user> </users> # Example usage of secure_xml_processing: result = secure_xml_processing(\\"path_to_file.xml\\") print(result) # Output: {\'JohnDoe\': \'johndoe@example.com\', \'JaneDoe\': \'janedoe@example.com\'} ``` Notes: - Handle any file reading exceptions gracefully. - Ensure secure parsing of XML to prevent vulnerabilities. Additional Resources: You may use the built-in `xml.etree.ElementTree` module for XML parsing but ensure to disable or handle any insecure features.","solution":"import xml.etree.ElementTree as ET import os def secure_xml_processing(file_path: str) -> dict: Reads and processes an XML file securely to extract user information. Arguments: file_path -- the path to the XML file Returns: A dictionary with username as the key and email as the value. # Ensure the file size is within the limit try: file_size = os.path.getsize(file_path) if file_size > 1 * 1024 * 1024: # 1 MB limit raise ValueError(\\"File size exceeds 1 MB limit\\") except FileNotFoundError: raise FileNotFoundError(\\"File not found\\") data = {} # Parse the XML file securely try: parser = ET.XMLParser() tree = ET.parse(file_path, parser=parser) root = tree.getroot() for user in root.findall(\'user\'): name = user.find(\'name\').text email = user.find(\'email\').text if name and email: data[name] = email except ET.ParseError: raise ValueError(\\"Error parsing XML file\\") return data"},{"question":"# Custom List-like Container with Special Methods Problem Statement Your task is to implement a custom class `CustomList` that mimics the behavior of a Python list. It should support typical list operations including indexing, setting items, deleting items, and iteration. Additionally, your `CustomList` class should implement a few special methods to allow for operator overloading and more flexible interactions. Requirements - Implement the following special methods: - `__getitem__(self, index)`: To support indexing. - `__setitem__(self, index, value)`: To support item assignment. - `__delitem__(self, index)`: To support item deletion. - `__len__(self)`: To get the length of the list. - `__iter__(self)`: To return an iterator over the list. - `__contains__(self, item)`: To support the `in` operator. - Implement the following standard methods for lists: - `append(self, item)`: To add an item to the end of the list. - `remove(self, item)`: To remove the first occurrence of an item. Usage Example ```python # Create an instance of CustomList clist = CustomList([1, 2, 3, 4]) print(len(clist)) # Output: 4 # Indexing print(clist[2]) # Output: 3 # Item assignment clist[2] = 10 print(clist[2]) # Output: 10 # Item deletion del clist[1] print(len(clist)) # Output: 3 # Append item clist.append(5) print(len(clist)) # Output: 4 # Remove item clist.remove(10) print(len(clist)) # Output: 3 # Iteration for item in clist: print(item) # Contains method print(5 in clist) # Output: True print(10 in clist) # Output: False ``` Constraints - The `CustomList` should only store items of any type. - The operations should run efficiently, keeping within average-case O(1) or O(n) depending on the method. Your Task Implement the `CustomList` class with the specified special methods and standard list methods to ensure it passes the usage example provided. Implementation Template ```python class CustomList: def __init__(self, initial=None): if initial is None: self.data = [] else: self.data = list(initial) def __getitem__(self, index): # Your code here pass def __setitem__(self, index, value): # Your code here pass def __delitem__(self, index): # Your code here pass def __len__(self): # Your code here pass def __iter__(self): # Your code here pass def __contains__(self, item): # Your code here pass def append(self, item): # Your code here pass def remove(self, item): # Your code here pass ``` Complete the class definition to ensure it meets the requirements.","solution":"class CustomList: def __init__(self, initial=None): if initial is None: self.data = [] else: self.data = list(initial) def __getitem__(self, index): return self.data[index] def __setitem__(self, index, value): self.data[index] = value def __delitem__(self, index): del self.data[index] def __len__(self): return len(self.data) def __iter__(self): return iter(self.data) def __contains__(self, item): return item in self.data def append(self, item): self.data.append(item) def remove(self, item): self.data.remove(item)"},{"question":"Implement a custom HTML parser that extracts specific elements from HTML content and stores them in a structured format. Your task is to extract all the URLs from \\"a\\" tags (hyperlinks) and all the headers (h1 to h6) along with their text content. The extracted information should be stored in a dictionary with the following structure: ```python { \\"links\\": [ {\\"href\\": url, \\"text\\": text_content}, ... ], \\"headers\\": [ {\\"tag\\": header_tag, \\"text\\": text_content}, ... ] } ``` # Requirements 1. Subclass the `html.parser.HTMLParser` class. 2. Override relevant methods to capture URLs from `a` tags and headers (`h1` to `h6`). 3. Implement a method `parse_html(self, html)` that accepts an HTML string and returns the structured dictionary. 4. Ensure proper handling of incomplete HTML and invalid markup. 5. Document your code and provide appropriate comments for clarity. # Input - A single string representing the HTML content to be parsed. # Output - A dictionary structured as described above. # Example Usage ```python html_content = \'\'\' <html> <body> <h1>Main Title</h1> <p>Some text with a <a href=\\"https://example.com\\">link</a>.</p> <div> <h2>Subtitle</h2> </div> </body> </html> \'\'\' parser = CustomHTMLParser() result = parser.parse_html(html_content) print(result) ``` # Expected Output ```python { \\"links\\": [ {\\"href\\": \\"https://example.com\\", \\"text\\": \\"link\\"} ], \\"headers\\": [ {\\"tag\\": \\"h1\\", \\"text\\": \\"Main Title\\"}, {\\"tag\\": \\"h2\\", \\"text\\": \\"Subtitle\\"} ] } ``` # Constraints - Handle invalid HTML gracefully. - Ensure that the `parse_html` method is efficient and can handle reasonably large HTML content. # Performance Requirements - The solution should handle HTML content up to 1 MB in size efficiently. _Hint: Use `handle_starttag`, `handle_endtag`, and `handle_data` methods to capture the relevant elements._ # Important Methods to Override - `handle_starttag(tag, attrs)` - `handle_endtag(tag)` - `handle_data(data)` Good luck!","solution":"from html.parser import HTMLParser class CustomHTMLParser(HTMLParser): def __init__(self): super().__init__() self.result = { \\"links\\": [], \\"headers\\": [] } self.current_link = None self.current_header = None def handle_starttag(self, tag, attrs): if tag == \\"a\\": href = None for attr in attrs: if attr[0] == \\"href\\": href = attr[1] break if href: self.current_link = {\\"href\\": href, \\"text\\": \\"\\"} elif tag in [\\"h1\\", \\"h2\\", \\"h3\\", \\"h4\\", \\"h5\\", \\"h6\\"]: self.current_header = {\\"tag\\": tag, \\"text\\": \\"\\"} def handle_endtag(self, tag): if tag == \\"a\\" and self.current_link: self.result[\\"links\\"].append(self.current_link) self.current_link = None elif tag in [\\"h1\\", \\"h2\\", \\"h3\\", \\"h4\\", \\"h5\\", \\"h6\\"] and self.current_header: self.result[\\"headers\\"].append(self.current_header) self.current_header = None def handle_data(self, data): if self.current_link: self.current_link[\\"text\\"] += data.strip() elif self.current_header: self.current_header[\\"text\\"] += data.strip() def parse_html(self, html): self.result = { \\"links\\": [], \\"headers\\": [] } self.current_link = None self.current_header = None self.feed(html) return self.result"},{"question":"# Pandas Assessment You are provided with data about employees and their salaries in different departments of a company. Your task is to write a function that processes this data to answer specific queries. # Task 1. **Create DataFrames:** - Create a DataFrame `df_employees` with the employee data given below. - Create a DataFrame `df_departments` with the department data given below. 2. **Data Manipulation:** - Add a column `Total` to the `df_employees` DataFrame that represents the total salary of each employee, calculated as `Base Salary` + `Bonus`. - Extract all employees who work in the \\"Engineering\\" department and have a total salary greater than 120,000 USD into a new DataFrame `df_engineering_high_salary`. 3. **Merging DataFrames:** - Merge the `df_employees` and `df_departments` DataFrames on the `Department ID` column to create a new DataFrame `df_merged`. 4. **Groupby Operation:** - Group the `df_merged` DataFrame by the `Department Name` and calculate the average total salary for each department. Return this as a new DataFrame `df_avg_salary`. # Input Data **Employee Data:** ``` | Employee ID | Name | Department ID | Base Salary | Bonus | | ----------- | -------- | ------------- | ----------- | ------ | | 1 | Alice | 1 | 95000 | 10000 | | 2 | Bob | 2 | 105000 | 15000 | | 3 | Charlie | 1 | 120000 | 5000 | | 4 | David | 3 | 83000 | 7000 | | 5 | Eve | 2 | 123000 | 12000 | ``` **Department Data:** ``` | Department ID | Department Name | | ------------- | --------------- | | 1 | Engineering | | 2 | Sales | | 3 | Marketing | ``` # Expected Function ```python def process_employee_data(): import pandas as pd # Create DataFrames df_employees = pd.DataFrame({ \\"Employee ID\\": [1, 2, 3, 4, 5], \\"Name\\": [\\"Alice\\", \\"Bob\\", \\"Charlie\\", \\"David\\", \\"Eve\\"], \\"Department ID\\": [1, 2, 1, 3, 2], \\"Base Salary\\": [95000, 105000, 120000, 83000, 123000], \\"Bonus\\": [10000, 15000, 5000, 7000, 12000] }) df_departments = pd.DataFrame({ \\"Department ID\\": [1, 2, 3], \\"Department Name\\": [\\"Engineering\\", \\"Sales\\", \\"Marketing\\"] }) # Add \'Total\' column df_employees[\\"Total\\"] = df_employees[\\"Base Salary\\"] + df_employees[\\"Bonus\\"] # Extract high salary engineering employees df_engineering_high_salary = df_employees[ (df_employees[\\"Department ID\\"] == 1) & (df_employees[\\"Total\\"] > 120000) ] # Merge DataFrames df_merged = pd.merge(df_employees, df_departments, on=\\"Department ID\\") # Groupby and calculate average salary df_avg_salary = df_merged.groupby(\\"Department Name\\")[\\"Total\\"].mean().reset_index() return df_engineering_high_salary, df_avg_salary # Example usage to see the outputs df_engineering_high_salary, df_avg_salary = process_employee_data() print(df_engineering_high_salary) print(df_avg_salary) ``` Constraints: - The solution should leverage pandas functions and methods effectively. - Ensure the code is efficient and follows best practices for pandas operations. - The function should return two DataFrames: `df_engineering_high_salary` and `df_avg_salary`.","solution":"def process_employee_data(): import pandas as pd # Create DataFrames df_employees = pd.DataFrame({ \\"Employee ID\\": [1, 2, 3, 4, 5], \\"Name\\": [\\"Alice\\", \\"Bob\\", \\"Charlie\\", \\"David\\", \\"Eve\\"], \\"Department ID\\": [1, 2, 1, 3, 2], \\"Base Salary\\": [95000, 105000, 120000, 83000, 123000], \\"Bonus\\": [10000, 15000, 5000, 7000, 12000] }) df_departments = pd.DataFrame({ \\"Department ID\\": [1, 2, 3], \\"Department Name\\": [\\"Engineering\\", \\"Sales\\", \\"Marketing\\"] }) # Add \'Total\' column df_employees[\\"Total\\"] = df_employees[\\"Base Salary\\"] + df_employees[\\"Bonus\\"] # Extract high salary engineering employees df_engineering_high_salary = df_employees[ (df_employees[\\"Department ID\\"] == 1) & (df_employees[\\"Total\\"] > 120000) ] # Merge DataFrames df_merged = pd.merge(df_employees, df_departments, on=\\"Department ID\\") # Groupby and calculate average salary df_avg_salary = df_merged.groupby(\\"Department Name\\")[\\"Total\\"].mean().reset_index() return df_engineering_high_salary, df_avg_salary # Example usage to see the outputs df_engineering_high_salary, df_avg_salary = process_employee_data() print(df_engineering_high_salary) print(df_avg_salary)"},{"question":"**Question: Handling the \\"None\\" Object in Python** # Background In Python, the \\"None\\" object represents the absence of a value or a null value. It is a singleton object, meaning that there is only one instance of \\"None\\" in a running Python environment. When working with \\"None\\", it is crucial to understand its behavior and appropriate handling in various scenarios. # Problem Statement Write a function `process_data` that processes a list of elements. Each element can either be a valid data point (an integer) or `None`. The function should calculate and return the sum of all valid data points (integers) in the list. Additionally, it should ensure that: 1. If `None` is encountered, it is skipped in the summation process. 2. If the list is empty or contains only `None` values, the function should return 0. # Function Signature ```python def process_data(data: List[Optional[int]]) -> int: pass ``` # Input - `data` (List[Optional[int]]): A list of integers and `None` values. List length: 0 <= len(data) <= 10^6. # Output - Return the sum of all integer values in the list. Return 0 if the list is empty or contains only `None` values. # Constraints - Your solution should handle large lists efficiently, considering performance implications. - You can use built-in functions and standard libraries as needed. # Examples ```python assert process_data([1, 2, None, 4, 5]) == 12 assert process_data([None, None, None]) == 0 assert process_data([None, 1, 2, 3, None, 4, 5, None]) == 15 assert process_data([]) == 0 assert process_data([0, None, -3, 7, None, None, 1]) == 5 ``` # Notes - To handle \\"None\\" correctly, remember that it should not contribute to the sum. - The function should be robust and handle edge cases like an empty list and lists composed entirely of `None`.","solution":"from typing import List, Optional def process_data(data: List[Optional[int]]) -> int: Processes a list of elements that may contain integers or None. Returns the sum of all integers in the list, skipping None values. If the list is empty or contains only None values, returns 0. return sum(x for x in data if x is not None)"},{"question":"# Advanced Coding Question Objective: You are required to demonstrate your understanding of the `multiprocessing.shared_memory` module by implementing a function that uses shared memory to perform efficient inter-process communication in a multiprocessing context. Problem Statement: Create a function `compute_large_prime_sums(n: int, k: int) -> List[int]` that concurrently calculates and returns the sums of the largest `k` prime numbers for each subset of `n` randomly generated integers. The function must distribute this task across multiple processes using shared memory and aggregate the results efficiently. # Requirements: 1. Generate a list of `n` random integers between 1 and 10000. 2. Divide the list into `m` approximately equal sublists, where `m` is the number of CPU cores available. 3. Each process should: - Access its assigned sublist. - Identify the largest `k` prime numbers in the sublist. - Compute the sum of these largest `k` prime numbers. - Store the sums in a shared memory list. 4. The main process should gather the sums from the shared memory list and return them as a Python list. # Constraints: - Use `multiprocessing`\'s `SharedMemory` or the `ShareableList` from `multiprocessing.shared_memory`. - Use parallel processing to concurrently compute the sums. - Ensure proper cleanup of shared memory resources. # Function Signature: ```python from typing import List import random import multiprocessing def compute_large_prime_sums(n: int, k: int) -> List[int]: pass ``` # Example: ```python import multiprocessing m = multiprocessing.cpu_count() # Let\'s say your machine has 4 cores (m = 4) n = 1000 k = 5 results = compute_large_prime_sums(n, k) # returns a list of length 4 with each entry being the sum of the largest 5 primes found in roughly 250 integers. ``` # Notes: - Use a helper function to determine if a number is prime. - Ensure proper synchronization and cleanup of shared memory to avoid resource leaks. - Utilize `multiprocessing.Pool` or `multiprocessing.Process` to manage your worker processes. - Handle edge cases where there might be fewer than `k` primes in a sublist.","solution":"from typing import List import random import multiprocessing from multiprocessing import shared_memory import numpy as np def is_prime(n: int) -> bool: Check if a number is prime. if n <= 1: return False if n <= 3: return True if n % 2 == 0 or n % 3 == 0: return False i = 5 while i * i <= n: if n % i == 0 or n % (i + 2) == 0: return False i += 6 return True def largest_k_primes_sum(arr: List[int], k: int) -> int: Compute the sum of the largest k primes in the list. primes = sorted([x for x in arr if is_prime(x)], reverse=True)[:k] return sum(primes) def worker(input_data: List[int], k: int, shared_mem_name: str, start_index: int, end_index: int): Worker function to compute sum of largest k primes for a sublist and store in shared memory. shared_mem = shared_memory.SharedMemory(name=shared_mem_name) sublist = input_data[start_index:end_index] result = largest_k_primes_sum(sublist, k) np_array = np.ndarray((len(input_data),), dtype=np.int64, buffer=shared_mem.buf) np_array[start_index // (end_index - start_index)] = result shared_mem.close() def compute_large_prime_sums(n: int, k: int) -> List[int]: Main function to coordinate the computation using shared memory and multiprocessing. num_cores = multiprocessing.cpu_count() random_integers = [random.randint(1, 10000) for _ in range(n)] shared_mem = shared_memory.SharedMemory(create=True, size=(num_cores * 8)) np_array = np.ndarray((num_cores,), dtype=np.int64, buffer=shared_mem.buf) chunk_size = n // num_cores processes = [] for i in range(num_cores): start_index = i * chunk_size end_index = (i + 1) * chunk_size if i < num_cores - 1 else n proc = multiprocessing.Process(target=worker, args=(random_integers, k, shared_mem.name, start_index, end_index)) processes.append(proc) proc.start() for proc in processes: proc.join() results = np_array.tolist() shared_mem.close() shared_mem.unlink() return results"},{"question":"# Python Coding Assessment Objective Demonstrate your understanding of the asyncio library by implementing an asynchronous TCP server and client system. This system should be capable of managing multiple client connections concurrently and processing client requests with a timeout mechanism. Problem Statement You are required to implement a TCP server and client using Python\'s asyncio library where: 1. The server listens for incoming client connections and assigns each client connection a unique identifier. 2. Each client should send a message to the server at random intervals (between 1 to 5 seconds), which the server should handle concurrently. 3. The server should echo back any message it receives from a client, prefixed with the client\'s identifier. 4. Implement a timeout mechanism such that if the server doesn\'t receive a message from a client within 10 seconds, it should close the connection for that client. 5. Ensure appropriate handling of exceptions such as timeouts and task cancellations. Guidelines 1. **Server Implementation**: - Create a TCP server that listens for incoming connections. - Assign each client a unique identifier upon connection. - Read messages from clients and echo them back with the client identifier, ensuring concurrency. - Implement a timeout for message receipt; close the connection after 10 seconds of inactivity. 2. **Client Implementation**: - Establish multiple client connections to the server. - Each client sends a message to the server at random intervals. - Clients handle echoed messages prefixed with their identifier. 3. **Exception Handling**: - Manage asyncio.TimeoutError when the server doesn\'t receive a client message for 10 seconds. - Handle the cancellation of tasks gracefully. Input and Output Format - There are no specific input and output formats. The focus is on the correct implementation of the asynchronous server-client system with the stated requirements. - You may demonstrate the functionality with print statements showing the interactions between clients and the server. Constraints and Performance Requirements - The client must send messages between 1 to 5 seconds intervals. - The server must timeout and close the client\'s connection after 10 seconds of inactivity. - The system should be capable of handling at least 5 clients concurrently. You can use the following template as a starting point: ```python import asyncio import random # Server Code async def handle_client(reader, writer): client_id = id(writer) print(f\'Client {client_id} connected.\') try: while True: data = await asyncio.wait_for(reader.read(100), timeout=10.0) if not data: break message = data.decode() response = f\'Client {client_id}: {message}\' writer.write(response.encode()) await writer.drain() except asyncio.TimeoutError: print(f\'Client {client_id} timeout. Closing connection.\') except asyncio.CancelledError: print(f\'Client {client_id} task cancelled.\') finally: writer.close() await writer.wait_closed() print(f\'Client {client_id} disconnected.\') async def run_server(): server = await asyncio.start_server(handle_client, \'127.0.0.1\', 8888) async with server: await server.serve_forever() # Client Code async def client(id): reader, writer = await asyncio.open_connection(\'127.0.0.1\', 8888) try: while True: message = f\'Hello from client {id}\' writer.write(message.encode()) await writer.drain() await asyncio.sleep(random.uniform(1, 5)) data = await reader.read(100) print(f\'Received: {data.decode()}\') except Exception as e: print(f\'Client {id} exception: {e}\') finally: writer.close() await writer.wait_closed() async def main(): # Run server server_task = asyncio.create_task(run_server()) # Run clients clients = [asyncio.create_task(client(i)) for i in range(5)] await asyncio.gather(*clients) await server_task if __name__ == \\"__main__\\": asyncio.run(main()) ``` Ensure to implement and test the complete solution adhering to the requirements. The provided template is a starting point, you may need to include handling of edge cases and additional functionality as per guidelines.","solution":"import asyncio import random # Server Code async def handle_client(reader, writer): client_id = id(writer) print(f\'Client {client_id} connected.\') try: while True: data = await asyncio.wait_for(reader.read(100), timeout=10.0) if not data: break message = data.decode() response = f\'Client {client_id}: {message}\' writer.write(response.encode()) await writer.drain() except asyncio.TimeoutError: print(f\'Client {client_id} timeout. Closing connection.\') except asyncio.CancelledError: print(f\'Client {client_id} task cancelled.\') finally: writer.close() await writer.wait_closed() print(f\'Client {client_id} disconnected.\') async def run_server(): server = await asyncio.start_server(handle_client, \'127.0.0.1\', 8888) async with server: await server.serve_forever() # Client Code async def client(id): reader, writer = await asyncio.open_connection(\'127.0.0.1\', 8888) try: while True: message = f\'Hello from client {id}\' writer.write(message.encode()) await writer.drain() await asyncio.sleep(random.uniform(1, 5)) data = await reader.read(100) print(f\'Received: {data.decode()}\') except Exception as e: print(f\'Client {id} exception: {e}\') finally: writer.close() await writer.wait_closed() async def main(): # Run server server_task = asyncio.create_task(run_server()) # Run clients clients = [asyncio.create_task(client(i)) for i in range(5)] await asyncio.sleep(10) # Let\'s allow clients to run for a certain period for c in clients: c.cancel() await asyncio.gather(*clients, return_exceptions=True) server_task.cancel() await server_task if __name__ == \\"__main__\\": asyncio.run(main())"},{"question":"**Coding Assessment Question: Seaborn Boxplots** **Objective:** Assess student\'s understanding and application of Seaborn\'s `boxplot` function to generate and customize data visualizations. **Problem Statement:** You are provided with the \'titanic\' dataset which can be loaded from Seaborn\'s datasets. Your task is to create and customize boxplots using this dataset as per the following requirements: 1. Load the \'titanic\' dataset using Seaborn. 2. Create a horizontal boxplot displaying the distribution of passengers\' fares (`fare` column). Include proper labels and a title for your plot. 3. Create a vertical boxplot with the following customizations: - Group the boxplots by the `pclass` category. - Use the `sex` column to create nested grouping (i.e., display separate boxes for male and female passengers within each class). - Add a small gap between the boxes. - Color the boxes differently for each sex for better distinction. - Customize the appearance of the median line to be red and thicker. - Include proper labels and a title for your plot. **Input and Output:** - Input: There is no external input needed; the dataset is loaded within the code. - Output: The function should output two matplotlib axes, each displaying the described boxplots. **Constraints:** - Use Seaborn\'s latest version (as referenced in the provided documentation). - Ensure that the plots are clear and well-labeled for easy interpretation. **Performance Requirements:** - The solution should generate the boxplots efficiently without unnecessary computations. - All customizations specified should be applied correctly. **Implementation:** Here is the structure of the function you need to implement: ```python import seaborn as sns import matplotlib.pyplot as plt def generate_boxplots(): # Load the \'titanic\' dataset titanic = sns.load_dataset(\\"titanic\\") # Horizontal boxplot for fares plt.figure(figsize=(10, 5)) axis1 = sns.boxplot(x=titanic[\\"fare\\"]) axis1.set_title(\'Distribution of Passengers\' Fares\') axis1.set_xlabel(\'Fare\') # Vertical boxplot with specified customizations plt.figure(figsize=(10, 5)) axis2 = sns.boxplot( data=titanic, x=\\"pclass\\", y=\\"age\\", hue=\\"sex\\", gap=0.1 ) axis2.set_title(\'Passenger Class vs Age by Sex\') axis2.set_xlabel(\'Class\') axis2.set_ylabel(\'Age\') plt.legend(title=\'Sex\') return axis1, axis2 # Call the function to generate the plots generate_boxplots() ``` Note: This function is designed to be self-contained. You can run the entire function to see the results directly.","solution":"import seaborn as sns import matplotlib.pyplot as plt def generate_boxplots(): # Load the \'titanic\' dataset titanic = sns.load_dataset(\\"titanic\\") # Horizontal boxplot for fares plt.figure(figsize=(10, 5)) axis1 = sns.boxplot(x=titanic[\\"fare\\"]) axis1.set_title(\\"Distribution of Passengers\' Fares\\") axis1.set_xlabel(\'Fare\') # Vertical boxplot with specified customizations plt.figure(figsize=(10, 7)) axis2 = sns.boxplot( data=titanic, x=\\"pclass\\", y=\\"age\\", hue=\\"sex\\", palette=\\"muted\\", width=0.6, # to add a small gap between boxes medianprops=dict(color=\\"red\\", linewidth=2) # Customize the median line ) axis2.set_title(\'Passenger Class vs Age by Sex\') axis2.set_xlabel(\'Class\') axis2.set_ylabel(\'Age\') axis2.legend(title=\'Sex\') return axis1, axis2 # Execute the function to generate the plots generate_boxplots() plt.show() # Display the plots"},{"question":"Coding Assessment Question # Objective You are to implement a Python class that simulates some of the concepts from the C-API regarding Python function objects. This will test your understanding of Python functions, closures, annotations, and argument defaults. # Problem Statement Implement a class `FunctionSimulator` that has the following methods: 1. `__init__(self, func)`: - Initializes the class with a Python function object `func`. - Store the function\'s code object, globals, defaults, closure, annotations, and module in appropriately named attributes. 2. `get_code(self)`: - Returns the code object associated with the stored function. 3. `get_globals(self)`: - Returns the globals dictionary of the stored function. 4. `get_defaults(self)`: - Returns the default argument values of the stored function. 5. `set_defaults(self, defaults)`: - Sets the default argument values for the stored function. `defaults` must be a tuple. 6. `get_annotations(self)`: - Returns the annotations of the stored function. 7. `set_annotations(self, annotations)`: - Sets the annotations for the stored function. `annotations` must be a dictionary. # Constraints - The function passed during initialization will not have a closure (free variables), simplify handling this aspect. - Methods that set attributes should raise a `ValueError` with an appropriate message if the input types are not as specified. - Ignore thread safety and concurrency issues for simplicity. Inputs - `__init__(self, func)`: `func` is a function object. - `set_defaults(self, defaults)`: `defaults` must be a tuple. - `set_annotations(self, annotations)`: `annotations` must be a dictionary. Outputs - `get_code(self)`: Returns the code object. - `get_globals(self)`: Returns the globals dictionary. - `get_defaults(self)`: Returns the tuple with default argument values or `None`. - `set_defaults(self, defaults)`: No return value, but modifies the function\'s default arguments. - `get_annotations(self)`: Returns the annotations dictionary or `None`. - `set_annotations(self, annotations)`: No return value, but modifies the function\'s annotations. # Example Usage ```python def sample_function(x: int, y: int = 42) -> int: return x + y # Initialize simulator sim = FunctionSimulator(sample_function) assert sim.get_code() == sample_function.__code__ assert sim.get_globals() == sample_function.__globals__ assert sim.get_defaults() == (42,) assert sim.get_annotations() == {\'x\': int, \'y\': int, \'return\': int} sim.set_defaults((10,)) assert sim.get_defaults() == (10,) sample_function(5) # Expected output: 15 new_annotations = {\'x\': str, \'y\': str, \'return\': str} sim.set_annotations(new_annotations) assert sim.get_annotations() == new_annotations try: sim.set_defaults([10]) # This should raise a ValueError except ValueError as e: print(e) # Output: Defaults must be a tuple ``` # Evaluation Your implementation will be evaluated based on: - Correctness: Properly implementing specified methods and constraints. - Robustness: Handling edge cases, such as incorrect input types with appropriate error handling. - Code Quality: Clear, concise, and well-documented code.","solution":"class FunctionSimulator: def __init__(self, func): if not callable(func): raise ValueError(\\"Input must be a callable function object\\") self.func = func self.code = func.__code__ self.globals = func.__globals__ self.defaults = func.__defaults__ self.closure = func.__closure__ self.annotations = func.__annotations__ self.module = func.__module__ def get_code(self): return self.code def get_globals(self): return self.globals def get_defaults(self): return self.defaults def set_defaults(self, defaults): if not isinstance(defaults, tuple): raise ValueError(\\"Defaults must be a tuple\\") self.func.__defaults__ = defaults self.defaults = defaults def get_annotations(self): return self.annotations def set_annotations(self, annotations): if not isinstance(annotations, dict): raise ValueError(\\"Annotations must be a dictionary\\") self.func.__annotations__ = annotations self.annotations = annotations"},{"question":"**Objective:** Define a custom Python class `CustomType` that mimics the behavior and features described in the provided documentation for a PyObject extension type. This class should handle attribute management, custom `__repr__` and `__str__` methods, customized comparison, and iteration protocol. **Requirements:** 1. **Attributes Management:** - The class should include an attribute `data` that is set during the initialization. 2. **Object Presentation:** - Implement `__repr__` and `__str__` methods to provide meaningful string representations of the instance. 3. **Object Comparison:** - Customize the comparison operations (`==`, `!=`, `<`, `<=`, `>`, `>=`) based on the `data` attribute. 4. **Iteration Protocol:** - Provide support for iteration where the instance holds a collection of items (e.g., a list) and can be iterated over. 5. **Finalization:** - Include a finalizer method (`__del__`) that performs cleanup actions when the object is garbage-collected. **Input and Output Formats:** - `__init__(self, data: Any)`: Initializes with the `data` attribute. - `__repr__(self) -> str`: Returns a detailed string representation. - `__str__(self) -> str`: Returns a simple string representation. - `__eq__(self, other: Any) -> bool`: Compares equality based on `data`. - `__ne__(self, other: Any) -> bool`: Compares inequality based on `data`. - `__lt__(self, other: Any) -> bool`: Less than comparison based on `data`. - `__le__(self, other: Any) -> bool`: Less than or equal comparison based on `data`. - `__gt__(self, other: Any) -> bool`: Greater than comparison based on `data`. - `__ge__(self, other: Any) -> bool`: Greater than or equal comparison based on `data`. - `__iter__(self) -> Iterator`: Provides an iterator. - `__next__(self) -> Any`: Returns the next item in the iteration. - `__del__(self)`: Finalizes and cleans up resources. **Constraints:** - The `data` attribute can be of any type that supports comparison operations. - The class should manage attributes and handle errors as appropriate. - The `__iter__` and `__next__` methods should handle a collection of items correctly. **Performance Requirements:** - Ensure that all attribute access and comparisons are efficient. - Iteration should be performed in linear time relative to the number of items. **Example Usage:** ```python class CustomType: def __init__(self, data): # Initialize the data attribute and prepare other necessary setups. pass def __repr__(self): # Return the detailed string representation. pass def __str__(self): # Return the simple string representation. pass def __eq__(self, other): # Compare self.data and other.data for equality. pass def __ne__(self, other): # Compare self.data and other.data for inequality. pass def __lt__(self, other): # Implement the less than comparison. pass def __le__(self, other): # Implement the less than or equal comparison. pass def __gt__(self, other): # Implement the greater than comparison. pass def __ge__(self, other): # Implement the greater than or equal comparison. pass def __iter__(self): # Provide support for iteration. pass def __next__(self): # Return the next item in the collection. pass def __del__(self): # Cleanup actions during finalization. pass # Example usage: obj = CustomType(data=[1, 2, 3, 4]) print(obj) # Should invoke __str__() repr(obj) # Should invoke __repr__() for item in obj: # Should iterate over items (1, 2, 3, 4) print(item) ```","solution":"class CustomType: def __init__(self, data): self.data = data # If the data is iterable, convert to an iterator for iteration handling if isinstance(data, (list, tuple, set, dict)): self.iterator = iter(data) else: self.iterator = iter([data]) def __repr__(self): return f\\"CustomType(data={self.data!r})\\" def __str__(self): return f\\"CustomType with data: {self.data}\\" def __eq__(self, other): if isinstance(other, CustomType): return self.data == other.data return False def __ne__(self, other): return not self.__eq__(other) def __lt__(self, other): if isinstance(other, CustomType): return self.data < other.data return False def __le__(self, other): if isinstance(other, CustomType): return self.data <= other.data return False def __gt__(self, other): if isinstance(other, CustomType): return self.data > other.data return False def __ge__(self, other): if isinstance(other, CustomType): return self.data >= other.data return False def __iter__(self): return self def __next__(self): return next(self.iterator) def __del__(self): print(f\\"CustomType object with data {self.data} is being deleted\\")"},{"question":"**Assessing Understanding of asyncio Futures in Python** You are required to write an asynchronous function and manage `Future` objects using Python\'s `asyncio` library. This task will test your understanding of creating, using, and managing `Future` objects. # Problem Statement Implement a function `async def fetch_data_and_calculate()` which performs the following steps: 1. Create a Future object to simulate fetching data which takes 2 seconds. 2. After fetching the data, set the result to a list of five integers. 3. Add a done callback to the Future that calculates the sum of the integers in the list and sets this as the result of another Future object. 4. Await both Futures and return their results as a tuple. # Detailed Steps - Define an asynchronous function `fetch_data_and_calculate()`: - Create an event loop using `asyncio.get_running_loop()`. - Create a Future object named `data_future` that will hold the simulated fetched data. - Create another Future object named `sum_future` that will hold the sum of the fetched data. - Make `data_future` await a 2-second sleep to simulate data fetching using `await asyncio.sleep(2)`. - Set `data_future` result to a predefined list of five integers (e.g., `[1, 2, 3, 4, 5]`). - Define a callback function `calculate_sum` that takes a Future object, retrieves its result, and sets the sum of its values as the result of `sum_future`. - Add `calculate_sum` as a done callback to `data_future`. - Await both `data_future` and `sum_future`. - Return their results as a tuple. # Example Usage ```python import asyncio async def main(): result = await fetch_data_and_calculate() print(result) # Expected: ([1, 2, 3, 4, 5], 15) asyncio.run(main()) ``` # Constraints - Only use functions and methods provided by Python\'s `asyncio` module. - Do not use additional libraries for asynchronous operations. - Ensure that callbacks and Future results are correctly handled to avoid any race conditions. # Hints - Make use of `add_done_callback` method on Futures for post-processing once the Future is done. - Use `set_result` method to manually set results for a Future. - Handling of event loop can be managed using `asyncio.get_running_loop()`.","solution":"import asyncio async def fetch_data_and_calculate(): loop = asyncio.get_running_loop() data_future = loop.create_future() sum_future = loop.create_future() async def fetch_data(): await asyncio.sleep(2) data_future.set_result([1, 2, 3, 4, 5]) def calculate_sum(fut): result = fut.result() sum_value = sum(result) sum_future.set_result(sum_value) data_future.add_done_callback(calculate_sum) asyncio.create_task(fetch_data()) data_result = await data_future sum_result = await sum_future return (data_result, sum_result)"},{"question":"# Multi-threading and Binary Data Manipulation in Python Problem Statement You are required to write a program that achieves the following tasks: 1. **Read a Binary File**: - Read and unpack binary data from a file format containing specific headers and data chunks. 2. **Multi-threading**: - Use multi-threading to handle intensive processing of these data chunks concurrently. 3. **Data Processing and Output Formatting**: - Process each chunk to extract relevant information and format the output in a readable manner. File Format - The binary file contains multiple data records. - Each record has a header containing metadata (8 bytes) and a data section containing the actual data. - The header structure is as follows (all in little-endian): - `record_id` (2 bytes): Unsigned short (`H`) - `timestamp` (4 bytes): Unsigned int (`I`) - `data_length` (2 bytes): Unsigned short (`H`) Concurrent Data Processing 1. **DataChunkProcessor Class**: - Define a class `DataChunkProcessor` extending `threading.Thread`. - It should take the binary `data` to process and extract the chunk based on the `data_length`. - After processing, it should store the result in a thread-safe collection. 2. **Main Program**: - The main function should read the binary file and extract records one by one. - For each record, it should instantiate and start a `DataChunkProcessor` thread. 3. **Output Format**: - Aggregate the processed results and print them with the following format: ``` Record ID: <record_id> Timestamp: <timestamp> Data Length: <data_length> Processed Data: <sum_of_data_bytes> ``` Example Assume the binary file `example.bin` contains: ``` Record 1: Header: (record_id=1, timestamp=1633029492, data_length=4) Data: [1, 2, 3, 4] Record 2: Header: (record_id=2, timestamp=1633029500, data_length=2) Data: [10, 20] ``` The program should produce the following output: ``` Record ID: 1 Timestamp: 1633029492 Data Length: 4 Processed Data: 10 Record ID: 2 Timestamp: 1633029500 Data Length: 2 Processed Data: 30 ``` Constraints - The binary file may be large, so efficient reading and processing are essential. - Use the `struct` module for unpacking binary data. - Use the `threading` module to handle concurrent processing. - Ensure thread safety while collecting results. Notes - You can save the binary data as `example.bin` or generate it using another script for testing purposes.","solution":"import threading import struct from queue import Queue class DataChunkProcessor(threading.Thread): def __init__(self, record_id, timestamp, data_length, data): threading.Thread.__init__(self) self.record_id = record_id self.timestamp = timestamp self.data_length = data_length self.data = data self.result_queue = DataChunkProcessor.result_queue def run(self): processed_data = sum(self.data) result = (self.record_id, self.timestamp, self.data_length, processed_data) self.result_queue.put(result) @staticmethod def init_queue(queue): DataChunkProcessor.result_queue = queue def read_and_process_binary_file(file_path): result_queue = Queue() DataChunkProcessor.init_queue(result_queue) records = [] threads = [] with open(file_path, \'rb\') as f: while True: header = f.read(8) # Read the header record if len(header) < 8: break # End of file record_id, timestamp, data_length = struct.unpack(\'<H I H\', header) data = list(f.read(data_length)) data_processor = DataChunkProcessor(record_id, timestamp, data_length, data) data_processor.start() threads.append(data_processor) for thread in threads: thread.join() # Ensure all threads have completed while not result_queue.empty(): records.append(result_queue.get()) # Print records for record in sorted(records, key=lambda x: x[0]): # Sort by record_id, just in case print(f\\"Record ID: {record[0]}\\") print(f\\"Timestamp: {record[1]}\\") print(f\\"Data Length: {record[2]}\\") print(f\\"Processed Data: {record[3]}n\\")"},{"question":"You have been tasked with developing a command-line utility that processes text files for various data extraction and manipulation tasks. For this assessment, you will focus on using the Python `re` module to implement specific text processing functionalities. Function: `process_text_file` Implement the function `process_text_file(file_path: str, pattern: str) -> List[str]` which takes the following parameters: - `file_path` (str): The path to the input text file. - `pattern` (str): A regular expression pattern to search within the text file. The function should return a list of strings containing all the unique matches of the given regular expression pattern found in the text file. The matches should be returned in the order of their first appearance in the text. Input: - The input text file contains multiple lines of text with varying structures. - The regular expression pattern will be a valid regex pattern. Output: - A list of unique matched strings. Constraints: - The text file may contain up to 100,000 lines. - Each line in the text file will be at most 1,000 characters long. - Your solution should be efficient in both time and space complexity. Example: Consider an input file `example.txt` with the following content: ``` Alice: 30 Bob: 25 Alice: 32 Charlie: 25 Alice: 30 ``` If the pattern is `bAlice: d+b`, the function call `process_text_file(\'example.txt\', r\'bAlice: d+b\')` should return: ``` [\'Alice: 30\', \'Alice: 32\'] ``` Implementation Requirements: - You can assume the input file path is valid and readable. - Utilize the `re` module for regex operations. - Ensure your solution handles large files efficiently. # Hints: - Use the `re.findall` method to find all non-overlapping matches of the pattern in the string. - Use a set to keep track of unique matches. - Read the file line by line to avoid memory issues with large files.","solution":"import re from typing import List def process_text_file(file_path: str, pattern: str) -> List[str]: unique_matches = set() ordered_matches = [] # Compile the provided regular expression pattern regex = re.compile(pattern) # Read and process the file line by line with open(file_path, \'r\') as file: for line in file: matches = regex.findall(line) for match in matches: if match not in unique_matches: unique_matches.add(match) ordered_matches.append(match) return ordered_matches"},{"question":"**Objective**: Implement a custom interactive Python interpreter using the `code` module that includes additional features such as command history and command execution time measurement. # Description You are required to create a class `CustomInteractiveInterpreter` that extends the functionality provided by the `code.InteractiveConsole` class. This customized interpreter should include: 1. **Command History**: Maintain a history of all commands entered by the user. The history should be stored in a list. 2. **Execution Time Measurement**: Measure and display the execution time for each command entered by the user. # Requirements 1. **Class Definition**: - Define a class `CustomInteractiveInterpreter` inheriting from `code.InteractiveConsole`. 2. **Command History**: - Implement a method `show_history()` that prints all the commands entered by the user. - Each command should be stored in a list `self.history`. 3. **Execution Time Measurement**: - Override the `push(line)` method to measure the execution time of each command using the built-in `time` module. - Display the execution time along with the command result. 4. **Main Loop**: - Implement a method `start()` that starts the custom interpreter with an introductory message and handles user input. - Allow a special command `history` that calls `show_history()` to display the command history. - Allow an exit command (`exit`, `quit`, or `Ctrl+D`) to exit the interpreter gracefully, displaying an exit message. # Constraints - Use only the standard library modules (e.g., `code`, `time`, `sys`). # Example Usage ```python if __name__ == \\"__main__\\": import sys import time import code class CustomInteractiveInterpreter(code.InteractiveConsole): def __init__(self, locals=None): super().__init__(locals) self.history = [] def push(self, line): import time start_time = time.time() self.history.append(line) more = super().push(line) end_time = time.time() print(f\\"Execution Time: {end_time - start_time:.6f} seconds\\") return more def show_history(self): for idx, command in enumerate(self.history, 1): print(f\\"{idx}: {command}\\") def start(self): print(\\"Custom Interactive Interpreter. Type \'history\' to view command history.\\") try: self.interact() except SystemExit: print(\\"Exiting Custom Interactive Interpreter\\") # Start the custom interpreter interpreter = CustomInteractiveInterpreter() interpreter.start() ``` # Testing the Implementation To test the implementation: 1. Run the script. 2. Enter Python commands and observe the output for execution time. 3. Enter the command `history` to see the list of all entered commands. 4. Exit the interpreter using `exit`, `quit`, or `Ctrl+D` and observe the exit message. # Performance Considerations - Ensure that the history and execution time measurement does not significantly slow down the interpreter.","solution":"import code import time class CustomInteractiveInterpreter(code.InteractiveConsole): def __init__(self, locals=None): super().__init__(locals) self.history = [] def push(self, line): start_time = time.time() self.history.append(line) more = super().push(line) end_time = time.time() print(f\\"Execution Time: {end_time - start_time:.6f} seconds\\") return more def show_history(self): for idx, command in enumerate(self.history, 1): print(f\\"{idx}: {command}\\") def start(self): print(\\"Custom Interactive Interpreter. Type \'history\' to view command history.\\") while True: try: line = input(\'>>> \') if line.strip().lower() == \'history\': self.show_history() else: self.push(line) except (EOFError, KeyboardInterrupt, SystemExit): print(\\"nExiting Custom Interactive Interpreter\\") break # This block is for running the interpreter directly if __name__ == \\"__main__\\": interpreter = CustomInteractiveInterpreter() interpreter.start()"},{"question":"You are tasked with evaluating the performance of machine learning models and understanding their behavior with respect to their hyperparameters and the amount of training data available. Task 1: Validation Curve Write a Python function `plot_validation_curve` that: - Uses the `validation_curve` function from scikit-learn. - Plots the training and validation scores for a specified hyperparameter over a range of values. - Inputs: - `estimator`: The model/estimator instance (e.g., `SVC(kernel=\\"linear\\")`). - `X`: Feature matrix (e.g., from the iris dataset). - `y`: Target array (e.g., from the iris dataset). - `param_name`: String, the name of the hyperparameter to evaluate (e.g., `\\"C\\"` for `SVC`). - `param_range`: Array-like, the range of values for the hyperparameter (e.g., `np.logspace(-3, 3, 5)`). ```python def plot_validation_curve(estimator, X, y, param_name, param_range): Plots the validation curve for a given estimator and hyperparameter. Parameters ---------- estimator : object The model/estimator instance (e.g., SVC(kernel=\\"linear\\")). X : array-like Feature matrix. Y : array-like Target array. param_name : str The name of the hyperparameter to evaluate (e.g., \\"C\\" for SVC). param_range : array-like The range of values for the hyperparameter (e.g., np.logspace(-3, 3, 5)). Returns ------- None. Displays the validation curve plot. from sklearn.model_selection import validation_curve import matplotlib.pyplot as plt train_scores, valid_scores = validation_curve( estimator, X, y, param_name=param_name, param_range=param_range, cv=5 ) train_scores_mean = train_scores.mean(axis=1) valid_scores_mean = valid_scores.mean(axis=1) plt.figure() plt.semilogx(param_range, train_scores_mean, label=\'Training score\') plt.semilogx(param_range, valid_scores_mean, label=\'Validation score\') plt.xlabel(param_name) plt.ylabel(\\"Score\\") plt.legend(loc=\\"best\\") plt.title(\\"Validation Curve\\") plt.show() ``` Task 2: Learning Curve Write a Python function `plot_learning_curve` that: - Uses the `learning_curve` function from scikit-learn. - Plots the training and validation scores against the number of training samples. - Inputs: - `estimator`: The model/estimator instance (e.g., `SVC(kernel=\\"linear\\")`). - `X`: Feature matrix (e.g., from the iris dataset). - `y`: Target array (e.g., from the iris dataset). - `train_sizes`: Array of integers, the sizes of the training sets to evaluate (e.g., `[50, 100, 150]`). ```python def plot_learning_curve(estimator, X, y, train_sizes): Plots the learning curve for a given estimator. Parameters ---------- estimator : object The model/estimator instance (e.g., SVC(kernel=\\"linear\\")). X : array-like Feature matrix. Y : array-like Target array. train_sizes : array-like The sizes of the training sets to evaluate (e.g., [50, 100, 150]). Returns ------- None. Displays the learning curve plot. from sklearn.model_selection import learning_curve import matplotlib.pyplot as plt train_sizes, train_scores, valid_scores = learning_curve( estimator, X, y, train_sizes=train_sizes, cv=5 ) train_scores_mean = train_scores.mean(axis=1) valid_scores_mean = valid_scores.mean(axis=1) plt.figure() plt.plot(train_sizes, train_scores_mean, label=\'Training score\') plt.plot(train_sizes, valid_scores_mean, label=\'Validation score\') plt.xlabel(\\"Training examples\\") plt.ylabel(\\"Score\\") plt.legend(loc=\\"best\\") plt.title(\\"Learning Curve\\") plt.show() ``` Evaluation - Create a synthetic dataset or use datasets available in scikit-learn (e.g., iris dataset). - Implement both functions and generate the corresponding plots. - Interpret the plots to discuss the observed trends in bias, variance, and impact of the training data on the model performance.","solution":"def plot_validation_curve(estimator, X, y, param_name, param_range): Plots the validation curve for a given estimator and hyperparameter. Parameters ---------- estimator : object The model/estimator instance (e.g., SVC(kernel=\\"linear\\")). X : array-like Feature matrix. Y : array-like Target array. param_name : str The name of the hyperparameter to evaluate (e.g., \\"C\\" for SVC). param_range : array-like The range of values for the hyperparameter (e.g., np.logspace(-3, 3, 5)). Returns ------- None. Displays the validation curve plot. from sklearn.model_selection import validation_curve import matplotlib.pyplot as plt train_scores, valid_scores = validation_curve( estimator, X, y, param_name=param_name, param_range=param_range, cv=5 ) train_scores_mean = train_scores.mean(axis=1) train_scores_std = train_scores.std(axis=1) valid_scores_mean = valid_scores.mean(axis=1) valid_scores_std = valid_scores.std(axis=1) plt.figure() plt.semilogx(param_range, train_scores_mean, label=\'Training score\') plt.semilogx(param_range, valid_scores_mean, label=\'Validation score\') plt.fill_between(param_range, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.2) plt.fill_between(param_range, valid_scores_mean - valid_scores_std, valid_scores_mean + valid_scores_std, alpha=0.2) plt.xlabel(param_name) plt.ylabel(\\"Score\\") plt.legend(loc=\\"best\\") plt.title(\\"Validation Curve\\") plt.show() def plot_learning_curve(estimator, X, y, train_sizes): Plots the learning curve for a given estimator. Parameters ---------- estimator : object The model/estimator instance (e.g., SVC(kernel=\\"linear\\")). X : array-like Feature matrix. Y : array-like Target array. train_sizes : array-like The sizes of the training sets to evaluate (e.g., [50, 100, 150]). Returns ------- None. Displays the learning curve plot. from sklearn.model_selection import learning_curve import matplotlib.pyplot as plt train_sizes, train_scores, valid_scores = learning_curve( estimator, X, y, train_sizes=train_sizes, cv=5 ) train_scores_mean = train_scores.mean(axis=1) train_scores_std = train_scores.std(axis=1) valid_scores_mean = valid_scores.mean(axis=1) valid_scores_std = valid_scores.std(axis=1) plt.figure() plt.plot(train_sizes, train_scores_mean, label=\'Training score\') plt.plot(train_sizes, valid_scores_mean, label=\'Validation score\') plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.2) plt.fill_between(train_sizes, valid_scores_mean - valid_scores_std, valid_scores_mean + valid_scores_std, alpha=0.2) plt.xlabel(\\"Training examples\\") plt.ylabel(\\"Score\\") plt.legend(loc=\\"best\\") plt.title(\\"Learning Curve\\") plt.show()"},{"question":"You are tasked with implementing a function `custom_zip_longest` that mimics the behavior of the `itertools.zip_longest` function. This function should iterate over multiple iterables in parallel, producing tuples with an item from each iterable. If the iterables are of uneven length, the unfinished iterables should be padded with a specified fill value until all iterables are exhausted. Function Signature ```python def custom_zip_longest(*iterables, fillvalue=None): # # Your code here # ``` Input - `*iterables`: Variable number of iterables (e.g., lists, tuples). The function should be able to handle any number and combination of iterables as input. - `fillvalue`: The value to use for padding the shorter iterables (default is `None`). Output - Returns an iterator that yields tuples containing items from each of the iterables. Constraints - You cannot use the `itertools.zip_longest` function. - You should follow similar behavior as `itertools.zip_longest` in terms of handling uneven lengths and using the `fillvalue` for padding. - The function should handle empty iterables properly. Example ```python # Example 1 list(custom_zip_longest([1, 2, 3], [\'a\', \'b\'])) # Output: [(1, \'a\'), (2, \'b\'), (3, None)] # Example 2 list(custom_zip_longest(range(3), [\'a\', \'b\', \'c\', \'d\'], (0, 1), fillvalue=\'fill\')) # Output: [(0, \'a\', 0), (1, \'b\', 1), (2, \'c\', \'fill\'), (\'fill\', \'d\', \'fill\')] ``` Function Implementation ```python def custom_zip_longest(*iterables, fillvalue=None): iterators = [iter(it) for it in iterables] num_active = len(iterators) while num_active: result = [] for it in iterators: try: item = next(it) except StopIteration: item = fillvalue num_active -= 1 result.append(item) if num_active: yield tuple(result) ``` Your task is to implement the `custom_zip_longest` function that meets the requirements and constraints outlined above.","solution":"def custom_zip_longest(*iterables, fillvalue=None): Mimics the behavior of itertools.zip_longest. Iterates over multiple iterables in parallel, producing tuples with an item from each iterable. If the iterables are of uneven length, the unfinished iterables are padded with the specified fill value. # Convert each iterable to an iterator iterators = [iter(it) for it in iterables] # Continue until all iterators are exhausted while iterators: result = [] for it in iterators: try: # Try to get the next item from each iterator result.append(next(it)) except StopIteration: # If an iterator is exhausted, append the fillvalue and keep it in the result result.append(fillvalue) # Check if all iterators are exhausted if all(x == fillvalue for x in result[len(result) - len(iterables) :]): break # Yield the result as a tuple yield tuple(result)"},{"question":"# Pandas Configuration Management You are given a task to manage Pandas options in a data analysis environment. Specifically, you should demonstrate your ability to configure and reset options using Pandas\' options API. Here are the requirements: 1. **Set an Option:** Write a function `set_option_example(option_name: str, value)` that sets a specified Pandas option to a given value. Your function should properly handle incorrect option names by reporting an appropriate message. 2. **Describe an Option:** Write a function `describe_an_option(option_name: str) -> str` that returns a description of a specified Pandas option. If the option name is incorrect or does not exist, the function should return an error message. 3. **Reset All Options:** Write a function `reset_all_options()` that resets all Pandas options to their default values. 4. **Float Formatting Context:** Write a function `float_formatting_context(precision)` that temporarily sets the floating point output precision to a specified number and demonstrates this by printing an example DataFrame within the context. After exiting the context, print another DataFrame to show that the precision has been reset. __Function Definitions:__ ```python def set_option_example(option_name: str, value): Sets a specified Pandas option to the given value. Parameters: - option_name (str): The name of the option to set. - value: The value to set for the option. Returns: - str: Success or error message. pass def describe_an_option(option_name: str) -> str: Returns the description of a specified Pandas option. Parameters: - option_name (str): The name of the option to describe. Returns: - str: Description or error message if the option does not exist. pass def reset_all_options(): Resets all Pandas options to their default values. pass def float_formatting_context(precision): Temporarily sets the floating point output precision to a specified number and prints an example DataFrame within the context. Parameters: - precision: The precision to set for floating point numbers in the context. pass ``` __Input/Output:__ - For `set_option_example`, the input is the option name and the value to be set. The output is a string message indicating success or failure. - For `describe_an_option`, the input is the option name and the output is a string description of the option or an error message. - For `reset_all_options`, there is no input or output; it affects the global state of Pandas options. - For `float_formatting_context`, the input is the precision to set, and it prints DataFrames demonstrating the context-based formatting. **Constraints:** - Only valid option names should be accepted. - Demonstrate appropriate exception handling for invalid inputs or operations. - Seek to use concise and efficient code practice. Implement these functions correctly to demonstrate your understanding of Pandas\' options management API, including handling and formatting.","solution":"import pandas as pd def set_option_example(option_name: str, value): Sets a specified Pandas option to the given value. Parameters: - option_name (str): The name of the option to set. - value: The value to set for the option. Returns: - str: Success or error message. try: pd.set_option(option_name, value) return f\\"Option \'{option_name}\' set to {value}.\\" except pd._config.config.OptionError: return f\\"Option \'{option_name}\' is not a valid option name.\\" def describe_an_option(option_name: str) -> str: Returns the description of a specified Pandas option. Parameters: - option_name (str): The name of the option to describe. Returns: - str: Description or error message if the option does not exist. try: description = pd.describe_option(option_name, _print_desc=False) return description except pd._config.config.OptionError: return f\\"Option \'{option_name}\' is not a valid option name.\\" def reset_all_options(): Resets all Pandas options to their default values. pd.reset_option(\\"all\\") def float_formatting_context(precision): Temporarily sets the floating point output precision to a specified number and prints an example DataFrame within the context. Parameters: - precision: The precision to set for floating point numbers in the context. df = pd.DataFrame({\'A\': [1.123456, 2.123456], \'B\': [3.654321, 4.654321]}) with pd.option_context(\'display.precision\', precision): print(\\"Within context:\\") print(df) print(\\"Outside context:\\") print(df)"},{"question":"# Advanced PyTorch Autograd Assessment **Objective:** Implement a custom autograd `Function` to compute the Jacobian and Hessian for a given function. Background: Autograd is PyTorch\'s automatic differentiation engine that powers neural network training. In this task, you will create a custom autograd `Function` that computes the Jacobian and Hessian matrices of a given scalar-valued function with respect to its inputs. **Definitions:** - The **Jacobian** matrix `J` of a scalar-valued function `f` with respect to an input vector `x` is a row vector of all first-order partial derivatives of `f` with respect to each component of `x`. - The **Hessian** matrix `H` of a scalar-valued function `f` with respect to an input vector `x` is a square matrix of second-order partial derivatives of `f`. **Task:** 1. Implement a custom autograd `Function` named `JacobianHessianFunction` that computes both the Jacobian and Hessian of a given scalar function `f` with respect to a tensor input `x`. 2. Your implementation should include the `forward`, `backward`, and `jvp` (Jacobian-vector product) methods of the `Function`. 3. Create a test case to verify that your `JacobianHessianFunction` correctly computes the Jacobian and Hessian for the function `f(x) = x_1^2 + 3 * x_2. x is a two-dimensional vector`. Function Signature: ```python import torch from torch.autograd import Function class JacobianHessianFunction(Function): @staticmethod def forward(ctx, input): Compute the forward pass. Parameters: - ctx: context object to save information for backward computation. - input: tensor of shape (n,) Returns: - output: scalar tensor # Implement the forward pass. pass @staticmethod def backward(ctx, grad_output): Compute the backward pass (Jacobian). Parameters: - ctx: context object containing saved tensors from forward pass. - grad_output: downstream gradient. Returns: - grad_input: gradient with respect to input tensor (Jacobian). # Implement the backward pass. pass @staticmethod def jvp(ctx, grad_input): Compute the Jacobian-vector product. Parameters: - ctx: context object containing saved tensors from the forward pass. - grad_input: input gradient vector. Returns: - jvp_output: output gradient vector (JVP). # Implement the Jacobian-vector product. pass def compute_jacobian_hessian(input): Compute both the Jacobian and Hessian for the test function using the custom autograd function. Parameters: - input: tensor of shape (2, ) Returns: - jacobian: tensor of shape (1, 2) - hessian: tensor of shape (2, 2) # Implement the function computation. pass # Test case. if __name__ == \'__main__\': input = torch.tensor([1.0, 2.0], requires_grad=True) jacobian, hessian = compute_jacobian_hessian(input) print(f\\"Jacobian: {jacobian}\\") print(f\\"Hessian: {hessian}\\") ``` **Constraints:** - The input tensor `x` will always be a 1D tensor. - You cannot use the `torch.autograd.functional.jacobian` or `torch.autograd.functional.hessian` directly in your implementation. **Performance Requirements:** - Ensure your solution is efficient and leverages PyTorch\'s autograd mechanisms properly. - Avoid unnecessary computations and ensure the backward pass reuses saved tensors from the forward pass efficiently.","solution":"import torch from torch.autograd import Function class JacobianHessianFunction(Function): @staticmethod def forward(ctx, input): Compute the forward pass. Parameters: - ctx: context object to save information for backward computation. - input: tensor of shape (n,) Returns: - output: scalar tensor x1, x2 = input[0], input[1] output = x1**2 + 3 * x2 ctx.save_for_backward(input) return output @staticmethod def backward(ctx, grad_output): Compute the backward pass (Jacobian). Parameters: - ctx: context object containing saved tensors from forward pass. - grad_output: downstream gradient. Returns: - grad_input: gradient with respect to input tensor (Jacobian). input, = ctx.saved_tensors x1, x2 = input[0], input[1] grad_input = torch.zeros_like(input) grad_input[0] = 2 * x1 * grad_output grad_input[1] = 3 * grad_output return grad_input @staticmethod def jvp(ctx, grad_input): Compute the Jacobian-vector product. Parameters: - ctx: context object containing saved tensors from the forward pass. - grad_input: input gradient vector. Returns: - jvp_output: output gradient vector (JVP). input, = ctx.saved_tensors x1, x2 = input[0], input[1] jvp_output = torch.zeros_like(grad_input) jvp_output[0] = 2 * grad_input[0] * x1 jvp_output[1] = 3 * grad_input[1] return jvp_output def compute_jacobian_hessian(input): Compute both the Jacobian and Hessian for the test function using the custom autograd function. Parameters: - input: tensor of shape (2, ) Returns: - jacobian: tensor of shape (1, 2) - hessian: tensor of shape (2, 2) input = input.requires_grad_(True) output = JacobianHessianFunction.apply(input) jacobian = torch.autograd.grad(outputs=output, inputs=input, create_graph=True)[0] hessian = torch.stack([torch.autograd.grad(outputs=jacobian[i], inputs=input, create_graph=True)[0] for i in range(input.size(0))], dim=0) return jacobian, hessian # Test run if __name__ == \'__main__\': input = torch.tensor([1.0, 2.0], requires_grad=True) jacobian, hessian = compute_jacobian_hessian(input) print(f\\"Jacobian: {jacobian}\\") print(f\\"Hessian: {hessian}\\")"},{"question":"# PyTorch Coding Assessment **Objective:** Implement a simple feedforward neural network model using PyTorch. Your implementation should ensure compatibility with TorchScript by avoiding unsupported constructs and adhering to the constraints mentioned in the provided documentation. **Task:** 1. Implement a class `SimpleFeedForwardNN` that defines a feedforward neural network using PyTorch\'s `torch.nn.Module`. 2. The network should contain: - An input layer that matches the input feature size. - One hidden layer with 128 units and a ReLU activation function. - An output layer with a size of 10 (assuming a classification problem with 10 classes) followed by a softmax activation function. 3. Write a function `convert_to_torchscript` that takes the implemented model as input and returns its TorchScript version. **Constraints:** - Do not use any functions, modules, or classes that are unsupported by TorchScript as mentioned in the documentation. - Ensure that tensor construction operations strictly follow the schema requirements when converting the model to TorchScript (e.g., explicit use of dtype, layout, device parameters if needed). **Input and Output:** - The `SimpleFeedForwardNN` class should take an integer `input_size` as an initializer parameter. - The `convert_to_torchscript` function should accept an instance of the `SimpleFeedForwardNN` model and return a compiled TorchScript version of the model. **Skeleton Code:** ```python import torch import torch.nn as nn import torch.nn.functional as F class SimpleFeedForwardNN(nn.Module): def __init__(self, input_size): super(SimpleFeedForwardNN, self).__init__() # Define the layers here self.hidden = nn.Linear(input_size, 128) self.output = nn.Linear(128, 10) def forward(self, x): # Define the forward pass here x = F.relu(self.hidden(x)) x = F.log_softmax(self.output(x), dim=1) return x def convert_to_torchscript(model): # Convert the PyTorch model to TorchScript torchscript_model = torch.jit.script(model) return torchscript_model # Example usage: # model = SimpleFeedForwardNN(input_size=784) # torchscript_model = convert_to_torchscript(model) # print(torchscript_model) ``` **Evaluation:** - Your code will be evaluated based on its correctness, adherence to constraints, and compatibility with TorchScript. - Ensure that your PyTorch model can be successfully converted to TorchScript and can handle tensor inputs as expected.","solution":"import torch import torch.nn as nn import torch.nn.functional as F class SimpleFeedForwardNN(nn.Module): def __init__(self, input_size): super(SimpleFeedForwardNN, self).__init__() # Define the layers here self.hidden = nn.Linear(input_size, 128) self.output = nn.Linear(128, 10) def forward(self, x): # Define the forward pass here x = F.relu(self.hidden(x)) x = F.log_softmax(self.output(x), dim=1) return x def convert_to_torchscript(model): # Convert the PyTorch model to TorchScript torchscript_model = torch.jit.script(model) return torchscript_model # Example usage: # model = SimpleFeedForwardNN(input_size=784) # torchscript_model = convert_to_torchscript(model) # print(torchscript_model)"},{"question":"**Question: Advanced Multi-Variable Plotting with Seaborn** You are required to generate a series of subplots using the `seaborn.objects` module. The goal is to visualize relationships between various automobile features using the available `mpg` dataset from Seaborn. # Task 1. Load the `mpg` dataset from Seaborn. 2. Create a plot that showcases the following: - Pair the `mpg` (miles per gallon) of automobiles against `horsepower` and `weight`. - Pair the `acceleration` of automobiles against `horsepower` and `weight`. - Use faceting to separate these visualizations by the `origin` of the automobiles. - Customize the x and y-axis labels to be more descriptive (e.g., \\"Horsepower (hp)\\", \\"Weight (lbs)\\", etc.). # Implementation 1. **Input**: None (use the internally available Seaborn `mpg` dataset). 2. **Output**: An advanced seaborn plot object displaying the specified relationships. # Constraints 1. You must use the `seaborn.objects` module to create the plot. 2. Follow the instructions for pairing and faceting correctly. 3. Ensure that the labels are correctly assigned to each axis as described. # Expected Output Format ```python import seaborn.objects as so from seaborn import load_dataset # Load the dataset mpg = load_dataset(\\"mpg\\") # Create the plot ( so.Plot(mpg) .pair(x=[\\"horsepower\\", \\"weight\\"], y=[\\"mpg\\", \\"acceleration\\"]) .facet(col=\\"origin\\") .label(x0=\\"Horsepower (hp)\\", x1=\\"Weight (lbs)\\", y0=\\"Miles per Gallon (MPG)\\", y1=\\"Acceleration (0-60 mph)\\") .add(so.Dots()) ) ``` # Evaluation Your solution should be evaluated based on: - Correct usage of the `seaborn.objects` module. - Accurate pairing of variables. - Proper faceting into subplots by origin. - Appropriate and clear labeling of the axes. Submit your completed Python script that accomplishes this task.","solution":"import seaborn.objects as so from seaborn import load_dataset def create_advanced_plot(): # Load the dataset mpg = load_dataset(\\"mpg\\") # Create the plot plot = ( so.Plot(mpg) .pair(x=[\\"horsepower\\", \\"weight\\"], y=[\\"mpg\\", \\"acceleration\\"]) .facet(col=\\"origin\\") .label(x0=\\"Horsepower (hp)\\", x1=\\"Weight (lbs)\\", y0=\\"Miles per Gallon (MPG)\\", y1=\\"Acceleration (0-60 mph)\\") .add(so.Dots()) ) return plot"},{"question":"**Objective**: Demonstrate your understanding of the `zlib` library by implementing a function that compresses and decompresses a stream of data, verifies its integrity using checksums, and handles errors appropriately. # Question: Implement a class `ZlibStreamHandler` that provides methods to compress and decompress a stream of data using the `zlib` library. Additionally, ensure data integrity by computing and verifying checksums for both compressed and uncompressed data. **Class Definition**: ```python class ZlibStreamHandler: def __init__(self, compression_level=-1): Initialize the handler with the given compression level. :param compression_level: An integer from 0 to 9 or -1 for default level. 1 is fastest, 9 is best compression. Default is zlib.Z_DEFAULT_COMPRESSION. pass def compress_stream(self, data_stream): Compresses a stream of data. :param data_stream: A list of byte strings to be compressed. Each element in the list represents a chunk of data. :return: A tuple containing: 1. The compressed data as a single byte string. 2. The checksum of the uncompressed data (Adler-32). pass def decompress_stream(self, compressed_data, original_checksum): Decompresses the compressed data and verifies its integrity. :param compressed_data: A byte string of compressed data. :param original_checksum: The Adler-32 checksum of the original uncompressed data. :return: The original uncompressed data as a byte string. :raises zlib.error: If decompression fails or checksum does not match. pass ``` **Input**: 1. `compression_level` (integer): Compression level to be used. 2. `data_stream` (list of byte strings): A list where each element is a chunk of data to be compressed. 3. `compressed_data` (byte string): The compressed data. 4. `original_checksum` (integer): The Adler-32 checksum of the original uncompressed data. **Output**: 1. `compress_stream`: - A tuple of `(compressed_data, original_checksum)`: - `compressed_data` (byte string): The compressed result. - `original_checksum` (integer): The Adler-32 checksum of the original uncompressed data. 2. `decompress_stream`: - The uncompressed data (byte string). - Raises `zlib.error` if decompression fails or if the checksum does not match the original. **Constraints**: 1. The list of byte strings (`data_stream`) can be of any length, but the individual byte strings should not exceed 1MB each. 2. The compressed data must be verified against the checksum to ensure integrity. **Example Usage**: ```python handler = ZlibStreamHandler(compression_level=6) data_stream = [b\'This is a test.\', b\'Another piece of data.\', b\'Yet another chunk.\'] compressed_data, checksum = handler.compress_stream(data_stream) print(f\'Checksum: {checksum}\') try: original_data = handler.decompress_stream(compressed_data, checksum) print(f\'Original Data: {original_data}\') except zlib.error as e: print(f\'Error: {e}\') ``` # Notes: 1. Please ensure your solution handles error conditions gracefully, such as invalid compression levels or corrupted data. 2. Use the `zlib` library for all compression and decompression operations.","solution":"import zlib class ZlibStreamHandler: def __init__(self, compression_level=-1): Initialize the handler with the given compression level. :param compression_level: An integer from 0 to 9 or -1 for default level. 1 is fastest, 9 is best compression. Default is zlib.Z_DEFAULT_COMPRESSION. self.compression_level = compression_level def compress_stream(self, data_stream): Compresses a stream of data. :param data_stream: A list of byte strings to be compressed. Each element in the list represents a chunk of data. :return: A tuple containing: 1. The compressed data as a single byte string. 2. The checksum of the uncompressed data (Adler-32). compressor = zlib.compressobj(self.compression_level) compressed_data = b\'\' original_data = b\'\'.join(data_stream) for chunk in data_stream: compressed_data += compressor.compress(chunk) compressed_data += compressor.flush() checksum = zlib.adler32(original_data) return compressed_data, checksum def decompress_stream(self, compressed_data, original_checksum): Decompresses the compressed data and verifies its integrity. :param compressed_data: A byte string of compressed data. :param original_checksum: The Adler-32 checksum of the original uncompressed data. :return: The original uncompressed data as a byte string. :raises zlib.error: If decompression fails or checksum does not match. decompressor = zlib.decompressobj() decompressed_data = decompressor.decompress(compressed_data) decompressed_data += decompressor.flush() checksum = zlib.adler32(decompressed_data) if checksum != original_checksum: raise zlib.error(\\"Decompressed data checksum does not match original checksum\\") return decompressed_data"},{"question":"**Question: Implement a Database Manager using the `dbm` module** You are required to implement a `DatabaseManager` class that encapsulates the functionality of the `dbm` module to manage a simple key-value store. The class should provide methods to interact with the database, handling different database types appropriately. # Requirements: 1. Implement a class `DatabaseManager` with the following methods: - `__init__(self, file_path: str, flag: str = \'c\', mode: int = 0o666)`: Initialize the database with the provided `file_path`, `flag`, and `mode`. - `set_item(self, key: str, value: str) -> None`: Store a key-value pair in the database. - `get_item(self, key: str, default: str = None) -> str`: Retrieve a value for the given key from the database. If the key does not exist, return the `default` value. - `delete_item(self, key: str) -> None`: Delete a key-value pair from the database. If the database is opened in read-only mode and deletion is attempted, catch the exception and handle it gracefully. - `list_keys(self) -> list`: Return a list of all keys in the database. - `close(self) -> None`: Close the database connection. 2. Use appropriate error handling where necessary, especially for cases where keys do not exist or operations are not permitted (e.g., modifying a read-only database). 3. Ensure that the keys and values are always stored as bytes in the database, as required by the `dbm` module. # Constraints: - The keys and values stored in the database should be strings. - You are not allowed to use any third-party libraries. - The class should handle different database backends (`dbm.gnu`, `dbm.ndbm`, and `dbm.dumb`) transparently. # Example Usage: ```python # Initialize the database manager db_manager = DatabaseManager(\'my_database\', \'c\') # Set items in the database db_manager.set_item(\'name\', \'John Doe\') db_manager.set_item(\'email\', \'john@example.com\') # Retrieve items from the database print(db_manager.get_item(\'name\')) # Output: John Doe print(db_manager.get_item(\'age\', \'Unknown\')) # Output: Unknown # List all keys in the database print(db_manager.list_keys()) # Output: [\'name\', \'email\'] # Delete an item from the database db_manager.delete_item(\'email\') # Close the database db_manager.close() ``` # Additional Note: Ensure your code is well-structured and follows standard coding conventions. Write comments where necessary to explain your logic.","solution":"import dbm class DatabaseManager: def __init__(self, file_path: str, flag: str = \'c\', mode: int = 0o666): self.db = dbm.open(file_path, flag, mode) def set_item(self, key: str, value: str) -> None: self.db[key.encode(\'utf-8\')] = value.encode(\'utf-8\') def get_item(self, key: str, default: str = None) -> str: try: return self.db[key.encode(\'utf-8\')].decode(\'utf-8\') except KeyError: return default def delete_item(self, key: str) -> None: try: del self.db[key.encode(\'utf-8\')] except (KeyError, dbm.error): pass def list_keys(self) -> list: return [key.decode(\'utf-8\') for key in self.db.keys()] def close(self) -> None: self.db.close()"},{"question":"# Advanced Telnet Client Implementation Objective Implement a simplified Telnet client using the `telnetlib` module that connects to a specified Telnet server, logs in with provided credentials, and performs a series of commands before disconnecting. This will demonstrate your understanding of networking, protocol handling, and interactive sessions in Python. Task Details You need to create a class `SimpleTelnetClient` that: 1. Connects to a Telnet server. 2. Logs in with a username and password. 3. Executes a series of commands. 4. Collects the responses for each command and returns them. Class Details **Class Name**: `SimpleTelnetClient` **Methods:** 1. `__init__(self, host: str, port: int = 23, timeout: int = 10)` - Initialize with server host, optional port, and optional timeout. 2. `connect(self)` - Establishes the connection to the server. 3. `login(self, username: str, password: str) -> bool` - Logs in using the given credentials. Returns `True` if login is successful, otherwise `False`. 4. `execute_commands(self, commands: list) -> list` - Takes a list of string commands, executes them sequentially, and returns their responses as a list of strings. 5. `disconnect(self)` - Closes the connection to the server. Usage Example ```python client = SimpleTelnetClient(host=\'localhost\') client.connect() login_success = client.login(username=\'user\', password=\'pass\') if login_success: responses = client.execute_commands([\'ls\', \'pwd\', \'whoami\']) for response in responses: print(response) client.disconnect() ``` Constraints - Use the `telnetlib` module for all interactions. - Assume standard commands for a UNIX-like Telnet server. - Handle potential exceptions and errors gracefully. - Ensure the solution is efficient and respects the provided timeout for operations. Performance Requirements - The solution should handle up to 10 commands in one execution efficiently. - Properly handle network interruptions and provide meaningful error messages. Input Format - **Initialization**: - `host` string, - optional `port` integer, - optional `timeout` integer. - **Methods**: - `login`: `username` and `password` strings. - `execute_commands`: List of string commands. Output Format - **login**: Boolean indicating success or failure. - **execute_commands**: List of string responses. Example ```python client = SimpleTelnetClient(\'localhost\', 23) client.connect() assert client.login(\'user\', \'password\') == True assert client.execute_commands([\'ls\']) == [\'file1nfile2n\'] client.disconnect() ``` Ensure your implementation handles network errors robustly and provides meaningful log messages for better debugging.","solution":"import telnetlib class SimpleTelnetClient: def __init__(self, host: str, port: int = 23, timeout: int = 10): self.host = host self.port = port self.timeout = timeout self.connection = None def connect(self): try: self.connection = telnetlib.Telnet(self.host, self.port, self.timeout) except Exception as e: print(f\\"Failed to connect to {self.host}:{self.port}. Error: {e}\\") raise def login(self, username: str, password: str) -> bool: if not self.connection: print(\\"Not connected to the server.\\") return False try: self.connection.read_until(b\\"login: \\") self.connection.write(username.encode(\'ascii\') + b\\"n\\") self.connection.read_until(b\\"Password: \\") self.connection.write(password.encode(\'ascii\') + b\\"n\\") response = self.connection.read_until(b\\"\\", self.timeout) if b\\"\\" in response: return True else: return False except Exception as e: print(f\\"Login failed. Error: {e}\\") return False def execute_commands(self, commands: list) -> list: if not self.connection: print(\\"Not connected to the server.\\") return [] responses = [] try: for command in commands: self.connection.write(command.encode(\'ascii\') + b\\"n\\") response = self.connection.read_until(b\\"\\", self.timeout).decode(\'ascii\') responses.append(response) return responses except Exception as e: print(f\\"Failed to execute commands. Error: {e}\\") return [] def disconnect(self): if self.connection: self.connection.close() self.connection = None # Example usage: # client = SimpleTelnetClient(\'localhost\', 23) # client.connect() # if client.login(\'user\', \'password\'): # responses = client.execute_commands([\'ls\', \'pwd\', \'whoami\']) # for response in responses: # print(response) # client.disconnect()"},{"question":"**Question: Extract and Process ZIP File Headers using Multithreading** *Objective*: Implement a Python function that extracts and processes the header information of files contained in a ZIP file using multithreading. This function will demonstrate your understanding of binary data manipulation with the `struct` module and multithreading using the `threading` module. **Function Specifications**: - **Function Name**: `extract_and_process_headers` - **Input**: - `zip_file_path` (str): The path to the ZIP file to be processed. - **Output**: - A list of dictionaries where each dictionary contains header information for a file within the ZIP file. Each dictionary should have the keys: `filename`, `crc32`, `compressed_size`, `uncompressed_size`. **Constraints**: - The function should create and start a separate thread for each file header it processes. - You may assume that the ZIP file contains no more than 100 files. - Handle any potential issues with thread synchronization appropriately. **Example**: ```python def extract_and_process_headers(zip_file_path: str) -> list: import struct import threading class HeaderThread(threading.Thread): def __init__(self, data, start): threading.Thread.__init__(self) self.data = data self.start = start self.result = {} def run(self): try: start = self.start start += 14 fields = struct.unpack(\'<IIIHH\', self.data[start:start + 16]) crc32, comp_size, uncomp_size, filenamesize, extra_size = fields start += 16 filename = self.data[start:start + filenamesize].decode(\'utf-8\') start += filenamesize self.result = { \'filename\': filename, \'crc32\': hex(crc32), \'compressed_size\': comp_size, \'uncompressed_size\': uncomp_size } except Exception as e: print(f\\"An error occurred while processing: {e}\\") try: with open(zip_file_path, \'rb\') as f: data = f.read() start = 0 threads = [] results = [] while start < len(data): thread = HeaderThread(data, start) thread.start() threads.append(thread) start += 30 # Move to next header. Adjust accordingly for thread in threads: thread.join() results.append(thread.result) return results except Exception as e: print(f\\"An error occurred: {e}\\") return [] # Example call zip_headers = extract_and_process_headers(\'myfile.zip\') print(zip_headers) ``` **Notes**: - Ensure your implementation handles threading in a way that extracts all headers most efficiently. - The function should handle any potential exceptions during file reading or header processing gracefully, logging appropriate error messages.","solution":"import struct import threading import zipfile from typing import List, Dict def extract_and_process_headers(zip_file_path: str) -> List[Dict[str, str]]: class HeaderThread(threading.Thread): def __init__(self, member): threading.Thread.__init__(self) self.member = member self.result = {} def run(self): try: with zip_file.open(self.member) as f: self.result = { \'filename\': self.member.filename, \'crc32\': hex(self.member.CRC), \'compressed_size\': self.member.compress_size, \'uncompressed_size\': self.member.file_size } except Exception as e: print(f\\"An error occurred while processing {self.member.filename}: {e}\\") self.result = {} try: with zipfile.ZipFile(zip_file_path, \'r\') as zip_file: members = zip_file.infolist() threads = [] results = [] for member in members: thread = HeaderThread(member) thread.start() threads.append(thread) for thread in threads: thread.join() results.append(thread.result) return [result for result in results if result] except Exception as e: print(f\\"An error occurred: {e}\\") return []"},{"question":"# Python Coding Assessment Question: Using the `pipes` Module This coding question will test your understanding of the Python `pipes` module, specifically the `Template` class and its methods. Although the module is deprecated, it is useful to understand its functionality and to demonstrate pipeline abstractions in Python. Objective You are required to create a data processing pipeline using the `pipes` module. The pipeline should read a file, transform the content using multiple shell commands, and save the transformed content into a new file. Requirements 1. **Input File**: The pipeline should read from a file `input.txt` located in the same directory as your script. 2. **Transformations**: - Convert all text from lowercase to uppercase. - Replace all spaces with underscores. 3. **Output File**: The transformed content should be saved into a new file `output.txt` in the same directory. 4. You need to handle file reading, writing, and necessary transformations using the `pipes.Template` class. Constraints - The `input.txt` file contains plain text with multiple lines. - Ensure that your solution is efficient and handles large files gracefully. Example Suppose `input.txt` contains the following text: ``` hello world this is a test ``` After processing, `output.txt` should contain: ``` HELLO_WORLD THIS_IS_A_TEST ``` Function Definition `def process_pipeline() -> None:` - The function should not take any parameters. - The function should not return anything. - All file operations and transformations should be handled within this function. Performance Requirements - The solution should handle files with sizes up to 100 MB efficiently. Note - Ensure your script is compatible with Python 3.10. - If you\'re using an environment where shell commands may not behave exactly as expected, handle such scenarios gracefully or simulate them in Python. Provide the complete implementation of the function. **Your implementation starts below:** ```python import pipes def process_pipeline(): pipeline = pipes.Template() # Transform text to uppercase pipeline.append(\'tr a-z A-Z\', \'--\') # Replace spaces with underscores pipeline.append(\\"sed \'s/ /_/g\'\\", \'--\') with pipeline.open(\'output.txt\', \'w\') as f_out: with open(\'input.txt\', \'r\') as f_in: f_out.write(f_in.read()) # Run the function to process the pipeline process_pipeline() ```","solution":"import pipes def process_pipeline(): pipeline = pipes.Template() # Transform text to uppercase pipeline.append(\'tr a-z A-Z\', \'--\') # Replace spaces with underscores pipeline.append(\\"sed \'s/ /_/g\'\\", \'--\') with pipeline.open(\'output.txt\', \'w\') as f_out: with open(\'input.txt\', \'r\') as f_in: f_out.write(f_in.read()) # Uncomment the line below to run the function # process_pipeline()"},{"question":"# Command-Line Parsing with `argparse` You are required to implement a Python script that acts as a command-line tool to manage a to-do list. This tool should support multiple functionalities like adding, listing, and removing tasks. The `argparse` module should be used to handle and parse the command-line arguments. Each sub-command (add, list, remove) should have its own set of arguments. Requirements 1. **Add Task**: Adds a new task to the to-do list. - Command: `add` - Arguments: - `--title` (required): Title of the task. - `--description`: Description of the task. - `--priority` (choices: high, medium, low, default: medium): Priority level of the task. 2. **List Tasks**: Lists all tasks in the to-do list in a table format. - Command: `list` - Arguments: - `--sort-by` (choices: title, priority, default: title): Sort tasks by specified attribute. 3. **Remove Task**: Removes a task from the to-do list. - Command: `remove` - Arguments: - `--title` (required): Title of the task to be removed. 4. **Help Message**: Generate a helpful usage message for users. Expected Behaviors - When adding a task, it should print the task added message. - When listing tasks, it should print all tasks sorted by the specified attribute. - When removing a task, it should print the task removed message. Performance Requirements - The script should handle invalid inputs gracefully and provide appropriate error messages. Constraints - You should store tasks in-memory as a list of dictionaries for this exercise. - You are allowed to use only the `argparse` and `sys` modules from the standard library. - The script should be written in Python 3.6 or later. Example ```bash python todo.py add --title \\"Buy Groceries\\" --description \\"Buy milk, eggs, and bread\\" --priority high Task added: Buy Groceries python todo.py add --title \\"Read Book\\" Task added: Read Book python todo.py list --sort-by priority Priority | Title | Description ------------------------------------------------ High | Buy Groceries | Buy milk, eggs, and bread Medium | Read Book | python todo.py remove --title \\"Read Book\\" Task removed: Read Book ``` Submission Submit a single Python script (`todo.py`) that meets the above requirements. **Hint**: Use `argparse.ArgumentParser` to create command-line arguments and sub-commands.","solution":"import argparse # In-memory storage for tasks tasks = [] def add_task(args): task = { \'title\': args.title, \'description\': args.description if args.description else \'\', \'priority\': args.priority, } tasks.append(task) print(f\\"Task added: {args.title}\\") def list_tasks(args): sorted_tasks = sorted(tasks, key=lambda x: x[args.sort_by]) if not sorted_tasks: print(\\"No tasks to list.\\") return print(f\\"{\'Priority\':<10} | {\'Title\':<20} | {\'Description\':<40}\\") print(\'-\' * 70) for task in sorted_tasks: print(f\\"{task[\'priority\']:<10} | {task[\'title\']:<20} | {task[\'description\']:<40}\\") def remove_task(args): global tasks tasks = [task for task in tasks if task[\'title\'] != args.title] print(f\\"Task removed: {args.title}\\") def main(): parser = argparse.ArgumentParser(description=\\"To-do list manager\\") subparsers = parser.add_subparsers() # Add sub-command parser_add = subparsers.add_parser(\'add\', help=\'Add a new task\') parser_add.add_argument(\'--title\', required=True, help=\'Title of the task\') parser_add.add_argument(\'--description\', help=\'Description of the task\') parser_add.add_argument(\'--priority\', choices=[\'high\', \'medium\', \'low\'], default=\'medium\', help=\'Priority of the task\') parser_add.set_defaults(func_add=add_task) # List sub-command parser_list = subparsers.add_parser(\'list\', help=\'List all tasks\') parser_list.add_argument(\'--sort-by\', choices=[\'title\', \'priority\'], default=\'title\', help=\'Sort tasks by specified attribute\') parser_list.set_defaults(func_list=list_tasks) # Remove sub-command parser_remove = subparsers.add_parser(\'remove\', help=\'Remove a task\') parser_remove.add_argument(\'--title\', required=True, help=\'Title of the task to remove\') parser_remove.set_defaults(func_remove=remove_task) args = parser.parse_args() # Call the appropriate function based on sub-command if \'func_add\' in args: args.func_add(args) elif \'func_list\' in args: args.func_list(args) elif \'func_remove\' in args: args.func_remove(args) else: parser.print_help() if __name__ == \\"__main__\\": main()"},{"question":"# CSV File Manipulation with Python\'s csv Module You are given two CSV files: `employees.csv` and `departments.csv`. Each row in the `employees.csv` file contains information about an employee, and each row in the `departments.csv` file contains information about a department. You need to write a Python script that reads data from these files, processes the data, and writes the results to a new CSV file. Input Format 1. **employees.csv**: ```csv employee_id,first_name,last_name,department_id,salary 1,John,Doe,101,50000 2,Jane,Smith,102,60000 3,Emily,Jones,101,55000 4,Michael,Brown,103,45000 5,Jessica,Wilson,104,70000 ``` 2. **departments.csv**: ```csv department_id,department_name,location 101,Sales,New York 102,Engineering,San Francisco 103,HR,Chicago 104,Marketing,Los Angeles ``` Output Requirements You need to perform the following tasks: 1. Compute the total salary paid in each department. 2. Merge the employee and department data and write to a new CSV file `merged_data.csv`. The new file should contain the following columns: - employee_id - first_name - last_name - department_name - location - salary Constraints - You must use the `csv` module for reading and writing CSV files. - Handle any possible exceptions that could occur during file operations. Function Signature ```python def process_csv_files(employees_csv: str, departments_csv: str, output_csv: str) -> None: # Your code here ``` Example Given the input files as described, the output in `merged_data.csv` should look like: ```csv employee_id,first_name,last_name,department_name,location,salary 1,John,Doe,Sales,New York,50000 2,Jane,Smith,Engineering,San Francisco,60000 3,Emily,Jones,Sales,New York,55000 4,Michael,Brown,HR,Chicago,45000 5,Jessica,Wilson,Marketing,Los Angeles,70000 ``` You should also print the following for total salary computation (format is an example and can be represented differently): ``` Total salary paid in each department: Sales: 105000 Engineering: 60000 HR: 45000 Marketing: 70000 ``` You can assume the input CSV files are correctly formatted and contain no missing values.","solution":"import csv from collections import defaultdict def process_csv_files(employees_csv: str, departments_csv: str, output_csv: str) -> None: # Read departments data departments = {} with open(departments_csv, mode=\'r\') as dep_file: dep_reader = csv.DictReader(dep_file) for row in dep_reader: departments[row[\'department_id\']] = { \'department_name\': row[\'department_name\'], \'location\': row[\'location\'] } # Compute total salary and merge data total_salary_by_dept = defaultdict(int) merged_data = [] with open(employees_csv, mode=\'r\') as emp_file: emp_reader = csv.DictReader(emp_file) for row in emp_reader: dept_id = row[\'department_id\'] salary = int(row[\'salary\']) total_salary_by_dept[departments[dept_id][\'department_name\']] += salary merged_data.append({ \'employee_id\': row[\'employee_id\'], \'first_name\': row[\'first_name\'], \'last_name\': row[\'last_name\'], \'department_name\': departments[dept_id][\'department_name\'], \'location\': departments[dept_id][\'location\'], \'salary\': row[\'salary\'] }) # Write merged data to output CSV with open(output_csv, mode=\'w\', newline=\'\') as out_file: fieldnames = [\'employee_id\', \'first_name\', \'last_name\', \'department_name\', \'location\', \'salary\'] writer = csv.DictWriter(out_file, fieldnames=fieldnames) writer.writeheader() writer.writerows(merged_data) # Print total salary computation print(\\"Total salary paid in each department:\\") for department, total_salary in total_salary_by_dept.items(): print(f\\"{department}: {total_salary}\\")"},{"question":"# Python Coding Question: **Resource Usage Summary Generator** You are tasked with creating a function to monitor and summarize the resource usage of the current Python process. This requires using the `resource` module to collect detailed usage statistics. Your function should focus on gathering this information and presenting it in a clear and structured format. Function Signature: ```python def resource_usage_summary() -> dict: pass ``` Objectives: 1. Use the `resource.getrusage()` function to collect resource usage statistics. 2. Organize the collected data into a dictionary with clear, human-readable keys. 3. Include the following resource usage metrics in your dictionary: - User CPU time used (in seconds) - System CPU time used (in seconds) - Maximum resident set size (in kilobytes) - Shared memory size (in kilobytes) - Unshared memory size (in kilobytes) - Unshared stack size (in kilobytes) - Page faults not requiring I/O - Page faults requiring I/O - Number of swap operations - Block input operations - Block output operations - Voluntary context switches - Involuntary context switches Input: - None Output: - A dictionary with the specified resource usage metrics, as detailed above. Example: ```python { \\"user_cpu_time\\": 0.25, \\"system_cpu_time\\": 0.05, \\"max_resident_set_size\\": 3000, \\"shared_memory_size\\": 1024, \\"unshared_memory_size\\": 512, \\"unshared_stack_size\\": 256, \\"page_faults_no_io\\": 20, \\"page_faults_io\\": 5, \\"num_swap_operations\\": 2, \\"block_input_ops\\": 10, \\"block_output_ops\\": 8, \\"voluntary_context_switches\\": 100, \\"involuntary_context_switches\\": 50 } ``` Constraints and Considerations: - Only the specified resource usage metrics need to be included in the output dictionary. - Ensure that the data is collected safely and without causing significant performance overhead. - The function should not raise any exceptions if the data collection is successful. Good luck!","solution":"import resource def resource_usage_summary() -> dict: usage = resource.getrusage(resource.RUSAGE_SELF) summary = { \\"user_cpu_time\\": usage.ru_utime, \\"system_cpu_time\\": usage.ru_stime, \\"max_resident_set_size\\": usage.ru_maxrss, \\"shared_memory_size\\": usage.ru_ixrss, \\"unshared_memory_size\\": usage.ru_idrss, \\"unshared_stack_size\\": usage.ru_isrss, \\"page_faults_no_io\\": usage.ru_minflt, \\"page_faults_io\\": usage.ru_majflt, \\"num_swap_operations\\": usage.ru_nswap, \\"block_input_ops\\": usage.ru_inblock, \\"block_output_ops\\": usage.ru_oublock, \\"voluntary_context_switches\\": usage.ru_nvcsw, \\"involuntary_context_switches\\": usage.ru_nivcsw } return summary"},{"question":"You are tasked with creating a Python class that emulates an advanced task manager with various functionalities. Each task in this manager will have a unique identifier, description, and status. You need to demonstrate the following concepts in your implementation: - Class instantiation and usage. - Assignment statements and augmented assignments. - Annotations and asserting conditions. - Control flow with break and continue statements. - Handling of exceptions with raise statements. - Importing modules. Detailed Requirements 1. **Create a class `TaskManager:`** - This class should initialize with an empty dictionary to hold the tasks. 2. **Task Structure:** - Each task must have a unique identifier (`task_id`), a description (`description`), and a status (`status`). The status should be either `\'active\'` or `\'inactive\'`. 3. **Methods:** - `add_task(self, task_id: int, description: str) -> None:` This method should add a task to the manager. If the `task_id` already exists, raise a `ValueError`. - `remove_task(self, task_id: int) -> None:` This method should remove a task from the manager. If the `task_id` does not exist, raise a `KeyError`. - `update_status(self, task_id: int, status: str) -> None:` This method should update the status of a task. Use an `assert` statement to ensure the status is either `\'active\'` or `\'inactive\'`. If the `task_id` does not exist, raise a `KeyError`. - `active_tasks(self) -> dict:` This method should return a dictionary of all tasks with an `\'active\'` status. - `import_tasks(self, filename: str) -> None:` This method should import tasks from a CSV file with columns: `task_id`, `description`, `status`. If `task_id` already exists, it should overwrite the existing task. 4. **Additional Tasks:** - Ensure the class has a well-defined `__str__` method that prints out all the tasks in a human-readable format. - Use `augmented assignment` in at least one place in the code. - Ensure proper use of `annotations` and `assert` to maintain integrity of your data. Constraints and Performance Requirements Constraints: - `task_id` is a positive integer. - `description` is a non-empty string. - `status` is either `\'active\'` or `\'inactive\'`. Performance: - The methods should handle up to 10,000 tasks efficiently. - The import functionality should be implemented with minimal time complexity. Implement the `TaskManager` class adhering to these specifications.","solution":"import csv class TaskManager: def __init__(self): Initialize the TaskManager with an empty dictionary to hold tasks. self.tasks = {} def add_task(self, task_id: int, description: str) -> None: Add a task to the TaskManager. Raises ValueError if task_id already exists. if task_id in self.tasks: raise ValueError(f\\"Task ID {task_id} already exists.\\") self.tasks[task_id] = { \'description\': description, \'status\': \'inactive\' } def remove_task(self, task_id: int) -> None: Remove a task from the TaskManager. Raises KeyError if task_id does not exist. if task_id not in self.tasks: raise KeyError(f\\"Task ID {task_id} does not exist.\\") del self.tasks[task_id] def update_status(self, task_id: int, status: str) -> None: Update the status of a task. Raises KeyError if task_id does not exist and AssertionError if status is not valid. assert status in [\'active\', \'inactive\'], \\"Status must be either \'active\' or \'inactive\'.\\" if task_id not in self.tasks: raise KeyError(f\\"Task ID {task_id} does not exist.\\") self.tasks[task_id][\'status\'] = status def active_tasks(self) -> dict: Return a dictionary of all tasks with an \'active\' status. return {task_id: task for task_id, task in self.tasks.items() if task[\'status\'] == \'active\'} def import_tasks(self, filename: str) -> None: Import tasks from a CSV file. Overwrites existing tasks with the same task_id. with open(filename, \'r\') as file: reader = csv.DictReader(file) for row in reader: task_id = int(row[\'task_id\']) description = row[\'description\'] status = row[\'status\'] assert status in [\'active\', \'inactive\'], \\"Status must be either \'active\' or \'inactive\'.\\" self.tasks[task_id] = { \'description\': description, \'status\': status } def __str__(self) -> str: Return all the tasks in a human-readable format. tasks_str = \'n\'.join([f\\"Task ID: {task_id}, Description: {task[\'description\']}, Status: {task[\'status\']}\\" for task_id, task in self.tasks.items()]) return f\\"Tasks:n{tasks_str}\\""},{"question":"You are required to implement a function that parses an email message and provides a summary of its content. # Function Signature ```python def parse_email_summary(input_source: Union[str, bytes, IO], is_binary: bool = True) -> dict: pass ``` # Input - `input_source`: This can be one of: * A string representing the text of an email message. * Bytes representing the binary content of an email message. * An IO-like object representing a file containing the text of an email message. - `is_binary`: A boolean flag indicating whether the `input_source` is binary (`True`) or text (`False`). Default is `True`. # Output - A dictionary containing the following keys: * `subject`: The subject of the email as a string. * `from`: The sender of the email as a string. * `to`: The recipient(s) of the email as a list of strings. * `is_multipart`: A boolean indicating whether the email is multipart. * `parts`: The number of parts if the email is multipart, 0 otherwise. * `defects`: A list of defects found in the email, if any; otherwise an empty list. # Constraints - You should handle the input source flexibly, detecting and parsing correctly whether it is a string, bytes, or a file object. - Assume the input email follows standard email formatting guidelines as per RFC 5322. # Example Usage ```python email_text = Subject: Test email From: sender@example.com To: recipient@example.com This is a test email. result = parse_email_summary(email_text, is_binary=False) print(result) # Output should be something like: # { # \\"subject\\": \\"Test email\\", # \\"from\\": \\"sender@example.com\\", # \\"to\\": [\\"recipient@example.com\\"], # \\"is_multipart\\": False, # \\"parts\\": 0, # \\"defects\\": [] # } ``` # Additional Notes - You might use the `email.message_from_string`, `email.message_from_bytes`, or `email.message_from_file` functions, based on the input type. - Utilize the `is_multipart` method to check if the email is multipart and `iter_parts` to count the parts if it is multipart. - Check the `defects` attribute of the message object to gather any defects.","solution":"from typing import Union, IO import email from email.parser import BytesParser, Parser from email.policy import default def parse_email_summary(input_source: Union[str, bytes, IO], is_binary: bool = True) -> dict: if is_binary: if isinstance(input_source, bytes): msg = BytesParser(policy=default).parsebytes(input_source) else: msg = BytesParser(policy=default).parse(input_source) else: if isinstance(input_source, str): msg = Parser(policy=default).parsestr(input_source) else: msg = Parser(policy=default).parse(input_source) summary = { \\"subject\\": msg.get(\\"subject\\", \\"\\"), \\"from\\": msg.get(\\"from\\", \\"\\"), \\"to\\": msg.get_all(\\"to\\", []), \\"is_multipart\\": msg.is_multipart(), \\"parts\\": sum(1 for _ in msg.iter_parts()) if msg.is_multipart() else 0, \\"defects\\": msg.defects } return summary"},{"question":"You are given a DataFrame containing two columns: `A` and `B`. Some entries in these columns are nullable Boolean values (True, False, or pandas.NA). Your task is to implement a function `process_dataframe(df: pd.DataFrame) -> pd.DataFrame` that performs the following operations: 1. Create a new column `C` which is the result of the logical AND (`&`) operation between columns `A` and `B` following BooleanArray Kleene logic. 2. Create another new column `D` which is the result of the logical OR (`|`) operation between columns `A` and `B`. 3. Fill any NA values in column `C` with False. 4. Fill any NA values in column `D` with True. 5. Return the modified DataFrame. # Input Format - A DataFrame `df` with columns `A` and `B` containing nullable Boolean values. # Output Format - The modified DataFrame with added columns `C` and `D` as described. # Example Input: ```python import pandas as pd df = pd.DataFrame({ \'A\': pd.Series([True, False, pd.NA], dtype=\\"boolean\\"), \'B\': pd.Series([pd.NA, True, False], dtype=\\"boolean\\") }) ``` Output: ```python process_dataframe(df) ``` Resulting DataFrame: ``` A B C D 0 True <NA> <NA> True 1 False True False True 2 <NA> False False True ``` # Constraints - You must use pandas\' BooleanArray and ensure your solution adheres to Kleene logic for any logical operations. - Handle NA values appropriately as described in steps 3 and 4. # Performance Requirements - The function should efficiently handle DataFrames with up to 100,000 rows. # Implementation ```python def process_dataframe(df: pd.DataFrame) -> pd.DataFrame: # Your code here pass ```","solution":"import pandas as pd def process_dataframe(df: pd.DataFrame) -> pd.DataFrame: Processes the DataFrame by adding two new columns \'C\' and \'D\'. Column \'C\' is the logical AND of columns \'A\' and \'B\'. Column \'D\' is the logical OR of columns \'A\' and \'B\'. NA values in column \'C\' are filled with False. NA values in column \'D\' are filled with True. df[\'C\'] = df[\'A\'] & df[\'B\'] df[\'D\'] = df[\'A\'] | df[\'B\'] df[\'C\'] = df[\'C\'].fillna(False) df[\'D\'] = df[\'D\'].fillna(True) return df"},{"question":"# Question: Naive Bayes Classifier Implementation and Comparison You are provided with a dataset consisting of various features and a target variable. Your task is to write a Python function that preprocesses the data, applies at least two different Naive Bayes classifiers, and compares their performance. Specifically, you need to implement the following steps: 1. **Data Preprocessing**: - Handle any missing values by either imputing them or dropping the affected rows. - Split the data into training and test sets. - Normalize the features if necessary (e.g., for GaussianNB). 2. **Model Training and Prediction**: - Train two different Naive Bayes classifiers from scikit-learn (e.g., GaussianNB and MultinomialNB). - Use the models to predict the target variable on the test set. 3. **Model Evaluation**: - Evaluate the performance of both models using appropriate metrics (e.g., accuracy, precision, recall, F1-score). - Print the performance metrics and compare the results. The function signature should be: ```python def compare_naive_bayes_classifiers(X: pd.DataFrame, y: pd.Series) -> None: Compare the performance of two different Naive Bayes classifiers on a given dataset. Parameters: X (pd.DataFrame): Feature matrix y (pd.Series): Target variable Returns: None pass ``` # Example Usage: ```python import pandas as pd from sklearn.datasets import load_iris # Load example data data = load_iris() X = pd.DataFrame(data.data, columns=data.feature_names) y = pd.Series(data.target) # Call the function compare_naive_bayes_classifiers(X, y) ``` Ensure your implementation handles data preprocessing appropriately and includes comments explaining your steps. # Constraints: - You must use at least two different Naive Bayes classifiers. - You should use the train_test_split method to split data into 80% training and 20% testing. - You need to use appropriate metrics to evaluate and compare model performance. # Additional Information: - You can assume the dataset does not contain categorical features directly, but if needed, you can convert them using techniques such as one-hot encoding for the MultinomialNB classifier. - You may utilize any standard libraries such as `pandas`, `numpy`, and `scikit-learn`.","solution":"import pandas as pd import numpy as np from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.naive_bayes import GaussianNB, MultinomialNB from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score def compare_naive_bayes_classifiers(X: pd.DataFrame, y: pd.Series) -> None: Compare the performance of two different Naive Bayes classifiers on a given dataset. Parameters: X (pd.DataFrame): Feature matrix y (pd.Series): Target variable Returns: None # Handle any missing values by dropping the affected rows X = X.dropna() y = y.loc[X.index] # Split the data into training and test sets (80% train, 20% test) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Normalize the features for GaussianNB scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) # Train GaussianNB gn_classifier = GaussianNB() gn_classifier.fit(X_train_scaled, y_train) y_pred_gn = gn_classifier.predict(X_test_scaled) # Collect performance metrics for GaussianNB gn_accuracy = accuracy_score(y_test, y_pred_gn) gn_precision = precision_score(y_test, y_pred_gn, average=\'weighted\') gn_recall = recall_score(y_test, y_pred_gn, average=\'weighted\') gn_f1 = f1_score(y_test, y_pred_gn, average=\'weighted\') # Train MultinomialNB mn_classifier = MultinomialNB() mn_classifier.fit(X_train, y_train) # MultinomialNB does not require scaling y_pred_mn = mn_classifier.predict(X_test) # Collect performance metrics for MultinomialNB mn_accuracy = accuracy_score(y_test, y_pred_mn) mn_precision = precision_score(y_test, y_pred_mn, average=\'weighted\') mn_recall = recall_score(y_test, y_pred_mn, average=\'weighted\') mn_f1 = f1_score(y_test, y_pred_mn, average=\'weighted\') # Print performance metrics and compare the results print(\\"GaussianNB Performance:\\") print(f\\"Accuracy: {gn_accuracy:.4f}\\") print(f\\"Precision: {gn_precision:.4f}\\") print(f\\"Recall: {gn_recall:.4f}\\") print(f\\"F1-score: {gn_f1:.4f}\\") print(\\"nMultinomialNB Performance:\\") print(f\\"Accuracy: {mn_accuracy:.4f}\\") print(f\\"Precision: {mn_precision:.4f}\\") print(f\\"Recall: {mn_recall:.4f}\\") print(f\\"F1-score: {mn_f1:.4f}\\")"},{"question":"**Q1. Implementing the CoW principles:** In this coding assessment, you will be tasked with implementing several operations on pandas DataFrames that adhere to the Copy-on-Write (CoW) principles. Your goal is to write a function to perform a series of operations and ensure isolation of changes among DataFrames as dictated by CoW. # Function Signature: ```python def enforce_copy_on_write(df: pd.DataFrame) -> (pd.DataFrame, pd.DataFrame, pd.DataFrame): pass ``` # Requirements: 1. **Input:** - A single pandas DataFrame `df` with at least 3 columns and several rows. 2. **Output:** - You must return a tuple of three DataFrames resulting from the operations listed below. Changes to one DataFrame should not affect any other. 3. **Operations:** 1. **`df_processed`**: Create a DataFrame by resetting the index of the input DataFrame `df` and setting the `drop=True` flag. Assign a new value to the element at the first row and first column of this DataFrame. 2. **`df_subset`**: Extract a subset of `df` by selecting the first column. Modify a value in this subset and ensure `df` remains unchanged. 3. **`df_modified`**: Create a DataFrame by renaming the columns of `df`. Ensure that the renaming does not affect the original DataFrame `df`. # Constraints: 1. Solutions should optimally comply with CoW principles where necessary. 2. Avoid using inplace operations that violate CoW rules. # Performance: - The function should be efficient in terms of memory and execution time when handling large DataFrames, owing to the inherent optimizations enforced by CoW. # Example: ```python import pandas as pd df = pd.DataFrame({ \\"A\\": [1, 2, 3], \\"B\\": [4, 5, 6], \\"C\\": [7, 8, 9] }) df_processed, df_subset, df_modified = enforce_copy_on_write(df) # Expected outputs could be # df_processed with modified first cell # df_subset with modified value and ensuring df is not changed # df_modified with renamed columns ensuring df is not changed print(df) print(df_processed) print(df_subset) print(df_modified) ``` Your solution should explicitly demonstrate the working principles of CoW and ensure that the function output reflects isolated changes among the DataFrames as per the CoW mechanism.","solution":"import pandas as pd def enforce_copy_on_write(df: pd.DataFrame) -> (pd.DataFrame, pd.DataFrame, pd.DataFrame): # Operation 1: Create df_processed by resetting the index df_processed = df.reset_index(drop=True).copy() df_processed.iloc[0, 0] = df_processed.iloc[0, 0] + 1 # Modify the first row and first column # Operation 2: Create df_subset and modify a value df_subset = df.iloc[:, [0]].copy() # Select the first column df_subset.iloc[0, 0] = df_subset.iloc[0, 0] + 10 # Modify the first value # Operation 3: Create df_modified by renaming the columns df_modified = df.rename(columns={col: f\\"new_{col}\\" for col in df.columns}).copy() return df_processed, df_subset, df_modified"},{"question":"# Complex Numbers and Linear Algebra Operations in PyTorch You are required to create a function `tensor_operations` that performs a series of operations on complex tensors. This function will demonstrate your understanding of complex tensor creation, manipulation, and linear algebra operations in PyTorch. Function Signature ```python def tensor_operations(real_matrix: torch.Tensor, imag_matrix: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]: pass ``` Parameters - `real_matrix` (torch.Tensor): A 2D tensor representing the real part of a complex matrix. - `imag_matrix` (torch.Tensor): A 2D tensor representing the imaginary part of a complex matrix. Returns - (torch.Tensor, torch.Tensor): A tuple containing two tensors. The first tensor is the product of the complex matrix with a given complex vector, and the second tensor is the result of applying Singular Value Decomposition (SVD) to the same complex matrix. Constraints 1. The input matrices (`real_matrix` and `imag_matrix`) have the same shape `(m, n)`. 2. You can assume the input matrices are of dtype `torch.float`. Instructions 1. **Create a Complex Tensor**: Using `real_matrix` and `imag_matrix`, create a complex tensor. 2. **Complex Vector Multiplication**: Create a complex vector of appropriate size and perform matrix-vector multiplication with the complex matrix. 3. **Singular Value Decomposition**: Apply SVD to the complex matrix. 4. **Return Results**: Return the result of the matrix-vector multiplication and the SVD result. Example Here\'s an example to guide the implementation: ```python import torch def tensor_operations(real_matrix: torch.Tensor, imag_matrix: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]: # Step 1: Create the complex tensor complex_matrix = torch.complex(real_matrix, imag_matrix) # Step 2: Create a complex vector m, n = complex_matrix.shape complex_vector = torch.complex(torch.randn(n), torch.randn(n)) # Step 3: Perform matrix-vector multiplication product = torch.mv(complex_matrix, complex_vector) # Step 4: Apply SVD U, S, Vh = torch.linalg.svd(complex_matrix) # Step 5: Return results return product, (U, S, Vh) # Test the function real_matrix = torch.randn(3, 4) imag_matrix = torch.randn(3, 4) product, svd_result = tensor_operations(real_matrix, imag_matrix) print(\\"Product:\\", product) print(\\"SVD Result:\\", svd_result) ``` In this example, the function `tensor_operations` first constructs a complex tensor using `real_matrix` and `imag_matrix`. It then performs multiplication of this complex matrix with a randomly generated complex vector and applies SVD on the complex matrix. The results are returned as a tuple. Make sure to handle edge cases and ensure that your code is efficient. Your implementation should demonstrate thorough understanding of complex tensors in PyTorch.","solution":"import torch from typing import Tuple def tensor_operations(real_matrix: torch.Tensor, imag_matrix: torch.Tensor) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor, torch.Tensor]]: Perform complex tensor operations: matrix-vector multiplication and singular value decomposition. Parameters: - real_matrix (torch.Tensor): A 2D tensor representing the real part of a complex matrix. - imag_matrix (torch.Tensor): A 2D tensor representing the imaginary part of a complex matrix. Returns: - Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor, torch.Tensor]]: A tuple containing: - The product of the complex matrix with a given complex vector. - The result of applying Singular Value Decomposition (SVD) to the same complex matrix. # Step 1: Create the complex tensor complex_matrix = torch.complex(real_matrix, imag_matrix) # Step 2: Create a complex vector m, n = complex_matrix.shape complex_vector = torch.complex(torch.randn(n), torch.randn(n)) # Step 3: Perform matrix-vector multiplication product = torch.matmul(complex_matrix, complex_vector) # Step 4: Apply SVD U, S, Vh = torch.linalg.svd(complex_matrix) # Step 5: Return results return product, (U, S, Vh)"},{"question":"# Question: **Data Encoder-Decoder and Integrity Checker** You are tasked with developing a class named `EncoderDecoder` that provides utility methods for encoding data to various formats (Base64, Quoted-Printable, UUencode, and Hex) and decoding from these formats back to binary data. Additionally, implement a method to compute CRC-32 checksum of given data to ensure data integrity. Your task is to implement the following methods in the `EncoderDecoder` class: 1. **encode_to_base64(self, data: bytes) -> str**: Convert binary data to a Base64 encoded string. 2. **decode_from_base64(self, base64_str: str) -> bytes**: Convert a Base64 encoded string back to binary data. 3. **encode_to_qp(self, data: bytes) -> str**: Convert binary data to a Quoted-Printable encoded string. 4. **decode_from_qp(self, qp_str: str) -> bytes**: Convert a Quoted-Printable encoded string back to binary data. 5. **encode_to_uu(self, data: bytes) -> str**: Convert binary data to a UUencoded string. 6. **decode_from_uu(self, uu_str: str) -> bytes**: Convert a UUencoded string back to binary data. 7. **encode_to_hex(self, data: bytes) -> str**: Convert binary data to a Hexadecimal encoded string. 8. **decode_from_hex(self, hex_str: str) -> bytes**: Convert a Hexadecimal encoded string back to binary data. 9. **compute_crc32(self, data: bytes) -> int**: Compute the CRC-32 checksum of the given binary data. **Constraints:** - `data` should be a bytes-like object. - `base64_str`, `qp_str`, `uu_str`, and `hex_str` should only contain valid encoded data. **Example Usage:** ```python ed = EncoderDecoder() # Example data to encode and decode data = b\'example data\' # Base64 encoding and decoding base64_encoded = ed.encode_to_base64(data) print(base64_encoded) # Output: \'ZXhhbXBsZSBkYXRhn\' print(ed.decode_from_base64(base64_encoded)) # Output: b\'example data\' # Quoted-Printable encoding and decoding qp_encoded = ed.encode_to_qp(data) print(qp_encoded) # Output example: \'example data\' print(ed.decode_from_qp(qp_encoded)) # Output: b\'example data\' # UUencode and decode uu_encoded = ed.encode_to_uu(data) print(uu_encoded) # Output example: \'begin 666 <data>n...\' print(ed.decode_from_uu(uu_encoded)) # Output: b\'example data\' # Hex encoding and decoding hex_encoded = ed.encode_to_hex(data) print(hex_encoded) # Output: \'6578616d706c652064617461\' print(ed.decode_from_hex(hex_encoded)) # Output: b\'example data\' # CRC32 checksum print(ed.compute_crc32(data)) # Output: CRC32 integer, e.g., 3964322768 ``` Implement the `EncoderDecoder` class such that it correctly encodes and decodes binary data to and from the specified formats and computes CRC-32 checksums. **Notes:** - Ensure to handle invalid inputs gracefully by raising appropriate exceptions. - You can use the `binascii` module to achieve the required functionalities.","solution":"import base64 import quopri import uu import binascii import zlib from io import BytesIO class EncoderDecoder: def encode_to_base64(self, data: bytes) -> str: return base64.b64encode(data).decode(\'ascii\') def decode_from_base64(self, base64_str: str) -> bytes: return base64.b64decode(base64_str) def encode_to_qp(self, data: bytes) -> str: return quopri.encodestring(data).decode(\'ascii\') def decode_from_qp(self, qp_str: str) -> bytes: return quopri.decodestring(qp_str) def encode_to_uu(self, data: bytes) -> str: out = BytesIO() uu.encode(BytesIO(data), out) return out.getvalue().decode(\'ascii\') def decode_from_uu(self, uu_str: str) -> bytes: out = BytesIO() uu.decode(BytesIO(uu_str.encode(\'ascii\')), out) return out.getvalue() def encode_to_hex(self, data: bytes) -> str: return binascii.hexlify(data).decode(\'ascii\') def decode_from_hex(self, hex_str: str) -> bytes: return binascii.unhexlify(hex_str) def compute_crc32(self, data: bytes) -> int: return zlib.crc32(data)"},{"question":"Objective: Implement a function that recursively copies a directory tree from a source location to a destination location using the `shutil` module, but with added custom functionality: 1. Log the names of all files and directories being copied. 2. Optional filtering of files based on a size threshold. 3. Preserve all file metadata. Function Signature: ```python import shutil import os def custom_copytree(src: str, dst: str, log_file: str, size_threshold: int = None): Recursively copy an entire directory tree rooted at `src` to a directory named `dst`. Log the names of all files and directories being copied to a specified log file. Parameters ---------- src : str Source directory. dst : str Destination directory. log_file : str Path to the log file where operations will be logged. size_threshold : int, optional Files larger than this size (in bytes) will not be copied. If None, all files will be copied. Returns ------- str The path to the destination directory. Raises ------ FileNotFoundError If the source directory does not exist. OSError If copying fails for some reason. pass ``` Requirements: 1. **Logging**: - Create or open the specified log file and write the name of each file and directory being copied. 2. **Size Filtering**: - If `size_threshold` is provided, only copy files that are smaller than the specified size in bytes. 3. **Preserve Metadata**: - Preserve the permission bits, last access time, last modification time, and flags of the files and directories. 4. **Parameters and Return**: - `src` and `dst` are strings representing the source and destination directory paths. - `log_file` is a string representing the path to the log file. - `size_threshold` is an optional integer specifying the maximum file size in bytes for copying. If not provided, all files should be copied. - The function should return the path to the destination directory upon successful copying. 5. **Exception Handling**: - Raise a `FileNotFoundError` if the source directory does not exist. - Raise an `OSError` if there is a failure during the copying process. Example Usage: ```python # Example usage of custom_copytree try: custom_copytree(\\"src_directory\\", \\"dst_directory\\", \\"copy_log.txt\\", 1024*1024) print(\\"Copy successful!\\") except (FileNotFoundError, OSError) as e: print(f\\"Copy failed: {e}\\") ``` Use the `shutil` module functions such as `shutil.copy2`, `shutil.copystat`, `shutil.copytree`, and logging mechanisms provided by the `logging` module or simple file write operations to implement the solution.","solution":"import shutil import os import logging def custom_copytree(src: str, dst: str, log_file: str, size_threshold: int = None): Recursively copy an entire directory tree rooted at `src` to a directory named `dst`. Log the names of all files and directories being copied to a specified log file. Parameters ---------- src : str Source directory. dst : str Destination directory. log_file : str Path to the log file where operations will be logged. size_threshold : int, optional Files larger than this size (in bytes) will not be copied. If None, all files will be copied. Returns ------- str The path to the destination directory. Raises ------ FileNotFoundError If the source directory does not exist. OSError If copying fails for some reason. if not os.path.exists(src): raise FileNotFoundError(f\\"Source directory \'{src}\' does not exist.\\") os.makedirs(dst, exist_ok=True) with open(log_file, \'a\') as log: for root, dirs, files in os.walk(src): relative_path = os.path.relpath(root, src) destination_root = os.path.join(dst, relative_path) os.makedirs(destination_root, exist_ok=True) log.write(f\\"Directory: {destination_root}n\\") for file_name in files: source_file = os.path.join(root, file_name) destination_file = os.path.join(destination_root, file_name) file_size = os.path.getsize(source_file) if size_threshold is None or file_size <= size_threshold: shutil.copy2(source_file, destination_file) log.write(f\\"File: {destination_file} (size: {file_size} bytes)n\\") return dst"},{"question":"Seaborn Advanced Plotting Objective This question assesses your ability to utilize Seaborn’s advanced plotting capabilities, including transformations, facets, and customizations, to derive insightful visualizations from a dataset. Problem Statement You are given the Titanic dataset, which contains information about passengers, including their class, age, sex, and survival status. Write a function `plot_titanic_data` that uses Seaborn to create visualizations with the following specifications: 1. A facet grid of histograms showing the age distribution of passengers, separated by the sex of the passengers. 2. Each histogram should be colored by the survival status of the passengers. 3. The histograms should be stacked bars showing age distribution. 4. Ensure the x-axis represents age and is binned by intervals of 10 years. 5. The y-axis should represent the count of passengers in each age bin. Function Signature: ```python def plot_titanic_data(titanic: pd.DataFrame): pass ``` Input - `titanic` : A pandas DataFrame containing the Titanic dataset. Output - This function does not return any value. It only plots the required visualizations. Constraints - Use Seaborn and Matplotlib for all visualizations. - No external plotting libraries aside from Seaborn and Matplotlib are allowed. - Ensure the plots are visually clear with appropriate labels and titles. Performance Expectations - The function should execute efficiently on typical data sizes without unnecessary computations or memory usage. Example You can use the following code to load the Titanic dataset: ```python import seaborn.objects as so from seaborn import load_dataset import pandas as pd import matplotlib.pyplot as plt titanic = load_dataset(\\"titanic\\").sort_values(\\"alive\\", ascending=False) plot_titanic_data(titanic) ``` Expected behavior: 1. A set of histograms will be displayed, one for each sex category. Each histogram will show the age distribution, stacked and colored by survival status.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def plot_titanic_data(titanic: pd.DataFrame): Plots a facet grid of histograms showing the age distribution of Titanic passengers, separated by the sex of the passengers. Histograms are colored by survival status. # Facet grid for sex and survival status g = sns.FacetGrid(titanic, col=\\"sex\\", hue=\\"survived\\", col_wrap=2, height=5) # Map histogram plot to the facet grid g.map(sns.histplot, \\"age\\", binwidth=10, multiple=\\"stack\\") # Add labels and adjust plot g.set_axis_labels(\\"Age\\", \\"Count\\") g.add_legend(title=\\"Survived\\") g.set_titles(col_template=\\"{col_name} Passengers\\") plt.show()"},{"question":"# Question: Implementing and Transforming a Neural Network for TorchScript You are required to implement a simple neural network using PyTorch and then transform it to be compatible with TorchScript. The task will test your comprehension of PyTorch\'s capabilities and the necessary modifications needed to adhere to TorchScript constraints. Part 1: Implement a Neural Network Implement a neural network class `SimpleNeuralNetwork` using PyTorch. The network should have: - One hidden layer with 128 neurons and ReLU activation. - An output layer that predicts ten classes with a softmax activation. ```python import torch import torch.nn as nn import torch.nn.functional as F class SimpleNeuralNetwork(nn.Module): def __init__(self): super(SimpleNeuralNetwork, self).__init__() self.fc1 = nn.Linear(784, 128) # Assuming input is of size 28x28 flattened self.fc2 = nn.Linear(128, 10) def forward(self, x): x = F.relu(self.fc1(x)) x = F.log_softmax(self.fc2(x), dim=1) return x ``` Part 2: Convert the Network to TorchScript To do this, you need to: 1. **Script the Network**: Convert the network to TorchScript using `torch.jit.script`. 2. **Handle Unsupported Constructs**: If any component is unsupported, rewrite parts of the code to ensure compatibility with TorchScript. ```python def script_neural_network(): # Script the neural network model = SimpleNeuralNetwork() scripted_model = torch.jit.script(model) return scripted_model ``` # Constraints and Requirements - Ensure the scripted model can be successfully compiled into TorchScript. - Any required changes for enabling compatibility should be implemented. - The input to the network is a batch of flattened 28x28 images (size: `[batch_size, 784]`). - Use `torch.jit.trace` if the scripting fails, but clearly state the reasons for doing so. # Input and Output Format - **Input**: There is no external input required for this task beyond what is mentioned. - **Output**: The function `script_neural_network` should return a TorchScript compatible model. # Performance Requirements - The implementation should ensure that the resulting TorchScript model retains the functionality of the original PyTorch model. - Ensure the modifications do not severely degrade the performance of the network. This problem requires a deep understanding of PyTorch\'s neural network implementation and the adaptation necessary for TorchScript compatibility. Good luck!","solution":"import torch import torch.nn as nn import torch.nn.functional as F class SimpleNeuralNetwork(nn.Module): def __init__(self): super(SimpleNeuralNetwork, self).__init__() self.fc1 = nn.Linear(784, 128) # Assuming input is of size 28x28 flattened self.fc2 = nn.Linear(128, 10) def forward(self, x): x = F.relu(self.fc1(x)) x = F.log_softmax(self.fc2(x), dim=1) return x def script_neural_network(): # Script the neural network model = SimpleNeuralNetwork() try: scripted_model = torch.jit.script(model) except Exception as e: print(f\\"Script failed with error: {e}\\") return None return scripted_model"},{"question":"California Housing Price Prediction **Objective:** Implement a machine learning model using scikit-learn to predict housing prices based on features from the California Housing dataset. **Instructions:** 1. Load the California Housing dataset using `fetch_california_housing` from `sklearn.datasets`. 2. Perform any necessary data preprocessing steps (e.g., handling missing values, scaling features). 3. Split the dataset into training and testing sets. 4. Implement a regression model to predict the target variable (house prices). 5. Use cross-validation to evaluate the model\'s performance. 6. Report the model\'s performance using appropriate metrics (e.g., Mean Squared Error, R^2 score). **Constraints:** - You must use scikit-learn for implementing the regression model. - You should divide the data into a training set (80%) and a testing set (20%). - Use at least one technique for hyperparameter tuning (e.g., Grid Search or Random Search). - Provide the code in a function named `predict_housing_prices`. **Function Signature:** ```python def predict_housing_prices(): # Load and preprocess the dataset ... # Split the data into training and testing sets ... # Implement a regression model ... # Perform cross-validation ... # Evaluate and return the model performance metrics ... ``` **Input:** No input parameters are required as the function should directly load the dataset and perform all necessary operations internally. **Output:** A dictionary containing the following keys: - `\'mean_squared_error\'`: Mean Squared Error on the test set. - `\'r2_score\'`: R^2 score on the test set. - `\'best_params\'`: Best hyperparameters found during tuning. Example: ```python { \'mean_squared_error\': 0.535, \'r2_score\': 0.632, \'best_params\': {\'alpha\': 0.1, \'l1_ratio\': 0.5} } ``` **Performance Requirements:** - Ensure that the model training and evaluation do not exceed a reasonable runtime (e.g., 5 minutes). **Additional Information:** - Refer to the [California Housing dataset documentation](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_california_housing.html) for more details. - Use `StandardScaler` from `sklearn.preprocessing` for scaling features if necessary. - Use `train_test_split` from `sklearn.model_selection` for splitting the data. - Use `GridSearchCV` or `RandomizedSearchCV` from `sklearn.model_selection` for hyperparameter tuning. - You may choose any regression model from `sklearn.linear_model`, `sklearn.tree`, or others available in scikit-learn. Good luck!","solution":"from sklearn.datasets import fetch_california_housing from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.preprocessing import StandardScaler from sklearn.linear_model import Ridge from sklearn.metrics import mean_squared_error, r2_score def predict_housing_prices(): # Load the dataset data = fetch_california_housing() X, y = data.data, data.target # Split the data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Scale the features scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) # Implement a regression model ridge = Ridge() parameters = {\'alpha\': [0.1, 1.0, 10.0]} grid_search = GridSearchCV(ridge, parameters, cv=5) grid_search.fit(X_train, y_train) # Best estimator best_model = grid_search.best_estimator_ # Predict on the test set y_pred = best_model.predict(X_test) # Evaluate performance mse = mean_squared_error(y_test, y_pred) r2 = r2_score(y_test, y_pred) # Return the performance metrics return { \'mean_squared_error\': mse, \'r2_score\': r2, \'best_params\': grid_search.best_params_ }"},{"question":"# Custom Python Interactive Interpreter You are to design a custom interactive Python interpreter that has additional functionalities built upon the standard ones provided by Python\'s `code` module. Specifications: 1. **Class Name**: `CustomInterpreter` 2. **Functions to Implement**: - `__init__(self)`: Initializes the custom interactive interpreter. - `run(self)`: Starts the interactive interpreter session. - `add_command(self, command_name: str, function)`: Adds a custom command to the interpreter. 3. The interpreter should support: - Basic Python code execution. - Command history. Users should be able to traverse through the previous commands using the up and down arrow keys. - Adding and executing custom commands. For example, a custom command `hello` that prints \\"Hello, World!\\". Requirements 1. **Input/Output**: - The interpreter should read inputs similar to the standard Python interactive interpreter. - When a custom command is executed, it should run the corresponding function and print any results or messages. 2. **Constraints**: - Implement at least one custom command (e.g., `hello`). - Ensure command history is functional. 3. **Test Case**: - Instantiate the `CustomInterpreter` class. - Add a command `hello` that prints `\\"Hello, World!\\"`. - Run the interpreter and demonstrate that typing `hello` and pressing `Enter` outputs `\\"Hello, World!\\"`. Example Usage: ```python # Example of creating and using the CustomInterpreter def custom_hello(): print(\\"Hello, World!\\") interpreter = CustomInterpreter() interpreter.add_command(\'hello\', custom_hello) interpreter.run() # Starts the interactive interpreter session ``` Note: Make sure to handle the interactive session appropriately, ensuring smooth functionality of all specified features.","solution":"import code import readline class CustomInterpreter: def __init__(self): self.interpreter = code.InteractiveConsole() self.custom_commands = {} def add_command(self, command_name, function): self.custom_commands[command_name] = function def run(self): readline.parse_and_bind(\'tab: complete\') readline.parse_and_bind(\'set editing-mode vi\') readline.set_history_length(1000) while True: try: user_input = input(\'>>> \') if user_input in self.custom_commands: self.custom_commands[user_input]() else: self.interpreter.push(user_input) except EOFError: break except Exception as e: print(f\\"Error: {e}\\") def custom_hello(): print(\\"Hello, World!\\")"},{"question":"**Objective:** Implement an SGDClassifier for a binary classification task. Your implementation should include proper preprocessing steps, including feature scaling and hyperparameter tuning. **Question:** Given a binary classification dataset, your task is to implement an SGDClassifier using scikit-learn\'s `SGDClassifier`. Ensure that you perform the following steps: 1. **Data Preprocessing**: - Load the dataset. - Standardize the features using `StandardScaler`. 2. **Model Implementation**: - Initialize the `SGDClassifier` with `loss=\\"hinge\\"` and `penalty=\\"l2\\"`. - Perform hyperparameter tuning for the `alpha` (regularization term) and `max_iter` (number of iterations). Use grid search or randomized search to find the best parameters. 3. **Model Training and Evaluation**: - Fit the classifier on the training data. - Evaluate the model on the test data and provide accuracy, precision, recall, and F1-score. **Constraints:** - Do not use any other machine learning libraries except scikit-learn. - Ensure the model handles potential class imbalances in the dataset, possibly using class weights. **Performance Requirements:** - The entire code should execute within reasonable time constraints on standard datasets (e.g., 10,000 samples with 100 features). **Input Format:** - A CSV file named `train.csv` with training data (features and target) and a CSV file named `test.csv` with test data. **Output Format:** - Print the best hyperparameters. - Print the evaluation metrics (accuracy, precision, recall, and F1-score) on the test data. **Sample Code Structure:** ```python import pandas as pd from sklearn.linear_model import SGDClassifier from sklearn.preprocessing import StandardScaler from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score # Load data train_data = pd.read_csv(\'train.csv\') test_data = pd.read_csv(\'test.csv\') # Separate features and target X_train = train_data.drop(\'target\', axis=1) y_train = train_data[\'target\'] X_test = test_data.drop(\'target\', axis=1) y_test = test_data[\'target\'] # Feature Scaling scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) # Define SGDClassifier sgd = SGDClassifier(loss=\\"hinge\\", penalty=\\"l2\\") # Hyperparameter tuning param_grid = { \'alpha\': [1e-4, 1e-3, 1e-2, 1e-1], \'max_iter\': [1000, 2000, 3000] } grid_search = GridSearchCV(sgd, param_grid, cv=5) grid_search.fit(X_train_scaled, y_train) # Get the best parameters best_params = grid_search.best_params_ print(f\\"Best parameters: {best_params}\\") # Train with the best parameters best_sgd = grid_search.best_estimator_ best_sgd.fit(X_train_scaled, y_train) # Predict and evaluate y_pred = best_sgd.predict(X_test_scaled) accuracy = accuracy_score(y_test, y_pred) precision = precision_score(y_test, y_pred) recall = recall_score(y_test, y_pred) f1 = f1_score(y_test, y_pred) print(f\\"Accuracy: {accuracy}\\") print(f\\"Precision: {precision}\\") print(f\\"Recall: {recall}\\") print(f\\"F1 Score: {f1}\\") ``` **Note:** This is a guide for the structure of your implementation. Be sure to include necessary imports, comments, and any additional code that is required.","solution":"import pandas as pd from sklearn.linear_model import SGDClassifier from sklearn.preprocessing import StandardScaler from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score def load_data(train_path, test_path): train_data = pd.read_csv(train_path) test_data = pd.read_csv(test_path) return train_data, test_data def preprocess_data(train_data, test_data): X_train = train_data.drop(\'target\', axis=1) y_train = train_data[\'target\'] X_test = test_data.drop(\'target\', axis=1) y_test = test_data[\'target\'] scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) return X_train_scaled, y_train, X_test_scaled, y_test def tune_and_train_model(X_train, y_train): sgd = SGDClassifier(loss=\\"hinge\\", penalty=\\"l2\\", class_weight=\'balanced\') param_grid = { \'alpha\': [1e-4, 1e-3, 1e-2, 1e-1], \'max_iter\': [1000, 2000, 3000] } grid_search = GridSearchCV(sgd, param_grid, cv=5) grid_search.fit(X_train, y_train) best_params = grid_search.best_params_ print(f\\"Best parameters: {best_params}\\") best_sgd = grid_search.best_estimator_ return best_sgd def evaluate_model(model, X_test, y_test): y_pred = model.predict(X_test) accuracy = accuracy_score(y_test, y_pred) precision = precision_score(y_test, y_pred) recall = recall_score(y_test, y_pred) f1 = f1_score(y_test, y_pred) print(f\\"Accuracy: {accuracy}\\") print(f\\"Precision: {precision}\\") print(f\\"Recall: {recall}\\") print(f\\"F1 Score: {f1}\\") return accuracy, precision, recall, f1 if __name__ == \\"__main__\\": train_path = \'train.csv\' test_path = \'test.csv\' train_data, test_data = load_data(train_path, test_path) X_train_scaled, y_train, X_test_scaled, y_test = preprocess_data(train_data, test_data) best_sgd = tune_and_train_model(X_train_scaled, y_train) best_sgd.fit(X_train_scaled, y_train) evaluate_model(best_sgd, X_test_scaled, y_test)"},{"question":"# Secure Token Generation using `hashlib` and `secrets` Modules Your task is to implement a function that generates a secure token using Python\'s `hashlib` and `secrets` modules. The function should accept a user-specific message and generate a secure token by performing the following operations: 1. **Generate a secure random number**: Use the `secrets` module to generate a random number of 16 bytes. 2. **Hash the message**: Use the `hashlib` module to hash the user-specific message using the SHA-256 hash algorithm. 3. **Combine and re-hash**: Concatenate the random number and the hashed message, then create a SHA-256 hash of the combined result to produce the final secure token. # Function Signature ```python def generate_secure_token(message: str) -> str: Generate a secure token using the hashlib and secrets modules. Parameters: - message (str): The user-specific message to be incorporated into the token. Returns: - str: The secure token in hexadecimal format. ``` # Input - `message`: A non-empty string containing the user-specific message. # Output - Returns a hexadecimal string representing the secure token. # Constraints - The message string should not be empty. # Example ```python token = generate_secure_token(\\"example_message\\") print(token) # Example output (will vary due to randomness): \\"e3b0c44298fc1c149afbf4c8996fb92427ae41e4...\\" ``` # Notes 1. Ensure your function is efficient and uses cryptographic practices as provided by the `hashlib` and `secrets` modules. 2. The output token should be unique and difficult to predict for different message inputs due to the incorporation of the secure random number. 3. You can use the `secrets.token_bytes(16)` method to generate the 16-byte random number.","solution":"import hashlib import secrets def generate_secure_token(message: str) -> str: Generate a secure token using the hashlib and secrets modules. Parameters: - message (str): The user-specific message to be incorporated into the token. Returns: - str: The secure token in hexadecimal format. if not message: raise ValueError(\\"The message should not be empty.\\") # Generate a secure random number of 16 bytes random_bytes = secrets.token_bytes(16) # Hash the user-specific message using SHA-256 hashed_message = hashlib.sha256(message.encode(\'utf-8\')).digest() # Concatenate the random number and the hashed message combined = random_bytes + hashed_message # Create a SHA-256 hash of the combined result final_token = hashlib.sha256(combined).hexdigest() return final_token"},{"question":"**File System Organizer** **Problem Statement:** You need to implement a Python function `organize_files_by_extension(directory: str) -> None` that organizes files in the given directory into subdirectories based on their file extensions. This task should demonstrate your knowledge of file and directory manipulation using the `pathlib` and `shutil` modules. **Input:** - `directory` (str): The path to the directory containing the files to organize. Assume this directory contains only files, no nested directories. **Output:** - The function does not need to return anything. It should create subdirectories in the provided directory, one for each unique file extension, and move corresponding files into these subdirectories. **Function Signature:** ```python def organize_files_by_extension(directory: str) -> None: pass ``` **Constraints:** - Do not use third-party libraries. Only use the standard Python library. - Handle various file extensions (e.g., `.txt`, `.jpg`, `.png`, etc.). - Files with no extensions should be moved to a directory named `no_extension`. - If a file already exists in the target subdirectory, you should skip moving that file to avoid overwriting. **Example:** Suppose the directory `/path/to/your/directory` contains following files: ``` document1.txt image1.jpg image2.png script1.py script2.py README ``` After running `organize_files_by_extension(\'/path/to/your/directory\')`, the directory structure should be: ``` /path/to/your/directory/ txt/ document1.txt jpg/ image1.jpg png/ image2.png py/ script1.py script2.py no_extension/ README ``` **Additional Information:** - Use `pathlib` for path manipulations and file operations. - Use `shutil` for moving files. - Assume the input directory exists and contains only files. - Ensure the solution is efficient and handles large numbers of files gracefully.","solution":"from pathlib import Path import shutil def organize_files_by_extension(directory: str) -> None: path = Path(directory) if not path.is_dir(): raise ValueError(f\\"The path {directory} is not a valid directory.\\") # Iterate over each file in the directory for file in path.iterdir(): if file.is_file(): # We only care about files ext = file.suffix[1:] # Get the file extension without the dot if not ext: # Handle files with no extension ext = \'no_extension\' target_dir = path / ext # Determine the target directory target_dir.mkdir(exist_ok=True) # Create target directory if it does not exist target_file = target_dir / file.name if not target_file.exists(): # Only move if the target file does not exist shutil.move(str(file), str(target_file))"},{"question":"**Question: Implement a Comprehensive Directory Backup Utility** **Objective:** Design and implement a Python function `backup_directory(src, dst, filename_patterns=None)` that creates a backup of a given source directory (`src`) and saves it into the destination directory (`dst`). The backup should be in the format of a compressed archive (.zip). Optionally, you can specify which files should be included in the backup using `filename_patterns`, which supports glob-style patterns. **Requirements:** 1. **Function Signature:** ```python def backup_directory(src: str, dst: str, filename_patterns: Optional[List[str]] = None) -> str: ``` 2. **Input:** - `src` (str): Path of the source directory to be backed up. - `dst` (str): Path of the destination directory where the backup will be stored. - `filename_patterns` (Optional[List[str]]): A list of glob-style patterns to include in the backup. If None, include all files. 3. **Output:** - Returns the path to the created archive (str). 4. **Constraints:** - Ensure `src` is a valid existing directory. - Ensure `dst` is a valid writable directory. - Handle any exceptions that may arise during the file operations. - The resulting archive must include all specified files and be compressed. 5. **Steps:** - Validate the input directories. - Create an archive of the `src` directory containing only the files matching `filename_patterns`. - If no patterns are provided, include all files in the directory. - Save the archive in the `dst` directory. - Return the full path to the created archive. **Example Usage:** ```python # Backup all files backup_path = backup_directory(\\"/path/to/source\\", \\"/path/to/destination\\") # Backup only .txt and .md files backup_path = backup_directory(\\"/path/to/source\\", \\"/path/to/destination\\", [\\"*.txt\\", \\"*.md\\"]) ``` **Performance Considerations:** - Be mindful of memory usage while reading and writing large files. - Ensure efficient file copying using `shutil` functions. - Handle symbolic links appropriately. Ensure the implementation is efficient and reliable, capable of handling various edge cases such as nested directories, symbolic links, and large files.","solution":"import os import zipfile import glob from typing import List, Optional def backup_directory(src: str, dst: str, filename_patterns: Optional[List[str]] = None) -> str: Creates a backup of the source directory and saves it as a zip file in the destination directory. :param src: Path of the source directory to be backed up. :param dst: Path of the destination directory where the backup will be stored. :param filename_patterns: Optional. A list of glob-style patterns to include in the backup. If None, include all files. :return: The path to the created archive. if not os.path.isdir(src): raise ValueError(f\\"Source directory \'{src}\' does not exist.\\") if not os.path.isdir(dst): raise ValueError(f\\"Destination directory \'{dst}\' does not exist or is not writable.\\") # Use the source directory name as the archive base name base_name = os.path.basename(os.path.normpath(src)) archive_name = os.path.join(dst, base_name + \\".zip\\") with zipfile.ZipFile(archive_name, \'w\', zipfile.ZIP_DEFLATED) as archive: for root, dirs, files in os.walk(src): for file in files: file_path = os.path.join(root, file) # Check if file matches any of the filename_patterns if filename_patterns: matches = [glob.fnmatch.fnmatch(file_path, pattern) for pattern in filename_patterns] if not any(matches): continue rel_path = os.path.relpath(file_path, src) archive.write(file_path, rel_path) return archive_name"},{"question":"# URL Shortener with Query Parameter Extraction **Objective:** Create a URL shortener service that not only shortens the URL but also extracts and prints the query parameters present in the original URL. # Function Signature: ```python def shorten_url(url: str) -> tuple: pass ``` # Input: - `url` (str): A string representing the original URL (e.g., \\"https://example.com/search?query=python&sort=ascending\\"). # Output: - A tuple containing: - A shortened version of the URL (str). - A dictionary of query parameters (dict). # Function Requirements: 1. **URL Shortening**: - The function should generate a shortened URL by converting the provided URL into a fixed format. You can simulate the shortening simply by hashing the URL. 2. **Query Parameter Extraction**: - Extract the query parameters from the original URL and return them as a dictionary. # Constraints: - You should handle HTTP/HTTPS URLs. - The generated shortened URL should be unique for distinct input URLs. - You need to manage and raise appropriate exceptions when the provided URL is invalid. Use `urllib.error` for this. # Performance: - Ensure that query parameter extraction is efficient and the entire process completes in a reasonable time for typical URL lengths found on the web. # Example: ```python url = \\"https://example.com/search?query=python&sort=ascending\\" shortened_url, params = shorten_url(url) print(shortened_url) # A shortened URL (e.g., \\"https://short.url/2fHdG7\\") print(params) # {\'query\': \'python\', \'sort\': \'ascending\'} ``` **Note**: For the URL shortening, you can use any string hashing mechanism to demonstrate shortened URL logic within the function. The focus should be on proper usage of `urllib.parse` for query extraction and handling exceptions using `urllib.error`.","solution":"import hashlib from urllib.parse import urlparse, parse_qs, urlunparse def shorten_url(url: str) -> tuple: Shortens the given URL and extracts the query parameters. Args: url (str): The original URL. Returns: tuple: A shortened version of the URL and a dictionary of query parameters. try: # Parse the URL parsed_url = urlparse(url) # Extract query parameters query_params = parse_qs(parsed_url.query) query_params = {k: v[0] for k, v in query_params.items()} # Generate a shortened URL using a hash url_hash = hashlib.md5(url.encode()).hexdigest()[:6] shortened_url = f\\"https://short.url/{url_hash}\\" return (shortened_url, query_params) except Exception as e: raise ValueError(f\\"Invalid URL provided: {e}\\") # Example usage url = \\"https://example.com/search?query=python&sort=ascending\\" shortened_url, params = shorten_url(url) print(shortened_url) # Expected shortened URL (e.g., \\"https://short.url/2fHdG7\\") print(params) # Expected {\'query\': \'python\', \'sort\': \'ascending\'}"},{"question":"**Question:** You need to implement a function using Python 3.10 advanced concepts that utilizes built-in types, custom sequences, iterators, and context managers. # Problem Description Implement a class `CustomSequence` that acts like a Python sequence object and supports iteration, where the class should extend both `collections.abc.Sequence` and `collections.abc.Iterable`. This class should be capable of storing any hashable elements. Additionally, implement a context manager `SequenceManager` that ensures any operation on a `CustomSequence` like `add`, `remove`, or `fetch` is logged and gracefully handles opening and closing of resources. **Requirements:** 1. **CustomSequence Class:** - Implement a sequence class `CustomSequence` that: - Supports indexing, slicing, and other sequence operations (`len`, `min`, `max`, `in`, etc.). - Implements an iterator for iteration over its elements. - Allows addition and removal of elements, ensuring no duplicates. - Provides a method `.fetch(index)` to safely retrieve an item by index (return `None` if out of bounds). 2. **SequenceManager Context Manager:** - Create a context manager `SequenceManager`: - Used to manage operations on `CustomSequence`. - Logs operations (addition, removal, fetching of elements). - Ensures resources are appropriately closed after operations. **Input and Output:** - There are no direct inputs or outputs functions. Your classes should raise exceptions for invalid operations, but operations like fetching out-of-bounds indices must gracefully return `None`. # Example Usage ```python from collections.abc import Sequence, Iterable class CustomSequence(Sequence, Iterable): # Your implementation here class SequenceManager: # Your implementation here # Example: sequence = CustomSequence() with SequenceManager(sequence) as manager: manager.add(\'apple\') manager.add(\'banana\') print(\'apple\' in sequence) # True print(sequence.fetch(1)) # \'banana\' print(sequence.fetch(2)) # None manager.remove(\'apple\') ``` Implement the `CustomSequence` and `SequenceManager` class according to the above requirements. # Constraints: - Use only built-in types and standard library modules. - Ensure that CustomSequence follows proper sequence behaviors and Iterable protocol. - Do not use external libraries.","solution":"from collections.abc import Sequence, Iterable class CustomSequence(Sequence, Iterable): def __init__(self): self._data = [] def __len__(self): return len(self._data) def __getitem__(self, index): return self._data[index] def __iter__(self): return iter(self._data) def add(self, element): if element not in self._data: self._data.append(element) def remove(self, element): if element in self._data: self._data.remove(element) def fetch(self, index): if 0 <= index < len(self._data): return self._data[index] return None class SequenceManager: def __init__(self, sequence): self.sequence = sequence self.log = [] def __enter__(self): return self def add(self, element): self.sequence.add(element) self.log.append(f\\"Added \'{element}\'\\") def remove(self, element): self.sequence.remove(element) self.log.append(f\\"Removed \'{element}\'\\") def fetch(self, index): element = self.sequence.fetch(index) self.log.append(f\\"Fetched element at index {index}: \'{element}\'\\") return element def __exit__(self, exc_type, exc_value, traceback): self.log.append(\\"Operation completed\\")"},{"question":"# Custom Protocol Implementation: Secure Text Transmission Introduction In this task, you are required to implement a custom protocol that will transmit text data securely between a server and a client using asyncio\'s transports and protocols. The protocol must integrate basic encryption and decryption of messages. Objective You will implement a custom protocol, `SecureTextProtocol`, to handle encrypted text data transmission over a TCP connection. The protocol should be able to: 1. Establish a connection. 2. Encrypt and send text data. 3. Decrypt and receive text data. 4. Close the connection gracefully. Requirements 1. Implement the `SecureTextProtocol` class inheriting from `asyncio.Protocol`. 2. Your protocol must handle connection establishment, data transmission, and connection termination. 3. Implement basic encryption and decryption using a simple algorithm like XOR with a given key. 4. Design corresponding client and server classes to test the protocol. Input and Output - The client will send a plaintext message to the server. - The server should log the decoded message and send a response back to the client. - The client should receive and log the server\'s response. Constraints - Use Python 3.8+. - Utilize `asyncio` for asynchronous operations. - Ensure proper handling of connection establishment and termination. Encryption/Decryption Algorithm - Use XOR encryption for simplicity: ```python def xor_cipher(data: str, key: int) -> str: return \'\'.join(chr(ord(char) ^ key) for char in data) ``` Example: ```python import asyncio KEY = 123 # Simple XOR key def xor_cipher(data: str, key: int) -> str: return \'\'.join(chr(ord(char) ^ key) for char in data) class SecureTextProtocol(asyncio.Protocol): def __init__(self): self.transport = None def connection_made(self, transport): self.transport = transport peername = transport.get_extra_info(\'peername\') print(f\'Connection established with {peername}\') def data_received(self, data): decrypted_data = xor_cipher(data.decode(), KEY) print(f\'Data received: {decrypted_data}\') response = xor_cipher(f\'Received your message: {decrypted_data}\', KEY) self.transport.write(response.encode()) def connection_lost(self, exc): print(\'Connection lost\') self.transport = None class SecureTextClientProtocol(asyncio.Protocol): def __init__(self, message, on_con_lost): self.message = message self.on_con_lost = on_con_lost def connection_made(self, transport): self.transport = transport encrypted_message = xor_cipher(self.message, KEY) transport.write(encrypted_message.encode()) print(f\'Data sent: {self.message}\') def data_received(self, data): decrypted_data = xor_cipher(data.decode(), KEY) print(f\'Server response: {decrypted_data}\') self.on_con_lost.set_result(True) def connection_lost(self, exc): print(\'The server closed the connection\') self.on_con_lost.set_result(True) async def run_server(): loop = asyncio.get_running_loop() server = await loop.create_server(SecureTextProtocol, \'127.0.0.1\', 8888) async with server: await server.serve_forever() async def run_client(message): loop = asyncio.get_running_loop() on_con_lost = loop.create_future() transport, protocol = await loop.create_connection(lambda: SecureTextClientProtocol(message, on_con_lost), \'127.0.0.1\', 8888) try: await on_con_lost finally: transport.close() if __name__ == \'__main__\': asyncio.run(run_server()) asyncio.run(run_client(\\"Hello, Server!\\")) ``` Task 1. Complete the `SecureTextProtocol` class to handle connection, encryption, and decryption as outlined. 2. Implement the client to send a plaintext message, using the `SecureTextClientProtocol` class. 3. Ensure that both the server and client log the correctly encrypted and decrypted messages. Feel free to modify the encryption algorithm or enhance the client/server functionality based on the given constraints.","solution":"import asyncio KEY = 123 # Simple XOR key def xor_cipher(data: str, key: int) -> str: return \'\'.join(chr(ord(char) ^ key) for char in data) class SecureTextProtocol(asyncio.Protocol): def __init__(self): self.transport = None def connection_made(self, transport): self.transport = transport peername = transport.get_extra_info(\'peername\') print(f\'Connection established with {peername}\') def data_received(self, data): decrypted_data = xor_cipher(data.decode(), KEY) print(f\'Data received: {decrypted_data}\') response = xor_cipher(f\'Received your message: {decrypted_data}\', KEY) self.transport.write(response.encode()) def connection_lost(self, exc): print(\'Connection lost\') self.transport = None class SecureTextClientProtocol(asyncio.Protocol): def __init__(self, message, on_con_lost): self.message = message self.on_con_lost = on_con_lost def connection_made(self, transport): self.transport = transport encrypted_message = xor_cipher(self.message, KEY) transport.write(encrypted_message.encode()) print(f\'Data sent: {self.message}\') def data_received(self, data): decrypted_data = xor_cipher(data.decode(), KEY) print(f\'Server response: {decrypted_data}\') self.on_con_lost.set_result(True) def connection_lost(self, exc): print(\'The server closed the connection\') self.on_con_lost.set_result(True) async def run_server(): loop = asyncio.get_running_loop() server = await loop.create_server(SecureTextProtocol, \'127.0.0.1\', 8888) async with server: await server.serve_forever() async def run_client(message): loop = asyncio.get_running_loop() on_con_lost = loop.create_future() transport, protocol = await loop.create_connection(lambda: SecureTextClientProtocol(message, on_con_lost), \'127.0.0.1\', 8888) try: await on_con_lost finally: transport.close()"},{"question":"Question: Advanced JSON Serialization and Deserialization # Objective Implement custom JSON serialization and deserialization in Python to handle specific data structures and requirements. # Problem Statement You are given a modified `Book` class that contains details of a book, and a `Library` class containing a collection of books. Implement custom serialization and deserialization for the `Library` class such that: 1. Each book in the library is represented using its own custom JSON format. 2. The `Library` class is serialized to JSON in a predefined format. # Custom JSON Format - A `Book` object should be serialized as a dictionary with keys `title`, `author`, `year`, and `copies`. - The `Library` object should be serialized to a JSON object with a key `books` pointing to a list of `Book` objects in their custom format. # Implementation Requirements - Implement the serialization using a custom `JSONEncoder` subclass. - Implement the deserialization using a custom `JSONDecoder` subclass with appropriate hooks. - Ensure that your implementation can handle non-ASCII characters and special float values (`NaN`, `Infinity`, `-Infinity`). # Classes Definitions ```python from typing import List import json class Book: def __init__(self, title: str, author: str, year: int, copies: int): self.title = title self.author = author self.year = year self.copies = copies class Library: def __init__(self): self.books: List[Book] = [] def add_book(self, book: Book): self.books.append(book) ``` # Custom JSON Encoder and Decoder Classes Implement the following custom classes: - `LibraryEncoder`: for serializing `Library` objects. - `LibraryDecoder`: for deserializing JSON data into a `Library` object. # Usage Examples ```python library = Library() library.add_book(Book(\\"Python 101\\", \\"Author A\\", 2020, 5)) library.add_book(Book(\\"Python 201\\", \\"Author B\\", 2021, 3)) # Serialize Library instance to JSON string json_str = json.dumps(library, cls=LibraryEncoder) print(json_str) # Deserialize JSON string to Library instance library_instance = json.loads(json_str, cls=LibraryDecoder) ``` # Constraints - Books\' `title` and `author` are strings containing Unicode characters. - Books\' `year` is an integer. - Books\' `copies` is an integer. # Performance Requirements - Your solution should efficiently handle a large number of books, ensuring minimal performance overhead. # Evaluation Your implementation will be evaluated on: - Correctness of serialization and deserialization. - Proper handling of special cases like non-ASCII characters and special float values. - Code readability and use of best practices.","solution":"import json from typing import List class Book: def __init__(self, title: str, author: str, year: int, copies: int): self.title = title self.author = author self.year = year self.copies = copies class Library: def __init__(self): self.books: List[Book] = [] def add_book(self, book: Book): self.books.append(book) class LibraryEncoder(json.JSONEncoder): def default(self, o): if isinstance(o, Book): return { \\"title\\": o.title, \\"author\\": o.author, \\"year\\": o.year, \\"copies\\": o.copies } elif isinstance(o, Library): return { \\"books\\": [self.default(book) for book in o.books] } return super().default(o) class LibraryDecoder(json.JSONDecoder): def __init__(self, *args, **kwargs): super().__init__(object_hook=self.object_hook, *args, **kwargs) def object_hook(self, obj): if \\"title\\" in obj and \\"author\\" in obj and \\"year\\" in obj and \\"copies\\" in obj: return Book(obj[\\"title\\"], obj[\\"author\\"], obj[\\"year\\"], obj[\\"copies\\"]) if \\"books\\" in obj: library = Library() for book in obj[\\"books\\"]: library.add_book(book) return library return obj"},{"question":"**Email Parsing with BytesFeedParser** # Objective: Write a Python function `extract_email_subjects` that takes a list of email messages in bytes format and returns a list of email subjects from those messages. The emails can be in any valid format that the `BytesFeedParser` can handle. # Function Signature: ```python def extract_email_subjects(emails: List[bytes]) -> List[str]: pass ``` # Input: - `emails`: A list of email messages in bytes format. Each email is a valid RFC 5322 (or RFC 6532 if applicable) formatted message. # Output: - A list of strings where each string is the subject of the corresponding email in the input list. # Constraints: - The emails list can contain up to 1000 email messages. - Each email message can be up to 1 MB in size. # Requirements: 1. Use `BytesFeedParser` from the `email.parser` module to parse the email messages incrementally. 2. Extract and return the \'Subject\' header from each parsed email message. 3. Handle any potential parsing errors gracefully and exclude subjects from invalid emails from the output list. 4. Assume that the input list will not be empty and there will be at least one valid email. # Example: ```python from typing import List def extract_email_subjects(emails: List[bytes]) -> List[str]: # Implementation here # Example usage: # bytes_email1 and bytes_email2 are byte representations of email messages. bytes_email1 = b\'From: user@example.comrnSubject: Test EmailrnrnThis is a test email.\' bytes_email2 = b\'From: anotheruser@example.comrnSubject: Another TestrnrnThis is another test email.\' email_list = [bytes_email1, bytes_email2] print(extract_email_subjects(email_list)) # Output: [\\"Test Email\\", \\"Another Test\\"] ``` # Notes: - You need to import necessary modules such as `email.parser` and handle any exceptions where needed. - The email messages are simple examples; ensure your function works with more complex and nested multipart email structures as well.","solution":"from typing import List from email.parser import BytesFeedParser from email.policy import default def extract_email_subjects(emails: List[bytes]) -> List[str]: subjects = [] for email_bytes in emails: parser = BytesFeedParser(policy=default) parser.feed(email_bytes) email_message = parser.close() subject = email_message.get(\'Subject\') if subject: subjects.append(subject) return subjects"},{"question":"**Objective:** Implement a Python function utilizing the `inspect` module to inspect a given module and compile a report of its functions and classes. # Problem Statement Write a function `inspect_module(module)` that takes a module object as an argument and returns a dictionary containing detailed information about all functions and classes within the module. **The dictionary should have the following structure:** ```python { \'functions\': { \'function_name\': { \'signature\': \'function_signature\', \'docstring\': \'function_docstring\' }, ... }, \'classes\': { \'class_name\': { \'docstring\': \'class_docstring\', \'methods\': { \'method_name\': { \'signature\': \'method_signature\', \'docstring\': \'method_docstring\' }, ... } }, ... } } ``` # Constraints and Requirements - You must use the `inspect` module to retrieve the required information. - The `signature` should include parameters and their types, if available. - If a function or method does not have a docstring, the value should be `None`. - The module should handle nested classes and methods within those classes. - Make sure to handle exceptions that might occur during the inspection process. # Example Consider a module named `example_module` with the following content: ```python # example_module.py def foo(a, b:int=2): This is function foo. pass class Bar: This is class Bar. def method1(self, x, y): This is method1 of class Bar. pass def method2(self): pass ``` Calling `inspect_module(example_module)` might return: ```python { \'functions\': { \'foo\': { \'signature\': \'(a, b: int = 2)\', \'docstring\': \'This is function foo.\' } }, \'classes\': { \'Bar\': { \'docstring\': \'This is class Bar.\', \'methods\': { \'method1\': { \'signature\': \'(self, x, y)\', \'docstring\': \'This is method1 of class Bar.\' }, \'method2\': { \'signature\': \'(self)\', \'docstring\': None } } } } } ``` # Submission Your submission should include the implementation of the `inspect_module` function as well as any helper functions you consider necessary.","solution":"import inspect def inspect_module(module): Inspects the given module and returns a dictionary containing information about functions and classes within the module. Arguments: module -- the module object to be inspected Returns: A dictionary containing details about functions and classes in the module. info = { \'functions\': {}, \'classes\': {} } # Gather information about functions for name, obj in inspect.getmembers(module, inspect.isfunction): info[\'functions\'][name] = { \'signature\': str(inspect.signature(obj)), \'docstring\': inspect.getdoc(obj) } # Gather information about classes for name, obj in inspect.getmembers(module, inspect.isclass): class_info = { \'docstring\': inspect.getdoc(obj), \'methods\': {} } for method_name, method in inspect.getmembers(obj, inspect.isfunction): class_info[\'methods\'][method_name] = { \'signature\': str(inspect.signature(method)), \'docstring\': inspect.getdoc(method) } info[\'classes\'][name] = class_info return info"},{"question":"Objective: To assess the student\'s ability to use the `ossaudiodev` module to interface with audio devices in OSS (Open Sound System) on Unix-like systems. Problem Statement: You are required to implement a function `play_audio_from_file(file_path)` that plays back an audio file using the `ossaudiodev` module. The playback should set the audio format, number of channels, and sampling rate based on common audio parameters. Requirements: 1. The function should open the default audio device (`/dev/dsp`) to play audio in write-only mode. 2. Set the audio format to `AFMT_S16_LE` (Signed, 16-bit audio, little-endian). 3. Set the audio to stereo (2 channels). 4. Set the sampling rate to 44100 Hz (CD quality audio). 5. Read audio data from the specified `file_path` and write it to the audio device. 6. Implement error handling to catch and appropriately respond to `OSSAudioError` or `OSError`. Input: - `file_path` (string): The path to the audio file to be played. Assume the file contains raw PCM data in the format specified above. Output: - None. The function will just play the audio. Example Usage: ```python play_audio_from_file(\\"path/to/audio/file\\") ``` Constraints: - The audio device (`/dev/dsp`) may only be accessed by one process at a time. Ensure proper handling and closing of the device. - Assume that the input file is well-formatted and contains the required audio data format. Performance Requirements: - The function should demonstrate efficient reading and writing to minimize latency in playback. Implementation Notes: - You may need to convert or process the file data to match the audio settings if they differ from the default OSS settings. However, for this task, assume the file data is already in the required format. ```python import ossaudiodev def play_audio_from_file(file_path): try: # Step 1: Open the audio device in write-only mode dsp = ossaudiodev.open(\'w\') # Step 2: Set the audio format to AFMT_S16_LE dsp.setfmt(ossaudiodev.AFMT_S16_LE) # Step 3: Set the audio to stereo (2 channels) dsp.channels(2) # Step 4: Set the sample rate to 44100 Hz dsp.speed(44100) # Step 5: Read audio data from the input file and write it to the audio device with open(file_path, \'rb\') as f: while True: data = f.read(4096) # Read in chunks of 4096 bytes if not data: break dsp.writeall(data) # Step 6: Close the audio device dsp.close() except (ossaudiodev.OSSAudioError, OSError) as e: print(f\\"An error occurred: {e}\\") if \'dsp\' in locals(): dsp.close() ``` **Note: This function assumes that the audio file is properly formatted and does not handle advanced audio data manipulation.**","solution":"import ossaudiodev def play_audio_from_file(file_path): try: # Step 1: Open the audio device in write-only mode dsp = ossaudiodev.open(\'w\') # Step 2: Set the audio format to AFMT_S16_LE dsp.setfmt(ossaudiodev.AFMT_S16_LE) # Step 3: Set the audio to stereo (2 channels) dsp.channels(2) # Step 4: Set the sample rate to 44100 Hz dsp.speed(44100) # Step 5: Read audio data from the input file and write it to the audio device with open(file_path, \'rb\') as f: while True: data = f.read(4096) # Read in chunks of 4096 bytes if not data: break dsp.writeall(data) # Step 6: Close the audio device dsp.close() except (ossaudiodev.OSSAudioError, OSError) as e: print(f\\"An error occurred: {e}\\") if \'dsp\' in locals(): dsp.close()"},{"question":"You are required to create a Python function that interacts with the C API for Python integer objects (`PyLongObject`). You will write a Python function that creates multiple `PyLongObject` instances using a range of C types and then converts these instances back into their equivalent C types. Ensure that you handle any potential errors appropriately. # Function Signature ```python def manage_pylongs(): pass ``` # Requirements 1. **Creating PyLongObject Instances:** - Create `PyLongObject` instances from various C types using the following functions: - `PyLong_FromLong` - `PyLong_FromUnsignedLong` - `PyLong_FromDouble` 2. **Converting PyLongObject Instances:** - Convert the created `PyLongObject` instances back to their respective C types using the following functions: - `PyLong_AsLong` - `PyLong_AsUnsignedLong` - `PyLong_AsDouble` 3. **Error Handling:** - Use `PyErr_Occurred()` to handle any error that might occur during the execution of the functions. - Ensure that your function does not crash on errors and handles them gracefully. # Constraints - Your function should be able to handle edge cases like large numbers and types at their limits. - Use appropriate API functions to check if objects are correctly created as `PyLongObject`. # Example Although the Python function `manage_pylongs()` doesn\'t take any parameters or return any value, it should demonstrate: - Creation of `PyLongObject` from different C types. - Conversion of these objects back to C types. - Handling and reporting of any errors correctly. ```python def manage_pylongs(): # Example snippet [Demonstrating functionality, actual implementation will depend on your environment] long_value = 42 ulong_value = 1234567890 double_value = 42.42 pylong_obj1 = PyLong_FromLong(long_value) if not pylong_obj1: if PyErr_Occurred(): print(\\"Error creating PyLongObject from long\\") pylong_obj2 = PyLong_FromUnsignedLong(ulong_value) if not pylong_obj2: if PyErr_Occurred(): print(\\"Error creating PyLongObject from unsigned long\\") pylong_obj3 = PyLong_FromDouble(double_value) if not pylong_obj3: if PyErr_Occurred(): print(\\"Error creating PyLongObject from double\\") converted_long = PyLong_AsLong(pylong_obj1) if converted_long == -1 and PyErr_Occurred(): print(\\"Error converting PyLongObject back to long\\") converted_ulong = PyLong_AsUnsignedLong(pylong_obj2) if converted_ulong == (unsigned long)-1 and PyErr_Occurred(): print(\\"Error converting PyLongObject back to unsigned long\\") converted_double = PyLong_AsDouble(pylong_obj3) if converted_double == -1.0 and PyErr_Occurred(): print(\\"Error converting PyLongObject back to double\\") ``` Write the complete function and ensure it operates under normal Python conditions and handles errors gracefully using the given API functions. This will test your understanding of creating and managing Python integers via the provided C API.","solution":"The manage_pylongs function demonstrates the creation and conversion of Python long objects (PyLongObject) using C API functions. The function creates PyLongObjects from various C types (long, unsigned long, double) and converts these objects back to their respective C types, handling any potential errors that might occur. def manage_pylongs(): try: import ctypes from ctypes import pythonapi, c_long, c_ulong, c_double, py_object # Define the necessary ctypes functions pythonapi.PyLong_FromLong.argtypes = [c_long] pythonapi.PyLong_FromLong.restype = py_object pythonapi.PyLong_FromUnsignedLong.argtypes = [c_ulong] pythonapi.PyLong_FromUnsignedLong.restype = py_object pythonapi.PyLong_FromDouble.argtypes = [c_double] pythonapi.PyLong_FromDouble.restype = py_object pythonapi.PyLong_AsLong.argtypes = [py_object] pythonapi.PyLong_AsLong.restype = c_long pythonapi.PyLong_AsUnsignedLong.argtypes = [py_object] pythonapi.PyLong_AsUnsignedLong.restype = c_ulong pythonapi.PyLong_AsDouble.argtypes = [py_object] pythonapi.PyLong_AsDouble.restype = c_double pythonapi.PyErr_Occurred.restype = py_object # Creating PyLongObject instances long_value = 42 ulong_value = 1234567890 double_value = 42.42 pylong_obj1 = pythonapi.PyLong_FromLong(long_value) if not pylong_obj1: if pythonapi.PyErr_Occurred(): print(\\"Error creating PyLongObject from long\\") pylong_obj2 = pythonapi.PyLong_FromUnsignedLong(ulong_value) if not pylong_obj2: if pythonapi.PyErr_Occurred(): print(\\"Error creating PyLongObject from unsigned long\\") pylong_obj3 = pythonapi.PyLong_FromDouble(double_value) if not pylong_obj3: if pythonapi.PyErr_Occurred(): print(\\"Error creating PyLongObject from double\\") # Converting PyLongObject instances back to C types converted_long = pythonapi.PyLong_AsLong(pylong_obj1) if converted_long == -1 and pythonapi.PyErr_Occurred(): print(\\"Error converting PyLongObject back to long\\") converted_ulong = pythonapi.PyLong_AsUnsignedLong(pylong_obj2) if converted_ulong == ctypes.c_ulong(-1).value and pythonapi.PyErr_Occurred(): print(\\"Error converting PyLongObject back to unsigned long\\") converted_double = pythonapi.PyLong_AsDouble(pylong_obj3) if converted_double == -1.0 and pythonapi.PyErr_Occurred(): print(\\"Error converting PyLongObject back to double\\") # For demonstration purpose, printing converted values print(\\"Converted long:\\", converted_long) print(\\"Converted unsigned long:\\", converted_ulong) print(\\"Converted double:\\", converted_double) except Exception as e: print(f\\"Exception occurred: {e}\\")"},{"question":"You are provided with a PyTorch model and its quantized version, and you need to evaluate the similarity between the outputs of these two models using the provided utility functions in the `torch.ao.ns.fx.utils` module. # Task Write a function `evaluate_model_similarity` that: 1. Takes as input: - `model`: A PyTorch model. - `quantized_model`: A quantized version of the same PyTorch model. - `data_loader`: A PyTorch DataLoader providing batches of input data. 2. Computes and returns a dictionary with the following metrics to assess the output similarity of `model` and `quantized_model`: - Signal-to-Quantization-Noise Ratio (SQNR) - Normalized L2 Error - Cosine Similarity # Function Signature ```python def evaluate_model_similarity(model: torch.nn.Module, quantized_model: torch.nn.Module, data_loader: torch.utils.data.DataLoader) -> dict: pass ``` # Expected Input and Output **Input:** - `model`: A PyTorch model (`torch.nn.Module`). - `quantized_model`: A quantized version of the same PyTorch model (`torch.nn.Module`). - `data_loader`: A DataLoader providing input data for the models. **Output:** - A dictionary with keys `\'sqnr\'`, `\'normalized_l2_error\'`, and `\'cosine_similarity\'` each mapping to a floating-point value representing the respective metric as calculated over the entire dataset. # Constraints - The models should be evaluated in evaluation mode (`model.eval()`). - You must use the provided utility functions `compute_sqnr`, `compute_normalized_l2_error`, and `compute_cosine_similarity` from `torch.ao.ns.fx.utils`. # Example ```python import torch import torch.nn as nn import torch.utils.data from torch.ao.ns.fx.utils import compute_sqnr, compute_normalized_l2_error, compute_cosine_similarity # Define your function here def evaluate_model_similarity(model, quantized_model, data_loader): # Implementation goes here pass # Example model, quantized model, and data loader (you need to provide real ones for actual use) model = nn.Linear(10, 2) quantized_model = torch.quantization.quantize_dynamic(model, {torch.nn.Linear}, dtype=torch.qint8) data_loader = torch.utils.data.DataLoader([torch.rand(10) for _ in range(100)], batch_size=10) # Evaluating similarity similarity_metrics = evaluate_model_similarity(model, quantized_model, data_loader) print(similarity_metrics) ``` # Note You might need to refer to the PyTorch documentation and the provided utility function docstrings for their specific usage and requirements.","solution":"import torch from torch.ao.ns.fx.utils import compute_sqnr, compute_normalized_l2_error, compute_cosine_similarity def evaluate_model_similarity(model: torch.nn.Module, quantized_model: torch.nn.Module, data_loader: torch.utils.data.DataLoader) -> dict: # Ensure models are in evaluation mode model.eval() quantized_model.eval() all_model_outputs = [] all_quantized_model_outputs = [] # Iterate through the data loader with torch.no_grad(): for data in data_loader: # Forward pass for both models model_outputs = model(data) quantized_model_outputs = quantized_model(data) # Collect outputs all_model_outputs.append(model_outputs) all_quantized_model_outputs.append(quantized_model_outputs) # Concatenate all outputs all_model_outputs = torch.cat(all_model_outputs) all_quantized_model_outputs = torch.cat(all_quantized_model_outputs) # Compute similarity metrics sqnr = compute_sqnr(all_model_outputs, all_quantized_model_outputs) normalized_l2_error = compute_normalized_l2_error(all_model_outputs, all_quantized_model_outputs) cosine_similarity = compute_cosine_similarity(all_model_outputs, all_quantized_model_outputs) return { \'sqnr\': sqnr.item(), \'normalized_l2_error\': normalized_l2_error.item(), \'cosine_similarity\': cosine_similarity.item(), }"},{"question":"**Question:** You are tasked with visualizing the relationship between different variables in the Titanic dataset using seaborn. The goal is to create a count plot that provides meaningful insights into the dataset. **Description:** 1. Load the Titanic dataset using `sns.load_dataset(\\"titanic\\")`. 2. Create a count plot that shows the number of passengers based on their travel class (`class`), their survival status (`survived`), and their gender (`sex`). 3. Normalize the counts to show percentages. 4. Customize the plot to have a legend, title, and appropriate axis labels. **Function Signature:** ```python def plot_titanic_data(): This function should: 1. Load the Titanic dataset. 2. Create a count plot showing the number of passengers based on \'class\', \'survived\', and \'sex\'. 3. Normalize the counts to show percentages. 4. Customize the plot with a legend, title, and axis labels. ``` **Constraints:** - Use seaborn for plotting. - The plot should include a legend for the \'survived\' variable. - Add a plot title \\"Titanic Passengers: Class, Survival, and Gender\\". - X-axis label should be \\"Travel Class\\". - Y-axis label should be \\"Percentage of Passengers\\". **Example Output:** A seaborn count plot with the following criteria: - X-axis: Travel class (First, Second, Third). - Hue: Survival status. - Col: Gender (Male, Female). - Counts normalized to show percentages. The function, when executed, should render the described plot.","solution":"import seaborn as sns import matplotlib.pyplot as plt def plot_titanic_data(): This function should: 1. Load the Titanic dataset. 2. Create a count plot showing the number of passengers based on \'class\', \'survived\', and \'sex\'. 3. Normalize the counts to show percentages. 4. Customize the plot with a legend, title, and axis labels. # Load the Titanic dataset titanic = sns.load_dataset(\\"titanic\\") # Create the count plot g = sns.catplot( data=titanic, kind=\\"count\\", x=\\"class\\", hue=\\"survived\\", col=\\"sex\\", height=4, aspect=0.7, palette=\\"pastel\\" ) # Normalize counts to show percentages for ax in g.axes.flatten(): total = float(len(titanic)) for p in ax.patches: percentage = \'{:.1f}%\'.format(100 * p.get_height() / total) ax.annotate(percentage, (p.get_x() + p.get_width() / 2., p.get_height()), ha = \'center\', va = \'center\', fontsize = 11, color = \'black\', xytext = (0, 8), textcoords = \'offset points\') # Customize the plot g.set_axis_labels(\\"Travel Class\\", \\"Number of Passengers\\") g.fig.suptitle(\\"Titanic Passengers: Class, Survival, and Gender\\", y=1.03) g.add_legend(title=\\"Survived\\") plt.show()"},{"question":"**TorchScript Model Transition and Debugging** You are given a PyTorch model written in pure Python which you need to transition to TorchScript. The model contains a control flow structure and makes use of both scripted and traced components. Additionally, you will be required to debug and inspect the TorchScript code and IR graphs produced. **Instructions**: 1. **Given PyTorch Model**: ```python import torch import torch.nn as nn import torch.nn.functional as F class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) self.fc1 = nn.Linear(20*20*20, 500) self.fc2 = nn.Linear(500, 10) def forward(self, x): x = F.relu(self.conv1(x)) x = F.relu(self.conv2(x)) x = x.view(-1, 20*20*20) x = F.relu(self.fc1(x)) x = self.fc2(x) return F.log_softmax(x, dim=1) def predict(self, input): with torch.no_grad(): output = self.forward(input) _, predicted = torch.max(output.data, 1) return predicted ``` 2. **Task 1**: Convert the `SimpleModel` class to TorchScript. Script the main `forward` method and trace the `conv1` and `conv2` components. Create a new `TorchScriptModel` using the converted components. 3. **Task 2**: Inspect the pretty-printed TorchScript code and the IR graph for the `forward` method of `TorchScriptModel`. Write a function called `print_torchscript_info` which prints this information. 4. **Task 3**: Debug the TorchScript model by disabling JIT compilation. Write a script based on `torchscript_debug_example.py` and run the model to demonstrate the debugging process. 5. **Submission**: - The script converting `SimpleModel` to TorchScript. - The implementation of `print_torchscript_info`. - Script demonstrating the debugging process. **Constraints**: - Ensure the TorchScript model performs equivalently to the original PyTorch model. - Use type annotations and `Final` constants for attributes where necessary. - Submit Python code along with any necessary comments/documentation. **Performance Requirements**: - The TorchScript model must be optimizable and executable in a non-Python environment. - The inspection must correctly capture model behavior in the converted TorchScript program. ```python # Your Solution Goes Here import torch import torch.nn as nn import torch.nn.functional as F from typing import Final class TorchScriptModel(nn.Module): # Define the TorchScript model with necessary annotations and converters pass def print_torchscript_info(model): # Implement inspection of pretty-printed code and IR graph pass def torchscript_debug_example(): # Implement debugging mechanism by disabling JIT as described pass # Example usages # 1. Creating the TorchScript model # 2. Printing TorchScript inspection info # 3. Demonstrating debugging process ```","solution":"import torch import torch.nn as nn import torch.nn.functional as F class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) self.fc1 = nn.Linear(20*20*20, 500) self.fc2 = nn.Linear(500, 10) def forward(self, x): x = F.relu(self.conv1(x)) x = F.relu(self.conv2(x)) x = x.view(-1, 20*20*20) x = F.relu(self.fc1(x)) x = self.fc2(x) return F.log_softmax(x, dim=1) def predict(self, input): with torch.no_grad(): output = self.forward(input) _, predicted = torch.max(output.data, 1) return predicted # Task 1: Convert to TorchScript # Create an instance of the SimpleModel simple_model = SimpleModel() # Script the model scripted_model = torch.jit.script(simple_model) # Task 2: Function to print TorchScript info def print_torchscript_info(model): print(\\"=== TorchScript Pretty-Printed Code ===\\") print(model.code) print(\\"n=== IR Graph of the Forward Method ===\\") print(model.forward.graph) # Task 3: TorchScript Debug Example def torchscript_debug_example(): input = torch.randn(1, 1, 28, 28) with torch.jit.optimized_execution(False): output = scripted_model(input) print(output) # Example usage if __name__ == \\"__main__\\": # Print TorchScript info print_torchscript_info(scripted_model) # Debug TorchScript model torchscript_debug_example()"},{"question":"You are tasked with designing a Python function to manage and process messages in different mailbox formats using the `mailbox` module. The function will move messages based on specific criteria from an mbox mailbox to a Maildir mailbox. You should implement functionality that ensures the integrity of mailbox operations and takes into account potential errors in message processing. # Requirements 1. **Function Signature:** ```python def sort_and_move_messages(mbox_path: str, maildir_path: str, criteria: dict) -> None: ``` 2. **Input Parameters:** - `mbox_path` (str): The file path to the `mbox` format mailbox. - `maildir_path` (str): The directory path to the `Maildir` format mailbox. - `criteria` (dict): A dictionary where keys are header fields and values are lists of strings. A message that contains any of the specified strings in the respective header field should be moved. 3. **Constraints:** - Preserve data integrity by using appropriate locking mechanisms. - Handle mailbox format-specific behaviors. - Ensure thread-safety as much as possible. - Ignore malformed messages that cannot be parsed. # Implementation Details Your function should: - Open and lock the mbox mailbox for reading. - Open the Maildir mailbox. Since Maildir format doesn\'t require locking, you can proceed directly. - Iterate over each message in the mbox mailbox and check if it meets the specified criteria. - If a message meets any of the criteria, copy it to the Maildir mailbox and then remove the message from the mbox mailbox. - Ensure that any changes made are flushed to the disk. - Use exception handling to manage errors from malformed messages or issues in locking/unlocking mailboxes. # Example ```python criteria = { \'subject\': [\'urgent\', \'important\'], \'from\': [\'boss@example.com\'] } sort_and_move_messages(\'/path/to/mbox\', \'/path/to/maildir\', criteria) ``` In this example, any messages from the mbox mailbox whose subject contains \'urgent\' or \'important\', or from the address \'boss@example.com\' should be moved to the Maildir mailbox. # Constraints - You should lock the mbox mailbox for reading and writing operations. - Ignore any malformed messages without terminating the program. - Use Python 3.7+ for implementation. # Note - Test your program thoroughly with different mailbox scenarios. - Consider edge cases such as empty mailboxes, no messages meeting criteria, and malformed messages.","solution":"import mailbox import shutil import os def sort_and_move_messages(mbox_path: str, maildir_path: str, criteria: dict) -> None: Moves messages from mbox to Maildir based on given criteria. Parameters: mbox_path (str): The mbox file path. maildir_path (str): The Maildir directory path. criteria (dict): Criteria to match messages. mbox = mailbox.mbox(mbox_path) maildir = mailbox.Maildir(maildir_path, factory=None, create=True) mbox.lock() try: keys_to_remove = [] for key, message in mbox.items(): if message_matches_criteria(message, criteria): copied_key = maildir.add(message) if copied_key: keys_to_remove.append(key) for key in keys_to_remove: del mbox[key] mbox.flush() mbox.unlock() finally: mbox.close() def message_matches_criteria(message, criteria): Checks if a message matches the given criteria. Parameters: message: A mailbox.message instance. criteria (dict): Criteria to match messages. Returns: bool: True if message matches criteria, otherwise False. for header, values in criteria.items(): header_value = message.get(header, \\"\\") if any(value in header_value for value in values): return True return False"},{"question":"Objective: Demonstrate your understanding of PyTorch\'s Core Aten IR by implementing a function that performs tensor operations using Core Aten IR operators. Task: Implement a function called `custom_tensor_transform` that: 1. Accepts a 2D PyTorch tensor `input_tensor` and an integer `n`. 2. Performs the following transformations on `input_tensor`: - Adds `n` to each element in the tensor. - Calculates the square of each element. - Computes the mean of all elements. The function should return a single float value which is the mean of the transformed tensor\'s elements. Requirements: - You **must** use Core Aten IR operators to perform the transformations. - Do not use inplace operations or `_out` variants. - The solution should avoid explicit loops to ensure efficient tensor operations. Input: - `input_tensor` (torch.Tensor): A 2D tensor of shape (m, k) - `n` (int): An integer to be added to each element of the tensor Output: - (float): The mean of the transformed tensor\'s elements Constraints: - The input tensor will have at most 10,000 elements. ```python def custom_tensor_transform(input_tensor: torch.Tensor, n: int) -> float: # Your implementation here pass ``` Example: ```python import torch input_tensor = torch.tensor([[1.0, 2.0], [3.0, 4.0]]) n = 2 # After adding n: [[3.0, 4.0], [5.0, 6.0]] # After squaring: [[9.0, 16.0], [25.0, 36.0]] # Mean: (9 + 16 + 25 + 36) / 4 = 21.5 result = custom_tensor_transform(input_tensor, n) print(result) # Output should be 21.5 ``` Notes: - Refer to PyTorch\'s documentation for Core Aten IR operators and their usage: https://pytorch.org/docs/stable/torch.html","solution":"import torch def custom_tensor_transform(input_tensor: torch.Tensor, n: int) -> float: # Add n to each element added_tensor = torch.ops.aten.add(input_tensor, n) # Square each element squared_tensor = torch.ops.aten.pow(added_tensor, 2) # Compute the mean of all elements mean_value = torch.ops.aten.mean(squared_tensor) # Return the result as a Python float value return mean_value.item()"},{"question":"# XML Document Manipulation Given the importance and frequent use of XML in data representation and exchange, this exercise requires you to demonstrate your understanding of the `xml.dom` package by manipulating an XML document\'s structure. Problem Description: You are required to write a Python function that performs the following operations on an XML document: 1. **Create a New XML Document:** - The root element should be named \\"library\\". 2. **Add Books to the Library:** - Each book should have a title and an author. - Use the `createElement` method to add book elements. Each book element should further contain: - A `Text` node for the title. - A `Text` node for the author. 3. **Modify a Book’s Information:** - Given a title, update the author of the book. 4. **Retrieve Titles by an Author:** - Given an author\'s name, return a list of all book titles by that author. 5. **Remove a Book by Title:** - Given the title of a book, remove it from the library. 6. **Normalize Text Nodes:** - Ensure that text nodes within elements are normalized using `normalize()` method. Function Signature: ```python def manipulate_library(actions: list) -> list: pass ``` Input: - `actions`: A list of tuples, where each tuple represents an action to be performed on the XML document. Actions can be: - `(\\"create_book\\", title: str, author: str)` - `(\\"modify_author\\", title: str, new_author: str)` - `(\\"get_titles_by_author\\", author: str)` - `(\\"remove_book\\", title: str)` - `(\\"normalize\\",)` Output: - Return a list of results based on the actions performed. For `get_titles_by_author`, return a list of book titles. For other actions, return `None`. Constraints: - Assume all titles and authors are unique strings. - Follow the given order of actions strictly. Examples: ```python actions = [ (\\"create_book\\", \\"Book One\\", \\"Author One\\"), (\\"create_book\\", \\"Book Two\\", \\"Author One\\"), (\\"create_book\\", \\"Book Three\\", \\"Author Two\\"), (\\"modify_author\\", \\"Book One\\", \\"Author Three\\"), (\\"get_titles_by_author\\", \\"Author One\\"), (\\"remove_book\\", \\"Book Two\\"), (\\"normalize\\",) ] result = manipulate_library(actions) print(result) # Output: [[\'Book Three\'], None, None] ``` Hints: - Use `xml.dom.minidom` package to work with the XML document. - Iteratively perform each action in the given order. - Ensure the methods and attributes from the `xml.dom` module are used appropriately when performing the operations on the XML document.","solution":"from xml.dom.minidom import Document def manipulate_library(actions): doc = Document() library = doc.createElement(\\"library\\") doc.appendChild(library) results = [] for action in actions: if action[0] == \\"create_book\\": title, author = action[1], action[2] book = doc.createElement(\\"book\\") title_element = doc.createElement(\\"title\\") title_text = doc.createTextNode(title) title_element.appendChild(title_text) book.appendChild(title_element) author_element = doc.createElement(\\"author\\") author_text = doc.createTextNode(author) author_element.appendChild(author_text) book.appendChild(author_element) library.appendChild(book) results.append(None) elif action[0] == \\"modify_author\\": title, new_author = action[1], action[2] books = library.getElementsByTagName(\\"book\\") for book in books: title_element = book.getElementsByTagName(\\"title\\")[0] if title_element.firstChild.nodeValue == title: author_element = book.getElementsByTagName(\\"author\\")[0] author_element.firstChild.nodeValue = new_author results.append(None) elif action[0] == \\"get_titles_by_author\\": author = action[1] titles = [] books = library.getElementsByTagName(\\"book\\") for book in books: author_element = book.getElementsByTagName(\\"author\\")[0] if author_element.firstChild.nodeValue == author: title_element = book.getElementsByTagName(\\"title\\")[0] titles.append(title_element.firstChild.nodeValue) results.append(titles) elif action[0] == \\"remove_book\\": title = action[1] books = library.getElementsByTagName(\\"book\\") for book in books: title_element = book.getElementsByTagName(\\"title\\")[0] if title_element.firstChild.nodeValue == title: library.removeChild(book) results.append(None) elif action[0] == \\"normalize\\": library.normalize() results.append(None) return results"},{"question":"Objective Implement a secure password management system that utilizes the `crypt` module to hash passwords and validate user login attempts. Tasks 1. **Password Hashing**: Implement a function `hash_password(password: str, method=None, rounds=None) -> str` that takes a plaintext password and hashes it using the strongest available method by default. If a method and/or number of rounds are provided, they should be used in generating the salt. 2. **Password Validation**: Implement a function `validate_password(stored_hash: str, password: str) -> bool` that takes a stored hashed password and a plaintext password to verify if the plaintext password matches the hashed one. Function Specifications 1. `hash_password(password: str, method=None, rounds=None) -> str`: - **Input**: - `password`: A plaintext string representing the user\'s password. - `method` (optional): A hashing method from the `crypt` module (e.g. `crypt.METHOD_SHA512`). - `rounds` (optional): Number of hashing rounds for applicable methods. - **Output**: A string representing the hashed password. 2. `validate_password(stored_hash: str, password: str) -> bool`: - **Input**: - `stored_hash`: A string representing the previously hashed password including the salt. - `password`: A plaintext string representing the user\'s password to be checked. - **Output**: A boolean value. `True` if the plaintext password matches the hashed password; `False` otherwise. # Constraints and Requirements 1. Ensure that the strongest available method (`crypt.METHOD_SHA512`) is used by default if no method is specified. 2. Properly handle the generation of salts and hashing rounds based on the method and system\'s capabilities. 3. Use `hmac.compare_digest()` or a similar constant-time comparison function to mitigate timing attacks in password validation. 4. The function should raise an appropriate exception if it encounters invalid inputs or unsupported functionality (e.g., shadow passwords). Example ```python import crypt from hmac import compare_digest as compare_hash def hash_password(password: str, method=None, rounds=None) -> str: # Implement the function based on the provided specification pass def validate_password(stored_hash: str, password: str) -> bool: # Implement the function based on the provided specification pass # Example usage: hashed_pwd = hash_password(\\"my_secure_password\\") assert validate_password(hashed_pwd, \\"my_secure_password\\") == True assert validate_password(hashed_pwd, \\"wrong_password\\") == False ``` Make sure your implementation is efficient and secure with correct handling of salts and methods as per the documentation.","solution":"import crypt from hmac import compare_digest def hash_password(password: str, method=None, rounds=None) -> str: if method is None: method = crypt.METHOD_SHA512 if rounds: salt = crypt.mksalt(method, rounds=rounds) else: salt = crypt.mksalt(method) hashed_password = crypt.crypt(password, salt) return hashed_password def validate_password(stored_hash: str, password: str) -> bool: hashed_password = crypt.crypt(password, stored_hash) return compare_digest(stored_hash, hashed_password)"},{"question":"# PyTorch Coding Assessment Question Objective: To assess students\' comprehension of tensor views in PyTorch, including creating views, modifying views, and ensuring contiguity. Problem Statement: You are provided with a 4-dimensional tensor `A` of shape (2, 3, 4, 5). Your task is to perform the following operations using PyTorch: 1. Create a tensor view `B` of `A` with shape (6, 4, 5). 2. Modify the first element of the tensor view `B` to be 3.14 and check if the corresponding element in `A` is updated. 3. Create a new tensor `C` which is a non-contiguous view of `A` by transposing the dimensions so that `C` has shape (4, 5, 2, 3). 4. Convert the non-contiguous view `C` into a contiguous tensor `D`. 5. Verify if `D` is indeed a contiguous tensor. Write a function `tensor_view_operations` that takes the tensor `A` as input and performs the required operations. The function should return `B`, the modified element from `A`, `C`, and the contiguous tensor `D`. Function Signature: ```python def tensor_view_operations(A: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]: pass ``` Input: - `A`: A 4-dimensional tensor of shape (2, 3, 4, 5). Output: - A tuple containing four tensors: 1. `B`: A tensor view of `A` with shape (6, 4, 5) 2. The modified element from `A` that corresponds to the first element of `B` 3. `C`: A non-contiguous tensor view of `A` with shape (4, 5, 2, 3) 4. `D`: A contiguous tensor converted from `C` Example: For a given tensor `A`, the function should return the corresponding tensors `B`, the modified element from `A`, `C`, and `D`. ```python import torch A = torch.arange(2 * 3 * 4 * 5).reshape(2, 3, 4, 5) B, modified_element, C, D = tensor_view_operations(A) print(\\"Shape of B:\\", B.shape) # Should be (6, 4, 5) print(\\"Modified element in A:\\", modified_element) # Should be 3.14 print(\\"Shape of C:\\", C.shape) # Should be (4, 5, 2, 3) print(\\"Is D contiguous:\\", D.is_contiguous()) # Should be True ``` Constraints: - Ensure `A` is a 4-dimensional tensor of shape (2, 3, 4, 5). - Use PyTorch functions to handle tensor operations. - Avoid unnecessary data copying for performance efficiency.","solution":"import torch from typing import Tuple def tensor_view_operations(A: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]: # Step 1: Create a tensor view `B` of `A` with shape (6, 4, 5) B = A.view(6, 4, 5) # Step 2: Modify the first element of the tensor view `B` to be 3.14 B[0, 0, 0] = 3.14 # Check if the corresponding element in `A` is updated modified_element = A[0, 0, 0, 0] # Step 3: Create a new tensor `C` which is a non-contiguous view of `A` C = A.permute(2, 3, 0, 1) # Step 4: Convert the non-contiguous view `C` into a contiguous tensor `D` D = C.contiguous() # Step 5: Verify if `D` is indeed a contiguous tensor (note: this is just for internal check, not needed in return) # is_contiguous = D.is_contiguous() return B, modified_element, C, D"},{"question":"**Dynamic Shape Handling and Guard Implementation in PyTorch** # Background In many deep learning models, input dimensions such as batch size or sequence length may vary from one run to another. PyTorch provides functionalities to handle these dynamic shapes through symbolic shapes and dynamic recompilation. Understanding and implementing these capabilities can be critical in optimizing models that require flexibility in input sizes. # Objective You are required to implement a PyTorch function that processes tensors of dynamic shapes, apply appropriate symbolic shape handling, and use guards to manage different conditions based on dynamic sizes. # Task 1. **Dynamic Shape Function:** - Implement a function named `process_dynamic_tensor` that takes two input tensors `x` and `y`, and performs the following operations: 1. Concatenate `x` and `y` along the first dimension. 2. Apply a condition: if the size of the concatenated tensor along the first dimension is greater than a specified threshold, multiply the entire tensor by 2. 3. Otherwise, add 2 to the entire tensor. 2. **Symbolic Shape Handling:** - Ensure that the function can manage dynamic shapes by marking the appropriate dimensions as dynamic. - Use symbolic propagation to propagate the size information through the tensor operations. 3. **Guards Implementation:** - Implement guards based on the dynamic sizes so that the appropriate branch of the condition is executed correctly. Ensure these guards are applied at runtime to check the conditions on tensor sizes. # Implementation Details - You can use `torch._dynamo.mark_dynamic` to mark the dimensions of the tensors as dynamic. - Ensure to handle the symbolic size propagation and set guards to address the conditional requirement. - Input tensors `x` and `y` can be of varying first dimension sizes, so the implementation should be able to handle these variations. # Expected Function Signature ```python import torch def process_dynamic_tensor(x: torch.Tensor, y: torch.Tensor, threshold: int) -> torch.Tensor: Process tensors with dynamic shapes and apply guards based on a threshold. Args: x (torch.Tensor): The first input tensor. y (torch.Tensor): The second input tensor. threshold (int): The threshold for conditional operation. Returns: torch.Tensor: The processed tensor after applying conditional operations. # Your implementation here pass ``` # Constraints - You must implement dynamic shape handling using symbolic shapes in PyTorch. - You should manage guards to handle different branches of the condition. # Example ```python x = torch.randn(3, 4) y = torch.randn(2, 4) threshold = 4 result = process_dynamic_tensor(x, y, threshold) # Result would be a tensor with size (5, 4), and modified based on the condition specified. ``` **Note**: The actual result will depend on the specific implementation and dynamic size conditions. # Evaluation - Correctness of the function implementation. - Proper use of symbolic shapes and dynamic recompilation. - Accurate implementation of guards to manage different branches. - Efficiency and performance considerations for handling dynamic shapes.","solution":"import torch def process_dynamic_tensor(x: torch.Tensor, y: torch.Tensor, threshold: int) -> torch.Tensor: Process tensors with dynamic shapes and apply guards based on a threshold. Args: x (torch.Tensor): The first input tensor. y (torch.Tensor): The second input tensor. threshold (int): The threshold for conditional operation. Returns: torch.Tensor: The processed tensor after applying conditional operations. # Concatenate the tensors along the first dimension concatenated = torch.cat((x, y), dim=0) # Get the size of the concatenated tensor along the first dimension concat_size = concatenated.size(0) # Apply conditional operation based on the threshold if concat_size > threshold: result = concatenated * 2 else: result = concatenated + 2 return result"},{"question":"Objective: Implement a function to analyze and optimize a pickled object using the `pickletools` module. Problem Statement: You are provided with a pickled object stored as a string. Your task is to write a function `analyze_and_optimize_pickle(pickle_str)` that: 1. Disassembles the provided pickle string and outputs a symbolic disassembly with annotations. 2. Returns an optimized pickle string eliminating unused \\"PUT\\" opcodes. Function Signature: ```python def analyze_and_optimize_pickle(pickle_str: str) -> str: pass ``` Input: - `pickle_str` (str): A string representing the pickled object. Output: - (str): A new optimized pickle string. Constraints: - You can use the `pickletools` module to perform the necessary operations. - Assume the input pickle string is always valid and can be disassembled and optimized. Example: ```python import pickle # Example object obj = (1, 2, \\"test\\", [3, 4], {\\"key\\": \\"value\\"}) # Pickling the object pickle_str = pickle.dumps(obj) # Analyzing and optimizing the pickled object optimized_pickle_str = analyze_and_optimize_pickle(pickle_str) # The optimized pickle string should also represent the same object assert pickle.loads(optimized_pickle_str) == obj ``` # Detailed Requirements: 1. **Disassembly**: Use `pickletools.dis` to generate a symbolic disassembly of the input pickle string and print it with annotations. 2. **Optimization**: Use `pickletools.optimize` to generate a new optimized pickle string. 3. Ensure the final optimized pickle string, when unpickled, represents the same original object. *Additional Information:* - Use the `pickletools` module, focusing on the `dis` and `optimize` functions. - The disassembly output should be clearly annotated to provide a readable inspection of the pickle. *Hints*: - You can capture the disassembly output by redirecting `sys.stdout`. - Ensure to compare the contents of the optimized pickle with the original pickle for verification.","solution":"import pickle import pickletools def analyze_and_optimize_pickle(pickle_str: bytes) -> bytes: Disassembles the provided pickle string and outputs a symbolic disassembly with annotations. Returns an optimized pickle string eliminating unused \\"PUT\\" opcodes. # Disassemble the input pickle string print(\\"Disassembly of the original pickle string:\\") pickletools.dis(pickle_str) # Optimize the pickle string optimized_pickle_str = pickletools.optimize(pickle_str) # Disassemble the optimized pickle string print(\\"nDisassembly of the optimized pickle string:\\") pickletools.dis(optimized_pickle_str) return optimized_pickle_str"},{"question":"# Advanced Python Coding Assessment Objective: You are required to demonstrate your understanding of Python\'s `importlib` module by implementing a function that dynamically imports a module and fetches a specific attribute from it. This will test your comprehension of dynamic importing techniques, error handling, and working with module attributes. Problem Statement: Implement a function `dynamic_import(module_name: str, attribute_name: str) -> any` that takes the name of a module and an attribute in that module as a string input and returns the value of the attribute. If the module or the attribute does not exist, the function should raise an `ImportError` with an appropriate error message. Expected Input and Output: 1. `dynamic_import(module_name: str, attribute_name: str) -> any` - **Input**: - `module_name`: A string representing the name of the module to be imported. Example: `\'math\'` - `attribute_name`: A string representing the attribute of the module to be retrieved. Example: `\'pi\'` - **Output**: The value of the specified attribute in the given module. Example: `3.141592653589793` Constraints: - Do not use the built-in `getattr` function; you must dynamically import the module and fetch the attribute using appropriate functions from `importlib`. - Ensure proper error handling to raise `ImportError` with a descriptive message if the module or attribute is not found. - Assume Python standard library modules for import. Example: ```python def dynamic_import(module_name: str, attribute_name: str) -> any: import importlib try: # Attempt to dynamically import the module module = importlib.import_module(module_name) except ModuleNotFoundError: raise ImportError(f\\"Module \'{module_name}\' not found.\\") try: # Fetch the attribute from the imported module attribute_value = importlib.import_module(f\\"{module_name}.{attribute_name}\\") except AttributeError: raise ImportError(f\\"Attribute \'{attribute_name}\' not found in module \'{module_name}\'.\\") return attribute_value # Examples: print(dynamic_import(\'math\', \'pi\')) # Should print: 3.141592653589793 print(dynamic_import(\'math\', \'e\')) # Should print: 2.718281828459045 print(dynamic_import(\'os\', \'path\')) # Should print: <module \'posixpath\' from \'...\'> ``` Evaluation Criteria: 1. **Correctness**: The solution should correctly import the specified module and retrieve the attribute value. 2. **Error Handling**: Proper error messages should be raised when the module or attribute is not found. 3. **Code Quality**: The code should be well-structured and adhere to Python conventions.","solution":"import importlib def dynamic_import(module_name: str, attribute_name: str) -> any: Dynamically imports a module and retrieves a specific attribute. Args: module_name (str): The name of the module to import. attribute_name (str): The name of the attribute within the module. Returns: any: The value of the specified attribute in the given module. Raises: ImportError: If the module or the attribute is not found. try: # Attempt to dynamically import the module module = importlib.import_module(module_name) except ModuleNotFoundError: raise ImportError(f\\"Module \'{module_name}\' not found.\\") try: # Fetch the attribute from the imported module attribute_value = getattr(module, attribute_name) except AttributeError: raise ImportError(f\\"Attribute \'{attribute_name}\' not found in module \'{module_name}.\'\\") return attribute_value"},{"question":"# Python Coding Assessment Question Objective: You are required to write a Python program that utilizes the `trace` module to trace the execution of a sample function, collect coverage data over multiple runs, and generate a final coverage report. Task: 1. Write a function `sample_function` that performs the following: - Takes a list of integers as input. - Returns a list with the squares of the even integers and the cubes of the odd integers. 2. Write a main function that: - Creates a `trace.Trace` object to count line executions. - Executes `sample_function` with different sets of input data to simulate multiple runs. - Accumulates and merges the coverage results. - Writes the final coverage report to a specified directory. Requirements: - You must use the `trace.Trace` class and its methods (`runfunc` and `results`). - You should merge coverage results from at least three different runs of `sample_function`. - Generate and save the final coverage report in a directory named \\"coverage_report\\". - The coverage report should include lines not executed (show missing lines). Input and Output Format: - Input: List of integers for the `sample_function` to process in each run. - Output: No direct output. Coverage results are to be saved as files in the \\"coverage_report\\" directory. Constraints: - The `sample_function` should handle lists of integers with length up to 10^4. - Ensure efficient execution and memory usage. Example: ```python def sample_function(data): result = [] for num in data: if num % 2 == 0: result.append(num ** 2) else: result.append(num ** 3) return result def main(): import sys import trace inputs = [ [1, 2, 3, 4, 5], [10, 15, 20, 25], [100, 101, 102, 103, 104] ] tracer = trace.Trace(trace=0, count=1) for input_data in inputs: tracer.runfunc(sample_function, input_data) results = tracer.results() results.write_results(show_missing=True, coverdir=\\"coverage_report\\") if __name__ == \\"__main__\\": main() ```","solution":"import trace def sample_function(data): Takes a list of integers as input. Returns a list with the squares of the even integers and the cubes of the odd integers. result = [] for num in data: if num % 2 == 0: result.append(num ** 2) else: result.append(num ** 3) return result def main(): inputs = [ [1, 2, 3, 4, 5], [10, 15, 20, 25], [100, 101, 102, 103, 104] ] tracer = trace.Trace(trace=0, count=1) for input_data in inputs: tracer.runfunc(sample_function, input_data) results = tracer.results() results.write_results(show_missing=True, coverdir=\\"coverage_report\\") if __name__ == \\"__main__\\": main()"},{"question":"You are working as a data analyst and have been given a task to organize and manipulate a set of time series data. The data contains daily sales records for products over a year. Your tasks involve creating the suitable indices for this data and performing various operations to retrieve insights. You are required to implement the following function: ```python import pandas as pd def sales_data_insights(dates, products, sales): Process and analyze the sales data using various pandas Index types. Parameters: dates (list of str): List of dates as strings in the format \'YYYY-MM-DD\'. products (list of str): List of product names corresponding to the dates. sales (list of float): List of sales amounts corresponding to the dates and products. Returns: tuple: A tuple containing the following: - min_sale_date (str): The date with the minimum total sales. - max_sale_product (str): The product with the maximum total sales. - date_check (bool): Whether the index of dates is a DatetimeIndex. - product_check (bool): Whether the product index has no duplicates. - missing_check (bool): Whether there are any missing values in the sales data. # Your code here # Example usage: dates = [\'2023-01-01\', \'2023-01-01\', \'2023-01-02\', \'2023-01-02\'] products = [\'Product_A\', \'Product_B\', \'Product_A\', \'Product_B\'] sales = [100.0, 150.0, 200.0, 50.0] print(sales_data_insights(dates, products, sales)) ``` # Constraints 1. `dates` must be a list of strings in the format \'YYYY-MM-DD\'. 2. `products` must be a list of strings with no particular format. 3. `sales` must be a list of floats representing the sales amount for corresponding dates and products. 4. All lists (`dates`, `products`, `sales`) will be of the same length. # Expected Output The function should return a tuple containing the following: 1. `min_sale_date`: The date with the minimum total sales. 2. `max_sale_product`: The product with the maximum total sales. 3. `date_check`: Boolean indicating whether the index of dates is a `DatetimeIndex`. 4. `product_check`: Boolean indicating whether the product index has no duplicates. 5. `missing_check`: Boolean indicating whether there are any missing values in the sales data. # Explanation 1. First, construct a DataFrame using the provided lists. 2. Set the appropriate index for the DataFrame using the date strings. 3. Ensure that there are no missing values in the sales data. 4. Compute the required outputs and return them as a tuple. Example Given the example usage data: ```python dates = [\'2023-01-01\', \'2023-01-01\', \'2023-01-02\', \'2023-01-02\'] products = [\'Product_A\', \'Product_B\', \'Product_A\', \'Product_B\'] sales = [100.0, 150.0, 200.0, 50.0] ``` The function should return: ```python (\'2023-01-01\', \'Product_A\', True, True, False) ``` This represents: - January 1, 2023, had the lowest total sales. - Product A had the highest total sales. - The index for dates is indeed a DatetimeIndex. - There are no duplicate product names in the index. - There are no missing values in the sales data.","solution":"import pandas as pd def sales_data_insights(dates, products, sales): Process and analyze the sales data using various pandas Index types. Parameters: dates (list of str): List of dates as strings in the format \'YYYY-MM-DD\'. products (list of str): List of product names corresponding to the dates. sales (list of float): List of sales amounts corresponding to the dates and products. Returns: tuple: A tuple containing the following: - min_sale_date (str): The date with the minimum total sales. - max_sale_product (str): The product with the maximum total sales. - date_check (bool): Whether the index of dates is a DatetimeIndex. - product_check (bool): Whether the product index has no duplicates. - missing_check (bool): Whether there are any missing values in the sales data. # Create DataFrame df = pd.DataFrame({ \'Date\': dates, \'Product\': products, \'Sales\': sales }) # Convert \'Date\' column to datetime df[\'Date\'] = pd.to_datetime(df[\'Date\']) # Calculate total sales per date total_sales_per_date = df.groupby(\'Date\')[\'Sales\'].sum() min_sale_date = total_sales_per_date.idxmin().strftime(\'%Y-%m-%d\') # Calculate total sales per product total_sales_per_product = df.groupby(\'Product\')[\'Sales\'].sum() max_sale_product = total_sales_per_product.idxmax() # Check if dates are DatetimeIndex date_check = pd.api.types.is_datetime64_any_dtype(df[\'Date\']) # Check for duplicates in product index product_check = df[\'Product\'].is_unique # Check for any missing values in the sales data missing_check = df[\'Sales\'].isnull().any() return min_sale_date, max_sale_product, date_check, product_check, missing_check"},{"question":"**Objective:** Assess the student\'s understanding of using PyTorch with HIP, including device management, memory handling, and performing tensor operations on a specified HIP device. **Problem Statement:** You are provided with a dataset in the form of a PyTorch tensor on CPU. Your task is to perform the following operations using PyTorch with HIP, ensuring that all computations are done on the specified HIP device: 1. **Device Initialization:** Check if HIP is available on the system. If HIP is not available, raise an appropriate error. 2. **Tensor Transfer and Operations:** - Transfer the given tensor to the default HIP device. - Initialize a random tensor of the same shape on the HIP device. - Perform element-wise addition of the two tensors. - Perform element-wise multiplication of the two tensors. 3. **Memory Management:** After the operations, monitor and print the memory allocated and reserved on the HIP device. 4. **Output:** Return the resulting tensors and the memory information. **Input:** - A tensor `input_tensor` of shape (N, M) on the CPU. **Output:** - Two tensors: `sum_tensor` and `product_tensor`, resulting from the element-wise addition and multiplication, respectively. - A dictionary with memory information containing keys: - `\\"memory_allocated\\"` - `\\"max_memory_allocated\\"` - `\\"memory_reserved\\"` - `\\"max_memory_reserved\\"` **Constraints:** - You may assume `input_tensor` is a 2D tensor with dimensions N and M being reasonably small to fit into GPU memory. **Function Signature:** ```python def process_with_hip(input_tensor: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, Dict[str, int]]: pass ``` **Example:** ```python input_tensor = torch.tensor([[1.0, 2.0], [3.0, 4.0]]) sum_tensor, product_tensor, memory_info = process_with_hip(input_tensor) # Output # sum_tensor: (Tensor on HIP device) [[<value>, <value>], [<value>, <value>]] # product_tensor: (Tensor on HIP device) [[<value>, <value>], [<value>, <value>]] # memory_info: {\'memory_allocated\': <value>, \'max_memory_allocated\': <value>, \'memory_reserved\': <value>, \'max_memory_reserved\': <value>} ``` **Notes:** - Ensure proper error handling for cases where HIP is not available. - Use appropriate PyTorch functions to transfer tensors between devices and manage memory efficiently. - Validate the results to ensure computations are performed on the HIP device.","solution":"import torch def process_with_hip(input_tensor: torch.Tensor): # Check if HIP is available if not torch.cuda.is_available(): raise RuntimeError(\\"HIP is not available on this system.\\") # Set the HIP device device = torch.device(\\"cuda\\") # Transfer the input tensor to the HIP device input_hip_tensor = input_tensor.to(device) # Initialize a random tensor of the same shape on the HIP device random_tensor = torch.rand_like(input_hip_tensor, device=device) # Perform element-wise addition sum_tensor = input_hip_tensor + random_tensor # Perform element-wise multiplication product_tensor = input_hip_tensor * random_tensor # Memory management memory_info = { \\"memory_allocated\\": torch.cuda.memory_allocated(device), \\"max_memory_allocated\\": torch.cuda.max_memory_allocated(device), \\"memory_reserved\\": torch.cuda.memory_reserved(device), \\"max_memory_reserved\\": torch.cuda.max_memory_reserved(device) } return sum_tensor, product_tensor, memory_info"},{"question":"**Objective:** Implement a function that takes a list of real numbers representing daily temperatures for a month and returns statistical insights about the temperatures, including mean, median, standard deviation, and the number of days with above-average temperatures. **Detailed Specifications:** 1. **Function Signature:** ```python def temperature_analysis(temperatures: list[float]) -> dict: ``` 2. **Input:** - A list of floating-point numbers, `temperatures`, where each number represents a daily temperature recorded for a month (length of the list will be 28 to 31). 3. **Output:** - A dictionary with the following keys and their corresponding values: * \\"mean\\": mean of the temperatures (float) * \\"median\\": median of the temperatures (float) * \\"std_dev\\": standard deviation of the temperatures (float) * \\"above_avg_days\\": number of days with temperatures above the mean (int) 4. **Constraints and Requirements:** - Use the `statistics` module to calculate mean, median, and standard deviation. - Iterate through the list to count the number of days with above-mean temperatures. - The function should handle edge cases, such as temperatures with a small range (e.g., all temperatures are the same). 5. **Performance:** - Ensure that the function runs efficiently within the context of typical use cases (i.e., lists of length up to 31). 6. **Example Usage:** ```python temperatures = [30.5, 32.0, 31.5, 30.0, 29.5, 33.0, 34.5, 32.5, 31.0, 30.5, 29.0, 35.0, 30.5, 28.5, 30.0, 31.5, 32.5, 33.0, 34.0, 35.5, 30.5, 32.0, 31.0, 30.0, 29.5, 28.0, 30.0, 31.0, 30.5, 32.5, 31.5] result = temperature_analysis(temperatures) print(result) # Expected Output: { # \\"mean\\": 31.0, # \\"median\\": 31.0, # \\"std_dev\\": 1.85, # \\"above_avg_days\\": 15 # } ``` Good luck and happy coding!","solution":"import statistics def temperature_analysis(temperatures): Returns statistical insights about the given temperatures. Parameters: temperatures (list): List of daily temperatures for a month. Returns: dict: Dictionary containing mean, median, standard deviation, and above average days. # Calculate mean, median, and standard deviation mean_temp = statistics.mean(temperatures) median_temp = statistics.median(temperatures) std_dev_temp = statistics.stdev(temperatures) # Calculate the number of days with temperatures above the mean above_avg_days = sum(1 for temp in temperatures if temp > mean_temp) return { \\"mean\\": mean_temp, \\"median\\": median_temp, \\"std_dev\\": std_dev_temp, \\"above_avg_days\\": above_avg_days }"},{"question":"# Question: Advanced Sequence Processing using Itertools You are tasked with designing a sequence processing function that uses several itertools functions to compute a final result based on given inputs. Problem Statement Write a function `process_sequence(data: List[int], operations: List[Tuple[str, Union[int, Callable]]]) -> List[int]` that processes an input sequence of integers (`data`) by applying a series of operations specified in the `operations` list. The operations should be applied in the order they appear in the list. Operations Each operation in the `operations` list is a tuple where: - The first element is a string specifying the operation. - The second element is either an integer or a callable (depending on the operation). The operations you should support are: 1. **\\"accumulate\\"**: Accumulate results of the binary function provided in the tuple. If no function is provided, sum is used by default. - Examples: `(\\"accumulate\\", operator.mul)` 2. **\\"dropwhile\\"**: Drop elements from the sequence as long as the predicate (provided callable) is true. - Examples: `(\\"dropwhile\\", lambda x: x < 5)` 3. **\\"takewhile\\"**: Take elements from the sequence as long as the predicate (provided callable) is true. - Examples: `(\\"takewhile\\", lambda x: x < 5)` 4. **\\"permutations\\"**: Generate all possible r-length permutations of the sequence. - Examples: `(\\"permutations\\", 2)` 5. **\\"combinations\\"**: Generate all possible r-length combinations of the sequence. - Examples: `(\\"combinations\\", 2)` 6. **\\"filterfalse\\"**: Filter elements from the sequence where the predicate (provided callable) is false. - Examples: `(\\"filterfalse\\", lambda x: x % 2 == 0)` Requirements - You must use the relevant itertools functions to perform each operation. - The function should return the final processed sequence as a list of integers. - If a given operation is not supported, raise a `ValueError` with a message specifying the unsupported operation. Function Signature ```python def process_sequence(data: List[int], operations: List[Tuple[str, Union[int, Callable]]]) -> List[int]: pass ``` Examples ```python import operator data = [1, 2, 3, 4, 5] # Example 1 operations = [(\\"accumulate\\", operator.mul), (\\"takewhile\\", lambda x: x < 100)] print(process_sequence(data, operations)) # Output: [1, 2, 6, 24] # Example 2 operations = [(\\"dropwhile\\", lambda x: x < 3), (\\"permutations\\", 2)] print(process_sequence(data, operations)) # Output: [(3, 4), (3, 5), (4, 3), ...] # Example 3 operations = [(\\"filterfalse\\", lambda x: x % 2 == 0), (\\"combinations\\", 2)] print(process_sequence(data, operations)) # Output: [(1, 3), (1, 5), (3, 5)] ``` **Note**: You may import any required functions from the itertools module and other standard libraries as necessary.","solution":"import itertools from typing import List, Tuple, Union, Callable def process_sequence(data: List[int], operations: List[Tuple[str, Union[int, Callable]]]) -> List[int]: for operation, param in operations: if operation == \\"accumulate\\": func = param if callable(param) else lambda x, y: x + y data = list(itertools.accumulate(data, func)) elif operation == \\"dropwhile\\": data = list(itertools.dropwhile(param, data)) elif operation == \\"takewhile\\": data = list(itertools.takewhile(param, data)) elif operation == \\"permutations\\": data = list(itertools.permutations(data, param)) elif operation == \\"combinations\\": data = list(itertools.combinations(data, param)) elif operation == \\"filterfalse\\": data = list(itertools.filterfalse(param, data)) else: raise ValueError(f\\"Unsupported operation: {operation}\\") return data"},{"question":"Coding Assessment Question # Objective Design a TCP server-client system using Python\'s `socket` module. The server will accept multiple client connections and echo back any messages it receives from them. # Requirements 1. **Server Implementation**: - Create a TCP server that listens on a specified port. - Accept multiple client connections. - For each connected client, receive messages sent by the client and echo the messages back to the same client. - Ensure the server handles each client in a separate thread to allow concurrent handling of multiple clients. 2. **Client Implementation**: - Create a TCP client that connects to the server and sends a series of messages. - After sending each message, the client should wait to receive the echoed message from the server and print it. - The client should also handle possible errors (e.g., connection loss, timeouts). # Expected Input and Output Formats - The server should run indefinitely, accepting and echoing messages from clients. - The client should connect to the server, send a predefined list of messages, and print the echoed responses. - Example messages for the client to send: `[\\"Hello, Server!\\", \\"How are you?\\", \\"Goodbye!\\"]`. # Constraints - Use the `socket` module only; avoid using higher-level networking libraries. - Properly handle exceptions and ensure sockets are closed gracefully. - Ensure the server can handle at least 5 simultaneous client connections without crashing. # Performance Requirements - Ensure the server and client programs are efficient and responsive. - Handle exceptions and edge cases appropriately to avoid crashes and ensure smooth operation. # Detailed Instructions 1. **Server Implementation**: - Create a class `TCPServer` with methods to start the server, accept client connections, and handle each client connection in a separate thread. - Use `socket.socket(socket.AF_INET, socket.SOCK_STREAM)` to create the server socket. - Bind the server socket to `(\'localhost\', <PORT>)`, where `<PORT>` is the port number on which the server listens. - Use `socket.listen()` to enable the server to accept connections. - For each accepted client connection, create a new thread to handle communication with that client. 2. **Client Implementation**: - Create a class `TCPClient` with methods to connect to the server, send messages, and receive echoed responses. - Use `socket.socket(socket.AF_INET, socket.SOCK_STREAM)` to create the client socket. - Connect to the server at `(\'localhost\', <PORT>)`, where `<PORT>` is the port number the server is listening on. - Send a predefined list of messages and print the received echoed responses. # Example Solution Framework ```python import socket import threading class TCPServer: def __init__(self, host=\'localhost\', port=50007): self.server_address = (host, port) self.server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM) self.server_socket.bind(self.server_address) self.server_socket.listen(5) # Listen for up to 5 connections def start(self): print(f\'Server listening on {self.server_address}\') try: while True: client_socket, client_address = self.server_socket.accept() print(f\'Connection from {client_address}\') threading.Thread(target=self.handle_client, args=(client_socket,)).start() except Exception as e: print(f\'Server error: {e}\') finally: self.server_socket.close() def handle_client(self, client_socket): try: with client_socket: while True: message = client_socket.recv(1024) if not message: break print(f\'Received: {message.decode()}\') client_socket.sendall(message) except Exception as e: print(f\'Client handling error: {e}\') class TCPClient: def __init__(self, host=\'localhost\', port=50007): self.server_address = (host, port) self.client_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM) def start(self): try: self.client_socket.connect(self.server_address) messages = [\\"Hello, Server!\\", \\"How are you?\\", \\"Goodbye!\\"] for msg in messages: self.client_socket.sendall(msg.encode()) response = self.client_socket.recv(1024) print(f\'Received echo: {response.decode()}\') except Exception as e: print(f\'Client error: {e}\') finally: self.client_socket.close() # To test, run the server and client in separate processes or threads. if __name__ == \'__main__\': server = TCPServer() threading.Thread(target=server.start).start() client = TCPClient() client.start() ``` # Task - Complete the implementation of the `TCPServer` and `TCPClient` classes by following the provided example framework. - Ensure your solution meets the requirements and constraints specified.","solution":"import socket import threading class TCPServer: def __init__(self, host=\'localhost\', port=50007): self.server_address = (host, port) self.server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM) self.server_socket.bind(self.server_address) self.server_socket.listen(5) # Listen for up to 5 connections def start(self): print(f\'Server listening on {self.server_address}\') try: while True: client_socket, client_address = self.server_socket.accept() print(f\'Connection from {client_address}\') threading.Thread(target=self.handle_client, args=(client_socket,)).start() except Exception as e: print(f\'Server error: {e}\') finally: self.server_socket.close() def handle_client(self, client_socket): try: with client_socket: while True: message = client_socket.recv(1024) if not message: break print(f\'Received: {message.decode()}\') client_socket.sendall(message) except Exception as e: print(f\'Client handling error: {e}\') finally: client_socket.close() class TCPClient: def __init__(self, host=\'localhost\', port=50007): self.server_address = (host, port) self.client_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM) def start(self): try: self.client_socket.connect(self.server_address) messages = [\\"Hello, Server!\\", \\"How are you?\\", \\"Goodbye!\\"] for msg in messages: print(f\'Sending: {msg}\') self.client_socket.sendall(msg.encode()) response = self.client_socket.recv(1024) print(f\'Received echo: {response.decode()}\') except Exception as e: print(f\'Client error: {e}\') finally: self.client_socket.close() # To test, run the server and client in separate processes or threads. if __name__ == \'__main__\': server = TCPServer() server_thread = threading.Thread(target=server.start) server_thread.start() client = TCPClient() client_thread = threading.Thread(target=client.start) client_thread.start() client_thread.join() server_thread.join()"},{"question":"# Objective This question assesses your ability to use the `doctest` module in Python to write and verify test cases within docstrings. You will implement a function and provide suitable examples in its docstring. You will then write a script to test these examples using `doctest`. # Question Statement Part 1: Function Implementation Implement a function `fizzbuzz(n)` that takes an integer `n` and returns a list of strings representing the FizzBuzz sequence up to `n`: - For multiples of 3, append \\"Fizz\\" instead of the number. - For multiples of 5, append \\"Buzz\\". - For numbers which are multiples of both 3 and 5, append \\"FizzBuzz\\". **Example:** ```python >>> fizzbuzz(15) [\'1\', \'2\', \'Fizz\', \'4\', \'Buzz\', \'Fizz\', \'7\', \'8\', \'Fizz\', \'Buzz\', \'11\', \'Fizz\', \'13\', \'14\', \'FizzBuzz\'] ``` Part 2: Writing Docstrings with Examples Add a detailed docstring to your `fizzbuzz` function. This docstring should include multiple examples showing the expected output for various inputs, similar to the example provided above. Ensure at least three different examples are provided in the docstring, covering edge cases and typical use cases. Part 3: Script to Test Using `doctest` Write a Python script that: 1. Imports the `doctest` module. 2. Executes the `doctest.testmod()` function to automatically test the examples you\'ve written in the docstring of `fizzbuzz`. Constraints - The function should only accept non-negative integers (0 <= n <= 100). - The function should handle edge cases like `fizzbuzz(0)` gracefully. # Input Format - A single non-negative integer `n`. # Output Format - A list of strings representing the FizzBuzz sequence up to `n`. # Example ```python >>> fizzbuzz(5) [\'1\', \'2\', \'Fizz\', \'4\', \'Buzz\'] >>> fizzbuzz(1) [\'1\'] >>> fizzbuzz(0) [] ``` Your task is to: 1. Implement the `fizzbuzz` function. 2. Write comprehensive docstrings with examples. 3. Create a script to test these examples using `doctest`. # Submission Submit the complete Python script implementing the function, the docstring examples, and the test script.","solution":"def fizzbuzz(n): Generates a list of strings representing the FizzBuzz sequence up to n. Args: n (int): A non-negative integer indicating the length of the FizzBuzz sequence. Returns: List[str]: A list of strings representing the FizzBuzz sequence. Examples: >>> fizzbuzz(5) [\'1\', \'2\', \'Fizz\', \'4\', \'Buzz\'] >>> fizzbuzz(1) [\'1\'] >>> fizzbuzz(15) [\'1\', \'2\', \'Fizz\', \'4\', \'Buzz\', \'Fizz\', \'7\', \'8\', \'Fizz\', \'Buzz\', \'11\', \'Fizz\', \'13\', \'14\', \'FizzBuzz\'] >>> fizzbuzz(0) [] >>> fizzbuzz(3) [\'1\', \'2\', \'Fizz\'] result = [] for i in range(1, n + 1): if i % 3 == 0 and i % 5 == 0: result.append(\\"FizzBuzz\\") elif i % 3 == 0: result.append(\\"Fizz\\") elif i % 5 == 0: result.append(\\"Buzz\\") else: result.append(str(i)) return result"},{"question":"You are tasked with creating functions to encode and decode Python version information using the macros and encoding scheme provided in the CPython documentation. Function 1: `encode_version` Write a function `encode_version(major: int, minor: int, micro: int, release_level: str, release_serial: int) -> int` that takes five arguments: - `major`: An integer representing the major version - `minor`: An integer representing the minor version - `micro`: An integer representing the micro version - `release_level`: A string representing the release level, which can be \\"a\\" (alpha), \\"b\\" (beta), \\"c\\" (release candidate), or \\"f\\" (final) - `release_serial`: An integer representing the release serial number The function should encode these values into a single integer according to the following scheme: - `major` is encoded in bits 1-8 - `minor` is encoded in bits 9-16 - `micro` is encoded in bits 17-24 - `release_level` is encoded in bits 25-28, with \\"a\\" as 0xA, \\"b\\" as 0xB, \\"c\\" as 0xC, and \\"f\\" as 0xF - `release_serial` is encoded in bits 29-32 Function 2: `decode_version` Write a function `decode_version(encoded_version: int) -> tuple` that takes one argument: - `encoded_version`: An integer representing the encoded version The function should return a tuple `(major, minor, micro, release_level, release_serial)` containing: - `major`: An integer representing the major version - `minor`: An integer representing the minor version - `micro`: An integer representing the micro version - `release_level`: A string representing the release level, which can be \\"a\\", \\"b\\", \\"c\\", or \\"f\\" - `release_serial`: An integer representing the release serial number # Constraints - `major`, `minor`, `micro`, and `release_serial` are non-negative integers. - `release_level` is one of \\"a\\", \\"b\\", \\"c\\", or \\"f\\". # Example ```python encode_version(3, 10, 0, \'f\', 0) -> 0x030a00f0 encode_version(3, 4, 1, \'a\', 2) -> 0x030401a2 decode_version(0x030a00f0) -> (3, 10, 0, \'f\', 0) decode_version(0x030401a2) -> (3, 4, 1, \'a\', 2) ``` The performance of the functions should be optimal since encoding and decoding a 32-bit number is computationally inexpensive.","solution":"def encode_version(major: int, minor: int, micro: int, release_level: str, release_serial: int) -> int: Encodes the version information into a single integer. Parameters: - major: int - minor: int - micro: int - release_level: str (\'a\', \'b\', \'c\', \'f\') - release_serial: int Returns: - int: Encoded version information release_level_mapping = {\'a\': 0xA, \'b\': 0xB, \'c\': 0xC, \'f\': 0xF} encoded_version = ( (major << 24) | (minor << 16) | (micro << 8) | (release_level_mapping[release_level] << 4) | release_serial ) return encoded_version def decode_version(encoded_version: int) -> tuple: Decodes the version information from a single integer. Parameters: - encoded_version: int Returns: - tuple: (major, minor, micro, release_level, release_serial) major = (encoded_version >> 24) & 0xFF minor = (encoded_version >> 16) & 0xFF micro = (encoded_version >> 8) & 0xFF release_level_int = (encoded_version >> 4) & 0xF release_serial = encoded_version & 0xF release_level_mapping = {0xA: \'a\', 0xB: \'b\', 0xC: \'c\', 0xF: \'f\'} release_level = release_level_mapping[release_level_int] return (major, minor, micro, release_level, release_serial)"},{"question":"**Question: Working with Real-World Datasets in Scikit-learn** You are provided with the `fetch_california_housing` dataset function from scikit-learn. Your task is to load this dataset, perform some exploratory data analysis (EDA), preprocess the data, and then build and evaluate a simple linear regression model to predict house prices. **Instructions:** 1. **Data Loading and Inspection:** - Load the California housing dataset using `fetch_california_housing` from `sklearn.datasets`. - Display the first 5 records of the dataset. - Provide basic statistics of the dataset (mean, median, standard deviation). 2. **Data Preprocessing:** - Check and handle any missing values if present. - Split the dataset into features (X) and target (y). The target variable is \'MedHouseVal\'. - Split the dataset into training and testing sets (80% train, 20% test). 3. **Model Building and Evaluation:** - Build a linear regression model using scikit-learn\'s `LinearRegression`. - Train the model on the training data. - Evaluate the model on the testing data and provide the Mean Squared Error (MSE) and R^2 score. **Constraints:** - You are required to use scikit-learn for data loading, preprocessing, and modeling. - Ensure reproducibility by setting a random seed while splitting the data. **Expected Input and Output:** - **Input:** None explicitly; the dataset is loaded within the function. - **Output:** - First 5 records of the dataset. - Basic statistics (mean, median, standard deviation). - Mean Squared Error (MSE) on the test set. - R^2 score on the test set. **Performance Requirements:** - The Mean Squared Error (MSE) should be calculated accurately, and the R^2 score should be provided as a measure of model performance. **Example Code Structure:** ```python from sklearn.datasets import fetch_california_housing from sklearn.model_selection import train_test_split from sklearn.linear_model import LinearRegression from sklearn.metrics import mean_squared_error, r2_score import pandas as pd # Step 1: Load and inspect the data housing_data = fetch_california_housing() # Display the first 5 records and basic statistics # (Complete this section) # Step 2: Data Preprocessing # Handle missing values if any X = housing_data.data y = housing_data.target # Split the data into training and testing sets # (Complete this section) # Step 3: Model Building and Evaluation model = LinearRegression() # Train the model # Evaluate the model and print MSE and R^2 score # (Complete this section) ``` Please complete the code according to the instructions provided.","solution":"from sklearn.datasets import fetch_california_housing from sklearn.model_selection import train_test_split from sklearn.linear_model import LinearRegression from sklearn.metrics import mean_squared_error, r2_score import pandas as pd import numpy as np def load_and_process_housing_data(): # Step 1: Load and inspect the data housing = fetch_california_housing(as_frame=True) df = housing.frame # Display the first 5 records of the dataset first_five_records = df.head() # Provide basic statistics of the dataset (mean, median, standard deviation) basic_statistics = df.describe().T stats_summary = { \'mean\': basic_statistics[\'mean\'], \'median\': df.median(), \'std\': basic_statistics[\'std\'] } # Step 2: Data Preprocessing # Check and handle any missing values if present # Note: fetch_california_housing dataset does not have missing values # Split the dataset into features (X) and target (y) X = df.drop(columns=\'MedHouseVal\') y = df[\'MedHouseVal\'] # Split the dataset into training and testing sets (80% train, 20% test) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Step 3: Model Building and Evaluation model = LinearRegression() # Train the model model.fit(X_train, y_train) # Evaluate the model on the testing data y_pred = model.predict(X_test) mse = mean_squared_error(y_test, y_pred) r2 = r2_score(y_test, y_pred) return first_five_records, stats_summary, mse, r2"},{"question":"**TarFile Handling and Safe Extraction** You are required to design a Python function to handle the following tasks using the `tarfile` module: 1. Create a function named `create_tar_archive` that: - Takes a list of filenames and an archive name as input. - Creates a tar archive file containing the specified files. - Uses gzip compression. 2. Create a function named `list_tar_archive_contents` that: - Takes the name of a tar archive file as input. - Prints a detailed list of files contained in the archive, similar to the output of the command `ls -l`. 3. Create a function named `safe_extract_tar_archive` that: - Takes the name of a tar archive file and an extraction path as input. - Extracts the contents of the tar archive to the specified path. - Ensures the extraction is done using the `data` filter to prevent malicious or harmful content. # Input Format - The `create_tar_archive` function: - List of filenames: `List[str]` - Archive name: `str` - The `list_tar_archive_contents` function: - Archive name: `str` - The `safe_extract_tar_archive` function: - Archive name: `str` - Extraction path: `str` # Output Format - The `list_tar_archive_contents` function: - Prints the detailed listing of the archive\'s contents, including file attributes like size, type, etc. # Constraints and Considerations - Ensure that the tarfile operations handle file paths correctly. - On extraction, the function should avoid any potential directory traversal issues and refuse absolute paths or links pointing outside the extraction directory. # Example ```python def create_tar_archive(file_list, archive_name): # Write your implementation here def list_tar_archive_contents(archive_name): # Write your implementation here def safe_extract_tar_archive(archive_name, extract_path): # Write your implementation here ``` **Usage:** ```python # Example file_list = [\\"file1.txt\\", \\"file2.txt\\", \\"dir/file3.txt\\"] archive_name = \\"example.tar.gz\\" # Create tar archive create_tar_archive(file_list, archive_name) # List contents of tar archive list_tar_archive_contents(archive_name) # Safely extract tar archive safe_extract_tar_archive(archive_name, \\"./extracted_files\\") ``` Ensure all edge cases are handled, and the functions are robust enough to handle unexpected scenarios gracefully.","solution":"import tarfile import os def create_tar_archive(file_list, archive_name): Create a tar archive containing the files in the file_list using gzip compression. with tarfile.open(archive_name, \\"w:gz\\") as tar: for filename in file_list: tar.add(filename, arcname=os.path.basename(filename)) def list_tar_archive_contents(archive_name): List the contents of a tar archive file. with tarfile.open(archive_name, \\"r:gz\\") as tar: for tarinfo in tar.getmembers(): print(f\\"{tarinfo.name} - {tarinfo.size} bytes\\") def is_within_directory(directory, target): Check if a target directory is within a base directory. abs_directory = os.path.abspath(directory) abs_target = os.path.abspath(target) return os.path.commonprefix([abs_directory, abs_target]) == abs_directory def safe_extract_tar_archive(archive_name, extract_path): Safely extract the contents of a tar archive to a specified path. with tarfile.open(archive_name, \\"r:gz\\") as tar: for member in tar.getmembers(): member_path = os.path.join(extract_path, member.name) if not is_within_directory(extract_path, member_path): raise Exception(\\"Attempted Path Traversal in Tar File\\") tar.extractall(path=extract_path)"},{"question":"# Bzip2 Compression and Decompression Utility You are provided with a directory that contains multiple text files. Your task is to implement a utility that compresses and decompresses these files using the `bz2` module. # Task 1. **Compression**: - Implement a function `compress_files(input_dir: str, output_dir: str, compresslevel: int = 9)` that takes as input: - `input_dir`: the path to the directory containing text files to compress. - `output_dir`: the path to the directory where compressed files should be stored. - `compresslevel`: compression level (default is 9). - Compress each text file in `input_dir` using the `BZ2Compressor` class incrementally (read in chunks) and store the compressed files with the same name but with a `.bz2` extension in `output_dir`. 2. **Decompression**: - Implement a function `decompress_files(input_dir: str, output_dir: str)` that takes as input: - `input_dir`: the path to the directory containing `.bz2` compressed files. - `output_dir`: the path to the directory where decompressed files should be restored. - Decompress each `.bz2` file in `input_dir` using the `BZ2Decompressor` class incrementally and store the decompressed files with their original names in `output_dir`. # Specifications - **Input/Output Formats**: - For compression, all input files are plain text files in `input_dir`, and output files should have `.bz2` extensions in `output_dir`. - For decompression, all input files are compressed with `.bz2` extensions in `input_dir`, and output files should be restored to their original filenames in `output_dir`. - **Constraints**: - Ensure proper handling of large files by reading and writing in chunks. - Use incremental compression and decompression methods (`BZ2Compressor` and `BZ2Decompressor`). - **Performance Requirements**: - Functions should efficiently handle files of size up to hundreds of megabytes without excessive memory usage. # Example Example directory structure: - input_dir: contains `file1.txt`, `file2.txt` - output_dir: should contain `file1.txt.bz2`, `file2.txt.bz2` after compression - After decompression - input_dir: contains `file1.txt.bz2`, `file2.txt.bz2` - output_dir: should contain `file1.txt`, `file2.txt` ```python import os import bz2 def compress_files(input_dir: str, output_dir: str, compresslevel: int = 9): # Create output directory if it doesn\'t exist os.makedirs(output_dir, exist_ok=True) for filename in os.listdir(input_dir): filepath = os.path.join(input_dir, filename) with open(filepath, \'rb\') as input_file, bz2.open(os.path.join(output_dir, filename + \'.bz2\'), \'wb\', compresslevel=compresslevel) as output_file: while (chunk := input_file.read(1024)): output_file.write(chunk) def decompress_files(input_dir: str, output_dir: str): # Create output directory if it doesn\'t exist os.makedirs(output_dir, exist_ok=True) for filename in os.listdir(input_dir): if filename.endswith(\'.bz2\'): filepath = os.path.join(input_dir, filename) with bz2.open(filepath, \'rb\') as input_file, open(os.path.join(output_dir, filename[:-4]), \'wb\') as output_file: while (chunk := input_file.read(1024)): output_file.write(chunk) # Example usage: # compress_files(\'input_dir\', \'output_dir\') # decompress_files(\'input_dir\', \'output_dir\') ``` # Notes: - The function should handle errors gracefully, such as file not found or read/write errors, and provide appropriate error messages. - Ensure that the compression and decompression are done efficiently, considering both time and space complexities.","solution":"import os import bz2 def compress_files(input_dir: str, output_dir: str, compresslevel: int = 9): Compress text files in the input directory and store them in the output directory using bz2 compression. Args: input_dir (str): Path to the directory containing text files to compress. output_dir (str): Path to the directory where compressed files should be stored. compresslevel (int): Compression level (default is 9). os.makedirs(output_dir, exist_ok=True) for filename in os.listdir(input_dir): input_filepath = os.path.join(input_dir, filename) output_filepath = os.path.join(output_dir, filename + \'.bz2\') with open(input_filepath, \'rb\') as input_file, bz2.open(output_filepath, \'wb\', compresslevel=compresslevel) as output_file: while (chunk := input_file.read(1024)): output_file.write(chunk) def decompress_files(input_dir: str, output_dir: str): Decompress bz2 files in the input directory and store them in the output directory. Args: input_dir (str): Path to the directory containing bz2 compressed files. output_dir (str): Path to the directory where decompressed files should be restored. os.makedirs(output_dir, exist_ok=True) for filename in os.listdir(input_dir): if filename.endswith(\'.bz2\'): input_filepath = os.path.join(input_dir, filename) output_filepath = os.path.join(output_dir, filename[:-4]) with bz2.open(input_filepath, \'rb\') as input_file, open(output_filepath, \'wb\') as output_file: while (chunk := input_file.read(1024)): output_file.write(chunk)"},{"question":"# Porting Exercise: Ensuring Python 2/3 Compatibility Objective In this exercise, you are tasked with porting a Python codebase to ensure it is compatible with both Python 2.7 and Python 3.7+. You will need to address differences in data types, division, file handling, and other aspects as highlighted in the provided documentation. The Legacy Code You are given the following legacy Python 2 code that: 1. Performs division 2. Handles text and binary data 3. Opens and reads from a file 4. Uses version detection ```python # Legacy Python 2 Code import sys def read_file(file_path): with open(file_path, \'r\') as f: content = f.read() return content def divide(a, b): return a / b def process_data(data): if isinstance(data, str): return data.encode(\'utf-8\') elif isinstance(data, bytes): return data.decode(\'utf-8\') return data def check_version(): if sys.version_info[0] == 2: print(\\"Running on Python 2\\") elif sys.version_info[0] == 3: print(\\"Running on Python 3\\") else: print(\\"Unknown Python version\\") if __name__ == \\"__main__\\": content = read_file(\\"data.txt\\") print(divide(5, 2)) print(process_data(content)) check_version() ``` Tasks 1. **Update Division**: Ensure that the division operation returns a float result in Python 2, similar to Python 3. 2. **Handle Text and Binary Data**: Modify the `process_data` function to properly handle text and binary data in both Python 2 and Python 3. 3. **File Handling**: Use `io.open()` for opening files to make it consistent across Python versions and ensure the correct modes are used. 4. **Feature Detection**: Replace the version detection logic with feature detection to ensure future compatibility. Input and Output Formats - **Input**: - A file named `data.txt` containing some text data. - Two integers `a` and `b` for the division function. - A string or bytes object for the `process_data` function. - **Output**: - Content of the file. - Result of the division. - Processed data (encoded or decoded). - Printed message indicating the Python version. Constraints - Ensure compatibility with Python 2.7 and Python 3.7+. - Avoid using external libraries that cannot be installed via `pip`. - Maintain the script\'s current functionality. Performance Requirements - The solution should handle typical use cases efficiently without noticeable performance degradation. # Your Task Port the provided legacy code to be compatible with both Python 2.7 and Python 3.7+. Implement the changes according to the tasks specified and ensure it works correctly with the provided inputs and outputs. ```python # Your updated code here ```","solution":"from __future__ import division, print_function # Ensure compatibility of division and print function import sys import io def read_file(file_path): with io.open(file_path, \'r\', encoding=\'utf-8\') as f: content = f.read() return content def divide(a, b): return a / b def process_data(data): if isinstance(data, str): return data.encode(\'utf-8\') elif isinstance(data, bytes): return data.decode(\'utf-8\') return data def check_version(): # Feature detection approach using print_function as a proxy for Python 3 try: eval(\'print(\\"\\")\', {}, {}) print(\\"Running on Python 3\\") except SyntaxError: print(\\"Running on Python 2\\") if __name__ == \\"__main__\\": content = read_file(\\"data.txt\\") print(divide(5, 2)) print(process_data(content)) check_version()"},{"question":"# Asynchronous Processing of Items Objective: Implement a function called `process_items` that processes items in an asynchronous iterator using an asynchronous processing function. This function will simulate processing a batch of items asynchronously, and you need to handle the iteration, collection, and possible exhaustion of the iterator. Function Signature: ```python import asyncio async def process_items(async_iterable, process_fn, batch_size, default=None): pass ``` Parameters: - `async_iterable`: An asynchronous iterable from which items are to be processed. - `process_fn`: An asynchronous function that processes a single item. It takes an item as an argument and returns a processed result. - `batch_size`: An integer representing the maximum number of items to process in one batch. - `default` (optional): A value to return if the iterator is exhausted. Expected Output: - Return a list of processed results. If the iterator is exhausted before completing a batch, return the results obtained up to that point. Constraints: - The processing of each item `process_fn` must be awaited. - The function should be capable of handling iterators that may exhaust at any point. - If `default` is provided, use the `default` when the iterator is exhausted, otherwise raise `StopAsyncIteration`. Example Usage: ```python import asyncio async def dummy_process(item): await asyncio.sleep(0.1) return item * 2 async def main(): async def generator(): for i in range(10): yield i await asyncio.sleep(0.1) async_iter = generator() result = await process_items(async_iter, dummy_process, 5) print(result) # Expected output: [0, 2, 4, 6, 8] result_with_default = await process_items(generator(), dummy_process, 15, default=999) print(result_with_default) # Expected output: [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 999] asyncio.run(main()) ``` # Performance Requirements: The function should handle large asynchronous iterators and efficiently process them in batches without running into memory issues.","solution":"import asyncio async def process_items(async_iterable, process_fn, batch_size, default=None): results = [] async for item in async_iterable: processed_item = await process_fn(item) results.append(processed_item) if len(results) == batch_size: return results if default is not None: results.append(default) return results"},{"question":"**Question: File Tree Processing Using `pathlib`** You have been given the task of creating a Python script that processes files and directories in a specified directory tree. The script should perform the following tasks: 1. List all the directories at the top level of the given directory. 2. Recursively list all the Python files (*.py) in the directory tree. 3. For each Python file found, perform the following: - Read the content of the file. - Count the number of lines of code (LOC) in the file (excluding empty lines and comments). - Print the path of the Python file and its LOC. Implement a function `process_directory_tree` that takes a single argument `base_directory` (a string representing the path to the base directory). The function should print the results as specified above. **Expected Input and Output Formats:** - **Input:** - `base_directory`: A string representing the path to the base directory. - **Output:** - List of directories at the top level of the `base_directory`. - Recursively list of all Python files along with their path and LOC. **Constraints:** - You may assume that the `base_directory` exists and is readable. - Focus on handling moderately large directory trees efficiently. **Example Usage:** ```python from pathlib import Path def process_directory_tree(base_directory: str): # Your implementation here # Example call of the function process_directory_tree(\'/path/to/base_directory\') ``` Output (sample): ``` Top level directories: - /path/to/base_directory/dir1 - /path/to/base_directory/dir2 Python files and their LOC: - /path/to/base_directory/dir1/file1.py: 10 LOC - /path/to/base_directory/dir2/file2.py: 20 LOC ... ``` **Hints:** - Use `Path.iterdir()` to list directories and files. - Use `Path.glob(\'**/*.py\')` to recursively find all Python files. - Consider using `Path.read_text()` to read the contents of the files. - Use string methods to count non-empty, non-comment lines.","solution":"from pathlib import Path def process_directory_tree(base_directory: str): base_path = Path(base_directory) # List all directories at the top level top_level_directories = [str(p) for p in base_path.iterdir() if p.is_dir()] # Print top-level directories print(\\"Top level directories:\\") for dir_path in top_level_directories: print(f\\"- {dir_path}\\") # Recursively list all Python files (*.py) in the directory tree python_files = base_path.glob(\'**/*.py\') print(\\"nPython files and their LOC:\\") for file_path in python_files: loc = count_lines_of_code(file_path) print(f\\"- {file_path}: {loc} LOC\\") def count_lines_of_code(file_path): content = file_path.read_text() lines = content.splitlines() code_lines = 0 for line in lines: stripped_line = line.strip() if stripped_line and not stripped_line.startswith(\'#\'): code_lines += 1 return code_lines"},{"question":"**Coding Assessment Question: Advanced Python Runtime Services** # Objective: Design and implement a Python function that utilizes data classes, context managers, and inspection tools to manage, analyze, and manipulate objects safely and effectively. # Problem Statement: Create a function named `manage_data_objects()` which performs the following tasks: 1. **Data Class Creation**: - Define a data class named `DataObject` with the following attributes: - `name` (type: `str`) - `value` (type: `int`) - `metadata` (type: `dict`, default: empty dictionary) 2. **Context Manager**: - Implement a context manager named `DataObjectManager` using `contextlib`. This context manager should: - Ensure that any metadata dictionary passed to the `DataObject` is unique for each instance. - Print the metadata upon entry and cleanup (empty the dictionary) upon exit. 3. **Object Inspection**: - Use the `inspect` module within the context manager to retrieve and print the names of all attributes of the `DataObject` instance dynamically. # Constraints: - The data class should be immutable (frozen) once created. - The context manager should enforce that no exceptions caused by invalid `metadata` alter the overall functionality. - Ensure the function is performance optimized and handles possible exceptions gracefully. # Input: The function should take the following parameters: - `object_name` (type: `str`): Name of the data object. - `object_value` (type: `int`): Value of the data object. - `object_metadata` (type: `dict`): Metadata associated with the data object. # Expected Output: The function should return a tuple containing: - The string representation of the `DataObject`. - The metadata dictionary before it was cleaned up. # Example Usage: ```python def manage_data_objects(object_name, object_value, object_metadata): # Your implementation here # Test result = manage_data_objects(\\"Sample\\", 42, {\\"key\\": \\"value\\"}) print(result) ``` **Expected Output**: ``` (\\"DataObject(name=\'Sample\', value=42, metadata={})\\", {\\"key\\": \\"value\\"}) ``` # Additional Information: - Make sure to structure the data class with the `dataclasses` module, the context manager with `contextlib`, and any inspection with the `inspect` module.","solution":"from dataclasses import dataclass, field from contextlib import contextmanager import inspect @dataclass(frozen=True) class DataObject: name: str value: int metadata: dict = field(default_factory=dict) @contextmanager def DataObjectManager(data_object: DataObject): try: # Ensure the metadata is unique and print it. unique_metadata = data_object.metadata.copy() print(\\"Entering context: metadata =\\", unique_metadata) # Retrieve and print names of all attributes of the DataObject instance. attributes = [attr for attr in inspect.getmembers(data_object) if not attr[0].startswith(\'__\')] print(\\"DataObject attributes:\\", attributes) yield data_object finally: # Cleanup the metadata print(\\"Exiting context: metadata =\\", unique_metadata) unique_metadata.clear() print(\\"Metadata after cleanup:\\", unique_metadata) def manage_data_objects(object_name, object_value, object_metadata): data_object = DataObject(name=object_name, value=object_value, metadata=object_metadata) with DataObjectManager(data_object) as managed_object: initial_metadata = managed_object.metadata.copy() return (str(managed_object), initial_metadata)"},{"question":"**Task:** Using Python\'s `sysconfig` module, create a Python script named `config_information.py` with the following functionality: 1. **Function Implementation: `get_all_paths`** - **Input:** A string corresponding to the scheme name. - **Output:** A dictionary containing all installation paths corresponding to this scheme. - **Constraints:** - You must handle the case where an invalid scheme name is provided by raising a `ValueError` with the message `\\"Invalid scheme name\\"`. - **Example:** ```python paths = get_all_paths(\'posix_prefix\') print(paths) # Output: # {\'stdlib\': \'/usr/local/lib/python3.10\', # \'platstdlib\': \'/usr/local/lib/python3.10\', # \'purelib\': \'/usr/local/lib/python3.10/site-packages\', # ...} ``` 2. **Function Implementation: `get_variable_info`** - **Input:** A list of strings where each string is a configuration variable name. - **Output:** A dictionary where keys are variable names and values are the corresponding values from the configuration. If a variable is not found, its value in the dictionary should be `None`. - **Constraints:** - You must handle the case where an empty list is provided by raising a `ValueError` with the message `\\"Variable list is empty\\"`. - **Example:** ```python variables = get_variable_info([\'Py_ENABLE_SHARED\', \'LIBDIR\', \'NON_EXISTENT_VAR\']) print(variables) # Output: # {\'Py_ENABLE_SHARED\': 0, # \'LIBDIR\': \'/usr/local/lib\', # \'NON_EXISTENT_VAR\': None} ``` 3. **Main Script Execution:** - When the script is run as the main module, it should: - Retrieve and print the current platform using the `get_platform()` method. - Retrieve and print the current Python version using the `get_python_version()` method. - Retrieve and print the default installation scheme for the current platform using the `get_default_scheme()` method. **Requirements:** - Your functions must use appropriate methods from the `sysconfig` module to gather the required information. - You should include error handling to manage invalid inputs as specified. - The script should be executable and produce the described outputs when run. ```python # config_information.py import sysconfig def get_all_paths(scheme): try: paths = sysconfig.get_paths(scheme=scheme) return paths except KeyError: raise ValueError(\\"Invalid scheme name\\") def get_variable_info(variables): if not variables: raise ValueError(\\"Variable list is empty\\") result = {} for var in variables: result[var] = sysconfig.get_config_var(var) return result if __name__ == \'__main__\': platform = sysconfig.get_platform() python_version = sysconfig.get_python_version() default_scheme = sysconfig.get_default_scheme() print(f\\"Platform: {platform}\\") print(f\\"Python version: {python_version}\\") print(f\\"Default installation scheme: {default_scheme}\\") ``` This question tests a student\'s ability to understand and utilize the `sysconfig` module, manage different forms of input, and handle errors appropriately.","solution":"import sysconfig def get_all_paths(scheme): Returns all installation paths corresponding to the provided scheme name. Parameters: scheme (str): The name of the scheme. Returns: dict: A dictionary containing all installation paths for the provided scheme. Raises: ValueError: If an invalid scheme name is provided. try: paths = sysconfig.get_paths(scheme=scheme) return paths except KeyError: raise ValueError(\\"Invalid scheme name\\") def get_variable_info(variables): Returns the configuration information for the provided variable names. Parameters: variables (list of str): A list of configuration variable names. Returns: dict: A dictionary containing the values of the provided configuration variables. Raises: ValueError: If the provided variable list is empty. if not variables: raise ValueError(\\"Variable list is empty\\") result = {} for var in variables: result[var] = sysconfig.get_config_var(var) return result if __name__ == \'__main__\': platform = sysconfig.get_platform() python_version = sysconfig.get_python_version() default_scheme = sysconfig.get_default_scheme() print(f\\"Platform: {platform}\\") print(f\\"Python version: {python_version}\\") print(f\\"Default installation scheme: {default_scheme}\\")"},{"question":"# Unix User Account and Password Database Analysis Objective: You are required to write a Python function that utilizes the `pwd` module to analyze Unix user account data and output specific information based on given criteria. Function Specification: - **Function Name**: `get_users_with_shell` - **Parameters**: - `shell`: A string representing the shell to search for. (e.g., `\\"/bin/bash\\"`) - **Returns**: - A dictionary where the keys are the user names (`pw_name`) and the values are the users\' home directories (`pw_dir`) for all users that use the specified shell. Constraints and Requirements: - The function should make use of the `pwd.getpwall()` function to retrieve the password database entries. - Filter out users whose shell matches the given `shell` parameter. - Raise a `ValueError` if no user is found with the specified shell. Input and Output Format: **Example 1:** Input: ```python shell = \\"/bin/bash\\" ``` Output: ```python { \\"user1\\": \\"/home/user1\\", \\"user2\\": \\"/home/user2\\" } ``` **Example 2:** Input: ```python shell = \\"/bin/zsh\\" ``` Output: ```python { \\"developer\\": \\"/home/developer\\" } ``` **Example 3:** Input: ```python shell = \\"/bin/invalidshell\\" ``` Output: ```python ValueError: No users found with the specified shell. ``` Additional Notes: - Ensure the code handles edge cases when the shell provided does not match any user\'s shell in the database. - Consider optimal performance in terms of iterating over the user list and filtering the results. You may assume that the system where this code runs has the standard Unix password database. The `pwd` module should be readily available in the Python standard library. Your Task: Implement the `get_users_with_shell` function based on the above specification.","solution":"import pwd def get_users_with_shell(shell): Returns a dictionary of users and their home directories for users using the specified shell. Parameters: shell (str): The shell to search for. Returns: dict: A dictionary with usernames as keys and home directories as values. Raises: ValueError: If no users are found with the specified shell. users_with_shell = {} for user in pwd.getpwall(): if user.pw_shell == shell: users_with_shell[user.pw_name] = user.pw_dir if not users_with_shell: raise ValueError(\\"No users found with the specified shell.\\") return users_with_shell"},{"question":"Title: Creating a Text-Based Markup Processor Objective: Demonstrate proficiency with Python\'s `string`, `re`, and `textwrap` modules. Problem Statement: You are tasked with creating a function `process_text` that processes a basic custom markup language for text formatting. The markup language includes the following elements: 1. **Bold Text:** Wrapped with `**`, e.g., `**bold**` should be converted to `<b>bold</b>`. 2. **Italic Text:** Wrapped with `*`, e.g., `*italic*` should be converted to `<i>italic</i>`. 3. **Headings:** Lines starting with `#` should be converted to HTML headings (e.g., `#` to `<h1>`, `` to `<h2>`, up to `` for `<h6>`). Additionally, your processed text should be wrapped to 80 characters per line. Function Signature: ```python def process_text(input_text: str) -> str: pass ``` Input: - `input_text`: A string containing the custom markup text. Constraints: 1 <= len(input_text) <= 10000. Output: - Returns a string containing the processed HTML text with lines wrapped to 80 characters. Example: ```python input_text = # Heading One This is some **bold text** and some *italic text*. Heading Two Here is another paragraph with more **bold** and *italic* items. processed_output = process_text(input_text) print(processed_output) ``` Expected Output: ```html <h1>Heading One</h1> <p>This is some <b>bold text</b> and some <i>italic text</i>.</p> <h2>Heading Two</h2> <p>Here is another paragraph with more <b>bold</b> and <i>italic</i> items.</p> ``` Constraints: - The input text can be assumed to always be valid against the custom markup rules described. - Line wrapping for paragraphs should start from the longest line that respects the 80 characters rule. Notes: - Assume paragraphs are separated by double newline characters (`nn`). - Utilize Python\'s `re` module for efficient string pattern matching and replacement. - Use Python\'s `textwrap` module to handle the line wrapping requirement. - You may use helper functions if necessary. Performance Requirements: - The function should handle the maximum input size efficiently, striving for optimal time complexity.","solution":"import re import textwrap def process_text(input_text: str) -> str: def transform_line(line): line = re.sub(r\'**(.*?)**\', r\'<b>1</b>\', line) line = re.sub(r\'*(.*?)*\', r\'<i>1</i>\', line) return line def transform_heading(line): heading_match = re.match(r\'^(#{1,6})s+(.*)\', line) if heading_match: level = len(heading_match.group(1)) content = heading_match.group(2) return f\'<h{level}>{content}</h{level}>\' return None def process_paragraph(paragraph): wrapped_paragraph = \\"n\\".join(textwrap.wrap(paragraph, width=80)) wrapped_paragraph = transform_line(wrapped_paragraph) return f\'<p>{wrapped_paragraph}</p>\' paragraphs = input_text.split(\'nn\') processed_lines = [] for paragraph in paragraphs: lines = paragraph.split(\'n\') for line in lines: heading = transform_heading(line) if heading: processed_lines.append(heading) else: processed_lines.append(process_paragraph(line)) return \\"n\\".join(processed_lines)"},{"question":"# Python Coding Assessment Objective Demonstrate your understanding of the `sunau` module by reading an AU file, manipulating its properties, and writing a new AU file. Task Write a Python function `convert_mono_to_stereo(input_file: str, output_file: str) -> None` that takes an input AU file, converts it from mono (one audio channel) to stereo (two audio channels), and writes the result to an output AU file. The audio data for the two channels in the output file should be identical (i.e., the second channel should be a copy of the first channel). Requirements 1. Read the input AU file to determine its properties and audio data. 2. Ensure the input file has exactly 1 audio channel. If not, raise an `sunau.Error` with an appropriate message. 3. Create a new AU file with the same properties except for the number of channels, which should be set to 2. 4. The audio data should be duplicated in the output file to create two identical channels. 5. Handle potential errors related to file operations or invalid data formats. Constraints - You may assume the input AU file is correctly formatted and accessible, but you must handle the specific case where the number of channels is not 1. - Ensure that the output file follows the correct AU file format with big-endian byte order. Function Signature ```python def convert_mono_to_stereo(input_file: str, output_file: str) -> None: pass ``` # Example Usage ```python convert_mono_to_stereo(\'input_mono.au\', \'output_stereo.au\') ``` Note - You must use the `sunau` module functions and objects to read and write the AU files. - Ensure the output file correctly represents stereo audio by verifying the number of channels and listening to the file if necessary.","solution":"import sunau import wave def convert_mono_to_stereo(input_file: str, output_file: str) -> None: with sunau.open(input_file, \'rb\') as input_au: n_channels, sample_width, framerate, n_frames, comptype, compname = input_au.getparams() # Ensure the input file has exactly 1 audio channel if n_channels != 1: raise sunau.Error(\\"Input file does not have exactly 1 audio channel.\\") audio_data = input_au.readframes(n_frames) with sunau.open(output_file, \'wb\') as output_au: output_au.setnchannels(2) output_au.setsampwidth(sample_width) output_au.setframerate(framerate) output_au.setcomptype(comptype, compname) stereo_audio_data = b\'\'.join( [audio_data[i:i+sample_width] * 2 for i in range(0, len(audio_data), sample_width)] ) output_au.writeframes(stereo_audio_data)"},{"question":"# HTTP Client Coding Challenge **Objective:** Implement a Python function using the `http.client` module to interact with a web service. Your task is to create a function that performs a sequence of HTTP operations and handles various scenarios such as redirection and error statuses. **Task Description:** Write a function `perform_http_operations(url, data)` that takes a URL and data parameters and performs the following operations: 1. **GET Request**: Make an HTTP GET request to the provided URL. 2. **POST Request**: If the GET request is successful (status code 200), make an HTTP POST request to the same URL with the provided data as the request body. 3. **Handle Redirection**: If the POST request results in a redirection (status code 302), follow the redirect to the new location. 4. **Handle Errors**: If any request results in a client error (status codes 400-499) or server error (status codes 500-599), handle the error by capturing the status code and reason. 5. **Response Extraction**: Extract and return the status code and response body of the final successful response. **Function Signature:** ```python def perform_http_operations(url: str, data: str) -> Tuple[int, str]: # Your implementation here ``` **Input:** - `url` (str): The URL to send the requests to. - `data` (str): The data to include in the POST request body. **Output:** - A tuple containing the status code (int) of the final successful response and the response body (str). **Constraints:** - Use only the `http.client` module for HTTP operations. - Handle both HTTP and HTTPS URLs. - Implement appropriate exception handling for network related errors. **Example Usage:** ```python url = \\"http://example.com/api\\" data = \'{\\"key\\": \\"value\\"}\' status_code, response_body = perform_http_operations(url, data) print(f\\"Status Code: {status_code}\\") print(f\\"Response Body: {response_body}\\") ``` **Notes:** - Be sure to handle redirections properly when a 302 status code is received. - For simplification, assume that the server will not send further redirects after the initial one.","solution":"import http.client import json from urllib.parse import urlparse def perform_http_operations(url: str, data: str): parsed_url = urlparse(url) connection = http.client.HTTPConnection(parsed_url.netloc) if parsed_url.scheme == \'http\' else http.client.HTTPSConnection(parsed_url.netloc) # Step 1: Perform GET request try: connection.request(\\"GET\\", parsed_url.path) response = connection.getresponse() except Exception as e: return 0, f\\"Network error: {e}\\" if response.status != 200: return response.status, response.reason response.read() # Read the response to clear the buffer # Step 2: Perform POST request if GET is successful try: headers = {\'Content-type\': \'application/json\'} connection.request(\\"POST\\", parsed_url.path, body=data, headers=headers) response = connection.getresponse() except Exception as e: return 0, f\\"Network error: {e}\\" response_body = response.read().decode() # Step 3: Handle redirection (status code 302) if response.status == 302: location = response.getheader(\'Location\') if not location: return response.status, \\"Redirection location not provided\\" connection = http.client.HTTPConnection(parsed_url.netloc) if parsed_url.scheme == \'http\' else http.client.HTTPSConnection(parsed_url.netloc) connection.request(\\"GET\\", location) response = connection.getresponse() response_body = response.read().decode() # Step 4: Handle client and server errors if 400 <= response.status < 600: return response.status, response.reason # Step 5: Return the final response code and body return response.status, response_body"},{"question":"Objective: The purpose of this assignment is to assess your understanding of the `zipfile` module, including creating, reading, writing, and extracting ZIP files. The task will require implementing functionalities that utilize various capabilities of the `zipfile` module. Problem Statement: You are required to implement a Python class, `ZipArchiveManager`, that provides several methods to handle ZIP archives. This class should be able to: 1. Create a ZIP archive from a list of files. 2. Add a file to an existing ZIP archive. 3. List the contents of an existing ZIP archive. 4. Extract specific files or all files from a ZIP archive to a given directory. 5. Validate the integrity of a ZIP archive. Here is the detailed specification for the methods: # Class: `ZipArchiveManager` 1. **Method: `create_zip(archive_name: str, files: list)`** - **Purpose**: Create a new ZIP archive containing the specified files. - **Parameters**: - `archive_name` (str): The name of the ZIP archive to create. - `files` (list): A list of file paths to include in the ZIP archive. - **Returns**: None 2. **Method: `add_file_to_zip(archive_name: str, file_path: str)`** - **Purpose**: Add a single file to an existing ZIP archive. - **Parameters**: - `archive_name` (str): The name of the ZIP archive to modify. - `file_path` (str): The path of the file to add to the ZIP archive. - **Returns**: None 3. **Method: `list_contents(archive_name: str) -> list`** - **Purpose**: List the names of all files in the specified ZIP archive. - **Parameters**: - `archive_name` (str): The name of the ZIP archive to list contents. - **Returns**: `list`: A list of file names contained within the ZIP archive. 4. **Method: `extract_files(archive_name: str, output_dir: str, files: list = None)`** - **Purpose**: Extract specific files or all files from a ZIP archive to the specified directory. - **Parameters**: - `archive_name` (str): The name of the ZIP archive to extract from. - `output_dir` (str): The directory where files should be extracted to. - `files` (list, optional): A list of file names to extract. If None, all files will be extracted. - **Returns**: None 5. **Method: `validate_zip(archive_name: str) -> bool`** - **Purpose**: Validate the integrity of the specified ZIP archive. - **Parameters**: - `archive_name` (str): The name of the ZIP archive to validate. - **Returns**: `bool`: `True` if the ZIP archive is valid, otherwise `False`. Example Usage: ```python # Create an instance of ZipArchiveManager manager = ZipArchiveManager() # Create a new ZIP archive manager.create_zip(\'example.zip\', [\'file1.txt\', \'file2.txt\']) # Add a new file to the existing ZIP archive manager.add_file_to_zip(\'example.zip\', \'file3.txt\') # List contents of the ZIP archive contents = manager.list_contents(\'example.zip\') print(contents) # Output: [\'file1.txt\', \'file2.txt\', \'file3.txt\'] # Extract specific files from the ZIP archive manager.extract_files(\'example.zip\', \'output_dir\', [\'file1.txt\']) # Validate the ZIP archive is_valid = manager.validate_zip(\'example.zip\') print(is_valid) # Output: True or False ``` Constraints: - You should handle exceptions appropriately, especially when dealing with file operations. - Ensure that the class methods are efficient and handle large files and archives gracefully. - Do not use any third-party libraries; only use the `zipfile` module and standard Python libraries.","solution":"import zipfile import os class ZipArchiveManager: def create_zip(self, archive_name: str, files: list): Create a new ZIP archive containing the specified files. with zipfile.ZipFile(archive_name, \'w\') as zipf: for file in files: zipf.write(file, os.path.basename(file)) def add_file_to_zip(self, archive_name: str, file_path: str): Add a single file to an existing ZIP archive. with zipfile.ZipFile(archive_name, \'a\') as zipf: zipf.write(file_path, os.path.basename(file_path)) def list_contents(self, archive_name: str) -> list: List the names of all files in the specified ZIP archive. with zipfile.ZipFile(archive_name, \'r\') as zipf: return zipf.namelist() def extract_files(self, archive_name: str, output_dir: str, files: list = None): Extract specific files or all files from a ZIP archive to the specified directory. with zipfile.ZipFile(archive_name, \'r\') as zipf: if files: for file in files: zipf.extract(file, output_dir) else: zipf.extractall(output_dir) def validate_zip(self, archive_name: str) -> bool: Validate the integrity of the specified ZIP archive. try: with zipfile.ZipFile(archive_name, \'r\') as zipf: return zipf.testzip() is None except zipfile.BadZipFile: return False"},{"question":"# Custom Pickler and Unpickler for Complex Stateful Class **Objective:** Write a Python class named `CustomTextReader` that can read lines from a text file and retains its state when pickled and unpickled. The class should be designed to handle the state of the file object correctly, ensuring that the line number and the current position in the file are preserved across the serialization and deserialization process. You\'ll need to implement custom methods to make this work correctly. **Requirements:** 1. Define a class `CustomTextReader` with the following attributes and methods: - `__init__(self, filename)`: Initializes the object with the filename and opens the file. - `readline(self)`: Reads the next line from the file, increments the line number. - `__getstate__(self)`: Returns the state of the object for pickling, excluding the file object. - `__setstate__(self, state)`: Restores the state of the object from the pickle data and reopens the file, seeking to the correct position. 2. Implement custom pickling and unpickling for the `CustomTextReader` class using the `pickle` module. Make sure to test your implementation with an example text file. **Input and Output:** - Input: A text file with multiple lines. - Output: Successfully pickled and unpickled `CustomTextReader` object that continues to read from the correct position in the file. **Constraints:** - The class must handle exceptions appropriately (e.g., file not found, I/O errors). - The solution should work for Python 3.6 and above. **Example Usage:** ```python import pickle # Assuming we have a file named \'sample.txt\' with multiple lines. class CustomTextReader: def __init__(self, filename): self.filename = filename self.file = open(filename) self.lineno = 0 def readline(self): self.lineno += 1 line = self.file.readline() if not line: return None return f\\"{self.lineno}: {line.rstrip()}\\" def __getstate__(self): state = self.__dict__.copy() del state[\'file\'] return state def __setstate__(self, state): self.__dict__.update(state) self.file = open(self.filename) for _ in range(self.lineno): self.file.readline() # Creating an instance of CustomTextReader reader = CustomTextReader(\'sample.txt\') # Reading few lines print(reader.readline()) print(reader.readline()) # Pickling the reader object pickled_reader = pickle.dumps(reader) # Assume some lines were added to \'sample.txt\' externally at this point # Unpickling the reader object unpickled_reader = pickle.loads(pickled_reader) print(unpickled_reader.readline()) print(unpickled_reader.readline()) ``` **Deliverables:** 1. Implement the `CustomTextReader` class with the specified methods. 2. Write example code demonstrating the pickling and unpickling process, including testing with an actual file. Submit your `CustomTextReader` class implementation and the example code as a script or Jupyter notebook.","solution":"import pickle class CustomTextReader: def __init__(self, filename): self.filename = filename self.file = open(filename) self.lineno = 0 def readline(self): self.lineno += 1 line = self.file.readline() if not line: return None return f\\"{self.lineno}: {line.rstrip()}\\" def __getstate__(self): state = self.__dict__.copy() del state[\'file\'] return state def __setstate__(self, state): self.__dict__.update(state) self.file = open(self.filename) for _ in range(self.lineno): self.file.readline() def close(self): self.file.close()"},{"question":"# Question: Compiling, Saving, and Loading a PyTorch Model with JIT You are required to demonstrate your understanding of the PyTorch JIT (Just-In-Time) compiler by performing the following tasks: 1. Define a simple neural network model using PyTorch. 2. Script the created model using the TorchScript JIT compiler. 3. Save the JIT-compiled model to a file. 4. Load the JIT-compiled model from the saved file. 5. Run a forward pass using the loaded JIT-compiled model to ensure it works correctly. Model Definition Define a simple feedforward neural network with the following structure: - An input layer with 10 units. - A hidden layer with 5 units and a ReLU activation. - An output layer with 2 units. Requirements 1. **Input Format**: - The model takes a single tensor `input_tensor` of shape `(1, 10)` as input. 2. **Output Format**: - The output should be a tensor of shape `(1, 2)`. Performance Constraints - The processing time for saving and loading the model should not exceed 2 seconds each. Here is the template to get you started: ```python import torch import torch.nn as nn import torch.jit as jit # 1. Define the model class SimpleNN(nn.Module): def __init__(self): super(SimpleNN, self).__init__() self.fc1 = nn.Linear(10, 5) self.relu = nn.ReLU() self.fc2 = nn.Linear(5, 2) def forward(self, x): x = self.fc1(x) x = self.relu(x) x = self.fc2(x) return x # Create an instance of the model model = SimpleNN() # 2. Script the model using TorchScript scripted_model = jit.script(model) # 3. Save the scripted model to a file jit.save(scripted_model, \'scripted_model.pt\') # 4. Load the scripted model from the file loaded_model = jit.load(\'scripted_model.pt\') # 5. Run a forward pass with a random input tensor input_tensor = torch.randn(1, 10) output = loaded_model(input_tensor) print(output) ``` Validate that the output of the loaded model is correct and matches the expected dimensions. Additional Notes - Ensure your scripted model can be loaded correctly from the file. - Verify the forward pass outputs a tensor of shape `(1, 2)`. Submit your complete implementation along with example inputs and outputs for validation.","solution":"import torch import torch.nn as nn import torch.jit as jit # 1. Define the model class SimpleNN(nn.Module): def __init__(self): super(SimpleNN, self).__init__() self.fc1 = nn.Linear(10, 5) self.relu = nn.ReLU() self.fc2 = nn.Linear(5, 2) def forward(self, x): x = self.fc1(x) x = self.relu(x) x = self.fc2(x) return x # Create an instance of the model model = SimpleNN() # 2. Script the model using TorchScript scripted_model = jit.script(model) # 3. Save the scripted model to a file jit.save(scripted_model, \'scripted_model.pt\') # 4. Load the scripted model from the file loaded_model = jit.load(\'scripted_model.pt\') # 5. Run a forward pass with a random input tensor def run_forward_pass(model, input_tensor): return model(input_tensor) # Example usage input_tensor = torch.randn(1, 10) output = run_forward_pass(loaded_model, input_tensor) print(output)"},{"question":"**Objective:** You are required to demonstrate your understanding of seaborn\'s style settings and customization options by creating specific visualizations. **Question:** 1. **Setting Default Styles:** - Write a function `default_style_plot` that takes no arguments. - This function should: - Set the seaborn style to `\\"whitegrid\\"`. - Create a bar plot using the following data: ```python categories = [\\"A\\", \\"B\\", \\"C\\"] values = [1, 3, 2] ``` - The x-axis should represent the categories and the y-axis should represent the values. - Display the plot. 2. **Customizing Styles:** - Write a function `custom_style_plot` that takes a dictionary as a parameter `style_params`. - This function should: - Set the seaborn style to `\\"darkgrid\\"` with the given `style_params` overriding the default parameters. - Create a line plot using the following data: ```python categories = [\\"A\\", \\"B\\", \\"C\\"] values = [1, 3, 2] ``` - The x-axis should represent the categories and the y-axis should represent the values. - Display the plot. **Constraints:** - Assume the seaborn library is imported as `sns` and matplotlib\'s `pyplot` is imported as `plt`. - No other external libraries should be used. **Performance Requirements:** - The plot creation should handle small datasets efficiently as provided. Optimization for large datasets is not necessary for this task. **Function Signature:** ```python def default_style_plot() -> None: pass def custom_style_plot(style_params: dict) -> None: pass ``` # Example ```python # Example usage of default_style_plot default_style_plot() # This will display a bar plot with the style \\"whitegrid\\" # Example usage of custom_style_plot style_params = {\\"grid.color\\": \\".6\\", \\"grid.linestyle\\": \\":\\"} custom_style_plot(style_params) # This will display a line plot with the style \\"darkgrid\\" and customized grid colors and linestyle ```","solution":"import seaborn as sns import matplotlib.pyplot as plt def default_style_plot(): Sets the seaborn style to \\"whitegrid\\" and displays a bar plot. sns.set_style(\\"whitegrid\\") categories = [\\"A\\", \\"B\\", \\"C\\"] values = [1, 3, 2] plt.figure() sns.barplot(x=categories, y=values) plt.show() def custom_style_plot(style_params): Sets the seaborn style to \\"darkgrid\\" with given style parameters and displays a line plot. Parameters: - style_params: dictionary containing style parameter overrides sns.set_style(\\"darkgrid\\", style_params) categories = [\\"A\\", \\"B\\", \\"C\\"] values = [1, 3, 2] plt.figure() sns.lineplot(x=categories, y=values) plt.show()"},{"question":"# Python Coding Assessment Question **Objective**: Demonstrate understanding of the `pwd` module and its application in accessing and managing Unix user accounts in Python. --- # Problem Statement You are required to implement a function `get_users_over_1000()`, which retrieves information about all user accounts on a Unix system where the user ID (`pw_uid`) is greater than 1000. This is a common criterion for identifying regular user accounts (as opposed to system accounts). # Function Signature ```python def get_users_over_1000() -> list: pass ``` # Input - The function does not take any input parameters. # Output - The function should return a list of dictionaries, where each dictionary contains user information for each user with a `pw_uid` greater than 1000. - Each dictionary should have the following keys corresponding to the attributes of the password database entry: - `name` (corresponding to `pw_name`) - `uid` (corresponding to `pw_uid`) - `gid` (corresponding to `pw_gid`) - `directory` (corresponding to `pw_dir`) - `shell` (corresponding to `pw_shell`) # Constraints - You can assume that the `pwd` module is available on the system where this code will run. # Examples ```python # Assuming the following entries in the password database: # [ # pwd.struct_passwd(pw_name=\'root\', pw_passwd=\'x\', pw_uid=0, pw_gid=0, pw_gecos=\'root\', pw_dir=\'/root\', pw_shell=\'/bin/bash\'), # pwd.struct_passwd(pw_name=\'user1\', pw_passwd=\'x\', pw_uid=1001, pw_gid=1001, pw_gecos=\'User One\', pw_dir=\'/home/user1\', pw_shell=\'/bin/bash\'), # pwd.struct_passwd(pw_name=\'user2\', pw_passwd=\'x\', pw_uid=1002, pw_gid=1002, pw_gecos=\'User Two\', pw_dir=\'/home/user2\', pw_shell=\'/bin/zsh\') # ] print(get_users_over_1000()) # Output: # [ # {\'name\': \'user1\', \'uid\': 1001, \'gid\': 1001, \'directory\': \'/home/user1\', \'shell\': \'/bin/bash\'}, # {\'name\': \'user2\', \'uid\': 1002, \'gid\': 1002, \'directory\': \'/home/user2\', \'shell\': \'/bin/zsh\'} # ] ``` # Notes - The function should leverage the `pwd.getpwall()` function to retrieve all password database entries and filter the results accordingly. - Ensure that the function handles possible edge cases, such as an empty password database or no users with `pw_uid` greater than 1000.","solution":"import pwd def get_users_over_1000(): Retrieves information about all user accounts on a Unix system where the user ID (pw_uid) is greater than 1000. users = pwd.getpwall() result = [] for user in users: if user.pw_uid > 1000: user_info = { \\"name\\": user.pw_name, \\"uid\\": user.pw_uid, \\"gid\\": user.pw_gid, \\"directory\\": user.pw_dir, \\"shell\\": user.pw_shell } result.append(user_info) return result"},{"question":"**Objective**: Demonstrate your understanding of the `pickletools` module and your ability to use its programmatic interface to analyze and optimize pickled data. # Problem Statement You are given several pickled files. Your task is to write a Python script that: 1. Reads each pickled file. 2. Disassembles the contents of each file and prints the disassembly to the console with annotations. 3. Optimizes the pickled string and prints the optimized pickled string to the console. # Requirements - You must use the `pickletools.dis` function to disassemble the pickled data and annotate each line with a short opcode description. - Use the `pickletools.optimize` function to optimize the pickled string by eliminating unused `PUT` opcodes. # Input - A list of file paths to pickled files. # Output - For each file: 1. Print the disassembled and annotated contents of the pickled file. 2. Print the optimized pickled string. # Constraints - Assume the list of file paths is provided as a command-line argument in the format: `python script.py file1.pickle file2.pickle ...`. - The pickled files can contain simple or complex Python objects. # Example Given the following pickled file `sample.pickle` containing a tuple `(1, 2)`: ``` python -m pickle sample.pickle (1, 2) python -m pickletools sample.pickle 0: x80 PROTO 3 2: K BININT1 1 4: K BININT1 2 6: x86 TUPLE2 7: q BINPUT 0 9: . STOP highest protocol among opcodes = 2 ``` Running your script with the command: ``` python script.py sample.pickle ``` Should output: ``` Disassembly of sample.pickle: 0: x80 PROTO 3 2: K BININT1 1 4: K BININT1 2 6: x86 TUPLE2 7: q BINPUT 0 9: . STOP highest protocol among opcodes = 2 <Annotated descriptions should be included here> Optimized pickle string: x80x02Kx01Kx02x86qx00. ``` # Implementation To help you get started, a skeleton of the script is provided below: ```python import sys import pickletools def disassemble_and_optimize(file_paths): for file_path in file_paths: with open(file_path, \'rb\') as f: pickle_data = f.read() print(f\\"Disassembly of {file_path}:\\") pickletools.dis(pickle_data, annotate=1) optimized_pickle = pickletools.optimize(pickle_data) print(\\"nOptimized pickle string:\\") print(optimized_pickle) if __name__ == \\"__main__\\": file_paths = sys.argv[1:] disassemble_and_optimize(file_paths) ``` # Notes - Ensure your script handles any potential errors gracefully, such as file not found or invalid pickle formats. - Test your script with multiple files and various pickle contents to verify its correctness.","solution":"import sys import pickletools def disassemble_and_optimize(file_paths): for file_path in file_paths: with open(file_path, \'rb\') as f: pickle_data = f.read() print(f\\"Disassembly of {file_path}:\\") pickletools.dis(pickle_data) optimized_pickle = pickletools.optimize(pickle_data) print(\\"nOptimized pickle string:\\") print(repr(optimized_pickle)) if __name__ == \\"__main__\\": file_paths = sys.argv[1:] disassemble_and_optimize(file_paths)"},{"question":"<|Analysis Begin|> The provided documentation is a segment from the \'sys\' module in Python, listing various functions, variables, and hooks related to system-specific parameters and functions. This module interacts closely with the Python interpreter and the system Python runs on. It includes details on: - Variables like `sys.argv`, `sys.byteorder`, `sys.version`, and many others. - Functions like `addaudithook()`, `audit()`, `getdefaultencoding()`, and more. - Hooks and internal methods used for debugging and profiling. - Performance and behavioral configurations (like `sys.getrecursionlimit()`, `sys.settrace()`). Several functions and attributes are highly handled by the internal workings of the Python interpreter but are exposed for debugging, profiling, system introspection, and configuration purposes. Considering the documentation scope and the need to design a coding assessment question, I will create a problem focused on students demonstrating their understanding of system-level interactions, specifically leveraging modules and features described. <|Analysis End|> <|Question Begin|> # Question: System-Level Introspection and Utilities Using the `sys` module, you are to implement a utility function in Python that can retrieve and format system-specific parameters. Specifically, you need to write a function `get_system_info` that returns a dictionary with the following details: 1. The version of the Python interpreter being used (`sys.version`). 2. The list of command-line arguments passed to the Python script (`sys.argv`). 3. The platform identifier (`sys.platform`). 4. The maximum recursion limit (`sys.getrecursionlimit()`). 5. The current limit on the length of integer string conversions (`sys.get_int_max_str_digits()`). The returned dictionary should have the following form: ```python { \\"version\\": \\"value\\", \\"argv\\": [\\"arg1\\", \\"arg2\\", ...], \\"platform\\": \\"value\\", \\"recursion_limit\\": value, \\"int_max_str_digits\\": value } ``` # Constraints and Requirements: - You **must** use the `sys` module to gather this information. - You can assume that the environment you run this in will have Python 3.10 or later. - Ensure that your solution handles cases where command-line arguments might not be provided (`argv` should include at least the script name itself). # Example: If this script is called with the command: ```sh python3 script.py arg1 arg2 ``` And the current Python version is `3.10.4`, running the function `get_system_info()` should produce a dictionary similar to: ```python { \\"version\\": \\"3.10.4 (default, Mar 21 2022, 13:50:51) [GCC 7.5.0]\\", \\"argv\\": [\\"script.py\\", \\"arg1\\", \\"arg2\\"], \\"platform\\": \\"linux\\", \\"recursion_limit\\": 3000, \\"int_max_str_digits\\": 4300 } ``` # Implementation: ```python import sys def get_system_info(): return { \\"version\\": sys.version, \\"argv\\": sys.argv, \\"platform\\": sys.platform, \\"recursion_limit\\": sys.getrecursionlimit(), \\"int_max_str_digits\\": sys.get_int_max_str_digits() } # Example usage if __name__ == \\"__main__\\": info = get_system_info() for key, value in info.items(): print(f\\"{key}: {value}\\") ``` Your task is to implement the `get_system_info` function as described above.","solution":"import sys def get_system_info(): Retrieves system-specific parameters using the sys module and returns them in a dictionary. return { \\"version\\": sys.version, \\"argv\\": sys.argv, \\"platform\\": sys.platform, \\"recursion_limit\\": sys.getrecursionlimit(), \\"int_max_str_digits\\": sys.get_int_max_str_digits() }"},{"question":"Custom Palette Analysis and Visualization with Seaborn Objective You are required to demonstrate your understanding of the seaborn library by creating and visualizing custom color palettes. Scenario You are tasked with creating a custom data visualization that demonstrates the use of seaborn\'s `blend_palette` function. The visualization should effectively use custom color palettes, interpolated between specified colors, to display a clear and meaningful result. Task 1. **Create Custom Palettes:** - Create three different color palettes using `sns.blend_palette`. One of these palettes should return a continuous colormap. - You should use different approaches and color formats for each palette (RGB hex, named colors, and continuous colormap). 2. **Generate Data:** - Generate a dataset suitable for a scatterplot and a heatmap using pandas and numpy. The scatterplot should use one of the discrete palettes to color the points, and the heatmap should use the continuous colormap. 3. **Visualize the Data:** - Create a scatterplot using seaborn\'s `scatterplot` function, and apply one of your custom discrete palettes to the points. - Create a heatmap using seaborn\'s `heatmap` function, and apply the continuous colormap to the heatmap. - Ensure that each visualization has a title and appropriate labels to convey the context clearly. Input and Output Formats - **Input:** - No specific input format. You are to generate your own dataset within the code. - **Output:** - A scatterplot with a custom discrete color palette. - A heatmap with a custom continuous colormap. Constraints - Ensure your code is well-documented, clean, and concise. - Use different interpolation methods and colors for each created palette. - Handle any potential errors gracefully. Example ```python import seaborn as sns import pandas as pd import numpy as np import matplotlib.pyplot as plt # Create a custom palette palette1 = sns.blend_palette([\\"#FF5733\\", \\"#DAF7A6\\"]) palette2 = sns.blend_palette([\\"b\\", \\"r\\", \\"g\\"], as_cmap=False) palette3 = sns.blend_palette([\\"#05668D\\", \\"#028090\\", \\"#00A896\\", \\"#02C39A\\"], as_cmap=True) # Generate Data np.random.seed(42) data = pd.DataFrame({\\"x\\": np.random.randn(100), \\"y\\": np.random.randn(100), \\"z\\": np.random.choice([0, 1, 2], size=100)}) heatmap_data = np.corrcoef(data[\\"x\\"], data[\\"y\\"]) # Visualize Scatterplot plt.figure(figsize=(12, 6)) sns.scatterplot(x=\'x\', y=\'y\', hue=\'z\', palette=palette2, data=data) plt.title(\'Custom Scatterplot with Discrete Palette\') plt.show() # Visualize Heatmap plt.figure(figsize=(12, 6)) sns.heatmap(heatmap_data, cmap=palette3, annot=True) plt.title(\'Custom Heatmap with Continuous Colormap\') plt.show() ``` This will create: 1. A scatterplot where points are colored based on the custom discrete palette. 2. A heatmap showcasing correlations using the continuous colormap palette. Show your seaborn expertise by making meaningful visualizations with customized palettes!","solution":"import seaborn as sns import pandas as pd import numpy as np import matplotlib.pyplot as plt def create_custom_palettes(): Creates three custom color palettes using sns.blend_palette. Returns: palette1: Color palette made with RGB hex colors. palette2: Color palette made with named colors. palette3: Continuous colormap. palette1 = sns.blend_palette([\\"#FF5733\\", \\"#DAF7A6\\"], n_colors=5) palette2 = sns.blend_palette([\\"blue\\", \\"red\\", \\"green\\"], n_colors=7) palette3 = sns.blend_palette([\\"#05668D\\", \\"#028090\\", \\"#00A896\\", \\"#02C39A\\"], as_cmap=True) return palette1, palette2, palette3 def generate_data(): Generates synthetic data for scatterplot and heatmap visualizations. Returns: data: DataFrame suitable for scatter plot. heatmap_data: 2D numpy array suitable for heatmap. np.random.seed(42) data = pd.DataFrame({ \\"x\\": np.random.randn(100), \\"y\\": np.random.randn(100), \\"z\\": np.random.choice([0, 1, 2], size=100) }) # Generate correlation data for heatmap heatmap_data = np.corrcoef(data[\\"x\\"], data[\\"y\\"]) return data, heatmap_data def visualize_data(data, heatmap_data, palette2, palette3): Creates and displays a scatterplot and a heatmap using custom color palettes. Args: data: DataFrame containing data for scatterplot. heatmap_data: 2D numpy array containing data for heatmap. palette2: Discrete color palette for scatterplot. palette3: Continuous colormap for heatmap. plt.figure(figsize=(10, 6)) scatter_plot = sns.scatterplot(x=\\"x\\", y=\\"y\\", hue=\\"z\\", palette=palette2, data=data) scatter_plot.set(title=\\"Custom Scatterplot with Discrete Palette\\") plt.show() plt.figure(figsize=(10, 6)) heatmap_plot = sns.heatmap(heatmap_data, cmap=palette3, annot=True, cbar=True) heatmap_plot.set(title=\\"Custom Heatmap with Continuous Colormap\\") plt.show() # Run the solution palette1, palette2, palette3 = create_custom_palettes() data, heatmap_data = generate_data() visualize_data(data, heatmap_data, palette2, palette3)"},{"question":"# Question: Implement a Mailcap Configuration Checker You are tasked with creating a Python function that utilizes the `mailcap` module to find appropriate command lines for handling files based on their MIME types. Your function should handle various MIME types, leverage user and system mailcap files, and ensure security by validating the inputs. Requirements: - Implement a function `get_mailcap_commands(mime_types: List[str], filename: str, params: List[str]) -> Dict[str, str]`. - Your function should: 1. Use `mailcap.getcaps()` to retrieve the mailcap configurations. 2. For each MIME type in `mime_types`, use `mailcap.findmatch()` to find a matching command. 3. Replace instances of `%s` in the command line with the provided `filename`. 4. Replace named parameters (e.g., `%{foo}`) with the corresponding values from `params`. 5. Validate inputs to avoid security issues, as described in the documentation. 6. Return a dictionary where keys are MIME types and values are the corresponding command lines. 7. If no command line can be found for a MIME type, provide a placeholder stating \\"No command found\\". Input: - `mime_types`: A list of MIME types (e.g., `[\'text/plain\', \'video/mpeg\']`). - `filename`: The filename to be handled (e.g., `\'document.txt\'`). - `params`: A list of parameter strings (e.g., `[\'width=1920\', \'height=1080\']`). Output: - A dictionary mapping each MIME type to its corresponding command line or \\"No command found\\" if no match is available. Constraints: - Ensure that the filename and parameters do not contain disallowed characters. If a disallowed character is found, treat it as \\"No command found\\" for that MIME type. - You may assume the `mailcap` module is already imported in your environment. Example Usage: ```python import mailcap def get_mailcap_commands(mime_types, filename, params): # Your implementation here # Example: mime_types = [\'text/plain\', \'video/mpeg\'] filename = \'video.mp4\' params = [\'width=1920\', \'height=1080\'] print(get_mailcap_commands(mime_types, filename, params)) # Expected output (example): # { # \'text/plain\': \'cat document.txt\', # \'video/mpeg\': \'xmpeg video.mp4 width=1920 height=1080\' # } ``` Note: Actual command output will depend on the mailcap files and configurations in your local environment.","solution":"import mailcap import re def validate_filename_and_params(filename, params): disallowed_chars = re.compile(r\'[^a-zA-Z0-9_-.,]\') if disallowed_chars.search(filename): return False for param in params: if disallowed_chars.search(param): return False return True def get_mailcap_commands(mime_types, filename, params): if not validate_filename_and_params(filename, params): return {mime_type: \\"No command found\\" for mime_type in mime_types} result = {} caps = mailcap.getcaps() param_dict = {param.split(\'=\')[0]: param.split(\'=\')[1] for param in params} for mime_type in mime_types: command, _ = mailcap.findmatch(caps, mime_type, filename=filename, plist=params) if command: for key, value in param_dict.items(): command = command.replace(f\'%{{{key}}}\', value) result[mime_type] = command else: result[mime_type] = \\"No command found\\" return result"},{"question":"**Coding Assessment Question: Mocking a REST API Client** # Objective: To assess students\' ability to use `unittest.mock` for creating and managing mock objects, particularly focusing on mocking method calls, tracking call order, and using side effects. # Problem Statement: You are working on a project that interacts with a REST API to fetch user data. The `RestClient` class provides methods to get user details and user posts. Your task is to implement unit tests for this class using `unittest.mock`. The `RestClient` class is defined as follows: ```python import requests class RestClient: def __init__(self, base_url): self.base_url = base_url def get_user(self, user_id): response = requests.get(f\\"{self.base_url}/users/{user_id}\\") return response.json() def get_posts(self, user_id): response = requests.get(f\\"{self.base_url}/users/{user_id}/posts\\") return response.json() ``` # Requirements: 1. **Create a Test Case Class**: - Define a test case class `TestRestClient` using `unittest.TestCase`. 2. **Mock HTTP Requests**: - Mock the `requests.get` method to simulate API responses. - Ensure the mocked `requests.get` returns appropriate JSON data for: - User details fetched via `get_user` method. - User posts fetched via `get_posts` method. 3. **Use Patch Decorators and Context Managers**: - Use patching techniques to replace `requests.get` with a mock within your tests. - Ensure the mock is correctly used and assertions verify the API endpoints were called with the expected URLs. 4. **Tracking Calls**: - Verify the order of calls made to `requests.get` when `get_user` and `get_posts` are called sequentially. 5. **Simulate Network Errors**: - Use `side_effect` to simulate network errors (e.g., connection errors) and ensure the error handling in `RestClient` is tested. 6. **Test Methods**: - Implement the following test methods: - `test_get_user_success` - `test_get_user_network_error` - `test_get_posts_success` - `test_get_posts_network_error` - `test_get_user_and_posts_sequence` # Constraints: 1. The `base_url` for the `RestClient` should be `\'http://example.com/api\'`. 2. Use the following dummy data for the mock responses: - User details: `{\\"id\\": 1, \\"name\\": \\"John Doe\\"}` - User posts: `[{\\"id\\": 1, \\"title\\": \\"Post 1\\"}, {\\"id\\": 2, \\"title\\": \\"Post 2\\"}]` # Example Implementation: Here\'s the skeleton for your test class: ```python import unittest from unittest.mock import patch, Mock, call from your_module import RestClient class TestRestClient(unittest.TestCase): @patch(\'your_module.requests.get\') def test_get_user_success(self, mock_get): # Implement this test pass @patch(\'your_module.requests.get\') def test_get_user_network_error(self, mock_get): # Implement this test pass @patch(\'your_module.requests.get\') def test_get_posts_success(self, mock_get): # Implement this test pass @patch(\'your_module.requests.get\') def test_get_posts_network_error(self, mock_get): # Implement this test pass @patch(\'your_module.requests.get\') def test_get_user_and_posts_sequence(self, mock_get): # Implement this test pass if __name__ == \'__main__\': unittest.main() ``` # Submission: Submit your implementation of the `TestRestClient` class with all required test methods.","solution":"import requests class RestClient: def __init__(self, base_url): self.base_url = base_url def get_user(self, user_id): response = requests.get(f\\"{self.base_url}/users/{user_id}\\") return response.json() def get_posts(self, user_id): response = requests.get(f\\"{self.base_url}/users/{user_id}/posts\\") return response.json()"},{"question":"**Problem Statement:** You are tasked with managing a large image processing application where memory management is crucial due to the size and volume of image data handled. Your goal is to implement a class `ImageCache` utilizing weak references to store images temporarily without preventing their garbage collection when memory is tight. This cache should ensure that as long as there is a strong reference to an image elsewhere, it should be retrievable from the cache; otherwise, it should be removed from the cache automatically. You are required to: 1. Implement the class `ImageCache` using `weakref.WeakValueDictionary`. 2. Add images to the cache with a method `add_image(name, image)`. 3. Retrieve images from the cache with a method `get_image(name)`. 4. Ensure that images are automatically removed from the cache when they are no longer strongly referenced elsewhere. 5. Implement a cleanup function that will be called when an image is garbage collected. **Requirements:** - Use `weakref.WeakValueDictionary` for storing images. - Implement a weak reference callback function that logs the removal of an image from the cache. - Use the `finalize` feature to ensure proper resource management upon object destruction. **Input and Output:** - Method `add_image(name, image)`: Takes a string `name` and an `image` object, adds it to the cache. - Method `get_image(name)`: Returns the image object associated with `name` if it still exists, otherwise returns `None`. **Constraints:** - The name of the image will be a unique string identifier. - Image objects can be of any type that supports weak references. ```python import weakref class ImageCache: def __init__(self): self._cache = weakref.WeakValueDictionary() def add_image(self, name, image): self._cache[name] = image weakref.finalize(image, self._image_finalized, name) def get_image(self, name): return self._cache.get(name) def _image_finalized(self, name): print(f\\"Image \'{name}\' has been garbage collected and removed from the cache.\\") # Example Usage: if __name__ == \\"__main__\\": image_cache = ImageCache() class Image: def __init__(self, id): self.id = id img1 = Image(1) img2 = Image(2) image_cache.add_image(\\"img1\\", img1) image_cache.add_image(\\"img2\\", img2) print(image_cache.get_image(\\"img1\\")) del img1 print(image_cache.get_image(\\"img1\\")) print(image_cache.get_image(\\"img2\\")) del img2 import gc gc.collect() print(image_cache.get_image(\\"img2\\")) ``` **Explanation:** 1. `ImageCache` uses `weakref.WeakValueDictionary` to store the images. 2. The `add_image` method associates the image with a name in the cache and sets up a finalizer to log a message when the image is garbage collected. 3. The `get_image` method retrieves the image from the cache if it still exists. 4. The example usage demonstrates adding images to the cache, retrieving them, and observing the behavior when images are garbage collected.","solution":"import weakref class ImageCache: def __init__(self): self._cache = weakref.WeakValueDictionary() def add_image(self, name, image): self._cache[name] = image weakref.finalize(image, self._image_finalized, name) def get_image(self, name): return self._cache.get(name) def _image_finalized(self, name): print(f\\"Image \'{name}\' has been garbage collected and removed from the cache.\\")"},{"question":"# Challenging Python Type Creation Exercise **Objective:** Design and implement a custom Python type with specific behavior using the `PyType_FromSpecWithBases` and related APIs provided in Python\'s type object management. **Problem Statement:** Implement a custom Python type called `CustomList` that inherits from `list`. Your `CustomList` should have additional functionality: 1. A method `even_sum` that returns the sum of all even integers in the list. 2. When an instance of `CustomList` is created, it should only accept a list of integers; otherwise, it should raise a `TypeError`. **Input and Output Formats:** - Input: A list of integers. - Example: `[1, 2, 3, 4, 5, 6]` - Output: An integer representing the sum of all even integers in the list. - Example: `12` (since 2 + 4 + 6 = 12). **Constraints:** 1. You must use the `PyType_FromSpecWithBases` or equivalent APIs to define your custom type. 2. The `even_sum` method should be callable via Python and should process integers as described. 3. Ensure proper error handling and raise appropriate exceptions if the input is invalid. **Performance Requirements:** - The solution should handle reasonably large lists efficiently. - Ensure that your type initialization and method execution are optimized for performance. **Example Usage:** ```python # Assuming the custom type is implemented and available as CustomList cl = CustomList([1, 2, 3, 4, 5, 6]) print(cl.even_sum()) # Output: 12 cl = CustomList([7, 8, 10, 13]) print(cl.even_sum()) # Output: 18 cl = CustomList([1, 1, 1, 1]) print(cl.even_sum()) # Output: 0 cl = CustomList([1, \'two\', 3, 4]) # Raises a TypeError ``` In your implementation, ensure to create the necessary type specifications and slots to achieve the desired functionalities using the APIs mentioned in the provided documentation.","solution":"class CustomList(list): def __init__(self, *args): if len(args) > 1 or not isinstance(args[0], list) or not all(isinstance(x, int) for x in args[0]): raise TypeError(\\"CustomList only accepts a list of integers\\") super().__init__(args[0]) def even_sum(self): return sum(x for x in self if x % 2 == 0)"},{"question":"Implement a Custom Communication Hook for Gradient Accumulation **Objective:** Implement a custom communication hook for PyTorch\'s DistributedDataParallel (DDP) that accumulates gradients over a specified number of iterations before performing an allreduce operation. This technique can be used to reduce the communication overhead in distributed training. **Problem Statement:** Write a function `accumulate_and_allreduce_hook` that accumulates gradients for `N` iterations before performing an allreduce operation. Additionally, create the necessary state object to keep track of the state between iterations. **Function Signature:** ```python def accumulate_and_allreduce_hook(state, bucket: torch.distributed.GradBucket) -> torch.futures.Future: pass class AccumulateState: def __init__(self, iterations: int): pass def __setstate__(self, state): pass def __getstate__(self): pass ``` # Details: 1. **Function `accumulate_and_allreduce_hook`**: - **Inputs**: - `state` (AccumulateState): The state object containing the current state of gradient accumulation. - `bucket` (torch.distributed.GradBucket): A bucket of gradient tensors to be allreduced. - **Outputs**: - A `torch.futures.Future` object that will contain the result of the allreduce operation when the accumulation condition is met. 2. **Class `AccumulateState`**: - **Attributes**: - `iterations` (int): The number of iterations to accumulate gradients before performing the allreduce. - `current_iteration` (int): The current iteration count. - `accumulated_gradients` (torch.Tensor): A tensor to accumulate gradients. - **Methods**: - `__init__(self, iterations: int)`: Initializes the state with the given number of iterations. - `__setstate__(self, state)`: Restores the state from a serialized form. - `__getstate__(self)`: Returns the current state in a serializable form. 3. **Constraints**: - Assume the model and tensor dimensions do not change during training. - Assume a minimum of 2 GPUs. - Ensure that the hook correctly accumulates gradients and performs the allreduce operation after the specified number of iterations. # Example Usage: 1. Instantiate the model, optimizer, and DDP module. 2. Create an instance of `AccumulateState` with `N` iterations. 3. Register the `accumulate_and_allreduce_hook` with the DDP model using the created state. 4. Train the model in a distributed setting, ensuring gradients are accumulated and reduced correctly. ```python import torch import torch.nn as nn import torch.optim as optim import torch.distributed as dist from torch.nn.parallel import DistributedDataParallel from torch.distributed.algorithms.ddp_comm_hooks import default_hooks class SimpleModel(nn.Module): def __init__(self): super().__init__() self.fc1 = nn.Linear(24, 24) self.relu = nn.ReLU() self.fc2 = nn.Linear(24, 12) def forward(self, x): return self.fc2(self.relu(self.fc1(x))) def train(rank, world_size): dist.init_process_group(\\"gloo\\", rank=rank, world_size=world_size) model = SimpleModel().to(rank) ddp_model = DistributedDataParallel(model, device_ids=[rank]) # Initialize state and register custom hook state = AccumulateState(iterations=5) ddp_model.register_comm_hook(state, accumulate_and_allreduce_hook) optimizer = optim.SGD(ddp_model.parameters(), lr=0.001) loss_fn = nn.MSELoss() # Dummy training loop inputs = torch.randn(10, 24).to(rank) labels = torch.randn(10, 12).to(rank) for epoch in range(10): optimizer.zero_grad() outputs = ddp_model(inputs) loss = loss_fn(outputs, labels) loss.backward() optimizer.step() dist.destroy_process_group() if __name__ == \\"__main__\\": torch.multiprocessing.spawn(train, args=(2,), nprocs=2) ``` **Note**: Ensure you handle gradient accumulation correctly and test your implementation in a distributed environment.","solution":"import torch import torch.distributed as dist from torch.distributed import ReduceOp from torch.futures import Future import torch.nn as nn import torch.optim as optim from torch.nn.parallel import DistributedDataParallel class AccumulateState: def __init__(self, iterations: int): Initializes the state with the given number of iterations. Args: iterations (int): Number of iterations to accumulate gradients before performing allreduce. self.iterations = iterations self.current_iteration = 0 self.accumulated_gradients = None def __setstate__(self, state): self.__dict__.update(state) def __getstate__(self): return self.__dict__ def accumulate_and_allreduce_hook(state, bucket: torch.distributed.GradBucket) -> torch.futures.Future: Hook to accumulate gradients for a certain number of iterations before performing an allreduce. Args: state (AccumulateState): State object to keep track of gradient accumulation. bucket (torch.distributed.GradBucket): Gradients bucket. Returns: torch.futures.Future: The Future object that will hold the result of the allreduce. if state.current_iteration == 0: state.accumulated_gradients = bucket.tensors().clone() else: state.accumulated_gradients += bucket.tensors() state.current_iteration += 1 if state.current_iteration == state.iterations: fut = dist.all_reduce(state.accumulated_gradients, op=ReduceOp.SUM).then( lambda _: (state.accumulated_gradients / state.iterations).detach() ) state.current_iteration = 0 return fut else: fut = Future() fut.set_result(bucket.tensors()) return fut"},{"question":"**Objective**: Demonstrate your understanding of file compression and decompression using Python\'s `gzip` module. Problem Statement You are given a large text file containing a collection of articles. For performance and storage efficiency, you need to implement two functions: 1. **`compress_file(input_path: str, output_path: str) -> None`**: - **Input**: - `input_path`: A string representing the path to the input text file that needs to be compressed. - `output_path`: A string representing the path where the compressed file should be saved. - **Output**: - This function should compress the contents of the input file and save it to the output path provided. - **Constraints**: - The input file can be very large, so ensure your solution is efficient in terms of memory usage. 2. **`decompress_file(input_path: str, output_path: str) -> None`**: - **Input**: - `input_path`: A string representing the path to the gzip-compressed file that needs to be decompressed. - `output_path`: A string representing the path where the decompressed file should be saved. - **Output**: - This function should decompress the contents of the gzip file and save it to the output path provided. - **Constraints**: - The compressed file could be large, so ensure your solution is efficient in terms of memory usage. Additional Requirement - Handle any exceptions that might be raised during the compression and decompression process. Example Usage ```python # Assuming the input text file is \'articles.txt\' and you want to save # the compressed file as \'articles.txt.gz\' and the decompressed file as \'articles_uncompressed.txt\' compress_file(\'articles.txt\', \'articles.txt.gz\') decompress_file(\'articles.txt.gz\', \'articles_uncompressed.txt\') ``` # Evaluation Criteria - **Correctness**: The functions should correctly compress and decompress the files. - **Efficiency**: The implementation should handle large files efficiently. - **Exception Handling**: Proper handling of exceptions during file operations. - **Code Quality**: The code should be clean, well-documented, and easy to understand.","solution":"import gzip import shutil def compress_file(input_path: str, output_path: str) -> None: Compress the content of the file at input_path and save it at output_path. Parameters: - input_path: the path to the input file that needs to be compressed. - output_path: the path where the compressed file should be saved. This function handles potential exceptions during file operations. try: with open(input_path, \'rb\') as f_in: with gzip.open(output_path, \'wb\') as f_out: shutil.copyfileobj(f_in, f_out) except Exception as e: print(f\\"An error occurred while compressing the file: {e}\\") def decompress_file(input_path: str, output_path: str) -> None: Decompress the content of the gzip file at input_path and save it at output_path. Parameters: - input_path: the path to the gzip-compressed file that needs to be decompressed. - output_path: the path where the decompressed file should be saved. This function handles potential exceptions during file operations. try: with gzip.open(input_path, \'rb\') as f_in: with open(output_path, \'wb\') as f_out: shutil.copyfileobj(f_in, f_out) except Exception as e: print(f\\"An error occurred while decompressing the file: {e}\\")"},{"question":"# Question: High-Performance Computation on MacOS using PyTorch MPS Backend You are given a PyTorch neural network model and you need to run it on a MacOS device using the MPS backend. Implement the following functions to demonstrate your understanding of using the MPS backend for high-performance computation: 1. **check_mps_available**: This function checks if the MPS backend is available on the current MacOS device. - **Input**: None - **Output**: Print appropriate messages based on the availability of the MPS backend. If MPS backend is available, return `True`. Otherwise, return `False`. 2. **move_to_mps**: This function moves a given tensor to the MPS device. - **Input**: A PyTorch tensor - **Output**: The tensor moved to the MPS device if MPS is available; otherwise, return the original tensor. 3. **train_model_on_mps**: This function moves the given model to the MPS device, performs a forward pass with some input data (which is also moved to the MPS device), and returns the model output. - **Input**: - A PyTorch model (e.g., `torch.nn.Module`). - A tensor representing input data. - **Output**: The output tensor resulting from the model\'s forward pass on the input data, both processed on the MPS device if available. ```python import torch def check_mps_available(): Checks if the MPS backend is available on the current MacOS device. Output: Prints appropriate messages based on the availability of the MPS backend. Returns: bool: True if MPS backend is available, False otherwise. if not torch.backends.mps.is_available(): if not torch.backends.mps.is_built(): print(\\"MPS not available because the current PyTorch install was not built with MPS enabled.\\") else: print(\\"MPS not available because the current MacOS version is not 12.3+ and/or you do not have an MPS-enabled device on this machine.\\") return False else: print(\\"MPS backend is available.\\") return True def move_to_mps(tensor): Moves a given tensor to the MPS device if MPS is available. Input: tensor (torch.Tensor): The input tensor to be moved. Output: torch.Tensor: The tensor moved to the MPS device if available, otherwise the original tensor. if check_mps_available(): mps_device = torch.device(\\"mps\\") return tensor.to(mps_device) else: return tensor def train_model_on_mps(model, data): Moves a given model and input data to the MPS device, performs a forward pass, and returns the model output. Input: model (torch.nn.Module): The PyTorch model to be moved to the MPS device. data (torch.Tensor): The input data tensor to be fed into the model. Output: torch.Tensor: The output tensor from the model\'s forward pass. if check_mps_available(): mps_device = torch.device(\\"mps\\") model.to(mps_device) data = data.to(mps_device) return model(data) else: return model(data) # Example usage: # Assuming you have a simple model defined as YourFavoriteNet and some input tensor x # model = YourFavoriteNet() # x = torch.ones(5, 4) # Example input # output = train_model_on_mps(model, x) # print(output) ``` **Constraints**: - Assume that the student will have a suitable MacOS device and necessary PyTorch installation to test the code. - YourFavoriteNet should be replaced with any pre-defined or custom PyTorch neural network class for actual testing. **Performance Requirements**: - Ensure that the checks for MPS availability do not excessively delay the program execution. - The tensor and model movements to the MPS device should be efficient, leveraging PyTorch\'s internal mechanisms designed for such operations.","solution":"import torch def check_mps_available(): Checks if the MPS backend is available on the current MacOS device. Output: Prints appropriate messages based on the availability of the MPS backend. Returns: bool: True if MPS backend is available, False otherwise. if not torch.backends.mps.is_available(): if not torch.backends.mps.is_built(): print(\\"MPS not available because the current PyTorch install was not built with MPS enabled.\\") else: print(\\"MPS not available because the current MacOS version is not 12.3+ and/or you do not have an MPS-enabled device on this machine.\\") return False else: print(\\"MPS backend is available.\\") return True def move_to_mps(tensor): Moves a given tensor to the MPS device if MPS is available. Input: tensor (torch.Tensor): The input tensor to be moved. Output: torch.Tensor: The tensor moved to the MPS device if available, otherwise the original tensor. if check_mps_available(): mps_device = torch.device(\\"mps\\") return tensor.to(mps_device) else: return tensor def train_model_on_mps(model, data): Moves a given model and input data to the MPS device, performs a forward pass, and returns the model output. Input: model (torch.nn.Module): The PyTorch model to be moved to the MPS device. data (torch.Tensor): The input data tensor to be fed into the model. Output: torch.Tensor: The output tensor from the model\'s forward pass. if check_mps_available(): mps_device = torch.device(\\"mps\\") model.to(mps_device) data = data.to(mps_device) return model(data) else: return model(data)"},{"question":"# Question: Advanced Visualization with Seaborn You are provided with a dataset of temperatures recorded for different cities over a span of years. Your task is to perform data manipulation and create a visualization that shows the temperature trends for two specified cities over the years using `seaborn`\'s objects interface. Dataset The dataset is in CSV format with the following columns: - `date`: Date of the temperature recording (format: YYYY-MM-DD). - `city`: The city where the temperature was recorded. - `temperature`: The recorded temperature in Celsius. Requirements 1. Load the dataset into a pandas DataFrame. 2. Filter the dataset to keep only the records for the cities \\"CityA\\" and \\"CityB\\". 3. Extract the year from the `date` column and add it as a new column. 4. Plot the data using seaborn’s objects interface: - The x-axis should represent the years. - The y-axis should represent the mean temperature for each year. - Create a line plot for each city showing the mean annual temperature. - Use different colors for the two cities. - Add a band around each line that represents the standard deviation of the annual temperatures for that city. Implementation 1. Write a function `plot_temperature_trends(filepath)` that: - Takes `filepath` as input, which is the path to the CSV file. - Loads and processes the data as described above. - Plots the data meeting the requirements. Example Assuming the dataset is saved as `temperatures.csv`, calling `plot_temperature_trends(\'temperatures.csv\')` should produce a plot resembling the structure specified. Constraints - Use seaborn and pandas for data manipulation and visualization. - Your function should handle datasets efficiently without unnecessary computations. **Input:** - A string `filepath` representing the path to the dataset CSV file. **Output:** - A seaborn plot showing the temperature trends for \\"CityA\\" and \\"CityB\\". ```python def plot_temperature_trends(filepath): # Your code here pass ``` **Note:** Ensure that you provide the necessary import statements in your solution.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def plot_temperature_trends(filepath): # Load the dataset df = pd.read_csv(filepath) # Filter the dataset for the cities \\"CityA\\" and \\"CityB\\" df_filtered = df[df[\'city\'].isin([\'CityA\', \'CityB\'])] # Extract the year from the `date` column and add it as a new column df_filtered[\'year\'] = pd.to_datetime(df_filtered[\'date\']).dt.year # Group by \'city\' and \'year\' to get mean and standard deviation of temperatures df_grouped = df_filtered.groupby([\'city\', \'year\']).agg( mean_temperature=(\'temperature\', \'mean\'), std_temperature=(\'temperature\', \'std\') ).reset_index() # Plotting using seaborn\'s objects interface p = sns.lmplot( data=df_grouped, x=\'year\', y=\'mean_temperature\', hue=\'city\', markers=\'o\', ci=None, aspect=2 ) # Add standard deviation as confidence interval band around the lines for city in df_grouped[\'city\'].unique(): city_data = df_grouped[df_grouped[\'city\'] == city] plt.fill_between( city_data[\'year\'], city_data[\'mean_temperature\'] - city_data[\'std_temperature\'], city_data[\'mean_temperature\'] + city_data[\'std_temperature\'], alpha=0.3 ) plt.xlabel(\'Year\') plt.ylabel(\'Mean Temperature (°C)\') plt.title(\'Temperature Trends for CityA and CityB\') plt.show()"},{"question":"**Question: Working with Complex Numbers in Python and C** You will create a Python class `PyCComplex` that simulates the C structure `Py_complex` for complex number operations. Your class should: 1. Initialize complex numbers using `real` and `imaginary` parts. 2. Implement common complex number operations (addition, subtraction, negation, multiplication, division, exponentiation) reflecting the C functions provided (`_Py_c_sum`, `_Py_c_diff`, etc.). 3. Offer methods to convert between your `PyCComplex` instances and Python complex number objects. # Class Definition and Requirements 1. **Class `PyCComplex`**: - **Attributes**: - `real`: The real part of the complex number. - `imag`: The imaginary part of the complex number. - **Methods**: - `__init__(self, real: float, imag: float)`: Initializes the complex number. - `__add__(self, other: \'PyCComplex\') -> \'PyCComplex\'`: Adds two `PyCComplex` numbers (`_Py_c_sum`). - `__sub__(self, other: \'PyCComplex\') -> \'PyCComplex\'`: Subtracts `other` from `self` (`_Py_c_diff`). - `__neg__(self) -> \'PyCComplex\'`: Negates the complex number (`_Py_c_neg`). - `__mul__(self, other: \'PyCComplex\') -> \'PyCComplex\'`: Multiplies two `PyCComplex` numbers (`_Py_c_prod`). - `__truediv__(self, other: \'PyCComplex\') -> \'PyCComplex\'`: Divides `self` by `other` (`_Py_c_quot`). - `__pow__(self, other: \'PyCComplex\') -> \'PyCComplex\'`: Raises `self` to the power of `other` (`_Py_c_pow`). - `to_pycomplex(self) -> complex`: Converts `PyCComplex` to the built-in Python complex type. - `from_pycomplex(cls, c: complex) -> \'PyCComplex\'`: Class method that converts a Python complex number to `PyCComplex`. # Example Usage ```python # Creating two PyCComplex instances c1 = PyCComplex(3.0, 4.0) c2 = PyCComplex(1.0, 2.0) # Performing operations sum_res = c1 + c2 diff_res = c1 - c2 neg_res = -c1 prod_res = c1 * c2 quot_res = c1 / c2 pow_res = c1 ** c2 # Conversions between PyCComplex and built-in complex py_c = c1.to_pycomplex() c3 = PyCComplex.from_pycomplex(py_c) ``` # Testing Your Implementation Ensure your class handles various scenarios and edge cases: - Regular operations with different real and imaginary parts. - Zero division check in the division. - Handling of large exponents in the power function. # Constraints - You can assume all inputs are valid and of the correct type. - Your methods should mimic the behavior of their C counterparts as closely as possible. - Performance is not the primary concern, but your implementation should be efficient for typical use cases. Implement the `PyCComplex` class and test it using different complex number operations to verify correctness.","solution":"import cmath class PyCComplex: def __init__(self, real: float, imag: float): self.real = real self.imag = imag def __add__(self, other: \'PyCComplex\') -> \'PyCComplex\': return PyCComplex(self.real + other.real, self.imag + other.imag) def __sub__(self, other: \'PyCComplex\') -> \'PyCComplex\': return PyCComplex(self.real - other.real, self.imag - other.imag) def __neg__(self) -> \'PyCComplex\': return PyCComplex(-self.real, -self.imag) def __mul__(self, other: \'PyCComplex\') -> \'PyCComplex\': real = self.real * other.real - self.imag * other.imag imag = self.real * other.imag + self.imag * other.real return PyCComplex(real, imag) def __truediv__(self, other: \'PyCComplex\') -> \'PyCComplex\': if other.real == 0 and other.imag == 0: raise ZeroDivisionError(\'division by zero\') denom = other.real ** 2 + other.imag ** 2 real = (self.real * other.real + self.imag * other.imag) / denom imag = (self.imag * other.real - self.real * other.imag) / denom return PyCComplex(real, imag) def __pow__(self, other: \'PyCComplex\') -> \'PyCComplex\': c1 = complex(self.real, self.imag) c2 = complex(other.real, other.imag) result = c1 ** c2 return PyCComplex(result.real, result.imag) def to_pycomplex(self) -> complex: return complex(self.real, self.imag) @classmethod def from_pycomplex(cls, c: complex) -> \'PyCComplex\': return cls(c.real, c.imag)"},{"question":"# Question: You are tasked with analyzing a dataset that contains information about different species of flowers, including attributes such as sepal length, sepal width, petal length, and petal width. You will use the Seaborn library to create visualizations that help in understanding the distribution of these attributes. Your task is to write a Python function `create_flower_visualization(data)`. This function should: 1. Accept a pandas DataFrame `data` that contains the flower dataset. 2. Generate a pairplot of the attributes using a light color palette specified in the function parameters. 3. The color palette should be created using the following requirements: - Use `light_palette()` to create a sequential palette. - Specify a base color using a hex code. - Optionally, increase the number of colors in the palette. Your function should meet the following specifications: - **Input**: A pandas DataFrame `data` with columns `[\'sepal_length\', \'sepal_width\', \'petal_length\', \'petal_width\', \'species\']`. - **Output**: The function should display a Seaborn pairplot using the specified light color palette. - You may assume that the DataFrame contains no missing values. - The base color for the palette should be `#3498db`. - The function should increase the number of colors to 10. # Function Signature: ```python import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def create_flower_visualization(data: pd.DataFrame) -> None: pass ``` # Example: ```python import pandas as pd # Example data data = pd.DataFrame({ \'sepal_length\': [5.1, 4.9, 4.7, 4.6, 5.0], \'sepal_width\': [3.5, 3.0, 3.2, 3.1, 3.6], \'petal_length\': [1.4, 1.4, 1.3, 1.5, 1.4], \'petal_width\': [0.2, 0.2, 0.2, 0.2, 0.2], \'species\': [\'setosa\', \'setosa\', \'setosa\', \'setosa\', \'setosa\'] }) # Generate visualization create_flower_visualization(data) ``` Upon executing the function above, a pairplot of the attributes should be displayed, using a light color palette of base color `#3498db` and containing 10 different shades.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def create_flower_visualization(data: pd.DataFrame) -> None: Generates a pairplot of the flower dataset attributes using a light color palette. :param data: A pandas DataFrame with columns \'sepal_length\', \'sepal_width\', \'petal_length\', \'petal_width\', \'species\'. # Create a light sequential color palette base_color = \\"#3498db\\" palette = sns.light_palette(base_color, n_colors=10) # Generate the pairplot sns.pairplot(data, hue=\'species\', palette=palette) plt.show()"},{"question":"Email Message Text Extraction **Objective:** Write a Python function that processes an email message object and extracts all the lines of text from its payloads, optionally decoding them. The function should filter to include only subparts that match a specified MIME type (maintype and subtype). **Function Signature:** ```python def extract_text_lines(msg, maintype=\'text\', subtype=None, decode=False): Extract all lines of text from the payloads of an email message object. Parameters: - msg: An email message object to process. - maintype (str): The main MIME type to filter subparts. Defaults to \'text\'. - subtype (str): The MIME subtype to filter subparts. Optional. - decode (bool): Whether to decode the payloads. Defaults to False. Returns: - List[str]: A list of strings, where each string is a line of text from the filtered subparts\' payloads. pass ``` **Inputs:** - `msg`: An email message object that you will process. The structure and payloads of the object will vary. - `maintype` (optional): The main MIME type to filter subparts. Defaults to \'text\'. - `subtype` (optional): The MIME subtype to filter subparts. When specified, filters subparts based on both main and subtype. If omitted, matches only the main type. - `decode` (optional): A boolean indicating whether to decode the payloads. Defaults to `False`. **Outputs:** - A list of strings, where each string is a line of text extracted from the payloads of the filtered subparts. **Constraints:** - The function should skip subparts that do not have a payload of Python string type. - The function should skip over all subpart headers. **Performance Requirements:** The function should efficiently iterate over the email message object tree and handle payload extraction. **Example:** ```python from email import message_from_string email_content = MIME-Version: 1.0 Content-Type: multipart/mixed; boundary=\\"===============5730799132695966272==\\" --===============5730799132695966272== Content-Type: text/plain; charset=\\"utf-8\\" Content-Transfer-Encoding: 7bit Hello, this is the first part. It is plain text. --===============5730799132695966272== Content-Type: text/html; charset=\\"utf-8\\" Content-Transfer-Encoding: 7bit <html> <body> <p>This is HTML content.</p> </body> </html> --===============5730799132695966272==-- msg = message_from_string(email_content) print(extract_text_lines(msg)) # Expected Output: # [\'Hello, this is the first part.\', \'It is plain text.\'] print(extract_text_lines(msg, maintype=\'text\', subtype=\'html\')) # Expected Output: # [\'<html>\', \'<body>\', \'<p>This is HTML content.</p>\', \'</body>\', \'</html>\'] ```","solution":"def extract_text_lines(msg, maintype=\'text\', subtype=None, decode=False): Extract all lines of text from the payloads of an email message object. Parameters: - msg: An email message object to process. - maintype (str): The main MIME type to filter subparts. Defaults to \'text\'. - subtype (str): The MIME subtype to filter subparts. Optional. - decode (bool): Whether to decode the payloads. Defaults to False. Returns: - List[str]: A list of strings, where each string is a line of text from the filtered subparts\' payloads. text_lines = [] if msg.is_multipart(): for part in msg.walk(): if part.get_content_maintype() == maintype and (subtype is None or part.get_content_subtype() == subtype): if decode: payload = part.get_payload(decode=True) if payload is not None: payload = payload.decode(part.get_content_charset() or \'utf-8\') else: payload = part.get_payload(decode=False) if isinstance(payload, str): lines = payload.splitlines() text_lines.extend(lines) else: if msg.get_content_maintype() == maintype and (subtype is None or msg.get_content_subtype() == subtype): if decode: payload = msg.get_payload(decode=True) if payload is not None: payload = payload.decode(msg.get_content_charset() or \'utf-8\') else: payload = msg.get_payload(decode=False) if isinstance(payload, str): lines = payload.splitlines() text_lines.extend(lines) return text_lines"},{"question":"**Question Title**: \\"Python Development Mode: Debugging and Resource Management\\" **Question Description**: You are asked to write a Python script that demonstrates the use of Python Development Mode to detect common issues in resource management and coroutine handling. By doing so, you will display your understanding of key features of Python Development Mode. # Part 1: Resource Management Write a Python function `count_lines_in_file(file_path)` that counts the number of lines in a text file. The function should: 1. Open the file specified by `file_path` and count the number of lines. 2. Ensure the file is properly closed using a context manager to avoid resource leaks. ```python def count_lines_in_file(file_path: str) -> int: pass ``` # Part 2: Coroutine Handling Write a coroutine `fetch_data(api_url)` that simulates fetching data from an API and processes it. 1. Use `asyncio.sleep` to simulate network delay. 2. Log a message indicating when the coroutine starts and ends. 3. Ensure all created coroutines are awaited to avoid warnings. ```python async def fetch_data(api_url: str) -> None: pass ``` # Main Script Combine both parts into a script `main.py` that: 1. Uses the `count_lines_in_file` function to count lines in a file specified via the command line. 2. Creates and runs multiple instances of `fetch_data` coroutine. 3. Ensures proper handling of any warnings or errors using Python Development Mode. # Example Usage ``` python3 -X dev main.py example.txt ``` # Constraints 1. Assume `example.txt` is a valid text file present in the same directory as the script. 2. The script should handle any potential warnings or errors gracefully. 3. Follow best practices for coroutine handling and resource management. # Expected Output Your script should print: 1. The number of lines in the provided text file. 2. Messages indicating the start and end of coroutine executions. Example: ``` File \'example.txt\' has X lines. Coroutine started for URL 1. Coroutine ended for URL 1. Coroutine started for URL 2. Coroutine ended for URL 2. ... ``` # Evaluation Criteria 1. Correct implementation of file handling and context management. 2. Proper implementation and execution of coroutines. 3. Demonstrate understanding of Python Development Mode by handling and displaying potential warnings. 4. Code quality, readability, and adherence to best practices.","solution":"import asyncio import logging import sys logging.basicConfig(level=logging.INFO) def count_lines_in_file(file_path: str) -> int: Counts the number of lines in a file specified by file_path. try: with open(file_path, \'r\') as file: lines = file.readlines() return len(lines) except FileNotFoundError: logging.error(f\\"File not found: {file_path}\\") return 0 async def fetch_data(api_url: str) -> None: Simulates fetching data from an API and processes it. Uses asyncio.sleep to simulate a network delay. Logs the start and end of the coroutine execution. logging.info(f\\"Coroutine started for URL {api_url}.\\") await asyncio.sleep(1) # Simulate network delay logging.info(f\\"Coroutine ended for URL {api_url}.\\") async def main(file_path: str): # Count lines in the file line_count = count_lines_in_file(file_path) logging.info(f\\"File \'{file_path}\' has {line_count} lines.\\") # Simulate fetching data from multiple URLs urls = [f\\"http://example.com/{i}\\" for i in range(3)] await asyncio.gather(*(fetch_data(url) for url in urls)) if __name__ == \\"__main__\\": if len(sys.argv) != 2: logging.error(\\"Usage: python3 -X dev main.py <file_path>\\") else: file_path = sys.argv[1] asyncio.run(main(file_path))"},{"question":"# Functional Programming Question Overview You are required to write a function that processes a list of dictionaries representing student records. Each dictionary contains the following keys: `name`, `grades`, and `attendance`. Details The student records contain the following keys: - `name`: A string representing the student\'s name. - `grades`: A list of integers representing the student\'s grades. - `attendance`: An integer representing the number of days the student attended out of a possible 100 days. Example of a student record: ```python { \\"name\\": \\"Alice\\", \\"grades\\": [75, 85, 90], \\"attendance\\": 95 } ``` Task You need to create a function `process_student_records(records)` that processes these records and returns a list of tuples. Each tuple contains (in order): 1. The student\'s name. 2. The average grade of the student. 3. A boolean value indicating whether the student\'s attendance is 90% or higher. The returned list should be sorted by the student\'s average grade in descending order. If two students have the same average grade, they should be sorted by their names in ascending order. Function Signature ```python def process_student_records(records: List[Dict[str, Union[str, List[int], int]]]) -> List[Tuple[str, float, bool]]: ``` Input - `records`: A list of dictionaries representing student records. The list contains at most 1000 records. Output - A list of tuples containing the processed information according to the specified format. Examples # Example 1 Input: ```python [ {\\"name\\": \\"Alice\\", \\"grades\\": [75, 85, 90], \\"attendance\\": 95}, {\\"name\\": \\"Bob\\", \\"grades\\": [80, 82, 85], \\"attendance\\": 88}, {\\"name\\": \\"Charlie\\", \\"grades\\": [95, 100, 100], \\"attendance\\": 97} ] ``` Output: ```python [ (\\"Charlie\\", 98.33, True), (\\"Alice\\", 83.33, True), (\\"Bob\\", 82.33, False) ] ``` # Example 2 Input: ```python [ {\\"name\\": \\"Eve\\", \\"grades\\": [100], \\"attendance\\": 100}, {\\"name\\": \\"Dan\\", \\"grades\\": [50, 60, 70], \\"attendance\\": 80} ] ``` Output: ```python [ (\\"Eve\\", 100.0, True), (\\"Dan\\", 60.0, False) ] ``` Constraints - Each student\'s name is unique. - The `grades` list will have at least one grade and at most 10 grades. - All grades are integers between 0 and 100. - Attendance is an integer between 0 and 100. Notes - You may use built-in Python functions and libraries to solve the problem. - Make use of functional programming concepts such as map, filter, lambda, and comprehensions to demonstrate your proficiency. # Performance Requirements - Ensure the solution is optimized to handle up to 1000 student records efficiently.","solution":"from typing import List, Dict, Union, Tuple def process_student_records(records: List[Dict[str, Union[str, List[int], int]]]) -> List[Tuple[str, float, bool]]: processed_records = [] for record in records: name = record[\'name\'] grades = record[\'grades\'] attendance = record[\'attendance\'] # Calculate the average grade average_grade = sum(grades) / len(grades) # Check if attendance is 90% or higher high_attendance = attendance >= 90 # Create the tuple processed_records.append((name, round(average_grade, 2), high_attendance)) # Sort the records first by average grade in descending order, then by name in ascending order processed_records.sort(key=lambda x: (-x[1], x[0])) return processed_records"},{"question":"You are tasked with designing a utility for processing CSV files containing details about an organization\'s employees. The input CSV file will have the fields `employee_id`, `name`, `age`, `department`, `salary`. However, there may be additional columns that should be ignored. Your task is to: 1. Read the input CSV file using the `csv` module. 2. Filter the employees based on the given criteria. 3. Write the filtered data to a new CSV file. 4. Use a custom dialect for the output CSV file. # Requirements 1. **Input Format:** - The input CSV file provided as `input_file_path` contains employee records. Each record will have at least the following columns: `employee_id`, `name`, `age`, `department`, `salary`. - The file may contain more columns, but only the mentioned columns need to be processed. - Each record is guaranteed to have valid data for these columns. 2. **Filtering Criteria:** - Employees older than 30 years. - Employees from the \\"IT\\" department. 3. **Output Format:** - The output CSV file should only contain the filtered employee records. - Use a custom CSV dialect for writing the output file. This dialect should adhere to the following: - Delimiter: `;` - Quote character: `\\"` - Quoting: `csv.QUOTE_MINIMAL` - The file should be written to the path provided as `output_file_path`. 4. **Function Signature:** ```python def process_employees(input_file_path: str, output_file_path: str) -> None: # Your implementation here ``` # Constraints - You can assume that the input CSV file will contain no more than 10,000 records. - The `age` column will always contain integer values. - The `department` column will contain string values. # Example Given an input CSV file `employees.csv`: ``` employee_id,name,age,department,salary,extra_column 1,John Doe,35,IT,60000,foo 2,Jane Smith,28,HR,50000,bar 3,Bob Johnson,40,IT,70000,baz 4,Alice White,32,Finance,72000,qux ``` Call to: ```python process_employees(\'employees.csv\', \'filtered_employees.csv\') ``` The output CSV file `filtered_employees.csv` should be: ``` employee_id;name;age;department;salary 1;\\"John Doe\\";35;IT;60000 3;\\"Bob Johnson\\";40;IT;70000 ``` # Note Make sure to: - Use appropriate CSV functionalities and handle the file operations correctly. - Register and use the custom dialect.","solution":"import csv def process_employees(input_file_path: str, output_file_path: str) -> None: Process employees from the input CSV file and write the filtered employees to the output CSV file. # Define the custom dialect csv.register_dialect(\'custom_dialect\', delimiter=\';\', quotechar=\'\\"\', quoting=csv.QUOTE_MINIMAL) with open(input_file_path, mode=\'r\', newline=\'\') as infile: reader = csv.DictReader(infile) with open(output_file_path, mode=\'w\', newline=\'\') as outfile: fieldnames = [\'employee_id\', \'name\', \'age\', \'department\', \'salary\'] writer = csv.DictWriter(outfile, fieldnames=fieldnames, dialect=\'custom_dialect\') writer.writeheader() for row in reader: if int(row[\'age\']) > 30 and row[\'department\'] == \'IT\': writer.writerow({field: row[field] for field in fieldnames})"},{"question":"Implementing and Testing Initialization Functions in PyTorch Objective: Assess your understanding of neural network parameter initialization using the PyTorch `torch.nn.init` module. Problem Statement: Implement a convolutional neural network (CNN) in PyTorch and initialize its layers using various initialization techniques provided in the `torch.nn.init` module. Specifically, you will: 1. Define a custom CNN class. 2. Write a function to initialize the network parameters using a specified initialization method. 3. Train the network on a dummy dataset after applying the different initialization methods and compare their effects on training performance. Detailed Instructions: 1. **Define the CNN architecture**: - Create a class `CustomCNN` inheriting from `nn.Module`. - The network should contain the following layers: - `Conv2d` layer with 3 input channels, 16 output channels, and kernel size 3. - `ReLU` activation function. - `MaxPool2d` layer with kernel size 2. - `Conv2d` layer with 16 input channels, 32 output channels, and kernel size 3. - `ReLU` activation function. - `MaxPool2d` layer with kernel size 2. - Fully connected (`Linear`) layer to transform the output to 10 classes. 2. **Initialization function**: - Write a function `initialize_weights(model, init_type)` that takes a model and an `init_type` as input. - Depending on `init_type`, use one of the following PyTorch initialization methods to initialize the parameters of the model: - \'uniform\': `torch.nn.init.uniform_` - \'normal\': `torch.nn.init.normal_` - \'xavier_uniform\': `torch.nn.init.xavier_uniform_` - \'xavier_normal\': `torch.nn.init.xavier_normal_` - \'kaiming_uniform\': `torch.nn.init.kaiming_uniform_` - \'kaiming_normal\': `torch.nn.init.kaiming_normal_` 3. **Training the network**: - Create a small dummy dataset with random input data and labels suitable for training the network. - Train the CNN with each initialization method for a fixed number of epochs (e.g., 5 epochs). - Monitor and print the training loss for each epoch. Input: - `init_type` (string): The initialization type to be used. It can be one of the following values: \'uniform\', \'normal\', \'xavier_uniform\', \'xavier_normal\', \'kaiming_uniform\', \'kaiming_normal\'. Output: - Print the training losses for each initialization method over the fixed number of epochs. Constraints: - Use the provided initialization methods only. - Ensure reproducibility by setting a random seed before initializing the weights. Example Usage: ```python # Example initialization of model and training model = CustomCNN() init_type = \'xavier_uniform\' initialize_weights(model, init_type) train_model(model) # Implement this function to handle training ``` Notes: - Use `torch.no_grad()` context manager where necessary to avoid tracking computation history during initialization. - Ensure to shuffle the dummy dataset for each epoch. By completing this task, you will demonstrate a strong understanding of neural network initialization techniques in PyTorch and how they impact the training process.","solution":"import torch import torch.nn as nn import torch.optim as optim import torch.nn.functional as F from torch.utils.data import DataLoader, TensorDataset class CustomCNN(nn.Module): def __init__(self): super(CustomCNN, self).__init__() self.conv1 = nn.Conv2d(3, 16, 3) self.conv2 = nn.Conv2d(16, 32, 3) self.fc1 = nn.Linear(32*6*6, 10) self.pool = nn.MaxPool2d(2, 2) self.relu = nn.ReLU() def forward(self, x): x = self.pool(self.relu(self.conv1(x))) x = self.pool(self.relu(self.conv2(x))) x = x.view(-1, 32*6*6) x = self.fc1(x) return x def initialize_weights(model, init_type): for m in model.modules(): if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear): if init_type == \'uniform\': nn.init.uniform_(m.weight) elif init_type == \'normal\': nn.init.normal_(m.weight) elif init_type == \'xavier_uniform\': nn.init.xavier_uniform_(m.weight) elif init_type == \'xavier_normal\': nn.init.xavier_normal_(m.weight) elif init_type == \'kaiming_uniform\': nn.init.kaiming_uniform_(m.weight, nonlinearity=\'relu\') elif init_type == \'kaiming_normal\': nn.init.kaiming_normal_(m.weight, nonlinearity=\'relu\') if m.bias is not None: nn.init.constant_(m.bias, 0) def train_model(model, train_loader, epochs=5): criterion = nn.CrossEntropyLoss() optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9) for epoch in range(epochs): running_loss = 0.0 for i, data in enumerate(train_loader, 0): inputs, labels = data optimizer.zero_grad() outputs = model(inputs) loss = criterion(outputs, labels) loss.backward() optimizer.step() running_loss += loss.item() print(f\'Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}\') if __name__ == \'__main__\': torch.manual_seed(42) dummy_inputs = torch.randn(100, 3, 32, 32) dummy_labels = torch.randint(0, 10, (100,)) train_dataset = TensorDataset(dummy_inputs, dummy_labels) train_loader = DataLoader(train_dataset, batch_size=10, shuffle=True) init_types = [\'uniform\', \'normal\', \'xavier_uniform\', \'xavier_normal\', \'kaiming_uniform\', \'kaiming_normal\'] for init_type in init_types: print(f\\"nInitialization type: {init_type}\\") model = CustomCNN() initialize_weights(model, init_type) train_model(model, train_loader)"},{"question":"# PyTorch Custom Dataset and DataLoader Implementation You are tasked with implementing a custom dataset class and using PyTorch\'s DataLoader to preprocess and iterate through the data. Your goal is to engineer a solution that handles basic data transformation tasks and allows efficient batch processing. Problem Statement: **1. Custom Dataset Implementation:** Implement a custom `TorchDataset` class that inherits from `torch.utils.data.Dataset`. This dataset should: - Take a list of file paths as an input. - Load these files, which are assumed to be text files for simplicity. - Transform the text data by splitting it into words. **2. DataLoader Integration:** Utilize `torch.utils.data.DataLoader` to: - Instantiate a DataLoader with the `TorchDataset`. - Provide batching functionality with a specified batch size. - Implement data shuffling and parallel data loading using multiple workers. Specific Tasks: 1. **TorchDataset Class:** - **Initialization (`__init__`):** Should accept a list of file paths. - **Length (`__len__`):** Should return the number of files. - **Get Item (`__getitem__`):** Should read the file, split the text into words, and return a list of words. 2. **DataLoader:** - Create an instance of `TorchDataset` with given file paths. - Implement a DataLoader with batch size of 2, shuffling enabled, and using 2 workers. Additional Requirements: - Add proper exception handling to manage file read errors. - Ensure your implementation is efficient and follows PyTorch best practices. - The code should be well-documented with comments explaining each part. Example Input and Output: Given the following text files: - `file1.txt` containing \\"Hello world from file1\\" - `file2.txt` containing \\"PyTorch data loading with custom dataset\\" Your implementation should process these files and produce batches of word lists. **Example:** ```python file_paths = [\\"path/to/file1.txt\\", \\"path/to/file2.txt\\"] torch_dataset = TorchDataset(file_paths) data_loader = DataLoader(torch_dataset, batch_size=2, shuffle=True, num_workers=2) for batch in data_loader: print(batch) ``` This would output batches of word lists. Since the DataLoader is shuffled, the output order may vary. Constraints and Limitations: - Assume all input files exist and are accessible. - Files are small enough to fit in memory. - Keep the solution within standard PyTorch utilities without relying on external packages. **Your task is to implement the `TorchDataset` class and usage of DataLoader as specified above.**","solution":"import os from torch.utils.data import Dataset, DataLoader import torch class TorchDataset(Dataset): Custom Dataset for loading text files and returning lists of words. def __init__(self, file_paths): Initialize the dataset with the list of file paths. :param file_paths: List of strings, each representing a file path. self.file_paths = file_paths def __len__(self): Return the number of files. :return: int return len(self.file_paths) def __getitem__(self, idx): Load the text file, split the text into words, and return the words. :param idx: int, index of the file to load. :return: List of words from the file. file_path = self.file_paths[idx] try: with open(file_path, \'r\') as file: text = file.read() words = text.split() return words except Exception as e: raise IOError(f\\"Error reading file {file_path}: {e}\\") def create_data_loader(file_paths, batch_size=2, shuffle=True, num_workers=2): Create a DataLoader with the specified batch size, shuffling, and number of workers. :param file_paths: List of strings, each representing a file path. :param batch_size: int, batch size for the DataLoader. :param shuffle: bool, whether to shuffle the data. :param num_workers: int, number of workers for data loading. :return: DataLoader instance. torch_dataset = TorchDataset(file_paths) data_loader = DataLoader(torch_dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers, collate_fn=lambda x: x) return data_loader"},{"question":"Objective: Demonstrate your understanding of data visualization using the `pandas.plotting` module. Problem Statement: You are given a Titanic dataset containing information about passengers. You need to visualize various aspects of this data using pandas plotting functions. Input: The input dataset will have the following columns: - `PassengerId`: Unique identifier for each passenger - `Survived`: 0 or 1 indicating whether the passenger survived - `Pclass`: Passenger class (1, 2, or 3) - `Name`: Name of the passenger - `Sex`: Sex of the passenger - `Age`: Age of the passenger - `SibSp`: Number of siblings/spouses aboard - `Parch`: Number of parents/children aboard - `Ticket`: Ticket number - `Fare`: Fare paid by the passenger - `Cabin`: Cabin number - `Embarked`: Port of embarkation (C, Q, S) You are required to implement a function `visualize_titanic_data` that reads this dataset and generates the following plots: 1. **Scatter Matrix**: Show relationships between \'Age\', \'Fare\', \'SibSp\', and \'Parch\'. 2. **Box Plot**: Compare the fare price distribution across different classes (`Pclass`). 3. **Parallel Coordinates**: Visualize relationships among \'Survived\', \'Pclass\', \'Age\', \'Fare\' and \'Sex\'. Use different colors for survived (1) and non-survived (0) passengers. 4. **Andrews Curves**: Visualize \'Survived\', \'Pclass\', \'Age\', \'Fare\', \'SibSp\', and \'Parch\' as Andrews curves. Constraints: - You may assume that the input dataset is in CSV format and is loaded into a pandas DataFrame within the function. - The DataFrame must be preprocessed to handle any missing values if needed for the plots. For example, you can fill missing values with the mean for numerical columns. - For the `parallel_coordinates` plot, ensure that the \'Sex\' column is converted into numerical form (i.e., male: 0, female: 1). Function Signature: ```python import pandas as pd def visualize_titanic_data(df: pd.DataFrame) -> None: pass ``` Example Usage: ```python import pandas as pd # Sample data to illustrate data = { \'PassengerId\': [1, 2, 3, 4, 5], \'Survived\': [0, 1, 1, 0, 1], \'Pclass\': [3, 1, 3, 1, 3], \'Name\': [\'A\', \'B\', \'C\', \'D\', \'E\'], \'Sex\': [\'male\', \'female\', \'female\', \'male\', \'female\'], \'Age\': [22, 38, 26, 35, 27], \'SibSp\': [1, 1, 0, 1, 0], \'Parch\': [0, 0, 0, 0, 0], \'Ticket\': [\'A/5 21171\', \'PC 17599\', \'STON/O2. 3101282\', \'113803\', \'373450\'], \'Fare\': [7.25, 71.2833, 7.925, 53.1, 8.05], \'Cabin\': [None, \'C85\', None, \'C123\', None], \'Embarked\': [\'S\', \'C\', \'S\', \'S\', \'S\'] } df = pd.DataFrame(data) visualize_titanic_data(df) ``` Expected Output: The function should display the four specified visualizations. --- The solution to this problem will involve using the functions `scatter_matrix`, `boxplot`, `parallel_coordinates`, and `andrews_curves` from the `pandas.plotting` module, requiring students to preprocess, transform, and visualize data effectively.","solution":"import pandas as pd import numpy as np import matplotlib.pyplot as plt from pandas.plotting import scatter_matrix, parallel_coordinates, andrews_curves def preprocess_data(df): Handle missing values and encode categorical variables. df[\'Age\'].fillna(df[\'Age\'].mean(), inplace=True) df[\'Fare\'].fillna(df[\'Fare\'].mean(), inplace=True) df[\'SibSp\'].fillna(df[\'SibSp\'].mean(), inplace=True) df[\'Parch\'].fillna(df[\'Parch\'].mean(), inplace=True) df[\'Sex\'] = df[\'Sex\'].map({\'male\': 0, \'female\': 1}) return df def visualize_titanic_data(df): Generate various visualizations for the Titanic dataset. df = preprocess_data(df) # Scatter Matrix plt.figure(figsize=(10, 10)) scatter_matrix(df[[\'Age\', \'Fare\', \'SibSp\', \'Parch\']]) plt.suptitle(\'Scatter Matrix\') plt.show() # Box Plot plt.figure(figsize=(8, 6)) df.boxplot(column=\'Fare\', by=\'Pclass\') plt.title(\'Box plot of Fare by Pclass\') plt.suptitle(\'\') plt.xlabel(\'Pclass\') plt.ylabel(\'Fare\') plt.show() # Parallel Coordinates plt.figure(figsize=(12, 8)) parallel_coordinates(df[[\'Survived\', \'Pclass\', \'Age\', \'Fare\', \'Sex\']], \'Survived\', color=(\'#556270\', \'#4ECDC4\')) plt.title(\'Parallel Coordinates Plot\') plt.show() # Andrews Curves plt.figure(figsize=(12, 8)) andrews_curves(df[[\'Survived\', \'Pclass\', \'Age\', \'Fare\', \'SibSp\', \'Parch\']], \'Survived\') plt.title(\'Andrews Curves Plot\') plt.show() # Example usage: if __name__ == \\"__main__\\": # Sample data to illustrate data = { \'PassengerId\': [1, 2, 3, 4, 5], \'Survived\': [0, 1, 1, 0, 1], \'Pclass\': [3, 1, 3, 1, 3], \'Name\': [\'A\', \'B\', \'C\', \'D\', \'E\'], \'Sex\': [\'male\', \'female\', \'female\', \'male\', \'female\'], \'Age\': [22, 38, 26, 35, 27], \'SibSp\': [1, 1, 0, 1, 0], \'Parch\': [0, 0, 0, 0, 0], \'Ticket\': [\'A/5 21171\', \'PC 17599\', \'STON/O2. 3101282\', \'113803\', \'373450\'], \'Fare\': [7.25, 71.2833, 7.925, 53.1, 8.05], \'Cabin\': [None, \'C85\', None, \'C123\', None], \'Embarked\': [\'S\', \'C\', \'S\', \'S\', \'S\'] } df = pd.DataFrame(data) visualize_titanic_data(df)"},{"question":"**Question: Understanding and Utilizing Python Built-in Constants** Python 3.10 introduces several built-in constants with specific properties and usage constraints. Your task is to implement a function that demonstrates comprehension of these constants and their correct application. # Task: 1. **Implement a function `constant_behavior(input_value)` which takes a single argument `input_value`**. 2. **Depending on the type and value of `input_value`, return a specific built-in constant**: - If `input_value` is a boolean and `True`, return the `True` constant. - If `input_value` is a boolean and `False`, return the `False` constant. - If `input_value` is `None`, return the `None` constant. - If `input_value` is not implemented in a certain context (e.g., a certain operation with it is not supported), return the `NotImplemented` constant. - If `input_value` is the string `\\"ellipsis\\"`, return the `Ellipsis` constant. - If `input_value` is the string `\\"debug\\"`, return the `__debug__` constant. 3. **For any other type or value of `input_value`, return `\\"Unknown input\\"`**. # Constraints and Considerations: - Do not use conditional reassignment of the constants, as it is illegal and causes a `SyntaxError`. - Ensure correct and authorized usage of each constant as specified. - Make sure to handle edge cases where the input does not match any expected type. # Example: ```python def constant_behavior(input_value): # Your code here pass # Sample tests print(constant_behavior(True)) # Output: True print(constant_behavior(False)) # Output: False print(constant_behavior(None)) # Output: None print(constant_behavior(\\"test\\")) # Output: \\"Unknown input\\" print(constant_behavior(\\"ellipsis\\")) # Output: Ellipsis print(constant_behavior(\\"debug\\")) # Output: __debug__ # This will print True or False depending on the execution mode print(constant_behavior(NotImplemented)) # Output: NotImplemented ``` Implement the function according to the described requirements and ensure the specified behavior for all anticipated input values.","solution":"def constant_behavior(input_value): Returns specific built-in Python constants based on the type and value of input_value. if isinstance(input_value, bool): return input_value elif input_value is None: return None elif input_value is NotImplemented: return NotImplemented elif input_value == \\"ellipsis\\": return Ellipsis elif input_value == \\"debug\\": return __debug__ else: return \\"Unknown input\\""},{"question":"**Question: Implement a Custom Transformation on a PyTorch Module Using torch.fx** You are required to implement a custom transformation using the `torch.fx` module in PyTorch. The goal is to transform any given PyTorch module by replacing all occurrences of the ReLU activation function with its mathematical decomposition: `(x > 0) * x`. You will create a function `relu_decomposition_transform` that performs this transformation. # Requirements 1. **Function Definition**: ```python def relu_decomposition_transform(module: torch.nn.Module) -> torch.nn.Module: Transforms the given module by replacing all ReLU activation functions with their mathematical decomposition: (x > 0) * x. Args: module (torch.nn.Module): The input PyTorch module to transform. Returns: torch.nn.Module: The transformed module with ReLU replaced. ``` 2. **Input**: - A PyTorch `torch.nn.Module` object. 3. **Output**: - A transformed `torch.nn.Module` object with all ReLU activations replaced. 4. **Constraints**: - Use `torch.fx` for symbolic tracing and graph manipulation. - You must handle both `torch.nn.ReLU` modules and `torch.nn.functional.relu` calls. - Ensure the transformed module produces the same output as the original for the same input. 5. **Example Usage**: ```python import torch import torch.nn as nn import torch.fx class SampleModel(nn.Module): def __init__(self): super(SampleModel, self).__init__() self.linear = nn.Linear(10, 10) self.relu = nn.ReLU() def forward(self, x): return self.relu(self.linear(x)) model = SampleModel() transformed_model = relu_decomposition_transform(model) input_tensor = torch.randn(1, 10) original_output = model(input_tensor) transformed_output = transformed_model(input_tensor) assert torch.allclose(original_output, transformed_output), \\"The transformed model\'s output doesn\'t match the original.\\" ``` # Notes - Remember to call `recompile` on the `GraphModule` if you modify the existing graph. - Use the `Proxy` class for capturing operations if required. - Test your function thoroughly to cover different use cases and edge cases. Good luck!","solution":"import torch import torch.nn as nn import torch.fx as fx def relu_decomposition_transform(module: torch.nn.Module) -> torch.nn.Module: Transforms the given module by replacing all ReLU activation functions with their mathematical decomposition: (x > 0) * x. Args: module (torch.nn.Module): The input PyTorch module to transform. Returns: torch.nn.Module: The transformed module with ReLU replaced. class ReluDecomposerTracer(fx.Tracer): def is_leaf_module(self, m, module_qualified_name): return m.__module__.startswith(\'torch.nn\') and not isinstance(m, nn.Sequential) graph = ReluDecomposerTracer().trace(module) graph_module = fx.GraphModule(module, graph) for node in graph.nodes: if node.op == \'call_module\' and isinstance(node.target, str): target = graph_module.get_submodule(node.target) if isinstance(target, nn.ReLU): with graph.inserting_after(node): new_node = graph.call_function(lambda x: (x > 0) * x, args=(node.args[0],)) node.replace_all_uses_with(new_node) graph.erase_node(node) elif node.op == \'call_function\' and node.target == torch.nn.functional.relu: with graph.inserting_after(node): new_node = graph.call_function(lambda x: (x > 0) * x, args=node.args) node.replace_all_uses_with(new_node) graph.erase_node(node) graph_module.recompile() return graph_module"},{"question":"Objective: Your task is to create a detailed visualization using seaborn\'s `violinplot` function. This visualization should help you analyze the `mpg` dataset available in seaborn, focusing primarily on the distribution of miles per gallon (`mpg`) across different `cylinders` and varying by `origin`. The visualization must be well-labeled and include customized features to effectively highlight the key insights. Instructions: 1. **Dataset:** Utilize the `mpg` dataset provided by seaborn. Load the dataset using: ```python import seaborn as sns df = sns.load_dataset(\\"mpg\\") ``` 2. **Plot Requirements:** - Create a violin plot showcasing the distribution of `mpg` for each `cylinders` category. - Split the violin plots to show the `origin` of the cars within each cylinders category. - Adjust the bandwidth of the KDE to 0.5 for smoothing. - Use `inner=\\"stick\\"` to represent individual observations inside the violin. - Ensure the width of each violin is normalized to represent the count of observations using `density_norm=\\"count\\"`. - Apply a title to the plot and properly label the x-axis and y-axis. 3. **Customization:** - Use the `native_scale=True` parameter to preserve the original scales for the `cylinders` axis. - Set the plot theme to `\\"whitegrid\\"` using `sns.set_theme(style=\\"whitegrid\\")`. - Optionally, modify the colors to enhance readability. 4. **Performance:** The solution should execute efficiently, given the moderate size of the dataset. Expected Input and Output Formats: - **Input:** No additional input is required. Use the provided `mpg` dataset. - **Output:** A visualization (violin plot) as described. Constraints: - Use only seaborn and matplotlib libraries for visualization. - Ensure the plot is clear and correctly represents the data distributions. Sample Code Structure: ```python import seaborn as sns import matplotlib.pyplot as plt # Load the dataset df = sns.load_dataset(\\"mpg\\") # Set the theme sns.set_theme(style=\\"whitegrid\\") # Create the violin plot sns.violinplot( data=df, x=\\"cylinders\\", y=\\"mpg\\", hue=\\"origin\\", split=True, bw_adjust=0.5, inner=\\"stick\\", density_norm=\\"count\\", native_scale=True ) # Add titles and labels plt.title(\\"Miles per Gallon Distribution by Cylinders and Origin\\") plt.xlabel(\\"Cylinders\\") plt.ylabel(\\"Miles per Gallon (mpg)\\") # Show the plot plt.show() ``` **Note:** Ensure your plot clearly reflects the `mpg` distribution for each cylinder category, split by the origin of the cars.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_violin_plot(): # Load the dataset df = sns.load_dataset(\\"mpg\\") # Set the theme sns.set_theme(style=\\"whitegrid\\") # Create the violin plot sns.violinplot( data=df, x=\\"cylinders\\", y=\\"mpg\\", hue=\\"origin\\", split=True, bw_adjust=0.5, inner=\\"stick\\", scale=\\"count\\", native_scale=True ) # Add titles and labels plt.title(\\"Miles per Gallon Distribution by Cylinders and Origin\\") plt.xlabel(\\"Cylinders\\") plt.ylabel(\\"Miles per Gallon (mpg)\\") # Show the plot plt.show() # Call the function to create and show the plot create_violin_plot()"},{"question":"**Coding Assessment Question** # Objective To assess the student\'s ability to use seaborn effectively for visualizing statistical relationships, both basic and advanced. # Problem Statement You are given a dataset containing information about various cars, including their fuel efficiency, engine size, horsepower, weight, and year of manufacture. The dataset is stored in a CSV file named `cars.csv` with the following columns: - `mpg`: Miles per gallon. - `cylinders`: Number of cylinders in the engine. - `displacement`: Engine displacement in cubic inches. - `horsepower`: Horsepower of the engine. - `weight`: Weight of the car in pounds. - `acceleration`: Time taken to accelerate from 0 to 60 mph in seconds. - `model_year`: Year the car model was manufactured. - `origin`: Origin of the car (1: USA, 2: Europe, 3: Japan). # Tasks 1. **Scatter Plot with Basic Semantics:** - Load the dataset into a pandas DataFrame. - Create a scatter plot using `relplot` to visualize the relationship between `weight` and `mpg`. - Use different hues for the `origin` of the cars. 2. **Advanced Scatter Plot:** - Enhance the scatter plot from task 1 by adding `horsepower` to the size dimension. - Use different marker styles for `cylinders`. 3. **Line Plot with Aggregation:** - Create a line plot using `relplot` to visualize the `mpg` over the `model_year`. - Aggregate the data to show the mean `mpg` with a 95% confidence interval for each year. 4. **Faceting:** - Create a faceted scatter plot to show the same relationship as in task 1 but faceted by the `model_year`. - Use columns to facet by `model_year`. # Input Format - The input is a CSV file named `cars.csv`. # Output Format Code should produce the following visualizations: 1. A scatter plot with `weight` vs. `mpg`, with points colored by `origin`. 2. An enhanced scatter plot with `weight` vs. `mpg`, colored by `origin`, with point sizes representing `horsepower` and marker styles representing `cylinders`. 3. A line plot showing the mean `mpg` over the `model_year` with a 95% confidence interval. 4. A faceted scatter plot showing `weight` vs. `mpg`, faceted by `model_year`. # Constraints - Ensure all plots have appropriate labels and legends. - Use seaborn and matplotlib for visualization. - Handle any missing data appropriately. # Example Solution ```python import pandas as pd import seaborn as sns import matplotlib.pyplot as plt # Task 1: Load the dataset and create a basic scatter plot cars = pd.read_csv(\'cars.csv\') sns.set_theme(style=\\"darkgrid\\") scatter_plot = sns.relplot(data=cars, x=\\"weight\\", y=\\"mpg\\", hue=\\"origin\\") scatter_plot.set_axis_labels(\\"Weight (lbs)\\", \\"Miles per Gallon\\") plt.show() # Task 2: Enhanced scatter plot enhanced_scatter = sns.relplot( data=cars, x=\\"weight\\", y=\\"mpg\\", hue=\\"origin\\", size=\\"horsepower\\", style=\\"cylinders\\", sizes=(40, 400), palette=\\"muted\\", alpha=0.7, height=6 ) enhanced_scatter.set_axis_labels(\\"Weight (lbs)\\", \\"Miles per Gallon\\") plt.show() # Task 3: Line plot with aggregation line_plot = sns.relplot( data=cars, x=\\"model_year\\", y=\\"mpg\\", kind=\\"line\\", ci=\\"sd\\" ) line_plot.set_axis_labels(\\"Model Year\\", \\"Miles per Gallon\\") plt.show() # Task 4: Faceted scatter plot faceted_scatter = sns.relplot( data=cars, x=\\"weight\\", y=\\"mpg\\", hue=\\"origin\\", col=\\"model_year\\", col_wrap=4, height=4, aspect=0.75, kind=\\"scatter\\" ) faceted_scatter.set_axis_labels(\\"Weight (lbs)\\", \\"Miles per Gallon\\") plt.show() ``` # Note Ensure your code is well-commented and follows best practices for data visualization. Good luck!","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def load_data(file_path): Load the dataset from a CSV file. return pd.read_csv(file_path) def basic_scatter_plot(df): Create a basic scatter plot to visualize the relationship between weight and mpg with hues for origin. scatter_plot = sns.relplot(data=df, x=\\"weight\\", y=\\"mpg\\", hue=\\"origin\\") scatter_plot.set_axis_labels(\\"Weight (lbs)\\", \\"Miles per Gallon\\") plt.show() def enhanced_scatter_plot(df): Create an enhanced scatter plot with weight vs. mpg, colored by origin, size by horsepower, and marker style by cylinders. enhanced_scatter = sns.relplot( data=df, x=\\"weight\\", y=\\"mpg\\", hue=\\"origin\\", size=\\"horsepower\\", style=\\"cylinders\\", sizes=(40, 400), palette=\\"muted\\", alpha=0.7, height=6 ) enhanced_scatter.set_axis_labels(\\"Weight (lbs)\\", \\"Miles per Gallon\\") plt.show() def line_plot_with_aggregation(df): Create a line plot to visualize the mean mpg over the model_year with a 95% confidence interval. line_plot = sns.relplot( data=df, x=\\"model_year\\", y=\\"mpg\\", kind=\\"line\\", ci=\\"sd\\" ) line_plot.set_axis_labels(\\"Model Year\\", \\"Miles per Gallon\\") plt.show() def faceted_scatter_plot(df): Create a faceted scatter plot to show the relationship between weight and mpg, faceted by model_year. faceted_scatter = sns.relplot( data=df, x=\\"weight\\", y=\\"mpg\\", hue=\\"origin\\", col=\\"model_year\\", col_wrap=4, height=4, aspect=0.75, kind=\\"scatter\\" ) faceted_scatter.set_axis_labels(\\"Weight (lbs)\\", \\"Miles per Gallon\\") plt.show() # Example usage: # cars = load_data(\'cars.csv\') # basic_scatter_plot(cars) # enhanced_scatter_plot(cars) # line_plot_with_aggregation(cars) # faceted_scatter_plot(cars)"},{"question":"# Generator Object Challenge Generators are a special type of iterable in Python that allow you to pause the function and resume from where it left off. They are created using functions with the `yield` statement. **Task:** You are required to implement and test a custom iterator using Python\'s generator functions. The generator should produce a sequence of values based on the following conditions: 1. The generator should start with a given initial value. 2. Each subsequent value should be the sum of the previous value and a fixed step value. 3. It should produce a specified number of values and then stop. **Function Signature:** ```python def custom_generator(start: int, step: int, count: int) -> Generator[int, None, None]: pass ``` **Parameters:** - `start` (int): The initial value of the sequence. - `step` (int): The fixed step value to add to the previous value to get the next value. - `count` (int): The number of values to generate. **Returns:** - An iterator (generator) that yields the sequence of values. **Example:** ```python g = custom_generator(10, 2, 5) print(list(g)) # Output: [10, 12, 14, 16, 18] ``` **Constraints:** - The value of `step` can be positive or negative. - The value of `count` will always be greater than or equal to 1. - The function should use the `yield` statement to produce generator values. **Additional Task:** Implement a function `is_generator(obj: Any) -> bool` that takes any object and returns `True` if the object is a generator, otherwise returns `False`. **Function Signature:** ```python def is_generator(obj: Any) -> bool: pass ``` **Example:** ```python print(is_generator(custom_generator(10, 2, 5))) # Output: True print(is_generator([1, 2, 3])) # Output: False ``` # Submissions: 1. Implement the `custom_generator` function. 2. Implement the `is_generator` function. 3. Write test cases to verify the correctness of both functions. **Note:** Make sure your code is clean, well-commented, and follows Python\'s best practices.","solution":"from typing import Generator, Any def custom_generator(start: int, step: int, count: int) -> Generator[int, None, None]: Generates a sequence of values starting from `start`, incremented by `step`, and produces `count` values. current_value = start for _ in range(count): yield current_value current_value += step def is_generator(obj: Any) -> bool: Returns True if the object is a generator, otherwise False. return (hasattr(obj, \'__iter__\') and not hasattr(obj, \'__len__\')) or hasattr(obj, \'__next__\')"},{"question":"# HTTP Client Implementation You are tasked with writing a Python function that performs several HTTP operations using the `http.client` module. The function should demonstrate your understanding of: 1. Establishing HTTP and HTTPS connections. 2. Sending different types of HTTP requests (`GET`, `POST`). 3. Reading and handling HTTP responses. 4. Properly handling exceptions and errors. # Function Signature ```python def http_operations(urls: List[str], post_data: Dict[str, str], headers: Dict[str, str]) -> Dict[str, Any]: pass ``` # Description - **Parameters**: - `urls` (List[str]): A list of URLs to send requests to. The first URL should be accessed with a `GET` request, and the second URL (if it exists) should be accessed with a `POST` request. - `post_data` (Dict[str, str]): A dictionary of data to be sent in the POST request. - `headers` (Dict[str, str]): A dictionary of HTTP headers to be included in the requests. - **Returns**: - Dict[str, Any]: A dictionary containing the status and reason of the responses for both `GET` and `POST` requests. The dictionary should have the following structure: ```python { \'get\': { \'status\': int, \'reason\': str, \'data\': str # first 100 characters of the response }, \'post\': { \'status\': int, \'reason\': str, \'data\': str # first 100 characters of the response } } ``` # Implementation Requirements 1. **GET Request**: - Establish an HTTP or HTTPS connection based on the URL. - Fetch the resource and retrieve the status and reason from the response. - Read the first 100 characters of the response body. - Handle potential exceptions such as `HTTPException`, `InvalidURL`, and others. 2. **POST Request**: - If there are at least two URLs, perform a POST request to the second URL. - Include the provided `post_data` and `headers` in the request. - Retrieve the status and reason from the response. - Read the first 100 characters of the response body. - Handle potential exceptions appropriately. 3. **Error Handling**: - Ensure that any error during the HTTP operations is caught and logged appropriately. - Provide a meaningful error message in the result dictionary in case of failure. You can use the following template to structure your function: ```python import http.client import urllib.parse from typing import List, Dict, Any def http_operations(urls: List[str], post_data: Dict[str, str], headers: Dict[str, str]) -> Dict[str, Any]: result = { \'get\': { \'status\': None, \'reason\': None, \'data\': None, \'error\': None }, \'post\': { \'status\': None, \'reason\': None, \'data\': None, \'error\': None } } # To be implemented: # 1. Handle GET request to the first URL # 2. Handle POST request to the second URL (if exists) # 3. Error handling return result ``` # Example Usage ```python urls = [ \\"https://www.python.org\\", \\"https://httpbin.org/post\\" ] post_data = {\\"name\\": \\"John\\", \\"age\\": \\"30\\"} headers = {\\"Content-Type\\": \\"application/x-www-form-urlencoded\\"} result = http_operations(urls, post_data, headers) print(result) ``` # Constraints - The URLs will always be valid HTTP or HTTPS URLs. - The `post_data` dictionary and `headers` dictionary will not be empty.","solution":"import http.client import urllib.parse from typing import List, Dict, Any def http_operations(urls: List[str], post_data: Dict[str, str], headers: Dict[str, str]) -> Dict[str, Any]: result = { \'get\': { \'status\': None, \'reason\': None, \'data\': None, \'error\': None }, \'post\': { \'status\': None, \'reason\': None, \'data\': None, \'error\': None } } def get_host_and_path(url): parsed_url = urllib.parse.urlparse(url) return parsed_url.netloc, parsed_url.path + \'?\' + parsed_url.query if parsed_url.query else parsed_url.path def perform_request(method, url, body=None, headers={}): is_https = url.startswith(\'https://\') host, path = get_host_and_path(url) try: conn = http.client.HTTPSConnection(host) if is_https else http.client.HTTPConnection(host) conn.request(method, path, body=body, headers=headers) response = conn.getresponse() status = response.status reason = response.reason data = response.read(100).decode(\'utf-8\') conn.close() return status, reason, data, None except Exception as e: return None, None, None, str(e) # GET request if urls: get_status, get_reason, get_data, get_error = perform_request(\'GET\', urls[0], headers=headers) result[\'get\'] = { \'status\': get_status, \'reason\': get_reason, \'data\': get_data, \'error\': get_error } # POST request if len(urls) > 1: post_body = urllib.parse.urlencode(post_data) post_headers = headers.copy() post_headers[\'Content-Length\'] = len(post_body) post_status, post_reason, post_data, post_error = perform_request(\'POST\', urls[1], body=post_body, headers=post_headers) result[\'post\'] = { \'status\': post_status, \'reason\': post_reason, \'data\': post_data, \'error\': post_error } return result"},{"question":"**Objective:** Write a Python function that interfaces with C functions to handle integers. You will be provided with a hypothetical C interface (mimicked by Python functions) for testing purposes. **Task:** 1. Implement a function `convert_and_add` that takes an integer, an unsigned long, and a double as inputs. 2. Convert these inputs to Python integer objects and add them together. 3. Return the result as a Python integer. **Specifications:** - You will use the following hypothetical C-to-Python interface functions: - `PyLong_FromLong(long v)` - `PyLong_FromUnsignedLong(unsigned long v)` - `PyLong_FromDouble(double v)` - These functions return a Python integer. - You will perform addition of these Python integer objects within your function. **Function signature:** ```python def convert_and_add(int_val, ulong_val, double_val): # Your code here ``` **Input:** - `int_val` (int): An integer. - `ulong_val` (int): An unsigned long integer. - `double_val` (float): A double value. **Output:** - Return a Python integer which is the sum of the converted inputs. **Example:** ```python result = convert_and_add(10, 20, 5.5) print(result) # Example output: 35 ``` **Constraints:** - The inputs will be within the range that can be handled by their respective C types. - You should handle any potential errors that might arise during the conversion from double to Python integer. **Notes:** - You do not need to implement the hypothetical C-to-Python interface functions, just assume they exist in your environment as described and work as specified. Good luck!","solution":"def PyLong_FromLong(v): Hypothetical C-to-Python interface function to convert long to Python integer. return int(v) def PyLong_FromUnsignedLong(v): Hypothetical C-to-Python interface function to convert unsigned long to Python integer. return int(v) def PyLong_FromDouble(v): Hypothetical C-to-Python interface function to convert double to Python integer. return int(v) def convert_and_add(int_val, ulong_val, double_val): Convert inputs to Python integers and add them together. Parameters: int_val (int): An integer. ulong_val (int): An unsigned long integer. double_val (float): A double value. Returns: int: The sum of the converted inputs as a Python integer. py_int = PyLong_FromLong(int_val) py_ulong = PyLong_FromUnsignedLong(ulong_val) py_double = PyLong_FromDouble(double_val) result = py_int + py_ulong + py_double return result"},{"question":"# Array Manipulation and Analysis Objective You are required to implement a function that utilizes the `array` module to perform various manipulations and analyses on an array of integers. Task 1. Implement a function `analyze_array(data: list) -> dict` that accepts a list of integers and returns a dictionary containing the following information: - `original_array`: The original array created from the list. - `reversed_array`: The array with the elements in reverse order. - `count_dict`: A dictionary with counts of each unique element in the array. - `byte_swapped_array`: The array with its elements byte-swapped. Function Signature ```python def analyze_array(data: list) -> dict: pass ``` Expected Input and Output - **Input**: - `data` (list of int): A list of integers, `0 <= len(data) <= 1000`, `-2**31 <= x <= 2**31 - 1` for `x` in `data`. - **Output**: - A dictionary with keys as mentioned and their corresponding values. Example ```python data = [1, 2, 3, 2, 1] result = analyze_array(data) # result should be: { \'original_array\': array(\'i\', [1, 2, 3, 2, 1]), \'reversed_array\': array(\'i\', [1, 2, 3, 2, 1]), \'count_dict\': {1: 2, 2: 2, 3: 1}, \'byte_swapped_array\': array(\'i\', swapped_values), # Note: \'swapped_values\' should be the list of integers with their bytes swapped } ``` Constraints and Notes - You should use the `array` module for creating and manipulating the arrays. - Use the `\'i\'` type code for the integer array. - The `byte_swapped_array` should demonstrate the use of the `byteswap` method. - Consider efficient methods to reverse the array and count occurrences of elements. Performance Requirements - The function should handle the input within a reasonable time frame for the given constraints.","solution":"from array import array from collections import Counter def analyze_array(data: list) -> dict: # Create the original array with type code \'i\' for integers original_array = array(\'i\', data) # Reverse the array reversed_array = array(\'i\', data[::-1]) # Count the occurrences of each unique element count_dict = dict(Counter(data)) # Create a byte-swapped array. Note we make a copy as byteswap is in-place. byte_swapped_array = array(\'i\', data) byte_swapped_array.byteswap() return { \'original_array\': original_array, \'reversed_array\': reversed_array, \'count_dict\': count_dict, \'byte_swapped_array\': byte_swapped_array }"},{"question":"PyTorch Coding Assessment # Objective: Demonstrate your understanding of PyTorch tensor views by implementing a function that performs specific tensor manipulations without copying data unnecessarily. # Task: Implement a function `manipulate_tensors` which takes an input tensor `t` of shape `(4, 4)` and performs the following operations: 1. Create a view of `t` with shape `(2, 8)`. 2. Verify that the view shares the same underlying data with `t`. 3. Modify the first element of the view to `42`. 4. Return the modified base tensor. # Expected Input and Output: - **Input:** - `t`: A 2D PyTorch tensor of shape `(4, 4)`. - **Output:** - A modified 2D PyTorch tensor of shape `(4, 4)`, where the first element is updated to `42`. # Constraints: 1. You must use tensor view operations to avoid unnecessary data copy. 2. Ensure the original tensor `t` shares the same underlying data with its view. 3. Pay attention to the contiguity of the tensors where applicable. # Example: ```python import torch def manipulate_tensors(t): # Step 1: Create a view of t with shape (2, 8) view_t = t.view(2, 8) # Step 2: Verify the underlying data is shared (not required in function but useful for understanding) assert t.storage().data_ptr() == view_t.storage().data_ptr() # Step 3: Modify the first element of the view view_t[0][0] = 42 # Step 4: Return the modified base tensor return t # Example usage: t = torch.zeros((4, 4)) modified_t = manipulate_tensors(t) print(modified_t) ``` ``` Output: tensor([[42., 0., 0., 0.], [ 0., 0., 0., 0.], [ 0., 0., 0., 0.], [ 0., 0., 0., 0.]]) ``` **Note:** Your solution will be evaluated for correct use of view operations, shared data verification, and accurate tensor modifications.","solution":"import torch def manipulate_tensors(t): # Create a view of t with shape (2, 8) view_t = t.view(2, 8) # Verify that the view shares the same underlying data with t assert t.storage().data_ptr() == view_t.storage().data_ptr() # Modify the first element of the view view_t[0][0] = 42 # Return the modified base tensor return t"},{"question":"# Advanced Python Programming Task - Working with Cell Objects and Closures Objective You are required to implement a function that creates, modifies, and inspects the values stored in Python\'s cell objects, utilizing nested functions and closures to simulate cell-like behavior using standard Python constructs. Function Specification Write a function `cell_operations` that performs the following operations: 1. **Create Cell**: Define a nested function structure that creates and stores a value. This should mimic the functionality of `PyCell_New`. 2. **Get Cell Content**: Retrieve and return the stored value, simulating `PyCell_Get`. 3. **Set Cell Content**: Update the stored value, similarly to `PyCell_Set`. Inputs and Outputs - The `cell_operations` function will take no inputs but internally will provide operations to set and get values in a nested scope. - The function should return a tuple of two functions: - A setter function that takes an integer as an input and updates the stored cell value. - A getter function that returns the current value stored in the cell. Example Usage ```python def cell_operations(): # Your implementation here # Creating the cell-like structure setter, getter = cell_operations() # Setting a value setter(10) # Getting the value print(getter()) # Output should be 10 # Updating the value setter(20) # Getting the updated value print(getter()) # Output should be 20 ``` Constraints - Ensure that the implementation follows Pythonic conventions and leverages closures and nested functions effectively. - Focus on the accurate simulation of cell object behavior without using additional libraries or modules. This task will help validate your understanding of closures, nested functions, and variable scope in Python, along with the conceptual application of cell objects.","solution":"def cell_operations(): Returns two functions: setter and getter to simulate cell behavior. The setter sets a given value and the getter returns the current set value. cell_value = None def setter(value): nonlocal cell_value cell_value = value def getter(): return cell_value return setter, getter"},{"question":"# Question: Optimizing a Computational Pipeline with `functools` You are given the task of optimizing a computational pipeline that involves several expensive data processing operations. Your objective is to leverage the `functools` module to enhance the performance and maintainability of this pipeline. **Requirements:** 1. Implement a class `DataPipeline` that processes a list of numbers. 2. This class should calculate the mean and the standard deviation of the list of numbers. These values should be cached because they might be expensive to recompute frequently. 3. Implement a method `larger_than_mean` that returns a filtered list of numbers that are greater than the mean. 4. Use single dispatch methods within the class to support data input in different formats (such as a list, tuple, or custom iterable). 5. Ensure to handle the preservation of function metadata when decorating functions. **Class Definition:** ```python from functools import cached_property, singledispatchmethod, wraps class DataPipeline: def __init__(self, data): self._data = data @cached_property def mean(self): Calculate and return the mean of the data. return sum(self._data) / len(self._data) @cached_property def stdev(self): Calculate and return the standard deviation of the data. mean = self.mean return (sum((x - mean) ** 2 for x in self._data) / len(self._data)) ** 0.5 @singledispatchmethod def add_data(self, data): Add data to the existing data pipeline. Supports different formats. raise NotImplementedError(\\"Unsupported data format\\") @add_data.register def _(self, data: list): self._data.extend(data) @add_data.register def _(self, data: tuple): self._data.extend(list(data)) # Method to filter data larger than mean def larger_than_mean(self): return [x for x in self._data if x > self.mean] # Decorator to retain metadata def retain_metadata(func): @wraps(func) def wrapper(*args, **kwargs): return func(*args, **kwargs) return wrapper ``` **Expected Input/Output:** - Input: A sequence of numbers in a list or tuple. - Output: Mean, standard deviation, and a filtered list of numbers larger than the mean. Use this template in your solution and ensure all requirements are met. Your implementation should showcase your understanding of the `functools` module\'s caching, single dispatch, and metadata preservation capabilities. # Example Usage: ```python # Initialize pipeline with list pipeline = DataPipeline([1, 2, 3, 4, 5, 6, 7, 8, 9, 10]) # Access mean and standard deviation print(pipeline.mean) # Output: 5.5 print(pipeline.stdev) # Output: Approx. 2.872 # Filter numbers larger than the mean print(pipeline.larger_than_mean()) # Output: [6, 7, 8, 9, 10] # Add more data in different formats pipeline.add_data([11, 12, 13]) pipeline.add_data((14, 15, 16)) # Re-access mean and standard deviation print(pipeline.mean) # Output: Updated mean print(pipeline.stdev) # Output: Updated standard deviation ``` Ensure to write appropriate test cases and handle edge cases effectively.","solution":"from functools import cached_property, singledispatchmethod, wraps import math class DataPipeline: def __init__(self, data): self._data = list(data) @cached_property def mean(self): Calculate and return the mean of the data. return sum(self._data) / len(self._data) @cached_property def stdev(self): Calculate and return the standard deviation of the data. mean = self.mean return (sum((x - mean) ** 2 for x in self._data) / len(self._data)) ** 0.5 @singledispatchmethod def add_data(self, data): Add data to the existing data pipeline. Supports different formats. raise NotImplementedError(\\"Unsupported data format\\") @add_data.register def _(self, data: list): self._data.extend(data) @add_data.register def _(self, data: tuple): self._data.extend(list(data)) # Method to filter data larger than mean def larger_than_mean(self): return [x for x in self._data if x > self.mean] # Decorator to retain metadata def retain_metadata(func): @wraps(func) def wrapper(*args, **kwargs): return func(*args, **kwargs) return wrapper"},{"question":"You are tasked with writing a Python function that plays a sine wave tone of a specific frequency and duration through an OSS-compatible audio device using the `ossaudiodev` module. # Function Signature ```python def play_sine_wave(frequency: int, duration: float, samplerate: int = 44100) -> None: ``` # Input Parameters - `frequency` (int): The frequency of the sine wave in Hertz. - `duration` (float): The duration for which the tone should play in seconds. - `samplerate` (int, optional): The number of samples per second, with a default value of 44100 (CD quality). # Constraints 1. Use the audio format `ossaudiodev.AFMT_S16_LE` (Signed, 16-bit, little-endian). 2. The function should open the default audio device in write-only mode. 3. Ensure the device is properly configured to play the given sine wave. 4. Implement error handling for potential issues related to device opening and configuring. # Behavior - The function should open an audio device, configure it, generate a sine wave of the specified frequency and duration, play the wave, and then close the device. # Example ```python # To play a 440 Hz tone (A4 note) for 2 seconds play_sine_wave(440, 2.0) ``` # Note You can use the `ossaudiodev` documentation provided above to understand how to open an audio device, set its parameters, and write audio data to it. # Hints - Use the `math` library to generate the sine wave. - The sine wave can be generated using the formula: [ text{sample} = text{int}(32767 cdot text{math.sin}(2 pi cdot text{frequency} cdot t / text{samplerate})) ] where `t` is the sample index. - Handle exceptions such as `OSSAudioError` and `OSError` appropriately.","solution":"import ossaudiodev import math import struct def play_sine_wave(frequency: int, duration: float, samplerate: int = 44100) -> None: Plays a sine wave tone of a specific frequency and duration through an OSS-compatible audio device. :param frequency: The frequency of the sine wave in Hertz. :param duration: The duration for which the tone should play in seconds. :param samplerate: The number of samples per second (default is 44100). try: # Open the audio device in write-only mode dsp = ossaudiodev.open(\'w\') # Set the audio device parameters dsp.setfmt(ossaudiodev.AFMT_S16_LE) # 16-bit signed little-endian format dsp.channels(1) # Mono dsp.speed(samplerate) # Sample rate # Calculate the total number of samples total_samples = int(duration * samplerate) # Generate the sine wave data sine_wave_data = b\'\' for t in range(total_samples): sample = int(32767 * math.sin(2 * math.pi * frequency * t / samplerate)) sine_wave_data += struct.pack(\'h\', sample) # Write the sine wave data to the audio device dsp.write(sine_wave_data) # Close the audio device dsp.close() except (ossaudiodev.OSSAudioError, OSError) as e: print(f\\"Error playing sine wave: {e}\\")"},{"question":"# Image Classification Task Using PyTorch Objective: To design a custom neural network using PyTorch for classifying images from a given dataset. This network should include convolutional layers, pooling layers, non-linear activation functions, and should be trained using an appropriate loss function. Task: 1. **Build a Convolutional Neural Network (CNN) class in PyTorch:** - The class should be initialized with the necessary layers including convolution (at least two conv layers), pooling (max pooling or average pooling), non-linear activation (e.g., ReLU), and fully connected layers. - Include methods for forward propagation that apply these layers in sequence. ```python import torch.nn as nn import torch.nn.functional as F class CustomCNN(nn.Module): def __init__(self, num_classes: int): super(CustomCNN, self).__init__() # Define layers self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1) self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0) self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1) self.fc1 = nn.Linear(32 * 8 * 8, 120) self.fc2 = nn.Linear(120, num_classes) def forward(self, x): # Apply layers in sequence x = self.pool(F.relu(self.conv1(x))) x = self.pool(F.relu(self.conv2(x))) x = x.view(-1, 32 * 8 * 8) # Flattening the output for the fully connected layer x = F.relu(self.fc1(x)) x = self.fc2(x) return x ``` 2. **Training the Network:** - Load a toy dataset: CIFAR-10 (Assume it is pre-downloaded). - Define a training loop to train your model using Cross-Entropy Loss and SGD optimizer. ```python import torch import torchvision import torchvision.transforms as transforms transform = transforms.Compose( [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]) batch_size = 4 trainset = torchvision.datasets.CIFAR10(root=\'./data\', train=True, download=True, transform=transform) trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2) testset = torchvision.datasets.CIFAR10(root=\'./data\', train=False, download=True, transform=transform) testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2) classes = (\'plane\', \'car\', \'bird\', \'cat\', \'deer\', \'dog\', \'frog\', \'horse\', \'ship\', \'truck\') # Assuming device is either \'cpu\' or \'cuda\' device = torch.device(\\"cuda:0\\" if torch.cuda.is_available() else \\"cpu\\") net = CustomCNN(num_classes=len(classes)) net.to(device) import torch.optim as optim criterion = nn.CrossEntropyLoss() optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9) for epoch in range(2): # loop over the dataset multiple times running_loss = 0.0 for i, data in enumerate(trainloader, 0): # get the inputs; data is a list of [inputs, labels] inputs, labels = data[0].to(device), data[1].to(device) # zero the parameter gradients optimizer.zero_grad() # forward + backward + optimize outputs = net(inputs) loss = criterion(outputs, labels) loss.backward() optimizer.step() # print statistics running_loss += loss.item() if i % 2000 == 1999: # print every 2000 mini-batches print(\'[%d, %5d] loss: %.3f\' % (epoch + 1, i + 1, running_loss / 2000)) running_loss = 0.0 print(\'Finished Training\') ``` Constraints and Notes: 1. Assume the CIFAR-10 dataset is pre-downloaded and available in the \'./data\' directory. 2. Remember to transfer your model and data to GPU if available. 3. You can modify the number of epochs and other hyperparameters to fine-tune the performance. 4. Calculate and print the accuracy of your model on the test data at the end of training. Expected Output: A well-documented and functional implementation of `CustomCNN` class and a training loop that trains the model on the CIFAR-10 dataset, printing the training loss periodically and the final accuracy on the test set.","solution":"import torch import torch.nn as nn import torch.nn.functional as F class CustomCNN(nn.Module): def __init__(self, num_classes: int): super(CustomCNN, self).__init__() # Define layers self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1) self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0) self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1) self.fc1 = nn.Linear(32 * 8 * 8, 120) self.fc2 = nn.Linear(120, num_classes) def forward(self, x): # Apply layers in sequence x = self.pool(F.relu(self.conv1(x))) x = self.pool(F.relu(self.conv2(x))) x = x.view(-1, 32 * 8 * 8) # Flattening the output for the fully connected layer x = F.relu(self.fc1(x)) x = self.fc2(x) return x"},{"question":"**HTML Entity Decoder** You are required to write a function that decodes a given string by replacing HTML entities with their corresponding Unicode characters. # Function Signature ```python def decode_html_entities(input_str: str) -> str: pass ``` # Input - `input_str` (str): A string containing HTML entities. # Output - `decoded_str` (str): The decoded string where all HTML entities have been replaced with their corresponding Unicode characters. # Constraints - The input string `input_str` may contain zero or more HTML entities. - You must use the dictionaries provided by the `html.entities` module. - Ensure the function handles both named references with and without the trailing semicolon. - Performance should be optimal as the input may be large, but functional correctness is paramount. # Example ```python print(decode_html_entities(\\"Hello &amp; welcome to the world of &lt;Python&gt;!\\")) # Output: \\"Hello & welcome to the world of <Python>!\\" print(decode_html_entities(\\"Temperature is 37&deg;C which is &plusmn;5&deg;\\")) # Output: \\"Temperature is 37°C which is ±5°\\" ``` # Additional Notes - The function should handle edge cases where the input string does not contain any HTML entities. - If an entity is not found in `html5`, `entitydefs`, `name2codepoint`, or `codepoint2name`, it should be left as it is in the output string. - Make sure to import `html.entities` in your solution and utilize the appropriate dictionaries.","solution":"from html import unescape def decode_html_entities(input_str: str) -> str: Decodes a given string by replacing HTML entities with their corresponding Unicode characters. Parameters: input_str (str): A string containing HTML entities. Returns: str: The decoded string where HTML entities have been replaced with their corresponding Unicode characters. return unescape(input_str)"},{"question":"# Custom Pickling for Complex Data Structures Background You are required to demonstrate your understanding of custom serialization in Python by implementing and registering custom pickling logic for a complex data structure using the `copyreg` module. This will help you understand advanced object serialization techniques, which can be crucial for optimizing storage and transmission of custom objects. Task 1. **Implement a nested data structure that includes**: - A class `Person` with attributes `name` (string) and `age` (integer). - A class `Team` with attributes `members` (list of `Person` instances) and `team_name` (string). 2. **Define custom pickling functions** for these classes: - `pickle_person(person)`: Should return a tuple with necessary information for reconstructing a `Person` object. - `pickle_team(team)`: Should return a tuple with necessary information for reconstructing a `Team` object. - Define corresponding constructor functions if necessary. 3. **Register these pickling functions using `copyreg.pickle`**. 4. **Demonstrate the custom serialization and deserialization**: - Create instances of `Person` and `Team`. - Serialize and deserialize using the `pickle` module. - Ensure the correctness and equivalence of original and deserialized objects. Specifics - **Input**: - `Person` instances with various `name` and `age` values. - `Team` instance with a `team_name` and a list of `Person` instances. - **Output**: - Serialized data as bytes. - Deserialized objects that match the original instances. - **Constraints**: - Ensure that attributes of the original and deserialized objects are equivalent. - Handle unexpected types or values appropriately. Example Usage ```python import copyreg import pickle # Step 1: Define the classes class Person: def __init__(self, name, age): self.name = name self.age = age class Team: def __init__(self, team_name, members): self.team_name = team_name self.members = members # Step 2: Define and register the custom pickling functions def pickle_person(person): return (Person, (person.name, person.age)) def pickle_team(team): return (Team, (team.team_name, team.members)) copyreg.pickle(Person, pickle_person) copyreg.pickle(Team, pickle_team) # Step 3: Demonstrate serialization and deserialization original_person = Person(\\"Alice\\", 30) original_team = Team(\\"Developers\\", [original_person, Person(\\"Bob\\", 25)]) pickled_data_person = pickle.dumps(original_person) pickled_data_team = pickle.dumps(original_team) deserialized_person = pickle.loads(pickled_data_person) deserialized_team = pickle.loads(pickled_data_team) # Ensure correctness assert original_person.name == deserialized_person.name assert original_person.age == deserialized_person.age assert original_team.team_name == deserialized_team.team_name assert len(original_team.members) == len(deserialized_team.members) for orig, des in zip(original_team.members, deserialized_team.members): assert orig.name == des.name assert orig.age == des.age print(\\"Custom pickling and unpickling works correctly.\\") ``` **Note**: When submitting your solution, ensure all necessary classes, functions, and registrations are implemented within a single script.","solution":"import copyreg import pickle # Step 1: Define the classes class Person: def __init__(self, name, age): self.name = name self.age = age class Team: def __init__(self, team_name, members): self.team_name = team_name self.members = members # Step 2: Define and register the custom pickling functions def pickle_person(person): return (Person, (person.name, person.age)) def pickle_team(team): return (Team, (team.team_name, team.members)) copyreg.pickle(Person, pickle_person) copyreg.pickle(Team, pickle_team) # Step 3: Demonstrate serialization and deserialization functions def serialize_person(person): return pickle.dumps(person) def deserialize_person(serialized_person): return pickle.loads(serialized_person) def serialize_team(team): return pickle.dumps(team) def deserialize_team(serialized_team): return pickle.loads(serialized_team)"},{"question":"**Title:** Implementing and Composing Function Transforms with torch.func **Objective:** Your task is to implement a function that performs gradient computation and vectorization using the `torch.func` module. You will compose the `grad` and `vmap` transforms to efficiently compute per-sample gradients of a given function. **Problem Statement:** You are given a function `f` which computes the square of the sum of the input tensor elements. Implement a function `per_sample_gradients` that takes a batch of input tensors and returns the per-sample gradients of `f` with respect to each input tensor in the batch. **Function Signature:** ```python import torch from torch.func import grad, vmap def per_sample_gradients(inputs: torch.Tensor) -> torch.Tensor: # Your implementation here pass ``` **Input:** - `inputs` (torch.Tensor): A batch of input tensors with shape `(batch_size, num_features)`. Each tensor in the batch has `num_features` elements. **Output:** - `torch.Tensor`: A tensor of shape `(batch_size, num_features)` containing the per-sample gradients of `f` with respect to each input tensor. **Constraints:** - You must use the `grad` and `vmap` transforms from the `torch.func` module to implement the `per_sample_gradients` function. - The function `f` is defined as `f(x) = (x.sum())**2`. **Example:** ```python inputs = torch.tensor([[1.0, 2.0], [3.0, 4.0]], requires_grad=True) result = per_sample_gradients(inputs) print(result) ``` **Expected Output:** ``` tensor([[ 6.0, 6.0], [14.0, 14.0]]) ``` **Explanation:** - For the input `[1.0, 2.0]`, the sum is `3.0` and the square is `9.0`. The gradient of `9.0` with respect to `[1.0, 2.0]` is `[6.0, 6.0]`. - For the input `[3.0, 4.0]`, the sum is `7.0` and the square is `49.0`. The gradient of `49.0` with respect to `[3.0, 4.0]` is `[14.0, 14.0]`. **Notes:** - Ensure that your implementation is efficient and leverages the composition of `grad` and `vmap` transforms as described in the `torch.func` documentation. Good luck!","solution":"import torch from torch.func import grad, vmap def f(x): return (x.sum())**2 def per_sample_gradients(inputs: torch.Tensor) -> torch.Tensor: grad_f = grad(f) per_sample_grads = vmap(grad_f)(inputs) return per_sample_grads"},{"question":"Objective: Implement a class-based system that demonstrates your understanding of class definitions, attributes, methods, inheritance, and iterators in Python. Problem Statement: You are tasked with creating a system to manage a library of books. The system should allow for the addition of books, issuance of books to users, and listing of available books. Additionally, the library should be iterable to list all books in the library one by one. Requirements: 1. **Class Definitions:** - Define a base class `Book` with the following attributes: - `title` (string) - `author` (string) - `isbn` (string) - Define a class `Library`, which contains a collection of `Book` objects. 2. **Methods:** - In the `Book` class, implement the following methods: - `__init__(self, title, author, isbn)`: Initializes the book with the provided title, author, and ISBN. - `__repr__(self)`: Returns a string representation of the book in the format `\\"<title> by <author> (ISBN: <isbn>)\\"`. - In the `Library` class, implement the following methods: - `__init__(self)`: Initializes an empty list to store books. - `add_book(self, book)`: Adds a `Book` object to the library. - `list_books(self)`: Returns a list of all books currently in the library. - `issue_book(self, isbn)`: Issues a book by ISBN if it exists in the library and is available; removes it from the library. - `__iter__(self)`: Returns an iterator for the library. - `__next__(self)`: Allows iteration over the books in the library. - Implement a private method `_find_book_by_isbn(self, isbn)` in the `Library` class to encapsulate the logic of finding a book by ISBN. 3. **Constraints:** - Each book\'s ISBN must be unique within the library. - The `Library` class should handle cases where operations are performed on non-existent books gracefully. Input and Output: - The `Book` class should be instantiated with strings representing the title, author, and ISBN. - The `Library` class should handle the addition, listing, and issuance of books as described. Example: ```python # Example usage: # Create books book1 = Book(\\"The Great Gatsby\\", \\"F. Scott Fitzgerald\\", \\"123456\\") book2 = Book(\\"1984\\", \\"George Orwell\\", \\"654321\\") book3 = Book(\\"To Kill a Mockingbird\\", \\"Harper Lee\\", \\"111111\\") # Create library and add books library = Library() library.add_book(book1) library.add_book(book2) library.add_book(book3) # List books print(library.list_books()) # Issue a book library.issue_book(\\"123456\\") # List books after issuance print(library.list_books()) # Iterate over library for book in library: print(book) ``` Expected output: ```python [\'The Great Gatsby by F. Scott Fitzgerald (ISBN: 123456)\', \'1984 by George Orwell (ISBN: 654321)\', \'To Kill a Mockingbird by Harper Lee (ISBN: 111111)\'] [\'1984 by George Orwell (ISBN: 654321)\', \'To Kill a Mockingbird by Harper Lee (ISBN: 111111)\'] 1984 by George Orwell (ISBN: 654321) To Kill a Mockingbird by Harper Lee (ISBN: 111111) ``` **Note**: The above example demonstrates the sequence of method calls and the corresponding outputs.","solution":"class Book: def __init__(self, title, author, isbn): self.title = title self.author = author self.isbn = isbn def __repr__(self): return f\\"{self.title} by {self.author} (ISBN: {self.isbn})\\" class Library: def __init__(self): self.books = [] self._current = 0 def add_book(self, book): if not self._find_book_by_isbn(book.isbn): self.books.append(book) def list_books(self): return [repr(book) for book in self.books] def issue_book(self, isbn): book = self._find_book_by_isbn(isbn) if book: self.books.remove(book) def _find_book_by_isbn(self, isbn): for book in self.books: if book.isbn == isbn: return book return None def __iter__(self): self._current = 0 return self def __next__(self): if self._current < len(self.books): result = self.books[self._current] self._current += 1 return result else: raise StopIteration"}]'),z={name:"App",components:{PoemCard:A},data(){return{searchQuery:"",visibleCount:4,poemsData:I,isLoading:!1}},computed:{filteredPoems(){const i=this.searchQuery.trim().toLowerCase();return i?this.poemsData.filter(e=>e.question&&e.question.toLowerCase().includes(i)||e.solution&&e.solution.toLowerCase().includes(i)):this.poemsData},displayedPoems(){return this.searchQuery.trim()?this.filteredPoems:this.filteredPoems.slice(0,this.visibleCount)},hasMorePoems(){return!this.searchQuery.trim()&&this.visibleCount<this.poemsData.length}},methods:{async loadMore(){this.isLoading=!0,await new Promise(i=>setTimeout(i,1e3)),this.visibleCount+=4,this.isLoading=!1}}},D={class:"search-container"},q={class:"card-container"},F={key:0,class:"empty-state"},R=["disabled"],j={key:0},N={key:1};function L(i,e,l,m,o,s){const h=_("PoemCard");return a(),n("section",null,[e[4]||(e[4]=t("div",{class:"top-banner"},[t("div",{class:"top-banner-title"},[t("div",{class:"top-banner-title-text"},"🤔prompts chat🧠")])],-1)),t("div",D,[e[3]||(e[3]=t("span",{class:"search-icon"},"🔍",-1)),g(t("input",{type:"text",class:"search-input","onUpdate:modelValue":e[0]||(e[0]=r=>o.searchQuery=r),placeholder:"Search..."},null,512),[[y,o.searchQuery]]),o.searchQuery?(a(),n("button",{key:0,class:"clear-search",onClick:e[1]||(e[1]=r=>o.searchQuery="")}," ✕ ")):d("",!0)]),t("div",q,[(a(!0),n(b,null,v(s.displayedPoems,(r,f)=>(a(),w(h,{key:f,poem:r},null,8,["poem"]))),128)),s.displayedPoems.length===0?(a(),n("div",F,' No results found for "'+c(o.searchQuery)+'". ',1)):d("",!0)]),s.hasMorePoems?(a(),n("button",{key:0,class:"load-more-button",disabled:o.isLoading,onClick:e[2]||(e[2]=(...r)=>s.loadMore&&s.loadMore(...r))},[o.isLoading?(a(),n("span",N,"Loading...")):(a(),n("span",j,"See more"))],8,R)):d("",!0)])}const O=p(z,[["render",L],["__scopeId","data-v-44e36ea7"]]),Y=JSON.parse('{"title":"","description":"","frontmatter":{"page":true},"headers":[],"relativePath":"library/59.md","filePath":"library/59.md"}'),M={name:"library/59.md"},B=Object.assign(M,{setup(i){return(e,l)=>(a(),n("div",null,[x(O)]))}});export{Y as __pageData,B as default};
