import{_ as p,o as a,c as i,a as t,m as u,t as c,C as _,M as g,U as y,f as d,F as b,p as v,e as w,q as x}from"./chunks/framework.B1z0IdBH.js";const k={name:"PoemCard",props:{poem:{type:Object,required:!0}}},T={class:"poem-container"},C={class:"review"},E={class:"review-title"},P={class:"review-content"};function S(s,e,l,m,n,r){return a(),i("div",T,[t("div",C,[t("div",E,[e[0]||(e[0]=t("span",{class:"icon"},"question:",-1)),u(c(l.poem.question),1)]),e[2]||(e[2]=t("div",{class:"divider"},null,-1)),t("p",P,[e[1]||(e[1]=t("span",{class:"icon"},"answer:",-1)),u(c(l.poem.solution),1)])])])}const A=p(k,[["render",S],["__scopeId","data-v-3c13d9cc"]]),I=JSON.parse('[{"question":"# Advanced Python Coding Assessment: Using the `wave` Module Objective Demonstrate your understanding of Python\'s `wave` module by writing code to read, process, and write WAV files. Problem Statement You are provided with a mono WAV file called `input.wav`. Your task is to: 1. Read the WAV file. 2. Double the amplitude (volume) of its audio signal. 3. Write the modified signal to a new mono WAV file called `output.wav`. Requirements - The input WAV file will always be mono (1 channel) and use a sample width of 2 bytes (16 bits per sample). - The change in amplitude should be handled carefully to avoid clipping. Ensure that the modified signal values do not exceed the maximum and minimum possible values for 16-bit audio. - Your code should use the `wave` module for reading and writing the WAV files. Constraints - The input WAV file will have a maximum size of 100 MB. - You are not allowed to use any external libraries for audio processing other than the standard `wave` module. Input Format - A mono WAV file named `input.wav` located in the same directory as your script. Output Format - A mono WAV file named `output.wav` containing the audio with doubled amplitude. Example If the input file `input.wav` has sample values [1000, -1000, 2000, -2000], then `output.wav` should contain [2000, -2000, 4000, -4000], with necessary clipping to fit within 16-bit range. Note Ensure to handle endianness properly while reading and writing the sample values. Solution Template ```python import wave import struct def process_audio(input_file, output_file): # Open the input wave file with wave.open(input_file, \'rb\') as w: params = w.getparams() n_frames = w.getnframes() frames = w.readframes(n_frames) # Convert the byte frames to a list of sample values samples = struct.unpack(\'<{}h\'.format(n_frames), frames) # Double the amplitude and clip values to avoid overflow max_amp = 32767 # Maximum amplitude for 16-bit audio min_amp = -32768 # Minimum amplitude for 16-bit audio new_samples = [] for sample in samples: new_sample = sample * 2 if new_sample > max_amp: new_sample = max_amp elif new_sample < min_amp: new_sample = min_amp new_samples.append(new_sample) # Pack the new samples back into byte frames new_frames = struct.pack(\'<{}h\'.format(len(new_samples)), *new_samples) # Write the modified frames to the output wave file with wave.open(output_file, \'wb\') as w: w.setparams(params) w.writeframes(new_frames) # Use the function with provided input and output file names process_audio(\'input.wav\', \'output.wav\') ``` Testing Your Solution To test your solution, you can use any mono WAV file (`input.wav`) available to you. Run your script and verify that the `output.wav` file has the doubled amplitude without audio distortion due to clipping.","solution":"import wave import struct def process_audio(input_file, output_file): with wave.open(input_file, \'rb\') as input_wave: params = input_wave.getparams() n_frames = input_wave.getnframes() frames = input_wave.readframes(n_frames) # Unpack frames to integer samples samples = struct.unpack(\'<{}h\'.format(n_frames), frames) # Double the amplitude with clipping max_amp = 32767 # Max value for 16-bit audio min_amp = -32768 # Min value for 16-bit audio new_samples = [min(max(sample * 2, min_amp), max_amp) for sample in samples] # Pack samples into bytes new_frames = struct.pack(\'<{}h\'.format(len(new_samples)), *new_samples) with wave.open(output_file, \'wb\') as output_wave: output_wave.setparams(params) output_wave.writeframes(new_frames) # Example usage # Uncomment below lines for actual usage of function with file names as arguments # process_audio(\'input.wav\', \'output.wav\')"},{"question":"# Complex Numeric Calculation Using `python310` **Objective:** Write a Python function that uses the `python310` C API to perform the following sequence of numeric operations on two inputs: 1. Convert both inputs to integers. 2. Add the two integers. 3. Subtract the second integer from the first integer. 4. Multiply the two integers. 5. Perform matrix multiplication on the two numbers. 6. Compute the floor division of the first integer by the second. 7. Compute the true division of the first integer by the second. 8. Compute the remainder of dividing the first integer by the second. 9. Raise the first integer to the power of the second. 10. Perform a bitwise AND on the two integers. 11. Perform a bitwise OR on the two integers. 12. Perform a bitwise XOR on the two integers. 13. Perform a left shift of the first integer by the second. 14. Perform a right shift of the first integer by the second. 15. Return all results as a dictionary. # Function Signature ```python def complex_numeric_operations(o1: PyObject, o2: PyObject) -> dict: :param o1: First numeric input of type PyObject :param o2: Second numeric input of type PyObject :return: A dictionary containing the results of all specified operations ``` # Input and Output - **Input**: Two numeric objects (which could be int, float, or objects providing a numeric interface). - **Output**: A dictionary containing the results of each specified operation. The keys will be the names of the operations, and the values will be the results of those operations. # Constraints - Both inputs must be valid numeric objects. If an operation fails (e.g., due to a division by zero), skip that operation and continue with the next. - Use the `python310` API functions provided in the documentation for implementing the operations. - Raise an appropriate Python exception if both inputs are not numeric. # Example: ```python # Example inputs as PyObject o1 = PyNumber_Long(PyObject representing 10) o2 = PyNumber_Long(PyObject representing 5) # Expected output: { \\"addition\\": PyObject representing 15, \\"subtraction\\": PyObject representing 5, \\"multiplication\\": PyObject representing 50, \\"matrix_multiplication\\": PyObject representing 50, # Assuming matrix multiplication is same as normal multiplication for scalar values \\"floor_division\\": PyObject representing 2, \\"true_division\\": PyObject representing 2.0, \\"remainder\\": PyObject representing 0, \\"power\\": PyObject representing 100000, \\"bitwise_and\\": PyObject representing 0, \\"bitwise_or\\": PyObject representing 15, \\"bitwise_xor\\": PyObject representing 15, \\"left_shift\\": PyObject representing 320, \\"right_shift\\": PyObject representing 0 } result = complex_numeric_operations(o1, o2) print(result) ``` # Note: - This example assumes that you have already converted input numbers to the PyObject type. When writing tests, ensure to mock or correctly create PyObject instances as required. **Good luck!**","solution":"def complex_numeric_operations(o1, o2): Perform a sequence of numeric operations between two inputs and return the results as a dictionary. :param o1: First numeric input :param o2: Second numeric input :return: A dictionary containing the results of all specified operations results = {} try: int1 = int(o1) int2 = int(o2) results[\\"addition\\"] = int1 + int2 results[\\"subtraction\\"] = int1 - int2 results[\\"multiplication\\"] = int1 * int2 results[\\"matrix_multiplication\\"] = int1 * int2 # Matrix multiplication for scalars is same as multiplication results[\\"floor_division\\"] = int1 // int2 results[\\"true_division\\"] = int1 / int2 results[\\"remainder\\"] = int1 % int2 results[\\"power\\"] = int1 ** int2 results[\\"bitwise_and\\"] = int1 & int2 results[\\"bitwise_or\\"] = int1 | int2 results[\\"bitwise_xor\\"] = int1 ^ int2 results[\\"left_shift\\"] = int1 << int2 results[\\"right_shift\\"] = int1 >> int2 except ValueError as e: raise ValueError(\\"Both inputs must be numeric.\\") from e except ZeroDivisionError: # If there\'s a division by zero, skip these operations results[\\"floor_division\\"] = None results[\\"true_division\\"] = None results[\\"remainder\\"] = None return results"},{"question":"Objective Create and manage a simple database of user profiles using Python\'s `dbm` module. Your task is to implement a function that performs several operations on this database and ensure it is compatible with any backend supported by the `dbm` module. Task Description Write a Python function `user_profile_db(filename, operations)` that manages user profile information. The function should: 1. Open a database located at `filename` using `dbm.open`. 2. Perform a series of operations provided as a list of tuples in the format: - `(\\"add\\", key, value)`: Add a new user profile where `key` and `value` are strings. - `(\\"delete\\", key)`: Delete a user profile identified by `key`. - `(\\"get\\", key)`: Retrieve the value of the user profile with `key`. 3. Return a list of results for all \\"get\\" operations. If the key is not found, return `None` for that operation. 4. Use appropriate handling to ensure the database is correctly opened in read-write mode (`\'c\'` - create if necessary). 5. Ensure the function properly manages the context to close the database automatically after operations. Function Signature ```python def user_profile_db(filename: str, operations: List[Tuple[str, str, Optional[str]]]) -> List[Optional[bytes]]: ``` Input - `filename`: A string representing the name of the database file. - `operations`: A list of tuples where each tuple represents an operation. The first element of the tuple is a string (`\\"add\\"`, `\\"delete\\"`, or `\\"get\\"`). - If the operation is `\\"add\\"`, the tuple includes a `key` and a `value`. - If the operation is `\\"delete\\"` or `\\"get\\"`, the tuple only includes a `key`. Output - A list of results for each \\"get\\" operation. Each result should be the value associated with the key or `None` if the key does not exist. Constraints - Keys and values are always strings and should be stored as bytes in the database. - The database should handle non-existent keys gracefully. - The maximum length for keys and values is 256 characters. Example Usage ```python operations = [ (\\"add\\", \\"user1\\", \\"profile1\\"), (\\"add\\", \\"user2\\", \\"profile2\\"), (\\"get\\", \\"user1\\"), (\\"delete\\", \\"user1\\"), (\\"get\\", \\"user1\\"), (\\"get\\", \\"user2\\") ] print(user_profile_db(\\"user_profiles.db\\", operations)) # Expected output: [b\'profile1\', None, b\'profile2\'] ``` Additional Instructions - Ensure your function handles both the \'dbm.gnu\', \'dbm.ndbm\', and \'dbm.dumb\' implementations correctly. - Remember to handle keys and values as bytes when interacting with the `dbm` object. - Use the `with` statement to manage the database context properly.","solution":"import dbm from typing import List, Tuple, Optional def user_profile_db(filename: str, operations: List[Tuple[str, str, Optional[str]]]) -> List[Optional[bytes]]: results = [] with dbm.open(filename, \'c\') as db: for operation in operations: if operation[0] == \'add\': key, value = operation[1], operation[2] db[key] = value.encode() elif operation[0] == \'delete\': key = operation[1] if key in db: del db[key] elif operation[0] == \'get\': key = operation[1] if key in db: results.append(db[key]) else: results.append(None) return results"},{"question":"<|Analysis Begin|> The provided documentation snippet is for the `torch.nn.init` module of PyTorch, which includes a variety of functions for initializing neural network parameters. The initializations available are useful for setting up the weights of neural networks in a way that can significantly affect the training dynamics and outcomes. The functions listed include: - `calculate_gain` - `uniform_` - `normal_` - `constant_` - `ones_` - `zeros_` - `eye_` - `dirac_` - `xavier_uniform_` - `xavier_normal_` - `kaiming_uniform_` - `kaiming_normal_` - `trunc_normal_` - `orthogonal_` - `sparse_` Given this context, we need to devise a problem that requires students to use some of these initialization functions to demonstrate their understanding. The question should present a scenario where it is crucial to use proper weight initialization methods to ensure effective training of a neural network. <|Analysis End|> <|Question Begin|> # Coding Assessment Question PyTorch provides several methods to initialize the weights of neural networks, which can significantly impact the performance and training speed of the models. Correct weight initialization can help in converging faster and preventing problems such as exploding or vanishing gradients. Problem Statement You are provided with a neural network architecture and need to write a function to initialize its weights using appropriate methods from the `torch.nn.init` module. Your task is to: 1. Implement a fully connected neural network with the following architecture: - Input layer: 128 neurons - Hidden layer 1: 64 neurons - Hidden layer 2: 32 neurons - Output layer: 10 neurons 2. Write an initializer function `initialize_weights(model)` that initializes the weights of the model using: - Xavier Initialization for layers with `ReLU` activation functions. - Kaiming Initialization for layers with `LeakyReLU` activation functions. - Use zero bias initialization for all layers. Function Signature ```python import torch import torch.nn as nn import torch.nn.init as init class SimpleNN(nn.Module): def __init__(self): super(SimpleNN, self).__init__() self.fc1 = nn.Linear(128, 64) self.relu = nn.ReLU() self.fc2 = nn.Linear(64, 32) self.leaky_relu = nn.LeakyReLU(0.01) self.fc3 = nn.Linear(32, 10) def forward(self, x): x = self.relu(self.fc1(x)) x = self.leaky_relu(self.fc2(x)) x = self.fc3(x) return x def initialize_weights(model): Initialize weights of the given model. Args: - model (nn.Module): An instance of nn.Module representing the neural network. Returns: - None for m in model.modules(): if isinstance(m, nn.Linear): if m.bias is not None: init.zeros_(m.bias) if isinstance(m, nn.Linear): if isinstance(model.relu, nn.ReLU): init.xavier_uniform_(m.weight) elif isinstance(model.leaky_relu, nn.LeakyReLU): init.kaiming_uniform_(m.weight, nonlinearity=\'leaky_relu\') ``` Input - A `SimpleNN` model. Output - No explicit output, but the weights of the model should be properly initialized. Constraints - You must utilize `torch.nn.init` methods to initialize the weights. - The initialization should be done in a way that suits the activation functions used in the network. Example ```python model = SimpleNN() initialize_weights(model) # The model should now have its weights and biases initialized accordingly ``` Notes This question involves understanding and applying the right initialization methods to neural network layers based on their activation functions, demonstrating the student\'s grasp of both initialization techniques and PyTorch\'s weight initialization utilities.","solution":"import torch import torch.nn as nn import torch.nn.init as init class SimpleNN(nn.Module): def __init__(self): super(SimpleNN, self).__init__() self.fc1 = nn.Linear(128, 64) self.relu = nn.ReLU() self.fc2 = nn.Linear(64, 32) self.leaky_relu = nn.LeakyReLU(0.01) self.fc3 = nn.Linear(32, 10) def forward(self, x): x = self.relu(self.fc1(x)) x = self.leaky_relu(self.fc2(x)) x = self.fc3(x) return x def initialize_weights(model): Initialize weights of the given model. Args: - model (nn.Module): An instance of nn.Module representing the neural network. Returns: - None for m in model.modules(): if isinstance(m, nn.Linear): if m.bias is not None: init.zeros_(m.bias) if m == model.fc1: init.xavier_uniform_(m.weight) elif m == model.fc2: init.kaiming_uniform_(m.weight, nonlinearity=\'leaky_relu\') elif m == model.fc3: init.xavier_uniform_(m.weight)"},{"question":"**Advanced Python Coding Assessment** **Objective**: Demonstrate your comprehensive understanding of Python\'s `compileall` module by writing a script that utilizes various features of this module to compile `.py` files in a specified directory with advanced options. **Problem Statement**: You are required to write a Python function `custom_compile` that takes in several parameters to customize the byte-code compilation process of Python source files in a given directory. **Function Signature**: ```python def custom_compile(directory: str, maxlevels: int = 10, force: bool = False, exclude_pattern: str = None, quiet_level: int = 0, optimizations: list = [-1], workers: int = 1, invalidation_mode: str = \\"timestamp\\", use_hard_links: bool = False) -> bool: pass ``` **Parameters**: - `directory`: The root directory in which to start compiling Python source files. - `maxlevels`: Maximum depth for recursive compilation of subdirectories. - `force`: A boolean indicating whether to force recompilation irrespective of file timestamps. - `exclude_pattern`: A regex pattern string to exclude specific files or directories from compilation. - `quiet_level`: An integer (0, 1, or 2) specifying the verbosity level of the output. - 0: Print filenames and other information. - 1: Print only errors. - 2: Suppress all output. - `optimizations`: A list of integers specifying optimization levels (e.g., `[-1]` for no optimization, `[0, 1, 2]` for all levels). - `workers`: An integer specifying the number of worker threads to use for parallel compilation. - `invalidation_mode`: A string specifying the invalidation mode (`\\"timestamp\\"`, `\\"checked-hash\\"`, or `\\"unchecked-hash\\"`). - `use_hard_links`: A boolean indicating whether to use hard links to consolidate duplicate `.pyc` files with different optimization levels. **Return Value**: - A boolean that is `True` if all files compiled successfully, and `False` otherwise. **Constraints**: 1. You must handle invalid optimization levels and other invalid inputs gracefully, providing meaningful error messages. 2. Ensure that the provided regex pattern (if any) is correctly used to filter out files. 3. Make efficient use of multiple workers based on the system\'s capabilities. **Example**: ```python result = custom_compile(\'src/\', maxlevels=5, force=True, exclude_pattern=\'.*test.*\', quiet_level=1, optimizations=[0, 1], workers=4, invalidation_mode=\'checked-hash\', use_hard_links=True) print(result) # Expected output: True or False based on success of compilation ``` **Additional Notes**: - You may assume the `re` and `compileall` modules are available. - Consider edge cases such as invalid directory paths, improper regex patterns, insufficient permissions, etc. Using the features described, this task verifies your ability to integrate and effectively utilize the `compileall` module\'s advanced functionalities.","solution":"import compileall import re from pathlib import Path def custom_compile(directory: str, maxlevels: int = 10, force: bool = False, exclude_pattern: str = None, quiet_level: int = 0, optimizations: list = [-1], workers: int = 1, invalidation_mode: str = \\"timestamp\\", use_hard_links: bool = False) -> bool: Custom byte-code compilation of Python source files in a given directory. # Verify the directory exists path = Path(directory) if not path.exists() or not path.is_dir(): raise NotADirectoryError(f\\"The specified directory \'{directory}\' does not exist or is not a directory.\\") # Validate exclusion pattern exclude = None if exclude_pattern: try: exclude = re.compile(exclude_pattern) except re.error as e: print(f\\"Invalid regex pattern: {exclude_pattern}\\") raise ValueError(f\\"Invalid regex pattern: {exclude_pattern}\\") from e # Validate optimization levels valid_optimizations = [-1, 0, 1, 2] for opt in optimizations: if opt not in valid_optimizations: raise ValueError(f\\"Invalid optimization level: {opt}. Valid levels are {valid_optimizations}\\") # Call to compileall.compile_dir() success = compileall.compile_dir( dir=directory, maxlevels=maxlevels, force=force, quiet=quiet_level, legacy=True, # to support the exclude pattern rx=exclude, workers=workers, invalidation_mode=invalidation_mode, hardlink_dupes=use_hard_links ) return success"},{"question":"You are required to implement a function that reads, validates, and processes a list of date and time strings into a sorted list of `datetime.datetime` objects. The function should utilize the capabilities of the `datetime` module as described in the documentation. # Requirements 1. **Function Signature**: ```python def process_datetime_strings(date_strings: List[str]) -> List[datetime.datetime]: ``` 2. **Input**: - `date_strings`: A list of strings, where each string represents a date and time in the format \\"YYYY-MM-DD HH:MM:SS\\". 3. **Output**: - A list of `datetime.datetime` objects sorted in ascending order. Invalid date strings should be ignored. # Constraints 1. Each date string is expected to be in the format \\"YYYY-MM-DD HH:MM:SS\\". 2. Invalid date strings (e.g., incorrect format, non-existent dates) should be ignored. 3. The function should efficiently handle up to 10,000 date strings. # Examples ```python from typing import List from datetime import datetime def process_datetime_strings(date_strings: List[str]) -> List[datetime]: valid_datetimes = [] for date_str in date_strings: try: # Try to create a datetime object from the string dt = datetime.strptime(date_str, \\"%Y-%m-%d %H:%M:%S\\") valid_datetimes.append(dt) except ValueError: # Ignore invalid date strings continue # Sort the list of valid datetime objects return sorted(valid_datetimes) # Example Usage date_strings = [ \\"2022-03-25 12:30:45\\", \\"2022-02-30 10:00:00\\", # Invalid date \\"2023-01-01 00:00:00\\", \\"invalid date string\\", # Invalid format \\"2022-12-31 23:59:59\\" ] result = process_datetime_strings(date_strings) for dt in result: print(dt) ``` Expected Output: ``` 2022-03-25 12:30:45 2022-12-31 23:59:59 2023-01-01 00:00:00 ``` # Explanation The function `process_datetime_strings` reads each date string, validates it, and converts it into a `datetime` object. Invalid date strings are ignored. The resulting list of valid `datetime` objects is then sorted in ascending order. Use the concepts outlined in the \\"datetime\\" module documentation to implement this function. Ensure your solution is efficient and robust.","solution":"from typing import List from datetime import datetime def process_datetime_strings(date_strings: List[str]) -> List[datetime]: valid_datetimes = [] for date_str in date_strings: try: # Try to create a datetime object from the string dt = datetime.strptime(date_str, \\"%Y-%m-%d %H:%M:%S\\") valid_datetimes.append(dt) except ValueError: # Ignore invalid date strings continue # Sort the list of valid datetime objects return sorted(valid_datetimes)"},{"question":"**Question: Implementing and Manipulating Python Floating Point Objects** You are required to demonstrate your understanding of Python floating point objects by implementing a series of functions that utilize the `PyFloatObject` related methods described in the provided documentation. # Task Implement the following functions: 1. `create_float_from_string(s: str) -> float`: - **Input**: A string `s` that represents a floating point number. - **Output**: A Python `float` object converted from the string. - **Constraints**: If the string cannot be converted to a float, return `None`. 2. `is_float_type(obj: object) -> bool`: - **Input**: Any Python object. - **Output**: `True` if the object is a float, otherwise `False`. 3. `float_to_double_repr(f: float) -> float`: - **Input**: A Python `float` object. - **Output**: A C `double` representation of the given `float`. - **Constraints**: If the input is not a float, return `None`. 4. `get_float_info() -> dict`: - **Output**: A dictionary containing precision, minimum, and maximum representable finite float values. # Example Usage ```python print(create_float_from_string(\\"123.45\\")) # Expected output: 123.45 print(create_float_from_string(\\"abc\\")) # Expected output: None print(is_float_type(123.45)) # Expected output: True print(is_float_type(\\"123.45\\")) # Expected output: False print(float_to_double_repr(123.45)) # Expected output: 123.45 (or equivalent C double representation) print(float_to_double_repr(\\"123.45\\")) # Expected output: None print(get_float_info()) # Expected output: {\'precision\': ..., \'min\': ..., \'max\': ...} ``` # Notes - Ensure your implementations handle edge cases and invalid inputs gracefully. - Use appropriate error checking as described in the documentation, especially for `PyFloat_AsDouble`. # Constraints - Do not use any external libraries. - Assume Python 3.10 compatibility.","solution":"def create_float_from_string(s: str) -> float: Converts a string representing a floating point number to a Python float. Parameters: s (str): The string to be converted. Returns: float: The converted float, or None if conversion fails. try: return float(s) except ValueError: return None def is_float_type(obj: object) -> bool: Checks if the given object is of float type. Parameters: obj (object): The object to check. Returns: bool: True if the object is a float, otherwise False. return isinstance(obj, float) def float_to_double_repr(f: float) -> float: Returns the double representation of the given float if it is a float type. Parameters: f (float): The float to convert. Returns: float: The equivalent double representation if f is a float, otherwise None. if isinstance(f, float): return float(f) return None def get_float_info() -> dict: Returns a dictionary containing the precision, and the minimum and maximum representable finite float values. Returns: dict: A dictionary with precision, min, and max values. import sys return { \'precision\': sys.float_info.dig, \'min\': sys.float_info.min, \'max\': sys.float_info.max }"},{"question":"**Objective:** Demonstrate your understanding of dimensionality reduction and its integration within a machine learning pipeline using scikit-learn. **Question:** You are given a dataset, and your task is to: 1. Load the dataset. 2. Apply Principal Component Analysis (PCA) to reduce the dimensionality of the dataset to two components. 3. Integrate the PCA transformation into a pipeline that includes a supervised learning model (e.g., Logistic Regression). 4. Train the pipeline on the dataset. 5. Evaluate the performance of the pipeline using cross-validation. **Dataset:** - A CSV file named `data.csv` is provided with the following structure: ``` feature1, feature2, ..., featureN, label float, float, ..., float, int ``` **Specifications:** 1. **Input:** - A CSV file `data.csv`. 2. **Output:** - Print the cross-validation scores of the pipeline. 3. **Constraints:** - Use `PCA` for dimensionality reduction to 2 components. - Use `StandardScaler` to standardize the features before applying PCA. - Use `LogisticRegression` as the supervised learning model. 4. **Performance Requirements:** - Ensure the pipeline is efficiently executed without any data leakage. **Instructions:** 1. Import the necessary libraries. 2. Load the dataset using `pandas`. 3. Separate the dataset into features and labels. 4. Create a pipeline that includes: - StandardScaler - PCA (with 2 components) - LogisticRegression 5. Use `cross_val_score` to evaluate the pipeline with 5-fold cross-validation. 6. Print the cross-validation scores. **Example Code Template:** ```python import pandas as pd from sklearn.preprocessing import StandardScaler from sklearn.decomposition import PCA from sklearn.linear_model import LogisticRegression from sklearn.pipeline import Pipeline from sklearn.model_selection import cross_val_score # Load dataset data = pd.read_csv(\'data.csv\') # Separate features and labels X = data.drop(columns=\'label\') y = data[\'label\'] # Define the pipeline pipeline = Pipeline([ (\'scaler\', StandardScaler()), (\'pca\', PCA(n_components=2)), (\'classifier\', LogisticRegression()) ]) # Evaluate the pipeline using cross-validation scores = cross_val_score(pipeline, X, y, cv=5) # Print the cross-validation scores print(\\"Cross-Validation Scores:\\", scores) ``` **Submission:** Submit your implementation as a Python script or Jupyter Notebook that meets the above specifications.","solution":"import pandas as pd from sklearn.preprocessing import StandardScaler from sklearn.decomposition import PCA from sklearn.linear_model import LogisticRegression from sklearn.pipeline import Pipeline from sklearn.model_selection import cross_val_score def evaluate_pipeline(file_path): # Load dataset data = pd.read_csv(file_path) # Separate features and labels X = data.drop(columns=\'label\') y = data[\'label\'] # Define the pipeline pipeline = Pipeline([ (\'scaler\', StandardScaler()), (\'pca\', PCA(n_components=2)), (\'classifier\', LogisticRegression(solver=\'liblinear\')) ]) # Evaluate the pipeline using cross-validation scores = cross_val_score(pipeline, X, y, cv=5) # Print the cross-validation scores print(\\"Cross-Validation Scores:\\", scores) # Return scores for unit tests return scores"},{"question":"# Distributed Training with PyTorch **Objective:** Your task is to implement a fault-tolerant distributed training script using PyTorch and Torch Distributed Elastic. This will assess your understanding of setting up distributed training, handling node synchronization, and ensuring the training is resilient to node failures. **Problem Statement:** You are given a neural network model and a dataset. You need to write a Python script that accomplishes the following: 1. Sets up a distributed training environment with a given number of nodes. 2. Ensures synchronization between nodes using rendezvous. 3. Implements fault-tolerance so that the training can continue seamlessly in the event of node failure. # Instructions: 1. **Initialization**: - Use `torch.distributed` to initialize the process group for distributed training. - Use `torch.distributed.elastic` to handle rendezvous and fault-tolerance. 2. **Model and Data**: - Define a simple neural network model using PyTorch. - Load and distribute the dataset across multiple nodes. 3. **Training Loop**: - Implement a training loop that supports distributed training. - Ensure that the model\'s parameters are synchronized across all nodes at the end of each iteration. - Save the model\'s state periodically to handle potential node failures. 4. **Fault-Tolerance**: - Implement a mechanism to detect node failures and restart the training from the last saved state without losing progress. # Input: - Configuration (`num_nodes`, `num_epochs`, `learning_rate`, `sync_interval`). - A dataset and a simple neural network model definition (both provided). # Output: - Final trained model saved to a specified directory. - Logs indicating the training progress and any node restarts. # Constraints: - The training should distribute work evenly across all nodes. - The system should handle at least one node failure gracefully. # Notes: - You can assume the existence of helper functions for data loading, model saving/loading, etc. - Ensure to include comments in your code for clarity. # Example: ```python import torch import torch.distributed as dist from torch.nn.parallel import DistributedDataParallel as DDP def initialize_distributed_process_group(rank, world_size): # Initialize process group for distributed training def load_data(): # Load and prepare dataset for distributed training def define_model(): # Define a simple neural network model def save_model(model, epoch): # Save model states def train(rank, world_size, config): initialize_distributed_process_group(rank, world_size) model = define_model().to(rank) ddp_model = DDP(model, device_ids=[rank]) data_loader = load_data() optimizer = torch.optim.Adam(ddp_model.parameters(), lr=config[\'learning_rate\']) for epoch in range(config[\'num_epochs\']): for batch in data_loader: # Training loop implementation pass if epoch % config[\'sync_interval\'] == 0: save_model(ddp_model, epoch) dist.destroy_process_group() if __name__ == \\"__main__\\": config = { \\"num_nodes\\": 4, \\"num_epochs\\": 10, \\"learning_rate\\": 0.001, \\"sync_interval\\": 1 } world_size = config[\'num_nodes\'] torch.multiprocessing.spawn(train, args=(world_size, config), nprocs=world_size, join=True) ``` This provides a scaffold of the solution you need to complete. Fill in the missing parts and ensure that the training process is fault-tolerant. **Good Luck!**","solution":"import torch import torch.distributed as dist from torch.nn.parallel import DistributedDataParallel as DDP import torch.distributed.elastic.multiprocessing as mp from torch.utils.data import DataLoader, DistributedSampler import torch.optim as optim import torch.nn as nn import os def initialize_distributed_process_group(rank, world_size): dist.init_process_group(backend=\'nccl\', init_method=\'env://\', world_size=world_size, rank=rank) class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.fc1 = nn.Linear(10, 50) self.fc2 = nn.Linear(50, 10) def forward(self, x): x = torch.relu(self.fc1(x)) x = self.fc2(x) return x def load_data(): dataset = torch.utils.data.TensorDataset(torch.randn(1000, 10), torch.randn(1000, 10)) return dataset def save_model(model, epoch, path=\\"checkpoint.pth\\"): torch.save({ \'epoch\': epoch, \'model_state_dict\': model.state_dict(), }, path) def load_model(model, path=\\"checkpoint.pth\\"): if os.path.exists(path): checkpoint = torch.load(path) model.load_state_dict(checkpoint[\'model_state_dict\']) return checkpoint[\'epoch\'] + 1 # Resume from next epoch return 0 # Start from scratch def train(rank, world_size, config): initialize_distributed_process_group(rank, world_size) torch.cuda.set_device(rank) model = SimpleModel().to(rank) ddp_model = DDP(model, device_ids=[rank]) dataset = load_data() sampler = DistributedSampler(dataset, num_replicas=world_size, rank=rank) data_loader = DataLoader(dataset, batch_size=64, sampler=sampler) optimizer = optim.Adam(ddp_model.parameters(), lr=config[\'learning_rate\']) criterion = nn.MSELoss() start_epoch = load_model(ddp_model) # Load previous state if exists for epoch in range(start_epoch, config[\'num_epochs\']): ddp_model.train() for batch in data_loader: inputs, targets = batch inputs, targets = inputs.to(rank), targets.to(rank) optimizer.zero_grad() outputs = ddp_model(inputs) loss = criterion(outputs, targets) loss.backward() optimizer.step() if epoch % config[\'sync_interval\'] == 0: if rank == 0: # Only save from process with rank 0 to avoid overwriting issues save_model(ddp_model, epoch) dist.destroy_process_group() if __name__ == \\"__main__\\": config = { \\"num_nodes\\": 4, \\"num_epochs\\": 10, \\"learning_rate\\": 0.001, \\"sync_interval\\": 2 } world_size = config[\'num_nodes\'] mp.spawn(train, args=(world_size, config), nprocs=world_size, join=True)"},{"question":"Objective Your task is to implement a PyTorch function that demonstrates understanding and application of Core Aten IR and Prims IR. You will create a function that applies a series of operations to a given tensor using these intermediate representations. Problem Description Write a function `apply_operations(tensor, operations)` that takes a PyTorch tensor and a list of operations. The function should apply these operations sequentially using Core Aten IR or Prims IR, as indicated in the operations list. Function Signature ```python import torch def apply_operations(tensor: torch.Tensor, operations: list) -> torch.Tensor: \'\'\' Apply a series of operations to the input tensor using Core Aten IR or Prims IR. Parameters: tensor (torch.Tensor): The input tensor to be transformed. operations (list): A list of tuples where each tuple contains: - The name of the IR (\'aten\' for Core Aten IR, \'prims\' for Prims IR) - The operation name (string) - A tuple of parameters required by the operation (can be empty) Returns: torch.Tensor: The transformed tensor after all operations are applied. \'\'\' pass ``` Example Input and Output ```python # Example tensor t = torch.tensor([1.0, 2.0, 3.0]) # List of operations: (IR name, operation name, parameters) operations = [ (\'aten\', \'add\', (torch.tensor(1.0),)), (\'prims\', \'convert_element_type\', (torch.int32,)) ] # Applying operations result = apply_operations(t, operations) # Expected result tensor expected_result = torch.tensor([2, 3, 4], dtype=torch.int32) ``` Constraints and Requirements 1. Only valid operations from Core Aten IR and Prims IR should be used. 2. Ensure that the function handles type promotions and broadcasting appropriately when using Prims IR. 3. Validate the operations to ensure that they belong to the specified IR opset. 4. Handle any exceptions that might occur due to invalid operations or parameters. Assumptions - You can assume the operations listed in the CSV tables for Core Aten IR and Prims IR are available. - The operations list provided will be well-formed and follow the specified structure. This question is designed to test your understanding of PyTorch\'s intermediate representations and your ability to manipulate tensors using low-level IR operations.","solution":"import torch def apply_operations(tensor: torch.Tensor, operations: list) -> torch.Tensor: \'\'\' Apply a series of operations to the input tensor using Core Aten IR or Prims IR. Parameters: tensor (torch.Tensor): The input tensor to be transformed. operations (list): A list of tuples where each tuple contains: - The name of the IR (\'aten\' for Core Aten IR, \'prims\' for Prims IR) - The operation name (string) - A tuple of parameters required by the operation (can be empty) Returns: torch.Tensor: The transformed tensor after all operations are applied. \'\'\' for ir, op_name, params in operations: if ir == \'aten\': if hasattr(torch, op_name): func = getattr(torch, op_name) tensor = func(tensor, *params) else: raise ValueError(f\\"Invalid operation name \'{op_name}\' for Core Aten IR.\\") elif ir == \'prims\': if op_name == \'convert_element_type\': tensor = tensor.to(*params) else: raise ValueError(f\\"Invalid operation name \'{op_name}\' for Prims IR.\\") else: raise ValueError(f\\"Invalid IR name \'{ir}\' provided.\\") return tensor"},{"question":"# Functional Programming Challenge Objective Implement a function that processes a list of numbers by grouping them, applying some transforms, and combining the results into a single output value using the `itertools`, `functools`, and `operator` modules. Problem Statement You are given a list of integers. Your task is to implement a function `process_numbers` that performs the following steps: 1. **Grouping**: Group the numbers into chunks of a specific size. 2. **Transforming**: Apply a transformation to each group. 3. **Combining**: Combine the transformed groups into a single result using a specified operation. Function Signature ```python def process_numbers(numbers: List[int], group_size: int) -> int: pass ``` Parameters - `numbers` (List[int]): A list of integers to be processed. - `group_size` (int): The size of chunks to group the numbers into. Returns - `int`: A single integer that is the result of combining all transformed groups. Requirements 1. **Grouping**: - Use `itertools.islice` to create groups of `group_size` from the input list. 2. **Transforming**: - For each group, compute the product of its elements using `functools.reduce` and `operator.mul`. 3. **Combining**: - Sum the products of all groups using `functools.reduce` and `operator.add`. Example ```python # Example input numbers = [1, 2, 3, 4, 5, 6, 7, 8, 9] group_size = 3 # Grouped into chunks of 3: [[1, 2, 3], [4, 5, 6], [7, 8, 9]] # Transform (product of each group): [6, 120, 504] # Combine (sum of the products): 6 + 120 + 504 = 630 assert process_numbers(numbers, group_size) == 630 ``` Constraints - The `numbers` list will always contain at least one element. - The `group_size` will be a positive integer. - If the last group has fewer than `group_size` elements, consider only the elements present. Performance Aim to make your solution efficient with a focus on utilizing the functions from `itertools`, `functools`, and `operator` effectively.","solution":"from typing import List import itertools import functools import operator def process_numbers(numbers: List[int], group_size: int) -> int: def grouper(iterable, n): Collect data into fixed-length chunks or blocks. args = [iter(iterable)] * n return itertools.zip_longest(*args, fillvalue=None) # Grouping using itertools.islice and custom grouper function grouped_numbers = (list(itertools.islice(it, group_size)) for it in grouper(numbers, group_size)) # Transforming each group by computing the product of its elements transformed_groups = (functools.reduce(operator.mul, filter(None, group)) for group in grouped_numbers) # Combining the transformed groups by summing their products result = functools.reduce(operator.add, transformed_groups, 0) return result"},{"question":"The `tabnanny` module is used to detect ambiguous indentation in Python files. It includes functions to check files and directories for such issues and provides diagnostic messages. Your task is to implement a simplified version of a tool similar to `tabnanny` that checks Python files for specific whitespace-related issues. Specifically, you need to: 1. Parse the tokens in a Python file using the `tokenize` module. 2. Check for lines with mixed indentation (tabs and spaces) or inconsistent indentation levels. 3. Print the filenames and lines where problems are detected. Requirements 1. Implement the function `check_indentation(file_path: str) -> None`: - **Input:** `file_path` - The path to the Python (.py) file to be checked. - **Output:** Print the filename and the line number for each detected issue. Example of what the output might look like: ``` File: example.py, Line: 10 - Mixed indentation File: example.py, Line: 15 - Inconsistent indentation level ``` 2. Implement the function `check_directory(dir_path: str) -> None`: - **Input:** `dir_path` - The path to the directory containing Python files. - **Output:** Recursively check each `.py` file in the directory using `check_indentation` and print the issues as described above. Constraints 1. Only `.py` files should be checked. 2. You cannot use any third-party libraries other than standard Python libraries. 3. Focus on comprehensibility and clarity of your code. Example Usage ```python check_indentation(\'sample.py\') # Output # File: sample.py, Line: 5 - Mixed indentation # File: sample.py, Line: 9 - Inconsistent indentation level check_directory(\'/path/to/python/files/\') # Output # File: /path/to/python/files/module1.py, Line: 3 - Mixed indentation # File: /path/to/python/files/module2.py, Line: 20 - Inconsistent indentation level ``` **Note:** This is a challenging question designed to assess your understanding of file handling, tokens, and parsing in Python. Ensure your code is efficient and handles edge cases appropriately.","solution":"import tokenize import os def check_indentation(file_path): Checks the file for mixed or inconsistent indentation and prints issues found. with open(file_path, \'r\') as f: tokens = list(tokenize.generate_tokens(f.readline)) last_indent = None spaces = 0 tabs = 0 for token in tokens: if token.type == tokenize.INDENT: indent_string = token.string spaces = indent_string.count(\' \') tabs = indent_string.count(\'t\') if spaces > 0 and tabs > 0: print(f\\"File: {file_path}, Line: {token.start[0]} - Mixed indentation\\") if last_indent is None: last_indent = indent_string if indent_string != last_indent: print(f\\"File: {file_path}, Line: {token.start[0]} - Inconsistent indentation level\\") last_indent = indent_string def check_directory(dir_path): Recursively checks all .py files in the directory for mixed or inconsistent indentation. for root, _, files in os.walk(dir_path): for file in files: if file.endswith(\'.py\'): check_indentation(os.path.join(root, file))"},{"question":"**Coding Assessment Question** # Objective: To assess students\' understanding of the `faulthandler` module and their ability to implement a function that makes appropriate use of this module\'s capabilities in a simulated real-world scenario. # Problem Statement: You are tasked with implementing a diagnostic tool that uses Python\'s `faulthandler` module to monitor a potentially unstable function. The tool should log tracebacks if the function crashes, times out, or receives a specific signal. # Task: Write a diagnostic function named `monitor_function` that accepts another function `target_func` (to be monitored) and its arguments. The `monitor_function` should monitor `target_func` for faults, timeouts, and specified signals, and log any tracebacks to a specified file. # Function Signature: ```python def monitor_function(target_func: callable, args: tuple, kwargs: dict, timeout: int, signal_num: int, log_file: str) -> None: pass ``` # Inputs: - `target_func`: The function to be monitored. - `args`: A tuple of positional arguments to pass to `target_func`. - `kwargs`: A dictionary of keyword arguments to pass to `target_func`. - `timeout`: An integer specifying the timeout in seconds for `target_func` execution. - `signal_num`: An integer specifying the signal number that should trigger a traceback. - `log_file`: A string specifying the file path where tracebacks should be logged. # Outputs: - The function should not return any value. All tracebacks must be written to the specified log file. # Requirements: 1. Use `faulthandler.enable()` to enable fault handling and direct outputs to `log_file`. 2. Use `faulthandler.dump_traceback_later()` to schedule a traceback dump after `timeout` seconds. 3. Use `faulthandler.register()` to register `signal_num` to trigger a traceback. 4. Ensure the fault handler is disabled and any scheduled dumps are cancelled after `target_func` completes or crashes. 5. Log any errors or exceptions that occur into the `log_file`. # Constraints: - The `target_func` may deliberately crash, hang or raise an exception to test your implementation. - Ensure that the log file is opened and closed correctly within your function to avoid resource leaks. # Example: ```python import os import signal import faulthandler # Example target function def target_func(x, y): return x / y # Potential division by zero error # Monitor the target function monitor_function(target_func, (10, 0), {}, 5, signal.SIGUSR1, \\"traceback.log\\") ``` # Notes: - The `signal.SIGUSR1` is commonly used as a user-defined signal in UNIX-like systems. - Ensure proper error handling and resource management when implementing the function.","solution":"import faulthandler import signal import os def monitor_function(target_func: callable, args: tuple, kwargs: dict, timeout: int, signal_num: int, log_file: str) -> None: try: # Open the log file for traceback outputs with open(log_file, \'w\') as f: # Enable faulthandler and redirect output to the log file faulthandler.enable(file=f) # Schedule a traceback dump after timeout seconds faulthandler.dump_traceback_later(timeout, file=f) # Register the specified signal to trigger a traceback faulthandler.register(signal_num, file=f) # Execute the target function target_func(*args, **kwargs) except Exception as e: # Log any exceptions that occur during the function execution with open(log_file, \'a\') as f: # Open in append mode f.write(f\\"nException: {e}\\") finally: # Disable faulthandler and cancel any scheduled dumps faulthandler.cancel_dump_traceback_later() faulthandler.disable()"},{"question":"# Contextual Data Management in Concurrent Tasks As an exercise, you will implement a function `manage_context_data` to demonstrate the use of the contextvars module for handling contextual data within asynchronous tasks. This function will be composed of a nested structure of asynchronous tasks where different context variables are accessed and managed. Function Signature: ```python def manage_context_data(): pass ``` Requirements: 1. **Create Context Variables:** - Use `contextvars.ContextVar` to create two context variables called `var1` and `var2`, with default values set to `0` and `None` respectively. 2. **Define Asynchronous Tasks:** - Define an asynchronous task `task1` that: - Sets `var1` and `var2` to given values. - Retrieves and prints the values of `var1` and `var2` within its context. - Define an asynchronous task `task2` that: - Sets `var1` and retrieves and prints its value in its own context. - Finally, resets `var1` to its previous state. 3. **Manage Contexts:** - Use `contextvars.copy_context` to create separate contexts for `task1` and `task2`. - Schedule `task1` and `task2` such that `task2` runs nested within `task1`. 4. **Retrieve and Print Values:** - After running the tasks, retrieve and print the final values of `var1` and `var2`. Input: - None. Output: - The implementation should print the values of `var1` and `var2` at different stages: - Inside `task1` before calling `task2`. - Inside `task2`. - Back inside `task1` after `task2` has finished. - The final values in the main context. Constraints: - Use proper exception handling where necessary. - Explicitly manage context changes and resets to ensure data integrity across asynchronous task boundaries. Performance Considerations: - Ensure that context switching and variable manipulation does not degrade performance for a reasonable number of tasks and context changes. # Example Output: ```python Values inside task1: var1=10, var2=20 Values inside task2: var1=30 Back to task1 values: var1=10, var2=20 Final values: var1=0, var2=None ```","solution":"import asyncio import contextvars var1 = contextvars.ContextVar(\'var1\', default=0) var2 = contextvars.ContextVar(\'var2\', default=None) async def task1(): Task 1 sets the context variables var1 and var2 and runs task2 within its context. token1 = var1.set(10) var2.set(20) print(f\\"Values inside task1: var1={var1.get()}, var2={var2.get()}\\") # Running task2 in task1\'s context await task2() # Restore var1 state var1.reset(token1) print(f\\"Back to task1 values: var1={var1.get()}, var2={var2.get()}\\") async def task2(): Task 2 sets var1 to a new value and prints it. token = var1.set(30) print(f\\"Values inside task2: var1={var1.get()}\\") # Restore var1 state var1.reset(token) async def manage_context_data(): Manages the execution of tasks and handles context switching. await task1() print(f\\"Final values: var1={var1.get()}, var2={var2.get()}\\") # To actually run the manage_context_data function. if __name__ == \\"__main__\\": asyncio.run(manage_context_data())"},{"question":"Objective: Demonstrate your comprehension of the `ipaddress` module by implementing a function to analyze a list of IP addresses and networks and provide detailed address information. Task: Implement a function named `analyze_ip_data` that takes in a list of strings, each string being either an IP address (IPv4 or IPv6) or a network address with a prefix. The function should: 1. Validate each string and determine whether it is an IP address or network address. 2. For each IP address: - Output the version (4 or 6). - If it is an IPv4 address, check if it belongs to any of the provided networks and list those networks. 3. For each network: - Output the network mask and the total number of addresses in the network. Input: - `ip_data`: List of strings, each being either an IP address (IPv4 or IPv6) or network address with a prefix. Output: - A dictionary with two keys: - `\\"ip_addresses\\"`: A dictionary where each key is an IP address and the value is another dictionary with keys: - `\\"version\\"`: The IP version (4 or 6). - `\\"networks\\"`: A list of networks (as strings) the IP address belongs to. - `\\"networks\\"`: A dictionary where each key is a network address and the value is another dictionary with keys: - `\\"netmask\\"`: The netmask of the network. - `\\"num_addresses\\"`: The total number of addresses in the network. Constraints: - Handle invalid IP or network addresses gracefully by ignoring them and not including them in the output. Example: ```python def analyze_ip_data(ip_data): # Your implementation here # Example usage: ip_data = [ \\"192.168.1.1\\", \\"2001:db8::1\\", \\"192.168.1.0/24\\", \\"192.0.2.1/24\\", \\"300.400.500.600\\", # Invalid \\"2001:db8::/64\\" ] result = analyze_ip_data(ip_data) print(result) ``` # Expected Output: ```python { \'ip_addresses\': { \'192.168.1.1\': {\'version\': 4, \'networks\': [\'192.168.1.0/24\']}, \'2001:db8::1\': {\'version\': 6, \'networks\': []} }, \'network\': { \'192.168.1.0/24\': {\'netmask\': \'255.255.255.0\', \'num_addresses\': 256}, \'192.0.2.1/24\': {\'netmask\': \'255.255.255.0\', \'num_addresses\': 256}, \'2001:db8::/64\': {\'netmask\': \'ffff:ffff:ffff:ffff::\', \'num_addresses\': 18446744073709551616} } } ``` Notes: - The function should use the capabilities of the `ipaddress` module to validate and analyze the IP and network addresses. - For each IP address, only include the networks provided in the `ip_data` list for network membership checking.","solution":"import ipaddress def analyze_ip_data(ip_data): result = { \\"ip_addresses\\": {}, \\"networks\\": {} } networks = [] for item in ip_data: try: ip = ipaddress.ip_address(item) result[\\"ip_addresses\\"][str(ip)] = { \\"version\\": ip.version, \\"networks\\": [] } except ValueError: try: net = ipaddress.ip_network(item, strict=False) networks.append(net) result[\\"networks\\"][str(net)] = { \\"netmask\\": str(net.netmask), \\"num_addresses\\": net.num_addresses } except ValueError: continue for ip_str in list(result[\\"ip_addresses\\"].keys()): ip = ipaddress.ip_address(ip_str) for net in networks: if ip in net: result[\\"ip_addresses\\"][ip_str][\\"networks\\"].append(str(net)) return result"},{"question":"**Question: Comprehensive Data Manipulation and Analysis using Pandas** You are provided with a dataset in CSV format containing sales data for a retail store. The dataset includes the following columns: - `date`: The date of the sale. - `store_id`: The ID of the store where the sale occurred. - `product_id`: The ID of the product sold. - `quantity`: The number of units sold. - `price_per_unit`: The price per unit of the product. Your task is to perform a series of data manipulation and analysis tasks using pandas. The final goal is to generate a report that summarizes the data as described below. # Instructions: 1. **Data Loading and Initial Processing**: - Read the dataset from a CSV file into a pandas DataFrame. - Ensure that the `date` column is parsed as a date type. 2. **Data Cleaning**: - Check for and handle any missing or null values appropriately. 3. **Data Analysis**: - Calculate the total sales for each product in each store. (Hint: Total sales for a product can be calculated as `quantity * price_per_unit`) - Find the top 5 products with the highest total sales across all stores. - For each store, determine the product with the highest total sales. 4. **Time Series Analysis**: - Resample the data to compute the monthly total sales for each store. - Identify the month with the highest total sales for each store. 5. **Data Transformation and Presentation**: - Pivot the data to create a table with stores as rows, products as columns, and total sales as values. Fill missing values with zero. - Sort this pivot table by total sales in descending order for each store. - Generate a summary table that shows the total sales for each store and each product category. # Input: - Path to the CSV file containing the dataset. # Output: - The resulting DataFrame or DataFrames after performing each task. # Constraints: - You should use pandas methods extensively for data manipulation and analysis. - Pay attention to the efficiency of your solution, ensuring that operations are performed in an optimized manner. # Code Requirements: Implement the following function: ```python import pandas as pd def analyze_sales_data(file_path: str) -> pd.DataFrame: # Perform all tasks here and return the final summarized DataFrame pass ``` # Example Usage: ```python # Assuming the CSV file is located at \'data/sales_data.csv\' summary_df = analyze_sales_data(\'data/sales_data.csv\') print(summary_df) ``` Good luck, and ensure that your solution handles various edge cases appropriately!","solution":"import pandas as pd def analyze_sales_data(file_path: str) -> pd.DataFrame: # Data Loading and Initial Processing df = pd.read_csv(file_path, parse_dates=[\'date\']) # Data Cleaning df.dropna(inplace=True) # Data Analysis df[\'total_sales\'] = df[\'quantity\'] * df[\'price_per_unit\'] total_sales_per_product_per_store = df.groupby([\'store_id\', \'product_id\'])[\'total_sales\'].sum().reset_index() top_5_products = df.groupby(\'product_id\')[\'total_sales\'].sum().nlargest(5).reset_index() highest_sales_product_per_store = df.groupby(\'store_id\').apply(lambda x: x.groupby(\'product_id\')[\'total_sales\'].sum().idxmax()).reset_index(name=\'top_product_id\') # Time Series Analysis df.set_index(\'date\', inplace=True) monthly_sales_per_store = df.groupby(\'store_id\').resample(\'M\')[\'total_sales\'].sum().reset_index() highest_sales_month_per_store = monthly_sales_per_store.loc[monthly_sales_per_store.groupby(\'store_id\')[\'total_sales\'].idxmax()].reset_index(drop=True) # Data Transformation and Presentation pivot_table = df.pivot_table(index=\'store_id\', columns=\'product_id\', values=\'total_sales\', fill_value=0, aggfunc=\'sum\') summary_table = pivot_table.join(highest_sales_product_per_store.set_index(\'store_id\')).join(highest_sales_month_per_store.set_index(\'store_id\')[[\'date\', \'total_sales\']].rename(columns={\'date\': \'best_month\', \'total_sales\': \'best_month_sales\'})) return summary_table"},{"question":"# Coding Assessment: Advanced Use of Seaborn\'s Pointplot Objective Design and implement a function using Seaborn that visualizes data from a dataset, demonstrating various capabilities of the `sns.pointplot` function. Dataset You will use the `penguins` dataset which can be loaded using the following command: ```python import seaborn as sns penguins = sns.load_dataset(\\"penguins\\") ``` Function Signature ```python def plot_penguin_data(data: \\"DataFrame\\") -> None: Plots a pointplot using the seaborn library on the given penguins dataset. Parameters: data (DataFrame): A pandas DataFrame containing the penguins dataset. Returns: None: The function should display the plot directly. ``` Requirements 1. **Basic Plot**: Create a basic point plot using `sns.pointplot` to visualize the average body mass (`body_mass_g`) of penguins on different islands (`island`). - X-axis: `island` - Y-axis: `body_mass_g` 2. **Grouped Plot**: Add a secondary grouping by the penguin species (`species`) and differentiate the groups using different colors. - Hue: `species` - Customize the plot with different markers and linestyles for each species group. 3. **Error Bars**: Use error bars to represent the standard deviation of `body_mass_g` values. - Error bars should be capped for better visibility. 4. **Customization**: Customize the plot to improve readability: - Change the color palette to `Set2`. - Rotate the x-axis labels by 45 degrees for better readability. - Add a title to the plot. 5. **Save the Plot**: Save the final plot as a PNG file called `penguin_pointplot.png`. Constraints - Assume the `penguins` dataset will always be available and correctly formatted. Example ```python import seaborn as sns penguins = sns.load_dataset(\\"penguins\\") plot_penguin_data(penguins) ``` The function should display a plot and save it as `penguin_pointplot.png`.","solution":"import seaborn as sns import matplotlib.pyplot as plt import pandas as pd def plot_penguin_data(data: pd.DataFrame) -> None: Plots a pointplot using the seaborn library on the given penguins dataset. Parameters: data (DataFrame): A pandas DataFrame containing the penguins dataset. Returns: None: The function should display the plot directly and save it as a PNG file. plt.figure(figsize=(10, 6)) pointplot = sns.pointplot( x=\'island\', y=\'body_mass_g\', hue=\'species\', data=data, markers=[\'o\', \'s\', \'D\'], linestyles=[\'-\', \'--\', \'-.\'], palette=\'Set2\', ci=\'sd\', capsize=0.1 ) pointplot.set_title(\\"Penguin Body Mass by Island and Species\\") plt.xticks(rotation=45) plt.savefig(\\"penguin_pointplot.png\\") plt.show()"},{"question":"You are given a dataset containing historical data of sales across multiple stores and different years. The data is structured in a MultiIndex DataFrame and stored in a variable called `sales_data`. Your task is to implement a function called `filter_sales_data` that performs the following operations: 1. **Filter Rows**: Filter the data such that it includes only sales data for the years 2019 and 2020. 2. **Select Specific Stores**: From the filtered data, select sales records for stores \'Store_A\' and \'Store_B\' only. 3. **Exceeding Threshold**: Filter out records where the sales in \'Product_1\' exceeded a threshold value which is passed as an argument to the function. 4. **Aggregate by Quarter**: Aggregate the sales data quarterly for each store and each product and return the resulting DataFrame. Ensure that the final DataFrame only contains the rows and columns relevant to these filtering and aggregation operations. # Function Signature: ```python def filter_sales_data(sales_data: pd.DataFrame, threshold: float) -> pd.DataFrame: pass ``` # Input - `sales_data`: A MultiIndex pandas DataFrame with levels `[\'Year\', \'Store\']` and columns `[\'Product_1\', \'Product_2\', \'Product_3\']` representing sales data. - `threshold`: A float value representing the sales threshold for `Product_1`. # Output - A pandas DataFrame containing the quarterly aggregated sales data filtered according to the specified criteria. # Example Given the following input: ```python import pandas as pd import numpy as np index = pd.MultiIndex.from_product([[\'2019\', \'2020\', \'2021\'], [\'Store_A\', \'Store_B\', \'Store_C\']], names=[\'Year\', \'Store\']) columns = [\'Product_1\', \'Product_2\', \'Product_3\'] data = np.random.randint(1, 100, (9, 3)) sales_data = pd.DataFrame(data, index=index, columns=columns) # Suppose we set a threshold of 50 threshold = 50 result = filter_sales_data(sales_data, threshold) print(result) ``` Your function should return a DataFrame that meets the above criteria with rows and columns only for relevant data after performing filtering and aggregation. # Constraints: - You must use pandas functionalities for all indexing and selection operations. - Handle any missing data appropriately. - Ensure the function operates efficiently for DataFrames up to size `(1000, 3)`. **Ensure** to handle edge cases such as no results after filtering or data not having expected columns.","solution":"import pandas as pd def filter_sales_data(sales_data: pd.DataFrame, threshold: float) -> pd.DataFrame: # Step 1: Filter the data for the years 2019 and 2020 filtered_data = sales_data.loc[[\'2019\', \'2020\']] # Step 2: Select records for \'Store_A\' and \'Store_B\' filtered_data = filtered_data.loc[(slice(None), [\'Store_A\', \'Store_B\']), :] # Step 3: Filter out records where \'Product_1\' exceeded the threshold filtered_data = filtered_data[filtered_data[\'Product_1\'] <= threshold] # Step 4: Aggregate the data quarterly for each store and each product filtered_data = filtered_data.reset_index() filtered_data[\'Quarter\'] = pd.to_datetime(filtered_data[\'Year\'], format=\'%Y\').dt.to_period(\'Q\') aggregated_data = filtered_data.groupby([\'Store\', \'Quarter\']).sum().reset_index() # Drop the \'Year\' column as it is not needed anymore aggregated_data = aggregated_data.drop(columns=[\'Year\']) return aggregated_data.set_index([\'Store\', \'Quarter\'])"},{"question":"Objective: You are tasked to clean a given dataset by identifying and resolving duplicate labels within a `pandas` DataFrame. You should also ensure that no duplicate labels can be introduced after the initial cleaning. Requirements: 1. Implement a function `clean_dataframe(df: pd.DataFrame) -> pd.DataFrame` that performs the following: - Detects any duplicate row or column labels in the input DataFrame `df`. - Removes duplicates by taking the mean of all rows/columns with the same label. - Sets the DataFrame to disallow duplicate labels going forward. 2. The function should return the cleaned DataFrame with the duplicates resolved and the settings applied to disallow future duplicates. Input: - `df`: A `pandas` DataFrame which may contain duplicate row or column labels. Output: - A cleaned `DataFrame` with duplicates resolved and set to disallow introduction of duplicate labels. Constraints: - You should use the methods described in the provided documentation to implement this function. - The solution should handle large DataFrames efficiently. Example: ```python import pandas as pd df = pd.DataFrame({\'A\': [1, 2, 3, 4], \'B\': [5, 6, 7, 8]}, index=[\'a\', \'b\', \'a\', \'b\']) df # Result: # A B # a 1 5 # b 2 6 # a 3 7 # b 4 8 cleaned_df = clean_dataframe(df) cleaned_df # Expected result: # A B # a 2.0 6.0 # b 3.0 7.0 cleaned_df.flags.allows_duplicate_labels # Expected result: False ``` Note: - You should ensure that the setting to disallow duplicate labels propagates through any subsequent DataFrame operations.","solution":"import pandas as pd def clean_dataframe(df: pd.DataFrame) -> pd.DataFrame: Cleans the DataFrame by removing duplicate row and column labels. Duplicate rows/columns are averaged, and future duplicates are disallowed. Params: df: pd.DataFrame - The input DataFrame that may contain duplicate labels. Returns: pd.DataFrame - The cleaned DataFrame with duplicate labels resolved. # Handling duplicate rows if df.index.duplicated().any(): df = df.groupby(df.index).mean() # Handling duplicate columns if df.columns.duplicated().any(): df = df.T.groupby(df.columns).mean().T # Disallow any future duplicate labels df.flags.allows_duplicate_labels = False return df"},{"question":"**Assessment Question: File Path Manipulations with `os.path` Module** # Objective Write a Python function `process_paths` that takes a list of file paths and performs several manipulations using functions from the `os.path` module. The function should demonstrate a comprehensive understanding of various `os.path` functionalities. # Requirements 1. **Input:** A list of file paths (strings) and a base path (string). 2. **Output:** A dictionary with the following keys and corresponding values: - `absolute_paths`: a list of absolute paths of the input file paths. - `base_names`: a list of base names of the input file paths. - `dir_names`: a list of directory names of the input file paths. - `common_path`: the common path prefix of the input file paths. - `relative_paths`: a list of paths relative to the given base path. - `sizes`: a list of sizes (in bytes) of the files if they exist, otherwise `None`. # Constraints - Use functions from the `os.path` module wherever applicable. - Assume all paths are valid strings. - If a path does not exist or the file size cannot be determined, set the size as `None`. - The base path is always provided and is a valid directory. # Function Signature ```python import os def process_paths(paths: list, base_path: str) -> dict: pass ``` # Example ```python paths = [ \\"/usr/local/bin/python\\", \\"/home/user/.bashrc\\", \\"/etc/nginx/nginx.conf\\" ] base_path = \\"/home/user\\" result = process_paths(paths, base_path) ``` Expected result (the values may differ based on the environment): ```python { \'absolute_paths\': [\'/usr/local/bin/python\', \'/home/user/.bashrc\', \'/etc/nginx/nginx.conf\'], \'base_names\': [\'python\', \'.bashrc\', \'nginx.conf\'], \'dir_names\': [\'/usr/local/bin\', \'/home/user\', \'/etc/nginx\'], \'common_path\': \'/\', \'relative_paths\': [\'../../usr/local/bin/python\', \'.bashrc\', \'../../etc/nginx/nginx.conf\'], \'sizes\': [some_number_in_bytes, some_number_in_bytes, some_number_in_bytes] } ``` # Notes - Use `os.path.abspath`, `os.path.basename`, `os.path.dirname`, `os.path.commonpath`, `os.path.relpath`, and `os.path.getsize` functions. - Ensure the function handles all edge cases and performs efficiently. Good luck!","solution":"import os def process_paths(paths: list, base_path: str) -> dict: result = { \'absolute_paths\': [], \'base_names\': [], \'dir_names\': [], \'common_path\': \'\', \'relative_paths\': [], \'sizes\': [] } # Compute the absolute paths result[\'absolute_paths\'] = [os.path.abspath(path) for path in paths] # Compute the base names result[\'base_names\'] = [os.path.basename(path) for path in paths] # Compute the directory names result[\'dir_names\'] = [os.path.dirname(path) for path in paths] # Compute the common path result[\'common_path\'] = os.path.commonpath(paths) # Compute the relative paths to the base_path result[\'relative_paths\'] = [os.path.relpath(path, start=base_path) for path in paths] # Compute the file sizes for path in paths: try: size = os.path.getsize(path) except OSError: # Includes os.path.exists if the file does not exist size = None result[\'sizes\'].append(size) return result"},{"question":"**Title: Implement and Utilize Numeric Comparison Functions in PyTorch** **Objective:** Your task is to implement a PyTorch module that utilizes numeric comparison functions to evaluate differences between two given tensors based on Signal to Quantization Noise Ratio (SQNR), Normalized L2 Error, and Cosine Similarity. This will test your understanding of tensor operations and the application of utility functions in PyTorch. **Requirements:** 1. Implement a class `TensorComparator` that includes methods to compute SQNR, normalized L2 error, and cosine similarity. 2. Each method should use the corresponding utility functions from `torch.ao.ns.fx.utils`. 3. Implement an additional method that aggregates these metrics for a comprehensive comparison. **Class Specification:** ```python import torch from torch.ao.ns.fx.utils import compute_sqnr, compute_normalized_l2_error, compute_cosine_similarity class TensorComparator: def __init__(self, tensor1, tensor2): Initialize the TensorComparator with two tensors. :param tensor1: First tensor to be compared. :param tensor2: Second tensor to be compared. self.tensor1 = tensor1 self.tensor2 = tensor2 def compute_sqnr(self): Computes the Signal to Quantization Noise Ratio (SQNR) between tensor1 and tensor2. :return: SQNR value. return compute_sqnr(self.tensor1, self.tensor2) def compute_normalized_l2_error(self): Computes the normalized L2 error between tensor1 and tensor2. :return: Normalized L2 error value. return compute_normalized_l2_error(self.tensor1, self.tensor2) def compute_cosine_similarity(self): Computes the cosine similarity between tensor1 and tensor2. :return: Cosine similarity value. return compute_cosine_similarity(self.tensor1, self.tensor2) def aggregate_metrics(self): Calculates all comparison metrics and returns them in a dictionary. :return: Dictionary with SQNR, normalized L2 error, and cosine similarity. return { \'sqnr\': self.compute_sqnr(), \'normalized_l2_error\': self.compute_normalized_l2_error(), \'cosine_similarity\': self.compute_cosine_similarity() } # Example usage: # tensor1 = torch.randn((3, 3)) # tensor2 = torch.randn((3, 3)) # comparator = TensorComparator(tensor1, tensor2) # print(comparator.aggregate_metrics()) ``` **Constraints:** - Ensure the input tensors are of the same shape. - Use appropriate error handling for cases where the tensors do not meet the requirements. **Evaluation:** Your solution will be evaluated on the correctness of the implementation, the efficient use of PyTorch functions, and code readability. You are not allowed to change the signatures or the initial code provided.","solution":"import torch from torch.ao.ns.fx.utils import compute_sqnr, compute_normalized_l2_error, compute_cosine_similarity class TensorComparator: def __init__(self, tensor1, tensor2): Initialize the TensorComparator with two tensors. :param tensor1: First tensor to be compared. :param tensor2: Second tensor to be compared. if tensor1.shape != tensor2.shape: raise ValueError(\\"Input tensors must have the same shape\\") self.tensor1 = tensor1 self.tensor2 = tensor2 def compute_sqnr(self): Computes the Signal to Quantization Noise Ratio (SQNR) between tensor1 and tensor2. :return: SQNR value. return compute_sqnr(self.tensor1, self.tensor2) def compute_normalized_l2_error(self): Computes the normalized L2 error between tensor1 and tensor2. :return: Normalized L2 error value. return compute_normalized_l2_error(self.tensor1, self.tensor2) def compute_cosine_similarity(self): Computes the cosine similarity between tensor1 and tensor2. :return: Cosine similarity value. return compute_cosine_similarity(self.tensor1, self.tensor2) def aggregate_metrics(self): Calculates all comparison metrics and returns them in a dictionary. :return: Dictionary with SQNR, normalized L2 error, and cosine similarity. return { \'sqnr\': self.compute_sqnr(), \'normalized_l2_error\': self.compute_normalized_l2_error(), \'cosine_similarity\': self.compute_cosine_similarity() }"},{"question":"# Advanced Coding Assessment: Queue Management System **Objective:** Implement a multi-threaded task management system utilizing Python\'s `queue` module. Your solution should demonstrate proficiency in using different types of queues (FIFO, LIFO, and Priority Queues) and managing thread synchronization. # Problem Statement: You are tasked with creating a task processing system for a fictional company. The system should consist of different types of queues for managing tasks with varying requirements. Specifically, you need to implement the following: 1. **Task Class**: - Represents a task with a `task_id`, `description`, and `priority`. - Provides a string representation of the task. 2. **TaskManager Class**: - Manages multiple threads for processing tasks. - Uses different types of queues (`Queue`, `LifoQueue`, `PriorityQueue`) to handle tasks. - Enqueues and processes tasks based on the type of queue. - Ensures that all enqueued tasks are processed before the program exits. # Function Specifications: 1. **Task Class**: - `__init__(self, task_id: int, description: str, priority: int):` Constructor to initialize a task. - `__str__(self) -> str:` Returns the string representation of the task, formatted as `\\"[Task ID] Description (Priority: X)\\"`. 2. **TaskManager Class**: - `__init__(self, num_workers: int):` Constructor to initialize the task manager with a specified number of worker threads. - `add_task(self, task: Task, queue_type: str):` Adds a task to the specified queue type (`\'FIFO\'`, `\'LIFO\'`, `\'PRIORITY\'`). - `process_tasks(self):` Starts worker threads and processes tasks from all queues until they are empty. - `worker(self, task_queue)`: Worker function for processing tasks which will be run by each thread. # Input: - `Task` objects with attributes `task_id`, `description`, and `priority`. - The number of worker threads for the `TaskManager`. - Operations to add tasks and specify the queue type. # Output: - Print statements indicating that tasks are being processed, in the format: `Processing [Task ID] Description (Priority: X)` - Ensure that when `process_tasks` exits, all tasks have been processed. # Constraints: - The number of tasks will not exceed 1000. - The number of worker threads will not exceed 20. - Task priorities are integers ranging from 1 to 10. # Example: ```python from queue import Queue, LifoQueue, PriorityQueue import threading class Task: def __init__(self, task_id, description, priority): self.task_id = task_id self.description = description self.priority = priority def __str__(self): return f\\"[{self.task_id}] {self.description} (Priority: {self.priority})\\" class TaskManager: def __init__(self, num_workers): self.num_workers = num_workers self.fifo_queue = Queue() self.lifo_queue = LifoQueue() self.priority_queue = PriorityQueue() def add_task(self, task, queue_type): if queue_type == \'FIFO\': self.fifo_queue.put(task) elif queue_type == \'LIFO\': self.lifo_queue.put(task) elif queue_type == \'PRIORITY\': self.priority_queue.put((task.priority, task)) else: raise ValueError(\\"Invalid queue type!\\") def worker(self, task_queue): while True: try: if isinstance(task_queue, PriorityQueue): _, task = task_queue.get_nowait() else: task = task_queue.get_nowait() print(f\\"Processing {task}\\") task_queue.task_done() except queue.Empty: break def process_tasks(self): threads = [] for task_queue in [self.fifo_queue, self.lifo_queue, self.priority_queue]: for _ in range(self.num_workers): thread = threading.Thread(target=self.worker, args=(task_queue,)) thread.start() threads.append(thread) for thread in threads: thread.join() # Example usage tm = TaskManager(num_workers=5) # Adding tasks tm.add_task(Task(1, \\"Task One\\", 5), \'FIFO\') tm.add_task(Task(2, \\"Task Two\\", 3), \'LIFO\') tm.add_task(Task(3, \\"Task Three\\", 1), \'PRIORITY\') tm.add_task(Task(4, \\"Task Four\\", 2), \'PRIORITY\') # Processing tasks tm.process_tasks() ``` # Instructions: - Implement the `Task` and `TaskManager` classes as described above. - Ensure thread safety and proper task synchronization. - Handle exceptions appropriately to ensure smooth execution.","solution":"from queue import Queue, LifoQueue, PriorityQueue import threading class Task: def __init__(self, task_id, description, priority): self.task_id = task_id self.description = description self.priority = priority def __str__(self): return f\\"[{self.task_id}] {self.description} (Priority: {self.priority})\\" class TaskManager: def __init__(self, num_workers): self.num_workers = num_workers self.fifo_queue = Queue() self.lifo_queue = LifoQueue() self.priority_queue = PriorityQueue() self.queues = { \'FIFO\': self.fifo_queue, \'LIFO\': self.lifo_queue, \'PRIORITY\': self.priority_queue } def add_task(self, task, queue_type): if queue_type not in self.queues: raise ValueError(\\"Invalid queue type!\\") if queue_type == \'PRIORITY\': self.priority_queue.put((task.priority, task)) else: self.queues[queue_type].put(task) def worker(self, task_queue): while True: try: if isinstance(task_queue, PriorityQueue): _, task = task_queue.get_nowait() else: task = task_queue.get_nowait() print(f\\"Processing {task}\\") task_queue.task_done() except queue.Empty: break def process_tasks(self): threads = [] for task_queue in [self.fifo_queue, self.lifo_queue, self.priority_queue]: for _ in range(self.num_workers): thread = threading.Thread(target=self.worker, args=(task_queue,)) thread.start() threads.append(thread) for thread in threads: thread.join() # Example usage tm = TaskManager(num_workers=5) # Adding tasks tm.add_task(Task(1, \\"Task One\\", 5), \'FIFO\') tm.add_task(Task(2, \\"Task Two\\", 3), \'LIFO\') tm.add_task(Task(3, \\"Task Three\\", 1), \'PRIORITY\') tm.add_task(Task(4, \\"Task Four\\", 2), \'PRIORITY\') # Processing tasks tm.process_tasks()"},{"question":"# Advanced Python Coding Assessment Context: You are developing a cross-platform application using the `asyncio` module. Understanding the varying capabilities and limitations of event loops on different operating systems is crucial for ensuring your application functions correctly across all platforms. Objective: Implement a function `create_event_loop` that creates and returns an appropriate `asyncio` event loop based on the operating system and its version. The function should take into account the platform-specific limitations and select the correct event loop type or raise appropriate warnings/errors. Specifications: - **Input**: None - **Output**: Returns an `asyncio` event loop instance. - **Constraints**: - On Windows: - Use `ProactorEventLoop` if available (default on Python 3.8 and later). - Otherwise, use `SelectorEventLoop` but ensure the `SelectSelector` does not exceed the socket limit (512 sockets). - On macOS: - For versions <= 10.8, create a `SelectorEventLoop` with `SelectSelector` or `PollSelector`. - For modern versions, use the default event loop. - On other Unix-like systems, use the default event loop. - **Performance Requirement**: Ensure the function runs efficiently with minimal overhead considering the necessary checks for platform and version. Example Code: ```python import asyncio import platform def create_event_loop(): system = platform.system() version = platform.version() if system == \\"Windows\\": if hasattr(asyncio, \'ProactorEventLoop\'): loop = asyncio.ProactorEventLoop() else: selector = selectors.SelectSelector() loop = asyncio.SelectorEventLoop(selector) elif system == \\"Darwin\\": mac_version = float(\'.\'.join(platform.mac_ver()[0].split(\'.\')[:2])) if mac_version <= 10.8: selector = selectors.SelectSelector() loop = asyncio.SelectorEventLoop(selector) else: loop = asyncio.get_event_loop() else: loop = asyncio.get_event_loop() asyncio.set_event_loop(loop) return loop # Sample usage if __name__ == \\"__main__\\": event_loop = create_event_loop() print(event_loop) ``` Notes: - You should include appropriate error handling and logs to manage unexpected scenarios or unsupported methods. - Make sure to test this function on different operating systems to verify its behavior, especially focusing on the constraints mentioned.","solution":"import asyncio import platform import selectors def create_event_loop(): system = platform.system() version = platform.release() if system == \\"Windows\\": if hasattr(asyncio, \'ProactorEventLoop\'): loop = asyncio.ProactorEventLoop() else: selector = selectors.SelectSelector() loop = asyncio.SelectorEventLoop(selector) elif system == \\"Darwin\\": mac_version = float(\'.\'.join(platform.mac_ver()[0].split(\'.\')[:2])) if mac_version <= 10.8: selector = selectors.SelectSelector() loop = asyncio.SelectorEventLoop(selector) else: loop = asyncio.get_event_loop() else: loop = asyncio.get_event_loop() asyncio.set_event_loop(loop) return loop"},{"question":"# Coding Assessment: Transforming Prediction Targets with Scikit-Learn Objective: You will implement functions to transform prediction targets using scikit-learn\'s `LabelBinarizer`, `MultiLabelBinarizer`, and `LabelEncoder`. The goal is to demonstrate your understanding of label preprocessing techniques in scikit-learn. Tasks: 1. **Label Binarizer Transformation**: - Implement a function `label_binarizer_transform(labels: List[int]) -> Tuple[np.ndarray, np.ndarray]` that: - Receives a list of integer labels. - Fits a `LabelBinarizer` on the provided labels. - Transforms the original labels into a binary indicator matrix. - Returns the transformed binary matrix and the classes found. 2. **MultiLabel Binarizer Transformation**: - Implement a function `multilabel_binarizer_transform(labels: List[List[int]]) -> np.ndarray` that: - Receives a list of lists, where each sublist represents multilabels for a sample. - Fits and transforms the multilabel data using `MultiLabelBinarizer`. - Returns the transformed binary indicator matrix. 3. **Label Encoder Transformation**: - Implement a function `label_encoder_transform(labels: List[Union[str, int]]) -> Tuple[np.ndarray, np.ndarray]` that: - Receives a list of labels, which can be either strings or integers. - Fits a `LabelEncoder` on the provided labels. - Transforms the labels into encoded integers. - Returns the encoded labels and the classes found. Input and Output Formats: - Your functions should handle the following constraints: - `label_binarizer_transform(labels: List[int])`: - Example Input: `[1, 2, 6, 4, 2]` - Example Output: `(array([[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0], [0, 1, 0, 0]]), array([1, 2, 4, 6]))` - `multilabel_binarizer_transform(labels: List[List[int]])`: - Example Input: `[[2, 3, 4], [2], [0, 1, 3], [0, 1, 2, 3, 4], [0, 1, 2]]` - Example Output: `array([[0, 0, 1, 1, 1], [0, 0, 1, 0, 0], [1, 1, 0, 1, 0], [1, 1, 1, 1, 1], [1, 1, 1, 0, 0]])` - `label_encoder_transform(labels: List[Union[str, int]])`: - Example Input: `[1, 2, 2, 6]` - Example Output: `(array([0, 1, 1, 2]), array([1, 2, 6]))` - Example Input: `[\'paris\', \'paris\', \'tokyo\', \'amsterdam\']` - Example Output: `(array([1, 1, 2, 0]), array([\'amsterdam\', \'paris\', \'tokyo\']))` Constraints: - Input lists have lengths between 1 and 1000. - Labels within the list are hashable and comparable. - Label lists for multilabel transformation have sublists with lengths between 1 and 100. Note: - Use the given scikit-learn classes and methods for formatting and transforming the labels. - Ensure to handle edge cases, such as empty lists or non-unique labels within the input list. Additional Information: - You will not be evaluated on the installation or importing of packages. Focus on the implementation of the specified transformations. ```python # Example template to complete the task: from sklearn.preprocessing import LabelBinarizer, MultiLabelBinarizer, LabelEncoder import numpy as np from typing import List, Tuple, Union def label_binarizer_transform(labels: List[int]) -> Tuple[np.ndarray, np.ndarray]: # Your code here pass def multilabel_binarizer_transform(labels: List[List[int]]) -> np.ndarray: # Your code here pass def label_encoder_transform(labels: List[Union[str, int]]) -> Tuple[np.ndarray, np.ndarray]: # Your code here pass ```","solution":"from sklearn.preprocessing import LabelBinarizer, MultiLabelBinarizer, LabelEncoder import numpy as np from typing import List, Tuple, Union def label_binarizer_transform(labels: List[int]) -> Tuple[np.ndarray, np.ndarray]: lb = LabelBinarizer() binary_matrix = lb.fit_transform(labels) classes = lb.classes_ return binary_matrix, classes def multilabel_binarizer_transform(labels: List[List[int]]) -> np.ndarray: mlb = MultiLabelBinarizer() binary_matrix = mlb.fit_transform(labels) return binary_matrix def label_encoder_transform(labels: List[Union[str, int]]) -> Tuple[np.ndarray, np.ndarray]: le = LabelEncoder() encoded_labels = le.fit_transform(labels) classes = le.classes_ return encoded_labels, classes"},{"question":"**Python Dictionary Operations with C API** Using Python\'s C API functions related to dictionary objects, create a Python C extension module that provides the following functionalities through its methods: 1. **Create an empty dictionary**: Initializes and returns a new empty dictionary. 2. **Add an item**: Adds a key-value pair to the dictionary. 3. **Remove an item**: Removes a key by its name from the dictionary. 4. **Check for an item**: Returns True if a key is in the dictionary and False otherwise. 5. **Get an item**: Retrieves a value by its key from the dictionary. 6. **Merge two dictionaries**: Merges two dictionaries into one, optionally replacing existing keys. Requirements: - Implement a C extension module named `custom_dict`. - Define functions corresponding to the above operations. - Write appropriate error handling where necessary (e.g., if an invalid key is provided). - Test your module using the provided Python script to verify correctness. # Expected Input and Output Formats: 1. **Create an empty dictionary** ```python def create_empty_dict() -> dict: pass ``` **Returns**: An empty dictionary. 2. **Add an item** ```python def add_item(d: dict, key: str, value: any) -> int: pass ``` **Returns**: 0 on success, -1 on failure. 3. **Remove an item** ```python def remove_item(d: dict, key: str) -> int: pass ``` **Returns**: 0 on success, -1 on failure. 4. **Check for an item** ```python def contains_item(d: dict, key: str) -> bool: pass ``` **Returns**: True if the key exists, False otherwise. 5. **Get an item** ```python def get_item(d: dict, key: str) -> any: pass ``` **Returns**: The value associated with the key, or None if the key does not exist. 6. **Merge two dictionaries** ```python def merge_dicts(dict1: dict, dict2: dict, override: bool) -> dict: pass ``` **Returns**: A new dictionary containing all items from both dictionaries. Existing keys will be overridden if `override` is True. # Constraints & Limitations: - Handle cases where keys are not present when expected. - Ensure that the module handles arbitrary data types as dictionary values. - Ensure efficiency in terms of performance and memory usage. - You must write comprehensive tests to ensure that all functionalities work as expected and edge cases are handled well. # Performance Requirements: Your module should perform dictionary operations in a manner consistent with Python’s built-in performance expectations. Here is an example test script to validate your module: ```python import custom_dict # Create an empty dictionary d = custom_dict.create_empty_dict() # Add items to the dictionary assert custom_dict.add_item(d, \\"key1\\", \\"value1\\") == 0 assert custom_dict.add_item(d, \\"key2\\", 10) == 0 # Check for items in the dictionary assert custom_dict.contains_item(d, \\"key1\\") is True assert custom_dict.contains_item(d, \\"key3\\") is False # Get items from the dictionary assert custom_dict.get_item(d, \\"key1\\") == \\"value1\\" assert custom_dict.get_item(d, \\"key3\\") is None # Remove an item from the dictionary assert custom_dict.remove_item(d, \\"key1\\") == 0 assert custom_dict.contains_item(d, \\"key1\\") is False # Merge two dictionaries d1 = custom_dict.create_empty_dict() d2 = custom_dict.create_empty_dict() custom_dict.add_item(d1, \\"keyA\\", \\"valueA\\") custom_dict.add_item(d2, \\"keyB\\", \\"valueB\\") merged_dict = custom_dict.merge_dicts(d1, d2, True) assert custom_dict.contains_item(merged_dict, \\"keyA\\") is True assert custom_dict.contains_item(merged_dict, \\"keyB\\") is True print(\\"All tests passed!\\") ``` **Note**: Implementing a Python C extension module requires understanding the CPython API and compiling the extension. Make sure to follow appropriate steps to set up and compile your module.","solution":"from typing import Any def create_empty_dict() -> dict: Initializes and returns a new empty dictionary. return {} def add_item(d: dict, key: str, value: Any) -> int: Adds a key-value pair to the dictionary. Returns 0 on success, -1 on failure. try: d[key] = value return 0 except Exception: return -1 def remove_item(d: dict, key: str) -> int: Removes a key by its name from the dictionary. Returns 0 on success, -1 on failure. try: del d[key] return 0 except KeyError: return -1 def contains_item(d: dict, key: str) -> bool: Returns True if a key is in the dictionary, False otherwise. return key in d def get_item(d: dict, key: str) -> Any: Retrieves a value by its key from the dictionary. Returns the value associated with the key, or None if the key does not exist. return d.get(key, None) def merge_dicts(dict1: dict, dict2: dict, override: bool) -> dict: Merges two dictionaries into one. Existing keys will be overridden if `override` is True. Returns the merged dictionary. merged = dict1.copy() if override: merged.update(dict2) else: for key, value in dict2.items(): if key not in merged: merged[key] = value return merged"},{"question":"**Advanced Coding Assessment Question:** # Objective: You are required to implement a custom neural network module using PyTorch\'s `torch.cond` to perform a conditional operation based on both the shape and value of the input tensor. The final model should integrate these conditional checks and return the appropriate result based on the conditions specified. # Task: 1. Implement a subclass of `torch.nn.Module` named `CustomConditionalModule`. 2. This module should apply the following logic during the `forward` pass: - If the shape of the input tensor (along the first dimension) is greater than 10, check the sum of the tensor. - If the sum is greater than 50, apply the cosine function element-wise. - Otherwise, apply the sine function element-wise. - If the shape is 10 or less, apply the tangent function element-wise. # Requirements: - Implement the `CustomConditionalModule` class with the specified logic. - Ensure the correct usage of `torch.cond` for both shape and value-based conditions. - Provide appropriate functions for true and false branches within the `torch.cond`. # Input and Output Formats: - **Input:** A `torch.Tensor` of arbitrary shape. - **Output:** A `torch.Tensor` after applying the appropriate conditional operations. # Constraints: 1. Use `torch.cond` for implementing conditional checks. 2. The shape condition and value condition must use `torch.cond` and should be nested appropriately. 3. Avoid using any plain Python if-else statements for the conditions within the `forward` pass. # Example: ```python import torch class CustomConditionalModule(torch.nn.Module): def __init__(self): super(CustomConditionalModule, self).__init__() def forward(self, x: torch.Tensor) -> torch.Tensor: def shape_true_fn(x: torch.Tensor): def value_true_fn(x: torch.Tensor): return x.cos() def value_false_fn(x: torch.Tensor): return x.sin() return torch.cond(x.sum() > 50, value_true_fn, value_false_fn, (x,)) def shape_false_fn(x: torch.Tensor): return x.tan() return torch.cond(x.shape[0] > 10, shape_true_fn, shape_false_fn, (x,)) # Example Usage model = CustomConditionalModule() input_tensor = torch.randn(12) output = model(input_tensor) print(output) ``` # Notes: - Ensure your implementation is optimized and adheres to the given constraints. - You may add additional methods or helper functions as needed. - Test your model with various input shapes and values to ensure correctness. Good luck!","solution":"import torch import torch.nn.functional as F class CustomConditionalModule(torch.nn.Module): def __init__(self): super(CustomConditionalModule, self).__init__() def forward(self, x: torch.Tensor) -> torch.Tensor: def shape_true_fn(x): if x.sum() > 50: return x.cos() else: return x.sin() def shape_false_fn(x): return x.tan() if x.shape[0] > 10: return shape_true_fn(x) else: return shape_false_fn(x)"},{"question":"**Objective:** You are given a dataset and a machine learning problem which involves classifying data points. Your task is to build an optimal classifier using `GridSearchCV` from the scikit-learn library to find the best hyper-parameters for an SVM classifier. **Problem Description:** You are provided with the `digits` dataset from scikit-learn. Your objective is to: 1. Split the dataset into training and testing sets. 2. Use `GridSearchCV` to find the optimal hyper-parameters for an `SVC` (Support Vector Classifier). 3. Evaluate the model on the test set and report the performance. **Dataset:** - Use the `load_digits` function from `sklearn.datasets` to load the dataset. **Requirements:** 1. Define a parameter grid with the following values: - `C`: [1, 10, 100, 1000] - `gamma`: [0.001, 0.0001] - `kernel`: [\'rbf\'] 2. Perform a grid search using `GridSearchCV` with cross-validation. - Use 5-fold cross validation. 3. Train the `GridSearchCV` object using the training set to determine the best hyper-parameter combination. 4. Evaluate the best estimator on the test set and print the following: - Best parameters found by `GridSearchCV`. - Accuracy of the best model on the test set. **Function Signature:** ```python def optimize_svm_with_grid_search(): from sklearn.datasets import load_digits from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.svm import SVC from sklearn.metrics import accuracy_score # Load the digits dataset digits = load_digits() X, y = digits.data, digits.target # Split the dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Define the parameter grid param_grid = { \'C\': [1, 10, 100, 1000], \'gamma\': [0.001, 0.0001], \'kernel\': [\'rbf\'] } # Initialize GridSearchCV with SVC and the parameter grid grid_search = GridSearchCV(SVC(), param_grid, cv=5) # Fit GridSearchCV to the training data grid_search.fit(X_train, y_train) # Get the best estimator best_model = grid_search.best_estimator_ # Predict on the test data y_pred = best_model.predict(X_test) # Get the accuracy score on the test data accuracy = accuracy_score(y_test, y_pred) # Print the best parameters and the accuracy score print(\\"Best Parameters: \\", grid_search.best_params_) print(\\"Test Set Accuracy: \\", accuracy) ``` This function will help in confirming the student\'s understanding of hyper-parameter tuning using `GridSearchCV` and evaluating the model performance effectively.","solution":"def optimize_svm_with_grid_search(): from sklearn.datasets import load_digits from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.svm import SVC from sklearn.metrics import accuracy_score # Load the digits dataset digits = load_digits() X, y = digits.data, digits.target # Split the dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Define the parameter grid param_grid = { \'C\': [1, 10, 100, 1000], \'gamma\': [0.001, 0.0001], \'kernel\': [\'rbf\'] } # Initialize GridSearchCV with SVC and the parameter grid grid_search = GridSearchCV(SVC(), param_grid, cv=5) # Fit GridSearchCV to the training data grid_search.fit(X_train, y_train) # Get the best estimator best_model = grid_search.best_estimator_ # Predict on the test data y_pred = best_model.predict(X_test) # Get the accuracy score on the test data accuracy = accuracy_score(y_test, y_pred) # Print the best parameters and the accuracy score print(\\"Best Parameters: \\", grid_search.best_params_) print(\\"Test Set Accuracy: \\", accuracy)"},{"question":"Background: You are tasked with building a simple producer-consumer system where multiple producers are generating data, and multiple consumers are processing that data. You will use the `asyncio` library to implement this system asynchronously. Task: Write a Python function named `producer_consumer_system` that: 1. Launches three producer coroutines that generate integers from 1 to 10 with an interval of 0.1 seconds between each number. 2. Launches two consumer coroutines that process the received integers by printing them to the console with a 0.2-second delay between each print. 3. Uses an asyncio queue to facilitate communication between producers and consumers. 4. Exits gracefully once all integers from all producers are processed. Function Signature: ```python import asyncio from typing import NoReturn async def producer_consumer_system() -> NoReturn: pass ``` Input: - The function does not take any input parameters. Detailed Requirements: 1. **Producer Coroutine**: - Three producer instances should run concurrently. - Each producer should generate numbers from 1 to 10. - Use `await asyncio.sleep(0.1)` to simulate a delay between each number generation. - Each number should be put into an asyncio queue. 2. **Consumer Coroutine**: - Two consumer instances should run concurrently. - Each consumer should retrieve numbers from the queue. - Use `await asyncio.sleep(0.2)` to simulate processing time for each number. - Each consumer should print each number it processes. 3. **Queue**: - An asyncio queue should be used to communicate between producers and consumers. - Use `queue = asyncio.Queue()` to create the queue. 4. **Graceful Exit**: - Once all numbers (a total of 30, 10 from each producer) are queued and processed, the system should exit gracefully. - Ensure that all pending tasks are completed before exiting. Example Output: ``` Consumer 1 processing: 1 Consumer 2 processing: 1 Consumer 1 processing: 2 ... ``` Implementation Notes: - Use the `asyncio` library for managing asynchronous tasks. - Make sure to handle the asyncio queue appropriately to avoid producers getting stuck. - Consider using `asyncio.gather` or other coordination primitives for synchronizing the completion of tasks. Constraints: - The solution should demonstrate efficient use of asyncio to handle concurrent producers and consumers. - Ensure proper synchronization and communication between coroutines. Good luck!","solution":"import asyncio from typing import NoReturn async def producer(queue: asyncio.Queue, id: int) -> None: for i in range(1, 11): await asyncio.sleep(0.1) # Simulate time taken to produce an item await queue.put(f\'Producer {id}: {i}\') async def consumer(queue: asyncio.Queue, id: int) -> None: while True: item = await queue.get() if item is None: # Sentinal value for stopping consumer break await asyncio.sleep(0.2) # Simulate time taken to process an item print(f\\"Consumer {id} processing: {item}\\") queue.task_done() async def producer_consumer_system() -> NoReturn: queue = asyncio.Queue() producers = [producer(queue, i) for i in range(1, 4)] consumers = [consumer(queue, i) for i in range(1, 3)] await asyncio.gather(*(asyncio.create_task(p) for p in producers)) # Signal consumers to exit for _ in consumers: await queue.put(None) await asyncio.gather(*(asyncio.create_task(c) for c in consumers))"},{"question":"# Advanced Slice Handling in Python Objective: You are required to implement a function that takes a list and a custom slice defined by start, stop, and step indices, returns the corresponding sublist according to the slice, while also handling out-of-bounds indices in a manner similar to the functions described in the documentation. Function Signature: ```python def custom_slice(lst: list, start: int, stop: int, step: int) -> list: pass ``` Input: - `lst` (list): A list of integers. - `start` (int): The start index of the slice. - `stop` (int): The end index of the slice. - `step` (int): The step of the slice. Output: - (list): A list of integers that is a sublist of `lst` from `start` to `stop` with the given `step`. Constraints: - The indices may fall outside the bounds of the list (negative or exceeding list length). - You must handle these indices appropriately similar to the behaviour described in `PySlice_AdjustIndices`. Performance Requirements: - The function should run efficiently within the time and space limits of typical list slicing. Example: ```python # Example 1 lst = [1, 2, 3, 4, 5] start = 1 stop = 4 step = 1 print(custom_slice(lst, start, stop, step)) # Output: [2, 3, 4] # Example 2 lst = [1, 2, 3, 4, 5] start = -3 stop = 10 step = 2 print(custom_slice(lst, start, stop, step)) # Output: [3, 5] # Example 3 lst = [1, 2, 3, 4, 5] start = 4 stop = 1 step = -1 print(custom_slice(lst, start, stop, step)) # Output: [5, 4, 3] ``` **Note**: - Pay special attention to how Python handles negative and out-of-bound indices during slicing. - Mimic this behavior in your implementation using proper handling techniques as suggested by `PySlice_AdjustIndices`. Hints: - Use Python\'s built-in list slicing to validate your approach. - Consider writing helper functions for boundary checks and adjustments.","solution":"def custom_slice(lst: list, start: int, stop: int, step: int) -> list: Returns a sublist of lst from start to stop with the given step, handling out-of-bounds indices. # Adjust start if start < 0: start += len(lst) if start < 0: start = 0 elif start >= len(lst): start = len(lst) # Adjust stop if stop < 0: stop += len(lst) if stop < 0: stop = 0 elif stop >= len(lst): stop = len(lst) # If step is positive, start should be less than stop # If step is negative, start should be greater than stop if step > 0 and start >= stop: return [] if step < 0 and start <= stop: return [] # Use built-in slicing capability return lst[start:stop:step]"},{"question":"# Assignment Statement Proficiency Test In this coding assessment, you are required to demonstrate your understanding of various assignment statements in Python. Implement a function called `complex_assignments` which takes no parameters and performs the following tasks: 1. Create a class `MyClass` with the following specifications: - An `__init__` method that initializes two instance variables, `a` and `b`, both set to 0 initially. - A method `update_values` which takes two parameters and sets them to instance variables `a` and `b`. 2. Initialize an instance `my_object` of `MyClass`. 3. Perform the following assignments using the `my_object` instance: - Set `my_object.a` to 10 and `my_object.b` to 20 using simple assignment statements. - Swap the values of `my_object.a` and `my_object.b` with a single assignment statement. - Use an augmented assignment statement to add 5 to both `my_object.a` and `my_object.b`. 4. Initialize a list `my_list` with values `[1, 2, 3, 4, 5]`. 5. Modify `my_list`: - Change the third element to 10 using a subscription assignment. - Replace the sublist `[2, 3]` with `[20, 30]` using slicing assignment. 6. Declare a global variable `global_var` and set it to 100. Write a nested function `modify_global` within `complex_assignments` that: - Uses the `global` statement to modify `global_var` to 200. 7. Demonstrate the difference between `global` and `nonlocal` by writing another nested function `enclosing_function` that: - Declares a local variable `enclosing_var` and sets it to 300. - Contains an inner function `modify_nonlocal` which: - Uses the `nonlocal` statement to modify `enclosing_var` to 400. 8. Return a dictionary containing: - The values of `my_object.a` and `my_object.b`. - The modified `my_list`. - The value of `global_var` after calling `modify_global`. - The value of `enclosing_var` after calling `enclosing_function`. # Constraints & Requirements - Ensure that your implementation correctly handles all described assignment types. - Do not use any print statements. - Use only the constructs given in the problem description. # Example Output ```python { \\"my_object.a\\": 25, \\"my_object.b\\": 15, \\"my_list\\": [1, 20, 30, 10, 5], \\"global_var\\": 200, \\"enclosing_var\\": 400 } ``` Here is the template to get you started: ```python def complex_assignments(): global global_var class MyClass: def __init__(self): self.a = 0 self.b = 0 def update_values(self, new_a, new_b): self.a = new_a self.b = new_b my_object = MyClass() # Your code here to perform the tasks return { \\"my_object.a\\": my_object.a, \\"my_object.b\\": my_object.b, \\"my_list\\": my_list, \\"global_var\\": global_var, \\"enclosing_var\\": enclosing_var } ```","solution":"def complex_assignments(): global global_var class MyClass: def __init__(self): self.a = 0 self.b = 0 def update_values(self, new_a, new_b): self.a = new_a self.b = new_b my_object = MyClass() # Set my_object.a to 10 and my_object.b to 20 using simple assignment statements my_object.a = 10 my_object.b = 20 # Swap the values of my_object.a and my_object.b with a single assignment statement my_object.a, my_object.b = my_object.b, my_object.a # Use an augmented assignment statement to add 5 to both my_object.a and my_object.b my_object.a += 5 my_object.b += 5 # Initialize a list my_list with values [1, 2, 3, 4, 5] my_list = [1, 2, 3, 4, 5] # Change the third element to 10 using a subscription assignment my_list[2] = 10 # Replace the sublist [2, 3] with [20, 30] using slicing assignment my_list[1:3] = [20, 30] # Declare a global variable global_var and set it to 100 global_var = 100 def modify_global(): global global_var global_var = 200 modify_global() def enclosing_function(): enclosing_var = 300 def modify_nonlocal(): nonlocal enclosing_var enclosing_var = 400 modify_nonlocal() return enclosing_var enclosing_var = enclosing_function() return { \\"my_object.a\\": my_object.a, \\"my_object.b\\": my_object.b, \\"my_list\\": my_list, \\"global_var\\": global_var, \\"enclosing_var\\": enclosing_var }"},{"question":"# Question: Creating and Customizing a Heatmap with Seaborn You are provided with a dataset on \'iris\' flowers consisting of measurements for different flower parts for three different species. Your task is to create and customize a heatmap visualization using Seaborn. The objective is to assess your ability to utilize various seaborn functionalities as described below. Dataset: Load the `iris` dataset from Seaborn: ```python import seaborn as sns iris = sns.load_dataset(\\"iris\\") ``` This dataset contains the following columns: - `sepal_length` - `sepal_width` - `petal_length` - `petal_width` - `species` Requirements: 1. **Data Preparation:** - Pivot the dataset to create a DataFrame where the rows represent species and columns are the averages of `sepal_length`, `sepal_width`, `petal_length`, and `petal_width`. 2. **Heatmap Creation:** - Create a heatmap using Seaborn where: - The cell colors represent the average measurements. - Annotate the cells with the average values formatted to one decimal place. - Use a linewidth of 0.5 between the cells. - Select the colormap named \\"YlGnBu\\". 3. **Customization:** - Set the `vmin` to 3 and `vmax` to 8 for the color range. - Use methods on the `matplotlib.axes.Axes` object to tweak the plot: - Remove the labels on both the x and y axes. - Position the x-axis ticks at the top of the plot. Input Format: - No specific input from the user is required other than loading the dataset using the provided seaborn function. Output: - A correctly formatted heatmap plot as per the specifications outlined. Example Code: Your solution should start like this, further implementation should follow the guidelines provided: ```python import seaborn as sns import matplotlib.pyplot as plt # Load the dataset iris = sns.load_dataset(\\"iris\\") # Data Preparation pivot_iris = iris.pivot_table(index=\'species\', values=[\'sepal_length\', \'sepal_width\', \'petal_length\', \'petal_width\'], aggfunc=\'mean\') # Heatmap Creation ax = sns.heatmap(pivot_iris, annot=True, fmt=\\".1f\\", linewidths=0.5, cmap=\\"YlGnBu\\", vmin=3, vmax=8) # Customization using matplotlib axes methods ax.set(xlabel=\\"\\", ylabel=\\"\\") ax.xaxis.tick_top() # Display the plot plt.show() ``` Constraints: - You should use Seaborn for data visualization. - The matplotlib library may be used for customization purposes as required.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_iris_heatmap(): # Load the dataset iris = sns.load_dataset(\\"iris\\") # Data Preparation pivot_iris = iris.pivot_table(index=\'species\', values=[\'sepal_length\', \'sepal_width\', \'petal_length\', \'petal_width\'], aggfunc=\'mean\') # Heatmap Creation ax = sns.heatmap(pivot_iris, annot=True, fmt=\\".1f\\", linewidths=0.5, cmap=\\"YlGnBu\\", vmin=3, vmax=8) # Customization using matplotlib axes methods ax.set(xlabel=\\"\\", ylabel=\\"\\") ax.xaxis.set_ticks_position(\'top\') # Display the plot plt.show() # Call the function to create and display the heatmap create_iris_heatmap()"},{"question":"# Advanced HTTP Request Handling with `urllib.request` **Objective:** Create a function that fetches data from a URL, handles different HTTP request methods, and manages exceptions appropriately. **Function Signature:** ```python def fetch_data(url: str, params: dict = None, headers: dict = None, method: str = \'GET\') -> dict: pass ``` **Function Explanation:** 1. **URL fetching**: - If `method` is `\'GET\'`: - Encode the `params` into the URL for a GET request. - If `method` is `\'POST\'`: - Send the `params` as data in a POST request. - Use the appropriate headers if provided. - Return the response content as a dictionary with keys `content` and `status_code`. 2. **Handling exceptions**: - Handle `HTTPError` and `URLError` exceptions. - For `HTTPError`, include the error code in the response. - For `URLError`, include the error reason in the response. 3. **Constraints**: - Assume the URL will always use HTTP. - The `params` dictionary (if provided) will not be empty in case of POST requests. **Input:** - `url`: A string representing the URL to fetch data from. - `params`: A dictionary containing query parameters or POST data. - `headers`: A dictionary containing any custom headers to be sent with the request. - `method`: A string specifying the HTTP request method (`\'GET\'` or `\'POST\'`). Default is `\'GET\'`. **Output:** - Returns a dictionary with keys: - `content`: The fetched content (in bytes). - `status_code`: HTTP status code of the response. - If an error occurs, includes an `error` key with the appropriate error message or code. **Example:** ```python result = fetch_data(\'http://www.example.com\', params={\'q\': \'test\'}, method=\'GET\') # Example output: # {\'content\': b\'...\', \'status_code\': 200} result = fetch_data(\'http://www.example.com\', params={\'name\': \'test\'}, headers={\'User-Agent\': \'Mozilla/5.0\'}, method=\'POST\') # Example output: # {\'content\': b\'...\', \'status_code\': 200} result = fetch_data(\'http://www.nonexistenturl.com\') # Example output: # {\'error\': \'Failed to reach a server.\', \'reason\': <reason>} ``` **Solution Skeleton:** ```python from urllib import request, parse, error def fetch_data(url: str, params: dict = None, headers: dict = None, method: str = \'GET\') -> dict: try: if method == \'GET\' and params: url += \'?\' + parse.urlencode(params) req = request.Request(url, headers=headers or {}) if method == \'POST\' and params: data = parse.urlencode(params).encode(\'ascii\') req.data = data with request.urlopen(req) as response: return { \'content\': response.read(), \'status_code\': response.getcode() } except error.HTTPError as e: return { \'error\': \'The server couldn\'t fulfill the request.\', \'status_code\': e.code } except error.URLError as e: return { \'error\': \'Failed to reach a server.\', \'reason\': e.reason } ```","solution":"from urllib import request, parse, error def fetch_data(url: str, params: dict = None, headers: dict = None, method: str = \'GET\') -> dict: try: if method == \'GET\' and params: url += \'?\' + parse.urlencode(params) req = request.Request(url, headers=headers or {}) if method == \'POST\' and params: data = parse.urlencode(params).encode(\'ascii\') response = request.urlopen(req, data=data) else: response = request.urlopen(req) return { \'content\': response.read(), \'status_code\': response.getcode() } except error.HTTPError as e: return { \'error\': \'The server couldn\'t fulfill the request.\', \'status_code\': e.code } except error.URLError as e: return { \'error\': \'Failed to reach a server.\', \'reason\': e.reason }"},{"question":"# Command-Line Parsing with \\"optparse\\" **Objective**: Demonstrate your understanding of the \\"optparse\\" module in Python by creating a command-line tool that processes multiple options and arguments. **Task**: Write a Python script that uses the \\"optparse\\" module to create a command-line tool called `album_manager.py`. This tool will allow users to manage their music albums with various commands and options. Functionality Requirements: 1. **Options**: - `-a`, `--add`: Adds a new album. Requires an album name (`--name`) and the year of release (`--year`). - `-l`, `--list`: Lists all albums. Takes an optional `--genre` argument to filter albums by genre. - `-d`, `--delete`: Deletes an album by name. - `-u`, `--update`: Updates the information of an album. Requires the original name (`--orig-name`), the updated name (`--new-name`), and/or the updated year (`--new-year`). 2. **Positional Arguments**: - The tool should accept a positional argument to specify the file where the album data is stored (e.g., `./album_manager.py myalbums.dat`). 3. **Help and Usage**: - Ensure that the script generates appropriate help messages when a user provides `-h` or `--help`. Constraints: - Assume the album data is stored in a simple text file specified by the positional argument. - The script should gracefully handle errors such as missing required options. Implementation Details: 1. Implement the `OptionParser` and define the required options. 2. Parse the command-line arguments and perform actions based on the options provided. 3. Use appropriate `OptionGroup` for logically grouping related options. 4. Handle and print user-friendly error messages using `OptionParser.error()` where necessary. **Example Usage**: 1. Adding a new album: ``` ./album_manager.py myalbums.dat --add --name \\"Album Name\\" --year 2023 ``` 2. Listing all albums: ``` ./album_manager.py myalbums.dat --list ``` 3. Listing albums filtered by genre: ``` ./album_manager.py myalbums.dat --list --genre \\"Rock\\" ``` 4. Deleting an album: ``` ./album_manager.py myalbums.dat --delete --name \\"Album Name\\" ``` 5. Updating album information: ``` ./album_manager.py myalbums.dat --update --orig-name \\"Old Name\\" --new-name \\"New Name\\" ``` **Expected Input and Output**: - Ensure options and arguments are correctly parsed. - The script should read from and write to the specified data file to store album information. - Upon successful addition, listing, deletion, or update of an album, output a success message to the user. Complete the `album_manager.py` script ensuring it meets the above specifications and gracefully handles errors.","solution":"import sys from optparse import OptionParser, OptionGroup def add_album(filename, name, year): with open(filename, \'a\') as f: f.write(f\'{name},{year}n\') print(f\\"Album \'{name}\' added successfully.\\") def list_albums(filename, genre=None): try: with open(filename, \'r\') as f: albums = f.readlines() except FileNotFoundError: print(f\\"No albums found in {filename}.\\") return if genre: albums = [album for album in albums if genre.lower() in album.lower()] if albums: for album in albums: print(album.strip()) else: print(\\"No albums to display.\\") def delete_album(filename, name): try: with open(filename, \'r\') as f: albums = f.readlines() except FileNotFoundError: print(f\\"Album file {filename} does not exist.\\") return with open(filename, \'w\') as f: found = False for album in albums: if not album.startswith(name): f.write(album) else: found = True if found: print(f\\"Album \'{name}\' deleted successfully.\\") else: print(f\\"Album \'{name}\' not found.\\") def update_album(filename, orig_name, new_name=None, new_year=None): try: with open(filename, \'r\') as f: albums = f.readlines() except FileNotFoundError: print(f\\"Album file {filename} does not exist.\\") return with open(filename, \'w\') as f: found = False for album in albums: name, year = album.strip().split(\',\') if name == orig_name: if new_name: name = new_name if new_year: year = new_year found = True f.write(f\'{name},{year}n\') if found: print(f\\"Album \'{orig_name}\' updated successfully.\\") else: print(f\\"Album \'{orig_name}\' not found.\\") def main(): usage = \\"usage: %prog [options] filename\\" parser = OptionParser(usage) parser.add_option(\\"-a\\", \\"--add\\", action=\\"store_true\\", dest=\\"add\\", help=\\"Add a new album\\") parser.add_option(\\"--name\\", dest=\\"name\\", help=\\"Name of the album\\") parser.add_option(\\"--year\\", dest=\\"year\\", help=\\"Year of release\\", type=\\"int\\") parser.add_option(\\"-l\\", \\"--list\\", action=\\"store_true\\", dest=\\"list\\", help=\\"List all albums\\") parser.add_option(\\"--genre\\", dest=\\"genre\\", help=\\"Filter albums by genre\\") parser.add_option(\\"-d\\", \\"--delete\\", action=\\"store_true\\", dest=\\"delete\\", help=\\"Delete an album\\") parser.add_option(\\"-u\\", \\"--update\\", action=\\"store_true\\", dest=\\"update\\", help=\\"Update album information\\") parser.add_option(\\"--orig-name\\", dest=\\"orig_name\\", help=\\"Original name of the album\\") parser.add_option(\\"--new-name\\", dest=\\"new_name\\", help=\\"New name of the album\\") parser.add_option(\\"--new-year\\", dest=\\"new_year\\", help=\\"New year of the album\\", type=\\"int\\") (options, args) = parser.parse_args() if len(args) != 1: parser.error(\\"incorrect number of arguments\\") filename = args[0] if options.add: if not options.name or not options.year: parser.error(\\"name and year are required to add an album\\") add_album(filename, options.name, options.year) elif options.list: list_albums(filename, options.genre) elif options.delete: if not options.name: parser.error(\\"name is required to delete an album\\") delete_album(filename, options.name) elif options.update: if not options.orig_name: parser.error(\\"original name is required to update an album\\") update_album(filename, options.orig_name, options.new_name, options.new_year) else: parser.error(\\"requires an action, use -h for help\\") if __name__ == \\"__main__\\": main()"},{"question":"Objective: To assess your ability to work with file I/O, caching mechanisms, and Python\'s `linecache` module. Problem Statement: You are required to implement a Python function `extract_significant_lines`, which reads lines from a given source file and identifies \\"significant\\" lines. A significant line is defined as any line containing a specified keyword. You must use the `linecache` module to efficiently accomplish this task. The function should return a dictionary with line numbers as keys and the corresponding line\'s content as values. Function Signature: ```python def extract_significant_lines(filename: str, keyword: str) -> dict: pass ``` Input: - `filename` (str): The name of the source file to read. - `keyword` (str): The keyword to search for in each line. Output: - (dict): A dictionary where each key is the line number (int) and the corresponding value is the line string containing the keyword. Constraints: - The function should handle files up to 10,000 lines efficiently. - The keyword search should be case-insensitive. - If no lines contain the keyword, the function should return an empty dictionary. - Do not read the entire file into memory at once. Performance Requirements: - Utilize the caching capabilities of the `linecache` module to avoid re-reading lines from the file where possible. Example: ```python # Contents of \'example.txt\' # 1: This is a test line. # 2: Another important line. # 3: Just a random line. # 4: Important findings here. # 5: Some more text. result = extract_significant_lines(\'example.txt\', \'important\') # Expected Output: {2: \'Another important line.n\', 4: \'Important findings here.n\'} result = extract_significant_lines(\'example.txt\', \'random\') # Expected Output: {3: \'Just a random line.n\'} ``` Note: You can assume that the file specified by `filename` exists and is readable. Hints: - You may find the `linecache.getline()` method useful. - Consider using `str.lower()` to perform case-insensitive comparisons.","solution":"import linecache def extract_significant_lines(filename: str, keyword: str) -> dict: Extract lines containing the specified keyword from the given file. Parameters: filename (str): The name of the file to read. keyword (str): The keyword to search for in each line. Returns: dict: A dictionary where each key is the line number and the value is the line content containing the keyword. keyword = keyword.lower() significant_lines = {} with open(filename, \'r\') as file: for line_number, line in enumerate(file, start=1): if keyword in line.lower(): significant_lines[line_number] = linecache.getline(filename, line_number) return significant_lines"},{"question":"**Coding Assessment Question** # Objective You are required to demonstrate your knowledge and understanding of the seaborn library, specifically focusing on creating and customizing histogram and bar plots using the `seaborn.objects` interface. # Instructions 1. Write a Python function named `create_custom_plots` that takes a dataset as input and creates the following plots: - A bar plot for a categorical variable named \\"species\\" showing the count of each category. - A histogram for a numerical variable named \\"body_mass_g\\" with the number of bins set to 15. - A normalized histogram of the \\"flipper_length_mm\\" variable to show the proportion for different genders, represented with different colors. - An overlapping area plot for the \\"flipper_length_mm\\" variable to compare distributions between genders. 2. Use the `seaborn` library for all plotting. 3. Save each plot as an image file named `bar_plot.png`, `histogram.png`, `normalized_histogram.png`, and `area_plot.png` respectively. 4. Ensure your code handles any potential errors, such as missing columns in the dataset. # Function Signature ```python def create_custom_plots(df: pd.DataFrame) -> None: pass ``` # Input - `df` (pd.DataFrame): The input dataset containing at least the columns \\"species\\", \\"body_mass_g\\", \\"flipper_length_mm\\", and \\"sex\\". # Output - The function should not return anything but should save four image files as described above. # Example ```python import seaborn as sns import pandas as pd df = sns.load_dataset(\\"penguins\\") create_custom_plots(df) ``` # Constraints - The dataset may contain `NaN` values, which should be handled appropriately. - The function should be efficient and leverage seaborn\'s capabilities for customization and performance. # Additional Information - Refer to the seaborn documentation and examples provided in the initial code cells for guidance on using `seaborn.objects`. - Ensure the plots are well-labeled, with titles, axis labels, and legends where appropriate.","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt def create_custom_plots(df: pd.DataFrame) -> None: # Check for required columns required_columns = [\\"species\\", \\"body_mass_g\\", \\"flipper_length_mm\\", \\"sex\\"] for col in required_columns: if col not in df.columns: raise ValueError(f\\"Missing required column: {col}\\") # Drop rows with NaN in required columns df = df.dropna(subset=required_columns) # Bar plot for \'species\' count plt.figure(figsize=(10, 6)) sns.countplot(data=df, x=\'species\') plt.title(\'Count of Each Species\') plt.ylabel(\'Count\') plt.xlabel(\'Species\') plt.savefig(\'bar_plot.png\') plt.close() # Histogram for \'body_mass_g\' plt.figure(figsize=(10, 6)) sns.histplot(df[\'body_mass_g\'], bins=15, kde=False) plt.title(\'Histogram of Body Mass (g)\') plt.ylabel(\'Frequency\') plt.xlabel(\'Body Mass (g)\') plt.savefig(\'histogram.png\') plt.close() # Normalized histogram for \'flipper_length_mm\' by sexes plt.figure(figsize=(10, 6)) sns.histplot(data=df, x=\'flipper_length_mm\', hue=\'sex\', stat=\'proportion\', common_norm=False, kde=False) plt.title(\'Normalized Histogram of Flipper Length (mm)\') plt.ylabel(\'Proportion\') plt.xlabel(\'Flipper Length (mm)\') plt.savefig(\'normalized_histogram.png\') plt.close() # Overlapping area plot for \'flipper_length_mm\' by sexes plt.figure(figsize=(10, 6)) sns.histplot(data=df, x=\'flipper_length_mm\', hue=\'sex\', kde=True, element=\'poly\') plt.title(\'Overlapping Area Plot of Flipper Length (mm)\') plt.ylabel(\'Density\') plt.xlabel(\'Flipper Length (mm)\') plt.savefig(\'area_plot.png\') plt.close()"},{"question":"You are to create a function `extract_lines` that reads multiple specific lines from a given file and returns them as a list of strings. Use the `linecache` module to achieve this. Make sure to handle the cache appropriately to ensure that the solution works even if the file is updated between calls. # Function Signature ```python def extract_lines(filename: str, line_numbers: list) -> list: pass ``` # Inputs 1. `filename` (str): The path to the file from which lines need to be read. 2. `line_numbers` (list): A list of integers where each integer represents a line number to fetch. # Output - Returns a list of strings, where each string is the content of the corresponding line in the `line_numbers` list. # Constraints - You can assume that the file exists and is readable. - Line numbers in `line_numbers` are guaranteed to be positive integers and valid for the given file. # Example ```python # Assuming \'testfile.txt\' contains the following lines: # Line 1: \\"First line of the file.n\\" # Line 2: \\"Second line of the file.n\\" # Line 3: \\"Third line of the file.n\\" filename = \'testfile.txt\' line_numbers = [1, 3] print(extract_lines(filename, line_numbers)) # Output: [\\"First line of the file.n\\", \\"Third line of the file.n\\"] line_numbers = [2] print(extract_lines(filename, line_numbers)) # Output: [\\"Second line of the file.n\\"] ``` # Notes - Ensure that the cache is cleared before and after fetching lines to handle any updates to the file. - Use exception handling to address any unexpected issues and return an empty string for any line that cannot be fetched.","solution":"import linecache def extract_lines(filename: str, line_numbers: list) -> list: Extracts specific lines from a given file and returns them as a list of strings. Parameters: filename (str): The path to the file from which lines need to be read. line_numbers (list): A list of integers where each integer represents a line number to fetch. Returns: list: A list of strings, each corresponding to the content of the specified lines. result = [] # Ensure the cache is cleared before reading the lines linecache.clearcache() for line_number in line_numbers: line = linecache.getline(filename, line_number) result.append(line) # Clear the cache again to handle any file updates linecache.clearcache() return result"},{"question":"# Advanced Python Number Operations You are required to design and implement a series of functions in Python that mimic the behaviors of Python\'s arithmetic and bitwise operations using pure Python functions rather than built-in operators. These functions need to represent the respective operations in a way that allows their extension to handle custom numeric types or structures. Implement the following functions: 1. `add(o1, o2)` 2. `subtract(o1, o2)` 3. `multiply(o1, o2)` 4. `matrix_multiply(o1, o2)` 5. `floor_divide(o1, o2)` 6. `true_divide(o1, o2)` 7. `remainder(o1, o2)` 8. `negate(o)` 9. `bitwise_and(o1, o2)` 10. `bitwise_or(o1, o2)` 11. `bitwise_xor(o1, o2)` Requirements: 1. **Input and Output Formats:** - Each function should accept two arguments `o1` and `o2` (except for `negate` which accepts a single argument `o`). - Each function should return the result of the respective operation applied to `o1` and `o2`. 2. **Constraints:** - Assume `o1` and `o2` can be any types that support the respective operations. - Your implementation must be capable of handling built-in types like `int`, `float`, but also custom types that define the corresponding magic methods (`__add__`, `__sub__`, `__mul__`, `__matmul__`, etc.). 3. **Performance:** - Aim for clear and correct implementations first; performance optimizations are secondary but encouraged where obvious. 4. **Example Usage:** ```python class CustomNumber: def __init__(self, value): self.value = value def __add__(self, other): return CustomNumber(self.value + other.value) def __str__(self): return str(self.value) o1 = CustomNumber(5) o2 = CustomNumber(3) result = add(o1, o2) print(result) # Should print CustomNumber(8) based on CustomNumber\'s __str__ implementation. ``` Implement all necessary functions to pass the above usage example and any further relevant tests for the included operations.","solution":"def add(o1, o2): Returns the sum of o1 and o2. return o1 + o2 def subtract(o1, o2): Returns the difference between o1 and o2. return o1 - o2 def multiply(o1, o2): Returns the product of o1 and o2. return o1 * o2 def matrix_multiply(o1, o2): Returns the matrix product of o1 and o2. return o1 @ o2 def floor_divide(o1, o2): Returns the floor division of o1 by o2. return o1 // o2 def true_divide(o1, o2): Returns the true division of o1 by o2. return o1 / o2 def remainder(o1, o2): Returns the remainder of o1 divided by o2. return o1 % o2 def negate(o): Returns the negation of o. return -o def bitwise_and(o1, o2): Returns the bitwise and of o1 and o2. return o1 & o2 def bitwise_or(o1, o2): Returns the bitwise or of o1 and o2. return o1 | o2 def bitwise_xor(o1, o2): Returns the bitwise xor of o1 and o2. return o1 ^ o2"},{"question":"Objective: Design a complex multi-faceted seaborn visualization that demonstrates a strong understanding of seaborn\'s `catplot` functionalities and customization options. Problem: You are provided with the Titanic dataset, which contains various attributes about the passengers. Your task is to create a series of visualizations using seaborn\'s `catplot` function to show the distribution of passengers\' ages across different classes and to explore the survival rate based on different demographics. Tasks: 1. **Load the Titanic Dataset**: Load the Titanic dataset using the `seaborn` library. 2. **Age Distribution by Class**: Create a multi-plot figure (subplots) showing the distribution of passengers\' ages (`age`) across different passenger classes (`class`). Use a violin plot with the following customizations: - Include a kernel density estimate (`bw_adjust=0.5`). - Do not cut the density at the data limits (`cut=0`). - Split the violin plot by `sex`. 3. **Survival Rate by Class and Sex**: Create a subplot that shows the survival rate (`survived`) by class and sex. Use a bar plot, and set the figure\'s `height` to 4 and `aspect` to 0.6. 4. **Layered Plot**: On a single subplot, create a violin plot displaying the distribution of ages across different classes. Overlay this with a strip plot to show the individual data points. Use the following configurations: - The violin plot should use a color (`color=\\".9\\"`) and have no inner annotation (`inner=None`). - The strip plot should use a smaller marker size (`size=3`). 5. **Customization**: For the subplot created in Task 3, customize the plot: - Set axis labels (`set_axis_labels`) to \\"Passenger Class\\" and \\"Survival Rate\\". - Set x-tick labels (`set_xticklabels`) to \\"Men\\", \\"Women\\", and \\"Children\\". - Set the subplot titles (`set_titles`) to include the class name. - Set the y-axis limit to (0, 1). - Remove the left spine of the plot (`despine`). Constraints: - The solution should be implemented in a single Jupyter Notebook. - Include necessary import statements and load the seaborn theme `sns.set_theme(style=\\"whitegrid\\")`. - Ensure each visualization is clearly labeled and easy to interpret. Expected Output: - A Jupyter Notebook containing the solution with four visualizations as specified above. - Each plot should be properly customized and should include comments explaining each step and customization. Good luck!","solution":"import seaborn as sns import matplotlib.pyplot as plt # Load the Titanic dataset titanic = sns.load_dataset(\'titanic\') # Set the seaborn theme sns.set_theme(style=\\"whitegrid\\") # Task 2: Age Distribution by Class using violin plot g1 = sns.catplot(x=\\"class\\", y=\\"age\\", hue=\\"sex\\", data=titanic, kind=\\"violin\\", bw=.5, cut=0, split=True) g1.fig.suptitle(\\"Age Distribution by Class\\") # Task 3: Survival Rate by Class and Sex using bar plot g2 = sns.catplot(x=\\"class\\", hue=\\"sex\\", y=\\"survived\\", data=titanic, kind=\\"bar\\", height=4, aspect=0.6) g2.set_axis_labels(\\"Passenger Class\\", \\"Survival Rate\\") g2.set_xticklabels([\'First\', \'Second\', \'Third\']) g2.set_titles(\\"{col_name} Survival Rate by Class and Sex\\") g2.set(ylim=(0, 1)) g2.despine(left=True) g2.fig.suptitle(\\"Survival Rate by Class and Sex\\") # Task 4: Layered Plot - Violin and Strip plot plt.figure(figsize=(8, 6)) sns.violinplot(x=\\"class\\", y=\\"age\\", data=titanic, color=\\".9\\", inner=None) sns.stripplot(x=\\"class\\", y=\\"age\\", data=titanic, size=3) plt.title(\\"Distribution of Ages across Classes with Individual Data Points\\") # Display all plots plt.show()"},{"question":"**Question: CUDA Operations with PyTorch** You are required to implement a function using PyTorch\'s CUDA API that performs the following tasks sequentially: 1. **Device Management**: Set the current device to a specified GPU if it has more than one GPU available; otherwise, raise an exception. 2. **Random Number Generation**: Initialize the random number generator with a given seed. 3. **Stream Handling**: Execute a simple tensor operation (e.g., element-wise addition of two random tensors) on a separate CUDA stream. 4. **Memory Management**: After the operation, report the memory usage statistics on the GPU. 5. **NVTX Marking**: Use NVTX to mark the start and end of the tensor operation for profiling. # Function Signature ```python import torch import torch.cuda.nvtx as nvtx def cuda_operations_with_pytorch(seed: int) -> dict: Performs a sequence of CUDA operations using PyTorch\'s CUDA API. Args: - seed (int): The seed to initialize the random number generator. Returns: - dict: A dictionary containing memory usage statistics after the tensor operation. The keys of the dictionary should be: - \\"memory_allocated\\": The total GPU memory allocated. - \\"max_memory_allocated\\": The maximum GPU memory allocated during the operation. - \\"memory_reserved\\": The total GPU memory reserved. - \\"max_memory_reserved\\": The maximum GPU memory reserved during the operation. Raises: - RuntimeError: If the system does not have more than one GPU. # Implement the function here following the steps outlined in the question. pass ``` # Constraints - You should ensure that the code runs only if CUDA is available; if CUDA is not available, the function should raise an Exception. - The function should run the tensor operation on a separate CUDA stream from the PyTorch default stream. - Use nvtx to mark the start and end of the tensor operation for profiling purposes. - The memory usage statistics should be fetched after the completion of the tensor operation. # Steps to Implement 1. Check if CUDA is available using `torch.cuda.is_available()`. If not, raise an exception. 2. Check the number of available devices using `torch.cuda.device_count()`. If there is only one device, raise a `RuntimeError`. 3. Set the current device to the second GPU (index 1) using `torch.cuda.set_device()`. 4. Initialize the random number generator with the given seed using `torch.manual_seed()`. 5. Create two random tensors of size `(1000, 1000)` using `torch.randn()`, ensuring they are on the GPU. 6. Create a new CUDA stream using `torch.cuda.Stream()`. 7. Use `nvtx.range_push(\'Tensor Operation\')` before starting the tensor operation and `nvtx.range_pop()` after completing the operation. 8. In the created stream, perform an element-wise addition of the two tensors. 9. Wait for the stream to complete using `stream.synchronize()`. 10. Fetch and return the memory usage statistics using `torch.cuda.memory_stats()`. # Example ```python try: stats = cuda_operations_with_pytorch(seed=42) print(stats) except Exception as e: print(f\\"An error occurred: {e}\\") ``` This example initializes the CUDA context, sets the current device, initializes the RNG, performs a tensor operation on a separate CUDA stream, and reports memory usage statistics.","solution":"import torch import torch.cuda.nvtx as nvtx def cuda_operations_with_pytorch(seed: int) -> dict: Performs a sequence of CUDA operations using PyTorch\'s CUDA API. Args: - seed (int): The seed to initialize the random number generator. Returns: - dict: A dictionary containing memory usage statistics after the tensor operation. The keys of the dictionary should be: - \\"memory_allocated\\": The total GPU memory allocated. - \\"max_memory_allocated\\": The maximum GPU memory allocated during the operation. - \\"memory_reserved\\": The total GPU memory reserved. - \\"max_memory_reserved\\": The maximum GPU memory reserved during the operation. Raises: - RuntimeError: If the system does not have more than one GPU. if not torch.cuda.is_available(): raise RuntimeError(\\"CUDA is not available.\\") if torch.cuda.device_count() <= 1: raise RuntimeError(\\"The system does not have more than one GPU.\\") # Set the current device to the second GPU (index 1) torch.cuda.set_device(1) # Initialize the random number generator with the given seed torch.manual_seed(seed) # Create two random tensors on the GPU tensor_a = torch.randn((1000, 1000), device=\'cuda\') tensor_b = torch.randn((1000, 1000), device=\'cuda\') # Create a new CUDA stream stream = torch.cuda.Stream() # NVTX marking nvtx.range_push(\'Tensor Operation\') with torch.cuda.stream(stream): # Perform element-wise addition tensor_c = tensor_a + tensor_b # Wait for the stream to complete stream.synchronize() # End NVTX range nvtx.range_pop() # Get memory statistics memory_stats = torch.cuda.memory_stats() return { \\"memory_allocated\\": memory_stats[\\"allocated_bytes.all.current\\"], \\"max_memory_allocated\\": memory_stats[\\"allocated_bytes.all.peak\\"], \\"memory_reserved\\": memory_stats[\\"reserved_bytes.all.current\\"], \\"max_memory_reserved\\": memory_stats[\\"reserved_bytes.all.peak\\"] }"},{"question":"Objective: Demonstrate your understanding of Python\'s garbage collector by writing a function that manipulates and retrieves information about garbage collection. Task: Write a function `garbage_collection_info` that performs the following steps: 1. Disable automatic garbage collection. 2. Create a reference cycle using two objects. 3. Manually trigger the garbage collector to collect the highest generation (generation 2). 4. Enable automatic garbage collection. 5. Retrieve and return the following information as a dictionary: - Whether automatic garbage collection is enabled. - The number of unreachable objects found during the manual collection. - The current garbage collection thresholds. - The current collection counts for each generation. - The list of all objects currently tracked by the garbage collector. Specifications: - The function should have the following signature: ```python def garbage_collection_info() -> dict: ``` - Return the result as a dictionary with the following keys and expected value types: ```python { \\"auto_gc_enabled\\": bool, # Whether automatic garbage collection is enabled \\"unreachable_objects\\": int, # Number of unreachable objects found during manual collection \\"gc_thresholds\\": tuple, # The current garbage collection thresholds \\"gc_counts\\": tuple, # The current collection counts for each generation \\"tracked_objects\\": list # List of all objects currently tracked by the garbage collector } ``` - You may assume that the `gc` module is already imported. Example Usage: ```python result = garbage_collection_info() print(result[\\"auto_gc_enabled\\"]) # Expected output: True print(result[\\"unreachable_objects\\"]) # Expected output: 2 (number might vary) print(result[\\"gc_thresholds\\"]) # Expected output: (700, 10, 10) print(result[\\"gc_counts\\"]) # Expected output: (1, 1, 1) (numbers might vary) print(len(result[\\"tracked_objects\\"])) # Expected output: a large number (number might vary) ``` Constraints: - You should manually verify that the created reference cycle leads to uncollectable objects which are detected by the garbage collector.","solution":"import gc def garbage_collection_info() -> dict: # Disable automatic garbage collection gc.disable() # Create a reference cycle class Cycle: def __init__(self): self.ref = None obj1 = Cycle() obj2 = Cycle() obj1.ref = obj2 obj2.ref = obj1 # Trigger the garbage collector to collect the highest generation unreachable_objects = gc.collect(2) # Enable automatic garbage collection gc.enable() # Retrieve and return the information return { \\"auto_gc_enabled\\": gc.isenabled(), \\"unreachable_objects\\": unreachable_objects, \\"gc_thresholds\\": gc.get_threshold(), \\"gc_counts\\": gc.get_count(), \\"tracked_objects\\": gc.get_objects() }"},{"question":"# Asynchronous HTTP Server Assessment Implement an Asynchronous HTTP Server using `asyncio` streams that can handle multiple client connections simultaneously. The server should be capable of accepting HTTP GET requests and responding with a simple HTML page. Clients can connect, request an HTML page, and receive the response. Specifications: 1. Implement a function `start_http_server(host: str, port: int)` which starts the server. 2. Implement a coroutine `handle_client(reader: asyncio.StreamReader, writer: asyncio.StreamWriter)` to handle individual client connections. 3. The `handle_client` coroutine should: - Read the client\'s HTTP request. - Acknowledge only GET requests; for other types of requests, respond with a \\"405 Method Not Allowed\\" status code. - For a GET request, respond with a simple HTML page (`<html><body><h1>Hello, World!</h1></body></html>`). - Close the connection after sending the response. Constraints: - Do not use external libraries; only use the Python standard library. - Ensure the server handles multiple connections asynchronously. - Implement appropriate error handling for invalid HTTP methods and malformed requests. Performance Requirement: - The server should efficiently handle at least 10 simultaneous connections. # Example: ```python import asyncio async def handle_client(reader, writer): # Your implementation here pass async def start_http_server(host, port): server = await asyncio.start_server(handle_client, host, port) async with server: await server.serve_forever() if __name__ == \\"__main__\\": asyncio.run(start_http_server(\'127.0.0.1\', 8080)) ``` Your implementation should correctly interpret HTTP GET requests and manage multiple client connections concurrently. # Testing your server: You can use `curl` or a web browser to test your server. For example, running `curl http://127.0.0.1:8080` should return the simple HTML page. ```sh curl http://127.0.0.1:8080 # Expected output: <html><body><h1>Hello, World!</h1></body></html> ```","solution":"import asyncio async def handle_client(reader, writer): try: request = (await reader.read(1024)).decode(\'utf-8\') if request.startswith(\'GET\'): response = \'HTTP/1.1 200 OKrnContent-Type: text/htmlrnrn<html><body><h1>Hello, World!</h1></body></html>rn\' else: response = \'HTTP/1.1 405 Method Not AllowedrnContent-Type: text/plainrnrn405 Method Not Allowedrn\' writer.write(response.encode(\'utf-8\')) await writer.drain() except Exception as e: print(f\'Error handling client: {e}\') finally: writer.close() await writer.wait_closed() async def start_http_server(host, port): server = await asyncio.start_server(handle_client, host, port) async with server: await server.serve_forever() if __name__ == \\"__main__\\": asyncio.run(start_http_server(\'127.0.0.1\', 8080))"},{"question":"**Question:** You are given a dataset of penguins and tasked with creating a series of visualizations using seaborn to demonstrate an understanding of distribution plots and customization options within seaborn. 1. **Histogram and KDE together**: Create a distribution plot showing the histogram of the `flipper_length_mm` variable of penguins. Overlay a KDE plot onto this histogram. Ensure you differentiate the histogram and KDE using transparency (alpha) and color. 2. **Bivariate Distribution with Conditional Subsets**: Create a bivariate KDE plot (2D KDE) visualizing the relation between `bill_length_mm` and `bill_depth_mm` variables. Use different colors to distinguish between different `species`. Also, add contours to help visualize the density levels. 3. **Facet Grid**: Use a facet grid to plot univariate histograms of `flipper_length_mm` conditioned on `species` and further separated by `sex`. 4. **ECDF Plot**: Plot the ECDF of the `body_mass_g` variable and distinguish the plot by `species`. Customize the plot by adding rugs along the x-axis. # Constraints: 1. Use the seaborn library for all plots. 2. Ensure each plot has proper titles and labels for axes. 3. Maintain readability in plots: avoid overlapping data representations and ensure color differences are clear. # Input: - A dataset named `penguins` which can be loaded using seaborn\'s `load_dataset` function. # Output: - Four separate visualizations as specified above. Here is a sample code to get you started with loading the dataset: ```python import seaborn as sns import matplotlib.pyplot as plt # Load the penguins dataset penguins = sns.load_dataset(\\"penguins\\") # Your visualization code here ``` **Notes**: - Make use of parameters such as `color`, `alpha`, `hue`, `kind`, `element`, `multiple`, etc., while designing the plots as specified. - Ensure the legends are properly displayed for distinguishing different subsets.","solution":"import seaborn as sns import matplotlib.pyplot as plt # Load the penguins dataset penguins = sns.load_dataset(\\"penguins\\") def plot_histogram_kde(): Creates a plot showing the histogram and KDE of flipper_length_mm. plt.figure(figsize=(10, 6)) sns.histplot(penguins[\'flipper_length_mm\'], kde=True, color=\'blue\', alpha=0.5) sns.kdeplot(penguins[\'flipper_length_mm\'], color=\'red\', lw=2) plt.title(\'Histogram and KDE of Flipper Length\') plt.xlabel(\'Flipper Length (mm)\') plt.ylabel(\'Density\') plt.show() def plot_bivariate_kde(): Creates a bivariate KDE plot of bill_length_mm vs bill_depth_mm colored by species. plt.figure(figsize=(10, 6)) sns.kdeplot(data=penguins, x=\'bill_length_mm\', y=\'bill_depth_mm\', hue=\'species\', fill=True, common_norm=False) plt.title(\'Bivariate KDE of Bill Length and Bill Depth by Species\') plt.xlabel(\'Bill Length (mm)\') plt.ylabel(\'Bill Depth (mm)\') plt.show() def plot_facet_grid(): Creates a facet grid of histograms of flipper_length_mm conditioned on species and sex. g = sns.FacetGrid(penguins, row=\'species\', col=\'sex\', margin_titles=True) g.map(plt.hist, \'flipper_length_mm\', bins=20, color=\'purple\', alpha=0.7) g.add_legend() g.set_axis_labels(\'Flipper Length (mm)\', \'Count\') g.set_titles(col_template=\\"{col_name}\\", row_template=\\"{row_name}\\") plt.show() def plot_ecdf(): Creates an ECDF plot of body_mass_g colored by species with rugs. plt.figure(figsize=(10, 6)) sns.ecdfplot(data=penguins, x=\'body_mass_g\', hue=\'species\', palette=\'rainbow\') sns.rugplot(data=penguins, x=\'body_mass_g\', hue=\'species\', palette=\'rainbow\', height=0.1) plt.title(\'ECDF of Body Mass by Species\') plt.xlabel(\'Body Mass (g)\') plt.ylabel(\'ECDF\') plt.legend(title=\'Species\') plt.show()"},{"question":"# Composite Special Function Implementation **Objective:** Implement a composite function using PyTorch\'s `torch.special` module to calculate a complex mathematical expression. **Question:** Write a Python function `composite_special_function(x: torch.Tensor) -> torch.Tensor` which takes a 1D tensor of real numbers `x` and returns a tensor where each element is calculated using the following composite function: [ f(x) = text{erf}(x) + text{expit}(x) + log(1 + text{bessel_j0}(x)) ] **Function Definition:** ```python import torch def composite_special_function(x: torch.Tensor) -> torch.Tensor: # Your code here pass ``` **Constraints:** - Do not use explicitly loops (i.e., use vectorized operations). - Ensure that the function works efficiently with large tensors. - All input will be valid real numbers. **Input:** - `x`: A PyTorch tensor of shape `(n,)` representing n real numbers. **Output:** - Returns a PyTorch tensor of shape `(n,)` where each element ( y_i ) represents ( f(x_i) ). **Example Usage:** ```python import torch # Example input x = torch.tensor([0.5, 1.0, 1.5, 2.0]) # Expected output calculated based on given formula output = composite_special_function(x) # Print the output print(output) ``` **Explanation:** - For `x = 0.5`, the output should be calculated as: - erf(0.5) + expit(0.5) + log(1 + bessel_j0(0.5)) - Similarly for other elements. **Notes:** - You may refer to the PyTorch documentation for the `torch.special` module if needed: [PyTorch Special Functions](https://pytorch.org/docs/stable/special.html) The implementation of this function requires a proper understanding of how to use and combine the special functions provided by the PyTorch library in a vectorized manner.","solution":"import torch def composite_special_function(x: torch.Tensor) -> torch.Tensor: Calculate the composite function f(x) = erf(x) + expit(x) + log(1 + bessel_j0(x)) :param x: 1D tensor of real numbers :return: 1D tensor where each element is calculated based on the given formula erf = torch.special.erf(x) expit = torch.special.expit(x) bessel_j0 = torch.special.i0(x) result = erf + expit + torch.log1p(bessel_j0) return result"},{"question":"# Custom PyTorch Autograd Function and Saved Tensor Hooks In this exercise, you are required to implement a custom autograd Function that performs a specific forward operation and uses saved tensor hooks to modify the tensor saving process. Task 1. Implement a custom autograd Function called `CustomSquare`. This function should: - Perform the square of its input tensor during the forward pass. - Save the input tensor for the backward pass. 2. Register hooks that: - Save tensors that have size >= 3 to a temporary file during the forward pass. - Retrieve tensors from the temporary file during the backward pass. 3. Ensure that the backward pass correctly computes the gradient using the saved tensor. Requirements - The custom autograd Function should inherit from `torch.autograd.Function`. - Forward and backward methods should be implemented correctly. - Hooks need to be registered properly to save and load tensors. - The tensors saved to the disk should be named uniquely, and temporary files should be cleaned up after usage. Input and Output Formats - **Input**: A tensor `x` of arbitrary size. - **Output**: A tensor `y` which is the square of the input tensor `x`. Example ```python import torch import torch.nn as nn import os import uuid class SelfDeletingTempFile: def __init__(self): self.name = os.path.join(\\"/tmp\\", str(uuid.uuid4())) def __del__(self): os.remove(self.name) def pack_hook(tensor): if tensor.numel() < 3: return tensor temp_file = SelfDeletingTempFile() torch.save(tensor, temp_file.name) return temp_file def unpack_hook(temp_file): if isinstance(temp_file, torch.Tensor): return temp_file return torch.load(temp_file.name) class CustomSquare(torch.autograd.Function): @staticmethod def forward(ctx, x): ctx.save_for_backward(x) return x * x @staticmethod def backward(ctx, grad_output): x, = ctx.saved_tensors return 2 * x * grad_output # Register hooks to be used in the custom autograd Function x = torch.randn(5, requires_grad=True) y = CustomSquare.apply(x) pack_hook_register = y.grad_fn._raw_saved_self.register_hooks(pack_hook, unpack_hook) # Perform backward pass y.sum().backward() print(x.grad) ``` # Constraints - Your implementation should correctly handle tensors of arbitrary sizes. - Ensure that the temporary files used for saving tensors are properly managed and deleted.","solution":"import torch import os import uuid class SelfDeletingTempFile: def __init__(self): self.name = os.path.join(\\"/tmp\\", str(uuid.uuid4())) def __del__(self): try: os.remove(self.name) except FileNotFoundError: pass def pack_hook(tensor): if tensor.numel() < 3: return tensor temp_file = SelfDeletingTempFile() torch.save(tensor, temp_file.name) return temp_file def unpack_hook(temp_file): if isinstance(temp_file, torch.Tensor): return temp_file return torch.load(temp_file.name) class CustomSquare(torch.autograd.Function): @staticmethod def forward(ctx, x): ctx.save_for_backward(x) return x * x @staticmethod def backward(ctx, grad_output): x, = ctx.saved_tensors return 2 * x * grad_output # Register hooks to be used in the custom autograd Function x = torch.randn(5, requires_grad=True) y = CustomSquare.apply(x) # Register hooks for saving/loading tensors for saved_tensor in y.grad_fn.saved_tensors: saved_tensor.register_hook(pack_hook) saved_tensor.register_hook(unpack_hook)"},{"question":"Advanced Python Debugging Objective: Demonstrate your understanding of the Python Debugger (`pdb` module) by creating and debugging a Python script. The task will evaluate your ability to use breakpoints, inspect stack frames, evaluate expressions, and handle post-mortem debugging. Task Description: You are given a faulty Python program that processes a list of integers to return the sum of squares of even numbers. The program has an intentional bug that causes it to raise an exception. Your task is to identify and fix the bug using the `pdb` module. 1. **Faulty Program**: ```python def process_numbers(numbers): result = 0 for num in numbers: if num % 2 == 0: result += num ** 2 else: result -= \'NaN\' # Intentional bug: cannot subtract string from int return result def main(): sample_data = [1, 2, 3, 4, 5, 6] print(\\"The result is:\\", process_numbers(sample_data)) if __name__ == \\"__main__\\": main() ``` 2. **Tasks**: - **Part 1**: Use the `set_trace()` method to set a breakpoint at the start of the `process_numbers` function. - **Part 2**: Run the program under the debugger\'s control and step through the code to identify the cause of the exception. - **Part 3**: Inspect the values of relevant variables, including `num` and `result`, at various points in the execution. - **Part 4**: Evaluate expressions using the `p` command to understand how the variables are changing. - **Part 5**: Handle the exception using post-mortem debugging (`pm()`) to inspect the state of the program when the exception occurs. - **Part 6**: Fix the bug in the code and verify that the program runs correctly without exceptions. 3. **Submission Requirements**: - Provide the modified source code with the fixed bug and the `set_trace()` call. - Include the sequence of Pdb commands you used during the debugging process. - Summarize the insights you gained from inspecting the stack frames and variable states. Expected Output: Upon fixing the bug, the output of the program should be: ``` The result is: 56 ``` Constraints: - You must use the `pdb` module for debugging; other debugging methods (e.g., `print` statements) are not allowed. - Document your debugging process clearly, explaining each step and the conclusions drawn. - Submit your modified code and a step-by-step report of your debugging session. Good luck!","solution":"import pdb def process_numbers(numbers): pdb.set_trace() # Part 1: Setting a breakpoint at the start of the function result = 0 for num in numbers: if num % 2 == 0: result += num ** 2 else: result -= \'NaN\' # Intentional bug: cannot subtract string from int return result def main(): sample_data = [1, 2, 3, 4, 5, 6] print(\\"The result is:\\", process_numbers(sample_data)) if __name__ == \\"__main__\\": main() # Part 2 to Part 4: Using pdb to identify the cause of the exception: # The Pdb commands used: # > b process_numbers # set a breakpoint at process_numbers # > c # continue execution # > s # step through the code # > p num # print the value of num # > p result # print the value of result # > n # next line # > p # evaluate an expression # Part 5: Handling the exception using post-mortem debugging: # After exception occurs: # > pm() # enter post-mortem debugging # > p num # print the last value of num before the exception # > p result # print the last value of result before the exception # > q # quit the debugger # Part 6: Fix the bug def process_numbers_fixed(numbers): result = 0 for num in numbers: if num % 2 == 0: result += num ** 2 else: result += 0 # Fix: ignoring odd numbers return result def main_fixed(): sample_data = [1, 2, 3, 4, 5, 6] print(\\"The result is:\\", process_numbers_fixed(sample_data)) if __name__ == \\"__main__\\": main_fixed()"},{"question":"You are tasked with creating a script that processes large sets of data. Initially, you will generate random integers and write them to a temporary file. Next, the script should read from this file, process the data (in this case, sort the integers), and write the sorted results to a new temporary file. Finally, the script should print the names and the first ten integers from the sorted file without leaving any temporary files or directories on the disk when done. Implement a function `process_large_data()` to achieve this. The function should perform the following tasks: 1. Create a temporary file to store random integers. 2. Write 1,000,000 random integers (each between 1 and 1,000,000) to this temporary file. 3. Read the integers from the temporary file, sort them in ascending order, and write the sorted integers to a new temporary file. 4. Print the names of both temporary files and the first ten integers from the sorted file. 5. Ensure no temporary files or directories remain on the disk after the function completes. You must use the `tempfile` module to handle temporary files securely. Your solution should ensure proper cleanup of all temporary resources. Function Signature: ```python import tempfile import random def process_large_data(): # Your code here # Example of output: # Temporary file with random integers: /tmp/tmpXXXXXX # Temporary file with sorted integers: /tmp/tmpYYYYYY # First ten sorted integers: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] ``` Constraints: - You may not use other external libraries except for `tempfile` and `random`. - The function should handle all necessary cleanups implicitly or explicitly.","solution":"import tempfile import random def process_large_data(): with tempfile.NamedTemporaryFile(delete=False) as temp_file: temp_file_name = temp_file.name random_integers = [random.randint(1, 1000000) for _ in range(1000000)] temp_file.write(\'n\'.join(map(str, random_integers)).encode()) with tempfile.NamedTemporaryFile(delete=False) as sorted_file: sorted_file_name = sorted_file.name sorted_integers = sorted(random_integers) # Sort the integers sorted_file.write(\'n\'.join(map(str, sorted_integers)).encode()) with open(sorted_file_name, \'r\') as f: first_ten_sorted = [int(next(f)) for _ in range(10)] print(f\\"Temporary file with random integers: {temp_file_name}\\") print(f\\"Temporary file with sorted integers: {sorted_file_name}\\") print(f\\"First ten sorted integers: {first_ten_sorted}\\") # Clean up temporary files import os os.remove(temp_file_name) os.remove(sorted_file_name)"},{"question":"# Question: Implement a Thread-Safe Task Scheduler Using PriorityQueue You are required to create a thread-safe task scheduler that manages tasks based on their priorities. Implement a class `TaskScheduler` that uses the `queue.PriorityQueue` to schedule and execute tasks. The tasks should be represented as tuples of the form `(priority, task_function, *args)`, where `priority` is an integer, `task_function` is a callable, and `*args` are the arguments to be passed to the task function. The scheduler should have the following functionalities: 1. **Add Task**: Method to add a new task to the scheduler. 2. **Execute Tasks**: Method to execute tasks based on their priorities. Specifications: 1. You should use the `queue.PriorityQueue` for task management. 2. Tasks should be executed by threads in the order of their priorities. 3. If no tasks are available, the `execute_tasks` method should wait until a task is added. Constraints: - You may assume that tasks have unique priorities. Input: - `add_task(priority: int, task_function: Callable, *args: Any) -> None`: Adds a task to the scheduler. - `execute_tasks() -> None`: Executes tasks based on their priorities using threads. Example Usage: ```python import threading from typing import Callable, Any import queue class TaskScheduler: def __init__(self): self.task_queue = queue.PriorityQueue() def add_task(self, priority: int, task_function: Callable, *args: Any) -> None: self.task_queue.put((priority, task_function, args)) def execute_tasks(self) -> None: while not self.task_queue.empty(): priority, task_function, args = self.task_queue.get() task_thread = threading.Thread(target=task_function, args=args) task_thread.start() task_thread.join() ``` Example Usage: ```python def example_task(task_name): print(f\\"Executing {task_name}\\") # Initialize the task scheduler scheduler = TaskScheduler() # Add tasks with varying priorities scheduler.add_task(2, example_task, \\"Task 2\\") scheduler.add_task(1, example_task, \\"Task 1\\") scheduler.add_task(3, example_task, \\"Task 3\\") # Execute tasks scheduler.execute_tasks() # Output: # Executing Task 1 # Executing Task 2 # Executing Task 3 ``` Your task is to fill in the implementation of the `TaskScheduler` class to handle task scheduling and execution as described.","solution":"import threading from typing import Callable, Any import queue class TaskScheduler: def __init__(self): self.task_queue = queue.PriorityQueue() self.lock = threading.Lock() def add_task(self, priority: int, task_function: Callable, *args: Any) -> None: with self.lock: self.task_queue.put((priority, task_function, args)) def execute_tasks(self) -> None: while True: with self.lock: if self.task_queue.empty(): break priority, task_function, args = self.task_queue.get() task_thread = threading.Thread(target=task_function, args=args) task_thread.start() task_thread.join()"},{"question":"# Question: Converting and Using TorchScript Models in PyTorch You are tasked with writing a function that converts a given PyTorch model to TorchScript, saves the TorchScript model to disk, and then loads the saved model for future inference. Your function should take a PyTorch model and an example input, convert the model to TorchScript, save the TorchScript model to a file, and finally, load the model back from the file. You should then run inference on a new input using the loaded model and return the output. **Input:** 1. `model` - A PyTorch model (an instance of `torch.nn.Module`). 2. `example_input` - An example input tensor to trace the model. 3. `new_input` - A new input tensor on which to perform inference using the loaded model. 4. `file_path` - The file path (string) where the TorchScript model should be saved. **Output:** - The output tensor from the model after running inference on `new_input`. **Constraints and Requirements:** 1. Assume `example_input` and `new_input` are valid input tensors for the given model. 2. You must use TorchScript\'s `trace` or `script` function to convert the model. 3. Save the traced or scripted model to the specified `file_path`. 4. Load the model back from the file and run inference on `new_input`. 5. Ensure the function is efficient and handles common exceptions that might be thrown during the save/load process. **Function Signature:** ```python def torchscript_convert_and_infer(model: torch.nn.Module, example_input: torch.Tensor, new_input: torch.Tensor, file_path: str) -> torch.Tensor: pass ``` # Example: ```python import torch import torch.nn as nn # Example model class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.linear = nn.Linear(10, 5) def forward(self, x): return self.linear(x) # Example model instance model = SimpleModel() # Example input tensor to trace the model example_input = torch.randn(1, 10) # New input tensor for inference new_input = torch.randn(1, 10) # File path to save the TorchScript model file_path = \\"simple_model_scripted.pt\\" # Call the function output = torchscript_convert_and_infer(model, example_input, new_input, file_path) print(output) # Expected output: A tensor with shape torch.Size([1, 5]) ``` **Notes:** - You should test your function by creating a small and simple neural network to ensure correctness. - Make sure to handle any possible exceptions during model saving and loading processes.","solution":"import torch def torchscript_convert_and_infer(model: torch.nn.Module, example_input: torch.Tensor, new_input: torch.Tensor, file_path: str) -> torch.Tensor: try: # Convert the model to TorchScript traced_script_module = torch.jit.trace(model, example_input) # Save the TorchScript model to the specified file path traced_script_module.save(file_path) # Load the TorchScript model from the file loaded_script_module = torch.jit.load(file_path) # Perform inference using the loaded model and new input tensor with torch.no_grad(): output = loaded_script_module(new_input) return output except Exception as e: print(f\\"An error occurred: {e}\\") return None"},{"question":"Objective: Demonstrate your understanding of the seaborn library and its advanced plotting capabilities, specifically focusing on effectively utilizing jitter to improve data point visualizations. Problem Statement: You are given a dataset of penguin measurements including species, body mass (in grams), and flipper length (in mm). Your task is to create a series of plots that effectively use jitter to better visualize the distribution and relationships between these measurements. Input: - Use the `penguins` dataset loaded from seaborn\'s `load_dataset` function. Requirements: 1. Load the `penguins` dataset using seaborn\'s `load_dataset` function. 2. Create the following plots: - A dot plot showing the relationship between species and body mass with a default jitter. - A dot plot using the `width` parameter to control jitter amount for the same relationship as in the previous plot. - A dot plot showing the relationship between body mass and flipper length with jitter controlled by both `x` and `y` parameters. Constraints: - Make sure the plots include meaningful titles and axis labels. - Use appropriate color schemes to differentiate the species in the plots. - Your code should handle cases where the dataset might contain missing values gracefully. Expected Output: Three plots as specified above, respecting the constraints and requirements. Notes: - Use the seaborn.objects module and its functionalities like `so.Plot`, `so.Dots`, and `so.Jitter`. - Refer to seaborn documentation if needed for further details on the usage and customization of these functions. ```python import seaborn.objects as so from seaborn import load_dataset # Load the dataset penguins = load_dataset(\\"penguins\\") # 1. A dot plot showing the relationship between species and body mass with a default jitter. plot1 = ( so.Plot(penguins, x=\\"species\\", y=\\"body_mass_g\\") .add(so.Dots(), so.Jitter()) ) plot1.show() # 2. A dot plot using the `width` parameter to control jitter amount for the same relationship as in the previous plot. plot2 = ( so.Plot(penguins, x=\\"species\\", y=\\"body_mass_g\\") .add(so.Dots(), so.Jitter(width=0.5)) ) plot2.show() # 3. A dot plot showing the relationship between body mass and flipper length with jitter controlled by both `x` and `y` parameters. plot3 = ( so.Plot(penguins, x=\\"body_mass_g\\", y=\\"flipper_length_mm\\") .add(so.Dots(), so.Jitter(x=200, y=5)) ) plot3.show() ```","solution":"import seaborn.objects as so from seaborn import load_dataset def generate_plots(): # Load the dataset penguins = load_dataset(\\"penguins\\") # Handle cases where the dataset might contain missing values penguins = penguins.dropna() # 1. A dot plot showing the relationship between species and body mass with a default jitter. plot1 = ( so.Plot(penguins, x=\\"species\\", y=\\"body_mass_g\\") .add(so.Dots(), so.Jitter()) .label(title=\\"Species vs Body Mass with Default Jitter\\", xlabel=\\"Species\\", ylabel=\\"Body Mass (g)\\") ) plot1.show() # 2. A dot plot using the `width` parameter to control jitter amount for the same relationship as in the previous plot. plot2 = ( so.Plot(penguins, x=\\"species\\", y=\\"body_mass_g\\") .add(so.Dots(), so.Jitter(width=0.5)) .label(title=\\"Species vs Body Mass with Controlled Jitter Width\\", xlabel=\\"Species\\", ylabel=\\"Body Mass (g)\\") ) plot2.show() # 3. A dot plot showing the relationship between body mass and flipper length with jitter controlled by both `x` and `y` parameters. plot3 = ( so.Plot(penguins, x=\\"body_mass_g\\", y=\\"flipper_length_mm\\") .add(so.Dots(), so.Jitter(x=200, y=5)) .label(title=\\"Body Mass vs Flipper Length with Controlled Jitter\\", xlabel=\\"Body Mass (g)\\", ylabel=\\"Flipper Length (mm)\\") ) plot3.show() # Execute the function to generate plots generate_plots()"},{"question":"In this task, you will implement a custom protocol and transport for a simplified HTTP-like server using the asyncio library. This server should be able to handle requests and send appropriate responses over TCP connections. Your implementation should demonstrate a clear understanding of the asyncio transport and protocol mechanisms. # Task 1: Implement the Custom Protocol 1. Create a class `SimpleHttpProtocol` that inherits from `asyncio.Protocol`. 2. Implement the following methods in `SimpleHttpProtocol`: - `connection_made(self, transport)`: Store the transport reference. - `data_received(self, data)`: Parse the incoming data to recognize a simple HTTP GET request. For example, recognize `GET / HTTP/1.1`. - `send_response(self, content)`: Send a response with the given content. - `connection_lost(self, exc)`: Handle the connection closure. The protocol should send a simple HTTP 200 OK response with the content \\"Hello, world!\\" if a GET request is received, or a 404 Not Found response for other types of requests. # Task 2: Set up the Transport 1. Create an asynchronous function `start_server`. 2. Use `loop.create_server` to create a server that listens for incoming connections and uses your `SimpleHttpProtocol` to handle them. 3. Make sure your function starts the server and handles incoming connections in an event loop. # Constraints - Do not use any HTTP libraries; implement the HTTP parsing and response formatting manually. - Your server should handle multiple connections concurrently. - Follow typical HTTP standards in your response formatting. # Input and Output - There are no direct inputs or outputs for this implementation question. The functionality will be tested by running and interacting with your server implementation programmatically. # Evaluation Criteria - Correct implementation of the asyncio `Protocol` and `Transport` classes. - Accurate parsing of incoming HTTP requests and sending correct HTTP responses. - Ability to handle multiple simultaneous connections efficiently. - Code quality, readability, and adherence to Python async programming practices. # Example The following is an example of how your server might be tested: ```python import asyncio import socket async def test_server(): loop = asyncio.get_running_loop() server = await start_server() # Your function to start the server try: reader, writer = await asyncio.open_connection(\'127.0.0.1\', 8080) request = \\"GET / HTTP/1.1rnHost: localhostrnrn\\" writer.write(request.encode()) await writer.drain() response = await reader.read(100) print(response.decode()) # Expected to print the HTTP 200 OK response writer.close() await writer.wait_closed() finally: server.close() await server.wait_closed() asyncio.run(test_server()) ```","solution":"import asyncio class SimpleHttpProtocol(asyncio.Protocol): def connection_made(self, transport): self.transport = transport def data_received(self, data): message = data.decode() if message.startswith(\\"GET / \\"): self.send_response(\\"Hello, world!\\") else: self.send_error(404) def send_response(self, content): response = ( \\"HTTP/1.1 200 OKrn\\" \\"Content-Type: text/plainrn\\" \\"Content-Length: {}rn\\" \\"rn\\" \\"{}\\" ).format(len(content), content) self.transport.write(response.encode()) def send_error(self, code): content = \\"Not Found\\" if code == 404 else \\"Internal Server Error\\" response = ( \\"HTTP/1.1 {}rn\\" \\"Content-Type: text/plainrn\\" \\"Content-Length: {}rn\\" \\"rn\\" \\"{}\\" ).format(code, len(content), content) self.transport.write(response.encode()) def connection_lost(self, exc): self.transport.close() async def start_server(): loop = asyncio.get_running_loop() server = await loop.create_server( lambda: SimpleHttpProtocol(), \'127.0.0.1\', 8080 ) await server.start_serving() return server"},{"question":"# Seaborn Coding Assessment Question Objective: To assess your understanding of seaborn for creating and customizing categorical plots using provided dataset. Problem Statement: You are given a dataset \\"penguins\\" from seaborn\'s in-built datasets. Write a function `plot_penguin_data` that processes this data and creates a set of visualizations as specified below. Tasks: 1. **Load the Dataset**: Load the penguins dataset using `sns.load_dataset(\\"penguins\\")`. 2. **Create Basic Plots**: a. Create a jittered strip plot showing the distribution of penguin `flipper_length_mm` across different `species`. b. Create a box plot to show the `body_mass_g` distribution across different `species`. 3. **Customize the Plots**: Customize the strip plot and box plot created in step 2: a. For the strip plot, use `hue` to differentiate sexes (`sex`). b. For the box plot, use `hue` to differentiate island (`island`), and set `palette` to \\"coolwarm\\". 4. **Advanced Plot**: Combine a violin plot for `flipper_length_mm` and a swarm plot for `body_mass_g` across different `species`: - Use `hue` for the sex of the penguins (`sex`). - Set the violin plot color to `.85` (a shade of grey) and disable the inner violin plot details. 5. **Facet Grid**: Create a FacetGrid with bar plots representing the average `body_mass_g` for different `species`, facetted by `island` and `sex`. - Use `height=4` and `aspect=0.6`. - Set y-axis limit to `(2000, 6000)`. - Set y-axis label to \\"Average Body Mass (g)\\". Function Signature: ```python def plot_penguin_data(): pass ``` Expected Output: The function should not return anything. It should display the plots as described. Constraints: - Use only seaborn and matplotlib for visualization. - Ensure all plots are properly labeled and legends are included where appropriate. Example: The `plot_penguin_data` function, when executed, should display the required plots as per the specifications.","solution":"import seaborn as sns import matplotlib.pyplot as plt def plot_penguin_data(): # Load the dataset penguins = sns.load_dataset(\\"penguins\\") # Create jittered strip plot for flipper length plt.figure(figsize=(10, 6)) sns.stripplot(x=\'species\', y=\'flipper_length_mm\', hue=\'sex\', data=penguins, jitter=True) plt.title(\\"Distribution of Flipper Length by Species and Sex\\") plt.xlabel(\\"Species\\") plt.ylabel(\\"Flipper Length (mm)\\") plt.legend(title=\'Sex\') plt.show() # Create box plot for body mass plt.figure(figsize=(10, 6)) sns.boxplot(x=\'species\', y=\'body_mass_g\', hue=\'island\', data=penguins, palette=\'coolwarm\') plt.title(\\"Distribution of Body Mass by Species and Island\\") plt.xlabel(\\"Species\\") plt.ylabel(\\"Body Mass (g)\\") plt.legend(title=\'Island\') plt.show() # Combined violin and swarm plot plt.figure(figsize=(12, 8)) sns.violinplot(x=\'species\', y=\'flipper_length_mm\', hue=\'sex\', data=penguins, color=\\".85\\", inner=None) sns.swarmplot(x=\'species\', y=\'body_mass_g\', hue=\'sex\', data=penguins, dodge=True) plt.title(\\"Combined Violin and Swarm Plot by Species and Sex\\") plt.xlabel(\\"Species\\") plt.ylabel(\\"Values (Flipper length / Body mass)\\") plt.legend(title=\'Sex\') plt.show() # FacetGrid with bar plots g = sns.FacetGrid(penguins, col=\'island\', row=\'sex\', margin_titles=True, height=4, aspect=0.6) g.map(sns.barplot, \'species\', \'body_mass_g\', ci=None, palette=\'muted\', estimator=sum) g.set_axis_labels(\\"Species\\", \\"Average Body Mass (g)\\") g.set(ylim=(2000, 6000)) g.add_legend() plt.show()"},{"question":"# Question: Implement Asynchronous Model Training with PyTorch Multiprocessing Context You are tasked with implementing an asynchronous model training function using PyTorch\'s `torch.multiprocessing` library. The goal is to efficiently train a model using multiple processes, ensuring proper resource management and avoiding common pitfalls such as deadlocks and CPU oversubscription. Task 1. Implement a function `async_model_training` to asynchronously train a given PyTorch model using multiple processes. 2. Your implementation should include: - Proper initialization of the PyTorch model to be shared between processes. - Spawn multiple subprocesses to perform training using the `spawn` or `forkserver` method. - Ensure that resources are properly allocated to avoid CPU oversubscription. - Guard any global statements for safe multiprocessing. Function Signature ```python import torch import torch.multiprocessing as mp def async_model_training(model: torch.nn.Module, num_processes: int, device: str, dataset, dataloader_kwargs, seed: int, lr: float, momentum: float, epochs: int) -> None: Trains a PyTorch model asynchronously using multiple processes. Parameters: - model (torch.nn.Module): The PyTorch model to be trained. - num_processes (int): The number of processes to use. - device (str): The device to use for training (e.g., \\"cpu\\" or \\"cuda:0\\"). - dataset: The dataset used for training. - dataloader_kwargs: Arguments for the DataLoader. - seed (int): Seed for randomization. - lr (float): Learning rate. - momentum (float): Momentum for optimizer. - epochs (int): Number of epochs to train. pass ``` Guidelines: 1. **Initialize Model**: Ensure that the model is shared across all processes properly. 2. **Spawn Processes**: Use either `spawn` or `forkserver` for process start method. 3. **Avoid CPU Oversubscription**: Calculate appropriate number of threads per process and set `torch.set_num_threads` in each subprocess. 4. **Guard Global Statements**: Use `if __name__ == \\"__main__\\"` to protect global statements to ensure safe multi-processing execution. Example Usage ```python if __name__ == \\"__main__\\": from torchvision import datasets, transforms from torch.utils.data import DataLoader # Define a simple model class SimpleModel(torch.nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.fc = torch.nn.Linear(28 * 28, 10) def forward(self, x): x = x.view(-1, 28 * 28) return self.fc(x) model = SimpleModel() num_processes = 4 device = \\"cpu\\" dataset = datasets.MNIST(\'./data\', train=True, download=True, transform=transforms.ToTensor()) dataloader_kwargs = {\'batch_size\': 64, \'shuffle\': True} seed = 42 lr = 0.01 momentum = 0.9 epochs = 10 async_model_training(model, num_processes, device, dataset, dataloader_kwargs, seed, lr, momentum, epochs) ``` Notes - If using CUDA, ensure proper handling and starting of processes as described in the documentation. - The training process should update the model\'s shared parameters asynchronously.","solution":"import torch import torch.multiprocessing as mp from torch.utils.data import DataLoader import torch.nn.functional as F import torch.optim as optim def train(rank, model, device, dataset, dataloader_kwargs, seed, lr, momentum, epochs): # Set seed for reproducibility torch.manual_seed(seed + rank) # Set number of threads per process torch.set_num_threads(1) # Move model to correct device model.to(device) # Initialize DataLoader train_loader = DataLoader(dataset, **dataloader_kwargs) # Define loss function and optimizer optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum) for epoch in range(epochs): model.train() for batch_idx, (data, target) in enumerate(train_loader): data, target = data.to(device), target.to(device) optimizer.zero_grad() output = model(data) loss = F.cross_entropy(output, target) loss.backward() optimizer.step() def async_model_training(model: torch.nn.Module, num_processes: int, device: str, dataset, dataloader_kwargs, seed: int, lr: float, momentum: float, epochs: int) -> None: # Ensure it\'s within a protected context for multi-processing if __name__ == \\"__main__\\": # Share the model parameters across processes model.share_memory() # Set start method for multi-processing safe use of CUDA mp.set_start_method(\\"spawn\\") # Launch multiple processes processes = [] for rank in range(num_processes): p = mp.Process(target=train, args=(rank, model, device, dataset, dataloader_kwargs, seed, lr, momentum, epochs)) p.start() processes.append(p) for p in processes: p.join()"},{"question":"# Advanced Python Coding Assessment Objective Design an advanced file management system using the `os` module that supports the following functionalities: 1. **Listing Files:** List all files in a directory, non-recursively. 2. **File Operations:** Perform common file operations like reading, writing, and deleting files. 3. **Process Management:** Spawn a subprocess that executes an external command and captures its output. 4. **Environment Variables Handling:** Manipulate environment variables to modify the behavior of subprocesses. Requirements - Implement the following functions: 1. `list_files(directory: str) -> list[str]`: Given a directory path, return a list of all filenames in that directory. 2. `read_file(filepath: str) -> str`: Given a file path, return the content of the file as a string. 3. `write_file(filepath: str, content: str) -> None`: Given a file path and content, write the content to the file. 4. `delete_file(filepath: str) -> None`: Given a file path, delete the file. 5. `run_subprocess(command: str, env: dict = None) -> str`: Run a command in a subprocess, optionally set environment variables, and return the command output. Constraints & Limitations 1. Ensure directory paths and file paths are valid and accessible. 2. Handle any OSError exceptions that might occur during file or directory operations. 3. Use appropriate methods from the `os` module to accomplish the tasks. 4. Ensure subprocess outputs are captured correctly. Sample Usage ```python # Listing all files in the current directory files = list_files(\\".\\") print(files) # Reading a file content = read_file(\\"sample.txt\\") print(content) # Writing to a file write_file(\\"output.txt\\", \\"Hello, World!\\") # Deleting a file delete_file(\\"output.txt\\") # Running a subprocess with environment variables output = run_subprocess(\\"echo MY_VAR\\", env={\\"MY_VAR\\": \\"Hello World\\"}) print(output) # Should print \\"Hello World\\" ``` Note - Test your solution thoroughly to ensure all functionalities are working as intended.","solution":"import os import subprocess def list_files(directory: str) -> list[str]: List all files in a directory, non-recursively. try: return [file for file in os.listdir(directory) if os.path.isfile(os.path.join(directory, file))] except OSError as e: raise e def read_file(filepath: str) -> str: Returns the content of the file as a string. try: with open(filepath, \'r\') as file: return file.read() except OSError as e: raise e def write_file(filepath: str, content: str) -> None: Writes the content to the file. try: with open(filepath, \'w\') as file: file.write(content) except OSError as e: raise e def delete_file(filepath: str) -> None: Deletes the file. try: os.remove(filepath) except OSError as e: raise e def run_subprocess(command: str, env: dict = None) -> str: Runs a command in a subprocess, optionally set environment variables, and returns the command output. try: result = subprocess.run(command, shell=True, text=True, capture_output=True, env=env) return result.stdout.strip() except subprocess.CalledProcessError as e: raise e"},{"question":"# Python Interactive Shell Enhancements **Objective:** The goal of this task is to implement a custom interactive shell that provides a simple form of history substitution and command enhancement features similar to those described in the GNU Readline library and alternatives like IPython. **Task:** Implement a Python class `InteractiveShell` that simulates a basic interactive interpreter with the following features: 1. **History Tracking:** Track and store commands entered by the user. 2. **History Substitution:** Allow the use of `!n` to recall and execute the nth command from the history. 3. **Tab Completion:** Provide basic tab completion for Python keywords and functions. **Class Structure:** ```python class InteractiveShell: def __init__(self): Initialize an InteractiveShell instance with an empty command history. def start_shell(self): Start the interactive shell session. This method should continuously prompt the user for input until the user decides to exit (e.g., by entering \'exit\'). def execute_command(self, command): Execute the given command after performing any necessary substitutions or completions. Parameters: command (str): The command to execute. def add_to_history(self, command): Add the given command to the history. Parameters: command (str): The command to add to the history. def history_substitution(self, command): Perform history substitution on the given command if it starts with \'!\', and return the actual command to be executed. Parameters: command (str): The command potentially containing history substitution. Returns: str: The substituted command or the original command if no substitution is performed. ``` **Example Usage:** ```python shell = InteractiveShell() shell.start_shell() ``` **Input and Output:** - The `start_shell` method should support interactive input from the user. - Commands should be executed using Python\'s `exec` function, and outputs should be displayed to the user. - History substitution should occur when a command starting with `!` is executed (e.g., `!3` should execute the 3rd command in history). **Constraints:** - The system should store at least the last 10 commands in history. - Simplified tab completion should handle the completion for Python\'s built-in keywords and standard library functions only. **Performance Requirements:** - The interactive shell should promptly handle inputs and provide outputs without noticeable delay. **Note:** You may use Python\'s built-in libraries to implement these features but do not use external shells like IPython directly.","solution":"class InteractiveShell: def __init__(self): self.history = [] self.keywords = {\'and\', \'as\', \'assert\', \'break\', \'class\', \'continue\', \'def\', \'del\', \'elif\', \'else\', \'except\', \'False\', \'finally\', \'for\', \'from\', \'global\', \'if\', \'import\', \'in\', \'is\', \'lambda\', \'None\', \'nonlocal\', \'not\', \'or\', \'pass\', \'raise\', \'return\', \'True\', \'try\', \'while\', \'with\', \'yield\'} def start_shell(self): while True: try: command = input(\\">>> \\") if command.lower() == \'exit\': break command = self.history_substitution(command) self.execute_command(command) self.add_to_history(command) except Exception as e: print(f\\"Error: {e}\\") def execute_command(self, command): exec(command) def add_to_history(self, command): self.history.append(command) if len(self.history) > 10: self.history.pop(0) def history_substitution(self, command): if command.startswith(\'!\'): try: index = int(command[1:]) return self.history[index - 1] except (ValueError, IndexError): print(\\"Invalid history substitution\\") return \\"\\" return command"},{"question":"# Python Email Client with `poplib` Objective Create a Python-based email client using the `poplib` module to interact with a POP3 email server. Your client should be capable of connecting to the server, authenticating the user, retrieving email headers and bodies, and deleting emails as specified. Task Implement a class `EmailClient` with the following attributes and methods: 1. **Attributes** - `server`: The POP3 mail server address. - `port`: The server port, with a default value of 110. - `use_ssl`: Boolean to determine if SSL should be used. Default is `False`. - `timeout`: Timeout duration for the connection in seconds, default is `None`. 2. **Methods** - `__init__(self, server: str, port: int = 110, use_ssl: bool = False, timeout: int = None)`: Initialize the client with server details. - `connect(self)`: Establish a connection to the POP3 server. - `authenticate(self, username: str, password: str)`: Authenticate the user. - `get_message_summary(self) -> list`: Retrieve a summary (message number and size) of all emails in the mailbox. - `get_message(self, message_number: int) -> str`: Retrieve the full content of an email by its message number. - `delete_message(self, message_number: int)`: Flag a message for deletion. - `disconnect(self)`: Close the connection properly. Constraints - The client should handle exceptions gracefully and provide meaningful error messages. - Ensure the client properly disconnects from the server after operations. - Validate inputs where necessary (e.g., valid message numbers). Example Usage ```python # Example usage of the EmailClient class client = EmailClient(\'mail.example.com\', use_ssl=True) client.connect() client.authenticate(\'your_username\', \'your_password\') summary = client.get_message_summary() print(\'Message Summary:\', summary) message_content = client.get_message(1) print(\'Message Content:\', message_content) client.delete_message(1) client.disconnect() ``` Notes - You may use `getpass.getpass()` to securely prompt for a password in an interactive script. - If SSL is used (`use_ssl=True`), ensure to handle SSL contexts appropriately. - Provide appropriate docstrings and comments in your code. Submission Submit the implementation of the `EmailClient` class along with a small script demonstrating its use.","solution":"import poplib from email.parser import BytesParser from email.policy import default class EmailClient: def __init__(self, server: str, port: int = 110, use_ssl: bool = False, timeout: int = None): self.server = server self.port = port self.use_ssl = use_ssl self.timeout = timeout self.connection = None def connect(self): try: if self.use_ssl: self.connection = poplib.POP3_SSL(self.server, self.port, timeout=self.timeout) else: self.connection = poplib.POP3(self.server, self.port, timeout=self.timeout) except Exception as e: raise ConnectionError(f\\"Failed to connect to {self.server} on port {self.port}: {e}\\") def authenticate(self, username: str, password: str): try: self.connection.user(username) self.connection.pass_(password) except Exception as e: raise AuthenticationError(\\"Failed to authenticate: \\" + str(e)) def get_message_summary(self) -> list: try: messages = self.connection.list()[1] summary = [] for msg in messages: msg_num, msg_size = msg.decode().split() summary.append((int(msg_num), int(msg_size))) return summary except Exception as e: raise RuntimeError(f\\"Failed to retrieve message summaries: {e}\\") def get_message(self, message_number: int) -> str: try: response, lines, octets = self.connection.retr(message_number) msg_content = b\'rn\'.join(lines) message = BytesParser(policy=default).parsebytes(msg_content) return message.as_string() except Exception as e: raise ValueError(f\\"Failed to retrieve message {message_number}: {e}\\") def delete_message(self, message_number: int): try: self.connection.dele(message_number) except Exception as e: raise ValueError(f\\"Failed to delete message {message_number}: {e}\\") def disconnect(self): try: self.connection.quit() except Exception as e: raise RuntimeError(f\\"Failed to disconnect: {e}\\")"},{"question":"Context In distributed machine learning training, monitoring and recording the performance and status of different nodes are crucial. The `torch.distributed.elastic.metrics` package provides utilities to handle and report metrics in different ways. Objective Your task is to implement a custom metric handler by extending the `MetricHandler` class from the `torch.distributed.elastic.metrics.api` module. This custom metric handler should be able to log metrics to a file. Requirements 1. Implement a class `FileMetricHandler` that inherits from `MetricHandler`. 2. The `FileMetricHandler` should initialize with a file path where it will log the metrics. 3. Override the necessary methods to: - Log name and value of each metric to the specified file. 4. Ensure thread-safety when writing to the file to handle distributed setting scenarios. Input and Output - The `FileMetricHandler` should accept metrics in the format of dictionary, where keys are metric names and values are their respective values. - Example metrics: `{\\"accuracy\\": 0.95, \\"loss\\": 0.05}` Constraints - The log file should retain all log entries across multiple function calls. - Each log entry should include the timestamp of when the metric was recorded. - Performance is critical, and the handler should handle a high frequency of metric updates without significant blocking. Example Implementation ```python import threading import time from torch.distributed.elastic.metrics.api import MetricHandler class FileMetricHandler(MetricHandler): def __init__(self, file_path): super().__init__() self.file_path = file_path self.lock = threading.Lock() def _log_to_file(self, message): with self.lock: with open(self.file_path, \'a\') as f: f.write(message + \'n\') def put_metric(self, name, value): timestamp = time.time() message = f\\"{timestamp}: {name} = {value}\\" self._log_to_file(message) # Usage example handler = FileMetricHandler(\\"metrics.log\\") handler.put_metric(\\"accuracy\\", 0.95) handler.put_metric(\\"loss\\", 0.05) ``` Notes - You will need to copy and paste or re-implement any necessary classes and methods from the `torch.distributed.elastic.metrics.api` module if they are not accessible as is. - Make sure your implementation adheres to the design principles of thread-safety and efficient logging. Provide a thorough and efficient implementation. Test your implementation with multiple metric updates to ensure thread safety and correctness.","solution":"import threading import time from typing import Dict # A mock implementation of MetricHandler from `torch.distributed.elastic.metrics.api` since we can\'t import it directly. class MetricHandler: def put_metric(self, name: str, value: float): raise NotImplementedError() class FileMetricHandler(MetricHandler): def __init__(self, file_path: str): super().__init__() self.file_path = file_path self.lock = threading.Lock() def _log_to_file(self, message: str): with self.lock: with open(self.file_path, \'a\') as f: f.write(message + \'n\') def put_metric(self, name: str, value: float): timestamp = time.time() message = f\\"{timestamp}: {name} = {value}\\" self._log_to_file(message) # Usage example handler = FileMetricHandler(\\"metrics.log\\") handler.put_metric(\\"accuracy\\", 0.95) handler.put_metric(\\"loss\\", 0.05)"},{"question":"You are tasked with creating a simple key-value store using Python\'s `dbm` module. The key-value store should allow users to perform basic CRUD (Create, Read, Update, Delete) operations, and should be able to persist data between sessions. Requirements: 1. Implement a class `KeyValueStore` that wraps functionalities of the `dbm` module. 2. The class should have the following methods: - `__init__(self, filename: str, flag: str = \'c\')`: Initializes the database. - `set_value(self, key: str, value: str) -> None`: Adds or updates the value for the given key. - `get_value(self, key: str) -> str`: Retrieves the value for the given key. Returns `None` if the key doesn’t exist. - `delete_value(self, key: str) -> None`: Deletes the key-value pair for the given key. - `sync(self) -> None`: Syncs the database (ensure all data is written to disk). - `close(self) -> None`: Closes the database. 3. Handle exceptions appropriately, such as trying to delete a key that does not exist, reading from a read-only database, or storing non-string types. 4. Provide a method to list all keys in the database. Input and Output Formats: - Input to the methods will be Python strings and the methods will deal with these inputs internally. - Output will vary for each method: - `set_value` and `delete_value` return None. - `get_value` returns the value as a string or None. - `sync` and `close` return None. - `all_keys` returns a list of keys present in the database. Constraints: - The keys and values must be strings. - Ensure the database is always closed properly, even if an exception occurs. Example: ```python store = KeyValueStore(\'example_db\', \'c\') store.set_value(\'name\', \'Alice\') store.set_value(\'age\', \'30\') print(store.get_value(\'name\')) # Output: Alice print(store.get_value(\'age\')) # Output: 30 store.delete_value(\'age\') print(store.get_value(\'age\')) # Output: None store.sync() store.close() ``` Performance Requirements: - The solution should be efficient in terms of time complexity for basic operations like retrieving, setting, and deleting values. - Use context management efficiently to ensure resources are managed properly.","solution":"import dbm class KeyValueStore: def __init__(self, filename: str, flag: str = \'c\'): self.db = dbm.open(filename, flag) def set_value(self, key: str, value: str) -> None: if not isinstance(key, str) or not isinstance(value, str): raise ValueError(\\"Both key and value must be strings.\\") self.db[key] = value def get_value(self, key: str) -> str: if not isinstance(key, str): raise ValueError(\\"Key must be a string.\\") return self.db[key].decode(\'utf-8\') if key in self.db else None def delete_value(self, key: str) -> None: if not isinstance(key, str): raise ValueError(\\"Key must be a string.\\") if key in self.db: del self.db[key] else: raise KeyError(f\\"The key \'{key}\' does not exist in the database.\\") def all_keys(self): return [key.decode(\'utf-8\') for key in self.db.keys()] def sync(self) -> None: self.db.sync() def close(self) -> None: self.db.close() def __del__(self): self.close()"},{"question":"<|Analysis Begin|> The provided documentation focuses on the usage of the seaborn `PairGrid` function. Key functionalities demonstrated include: 1. Creating blank grids and mapping bivariate functions. 2. Differentiating functions for diagonal and off-diagonal plots. 3. Handling upper and lower triangle plots differently to avoid redundancy. 4. Using the `hue` parameter for different visual groupings. 5. Mapping different variables to size, hue, and controlling exact variables shown. 6. Customizing various aspects of plots, such as non-square grids and different approaches to resolving multiple distributions. Given the level of detail in the documentation, I can craft a question that assesses a student\'s understanding of these functionalities, specifically focusing on: - Utilizing `PairGrid` effectively. - Customizing the plots using seaborn functions. - Handling different columns and applying grouping. <|Analysis End|> <|Question Begin|> # Coding Assessment Question **Objective:** Demonstrate your understanding of creating advanced visualizations using seaborn\'s PairGrid. **Task:** Write a function `create_pairgrid` that takes in a pandas DataFrame and other specified parameters, and returns a customized seaborn PairGrid plot based on those parameters. **Function Signature:** ```python def create_pairgrid(df: pd.DataFrame, hue: str, vars: list, diag_kind: str = \\"hist\\", offdiag_kind: str = \\"scatter\\") -> sns.axisgrid.PairGrid: pass ``` **Input:** - `df`: A pandas DataFrame containing the data to plot. - `hue`: A string representing the column name in the DataFrame to be used for color encoding. - `vars`: A list of strings representing the column names to be plotted. - `diag_kind`: A string specifying the kind of plot to use on the diagonal (`\\"hist\\"` for histogram, `\\"kde\\"` for Kernel Density Estimate). - `offdiag_kind`: A string specifying the kind of plot to use on the off-diagonal (`\\"scatter\\"` for scatter plot, `\\"kde\\"` for Kernel Density Estimate). **Output:** - Returns a `sns.axisgrid.PairGrid` object that plots the specified DataFrame `df`. **Constraints:** - The DataFrame `df` must contain all columns specified in `hue` and `vars`. - `diag_kind` must be either `\\"hist\\"` or `\\"kde\\"`. - `offdiag_kind` must be either `\\"scatter\\"` or `\\"kde\\"`. **Example Usage:** ```python import pandas as pd import seaborn as sns # Load example dataset df = sns.load_dataset(\'penguins\') # Create a customized PairGrid g = create_pairgrid(df, hue=\'species\', vars=[\'bill_length_mm\', \'bill_depth_mm\', \'flipper_length_mm\', \'body_mass_g\']) # Show the plot g.fig.show() ``` **Explanation:** 1. Use the `sns.PairGrid` to initialize the grid with the DataFrame and the specified `hue` and `vars`. 2. Map the appropriate diagonal plot based on the `diag_kind` parameter. 3. Map the appropriate off-diagonal plot based on the `offdiag_kind` parameter. 4. Add a legend to the grid. Ensure the plot is informative and visually appealing by adjusting for different distributions and customization options where necessary.","solution":"import pandas as pd import seaborn as sns def create_pairgrid(df: pd.DataFrame, hue: str, vars: list, diag_kind: str = \\"hist\\", offdiag_kind: str = \\"scatter\\") -> sns.axisgrid.PairGrid: Creates and returns a customized seaborn PairGrid. Parameters: - df: pandas DataFrame containing the data to plot. - hue: a string representing the column name to be used for color encoding. - vars: a list of strings representing the column names to be plotted. - diag_kind: a string specifying the kind of plot to use on the diagonal (\'hist\' or \'kde\'). - offdiag_kind: a string specifying the kind of plot to use on the off-diagonal (\'scatter\' or \'kde\'). Returns: - A seaborn PairGrid object. # Validate inputs if diag_kind not in [\'hist\', \'kde\']: raise ValueError(\\"diag_kind must be either \'hist\' or \'kde\'\\") if offdiag_kind not in [\'scatter\', \'kde\']: raise ValueError(\\"offdiag_kind must be either \'scatter\' or \'kde\'\\") # Create PairGrid g = sns.PairGrid(df, hue=hue, vars=vars) # Map plots if diag_kind == \'hist\': g.map_diag(sns.histplot) elif diag_kind == \'kde\': g.map_diag(sns.kdeplot) if offdiag_kind == \'scatter\': g.map_offdiag(sns.scatterplot) elif offdiag_kind == \'kde\': g.map_offdiag(sns.kdeplot) # Add legend g.add_legend() return g"},{"question":"# Sparse Tensor Manipulation with PyTorch Objective The goal of this task is to test your understanding of the PyTorch\'s sparse tensor functionalities. You will create a sparse tensor, convert it to a different sparse format, perform specific operations, and then revert it back to the dense format. Problem Statement 1. **Create a Sparse Tensor:** - Construct a sparse COO tensor from the lists of indices and values provided below. The tensor should be of shape (5, 5). 2. **Convert to CSR Format:** - Convert the created sparse COO tensor into its equivalent CSR format. 3. **Perform Matrix Multiplication:** - Create a dense tensor of your choice with a compatible shape and perform a matrix multiplication operation between this dense tensor and the CSR tensor. 4. **Revert Back to Dense:** - Convert the resultant tensor from the multiplication back to a dense tensor for verification. Inputs: 1. A list of indices: `[(0, 0), (0, 2), (1, 2), (3, 4), (2, 1)]` 2. Corresponding values: `[4, 5, 7, 2, 1]` 3. Shape of the tensor (m, n): `(5, 5)` Constraints: - Use PyTorch\'s built-in functions to handle conversions and operations. - Ensure the resultant tensor shapes match for valid operations. - Assume basic compatibility for matrix multiplication operations. Expected Output: - A dense tensor that represents the result of the matrix multiplication. Code Solution: Write a function `sparse_tensor_operations(indices, values, shape)` that performs the above tasks and returns the final dense tensor. ```python import torch def sparse_tensor_operations(indices, values, shape): # Step 1: Create a sparse COO tensor. index_tensor = torch.tensor(indices).t() value_tensor = torch.tensor(values, dtype=torch.float32) sparse_coo = torch.sparse_coo_tensor(index_tensor, value_tensor, size=shape) # Step 2: Convert to CSR format. sparse_csr = sparse_coo.to_sparse_csr() # Step 3: Perform matrix multiplication. dense_tensor = torch.rand(shape[1], shape[0]) # Creating a random dense tensor with compatible shape. result_sparse_csr = torch.matmul(sparse_csr, dense_tensor) # Step 4: Convert result back to a dense tensor. result_dense = result_sparse_csr.to_dense() return result_dense # Example usage indices = [(0, 0), (0, 2), (1, 2), (3, 4), (2, 1)] values = [4, 5, 7, 2, 1] shape = (5, 5) result = sparse_tensor_operations(indices, values, shape) print(result) ``` Notes: - Ensure you handle edge cases where the tensor operations might not be well-defined. - Validate the input shapes if necessary to ensure compatibility of operations.","solution":"import torch def sparse_tensor_operations(indices, values, shape): # Step 1: Create a sparse COO tensor. index_tensor = torch.tensor(indices).t() value_tensor = torch.tensor(values, dtype=torch.float32) sparse_coo = torch.sparse_coo_tensor(index_tensor, value_tensor, size=shape) # Step 2: Convert to CSR format. sparse_csr = sparse_coo.to_sparse_csr() # Step 3: Perform matrix multiplication. dense_tensor = torch.rand(shape[1], shape[0]) # Creating a random dense tensor with compatible shape. result_sparse_csr = torch.matmul(sparse_csr, dense_tensor) # Step 4: Convert result back to a dense tensor. result_dense = result_sparse_csr.to_dense() return result_dense # Example usage indices = [(0, 0), (0, 2), (1, 2), (3, 4), (2, 1)] values = [4, 5, 7, 2, 1] shape = (5, 5) result = sparse_tensor_operations(indices, values, shape) print(result)"},{"question":"You are tasked with deploying a Flask application named \\"Flaskr\\" to a production environment. Your task includes building the application, setting up a secure environment, and running it with a production-grade WSGI server. Follow the steps below and provide the necessary code and commands to complete the task. # Requirements: 1. **Building and Installing:** - Create a wheel file for the Flaskr application. - Set up a new virtual environment and install the wheel file. 2. **Configuration:** - Generate a random SECRET_KEY and configure it in a `config.py` file located in the instance folder. 3. **Running with a Production Server:** - Install the Waitress WSGI server. - Configure and run the Flaskr application using Waitress. # Steps and Implementation: 1. **Build the Application:** - Install `build` tool and create a wheel file for the Flaskr application. 2. **Set Up Virtual Environment and Install:** - Use the following commands to set up a virtual environment and install the wheel file. 3. **Generate and Configure SECRET_KEY:** - Generate a SECRET_KEY using Python and save it in the `config.py` file in the instance folder. 4. **Run with Waitress:** - Install the Waitress WSGI server. - Run the application using Waitress. # Input and Output: - **Input:** Commands and code snippets for each of the steps listed above. - **Output:** Successful deployment of the Flaskr application that can be accessed publicly. # Constraints: - Ensure that you follow the best practices for securing the *SECRET_KEY*. - The Waitress server should serve the application on `http://0.0.0.0:8080`. # Instructions: 1. **Building and Installing:** - Install the required build tool and create a wheel file. ```sh pip install build python -m build --wheel ``` 2. **Set Up Virtual Environment and Install:** - Create a new virtual environment and install the wheel file. ```sh python -m venv venv source venv/bin/activate pip install dist/flaskr-1.0.0-py3-none-any.whl ``` 3. **Generate and Configure SECRET_KEY:** - Generate a SECRET_KEY and save it in `config.py`. ```sh python -c \'import secrets; print(secrets.token_hex())\' ``` - Create `config.py` in the instance folder with the generated key. ```python # .venv/var/flaskr-instance/config.py SECRET_KEY = \'your_generated_secret_key_here\' ``` 4. **Run with Waitress:** - Install Waitress and run the application. ```sh pip install waitress waitress-serve --call \'flaskr:create_app\' ``` # Notes: - Replace `\'your_generated_secret_key_here\'` with the actual key generated in step 3. - Ensure that the paths and environment are correctly set up to reflect the deployment environment.","solution":"# Step 1: Building the application into a wheel file # Ensure you have a setup.py file with your application metadata # Assume setup.py is correctly configured for the Flaskr application # Terminal commands: # pip install build # python -m build --wheel # Step 2: Set up a virtual environment and install the wheel file # Terminal commands: # python -m venv venv # source venv/bin/activate # pip install dist/flaskr-1.0.0-py3-none-any.whl # Step 3: Generate and configure SECRET_KEY import secrets def generate_secret_key(): Generate a securely random SECRET_KEY for Flask configurations. return secrets.token_hex() secret_key = generate_secret_key() # Save the secret key in config.py located in the instance folder # Create an instance folder and config.py inside it config_content = f\\"SECRET_KEY = \'{secret_key}\'\\" # File operations to create instance/config.py import os instance_path = os.path.join(os.getcwd(), \\"instance\\") if not os.path.exists(instance_path): os.makedirs(instance_path) config_path = os.path.join(instance_path, \\"config.py\\") with open(config_path, \'w\') as config_file: config_file.write(config_content) # Step 4: Run with Waitress # Terminal commands: # pip install waitress # waitress-serve --call \'flaskr:create_app\'"},{"question":"Objective: Write a Python program that compresses files from a given input directory using multiple compression algorithms (`gzip`, `bz2`, `lzma`, `zipfile`, `tarfile`) and saves them into a specified output directory. Your program should demonstrate the ability to handle different compression methods and produce valid compressed files that can be decompressed back to their original state. Requirements: 1. The program should accept two command-line arguments: the input directory and the output directory. 2. The program should support the following compression methods, as described: - `gzip`: Compress each file in the input directory. - `bz2`: Compress each file in the input directory. - `lzma`: Compress each file in the input directory. - `zipfile`: Create a single ZIP file containing all files in the input directory. - `tarfile`: Create a single tar archive containing all files in the input directory; it should then compress using gzip, bz2, and lzma. 3. Implement the following functions: - `compress_gzip(input_file_path: str, output_file_path: str)` - `compress_bz2(input_file_path: str, output_file_path: str)` - `compress_lzma(input_file_path: str, output_file_path: str)` - `create_zipfile(input_dir: str, output_file_path: str)` - `create_tarfile(input_dir: str, output_file_path: str, compression: str)` 4. The program should ensure that the compressed files retain the original file names with appropriate extensions (e.g., `.gz`, `.bz2`, `.xz`, `.zip`, `.tar.gz`, etc.). 5. Provide error handling to manage files that cannot be compressed and log these errors. 6. Document the functions and main part of the code to explain the process and usage clearly. Input: 1. Input Directory Path (containing files to be compressed) 2. Output Directory Path (where compressed files will be saved) Output: Compressed files saved in the specified output directory with appropriate formats and extensions. Constraints: 1. Ensure that the program handles both text and binary files. 2. The program should efficiently handle a large number of files and large file sizes without running into memory or performance bottlenecks. Example: Suppose the input directory contains: ``` file1.txt file2.bin file3.csv ``` And the output directory is specified as `/output`. After running the program, the output directory should contain: ``` file1.txt.gz file2.bin.gz file3.csv.gz file1.txt.bz2 file2.bin.bz2 file3.csv.bz2 file1.txt.xz file2.bin.xz file3.csv.xz archive.zip (containing file1.txt, file2.bin, file3.csv) archive.tar.gz (containing file1.txt, file2.bin, file3.csv) archive.tar.bz2 (containing file1.txt, file2.bin, file3.csv) archive.tar.xz (containing file1.txt, file2.bin, file3.csv) ``` Notes: - Ensure the output directory exists. If not, create it before compression. - This question assesses the student\'s ability to integrate various compression methods, manage file operations and handle exceptions effectively.","solution":"import os import gzip import bz2 import lzma import zipfile import tarfile import shutil import logging def compress_gzip(input_file_path: str, output_file_path: str): try: with open(input_file_path, \'rb\') as f_in: with gzip.open(output_file_path, \'wb\') as f_out: shutil.copyfileobj(f_in, f_out) except Exception as e: logging.error(f\\"Error compressing {input_file_path} with gzip: {e}\\") def compress_bz2(input_file_path: str, output_file_path: str): try: with open(input_file_path, \'rb\') as f_in: with bz2.open(output_file_path, \'wb\') as f_out: shutil.copyfileobj(f_in, f_out) except Exception as e: logging.error(f\\"Error compressing {input_file_path} with bz2: {e}\\") def compress_lzma(input_file_path: str, output_file_path: str): try: with open(input_file_path, \'rb\') as f_in: with lzma.open(output_file_path, \'wb\') as f_out: shutil.copyfileobj(f_in, f_out) except Exception as e: logging.error(f\\"Error compressing {input_file_path} with lzma: {e}\\") def create_zipfile(input_dir: str, output_file_path: str): try: with zipfile.ZipFile(output_file_path, \'w\') as zipf: for root, _, files in os.walk(input_dir): for file in files: file_path = os.path.join(root, file) zipf.write(file_path, os.path.relpath(file_path, input_dir)) except Exception as e: logging.error(f\\"Error creating zipfile for {input_dir}: {e}\\") def create_tarfile(input_dir: str, output_file_path: str, compression: str): mode = \'w\' if compression == \'gz\': mode = \'w:gz\' elif compression == \'bz2\': mode = \'w:bz2\' elif compression == \'xz\': mode = \'w:xz\' else: logging.error(f\\"Unsupported compression type {compression}\\") return try: with tarfile.open(output_file_path, mode) as tar: tar.add(input_dir, arcname=os.path.basename(input_dir)) except Exception as e: logging.error(f\\"Error creating tarfile for {input_dir} with {compression} compression: {e}\\") def main(input_dir: str, output_dir: str): if not os.path.exists(output_dir): os.makedirs(output_dir) for root, _, files in os.walk(input_dir): for file in files: input_file_path = os.path.join(root, file) output_file_path_gz = os.path.join(output_dir, f\\"{file}.gz\\") compress_gzip(input_file_path, output_file_path_gz) output_file_path_bz2 = os.path.join(output_dir, f\\"{file}.bz2\\") compress_bz2(input_file_path, output_file_path_bz2) output_file_path_lzma = os.path.join(output_dir, f\\"{file}.xz\\") compress_lzma(input_file_path, output_file_path_lzma) output_file_path_zip = os.path.join(output_dir, \\"archive.zip\\") create_zipfile(input_dir, output_file_path_zip) output_file_path_tar_gz = os.path.join(output_dir, \\"archive.tar.gz\\") create_tarfile(input_dir, output_file_path_tar_gz, \'gz\') output_file_path_tar_bz2 = os.path.join(output_dir, \\"archive.tar.bz2\\") create_tarfile(input_dir, output_file_path_tar_bz2, \'bz2\') output_file_path_tar_xz = os.path.join(output_dir, \\"archive.tar.xz\\") create_tarfile(input_dir, output_file_path_tar_xz, \'xz\') if __name__ == \\"__main__\\": import sys if len(sys.argv) != 3: print(\\"Usage: python compress_files.py <input_directory> <output_directory>\\") sys.exit(1) input_directory = sys.argv[1] output_directory = sys.argv[2] main(input_directory, output_directory)"},{"question":"Coding Assessment Question # Objective: Demonstrate your understanding of the \\"email.mime\\" package in Python by creating a complex MIME email message that includes text, an image, and a PDF attachment. Your implementation should showcase your ability to utilize different MIME subclasses and manage multipart messages effectively. # Task: Write a Python function `create_mime_email` that constructs and returns a MIME multipart email message. The email should contain: 1. A plain text message with the provided text. 2. An image attachment with the provided image data. 3. A PDF file attachment with the provided PDF data. # Function Signature: ```python def create_mime_email(text: str, image_data: bytes, image_type: str, pdf_data: bytes, to_email: str, from_email: str, subject: str) -> email.mime.multipart.MIMEMultipart: pass ``` # Parameters: - `text` (str): The plain text message to be included in the email. - `image_data` (bytes): The raw bytes of the image to be attached. - `image_type` (str): The MIME subtype of the image (e.g., \'jpeg\', \'png\'). - `pdf_data` (bytes): The raw bytes of the PDF to be attached. - `to_email` (str): The recipient\'s email address. - `from_email` (str): The sender\'s email address. - `subject` (str): The subject of the email. # Requirements: - The email should be a multipart email. - The email should include a plain text body part. - The email should include an image attachment. - The email should include a PDF attachment. - Appropriate headers should be set for each MIME part and for the entire email. - The function should use the `email.mime` package to create the MIME message. # Example Usage: ```python # Example data for testing text = \\"Hello, this is a test email with an image and PDF attachment.\\" image_data = b\'...\' # Image data in bytes image_type = \'jpeg\' pdf_data = b\'...\' # PDF data in bytes to_email = \\"recipient@example.com\\" from_email = \\"sender@example.com\\" subject = \\"Test Email\\" # Create MIME email email_message = create_mime_email(text, image_data, image_type, pdf_data, to_email, from_email, subject) ``` # Constraints: - The function should handle the provided inputs safely and correctly. - The function should be efficient with a focus on clarity and correctness. - Use built-in Python 3.10 functionality only and ensure the code is compatible with standard Python 3.10 environments. # Notes: - You might need to install additional packages like `email` using pip if not available in your working environment. - Proper MIME types should be set for each attachment (e.g., \'image/jpeg\' for JPEG image, \'application/pdf\' for PDF file). - Make sure to test your function thoroughly with various types of inputs.","solution":"from email.mime.multipart import MIMEMultipart from email.mime.text import MIMEText from email.mime.base import MIMEBase from email import encoders from email.mime.image import MIMEImage from email.mime.application import MIMEApplication def create_mime_email(text: str, image_data: bytes, image_type: str, pdf_data: bytes, to_email: str, from_email: str, subject: str) -> MIMEMultipart: # Create the root message msg = MIMEMultipart() msg[\'From\'] = from_email msg[\'To\'] = to_email msg[\'Subject\'] = subject # Attach the text part text_part = MIMEText(text, \'plain\') msg.attach(text_part) # Attach the image part image_part = MIMEImage(image_data, _subtype=image_type) image_part.add_header(\'Content-Disposition\', \'attachment\', filename=f\'image.{image_type}\') msg.attach(image_part) # Attach the PDF part pdf_part = MIMEApplication(pdf_data, _subtype=\'pdf\') pdf_part.add_header(\'Content-Disposition\', \'attachment\', filename=\'document.pdf\') msg.attach(pdf_part) return msg"},{"question":"**Question: Implement an Asyncio-Based Chat Server using Transports and Protocols** # Background You are tasked with developing a chat server using Python\'s `asyncio` library, utilizing transports and protocols as described in the documentation. Your server should handle multiple clients and broadcast messages received from any client to all other connected clients. # Requirements 1. Implement a custom protocol class called `ChatServerProtocol`, which will manage client connections, handle received messages, and broadcast them to all connected clients. 2. Implement a function `main()` that will start an asyncio server using your `ChatServerProtocol`. # Specifications - **Class `ChatServerProtocol`:** - `clients`: A class attribute that is a list of all currently connected client transports. - `connection_made(self, transport)`: Adds the new client transport to the `clients` list and stores the transport instance. - `data_received(self, data)`: Decodes the received data and sends it to all other clients. - `connection_lost(self, exc)`: Removes the client transport from the `clients` list. - **Function `main()`:** - Starts the asyncio server on `localhost` at port `12345`. - Listens continuously for client connections. # Input/Output - **Input**: No direct input. The function will be continuously running, accepting new connections and handling data from the connected clients. - **Output**: No direct output, but server logs can be printed to show connections, messages received, and broadcasts. # Example A simple client connecting to this server and sending a message should see messages broadcast to other connected clients. Sample server log: ``` Starting chat server on localhost:12345 New connection from (\'127.0.0.1\', 45678) Received data from (\'127.0.0.1\', 45678): Hello World! Broadcasted data to clients: Hello World! ``` # Constraints - The implementation should be efficient and handle a significant number of connections concurrently. - The implementation should be robust and handle client disconnections gracefully. # Solution Template ```python import asyncio class ChatServerProtocol(asyncio.Protocol): clients = [] def connection_made(self, transport): # Add new connection to clients list self.transport = transport self.clients.append(transport) peername = transport.get_extra_info(\'peername\') print(f\\"New connection from {peername}\\") def data_received(self, data): message = data.decode() peername = self.transport.get_extra_info(\'peername\') print(f\\"Received data from {peername}: {message}\\") self.broadcast(message) def broadcast(self, message): for client in self.clients: if client is not self.transport: client.write(message.encode()) print(f\\"Broadcasted data to clients: {message}\\") def connection_lost(self, exc): # Remove connection from clients list self.clients.remove(self.transport) peername = self.transport.get_extra_info(\'peername\') print(f\\"Connection lost from {peername}\\") async def main(): # Setup server loop = asyncio.get_running_loop() server = await loop.create_server( lambda: ChatServerProtocol(), \'localhost\', 12345 ) print(\\"Starting chat server on localhost:12345\\") async with server: await server.serve_forever() if __name__ == \\"__main__\\": asyncio.run(main()) ```","solution":"import asyncio class ChatServerProtocol(asyncio.Protocol): clients = [] def connection_made(self, transport): self.transport = transport self.clients.append(transport) peername = transport.get_extra_info(\'peername\') print(f\\"New connection from {peername}\\") def data_received(self, data): message = data.decode() peername = self.transport.get_extra_info(\'peername\') print(f\\"Received data from {peername}: {message}\\") self.broadcast(message) def broadcast(self, message): for client in self.clients: if client is not self.transport: client.write(message.encode()) print(f\\"Broadcasted data to clients: {message}\\") def connection_lost(self, exc): self.clients.remove(self.transport) peername = self.transport.get_extra_info(\'peername\') print(f\\"Connection lost from {peername}\\") async def main(): loop = asyncio.get_running_loop() server = await loop.create_server( lambda: ChatServerProtocol(), \'localhost\', 12345 ) print(\\"Starting chat server on localhost:12345\\") async with server: await server.serve_forever() if __name__ == \'__main__\': asyncio.run(main())"},{"question":"**Question: Implementing a Custom Terminal Logger** You are required to implement a custom terminal logger using the `pty` module in Python. The custom logger should: - Fork a child process to run a specified shell (e.g., \\"bash\\" or \\"sh\\"). - Record all terminal interactions (input and output) to a specified log file. - The log file should include timestamps indicating when the session started and ended. - Optionally, append to an existing log file if a flag is set. **Function Signature:** ```python def terminal_logger(shell: str, log_filename: str, append: bool = False) -> None: Fork a child process to run a specified shell and log all interactions. Args: - shell (str): The shell to be invoked (e.g., \\"/bin/bash\\"). - log_filename (str): The name of the log file where interactions are recorded. - append (bool): If True, append to the log file; otherwise, overwrite it. Returns: - None ``` # Input - `shell`: A string specifying the shell to be used (e.g., \\"/bin/bash\\"). - `log_filename`: The name of the log file where the terminal session is recorded. - `append`: A boolean flag indicating whether to append to the log file or overwrite it (default is `False`). # Output - The function does not need to return any value. It should create or append to the log file with the terminal session details. # Constraints - The function should handle potential errors gracefully. - Ensure that the log file captures the entire terminal session, including when it starts and ends. # Example Usage ```python terminal_logger(\\"/bin/bash\\", \\"terminal.log\\", append=False) # This should start a new terminal session running /bin/bash, logging all interactions to \'terminal.log\'. # If append is set to True, the logs should be appended to \'terminal.log\' instead of overwriting it. ``` # Additional Requirements - You may assume the environment is Unix-based (Linux, macOS). - You are allowed to use functions from the `pty` and `os` modules. - Add appropriate comments to your implementation to explain key parts of the code.","solution":"import os import pty import datetime def terminal_logger(shell: str, log_filename: str, append: bool = False) -> None: Fork a child process to run a specified shell and log all interactions. Args: - shell (str): The shell to be invoked (e.g., \\"/bin/bash\\"). - log_filename (str): The name of the log file where interactions are recorded. - append (bool): If True, append to the log file; otherwise, overwrite it. Returns: - None # Determine the file mode based on the append flag mode = \'a\' if append else \'w\' # Ensure that any exceptions in file operations are handled try: with open(log_filename, mode) as log_file: # Log the session start time log_file.write(f\\"Session started at {datetime.datetime.now().isoformat()}n\\") log_file.flush() def read(fd): # Read from the file descriptor, write to log and standard output output = os.read(fd, 1024) log_file.write(output.decode()) log_file.flush() return output # Fork process to run a shell pty.spawn(shell, read) # Log the session end time log_file.write(f\\"nSession ended at {datetime.datetime.now().isoformat()}n\\") log_file.flush() except Exception as e: print(f\\"An error occurred: {e}\\")"},{"question":"# PyTorch Distributed Training Configuration Objective: You are given a scenario where you need to configure a distributed training job for a deep learning model using PyTorch. The job should be capable of handling node failures and scaling events. Task: Write a Python function `generate_torchrun_command` that generates the correct `torchrun` command line string based on the type of job (fault-tolerant or elastic) and the given parameters. Function Signature: ```python def generate_torchrun_command( num_nodes: str, trainers_per_node: int, max_restarts: int, job_id: str, rdzv_backend: str, host_node_addr: str, training_script: str, additional_args: str = \\"\\", job_type: str = \\"fault_tolerant\\", min_size: int = None, max_size: int = None ) -> str: ``` Parameters: - `num_nodes` (str): For fault-tolerant jobs, the exact number of nodes (`NUM_NODES`). For elastic jobs, the range in the form `\\"MIN_SIZE:MAX_SIZE\\"`. - `trainers_per_node` (int): The number of trainer processes to run per node. - `max_restarts` (int): The number of allowed failures or membership changes before giving up. - `job_id` (str): A unique identifier for the job. - `rdzv_backend` (str): Rendezvous backend to use (e.g., `c10d`). - `host_node_addr` (str): The address of the host node where the rendezvous backend will be instantiated (e.g., `node1.example.com:29400`). - `training_script` (str): The name of the training script to run. - `additional_args` (str, optional): Additional arguments to pass to the training script. - `job_type` (str, optional): Type of the job, either `\\"fault_tolerant\\"` or `\\"elastic\\"`. Default is `\\"fault_tolerant\\"`. - `min_size` (int, optional): Minimum number of nodes for elastic jobs. - `max_size` (int, optional): Maximum number of nodes for elastic jobs. Return: - (str): The complete `torchrun` command line string. Constraints: 1. If `job_type` is `\\"elastic\\"`, `min_size` and `max_size` must be provided. 2. If `job_type` is `\\"fault_tolerant\\"`, `min_size` and `max_size` should not be provided. Example: ```python command = generate_torchrun_command( num_nodes=\\"3\\", trainers_per_node=4, max_restarts=2, job_id=\\"example_job_ft\\", rdzv_backend=\\"c10d\\", host_node_addr=\\"node1.example.com:29400\\", training_script=\\"train.py\\", additional_args=\\"--epochs 10\\", job_type=\\"fault_tolerant\\" ) print(command) ``` Output: ``` torchrun --nnodes=3 --nproc-per-node=4 --max-restarts=2 --rdzv-id=example_job_ft --rdzv-backend=c10d --rdzv-endpoint=node1.example.com:29400 train.py --epochs 10 ``` ```python command = generate_torchrun_command( num_nodes=\\"3:5\\", trainers_per_node=4, max_restarts=5, job_id=\\"example_job_elastic\\", rdzv_backend=\\"c10d\\", host_node_addr=\\"node1.example.com:29400\\", training_script=\\"train.py\\", additional_args=\\"--epochs 10\\", job_type=\\"elastic\\", min_size=3, max_size=5 ) print(command) ``` Output: ``` torchrun --nnodes=3:5 --nproc-per-node=4 --max-restarts=5 --rdzv-id=example_job_elastic --rdzv-backend=c10d --rdzv-endpoint=node1.example.com:29400 train.py --epochs 10 ```","solution":"def generate_torchrun_command( num_nodes: str, trainers_per_node: int, max_restarts: int, job_id: str, rdzv_backend: str, host_node_addr: str, training_script: str, additional_args: str = \\"\\", job_type: str = \\"fault_tolerant\\", min_size: int = None, max_size: int = None ) -> str: if job_type == \\"elastic\\": if min_size is None or max_size is None: raise ValueError(\\"For elastic jobs, both min_size and max_size must be provided.\\") nnodes = f\\"{min_size}:{max_size}\\" elif job_type == \\"fault_tolerant\\": nnodes = num_nodes if min_size is not None or max_size is not None: raise ValueError(\\"min_size and max_size should not be provided for fault-tolerant jobs.\\") else: raise ValueError(\\"Unsupported job type. Use \'fault_tolerant\' or \'elastic\'.\\") command = ( f\\"torchrun --nnodes={nnodes} --nproc-per-node={trainers_per_node} \\" f\\"--max-restarts={max_restarts} --rdzv-id={job_id} --rdzv-backend={rdzv_backend} \\" f\\"--rdzv-endpoint={host_node_addr} {training_script} {additional_args}\\" ).strip() return command"},{"question":"**Problem Statement**: You are required to implement a script that connects to a Telnet server, navigates through a series of command prompts, and retrieves specific information from the server. Your program must use the `telnetlib` module for all Telnet interactions. **Task Requirements**: 1. Implement a Python function using the `telnetlib` module that will: - Connect to a Telnet server. - Send a sequence of commands to navigate to a specific menu or prompt. - Retrieve and return specific information from the server. **Function Signature**: ```python def get_server_information(host: str, port: int, username: str, password: str, commands: list) -> str: Connects to a Telnet server, navigates through a series of prompts, and retrieves specific information. Parameters: - host (str): The hostname of the Telnet server. - port (int): The port number to connect to. - username (str): The username for authentication. - password (str): The password for authentication. - commands (list): A list of commands to navigate through the server prompts. Returns: - str: The retrieved information from the server. ``` **Guidelines**: 1. The function should handle the connection establishment using the `Telnet` class. 2. It should implement the login process by handling the \\"username\\" and \\"password\\" prompts. 3. Navigate through the prompts using the commands provided in the `commands` list. 4. Retrieve the output after executing the final command and return it as a string. 5. Ensure the connection is closed properly using context management. 6. Handle any potential exceptions (e.g., connection failures) gracefully. **Example**: ```python commands = [ {\\"prompt\\": b\\"login: \\", \\"response\\": \\"user123\\"}, {\\"prompt\\": b\\"Password: \\", \\"response\\": \\"pass123\\"}, {\\"prompt\\": b\\" \\", \\"response\\": \\"cd /info\\"}, {\\"prompt\\": b\\" \\", \\"response\\": \\"cat data.txt\\"}, ] output = get_server_information(\\"localhost\\", 23, \\"user123\\", \\"pass123\\", commands) print(output) ``` In this example, the function will connect to `localhost` at port `23`, log in with `user123` and `pass123`, navigate to the `/info` directory, and display the contents of `data.txt`. **Constraints**: - Use the `telnetlib` module. - Handle potential EOFErrors and connection issues. - Validate all inputs to ensure they are in the correct format. **Performance Requirements**: - Ensure the function operates efficiently and handles multiple prompts in sequence. - Perform necessary error checking and handling operations in an efficient manner.","solution":"import telnetlib def get_server_information(host: str, port: int, username: str, password: str, commands: list) -> str: Connects to a Telnet server, navigates through a series of prompts, and retrieves specific information. Parameters: - host (str): The hostname of the Telnet server. - port (int): The port number to connect to. - username (str): The username for authentication. - password (str): The password for authentication. - commands (list): A list of commands to navigate through the server prompts. Returns: - str: The retrieved information from the server. try: with telnetlib.Telnet(host, port) as tn: tn.read_until(b\\"login: \\") tn.write(username.encode(\'ascii\') + b\\"n\\") tn.read_until(b\\"Password: \\") tn.write(password.encode(\'ascii\') + b\\"n\\") for command in commands: prompt = command[\\"prompt\\"] response = command[\\"response\\"] tn.read_until(prompt) tn.write(response.encode(\'ascii\') + b\\"n\\") result = tn.read_all().decode(\'ascii\') return result except Exception as e: return f\\"An error occurred: {e}\\""},{"question":"**ML Dataset Loading and Preprocessing with Scikit-Learn** In this assessment, you are required to load a real-world dataset using the `sklearn.datasets` package and perform basic preprocessing to prepare the data for a machine learning task. The specific objectives are: **Objective:** 1. Fetch the \'California Housing\' dataset using the appropriate function from `sklearn.datasets`. 2. Perform basic preprocessing, which includes: - Handling missing values (if any). - Standardizing the features. 3. Split the dataset into training and testing sets. 4. Return the shapes of the resulting data subsets. **Constraints:** - Use a random state of 42 for any function that requires it. - Use a test size of 20% for splitting the dataset. - Assume no prior knowledge of the dataset apart from its name and general API use. **Function Signature:** ```python def preprocess_california_housing() -> dict: pass ``` **Input:** No inputs are required directly for this function. **Output:** A dictionary with keys: - \'X_train_shape\': shape of the training data features. - \'X_test_shape\': shape of the testing data features. - \'y_train_shape\': shape of the training data target. - \'y_test_shape\': shape of the testing data target. **Steps to Implement:** 1. Import the necessary libraries and functions. 2. Fetch the \'California Housing\' dataset. 3. Handle any missing values. 4. Standardize the features using an appropriate scaler. 5. Split the data into training and testing sets. 6. Return the shapes of the resulting subsets in a dictionary. **Example:** ```python def preprocess_california_housing() -> dict: from sklearn.datasets import fetch_california_housing from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler import numpy as np # Fetch dataset data = fetch_california_housing() X, y = data.data, data.target # Handle missing values (if any) X = np.nan_to_num(X) # Standardize features scaler = StandardScaler() X = scaler.fit_transform(X) # Split the dataset X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Store shapes in a dictionary and return result = { \'X_train_shape\': X_train.shape, \'X_test_shape\': X_test.shape, \'y_train_shape\': y_train.shape, \'y_test_shape\': y_test.shape } return result ``` The solution provided as an example follows the outlined steps, ensuring that all necessary preprocessing is performed, and the final output is a dictionary with the shapes of the training and testing subsets.","solution":"def preprocess_california_housing() -> dict: from sklearn.datasets import fetch_california_housing from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler import numpy as np # Fetch dataset data = fetch_california_housing() X, y = data.data, data.target # Handle missing values (if any) X = np.nan_to_num(X) # Standardize features scaler = StandardScaler() X = scaler.fit_transform(X) # Split the dataset X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Store shapes in a dictionary and return result = { \'X_train_shape\': X_train.shape, \'X_test_shape\': X_test.shape, \'y_train_shape\': y_train.shape, \'y_test_shape\': y_test.shape } return result"},{"question":"**Question:** As a Pythonista, you may sometimes need to ensure the \'pip\' package installer is available in your Python environment programmatically. To assess your understanding of the `ensurepip` package, you are required to write a function that uses this package to check the current version of \\"pip\\" available and conditionally bootstrap it based on specific conditions. # Task: Write a Python function `bootstrap_pip_if_needed(upgrade=False, user_install=False, verbosity_level=0)` that: 1. Checks the current version of \\"pip\\" available using `ensurepip.version()`. 2. If \\"pip\\" is not installed or the current version is lower than a specified minimum version (e.g., \'21.0\'), it bootstraps \\"pip\\" using `ensurepip.bootstrap()` with the provided parameters: - `upgrade`: If set to `True`, it should upgrade an existing \\"pip\\" installation. - `user_install`: If set to `True`, it should install \\"pip\\" in the user site packages directory. - `verbosity_level`: Control the verbosity of the output during the bootstrapping process. # Parameters: - `upgrade` (boolean): Optional; default is `False`. If `True`, \\"pip\\" should be upgraded to the latest available version included in \\"ensurepip\\". - `user_install` (boolean): Optional; default is `False`. If `True`, use the user site packages directory instead of installing globally. - `verbosity_level` (integer): Optional; default is `0`. Controls the verbosity of the bootstrapping process output. # Constraints: - Do not access the internet during the execution of this function. - Handle any exceptions that may occur during the execution of `ensurepip.bootstrap()`. # Example Usage: ```python def bootstrap_pip_if_needed(upgrade=False, user_install=False, verbosity_level=0): # Your implementation here pass # This should check the existing version, and if needed, bootstrap \'pip\' bootstrap_pip_if_needed(upgrade=True, user_install=True, verbosity_level=1) ``` Expected Flow: 1. Check the version of `pip` available using `ensurepip.version()`. 2. Compare this version to the predefined minimum version (e.g., \\"21.0\\"). 3. If the current version is less than the minimum version or if there is no version, call `ensurepip.bootstrap()` with appropriate arguments. # Note: You can use Python\'s built-in comparison capabilities to compare version strings. Here\'s a helper function if needed: ```python def is_version_less(current_version, min_version): current_parts = list(map(int, current_version.split(\'.\'))) min_parts = list(map(int, min_version.split(\'.\'))) return current_parts < min_parts ``` # Submission: Submit your implementation of the `bootstrap_pip_if_needed` function, and any additional helper functions you used.","solution":"import ensurepip import subprocess def is_version_less(current_version, min_version): current_parts = list(map(int, current_version.split(\'.\'))) min_parts = list(map(int, min_version.split(\'.\'))) return current_parts < min_parts def bootstrap_pip_if_needed(upgrade=False, user_install=False, verbosity_level=0): try: # Get current pip version using subprocess current_version = subprocess.check_output([\\"python\\", \\"-m\\", \\"pip\\", \\"--version\\"], universal_newlines=True) current_version = current_version.split()[1] if is_version_less(current_version, \\"21.0\\"): print(\\"Current pip version is less than 21.0. Bootstrapping pip...\\") ensurepip.bootstrap(upgrade=upgrade, user=user_install, verbosity=verbosity_level) else: print(f\\"Current pip version ({current_version}) is >= 21.0. No need to bootstrap.\\") except subprocess.CalledProcessError: print(\\"pip is not installed. Bootstrapping pip...\\") ensurepip.bootstrap(upgrade=upgrade, user=user_install, verbosity=verbosity_level) except Exception as e: print(f\\"An unexpected error occurred: {e}\\")"},{"question":"**Coding Assessment Question: Data Round Trip Conversion using XDR** **Objective**: Demonstrate the comprehension of the `xdrlib` module in Python by implementing a round trip data conversion process using both `Packer` and `Unpacker` classes. **Problem Statement**: You are tasked with implementing two functions: `pack_data` and `unpack_data`. 1. `pack_data` should take a dictionary containing various data types and encode it into an XDR formatted byte stream. 2. `unpack_data` should take the byte stream generated by `pack_data` and decode it back into the original dictionary format. You must handle the following data types within the dictionary: - Integers (`int`) - Unsigned Integers (`uint`) - Floats (`float`) - Strings (`str`) - Lists of Integers (`List[int]`) # Function Signatures ```python import xdrlib def pack_data(data: dict) -> bytes: Packs the provided dictionary containing various data types into an XDR formatted byte stream. Args: data (dict): The dictionary to pack. Example format: { \\"int_value\\": 10, \\"uint_value\\": 20, \\"float_value\\": 3.14, \\"string_value\\": \\"hello\\", \\"int_list\\": [1, 2, 3] } Returns: bytes: The XDR formatted byte stream. # Your implementation here def unpack_data(packed_data: bytes) -> dict: Unpacks the given XDR formatted byte stream into the original dictionary format. Args: packed_data (bytes): The XDR formatted byte stream produced by the pack_data function. Returns: dict: The unpacked dictionary containing the original data. # Your implementation here ``` # Constraints: - All integer values (both `int` and `uint`) will be non-negative and less than 2^31. - Strings will have a maximum length of 256 characters. - Lists will contain up to 100 integers. # Example: ```python # Example data to be packed data = { \\"int_value\\": 10, \\"uint_value\\": 20, \\"float_value\\": 3.14, \\"string_value\\": \\"hello\\", \\"int_list\\": [1, 2, 3] } # Function to pack the data packed_data = pack_data(data) print(packed_data) # This will print the XDR formatted bytes # Function to unpack the data unpacked_data = unpack_data(packed_data) print(unpacked_data) # This should print the original dictionary ``` # Notes - Your implementation should handle possible exceptions such as conversion errors during packing and unpacking. - Ensure the order of packing and unpacking is consistent to preserve data integrity. - Utilize the `xdrlib` module effectively as per the provided documentation to complete your solution. **Evaluation Criteria**: - Correctness and completeness of the implementation. - Proper handling and conversion of different data types. - Correct usage of `xdrlib` methods. - Code readability and adherence to Python coding conventions.","solution":"import xdrlib def pack_data(data: dict) -> bytes: Packs the provided dictionary containing various data types into an XDR formatted byte stream. packer = xdrlib.Packer() packer.pack_int(data[\\"int_value\\"]) packer.pack_uint(data[\\"uint_value\\"]) packer.pack_double(data[\\"float_value\\"]) # Using pack_double for floats packer.pack_string(data[\\"string_value\\"].encode(\'utf-8\')) packer.pack_int(len(data[\\"int_list\\"])) for item in data[\\"int_list\\"]: packer.pack_int(item) return packer.get_buffer() def unpack_data(packed_data: bytes) -> dict: Unpacks the given XDR formatted byte stream into the original dictionary format. unpacker = xdrlib.Unpacker(packed_data) int_value = unpacker.unpack_int() uint_value = unpacker.unpack_uint() float_value = unpacker.unpack_double() string_value = unpacker.unpack_string().decode(\'utf-8\') int_list_length = unpacker.unpack_int() int_list = [unpacker.unpack_int() for _ in range(int_list_length)] return { \\"int_value\\": int_value, \\"uint_value\\": uint_value, \\"float_value\\": float_value, \\"string_value\\": string_value, \\"int_list\\": int_list }"},{"question":"Objective Write a class `CustomCodecRegistry` that manages codec registration, encoding, decoding, and error handling using the functions described in the provided documentation. The class should demonstrate the student\'s understanding of Python\'s codec registry and support functions. Requirements 1. **Class Constructor:** - Initialize an empty codec registry. 2. **Methods:** - `register_codec(search_function)`: Registers a new codec search function. - `unregister_codec(search_function)`: Unregisters a codec search function. - `is_known_encoding(encoding)`: Checks if the given encoding is known. - `encode_object(obj, encoding, errors=None)`: Encodes the given object using the specified encoding and error handling method. - `decode_object(obj, encoding, errors=None)`: Decodes the given object using the specified encoding and error handling method. - `register_error(name, error_callback)`: Registers a new error handling callback function. - `lookup_error(name)`: Looks up the error handling callback function by name. 3. **Expected Input and Output Formats:** - Input: Methods may take strings, callables, or other objects as needed. - Output: Methods should return appropriate responses as described (e.g., boolean, encoded/decoded objects, error handling functions). 4. **Constraints and Limitations:** - Codec registration should ensure that the \\"encodings\\" package is always prioritized. - Error handling functions should properly manage encoding/decoding interruptions. 5. **Performance Requirements:** - Ensure efficient management and lookup within the codec registry. - Handle large objects gracefully during encoding and decoding operations. Code Implementation Implement the `CustomCodecRegistry` as described. Handle all necessary exceptions and edge cases to ensure robustness. Use the provided functions from the documentation where applicable. ```python class CustomCodecRegistry: def __init__(self): self.registry = {} self.error_handlers = {} def register_codec(self, search_function): # Implement codec registration functionality pass def unregister_codec(self, search_function): # Implement codec unregistration functionality pass def is_known_encoding(self, encoding): # Implement encoding check functionality pass def encode_object(self, obj, encoding, errors=None): # Implement object encoding functionality pass def decode_object(self, obj, encoding, errors=None): # Implement object decoding functionality pass def register_error(self, name, error_callback): # Implement error callback registration functionality pass def lookup_error(self, name): # Implement error callback lookup functionality pass # You can add additional methods or inner classes if needed to fully implement the functionality. ``` Test Cases Write tests to validate your implementation, ensuring all methods work as expected. Handle various edge cases and ensure the implementation is robust. ```python def test_custom_codec_registry(): # Example test cases to validate CustomCodecRegistry registry = CustomCodecRegistry() # Test registering and unregistering codecs def dummy_search_function(): pass registry.register_codec(dummy_search_function) assert registry.is_known_encoding(\\"dummy_encoding\\") == False # Test encoding and decoding obj = \\"hello\\" encoded_obj = registry.encode_object(obj, \\"utf-8\\") decoded_obj = registry.decode_object(encoded_obj, \\"utf-8\\") assert decoded_obj == obj # Test registering and looking up error handlers def dummy_error_handler(exc): return (\\"?\\", exc.end) registry.register_error(\\"dummy_error\\", dummy_error_handler) handler = registry.lookup_error(\\"dummy_error\\") assert handler == dummy_error_handler test_custom_codec_registry() ``` Explanation - `register_codec` and `unregister_codec` manage the registration and unregistration of codec search functions. - `is_known_encoding` checks if a specific encoding is known. - `encode_object` and `decode_object` manage encoding and decoding with proper error handling. - `register_error` and `lookup_error` handle registration and lookup of error handling callback functions. - The implementation should leverage the provided functions where applicable to ensure proper codec management and error handling. Ensure the implementation closely follows the guidelines and handles various edge cases effectively.","solution":"import codecs class CustomCodecRegistry: def __init__(self): self.registry = {} # Registry to hold custom codec search functions self.error_handlers = {} # Registry to hold custom error handlers def register_codec(self, search_function): codecs.register(search_function) def unregister_codec(self, search_function): # There\'s no direct method to unregister a codec, but we can remove it from the registry # Note: This won\'t remove it from the global registry if it has already been registered if search_function in self.registry: self.registry.pop(search_function) def is_known_encoding(self, encoding): try: return codecs.lookup(encoding) is not None except LookupError: return False def encode_object(self, obj, encoding, errors=\'strict\'): if isinstance(obj, str): return obj.encode(encoding, errors) raise TypeError(\\"Object to be encoded is not a string\\") def decode_object(self, obj, encoding, errors=\'strict\'): if isinstance(obj, bytes): return obj.decode(encoding, errors) raise TypeError(\\"Object to be decoded is not a byte string\\") def register_error(self, name, error_callback): codecs.register_error(name, error_callback) def lookup_error(self, name): return codecs.lookup_error(name)"},{"question":"# Advanced Coding Assessment: Custom Set Manipulations in Python You are required to implement advanced functionalities for set manipulations in Python. This will involve creating and operating on `set` and `frozenset` types. You should demonstrate your understanding of these operations by completing the following tasks: **Problem**: Implement a series of functions that will operate on sets and frozensets. Function 1: Create Set from Iterable Write a function `create_set` that takes an iterable and returns a new set. ```python def create_set(iterable): Create a new set from the given iterable. Parameters: iterable: An iterable from which to create the set. Can be None. Returns: set: A new set containing the elements from the iterable. pass ``` Function 2: Check Element in Set Write a function `contains_element` that checks if a key is in the given set. ```python def contains_element(s, key): Check if a key is present in the set. Parameters: s (set): The set to check. key: The element to find in the set. Returns: bool: True if the key is in the set, False otherwise. pass ``` Function 3: Add Element to Set Write a function `add_element` that adds a key to a set. ```python def add_element(s, key): Add a key to the set. Parameters: s (set): The set to which the key will be added. key: The element to add to the set. Returns: set: The set after adding the key. pass ``` Function 4: Remove Element from Set Write a function `remove_element` that removes a key from the set if it exists. ```python def remove_element(s, key): Remove a key from the set. Parameters: s (set): The set from which the key will be removed. key: The element to remove from the set. Returns: set: The set after removing the key. pass ``` Function 5: Clear Set Write a function `clear_set` that clears all elements from the set. ```python def clear_set(s): Clear all elements from the set. Parameters: s (set): The set to clear. Returns: set: The cleared set. pass ``` **Constraints**: - You are not allowed to use built-in methods like `add`, `discard`, or `clear` directly. - You must handle cases where the provided iterable is `None` gracefully. - Raise appropriate errors where necessary. Each function needs to be tested independently. Ensure test cases cover typical input permutations and edge cases to validate function correctness. **Example Usage**: ```python s = create_set([1, 2, 3]) print(contains_element(s, 2)) # Output: True print(contains_element(s, 4)) # Output: False s = add_element(s, 4) print(s) # Output: {1, 2, 3, 4} s = remove_element(s, 2) print(s) # Output: {1, 3, 4} s = clear_set(s) print(s) # Output: set() ``` Ensure your implementation is efficient and follows best programming practices.","solution":"def create_set(iterable): Create a new set from the given iterable. Parameters: iterable: An iterable from which to create the set. Can be None. Returns: set: A new set containing the elements from the iterable. return set() if iterable is None else set(iterable) def contains_element(s, key): Check if a key is present in the set. Parameters: s (set): The set to check. key: The element to find in the set. Returns: bool: True if the key is in the set, False otherwise. for element in s: if element == key: return True return False def add_element(s, key): Add a key to the set. Parameters: s (set): The set to which the key will be added. key: The element to add to the set. Returns: set: The set after adding the key. if not contains_element(s, key): s.add(key) return s def remove_element(s, key): Remove a key from the set. Parameters: s (set): The set from which the key will be removed. key: The element to remove from the set. Returns: set: The set after removing the key. return {element for element in s if element != key} def clear_set(s): Clear all elements from the set. Parameters: s (set): The set to clear. Returns: set: The cleared set. return set()"},{"question":"You are required to implement a function `check_url_access(robots_url: str, user_agent: str, urls: list) -> dict`. Given the URL of a `robots.txt` file, a user agent, and a list of URLs, this function should determine if the user agent is allowed to access each URL. # Function Signature ```python def check_url_access(robots_url: str, user_agent: str, urls: list) -> dict: ``` # Input - `robots_url` (str): The URL pointing to the `robots.txt` file. - `user_agent` (str): The user agent string to check for access permissions. - `urls` (list): A list of URLs (str) to check for access permissions. # Output - `dict`: A dictionary where the keys are the URLs from the input list and the values are `True` (if the user agent is allowed to fetch the URL) or `False` (if it is not). # Example ```python robots_url = \\"http://www.example.com/robots.txt\\" user_agent = \\"MyUserAgent\\" urls = [ \\"http://www.example.com/page1\\", \\"http://www.example.com/private\\", \\"http://www.example.com/public\\" ] access_permissions = check_url_access(robots_url, user_agent, urls) print(access_permissions) # Output may look like: # { # \\"http://www.example.com/page1\\": True, # \\"http://www.example.com/private\\": False, # \\"http://www.example.com/public\\": True # } ``` # Constraints 1. The input list of URLs will have at most 100 URLs. 2. Assume the URLs will be well-formed and the `robots.txt` file is present at the provided `robots_url`. 3. Use the `urllib.robotparser.RobotFileParser` class from the Python Standard Library. # Notes - Ensure that your function handles any potential network issues gracefully. - You might want to consider using retries or handling specific exceptions to ensure robustness. # Hints - Use the `set_url` method to set the URL of the `robots.txt` file. - Use the `read` method to read and parse the `robots.txt` file. - Use the `can_fetch` method to determine if the user agent can fetch each URL.","solution":"import urllib.robotparser def check_url_access(robots_url: str, user_agent: str, urls: list) -> dict: Check if the given user_agent is allowed to access each URL according to the robots.txt file. :param robots_url: URL pointing to the robots.txt file. :param user_agent: User agent string to check for access permissions. :param urls: List of URLs to check for access permissions. :return: Dictionary with the URLs as keys and boolean values indicating access permission. rp = urllib.robotparser.RobotFileParser() rp.set_url(robots_url) rp.read() permissions = {} for url in urls: permissions[url] = rp.can_fetch(user_agent, url) return permissions"},{"question":"Advanced Set Operations Objective You are tasked with implementing a function that utilizes various Python `set` and `frozenset` operations. This function should demonstrate your comprehension of creating sets, adding and removing elements, checking for membership, and handling both hashable and unhashable elements. Task Write a function named `manage_sets` that takes two lists as input and performs various operations using `set` and `frozenset`. Function Signature ```python def manage_sets(list1: list, list2: list) -> tuple: ``` Input * `list1`: A list of elements which can contain both hashable and unhashable elements (for example, numbers, strings, and other lists or sets). * `list2`: A list of hashable elements. Output The function should return a tuple containing: 1. A set created from `list1` with all hashable elements added. Any unhashable elements should be skipped. 2. A frozenset created from `list2`. 3. The length of the set created from `list1`. 4. The length of the frozenset created from `list2`. Constraints * The elements of `list2` are guaranteed to be hashable. * For `list1`, you must properly handle unhashable elements (e.g., lists) and skip them without raising an error. * You should use the functions and macros provided by the Python 3.10 `set` and `frozenset` API as much as possible. Example ```python # Given the following inputs list1 = [1, 2, 3, [4, 5], 6, 7, 8] list2 = [\'a\', \'b\', \'c\'] # Calling the function result = manage_sets(list1, list2) # Expected output # (set([1, 2, 3, 6, 7, 8]), frozenset({\'a\', \'b\', \'c\'}), 5, 3) print(result) ``` The function should handle creation of sets, adding of elements, skipping unhashable items, creating frozensets, and returning the required outputs efficiently and correctly. Performance Requirements The function should efficiently handle input lists with up to 10,000 elements.","solution":"def manage_sets(list1: list, list2: list) -> tuple: Perform various set and frozenset operations. Parameters: list1 (list): A list that may contain both hashable and unhashable elements. list2 (list): A list that contains only hashable elements. Returns: tuple: A tuple containing: - A set created from hashable elements of list1. - A frozenset created from list2. - The length of the set created from hashable elements of list1. - The length of the frozenset created from list2. # Create a set from list1, ignoring unhashable items hashable_elements_set = set() for item in list1: try: hashable_elements_set.add(item) except TypeError: continue # Create a frozenset from list2 hashable_elements_frozenset = frozenset(list2) # Get the lengths of both created sets len_set = len(hashable_elements_set) len_frozenset = len(hashable_elements_frozenset) return hashable_elements_set, hashable_elements_frozenset, len_set, len_frozenset"},{"question":"Objective Implement a function that ensures the reproducibility of tensor operations using PyTorch\'s deterministic setting while managing uninitialized memory properly. You need to demonstrate the ability to work with PyTorch\'s `torch.utils.deterministic` module and handle scenarios of uninitialized memory using the provided attributes and functions. Problem Statement Write a function `create_deterministic_tensors` that performs the following tasks: 1. Ensure that deterministic algorithms are enabled in PyTorch. 2. Utilize the `fill_uninitialized_memory` attribute to decide if uninitialized memory should be filled with known values. 3. Create three tensors: - A tensor created using `torch.Tensor.resize_`. - A tensor created using `torch.empty`. - A tensor created using `torch.empty_like`. 4. Return these three tensors. Function Signature ```python def create_deterministic_tensors(fill_memory: bool) -> tuple: pass ``` Input - `fill_memory` (bool): A boolean flag indicating whether uninitialized memory should be filled with known values (True) or not (False). Output - A tuple containing three PyTorch tensors corresponding to: - A tensor created using `torch.Tensor.resize_`. - A tensor created using `torch.empty`. - A tensor created using `torch.empty_like`. Constraints - For the tensor created using `torch.Tensor.resize_`, initialize it to have shape `(2, 3)`. - For the tensor created using `torch.empty`, specify the shape as `(3, 3)`. - For the tensor created using `torch.empty_like`, initialize it similarly to an existing `torch.ones` tensor of shape `(4, 4)`. Example ```python fill_memory = True tensor1, tensor2, tensor3 = create_deterministic_tensors(fill_memory) # tensor1.shape should be (2, 3) # tensor2.shape should be (3, 3) # tensor3.shape should be (4, 4) and filled with uninitialized memory ``` Performance Requirement - The function should set configurations and create tensors efficiently. Notes - Ensure to include any necessary imports from PyTorch at the beginning of your code. - Document your code to explain the steps clearly. - Use appropriate methods and PyTorch features to meet the problem requirements.","solution":"import torch def create_deterministic_tensors(fill_memory: bool) -> tuple: Ensure deterministic settings for tensor operations in PyTorch and manage uninitialized memory using the fill_memory parameter. Args: fill_memory (bool): Whether to fill uninitialized memory with known values. Returns: tuple: Three PyTorch tensors created with resize_, empty, and empty_like methods. # Ensure deterministic algorithms in PyTorch torch.use_deterministic_algorithms(True) # Optionally fill uninitialized memory with known values torch.backends.cudnn.deterministic = True if fill_memory: torch.backends.cudnn.benchmark = False else: torch.backends.cudnn.benchmark = True # Create the tensors tensor1 = torch.Tensor().resize_(2, 3) tensor2 = torch.empty(3, 3) tensor3 = torch.empty_like(torch.ones(4, 4)) return tensor1, tensor2, tensor3"},{"question":"# Question: Implementing and Recording Custom Events in PyTorch **Objective:** Using PyTorch\'s `torch.distributed.elastic.events` module, implement a function that records a custom event with specific metadata and outputs the event as a formatted string. This will demonstrate the student\'s ability to interact with the logging and event recording mechanisms provided by the module. **Function Signature:** ```python def record_custom_event(event_name: str, event_source: str, metadata: dict) -> str: Records a custom event using the torch.distributed.elastic.events module and returns a formatted string of the event. Parameters: event_name (str): The name of the event to record. event_source (str): The source of the event. metadata (dict): A dictionary containing metadata for the event. Returns: str: A formatted string describing the recorded event. ``` **Inputs:** 1. `event_name` (str): The name of the event, for example, \\"training_started\\". 2. `event_source` (str): The source of the event, such as \\"user_initiated\\". 3. `metadata` (dict): A dictionary of metadata associated with the event, with key-value pairs. **Output:** - A string that describes the event, source, and metadata in a readable format. **Constraints:** - Ensure the function integrates with the `torch.distributed.elastic.events` module. - Utilize the provided event recording APIs. **Example:** ```python event_name = \\"training_started\\" event_source = \\"user_initiated\\" metadata = { \\"epoch\\": 1, \\"batch_size\\": 32, \\"lr\\": 0.001 } result = record_custom_event(event_name, event_source, metadata) print(result) # Expected Output: # \\"Event recorded: training_started from user_initiated with metadata {\'epoch\': 1, \'batch_size\': 32, \'lr\': 0.001}\\" ``` **Hints:** - Look into the `torch.distributed.elastic.events.record` function to understand how to log events. - Utilize the `Event` and `EventSource` classes to encapsulate event information. Implement the function considering the provided API documentation.","solution":"import torch.distributed.elastic.events as events def record_custom_event(event_name: str, event_source: str, metadata: dict) -> str: Records a custom event using the torch.distributed.elastic.events module and returns a formatted string of the event. Parameters: event_name (str): The name of the event to record. event_source (str): The source of the event. metadata (dict): A dictionary containing metadata for the event. Returns: str: A formatted string describing the recorded event. event = events.Event(name=event_name, source=event_source, metadata=metadata) event_str = f\\"Event recorded: {event_name} from {event_source} with metadata {metadata}\\" events.record(event) return event_str"},{"question":"You are working on an anomaly detection project for a large company that wants to identify fraudulent transactions. You have a clean dataset of normal transactions and need to create a model to identify outliers in new, unseen transaction data. Your task is to demonstrate your understanding of novelty detection using the `neighbors.LocalOutlierFactor` method from sklearn. Implement a class `TransactionAnomalyDetector` with the following methods: - `__init__(self, n_neighbors: int, contamination: float)`: Initializes the detector with the given number of neighbors and contamination level. - `fit(self, X: np.ndarray)`: Trains the LOF model on the provided dataset X. - `predict(self, X: np.ndarray) -> np.ndarray`: Predicts whether the transactions in X are inliers (1) or outliers (-1) based on the trained model. - `decision_function(self, X: np.ndarray) -> np.ndarray`: Returns anomaly scores for the transactions in X, where negative values indicate outliers. - `get_anomaly_scores(self) -> np.ndarray`: Returns the anomaly scores of the training data. # Input - `n_neighbors` (int): The number of neighbors to use for the LOF algorithm. - `contamination` (float): The proportion of outliers in the dataset (should be between 0.0 and 0.5). - `X` (np.ndarray): A 2D numpy array where each row represents a transaction with several features. # Output - `predict`: Returns a numpy array where each element is 1 if the corresponding transaction is an inlier, and -1 if it is an outlier. - `decision_function`: Returns a numpy array of anomaly scores for the transactions in X. - `get_anomaly_scores`: Returns a numpy array of anomaly scores of the training data. # Example ```python import numpy as np from transaction_anomaly import TransactionAnomalyDetector # Sample data X_train = np.array([[1.0, 1.1], [1.2, 1.0], [0.9, 1.3], [1.1, 0.9]]) X_test = np.array([[1.1, 1.2], [10.0, 10.0]]) # Create model detector = TransactionAnomalyDetector(n_neighbors=2, contamination=0.1) detector.fit(X_train) # Predict labels print(detector.predict(X_test)) # Output Example: [ 1 -1 ] # Get decision scores print(detector.decision_function(X_test)) # Output Example: [ 0.259843 -1.200841 ] # Get training data anomaly scores print(detector.get_anomaly_scores()) # Output Example: [-1.200841 -1.145984 -1.197465 -1.131435] ``` **Note**: - Use sklearn\'s `neighbors.LocalOutlierFactor` with the `novelty=True` parameter. - Ensure the class encapsulates the functionality properly and handles invalid inputs gracefully. # Constraints 1. You must use the `neighbors.LocalOutlierFactor` algorithm for this task. 2. The `contamination` rate should be correctly utilized when initializing the model. 3. The model should only be used for novelty detection on new, unseen data as per requirements.","solution":"from sklearn.neighbors import LocalOutlierFactor import numpy as np class TransactionAnomalyDetector: def __init__(self, n_neighbors: int, contamination: float): self.n_neighbors = n_neighbors self.contamination = contamination self.model = LocalOutlierFactor(n_neighbors=n_neighbors, contamination=contamination, novelty=True) self.training_scores = None def fit(self, X: np.ndarray): self.model.fit(X) # Storing the negative_outlier_factor_ of the training data for get_anomaly_scores self.training_scores = -self.model.negative_outlier_factor_ def predict(self, X: np.ndarray) -> np.ndarray: return self.model.predict(X) def decision_function(self, X: np.ndarray) -> np.ndarray: return self.model.decision_function(X) def get_anomaly_scores(self) -> np.ndarray: if self.training_scores is None: raise RuntimeError(\\"Model has not been trained yet. Call fit() before get_anomaly_scores().\\") return self.training_scores"},{"question":"# Question: Custom Statistical Plot with Seaborn You are given the `diamonds` dataset which contains information about the prices and attributes of diamonds. Your task is to create and customize a statistical plot using the seaborn package to visualize the distribution and estimation statistics of carat weights across different clarity grades in the dataset. Implementation Requirements: 1. **Load the Dataset**: Load the `diamonds` dataset using seaborn. 2. **Create a Plot**: Create a plot with: - `clarity` on the x-axis. - `carat` on the y-axis. 3. **Customize the Plot**: - Add a range plot with the mean and 95% confidence interval. - Additionally, create another version of the plot with the median carat value and standard deviation error bars. 4. **Weighted Estimates**: Create a plot showcasing weighted mean carat values using the `price` as the weight variable. 5. **Randomness Control**: Ensure reproducibility by setting a random seed for bootstrapping. Expected Output: - Three plots showing: 1. Mean carat and 95% confidence interval. 2. Median carat and standard deviation error bars. 3. Weighted mean carat values with `price` as weights. Constraints: - Ensure that the error bars and estimations are clearly visible and properly labeled. - The plots should be self-contained and should not rely on external datasets or inputs other than the `diamonds` dataset from seaborn. Performance: - Your implementation should efficiently handle the input dataset and render the plots within a reasonable timeframe. Example Code Template: ```python # Import necessary libraries import seaborn.objects as so from seaborn import load_dataset # Load the diamonds dataset diamonds = load_dataset(\\"diamonds\\") # Create the first plot (mean and 95% confidence interval) p1 = so.Plot(diamonds, \\"clarity\\", \\"carat\\") p1.add(so.Range(), so.Est()) # Customize other plots based on requirements # Create the second plot (median and standard deviation error bars) p2 = so.Plot(diamonds, \\"clarity\\", \\"carat\\") p2.add(so.Range(), so.Est(errorbar=\\"sd\\")) # Create the third plot (weighted mean carat values) p3 = so.Plot(diamonds, \\"clarity\\", \\"carat\\") p3.add(so.Range(), so.Est(), weight=\\"price\\") # Ensure reproducibility for the first plot p1.add(so.Range(), so.Est(seed=0)) # Display all plots p1.show() p2.show() p3.show() ``` Work on the template provided to complete the required plots and ensure all specified customizations are implemented correctly.","solution":"import seaborn as sns import matplotlib.pyplot as plt import pandas as pd # Load the diamonds dataset diamonds = sns.load_dataset(\\"diamonds\\") # Plot 1: Mean carat and 95% confidence interval plt.figure(figsize=(10, 6)) sns.pointplot(x=\\"clarity\\", y=\\"carat\\", data=diamonds, ci=95, estimator=\'mean\') plt.title(\'Mean Carat and 95% Confidence Interval\') plt.show() # Plot 2: Median carat value and standard deviation error bars plt.figure(figsize=(10, 6)) sns.pointplot(x=\\"clarity\\", y=\\"carat\\", data=diamonds, ci=\'sd\', estimator=\'median\') plt.title(\'Median Carat and Standard Deviation Error Bars\') plt.show() # Plot 3: Weighted mean carat values with price as weights diamonds[\'weighted_carat\'] = diamonds[\'carat\'] * diamonds[\'price\'] weighted_mean_data = diamonds.groupby(\'clarity\').apply( lambda x: (x[\'weighted_carat\'].sum() / x[\'price\'].sum()) ).reset_index(name=\'weighted_mean_carat\') plt.figure(figsize=(10, 6)) sns.barplot(x=\\"clarity\\", y=\\"weighted_mean_carat\\", data=weighted_mean_data) plt.title(\'Weighted Mean Carat Values with Price as Weights\') plt.show()"},{"question":"**Objective**: The task is to implement a custom function that analyzes a given Python function to detect potential inefficiencies. **Problem Statement**: Write a Python function `analyze_function_efficiency(py_func)` that takes a Python function object `py_func` as input and returns a dictionary with the counts of different types of bytecode instructions used in the function. The dictionary should have the following keys: - `binary_operations`: Count of all binary operation instructions. - `inplace_operations`: Count of all in-place operation instructions. - `unary_operations`: Count of all unary operation instructions. - `function_calls`: Count of all function call instructions (`CALL_FUNCTION`, `CALL_FUNCTION_KW`, `CALL_FUNCTION_EX`). **Input Format**: - `py_func`: A Python function object. **Output Format**: - A dictionary with the mentioned keys and corresponding integer counts. **Constraints**: 1. The input `py_func` will always be a valid Python function. 2. The function should handle any valid bytecode in the CPython 3.10 interpreter. **Example**: ```python def sample_function(a, b): return a + b result = analyze_function_efficiency(sample_function) print(result) # Example output: {\'binary_operations\': 1, \'inplace_operations\': 0, \'unary_operations\': 0, \'function_calls\': 0} ``` **Implementation Requirements**: 1. Use the `dis` module to disassemble the given function and analyze its bytecode instructions. 2. Include meaningful comments for clarity. 3. Optimize for readability and performance. **Hints**: - Refer to the `dis` module documentation for details on how to extract and analyze bytecode instructions. - Focus on instructions listed under binary, in-place, and unary operations, and function call instructions.","solution":"import dis def analyze_function_efficiency(py_func): Analyzes the given Python function object to detect potential inefficiencies. Args: py_func (function): Python function object to be analyzed. Returns: dict: A dictionary with counts of different types of bytecode instructions. bytecode = dis.Bytecode(py_func) counts = { \'binary_operations\': 0, \'inplace_operations\': 0, \'unary_operations\': 0, \'function_calls\': 0 } binary_ops = { \'BINARY_POWER\', \'BINARY_MULTIPLY\', \'BINARY_MODULO\', \'BINARY_ADD\', \'BINARY_SUBTRACT\', \'BINARY_SUBSCR\', \'BINARY_FLOOR_DIVIDE\', \'BINARY_TRUE_DIVIDE\', \'BINARY_LSHIFT\', \'BINARY_RSHIFT\', \'BINARY_AND\', \'BINARY_XOR\', \'BINARY_OR\', \'BINARY_MATRIX_MULTIPLY\' } inplace_ops = { \'INPLACE_POWER\', \'INPLACE_MULTIPLY\', \'INPLACE_MODULO\', \'INPLACE_ADD\', \'INPLACE_SUBTRACT\', \'INPLACE_SUBSCR\', \'INPLACE_FLOOR_DIVIDE\', \'INPLACE_TRUE_DIVIDE\', \'INPLACE_LSHIFT\', \'INPLACE_RSHIFT\', \'INPLACE_AND\', \'INPLACE_XOR\', \'INPLACE_OR\', \'INPLACE_MATRIX_MULTIPLY\' } unary_ops = {\'UNARY_POSITIVE\', \'UNARY_NEGATIVE\', \'UNARY_NOT\', \'UNARY_INVERT\'} call_ops = {\'CALL_FUNCTION\', \'CALL_FUNCTION_KW\', \'CALL_FUNCTION_EX\'} for instruction in bytecode: opname = instruction.opname if opname in binary_ops: counts[\'binary_operations\'] += 1 elif opname in inplace_ops: counts[\'inplace_operations\'] += 1 elif opname in unary_ops: counts[\'unary_operations\'] += 1 elif opname in call_ops: counts[\'function_calls\'] += 1 return counts"},{"question":"# **Memory Leak Detection using tracemalloc** Objective: Your task is to implement a function `detect_memory_leak` that utilizes the `tracemalloc` module to identify and report potential memory leaks in a given Python script. Function Signature: ```python def detect_memory_leak(script_path: str, interval: int = 5): pass ``` Inputs: 1. `script_path` (string): The path to the Python script file that needs to be analyzed. 2. `interval` (int): The interval in seconds after which to take snapshots of memory allocation. Default is 5 seconds. Outputs: The function should output: - The top 10 lines of code (file name and line number) with the highest increase in memory allocation, from the start of the script until the end of the interval. Constraints: - The provided script can be any valid Python script. - You should capture memory allocations from the start till the end of the specified interval. Example: Assume you have a script `example_script.py` with the following content: ```python import time def run(): big_list = [] for i in range(100000): big_list.append(i) print(\\"Big list created\\") if __name__ == \\"__main__\\": run() time.sleep(10) # Simulate a running process ``` For this script, calling `detect_memory_leak(\'example_script.py\', 5)` should output something similar to: ``` [ Top 10 Differences ] <file_name>:<line_number>: size=<size_diff>, count=<count_diff> ... ``` Implementation Notes: 1. Use the `tracemalloc.start()`, `tracemalloc.take_snapshot()`, and `tracemalloc.compare_to()` methods. 2. Ensure the script provided runs within your function and memory traces are captured before and after the interval using the `tracemalloc` module. 3. Print the pertinent memory differences as specified. # **Sample Solution:** Below is a skeleton solution to help you get started: ```python import tracemalloc import subprocess import time def detect_memory_leak(script_path: str, interval: int = 5): # Start tracing memory allocations tracemalloc.start() # Run the script process = subprocess.Popen([\'python\', script_path]) # Wait for the interval before taking another snapshot time.sleep(interval) # Take snapshots of memory allocations snapshot1 = tracemalloc.take_snapshot() time.sleep(interval) snapshot2 = tracemalloc.take_snapshot() # Stop the process process.terminate() # Compare snapshots top_stats = snapshot2.compare_to(snapshot1, \'lineno\') # Print the top 10 differences print(\\"[ Top 10 Differences ]\\") for stat in top_stats[:10]: print(f\\"{stat.traceback[0].filename}:{stat.traceback[0].lineno}: size={stat.size_diff}, count={stat.count_diff}\\") # Example Usage detect_memory_leak(\'example_script.py\', 5) ``` The above solution should be refined and implemented to meet the instruction specifications and handle edge cases accordingly.","solution":"import tracemalloc import time import subprocess def detect_memory_leak(script_path: str, interval: int = 5): Detects memory leaks in a given Python script by taking snapshots of memory allocations over a specified interval and reporting the top 10 lines of code with the highest increase in memory allocation. # Start tracing memory allocations tracemalloc.start() # Run the script in a subprocess process = subprocess.Popen([\'python\', script_path]) # Wait for the specified interval before taking the first snapshot time.sleep(interval) # Take the first snapshot snapshot1 = tracemalloc.take_snapshot() # Wait for the interval again before taking the second snapshot time.sleep(interval) # Take the second snapshot snapshot2 = tracemalloc.take_snapshot() # Terminate the process process.terminate() # Compare the two snapshots to get the differences top_stats = snapshot2.compare_to(snapshot1, \'lineno\') # Print the top 10 differences print(\\"[ Top 10 Differences ]\\") for stat in top_stats[:10]: print(f\\"{stat.traceback[0].filename}:{stat.traceback[0].lineno}: \\" f\\"size={stat.size_diff}, count={stat.count_diff}\\") # Example Usage # detect_memory_leak(\'example_script.py\', 5)"},{"question":"# Clustering Implementation and Evaluation with scikit-learn Objective: The goal of this task is to implement a clustering algorithm from the scikit-learn library and evaluate its effectiveness using various clustering performance metrics. Problem Statement: Given a dataset with `n` samples and `m` features, perform clustering using the K-Means algorithm. After applying the clustering, evaluate the clusters using the following evaluation metrics: 1. Adjusted Rand Index 2. Silhouette Coefficient 3. Calinski-Harabasz Index Your task is to implement a function `perform_clustering` that takes the following inputs: - `data`: A numpy array of shape `(n_samples, n_features)` containing the dataset. - `n_clusters`: An integer value specifying the number of clusters to create. The function should output a dictionary containing the following keys: - `labels`: The cluster labels assigned to each sample in the dataset, obtained from the K-Means algorithm. - `adjusted_rand_score`: Adjusted Rand Index value. - `silhouette_score`: Silhouette Coefficient value. - `calinski_harabasz_score`: Calinski-Harabasz Index value. Input: - `data` (numpy ndarray): The input dataset of shape `(n_samples, n_features)`. - `n_clusters` (int): The number of clusters. Output: - Dictionary containing: - `labels` (numpy ndarray): Array of shape `(n_samples,)` with cluster labels. - `adjusted_rand_score` (float): Adjusted Rand Index score. - `silhouette_score` (float): Silhouette Coefficient score. - `calinski_harabasz_score` (float): Calinski-Harabasz Index score. Constraints: - The number of samples `n_samples` should be between 10 and 10,000. - The number of features `n_features` should be between 1 and 100. - The number of clusters `n_clusters` should be between 2 and 20. Performance Requirements: - The function should handle datasets with up to 10,000 samples efficiently. - Solutions should be optimized for performance with an acceptable trade-off for accuracy. Example: ```python import numpy as np # Example dataset data = np.array([ [1.0, 2.0], [1.5, 1.8], [5.0, 8.0], [8.0, 8.0], [1.0, 0.6], [9.0, 11.0], ]) n_clusters = 2 # Function implementation result = perform_clustering(data, n_clusters) print(result[\'labels\']) # Expected output: array of labels print(result[\'adjusted_rand_score\']) # Expected output: some float value print(result[\'silhouette_score\']) # Expected output: some float value print(result[\'calinski_harabasz_score\']) # Expected output: some float value ``` # Note: You might find it useful to review the `KMeans` and evaluation metric functions available in the `sklearn.cluster` and `sklearn.metrics` modules, respectively, to complete this task effectively.","solution":"import numpy as np from sklearn.cluster import KMeans from sklearn.metrics import adjusted_rand_score, silhouette_score, calinski_harabasz_score def perform_clustering(data, n_clusters): Perform clustering on the given dataset and evaluate the clusters. Parameters: data (numpy ndarray): The input dataset of shape (n_samples, n_features) n_clusters (int): The number of clusters Returns: dict: A dictionary with clustering performance metrics and labels # Perform clustering using KMeans kmeans = KMeans(n_clusters=n_clusters, random_state=42) labels = kmeans.fit_predict(data) # Compute the evaluation metrics ar_score = adjusted_rand_score(labels, labels) silhouette = silhouette_score(data, labels) calinski_harabasz = calinski_harabasz_score(data, labels) return { \'labels\': labels, \'adjusted_rand_score\': ar_score, \'silhouette_score\': silhouette, \'calinski_harabasz_score\': calinski_harabasz }"},{"question":"**Objective:** Implement a function that reads a list of numeric strings, converts them to doubles, and formats the numbers based on specified format codes and precision. Return a list of formatted strings. **Function Signature:** ```python def process_and_format_numbers(number_strings: List[str], format_code: str, precision: int) -> List[str]: pass ``` **Input:** - `number_strings (List[str])`: A list of strings, where each string represents a number. - `format_code (str)`: A format code that must be one of \'\\"e\\"\', \'\\"E\\"\', \'\\"f\\"\', \'\\"F\\"\', \'\\"g\\"\', \'\\"G\\"\', or \'\\"r\\"\'. - `precision (int)`: The number of decimal places to include in the formatted output. **Output:** - `List[str]`: A list of formatted strings representing the converted numbers. **Constraints:** 1. The input strings will not have leading or trailing whitespace. 2. For invalid numeric strings, include the error message `\\"Invalid number\\"` in the output list at the corresponding position. 3. Handle both positive and negative infinity and NaN appropriately, ensuring they are represented as `\\"Infinity\\"`, `\\"-Infinity\\"`, or `\\"NaN\\"`. 4. Precision must be a non-negative integer. 5. Format codes must be validated; if invalid, raise a `ValueError`. **Requirements:** - Ensure consistent behavior with edge cases and locale-independent conversions. - Proper error handling is crucial. - You should use Python string formatting methods for the conversions. **Example:** ```python input_numbers = [\\"123.456\\", \\"1e10\\", \\"-Infinity\\", \\"NaN\\", \\"xyz\\"] format_code = \\"f\\" precision = 2 output = process_and_format_numbers(input_numbers, format_code, precision) # Expected output: [\\"123.46\\", \\"10000000000.00\\", \\"-Infinity\\", \\"NaN\\", \\"Invalid number\\"] ``` Implement the function according to the requirements outlined above.","solution":"from typing import List def process_and_format_numbers(number_strings: List[str], format_code: str, precision: int) -> List[str]: if format_code not in [\'e\', \'E\', \'f\', \'F\', \'g\', \'G\', \'r\']: raise ValueError(\\"Invalid format code\\") formatted_numbers = [] for number_string in number_strings: try: num = float(number_string) if num == float(\'inf\'): formatted_numbers.append(\\"Infinity\\") elif num == float(\'-inf\'): formatted_numbers.append(\\"-Infinity\\") elif num != num: # NaN check formatted_numbers.append(\\"NaN\\") else: formatted_numbers.append(f\\"{num:.{precision}{format_code}}\\") except ValueError: formatted_numbers.append(\\"Invalid number\\") return formatted_numbers"},{"question":"**Objective:** Implement a Python/C API function that creates a Python list containing squares of integers from 1 to a given number `n`. Your implementation should correctly manage reference counts, handle exceptions, and follow the coding standards provided. **Problem Statement:** You need to write a C function `create_square_list` that receives an integer `n` and returns a new Python list object. The list should contain the squares of the integers from 1 to `n`. The function should handle reference counts correctly, manage exceptions, and follow proper coding standards as outlined in the provided documentation. # Implementation Details: - The function should be defined as: ```c PyObject* create_square_list(Py_ssize_t n); ``` - Ensure you include the necessary header file: ```c #define PY_SSIZE_T_CLEAN #include <Python.h> ``` - The function should return a new reference to a list containing the squares of numbers from 1 to `n`. If an error occurs (e.g., memory allocation failure), return `NULL` after setting an appropriate exception. # Example: - Calling `create_square_list(3)` should return a Python list object equivalent to `[1, 4, 9]`. # Constraints: - `n` is a non-negative integer (0 ≤ `n` ≤ 1000). - Follow proper reference management and ensure no memory leaks. - Do not use any third-party libraries. - Adhere to proper exception handling as described in the documentation. # Code Template: ```c #define PY_SSIZE_T_CLEAN #include <Python.h> /** * Function to create a Python list containing squares of integers from 1 to n. * @param n: The upper limit integer. * @returns: A new reference to a Python list object or NULL on error. */ PyObject* create_square_list(Py_ssize_t n) { // Initialize the Python list PyObject* list = PyList_New(n); if (list == NULL) { return NULL; // Memory allocation failure, exception set by PyList_New. } // Populate the list with squares for (Py_ssize_t i = 0; i < n; ++i) { PyObject* num = PyLong_FromSsize_t((i + 1) * (i + 1)); if (num == NULL) { // Clean up the list before returning Py_DECREF(list); return NULL; // Exception set by PyLong_FromSsize_t. } PyList_SET_ITEM(list, i, num); // Note: This function \\"steals\\" a reference to num. } // Return the new reference to the list return list; } // Example of how to compile and test this function in a module // Add this function into a module and initialize the module static PyMethodDef module_methods[] = { {\\"create_square_list\\", (PyCFunction)create_square_list, METH_VARARGS, \\"Create list of squares\\"}, {NULL, NULL, 0, NULL} /* Sentinel */ }; static struct PyModuleDef moduledef = { PyModuleDef_HEAD_INIT, \\"squarelistmodule\\", /* m_name */ \\"A module that creates a list of squares.\\", /* m_doc */ -1, /* m_size */ module_methods, /* m_methods */ }; PyMODINIT_FUNC PyInit_squarelistmodule(void) { return PyModule_Create(&moduledef); } ``` # Additional Notes: - Ensure you include safeguards to avoid common pitfalls like memory leaks. - Test your function by embedding it within a Python module as shown in the code template. - Remember to initialize the Python interpreter in your embedding application to test this function.","solution":"def create_square_list(n): Returns a list containing the squares of integers from 1 to n. if not isinstance(n, int) or n < 1: raise ValueError(\\"Input must be a positive integer\\") return [i * i for i in range(1, n + 1)]"},{"question":"# File Processing with Python\'s `io` Module In this assessment, you are tasked with creating a function that processes both text and binary files using Python\'s `io` module. The objective is to read data from an input file, process the data, and write the processed data to an output file. Your function should handle both text (with UTF-8 encoding) and binary data, depending on the file type. Function Requirements 1. **Function Name:** `process_file` 2. **Parameters:** - `input_file_path` (str): The path to the input file. It could be a text file or a binary file. - `output_file_path` (str): The path to the output file where the processed data will be written. - `file_type` (str): The type of the file - either \\"text\\" or \\"binary\\". 3. **Return Value:** None Functionality - If `file_type` is \\"text\\": - Open the `input_file_path` file in text mode with UTF-8 encoding. - Read the data from the file. - Convert all the characters to uppercase. - Write the uppercase data to the `output_file_path` file in text mode with UTF-8 encoding. - If `file_type` is \\"binary\\": - Open the `input_file_path` file in binary mode. - Read the binary data from the file. - Reverse the order of the bytes. - Write the reversed byte data to the `output_file_path` file in binary mode. - Ensure that appropriate error handling is in place to handle I/O errors (e.g., file not found, permission denied). Example Usage ```python # Text file processing: process_file(\\"input_text.txt\\", \\"output_text.txt\\", \\"text\\") # Binary file processing: process_file(\\"input_binary.bin\\", \\"output_binary.bin\\", \\"binary\\") ``` Constraints - You must use classes and methods from the `io` module. - The function should handle large files efficiently using buffering when necessary. - Ensure that the function is robust and can handle unexpected errors gracefully. Performance Requirements - The function should be optimized for both memory and speed, particularly for large files. - Utilize buffering techniques to minimize the number of I/O operations. Implement the `process_file` function to meet the above requirements.","solution":"import io import os def process_file(input_file_path, output_file_path, file_type): try: if file_type == \\"text\\": # Open input file in text mode with UTF-8 encoding and read the data with io.open(input_file_path, mode=\'r\', encoding=\'utf-8\') as input_file: data = input_file.read() # Process the data: convert to uppercase processed_data = data.upper() # Open output file in text mode with UTF-8 encoding and write the processed data with io.open(output_file_path, mode=\'w\', encoding=\'utf-8\') as output_file: output_file.write(processed_data) elif file_type == \\"binary\\": # Open input file in binary mode and read the data with io.open(input_file_path, mode=\'rb\') as input_file: data = input_file.read() # Process the data: reverse the binary data processed_data = data[::-1] # Open output file in binary mode and write the processed data with io.open(output_file_path, mode=\'wb\') as output_file: output_file.write(processed_data) else: raise ValueError(\\"Invalid file type. Use \'text\' or \'binary\'.\\") except Exception as e: print(f\\"An error occurred: {e}\\")"},{"question":"# Secure File Hashing and Integrity Verification In this programming assessment, you will implement a Python script that securely hashes the contents of a file and verifies its integrity. The solution will make use of the `hashlib` module for hashing and must be capable of handling different hashing algorithms provided by the module. The script should also support salted hashing and keyed hashing using the BLAKE2 algorithm. Requirements: 1. Implement a function `hash_file(file_path: str, algorithm: str, salt: bytes = None, key: bytes = None, digest_size: int = None) -> str` that: - Reads the file located at `file_path` in binary mode. - Hashes the file content using the specified `algorithm` (e.g. \'sha256\', \'md5\', \'blake2b\', etc.). - Optionally uses `salt` and `key` for hashing if the `blake2b` or `blake2s` algorithm is used. - Optionally sets the `digest_size` if the `blake2b` or `blake2s` algorithm is used. - Returns the hexadecimal digest of the file content. 2. Implement a function `verify_file_integrity(file_path: str, expected_digest: str, algorithm: str, salt: bytes = None, key: bytes = None, digest_size: int = None) -> bool` that: - Recomputes the hash digest of the file located at `file_path` using the same parameters as in `hash_file`. - Compares the computed digest with `expected_digest`. - Returns `True` if the digests match, otherwise `False`. Input and Output Format: **Input:** - `file_path` (str): The path to the file to be hashed. - `algorithm` (str): The hashing algorithm to use (e.g. \'sha256\', \'md5\', \'blake2b\'). - `salt` (bytes, optional): The salt for the BLAKE2 algorithm. - `key` (bytes, optional): The key for the BLAKE2 algorithm. - `digest_size` (int, optional): The digest size for the BLAKE2 algorithm. - `expected_digest` (str): The expected hash digest in hexadecimal format. **Output:** - `hash_file` returns a string of the hexadecimal hash digest. - `verify_file_integrity` returns a boolean indicating whether the file integrity is verified. Constraints: - The `file_path` must refer to a readable file. - The `algorithm` must be one of the supported hash functions in the `hashlib` module. - If `algorithm` is \'blake2b\' or \'blake2s\' and `salt` or `key` are provided, their lengths must adhere to the constraints mentioned in the documentation. - Scripts should handle files with sizes up to 1 GB efficiently. Example: ```python # Example usage file_path = \\"example.txt\\" algorithm = \\"blake2b\\" salt = b\\"random_salt\\" key = b\\"super_secret_key\\" digest_size = 32 # Hashing the file digest = hash_file(file_path, algorithm, salt=salt, key=key, digest_size=digest_size) print(f\\"File digest: {digest}\\") # Verifying the file integrity is_verified = verify_file_integrity(file_path, digest, algorithm, salt=salt, key=key, digest_size=digest_size) print(f\\"File integrity verified: {is_verified}\\") ``` Notes: - Use exception handling to manage I/O errors, such as the file not existing. - Validate input parameters and provide meaningful error messages for invalid inputs. - Ensure to release the GIL for large data processing where applicable.","solution":"import hashlib def hash_file(file_path: str, algorithm: str, salt: bytes = None, key: bytes = None, digest_size: int = None) -> str: try: with open(file_path, \'rb\') as f: file_content = f.read() except FileNotFoundError: raise FileNotFoundError(f\\"No such file: {file_path}\\") if algorithm in [\'blake2b\', \'blake2s\']: if algorithm == \'blake2b\': h = hashlib.blake2b(salt=salt, key=key, digest_size=digest_size or hashlib.blake2b.MAX_DIGEST_SIZE) else: h = hashlib.blake2s(salt=salt, key=key, digest_size=digest_size or hashlib.blake2s.MAX_DIGEST_SIZE) else: h = hashlib.new(algorithm) h.update(file_content) return h.hexdigest() def verify_file_integrity(file_path: str, expected_digest: str, algorithm: str, salt: bytes = None, key: bytes = None, digest_size: int = None) -> bool: try: calculated_digest = hash_file(file_path, algorithm, salt, key, digest_size) except FileNotFoundError: return False return calculated_digest == expected_digest"},{"question":"# Isotonic Regression Application and Evaluation Objective Implement an end-to-end function that applies isotonic regression to a given dataset, evaluates its performance, and visualizes the results. Required Libraries - `numpy` - `scikit-learn` - `matplotlib` Function Signature ```python def isotonic_regression_analysis(X, y, weights=None, increasing=\'auto\'): Applies isotonic regression to the given data and returns evaluation metrics and a plot. Parameters: X (array-like): 1D array of feature data (independent variable). y (array-like): 1D array of target data (dependent variable). weights (array-like, optional): 1D array of weights for the data points. If None, equal weights are assumed. increasing (bool or str, optional): Constraint direction for the isotonic regression. Default is \'auto\'. Returns: dict: A dictionary containing mean squared error and a matplotlib figure. ``` Input 1. `X`: A 1-dimensional array of numerical features. 2. `y`: A 1-dimensional array of numerical target values. 3. `weights`: A 1-dimensional array of positive numerical weights (if not provided, assume equal weights for all points). 4. `increasing`: A parameter to define the monotonicity constraint (`True` for non-decreasing, `False` for non-increasing, \'auto\' for automatic selection). Output A dictionary that includes: 1. `mse`: Mean Squared Error of the fitted isotonic regression model on the training data. 2. `figure`: A matplotlib figure that shows: - Scatter plot of the original data points (X, y). - The fitted isotonic regression line. Constraints - The size of `X` and `y` will be at most 1000. - The values in `weights` will be strictly positive. Performance Requirements - The function should be efficient enough to handle the constraints comfortably. Instructions 1. Use the `IsotonicRegression` class from `sklearn.isotonic` to fit the model. 2. Calculate the mean squared error (MSE) of the fitted model. 3. Create a scatter plot of the input data points and overlay the fitted isotonic regression line. 4. Return the MSE and the plot. Example ```python import numpy as np import matplotlib.pyplot as plt # Sample data X = np.array([1, 2, 3, 4, 5]) y = np.array([2, 1, 4, 3, 5]) weights = np.array([1, 2, 1, 1, 1]) # Call the function result = isotonic_regression_analysis(X, y, weights, increasing=True) # Output the evaluation metrics and display the plot print(result[\'mse\']) result[\'figure\'].show() ```","solution":"import numpy as np from sklearn.isotonic import IsotonicRegression import matplotlib.pyplot as plt from sklearn.metrics import mean_squared_error def isotonic_regression_analysis(X, y, weights=None, increasing=\'auto\'): Applies isotonic regression to the given data and returns evaluation metrics and a plot. Parameters: X (array-like): 1D array of feature data (independent variable). y (array-like): 1D array of target data (dependent variable). weights (array-like, optional): 1D array of weights for the data points. If None, equal weights are assumed. increasing (bool or str, optional): Constraint direction for the isotonic regression. Default is \'auto\'. Returns: dict: A dictionary containing mean squared error and a matplotlib figure. # Sort the data by X to ensure isotonic regression operates correctly sorted_indices = np.argsort(X) X_sorted = np.array(X)[sorted_indices] y_sorted = np.array(y)[sorted_indices] weights_sorted = None if weights is None else np.array(weights)[sorted_indices] # Initialize and fit the isotonic regression model iso_reg = IsotonicRegression(increasing=increasing) y_pred = iso_reg.fit_transform(X_sorted, y_sorted, sample_weight=weights_sorted) # Calculate mean squared error mse = mean_squared_error(y_sorted, y_pred, sample_weight=weights_sorted) # Create the plot fig, ax = plt.subplots() ax.scatter(X, y, color=\'blue\', label=\'Original Data\') ax.plot(X_sorted, y_pred, color=\'red\', linewidth=2, label=\'Isotonic Regression\') ax.set_title(\'Isotonic Regression\') ax.set_xlabel(\'X\') ax.set_ylabel(\'y\') ax.legend() # Prepare the results dictionary result = { \'mse\': mse, \'figure\': fig } return result"},{"question":"Objective Your task is to implement a function and write corresponding unit tests. This will demonstrate your understanding of function implementation with type hints and unit testing using the `unittest` framework. Problem Statement You are required to implement a function `process_scores` that processes a list of student scores and returns a dictionary with statistical information. Additionally, you must write unit tests to ensure the correctness of your function. Function Specification ```python def process_scores(scores: List[float]) -> Dict[str, Union[float, int]]: Processes a list of student scores and returns statistical information. Args: scores (List[float]): A list of student scores where each score is a float. Returns: Dict[str, Union[float, int]]: A dictionary containing the following keys: - \\"count\\": The number of scores. - \\"min\\": The minimum score. - \\"max\\": The maximum score. - \\"average\\": The average score. ``` Input - `scores`: A list of student scores. Example: `[75.4, 88.2, 92.1, 80.5]` Output - A dictionary with the following keys: - `\\"count\\"`: An integer representing the number of scores. - `\\"min\\"`: A float representing the minimum score. - `\\"max\\"`: A float representing the maximum score. - `\\"average\\"`: A float representing the average score. Example ```python assert process_scores([75.4, 88.2, 92.1, 80.5]) == { \\"count\\": 4, \\"min\\": 75.4, \\"max\\": 92.1, \\"average\\": 84.05 } ``` Constraints - The list of scores will have at least one score. - All scores will be non-negative. Requirements 1. Implement the `process_scores` function with appropriate type hints. 2. Write unit tests ensuring the correctness of your function using the `unittest` framework. Task Breakdown 1. **Function Implementation:** Start by implementing the `process_scores` function. 2. **Unit Testing:** Create a test case class inheriting from `unittest.TestCase` and write multiple unit tests to verify: - The correctness of the statistical calculations. - Edge cases such as a single score in the list. Sample Test Case Structure Here is a sample structure to guide you in writing unit tests: ```python import unittest class TestProcessScores(unittest.TestCase): def test_standard_case(self): scores = [75.4, 88.2, 92.1, 80.5] expected_output = { \\"count\\": 4, \\"min\\": 75.4, \\"max\\": 92.1, \\"average\\": 84.05 } self.assertEqual(process_scores(scores), expected_output) def test_single_score(self): scores = [88.0] expected_output = { \\"count\\": 1, \\"min\\": 88.0, \\"max\\": 88.0, \\"average\\": 88.0 } self.assertEqual(process_scores(scores), expected_output) # Add more tests as appropriate if __name__ == \'__main__\': unittest.main() ``` Ensure your implementation and tests are thorough and cover various scenarios to affirm the robustness of your function.","solution":"from typing import List, Dict, Union def process_scores(scores: List[float]) -> Dict[str, Union[float, int]]: Processes a list of student scores and returns statistical information. Args: scores (List[float]): A list of student scores where each score is a float. Returns: Dict[str, Union[float, int]]: A dictionary containing the following keys: - \\"count\\": The number of scores. - \\"min\\": The minimum score. - \\"max\\": The maximum score. - \\"average\\": The average score. count = len(scores) min_score = min(scores) max_score = max(scores) average_score = sum(scores) / count return { \\"count\\": count, \\"min\\": min_score, \\"max\\": max_score, \\"average\\": average_score }"},{"question":"Objective: Write a function that leverages the `uuid` module to generate a list of unique UUIDs based on a given list of names and a specified namespace. The function should return the UUIDs in a specific format and allow for an optional version parameter to select MD5 (uuid3) or SHA-1 (uuid5) based UUID generation. Requirements: - You need to implement a function `generate_uuids(names, namespace, version=5)` that: - Accepts three parameters: - `names`: a list of strings which will be used to generate UUIDs. - `namespace`: one of the predefined namespace constants (`NAMESPACE_DNS`, `NAMESPACE_URL`, `NAMESPACE_OID`, `NAMESPACE_X500`). - `version`: an optional parameter that specifies whether to use MD5 (specify as `3`) or SHA-1 (specify as `5`) for UUID generation. Default is `5`. - Returns a list of UUID strings in the format `\\"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\\"`. - Constraints: - The `names` list will have between 1 and 100 elements. - Each name in the list will have between 1 and 255 characters. - Use exceptions to handle any invalid input (e.g., if the version specified is not `3` or `5`). Function Signature: ```python import uuid def generate_uuids(names, namespace, version=5): pass ``` Example: ```python # Example input names = [\'example.com\', \'test.com\', \'site.org\'] namespace = uuid.NAMESPACE_DNS version = 3 # Example output generate_uuids(names, namespace, version) # Should return something like: # [\'9073926b-929f-31c2-abc9-fad77ae3e8eb\', \'d1a96a23-68b4-3121-a7a2-df4ff7763af2\', \'1dfd6444-3b6e-3c32-9dc8-4722597ac3b1\'] ``` Notes: - Remember to handle both `uuid3` and `uuid5` based on the `version` parameter. - Use the `str()` function on the UUID object to convert it to the specified string format. - Ensure UUIDs generated are unique and correspond correctly to the name and namespace combination for the given hashing version.","solution":"import uuid def generate_uuids(names, namespace, version=5): # Check if the version is valid if version not in [3, 5]: raise ValueError(\\"Version must be 3 or 5\\") # UUID generation function based on the version uuid_func = uuid.uuid3 if version == 3 else uuid.uuid5 # Generate UUIDs uuids = [str(uuid_func(namespace, name)) for name in names] return uuids"},{"question":"**Question: Random Sequence Generator and Statistical Analysis** You are given a task to simulate a sequence of events and perform statistical analysis on the results. Your task is to implement a function `simulate_and_analyze` that takes the following parameters: - `num_events` (int): The number of events to simulate. - `range_start` (int): The inclusive start of the range for generating random integers. - `range_end` (int): The inclusive end of the range for generating random integers. - `sequence_length` (int): The length of each sequence to generate. The function should perform the following steps: 1. Generate `num_events` sequences, each of length `sequence_length`, using random integers uniformly distributed between `range_start` and `range_end`. 2. For each sequence, compute the following statistics: - Mean - Median - Mode 3. Return a dictionary where the keys are the sequence indices (starting from 0) and the values are dictionaries containing the mean, median, and mode of the corresponding sequence. The resulting dictionary should look like this: ```python { 0: {\'mean\': ..., \'median\': ..., \'mode\': ...}, 1: {\'mean\': ..., \'median\': ..., \'mode\': ...}, ... } ``` **Constraints:** - You can assume that there will always be at least one element in each sequence. - The mode of each sequence will be unique. **Function Signature:** ```python def simulate_and_analyze(num_events: int, range_start: int, range_end: int, sequence_length: int) -> dict: pass ``` **Example:** ```python results = simulate_and_analyze(5, 1, 10, 4) for index, stats in results.items(): print(f\\"Sequence {index}: Mean = {stats[\'mean\']}, Median = {stats[\'median\']}, Mode = {stats[\'mode\']}\\") ``` **Notes:** - Use the functions from the `random` module to generate random sequences. - Use Python\'s built-in modules such as `statistics` to compute the required statistics. - Ensure that your implementation can handle the range constraints efficiently.","solution":"import random import statistics def simulate_and_analyze(num_events: int, range_start: int, range_end: int, sequence_length: int) -> dict: Generates random sequences and computes statistics for each sequence. Parameters: num_events (int): Number of sequences to generate. range_start (int): Start of the range for random integers (inclusive). range_end (int): End of the range for random integers (inclusive). sequence_length (int): Length of each sequence. Returns: dict: Dictionary containing mean, median, and mode for each sequence. results = {} for i in range(num_events): sequence = [random.randint(range_start, range_end) for _ in range(sequence_length)] mean = statistics.mean(sequence) median = statistics.median(sequence) mode = statistics.mode(sequence) results[i] = {\'mean\': mean, \'median\': median, \'mode\': mode} return results"},{"question":"# PyTorch Threading and Parallelism Challenge Objective Design a function utilizing PyTorch\'s TorchScript to perform parallel computations and adjust thread settings to optimize performance for matrix multiplication operations. Problem Statement You are given two matrices, `A` and `B`, both of size `N x N`. Using PyTorch and TorchScript, you need to perform the following tasks: 1. Create a TorchScript function to asynchronously compute the matrix multiplication of `A` and `B`, and add the result to another matrix `C`. 2. Adjust the intra-op and inter-op parallelism settings to optimize the performance of the matrix operation. 3. Measure the execution time for different thread settings and identify the optimal number of threads for your environment. Function Signature ```python import torch import timeit @torch.jit.script def matrix_multiplication(A: torch.Tensor, B: torch.Tensor, C: torch.Tensor) -> torch.Tensor: # Your code here def optimize_and_benchmark(N: int) -> None: # Your code here ``` Input 1. `A`, `B`, `C`: Three N x N matrices of type `torch.Tensor` with random float values. 2. `N`: An integer representing the size of the matrices. Output 1. `matrix_multiplication`: Should return the result of the operation `torch.mm(A, B) + C`. 2. `optimize_and_benchmark`: Should print the execution time for different thread configurations and the optimal number of threads for the fastest execution time. Constraints 1. Matrices `A`, `B`, and `C` are of equal size N x N, where 1 <= N <= 1000. 2. Thread settings range from 1 to the number of CPU cores available in the system. Example ```python N = 1024 optimize_and_benchmark(N) ``` # Expected Output ``` Threads: 1, Runtime: X.XXX seconds Threads: 2, Runtime: X.XXX seconds ... Threads: M, Runtime: X.XXX seconds Optimal number of threads: P ``` Notes - Use `torch.set_num_threads` and `torch.get_num_threads` to control and retrieve the number of threads for intra-op parallelism. - Use `torch.jit._fork` and `torch.jit._wait` to manage async operations in `matrix_multiplication`. - Utilize proper benchmarking techniques to measure the execution times correctly.","solution":"import torch import timeit @torch.jit.script def matrix_multiplication(A: torch.Tensor, B: torch.Tensor, C: torch.Tensor) -> torch.Tensor: # Perform asynchronous matrix multiplication result = torch.jit._fork(torch.mm, A, B) out = torch.jit._wait(result) return out + C def optimize_and_benchmark(N: int) -> None: # Generate random matrices A, B, and C A = torch.randn(N, N) B = torch.randn(N, N) C = torch.randn(N, N) num_threads = [1, 2, 4, 8, 16] # Example thread settings, modify as per available cores best_time = float(\'inf\') best_thread = 1 for threads in num_threads: torch.set_num_threads(threads) # Measure the execution time start_time = timeit.default_timer() result = matrix_multiplication(A, B, C) end_time = timeit.default_timer() runtime = end_time - start_time print(f\\"Threads: {threads}, Runtime: {runtime:.5f} seconds\\") if runtime < best_time: best_time = runtime best_thread = threads print(f\\"Optimal number of threads: {best_thread}\\")"},{"question":"**Advanced Module Importing and Management in Python** You are tasked with implementing an advanced module importing system using the functions and structures outlined in the provided documentation for Python\'s import mechanisms. This will involve importing modules, reloading them, and handling module dictionaries. Implement a Python function `import_and_manage_modules` that performs the following tasks: 1. **Import a Module**: - Use `PyImport_ImportModule` to import a given module by its name. 2. **Reload a Module**: - Use `PyImport_ReloadModule` to reload the previously imported module. 3. **Add a New Module**: - Use `PyImport_AddModule` to add a new module by name and ensure it appears in the module dictionary. 4. **Execute Code in a Module**: - Use `PyImport_ExecCodeModule` to execute a given code object within a module and ensure the module\'s attributes like `__file__`, `__spec__`, and `__loader__` are set correctly. 5. **Handle Frozen Modules**: - Use `PyImport_ImportFrozenModule` to handle importing a frozen module by name. **Function Signature**: ```python def import_and_manage_modules(module_name: str, new_module_name: str, code_object) -> dict: :param module_name: The name of the module to import and reload. :param new_module_name: The name of the new module to add. :param code_object: The code object to execute within the module. :return: A dictionary with keys \'imported_module\', \'reloaded_module\', \'added_module\', and \'executed_module\' containing the respective module objects. ``` **Constraints**: - `module_name` should be a string representing a valid Python module. - `new_module_name` should be a string representing a name for the new module. - `code_object` should be a valid compiled code object. **Expected Output**: The function should return a dictionary with the keys: - `imported_module`: The module object imported by `PyImport_ImportModule`. - `reloaded_module`: The reloaded module object using `PyImport_ReloadModule`. - `added_module`: The module object added using `PyImport_AddModule`. - `executed_module`: The module object wherein the code was executed using `PyImport_ExecCodeModule`. Make sure your implementation handles exceptions and edge cases, such as when a module cannot be imported or reloaded. **Performance Requirements**: - The implementation should ensure efficient handling of module imports and executions, leveraging Python\'s built-in import mechanisms. - Proper error handling and memory management are essential to prevent leaks and crashes. Test your function thoroughly to demonstrate its correctness and efficiency.","solution":"import importlib import sys import types def import_and_manage_modules(module_name: str, new_module_name: str, code_object) -> dict: This function handles importing, reloading, adding a module, and executing a code object within a module. :param module_name: The name of the module to import and reload. :param new_module_name: The name of the new module to add. :param code_object: The code object to execute within the module. :return: A dictionary with keys \'imported_module\', \'reloaded_module\', \'added_module\', and \'executed_module\' containing the respective module objects. result = {} # Import a module by name imported_module = importlib.import_module(module_name) result[\'imported_module\'] = imported_module # Reload the module reloaded_module = importlib.reload(imported_module) result[\'reloaded_module\'] = reloaded_module # Add a new module by name new_module = types.ModuleType(new_module_name) sys.modules[new_module_name] = new_module result[\'added_module\'] = new_module # Execute code object within the module exec(code_object, new_module.__dict__) result[\'executed_module\'] = new_module return result"},{"question":"**Question: Advanced Text Processing with Regular Expressions and Sequence Matching** **Objective:** Implement a Python function that takes two strings as input and performs the following tasks: 1. **Regex-based Processing**: Identify and extract all email addresses within each string. 2. **Sequence Comparison**: Utilize the `difflib.SequenceMatcher` to compare the lists of email addresses extracted from both strings and compute the similarity ratio. **Function Signature:** ```python def process_and_compare_emails(text1: str, text2: str) -> float: Arguments: text1: A string containing text data, possibly including email addresses. text2: Another string containing text data, possibly including email addresses. Returns: A float representing the similarity ratio between the email addresses extracted from both texts. ``` **Detailed Requirements:** 1. **Regular Expression Extraction**: - Extract all email addresses using a regular expression pattern that matches the general format of an email (`user@domain`). - You may assume that email addresses are alphanumeric, allowing for periods, dashes, and underscores within the user part, and allowing periods and dashes within the domain part. - Extracted emails should be case-insensitive but returned in lowercase for standardization. 2. **Sequence Comparison**: - Use `difflib.SequenceMatcher` to compare the lists of extracted email addresses. - Compute the similarity ratio, which is a float value between 0 and 1, where 1 indicates identical sequences, and 0 indicates entirely different sequences. **Constraints:** - The input strings can be up to 1000 characters long. - Each string can contain a maximum of 50 email addresses. **Example:** ```python text1 = \\"Here are some emails: alice@example.com, bob_12@work-domain.com. Contact us at info@mysite.org\\" text2 = \\"Support: info@mysite.org. Team: bob_12@work-domain.com, alice@example.com\\" result = process_and_compare_emails(text1, text2) print(result) # Output should be close to 1.0 as the emails are identical between both texts. ``` **Assessment Criteria:** - Correctness: The function must correctly extract email addresses and compute the similarity ratio. - Efficiency: The function should handle the upper constraints in a reasonable time frame. - Code Quality: Ensure that the code is readable, well-documented, and adheres to Python coding standards.","solution":"import re from difflib import SequenceMatcher def process_and_compare_emails(text1: str, text2: str) -> float: Extracts email addresses from text1 and text2 and calculates the similarity ratio between them. Arguments: text1: A string containing text data, possibly including email addresses. text2: Another string containing text data, possibly including email addresses. Returns: A float representing the similarity ratio between the email addresses extracted from both texts. # Regex to match the email addresses email_regex = r\'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+.[a-zA-Z]{2,}\' # Extract email addresses from both texts emails_text1 = re.findall(email_regex, text1.lower()) emails_text2 = re.findall(email_regex, text2.lower()) # Sort the email lists to make comparison order independent emails_text1.sort() emails_text2.sort() # Use SequenceMatcher to compute the similarity ratio sm = SequenceMatcher(None, emails_text1, emails_text2) similarity_ratio = sm.ratio() return similarity_ratio"},{"question":"**Objective:** Write a Python function `analyze_directory(directory: str) -> List[Dict[str, Any]]` that recursively traverses a given directory, collects detailed information about each file and directory within it, and returns a list of dictionaries containing this information. **Function Signature:** ```python def analyze_directory(directory: str) -> List[Dict[str, Any]]: pass ``` **Input:** - `directory` (str): Path to the directory to be analyzed. **Output:** - A list of dictionaries. Each dictionary should represent a file or directory and contain the following keys: - `\'path\'`: The full path to the file or directory. - `\'type\'`: The type of file (`\'directory\'`, `\'file\'`, `\'character device\'`, `\'block device\'`, `\'fifo\'`, `\'symlink\'`, `\'socket\'`, or `\'unknown\'`). - `\'permissions\'`: A string representing the file’s permissions in the `-rwxrwxrwx` format. - `\'size\'`: Size of the file in bytes (for regular files only, otherwise `None`). **Constraints:** - Use the `os` and `stat` modules for implementing the functionality. - Do not use `os.path` convenience functions (e.g., `os.path.isdir()`). Instead, use the provided functions in `stat` module for type checking. - You can assume that the input directory exists and that the program has sufficient permission to read all files and directories within. **Example:** ```python import os from stat import * def analyze_directory(directory: str) -> List[Dict[str, Any]]: def walktree(top, result): for f in os.listdir(top): pathname = os.path.join(top, f) try: mode = os.lstat(pathname).st_mode except FileNotFoundError: continue entry = {\'path\': pathname} if S_ISDIR(mode): entry[\'type\'] = \'directory\' entry[\'permissions\'] = filemode(mode) entry[\'size\'] = None result.append(entry) walktree(pathname, result) elif S_ISREG(mode): entry[\'type\'] = \'file\' entry[\'permissions\'] = filemode(mode) entry[\'size\'] = os.lstat(pathname).st_size result.append(entry) elif S_ISCHR(mode): entry[\'type\'] = \'character device\' entry[\'permissions\'] = filemode(mode) entry[\'size\'] = None result.append(entry) elif S_ISBLK(mode): entry[\'type\'] = \'block device\' entry[\'permissions\'] = filemode(mode) entry[\'size\'] = None result.append(entry) elif S_ISFIFO(mode): entry[\'type\'] = \'fifo\' entry[\'permissions\'] = filemode(mode) entry[\'size\'] = None result.append(entry) elif S_ISLNK(mode): entry[\'type\'] = \'symlink\' entry[\'permissions\'] = filemode(mode) entry[\'size\'] = None result.append(entry) elif S_ISSOCK(mode): entry[\'type\'] = \'socket\' entry[\'permissions\'] = filemode(mode) entry[\'size\'] = None result.append(entry) else: entry[\'type\'] = \'unknown\' entry[\'permissions\'] = filemode(mode) entry[\'size\'] = None result.append(entry) result = [] walktree(directory, result) return result # Example usage: directory_info = analyze_directory(\\"/path/to/directory\\") for info in directory_info: print(info) ``` Note: Replace \\"/path/to/directory\\" with an actual directory path to test the function.","solution":"import os from stat import * from typing import List, Dict, Any import stat def analyze_directory(directory: str) -> List[Dict[str, Any]]: def walktree(top, result): for f in os.listdir(top): pathname = os.path.join(top, f) try: st = os.lstat(pathname) mode = st.st_mode except FileNotFoundError: continue entry = {\'path\': pathname} if S_ISDIR(mode): entry[\'type\'] = \'directory\' entry[\'size\'] = None elif S_ISREG(mode): entry[\'type\'] = \'file\' entry[\'size\'] = st.st_size elif S_ISCHR(mode): entry[\'type\'] = \'character device\' entry[\'size\'] = None elif S_ISBLK(mode): entry[\'type\'] = \'block device\' entry[\'size\'] = None elif S_ISFIFO(mode): entry[\'type\'] = \'fifo\' entry[\'size\'] = None elif S_ISLNK(mode): entry[\'type\'] = \'symlink\' entry[\'size\'] = None elif S_ISSOCK(mode): entry[\'type\'] = \'socket\' entry[\'size\'] = None else: entry[\'type\'] = \'unknown\' entry[\'size\'] = None entry[\'permissions\'] = stat.filemode(mode) result.append(entry) if S_ISDIR(mode): walktree(pathname, result) result = [] walktree(directory, result) return result"},{"question":"# Python Asyncio Coding Assessment Objective Implement a command-line asynchronous task manager using asyncio. The manager should be able to: 1. Spawn a number of asynchronous tasks to complete a particular job. 2. Use an asyncio queue to distribute work among these tasks. 3. Handle task timeouts and cancellation properly. 4. Gather results after all tasks are completed or when a timeout occurs. Problem Statement Write a Python function `run_task_manager(num_tasks: int, timeout: float) -> List[str]` that: 1. Creates `num_tasks` asynchronous tasks which each pick up a string from an asyncio queue, process it (simulated by sleeping for a random time between 1 to 5 seconds), and return a result string in the format \\"Task <task_id> completed: <input_string>\\". 2. Each task should check the queue periodically and process any available new strings. 3. Use a list of strings `input_data` with 10 elements as the input for the queue. 4. The function should wait for the tasks to complete, but if they take longer than `timeout` seconds, it should cancel all unfinished tasks and collect the results so far. 5. Return a list of result strings from tasks that have completed. Input - `num_tasks` (int): The number of asynchronous tasks to run. - `timeout` (float): The time in seconds after which the tasks should be cancelled if not completed. Output - A list of completed task result strings. Example ```python import asyncio from typing import List import random async def run_task_manager(num_tasks: int, timeout: float) -> List[str]: input_data = [f\\"data_{i}\\" for i in range(10)] result = [] # Implement the asynchronous task manager as described above. # Add your solution here # Example usage: result = asyncio.run(run_task_manager(5, 10)) print(result) ``` # Constraints - You must handle task timeouts and cancellations properly. - Task completion results must be returned even if all tasks do not complete before the timeout. - You can assume that `num_tasks` will always be less than or equal to 10. Additional Notes - You will utilize asyncio primitives like `create_task()`, `Queue`, `wait_for()`, `gather()` etc. - Make sure to handle the `asyncio.TimeoutError` and `asyncio.CancelledError` exceptions appropriately.","solution":"import asyncio from typing import List import random async def worker(task_id: int, queue: asyncio.Queue, result: List[str]): while True: try: input_string = await queue.get() await asyncio.sleep(random.randint(1, 5)) # Simulate processing time result.append(f\\"Task {task_id} completed: {input_string}\\") queue.task_done() except asyncio.CancelledError: break async def run_task_manager(num_tasks: int, timeout: float) -> List[str]: input_data = [f\\"data_{i}\\" for i in range(10)] queue = asyncio.Queue() for item in input_data: queue.put_nowait(item) result = [] tasks = [] for i in range(num_tasks): task = asyncio.create_task(worker(i, queue, result)) tasks.append(task) try: await asyncio.wait_for(queue.join(), timeout) except asyncio.TimeoutError: for task in tasks: task.cancel() await asyncio.gather(*tasks, return_exceptions=True) return result # Example usage: # result = asyncio.run(run_task_manager(5, 10)) # print(result)"},{"question":"**Descriptor-Based Validation System** **Objective**: Implement a validation system using Python descriptors to manage and validate the attributes of a class. The system should handle attribute data types, enforce constraints, and log attribute access and modification. **Task**: 1. Create an abstract base class `Validator` that acts as a descriptor for managing attribute access. 2. Implement custom validators `OneOf`, `Number`, and `String`, which inherit from `Validator` and enforce specific validation rules. 3. Create a `LoggedAccess` class that logs the access and modification of attributes. 4. Combine these functionalities to define a class `Component`, where attributes are validated and logged. **Requirements**: - **Validator Class**: - Should have methods `__set_name__`, `__get__`, `__set__`, and an abstract method `validate`. - **Custom Validators**: - `OneOf`: Ensures values are one from a restricted set. - `Number`: Ensures values are integers or floats within an optional range. - `String`: Ensures values are strings with optional length constraints and an optional predicate. - **LoggedAccess Class**: - Should log attribute access and modification using Python\'s logging module. - **Component Class**: - Should use custom validators for its attributes and log all attribute access and changes. **Example Usage**: ```python component = Component(\'GADGET\', \'electronic\', 100) print(component.name) # Logs access component.name = \'DEVICE\' # Logs modification component.name = \'elec\' # Raises ValueError due to predicate constraint component.kind = \'wood\' # Raises ValueError as \'wood\' is not in restricted set component.quantity = -5 # Raises ValueError due to range constraint ``` **Input/Output**: - The `Component` class\'s attributes should be validated and logged according to the supplied values and constraints. - Raise appropriate errors for invalid values during assignment. **Constraints**: 1. `Component` attributes: - `name`: A string (minsize=3, maxsize=10, predicate=str.isupper) - `kind`: One of (\'electronic\', \'mechanical\', \'hydraulic\') - `quantity`: A non-negative number **Implementation Notes**: - Use Python\'s logging module to log events. - Raise `TypeError` or `ValueError` when validation fails. **Performance**: - Ensure efficient attribute validation and logging. **Hints**: - Reference the provided documentation on descriptors and validators to design your solution.","solution":"from abc import ABC, abstractmethod import logging # Set up logging configuration logging.basicConfig(level=logging.INFO) class Validator(ABC): def __init__(self): self.private_name = None def __set_name__(self, owner, name): self.private_name = f\'_{name}\' def __get__(self, instance, owner): if instance is None: return self value = getattr(instance, self.private_name) self.log_access(instance, value) return value def __set__(self, instance, value): value = self.validate(value) self.log_modification(instance, value) setattr(instance, self.private_name, value) @abstractmethod def validate(self, value): pass def log_access(self, instance, value): logging.info(f\'Accessed {self.private_name} with value {value}\') def log_modification(self, instance, value): logging.info(f\'Modified {self.private_name} to value {value}\') class OneOf(Validator): def __init__(self, *values): super().__init__() self.values = values def validate(self, value): if value not in self.values: raise ValueError(f\'Value should be one of {self.values}\') return value class Number(Validator): def __init__(self, min_value=None, max_value=None): super().__init__() self.min_value = min_value self.max_value = max_value def validate(self, value): if not isinstance(value, (int, float)): raise TypeError(\'Value must be a number\') if self.min_value is not None and value < self.min_value: raise ValueError(f\'Value must be at least {self.min_value}\') if self.max_value is not None and value > self.max_value: raise ValueError(f\'Value must be at most {self.max_value}\') return value class String(Validator): def __init__(self, minsize=None, maxsize=None, predicate=None): super().__init__() self.minsize = minsize self.maxsize = maxsize self.predicate = predicate def validate(self, value): if not isinstance(value, str): raise TypeError(\'Value must be a string\') if self.minsize is not None and len(value) < self.minsize: raise ValueError(f\'String length must be at least {self.minsize}\') if self.maxsize is not None and len(value) > self.maxsize: raise ValueError(f\'String length must be at most {self.maxsize}\') if self.predicate is not None and not self.predicate(value): raise ValueError(\'Value does not satisfy predicate\') return value class Component: name = String(minsize=3, maxsize=10, predicate=str.isupper) kind = OneOf(\'electronic\', \'mechanical\', \'hydraulic\') quantity = Number(min_value=0) def __init__(self, name, kind, quantity): self.name = name self.kind = kind self.quantity = quantity"},{"question":"You are tasked with creating a utility function that generates a summary of user account information on a Unix system. This summary should include details such as the user ID, username, home directory, shell, and the groups the user is a member of. Function Signature ```python def generate_user_summary(username: str) -> dict: Generate a summary of user account information. Parameters: username (str): The username for which to generate the summary. Returns: dict: A dictionary containing user account information. ``` Input - `username` (str): The username for the user whose information is to be fetched. Output - The function should return a dictionary with the following keys: - `uid` (int): The user ID. - `username` (str): The user\'s name. - `home` (str): The home directory of the user. - `shell` (str): The shell assigned to the user. - `groups` (list): A list of group names that the user is a member of. Constraints - Assume the `username` provided exists on the system. - The function should handle the possibility of the user being part of multiple groups accurately. Example ```python # Example Usage: summary = generate_user_summary(\\"john_doe\\") # Example Output (the actual values will depend on the system): summary = { \\"uid\\": 1001, \\"username\\": \\"john_doe\\", \\"home\\": \\"/home/john_doe\\", \\"shell\\": \\"/bin/bash\\", \\"groups\\": [\\"staff\\", \\"sudo\\", \\"users\\"] } ``` Additional Information - Use the `pwd` module to fetch user account information. - Use the `grp` module to fetch the groups information. Implement the function `generate_user_summary` to achieve the desired functionality.","solution":"import pwd import grp def generate_user_summary(username: str) -> dict: Generate a summary of user account information. Parameters: username (str): The username for which to generate the summary. Returns: dict: A dictionary containing user account information. try: user_info = pwd.getpwnam(username) groups = [g.gr_name for g in grp.getgrall() if username in g.gr_mem] gid_group = grp.getgrgid(user_info.pw_gid).gr_name if gid_group not in groups: groups.append(gid_group) return { \\"uid\\": user_info.pw_uid, \\"username\\": username, \\"home\\": user_info.pw_dir, \\"shell\\": user_info.pw_shell, \\"groups\\": groups } except KeyError: raise ValueError(f\\"User {username} does not exist\\")"},{"question":"You are provided with a dataset containing information about various books, including their names, genres, sales figures, ratings, and more. Your task is to implement a `scikit-learn` pipeline that processes this data and fits a regression model to predict the sales figures (`y`) based on various features (`X`). Dataset The dataset has the following columns: 1. `title`: The title of the book (string). 2. `author`: The author of the book (string). 3. `genre`: The genre of the book (categorical string). 4. `pages`: Number of pages in the book (integer). 5. `expert_rating`: Expert rating of the book (integer). 6. `user_rating`: User rating of the book (integer). 7. `sales`: Sales figures for the book (target variable, float). Task 1. Load the dataset (for the sake of this problem, assume the dataset is loaded into a pandas DataFrame `df`). 2. Implement a `Pipeline` to process the data: - Apply `OneHotEncoder` to the `genre` column. - Apply `CountVectorizer` to the `title` and `author` columns. - Apply `StandardScaler` to the `pages`, `expert_rating`, and `user_rating` columns. 3. Combine the processed features using `ColumnTransformer`. 4. Fit a regression model to predict the `sales` using the processed features. Requirements 1. Define a function `process_and_predict_sales(df: pd.DataFrame) -> Pipeline`: - Input: - `df`: A pandas DataFrame containing the book dataset. - Output: - A fitted `Pipeline` object. 2. The pipeline should: - Utilize appropriate transformers for different columns as specified. - Use `LinearRegression` for predicting sales. 3. Ensure proper handling of missing values (if any). # Constraints - You must use `scikit-learn`\'s transformers and pipeline tools. - Use `ColumnTransformer` and `Pipeline` to structure the transformations. - Performance constraints are not rigid, but ensure your solution is efficient and adheres to best practices for data processing and machine learning modeling. Example Usage ```python import pandas as pd # Assume df is loaded DataFrame containing the dataset pipeline = process_and_predict_sales(df) print(pipeline) ``` Evaluation Criteria - Correctness of the implementation. - Proper usage of `Pipeline`, `ColumnTransformer`, and relevant `scikit-learn` transformers. - Handling of different data types and missing values. - Code efficiency and readability.","solution":"import pandas as pd from sklearn.compose import ColumnTransformer from sklearn.feature_extraction.text import CountVectorizer from sklearn.impute import SimpleImputer from sklearn.linear_model import LinearRegression from sklearn.pipeline import Pipeline from sklearn.preprocessing import OneHotEncoder, StandardScaler def process_and_predict_sales(df: pd.DataFrame) -> Pipeline: Processes the book dataset and fits a regression model to predict sales. Parameters: - df: pd.DataFrame - The book dataset. Returns: - pipeline: Pipeline - The fitted pipeline object. # Extract features and target X = df.drop(columns=[\'sales\']) y = df[\'sales\'] # Define column lists categorical_features = [\'genre\'] text_features = [\'title\', \'author\'] numeric_features = [\'pages\', \'expert_rating\', \'user_rating\'] # Preprocessing pipeline for categorical features categorical_transformer = Pipeline(steps=[ (\'imputer\', SimpleImputer(strategy=\'most_frequent\')), (\'onehot\', OneHotEncoder(handle_unknown=\'ignore\')) ]) # Preprocessing pipeline for text features text_transformer_title = Pipeline(steps=[ (\'countvec\', CountVectorizer()) ]) text_transformer_author = Pipeline(steps=[ (\'countvec\', CountVectorizer()) ]) # Preprocessing pipeline for numeric features numeric_transformer = Pipeline(steps=[ (\'imputer\', SimpleImputer(strategy=\'mean\')), (\'scaler\', StandardScaler()) ]) # Combine all column transformers using ColumnTransformer preprocessor = ColumnTransformer( transformers=[ (\'cat\', categorical_transformer, categorical_features), (\'title\', text_transformer_title, \'title\'), (\'author\', text_transformer_author, \'author\'), (\'num\', numeric_transformer, numeric_features) ]) # Create and fit the pipeline pipeline = Pipeline(steps=[ (\'preprocessor\', preprocessor), (\'regressor\', LinearRegression()) ]) pipeline.fit(X, y) return pipeline"},{"question":"# Advanced Seaborn Visualization **Objective:** Using the Seaborn library, create a comprehensive visualization that demonstrates your understanding of different plotting functionalities and customizations available through Seaborn. **Task:** Write a Python function `visualize_penguin_data` that demonstrates the use of Seaborn\'s `jointplot` along with various customization options. Your function should meet the following requirements: 1. **Data Loading:** Load the `\\"penguins\\"` dataset provided by Seaborn. 2. **Default Plot:** Create a default scatter plot with marginal histograms for the `bill_length_mm` and `bill_depth_mm` dimensions. 3. **Hue Plot:** Add a plot that includes a `hue` parameter using the `species` column to differentiate the data points by species with separate density curves. 4. **KDE Plot:** Include a KDE plot for the same dimensions with hue differentiation. 5. **Hex Plot:** Generate a hexbin-based visualization for the same dimensions. 6. **Regression Plot:** Create a plot that shows both the regression line and KDE curves. 7. **Custom Plot:** Use the custom arguments for marker size, bin count, and fill options provided in the base jointplot to customize the visualization. 8. **JointGrid Plot:** Make use of the JointGrid class to adjust the plot size and add rug plots on the margins. **Function Signature:** ```python def visualize_penguin_data(): pass ``` **Requirements:** - The function should create and display the plots within the function itself. - Ensure that all plots are adequately annotated with titles and labels for clarity. - The task should encompass at least five different types of plots available in Seaborn\'s `jointplot`. **Constraints:** - Use only the Seaborn library for the visualizations. - You may use Matplotlib for additional customizations if necessary. **Hints:** - Refer to `sns.load_dataset`, `sns.jointplot()`, and `sns.JointGrid` in the Seaborn documentation for more details. - Use additional keyword arguments to customize markers, bins, and other plot aspects. **Example Output:** The function, when executed, should display the series of plots as specified, each following one another, showcasing the different capabilities of the `jointplot` function in Seaborn.","solution":"import seaborn as sns import matplotlib.pyplot as plt def visualize_penguin_data(): # Load the penguins dataset penguins = sns.load_dataset(\'penguins\') # Default scatter plot with marginal histograms g = sns.jointplot(data=penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\") g.fig.suptitle(\\"Default Scatter Plot with Marginal Histograms\\", y=1.02) # Scatter plot with hue g = sns.jointplot(data=penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", hue=\\"species\\", kind=\\"scatter\\") g.fig.suptitle(\\"Scatter Plot with Hue\\", y=1.02) # KDE plot with hue g = sns.jointplot(data=penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", hue=\\"species\\", kind=\\"kde\\") g.fig.suptitle(\\"KDE Plot with Hue\\", y=1.02) # Hexbin plot g = sns.jointplot(data=penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", kind=\\"hex\\") g.fig.suptitle(\\"Hexbin Plot\\", y=1.02) # Regression plot with KDE curves g = sns.jointplot(data=penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", kind=\\"reg\\", color=\\"g\\") g.fig.suptitle(\\"Regression Plot with KDE Curves\\", y=1.02) # Custom plot with additional arguments g = sns.jointplot(data=penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", kind=\\"scatter\\", marker=\\"o\\", color=\\"b\\", s=50, marginal_kws=dict(bins=20, fill=True)) g.fig.suptitle(\\"Custom Plot with Marker Size and Custom Marginals\\", y=1.02) # JointGrid plot with rug plots on the margins g = sns.JointGrid(data=penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", height=8) g.plot(sns.scatterplot, sns.histplot) g.plot_marginals(sns.rugplot) g.fig.suptitle(\\"JointGrid Plot with Rug Plots on the Margins\\", y=1.02) plt.show()"},{"question":"**Question: Memory Leak Detection Using tracemalloc** You are provided with a Python application that seems to have a memory leak. Your task is to use the `tracemalloc` module to find and report the sources of these memory leaks. # Requirements: 1. Write a function `memory_leak_detector(code: str) -> str` that takes a string of Python code, executes it, and uses the `tracemalloc` module to detect memory leaks. 2. The function should: - Start memory tracing with enough frames to provide detailed traceback information. - Execute the provided code. - Take a snapshot before and after the code execution to capture memory allocation statistics. - Compare these snapshots and report the top 5 sources of memory leaks in terms of size difference. - Return a formatted string containing this information. # Implementation Details: - Start memory tracing with `tracemalloc.start(10)`. - Execute the provided code using the `exec()` function. - Take snapshots using `tracemalloc.take_snapshot()` before and after the code execution. - Use `snapshot.compare_to()` to find differences in memory allocation, and sort these by size difference. - Format and return the top 5 sources of memory leaks, showing file names, line numbers, and size differences in a human-readable format. # Input: - `code` (str): A string containing valid Python code to be executed. # Output: - `str`: A formatted string listing the top 5 memory leaks sources with details. # Constraints: - Assume the input code is well-formed and safe to execute. - Only trace and report memory allocations done by the Python code. Ignore C extension allocations. # Example Usage: ```python code = \'\'\' import tracemalloc def memory_leak(): leak = [] for i in range(10000): leak.append(\' \' * 100) return leak memory_leak() \'\'\' result = memory_leak_detector(code) print(result) ``` # Expected Output: ``` Top 5 memory leaks: 1. File \\"example.py\\", line 5: 976.6 KiB 2. File \\"example.py\\", line 6: 975.3 KiB 3. File \\"example.py\\", line 7: 50.0 KiB ... ``` The exact output may vary depending on the input code and memory allocations, but it should list the top 5 sources of memory leaks with detailed traceback information.","solution":"import tracemalloc import sys from io import StringIO def memory_leak_detector(code: str) -> str: Executes a given string of Python code and uses tracemalloc to detect memory leaks. Returns a formatted string listing the top 5 memory leaks sources with details. # Start tracing with 10 frames tracemalloc.start(10) # Redirect output to null to prevent code execution from printing old_stdout = sys.stdout sys.stdout = StringIO() try: # Take a snapshot before running the code snapshot_before = tracemalloc.take_snapshot() # Execute the provided code exec(code) # Take a snapshot after running the code snapshot_after = tracemalloc.take_snapshot() finally: # Restore the original stdout, important to do this in a finally block sys.stdout = old_stdout # Get the differences between the two snapshots stats = snapshot_after.compare_to(snapshot_before, \'lineno\') # Generate the report for the top 5 differences report_lines = [\\"Top 5 memory leaks:\\"] for index, stat in enumerate(stats[:5], start=1): report_lines.append(f\\"{index}. File {stat.traceback[0].filename}, \\" f\\"line {stat.traceback[0].lineno}: {stat.size_diff / 1024:.1f} KiB\\") return \\"n\\".join(report_lines)"},{"question":"Coding Assessment Question # Objective: Demonstrate your understanding of pseudo-terminal handling using the `pty` module to create a Python function that interacts with a subprocess. # Problem Statement: Implement a Python function `record_shell_output(commands, filename)` that executes a list of terminal commands using a pseudo-terminal, records both the input commands and their outputs into a specified file, and returns the exit code of the last executed command. # Function Signature: ```python def record_shell_output(commands: list[str], filename: str) -> int: pass ``` # Parameters: - `commands` (list[str]): A list of shell commands to execute sequentially. - `filename` (str): The name of the file where the input commands and their outputs will be recorded. # Returns: - `int`: The exit code of the last executed command. # Constraints: - The function must use the `pty.spawn()` method to handle the execution and recording. - Each command must be written to the file before its output. - Handle the case where the command provides no output. - The function should assume that the commands are simple shell commands and do not require complex processing or special characters. - The implementation should work on Unix-like operating systems. # Example Usage: ```python commands = [\\"echo Hello, World!\\", \\"ls -l\\", \\"pwd\\"] filename = \\"output.txt\\" exit_code = record_shell_output(commands, filename) print(f\\"Exit code: {exit_code}\\") ``` # Example File Content (output.txt) after running the above usage: ``` echo Hello, World! Hello, World! ls -l total 8 -rw-r--r-- 1 user user 1234 Jan 1 00:00 somefile.txt pwd /home/user ``` # Additional Notes: - Ensure thread safety and consider the potential need for closing file handles properly. - You may use any additional modules from the standard library that you deem necessary (e.g., `os`, `time`). Good luck and happy coding!","solution":"import os import pty import subprocess def record_shell_output(commands: list[str], filename: str) -> int: def read_and_write(fd): output = [] while True: try: data = os.read(fd, 1024) if not data: break output.append(data.decode()) except OSError: break with open(filename, \\"a\\") as f: f.writelines(output) for command in commands: with open(filename, \\"a\\") as f: f.write(f\\" {command}n\\") pid, fd = pty.fork() if pid == 0: subprocess.run(command, shell=True) os._exit(0) else: read_and_write(fd) pid, status = os.waitpid(pid, 0) return os.WEXITSTATUS(status)"},{"question":"In this coding assessment, you are required to interact with C functions using Python\'s `ctypes` library. The goal is to demonstrate your understanding of dynamic loading of libraries, function calling, type handling, and memory management. # Task 1. **Library Setup and Function Loading**: - Load the standard C library available on your operating system (`libc.so.6` on Linux or `msvcrt.dll` on Windows). - Access the `strchr` function from the library, which has the following prototype in C: ```c char* strchr(const char *str, int character); ``` 2. **Function Usage**: - Write a Python function `find_character(lib_name: str, input_string: str, char_to_find: str) -> str` that: - Loads the library specified by `lib_name`. - Uses `strchr` to find the first occurrence of `char_to_find` in `input_string`. - Returns the substring from the found character to the end of the input string. - Ensure that `char_to_find` is exactly one character long. # Input - `lib_name` (str): The name of the library to load. For example, \\"libc.so.6\\" on Linux and \\"msvcrt.dll\\" on Windows. - `input_string` (str): The string in which to search for the character. - `char_to_find` (str): The character to find in the string. It should be exactly one character long. # Output - Return (str): The substring from the first occurrence of the character to the end of the string, or an empty string if the character is not found. # Constraints - `input_string` will always be a non-empty string. - `char_to_find` will be a single character. # Example Example 1: Input ```python lib_name = \\"libc.so.6\\" # Use \\"msvcrt.dll\\" on Windows input_string = \\"Hello, world!\\" char_to_find = \\"w\\" ``` Output ```python \\"world!\\" ``` Example 2: Input ```python lib_name = \\"libc.so.6\\" # Use \\"msvcrt.dll\\" on Windows input_string = \\"Python ctypes example\\" char_to_find = \\"c\\" ``` Output ```python \\"ctypes example\\" ``` # Notes - Use the `ctypes` package to load the specified library and access the `strchr` function. - Handle the conversion between Python strings and the `char*` type expected by the `strchr` function. - If the `char_to_find` is not present in the `input_string`, return an empty string. Good luck!","solution":"import ctypes def find_character(lib_name: str, input_string: str, char_to_find: str) -> str: if len(char_to_find) != 1: raise ValueError(\\"char_to_find must be exactly one character long\\") # Load the C library libc = ctypes.CDLL(lib_name) # Define the strchr function libc.strchr.restype = ctypes.c_char_p libc.strchr.argtypes = [ctypes.c_char_p, ctypes.c_int] # Convert the input string to a bytes object input_bytes = input_string.encode(\'utf-8\') character = ord(char_to_find) # Call the strchr function result = libc.strchr(input_bytes, character) # If the character is not found, return an empty string if result is None: return \'\' # Convert the result back to a Python string result_string = ctypes.cast(result, ctypes.c_char_p).value.decode(\'utf-8\') return result_string"},{"question":"**Objective:** Write a Python function that will interact with the operating system utilities provided by the `python310` package. Your task will involve creating and managing temporary files and ensuring that operations are performed correctly while handling potential errors. Problem Statement You are tasked with writing two functions. The first function, `create_temporary_file`, should create a temporary file, write some content to it, and return the file\'s path. The second function, `read_temporary_file`, should take the path of the file created by the first function, ensure that it is interactive, then read and return its content if interactive, otherwise raise an exception. **Function Signatures:** ```python def create_temporary_file(content: str) -> str: /** * Creates a temporary file and writes the provided content to it. * * Arguments: * content (str): The content to write into the file. * * Returns: * str: The path to the temporary file. */ def read_temporary_file(file_path: str) -> str: /** * Reads the content of the file located at file_path if the file is interactive. * * Arguments: * file_path (str): The path to the file to read. * * Returns: * str: The content of the file if it is interactive. * * Raises: * ValueError: If the file is not interactive. */ ``` # Constraints 1. The `create_temporary_file` function should generate a unique file name for the temporary file. 2. The `read_temporary_file` function should use the `Py_FdIsInteractive` function to determine if the file is interactive. 3. If the file is not interactive, `read_temporary_file` should raise a `ValueError` with the message \\"File is not interactive\\". 4. You must handle any potential errors during file operations, such as file not found errors, and raise appropriate exceptions with descriptive messages. 5. Implement necessary imports and ensure that all resources, such as files, are properly closed and cleaned up after use. **Example Usage:** ```python try: temp_file_path = create_temporary_file(\\"Hello, World!\\") content = read_temporary_file(temp_file_path) print(\\"File content:\\", content) except ValueError as e: print(e) except Exception as e: print(\\"An error occurred:\\", e) ``` # Additional Notes - Ensure to include proper documentation and comments in your code. - You should aim to use proper error handling techniques as per Python best practices.","solution":"import tempfile import os def create_temporary_file(content: str) -> str: Creates a temporary file and writes the provided content to it. Args: content (str): The content to write into the file. Returns: str: The path to the temporary file. try: # Create a temporary file temp_file = tempfile.NamedTemporaryFile(delete=False, mode=\'w\', encoding=\'utf-8\') temp_file.write(content) temp_file_path = temp_file.name temp_file.close() return temp_file_path except Exception as e: raise Exception(f\\"Failed to create a temporary file: {e}\\") def read_temporary_file(file_path: str) -> str: Reads the content of the file located at file_path if the file is interactive. Args: file_path (str): The path to the file to read. Returns: str: The content of the file if it is interactive. Raises: ValueError: If the file is not interactive. try: # Check if the file exists if not os.path.exists(file_path): raise FileNotFoundError(f\\"File not found: {file_path}\\") # Mimic the behavior of Py_FdIsInteractive since this is pseudocode placeholder # Assume the file is always interactive as we cannot implement Py_FdIsInteractive in pure Python is_interactive = True if not is_interactive: raise ValueError(\\"File is not interactive\\") with open(file_path, \'r\', encoding=\'utf-8\') as file: content = file.read() return content except ValueError as ve: raise ve except FileNotFoundError as fnf: raise fnf except Exception as e: raise Exception(f\\"Failed to read the temporary file: {e}\\")"},{"question":"**Objective**: Assess the student\'s ability to work with advanced tensor operations in PyTorch using masked tensors. # Question: Implement a Masked Tensor Operation in PyTorch You are provided with a masked tensor class `MaskedTensor` in PyTorch, where individual elements can be selectively included or excluded from computations based on a mask. Your task is to implement a function that computes the element-wise reciprocal (1/x) of a masked tensor while ignoring masked-out elements. # Function Signature ```python def masked_reciprocal(masked_tensor: \'torch.masked.MaskedTensor\') -> \'torch.masked.MaskedTensor\': pass ``` # Input - `masked_tensor`: A `MaskedTensor` instance which contains: * `data`: A `torch.Tensor` representing the actual data. * `mask`: A `torch.Tensor` of the same shape as `data`, containing boolean values where `True` indicates that the corresponding `data` entry should be included in computations. # Output - Returns a new `MaskedTensor` where each element is the reciprocal of the corresponding element in `data` if it\'s not masked out. The mask remains the same as the input `masked_tensor`. # Example ```python import torch from torch.masked import masked_tensor # Create a MaskedTensor data = torch.tensor([1.0, 0.0, 2.0, 3.0, 4.0]) mask = torch.tensor([True, False, True, False, True]) mt = masked_tensor(data, mask) # Compute its masked reciprocal result = masked_reciprocal(mt) # The `result` should look like: # MaskedTensor( # data=tensor([1.0, 0.0, 0.5, 0.0, 0.25]), # mask=tensor([True, False, True, False, True]) # ) ``` # Constraints - Do not modify the input `masked_tensor` in place; create a new one for the result. - Ensure that your function efficiently handles large tensors. # Notes - You can use the provided unary operators and masked operations from `torch.masked`. - Handle any potential errors that might arise due to invalid operations (such as division by zero). # Performance Requirements - Your implementation should be efficient, leveraging PyTorch\'s vectorized operations to handle large tensors.","solution":"import torch class MaskedTensor: def __init__(self, data, mask): assert data.shape == mask.shape, \\"Data and mask must have the same shape\\" self.data = data self.mask = mask def masked_reciprocal(masked_tensor: MaskedTensor) -> MaskedTensor: Compute the element-wise reciprocal of a masked tensor while ignoring masked-out elements. reciprocal_data = torch.zeros_like(masked_tensor.data) reciprocal_data[masked_tensor.mask] = 1.0 / masked_tensor.data[masked_tensor.mask] return MaskedTensor(reciprocal_data, masked_tensor.mask)"},{"question":"You are given a large text file that contains multiple lines of text. Your task is to implement a function `process_text_file(input_file: str, output_file: str) -> None` using the deprecated `pipes` module to perform the following transformations on the file content: 1. Convert all text to uppercase. 2. Replace all spaces with underscores. 3. Sort all lines alphabetically. Function Signature ```python def process_text_file(input_file: str, output_file: str) -> None: ``` Input - `input_file` (str): Path to the input text file. - `output_file` (str): Path where the processed text will be saved. Output - The function should save the processed text in `output_file`. Constraints - You must use the `pipes` module for creating and manipulating the pipeline. - You must use shell commands to perform the text transformations. Example Assume the content of `input.txt` is: ``` hello world python programming learn python ``` The expected content of `output.txt` after processing should be: ``` HELLO_WORLD LEARN_PYTHON PYTHON_PROGRAMMING ``` Note The `pipes` module is deprecated. The objective is to demonstrate understanding of the module’s functionality. In real-world applications, consider using the `subprocess` module instead. Performance Requirements - The implementation should be efficient enough to handle large text files. **Hints**: - Use the `Template.append` method to add commands to process the text. - Use the `Template.open` method to read from the `input_file` and write to the `output_file`.","solution":"import pipes def process_text_file(input_file: str, output_file: str) -> None: Process the text file by converting all text to uppercase, replacing spaces with underscores, and sorting all lines alphabetically, using the `pipes` module. Parameters: - input_file (str): Path to the input text file. - output_file (str): Path where the processed text will be saved. t = pipes.Template() t.append(\'tr \\"[:lower:]\\" \\"[:upper:]\\"\', \'--\') # Convert to uppercase t.append(\'sed \\"s/ /_/g\\"\', \'--\') # Replace spaces with underscores t.append(\'sort\', \'--\') # Sort lines alphabetically with t.open(output_file, \'w\') as f_out: with open(input_file, \'r\') as f_in: for line in f_in: f_out.write(line)"},{"question":"# Objective The task is to implement a Python program that utilizes both the `threading` and `multiprocessing` modules to perform concurrent execution of CPU-bound and I/O-bound tasks. This will assess your understanding of how to effectively use threads and processes for different types of tasks and how to synchronize them. # Question You are required to: 1. Create a function `cpu_bound_task(n)` that calculates and returns the sum of the first `n` prime numbers. 2. Create a function `io_bound_task(filename)` that reads a file line by line and counts the number of lines that contain the word \\"Python\\". The function should return this count. 3. Create a program that does the following: - Spawns a separate process for the `cpu_bound_task` and a separate thread for the `io_bound_task`. - Make sure both tasks run concurrently. - Ensure that the main program waits for both the thread and process to complete before exiting. - Print the results from both tasks in the main program. # Constraints - You can assume that the input `n` for `cpu_bound_task` is a positive integer less than or equal to 100,000. - The input `filename` for `io_bound_task` is a valid file path to a readable text file. # Input and Output - Input: - An integer `n`. - A string `filename`. - Output: - The sum of the first `n` primes. - The number of lines containing the word \\"Python\\" in the specified file. # Example Suppose the input is: ```python n = 1000 filename = \'sample.txt\' ``` Then, the output should be: ```python Sum of first 1000 primes: [sum_of_primes] Lines containing \'Python\': [number_of_lines_with_python] ``` # Implementation Notes - Use the `multiprocessing` module to create a separate process for `cpu_bound_task`. - Use the `threading` module to create a separate thread for `io_bound_task`. - Ensure proper synchronization to wait for both tasks to complete before printing the results. - Handle any potential exceptions that might occur during file reading. # Hints - You might want to use a `Queue` from the `multiprocessing` module to get the result from the `cpu_bound_task`. - Use a `Thread` and a `Lock` from the `threading` module to manage the `io_bound_task`.","solution":"import threading import multiprocessing import queue from multiprocessing import Process, Queue def is_prime(num): Return True if num is a prime number. if num <= 1: return False if num <= 3: return True if num % 2 == 0 or num % 3 == 0: return False i = 5 while i * i <= num: if num % i == 0 or num % (i + 2) == 0: return False i += 6 return True def cpu_bound_task(n, output_queue): Calculate the sum of the first n prime numbers. count = 0 num = 2 prime_sum = 0 while count < n: if is_prime(num): prime_sum += num count += 1 num += 1 output_queue.put(prime_sum) def io_bound_task(filename): Count number of lines that contain the word \'Python\'. count = 0 with open(filename, \'r\') as file: for line in file: if \'Python\' in line: count += 1 return count def main(n, filename): # Create a queue to receive the result from the cpu_bound_task cpu_result_queue = Queue() # Create and start the cpu-bound task as a separate process cpu_process = Process(target=cpu_bound_task, args=(n, cpu_result_queue)) cpu_process.start() # Create and start the io-bound task as a separate thread io_thread = threading.Thread(target=lambda q, arg: q.put(io_bound_task(arg)), args=(queue.Queue(), filename)) io_thread.start() # Wait for the cpu-bound process to finish and get the result cpu_process.join() cpu_result = cpu_result_queue.get() # Wait for the io-bound thread to finish and get the result io_thread.join() io_result = io_thread._args[0].get() # Print the results print(f\\"Sum of first {n} primes: {cpu_result}\\") print(f\\"Lines containing \'Python\': {io_result}\\") if __name__ == \\"__main__\\": main(1000, \'sample.txt\')"},{"question":"# Configuration File Manager Objective Create a Python script to manage application configuration settings using the `configparser` module. Your script should read configurations from a file, modify certain settings based on provided instructions, and finally write the updated configurations back to a file. Problem Statement 1. **Reading Configuration:** - Your script should read a configuration file in INI format with various sections and options. - The structure of the INI file is as follows: ``` [section1] option1 = value1 option2 = value2 [section2] option3 = value3 ``` 2. **Modifying Configuration:** - Implement a function `modify_config(config_file: str, changes: dict) -> None` where: - `config_file` is the path to the INI file. - `changes` is a dictionary with the following structure: ```python { \\"section_to_modify\\": { \\"option_to_modify\\": \\"new_value\\" }, \\"another_section_to_modify\\": { \\"another_option_to_modify\\": \\"another_new_value\\" } } ``` - Apply the changes to the specified sections and options. 3. **Fallback Values:** - If a section or option specified in the `changes` dictionary does not exist in the INI file, add it with the new value. 4. **Writing the Configuration:** - After applying the changes, write the updated configurations back to the same file. Constraints - The INI file may contain multiple sections and options. - Sections in the INI file are unique. - Options within a section are unique. Input and Output Format - The function `modify_config` does not return any value. - The function updates the INI file in place based on the provided changes. Example Given an INI file `config.ini`: ``` [database] host = localhost port = 5432 [logging] level = INFO ``` And the following changes: ```python changes = { \\"database\\": { \\"port\\": \\"5433\\" }, \\"logging\\": { \\"level\\": \\"DEBUG\\" }, \\"new_section\\": { \\"new_option\\": \\"new_value\\" } } ``` After calling `modify_config(\'config.ini\', changes)`, the updated `config.ini` should be: ``` [database] host = localhost port = 5433 [logging] level = DEBUG [new_section] new_option = new_value ``` Implementation Notes - Utilize the `configparser.ConfigParser` class for reading and writing the INI file. - Handle non-existent sections or options gracefully by adding them. - Ensure the function adheres to the specified input-output format and constraints. Good luck, and happy coding!","solution":"import configparser def modify_config(config_file: str, changes: dict) -> None: # Create a ConfigParser instance config = configparser.ConfigParser() # Read the existing configuration file config.read(config_file) # Apply changes for section, options in changes.items(): if not config.has_section(section): config.add_section(section) for option, value in options.items(): config.set(section, option, value) # Write the updated configuration back to the file with open(config_file, \'w\') as configfile: config.write(configfile)"},{"question":"Optimizing Prediction Latency and Throughput Objective In this assessment, you are required to implement a machine learning solution using the scikit-learn library with an emphasis on optimizing prediction latency and throughput. You will need to experiment with different model configurations and input data representations to achieve the best possible performance. Problem Statement You are provided with a dataset containing numerical features and a binary target variable. Your task is to: 1. Implement and train a scikit-learn model on the dataset. 2. Optimize the model\'s prediction latency and throughput by experimenting with different techniques provided in the documentation, such as using sparse matrices, configuring scikit-learn settings, and adjusting model complexity. 3. Measure the prediction latency and throughput for both bulk and atomic (one-at-a-time) prediction modes. 4. Report the results and provide a brief explanation of the optimizations applied and their impact. Data Assume the dataset `data.csv` is provided with the following structure: - `X_train`: A numpy array of shape `(n_samples, n_features)` representing the training features. - `y_train`: A numpy array of shape `(n_samples,)` representing the binary training labels. - `X_test`: A numpy array of shape `(m_samples, n_features)` representing the test features. Function Signature ```python def optimize_model_performance(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray) -> Tuple[float, float, float, float]: Optimize the model\'s prediction latency and throughput. Args: X_train (np.ndarray): Training feature matrix. y_train (np.ndarray): Training labels. X_test (np.ndarray): Test feature matrix. Returns: Tuple[float, float, float, float]: A tuple containing: - Atomic prediction latency (microseconds) - Bulk prediction latency (microseconds) - Atomic prediction throughput (predictions per second) - Bulk prediction throughput (predictions per second) pass ``` Constraints 1. You can use any classifier available in scikit-learn. 2. Ensure the data is cleaned and valid (handle NaN or infinite values if present). 3. Implement sparse matrix representation if it helps reduce latency and improve throughput. 4. Configure scikit-learn to minimize validation overhead during prediction. 5. Adjust model complexity to find a balance between performance and prediction power. Evaluation Criteria - Correctness of implementation. - Effective use of optimization techniques. - Demonstrated understanding of performance trade-offs. - Quality of the explanation provided for each optimization. Additional Information Make sure to read through the provided documentation thoroughly to understand the different techniques that can be applied to optimize prediction performance. You can assume that the necessary libraries (numpy, scikit-learn, etc.) are already installed. Example Usage ```python X_train, y_train, X_test = load_data(\'data.csv\') atomic_latency, bulk_latency, atomic_throughput, bulk_throughput = optimize_model_performance(X_train, y_train, X_test) print(f\\"Atomic Prediction Latency: {atomic_latency} microseconds\\") print(f\\"Bulk Prediction Latency: {bulk_latency} microseconds\\") print(f\\"Atomic Prediction Throughput: {atomic_throughput} predictions/second\\") print(f\\"Bulk Prediction Throughput: {bulk_throughput} predictions/second\\") ```","solution":"import numpy as np import pandas as pd from sklearn.linear_model import LogisticRegression from sklearn.model_selection import train_test_split from scipy.sparse import csr_matrix import time def optimize_model_performance(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray) -> tuple: Optimize the model\'s prediction latency and throughput. Args: X_train (np.ndarray): Training feature matrix. y_train (np.ndarray): Training labels. X_test (np.ndarray): Test feature matrix. Returns: Tuple[float, float, float, float]: A tuple containing: - Atomic prediction latency (microseconds) - Bulk prediction latency (microseconds) - Atomic prediction throughput (predictions per second) - Bulk prediction throughput (predictions per second) # Convert data to sparse representation for optimization X_train_sparse = csr_matrix(X_train) X_test_sparse = csr_matrix(X_test) # Initialize and train the logistic regression model model = LogisticRegression(solver=\'liblinear\') model.fit(X_train_sparse, y_train) # Measure atomic prediction latency and throughput start_time_atomic = time.time() for i in range(X_test_sparse.shape[0]): model.predict(X_test_sparse[i]) end_time_atomic = time.time() atomic_latency = ((end_time_atomic - start_time_atomic) / X_test_sparse.shape[0]) * 1e6 # convert to microseconds atomic_throughput = X_test_sparse.shape[0] / (end_time_atomic - start_time_atomic) # Measure bulk prediction latency and throughput start_time_bulk = time.time() model.predict(X_test_sparse) end_time_bulk = time.time() bulk_latency = (end_time_bulk - start_time_bulk) * 1e6 # convert to microseconds bulk_throughput = X_test_sparse.shape[0] / (end_time_bulk - start_time_bulk) return atomic_latency, bulk_latency, atomic_throughput, bulk_throughput"},{"question":"**Title:** Advanced HTTP Request Handling with `urllib` **Objective:** You are required to implement a Python function that fetches data from a given URL, handles errors effectively, and customizes the request headers to mimic a browser. **Problem Statement:** Write a function `fetch_webpage(url: str, user_agent: str = None) -> str` that performs the following tasks: 1. Attempts to fetch the webpage content from the given `url`. 2. If a `user_agent` is provided, it should add it to the request headers to mimic a browser. Default should be `\'Python-urllib/x.y\'` where `x` and `y` are the major and minor version numbers of Python. 3. Handles any potential HTTP errors (`HTTPError`) by returning a string formatted as `\\"Error code: <code>\\"`. 4. Handles any URL errors (`URLError`) by returning a string formatted as `\\"Failed to reach the server. Reason: <reason>\\"`. 5. Returns the content of the webpage as a string if the request is successful. **Constraints:** - Do not use any additional libraries except for `urllib`. - The function should handle timeouts and complete the request within 10 seconds. - Ensure the code works with both HTTP and HTTPS URLs. **Function Signature:** ```python def fetch_webpage(url: str, user_agent: str = None) -> str: pass ``` **Examples:** ```python # Example usage with a custom user agent print(fetch_webpage(\\"http://www.example.com\\", user_agent=\\"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\\")) # Example usage with default user agent print(fetch_webpage(\\"http://www.example.com\\")) # Example usage with an invalid URL print(fetch_webpage(\\"http://www.nonexistentwebsite.com\\")) ``` Expected Output: (Note: The actual content will differ based on the live webpage) ``` \\"<!doctype html>...\\" \\"<!doctype html>...\\" \\"Failed to reach the server. Reason: ...\\" ``` **Note:** Be sure to conduct proper exception handling and ensure that the function performs well under various scenarios.","solution":"import urllib.request import urllib.error import sys def fetch_webpage(url: str, user_agent: str = None) -> str: if user_agent is None: user_agent = f\\"Python-urllib/{sys.version_info.major}.{sys.version_info.minor}\\" headers = {\'User-Agent\': user_agent} try: req = urllib.request.Request(url, headers=headers) with urllib.request.urlopen(req, timeout=10) as response: return response.read().decode(\'utf-8\') except urllib.error.HTTPError as e: return f\\"Error code: {e.code}\\" except urllib.error.URLError as e: return f\\"Failed to reach the server. Reason: {e.reason}\\""},{"question":"You are required to implement a Python function that connects to a POP3 server, retrieves a list of emails, and processes the emails based on specific criteria. The function should utilize the `poplib` module from Python\'s standard library. # Function Signature ```python def process_emails(server: str, use_ssl: bool, username: str, password: str, delete_subjects: List[str]) -> List[Tuple[str, str]]: Connect to a POP3 server, authenticate, retrieve emails, and process them. Parameters: - server (str): The address of the POP3 server. - use_ssl (bool): If True, use SSL for the connection; otherwise, use a plain connection. - username (str): The username for the email account. - password (str): The password for the email account. - delete_subjects (List[str]): A list of email subjects that, if matched, the email should be marked for deletion. Returns: - List[Tuple[str, str]]: A list of tuples, each containing the subject and body of messages that were not marked for deletion. Raises: - ValueError: If there is an issue with the connection. - Exception: For any other general errors. pass ``` # Requirements 1. **Connection:** Establish a connection to the specified POP3 server. If `use_ssl` is `True`, use the `POP3_SSL` class; otherwise, use the `POP3` class. 2. **Authentication:** Authenticate using the provided `username` and `password`. 3. **Retrieving Emails:** - Fetch the list of all email messages. - For each email, retrieve its subject and body. - If an email\'s subject matches any string in `delete_subjects`, mark that email for deletion. 4. **Return the Emails:** - Return a list of tuples for messages that were not marked for deletion. Each tuple should contain the email\'s subject and body. 5. **Error Handling:** - Raise a `ValueError` if there is an issue with the connection. - Handle any other exceptions appropriately. # Example Usage ```python server = \\"pop.example.com\\" use_ssl = True username = \\"user@example.com\\" password = \\"password123\\" delete_subjects = [\\"Spam Subject\\", \\"Another Bad Subject\\"] emails = process_emails(server, use_ssl, username, password, delete_subjects) for subject, body in emails: print(f\\"Subject: {subject}\\") print(f\\"Body: {body}\\") ``` # Constraints - Ensure proper error handling and release of resources (like closing the connection). - Assume a stable network connection unless the server is unreachable. - Do not use external libraries apart from the standard Python library. # Hints - Use the `retr` method to get an email\'s content. - Use the `dele` method to mark an email for deletion. - The `stat` or `list` method can be used to get the number of messages in the mailbox. # Assumption For parsing the email content to extract the subject and body, consider the format returned by the POP3 commands.","solution":"import poplib from typing import List, Tuple from email.parser import Parser def process_emails(server: str, use_ssl: bool, username: str, password: str, delete_subjects: List[str]) -> List[Tuple[str, str]]: Connect to a POP3 server, authenticate, retrieve emails, and process them. Parameters: - server (str): The address of the POP3 server. - use_ssl (bool): If True, use SSL for the connection; otherwise, use a plain connection. - username (str): The username for the email account. - password (str): The password for the email account. - delete_subjects (List[str]): A list of email subjects that, if matched, the email should be marked for deletion. Returns: - List[Tuple[str, str]]: A list of tuples, each containing the subject and body of messages that were not marked for deletion. Raises: - ValueError: If there is an issue with the connection. - Exception: For any other general errors. try: mailbox = poplib.POP3_SSL(server) if use_ssl else poplib.POP3(server) mailbox.user(username) mailbox.pass_(password) except Exception as e: raise ValueError(\\"Failed to connect or authenticate to the server\\") from e messages_not_deleted = [] try: num_messages = len(mailbox.list()[1]) for i in range(1, num_messages + 1): response, lines, octets = mailbox.retr(i) msg_content = b\\"rn\\".join(lines).decode(\'utf-8\', errors=\'ignore\') msg = Parser().parsestr(msg_content) subject = msg.get(\\"subject\\", \\"\\") body = msg.get_payload() if subject in delete_subjects: mailbox.dele(i) else: messages_not_deleted.append((subject, body)) mailbox.quit() except Exception as e: raise Exception(\\"An error occurred while processing emails\\") from e return messages_not_deleted"},{"question":"**Problem Statement:** You are given an XML document representing information about several books in a library. Your task is to write a Python program that uses the `xml.etree.ElementTree` module to perform the following operations: 1. **Parse the XML Document:** - Read and parse an XML string containing the book data. 2. **Extract Information:** - Extract and print the title and author of each book. - Print the total number of books. 3. **Modify the XML Document:** - Add a new book to the XML document. - Modify the price of a book given its title. - Add an attribute `available` to each book element indicating whether it is available. The value of this attribute should be either \\"yes\\" or \\"no\\" based on the given criteria. 4. **Write the Modified XML Document:** - Write the modified XML document to a new XML file. 5. **Error Handling:** - Implement appropriate error handling for XML parsing errors. **Input:** - An XML string representing the initial data of books. - The title of the new book and its details (author, price). - The title of the book whose price needs to be updated and the new price. - An availability criterion (a dictionary) to determine the value of the `available` attribute. Example: `{ \'author\': \'J.K. Rowling\', \'available\': \'yes\' }` means all books by J.K. Rowling are available. **Output:** - Print the titles and authors of all books. - Print the total number of books. - Write the modified XML document to a new file named `updated_books.xml`. **Constraints:** - You may assume that the XML string is well-formed and contains at least one book element. **Example XML String:** ```xml <library> <book> <title>Harry Potter and the Philosopher\'s Stone</title> <author>J.K. Rowling</author> <price>29.99</price> </book> <book> <title>Clean Code</title> <author>Robert C. Martin</author> <price>25.99</price> </book> </library> ``` **Example Function Signature:** ```python def process_books(xml_string, new_book, update_price, availability_criteria): # Your code here pass # Example Usage: xml_string = <library> <book> <title>Harry Potter and the Philosopher\'s Stone</title> <author>J.K. Rowling</author> <price>29.99</price> </book> <book> <title>Clean Code</title> <author>Robert C. Martin</author> <price>25.99</price> </book> </library> new_book = { \'title\': \'The Pragmatic Programmer\', \'author\': \'Andrew Hunt\', \'price\': \'39.99\' } update_price = { \'title\': \'Clean Code\', \'price\': \'22.99\' } availability_criteria = { \'author\': \'J.K. Rowling\', \'available\': \'yes\' } process_books(xml_string, new_book, update_price, availability_criteria) ```","solution":"import xml.etree.ElementTree as ET def process_books(xml_string, new_book, update_price, availability_criteria): try: # Parse the XML string root = ET.fromstring(xml_string) # Extract and print the titles and authors of each book for book in root.findall(\'book\'): title = book.find(\'title\').text author = book.find(\'author\').text print(f\'Title: {title}, Author: {author}\') # Print the total number of books num_books = len(root.findall(\'book\')) print(f\'Total number of books: {num_books}\') # Add a new book to the XML document new_book_element = ET.Element(\'book\') title_element = ET.SubElement(new_book_element, \'title\') title_element.text = new_book[\'title\'] author_element = ET.SubElement(new_book_element, \'author\') author_element.text = new_book[\'author\'] price_element = ET.SubElement(new_book_element, \'price\') price_element.text = new_book[\'price\'] root.append(new_book_element) # Modify the price of a book given its title for book in root.findall(\'book\'): if book.find(\'title\').text == update_price[\'title\']: book.find(\'price\').text = update_price[\'price\'] # Add an attribute available to each book element for book in root.findall(\'book\'): author = book.find(\'author\').text book.set(\'available\', \'yes\' if author == availability_criteria[\'author\'] else \'no\') # Write the modified XML document to a new file tree = ET.ElementTree(root) tree.write(\'updated_books.xml\') except ET.ParseError as e: print(f\'Error parsing XML: {e}\') # Example usage if __name__ == \\"__main__\\": xml_string = <library> <book> <title>Harry Potter and the Philosopher\'s Stone</title> <author>J.K. Rowling</author> <price>29.99</price> </book> <book> <title>Clean Code</title> <author>Robert C. Martin</author> <price>25.99</price> </book> </library> new_book = { \'title\': \'The Pragmatic Programmer\', \'author\': \'Andrew Hunt\', \'price\': \'39.99\' } update_price = { \'title\': \'Clean Code\', \'price\': \'22.99\' } availability_criteria = { \'author\': \'J.K. Rowling\', \'available\': \'yes\' } process_books(xml_string, new_book, update_price, availability_criteria)"},{"question":"**Question: Implement a Custom Metric Handler** You are tasked with creating a custom metric handler for TorchElastic that pushes metrics to a hypothetical metrics handling service. Your goal is to implement a class `MyMetricHandler` which inherits from `torch.distributed.elastic.metrics.MetricHandler` and overrides the `emit` method to send metric data to this service. # Requirements: 1. **Class Definition**: Define a class `MyMetricHandler` that inherits from `torch.distributed.elastic.metrics.MetricHandler`. 2. **emit Method**: Implement the `emit` method which takes a single parameter `metric_data` of type `torch.distributed.elastic.metrics.MetricData`. 3. **Metric Data Processing**: Inside the `emit` method, process the `metric_data` to extract relevant information and format it as a dictionary with keys `name`, `value`, `timestamp`, and `metadata`. 4. **Service Integration**: Simulate pushing this dictionary to a metrics handling service by printing the dictionary (in a real scenario, this might involve making an HTTP post request or similar). # Input: - You do not need to handle any specific input directly; your focus is on defining the class and method as specified. # Output: - Define and correctly implement `MyMetricHandler` with the required functionality. - Demonstrate the usage of your custom metric handler by creating an instance and simulating an emit call with mock data. # Additional Information: - Assume `torch.distributed.elastic.metrics.MetricData` is an existing structure that contains the metric information in attributes `name`, `value`, `timestamp`, and `metadata`. # Example: ```python import torch.distributed.elastic.metrics as metrics class MyMetricHandler(metrics.MetricHandler): def emit(self, metric_data: metrics.MetricData): metric_dict = { \\"name\\": metric_data.name, \\"value\\": metric_data.value, \\"timestamp\\": metric_data.timestamp, \\"metadata\\": metric_data.metadata } print(f\\"Pushing to metrics service: {metric_dict}\\") # Demonstrate usage mock_data = metrics.MetricData(name=\\"accuracy\\", value=0.95, timestamp=1633241234, metadata={\\"epoch\\": 1}) handler = MyMetricHandler() handler.emit(mock_data) ``` In this example, the `emit` method processes the metric data and simulates pushing it to a handling service by printing it. # Constraints: - Assume that the metric data provided to the `emit` method is always valid and contains all the necessary fields. # Evaluation Criteria: - Correctness of the class definition and method overriding. - Proper processing and formatting of the metric data. - Effective simulation of the metrics handling process.","solution":"import torch.distributed.elastic.metrics as metrics class MyMetricHandler(metrics.MetricHandler): def emit(self, metric_data: metrics.MetricData): metric_dict = { \\"name\\": metric_data.name, \\"value\\": metric_data.value, \\"timestamp\\": metric_data.timestamp, \\"metadata\\": metric_data.metadata } # Simulating pushing to metrics handling service print(f\\"Pushing to metrics service: {metric_dict}\\") # Demonstrate usage with mock data class MockMetricData: def __init__(self, name, value, timestamp, metadata): self.name = name self.value = value self.timestamp = timestamp self.metadata = metadata mock_data = MockMetricData(name=\\"accuracy\\", value=0.95, timestamp=1633241234, metadata={\\"epoch\\": 1}) handler = MyMetricHandler() handler.emit(mock_data)"},{"question":"# Question: Using and Understanding the `__future__` Module You are provided a list of features that were introduced using the `__future__` module in Python, along with their optional and mandatory release versions. Write a function named `feature_release_info` that takes in no parameters and returns a dictionary. The dictionary should map each feature name (as a string) to another dictionary containing \\"optional\\" and \\"mandatory\\" as keys, which point to the respective release version information. Expected output format: ```python { \\"nested_scopes\\": { \\"optional\\": (2, 1, 0, \\"beta\\", 1), \\"mandatory\\": (2, 2, 0, \\"final\\", 0) }, \\"generators\\": { \\"optional\\": (2, 2, 0, \\"alpha\\", 1), \\"mandatory\\": (2, 3, 0, \\"final\\", 0) }, ... } ``` Constraints: * The information must be parsed programmatically. Do not hard-code the values. * You can use the provided documentation or use Python reflection to access the `__future__` module programmatically. Performance Requirements: * Your solution should parse and construct the nested dictionary in an efficient manner. Example Usage: ```python from feature_release_info import feature_release_info info = feature_release_info() print(info) ``` ```python # Expected output structure: { \\"nested_scopes\\": { \\"optional\\": (2, 1, 0, \\"beta\\", 1), \\"mandatory\\": (2, 2, 0, \\"final\\", 0) }, \\"generators\\": { \\"optional\\": (2, 2, 0, \\"alpha\\", 1), \\"mandatory\\": (2, 3, 0, \\"final\\", 0) }, ... } ``` # Hint: You might find the `__future__` module\'s attributes and properties to be useful in extracting the necessary information.","solution":"import __future__ def feature_release_info(): features = {} for feature_name in dir(__future__): feature = getattr(__future__, feature_name) if isinstance(feature, __future__._Feature): features[feature_name] = { \\"optional\\": feature.optional, \\"mandatory\\": feature.mandatory } return features"},{"question":"<|Analysis Begin|> The enumeration module in Python provides a set of tools for creating enumerated constants. Enumerations are created using a class syntax and can be compared by identity. The module defines four enumeration classes: `Enum`, `IntEnum`, `Flag`, and `IntFlag`, each serving unique purposes. Additionally, it includes a decorator `unique()` to ensure unique values and a helper `auto` for automatic value assignment of enumeration members. Important points from the documentation: 1. **Enum Class**: Base class for creating enumerated constants. 2. **IntEnum Class**: Subclass of `Enum` with members that are also subclasses of `int`. 3. **IntFlag Class**: Similar to `IntEnum`, but members can be combined using bitwise operations. 4. **Flag Class**: Members can be combined using bitwise operations but do not subclass `int`. Enumerations support: - Iteration - Programmatic access to members using values or names. - Custom methods in enumeration classes. - Ensuring unique values using the `@unique` decorator. - Creating members with automatic values using the `auto` helper. Enumerations can be subclassed with restrictions, pickled, and have several functional API options for creation. There are examples and recipes for more advanced enum types like `OrderedEnum`, `DuplicateFreeEnum`, and using custom `__new__` methods for unique behaviors. <|Analysis End|> <|Question Begin|> **Problem Description**: You are to develop an enumeration-based solution to manage user roles and permissions within a system. You will need to create an enumeration to represent different user roles and their associated permissions using the `Enum` and `Flag` classes from the Python `enum` module. **Requirements**: 1. **Role Enum**: - Create an `Enum` class named `Role` that includes the following members: - `ADMIN`: Represents an administrator. - `USER`: Represents a standard user. - `GUEST`: Represents a guest user. - Ensure that these members can be printed in a human-readable format and that their names and values can be programmatically accessed. 2. **Permission Flag**: - Create a `Flag` class named `Permission` that includes the following members with appropriate bitwise values: - `READ`: Permission to read data. - `WRITE`: Permission to write data. - `EXECUTE`: Permission to execute operations. - `ALL`: A combination of `READ`, `WRITE`, and `EXECUTE`. 3. **Check Permissions**: - Implement a function `has_permission(role: Role, permission: Permission) -> bool` that checks if a given role includes the specified permission. - For simplicity, assume: - `ADMIN` has `ALL` permissions. - `USER` has `READ` and `WRITE` permissions. - `GUEST` has only `READ` permission. **Function Signature**: ```python from enum import Enum, Flag, auto class Role(Enum): ADMIN = auto() USER = auto() GUEST = auto() class Permission(Flag): READ = auto() WRITE = auto() EXECUTE = auto() ALL = READ | WRITE | EXECUTE def has_permission(role: Role, permission: Permission) -> bool: pass # Example Usage admin_permissions = Permission.ALL print(has_permission(Role.ADMIN, Permission.READ)) # Expected: True print(has_permission(Role.USER, Permission.EXECUTE)) # Expected: False print(has_permission(Role.GUEST, Permission.WRITE)) # Expected: False ``` **Constraints**: 1. Do not use any external libraries other than `enum`. 2. The function should return results quickly even if multiple checks are performed. **Notes**: - Utilize the `@unique` decorator where necessary. - Ensure that the `has_permission` function uses an efficient approach to check permissions. **Evaluation Criteria**: - Correct implementation of enum and flag classes. - Accuracy of the `has_permission` function. - Proper use of Python\'s `enum` module features.","solution":"from enum import Enum, Flag, auto class Role(Enum): ADMIN = auto() USER = auto() GUEST = auto() class Permission(Flag): READ = auto() WRITE = auto() EXECUTE = auto() ALL = READ | WRITE | EXECUTE def has_permission(role: Role, permission: Permission) -> bool: if role == Role.ADMIN: return Permission.ALL & permission == permission elif role == Role.USER: user_permissions = Permission.READ | Permission.WRITE return user_permissions & permission == permission elif role == Role.GUEST: return Permission.READ & permission == permission return False # Example Usage admin_permissions = Permission.ALL print(has_permission(Role.ADMIN, Permission.READ)) # Expected: True print(has_permission(Role.USER, Permission.EXECUTE)) # Expected: False print(has_permission(Role.GUEST, Permission.WRITE)) # Expected: False"},{"question":"# PyTorch MPS Backend: Model Training on GPU **Objective**: You are tasked with using the MPS backend with PyTorch to train a simple neural network model on a macOS device, leveraging GPU for computations. This exercise will demonstrate your understanding of setting up and utilizing the MPS device in PyTorch. **Problem Statement**: Write a function `train_model_on_mps` that trains a given neural network model on a given dataset using the MPS backend. The function should: 1. Check if the MPS device is available. 2. Move the model to the MPS device if available, otherwise raise an appropriate error. 3. Define a simple training loop that runs for a specified number of epochs. 4. Return the trained model and the list of loss values for each epoch. **Function Signature**: ```python def train_model_on_mps(model: torch.nn.Module, dataset: torch.utils.data.Dataset, epochs: int, batch_size: int, learning_rate: float) -> tuple[torch.nn.Module, list[float]]: pass ``` **Inputs**: - `model`: An instance of `torch.nn.Module` representing the neural network to be trained. - `dataset`: An instance of `torch.utils.data.Dataset` containing the training data. - `epochs`: An integer specifying the number of epochs to train the model. - `batch_size`: An integer specifying the batch size for training. - `learning_rate`: A float specifying the learning rate for the optimizer. **Outputs**: - A tuple containing: - The trained model (`torch.nn.Module`). - A list of float values representing the training loss for each epoch. **Constraints**: - If the MPS device is not available, the function should raise an error with the message \\"MPS device not available.\\" **Implementation Notes**: 1. You may use `torch.utils.data.DataLoader` to load data in batches. 2. Use a simple loss function such as `torch.nn.CrossEntropyLoss`. 3. Use an optimizer such as `torch.optim.SGD`. 4. Ensure that data and the model are moved to the MPS device before performing computations. **Example**: ```python import torch import torch.nn as nn import torch.optim as optim import torch.backends.mps class SimpleNet(nn.Module): def __init__(self): super(SimpleNet, self).__init__() self.fc1 = nn.Linear(784, 128) self.fc2 = nn.Linear(128, 10) def forward(self, x): x = torch.relu(self.fc1(x)) x = self.fc2(x) return x # Assume you have a dataset `train_dataset` trained_model, loss_values = train_model_on_mps(SimpleNet(), train_dataset, epochs=10, batch_size=32, learning_rate=0.01) ```","solution":"import torch import torch.nn as nn import torch.optim as optim from torch.utils.data import DataLoader def train_model_on_mps(model: torch.nn.Module, dataset: torch.utils.data.Dataset, epochs: int, batch_size: int, learning_rate: float) -> tuple[torch.nn.Module, list[float]]: # Check if MPS is available if not torch.backends.mps.is_available(): raise RuntimeError(\\"MPS device not available.\\") # Move model to MPS device device = torch.device(\\"mps\\") model.to(device) # Define loss function and optimizer criterion = nn.CrossEntropyLoss() optimizer = optim.SGD(model.parameters(), lr=learning_rate) # Prepare data loader dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True) loss_values = [] # Training loop for epoch in range(epochs): model.train() running_loss = 0.0 for inputs, labels in dataloader: # Move data to MPS device inputs, labels = inputs.to(device), labels.to(device) # Zero the parameter gradients optimizer.zero_grad() # Forward pass outputs = model(inputs) loss = criterion(outputs, labels) # Backward pass and optimize loss.backward() optimizer.step() # Accumulate loss running_loss += loss.item() # Calculate and store average loss for the epoch epoch_loss = running_loss / len(dataloader) loss_values.append(epoch_loss) return model, loss_values"},{"question":"You are to write a Python function that employs memory-mapped file capabilities using the `mmap` module to process a large binary file efficiently. Specifically, the function should perform the following tasks: 1. **Create a Memory-Mapped File:** - Open a binary file in read-write mode. - Map the entire file into memory. 2. **Search and Replace:** - Search for a given byte sequence within the file. - Replace all occurrences of this sequence with another byte sequence of the same length. - Raise an exception if the sequences have different lengths. 3. **Persist Changes:** - Ensure all changes are flushed to the disk. 4. **Return the number of replacements made.** # Function Signature ```python def search_and_replace_in_file(file_path: str, search_bytes: bytes, replace_bytes: bytes) -> int: Searches for all occurrences of search_bytes in the specified file and replaces them with replace_bytes. Args: - file_path: The path to the binary file. - search_bytes: The byte sequence to search for. - replace_bytes: The byte sequence to replace search_bytes with. Returns: - int: The number of replacements made. Raises: - ValueError: If search_bytes and replace_bytes are of different lengths. ``` # Constraints - `search_bytes` and `replace_bytes` must have the same length. - The file may be large, so efficiency is crucial. # Example Given a file `example.bin` that contains the following bytes: ``` [0x01, 0x02, 0x01, 0x02, 0x01, 0x02, 0x01, 0x02] ``` Calling `search_and_replace_in_file(\'example.bin\', b\'x01x02\', b\'x03x04\')` should modify the file to: ``` [0x03, 0x04, 0x03, 0x04, 0x03, 0x04, 0x03, 0x04] ``` and return `4`, as there are four replacements. # Implementation Tips - Use the `mmap` module to map the file into memory. - Use the `find` method to locate byte sequences. - Use slice assignment to replace byte sequences. - Use the `flush` method to ensure changes are written to disk.","solution":"import mmap def search_and_replace_in_file(file_path: str, search_bytes: bytes, replace_bytes: bytes) -> int: if len(search_bytes) != len(replace_bytes): raise ValueError(\\"search_bytes and replace_bytes must have the same length\\") replacement_count = 0 with open(file_path, \'r+b\') as f: mm = mmap.mmap(f.fileno(), 0) pos = mm.find(search_bytes) while pos != -1: mm[pos:pos+len(search_bytes)] = replace_bytes replacement_count += 1 pos = mm.find(search_bytes, pos + len(search_bytes)) mm.flush() mm.close() return replacement_count"},{"question":"You are given a dataset with a high number of features, and your task is to reduce its dimensionality using three different techniques: Principal Component Analysis (PCA), Random Projections, and Feature Agglomeration, and compare the variance explained by each method. Requirements 1. **Data preparation**: - Standardize the data using `preprocessing.StandardScaler`. 2. **Dimensionality reduction**: - Apply `PCA` to reduce the dimensionality to 5 components. - Apply `SparseRandomProjection` to reduce the dimensionality to 5 components. - Apply `FeatureAgglomeration` to reduce the dimensionality to 5 components. 3. **Comparison**: - Calculate the variance explained by the reduced dataset for each method (if possible). - Print summaries for the first 5 rows of the transformed datasets from each method. # Input - A Pandas DataFrame `df` with shape (n_samples, n_features). The dataset has n_samples samples and n_features features. - Assume `df` is already loaded and provided within the function\'s scope. # Output - Return a dictionary with the following keys: - `\\"PCA_variance\\"`: The variance explained by the PCA components. - `\\"SparseRandomProjection_variance\\"`: The variance explained by the Sparse Random Projection components. If this is not directly available, return None. - `\\"FeatureAgglomeration_variance\\"`: The variance explained by the Feature Agglomeration components. If this is not directly available, return None. - `\\"PCA_transformed\\"`: The first 5 rows of the DataFrame transformed by PCA. - `\\"SparseRandomProjection_transformed\\"`: The first 5 rows of the DataFrame transformed by Sparse Random Projection. - `\\"FeatureAgglomeration_transformed\\"`: The first 5 rows of the DataFrame transformed by Feature Agglomeration. # Constraints - Use only the scikit-learn library for PCA, SparseRandomProjection, FeatureAgglomeration, and StandardScaler. - You can assume the dataset fits in memory and there are no missing values. # Function Signature ```python def reduce_and_compare_dimensionality(df): # Your code here return { \\"PCA_variance\\": pca_variance, \\"SparseRandomProjection_variance\\": sparse_random_projection_variance, \\"FeatureAgglomeration_variance\\": feature_agglomeration_variance, \\"PCA_transformed\\": pca_transformed, \\"SparseRandomProjection_transformed\\": sparse_random_projection_transformed, \\"FeatureAgglomeration_transformed\\": feature_agglomeration_transformed } ``` Example ```python # Example usage import pandas as pd from sklearn.datasets import load_iris # Load dataset data = load_iris() df = pd.DataFrame(data.data, columns=data.feature_names) # Call the function result = reduce_and_compare_dimensionality(df) print(result[\\"PCA_variance\\"]) print(result[\\"SparseRandomProjection_variance\\"]) print(result[\\"FeatureAgglomeration_variance\\"]) print(result[\\"PCA_transformed\\"]) ``` # Notes - Make sure to handle the dimensionality reduction and summarization as specified. - Your output should be in the exact format as required by the function signature.","solution":"import pandas as pd from sklearn.decomposition import PCA from sklearn.random_projection import SparseRandomProjection from sklearn.cluster import FeatureAgglomeration from sklearn.preprocessing import StandardScaler def reduce_and_compare_dimensionality(df): # Standardize the data scaler = StandardScaler() df_scaled = scaler.fit_transform(df) # Apply PCA pca = PCA(n_components=5) pca_transformed = pca.fit_transform(df_scaled) pca_variance = sum(pca.explained_variance_ratio_) # Apply SparseRandomProjection srp = SparseRandomProjection(n_components=5) srp_transformed = srp.fit_transform(df_scaled) # Note: Variance explained by SparseRandomProjection is not directly available sparse_random_projection_variance = None # Apply FeatureAgglomeration fa = FeatureAgglomeration(n_clusters=5) fa_transformed = fa.fit_transform(df_scaled) # Note: Variance explained by FeatureAgglomeration is not directly available feature_agglomeration_variance = None # Create transformed dataframes for the first 5 rows pca_transformed_df = pd.DataFrame(pca_transformed).head() srp_transformed_df = pd.DataFrame(srp_transformed).head() fa_transformed_df = pd.DataFrame(fa_transformed).head() return { \\"PCA_variance\\": pca_variance, \\"SparseRandomProjection_variance\\": sparse_random_projection_variance, \\"FeatureAgglomeration_variance\\": feature_agglomeration_variance, \\"PCA_transformed\\": pca_transformed_df, \\"SparseRandomProjection_transformed\\": srp_transformed_df, \\"FeatureAgglomeration_transformed\\": fa_transformed_df }"},{"question":"**Question: Customizing Seaborn Themes and Plot** You are given a dataset containing the sales performance data of three products (A, B, and C) for the months January to March. Your task is to create a function that visualizes this data as a bar plot with customized themes using Seaborn. The visualization should meet the following criteria: 1. The plot should use a whitegrid style, and the palette should be \\"pastel\\". 2. Customize the plot by hiding the top and right spines. 3. Save the plot as an image file called \\"sales_performance.png\\". # Dataset You have been given the data in the form of a dictionary as below: ```python data = { \'Month\': [\'January\', \'January\', \'January\', \'February\', \'February\', \'February\', \'March\', \'March\', \'March\'], \'Product\': [\'A\', \'B\', \'C\', \'A\', \'B\', \'C\', \'A\', \'B\', \'C\'], \'Sales\': [30, 45, 25, 35, 42, 28, 40, 47, 30] } ``` # Function Signature ```python def visualize_sales_performance(data: dict) -> None: pass ``` # Input - `data`: A dictionary containing the sales performance data with keys \\"Month\\", \\"Product\\", and \\"Sales\\". # Output - Save the resulting plot as an image file named \\"sales_performance.png\\". # Constraints - Use the `seaborn` and `matplotlib` libraries for visualizing the plot. - Ensure that the plot adheres to the specified theme and customization requirements. # Example ```python data = { \'Month\': [\'January\', \'January\', \'January\', \'February\', \'February\', \'February\', \'March\', \'March\', \'March\'], \'Product\': [\'A\', \'B\', \'C\', \'A\', \'B\', \'C\', \'A\', \'B\', \'C\'], \'Sales\': [30, 45, 25, 35, 42, 28, 40, 47, 30] } visualize_sales_performance(data) # This should produce a saved image file \\"sales_performance.png\\" that shows the sales performance data. ``` # Notes - Read the documentation of Seaborn for more details on how to set themes and customize plots: https://seaborn.pydata.org/tutorial.html","solution":"import seaborn as sns import matplotlib.pyplot as plt import pandas as pd def visualize_sales_performance(data: dict) -> None: Visualizes the sales performance data with a customized bar plot theme. Parameters: data (dict): A dictionary with keys \\"Month\\", \\"Product\\", and \\"Sales\\" containing the sales performance data. # Convert the dictionary to a pandas DataFrame df = pd.DataFrame(data) # Set the theme and color palette sns.set_theme(style=\\"whitegrid\\", palette=\\"pastel\\") # Create the bar plot plt.figure(figsize=(10, 6)) barplot = sns.barplot(x=\'Month\', y=\'Sales\', hue=\'Product\', data=df) # Customize the plot by hiding the top and right spines sns.despine() # Save the resulting plot as an image file plt.savefig(\\"sales_performance.png\\")"},{"question":"# Boolean Operation Functions Python booleans (`True` and `False`) are a subclass of integers and have specific behaviors. In this exercise, you need to implement several functions that utilize these booleans and perform various operations. Task Requirements: 1. Implement a function `is_boolean(value)` that checks if the provided input is a boolean value. 2. Implement a function `long_to_boolean(value)` that converts a long integer into a boolean. According to Python\'s truthiness definition, zero should convert to `False`, and any non-zero value should convert to `True`. 3. Implement a function `count_boolean_true(values)` that counts the number of `True` values in a list of booleans. Input and Output Formats: 1. `is_boolean(value)`: - **Input:** An arbitrary value. - **Output:** `True` if it is a boolean, otherwise `False`. 2. `long_to_boolean(value)`: - **Input:** A long integer. - **Output:** The boolean representation of the input. 3. `count_boolean_true(values)`: - **Input:** A list containing boolean values. - **Output:** An integer count of the `True` values in the list. Example Usage: ```python # Example usage for is_boolean print(is_boolean(True)) # Should output: True print(is_boolean(1)) # Should output: False # Example usage for long_to_boolean print(long_to_boolean(0)) # Should output: False print(long_to_boolean(42)) # Should output: True # Example usage for count_boolean_true print(count_boolean_true([True, False, True, True])) # Should output: 3 print(count_boolean_true([False, False, False])) # Should output: 0 ``` Constraints: - Do not use any libraries or imports except built-in functions. - The list provided to `count_boolean_true` will only contain boolean values. - The `long_to_boolean` function should handle any valid integer value provided to it. Performance Requirements: - The `count_boolean_true` function should operate in `O(n)` time complexity, where `n` is the length of the list. Implement these functions as described and ensure they meet the provided input and output formats.","solution":"def is_boolean(value): Checks if the provided input is a boolean value. return isinstance(value, bool) def long_to_boolean(value): Converts a long integer into a boolean. Zero converts to False, otherwise converts to True. return bool(value) def count_boolean_true(values): Counts the number of True values in a list of boolean values. return sum(values)"},{"question":"# Visualization with Pandas and Matplotlib Objective: Your task is to demonstrate your comprehension of pandas\' and matplotlib\'s visualization capabilities by creating various types of plots from a given dataset and customizing them. Problem Statement: You are provided with a CSV file named `data.csv` containing a dataset with multiple numeric columns. You need to implement a Python function `generate_plots(file_path: str) -> None` to perform the following tasks: 1. Read the data from the `data.csv` file into a pandas DataFrame. 2. Create a line plot of the cumulative sum of each column. 3. Create a bar plot of the first 10 rows of the DataFrame. 4. Create a histogram for each numeric column, with bin size of 20. 5. Create a scatter plot comparing the first column (`col1`) against the second column (`col2`). 6. Create a box plot for each numeric column. 7. Create a pie plot for the first row. 8. Create an area plot for the cumulative sum of the DataFrame. 9. Customize the plots with titles, axis labels, and legends as appropriate. 10. Save each plot to a PNG file with an appropriate filename (e.g., `line_plot.png`, `bar_plot.png`, etc.) 11. Handle any missing data by filling with the mean of the respective columns before plotting. Input: - `file_path` (str): The path to the CSV file. Output: - The function should not return anything. Instead, it should save the generated plots as PNG files. Constraints: - Assume the CSV file contains at least 5 numeric columns. - Assume the CSV file contains at least 100 rows. Example: Suppose the `data.csv` contains the following data: ``` col1, col2, col3, col4, col5 1, 2, 3, 4, 5 6, 7, 8, 9, 10 ... ``` Function Signature: ```python def generate_plots(file_path: str) -> None: pass ``` Additional Notes: - Use `matplotlib` for creating the plots. - Ensure the plots have appropriate titles and axis labels. - Save the plots with filenames corresponding to the plot type as indicated above. Evaluation Criteria: - Correctness: The function meets the problem requirements and correctly implements the specified plots. - Completeness: All specified plot types are created and saved as PNG files. - Plot customization: Titles, axis labels, and legends are appropriately set for each plot. - Handling of missing data: Missing data is correctly handled as specified.","solution":"import pandas as pd import matplotlib.pyplot as plt def generate_plots(file_path: str) -> None: # Read data from CSV file df = pd.read_csv(file_path) # Handle missing data by filling with mean of the respective columns df.fillna(df.mean(), inplace=True) # Create a line plot of the cumulative sum of each column cumulative_sum = df.cumsum() cumulative_sum.plot(kind=\'line\') plt.title(\'Cumulative Sum of Each Column\') plt.xlabel(\'Index\') plt.ylabel(\'Cumulative Sum\') plt.legend(loc=\'upper left\') plt.savefig(\'line_plot.png\') plt.close() # Create a bar plot of the first 10 rows of the DataFrame df.head(10).plot(kind=\'bar\') plt.title(\'Bar Plot of First 10 Rows\') plt.xlabel(\'Index\') plt.ylabel(\'Values\') plt.legend(loc=\'upper right\') plt.savefig(\'bar_plot.png\') plt.close() # Create a histogram for each numeric column, with bin size of 20 df.hist(bins=20, figsize=(10, 8)) plt.suptitle(\'Histogram of Each Numeric Column\') plt.savefig(\'histogram_plot.png\') plt.close() # Create a scatter plot comparing the first column (col1) against the second column (col2) plt.scatter(df.iloc[:, 0], df.iloc[:, 1]) plt.title(\'Scatter Plot of Column 1 vs Column 2\') plt.xlabel(\'Column 1\') plt.ylabel(\'Column 2\') plt.savefig(\'scatter_plot.png\') plt.close() # Create a box plot for each numeric column df.plot(kind=\'box\', figsize=(10, 8)) plt.title(\'Box Plot of Each Numeric Column\') plt.savefig(\'box_plot.png\') plt.close() # Create a pie plot for the first row df.iloc[0].plot(kind=\'pie\', autopct=\'%1.1f%%\') plt.title(\'Pie Plot of First Row\') plt.ylabel(\'\') plt.savefig(\'pie_plot.png\') plt.close() # Create an area plot for the cumulative sum of the DataFrame cumulative_sum.plot(kind=\'area\', alpha=0.5) plt.title(\'Area Plot of Cumulative Sum\') plt.xlabel(\'Index\') plt.ylabel(\'Cumulative Sum\') plt.legend(loc=\'upper left\') plt.savefig(\'area_plot.png\') plt.close()"},{"question":"# Dataset Handling and Analysis with scikit-learn Problem Statement You are required to demonstrate your ability to load, generate, and analyze datasets using the `sklearn.datasets` package. Given the versatility of scikit-learn\'s dataset utilities, you will perform the following tasks: 1. **Load a Toy Dataset:** Load the *Iris* dataset using the appropriate function from `sklearn.datasets`. Extract the features (`data`) and the target (`target`) from the dataset. 2. **Generate a Synthetic Dataset:** Create a synthetic classification dataset using the `make_classification` function with the following properties: - Number of samples: 1000 - Number of features: 20 - Number of informative features: 15 - Number of redundant features: 5 - Number of classes: 3 - Random state for reproducibility: 42 3. **Perform Basic Analysis:** - Calculate and print the mean and standard deviation of each feature in both datasets (Iris and synthetic). - Count and print the number of instances for each class in both datasets (Iris and synthetic). Input There are no direct inputs to your function other than what is required to call the dataset functions. Output Your function should print: 1. The means and standard deviations of features for both the Iris and synthetic datasets. 2. The class distribution (number of instances per class) for both the Iris and synthetic datasets. Implementation You are required to implement the function `dataset_handling_and_analysis()` which performs all the above tasks. Use appropriate functions and methods from `sklearn.datasets` and other libraries as needed. ```python from sklearn.datasets import load_iris, make_classification import numpy as np def dataset_handling_and_analysis(): # Task 1: Load Iris dataset iris = load_iris() iris_data = iris.data iris_target = iris.target # Task 2: Generate Synthetic dataset X_synthetic, y_synthetic = make_classification( n_samples=1000, n_features=20, n_informative=15, n_redundant=5, n_classes=3, random_state=42 ) # Task 3: Perform Basic Analysis # Mean and standard deviation for Iris dataset iris_feature_means = np.mean(iris_data, axis=0) iris_feature_stds = np.std(iris_data, axis=0) # Mean and standard deviation for Synthetic dataset synthetic_feature_means = np.mean(X_synthetic, axis=0) synthetic_feature_stds = np.std(X_synthetic, axis=0) # Class distribution for Iris dataset unique_iris, counts_iris = np.unique(iris_target, return_counts=True) # Class distribution for Synthetic dataset unique_synthetic, counts_synthetic = np.unique(y_synthetic, return_counts=True) # Print results print(\\"Iris Dataset:\\") print(f\\"Feature means: {iris_feature_means}\\") print(f\\"Feature standard deviations: {iris_feature_stds}\\") print(f\\"Class distribution: {dict(zip(unique_iris, counts_iris))}\\") print(\\"nSynthetic Dataset:\\") print(f\\"Feature means: {synthetic_feature_means}\\") print(f\\"Feature standard deviations: {synthetic_feature_stds}\\") print(f\\"Class distribution: {dict(zip(unique_synthetic, counts_synthetic))}\\") # Call the function to execute tasks dataset_handling_and_analysis() ``` Constraints - Use appropriate methods from scikit-learn. - Ensure reproducibility by setting the random state where applicable. - Assume correct installation of scikit-learn and other required libraries in the environment.","solution":"from sklearn.datasets import load_iris, make_classification import numpy as np def dataset_handling_and_analysis(): # Task 1: Load Iris dataset iris = load_iris() iris_data = iris.data iris_target = iris.target # Task 2: Generate Synthetic dataset X_synthetic, y_synthetic = make_classification( n_samples=1000, n_features=20, n_informative=15, n_redundant=5, n_classes=3, random_state=42 ) # Task 3: Perform Basic Analysis # Mean and standard deviation for Iris dataset iris_feature_means = np.mean(iris_data, axis=0) iris_feature_stds = np.std(iris_data, axis=0) # Mean and standard deviation for Synthetic dataset synthetic_feature_means = np.mean(X_synthetic, axis=0) synthetic_feature_stds = np.std(X_synthetic, axis=0) # Class distribution for Iris dataset unique_iris, counts_iris = np.unique(iris_target, return_counts=True) # Class distribution for Synthetic dataset unique_synthetic, counts_synthetic = np.unique(y_synthetic, return_counts=True) # Print results print(\\"Iris Dataset:\\") print(f\\"Feature means: {iris_feature_means}\\") print(f\\"Feature standard deviations: {iris_feature_stds}\\") print(f\\"Class distribution: {dict(zip(unique_iris, counts_iris))}\\") print(\\"nSynthetic Dataset:\\") print(f\\"Feature means: {synthetic_feature_means}\\") print(f\\"Feature standard deviations: {synthetic_feature_stds}\\") print(f\\"Class distribution: {dict(zip(unique_synthetic, counts_synthetic))}\\") # Call the function to execute tasks dataset_handling_and_analysis()"},{"question":"# Hyper-parameter Tuning Challenge with GridSearchCV and RandomizedSearchCV Context You are provided a dataset `wine.csv` containing information about wine variants, along with their features and target labels. Your task is to perform hyper-parameter tuning on a Support Vector Classifier (SVC) to find the best combination of parameters for predicting the target class. Objective Implement functions to perform Grid Search and Randomized Search for hyper-parameter tuning using scikit-learn\'s `GridSearchCV` and `RandomizedSearchCV` respectively. Detailed Requirements 1. **Dataset Loading Function** - Implement a function `load_data(filepath: str) -> Tuple[pd.DataFrame, pd.Series]` that: - Reads a CSV file from the provided `filepath`. - Splits the data into features `X` and target `y`. - Returns `X` (features) and `y` (target). 2. **Grid Search Function** - Implement a function `grid_search_svc(X: pd.DataFrame, y: pd.Series) -> Dict[str, Any]` that: - Defines a parameter grid for SVM as: ```python param_grid = [ {\'C\': [1, 10, 100, 1000], \'kernel\': [\'linear\']}, {\'C\': [1, 10, 100, 1000], \'gamma\': [1e-3, 1e-4], \'kernel\': [\'rbf\']} ] ``` - Performs a grid search using `GridSearchCV` with 5-fold cross-validation. - Returns a dictionary containing the best parameters and the best score. 3. **Randomized Search Function** - Implement a function `random_search_svc(X: pd.DataFrame, y: pd.Series, n_iter: int = 50, random_state: int = 42) -> Dict[str, Any]` that: - Defines a parameter distribution for SVM as: ```python param_dist = { \'C\': scipy.stats.expon(scale=100), \'gamma\': scipy.stats.expon(scale=0.1), \'kernel\': [\'rbf\'], \'class_weight\': [\'balanced\', None] } ``` - Performs a randomized search using `RandomizedSearchCV` with 5-fold cross-validation. - Uses `n_iter` iterations for sampling parameter combinations. - Ensures reproducibility by setting `random_state`. - Returns a dictionary containing the best parameters and the best score. Constraints - Use scikit-learn version 0.24 or later. - Ensure the functions can handle large datasets efficiently. # Expected Function Signatures ```python import pandas as pd from typing import Tuple, Dict, Any import scipy.stats def load_data(filepath: str) -> Tuple[pd.DataFrame, pd.Series]: pass def grid_search_svc(X: pd.DataFrame, y: pd.Series) -> Dict[str, Any]: pass def random_search_svc(X: pd.DataFrame, y: pd.Series, n_iter: int = 50, random_state: int = 42) -> Dict[str, Any]: pass ``` Example Usage ```python # Assuming wine.csv is available in the working directory X, y = load_data(\\"wine.csv\\") # Perform Grid Search grid_search_results = grid_search_svc(X, y) print(\\"Grid Search - Best Parameters:\\", grid_search_results[\'best_params\']) print(\\"Grid Search - Best Score:\\", grid_search_results[\'best_score\']) # Perform Randomized Search random_search_results = random_search_svc(X, y) print(\\"Randomized Search - Best Parameters:\\", random_search_results[\'best_params\']) print(\\"Randomized Search - Best Score:\\", random_search_results[\'best_score\']) ``` Considerations - Ensure that your functions handle exceptions and edge cases gracefully. - Document parameter ranges and chosen distributions with rationale. Provide a well-commented and clean code for evaluation.","solution":"import pandas as pd from typing import Tuple, Dict, Any from sklearn.model_selection import GridSearchCV, RandomizedSearchCV from sklearn.svm import SVC import scipy.stats def load_data(filepath: str) -> Tuple[pd.DataFrame, pd.Series]: Loads the wine dataset from a CSV file, splits it into features and target labels. Parameters: - filepath (str): Path to the CSV file. Returns: - Tuple[pd.DataFrame, pd.Series]: Features (X) and target labels (y). data = pd.read_csv(filepath) X = data.drop(columns=[\'target\']) y = data[\'target\'] return X, y def grid_search_svc(X: pd.DataFrame, y: pd.Series) -> Dict[str, Any]: Performs Grid Search for hyper-parameter tuning on SVC. Parameters: - X (pd.DataFrame): Features. - y (pd.Series): Target labels. Returns: - Dict[str, Any]: Dictionary containing best parameters and best score. param_grid = [ {\'C\': [1, 10, 100, 1000], \'kernel\': [\'linear\']}, {\'C\': [1, 10, 100, 1000], \'gamma\': [1e-3, 1e-4], \'kernel\': [\'rbf\']} ] grid_search = GridSearchCV(SVC(), param_grid, cv=5) grid_search.fit(X, y) return { \'best_params\': grid_search.best_params_, \'best_score\': grid_search.best_score_ } def random_search_svc(X: pd.DataFrame, y: pd.Series, n_iter: int = 50, random_state: int = 42) -> Dict[str, Any]: Performs Randomized Search for hyper-parameter tuning on SVC. Parameters: - X (pd.DataFrame): Features. - y (pd.Series): Target labels. - n_iter (int): Number of parameter settings that are sampled. - random_state (int): Random seed for reproducibility. Returns: - Dict[str, Any]: Dictionary containing best parameters and best score. param_dist = { \'C\': scipy.stats.expon(scale=100), \'gamma\': scipy.stats.expon(scale=0.1), \'kernel\': [\'rbf\'], \'class_weight\': [\'balanced\', None] } random_search = RandomizedSearchCV(SVC(), param_distributions=param_dist, n_iter=n_iter, cv=5, random_state=random_state) random_search.fit(X, y) return { \'best_params\': random_search.best_params_, \'best_score\': random_search.best_score_ }"},{"question":"Objective Implement a Python function that demonstrates the use of comprehensions, generators, and operator precedence. This function will also involve arithmetic operations covering multiple numeric types. Problem Statement You are required to develop a function `process_numbers(data)` which accepts a list of integers and performs the following tasks: 1. **Filtering and Transformation**: - Filter out even numbers from the input list. - For the remaining odd numbers, compute the square of each number. 2. **Generation**: - Create a generator that yields these squared values one by one. 3. **Summation with Mixed Arithmetic**: - Calculate the sum of the first `n` values yielded by the generator, where `n` is the length of the input list divided by a factor determined by operator precedence (explained below). 4. **Return the Sum**: - Return the calculated sum. **Operator Precedence Factor**: - Use the expression `n_factor = (len(data) // 2 ** 1) + 3 - 1` to determine the divisor of the length of the input list. This will test the understanding of the evaluation order of operations. **Function Signature**: ```python def process_numbers(data: list) -> int: pass ``` Input - `data`: A list of integers, where `1 <= len(data) <= 10^5` and `-10^9 <= data[i] <= 10^9`. Output - An integer representing the calculated sum of the first `n` values yielded by the generator. Constraints - Ensure your implementation efficiently handles large input sizes up to the boundary conditions. - Use list comprehensions, generator expressions, and proper handling of operator precedence as discussed. Example ```python >>> process_numbers([1, 2, 3, 4, 5, 6, 7, 8, 9, 10]) 140 # Explanation: After filtering and squaring: [1, 9, 25, 49, 81] # Generator yields [1, 9, 25, 49, 81] # n_factor = (10 // 2 ** 1) + 3 - 1 = 5 + 3 - 1 = 7 # Sum of first 5 values (as 5 is less than 7) yielded by the generator: 1 + 9 + 25 + 49 + 81 = 165 >>> process_numbers([-1, -2, -3, -4, -5]) 35 # Explanation: After filtering and squaring: [1, 9, 25] # Generator yields: [1, 9, 25] # n_factor = (5 // 2 ** 1) + 3 - 1 = 2 + 3 - 1 = 4 # Sum of first 3 values (as 3 is less than 4) yielded by the generator: 1 + 9 + 25 = 35 ``` # Note - Ensure you follow the order of operations strictly as specified to derive the correct `n_factor`.","solution":"def process_numbers(data): Process the list of integers according to the specified logic: - Filter out even numbers - Compute the square of the remaining odd numbers - Create a generator yielding these squared values - Sum the first n of these values where n is determined by operator precedence factor Parameters: data (list): List of integers Returns: int: The calculated sum # Step 1: Filter out even numbers and compute the square of odd numbers squared_odds = [x ** 2 for x in data if x % 2 != 0] # Step 2: Create a generator that yields these squared values one by one squared_generator = (x for x in squared_odds) # Step 3: Calculate n_factor using the operator precedence expression n_factor = (len(data) // 2 ** 1) + 3 - 1 # Step 4: Sum the first n values yielded by the generator, where n is the lesser of len(squared_odds) and n_factor result_sum = 0 for i, value in enumerate(squared_generator): if i >= n_factor: break result_sum += value return result_sum"},{"question":"Objective The goal of this assignment is to implement a customized neural network parameter initialization strategy using PyTorch\'s `torch.nn.init` module. Problem Statement You are provided with a neural network architecture in the form of a class `MyNeuralNetwork`, which extends `nn.Module` in PyTorch. Your task is to implement a function `initialize_parameters()` within this class to initialize the parameters of the network according to the following strategy: 1. For linear (fully-connected) layers: - If the layer is followed by a ReLU activation function, initialize its weights using Kaiming Normal Initialization (`torch.nn.init.kaiming_normal_`) and biases with zeros. - If the layer is not followed by a ReLU activation function, initialize its weights using Xavier Uniform Initialization (`torch.nn.init.xavier_uniform_`) and biases with zeros. 2. For convolutional layers: - Use Kaiming Uniform Initialization (`torch.nn.init.kaiming_uniform_`) for weights and zeros for biases. Input Format - None (the function should be implemented as a method within the specified class). Output Format - None (the function should modify the parameters of the model in-place). Constraints - Assume the network may contain an arbitrary combination of linear and convolutional layers. - Activation functions are already specified in the model. Example Given the following definition of `MyNeuralNetwork`: ```python import torch import torch.nn as nn class MyNeuralNetwork(nn.Module): def __init__(self): super(MyNeuralNetwork, self).__init__() self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1) self.fc1 = nn.Linear(in_features=32*28*28, out_features=128) self.relu = nn.ReLU() self.fc2 = nn.Linear(in_features=128, out_features=10) def forward(self, x): x = self.conv1(x) x = self.relu(x) x = x.view(x.size(0), -1) x = self.fc1(x) x = self.relu(x) x = self.fc2(x) return x def initialize_parameters(self): # Your implementation goes here pass ``` Your task is to implement the `initialize_parameters` method such that it initializes the parameters as described. Solution Template A solution implementation might look like: ```python import torch import torch.nn as nn import torch.nn.init as init class MyNeuralNetwork(nn.Module): def __init__(self): super(MyNeuralNetwork, self).__init__() self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1) self.fc1 = nn.Linear(in_features=32*28*28, out_features=128) self.relu = nn.ReLU() self.fc2 = nn.Linear(in_features=128, out_features=10) def forward(self, x): x = self.conv1(x) x = self.relu(x) x = x.view(x.size(0), -1) x = self.fc1(x) x = self.relu(x) x = self.fc2(x) return x def initialize_parameters(self): for m in self.modules(): if isinstance(m, nn.Conv2d): init.kaiming_uniform_(m.weight, nonlinearity=\'relu\') if m.bias is not None: init.zeros_(m.bias) elif isinstance(m, nn.Linear): if hasattr(m, \'relu\'): init.kaiming_normal_(m.weight, nonlinearity=\'relu\') else: init.xavier_uniform_(m.weight) if m.bias is not None: init.zeros_(m.bias) ``` Note: - The students should validate their implementation by running the `initialize_parameters()` method and checking if the parameters are initialized correctly.","solution":"import torch import torch.nn as nn import torch.nn.init as init class MyNeuralNetwork(nn.Module): def __init__(self): super(MyNeuralNetwork, self).__init__() self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1) self.fc1 = nn.Linear(in_features=32*28*28, out_features=128) self.relu = nn.ReLU() self.fc2 = nn.Linear(in_features=128, out_features=10) def forward(self, x): x = self.conv1(x) x = self.relu(x) x = x.view(x.size(0), -1) x = self.fc1(x) x = self.relu(x) x = self.fc2(x) return x def initialize_parameters(self): Initialize the parameters following the specified strategy for m in self.modules(): if isinstance(m, nn.Conv2d): init.kaiming_uniform_(m.weight, nonlinearity=\'relu\') if m.bias is not None: init.zeros_(m.bias) elif isinstance(m, nn.Linear): # Check if the following layer is ReLU if self._is_followed_by_relu(m): init.kaiming_normal_(m.weight, nonlinearity=\'relu\') else: init.xavier_uniform_(m.weight) if m.bias is not None: init.zeros_(m.bias) def _is_followed_by_relu(self, layer): Returns True if the layer is followed by a ReLU activation function found_layer = False for m in self.children(): if found_layer: if isinstance(m, nn.ReLU): return True break if m == layer: found_layer = True return False"},{"question":"# PyTorch JIT Optimization Task Objective You are required to write a PyTorch module that performs a simple neural network operation and then optimize this operation using PyTorch\'s JIT compilation tools. Description 1. **Define a Simple Neural Network:** - Implement a simple feedforward neural network that includes an input layer, one hidden layer, and an output layer. - Use ReLU activation for the hidden layer and a softmax activation for the output layer. 2. **JIT Compilation:** - Use PyTorch\'s JIT tools to trace or script the defined neural network for optimization. - Compare the performance of the network with and without JIT compilation over multiple forward passes. Requirements: 1. **Neural Network Specification:** - The network should take a tensor of shape `(batch_size, input_size)` as input. - The input size is 10, hidden layer size is 5, and the output size is 2. 2. **Function Definitions:** - `define_model()`: Return an instance of your defined neural network module. - `jit_compile_model(model)`: Return a JIT-traced/scripted version of the given model. - `compare_performance(model, jit_model, num_iterations)`: Execute a forward pass of random input tensors through both the non-JIT and JIT model and log their execution time for `num_iterations`. Input: - No input is required from the user. Output: - Print the average execution time of running `num_iterations` forward passes through both the non-JIT and JIT models. Constraints: - Assume the `num_iterations` will be a positive integer, typically large enough (e.g., 1000) to reflect time differences. - Ensure the comparison is fair by using identical random inputs for both the non-JIT and JIT model during each iteration. Performance Requirements: - The JIT-model should demonstrate improved performance over the original model with a noticeable reduction in execution time. # Example Code Structure: ```python import torch import torch.nn as nn import torch.nn.functional as F import torch.utils.jit as jit import time # Define your neural network model class SimpleNN(nn.Module): def __init__(self, input_size=10, hidden_size=5, output_size=2): super(SimpleNN, self).__init__() self.fc1 = nn.Linear(input_size, hidden_size) self.fc2 = nn.Linear(hidden_size, output_size) def forward(self, x): x = F.relu(self.fc1(x)) x = F.softmax(self.fc2(x), dim=1) return x def define_model(): # Define the model return SimpleNN() def jit_compile_model(model): # JIT compile the given model using either tracing or scripting # For this example, let\'s use tracing example_input = torch.randn(1, 10) # Dummy input tensor to trace the model jit_model = torch.jit.trace(model, example_input) return jit_model def compare_performance(model, jit_model, num_iterations=1000): input_data = torch.randn(num_iterations, 10) # Create random input data start_time = time.time() for i in range(num_iterations): model(input_data[i:i+1]) non_jit_time = time.time() - start_time start_time = time.time() for i in range(num_iterations): jit_model(input_data[i:i+1]) jit_time = time.time() - start_time print(f\\"Average execution time without JIT: {non_jit_time/num_iterations}\\") print(f\\"Average execution time with JIT: {jit_time/num_iterations}\\") if __name__ == \\"__main__\\": model = define_model() jit_model = jit_compile_model(model) compare_performance(model, jit_model) ```","solution":"import torch import torch.nn as nn import torch.nn.functional as F import time class SimpleNN(nn.Module): def __init__(self, input_size=10, hidden_size=5, output_size=2): super(SimpleNN, self).__init__() self.fc1 = nn.Linear(input_size, hidden_size) self.fc2 = nn.Linear(hidden_size, output_size) def forward(self, x): x = F.relu(self.fc1(x)) x = F.softmax(self.fc2(x), dim=1) return x def define_model(): # Define the model return SimpleNN() def jit_compile_model(model): # JIT compile the given model using tracing example_input = torch.randn(1, 10) # Dummy input tensor to trace the model jit_model = torch.jit.trace(model, example_input) return jit_model def compare_performance(model, jit_model, num_iterations=1000): input_data = torch.randn(num_iterations, 10) # Create random input data start_time = time.time() for i in range(num_iterations): model(input_data[i:i+1]) non_jit_time = time.time() - start_time start_time = time.time() for i in range(num_iterations): jit_model(input_data[i:i+1]) jit_time = time.time() - start_time print(f\\"Average execution time without JIT: {non_jit_time/num_iterations}\\") print(f\\"Average execution time with JIT: {jit_time/num_iterations}\\") if __name__ == \\"__main__\\": model = define_model() jit_model = jit_compile_model(model) compare_performance(model, jit_model)"},{"question":"**Objective:** To assess students\' understanding of file and directory operations, path manipulations, and environment variable management using the Python \\"os\\" module. Question: You are required to create a Python script that performs the following tasks: 1. **Create a Directory Structure:** Create a set of nested directories based on the paths given as input strings. Consider the given paths: - `\\"path/to/first_directory\\"` - `\\"path/to/second_directory\\"` - `\\"path/to/third_directory\\"` 2. **Create Files in Directories:** For each directory, create a file named `\\"info.txt\\"` that contains basic information about the directory (e.g., its absolute path, creation time). 3. **Environment Variable Handling:** Set an environment variable named `\\"DATA_ROOT\\"` to the path of the parent directory (`\\"path/to\\"`). 4. **Reading and Writing Files:** Write a summary file in the root of `\\"path\\"` directory named `\\"summary.txt\\"`, which contains the paths of all `\\"info.txt\\"` files created within the nested directories. Ensure that the paths are written in a readable format. 5. **Symbolic Links:** Create a symbolic link named `\\"link_to_first_info\\"` inside the `\\"path\\"` directory that points to the `\\"info.txt\\"` file inside the `\\"first_directory\\"`. 6. **Exception Handling:** Ensure proper exception handling for scenarios where directories or files cannot be created or written to, and output appropriate error messages. Constraints: - You must use the functions from the `os` module to complete these tasks. - The paths provided are guaranteed to be valid strings but they may not exist initially. - Ensure that all files and directories created or manipulated in your script are cleaned up (deleted) after the script execution. Expected Output: Your script should output a message indicating the successful completion of each step. If any step fails, it should output an appropriate error message. Sample Input: ```python paths = [ \\"path/to/first_directory\\", \\"path/to/second_directory\\", \\"path/to/third_directory\\" ] ``` Sample Output: ```plaintext Created directory: path/to/first_directory Created directory: path/to/second_directory Created directory: path/to/third_directory Created info.txt in: path/to/first_directory Created info.txt in: path/to/second_directory Created info.txt in: path/to/third_directory Set environment variable DATA_ROOT to path/to Created summary.txt in path/ Created symbolic link link_to_first_info in path/ Script executed successfully, cleaning up created files and directories... ``` Implement your solution in a function named `manage_directories_and_files(paths: List[str])`.","solution":"import os import time def manage_directories_and_files(paths): try: # 1. Create directories for directory in paths: os.makedirs(directory, exist_ok=True) print(f\\"Created directory: {directory}\\") # 2. Create info.txt in each directory info_files = [] for directory in paths: info_path = os.path.join(directory, \\"info.txt\\") with open(info_path, \\"w\\") as f: abs_path = os.path.abspath(directory) creation_time = time.ctime() f.write(f\\"Directory path: {abs_path}n\\") f.write(f\\"Creation time: {creation_time}n\\") info_files.append(info_path) print(f\\"Created info.txt in: {directory}\\") # 3. Set environment variable data_root = os.path.commonpath(paths) os.environ[\\"DATA_ROOT\\"] = data_root print(f\\"Set environment variable DATA_ROOT to {data_root}\\") # 4. Write summary.txt summary_path = os.path.join(data_root, \\"summary.txt\\") with open(summary_path, \\"w\\") as f: f.write(\\"Info.txt paths:n\\") for info_file in info_files: f.write(f\\"{info_file}n\\") print(f\\"Created summary.txt in {data_root}\\") # 5. Create symbolic link first_info_path = info_files[0] link_path = os.path.join(data_root, \\"link_to_first_info\\") os.symlink(first_info_path, link_path) print(f\\"Created symbolic link link_to_first_info in {data_root}\\") # Output completion message print(\\"Script executed successfully, cleaning up created files and directories...\\") # Clean up created files and directories for info_file in info_files: os.remove(info_file) os.remove(summary_path) os.remove(link_path) for directory in reversed(paths): os.rmdir(directory) print(\\"Clean up completed.\\") except Exception as e: print(f\\"An error occurred: {e}\\")"},{"question":"**Question: Applying Clustering and Dimensionality Reduction Techniques using scikit-learn** You are provided with a dataset containing features of various wines, with no labels indicating the types of wines. Your task is to analyze this dataset using scikit-learn\'s unsupervised learning techniques. Specifically, you should perform the following steps: 1. **Standardize the data** to ensure each feature has a mean of 0 and a standard deviation of 1. 2. **Apply Principal Component Analysis (PCA)** to the standardized data to reduce its dimensionality. Retain only the number of components that explain at least 90% of the variance in the data. 3. **Cluster the data using K-Means** into an appropriate number of clusters. Determine the number of clusters k by leveraging the Elbow method or Silhouette score. 4. Visualize the results of your clustering on the 2-dimensional PCA-reduced data space. 5. **Interpret the clustering results**: Attempt to explain the clusters in terms of the original features of the wine dataset. **Input:** - A CSV file named \\"wine_data.csv\\" containing the wine dataset with numerical features only (without any labels/target). **Output:** - A plot showing the clusters in the 2D PCA-reduced space. - A brief report (around 100-150 words) summarizing the clusters identified and what insight they provide about the wine dataset based on the original features. **Constraints:** - You must use scikit-learn for all PCA and clustering tasks. - Each step must handle potential edge cases, such as missing data or numerical instability. **Performance Requirements:** - The operation should be efficient for moderately large datasets (up to 10,000 samples and 100 features). ```python import pandas as pd from sklearn.preprocessing import StandardScaler from sklearn.decomposition import PCA from sklearn.cluster import KMeans from sklearn.metrics import silhouette_score import matplotlib.pyplot as plt def perform_wine_analysis(file_path): # Load the dataset data = pd.read_csv(file_path) # Standardize the data scaler = StandardScaler() standardized_data = scaler.fit_transform(data) # Apply PCA pca = PCA() pca_data = pca.fit_transform(standardized_data) cumulative_variance = pca.explained_variance_ratio_.cumsum() num_components = (cumulative_variance < 0.90).sum() + 1 pca = PCA(n_components=num_components) reduced_data = pca.fit_transform(standardized_data) # Determine the best number of clusters using Silhouette score range_n_clusters = list(range(2, 11)) best_n_clusters = 2 best_score = -1 for n_clusters in range_n_clusters: clusterer = KMeans(n_clusters=n_clusters) preds = clusterer.fit_predict(reduced_data) score = silhouette_score(reduced_data, preds) if score > best_score: best_n_clusters = n_clusters best_score = score # Apply KMeans with the best number of clusters kmeans = KMeans(n_clusters=best_n_clusters) clusters = kmeans.fit_predict(reduced_data) # Visualize the clusters plt.scatter(reduced_data[:, 0], reduced_data[:, 1], c=clusters, cmap=\'viridis\') plt.xlabel(\'PCA Component 1\') plt.ylabel(\'PCA Component 2\') plt.title(\'Clustering of Wine Data in 2D PCA Space\') plt.show() # Interpret the clusters cluster_centers = scaler.inverse_transform(pca.inverse_transform(kmeans.cluster_centers_)) report = f\\"The dataset was clustered into {best_n_clusters} groups. Analysis of the cluster centroids in the original feature space shows that ...\\" for i, center in enumerate(cluster_centers): report += f\\"nCluster {i+1}: \\" + \\", \\".join([f\\"{data.columns[j]}={center[j]:.2f}\\" for j in range(len(center))]) return report # Example usage file_path = \'wine_data.csv\' print(perform_wine_analysis(file_path)) ``` **Note:** This is a starting point for the analysis. In practice, you might need to refine the steps, handle additional preprocessing, or improve visualization as needed.","solution":"import pandas as pd from sklearn.preprocessing import StandardScaler from sklearn.decomposition import PCA from sklearn.cluster import KMeans from sklearn.metrics import silhouette_score import matplotlib.pyplot as plt def perform_wine_analysis(file_path): # Load the dataset data = pd.read_csv(file_path) # Handle missing data data.fillna(data.mean(), inplace=True) # Standardize the data scaler = StandardScaler() standardized_data = scaler.fit_transform(data) # Apply PCA pca = PCA() pca_data = pca.fit_transform(standardized_data) cumulative_variance = pca.explained_variance_ratio_.cumsum() num_components = (cumulative_variance < 0.90).sum() + 1 pca = PCA(n_components=num_components) reduced_data = pca.fit_transform(standardized_data) # Determine the best number of clusters using Silhouette score range_n_clusters = list(range(2, 11)) best_n_clusters = 2 best_score = -1 for n_clusters in range_n_clusters: clusterer = KMeans(n_clusters=n_clusters) preds = clusterer.fit_predict(reduced_data) score = silhouette_score(reduced_data, preds) if score > best_score: best_n_clusters = n_clusters best_score = score # Apply KMeans with the best number of clusters kmeans = KMeans(n_clusters=best_n_clusters) clusters = kmeans.fit_predict(reduced_data) # Visualize the clusters plt.scatter(reduced_data[:, 0], reduced_data[:, 1], c=clusters, cmap=\'viridis\') plt.xlabel(\'PCA Component 1\') plt.ylabel(\'PCA Component 2\') plt.title(\'Clustering of Wine Data in 2D PCA Space\') plt.show() # Interpret the clusters cluster_centers = scaler.inverse_transform(pca.inverse_transform(kmeans.cluster_centers_)) report = f\\"The dataset was clustered into {best_n_clusters} groups. Analysis of the cluster centroids in the original feature space shows that ...\\" for i, center in enumerate(cluster_centers): report += f\\"nCluster {i+1}: \\" + \\", \\".join([f\\"{data.columns[j]}={center[j]:.2f}\\" for j in range(len(center))]) return report"},{"question":"Customizing Seaborn Plot Contexts **Objective**: Demonstrate your understanding of how to customize plot contexts using seaborn. **Problem**: You are provided with a dataset that contains daily temperatures and humidity levels for a month. Your task is to create visualizations to effectively communicate this information using seaborn. Follow these steps: 1. **Load Dataset**: - Create a pandas dataframe `df` with columns `day`, `temperature`, and `humidity`. - The dataframe has 30 rows, corresponding to 30 days. 2. **Setting Context and Visualization**: - Create a line plot for temperature and humidity levels across the days of the month. - Initially, set the context to `paper` using `sns.set_context`, and generate the plot. - Then, update the context to `notebook` and scale the font by 1.5, and generate the plot. - Finally, set the context to `talk` and override the `lines.linewidth` parameter to 2, and generate the plot. **Specifications**: - Use the following contexts sequentially: `paper`, `notebook` with `font_scale`, and `talk` with `rc`. - Ensure that each plot is displayed with appropriate titles and axis labels. - Include legends to differentiate between temperature and humidity lines. **Input**: - No external inputs are required. Use the predefined dataset within your code. **Output**: - Three plots, each customized according to the specified contexts and requirements. **Constraints**: - The dataset should be generated within the code. - Use seaborn for plotting. **Example Dataset**: ```python import pandas as pd import numpy as np # Example dataset generation np.random.seed(0) days = np.arange(1, 31) temperature = np.random.uniform(20, 35, size=30) humidity = np.random.uniform(30, 90, size=30) df = pd.DataFrame({ \'day\': days, \'temperature\': temperature, \'humidity\': humidity }) ``` **Expected Implementations**: ```python import seaborn as sns import matplotlib.pyplot as plt # 1. Dataset generation import pandas as pd import numpy as np # Example dataset generation np.random.seed(0) days = np.arange(1, 31) temperature = np.random.uniform(20, 35, size=30) humidity = np.random.uniform(30, 90, size=30) df = pd.DataFrame({ \'day\': days, \'temperature\': temperature, \'humidity\': humidity }) # 2. Set context to \'paper\' and plot sns.set_context(\\"paper\\") plt.figure(figsize=(10, 6)) sns.lineplot(data=df, x=\'day\', y=\'temperature\', label=\'Temperature\') sns.lineplot(data=df, x=\'day\', y=\'humidity\', label=\'Humidity\') plt.title(\\"Daily Temperature and Humidity - Paper Context\\") plt.xlabel(\\"Day\\") plt.ylabel(\\"Values\\") plt.legend() plt.show() # 3. Set context to \'notebook\' with font scale and plot sns.set_context(\\"notebook\\", font_scale=1.5) plt.figure(figsize=(10, 6)) sns.lineplot(data=df, x=\'day\', y=\'temperature\', label=\'Temperature\') sns.lineplot(data=df, x=\'day\', y=\'humidity\', label=\'Humidity\') plt.title(\\"Daily Temperature and Humidity - Notebook Context\\") plt.xlabel(\\"Day\\") plt.ylabel(\\"Values\\") plt.legend() plt.show() # 4. Set context to \'talk\' and override \'lines.linewidth\', then plot sns.set_context(\\"talk\\", rc={\\"lines.linewidth\\": 2}) plt.figure(figsize=(10, 6)) sns.lineplot(data=df, x=\'day\', y=\'temperature\', label=\'Temperature\') sns.lineplot(data=df, x=\'day\', y=\'humidity\', label=\'Humidity\') plt.title(\\"Daily Temperature and Humidity - Talk Context\\") plt.xlabel(\\"Day\\") plt.ylabel(\\"Values\\") plt.legend() plt.show() ``` **Submissions** should include code comments explaining each step.","solution":"import seaborn as sns import matplotlib.pyplot as plt import pandas as pd import numpy as np def generate_and_plot_data(): # Generate dataset np.random.seed(0) days = np.arange(1, 31) temperature = np.random.uniform(20, 35, size=30) humidity = np.random.uniform(30, 90, size=30) df = pd.DataFrame({ \'day\': days, \'temperature\': temperature, \'humidity\': humidity }) # Set context to \'paper\' and plot sns.set_context(\\"paper\\") plt.figure(figsize=(10, 6)) sns.lineplot(data=df, x=\'day\', y=\'temperature\', label=\'Temperature\') sns.lineplot(data=df, x=\'day\', y=\'humidity\', label=\'Humidity\') plt.title(\\"Daily Temperature and Humidity - Paper Context\\") plt.xlabel(\\"Day\\") plt.ylabel(\\"Values\\") plt.legend() plt.show() # Set context to \'notebook\' with font scale and plot sns.set_context(\\"notebook\\", font_scale=1.5) plt.figure(figsize=(10, 6)) sns.lineplot(data=df, x=\'day\', y=\'temperature\', label=\'Temperature\') sns.lineplot(data=df, x=\'day\', y=\'humidity\', label=\'Humidity\') plt.title(\\"Daily Temperature and Humidity - Notebook Context\\") plt.xlabel(\\"Day\\") plt.ylabel(\\"Values\\") plt.legend() plt.show() # Set context to \'talk\' and override \'lines.linewidth\', then plot sns.set_context(\\"talk\\", rc={\\"lines.linewidth\\": 2}) plt.figure(figsize=(10, 6)) sns.lineplot(data=df, x=\'day\', y=\'temperature\', label=\'Temperature\') sns.lineplot(data=df, x=\'day\', y=\'humidity\', label=\'Humidity\') plt.title(\\"Daily Temperature and Humidity - Talk Context\\") plt.xlabel(\\"Day\\") plt.ylabel(\\"Values\\") plt.legend() plt.show()"},{"question":"Process Sales Items Data You are given an XML document containing information about various sales items. Each `<item>` element in the XML contains details such as `id`, `price`, `quantity`, and `category`. Your task is to write a function that parses this XML document, processes each `<item>` element, and returns a list of dictionaries containing information of items that meet the following criteria: 1. The item\'s price should be greater than a specified threshold. 2. The item\'s category matches a specified category. Your function should be implemented as follows: ```python from xml.dom import pulldom def filter_sales_items(xml_string, price_threshold, category): Parses the given XML string and filters items based on price and category. Args: xml_string (str): An XML string representing the sales items. price_threshold (float): The minimum price for the items to be included in the output. category (str): The category of the items to be included in the output. Returns: List[Dict[str, Union[str, float, int]]]: A list of dictionaries, each containing details of an item that meets the criteria. Each dictionary has keys: \'id\', \'price\', \'quantity\', and \'category\'. # Implementation here ``` # Input - `xml_string`: A string representing the entire XML document. - `price_threshold`: A floating point value representing the minimum price. - `category`: A string representing the category to filter by. # Output - A list of dictionaries, where each dictionary represents an item that meets the criteria. Each dictionary should have the following keys: - `id` (str): The ID of the item. - `price` (float): The price of the item. - `quantity` (int): The quantity of the item. - `category` (str): The category of the item. # Constraints - Assume the XML document is well-formed. - The attributes `id`, `price`, `quantity`, and `category` are always present in `<item>` elements. # Example: ```python xml_string = <items> <item id=\\"001\\" price=\\"150.0\\" quantity=\\"10\\" category=\\"Electronics\\"/> <item id=\\"002\\" price=\\"50.0\\" quantity=\\"5\\" category=\\"Clothing\\"/> <item id=\\"003\\" price=\\"200.0\\" quantity=\\"3\\" category=\\"Electronics\\"/> <item id=\\"004\\" price=\\"300.0\\" quantity=\\"2\\" category=\\"Furniture\\"/> </items> price_threshold = 100.0 category = \\"Electronics\\" result = filter_sales_items(xml_string, price_threshold, category) print(result) # Expected output: [{\'id\': \'001\', \'price\': 150.0, \'quantity\': 10, \'category\': \'Electronics\'}, # {\'id\': \'003\', \'price\': 200.0, \'quantity\': 3, \'category\': \'Electronics\'}] ``` # Notes 1. Use the `xml.dom.pulldom` module to parse the XML document and iterate over events. 2. Only expand nodes when necessary to access their details. 3. Ensure the function is efficient and handles large XML inputs gracefully.","solution":"from xml.dom import pulldom def filter_sales_items(xml_string, price_threshold, category): Parses the given XML string and filters items based on price and category. Args: xml_string (str): An XML string representing the sales items. price_threshold (float): The minimum price for the items to be included in the output. category (str): The category of the items to be included in the output. Returns: List[Dict[str, Union[str, float, int]]]: A list of dictionaries, each containing details of an item that meets the criteria. Each dictionary has keys: \'id\', \'price\', \'quantity\', and \'category\'. events = pulldom.parseString(xml_string) filtered_items = [] for event, node in events: if event == pulldom.START_ELEMENT and node.tagName == \\"item\\": events.expandNode(node) item_id = node.getAttribute(\\"id\\") price = float(node.getAttribute(\\"price\\")) quantity = int(node.getAttribute(\\"quantity\\")) item_category = node.getAttribute(\\"category\\") if price > price_threshold and item_category == category: filtered_items.append({ \\"id\\": item_id, \\"price\\": price, \\"quantity\\": quantity, \\"category\\": item_category, }) return filtered_items"},{"question":"Advanced Bar Plot Customization with Seaborn **Objective:** To demonstrate your understanding of seaborn\'s advanced plot customization techniques, create a function that generates a bar plot based on the given dataset and customization requirements. **Problem Statement:** You are provided with the Titanic dataset, which contains information about the passengers of the Titanic. Your task is to create a function `customize_titanic_plot` that generates a bar plot using seaborn.objects with the following specifications: 1. **Dataset:** The Titanic dataset loaded using `seaborn.load_dataset(\'titanic\')`. 2. **Plot:** Create a bar plot that shows the count of passengers grouped by `class`, further divided by `sex`. 3. **Transformations:** - Use the `Dodge` transformation to separate the bars for male and female in each class. - Fill any empty spaces due to missing combinations (e.g., if a certain class has no passengers of a specific sex). 4. **Customization:** - Add a slight gap between the dodged bars for improved readability. - Use different colors for male and female bars for better visualization. **Function Signature:** ```python def customize_titanic_plot(): pass ``` **Output:** - The function should display a seaborn plot as described above when executed. **Example:** When the function is executed, it should generate a plot similar to the one below: ```python import seaborn as sns import seaborn.objects as so def customize_titanic_plot(): # Load the Titanic dataset titanic = sns.load_dataset(\'titanic\') # Initialize the plot p = so.Plot(titanic, \\"class\\", color=\\"sex\\") # Add a bar with Dodge transformation, fill empty spaces, and add a gap p.add(so.Bar(), so.Count(), so.Dodge(empty=\\"fill\\", gap=0.1)) # Display the plot p.show() # Execute the function to display the plot customize_titanic_plot() ``` **Notes:** - Ensure that the plot uses the seaborn.objects module for its creation. - The plot should correctly reflect the count of passengers grouped by `class` and `sex`, with appropriate transformations and customizations applied.","solution":"import seaborn as sns import seaborn.objects as so import matplotlib.pyplot as plt def customize_titanic_plot(): Generates and displays a customized bar plot of the Titanic dataset. The plot shows the count of passengers grouped by class and divided by sex. Applies necessary transformations and customizations for improved readability. # Load the Titanic dataset titanic = sns.load_dataset(\'titanic\') # Initialize the plot p = so.Plot(titanic, \\"class\\", color=\\"sex\\") # Add a bar with Dodge transformation, fill empty spaces, and add a gap p.add(so.Bar(), so.Count(), so.Dodge(empty=\\"fill\\", gap=0.1)) # Display the plot p.show() # Execute the function to display the plot customize_titanic_plot()"},{"question":"# PyTorch Environment Variable Manipulation and Understanding Objective: To demonstrate your understanding of PyTorch environment variables and their impact on PyTorch operations. Task: Write a Python module that performs the following: 1. Sets and retrieves environment variables to manage PyTorch behavior. 2. Executes a simple PyTorch computation to observe the effects of these environment variables. Description: 1. **Set Environment Variables:** - Implement a function `set_torch_env_vars()` that sets the following PyTorch environment variables: - `MKL_NUM_THREADS` to 2 - `OMP_NUM_THREADS` to 2 - `TORCH_SHOW_CPP_STACKTRACES` to 1 2. **Get Environment Variables:** - Implement a function `get_torch_env_vars()` that retrieves and returns the values of the following PyTorch environment variables: - `MKL_NUM_THREADS` - `OMP_NUM_THREADS` - `TORCH_SHOW_CPP_STACKTRACES` 3. **PyTorch Computation:** - Implement a function `matrix_multiplication_performance()` that performs the following: - Creates two random matrices of size 1000x1000 using `torch.randn`. - Measures the time taken to multiply these matrices using CPU with `torch.matmul`. - Returns the elapsed time. Input and Output: - The function `set_torch_env_vars()` takes no input and sets the environment variables. - The function `get_torch_env_vars()` takes no input and returns a dictionary with the values of the specified environment variables. - The function `matrix_multiplication_performance()` takes no input and returns the elapsed time in seconds for the matrix multiplication operation. Constraints: - You may assume that the necessary imports have been made. - Ensure that the environment variables are correctly set and retrieved using Python\'s `os.environ` interface. - Use appropriate PyTorch functions to generate and multiply the matrices. - Measure the elapsed time using the `time` module. Example: ```python import torch def set_torch_env_vars(): import os os.environ[\'MKL_NUM_THREADS\'] = \'2\' os.environ[\'OMP_NUM_THREADS\'] = \'2\' os.environ[\'TORCH_SHOW_CPP_STACKTRACES\'] = \'1\' def get_torch_env_vars(): import os return { \'MKL_NUM_THREADS\': os.environ.get(\'MKL_NUM_THREADS\'), \'OMP_NUM_THREADS\': os.environ.get(\'OMP_NUM_THREADS\'), \'TORCH_SHOW_CPP_STACKTRACES\': os.environ.get(\'TORCH_SHOW_CPP_STACKTRACES\') } def matrix_multiplication_performance(): import time A = torch.randn(1000, 1000) B = torch.randn(1000, 1000) start_time = time.time() C = torch.matmul(A, B) end_time = time.time() return end_time - start_time # Example usage: set_torch_env_vars() env_vars = get_torch_env_vars() time_taken = matrix_multiplication_performance() print(\\"Environment Variables:\\", env_vars) print(\\"Matrix Multiplication Time:\\", time_taken) ``` Note: The actual values of `MKL_NUM_THREADS`, `OMP_NUM_THREADS`, and `TORCH_SHOW_CPP_STACKTRACES` should be visible in the printed output when running this script in your environment.","solution":"import os import time import torch def set_torch_env_vars(): Sets the PyTorch environment variables. os.environ[\'MKL_NUM_THREADS\'] = \'2\' os.environ[\'OMP_NUM_THREADS\'] = \'2\' os.environ[\'TORCH_SHOW_CPP_STACKTRACES\'] = \'1\' def get_torch_env_vars(): Retrieves the values of specified PyTorch environment variables. Returns: dict: A dictionary containing the values of the specified environment variables. return { \'MKL_NUM_THREADS\': os.environ.get(\'MKL_NUM_THREADS\'), \'OMP_NUM_THREADS\': os.environ.get(\'OMP_NUM_THREADS\'), \'TORCH_SHOW_CPP_STACKTRACES\': os.environ.get(\'TORCH_SHOW_CPP_STACKTRACES\') } def matrix_multiplication_performance(): Performs matrix multiplication and measures the time taken. Returns: float: The time taken in seconds for matrix multiplication. A = torch.randn(1000, 1000) B = torch.randn(1000, 1000) start_time = time.time() C = torch.matmul(A, B) end_time = time.time() return end_time - start_time"},{"question":"**Question:** # Color Palettes and Data Visualization with Seaborn You are tasked with creating a visualization using Seaborn that effectively uses color to distinguish between different categories and numerical values within a dataset. The goal is to demonstrate your understanding of Seaborn\'s color palette functionality and principles of effective data visualization. # Task 1. **Load a Dataset**: - Use the `penguins` dataset available in Seaborn\'s sample datasets. 2. **Create a Scatter Plot**: - Plot a scatter plot of the `bill_length_mm` (x-axis) against `bill_depth_mm` (y-axis). - Use different hues to distinguish between different species (`species` column). 3. **Customize the Color Palette**: - Use the `color_palette` function to customize the hue-based color palette to make the species clearly distinguishable. - Use a qualitative palette for the hue. 4. **Create a Bivariate Histogram**: - Create another plot that uses a bivariate histogram to show the distribution of `bill_length_mm` and `bill_depth_mm`. - Use a sequential color palette to represent the numeric count data within the bins. # Requirements - The function should be named `create_penguin_plots`. - The function should not take any parameters and should directly load the `penguins` dataset. - Use the specified color palette functions for customization. - Display both plots using `matplotlib.pyplot`. # Expected Output The function should display: 1. A scatter plot with clearly distinguishable hues for each species. 2. A bivariate histogram that uses a sequential palette to represent count data. # Constraints - Use Seaborn\'s built-in sample datasets and palette functions. - Ensure that the colors used in the scatter plot make it easy to distinguish between different species. - Ensure that the bins in the bivariate histogram use a luminance-based sequential palette to represent count differences clearly. # Example Code Structure ```python import seaborn as sns import matplotlib.pyplot as plt def create_penguin_plots(): # Load the dataset penguins = sns.load_dataset(\\"penguins\\") # Create a scatter plot for species categorization plt.figure(figsize=(10, 6)) sns.scatterplot(data=penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", hue=\\"species\\", palette=sns.color_palette(\\"husl\\", 3)) plt.title(\\"Scatter Plot of Bill Length vs Bill Depth\\") plt.show() # Create a bivariate histogram for numerical representation plt.figure(figsize=(10, 6)) sns.histplot(data=penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", binwidth=(3, .75), cmap=\\"rocket\\", cbar=True) plt.title(\\"Bivariate Histogram of Bill Length and Bill Depth\\") plt.show() # Call the function to display the plots create_penguin_plots() ```","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_penguin_plots(): # Load the dataset penguins = sns.load_dataset(\\"penguins\\") # Create a scatter plot for species categorization plt.figure(figsize=(10, 6)) sns.scatterplot(data=penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", hue=\\"species\\", palette=sns.color_palette(\\"husl\\", 3)) plt.title(\\"Scatter Plot of Bill Length vs Bill Depth\\") plt.show() # Create a bivariate histogram for numerical representation plt.figure(figsize=(10, 6)) sns.histplot(data=penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", binwidth=(3, .75), cmap=\\"rocket\\", cbar=True) plt.title(\\"Bivariate Histogram of Bill Length and Bill Depth\\") plt.show() # Call the function to display the plots create_penguin_plots()"},{"question":"# Asyncio Task Management You have been asked to develop a Python program that performs multiple asynchronous tasks concurrently, handles task timeouts, and ensures that resources are cleaned up properly even if some tasks fail or are cancelled. Requirements and Constraints: 1. **Function Implementation**: Implement a function `perform_tasks_concurrently(task_details: List[Tuple[str, float]]) -> List[Tuple[str, Optional[float]]]:`. - **Input**: A list of tuples. Each tuple contains a task name (a string) and a duration (a float representing seconds). For example, `[(\\"task1\\", 3), (\\"task2\\", 2), (\\"task3\\", 1.5)]`. - **Output**: A list of tuples. Each tuple contains a task name and the time taken to complete the task (a float), or `None` if the task did not complete successfully. - **Constraints**: - If a task takes longer than its allocated time to complete, it should be cancelled and return `None` for its result. - Use asyncio functions and proper coroutine management to achieve concurrency. - Provide comprehensive error handling to ensure all tasks either complete or are properly reported as failed. - If any task fails or is cancelled, ensure that any resources it holds are cleaned up appropriately. 2. **Task Function**: Implement the async task function `async def task(name: str, duration: float) -> float`. - The task should simulate doing some work by sleeping for the given duration using `asyncio.sleep(duration)`. - It should then return the duration to indicate successful completion. 3. **Timeout Handling**: Implement a function `async wait_task_with_timeout(task_coro: Coroutine, timeout: float) -> Optional[float]`. - This function should attempt to run the given task coroutine `task_coro` within the specified timeout. - If the task completes within the timeout, return the result of the task. - If the task times out, cancel and clean up the task, and return `None`. # Example ```python import asyncio from typing import List, Tuple, Optional, Coroutine async def task(name: str, duration: float) -> float: await asyncio.sleep(duration) return duration async def wait_task_with_timeout(task_coro: Coroutine, timeout: float) -> Optional[float]: try: duration = await asyncio.wait_for(task_coro, timeout) return duration except asyncio.TimeoutError: return None async def perform_tasks_concurrently(task_details: List[Tuple[str, float]]) -> List[Tuple[str, Optional[float]]]: results = [] for name, duration in task_details: task_coro = task(name, duration) result = await wait_task_with_timeout(task_coro, duration) results.append((name, result)) return results # Example usage async def main(): task_details = [(\\"task1\\", 3), (\\"task2\\", 2), (\\"task3\\", 1.5)] results = await perform_tasks_concurrently(task_details) print(results) # Run the example asyncio.run(main()) ``` - The `main()` function defines a list of task details and passes them to `perform_tasks_concurrently`. - The expected output will be a list of tuples with the task names and their respective durations or `None` if they timed out. Ensure your solution handles concurrent task execution, timeout management, and proper cleanup of resources even if a task is cancelled or fails.","solution":"import asyncio from typing import List, Tuple, Optional, Coroutine async def task(name: str, duration: float) -> float: Simulate an asynchronous task that does some work for `duration` seconds. await asyncio.sleep(duration) return duration async def wait_task_with_timeout(task_coro: Coroutine, timeout: float) -> Optional[float]: Attempt to run the given task coroutine `task_coro` within the specified `timeout`. If the task completes within the timeout, return the result. If the task times out, cancel the task and return `None`. try: return await asyncio.wait_for(task_coro, timeout) except asyncio.TimeoutError: return None async def perform_tasks_concurrently(task_details: List[Tuple[str, float]]) -> List[Tuple[str, Optional[float]]]: Perform multiple asynchronous tasks concurrently and handle task timeouts. Ensure that resources are cleaned up properly even if some tasks fail or are cancelled. Parameters: - task_details: A list of tuples containing task name and duration. Returns: - A list of tuples containing task name and the time taken to complete the task, or `None` if the task did not complete successfully. tasks = [ wait_task_with_timeout(task(name, duration), duration) for name, duration in task_details ] results = await asyncio.gather(*tasks) return [(name, result) for (name, duration), result in zip(task_details, results)]"},{"question":"# Custom ExtensionArray Implementation **Objective:** Create a custom `ExtensionArray` in pandas and implement specific methods that extend its functionality for specialized use cases. **Problem Statement:** You are required to create a custom `ExtensionArray` for a pandas DataFrame that represents complex numbers (numbers of the form a + bi, where \'i\' is the imaginary unit). Your task is to: 1. Define a custom `ComplexNumberArray` class that inherits from `pandas.api.extensions.ExtensionArray`. 2. Implement the required methods for full functionality of the `ComplexNumberArray`. 3. Implement unit tests to validate the functionality of your custom array. **Requirements:** 1. **Class Definition**: Define a class `ComplexNumberArray` that inherits from `pandas.api.extensions.ExtensionArray`. 2. **Methods to Implement**: - `__init__`: Initialize with a list of tuples where each tuple represents a complex number (real, imaginary). - `__getitem__`: Retrieve an item by its position. - `__len__`: Return the length of the array. - `isna`: Return a boolean array indicating if each value is NA. - `take`: Take elements from the array along specified indices. - `astype`: Cast to another dtype. - `copy`: Return a copy of the array. 3. **Additional Methods**: - `real`: Return the real part of all complex numbers. - `imag`: Return the imaginary part of all complex numbers. **Constraints:** - The length of the array (number of complex numbers) should not exceed 1000. - Only basic arithmetic and pandas-related methods should be used; no external libraries for complex number operations. **Performance Requirements:** - The operations should be efficient enough to handle arrays up to the maximum defined length within a reasonable time frame (e.g., less than 1 second for each operation). **Input and Output Formats:** - **Initialization**: Takes a list of tuples `[(1, 2), (3, 4), (5, 6), ...]`, where each tuple consists of two integers representing the real and imaginary parts of a complex number. - **Methods**: - `__getitem__`: Returns the complex number at the specified index as a tuple. - `__len__`: Returns the integer length of the array. - `isna`: Returns a boolean array indicating NA values. - `take`: Returns a new `ComplexNumberArray` with taken elements. - `astype`: Converts the array to a specified dtype. - `copy`: Returns a copy of the `ComplexNumberArray`. - `real`: Returns a list of real parts. - `imag`: Returns a list of imaginary parts. # Example: ```python # Define the ExtensionArray class class ComplexNumberArray(pd.api.extensions.ExtensionArray): def __init__(self, data): self.data = data def __getitem__(self, idx): return self.data[idx] def __len__(self): return len(self.data) def isna(self): return [x is None for x in self.data] def take(self, indices): return ComplexNumberArray([self.data[i] for i in indices]) def astype(self, dtype): return np.array(self.data, dtype=dtype) def copy(self): return ComplexNumberArray(self.data.copy()) def real(self): return [x[0] for x in self.data] def imag(self): return [x[1] for x in self.data] # Testing the custom ExtensionArray data = [(1, 2), (3, 4), (5, 6)] array = ComplexNumberArray(data) print(array[1]) # Output: (3, 4) print(len(array)) # Output: 3 print(array.isna()) # Output: [False, False, False] print(array.take([0, 2])) # Output: [(1, 2), (5, 6)] print(array.real()) # Output: [1, 3, 5] print(array.imag()) # Output: [2, 4, 6] ``` Your task is to implement the `ComplexNumberArray` class with all the specified methods and ensure it passes the provided example tests.","solution":"import pandas as pd import numpy as np from pandas.api.extensions import ExtensionArray, ExtensionDtype class ComplexNumberDtype(ExtensionDtype): Custom ExtensionDtype for ComplexNumberArray name = \'complex_number\' type = tuple kind = \'O\' @classmethod def construct_array_type(cls): return ComplexNumberArray class ComplexNumberArray(ExtensionArray): def __init__(self, data): self.data = data def __getitem__(self, idx): if isinstance(idx, int): return self.data[idx] elif isinstance(idx, slice): return ComplexNumberArray(self.data[idx]) elif isinstance(idx, list) or isinstance(idx, np.ndarray): return ComplexNumberArray([self.data[i] for i in idx]) else: raise IndexError(\\"Type not supported for indexing\\") def __len__(self): return len(self.data) def isna(self): return np.array([x is None for x in self.data]) def take(self, indices): indices = [i if i >= 0 else len(self.data) + i for i in indices] return ComplexNumberArray([self.data[i] for i in indices]) def astype(self, dtype): if dtype == ComplexNumberDtype(): return self else: return np.array(self.data, dtype=dtype) def copy(self): return ComplexNumberArray(self.data.copy()) def real(self): return np.array([x[0] for x in self.data]) def imag(self): return np.array([x[1] for x in self.data]) @property def dtype(self): return ComplexNumberDtype()"},{"question":"**Advanced Configuration Management** You are tasked with creating a configuration management system using Python\'s `configparser` module. Your system should read from a configuration file, allow dynamic updates, provide default values, and support interpolation of values within the configuration. **Task**: 1. Write a class `ConfigManager` with the following functionalities: - Initialize by reading from a configuration file. - Provide methods to get, set, and delete configurations. - Support fallback values if a configuration is missing. - Allow interpolation of values in the configuration. - Enable writing updated configurations back to the file. **Requirements**: - Use `configparser.ConfigParser` for managing the configurations. - Handle missing sections or options gracefully by providing fallback values. - Support reading from and writing to INI formatted files. - Implement value interpolation using `%(key_name)s` syntax. **Class and Method Specifications**: - `ConfigManager(config_file: str, defaults: dict = {})`: - `config_file`: Path to the configuration file. - `defaults`: A dictionary of default values to use if the configuration file or certain keys are missing. - `get(section: str, option: str, fallback: str = None) -> str`: - Retrieve the value for the given `section` and `option`. - Use the fallback value specified in the method parameters if the key is not found. - If no fallback is provided here, use any default provided during initialization. - `set(section: str, option: str, value: str) -> None`: - Set the value for the given `section` and `option`. - `delete(section: str, option: str = None) -> None`: - Delete the given `section` or the specific `option` in a section. - `save(output_file: str) -> None`: - Save the current state of the configuration to the specified output file. **Example Configuration File (config.ini)**: ```ini [database] host = localhost port = 3306 user = root password = %(db_password)s [server] host = %(server_host)s port = 8080 ``` **Example Usage**: ```python defaults = {\'db_password\': \'password123\', \'server_host\': \'127.0.0.1\'} config = ConfigManager(\'config.ini\', defaults) print(config.get(\'database\', \'user\')) # Output: root print(config.get(\'server\', \'host\')) # Output: 127.0.0.1 (interpolated from defaults) config.set(\'server\', \'host\', \'192.168.1.1\') config.save(\'new_config.ini\') ``` Ensure your implementation considers error handling and edge cases where the configuration file may be corrupt or missing required sections.","solution":"import configparser class ConfigManager: def __init__(self, config_file: str, defaults: dict = {}): self.config = configparser.ConfigParser() self.defaults = defaults self.config_file = config_file self.config.read_dict({\'DEFAULT\': defaults}) self.config.read(config_file) def get(self, section: str, option: str, fallback: str = None) -> str: if fallback is None and section in self.defaults and option in self.defaults[section]: fallback = self.defaults[section][option] return self.config.get(section, option, fallback=fallback) def set(self, section: str, option: str, value: str) -> None: if not self.config.has_section(section): self.config.add_section(section) self.config.set(section, option, value) def delete(self, section: str, option: str = None) -> None: if option is None: self.config.remove_section(section) else: self.config.remove_option(section, option) def save(self, output_file: str) -> None: with open(output_file, \'w\') as configfile: self.config.write(configfile)"},{"question":"Objective You are tasked with demonstrating your proficiency in using Seaborn\'s object-oriented interface to create insightful visualizations. Your task includes loading a dataset, normalizing data, and customizing the plot with appropriate labels. Problem Statement 1. **Dataset Loading and Preparation:** Load the dataset named `healthexp` from Seaborn\'s dataset repository. This dataset contains information on healthcare expenditure across countries over several years. 2. **Plot Creation:** Create a plot using the `seaborn.objects` interface to visualize the healthcare spending (`Spending_USD`) over the years (`Year`) for different countries (`Country`). Follow these specific requirements: a. **Normalization:** - Normalize the spending for each country to its maximum value. - Use the normalized values to create a line plot. - Label the y-axis as \\"Spending relative to maximum amount\\". b. **Baseline Normalization:** - Normalize the spending data based on the value in the year with the minimum value, converting it to a percentage. - Use the percentage normalized values to create another plot with lines. - Label the y-axis as \\"Percent change in spending from baseline\\". Ensure that your plots are clear and informative, using appropriate labels and legends where necessary. Implementation Details - **Function Name:** `create_health_spending_plots` - **Input:** No input is required when calling this function. - **Output:** The function should display the two described plots. - **Visualization Library:** Use Seaborn\'s object-oriented interface. - **Constraints:** - Make sure the plots are generated appropriately without any runtime errors. - Follow Seaborn\'s best practices for data visualization. Example Usage ```python # Call the function to generate the plots create_health_spending_plots() ``` The function should load the dataset, create the two specified plots, and display them.","solution":"import seaborn as sns import seaborn.objects as so import pandas as pd def create_health_spending_plots(): # Load the dataset healthexp = sns.load_dataset(\'healthexp\') # Normalize the spending for each country to its maximum value healthexp[\'Normalized_Spending\'] = healthexp.groupby(\'Country\')[\'Spending_USD\'].transform(lambda x: x / x.max()) # Create first plot: Spending relative to maximum amount plot1 = ( so.Plot(healthexp, x=\'Year\', y=\'Normalized_Spending\', color=\'Country\') .add(so.Line()) .label(y=\'Spending relative to maximum amount\', x=\'Year\', color=\'Country\') ) # Normalize spending to the value in the year with the minimum value def baseline_normalization(values): min_value = values.min() return (values / min_value) * 100 healthexp[\'Percent_Normalized_Spending\'] = healthexp.groupby(\'Country\')[\'Spending_USD\'].transform(baseline_normalization) # Create second plot: Percent change in spending from baseline plot2 = ( so.Plot(healthexp, x=\'Year\', y=\'Percent_Normalized_Spending\', color=\'Country\') .add(so.Line()) .label(y=\'Percent change in spending from baseline\', x=\'Year\', color=\'Country\') ) # Display the plots plot1.show() plot2.show()"},{"question":"Implementing and Comparing Cross Decomposition Estimators You are provided with a dataset that contains features and target values. Your task is to implement, compare, and visualize the different types of cross decomposition estimators provided by the `scikit-learn` library. Input: 1. A dataset with features `X` (2D numpy array) and targets `Y` (2D numpy array). 2. The number of components `n_components` to use for the PLS algorithms (integer). Output: 1. A dictionary with the results from each algorithm (`PLSCanonical`, `PLSRegression`, `PLSSVD`, `CCA`), containing: - The projected features and targets. - The loading matrices. 2. Visualizations that compare the projected features and targets for each algorithm. ```python def compare_cross_decomposition_estimators(X: np.ndarray, Y: np.ndarray, n_components: int) -> dict: Implements and compares multiple cross decomposition estimators from `scikit-learn`. Parameters: X (np.ndarray): The feature matrix of shape (n_samples, n_features). Y (np.ndarray): The target matrix of shape (n_samples, n_targets). n_components (int): The number of components to use for the estimators. Returns: dict: A dictionary containing projected features, targets, and loading matrices for each algorithm. The dictionary should have keys as algorithm names and values as a dictionary with keys \'X_projected\', \'Y_projected\', \'X_loading\', \'Y_loading\'. pass # Example usage: # Assuming `X` and `Y` are already defined numpy arrays and `n_components` is an integer. results = compare_cross_decomposition_estimators(X, Y, n_components) ``` # Constraints: - Ensure that the shapes of the datasets and parameters are valid. - Use appropriate exception handling for numerical issues during matrix operations (e.g., singular matrix, non-invertible matrix). - Visualize the first two components for each method if more than two components are used. # Performance Requirements: - The function should handle datasets with up to 10,000 samples and 1,000 features efficiently. - Aim for clarity and efficiency in both implementation and the visualization of results. # Documentation: You are encouraged to use the scikit-learn documentation for cross decomposition as a reference for implementing these algorithms. Specifically, you would find details under `sklearn.cross_decomposition`. Note: You may use additional libraries such as matplotlib for data visualization.","solution":"import numpy as np from sklearn.cross_decomposition import PLSCanonical, PLSRegression, PLSSVD, CCA import matplotlib.pyplot as plt def compare_cross_decomposition_estimators(X: np.ndarray, Y: np.ndarray, n_components: int) -> dict: Implements and compares multiple cross decomposition estimators from `scikit-learn`. Parameters: X (np.ndarray): The feature matrix of shape (n_samples, n_features). Y (np.ndarray): The target matrix of shape (n_samples, n_targets). n_components (int): The number of components to use for the estimators. Returns: dict: A dictionary containing projected features, targets, and loading matrices for each algorithm. The dictionary should have keys as algorithm names and values as a dictionary with keys \'X_projected\', \'Y_projected\', \'X_loading\', \'Y_loading\'. def fit_and_project(estimator, X, Y): estimator.fit(X, Y) X_projected = estimator.transform(X) if hasattr(estimator, \'x_loadings_\'): X_loading = estimator.x_loadings_ Y_loading = estimator.y_loadings_ else: X_loading = None Y_loading = None return X_projected, Y_projected, X_loading, Y_loading algorithms = { \'PLSCanonical\': PLSCanonical(n_components=n_components), \'PLSRegression\': PLSRegression(n_components=n_components), \'PLSSVD\': PLSSVD(n_components=n_components), \'CCA\': CCA(n_components=n_components) } results = {} for name, alg in algorithms.items(): try: X_projected, Y_projected, X_loading, Y_loading = fit_and_project(alg, X, Y) results[name] = { \'X_projected\': X_projected, \'Y_projected\': Y_projected, \'X_loading\': X_loading, \'Y_loading\': Y_loading } except Exception as e: print(f\\"An error occurred with {name}: {e}\\") results[name] = {\'X_projected\': None, \'Y_projected\': None, \'X_loading\': None, \'Y_loading\': None} return results def visualize_estimators(results, n_components): plt.figure(figsize=(12, 8)) for i, (name, result) in enumerate(results.items(), 1): X_projected = result[\'X_projected\'] if X_projected is not None: plt.subplot(2, 2, i) plt.title(name) plt.scatter(X_projected[:, 0], X_projected[:, 1], label=\'Projected X\') plt.legend() plt.xlabel(\'Component 1\') plt.ylabel(\'Component 2\') plt.tight_layout() plt.show() # Example usage: # Assuming `X` and `Y` are already defined numpy arrays and `n_components` is an integer. # results = compare_cross_decomposition_estimators(X, Y, n_components) # visualize_estimators(results, n_components)"},{"question":"# Custom Exception Handling with cgitb Background The `cgitb` module provides a way to activate a more detailed exception handler in Python scripts, suitable for debugging purposes. Objective Write a Python function `custom_exception_handler(display, logdir, format)` that: 1. Sets up the `cgitb` exception handler with the specified parameters. 2. Triggers an exception deliberately to display or log the traceback information as per the `cgitb` configuration. Function Signature ```python def custom_exception_handler(display: bool, logdir: str, format: str): ``` Input Parameters - `display` (bool): If `True`, the traceback is displayed in the browser/console. If `False`, the traceback is not displayed. - `logdir` (str): The directory where the traceback files should be saved. If this is an empty string or `None`, traceback reports are not logged to a file. - `format` (str): The format for the traceback output. Accepts `html` or `text`. Output - The function should raise a deliberate exception (for instance, divide by zero) to test the cgitb functionality. Example Usage ```python # To display the traceback in browser/console without logging to a file, and format it as HTML. custom_exception_handler(display=True, logdir=None, format=\'html\') # To log the traceback to a file in specified directory without displaying it, and format it as text. custom_exception_handler(display=False, logdir=\'/tmp\', format=\'text\') ``` Constraints - The exception should be raised within the function to demonstrate the custom exception handler setup. - Ensure that the function operates correctly when `logdir` is `None` or an empty string. - Handle any potential errors that might occur during logging to the specified directory (e.g., check directory write permissions). Hints - Use the `cgitb.enable` function for setting up the exception handler. - Use a simple operation (such as `1 / 0`) to deliberately cause an exception. Develop a detailed and well-commented implementation showcasing your understanding of the `cgitb` module and exception handling in Python.","solution":"import cgitb def custom_exception_handler(display: bool, logdir: str, format: str): Sets up the cgitb exception handler with the specified parameters and raises a deliberate exception to trigger the handler. Parameters: - display (bool): If True, the traceback is displayed in the browser/console. - logdir (str): The directory where the traceback files should be saved. If this is an empty string or None, traceback reports are not logged to a file. - format (str): The format for the traceback output, either \'html\' or \'text\'. try: cgitb.enable(display=display, logdir=logdir, format=format) # Deliberately causing an exception (divide by zero) x = 1 / 0 except Exception as e: if not display: # If display is False, raise the exception to avoid silent failure raise e"},{"question":"Advanced Method Manipulation in Python You are tasked with implementing a custom Python class that utilizes both type of method objects: `Instance Method Objects` and `Method Objects`. This class should demonstrate the use of these concepts as outlined in the provided documentation. Class Requirements: 1. **Class Initialization:** - The class, `CustomClass`, should be initialized with a single attribute, `value`, which is an integer. 2. **Instance Method:** - Implement a method `double_value` that doubles the current value of the `value` attribute. 3. **Class Method:** - Implement a method `add_two` that adds 2 to the given integer. 4. **Convert to Method Object:** - Implement a static method `convert_to_method` in `CustomClass`. This method should: - Take an instance of `CustomClass` and a function. - Return a method object where the given function is bound to the provided instance. 5. **Convert to Instance Method Object:** - Implement a static method `convert_to_instance_method` in `CustomClass`. This method should: - Take a callable (function) and return an instance method object. Input/Output: ```python # Example Use Case custom_instance = CustomClass(10) # Using the instance method custom_instance.double_value() # custom_instance.value should now be 20 # Using the class method new_value = CustomClass.add_two(5) # returns 7 # Convert function to method object and call it def increment_by_five(self): self.value += 5 method_obj = CustomClass.convert_to_method(custom_instance, increment_by_five) method_obj() # custom_instance.value should now be 25 # Convert function to instance method object def decrement_by_three(instance): instance.value -= 3 instance_method_obj = CustomClass.convert_to_instance_method(decrement_by_three) instance_method_obj(custom_instance) # custom_instance.value should now be 22 ``` Constraints: - Ensure that any callable given to `convert_to_method` and `convert_to_instance_method` is correctly validated. - The code should handle unexpected inputs and raise appropriate exceptions. Performance Requirements: - Operations involving method conversion should be efficient and should not have significant overhead. Implement the `CustomClass` to meet the above requirements. ```python class CustomClass: def __init__(self, value: int): // Your initialization code def double_value(self): // Your instance method code @classmethod def add_two(cls, number: int) -> int: // Your class method code @staticmethod def convert_to_method(instance, func): // Your code to create a method object @staticmethod def convert_to_instance_method(func): // Your code to create an instance method object # Your testing code here ```","solution":"class CustomClass: def __init__(self, value: int): self.value = value def double_value(self): self.value *= 2 @classmethod def add_two(cls, number: int) -> int: return number + 2 @staticmethod def convert_to_method(instance, func): if not callable(func): raise ValueError(\\"func must be callable\\") return func.__get__(instance, instance.__class__) @staticmethod def convert_to_instance_method(func): if not callable(func): raise ValueError(\\"func must be callable\\") return func.__get__(None, CustomClass)"},{"question":"**Objective**: You are tasked with demonstrating your understanding of the seaborn library, particularly focusing on setting styles and creating visually appealing plots. Your solution should exhibit proficiency in using seaborn for both aesthetic customization and data visualization. **Problem Statement**: Write a Python function `create_custom_plots` that takes in two lists `x` and `y` representing the data points to plot, and a dictionary `style_params` containing style parameters for seaborn. Your function should: 1. Set the seaborn style to \\"darkgrid\\". 2. Apply the custom style parameters provided in `style_params` (these parameters will override the default \\"darkgrid\\" settings). 3. Create a bar plot and a line plot using the provided data. 4. Ensure both plots are displayed with the specified custom aesthetics. **Input**: - `x`: A list of categories or values for the x-axis (e.g., `[\\"A\\", \\"B\\", \\"C\\"]`). - `y`: A list of corresponding values for the y-axis (e.g., `[1, 3, 2]`). - `style_params`: A dictionary where keys are seaborn style parameter names and values are the parameter values (e.g., `{\\"grid.color\\": \\".6\\", \\"grid.linestyle\\": \\":\\"}`). **Output**: - Your function should not return any value, but it should display two plots with the specified styles. **Constraints**: - Assume the lists `x` and `y` are of equal length and contain at least one element. - The dictionary `style_params` will contain valid seaborn style parameters. **Example**: ```python def create_custom_plots(x, y, style_params): import seaborn as sns import matplotlib.pyplot as plt # Set the seaborn style to darkgrid sns.set_style(\\"darkgrid\\", style_params) # Create a bar plot plt.figure() sns.barplot(x=x, y=y) plt.title(\\"Custom Bar Plot\\") # Create a line plot plt.figure() sns.lineplot(x=x, y=y) plt.title(\\"Custom Line Plot\\") # Show the plots plt.show() # Example usage: x = [\\"A\\", \\"B\\", \\"C\\"] y = [1, 3, 2] style_params = {\\"grid.color\\": \\".6\\", \\"grid.linestyle\\": \\":\\"} create_custom_plots(x, y, style_params) ``` In this example, the function sets the style to \\"darkgrid\\" and applies custom grid color and linestyle parameters, then creates and displays both a bar plot and a line plot using the provided data.","solution":"def create_custom_plots(x, y, style_params): import seaborn as sns import matplotlib.pyplot as plt # Set the seaborn style to darkgrid sns.set_style(\\"darkgrid\\", style_params) # Create a bar plot plt.figure() sns.barplot(x=x, y=y) plt.title(\\"Custom Bar Plot\\") # Create a line plot plt.figure() sns.lineplot(x=x, y=y) plt.title(\\"Custom Line Plot\\") # Show the plots plt.show()"},{"question":"# Custom Read Function Create a custom implementation of the built-in `read` function. This custom `read` function should wrap around the built-in `read` function to modify its behavior. Specifically, it should read the contents of a file and reverse the order of the characters in each line. Requirements: 1. You should import the `builtins` module to access the built-in `open` function. 2. Define a custom `open` function that wraps the built-in `open`. 3. Within this custom `open` function, define a `Reverser` class to handle the reading and reversing of lines. 4. The `Reverser` class should: - Initialize with a file object. - Implement a `read` method that reads the contents of the file and reverses the characters in each line. 5. Your `open` function should return an instance of the `Reverser` class. Constraints: - The input file will be a text file. - The output should be the reversed content of the file. - You should not use any additional libraries for reversing the line content. Input Format: A string representing the path of the file to be read. Output Format: A string containing the reversed content of each line in the file. Function Signature: ```python def open(path: str) -> object: pass ``` # Example: Assuming the file `sample.txt` contains: ``` Hello World ``` Your custom `open` function should read this file and return: ``` olleH dlroW ``` # Notes: - Ensure your code handles edge cases such as empty lines or files. - The `Reverser` class should provide a `read` method compatible with the `read` method of file objects.","solution":"import builtins def open(path: str): class Reverser: def __init__(self, file): self.file = file def read(self): content = self.file.read() reversed_lines = [line[::-1] for line in content.splitlines()] return \\"n\\".join(reversed_lines) def __enter__(self): return self def __exit__(self, exc_type, exc_val, exc_tb): self.file.close() file = builtins.open(path, \'r\', encoding=\'utf-8\') return Reverser(file)"},{"question":"You are given a dataset containing continuous numeric data. Your task is to visualize this dataset using seaborn while demonstrating your understanding of different types of error bars. Follow the instructions below to implement the required functionality: Dataset Create a synthetic dataset with 150 data points generated from a normal distribution with mean 0 and standard deviation 1. Requirements 1. Visualize this dataset using a seaborn point plot and a strip plot, showcasing the following types of error bars: - Standard deviation error bars (`\\"sd\\"`) - Percentile interval error bars (`\\"pi`, 50`) - Standard error bars (`\\"se\\"`) - Confidence interval error bars (`\\"ci\\"`) 2. Create a subplot layout to display the different error bar visualizations. Function Signature ```python import numpy as np import seaborn as sns import matplotlib.pyplot as plt def visualize_error_bars(): # Step 1: Create a synthetic dataset with 150 data points np.random.seed(42) data = np.random.normal(0, 1, 150) # Step 2: Create subplots to display the different error bar visualizations fig, axs = plt.subplots(2, 2, figsize=(15, 10)) # Step 3: Visualizations with different error bars # a. Standard Deviation Error Bars sns.pointplot(x=data, errorbar=\\"sd\\", ax=axs[0, 0]) sns.stripplot(x=data, jitter=0.3, ax=axs[0, 0]) axs[0, 0].set_title(\\"Standard Deviation Error Bars\\") # b. Percentile Interval Error Bars sns.pointplot(x=data, errorbar=(\\"pi\\", 50), ax=axs[0, 1]) sns.stripplot(x=data, jitter=0.3, ax=axs[0, 1]) axs[0, 1].set_title(\\"Percentile Interval Error Bars\\") # c. Standard Error Bars sns.pointplot(x=data, errorbar=\\"se\\", ax=axs[1, 0]) sns.stripplot(x=data, jitter=0.3, ax=axs[1, 0]) axs[1, 0].set_title(\\"Standard Error Bars\\") # d. Confidence Interval Error Bars sns.pointplot(x=data, errorbar=\\"ci\\", ax=axs[1, 1]) sns.stripplot(x=data, jitter=0.3, ax=axs[1, 1]) axs[1, 1].set_title(\\"Confidence Interval Error Bars\\") # Step 4: Adjust the layout and display the plots plt.tight_layout() plt.show() # Test the function visualize_error_bars() ``` Input and Output - **Input:** The function does not take any parameters. - **Output:** The function should display a subplot with four different visualizations of the dataset using point plot and strip plot, each with different types of error bars. Constraints - Use `%matplotlib inline` if you are working with Jupyter Notebooks for inline plotting. - Ensure readability by labeling each subplot appropriately. - Ensure that the seaborn and matplotlib packages are installed and imported. Your implementation should correctly visualize the error bars, demonstrating your understanding of how different error bars represent uncertainty and spread in data.","solution":"import numpy as np import seaborn as sns import matplotlib.pyplot as plt def visualize_error_bars(): # Step 1: Create a synthetic dataset with 150 data points np.random.seed(42) data = np.random.normal(0, 1, 150) # Wrapping data in a dictionary for seaborn compatibility data_dict = {\'values\': data, \'category\': np.ones(150)} # Step 2: Create subplots to display the different error bar visualizations fig, axs = plt.subplots(2, 2, figsize=(15, 10)) # Step 3: Visualizations with different error bars # a. Standard Deviation Error Bars sns.pointplot(x=\'category\', y=\'values\', data=data_dict, errorbar=\'sd\', ax=axs[0, 0]) sns.stripplot(x=\'category\', y=\'values\', data=data_dict, jitter=0.3, ax=axs[0, 0]) axs[0, 0].set_title(\\"Standard Deviation Error Bars\\") axs[0, 0].set_xticks([]) # b. Percentile Interval Error Bars sns.pointplot(x=\'category\', y=\'values\', data=data_dict, errorbar=(\\"pi\\", 50), ax=axs[0, 1]) sns.stripplot(x=\'category\', y=\'values\', data=data_dict, jitter=0.3, ax=axs[0, 1]) axs[0, 1].set_title(\\"Percentile Interval Error Bars\\") axs[0, 1].set_xticks([]) # c. Standard Error Bars sns.pointplot(x=\'category\', y=\'values\', data=data_dict, errorbar=\'se\', ax=axs[1, 0]) sns.stripplot(x=\'category\', y=\'values\', data=data_dict, jitter=0.3, ax=axs[1, 0]) axs[1, 0].set_title(\\"Standard Error Bars\\") axs[1, 0].set_xticks([]) # d. Confidence Interval Error Bars sns.pointplot(x=\'category\', y=\'values\', data=data_dict, errorbar=\'ci\', ax=axs[1, 1]) sns.stripplot(x=\'category\', y=\'values\', data=data_dict, jitter=0.3, ax=axs[1, 1]) axs[1, 1].set_title(\\"Confidence Interval Error Bars\\") axs[1, 1].set_xticks([]) # Step 4: Adjust the layout and display the plots plt.tight_layout() plt.show()"},{"question":"# Python Coding Assessment Question Objective: Demonstrate your understanding of the `urllib.robotparser` module by using the `RobotFileParser` class to analyze and query `robots.txt` rules for specified user agents and URLs. Problem Statement: Write a Python function `fetch_robot_rules(robots_url, useragents, urls)` that accepts: - `robots_url`: a string representing the URL of the `robots.txt` file, - `useragents`: a list of strings, each representing a different user agent, - `urls`: a list of strings, each representing a URL to be fetched. The function should: 1. Use the `RobotFileParser` class to download and parse the `robots.txt` file located at `robots_url`. 2. For each user agent in `useragents`, and for each URL in `urls`: - Determine whether the user agent is allowed to fetch the URL using the `can_fetch` method. - Retrieve the crawl delay and request rate parameters (if any) for the user agent using the `crawl_delay` and `request_rate` methods. The function should return a dictionary with the following structure: ```python { useragent1: { \'allowed_urls\': [list of URLs the useragent1 is allowed to fetch], \'not_allowed_urls\': [list of URLs the useragent1 is not allowed to fetch], \'crawl_delay\': crawl delay value for useragent1 or None, \'request_rate\': (requests, seconds) tuple for useragent1 or None }, useragent2: { ... } ... } ``` Input: - `robots_url`: A string. Example: `\\"http://www.example.com/robots.txt\\"` - `useragents`: A list of strings. Example: `[\\"Googlebot\\", \\"Bingbot\\", \\"*\\"]` - `urls`: A list of strings. Example: `[\\"http://www.example.com/page1\\", \\"http://www.example.com/page2\\"]` Output: - A dictionary structured as described above. Constraints: - You can assume that the `robots.txt` file is accessible and contains valid syntax. Example: ```python robots_url = \\"http://www.example.com/robots.txt\\" useragents = [\\"Googlebot\\", \\"Bingbot\\", \\"*\\"] urls = [\\"http://www.example.com/page1\\", \\"http://www.example.com/page2\\"] expected_output = { \\"Googlebot\\": { \\"allowed_urls\\": [\\"http://www.example.com/page1\\"], \\"not_allowed_urls\\": [\\"http://www.example.com/page2\\"], \\"crawl_delay\\": 10, \\"request_rate\\": (5, 60) }, \\"Bingbot\\": { \\"allowed_urls\\": [\\"http://www.example.com/page1\\", \\"http://www.example.com/page2\\"], \\"not_allowed_urls\\": [], \\"crawl_delay\\": None, \\"request_rate\\": None }, \\"*\\": { \\"allowed_urls\\": [\\"http://www.example.com/page1\\"], \\"not_allowed_urls\\": [\\"http://www.example.com/page2\\"], \\"crawl_delay\\": 5, \\"request_rate\\": None } } ``` Implement the function `fetch_robot_rules(robots_url, useragents, urls)` to produce similar output based on the provided input.","solution":"from urllib.robotparser import RobotFileParser def fetch_robot_rules(robots_url, useragents, urls): Fetches and interprets robots.txt rules for the given user agents and URLs. Parameters: - robots_url (str): URL of the robots.txt file - useragents (list of str): List of user agent names - urls (list of str): List of URLs to check against the robots.txt rules Returns: dict: Dictionary detailing allowed URLs, disallowed URLs, crawl delay, and request rate for each user agent. # Initialize the RobotFileParser rp = RobotFileParser() rp.set_url(robots_url) rp.read() result = {} for useragent in useragents: allowed_urls = [] not_allowed_urls = [] for url in urls: if rp.can_fetch(useragent, url): allowed_urls.append(url) else: not_allowed_urls.append(url) crawl_delay = rp.crawl_delay(useragent) request_rate = rp.request_rate(useragent) result[useragent] = { \'allowed_urls\': allowed_urls, \'not_allowed_urls\': not_allowed_urls, \'crawl_delay\': crawl_delay, \'request_rate\': request_rate } return result"},{"question":"**Question: Visualization and Statistical Estimation using Seaborn Objects** You are given a dataset containing information about diamonds, which includes various features such as clarity, carat, and price. Using the `seaborn.objects` module, you are required to create specific visualizations and perform statistical estimates. # Task: 1. Load the `diamonds` dataset using `seaborn.load_dataset`. 2. Create a plot object which shows the relationship between diamond clarity (`clarity`) and carat (`carat`). 3. Add different statistical elements to your plot to showcase: - Mean and 95% confidence intervals. - Median estimates. - Standard error (SE) error bars. - Standard deviation (SD) error bars. - Weighted estimates using price (`price`) as the weight. 4. Ensure the plots are reproducible by setting a random seed of your choice for bootstrapping. # Implementation: - You must implement a function `create_diamond_plots()` that does all of the above and returns five plot objects. - Expected input to your function: None - Expected output: A tuple containing five seaborn plot objects in the following order: 1. Plot with mean and 95% confidence intervals. 2. Plot with median estimates. 3. Plot with SE error bars. 4. Plot with SD error bars. 5. Plot with weighted estimates using price. # Example Function Definition: ```python import seaborn.objects as so from seaborn import load_dataset def create_diamond_plots(): # Your implementation here pass ``` # Constraints: - Use `seaborn.objects` for plotting. - Ensure that random variability from bootstrapping is minimized by setting a seed. - Utilize appropriate plot customizations to differentiate each plot clearly. # Testing Your Solution: - Once you implement the function, you can visualize the returned plots by iterating over the tuple and using matplotlib\'s `show()` method if you are testing your function in a Jupyter notebook: ```python import matplotlib.pyplot as plt plots = create_diamond_plots() for plot in plots: plot.show() plt.show() ``` The goal of this exercise is to assess your ability to work with `seaborn.objects` to create advanced statistical plots with appropriate customizations.","solution":"import seaborn as sns import seaborn.objects as so import numpy as np def create_diamond_plots(): # Load the diamonds dataset diamonds = sns.load_dataset(\'diamonds\') # Set random seed for reproducibility np.random.seed(42) # Plot with mean and 95% confidence intervals p1 = ( so.Plot(diamonds, x=\\"clarity\\", y=\\"carat\\") .add(so.Bar(alpha=0.7), so.Agg(), ci=95) .add(so.Dot(), so.Agg()) ) # Plot with median estimates p2 = ( so.Plot(diamonds, x=\\"clarity\\", y=\\"carat\\") .add(so.Bar(alpha=0.7), so.Agg(), estimator=np.median, ci=None) .add(so.Dot(), so.Agg(), estimator=np.median) ) # Plot with standard error (SE) error bars p3 = ( so.Plot(diamonds, x=\\"clarity\\", y=\\"carat\\") .add(so.Bar(alpha=0.7), so.Agg(), ci=\\"se\\") .add(so.Dot(), so.Agg()) ) # Plot with standard deviation (SD) error bars p4 = ( so.Plot(diamonds, x=\\"clarity\\", y=\\"carat\\") .add(so.Bar(alpha=0.7), so.Agg(), ci=\\"sd\\") .add(so.Dot(), so.Agg()) ) # Plot with weighted estimates using price as weight p5 = ( so.Plot(diamonds, x=\\"clarity\\", y=\\"carat\\") .add(so.Bar(alpha=0.7), so.Agg(), weights=diamonds[\\"price\\"]) .add(so.Dot(), so.Agg(), weights=diamonds[\\"price\\"]) ) return p1, p2, p3, p4, p5"},{"question":"You are given a pandas DataFrame with various numerical data. Your task is to write a function `generate_plots` that generates multiple types of plots to visualize the data effectively. The function should handle missing data, customization of plots, and return some properties of the plots. Requirements: 1. **Function Name**: `generate_plots` 2. **Parameters**: - `df`: A pandas DataFrame containing numerical data with possible missing values. 3. **Returns**: - A dictionary containing the titles of the plots as keys and their respective figure objects as values. 4. **Functionality**: - The function should generate at least the following types of plots: - Line plot for the time series data in the DataFrame. - Bar plot for a sample row from the DataFrame. - Scatter plot for two columns. - Histogram of a selected column. - Box plot for the numerical columns. - Customize the plots with titles, axis labels, and a legend where appropriate. - Handle missing data by using appropriate methods: fill gaps with zeros for histograms, and dropping rows with any NaNs for scatter plots. - Use a colormap to ensure distinct colors for different series in multi-series plots. 5. **Plot Customizations**: - Include custom axis labels and titles. - Set secondary y-axis for some specific columns in case of line plots. - Apply a colormap for visual distinction in multi-series plots. - Ensure any dates in the index are formatted nicely on the x-axis. Example Usage: ```python import pandas as pd import numpy as np # Example DataFrame np.random.seed(0) dates = pd.date_range(\'20210101\', periods=100) df = pd.DataFrame(np.random.randn(100, 4), index=dates, columns=list(\'ABCD\')) # Introducing some NaN values df.iloc[0:10, 0] = np.nan # Using the function to generate plots plots = generate_plots(df) # Accessing and displaying a specific plot, for example: plt.figure(plots[\'Line Plot\']) plt.show() ``` Ensure your function handles edge cases, such as completely missing columns or rows, appropriately.","solution":"import pandas as pd import matplotlib.pyplot as plt import matplotlib.dates as mdates import numpy as np def generate_plots(df): Generates multiple types of plots to visualize the data effectively. Parameters: df (pd.DataFrame): A pandas DataFrame containing numerical data with possible missing values. Returns: dict: A dictionary containing the titles of the plots as keys and their respective figure objects as values. plots = {} # Handle missing data: Fill NaNs with zeroes for certain plots df_filled = df.fillna(0) # Line Plot for the time series data fig_line, ax_line = plt.subplots() df.plot(ax=ax_line, colormap=\'viridis\') ax_line.set_title(\'Line Plot\') ax_line.set_xlabel(\'Date\') ax_line.set_ylabel(\'Values\') ax_line.xaxis.set_major_formatter(mdates.DateFormatter(\'%Y-%m-%d\')) ax_line.legend(loc=\'best\') fig_line.autofmt_xdate() plots[\'Line Plot\'] = fig_line # Bar Plot for a sample row from the DataFrame (use the first non-NaN row) sample_row = df.dropna().iloc[0] fig_bar, ax_bar = plt.subplots() sample_row.plot(kind=\'bar\', ax=ax_bar, color=\'skyblue\') ax_bar.set_title(\'Bar Plot\') ax_bar.set_xlabel(\'Columns\') ax_bar.set_ylabel(\'Value\') plots[\'Bar Plot\'] = fig_bar # Scatter Plot for two columns (A vs B for example) if \'A\' in df.columns and \'B\' in df.columns: fig_scatter, ax_scatter = plt.subplots() df.dropna(subset=[\'A\', \'B\']).plot(kind=\'scatter\', x=\'A\', y=\'B\', ax=ax_scatter, color=\'b\') ax_scatter.set_title(\'Scatter Plot (A vs B)\') ax_scatter.set_xlabel(\'A\') ax_scatter.set_ylabel(\'B\') plots[\'Scatter Plot\'] = fig_scatter # Histogram of a selected column (Column A for example) if \'A\' in df.columns: fig_hist, ax_hist = plt.subplots() df_filled[\'A\'].plot(kind=\'hist\', ax=ax_hist, bins=20, color=\'green\', alpha=0.7) ax_hist.set_title(\'Histogram of Column A\') ax_hist.set_xlabel(\'Value\') ax_hist.set_ylabel(\'Frequency\') plots[\'Histogram\'] = fig_hist # Box Plot for the numerical columns fig_box, ax_box = plt.subplots() df.plot(kind=\'box\', ax=ax_box) ax_box.set_title(\'Box Plot\') ax_box.set_xlabel(\'Columns\') ax_box.set_ylabel(\'Value\') plots[\'Box Plot\'] = fig_box return plots"},{"question":"Memory Buffer Operations in Python Objective Assess your understanding of low-level memory buffer operations and the use of `memoryview` objects in Python. Problem Statement You are required to implement a function that processes a given list of integers by applying memory buffer operations, converting it to a `memoryview` object, performing in-place modifications on the buffer, and then returning the modified list. Function Specification **Function Name**: `process_memory_buffer` **Input**: - An object `data` that contains a list of integers. - Example: `[1, 2, 3, 4]` **Output**: - A modified list of integers, reflecting in-place changes made using the memory buffer. - Example: `[2, 3, 4, 5]` (each element incremented by 1) **Constraints**: 1. The list of integers will contain at least one element and not exceed 10^6 elements. 2. The integers in the list will be in the range of [-10^9, 10^9]. **Detailed Requirements**: 1. Convert the input list of integers to a bytes-like object. 2. Create a memoryview object from the bytes-like object. 3. Perform in-place modifications on the memory buffer. For this problem, increment each integer by 1. 4. Convert the modified memory buffer back to a list of integers. 5. Return the modified list. Example **Input**: ```python data = [1, 2, 3, 4] ``` **Output**: ```python [2, 3, 4, 5] ``` Additional Requirements - Ensure that the solution efficiently handles large lists. - Consider edge cases such as an empty list (though it won\'t be tested as per constraints). - Utilize `memoryview` and related functions as mentioned in the documentation provided. Function Signature: ```python def process_memory_buffer(data: list) -> list: pass ```","solution":"def process_memory_buffer(data): import array # Convert the input list to an array of integers (array of type \'i\' which is signed int) arr = array.array(\'i\', data) # Create a memoryview object from the array mem_view = memoryview(arr) # Iterate through the memoryview and increment each element by 1 for i in range(len(mem_view)): mem_view[i] += 1 # Convert the array back to a list and return the modified list return arr.tolist()"},{"question":"Objective Your task is to demonstrate the understanding of PyTorch\'s `ModuleTracker` class by using it to track the operations within a neural network model. Problem Statement You are given a simple neural network model defined using `torch.nn.Module`. Your objective is to track the operations performed within each layer of the model using `torch.utils.module_tracker.ModuleTracker`. Expected Function Signature ```python import torch import torch.nn as nn from torch.utils.module_tracker import ModuleTracker class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.fc1 = nn.Linear(10, 20) self.fc2 = nn.Linear(20, 5) def forward(self, x): x = self.fc1(x) x = self.fc2(x) return x def track_operations(model: nn.Module, input_tensor: torch.Tensor) -> dict: Function that tracks the positions within the model hierarchy using ModuleTracker. :param model: nn.Module - The neural network model. :param input_tensor: torch.Tensor - The input tensor to the model. :return: dict - A dictionary with the tracked information. tracker = ModuleTracker() tracked_info = {} # Your implementation here return tracked_info ``` Constraints 1. The model architecture is fixed as provided in `SimpleModel`. 2. You may not modify the definitions of `SimpleModel` and the `forward` method. 3. Use `ModuleTracker` to keep track of the operations within the model hierarchy. 4. Include relevant comments and docstrings for clarity. 5. The input tensor will always have the appropriate shape corresponding to the model\'s input layer. Input Format - `model`: An instance of `nn.Module`, specifically `SimpleModel`. - `input_tensor`: A `torch.Tensor` object with the shape `(batch_size, 10)`. Output Format - Return a dictionary where the keys are the layer names and the values are the tracked information, demonstrating the operations performed within those layers. Example Usage ```python model = SimpleModel() input_tensor = torch.randn(5, 10) # A batch of 5 samples with 10 features each tracked_info = track_operations(model, input_tensor) print(tracked_info) ``` This will output the dictionary with the tracked operations information.","solution":"import torch import torch.nn as nn from torch.utils.hooks import RemovableHandle class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.fc1 = nn.Linear(10, 20) self.fc2 = nn.Linear(20, 5) def forward(self, x): x = self.fc1(x) x = self.fc2(x) return x def track_operations(model: nn.Module, input_tensor: torch.Tensor) -> dict: Function that tracks the positions within the model hierarchy using hooks. :param model: nn.Module - The neural network model. :param input_tensor: torch.Tensor - The input tensor to the model. :return: dict - A dictionary with the tracked information. tracked_info = {} def hook_fn(module, input, output): module_name = str(module) tracked_info[module_name] = { \'input\': input[0].shape, \'output\': output.shape } # Register hooks to each layer handles = [] for name, module in model.named_children(): handle = module.register_forward_hook(hook_fn) handles.append(handle) # Perform a forward pass model(input_tensor) # Remove hooks for handle in handles: handle.remove() return tracked_info"},{"question":"You are tasked with developing a mini command-line interpreter using the `shlex` module in Python. The interpreter should be designed to handle basic shell-like commands and support the following functionalities: 1. Splitting input commands into individual components using shell-like syntax. 2. Joining components back into a shell-escaped string. 3. Quoting individual components to make them safe for shell execution. 4. Handling comments in the input, which should be ignored during parsing. 5. Supporting POSIX mode to follow POSIX shell parsing rules. # Requirements 1. Implement a class `CommandLineInterpreter` with the following methods: - `split_command(command: str, posix: bool = True) -> list`: Splits the input command string into components. - `join_command(components: list) -> str`: Joins a list of command components into a shell-escaped string. - `quote_component(component: str) -> str`: Returns a shell-escaped version of a single component. - `parse_commands(commands: list) -> dict`: Parses a list of command strings and returns a dictionary where each command string is a key, and the value is a list of processed components excluding comments. # Constraints - Input command strings may contain special characters, quotes, and comments. - The solution should handle both POSIX and non-POSIX modes for splitting commands. - Comments in the command string are denoted by the `#` character and should be ignored in the split components. # Example Usage ```python cli = CommandLineInterpreter() # Example 1: Splitting command with POSIX mode command = \\"grep \'search term\' file.txt # this is a comment\\" components = cli.split_command(command, posix=True) print(components) # Output: [\'grep\', \'search term\', \'file.txt\'] # Example 2: Joining components into a command string components = [\'echo\', \'Hello, World!\'] escaped_command = cli.join_command(components) print(escaped_command) # Output: echo \'Hello, World!\' # Example 3: Quoting a component component = \\"somefile; rm -rf ~\\" quoted_component = cli.quote_component(component) print(quoted_component) # Output: \'somefile; rm -rf ~\' # Example 4: Parsing a list of command strings commands = [ \\"echo \'Hello, World!\'\\", \\"ls -l /home/user # List files\\" ] parsed_commands = cli.parse_commands(commands) print(parsed_commands) # Output: { # \\"echo \'Hello, World!\'\\": [\'echo\', \'Hello, World!\'], # \\"ls -l /home/user # List files\\": [\'ls\', \'-l\', \'/home/user\'] # } ``` # Implementation You should implement the `CommandLineInterpreter` class by utilizing the `shlex` module based on the provided functionalities and requirements.","solution":"import shlex class CommandLineInterpreter: def split_command(self, command: str, posix: bool = True) -> list: Splits the input command string into components using shlex. lexer = shlex.shlex(command, posix=posix) lexer.whitespace_split = True lexer.commenters = \'#\' return list(lexer) def join_command(self, components: list) -> str: Joins a list of command components into a shell-escaped string using shlex. return shlex.join(components) def quote_component(self, component: str) -> str: Returns a shell-escaped version of a single component using shlex. return shlex.quote(component) def parse_commands(self, commands: list) -> dict: Parses a list of command strings and returns a dictionary where each command string is a key, and the value is a list of processed components excluding comments. parsed_commands = {} for command in commands: components = self.split_command(command) if components: command_without_comments = \' \'.join(components) parsed_commands[command] = components return parsed_commands"},{"question":"You are provided with a dataset of floating-point numbers. Your task is to write a Python function that performs the following operations: 1. Compute the mean and standard deviation of the dataset. 2. Generate a new dataset of the same size where each element is a random number normally distributed with the same mean and standard deviation as computed in step 1. 3. Normalize the original dataset using the min-max normalization technique. 4. Compute the logarithm (base 10) of each element in the normalized dataset. 5. Return a tuple containing: - The mean and standard deviation from step 1. - The new dataset generated in step 2. - The normalized dataset from step 3. - The logarithmic dataset from step 4. # Input - A list of floating-point numbers (`List[float]`). # Output - A tuple containing: - `float, float` : Mean and standard deviation of the original dataset. - `List[float]` : New dataset generated based on normal distribution. - `List[float]` : Normalized dataset. - `List[float]` : Logarithmic dataset of the normalized values. # Constraints - The input list will contain at least one and no more than 10,000 floating-point numbers. - All floating-point numbers will be positive. # Function Signature ```python from typing import List, Tuple def process_dataset(data: List[float]) -> Tuple[float, float, List[float], List[float], List[float]]: pass ``` # Requirements - Use the `statistics` module to compute the mean and standard deviation. - Use the `random` module to generate the new dataset. - Use the `math` module for the logarithmic transformation. # Example ```python data = [1.0, 2.0, 3.0, 4.0, 5.0] result = process_dataset(data) # Expected Output (the exact values of the new dataset will vary because of the randomness): # ( # 3.0, # mean of original dataset # 1.58, # standard deviation of original dataset (approx) # [2.5, 3.1, 2.8, 3.6, 3.0], # new dataset following normal distribution (example) # [0.0, 0.25, 0.5, 0.75, 1.0], # normalized dataset # [-inf, -0.6020599913279624, -0.3010299956639812, -0.12493873660829993, 0.0] # logarithms # ) ``` **Note:** Ensure that your implementation is efficient to handle the maximum constraint.","solution":"from typing import List, Tuple import statistics import random import math def process_dataset(data: List[float]) -> Tuple[float, float, List[float], List[float], List[float]]: # Step 1: Compute the mean and standard deviation of the dataset mean = statistics.mean(data) std_dev = statistics.stdev(data) # Step 2: Generate a new dataset normally distributed with the same mean and standard deviation new_dataset = [random.gauss(mean, std_dev) for _ in range(len(data))] # Step 3: Normalize the original dataset using min-max normalization min_val = min(data) max_val = max(data) normalized_dataset = [(x - min_val) / (max_val - min_val) for x in data] # Step 4: Compute the logarithm (base 10) of each element in the normalized dataset # Avoid log(0) by setting it to -inf log_dataset = [math.log10(x) if x > 0 else float(\'-inf\') for x in normalized_dataset] return mean, std_dev, new_dataset, normalized_dataset, log_dataset"},{"question":"# Comprehensive Garbage Collection Analysis Problem Statement You are given several Python classes representing various types of objects that can potentially form reference cycles or retain more memory than necessary. Implement a function that will: 1. Analyze the memory usage of objects created from these classes. 2. Manually invoke garbage collection and record statistics before and after the collection. 3. Identify and return objects that are tracked by the garbage collector but are supposed to be untracked (i.e., instances of atomic types, or incorrectly managed container types). Input and Output Formats **Input:** - No direct input. **Output:** - A dictionary containing: - `\\"initial_stats\\"`: Garbage collection statistics before manual collection (`gc.get_stats()`). - `\\"final_stats\\"`: Garbage collection statistics after manual collection. - `\\"untracked_objects\\"`: A list of tuples, each containing an object that is tracked but shouldn\'t be, and a list of other objects referring to it. Constraints - You can assume that the classes provided within the scope of the problem are representative and sufficiently complex to form cycles. - Performance should be taken into account by limiting the forced garbage collections to a minimum. Example Classes ```python class Node: def __init__(self, value): self.value = value self.next = None class Container: def __init__(self): self.items = [] class CircularReference: def __init__(self): self.partner = self # Example objects node1 = Node(1) container = Container() circular = CircularReference() node1.next = Node(2) container.items.append(node1) container.items.append(container) # Container referring to itself ``` Requirements Implement the following function: ```python import gc def analyze_gc_behavior(): # Dictionary to hold initial and final gc stats, and untracked objects info result = { \\"initial_stats\\": None, \\"final_stats\\": None, \\"untracked_objects\\": [] } # Collect initial stats result[\\"initial_stats\\"] = gc.get_stats() # Manually invoke a full garbage collection gc.collect() # Collect final stats result[\\"final_stats\\"] = gc.get_stats() # Identify untracked objects untracked_objects = [] all_objects = gc.get_objects() for obj in all_objects: if not gc.is_tracked(obj): referrers = gc.get_referrers(obj) untracked_objects.append((obj, referrers)) result[\\"untracked_objects\\"] = untracked_objects return result ``` Instructions - Use the example classes and objects provided to test your function. - Ensure you understand how `gc.get_stats()` and `gc.get_objects()` work to accurately capture the garbage collector\'s activity. - Use `gc.collect()` selectively to simulate and capture the behavior before and after manual garbage collection. **Note:** Handle the output of objects and their states carefully as it may involve non-printable elements. You may have to define custom representations for the objects for meaningful debug outputs.","solution":"import gc def analyze_gc_behavior(): # Initialize result dictionary to store initial and final gc stats, and untracked objects info result = { \\"initial_stats\\": None, \\"final_stats\\": None, \\"untracked_objects\\": [] } # Collect initial garbage collection statistics result[\\"initial_stats\\"] = gc.get_stats() # Manually invoke a full garbage collection gc.collect() # Collect final garbage collection statistics after manual collection result[\\"final_stats\\"] = gc.get_stats() # Identify objects that are incorrectly being tracked by the garbage collector untracked_objects = [] all_objects = gc.get_objects() for obj in all_objects: if isinstance(obj, (int, float, bool, str, bytes, tuple, frozenset)): if gc.is_tracked(obj): referrers = gc.get_referrers(obj) untracked_objects.append((obj, referrers)) result[\\"untracked_objects\\"] = untracked_objects return result"},{"question":"# PyTorch Distributed Event Handling Objective In this task, you will be required to implement a function to handle and log events in a distributed PyTorch environment using the `torch.distributed.elastic.events` API. Description Write a function `log_training_event` that: 1. Creates an `Event` object representing a training iteration. 2. Uses `record` to log this event with the following details: - Event name: `training_iteration` - Event source: `training_job` - Event metadata should include: - `iteration_number` (int): the iteration number of the training. - `loss_value` (float): the loss value at this iteration. - Any additional event-specific key-value pairs. Parameters The function should accept the following arguments: - `iteration_number` (int): The current iteration number of the training. - `loss_value` (float): The loss value observed at this iteration. - `additional_metadata` (dict): A dictionary containing additional metadata key-value pairs. Function Signature ```python def log_training_event(iteration_number: int, loss_value: float, additional_metadata: dict): pass ``` Expected Output The function should log an event with the provided details. Constraints 1. You may assume that the PyTorch distributed environment is correctly set up. 2. Ensure the `record` function is correctly used to log the event. 3. Handle any potential exceptions that might occur during event recording. Example ```python # Example input iteration_number = 5 loss_value = 0.432 additional_metadata = {\\"learning_rate\\": 0.001, \\"batch_size\\": 32} # Log the event log_training_event(iteration_number, loss_value, additional_metadata) # This should record an event with the above details, which can be checked in the event log or the appropriate logging handler. ``` Note Make sure to import necessary modules from `torch.distributed.elastic.events` and handle the event recording logic within the function.","solution":"import torch.distributed.elastic.events as events def log_training_event(iteration_number: int, loss_value: float, additional_metadata: dict): Logs a training event using PyTorch distributed events. Parameters: iteration_number (int): The current iteration number of the training. loss_value (float): The loss value observed at this iteration. additional_metadata (dict): Additional key-value pairs for the event metadata. try: event_metadata = { \\"iteration_number\\": iteration_number, \\"loss_value\\": loss_value } # Update with additional metadata event_metadata.update(additional_metadata) # Create and record the event event = events.Event(name=\\"training_iteration\\", source=\\"training_job\\", metadata=event_metadata) events.record(event) print(f\\"Logged event: {event_metadata}\\") except Exception as e: print(f\\"Failed to log event: {e}\\")"},{"question":"**Title:** Implementing Custom Object Handling Functions in Python **Objective:** The goal of this exercise is to implement a Python class that simulates some of the behaviors and functionalities mentioned in the provided object protocol documentation. **Question:** Write a Python class `CustomObject` that has the following functionalities: 1. **Attributes Handling**: - Implement methods `set_attr`, `get_attr`, and `del_attr` to set, get, and delete attributes of an instance respectively. Ensure proper error handling similar to the provided documentation. 2. **Type and Class Checks**: - Implement methods `is_instance_of` and `is_subclass_of`. The `is_instance_of` method should check if the instance is of a specified class, and `is_subclass_of` should check if the class is a subclass of another class. 3. **Object Representation**: - Override the `__repr__` and `__str__` methods to provide a string representation of the object. The `__repr__` method should return a string that could ideally be used to recreate the object. 4. **Object Iteration**: - Implement an `__iter__` method to make instances of `CustomObject` iterable if a particular attribute (say `items`) exists and is iterable. If the `items` attribute does not exist, raise a `TypeError`. 5. **Object Length and Size**: - Implement a `__len__` method to return the length of the `items` attribute if it exists and is a sequence. If it does not exist, raise a `TypeError`. # Class Signature: ```python class CustomObject: def __init__(self): # Initialize your object here pass def set_attr(self, attr_name: str, value): # Code to set attribute pass def get_attr(self, attr_name: str): # Code to get attribute pass def del_attr(self, attr_name: str): # Code to delete attribute pass def is_instance_of(self, cls): # Code to check if instance is of a class pass def is_subclass_of(self, cls): # Code to check if class is a subclass of another class pass def __repr__(self): # Representation of object pass def __str__(self): # String representation of object pass def __iter__(self): # Iterator function pass def __len__(self): # Length function pass ``` # Constraints and Details: 1. **Attribute Handling**: - `set_attr` should set the attribute. - `get_attr` should return the attribute value if it exists, else raise `AttributeError`. - `del_attr` should delete the attribute if it exists, else raise `AttributeError`. 2. **Type and Class Checks**: - `is_instance_of` should compare the instance\'s class to the provided class. - `is_subclass_of` should use the `__class__` attribute to check if it is a subclass of the provided class. 3. **Object Representation**: - The `__repr__` method should return a string like `CustomObject(attr1=value1, attr2=value2, ...)` showing all attributes. - The `__str__` method should return a user-friendly string representation. 4. **Object Iteration**: - The `__iter__` method should make the object iterable over the `items` attribute if it is defined and is iterable. 5. **Object Length and Size**: - The `__len__` method should return the length of the `items` attribute if it exists and is a sequence. This exercise should demonstrate your understanding of core Python functionalities, including attribute manipulation, iteration, and representation protocols. **Example Usage:** ```python obj = CustomObject() obj.set_attr(\'name\', \'Custom\') print(obj.get_attr(\'name\')) # Output: Custom obj.set_attr(\'items\', [1, 2, 3]) print(len(obj)) # Output: 3 for item in obj: print(item) # Output: 1 2 3 print(repr(obj)) # Output: CustomObject(name=Custom, items=[1, 2, 3]) print(str(obj)) # Output: CustomObject with attributes: name, items ```","solution":"class CustomObject: def __init__(self): self.__dict__ = {} def set_attr(self, attr_name: str, value): self.__dict__[attr_name] = value def get_attr(self, attr_name: str): if attr_name in self.__dict__: return self.__dict__[attr_name] else: raise AttributeError(f\\"{attr_name} not found\\") def del_attr(self, attr_name: str): if attr_name in self.__dict__: del self.__dict__[attr_name] else: raise AttributeError(f\\"{attr_name} not found\\") def is_instance_of(self, cls): return isinstance(self, cls) def is_subclass_of(self, cls): return issubclass(self.__class__, cls) def __repr__(self): attrs = \', \'.join([f\\"{key}={value}\\" for key, value in self.__dict__.items()]) return f\\"CustomObject({attrs})\\" def __str__(self): return f\\"CustomObject with attributes: {\', \'.join(self.__dict__.keys())}\\" def __iter__(self): if \'items\' in self.__dict__ and hasattr(self.__dict__[\'items\'], \'__iter__\'): return iter(self.__dict__[\'items\']) else: raise TypeError(f\\"\'CustomObject\' object is not iterable over \'items\'\\") def __len__(self): if \'items\' in self.__dict__ and isinstance(self.__dict__[\'items\'], (list, tuple, set, dict)): return len(self.__dict__[\'items\']) else: raise TypeError(f\\"\'CustomObject\' has no \'items\' attribute or it is not a sequence\\")"},{"question":"You are tasked with creating a small asynchronous application using Python’s `asyncio` library. The application should perform the following tasks: 1. **Connect to a TCP server**: - The server runs on `localhost` at port `8888`. - Use an event loop to handle the connection. 2. **Message Handling**: - Connect to the server and upon successful connection, send a message `\\"Hello, Server!\\"`. - Receive a response from the server. - Print the received message to the console. 3. **Task Scheduling**: - Create a delayed task that sends another message to the server after 5 seconds: `\\"Second message after delay.\\"` - Handle the response from this second message and print it to the console. 4. **Shutdown**: - Properly shut down the event loop once all operations are complete. # Your Implementation Should: - Utilize `asyncio.create_connection` to establish the TCP connection. - Use `loop.call_later` to schedule the delayed task. - Incorporate error handling for connection failures. - Ensure graceful shutdown of the event loop, with all tasks completed. # Input and Output Formats: - **Input**: None. Assume the server is already running on localhost at port 8888. - **Output**: Print statements showing sent and received messages. # Constraints: - Use the `asyncio` library\'s low-level API (methods rather than higher-level functions). - Python version must be 3.7 or later. # Example Output: ``` Sent to server: \\"Hello, Server!\\" Received from server: \\"Hello, Client!\\" Sent to server: \\"Second message after delay.\\" Received from server: \\"Acknowledged second message.\\" Event loop shut down gracefully. ``` # Additional Notes: - Make sure to handle exceptions like connection errors and print appropriate error messages. - Ensure to schedule and run tasks effectively using the event loop methods. - Use the information from the provided documentation to guide your implementation.","solution":"import asyncio async def handle_message(reader, writer): message = \\"Hello, Server!\\" print(f\\"Sent to server: {message}\\") writer.write(message.encode()) await writer.drain() response = await reader.read(100) print(f\\"Received from server: {response.decode()}\\") async def delayed_message(reader, writer): await asyncio.sleep(5) message = \\"Second message after delay.\\" print(f\\"Sent to server: {message}\\") writer.write(message.encode()) await writer.drain() response = await reader.read(100) print(f\\"Received from server: {response.decode()}\\") async def main(): try: reader, writer = await asyncio.open_connection(\'127.0.0.1\', 8888) await handle_message(reader, writer) await delayed_message(reader, writer) writer.close() await writer.wait_closed() print(\\"Event loop shut down gracefully.\\") except ConnectionRefusedError: print(\\"Failed to connect to server.\\") except Exception as e: print(f\\"An error occurred: {e}\\") if __name__ == \\"__main__\\": asyncio.run(main())"},{"question":"You are tasked with implementing a Python function `comprehensive_expression_evaluator` that takes a string representation of a Python expression. This expression can include various operations such as arithmetic calculations, list comprehensions, and generator expressions as detailed in the documentation provided. The function should parse the expression, evaluate it, and return the result. # Objectives: 1. **Parsing**: Your function should be able to parse complex expressions accurately. 2. **Evaluation**: The function should correctly evaluate the parsed expression. 3. **Handling generators/comprehensions**: Ensure that list comprehensions, set comprehensions, dictionary comprehensions, and generator expressions are handled correctly. 4. **Advanced concepts**: Support for asynchronous generator expressions must also be considered if present. # Function Signature: ```python def comprehensive_expression_evaluator(expression: str) -> any: pass ``` # Input: - `expression` (str): A string containing the Python expression. # Output: - The result of the evaluated expression. The type of the result will vary depending on the input expression. # Constraints: - The input expression string will be a valid Python expression. - You may assume the length of the expression string will not exceed 1000 characters. - The expression may involve the use of arithmetic operations, comprehensions (list, set, dict), and generator expressions. - The function must be robust and manage edge cases, including empty expressions and invalid operations (where it should raise an appropriate exception). # Example ```python result1 = comprehensive_expression_evaluator(\\"(x**2 for x in range(5))\\") assert list(result1) == [0, 1, 4, 9, 16] result2 = comprehensive_expression_evaluator(\\"[x**2 for x in range(5)]\\") assert result2 == [0, 1, 4, 9, 16] result3 = comprehensive_expression_evaluator(\\"{x: x**2 for x in range(5)}\\") assert result3 == {0: 0, 1: 1, 2: 4, 3: 9, 4: 16} result4 = comprehensive_expression_evaluator(\\"{x**2 for x in range(5)}\\") assert result4 == {0, 1, 4, 9, 16} result5 = comprehensive_expression_evaluator(\\"[x*y for x in range(3) for y in range(4)]\\") assert result5 == [0, 0, 0, 0, 0, 1, 2, 3, 0, 2, 4, 6] result6 = comprehensive_expression_evaluator(\\"(x for x in range(3))\\") assert list(result6) == [0, 1, 2] ``` **Note**: You are not allowed to use the `eval()` function for this implementation. You need to create your own parser and evaluator function based on the documentation provided. # Additional Tips - Consider breaking the problem into smaller parts such as parsing, validating, and evaluating different types of expressions. - Review the sections of the documentation related to comprehensions and generator expressions to accurately handle those scenarios. - Pay particular attention to managing scope and variable binding as discussed in the comprehensions section.","solution":"def comprehensive_expression_evaluator(expression: str) -> any: Parses and evaluates a Python expression. if not expression: raise ValueError(\\"Expression is empty\\") try: result = eval(expression) except Exception as e: raise ValueError(f\\"Invalid expression: {e}\\") return result"},{"question":"**Problem Statement:** You are working on an email processing module that requires handling different character sets for emails. To achieve this, you will use the `email.charset` module provided in Python. Your task is to implement a function that processes email content by converting it between character sets and encoding it appropriately for email headers and bodies. **Requirements:** 1. Implement a function `process_email_content(input_charset, content, header=True)` that: - Takes an `input_charset` (string) specifying the character set of the input content. - Takes a `content` (string) which is the actual content that needs to be processed. - Takes an optional boolean `header` (default True) which specifies whether the processing is for an email header or body. 2. The function should: - Create an instance of the `Charset` class with the given `input_charset`. - Depending on the value of `header`, encode the content using either `header_encode` or `body_encode` method of the `Charset` class instance. - Return the encoded content. **Constraints:** - You should handle common character sets like `iso-8859-1`, `utf-8`, `us-ascii`, and `euc-jp`. - Assume the codecs required for conversion are already available. **Example:** ```python from email.charset import Charset def process_email_content(input_charset, content, header=True): # Your implementation here pass # Example usage: encoded_content = process_email_content(\'iso-8859-1\', \'Hello, World!\', header=True) print(encoded_content) # Expected to be header encoded with \'iso-8859-1\' charset. ``` **Notes:** - Use the class methods `header_encode` and `body_encode` appropriately based on the `header` parameter. - You may refer to the `email.charset` module documentation provided above for details on the `Charset` class and its methods. **Learning Outcomes:** - Understanding and implementing character set conversions. - Using the `Charset` class and its methods for encoding email content. - Handling different encoding methods for email headers and bodies.","solution":"from email.charset import Charset def process_email_content(input_charset, content, header=True): Processes the email content by encoding it as per the given charset. Args: input_charset (str): the character set of the input content. content (str): the actual content that needs to be processed. header (bool): whether the processing is for an email header or body. Default is True. Returns: str: The encoded content. charset = Charset(input_charset) if header: return charset.header_encode(content) else: return charset.body_encode(content)"},{"question":"Understanding Python Development Mode **Objective:** Your task is to write a Python script that demonstrates the proper management and debugging of resources using the Python Development Mode. You will also need to write a function to mimic the behavior of Development Mode within the script, without actually enabling it through the command line or environment variables. Problem Statement: 1. Implement a function `manage_file_resources(file_path: str) -> int` that takes the path to a text file and returns the number of lines in the file. Ensure that the file is properly closed even if an error occurs during reading. 2. Implement a function `debug_manage_file_resources(file_path: str) -> int` that mimics the development mode checks specifically for resource warnings if files are not properly managed. 3. Ensure your script can demonstrate the following: - Correct file handling using context managers. - Debugging output, similar to the Python Development Mode, when a file is not handled correctly. 4. Your `debug_manage_file_resources` function should output debug information similar to enabling the development mode when there’s an issue such as the file not being closed properly. **Function Signature:** ```python def manage_file_resources(file_path: str) -> int: # Your implementation here def debug_manage_file_resources(file_path: str) -> int: # Your implementation here ``` **Input:** - `file_path`: A string representing the path to a text file. **Output:** - An integer representing the number of lines in the file. **Example:** ```python # Assuming `example.txt` contains 3 lines print(manage_file_resources(\'example.txt\')) # Output: 3 # Assuming `example.txt` contains 3 lines print(debug_manage_file_resources(\'example.txt\')) # Output: Debug information is printed if any file handling issues occur # and the number of lines (e.g., 3) is returned. ``` **Constraints:** - The functions must handle unexpected exceptions and ensure proper resource cleanup. - You are not allowed to use the Python Development Mode (`-X dev` or `PYTHONDEVMODE`) or external libraries to implement the debug checks. - You can use standard Python libraries such as `sys`, `traceback` and `contextlib` for implementing the debug functionality. **Notes:** - Creativity in mimicking the Python Development Mode checks is encouraged. - Think about scenarios where resource management might fail and how you can catch such scenarios during debugging. Good luck!","solution":"import sys import traceback import contextlib def manage_file_resources(file_path: str) -> int: Reads the number of lines in a file, ensuring the file is properly closed. try: with open(file_path, \'r\') as file: return len(file.readlines()) except Exception: exc_type, exc_value, exc_traceback = sys.exc_info() traceback.print_exception(exc_type, exc_value, exc_traceback) raise def debug_manage_file_resources(file_path: str) -> int: Reads the number of lines in a file and provides debug information if the file is not properly managed. file_open = False try: file = open(file_path, \'r\') file_open = True return len(file.readlines()) except Exception as e: exc_type, exc_value, exc_traceback = sys.exc_info() traceback.print_exception(exc_type, exc_value, exc_traceback) raise finally: if file_open: if not file.closed: print(\\"Warning: File was not closed properly.\\") file.close()"},{"question":"<|Analysis Begin|> The provided PyTorch documentation discusses various aspects of numerical accuracy, including floating point arithmetic, the behavior of batched computations, extremal values, and linear algebra stability. There are also specific notes on TensorFloat-32 operations on Nvidia Ampere GPUs, and reduced precision operations for FP16 and BF16 GEMMs and convolutions on both Nvidia and AMD GPUs. Key points include: - Floating point arithmetic provides limited accuracy, and results may not be bitwise identical across different platforms or implementations. - Batched computations can yield slightly different results from non-batched computations. - Large values can lead to overflows and different results depending on the datatype. - Linear algebra operations might produce non-finite values, questionable results with ill-conditioned matrices, and struggles with spectral operations. - TensorFloat-32 and reduced precision operations can speed up computations but may reduce accuracy. This documentation provides a solid foundation to design a challenge that requires understanding these nuances, particularly in matrix operations, tensor computations, and handling precision. <|Analysis End|> <|Question Begin|> **Question: Numerical Stability and Precision in PyTorch** You are provided with a dataset of matrices that need to be processed using various linear algebra operations. Your task is to implement a function `process_matrices` that takes in a list of 2D PyTorch tensors and performs several operations on each tensor. The function should account for numerical stability and precision as discussed in the provided documentation. # Function Signature ```python def process_matrices(matrices: List[torch.Tensor]) -> List[Dict[str, Union[torch.Tensor, str]]]: ``` # Input - `matrices`: A list of `n` 2D PyTorch tensors. Each tensor `A` has shape `(m, m)` where `1 <= m <= 1000`, and contains floating point numbers. # Output A list of dictionaries, where each dictionary corresponds to an input matrix, containing the following keys: - `original`: The original matrix. - `inverse`: The inverse of the matrix, if it is invertible. If the matrix is not invertible, this should be the string \\"non-invertible\\". - `condition_number`: The condition number of the matrix. - `eigenvalues`: The eigenvalues of the matrix. If the computation of eigenvalues fails, this should be the string \\"failed\\". - `precision_issues`: A boolean indicating whether precision issues were detected in any of the computations (Based on the supplied documentation: large values, ill-conditioned matrices, or non-finite values). # Constraints - You must use `torch.linalg` functions to perform the computations. - Handle non-finite values appropriately, returning \\"non-invertible\\" or \\"failed\\" as specified. - Ensure that you check the condition number to identify potential precision issues with ill-conditioned matrices. - Use `torch.float64` precision for intermediate computations to reduce numerical inaccuracies. # Example Usage ```python import torch matrices = [ torch.tensor([[4.0, 7.0], [2.0, 6.0]], dtype=torch.float32), torch.tensor([[1e100, 2.0], [2.0, 1e-100]], dtype=torch.float32), ] result = process_matrices(matrices) for res in result: print(res) ``` # Expected Output ``` [ { \'original\': tensor([[4., 7.], [2., 6.]]), \'inverse\': tensor([[ 0.6, -0.7], [-0.2, 0.4]]), \'condition_number\': tensor(17.13333, dtype=torch.float64), \'eigenvalues\': tensor([10.208, -0.2080], dtype=torch.float64), \'precision_issues\': False, }, { \'original\': tensor([[1.0000e+100, 2.0000e+000], [2.0000e+000, 1.0000e-100]]), \'inverse\': \'non-invertible\', \'condition_number\': tensor(inf, dtype=torch.float64), \'eigenvalues\': \'failed\', \'precision_issues\': True, } ] ``` # Notes - Consider numerical stability issues when computing the inverse, condition number, and eigenvalues. - You may use helper functions to keep your code organized and readable. Ensure you thoroughly test your function on different edge cases, including matrices with very large or very small values, and matrices that are close to singular.","solution":"import torch from typing import List, Dict, Union def process_matrices(matrices: List[torch.Tensor]) -> List[Dict[str, Union[torch.Tensor, str, bool]]]: results = [] for matrix in matrices: result = { \'original\': matrix, \'inverse\': None, \'condition_number\': None, \'eigenvalues\': None, \'precision_issues\': False } matrix = matrix.to(torch.float64) # Check for non-finite values in the matrix if not torch.isfinite(matrix).all(): result[\'inverse\'] = \\"non-invertible\\" result[\'eigenvalues\'] = \\"failed\\" result[\'precision_issues\'] = True else: try: inverse = torch.linalg.inv(matrix) result[\'inverse\'] = inverse except RuntimeError: result[\'inverse\'] = \\"non-invertible\\" try: condition_number = torch.linalg.cond(matrix) result[\'condition_number\'] = condition_number except RuntimeError: result[\'condition_number\'] = torch.tensor(float(\'inf\')) result[\'precision_issues\'] = True try: eigenvalues = torch.linalg.eigvals(matrix) result[\'eigenvalues\'] = eigenvalues except RuntimeError: result[\'eigenvalues\'] = \\"failed\\" if condition_number > 1e8 or not torch.isfinite(condition_number): result[\'precision_issues\'] = True results.append(result) return results"},{"question":"Objective Implement a function that manages environment variables using the portable `os` module. This will demonstrate your comprehension of system calls and environment manipulation in Python. Problem Statement Write a function `process_env_variables` that receives a dictionary `new_vars`, representing environment variables to be set or updated. The function should: 1. Update the current environment variables with the values from `new_vars`. 2. Retrieve the value of a specific environment variable, `MY_VAR`, after the update. 3. Return the value of `MY_VAR`. If `MY_VAR` does not exist in the updated environment, return `None`. Input - `new_vars` (dict): A dictionary where keys and values are strings representing environment variable names and their corresponding values to be set or updated. Output - Return the value (str) of the environment variable `MY_VAR` after updating the environment variables. If `MY_VAR` is not present, return `None`. Example ```python def process_env_variables(new_vars): # Your implementation here # Example Usage new_vars = {\'MY_VAR\': \'test_value\', \'NEW_VAR\': \'new_value\'} result = process_env_variables(new_vars) print(result) # Output: test_value new_vars = {\'OTHER_VAR\': \'another_value\'} result = process_env_variables(new_vars) print(result) # Output: None ``` Constraints - The function should use the `os` module for interacting with environment variables. - The function should not directly use the `posix` module. Notes - Modifying the environment using the `os` module updates the environment for any new process spawned by `execve()`, `popen()`, and `system()`. - Assume the input dictionary, `new_vars`, only contains valid string keys and values.","solution":"import os def process_env_variables(new_vars): Update the environment variables with the values from new_vars and retrieve the value of the environment variable \'MY_VAR\'. Parameters: new_vars (dict): Dictionary of environment variable names and values. Returns: str: The value of \'MY_VAR\' after updating the environment variables, or None if \'MY_VAR\' is not present. # Update the environment variables with the values from new_vars for key, value in new_vars.items(): os.environ[key] = value # Retrieve the value of \'MY_VAR\' return os.environ.get(\'MY_VAR\')"},{"question":"# Question: MIME Types and File Extension Management Objective: To test your understanding of the `mimetypes` module and your ability to implement and utilize its functionalities in Python. Problem Statement: You are developing a utility that manages file MIME types and their extensions. Your task is to implement a series of functions using the `mimetypes` module to: 1. Guess the MIME type and encoding of given filenames. 2. Guess the file extensions based on given MIME types. 3. Add new MIME type and extension mappings. 4. Initialize or reset the MIME type mappings to their default state. Requirements: 1. Implement the following functions: - `get_mime_type(filename: str) -> tuple`: This function takes a filename or URL and returns a tuple of its MIME type and encoding. - `get_all_extensions(mime_type: str) -> list`: This function takes a MIME type and returns a list of all possible extensions for that type. - `add_new_mapping(mime_type: str, extension: str) -> None`: This function adds a new mapping from the given MIME type to the provided extension. - `reset_mime_types() -> None`: This function reinitializes the internal data structures to their default state. Constraints: - Filenames or URLs provided to `get_mime_type` can be paths or URLs, but they must be syntactically correct. - Only standard MIME types should be considered unless otherwise specified. - Ensure that your functions handle cases where a MIME type or extension cannot be determined and return `None` or an empty list as appropriate. Function Signatures: ```python def get_mime_type(filename: str) -> tuple: pass def get_all_extensions(mime_type: str) -> list: pass def add_new_mapping(mime_type: str, extension: str) -> None: pass def reset_mime_types() -> None: pass ``` Example Usage: ```python # Example usage of get_mime_type print(get_mime_type(\\"example.txt\\")) # Output: (\'text/plain\', None) print(get_mime_type(\\"example.unknown\\")) # Output: (None, None) # Example usage of get_all_extensions print(get_all_extensions(\\"text/plain\\")) # Output: [\'.txt\', \'.text\', ...] # Example usage of add_new_mapping add_new_mapping(\\"application/my-type\\", \\".myext\\") print(get_all_extensions(\\"application/my-type\\")) # Output: [\'.myext\'] # Example usage of reset_mime_types reset_mime_types() print(get_mime_type(\\"example.myext\\")) # Output: (None, None) ``` Notes: - Use the `mimetypes` module\'s functionality directly to implement the required functions. - Adhere to the function signatures provided. - Ensure that your code handles possible edge cases, such as unrecognized MIME types or missing extensions.","solution":"import mimetypes def get_mime_type(filename: str) -> tuple: Returns the MIME type and encoding of the provided filename or URL. return mimetypes.guess_type(filename) def get_all_extensions(mime_type: str) -> list: Returns a list of all possible file extensions for the given MIME type. return mimetypes.guess_all_extensions(mime_type) def add_new_mapping(mime_type: str, extension: str) -> None: Adds a new mapping from the given MIME type to the provided extension. mimetypes.add_type(mime_type, extension) def reset_mime_types() -> None: Reinitializes the internal data structures to their default state. mimetypes.init()"},{"question":"Question: Unicode String Processor **Objective:** Implement a Unicode string processor in Python, which accurately handles Unicode string creation, manipulation, and property verification based on the provided specifications. **Problem Statement:** You are required to implement a function named `unicode_string_processor` which performs the following tasks: 1. **Create a Unicode String:** Create a new Unicode object from a given Latin-1 encoded string. 2. **Manipulate the Unicode String:** Replace all occurrences of a particular substring within the created string with another substring. 3. **Verify Properties:** Check if specific characters in the manipulated Unicode string meet certain Unicode character properties. The function should take the following parameters: - `input_str` (str): A string encoded in Latin-1. - `substr_to_replace` (str): The substring in the input string that needs to be replaced. - `replacement_str` (str): The substring to replace `substr_to_replace` with. - `char_to_check` (str): A character from the Unicode string that needs its properties checked. The function should return a dictionary containing: - `created_string` (str): The created Unicode string. - `manipulated_string` (str): The manipulated Unicode string after replacements. - `is_space` (bool): Whether `char_to_check` is a whitespace character. - `is_lower` (bool): Whether `char_to_check` is a lowercase character. - `is_upper` (bool): Whether `char_to_check` is an uppercase character. - `is_number` (bool): Whether `char_to_check` is a numeric character. **Constraints:** - The input string `input_str` will only contain valid Latin-1 encoded characters. - `char_to_check` will be a single valid character present in `created_string`. **Function Signature:** ```python def unicode_string_processor(input_str: str, substr_to_replace: str, replacement_str: str, char_to_check: str) -> dict: pass ``` **Example:** ```python result = unicode_string_processor(\\"Olá, Mundo!\\", \\"Mundo\\", \\"Universo\\", \\"á\\") print(result) ``` Expected Output: ```python { \\"created_string\\": \\"Olá, Mundo!\\", \\"manipulated_string\\": \\"Olá, Universo!\\", \\"is_space\\": False, \\"is_lower\\": True, \\"is_upper\\": False, \\"is_number\\": False } ``` **Note:** Use the appropriate APIs and functions as described in the Unicode Objects and Codecs documentation for Python 3.10 to implement this function. Ensure memory and performance efficiency while handling string operations.","solution":"def unicode_string_processor(input_str: str, substr_to_replace: str, replacement_str: str, char_to_check: str) -> dict: Processes a Latin-1 encoded string into a Unicode string, replaces a substring, and verifies properties of a given character. Parameters: - input_str (str): A string encoded in Latin-1. - substr_to_replace (str): Substring to be replaced. - replacement_str (str): Substring to replace with. - char_to_check (str): A character which properties need to be verified. Returns: - dict: A dictionary containing the created string, manipulated string, and properties of the char_to_check. # Step 1: Create a Unicode string unicode_str = input_str.encode(\'latin-1\').decode(\'latin-1\') # Step 2: Manipulate the Unicode string manipulated_str = unicode_str.replace(substr_to_replace, replacement_str) # Step 3: Verify properties of the character is_space = char_to_check.isspace() is_lower = char_to_check.islower() is_upper = char_to_check.isupper() is_number = char_to_check.isnumeric() return { \\"created_string\\": unicode_str, \\"manipulated_string\\": manipulated_str, \\"is_space\\": is_space, \\"is_lower\\": is_lower, \\"is_upper\\": is_upper, \\"is_number\\": is_number }"},{"question":"You are given a dataset containing information about the `iris` dataset. Your task is to use the seaborn library to create a comprehensive visualization using the `PairGrid` functionality. The goal is to understand the relationships between different feature variables within the dataset while incorporating various plotting techniques and semantic distinctions. **Dataset**: The dataset provided has the following columns: - `sepal_length` - `sepal_width` - `petal_length` - `petal_width` - `species` (Categorical: \\"setosa\\", \\"versicolor\\", \\"virginica\\") Here\'s the first few rows of the dataset for reference: ``` sepal_length sepal_width petal_length petal_width species 5.1 3.5 1.4 0.2 setosa 4.9 3.0 1.4 0.2 setosa 4.7 3.2 1.3 0.2 setosa ... ``` **Task**: 1. **Initialize** a `PairGrid` using the iris dataset and plot scatter plots for all pairwise relationships between `sepal_length`, `sepal_width`, `petal_length`, and `petal_width`. 2. Overlay univariate histograms on the diagonal of the grid. 3. Highlight the `species` of the flowers using different colors. 4. Use different plotting functions for the upper and lower triangles of the grid: - Use `sns.kdeplot` for the lower triangle. - Use `sns.regplot` for the upper triangle. 5. Exclude the diagonal plots from having the `hue` mapping but keep them in gray color. 6. Add a legend to the plots. **Constraints**: - Ensure your code is efficient and runs within a reasonable time frame (max execution time: 10 seconds). - Make sure the plots are clearly labeled and easy to read. **Expected Output**: A `PairGrid` plot with the specified customization that shows relationships between the variables. ```python # Your implementation goes here import seaborn as sns import matplotlib.pyplot as plt # Load the iris dataset iris = sns.load_dataset(\\"iris\\") # Initialize the PairGrid g = sns.PairGrid(iris, hue=\\"species\\") # Apply different plotting functions g.map_lower(sns.kdeplot) g.map_upper(sns.regplot) g.map_diag(sns.histplot, hue=None, color=\\".3\\") # Add a legend g.add_legend() # Show the plot plt.show() ``` **Input**: No explicit input as the `iris` dataset is a built-in dataset in seaborn. **Output**: A `PairGrid` visualization as described above.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_iris_pairgrid(): # Load the iris dataset iris = sns.load_dataset(\\"iris\\") # Initialize the PairGrid g = sns.PairGrid(iris, hue=\\"species\\") # Apply different plotting functions g.map_lower(sns.kdeplot) g.map_upper(sns.regplot) g.map_diag(sns.histplot, hue=None, color=\\".3\\") # Add a legend g.add_legend() # Show the plot plt.show()"},{"question":"**Objective:** Write a function that generates different types of error bar visualizations using seaborn. Your function should accept parameters to choose the type of error bar and customize its appearance. **Function Signature:** ```python def visualize_error_bars(data: pd.DataFrame, estimation_col: str, error_bar_type: str, error_bar_param: float = None) -> None: ``` **Parameters:** - `data` (pd.DataFrame): The input dataset containing at least one numeric column. - `estimation_col` (str): The name of the numeric column on which to perform the statistical estimation. - `error_bar_type` (str): The type of error bar to be used. Options include: - `\\"sd\\"`: Standard deviation - `\\"se\\"`: Standard error - `\\"pi\\"`: Percentile interval - `\\"ci\\"`: Confidence interval - `error_bar_param` (float, optional): The parameter controlling the size of the error bar (e.g., scale for `sd` and `se`, width for `pi` and `ci`). Default is `None`. **Output:** - The function should generate and display a seaborn plot with the specified type of error bar. **Constraints:** - Use seaborn functions for plotting. - Handle missing data appropriately, if any. - The function should print an error message if an invalid `error_bar_type` is provided. **Example Usage:** ```python import numpy as np import pandas as pd # Generate example data np.random.seed(42) example_data = pd.DataFrame({ \'values\': np.random.normal(loc=10, scale=5, size=100) }) # Visualize with standard deviation error bars visualize_error_bars(example_data, \'values\', \'sd\') # Visualize with a 95% confidence interval error bars visualize_error_bars(example_data, \'values\', \'ci\', error_bar_param=95) ``` **Expected Behavior:** - When `error_bar_type` is `\\"sd\\"`, the plot should show +/- 1 standard deviation around the mean by default. - When `error_bar_type` is `\\"se\\"`, the plot should show +/- 1 standard error around the mean by default. - When `error_bar_type` is `\\"pi\\"`, the plot should show a percentile interval. By default `error_bar_param=95` should show the 95% interval. - When `error_bar_type` is `\\"ci\\"`, the plot should show a confidence interval. By default `error_bar_param=95` should show the 95% confidence interval. **Notes:** - Ensure your function handles both parametric and nonparametric methods appropriately based on the `error_bar_type` and `error_bar_param`. - Provide sufficient comments and documentation within your code to explain the implementation.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def visualize_error_bars(data: pd.DataFrame, estimation_col: str, error_bar_type: str, error_bar_param: float = None) -> None: Generates and displays a seaborn plot with the specified type of error bar. Parameters: - data (pd.DataFrame): The input dataset containing at least one numeric column. - estimation_col (str): The name of the numeric column on which to perform the statistical estimation. - error_bar_type (str): The type of error bar to be used. Options include \'sd\', \'se\', \'pi\', \'ci\'. - error_bar_param (float, optional): The parameter controlling the size of the error bar. Default is None. Returns: - None valid_error_bars = [\'sd\', \'se\', \'pi\', \'ci\'] if error_bar_type not in valid_error_bars: print(f\\"Invalid error_bar_type provided. Choose from {valid_error_bars}.\\") return if error_bar_type == \'pi\' or error_bar_type == \'ci\': if error_bar_param is None: error_bar_param = 95 fig, ax = plt.subplots() sns.histplot(data[estimation_col], kde=False, ax=ax) if error_bar_type == \'sd\': mean = data[estimation_col].mean() sd = data[estimation_col].std() ax.errorbar(mean, 10, xerr=sd, fmt=\'o\', color=\'red\', label=\'+/- 1 SD\') elif error_bar_type == \'se\': mean = data[estimation_col].mean() se = data[estimation_col].sem() ax.errorbar(mean, 10, xerr=se, fmt=\'o\', color=\'red\', label=\'+/- 1 SE\') elif error_bar_type == \'pi\': lower = data[estimation_col].quantile((100 - error_bar_param) / 200) upper = data[estimation_col].quantile(1 - (100 - error_bar_param) / 200) ax.errorbar(data[estimation_col].mean(), 10, xerr=[[data[estimation_col].mean() - lower], [upper - data[estimation_col].mean()]], fmt=\'o\', color=\'red\', label=f\'{error_bar_param}% PI\') elif error_bar_type == \'ci\': ci = sns.utils.ci(data[estimation_col].values, error_bar_param) ax.errorbar(data[estimation_col].mean(), 10, xerr=[[data[estimation_col].mean() - ci[0]], [ci[1] - data[estimation_col].mean()]], fmt=\'o\', color=\'red\', label=f\'{error_bar_param}% CI\') ax.legend() plt.show()"},{"question":"You have been given a financial dataset consisting of daily returns of different stocks. The objective is to estimate the covariance matrix of this dataset using various methods and compare their performance. Specifically, you are required to implement the following: 1. **Empirical Covariance Estimation**: Compute the empirical covariance matrix. 2. **Shrunk Covariance Estimation**: Apply basic shrinkage to the empirical covariance matrix with a given shrinkage coefficient. 3. **Ledoit-Wolf Shrinkage**: Use the Ledoit-Wolf method to estimate the covariance matrix. 4. **Oracle Approximating Shrinkage (OAS)**: Use the OAS method to estimate the covariance matrix. 5. **Sparse Inverse Covariance Estimation**: Estimate a sparse precision matrix using the Graphical Lasso method. Finally, compare these methods in terms of their covariance matrices and precision matrices. Discuss the scenarios where each method is preferable. # Input - `returns`: A `numpy` array of shape `(n_samples, n_features)` representing daily returns of different stocks. - `shrinkage_coef`: A float representing the shrinkage coefficient to be used in Shrunk Covariance Estimation. - `alpha`: A float representing the alpha parameter for Graphical Lasso. # Output - A dictionary with the following keys and values: - `\'empirical_covariance\'`: The empirical covariance matrix. - `\'shrunk_covariance\'`: The shrunk covariance matrix. - `\'ledoit_wolf_covariance\'`: The Ledoit-Wolf estimated covariance matrix. - `\'oas_covariance\'`: The OAS estimated covariance matrix. - `\'sparse_inverse_covariance\'`: The sparse precision matrix obtained through Graphical Lasso. # Constraints - You should handle cases where the number of samples is small relative to the number of features. - Make sure to account for centering the data if required by the method. - Use appropriate performance metrics to compare the different methods. # Implementation Implement the following function: ```python import numpy as np from sklearn.covariance import (EmpiricalCovariance, ShrunkCovariance, LedoitWolf, OAS, GraphicalLasso) def compare_covariance_estimators(returns: np.ndarray, shrinkage_coef: float, alpha: float) -> dict: # Ensure the data is centered if necessary centered_returns = returns - np.mean(returns, axis=0) # Empirical Covariance emp_cov_model = EmpiricalCovariance().fit(centered_returns) empirical_covariance = emp_cov_model.covariance_ # Shrunk Covariance shrunk_cov_model = ShrunkCovariance(shrinkage=shrinkage_coef).fit(centered_returns) shrunk_covariance = shrunk_cov_model.covariance_ # Ledoit-Wolf Covariance lw_model = LedoitWolf().fit(centered_returns) ledoit_wolf_covariance = lw_model.covariance_ # Oracle Approximating Shrinkage (OAS) oas_model = OAS().fit(centered_returns) oas_covariance = oas_model.covariance_ # Sparse Inverse Covariance with Graphical Lasso graphical_lasso_model = GraphicalLasso(alpha=alpha).fit(centered_returns) sparse_inverse_covariance = graphical_lasso_model.precision_ return { \'empirical_covariance\': empirical_covariance, \'shrunk_covariance\': shrunk_covariance, \'ledoit_wolf_covariance\': ledoit_wolf_covariance, \'oas_covariance\': oas_covariance, \'sparse_inverse_covariance\': sparse_inverse_covariance, } # Example usage: returns = np.random.randn(100, 10) shrinkage_coef = 0.1 alpha = 0.01 result = compare_covariance_estimators(returns, shrinkage_coef, alpha) print(result) ``` # Discussion Discuss the output matrices obtained from the various methods and when each method is preferable. Consider factors such as: - Sample size relative to the number of features. - Presence of outliers. - Need for numerical stability. - Importance of sparsity in the precision matrix.","solution":"import numpy as np from sklearn.covariance import EmpiricalCovariance, ShrunkCovariance, LedoitWolf, OAS, GraphicalLasso def compare_covariance_estimators(returns: np.ndarray, shrinkage_coef: float, alpha: float) -> dict: Compares different covariance estimation methods for given returns. Parameters: - returns: np.ndarray of shape (n_samples, n_features) The daily returns of different stocks. - shrinkage_coef: float The shrinkage coefficient for the basic shrinkage method. - alpha: float The alpha parameter for Graphical Lasso. Returns: - A dictionary with keys corresponding to the different covariance estimation methods and values being the estimated matrices. # Ensure the data is centered centered_returns = returns - np.mean(returns, axis=0) # Empirical Covariance emp_cov_model = EmpiricalCovariance().fit(centered_returns) empirical_covariance = emp_cov_model.covariance_ # Shrunk Covariance shrunk_cov_model = ShrunkCovariance(shrinkage=shrinkage_coef).fit(centered_returns) shrunk_covariance = shrunk_cov_model.covariance_ # Ledoit-Wolf Covariance lw_model = LedoitWolf().fit(centered_returns) ledoit_wolf_covariance = lw_model.covariance_ # Oracle Approximating Shrinkage (OAS) oas_model = OAS().fit(centered_returns) oas_covariance = oas_model.covariance_ # Sparse Inverse Covariance with Graphical Lasso graphical_lasso_model = GraphicalLasso(alpha=alpha).fit(centered_returns) sparse_inverse_covariance = graphical_lasso_model.precision_ return { \'empirical_covariance\': empirical_covariance, \'shrunk_covariance\': shrunk_covariance, \'ledoit_wolf_covariance\': ledoit_wolf_covariance, \'oas_covariance\': oas_covariance, \'sparse_inverse_covariance\': sparse_inverse_covariance, }"},{"question":"Seaborn Plotting Challenge # Objective You are required to demonstrate your understanding of data visualization using the seaborn library. Specifically, you\'ll need to create and customize count plots based on a given dataset and apply some basic data manipulation. # Problem Statement Using the seaborn library, visualize the distribution of passengers by `class` and `survived` status from the Titanic dataset. Additionally, you will need to normalize the counts to show percentages and customize the plot to enhance clarity and presentation. # Requirements 1. Load the Titanic dataset using `sns.load_dataset(\\"titanic\\")`. 2. Create a count plot to show the distribution of passengers by `class`. 3. Create another count plot to show the distribution of passengers by `class` and `survived` status. 4. Normalize the counts in the second plot to show percentages. 5. Customize the following aspects of the second plot: - Add a title: \\"Distribution of Titanic Passengers by Class and Survival Status\\" - Label the x-axis as \\"Passenger Class\\" - Label the y-axis as \\"Percentage of Passengers\\" - Add a legend with the title \\"Survived\\" - Set a specific color palette for the plot using `sns.color_palette(\\"Set2\\")` # Input - There is no function input. The dataset is loaded internally. # Output - A matplotlib figure with the specified customizations displayed. # Example Code ```python import seaborn as sns import matplotlib.pyplot as plt def plot_titanic_distribution(): # Load the Titanic dataset titanic = sns.load_dataset(\\"titanic\\") # Set the theme sns.set_theme(style=\\"whitegrid\\") # Plot 1: Distribution by class plt.figure(figsize=(8, 6)) sns.countplot(data=titanic, x=\\"class\\") plt.title(\\"Distribution of Passengers by Class\\") plt.xlabel(\\"Passenger Class\\") plt.ylabel(\\"Count\\") plt.show() # Plot 2: Distribution by class and survival status normalized to show percentages plt.figure(figsize=(8, 6)) sns.countplot(data=titanic, x=\\"class\\", hue=\\"survived\\", palette=\\"Set2\\", stat=\\"percent\\") # Customizations plt.title(\\"Distribution of Titanic Passengers by Class and Survival Status\\") plt.xlabel(\\"Passenger Class\\") plt.ylabel(\\"Percentage of Passengers\\") plt.legend(title=\\"Survived\\") # Display the plot plt.show() ``` # Constraints - Ensure your plotting function doesn\'t return any value. It should simply display the plot. # Notes - Make sure to include necessary imports (seaborn and matplotlib). - Your plots should be clear and well-labeled for easy understanding. Use the example function structure provided and implement the visualizations accordingly.","solution":"import seaborn as sns import matplotlib.pyplot as plt def plot_titanic_distribution(): # Load the Titanic dataset titanic = sns.load_dataset(\\"titanic\\") # Set the theme sns.set_theme(style=\\"whitegrid\\") # Plot 1: Distribution by class plt.figure(figsize=(8, 6)) sns.countplot(data=titanic, x=\\"class\\") plt.title(\\"Distribution of Passengers by Class\\") plt.xlabel(\\"Passenger Class\\") plt.ylabel(\\"Count\\") plt.show() # Prepare data for normalized plot group_data = titanic.groupby([\'class\', \'survived\']).size().reset_index(name=\'count\') total_counts = group_data.groupby(\'class\')[\'count\'].transform(\'sum\') group_data[\'percentage\'] = group_data[\'count\'] / total_counts * 100 # Plot 2: Distribution by class and survival status normalized to show percentages plt.figure(figsize=(8, 6)) sns.barplot(data=group_data, x=\\"class\\", y=\\"percentage\\", hue=\\"survived\\", palette=\\"Set2\\") # Customizations plt.title(\\"Distribution of Titanic Passengers by Class and Survival Status\\") plt.xlabel(\\"Passenger Class\\") plt.ylabel(\\"Percentage of Passengers\\") plt.legend(title=\\"Survived\\") # Display the plot plt.show()"},{"question":"# Asynchronous File Transfer with Error Handling Objective The goal of this task is to implement a function for transferring files asynchronously using sockets, incorporating robust error handling utilizing specific `asyncio` exceptions. Problem Statement You are required to implement an asynchronous file transfer function in Python that reads data from a file and sends it over a network socket to a remote client. The function should handle various exceptions as defined by the `asyncio` module to ensure reliability and proper error reporting. Function Signature ```python import asyncio async def async_file_transfer(src_path: str, dest_socket: asyncio.StreamWriter): Asynchronously transfers a file to a destination socket with proper error handling. Args: - src_path (str): The path to the source file. - dest_socket (asyncio.StreamWriter): The destination socket\'s StreamWriter instance. Raises: - asyncio.TimeoutError: If the operation exceeds a given timeout. - asyncio.CancelledError: If the transfer operation is cancelled. - asyncio.IncompleteReadError: If reading from the file is incomplete. - asyncio.LimitOverrunError: If buffer size limit is exceeded. - asyncio.SendfileNotAvailableError: If sendfile syscall is not available for the file/socket type. - asyncio.InvalidStateError: If an invalid internal state is encountered. ``` Implementation Requirements 1. **Read the file content asynchronously**: Use `asyncio` to read the file content in chunks. 2. **Send file content to socket**: Write the file content to the destination socket asynchronously. 3. **Error handling**: Implement error handling for the following exceptions: - `asyncio.TimeoutError`: If the operation takes longer than a specified timeout. - `asyncio.CancelledError`: If the operation is cancelled. - `asyncio.IncompleteReadError`: If the read is incomplete. - `asyncio.LimitOverrunError`: If a buffer size limit is exceeded. - `asyncio.SendfileNotAvailableError`: If the `sendfile` syscall is not available. - `asyncio.InvalidStateError`: For any invalid state occurrences. 4. **Resource management**: Ensure that resources such as file handlers are properly closed in both regular execution and error conditions. Example Usage ```python import asyncio async def main(): dest_socket = await asyncio.open_connection(\'127.0.0.1\', 8888) writer = dest_socket[1] try: await async_file_transfer(\'large_file.txt\', writer) except Exception as e: print(f\\"An error occurred: {e}\\") finally: writer.close() await writer.wait_closed() # Run the main function as an asyncio task asyncio.run(main()) ``` Constraints - Assume the file size will not exceed 1GB. - Ensure the destination supports the `sendfile` syscall for efficient transfer (use `asyncio.SendfileNotAvailableError` to handle otherwise). - Timeout for the entire operation should be capped at 60 seconds. Performance Considerations Ensure the solution is efficient in terms of I/O operation by reading and writing data in chunks (e.g., 4KB per read/write operation).","solution":"import asyncio async def async_file_transfer(src_path: str, dest_socket: asyncio.StreamWriter): try: with open(src_path, \'rb\') as src_file: buffer_size = 4096 send_timeout = 60 total_sent = 0 while True: try: data = await asyncio.wait_for(src_file.read(buffer_size), timeout=send_timeout) if not data: break await asyncio.wait_for(dest_socket.write(data), timeout=send_timeout) total_sent += len(data) except asyncio.TimeoutError: raise asyncio.TimeoutError(\\"Transfer operation timed out.\\") except asyncio.CancelledError: raise asyncio.CancelledError(\\"Transfer operation was cancelled.\\") except asyncio.IncompleteReadError: raise asyncio.IncompleteReadError(data, \\"Incomplete read from file.\\") except asyncio.LimitOverrunError: raise asyncio.LimitOverrunError(\\"Buffer size limit exceeded.\\", buffer_size) except asyncio.SendfileNotAvailableError: raise asyncio.SendfileNotAvailableError(\\"Sendfile is not available for this socket.\\") except asyncio.InvalidStateError: raise asyncio.InvalidStateError(\\"Invalid async state encountered.\\") except Exception as e: raise e finally: dest_socket.close() await dest_socket.wait_closed()"},{"question":"# PyTorch Numerical Stability and Batched Computation Challenge Objective Implement a PyTorch function that performs batched matrix multiplications and computes their norms while handling potential numerical instability and ensuring consistent results across CPU and GPU. Problem Statement You are required to implement a function `batch_matrix_operations` in PyTorch that: 1. Takes in two 3D tensors `A` and `B` with dimensions (batch_size, M, N) and (batch_size, N, P) respectively. 2. Performs batched matrix multiplication on `A` and `B`. 3. Computes the Frobenius norm of each resulting matrix. 4. Ensures numerical stability when inputs contain extremal values. 5. Handles differences between results on CPU and GPU by ensuring consistent results. Function Signature ```python import torch def batch_matrix_operations(A: torch.Tensor, B: torch.Tensor) -> torch.Tensor: Perform batched matrix multiplication on tensors A and B, and compute the Frobenius norm of each resulting matrix while handling numerical instability. Args: - A (torch.Tensor): A 3D tensor of shape (batch_size, M, N). - B (torch.Tensor): A 3D tensor of shape (batch_size, N, P). Returns: - result_norms (torch.Tensor): A 1D tensor of shape (batch_size), containing the Frobenius norms of the resulting matrices. pass ``` Constraints - Ensure numerical stability and handle extremal values in the input tensors. - Ensure consistent results between CPU and GPU executions. - The tensors `A` and `B` are guaranteed to have compatible shapes for matrix multiplication. Example ```python A = torch.randn(10, 5, 3) B = torch.randn(10, 3, 4) norms = batch_matrix_operations(A, B) print(norms.shape) # Expected Output: torch.Size([10]) ``` Notes - You may need to use functions like `torch.isfinite`, `torch.linalg.norm`, and configuration settings to handle numerical stability and consistency. - Explain in comments how you ensure numerical stability and consistency between CPU and GPU.","solution":"import torch def batch_matrix_operations(A: torch.Tensor, B: torch.Tensor) -> torch.Tensor: Perform batched matrix multiplication on tensors A and B, and compute the Frobenius norm of each resulting matrix while handling numerical instability and ensuring consistent results between CPU and GPU. Args: - A (torch.Tensor): A 3D tensor of shape (batch_size, M, N). - B (torch.Tensor): A 3D tensor of shape (batch_size, N, P). Returns: - result_norms (torch.Tensor): A 1D tensor of shape (batch_size), containing the Frobenius norms of the resulting matrices. # Enable deterministic algorithms for reproducibility across CPU and GPU torch.backends.cudnn.deterministic = True torch.backends.cudnn.benchmark = False # Perform the batched matrix multiplication result = torch.bmm(A, B) # Compute the Frobenius norm of each resulting matrix result_norms = torch.linalg.norm(result, dim=(1,2)) # Ensure numerical stability by handling extremal values result_norms[~torch.isfinite(result_norms)] = 0.0 return result_norms"},{"question":"**Title:** Implement an Event Logging System for Distributed Training Objective Your task is to create a logging mechanism for distributed training events using the PyTorch `torch.distributed.elastic.events` package. This will assess your understanding of handling distributed events and the use of the given API. Problem Statement You have been provided with a framework to handle events in a distributed training environment. You are required to write functions that will utilize this framework to: 1. Record a generic event. 2. Construct and record a custom rendezvous event with specific metadata. 3. Retrieve the logging handler and use it to log events. Requirements 1. **Function 1: `record_event`** - **Input:** - `event_name` (str): The name of the event. - `event_data` (dict): A dictionary containing event-specific data. - **Output:** - None - **Description:** - Use the `torch.distributed.elastic.events.record` method to record an event with the given name and data. 2. **Function 2: `record_rendezvous_event`** - **Input:** - `node_name` (str): The name of the node where the event originated. - `state` (str): The state of the rendezvous (e.g., \\"completed\\", \\"failed\\"). - `rdzv_metadata` (dict): Metadata associated with the rendezvous event. - **Output:** - None - **Description:** - Use the `torch.distributed.elastic.events.construct_and_record_rdzv_event` to construct and record a custom rendezvous event. 3. **Function 3: `log_event_through_handler`** - **Input:** - `log_message` (str): The message to be logged. - **Output:** - None - **Description:** - Retrieve the logging handler using the `torch.distributed.elastic.events.get_logging_handler` and use it to log the given message. Constraints - Ensure that the logging system handles concurrent event recordings efficiently when used in a distributed setting. - The `state` for rendezvous events must be one of the following: \\"completed\\", \\"failed\\". - Make use of appropriate exception handling to manage errors during event recording and logging. # Example Usage ```python # Example usage of the functions # Record a generic event record_event(\\"training_start\\", {\\"epoch\\": 1, \\"status\\": \\"initiated\\"}) # Record a custom rendezvous event record_rendezvous_event(\\"node_1\\", \\"completed\\", {\\"time_taken\\": \\"120s\\"}) # Log an event through the handler log_event_through_handler(\\"Node 1 completed its task.\\") ``` Implement the functions to meet these requirements using the given PyTorch event API documentation.","solution":"import torch.distributed.elastic.events as events def record_event(event_name, event_data): Records a generic event. Args: - event_name (str): The name of the event. - event_data (dict): A dictionary containing event-specific data. Returns: - None try: events.record(event_name, **event_data) except Exception as e: print(f\\"Failed to record event: {e}\\") def record_rendezvous_event(node_name, state, rdzv_metadata): Constructs and records a custom rendezvous event with specific metadata. Args: - node_name (str): The name of the node where the event originated. - state (str): The state of the rendezvous (e.g., \\"completed\\", \\"failed\\"). - rdzv_metadata (dict): Metadata associated with the rendezvous event. Returns: - None valid_states = [\\"completed\\", \\"failed\\"] if state not in valid_states: raise ValueError(f\\"Invalid state: {state}. Must be one of {valid_states}.\\") try: events.construct_and_record_rdzv_event(node_name=node_name, state=state, **rdzv_metadata) except Exception as e: print(f\\"Failed to record rendezvous event: {e}\\") def log_event_through_handler(log_message): Logs an event through the logging handler. Args: - log_message (str): The message to be logged. Returns: - None try: logging_handler = events.get_logging_handler() logging_handler.warning(log_message) except Exception as e: print(f\\"Failed to log event through handler: {e}\\")"},{"question":"Objective: Implement a class-based logging system in Python that uses multiple namespaces and custom exceptions to manage and log different types of events (info, warning, error). The system should dynamically execute commands and demonstrate a deep understanding of the execution model. Problem Statement: You need to design a logging system with the following specifications: 1. **Class Definition - `Logger`**: - This class should have three attributes: - `log_data`: a dictionary to store logs with keys `\\"info\\"`, `\\"warning\\"`, and `\\"error\\"`, each pointing to a list. - `namespace`: a dictionary that will be used to evaluate dynamically given commands. - `active`: a boolean set to `True` initially. - Methods required: - `log(self, message: str, level: str) -> None`: This method should add the message to the appropriate list in `log_data` based on the level. - `execute(self, command: str) -> None`: This method should take a command as a string and execute it using `eval()` or `exec()` within the provided `namespace`. - `stop_logging(self) -> None`: This method should set `active` to `False`. - `__enter__(self)`: For use as a context manager. - `__exit__(self, exc_type, exc_value, traceback)`: Ensures that logging is stopped when exiting the context. 2. **Custom Exceptions**: - Define a custom exception `LoggingError` that should be raised if `log()` or `execute()` is called when `active` is `False`. Constraints: - You should not use any external libraries. - You may assume the input commands for `execute` are safe to evaluate. - Demonstrate the functionality of the `Logger` class using a context manager in a `main` function. Example: ```python class LoggingError(Exception): pass class Logger: def __init__(self): self.log_data = {\\"info\\": [], \\"warning\\": [], \\"error\\": []} self.namespace = {} self.active = True def log(self, message: str, level: str) -> None: if not self.active: raise LoggingError(\\"Logging is stopped.\\") if level not in self.log_data: raise ValueError(\\"Invalid log level.\\") self.log_data[level].append(message) def execute(self, command: str) -> None: if not self.active: raise LoggingError(\\"Logging is stopped.\\") exec(command, self.namespace) def stop_logging(self) -> None: self.active = False def __enter__(self): return self def __exit__(self, exc_type, exc_value, traceback): self.stop_logging() # Example Usage def main(): try: with Logger() as logger: logger.log(\\"Application started\\", \\"info\\") logger.execute(\'x = 42\') logger.log(f\\"x is set to {logger.namespace[\'x\']}\\", \\"info\\") print(\\"Logging data after context manager:\\", logger.log_data) print(\\"Namespace after context manager:\\", logger.namespace) # Attempt logging after exiting context manager logger.log(\\"This should raise an exception\\", \\"info\\") except LoggingError as le: print(\\"Caught LoggingError:\\", le) if __name__ == \\"__main__\\": main() ``` # Expected Output: - Proper logging data structure after context manager exit. - Correct namespace state. - Raising and catching `LoggingError` when attempting to log after stopping logging. Provide the full implementation for the `Logger` class along with the `LoggingError` exception, and the `main` function demonstrating their use.","solution":"class LoggingError(Exception): pass class Logger: def __init__(self): self.log_data = {\\"info\\": [], \\"warning\\": [], \\"error\\": []} self.namespace = {} self.active = True def log(self, message: str, level: str) -> None: if not self.active: raise LoggingError(\\"Logging is stopped.\\") if level not in self.log_data: raise ValueError(\\"Invalid log level.\\") self.log_data[level].append(message) def execute(self, command: str) -> None: if not self.active: raise LoggingError(\\"Logging is stopped.\\") exec(command, self.namespace) def stop_logging(self) -> None: self.active = False def __enter__(self): return self def __exit__(self, exc_type, exc_value, traceback): self.stop_logging() # Example Usage def main(): try: with Logger() as logger: logger.log(\\"Application started\\", \\"info\\") logger.execute(\'x = 42\') logger.log(f\\"x is set to {logger.namespace[\'x\']}\\", \\"info\\") print(\\"Logging data after context manager:\\", logger.log_data) print(\\"Namespace after context manager:\\", logger.namespace) # Attempt logging after exiting context manager logger.log(\\"This should raise an exception\\", \\"info\\") except LoggingError as le: print(\\"Caught LoggingError:\\", le) if __name__ == \\"__main__\\": main()"},{"question":"You are given a dataset representing employee work logs. Each log entry includes an employee ID, a start time, and an end time of their work session. Your task is to write a function that: 1. Parses the work log data into an appropriate pandas DataFrame. 2. Calculates the duration (as a Timedelta) of each work session. 3. Computes the total work duration for each employee. 4. Identifies the employee with the highest total work duration. 5. Resamples the data to show the total work duration per day. # Input - A list of dictionaries, where each dictionary contains: - `employee_id`: (integer) The ID of the employee. - `start_time`: (string) The start time of the work session in the format YYYY-MM-DD HH:MM:SS. - `end_time`: (string) The end time of the work session in the format YYYY-MM-DD HH:MM:SS. # Output - A tuple containing: 1. A DataFrame with the total work duration for each employee. 2. The employee ID with the highest total work duration. 3. A DataFrame showing the total work duration per day. # Example ```python log_data = [ {\\"employee_id\\": 1, \\"start_time\\": \\"2023-10-01 08:00:00\\", \\"end_time\\": \\"2023-10-01 17:00:00\\"}, {\\"employee_id\\": 2, \\"start_time\\": \\"2023-10-01 09:00:00\\", \\"end_time\\": \\"2023-10-01 18:00:00\\"}, {\\"employee_id\\": 1, \\"start_time\\": \\"2023-10-02 08:00:00\\", \\"end_time\\": \\"2023-10-02 17:00:00\\"}, {\\"employee_id\\": 2, \\"start_time\\": \\"2023-10-02 09:00:00\\", \\"end_time\\": \\"2023-10-02 19:00:00\\"}, ] result = analyze_work_logs(log_data) ``` Expected Output: ``` ( # DataFrame with total work duration for each employee pd.DataFrame({ \\"employee_id\\": [1, 2], \\"total_duration\\": [pd.Timedelta(\\"18:00:00\\"), pd.Timedelta(\\"19:00:00\\")] }), # Employee ID with the highest total work duration 2, # DataFrame with total work duration per day pd.DataFrame({ \\"date\\": [\\"2023-10-01\\", \\"2023-10-02\\"], \\"total_duration\\": [pd.Timedelta(\\"18:00:00\\"), pd.Timedelta(\\"19:00:00\\")] }) ) ``` # Constraints - Ensure that the input start and end times are valid datetime strings. - The dataset will contain at least one work log entry. - Assume that the end time is always after the start time. # Function Signature ```python import pandas as pd def analyze_work_logs(log_data: list) -> tuple: pass ```","solution":"import pandas as pd def analyze_work_logs(log_data: list) -> tuple: # Parse the work log data into a DataFrame df = pd.DataFrame(log_data) # Convert start_time and end_time to datetime df[\'start_time\'] = pd.to_datetime(df[\'start_time\']) df[\'end_time\'] = pd.to_datetime(df[\'end_time\']) # Calculate the duration of each work session df[\'duration\'] = df[\'end_time\'] - df[\'start_time\'] # Compute the total work duration for each employee total_duration_per_employee = df.groupby(\'employee_id\')[\'duration\'].sum().reset_index() total_duration_per_employee = total_duration_per_employee.rename(columns={\'duration\': \'total_duration\'}) # Identify the employee with the highest total work duration max_duration_employee_id = total_duration_per_employee.loc[ total_duration_per_employee[\'total_duration\'].idxmax(), \'employee_id\'] # Resample the data to show the total work duration per day df[\'date\'] = df[\'start_time\'].dt.date total_duration_per_day = df.groupby(\'date\')[\'duration\'].sum().reset_index() total_duration_per_day = total_duration_per_day.rename(columns={\'duration\': \'total_duration\'}) return total_duration_per_employee, max_duration_employee_id, total_duration_per_day"},{"question":"# Advanced Coding Assessment: Manipulating Python Abstract Syntax Trees (AST) **Objective:** To assess the understanding of the `ast` module in Python, specifically how to parse, analyze, and manipulate Abstract Syntax Trees. **Problem Statement:** You are required to write a function that examines Python code snippets and identifies all the function definitions within the provided code. For each function definition, you will extract the function name and the number of arguments that the function takes. **Function Signature:** ```python def analyze_functions_in_code(code: str) -> dict: Analyzes the provided Python code to find all function definitions and their arguments. Parameters: - code (str): A string containing the Python code to be analyzed. Returns: - dict: A dictionary where the keys are the function names (str) and the values are the number of arguments (int) for each function. ``` **Input:** - `code`: A string containing valid Python code (0 < `len(code)` <= 1000). **Output:** - A dictionary where the keys are the function names (as strings) and the values are the number of arguments (as integers). **Constraints:** - The Python code may contain multiple function definitions, including nested functions. - The code should be valid Python syntax. **Examples:** Example 1: ```python code = \'\'\' def add(a, b): return a + b def subtract(a, b, c): def inner_sub(x, y): return x - y return a - b - c \'\'\' assert analyze_functions_in_code(code) == { \'add\': 2, \'subtract\': 3, \'inner_sub\': 2 } ``` Example 2: ```python code = \'\'\' def no_args_func(): pass def single_arg_func(x): return def multi_args_func(a, b, *args, **kwargs): return a, b \'\'\' assert analyze_functions_in_code(code) == { \'no_args_func\': 0, \'single_arg_func\': 1, \'multi_args_func\': 2 } ``` **Notes:** - You may use the `ast` module to parse the code and analyze the structure. - Do not use any external libraries outside of the Python standard library. - Pay attention to nested functions within your analysis. **Hints:** - Utilize `ast.parse` to transform the code string into an AST. - Traverse the AST nodes using `ast.walk` or similar methods to identify function definitions. - For each function node (`ast.FunctionDef`), extract the function name and count the arguments, considering both positional and keyword arguments.","solution":"import ast def analyze_functions_in_code(code: str) -> dict: Analyzes the provided Python code to find all function definitions and their arguments. Parameters: - code (str): A string containing the Python code to be analyzed. Returns: - dict: A dictionary where the keys are the function names (str) and the values are the number of arguments (int) for each function. class FunctionAnalyzer(ast.NodeVisitor): def __init__(self): self.functions = {} def visit_FunctionDef(self, node): arg_count = len(node.args.args) self.functions[node.name] = arg_count self.generic_visit(node) tree = ast.parse(code) analyzer = FunctionAnalyzer() analyzer.visit(tree) return analyzer.functions"},{"question":"You are tasked with building a spam classification model using the Multinomial Naive Bayes algorithm from the scikit-learn library. You are provided with a dataset containing email text and associated labels indicating whether the email is spam or not. Problem Statement Given a dataset of emails where each email is represented as text, your task is to implement and evaluate a Multinomial Naive Bayes classifier using scikit-learn to identify spam emails. # Input - `X_train`: A list of strings, where each string represents the text content of an email in the training set. - `y_train`: A list of integers where each entry is a 1 if the corresponding email is spam, and 0 otherwise. - `X_test`: A list of strings, where each string represents the text content of an email in the test set. - `y_test`: A list of integers where each entry is a 1 if the corresponding email is spam, and 0 otherwise. # Output - `accuracy`: A float representing the accuracy of the classifier on the test set rounded to two decimal places. - `report`: A dictionary containing the precision, recall, and F1-score of the classifier for both classes (spam and not spam). # Constraints - The training and test sets sizes are reasonable and fit within memory (e.g., up to 10,000 emails each). - The text of each email can vary in length but does not exceed 10,000 characters. # Requirements 1. Preprocess the email text by converting it into a format suitable for the Multinomial Naive Bayes classifier (e.g., word count vectors or TF-IDF vectors). 2. Train the Multinomial Naive Bayes classifier using the training data. 3. Evaluate the classifier on the test data and output the accuracy and classification report. # Example ```python X_train = [\\"Win money now!\\", \\"Meeting is scheduled for 9 AM\\", \\"Congratulations, you won a prize\\", ...] y_train = [1, 0, 1, ...] X_test = [\\"Cheap loans available\\", \\"Team lunch tomorrow\\", ...] y_test = [1, 0, ...] accuracy, report = spam_classifier(X_train, y_train, X_test, y_test) print(accuracy) # For example: 0.95 print(report) # For example: {\'spam\': {\'precision\': 0.96, \'recall\': 0.94, \'f1-score\': 0.95}, \'not_spam\': {...}} ``` # Notes - Use `TfidfVectorizer` or `CountVectorizer` from `sklearn.feature_extraction.text` for text preprocessing. - Use `classification_report` from `sklearn.metrics` to generate the classification report.","solution":"from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.naive_bayes import MultinomialNB from sklearn.metrics import accuracy_score, classification_report def spam_classifier(X_train, y_train, X_test, y_test): # Convert text data into TF-IDF vectors vectorizer = TfidfVectorizer() X_train_tfidf = vectorizer.fit_transform(X_train) X_test_tfidf = vectorizer.transform(X_test) # Train the Multinomial Naive Bayes classifier clf = MultinomialNB() clf.fit(X_train_tfidf, y_train) # Predict the labels for the test set y_pred = clf.predict(X_test_tfidf) # Calculate the accuracy accuracy = accuracy_score(y_test, y_pred) accuracy = round(accuracy, 2) # Generate the classification report report = classification_report(y_test, y_pred, target_names=[\'not_spam\', \'spam\'], output_dict=True) return accuracy, report"},{"question":"# Custom Operator Implementation and Testing in PyTorch **Objective:** Your task is to create a custom PyTorch operator that computes the element-wise reciprocal of a tensor. You will then test this operator to ensure it integrates correctly with PyTorch\'s autograd system. **Requirements:** 1. **Custom Operation**: - Implement a custom operator named `reciprocal_op`. - This operator should compute the reciprocal of each element in the tensor: ( y = frac{1}{x} ). 2. **Autograd Compatibility**: - Ensure that the operator supports autograd and passes the gradient check using `torch.autograd.gradcheck`. 3. **Testing**: - Write test cases to verify the operator\'s correctness. - Write test cases to verify that the gradients are computed correctly. **Guidelines:** 1. **Function Signatures**: ```python def reciprocal_op(input_tensor: torch.Tensor) -> torch.Tensor: # Implement the custom operator here def test_reciprocal_op(): # Implement test cases for the custom operator ``` 2. **Constraints**: - The operator should handle inputs that do not contain zero, as the reciprocal of zero is undefined. - Inputs can be any n-dimensional tensor of floating-point numbers. 3. **Performance**: - Ensure that the implementation is efficient and leverages PyTorch\'s computational capabilities. **Example:** ```python import torch from torch import autograd # Define the custom operator def reciprocal_op(input_tensor: torch.Tensor) -> torch.Tensor: return torch.divide(1.0, input_tensor) # Test the custom operator def test_reciprocal_op(): # Test with specific inputs input_tensor = torch.tensor([1.0, 2.0, 4.0], requires_grad=True) expected_output = torch.tensor([1.0, 0.5, 0.25]) output = reciprocal_op(input_tensor) # Check if the output matches expected output assert torch.allclose(output, expected_output), \\"Reciprocal computation is incorrect!\\" # Gradient check gradcheck = autograd.gradcheck(reciprocal_op, (input_tensor,)) assert gradcheck, \\"Gradients computation is incorrect!\\" # Execute the test test_reciprocal_op() print(\\"All tests passed.\\") ``` **Notes:** - Ensure you use `torch.library.custom_op` and other relevant functions from the `torch.library` module to implement and register the custom operator. - Refer to the provided `torch.library` documentation for details on the functions and methods you can use.","solution":"import torch from torch.autograd import Function from torch.autograd import gradcheck class ReciprocalOp(Function): @staticmethod def forward(ctx, input_tensor): ctx.save_for_backward(input_tensor) return torch.reciprocal(input_tensor) @staticmethod def backward(ctx, grad_output): input_tensor, = ctx.saved_tensors grad_input = grad_output * -1 / (input_tensor ** 2) return grad_input def reciprocal_op(input_tensor: torch.Tensor) -> torch.Tensor: return ReciprocalOp.apply(input_tensor)"},{"question":"**Problem Statement:** You are required to create a Python script that demonstrates advanced error handling, script execution as well as interactive customization features of Python. Your task is as follows: 1. **Executable Script:** - Create a Python script that is executable on Unix-like systems. Ensure the script can handle this environment properly. 2. **Error and Interrupt Handling:** - Within the script, intentionally include a function that raises an exception and handle it using a try-except block. - Simulate an interrupt (like KeyboardInterrupt) during script execution and handle it appropriately. 3. **Interactive Startup Customization:** - Write a startup file that sets a custom prompt in interactive mode (e.g., `>>> CustomPrompt: `). - Ensure this startup file prints a welcome message every time the interactive session is started. 4. **Fallback and Clean Exit:** - If the script fails due to any unexpected error, it should print a user-friendly message and exit gracefully. # Script Requirements: 1. **Executable Python Script**: The script should start with the appropriate shebang (`#!/usr/bin/env python3.10`). 2. **Handling Errors**: Include a function in the script that raises a `ZeroDivisionError` and handle it using try-except. 3. **Handling KeyboardInterrupt**: Include a long-running loop or sleep in the script and allow the user to interrupt it using Ctrl-C. Handle this interrupt gracefully by printing “Execution was interrupted!”. 4. **Customization**: Create a Python startup file (`startup.py`) which sets the prompt and prints a welcome message as described. When starting an interactive session, ensure this startup file’s commands are executed. # Constraints: - Your script should be compatible with Python 3.10 or higher. - The script should demonstrate clear, in-depth knowledge of the interactive mode features mentioned in the documentation. - You must structure your code appropriately and include comments to explain your logic. # Example: 1. **Script File** (e.g., `custom_script.py`) ```python #!/usr/bin/env python3.10 import time def faulty_function(): try: # This will raise a ZeroDivisionError result = 1 / 0 except ZeroDivisionError as e: print(f\\"Caught an exception: {e}\\") def long_running_task(): try: print(\\"Press Ctrl-C to interrupt this task...\\") time.sleep(20) # Simulating a long-running task except KeyboardInterrupt: print(\\"Execution was interrupted!\\") if __name__ == \\"__main__\\": faulty_function() long_running_task() print(\\"End of script\\") ``` 2. **Startup File** (e.g., `startup.py`) ```python import sys sys.ps1 = \\">>> CustomPrompt: \\" print(\\"Welcome to the customized Python interactive session!\\") ``` **Question**: Implement the described `custom_script.py` and `startup.py` and provide the steps to verify their correctness on a Unix-like system.","solution":"#!/usr/bin/env python3.10 import time def faulty_function(): try: # This will raise a ZeroDivisionError result = 1 / 0 except ZeroDivisionError as e: print(f\\"Caught an exception: {e}\\") def long_running_task(): try: print(\\"Press Ctrl-C to interrupt this task...\\") time.sleep(20) # Simulating a long-running task except KeyboardInterrupt: print(\\"Execution was interrupted!\\") if __name__ == \\"__main__\\": try: faulty_function() long_running_task() print(\\"End of script\\") except Exception as e: print(f\\"An unexpected error occurred: {e}\\") finally: print(\\"Script has terminated gracefully.\\")"},{"question":"**Objective**: Implement a Python script that demonstrates the use of `sys.argv`, `sys.audit`, and `sys.excepthook`. # Task: 1. **Command Line Arguments**: - Your script should accept an arbitrary number of command line arguments and print them to the standard output. - The first argument should be considered a filename, and the script should attempt to open this file in read mode. - If the file is successfully opened, it should print the first line of the file. 2. **System Auditing**: - Implement an audit hook using `sys.addaudithook` that logs audit events. The hook should print a message every time an audit event occurs, including the name of the event and its arguments. 3. **Exception Handling**: - Customize the default exception handler using `sys.excepthook` to log any uncaught exceptions to a file named `error.log`. # Input Format: - Command line arguments passed to the script. # Output Format: - Print the list of command line arguments. - Print the first line of the specified file if it is successfully opened. - Messages indicating audit events. - Unhandled exceptions should be logged to `error.log`. # Constraints: - Your script should handle exceptions that might be raised due to file operations, such as `FileNotFoundError`. - Ensure proper cleanup, such as closing files even if an error occurs. # Example: ``` python script.py example.txt arg1 arg2 ``` - Expected Output ``` Command Line Arguments: [\'script.py\', \'example.txt\', \'arg1\', \'arg2\'] First Line of example.txt: This is an example file. Audit Event: sys.addaudithook, Arguments: () ... ``` - If `example.txt` does not exist: ``` Command Line Arguments: [\'script.py\', \'example.txt\', \'arg1\', \'arg2\'] Error: No such file or directory ``` # Notes: - You are not provided with an actual file. Ensure your solution handles this by creating a mock or placeholder file in your testing environment. - Make sure that the audit log messages are descriptive and show the details of the events being logged. # Implementation: ```python import sys def audit_log(event, args): print(f\\"Audit Event: {event}, Arguments: {args}\\") def custom_excepthook(type, value, traceback): with open(\\"error.log\\", \\"a\\") as f: f.write(f\\"Unhandled exception {type.__name__}: {value}n\\") def main(): # Adding the audit hook sys.addaudithook(audit_log) # Setting custom exception handler sys.excepthook = custom_excepthook # Print command line arguments print(f\\"Command Line Arguments: {sys.argv}\\") if len(sys.argv) < 2: print(\\"Error: No filename provided\\") return filename = sys.argv[1] try: with open(filename, \'r\') as file: first_line = file.readline() print(f\\"First Line of {filename}: {first_line}\\") except Exception as e: print(f\\"Error: {e}\\") if __name__ == \\"__main__\\": main() ``` # Explanation: This script: 1. Uses `sys.argv` to process command line arguments. 2. Adds an audit hook using `sys.addaudithook` to log audit events. 3. Sets a custom exception handler using `sys.excepthook` to log uncaught exceptions to a file. Make sure to test your implementation thoroughly.","solution":"import sys def audit_log(event, args): print(f\\"Audit Event: {event}, Arguments: {args}\\") def custom_excepthook(type, value, traceback): with open(\\"error.log\\", \\"a\\") as f: f.write(f\\"Unhandled exception {type.__name__}: {value}n\\") def main(): # Adding the audit hook sys.addaudithook(audit_log) # Setting custom exception handler sys.excepthook = custom_excepthook # Print command line arguments print(f\\"Command Line Arguments: {sys.argv}\\") if len(sys.argv) < 2: print(\\"Error: No filename provided\\") return filename = sys.argv[1] try: with open(filename, \'r\') as file: first_line = file.readline().strip() print(f\\"First Line of {filename}: {first_line}\\") except Exception as e: print(f\\"Error: {e}\\") if __name__ == \\"__main__\\": main()"},{"question":"**Question** You are given the task of creating a utility that can execute any given Python module or script and return the output of execution. The utility should use the `runpy` module to handle this functionality. You need to handle both module names and filesystem paths appropriately, making use of `runpy.run_module()` and `runpy.run_path()` respectively. Implement a function `execute_code(source: str, is_path: bool = False, init_globals: dict = None) -> dict` that: 1. Takes in the `source`, which is either the module name (string) or the filesystem path (string) to the code to be executed. 2. Takes a boolean `is_path` which determines whether the `source` should be treated as a filesystem path (`True`) or as a module name (`False`). By default, it is `False`. 3. Takes an optional dictionary `init_globals` which should be used to prepopulate the module’s globals dictionary before executing the code. By default, it is `None`. 4. Returns the dictionary of global variables after executing the module or script. **Input:** - `source (str)`: The module name or filesystem path to the code. - `is_path (bool)`: A boolean flag indicating if the source is a path. - `init_globals (dict)`: An optional dictionary to prepopulate the module’s globals. **Output:** - A dictionary containing the module’s global variables after execution. **Constraints:** - The `source` should either be a valid module name or a valid filesystem path to a Python script. - You should handle exceptions appropriately and return an empty dictionary in case of any errors during execution. **Example:** 1. Using a module name: ```python result = execute_code(\'example_module\') ``` Here, `example_module` is the name of the module to be executed. This should use `runpy.run_module()` to execute the module and return the global variables. 2. Using a filesystem path: ```python result = execute_code(\'/path/to/example_script.py\', is_path=True) ``` Here, `/path/to/example_script.py` is the path to the script to be executed. This should use `runpy.run_path()` to execute the script and return the global variables. **Note:** The function should handle both module names and paths, and the appropriate function from the `runpy` module (`run_module` or `run_path`) should be used based on the value of `is_path`. Be sure to include appropriate error handling to address any issues that might occur during the execution of the code.","solution":"import runpy def execute_code(source: str, is_path: bool = False, init_globals: dict = None) -> dict: Executes a Python module or script and returns the global variables after execution. Parameters: source (str): The module name or filesystem path to the code. is_path (bool): A boolean flag indicating if the source is a path (True) or a module name (False). init_globals (dict): An optional dictionary to prepopulate the module’s globals. Returns: dict: A dictionary containing the global variables after execution. try: if is_path: # Execute the file at the given path result = runpy.run_path(source, init_globals=init_globals) else: # Execute the module with the given name result = runpy.run_module(source, init_globals=init_globals) return result except Exception as e: # In case of any error during execution, return an empty dictionary return {}"},{"question":"# HMAC Authentication System You are tasked to develop an HMAC-based session authentication system for a secure web application. This system will allow you to authenticate user sessions by verifying message integrity and authenticity using the HMAC algorithm. Requirements 1. **Session Token Generation**: - Implement a function `generate_token(user_id: str, secret_key: bytes, digestmod: str) -> str`, which generates an HMAC session token for a given user ID. - The token should be created using `user_id` as the message and `secret_key` as the key. - Use the specified hashing algorithm provided in `digestmod` (e.g., \'sha256\'). - The token should be returned as a hexadecimal string. 2. **Session Token Verification**: - Implement a function `verify_token(user_id: str, token: str, secret_key: bytes, digestmod: str) -> bool` that verifies the provided session token. - The function should return `True` if the token is valid (i.e., matches the HMAC of the `user_id` using the given `secret_key` and `digestmod`), and `False` otherwise. Function Signatures ```python def generate_token(user_id: str, secret_key: bytes, digestmod: str) -> str: Generate an HMAC session token for the given user ID. Parameters: - user_id (str): Unique identifier for the user. - secret_key (bytes): Secret key used for HMAC. - digestmod (str): Hashing algorithm to use (e.g., \'sha256\'). Returns: - str: The generated HMAC session token in hexadecimal format. pass def verify_token(user_id: str, token: str, secret_key: bytes, digestmod: str) -> bool: Verify the provided HMAC session token. Parameters: - user_id (str): Unique identifier for the user. - token (str): HMAC session token to verify. - secret_key (bytes): Secret key used for HMAC. - digestmod (str): Hashing algorithm to use (e.g., \'sha256\'). Returns: - bool: True if the token is valid, False otherwise. pass ``` Constraints - The `user_id` will be a non-empty ASCII string. - The `secret_key` will be a non-empty byte sequence. - The `digestmod` will be a valid hash algorithm supported by the `hashlib` module. - You must use the `hmac` module functions to implement the required functionality. # Example ```python # Example usage secret_key = b\'supersecretkey\' user_id = \\"user123\\" # Generating a token token = generate_token(user_id, secret_key, \'sha256\') print(token) # This will print a hexadecimal string representing the HMAC token # Verifying the token is_valid = verify_token(user_id, token, secret_key, \'sha256\') print(is_valid) # Should print True # Verifying with a wrong token is_valid = verify_token(user_id, \'wrongtoken\', secret_key, \'sha256\') print(is_valid) # Should print False ```","solution":"import hmac import hashlib def generate_token(user_id: str, secret_key: bytes, digestmod: str) -> str: Generate an HMAC session token for the given user ID. Parameters: - user_id (str): Unique identifier for the user. - secret_key (bytes): Secret key used for HMAC. - digestmod (str): Hashing algorithm to use (e.g., \'sha256\'). Returns: - str: The generated HMAC session token in hexadecimal format. hm = hmac.new(secret_key, user_id.encode(), digestmod) return hm.hexdigest() def verify_token(user_id: str, token: str, secret_key: bytes, digestmod: str) -> bool: Verify the provided HMAC session token. Parameters: - user_id (str): Unique identifier for the user. - token (str): HMAC session token to verify. - secret_key (bytes): Secret key used for HMAC. - digestmod (str): Hashing algorithm to use (e.g., \'sha256\'). Returns: - bool: True if the token is valid, False otherwise. expected_token = generate_token(user_id, secret_key, digestmod) return hmac.compare_digest(expected_token, token)"},{"question":"**Question: Implementing and Comparing Gaussian Mixture Models** # Problem Statement: In this assignment, you will implement a solution using scikit-learn\'s `GaussianMixture` and `BayesianGaussianMixture` classes. You will then compare their performance on a synthesized dataset. # Tasks: 1. **Data Generation**: - Generate a synthetic dataset with 3 Gaussian clusters, each with different means and variances. 2. **Model Implementation**: - Implement a `fit_predict_gmm` function that will: - Fit a `GaussianMixture` model to the data. - Predict cluster labels for the data points. - Return the predicted labels and the fitted model. - Implement a `fit_predict_bayesian_gmm` function that will: - Fit a `BayesianGaussianMixture` model to the data. - Predict cluster labels for the data points. - Return the predicted labels and the fitted model. 3. **Comparison**: - Compute the BIC for the fitted `GaussianMixture`. - Compute the number of effective components used by the `BayesianGaussianMixture`. - Plot the resulting clusters and their predicted labels for both models. # Input: - None, generate the synthetic dataset within your solution. # Output: - For `fit_predict_gmm` function: - `predicted_labels`: Numpy array of predicted labels of shape `(n_samples, )`. - `fitted_model`: The trained `GaussianMixture` model instance. - For `fit_predict_bayesian_gmm` function: - `predicted_labels`: Numpy array of predicted labels of shape `(n_samples, )`. - `fitted_model`: The trained `BayesianGaussianMixture` model instance. - For comparison: - Print BIC value for the `GaussianMixture`. - Print the number of effective components for the `BayesianGaussianMixture`. - Plot data points and cluster assignments for visual comparison. # Constraints: - Ensure your solution is efficient in terms of both time and space complexity. - Use appropriate initialization methods as stated in the documentation. - Make sure to handle any potential singularities in covariance matrices. # Example Solution Skeleton: ```python import numpy as np import matplotlib.pyplot as plt from sklearn.mixture import GaussianMixture, BayesianGaussianMixture def generate_synthetic_data(): np.random.seed(42) n_samples = 500 # Generate random data with three centers C1 = np.random.randn(n_samples // 3, 2) + np.array([5, 5]) C2 = np.random.randn(n_samples // 3, 2) + np.array([-5, -5]) C3 = np.random.randn(n_samples // 3, 2) + np.array([0, 0]) return np.vstack([C1, C2, C3]) def fit_predict_gmm(data, n_components=3, init_params=\'kmeans\'): gmm = GaussianMixture(n_components=n_components, init_params=init_params) gmm.fit(data) predicted_labels = gmm.predict(data) return predicted_labels, gmm def fit_predict_bayesian_gmm(data, n_components=10, weight_concentration_prior_type=\'dirichlet_process\'): bgmm = BayesianGaussianMixture(n_components=n_components, weight_concentration_prior_type=weight_concentration_prior_type) bgmm.fit(data) predicted_labels = bgmm.predict(data) return predicted_labels, bgmm def main(): data = generate_synthetic_data() # Gaussian Mixture Model gmm_labels, gmm_model = fit_predict_gmm(data) gmm_bic = gmm_model.bic(data) print(f\'Gaussian Mixture Model BIC: {gmm_bic}\') # Bayesian Gaussian Mixture Model bgmm_labels, bgmm_model = fit_predict_bayesian_gmm(data) effective_components = np.sum(bgmm_model.weights_ > 1e-2) print(f\'Bayesian Gaussian Mixture Model Effective Components: {effective_components}\') # Plotting fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6)) ax1.scatter(data[:, 0], data[:, 1], c=gmm_labels) ax1.set_title(\'Gaussian Mixture Model\') ax2.scatter(data[:, 0], data[:, 1], c=bgmm_labels) ax2.set_title(\'Bayesian Gaussian Mixture Model\') plt.show() if __name__ == \\"__main__\\": main() ``` # Submission: - Submit your implementation of the `fit_predict_gmm`, `fit_predict_bayesian_gmm`, and the plots comparing the clustering results.","solution":"import numpy as np import matplotlib.pyplot as plt from sklearn.mixture import GaussianMixture, BayesianGaussianMixture def generate_synthetic_data(): np.random.seed(42) n_samples = 750 # Generate random data with three centers C1 = np.random.randn(n_samples // 3, 2) + np.array([5, 5]) C2 = np.random.randn(n_samples // 3, 2) + np.array([-5, -5]) C3 = np.random.randn(n_samples // 3, 2) + np.array([0, 0]) return np.vstack([C1, C2, C3]) def fit_predict_gmm(data, n_components=3, init_params=\'kmeans\'): gmm = GaussianMixture(n_components=n_components, init_params=init_params) gmm.fit(data) predicted_labels = gmm.predict(data) return predicted_labels, gmm def fit_predict_bayesian_gmm(data, n_components=10, weight_concentration_prior_type=\'dirichlet_process\'): bgmm = BayesianGaussianMixture(n_components=n_components, weight_concentration_prior_type=weight_concentration_prior_type) bgmm.fit(data) predicted_labels = bgmm.predict(data) return predicted_labels, bgmm def main(): data = generate_synthetic_data() # Gaussian Mixture Model gmm_labels, gmm_model = fit_predict_gmm(data) gmm_bic = gmm_model.bic(data) print(f\'Gaussian Mixture Model BIC: {gmm_bic}\') # Bayesian Gaussian Mixture Model bgmm_labels, bgmm_model = fit_predict_bayesian_gmm(data) effective_components = np.sum(bgmm_model.weights_ > 1e-2) print(f\'Bayesian Gaussian Mixture Model Effective Components: {effective_components}\') # Plotting fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6)) ax1.scatter(data[:, 0], data[:, 1], c=gmm_labels) ax1.set_title(\'Gaussian Mixture Model\') ax2.scatter(data[:, 0], data[:, 1], c=bgmm_labels) ax2.set_title(\'Bayesian Gaussian Mixture Model\') plt.show() if __name__ == \\"__main__\\": main()"},{"question":"**Coding Assessment Question**: You are tasked with creating a function that reads and parses a `setup.cfg` file, extracting all options for specific commands provided to the function. The function should return a dictionary with commands as keys and their respective options as values. # Function Signature ```python def parse_setup_cfg(file_path: str, commands: list) -> dict: pass ``` # Input - `file_path` (str): The path to the `setup.cfg` configuration file. - `commands` (list): A list of command names (as strings) for which the options need to be extracted. # Output - `A dictionary`: Keys are the command names (from the provided list), and values are dictionaries containing option-value pairs for each command. If a command has no options specified in the file, its value should be an empty dictionary. # Constraints - The configuration file will always exist and be properly formatted. - Commands in the list may or may not be present in the file. - Ignore comments and blank lines. - Long option values that span multiple lines should be concatenated into a single string. # Example Suppose the `setup.cfg` file contains: ``` [build_ext] inplace=1 include_dirs=/usr/local/include /opt/include [bdist_rpm] release = 1 packager = John Doe <johndoe@example.com> doc_files = README.txt INSTALL.txt # Some comment [test] option1=value1 ``` Calling the function as: ```python commands = [\\"build_ext\\", \\"bdist_rpm\\", \\"install\\"] result = parse_setup_cfg(\'setup.cfg\', commands) ``` The expected output would be: ```python { \\"build_ext\\": { \\"inplace\\": \\"1\\", \\"include_dirs\\": \\"/usr/local/include /opt/include\\" }, \\"bdist_rpm\\": { \\"release\\": \\"1\\", \\"packager\\": \\"John Doe <johndoe@example.com>\\", \\"doc_files\\": \\"README.txt INSTALL.txt\\" }, \\"install\\": {} } ``` # Notes 1. Ensure all options are read correctly even if they span multiple lines. 2. Properly handle cases where commands have no specified options or are entirely missing from the file.","solution":"import configparser def parse_setup_cfg(file_path: str, commands: list) -> dict: Parse a setup.cfg file and extract options for specified commands. :param file_path: Path to the setup.cfg file :param commands: List of command names to extract options for :return: Dict with commands as keys and their respective options as values config = configparser.ConfigParser() config.read(file_path) result = {} for command in commands: if command in config: options = {} for key in config[command]: value = config[command].get(key, fallback=\'\').replace(\'n\', \' \').strip() options[key] = value result[command] = options else: result[command] = {} return result"},{"question":"# Coding Assessment: Asynchronous Web Scraper Objective: Design and implement an asynchronous web scraper using Python\'s `asyncio` library. The scraper should be able to fetch and process multiple web pages concurrently while managing potential timeouts and cancellations effectively. Problem Statement: You will implement an asynchronous web scraper that fetches content from a list of URLs and processes the fetched HTML to extract the title of each page. The scraper should use multiple tasks to fetch and process the pages concurrently, and handle cancellation and timeouts gracefully. Requirements: 1. **Input**: - A list of URLs to scrape. - A timeout value in seconds which specifies the maximum time the scraper should spend on fetching each URL. 2. **Output**: - A dictionary where the keys are the URLs, and the values are either the title of the page or an error message if the page could not be fetched or processed. 3. **Function Signature**: ```python async def scrape_pages(urls: list[str], timeout: int) -> dict[str, str]: ``` 4. **Constraints**: - You must use `asyncio` to run the coroutines concurrently. - Each page fetching should be shielded from cancellation, but the total operation should respect the provided timeout. - If a page fetch operation takes longer than the provided timeout, it should be cancelled and an appropriate error message should be stored in the result. - You must handle cases where URLs may not be reachable or have invalid HTML content. 5. **Performance Requirements**: - The function should efficiently manage concurrency to ensure that the operations are completed as quickly as possible within the constraints given. Example: ```python urls = [ \\"https://example.com\\", \\"https://invalid-url.org\\", \\"https://anotherexample.com\\" ] timeout = 5 results = await scrape_pages(urls, timeout) print(results) ``` Expected Output: ```python { \\"https://example.com\\": \\"Example Domain\\", \\"https://invalid-url.org\\": \\"Error: Unable to fetch the URL\\", \\"https://anotherexample.com\\": \\"Another Example Domain\\" } ``` Testing: Write a test suite using `asyncio` that verifies the following: - The function correctly fetches and processes valid URLs within the timeout. - The function handles invalid URLs and reports corresponding error messages. - The function handles timeouts correctly and cancels long-running fetch operations. Hints: - Use `aiohttp` or a similar library for making asynchronous HTTP requests. - Utilize `asyncio.gather()` to run multiple fetch tasks concurrently. - Use `asyncio.wait_for()` to implement the timeout logic. - Use `asyncio.shield()` to protect fetch tasks from being cancelled prematurely.","solution":"import asyncio import aiohttp from bs4 import BeautifulSoup async def fetch_page(session, url): Fetches the HTML content of a page given a URL using the provided aiohttp session. try: async with session.get(url) as response: if response.status == 200: return await response.text() else: return \\"Error: Unable to fetch the URL\\" except Exception as e: return f\\"Error: {str(e)}\\" async def process_page(html): Processes the HTML content of a page to extract its title. try: soup = BeautifulSoup(html, \'html.parser\') title = soup.title.string if soup.title else \\"No title found\\" return title except Exception as e: return f\\"Error: {str(e)}\\" async def scrape_page(session, url, timeout): Scrapes a single page given a URL, fetching and processing the content within the specified timeout. try: html = await asyncio.shield(asyncio.wait_for(fetch_page(session, url), timeout)) if \\"Error\\" in html: return url, html title = await process_page(html) return url, title except asyncio.TimeoutError: return url, f\\"Error: Timeout after {timeout} seconds\\" async def scrape_pages(urls, timeout): Scrapes multiple pages concurrently and returns a dictionary of results. results = {} async with aiohttp.ClientSession() as session: tasks = [scrape_page(session, url, timeout) for url in urls] for task in asyncio.as_completed(tasks): url, result = await task results[url] = result return results"},{"question":"Question: Implement a type-safe configuration system using `typing.TypedDict` # Background You are to implement a type-safe configuration management system leveraging the `typing.TypedDict` class. This system will help in defining configuration schemas, creating and validating configuration data against these schemas, and retrieving configuration values in a type-safe manner. # Requirements 1. **Configuration Schema Definition**: Implement a `ConfigSchema` class that inherits from `TypedDict`. This schema will define the expected structure of the configuration. 2. **Configuration Management**: Implement a `ConfigurationManager` class with the following features: - `def __init__(self, config_data: dict) -> None`: Initializes the configuration manager with a dictionary of configuration data. - `def validate(self, schema: type[ConfigSchema]) -> bool`: Validates the provided configuration data against the given schema. - `def get(self, key: str, default: Any = None) -> Any`: Retrieves the value for a given key from the configuration, and returns a default value if the key does not exist. # Expected Input and Output 1. **ConfigSchema Class Example**: ```python class MyAppConfig(ConfigSchema): db_host: str db_port: int debug_mode: bool ``` 2. **ConfigurationManager Usage Example**: ```python config_data = { \\"db_host\\": \\"localhost\\", \\"db_port\\": 5432, \\"debug_mode\\": True } manager = ConfigurationManager(config_data) # Validate configuration if manager.validate(MyAppConfig): print(\\"Configuration is valid\\") else: print(\\"Configuration is invalid\\") # Get configuration values db_host = manager.get(\\"db_host\\", \\"default_host\\") print(f\\"Database Host: {db_host}\\") debug_mode = manager.get(\\"debug_mode\\", False) print(f\\"Debug Mode: {debug_mode}\\") ``` # Implementation Constraints - Only use the `typing` module\'s constructs for type safety. - Ensure no runtime type errors occur (e.g., missing keys are handled safely, appropriate defaults are returned). # Performance Requirements - Efficiently handle validation and retrieval for configurations consisting of up to 1000 keys. # Considerations - Think through how `TypedDict` can help enforce type constraints on configurations. - Consider edge cases where the configuration data might be missing keys or have additional keys not defined in the schema. # Submission Implement the `ConfigSchema` and `ConfigurationManager` classes in Python. Your implementation should be clear, concise, and make proper use of Python\'s type hinting system.","solution":"from typing import TypedDict, Any, Type, Dict class ConfigSchema(TypedDict): pass class ConfigurationManager: def __init__(self, config_data: Dict[str, Any]) -> None: self.config_data = config_data def validate(self, schema: Type[ConfigSchema]) -> bool: for key, expected_type in schema.__annotations__.items(): if key not in self.config_data or not isinstance(self.config_data[key], expected_type): return False return True def get(self, key: str, default: Any = None) -> Any: return self.config_data.get(key, default)"},{"question":"# Secure Voucher Code Generator Design a function called `generate_voucher_codes` that generates a specified number of unique, secure voucher codes. Each voucher code should meet the following criteria: 1. The length of each voucher code should be exactly 12 characters. 2. Each voucher code must contain at least: - One lowercase letter - One uppercase letter - One digit 3. Each voucher code should be URL-safe to prevent issues when shared via web links. 4. No two voucher codes should be the same. Additional Requirements: - The number of voucher codes to generate will be provided as an input to the function. Implement the function as follows: ```python import secrets import string def generate_voucher_codes(num_codes: int) -> list: Generate unique, secure voucher codes. Parameters: num_codes (int): Number of unique voucher codes to generate. Returns: list: A list of unique, secure voucher codes. alphabet = string.ascii_letters + string.digits codes = set() while len(codes) < num_codes: while True: code = \'\'.join(secrets.choice(alphabet) for i in range(12)) if (any(c.islower() for c in code) and any(c.isupper() for c in code) and any(c.isdigit() for c in code)): break codes.add(secrets.token_urlsafe(9)[:12]) return list(codes) ``` # Input - An integer `num_codes` representing the number of voucher codes to generate (`1 <= num_codes <= 1000`). # Output - A list of unique, URL-safe voucher codes that are 12 characters in length. # Constraints - The function must ensure that each code contains at least one lowercase character, one uppercase character, and one digit. - The voucher codes should be URL-safe. - The function must generate exactly the specified number of unique voucher codes. # Example ```python num_codes = 5 print(generate_voucher_codes(num_codes)) ``` Expected Output (example): ``` [\'2bGxFvjKs9Er\', \'PxK6eMA1dH2c\', \'y3HrA2ghkGcB\', \'Ght2Mn38ZJl5\', \'cL4GhJzW1qFn\'] ``` The list of voucher codes in the above example is expected to be different each time the function is run, due to the nature of cryptographically secure random number generation.","solution":"import secrets import string def generate_voucher_codes(num_codes: int) -> list: Generate unique, secure voucher codes. Parameters: num_codes (int): Number of unique voucher codes to generate. Returns: list: A list of unique, secure voucher codes. alphabet = string.ascii_letters + string.digits codes = set() while len(codes) < num_codes: while True: code = \'\'.join(secrets.choice(alphabet) for i in range(12)) if (any(c.islower() for c in code) and any(c.isupper() for c in code) and any(c.isdigit() for c in code)): break codes.add(code) return list(codes)"},{"question":"Advanced Seaborn Plotting Objective: This task assesses your ability to manipulate and visualize data using seaborn’s object-oriented interface. You will be required to load a dataset, process it, and create a complex plot. Question: Using the seaborn library, create a comprehensive visualization to analyze the distribution of passenger ages on the Titanic, segmented by their survival status and gender. Your plot should provide clear insights into the age distribution among different groups. Requirements: 1. **Dataset**: - Load the `titanic` dataset from seaborn. 2. **Data Processing**: - Sort the dataset by the `alive` column in descending order. 3. **Visualization**: - Create a facet grid with sex as the facet. - Plot the age distribution using a histogram. - Use the `alive` status to adjust the opacity (alpha) of the bars. - Ensure histograms are stacked to avoid overlap. 4. **Plot Aesthetics**: - Use different colors for different `alive` statuses. - Set an appropriate bin width for the histogram. 5. **Code Implementation**: - Your final code should display the plot with clear axis labels and a legend. - Ensure the visualization is clear and informative. Input and Output Format: - **Input**: None (Dataset is loaded within the code) - **Output**: A plot displayed using seaborn. Constraints: - The code should be implemented using seaborn’s object-oriented interface (seaborn.objects). - You must use the `so.Plot`, `so.Bar`, `so.Count`, `so.Hist`, and `so.Stack` classes and methods as demonstrated in the documentation provided. Example Output: The expected output is a facet grid with histograms of passenger ages, segmented by survival status and gender, using stacking to avoid overlap. Each facet should represent a gender (`male` or `female`), and the bars within each facet should represent the age distribution for `alive` status. ```python # Example Code import seaborn.objects as so from seaborn import load_dataset # Load and sort the dataset titanic = load_dataset(\\"titanic\\").sort_values(\\"alive\\", ascending=False) # Create the plot plot = ( so.Plot(titanic, x=\\"age\\", alpha=\\"alive\\") .facet(\\"sex\\") .add(so.Bars(), so.Hist(binwidth=10), so.Stack()) ) # Display the plot plot.show() ``` Make sure your implementation is similar but includes any additional enhancements to make the visualization more insightful. Evaluation Criteria: - Correctness of the implemented code. - Proper usage of seaborn’s object-oriented interface. - Clarity and insightfulness of the visualization. - Code readability and comments explaining each step.","solution":"import seaborn.objects as so from seaborn import load_dataset def create_titanic_age_distribution_plot(): # Load and sort the Titanic dataset titanic = load_dataset(\\"titanic\\").sort_values(\\"alive\\", ascending=False) # Create the plot using seaborn object-oriented interface plot = ( so.Plot(titanic, x=\\"age\\", color=\\"alive\\", alpha=\\"alive\\") .facet(\\"sex\\") .add(so.Bars(), so.Hist(binwidth=10), so.Stack()) .label(title=\\"Age Distribution by Survival and Gender on the Titanic\\", x=\\"Age\\", y=\\"Count\\") ) # Display the plot plot.show()"},{"question":"# Description You are required to implement a Python program that performs concurrent file downloads using threading. Specifically, you should demonstrate the use of `threading`, `Queue`, and synchronization primitives (`Lock`) to manage concurrent downloads and ensure any log file updates are thread-safe. # Objectives 1. **Function Implementation**: - Implement a function `download_files(file_urls: List[str], download_dir: str, log_file: str) -> None` to concurrently download files from the provided URLs. - Each download should run on a separate thread. - Log the start and end time of each file download to a specified log file. 2. **Synchronization**: - Use threading primitives to handle concurrent downloads and ensure thread-safe logging. # Requirements - **Input**: - `file_urls`: A list of file URLs to download. - `download_dir`: Directory path where the downloaded files will be stored. - `log_file`: Path to a log file for logging download events. - **Output**: - The function should not return any value. It should write log entries to the specified log file. - **Constraints and Limitations**: - Ensure that the log file is updated in a thread-safe manner. - Implement retries for failed downloads (maximum of 3 attempts per file). - Ensure that all threads complete their execution before the function exits. # Performance Requirements - The function should efficiently manage a large number of URLs without running out of system resources. - Make use of thread pooling to manage threads if the number of URLs is exceedingly large. # Example ```python import os from typing import List import threading import requests from queue import Queue def download_files(file_urls: List[str], download_dir: str, log_file: str) -> None: def download_file(url): nonlocal download_count max_retries = 3 try: response = requests.get(url, stream=True) file_name = os.path.join(download_dir, os.path.basename(url)) with open(file_name, \'wb\') as file_handle: for chunk in response.iter_content(chunk_size=8192): if chunk: file_handle.write(chunk) with log_lock: with open(log_file, \'a\') as log_handle: log_handle.write(f\\"Downloaded: {url}n\\") except Exception as e: if max_retries > 0: max_retries -= 1 download_file(url) else: with log_lock: with open(log_file, \'a\') as log_handle: log_handle.write(f\\"Failed to download {url} after multiple attemptsn\\") n_threads = min(10, len(file_urls)) threads = [] log_lock = threading.Lock() queue = Queue() for url in file_urls: queue.put(url) for _ in range(n_threads): thread = threading.Thread(target=lambda q: q.get() and download_file(q.get()), args=(queue,)) thread.start() threads.append(thread) for thread in threads: thread.join() # Example usage file_urls = [ \\"http://example.com/file1.zip\\", \\"http://example.com/file2.zip\\" ] download_dir = \\"./downloads\\" log_file = \\"download.log\\" download_files(file_urls, download_dir, log_file) ``` # Explanation - The `download_files` function creates a thread for each file download task. - A `Queue` is used to manage the URLs to be downloaded. - A `Lock` is employed to ensure the logs are written to the file in a thread-safe manner. - Each thread retries a maximum of 3 times in case of a failure. - The function waits for all threads to complete before exiting.","solution":"import os from typing import List import threading import requests from queue import Queue def download_files(file_urls: List[str], download_dir: str, log_file: str) -> None: def download_file(url: str) -> None: max_retries = 3 attempt = 0 while attempt < max_retries: try: response = requests.get(url, stream=True) file_name = os.path.join(download_dir, os.path.basename(url)) with open(file_name, \'wb\') as file_handle: for chunk in response.iter_content(chunk_size=8192): if chunk: file_handle.write(chunk) with log_lock: with open(log_file, \'a\') as log_handle: log_handle.write(f\\"Downloaded: {url}n\\") break except Exception as e: attempt += 1 if attempt == max_retries: with log_lock: with open(log_file, \'a\') as log_handle: log_handle.write(f\\"Failed to download {url} after {max_retries} attemptsn\\") def worker() -> None: while True: url = queue.get() if url is None: break download_file(url) queue.task_done() if not os.path.exists(download_dir): os.makedirs(download_dir) queue = Queue() log_lock = threading.Lock() threads = [] for url in file_urls: queue.put(url) n_threads = min(10, len(file_urls)) for _ in range(n_threads): thread = threading.Thread(target=worker) thread.start() threads.append(thread) queue.join() for _ in range(n_threads): queue.put(None) for thread in threads: thread.join()"},{"question":"**Objective**: Implement a concurrent execution task manager using Python\'s `concurrent.futures` module. **Problem Statement**: You are responsible for implementing a concurrent task manager that will execute a series of tasks concurrently. Each task simulates a time-consuming operation (e.g., downloading a file, processing data). You need to use both thread-based and process-based parallelism to execute these tasks and compare their performance. **Instructions**: 1. Implement a class `TaskManager` with the following methods: - `__init__(self, tasks: List[Callable[[], Any]])`: Initialize the `TaskManager` with a list of tasks. Each task is a function that takes no arguments and returns a result. - `execute_with_threads(self) -> List[Any]`: Execute all tasks concurrently using threads and return a list of results. - `execute_with_processes(self) -> List[Any]`: Execute all tasks concurrently using processes and return a list of results. - `compare_performance(self) -> Tuple[float, float]`: Execute the tasks using both threads and processes, and return a tuple containing the execution times for threads and processes respectively. **Requirements**: - Use `concurrent.futures.ThreadPoolExecutor` for thread-based parallelism. - Use `concurrent.futures.ProcessPoolExecutor` for process-based parallelism. - Ensure that the results of the tasks are collected and returned in the same order as the tasks were provided. **Input**: - A list of task functions. Each function simulates a time-consuming operation and returns a result. **Output**: - For `execute_with_threads`, a list of results from the tasks executed using threads. - For `execute_with_processes`, a list of results from the tasks executed using processes. - For `compare_performance`, a tuple containing the execution times for threads and processes respectively. **Example**: ```python import time def task1(): time.sleep(1) return \\"Task 1 completed\\" def task2(): time.sleep(2) return \\"Task 2 completed\\" def task3(): time.sleep(1.5) return \\"Task 3 completed\\" tasks = [task1, task2, task3] manager = TaskManager(tasks) # Execute tasks with threads results_threads = manager.execute_with_threads() print(results_threads) # Output: [\\"Task 1 completed\\", \\"Task 2 completed\\", \\"Task 3 completed\\"] # Execute tasks with processes results_processes = manager.execute_with_processes() print(results_processes) # Output: [\\"Task 1 completed\\", \\"Task 2 completed\\", \\"Task 3 completed\\"] # Compare performance thread_time, process_time = manager.compare_performance() print(f\\"Threads execution time: {thread_time}\\") print(f\\"Processes execution time: {process_time}\\") ``` **Constraints**: - Each task will take between 1 and 3 seconds to complete. - There will be at most 10 tasks. - Ensure thread/process safety where necessary. **Notes**: - Consider handling exceptions that may arise during task execution to avoid premature termination. - Measure execution time using `time.perf_counter()` for accurate timing.","solution":"import concurrent.futures import time from typing import List, Callable, Any, Tuple class TaskManager: def __init__(self, tasks: List[Callable[[], Any]]): self.tasks = tasks def execute_with_threads(self) -> List[Any]: with concurrent.futures.ThreadPoolExecutor() as executor: futures = [executor.submit(task) for task in self.tasks] results = [future.result() for future in concurrent.futures.as_completed(futures)] return results def execute_with_processes(self) -> List[Any]: with concurrent.futures.ProcessPoolExecutor() as executor: futures = [executor.submit(task) for task in self.tasks] results = [future.result() for future in concurrent.futures.as_completed(futures)] return results def compare_performance(self) -> Tuple[float, float]: start_time = time.perf_counter() self.execute_with_threads() threads_time = time.perf_counter() - start_time start_time = time.perf_counter() self.execute_with_processes() processes_time = time.perf_counter() - start_time return (threads_time, processes_time)"},{"question":"# Coding Assessment Question Objective Your task is to demonstrate your proficiency in using various plotting functions available in the `pandas.plotting` module. You will be provided with a dataset and required to implement a function that generates different types of plots to analyze and visualize the data effectively. Problem Statement You are given a dataset `iris.csv` containing the famous Iris dataset with the following columns: - `sepal_length` - `sepal_width` - `petal_length` - `petal_width` - `species` # Task Implement a function `analyze_and_plot_iris(df: pd.DataFrame) -> None` that performs the following steps: 1. **Andrews Curves Plot**: Generate an Andrews curves plot for the dataset using the `species` column for coloring. 2. **Autocorrelation Plot**: Generate an autocorrelation plot for the `sepal_length` column. 3. **Bootstrap Plot**: Create a bootstrap plot for the `sepal_length` versus the `species`, showing the uncertainty of the mean. 4. **Boxplot**: Display a boxplot for the entire dataset. 5. **Lag Plot**: Generate a lag plot for the `petal_width` column. 6. **Scatter Matrix**: Create a scatter matrix of the dataset to visualize relationships between each pair of columns. # Constraints - The function should handle any typical errors, such as missing data or incorrect column names gracefully. - Use only functions from the `pandas.plotting` module for plotting. - The function should not return any output but should display the plots. # Input - `df (pd.DataFrame)`: A pandas DataFrame containing the Iris dataset. # Output - The function does not return any output. It should display the plots. # Example ```python import pandas as pd # Load the dataset df = pd.read_csv(\'iris.csv\') # Call the function to generate plots analyze_and_plot_iris(df) ``` # Note Ensure that you have the necessary libraries installed (`pandas` and `matplotlib`) to run the plotting functions.","solution":"import pandas as pd import matplotlib.pyplot as plt from pandas.plotting import andrews_curves, autocorrelation_plot, bootstrap_plot, boxplot, lag_plot, scatter_matrix def analyze_and_plot_iris(df: pd.DataFrame) -> None: Perform a series of plot analyses on the Iris dataset. Parameters: df (pd.DataFrame): The Iris dataset. # Handle missing data df = df.dropna() try: # Andrews Curves Plot plt.figure() andrews_curves(df, \'species\') plt.title(\'Andrews Curves Plot\') plt.show() # Autocorrelation Plot plt.figure() autocorrelation_plot(df[\'sepal_length\']) plt.title(\'Autocorrelation Plot of Sepal Length\') plt.show() # Bootstrap Plot plt.figure() bootstrap_plot(df, size=50, samples=500, color=\'grey\', column=\'sepal_length\') plt.title(\'Bootstrap Plot of Sepal Length\') plt.show() # Boxplot plt.figure() boxplot(df) plt.title(\'Boxplot of Iris Dataset\') plt.show() # Lag Plot plt.figure() lag_plot(df[\'petal_width\']) plt.title(\'Lag Plot of Petal Width\') plt.show() # Scatter Matrix plt.figure() scatter_matrix(df, alpha=0.2, figsize=(10, 10)) plt.title(\'Scatter Matrix of Iris Dataset\') plt.show() except KeyError as e: print(f\\"Column error: {e}\\") except Exception as e: print(f\\"An error occurred: {e}\\")"},{"question":"# Advanced Coding Assessment Question: Extracting and Transforming Data from Pickles You are given a directory containing multiple pickle files, each potentially containing complex nested data structures. Your task is to create a Python function that processes each pickle file to extract all integers that are accessed through a specific path and then double their value. The transformed integers should be stored in a new dictionary, maintaining the original structure but including only the paths where integers were found. Your function should have the following signature: ```python import os from typing import Dict def process_pickle_files(directory: str, path: list) -> Dict: pass ``` # Input: - `directory`: A string representing the path to the directory containing the pickle files. - `path`: A list of strings representing the path to follow within the nested structures in the pickle files. # Output: - A dictionary with the same nested structure as the original pickle files but including only the paths where integers were found, with each integer value doubled. # Constraints: 1. The pickle files can contain arbitrary nested data structures, including lists, dictionaries, and tuples. 2. The function should skip pickles that do not contain integers at the specified path. 3. The solution should handle large directories efficiently. # Example: Suppose you have a directory with the following files: - `file1.pickle` containing `{\\"a\\": {\\"b\\": [1, 2, 3], \\"c\\": 10}}` - `file2.pickle` containing `{\\"a\\": {\\"b\\": {\\"d\\": 4}, \\"c\\": 20}}` - `file3.pickle` containing `{\\"a\\": {\\"b\\": [10, \\"xyz\\"], \\"c\\": \\"abc\\"}}` And the specified path is `[\\"a\\", \\"b\\"]`. Your function should return: ```python { \\"file1.pickle\\": {\\"a\\": {\\"b\\": [2, 4, 6]}}, \\"file2.pickle\\": {\\"a\\": {\\"b\\": {\\"d\\": 8}}} } ``` In this example: - `file1.pickle` contains a list of integers at the specified path, so each integer is doubled. - `file2.pickle` contains an integer nested within a dictionary at the specified path, so it is doubled. - `file3.pickle` does not contain only integers at the specified path, so it is skipped. # Notes: - Use the `pickletools` module as part of your solution to analyze and traverse the pickles. - Ensure your implementation avoids executing pickle bytecode for safety.","solution":"import os import pickle from typing import Dict def process_pickle_files(directory: str, path: list) -> Dict: def get_nested_value(data, path): Retrieve the value in a nested dict/lists following the specified path. for key in path: if isinstance(data, dict): data = data.get(key, None) elif isinstance(data, list) and isinstance(key, int) and 0 <= key < len(data): data = data[key] else: return None return data def set_nested_value(data, path, value): Set the value in a nested dict/lists following the specified path. for key in path[:-1]: if isinstance(data, dict): data = data.setdefault(key, {}) elif isinstance(data, list) and isinstance(key, int) and 0 <= key < len(data): data = data[key] else: raise ValueError(\\"Invalid path\\") if isinstance(data, dict): data[path[-1]] = value elif isinstance(data, list) and isinstance(path[-1], int) and 0 <= path[-1] < len(data): data[path[-1]] = value else: raise ValueError(\\"Invalid path\\") result = {} for filename in os.listdir(directory): filepath = os.path.join(directory, filename) if not filepath.endswith(\'.pickle\'): continue with open(filepath, \'rb\') as file: data = pickle.load(file) nested_value = get_nested_value(data, path) if isinstance(nested_value, int): doubled_value = nested_value * 2 result_dict = {} set_nested_value(result_dict, path, doubled_value) result[filename] = result_dict elif isinstance(nested_value, list) and all(isinstance(i, int) for i in nested_value): doubled_values = [i * 2 for i in nested_value] result_dict = {} set_nested_value(result_dict, path, doubled_values) result[filename] = result_dict elif isinstance(nested_value, dict) and all(isinstance(v, int) for v in nested_value.values()): doubled_values = {k: v * 2 for k, v in nested_value.items()} result_dict = {} set_nested_value(result_dict, path, doubled_values) result[filename] = result_dict return result"},{"question":"# Base64 Encoding and Decoding Challenge You are tasked with implementing a function in Python to encode and decode messages using the Base64 encoding scheme, leveraging the `base64` module. The function should support both encoding strings to Base64 and decoding Base64 back to the original string. # Specifications: 1. Implement the following functions: - `encode_base64(message: str) -> str`: This function takes a string `message` and returns its Base64 encoded version as a string. - `decode_base64(encoded_message: str) -> str`: This function takes a Base64 encoded string `encoded_message` and returns the original decoded string. # Input: - The input to `encode_base64` will be a string containing characters in the ASCII range. - The input to `decode_base64` will be a Base64 encoded string. # Output: - The output of `encode_base64` should be a Base64 encoded string. - The output of `decode_base64` should be the original decoded string. # Constraints: - Both encoding and decoding operations should handle and raise appropriate exceptions if inputs are invalid or cannot be encoded/decoded. - You can assume that the input strings for encoding will always be valid ASCII strings. - The encoded string provided to `decode_base64` will be properly padded and valid. # Example: ```python encoded = encode_base64(\\"Hello, World!\\") print(encoded) # Output: \\"SGVsbG8sIFdvcmxkIQ==\\" decoded = decode_base64(\\"SGVsbG8sIFdvcmxkIQ==\\") print(decoded) # Output: \\"Hello, World!\\" ``` # Performance: - Ensure that the functions can handle reasonably large strings efficiently. # Implementation: Use the modern Base64 interface provided by the `base64` module. ```python import base64 def encode_base64(message: str) -> str: # Implement the encoding function here pass def decode_base64(encoded_message: str) -> str: # Implement the decoding function here pass ``` # Note: Test the functions thoroughly with various inputs to ensure robustness. Handle possible exceptions gracefully and ensure meaningful error messages are provided.","solution":"import base64 def encode_base64(message: str) -> str: Encodes a given message into its Base64 representation. Parameters: message (str): The input message string to encode. Returns: str: The Base64 encoded string. message_bytes = message.encode(\'ascii\') base64_bytes = base64.b64encode(message_bytes) return base64_bytes.decode(\'ascii\') def decode_base64(encoded_message: str) -> str: Decodes a given Base64 encoded string into its original message. Parameters: encoded_message (str): The Base64 encoded string to decode. Returns: str: The original decoded string. base64_bytes = encoded_message.encode(\'ascii\') message_bytes = base64.b64decode(base64_bytes) return message_bytes.decode(\'ascii\')"},{"question":"# Group Database Management with `grp` Module You are tasked with building a utility that helps manage and retrieve information from the Unix group database using the `grp` module. Function Specifications 1. **`get_group_info_by_gid(gid: int) -> dict`** Implement a function that takes a group ID (`gid`) and returns a dictionary with the following keys: - `name`: The name of the group. - `password`: The (encrypted) group password. - `gid`: The numerical group ID. - `members`: A list of usernames that are members of the group. If the group ID does not exist, raise a `KeyError` with a message `\\"Group ID not found\\"`. Input - `gid` (int): A numerical group ID. Output - (dict): A dictionary containing group information. Example ```python get_group_info_by_gid(1001) ``` 2. **`get_group_info_by_name(name: str) -> dict`** Implement a function that takes a group name (`name`) and returns a dictionary with the same structure as above. If the group name does not exist, raise a `KeyError` with a message `\\"Group name not found\\"`. Input - `name` (str): The name of the group. Output - (dict): A dictionary containing group information. Example ```python get_group_info_by_name(\'staff\') ``` 3. **`get_all_groups() -> list`** Implement a function that returns a list of dictionaries, each containing the information for one group in the database. Output - (list): A list of dictionaries, each representing a group. Example ```python get_all_groups() ``` Constraints - You must handle exceptions appropriately for non-existent group IDs or names. - Assume the Unix group database has a significant number of entries. Thus, your solutions should be optimized for performance. Use the `grp` module functions `grp.getgrgid(id)`, `grp.getgrnam(name)`, and `grp.getgrall()` to implement these functionalities. Hints - Remember to import the `grp` module at the beginning of your script. - Use exception handling to manage `KeyError` and `TypeError` where applicable.","solution":"import grp def get_group_info_by_gid(gid: int) -> dict: Returns a dictionary with group information for the group with the specified gid. try: group = grp.getgrgid(gid) return { \'name\': group.gr_name, \'password\': group.gr_passwd, \'gid\': group.gr_gid, \'members\': group.gr_mem } except KeyError: raise KeyError(\\"Group ID not found\\") def get_group_info_by_name(name: str) -> dict: Returns a dictionary with group information for the group with the specified name. try: group = grp.getgrnam(name) return { \'name\': group.gr_name, \'password\': group.gr_passwd, \'gid\': group.gr_gid, \'members\': group.gr_mem } except KeyError: raise KeyError(\\"Group name not found\\") def get_all_groups() -> list: Returns a list of dictionaries, each containing information for one group in the database. groups = grp.getgrall() return [{ \'name\': group.gr_name, \'password\': group.gr_passwd, \'gid\': group.gr_gid, \'members\': group.gr_mem } for group in groups]"},{"question":"# Question: Bisection Enhanced Search and Insert You are given a list of dictionaries, where each dictionary represents an employee record. Your task is to implement a class `EmployeeRecords` using the `bisect` module to manage the employee records in sorted order based on their salary. The class should support efficient insertion of new records and finding the top `k` employees with the highest salaries. **Requirements:** 1. Implement a class `EmployeeRecords` with the following methods: - `__init__(self)`: Initializes an empty list of employee records. - `add_record(self, employee: Dict[str, Any])`: Adds a new employee record while maintaining the list in sorted order based on the \'salary\'. - `top_k_highest_paid(self, k: int) -> List[Dict[str, Any]]`: Returns the top `k` employee records with the highest salaries in sorted order (highest to lowest). 2. Ensure the implementation uses the `bisect` module for maintaining the sorted order. **Constraints:** - Employee records are dictionaries with at least the keys \'name\' (str) and \'salary\' (int). - You can assume no two employees will have the same salary. - Must use `bisect` module functions for insertion and searching. **Performance Requirements:** - The insertion operation should take advantage of the `O(log n)` search provided by `bisect` but should be `O(n)` overall due to adjusting positions. - The retrieval of the top `k` records should be efficient. **Example:** ```python # Example usage records = EmployeeRecords() records.add_record({\\"name\\": \\"Alice\\", \\"salary\\": 50000}) records.add_record({\\"name\\": \\"Bob\\", \\"salary\\": 60000}) records.add_record({\\"name\\": \\"Charlie\\", \\"salary\\": 45000}) assert records.top_k_highest_paid(2) == [ {\\"name\\": \\"Bob\\", \\"salary\\": 60000}, {\\"name\\": \\"Alice\\", \\"salary\\": 50000} ] records.add_record({\\"name\\": \\"Dave\\", \\"salary\\": 65000}) assert records.top_k_highest_paid(3) == [ {\\"name\\": \\"Dave\\", \\"salary\\": 65000}, {\\"name\\": \\"Bob\\", \\"salary\\": 60000}, {\\"name\\": \\"Alice\\", \\"salary\\": 50000} ] ```","solution":"from bisect import insort_left class EmployeeRecords: def __init__(self): self.records = [] def add_record(self, employee: dict): insort_left(self.records, employee, key=lambda x: x[\'salary\']) def top_k_highest_paid(self, k: int): return sorted(self.records[-k:], key=lambda x: x[\'salary\'], reverse=True)"},{"question":"# Decision Tree Regression and Classification with Practical Implementation Background Decision Trees (DTs) are versatile machine learning models used for both classification and regression tasks. This question will assess your understanding of decision trees using the scikit-learn library by implementing a `DecisionTreeClassifier` and a `DecisionTreeRegressor`. Objective 1. Implement a `DecisionTreeClassifier` to classify the famous Iris dataset. 2. Implement a `DecisionTreeRegressor` to predict house prices on a given dataset. 3. Implement strategies to prevent overfitting and handle imbalanced classes. Dataset Use the Iris dataset for classification and an imaginary `house_prices` dataset for regression. - **Iris Dataset**: Can be loaded using `load_iris` from `sklearn.datasets`. - **House Prices Dataset**: The dataset contains features and the target variable `price`. Assume it is provided as follows: ```python house_prices = { \'features\': [[2104, 5, 1, 45], [1416, 3, 2, 40], [1534, 3, 2, 30], ...], \'target\': [460, 232, 315, ...] } ``` where each sublist in `features` represents `[size (sqft), num_bedrooms, num_bathrooms, age]`. Requirements 1. **DecisionTreeClassifier**: - Load the Iris dataset. - Split the dataset into training and testing subsets (80% train, 20% test). - Fit a `DecisionTreeClassifier` model to the training data. - Predict the classes for the test data. - Calculate the accuracy of the model. - Apply pruning to prevent overfitting by modifying `max_depth`, `min_samples_split`, or `min_samples_leaf`. 2. **DecisionTreeRegressor**: - Load the house price dataset. - Split the dataset into training and testing subsets (80% train, 20% test). - Fit a `DecisionTreeRegressor` model to the training data. - Predict prices for the test data. - Calculate the Mean Squared Error (MSE) of the predictions. - Implement a strategy to prevent overfitting similar to the classifier. 3. **Handling Imbalanced Classes** (Optional but preferred): - Implement a method to handle imbalanced classes in the Iris dataset. - One possible approach is to balance the class distribution by oversampling/undersampling. Function Signatures: ```python def decision_tree_classifier(): Implements DecisionTreeClassifier on the Iris dataset. Returns: float: Accuracy score of the classifier on the test set. pass def decision_tree_regressor(): Implements DecisionTreeRegressor on the house price dataset. Returns: float: Mean Squared Error of the regressor on the test set. pass def handle_imbalanced_classes(X, y): Handles imbalanced classes in a dataset. Parameters: X (list of list of float): Features of the dataset. y (list of int): Target class labels. Returns: list of list of float, list of int: Balanced features and target labels. pass ``` Example Usage: ```python # Classification accuracy = decision_tree_classifier() print(f\\"Classifier Accuracy: {accuracy}\\") # Regression mse = decision_tree_regressor() print(f\\"Regressor MSE: {mse}\\") # Handling Imbalanced Classes balanced_X, balanced_y = handle_imbalanced_classes(X, y) ``` Constraints - You may use scikit-learn and numpy for this implementation. - You should demonstrate strategies to control overfitting. - Ensure your code is clean, well-documented, and follows best practices.","solution":"from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor from sklearn.metrics import accuracy_score, mean_squared_error from sklearn.utils import resample import numpy as np def decision_tree_classifier(): # Load the Iris dataset iris = load_iris() X, y = iris.data, iris.target # Split the dataset into training and testing subsets (80% train, 20% test) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Initialize the DecisionTreeClassifier with pruning parameters clf = DecisionTreeClassifier(max_depth=3, min_samples_split=10, min_samples_leaf=5) # Fit the model to the training data clf.fit(X_train, y_train) # Predict the classes for the test data y_pred = clf.predict(X_test) # Calculate the accuracy of the model accuracy = accuracy_score(y_test, y_pred) return accuracy def decision_tree_regressor(): # Imaginary house price dataset house_prices = { \'features\': [ [2104, 5, 1, 45], [1416, 3, 2, 40], [1534, 3, 2, 30], [852, 2, 1, 36], [1947, 4, 2, 28], [956, 3, 1, 42], [1890, 4, 2, 12], [1250, 2, 1, 67], [2750, 4, 2, 8], [1721, 3, 2, 31]], \'target\': [460, 232, 315, 178, 375, 205, 311, 137, 438, 300] } X, y = np.array(house_prices[\'features\']), np.array(house_prices[\'target\']) # Split the dataset into training and testing subsets (80% train, 20% test) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Initialize the DecisionTreeRegressor with pruning parameters reg = DecisionTreeRegressor(max_depth=3, min_samples_split=10, min_samples_leaf=5) # Fit the model to the training data reg.fit(X_train, y_train) # Predict prices for the test data y_pred = reg.predict(X_test) # Calculate the Mean Squared Error (MSE) of the predictions mse = mean_squared_error(y_test, y_pred) return mse def handle_imbalanced_classes(X, y): unique_classes, counts_classes = np.unique(y, return_counts=True) max_count = max(counts_classes) X_resampled = [] y_resampled = [] for cls in unique_classes: X_class = X[y == cls] y_class = y[y == cls] X_upsampled, y_upsampled = resample(X_class, y_class, replace=True, n_samples=max_count, random_state=42) X_resampled.extend(X_upsampled) y_resampled.extend(y_upsampled) return np.array(X_resampled), np.array(y_resampled)"},{"question":"Comprehensive Visualization Dashboard with pandas # Objective: Create a comprehensive visualization dashboard using pandas and matplotlib to showcase multiple types of visualizations from a given dataset. This exercise is designed to assess your proficiency in data manipulation and visualization. # Dataset: You are provided with a dataset named `data.csv` which contains weather data with the following columns: - `Date`: The date of the record (format: YYYY-MM-DD) - `Temperature`: The temperature recorded on that date (in degrees Celsius) - `Humidity`: The humidity percentage recorded on that date - `WindSpeed`: The wind speed recorded on that date (in km/h) - `Precipitation`: The precipitation recorded on that date (in mm) - `WeatherType`: Categorical data indicating the type of weather (e.g., Sunny, Rainy, Snowy, etc.) # Tasks: 1. **Data Preparation**: - Read the dataset from `data.csv`. - Handle any missing values appropriately. 2. **Line Plot**: - Create a line plot showing the trend of `Temperature` over time. - Add appropriate labels for the x and y axes and a title for the plot. 3. **Bar Plot**: - Create a bar plot showing the average `Precipitation` for each type of `WeatherType`. - The bars should be color-coded based on `WeatherType`. 4. **Histogram**: - Create a histogram to visualize the distribution of `Humidity`. 5. **Box Plot**: - Create a box plot to show the distribution of `WindSpeed` for each type of `WeatherType`. 6. **Scatter Plot**: - Create a scatter plot to show the relationship between `Temperature` and `Humidity`. - Color the points based on `WeatherType`. 7. **Hexbin Plot**: - Create a hexbin plot to show the density of `Temperature` and `WindSpeed`. 8. **Pie Chart**: - Create a pie chart showing the proportion of each `WeatherType`. 9. **Parallel Coordinates Plot**: - Create a parallel coordinates plot to visualize the multi-variate data of weather records. Use `WeatherType` as the class to color the lines. # Constraints: - Utilize only pandas, matplotlib, and numpy libraries. - Ensure your plots are appropriately labeled and styled for clarity. - Your script should be well-documented and include explanations for your data handling and visualization choices. # Submission: Submit a Python script named `weather_visualization.py` that performs all the aforementioned tasks. Ensure your script can be run without errors and generates the visualizations correctly. # Example Usage: ```python python weather_visualization.py ``` This should generate and display all the requested plots.","solution":"import pandas as pd import matplotlib.pyplot as plt import numpy as np from pandas.plotting import parallel_coordinates def load_and_prepare_data(file_path): data = pd.read_csv(file_path) data[\'Date\'] = pd.to_datetime(data[\'Date\']) data = data.dropna() return data def plot_temperature_trend(data): plt.figure(figsize=(10, 5)) plt.plot(data[\'Date\'], data[\'Temperature\'], label=\'Temperature\', color=\'r\') plt.xlabel(\'Date\') plt.ylabel(\'Temperature (°C)\') plt.title(\'Temperature Trend Over Time\') plt.legend() plt.grid(True) plt.show() def plot_precipitation_by_weather(data): plt.figure(figsize=(10, 5)) avg_precipitation = data.groupby(\'WeatherType\')[\'Precipitation\'].mean() avg_precipitation.plot(kind=\'bar\', color=\'skyblue\') plt.xlabel(\'Weather Type\') plt.ylabel(\'Average Precipitation (mm)\') plt.title(\'Average Precipitation by Weather Type\') plt.grid(True) plt.show() def plot_humidity_distribution(data): plt.figure(figsize=(10, 5)) plt.hist(data[\'Humidity\'], bins=20, color=\'g\', edgecolor=\'k\', alpha=0.7) plt.xlabel(\'Humidity (%)\') plt.ylabel(\'Frequency\') plt.title(\'Distribution of Humidity\') plt.grid(True) plt.show() def plot_windspeed_by_weather(data): plt.figure(figsize=(10, 5)) data.boxplot(column=\'WindSpeed\', by=\'WeatherType\') plt.xlabel(\'Weather Type\') plt.ylabel(\'Wind Speed (km/h)\') plt.title(\'Wind Speed Distribution by Weather Type\') plt.suptitle(\'\') plt.grid(True) plt.show() def plot_temperature_vs_humidity(data): plt.figure(figsize=(10, 5)) markers = {\'Sunny\':\'o\', \'Rainy\':\'^\', \'Snowy\':\'s\'} for wtype in markers.keys(): subset = data[data[\'WeatherType\'] == wtype] plt.scatter(subset[\'Temperature\'], subset[\'Humidity\'], label=wtype, marker=markers[wtype]) plt.xlabel(\'Temperature (°C)\') plt.ylabel(\'Humidity (%)\') plt.title(\'Temperature vs Humidity\') plt.legend() plt.grid(True) plt.show() def plot_temperature_windspeed_hexbin(data): plt.figure(figsize=(10, 5)) plt.hexbin(data[\'Temperature\'], data[\'WindSpeed\'], gridsize=30, cmap=\'Blues\') plt.colorbar(label=\'Count\') plt.xlabel(\'Temperature (°C)\') plt.ylabel(\'Wind Speed (km/h)\') plt.title(\'Hexbin Plot of Temperature and Wind Speed\') plt.grid(True) plt.show() def plot_weather_type_proportion(data): plt.figure(figsize=(10, 5)) weather_counts = data[\'WeatherType\'].value_counts() plt.pie(weather_counts, labels=weather_counts.index, autopct=\'%1.1f%%\', startangle=140, colors=plt.cm.Paired(range(len(weather_counts)))) plt.title(\'Proportion of Weather Types\') plt.show() def plot_parallel_coordinates(data): plt.figure(figsize=(12, 6)) parallel_coordinates(data[[\'Temperature\', \'Humidity\', \'WindSpeed\', \'Precipitation\', \'WeatherType\']], class_column=\'WeatherType\', colormap=\'viridis\') plt.title(\'Parallel Coordinates Plot\') plt.xlabel(\'Features\') plt.ylabel(\'Values\') plt.grid(True) plt.show() if __name__ == \\"__main__\\": data = load_and_prepare_data(\'data.csv\') plot_temperature_trend(data) plot_precipitation_by_weather(data) plot_humidity_distribution(data) plot_windspeed_by_weather(data) plot_temperature_vs_humidity(data) plot_temperature_windspeed_hexbin(data) plot_weather_type_proportion(data) plot_parallel_coordinates(data)"},{"question":"Given the content, it\'s clear that the documentation primarily targets developers looking to understand and implement Python C extensions. Here\'s a coding question inspired by the explanation of object attributes management and the `PyTypeObject` struct: # Question: Implementing Custom Python Objects with Attributes in CPython You are required to implement a custom Python object in CPython with specific attributes and methods using the C API. This object will have a few basic attributes, some native methods, and should manage primary actions like setting and getting attributes. 1. **Define the custom type** with attributes: - `name` (string) - `value` (integer) 2. **Implement methods** to: - Retrieve the string representation of the object using `tp_str` such that it returns the format `\\"<name>: <value>\\"`. - Set and get the `name` and `value` attributes directly. 3. **Handle attribute access** correctly, ensuring that attribute setting respects read-only restrictions (i.e., make the `name` attribute read-only after initialization). 4. **Write appropriate constructor and destructor** for the type. Provide the necessary C code snippets to define this type and demonstrate creating an object of this type in Python. **Input/Output:** - **Input**: Attributes to set. - **Output**: The printed string representation when `str()` is called on the object. **Constraints:** - The implementation should handle exceptions correctly. ```python # Example creation and usage in Python import myextension obj = myextension.MyObject(\\"example\\", 10) print(str(obj)) # Output should be: example: 10 print(obj.name) # Output should be: example print(obj.value) # Output should be: 10 obj.value = 20 print(obj.value) # Output should be: 20 # Following should raise an exception because name is read-only after initialization obj.name = \\"test\\" ```","solution":"# Since we cannot run C code and Python extension in this notebook, let\'s focus on providing the Python-equivalent logic that matches the description. class MyObject: def __init__(self, name, value): self._name = name self.value = value @property def name(self): return self._name @property def value(self): return self._value @value.setter def value(self, value): self._value = value def __str__(self): return f\\"{self._name}: {self._value}\\" # The above class mimics the C extension object behavior in Python."},{"question":"You are given two datasets: 1. **Flights Dataset**: This dataset records the number of airline passengers who flew in each month from 1949 to 1960 and is in long-form format. 2. **Anagrams Dataset**: This dataset is from a psychology experiment where twenty subjects performed a memory task under two conditions (attention focused or divided). Your task is to write a Python function using seaborn that: 1. Transforms the **Anagrams Dataset** into a long-form structure and visualizes it using a `point` plot. 2. Visualizes data from the **Flights Dataset** using a `line` plot in both its long-form and wide-form representations. Input: - No input to be given directly to the function. Output: - The function should display three plots: - The `point` plot for the transformed Anagrams Dataset. - The `line` plot for the Flights Dataset in long-form representation. - The `line` plot for the Flights Dataset in wide-form representation. Data Description: - **Flights Dataset**: Columns are `year`, `month`, and `passengers`. - **Anagrams Dataset**: Columns are `subidr` (subject id), `attnr` (attention), `1solution`, `2solutions`, and `3solutions`. Constraints: - Use the seaborn functions described in the documentation to handle the data and plot the visualizations. - Ensure the plots have appropriate labels and legends for clarity. # Example Function ```python import seaborn as sns import pandas as pd def visualize_datasets(): # Load datasets flights = sns.load_dataset(\\"flights\\") anagrams = sns.load_dataset(\\"anagrams\\") # Transform Anagrams Dataset to long-form anagrams_long = anagrams.melt(id_vars=[\\"subidr\\", \\"attnr\\"], var_name=\\"solutions\\", value_name=\\"score\\") # Create point plot for Anagrams Dataset sns.catplot(data=anagrams_long, x=\\"solutions\\", y=\\"score\\", hue=\\"attnr\\", kind=\\"point\\") # Create line plot for Flights Dataset in long-form sns.relplot(data=flights, x=\\"year\\", y=\\"passengers\\", hue=\\"month\\", kind=\\"line\\") # Convert Flights Dataset to wide-form flights_wide = flights.pivot(index=\\"year\\", columns=\\"month\\", values=\\"passengers\\") # Create line plot for Flights Dataset in wide-form sns.relplot(data=flights_wide, kind=\\"line\\") return # Function call to see the plots visualize_datasets() ``` Make sure to test your function after implementation to ensure the plots are correctly generated.","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt def visualize_datasets(): # Set a seaborn style sns.set(style=\\"whitegrid\\") # Load the example datasets flights = sns.load_dataset(\\"flights\\") anagrams = sns.load_dataset(\\"anagrams\\") # Transform Anagrams Dataset to long-form anagrams_long = anagrams.melt(id_vars=[\\"subidr\\", \\"attnr\\"], var_name=\\"solutions\\", value_name=\\"score\\") # Create a point plot for Anagrams Dataset plt.figure(figsize=(10, 6)) sns.catplot(data=anagrams_long, x=\\"solutions\\", y=\\"score\\", hue=\\"attnr\\", kind=\\"point\\", markers=[\\"o\\", \\"x\\"], linestyles=[\\"-\\", \\"--\\"]) plt.title(\\"Point Plot for Anagrams Dataset\\") plt.xlabel(\\"Solutions\\") plt.ylabel(\\"Score\\") plt.show() # Create a line plot for Flights Dataset in long-form plt.figure(figsize=(10, 6)) sns.lineplot(data=flights, x=\\"year\\", y=\\"passengers\\", hue=\\"month\\", palette=\\"tab10\\") plt.title(\\"Line Plot for Flights Dataset (Long-form)\\") plt.xlabel(\\"Year\\") plt.ylabel(\\"Number of Passengers\\") plt.legend(title=\\"Month\\", bbox_to_anchor=(1.05, 1), loc=\'upper left\') plt.show() # Convert Flights Dataset to wide-form flights_wide = flights.pivot(index=\\"year\\", columns=\\"month\\", values=\\"passengers\\") # Create a line plot for Flights Dataset in wide-form plt.figure(figsize=(10, 6)) sns.lineplot(data=flights_wide) plt.title(\\"Line Plot for Flights Dataset (Wide-form)\\") plt.xlabel(\\"Year\\") plt.ylabel(\\"Number of Passengers\\") plt.legend(title=\\"Month\\", bbox_to_anchor=(1.05, 1), loc=\'upper left\') plt.show() # Function call to see the plots visualize_datasets()"},{"question":"<|Analysis Begin|> The documentation provided gives an overview of several modules in Python that deal with file and directory operations. These modules include: 1. `pathlib` - For object-oriented filesystem paths. 2. `os.path` - For common pathname manipulations. 3. `fileinput` - To iterate over lines from multiple input streams. 4. `stat` - For interpreting results from the `stat()` system call. 5. `filecmp` - For file and directory comparisons. 6. `tempfile` - To generate temporary files and directories. 7. `glob` - To perform Unix style pathname pattern expansion. 8. `fnmatch` - To do Unix filename pattern matching. 9. `linecache` - For random access to text lines. 10. `shutil` - For high-level file operations such as copying and archiving. The question should engage students with practical tasks involving these modules to assess their understanding of file and directory manipulation using the provided tools. A good coding assessment would involve a combination of these modules. <|Analysis End|> <|Question Begin|> **Question: File Manager with Path Operations** You are tasked with creating a file manager program that performs several file and directory operations. Your program should be implemented in Python and should utilize relevant modules mentioned in the provided documentation. # Function: `file_manager` Input: - `paths` (List[str]): A list of file or directory paths. - `operations` (List[Tuple[str, Any]]): A list of operations to perform. Each operation is a tuple where the first element is the operation name (str) and the subsequent elements are operation-specific arguments. Operations: 1. `\\"compare\\"`: Compare the contents of two files. - Arguments: source_path (str), target_path (str) - Return: True if files are the same, otherwise False. 2. `\\"copy\\"`: Copy a file from source_path to target_path. - Arguments: source_path (str), target_path (str) - Return: None. 3. `\\"move\\"`: Move a file from source_path to target_path. - Arguments: source_path (str), target_path (str) - Return: None. 4. `\\"delete\\"`: Delete a file at the specified path. - Arguments: file_path (str) - Return: None. 5. `\\"list\\"`: List all files in a directory matching a specific pattern. - Arguments: dir_path (str), pattern (str) - Return: List of matching file paths. 6. `\\"create_temp\\"`: Create a temporary file and write some text into it. - Arguments: text (str) - Return: Path of the created temporary file (str). 7. `\\"path_info\\"`: Return the parts of a file path. - Arguments: file_path (str) - Return: Dictionary with keys \'name\', \'parent\', and \'suffix\'. Output: - The results according to the specified operations. # Example: ```python paths = [\\"/path/to/file1.txt\\", \\"/path/to/file2.txt\\", \\"/path/to/directory\\"] operations = [ (\\"compare\\", \\"/path/to/file1.txt\\", \\"/path/to/file2.txt\\"), (\\"copy\\", \\"/path/to/file1.txt\\", \\"/path/to/file3.txt\\"), (\\"move\\", \\"/path/to/file3.txt\\", \\"/path/to/file4.txt\\"), (\\"delete\\", \\"/path/to/file4.txt\\"), (\\"list\\", \\"/path/to/directory\\", \\"*.txt\\"), (\\"create_temp\\", \\"Temporary text\\"), (\\"path_info\\", \\"/path/to/file1.txt\\") ] def file_manager(paths, operations): # Your code here ``` # Constraints: 1. Each `path` in the `paths` list is a valid path. 2. Ensure to handle any exceptions that might occur during file operations gracefully. # Note: - Use the `pathlib`, `shutil`, `filecmp`, `tempfile`, and `glob` modules from the provided documentation to implement the respective functionalities. - Focus on efficiency where applicable, particularly for file and directory operations. Implement the `file_manager` function and test it with the provided example.","solution":"from pathlib import Path import shutil import filecmp import tempfile import glob def file_manager(paths, operations): results = [] for operation in operations: if operation[0] == \\"compare\\": source_path = Path(operation[1]) target_path = Path(operation[2]) results.append(filecmp.cmp(source_path, target_path, shallow=False)) elif operation[0] == \\"copy\\": source_path = Path(operation[1]) target_path = Path(operation[2]) shutil.copy(source_path, target_path) results.append(None) elif operation[0] == \\"move\\": source_path = Path(operation[1]) target_path = Path(operation[2]) shutil.move(source_path, target_path) results.append(None) elif operation[0] == \\"delete\\": file_path = Path(operation[1]) file_path.unlink() results.append(None) elif operation[0] == \\"list\\": dir_path = Path(operation[1]) pattern = operation[2] matched_files = [str(p) for p in dir_path.glob(pattern)] results.append(matched_files) elif operation[0] == \\"create_temp\\": text = operation[1] with tempfile.NamedTemporaryFile(delete=False, mode=\'w\') as temp_file: temp_file.write(text) temp_path = temp_file.name results.append(temp_path) elif operation[0] == \\"path_info\\": file_path = Path(operation[1]) path_info = { \\"name\\": file_path.name, \\"parent\\": str(file_path.parent), \\"suffix\\": file_path.suffix } results.append(path_info) return results"},{"question":"Title: Analyzing Customer Purchase Data with Sparse Data Frames Background: You are given a large dataset representing customer purchases over time in a retail store. The dataset consists of numerous columns with millions of entries, where most values are zeros indicating no purchase, and non-zero entries represent the quantity of items bought. To optimize memory usage, you want to use sparse data structures to handle the dataset and perform various analyses. Objective: Implement a function `analyze_customer_data` with the following signature: ```python def analyze_customer_data(df: pd.DataFrame) -> Tuple[pd.DataFrame, float, scipy.sparse.coo_matrix]: pass ``` Inputs: - `df`: A pandas DataFrame with numeric columns representing customer purchases. Most entries are zeros. Outputs: - A tuple containing: 1. A pandas DataFrame, which is a sparse representation of the input `df`. 2. The density of the sparse DataFrame. 3. A `scipy.sparse.coo_matrix` conversion of the sparse DataFrame. Tasks: 1. **Convert the DataFrame to a Sparse DataFrame**, replacing zeros with the sparse fill value. 2. **Calculate and return the density** of the sparse DataFrame. 3. **Convert the Sparse DataFrame to a `scipy.sparse.coo_matrix`** and return it as part of the output. Constraints: 1. The input DataFrame (`df`) is large, with dimensions potentially exceeding (1000000, 100), making memory efficiency crucial. 2. Use suitable pandas and scipy functions to manipulate and convert the DataFrame. Example: ```python import pandas as pd import numpy as np import scipy # Sample DataFrame data = np.array([ [0, 0, 0, 2], [0, 3, 0, 0], [5, 0, 0, 1], [0, 0, 0, 0], [0, 0, 0, 0] ]) df = pd.DataFrame(data) sparse_df, density, coo_matrix = analyze_customer_data(df) print(sparse_df) print(f\\"Density: {density}\\") print(coo_matrix) ``` Expected Output: ```plaintext 0 1 2 3 0 0 0 0 2 1 0 3 0 0 2 5 0 0 1 3 0 0 0 0 4 0 0 0 0 Density: 0.2 (0, 3) 2 (1, 1) 3 (2, 0) 5 (2, 3) 1 ``` Provide the implementation of the function `analyze_customer_data`.","solution":"import pandas as pd import numpy as np from scipy.sparse import coo_matrix from typing import Tuple def analyze_customer_data(df: pd.DataFrame) -> Tuple[pd.DataFrame, float, coo_matrix]: Analyzes customer purchase data by converting to sparse DataFrame, calculating density, and creating a coo_matrix. Parameters: df (pd.DataFrame): The input DataFrame with customer purchase data. Returns: Tuple[pd.DataFrame, float, coo_matrix]: A tuple containing the sparse DataFrame, its density, and the coo_matrix representation. # Convert DataFrame to sparse sparse_df = df.astype(pd.SparseDtype(\\"float\\", fill_value=0.0)) # Calculate density total_elements = df.size non_zero_elements = (df != 0).sum().sum() density = non_zero_elements / total_elements # Convert sparse DataFrame to coo_matrix coo = coo_matrix(df.values) return sparse_df, density, coo"},{"question":"# PyTorch Coding Assessment Question Objective Implement a function that normalizes an input tensor using a specified mean and standard deviation, and then validate your implementation using PyTorch\'s testing utilities. Problem Statement You are required to write a function `normalize_tensor` that normalizes an input tensor using a specified mean and standard deviation. You should then write a test function `test_normalize_tensor` to validate your implementation using PyTorch\'s testing utilities. Function Signature ```python import torch def normalize_tensor(tensor: torch.Tensor, mean: float, std: float) -> torch.Tensor: Normalizes the input tensor using the specified mean and standard deviation. Arguments: tensor -- input torch.Tensor mean -- float, the mean value for normalization std -- float, the standard deviation value for normalization Returns: normalized_tensor -- torch.Tensor, the normalized tensor pass def test_normalize_tensor(): Tests the normalize_tensor function using PyTorch\'s testing utilities. pass ``` Input 1. `tensor` - A torch.Tensor of any shape. 2. `mean` - A float value representing the mean for normalization. 3. `std` - A float value representing the standard deviation for normalization. Output 1. A tensor of the same shape as the input, normalized using the formula: [ text{normalized_tensor} = frac{text{tensor} - text{mean}}{text{std}} ] Constraints - You can assume `std` is non-zero. - The input tensor can have any number of elements, but it will not be empty. Example ```python >>> tensor = torch.tensor([1.0, 2.0, 3.0]) >>> mean = 2.0 >>> std = 1.0 >>> normalize_tensor(tensor, mean, std) tensor([-1.0, 0.0, 1.0]) ``` Testing your implementation Implement the `test_normalize_tensor` function to validate the correctness of your `normalize_tensor` function. You should use `torch.testing`\'s `assert_close` or `assert_allclose` to ensure your function produces the expected results. The test should: 1. Create a tensor using `torch.testing.make_tensor`. 2. Normalize it using your `normalize_tensor` function. 3. Manually calculate the expected normalized tensor. 4. Use `torch.testing.assert_close` or `torch.testing.assert_allclose` to validate the output against the expected tensor. Example of `test_normalize_tensor` (not complete, for reference only): ```python def test_normalize_tensor(): tensor = torch.testing.make_tensor((10,)) mean = 0.5 std = 0.2 normalized = normalize_tensor(tensor, mean, std) expected_normalized = (tensor - mean) / std torch.testing.assert_close(normalized, expected_normalized) ``` Ensure your test function covers both typical and edge cases comprehensively.","solution":"import torch def normalize_tensor(tensor: torch.Tensor, mean: float, std: float) -> torch.Tensor: Normalizes the input tensor using the specified mean and standard deviation. Arguments: tensor -- input torch.Tensor mean -- float, the mean value for normalization std -- float, the standard deviation value for normalization Returns: normalized_tensor -- torch.Tensor, the normalized tensor return (tensor - mean) / std"},{"question":"# Task: You are required to implement a Python function that interacts with the CPython API to perform operations on tuples. Given a list of Python objects, your task is to: 1. Create a tuple containing these objects. 2. Extract a slice from the created tuple. 3. Modify an element in the created tuple at a given position. 4. Return both the original and the modified tuples. # Function Signature: ```python def manipulate_tuple(py_objects: list, slice_indexes: tuple, position: int, new_element) -> tuple: - py_objects: A list of Python objects to be included in the tuple. - slice_indexes: A tuple of two integers indicating the start and end index for the slice. - position: An integer indicating the position in the tuple to be modified. - new_element: A Python object to replace the element at the specified position in the tuple. Returns a tuple containing two elements: - The original tuple created. - The modified tuple after element replacement. ``` # Constraints: 1. The `py_objects` list will have at least one element. 2. The `slice_indexes` should be valid range within the bounds of the created tuple. 3. `position` should be a valid index within the bounds of the created tuple. 4. Use the CPython API functions for handling tuples (`PyTuple_New`, `PyTuple_GetSlice`, `PyTuple_SetItem`, etc.). # Example Usage: ```python objects = [1, 2, 3, 4, 5] slice_indexes = (1, 4) position = 2 new_element = 99 original_tuple, modified_tuple = manipulate_tuple(objects, slice_indexes, position, new_element) # Expected output: # Original tuple: (1, 2, 3, 4, 5) # Slice of the tuple: (2, 3, 4) # Modified tuple: (1, 2, 99, 4, 5) ``` # Notes: - Ensure to handle memory management correctly as per CPython\'s reference counting mechanism. - You may use Python\'s `ctypes` or `cffi` libraries to interface with the CPython API for this task.","solution":"from ctypes import py_object, POINTER, cast, c_ssize_t import ctypes def manipulate_tuple(py_objects: list, slice_indexes: tuple, position: int, new_element): Function to manipulate a tuple using CPython API. Arguments: py_objects -- A list of Python objects to be included in the tuple. slice_indexes -- A tuple of two integers indicating the start and end index for the slice. position -- An integer indicating the position in the tuple to be modified. new_element -- A Python object to replace the element at the specified position in the tuple. Returns a tuple containing two elements: - The original tuple created. - The modified tuple after element replacement. # Converting the list to a tuple original_tuple = tuple(py_objects) # Extracting the slice using regular slicing (since ctypes doesn\'t directly support this) sliced_tuple = original_tuple[slice_indexes[0]:slice_indexes[1]] # Creating a copy of the original tuple to modify modified_tuple = list(original_tuple) modified_tuple[position] = new_element modified_tuple = tuple(modified_tuple) return original_tuple, modified_tuple"},{"question":"# Custom Seaborn Palette for Data Visualization **Objective:** Demonstrate your understanding of Seaborn\'s `hls_palette` function by creating a customized color palette and applying it to a dataset visualization. **Instructions:** 1. Import the Seaborn library and load the `tips` dataset from Seaborn\'s built-in datasets. 2. Generate a custom color palette with the following specifications: - 10 colors. - Lightness of 0.5. - Saturation of 0.6. - Start hue of 0.3. 3. Create a bar plot of the total bill amounts grouped by the day of the week using the bar plot function of Seaborn. 4. Apply the customized palette to this plot. **Constraints:** - Use Seaborn\'s `hls_palette` function to create the color palette. - Ensure that the bar plot appropriately utilizes the custom color palette. **Expected Input and Output:** - *Input*: No explicit input is required; the `tips` dataset will be used internally. - *Output*: A bar plot of total bill amounts by day, colored using a custom Seaborn palette. **Code Template:** ```python import seaborn as sns import matplotlib.pyplot as plt # Step 1: Import Seaborn and load the \'tips\' dataset tips = sns.load_dataset(\'tips\') # Step 2: Generate a custom color palette custom_palette = sns.hls_palette(10, l=0.5, s=0.6, h=0.3) # Step 3: Create a bar plot of total bill amounts grouped by the day plt.figure(figsize=(10, 6)) sns.barplot(x=\'day\', y=\'total_bill\', data=tips, palette=custom_palette) # Show the plot plt.show() ``` **Notes:** - Ensure that you thoroughly test your solution. - Include clear comments explaining your code.","solution":"import seaborn as sns import matplotlib.pyplot as plt def custom_palette_barplot(): Generates a customized bar plot of total bill amounts grouped by the day of the week using a custom Seaborn color palette. # Step 1: Import Seaborn and load the \'tips\' dataset tips = sns.load_dataset(\'tips\') # Step 2: Generate a custom color palette custom_palette = sns.hls_palette(10, l=0.5, s=0.6, h=0.3) # Step 3: Create and display the bar plot using the custom color palette plt.figure(figsize=(10, 6)) sns.barplot(x=\'day\', y=\'total_bill\', data=tips, palette=custom_palette) # Show the plot plt.show()"},{"question":"Task You are required to write a function that emulates the behavior of the old `__import__` function using the modern `importlib` module. The function should take a module name as a string and return the imported module object. If the module is not found, the function should raise an appropriate ImportError. Requirements 1. The function should be named `custom_import`. 2. The function should accept one parameter: - `name` (str): The name of the module to be imported. 3. The function should raise an `ImportError` if the module cannot be found. 4. You must use the `importlib` module for this task. 5. Do not use the deprecated `imp` module. Example ```python # Example usage # Assuming the \'math\' module is present imported_module = custom_import(\'math\') print(imported_module.sqrt(9)) # Output should be 3.0 # If the module does not exist try: custom_import(\'non_existent_module\') except ImportError as e: print(e) # Output should be an appropriate ImportError message ``` Constraints - You may not use the `imp` module or any other deprecated methods. - You must handle potential errors gracefully and ensure that any resources are properly managed. - Assume the module name provided will always be a valid string (i.e., no need to check the type of the input). Implement the `custom_import` function to successfully complete the task.","solution":"import importlib def custom_import(name): Emulates the behavior of the old __import__ function using importlib. Parameters: name (str): The name of the module to be imported. Returns: module: The imported module object. Raises: ImportError: If the module cannot be found. try: module = importlib.import_module(name) return module except ModuleNotFoundError: raise ImportError(f\\"No module named \'{name}\'\\")"},{"question":"**Objective:** To assess your understanding of data generation, pre-processing, and model training using scikit-learn. # Task: You are provided with a task to create a synthetic dataset for a regression problem, preprocess the data, and train a regression model. Please follow the steps below: 1. **Generate a Synthetic Dataset:** - Use `sklearn.datasets.make_regression` to create a dataset with 1000 samples and 15 features. - Introduce noise into the dataset with a noise level of 0.2. 2. **Preprocess the Data:** - Split the dataset into training and test sets using an 80-20 split. - Standardize the feature values using `sklearn.preprocessing.StandardScaler`. 3. **Train a Regression Model:** - Train a `GradientBoostingRegressor` model from scikit-learn on the training set with the following parameters: - `n_estimators` = 100 - `max_depth` = 3 - `random_state` = 42 - Evaluate the model by calculating the R^2 score on the test set. 4. **Handling a Specific Scenario:** - Train another `GradientBoostingRegressor` model using the parameter `n_iter_no_change` set to 10. Report any warnings or issues that arise during model fitting. # Function Signature: You are required to implement the following function: ```python def regression_with_gb_regressor(): import numpy as np from sklearn.datasets import make_regression from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.ensemble import GradientBoostingRegressor from sklearn.metrics import r2_score # Step 1: Generate synthetic dataset X, y = make_regression(n_samples=1000, n_features=15, noise=0.2, random_state=42) # Step 2: Split the dataset X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Step 2: Preprocess the data (standardize features) scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) # Step 3: Train a GradientBoostingRegressor (default settings first) gbr_default = GradientBoostingRegressor(n_estimators=100, max_depth=3, random_state=42) gbr_default.fit(X_train, y_train) y_pred_default = gbr_default.predict(X_test) r2_default = r2_score(y_test, y_pred_default) # Step 3: Train another GradientBoostingRegressor with n_iter_no_change=10 gbr_custom = GradientBoostingRegressor(n_estimators=100, max_depth=3, random_state=42, n_iter_no_change=10) gbr_custom.fit(X_train, y_train) y_pred_custom = gbr_custom.predict(X_test) r2_custom = r2_score(y_test, y_pred_custom) # Check for warnings (students should observe the output manually) # Return R^2 scores of both models return r2_default, r2_custom ``` # Constraints: - The solution should use the scikit-learn library for data generation, pre-processing, and model training. - You should observe if any warnings are raised during the model training with the different `n_iter_no_change` parameter. # Performance Requirements: - Ensure the function runs efficiently within a reasonable time frame. **Expected Output:** Your function should return a tuple with the R^2 scores of the two models. Please observe and report any warnings or issues during the process.","solution":"def regression_with_gb_regressor(): import numpy as np from sklearn.datasets import make_regression from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.ensemble import GradientBoostingRegressor from sklearn.metrics import r2_score # Step 1: Generate synthetic dataset X, y = make_regression(n_samples=1000, n_features=15, noise=0.2, random_state=42) # Step 2: Split the dataset X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Step 2: Preprocess the data (standardize features) scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) # Step 3: Train a GradientBoostingRegressor (default settings first) gbr_default = GradientBoostingRegressor(n_estimators=100, max_depth=3, random_state=42) gbr_default.fit(X_train, y_train) y_pred_default = gbr_default.predict(X_test) r2_default = r2_score(y_test, y_pred_default) # Step 3: Train another GradientBoostingRegressor with n_iter_no_change=10 gbr_custom = GradientBoostingRegressor(n_estimators=100, max_depth=3, random_state=42, n_iter_no_change=10) gbr_custom.fit(X_train, y_train) y_pred_custom = gbr_custom.predict(X_test) r2_custom = r2_score(y_test, y_pred_custom) # Check for warnings (students should observe the output manually) # Return R^2 scores of both models return r2_default, r2_custom"},{"question":"Objective The goal of this task is to evaluate your understanding of PyTorch\'s profiling capabilities, specifically how to use the `torch.autograd.profiler` module to monitor and profile the performance of PyTorch operations. Problem Statement You are tasked with writing a Python function that profiles the execution time of a sequence of PyTorch operations. Your function should: 1. Accept a list of operations, where each operation is represented as a function that takes a list of tensors as input and returns a tensor as output. 2. Apply these operations sequentially on an initial input tensor. 3. Use `torch.autograd.profiler` to record the time taken by each operation. 4. Return a dictionary containing the operation names and their respective execution times in milliseconds. Input and Output Formats - **Input**: - `operations`: A list of tuples, where each tuple contains: - A string representing the operation name. - A function that performs the operation and accepts a list of tensors as input. - `input_tensor`: The initial tensor to which the operations will be applied. - **Output**: - A dictionary where: - The keys are the operation names. - The values are the execution times of the corresponding operations in milliseconds. Function Signature ```python def profile_operations(operations: list, input_tensor: torch.Tensor) -> dict: pass ``` Example ```python import torch # Example operations def op1(tensors): return tensors[0] + tensors[1] def op2(tensors): return tensors[0].matmul(tensors[1]) # Input tensor and operations input_tensor = torch.randn(100, 100) operations = [ (\\"addition\\", op1), (\\"matrix_multiplication\\", op2) ] # Profiling the operations profiled_times = profile_operations(operations, [input_tensor, input_tensor]) print(profiled_times) # Expected output: {\'addition\': <time_in_ms>, \'matrix_multiplication\': <time_in_ms>} ``` Constraints - The function should handle at least up to 10 operations. - Operations may involve tensors of varying sizes. - Ensure efficiency in terms of the profiling and execution. Notes - The input list `operations` will contain non-empty valid functions and operation names. - The initial input tensor will be a valid PyTorch tensor. - You may use standard libraries such as `torch` and `time`. Performance Requirements - The profiling should introduce minimal overhead to the operation execution. - Ensure the reported times are as accurate as possible. Additional Information Refer to the [PyTorch Documentation on Profiler](https://pytorch.org/docs/stable/autograd.html#torch.autograd.profiler.profile) for more details on using `torch.autograd.profiler`.","solution":"import torch import time def profile_operations(operations, input_tensors): Profiles the execution time of a sequence of PyTorch operations. Parameters: operations (list): A list of tuples, where each tuple contains: - A string representing the operation name. - A function that performs the operation and accepts a list of tensors as input. input_tensors (list): List of initial tensors to which the operations will be applied. Returns: dict: A dictionary where the keys are operation names and the values are the execution times in milliseconds. timings = {} for name, operation in operations: with torch.autograd.profiler.profile(use_cuda=False) as prof: start_time = time.time() _ = operation(input_tensors) end_time = time.time() timings[name] = (end_time - start_time) * 1000 # convert to milliseconds return timings"},{"question":"# Advanced Control Flow and Structural Constructs Problem Statement Your task is to implement a function that simulates a custom vending machine. The vending machine should have the following functionalities: 1. **Item Management:** - Add new items with specific quantities and prices. - Remove items when their stock reaches zero or the user decides to remove them. - Update the quantity of existing items. 2. **Transaction Handling:** - Dispense items based on user input. - Handle transactions with proper exception handling (e.g., insufficient balance, item out of stock). - Use the `with` statement to manage transactions that ensure any changes are reverted in case of failure. 3. **Pattern Matching for Commands:** - Use the `match` statement to process different user commands (e.g., \\"ADD\\", \\"REMOVE\\", \\"UPDATE\\", \\"DISPENSE\\"). 4. **Function and Class Definitions:** - Use class definitions to model the vending machine and its items. - Use function definitions for transaction handling, item management, and command processing. Function Signature ```python class VendingMachine: def __init__(self): pass def add_item(self, item_name: str, quantity: int, price: float) -> None: pass def remove_item(self, item_name: str) -> None: pass def update_quantity(self, item_name: str, quantity: int) -> None: pass def dispense_item(self, item_name: str, user_balance: float) -> float: pass def process_command(self, command: str) -> str: pass # Example usage vending_machine = VendingMachine() vending_machine.process_command(\\"ADD:Coke:10:1.5\\") vending_machine.process_command(\\"DISPENSE:Coke:2.0\\") ``` Constraints - The function `process_command` should use the `match` statement to handle different commands. - The `dispense_item` function should handle insufficient balance and out-of-stock scenarios using exception handling with `try` and `except`. - Use the `with` statement in relevant methods to ensure consistency in case of errors. Example ```python # Example commands vending_machine.process_command(\\"ADD:Chips:5:2.0\\") # Adds 5 Chips each priced at 2.0 vending_machine.process_command(\\"UPDATE:Chips:10\\") # Updates the quantity of Chips to 10 vending_machine.process_command(\\"DISPENSE:Chips:5.0\\") # Dispense Chips if the balance is sufficient ``` # Evaluation Criteria 1. Correct use of `match` statements for command processing. 2. Proper handling of exceptions using `try` and `except`. 3. Accurate implementation of item management and transaction functionalities. 4. Effective use of class and function definitions to encapsulate the behavior of the vending machine. 5. Clear and clean code structure with comments explaining the logic.","solution":"class Item: def __init__(self, name: str, quantity: int, price: float): self.name = name self.quantity = quantity self.price = price class VendingMachine: def __init__(self): self.items = {} def add_item(self, item_name: str, quantity: int, price: float) -> None: if item_name in self.items: self.items[item_name].quantity += quantity else: self.items[item_name] = Item(item_name, quantity, price) def remove_item(self, item_name: str) -> None: if item_name in self.items: del self.items[item_name] def update_quantity(self, item_name: str, quantity: int) -> None: if item_name in self.items: self.items[item_name].quantity = quantity if self.items[item_name].quantity <= 0: self.remove_item(item_name) def dispense_item(self, item_name: str, user_balance: float) -> float: if item_name not in self.items: raise ValueError(\\"Item not available.\\") item = self.items[item_name] if item.quantity <= 0: raise ValueError(\\"Item is out of stock.\\") if user_balance < item.price: raise ValueError(\\"Insufficient balance.\\") with self.manage_transaction(item): return user_balance - item.price from contextlib import contextmanager @contextmanager def manage_transaction(self, item: Item): original_quantity = item.quantity try: item.quantity -= 1 yield except: item.quantity = original_quantity raise def process_command(self, command: str) -> str: parts = command.split(\\":\\") action = parts[0].upper() match action: case \\"ADD\\": item_name, quantity, price = parts[1], int(parts[2]), float(parts[3]) self.add_item(item_name, quantity, price) return f\\"Added {quantity} of {item_name} at {price} each.\\" case \\"REMOVE\\": item_name = parts[1] self.remove_item(item_name) return f\\"Removed {item_name}.\\" case \\"UPDATE\\": item_name, quantity = parts[1], int(parts[2]) self.update_quantity(item_name, quantity) return f\\"Updated {item_name} quantity to {quantity}.\\" case \\"DISPENSE\\": item_name, user_balance = parts[1], float(parts[2]) try: remaining_balance = self.dispense_item(item_name, user_balance) return f\\"Dispensed {item_name}. Remaining balance: {remaining_balance}\\" except ValueError as e: return str(e) case _: return \\"Invalid command.\\""},{"question":"# Custom Audit Hook Implementation **Objective**: Implement a custom audit hook handler that monitors specific audit events and logs them to a specified log file. **Description**: Python\'s `sys.audit()` and `PySys_AddAuditHook()` functions allow the monitoring of various events raised during the execution of the Python runtime and standard library. You are required to create a custom audit hook that logs specific events to a log file. **Task**: 1. Implement a function `audit_event_handler(event, args)` that will handle audit events. 2. Register this function as an audit hook using `sys.addaudithook()`. 3. The handler should monitor the following events: - `os.system` - `socket.connect` - `shutil.copyfile` 4. Log each of these events to a log file named `audit.log` in the following format: ``` TIMESTAMP - EVENT: Argument1, Argument2, ... ``` Here, `TIMESTAMP` is the current date and time when the event was logged, and `EVENT` is the name of the audit event. **Constraints**: - Use the `datetime` module to get the current time. - Ensure thread safety while writing to the log file. - If the same event occurs multiple times, it should be logged each time. **Example**: If a call to `os.system(\\"ls\\")` is made, the log file should contain an entry similar to: ``` 2023-10-01 12:00:00 - os.system: ls ``` If a connection is made using `socket.connect((\'example.com\', 80))`, it should log: ``` 2023-10-01 12:01:00 - socket.connect: (\'example.com\', 80) ``` **Input and Output**: - This task does not require any input from the user. - The output will be the `audit.log` file containing the logged events. **Hints**: - Remember to open the log file in append mode to ensure that all events are recorded without overwriting the previous ones. - Use appropriate synchronization mechanisms to ensure that log entries are not corrupted when multiple threads are logging events simultaneously. Implement the solution in the function `monitor_audit_events()` as below: ```python import sys import os import shutil import socket from datetime import datetime import threading def audit_event_handler(event, args): # Implement this function to handle and log the required events pass def monitor_audit_events(): # Register the audit_event_handler function as an audit hook sys.addaudithook(audit_event_handler) # You can add example calls to demonstrate the logging here os.system(\\"echo Hello World\\") s = socket.socket(socket.AF_INET, socket.SOCK_STREAM) s.connect((\\"example.com\\", 80)) shutil.copyfile(\\"source.txt\\", \\"destination.txt\\") ```","solution":"import sys import os import shutil import socket from datetime import datetime import threading # Lock for thread-safe file operations log_lock = threading.Lock() def audit_event_handler(event, args): events_to_monitor = [\'os.system\', \'socket.connect\', \'shutil.copyfile\'] if event in events_to_monitor: timestamp = datetime.now().strftime(\'%Y-%m-%d %H:%M:%S\') log_entry = f\\"{timestamp} - {event}: {\', \'.join(map(str, args))}\\" with log_lock: with open(\'audit.log\', \'a\') as log_file: log_file.write(log_entry + \'n\') def monitor_audit_events(): # Register the audit_event_handler function as an audit hook sys.addaudithook(audit_event_handler) # Example calls to demonstrate the logging os.system(\\"echo Hello World\\") s = socket.socket(socket.AF_INET, socket.SOCK_STREAM) s.connect((\\"example.com\\", 80)) with open(\'source.txt\', \'w\') as f: f.write(\\"Dummy content\\") shutil.copyfile(\\"source.txt\\", \\"destination.txt\\")"},{"question":"# Garbage Collection and Memory Management Python\'s garbage collector (gc) module provides a wide range of functions to manage memory and object lifecycle in a program. This task requires you to implement a function leveraging various aspects of the gc module to manage and inspect memory usage within an application. You will create a class that tracks the creation and destruction of its instances, and then use the gc module to manage and report this behavior. Objective Implement a Python class and a function to achieve the following: 1. Create a class `TrackedObject` that: - Has a class variable counting the number of active instances (`active_instances`). - Increments count in the constructor (__init__). - Decrements count in the destructor (__del__). 2. Write a function `memory_management_and_debugging` that: - Takes an integer `n` as input, creates `n` instances of `TrackedObject`, and stores them in a list. - Forces garbage collection after list creation. - Retrieves and prints the memory statistics using `gc.get_stats()`. - Prints the number of uncollectable objects if any. - Prints the count of `TrackedObject.active_instances` before and after garbage collection. Input - An integer `n` specifying the number of instances to create. Output - Print statements showing: - Memory statistics before and after garbage collection. - Number of uncollectable objects found if any. - Count of `TrackedObject.active_instances` before and after garbage collection. Constraints - You must not disable the automatic garbage collection using `gc.disable()`. - Assume `n` will be a non-negative integer. Example ```python import gc class TrackedObject: active_instances = 0 def __init__(self): TrackedObject.active_instances += 1 def __del__(self): TrackedObject.active_instances -= 1 def memory_management_and_debugging(n): # Step 1: Enable garbage collection gc.enable() # Step 2: Create n instances of TrackedObject objects_list = [TrackedObject() for _ in range(n)] # Step 3: Print memory statistics before garbage collection print(\\"Memory stats before GC:\\") print(gc.get_stats()) # Step 4: Force garbage collection collected = gc.collect() # Step 5: Print memory statistics after garbage collection print(\\"Memory stats after GC:\\") print(gc.get_stats()) # Step 6: Print the number of uncollectable objects print(f\\"Uncollectable objects: {len(gc.garbage)}\\") # Step 7: Print active instance count print(f\\"Active instances before del: {TrackedObject.active_instances}\\") # Step 8: Delete the list manually to force __del__ calls del objects_list # Step 9: Force garbage collection again collected = gc.collect() # Step 10: Print active instance count after GC print(f\\"Active instances after del: {TrackedObject.active_instances}\\") # Test the function with a sample input memory_management_and_debugging(10) ``` In this example, the class `TrackedObject` carefully tracks its active instances, and `memory_management_and_debugging` manages garbage collection and prints relevant debug information.","solution":"import gc class TrackedObject: active_instances = 0 def __init__(self): TrackedObject.active_instances += 1 def __del__(self): TrackedObject.active_instances -= 1 def memory_management_and_debugging(n): # Step 1: Enable garbage collection gc.enable() # Step 2: Create n instances of TrackedObject objects_list = [TrackedObject() for _ in range(n)] # Step 3: Print memory statistics before garbage collection print(\\"Memory stats before GC:\\") print(gc.get_stats()) # Step 4: Force garbage collection collected = gc.collect() # Step 5: Print memory statistics after garbage collection print(\\"Memory stats after GC:\\") print(gc.get_stats()) # Step 6: Print the number of uncollectable objects print(f\\"Uncollectable objects: {len(gc.garbage)}\\") # Step 7: Print active instance count print(f\\"Active instances before del: {TrackedObject.active_instances}\\") # Step 8: Delete the list manually to force __del__ calls del objects_list # Step 9: Force garbage collection again collected = gc.collect() # Step 10: Print active instance count after GC print(f\\"Active instances after del: {TrackedObject.active_instances}\\") # Test the function with a sample input memory_management_and_debugging(10)"},{"question":"**Objective:** Implement a function that generates and customizes a seaborn distribution plot based on input parameters. **Function Signature:** ```python def customized_displot(data: pd.DataFrame, x: str, y: str = None, kind: str = \\"hist\\", hue: str = None, col: str = None, height: float = 5, aspect: float = 1, kde: bool = False, rug: bool = False) -> sns.FacetGrid: pass ``` **Inputs:** - `data` (pd.DataFrame): The dataset to be used for plotting. - `x` (str): The column to be visualized on the x-axis. - `y` (str, optional): The column to be visualized on the y-axis for bivariate plots. Default is `None`. - `kind` (str, optional): Type of plot to draw. One of {\\"hist\\", \\"kde\\", \\"ecdf\\"}. Default is \\"hist\\". - `hue` (str, optional): Semantic variable that is mapped to determine the color of plot elements. Default is `None`. - `col` (str, optional): Subplots separated by the categorical variable given. Default is `None`. - `height` (float, optional): Height of each facet in inches. Default is 5. - `aspect` (float, optional): Aspect ratio of each facet, so that aspect * height gives the width of each facet in inches. Default is 1. - `kde` (bool, optional): If True, add a kernel density estimate to the plot. Applicable only for histograms. Default is `False`. - `rug` (bool, optional): If True, add a marginal rug of all observations. Default is `False`. **Outputs:** - Returns a `FacetGrid` object with the customized plot. **Constraints:** - Only \\"penguins\\" dataset from seaborn should be used for this task. - The function should handle cases where `y` is provided or not, accurately. **Instructions:** 1. Load the \\"penguins\\" dataset using seaborn. 2. Create a distribution plot using `sns.displot` with the provided parameters. 3. Customize the plot to add a KDE curve if `kde` is `True` and `kind` is \\"hist\\". 4. Add a rug plot if `rug` is `True`. 5. Set appropriate labels for the axes using `set_axis_labels`. 6. Return the `FacetGrid` object. **Example Usage:** ```python import seaborn as sns import pandas as pd # Load the penguins dataset penguins = sns.load_dataset(\\"penguins\\") # Call the function g = customized_displot(data=penguins, x=\\"flipper_length_mm\\", kind=\\"kde\\", hue=\\"species\\", col=\\"sex\\", height=4, aspect=.7) # Further customize if required g.set_titles(\\"{col_name} penguins\\") ``` **Note:** - Ensure the function is well-documented and handles edge cases gracefully. - Test the function with different parameter combinations to demonstrate its flexibility.","solution":"import seaborn as sns import pandas as pd def customized_displot(data: pd.DataFrame, x: str, y: str = None, kind: str = \\"hist\\", hue: str = None, col: str = None, height: float = 5, aspect: float = 1, kde: bool = False, rug: bool = False) -> sns.FacetGrid: Generates a customized seaborn distribution plot. Parameters: - data (pd.DataFrame): The dataset to be used for plotting. - x (str): The column to be visualized on the x-axis. - y (str, optional): The column to be visualized on the y-axis for bivariate plots. Default is None. - kind (str, optional): Type of plot to draw. One of {\\"hist\\", \\"kde\\", \\"ecdf\\"}. Default is \\"hist\\". - hue (str, optional): Semantic variable that is mapped to determine the color of plot elements. Default is None. - col (str, optional): Subplots separated by the categorical variable given. Default is None. - height (float, optional): Height of each facet in inches. Default is 5. - aspect (float, optional): Aspect ratio of each facet, so that aspect * height gives the width of each facet in inches. Default is 1. - kde (bool, optional): If True, add a kernel density estimate to the plot. Applicable only for histograms. Default is False. - rug (bool, optional): If True, add a marginal rug of all observations. Default is False. Returns: - A sns.FacetGrid object with the customized plot. # Create the distribution plot g = sns.displot(data=data, x=x, y=y, kind=kind, hue=hue, col=col, height=height, aspect=aspect) # Add optional kde curve if kind == \'hist\' and kde: g.map(sns.kdeplot, x, hue=hue, fill=True) # Add optional rug plot if rug: g.map(sns.rugplot, x, hue=hue) # Set axis labels g.set_axis_labels(x, y if y else \\"\\") return g # Sample usage with the penguins dataset if __name__ == \\"__main__\\": penguins = sns.load_dataset(\\"penguins\\") g = customized_displot(data=penguins, x=\\"flipper_length_mm\\", kind=\\"kde\\", hue=\\"species\\", col=\\"sex\\", height=4, aspect=.7) g.set_titles(\\"{col_name} penguins\\")"},{"question":"Objective: Implement a Python class that defines a custom mapping and utilize the provided mapping protocol functions to interact with this custom mapping. Prompt: 1. Create a class called `CustomMapping` which supports mapping operations similar to a dictionary. 2. Implement the following methods in the `CustomMapping` class: - `__getitem__(self, key)`: Retrieve an item by key. - `__setitem__(self, key, value)`: Set an item by key. - `__delitem__(self, key)`: Delete an item by key. - `__contains__(self, key)`: Check if a key exists. - `keys(self)`: Return a list of keys. - `values(self)`: Return a list of values. - `items(self)`: Return a list of key-value pairs. - `__len__(self)`: Return the number of items. 3. Use the functions from the provided mapping protocol documentation to interact with an instance of `CustomMapping`. Constraints: - Only string keys are allowed. - Ensure that all operations are performed in average O(1) time. Example Code: Here is how your class should behave: ```python # Create an instance of CustomMapping cm = CustomMapping() # Add items to the mapping cm[\'a\'] = 10 cm[\'b\'] = 20 # Get the number of items size = PyMapping_Size(cm) print(size) # Output: 2 # Retrieve an item value = PyMapping_GetItemString(cm, \'a\') print(value) # Output: 10 # Check if a key exists exists = PyMapping_HasKeyString(cm, \'b\') print(exists) # Output: 1 # Remove an item PyMapping_DelItemString(cm, \'a\') # Get all keys, values, and items keys = PyMapping_Keys(cm) values = PyMapping_Values(cm) items = PyMapping_Items(cm) print(keys) # Output: [\'b\'] print(values) # Output: [20] print(items) # Output: [(\'b\', 20)] ``` **Performance Requirements:** - All methods should have an average time complexity of O(1). Notes: - You should implement the provided methods to mirror the behavior of dictionary operations in a custom class. - You will interact with this class via the predefined mapping protocol functions.","solution":"class CustomMapping: def __init__(self): self._data = {} def __getitem__(self, key): if not isinstance(key, str): raise TypeError(\\"Key must be a string\\") return self._data[key] def __setitem__(self, key, value): if not isinstance(key, str): raise TypeError(\\"Key must be a string\\") self._data[key] = value def __delitem__(self, key): if not isinstance(key, str): raise TypeError(\\"Key must be a string\\") del self._data[key] def __contains__(self, key): if not isinstance(key, str): raise TypeError(\\"Key must be a string\\") return key in self._data def keys(self): return list(self._data.keys()) def values(self): return list(self._data.values()) def items(self): return list(self._data.items()) def __len__(self): return len(self._data) def PyMapping_Size(mapping): return len(mapping) def PyMapping_GetItemString(mapping, key): return mapping[key] def PyMapping_HasKeyString(mapping, key): return 1 if key in mapping else 0 def PyMapping_DelItemString(mapping, key): del mapping[key] def PyMapping_Keys(mapping): return mapping.keys() def PyMapping_Values(mapping): return mapping.values() def PyMapping_Items(mapping): return mapping.items()"},{"question":"Objective In this task, you are required to implement the `optimize_decision_threshold` function. This function takes a binary classification model, training data, and a specific scoring metric to tune the decision threshold of the classifier and apply it on test data. You will demonstrate your understanding of `TunedThresholdClassifierCV` in scikit-learn by training a model and optimizing its decision threshold based on the input metric. Function Specification ```python def optimize_decision_threshold(model, X_train, y_train, X_test, y_test, scoring_metric): Optimizes the decision threshold of a given binary classification model using a specified scoring metric and evaluates it on test data. Parameters: - model (sklearn.base.BaseEstimator): The binary classification model to be optimized. - X_train (numpy.ndarray or pandas.DataFrame): The input features of the training set. - y_train (numpy.ndarray or pandas.Series): The labels of the training set. - X_test (numpy.ndarray or pandas.DataFrame): The input features of the test set. - y_test (numpy.ndarray or pandas.Series): The labels of the test set. - scoring_metric (str): The scoring metric to be optimized. Must be a valid metric name from sklearn.metrics.get_scorer_names(). Returns: - float: The optimized score evaluated on the test set. pass ``` Input Constraints - `model` should be a valid scikit-learn binary classifier. - `X_train`, `y_train`, `X_test`, and `y_test` should all be numpy arrays or pandas data structures. - `scoring_metric` should be a valid scoring metric name from `sklearn.metrics.get_scorer_names()`. Requirements 1. Use `TunedThresholdClassifierCV` to optimize the decision threshold based on the provided `scoring_metric`. 2. Evaluate and return the optimized score on the test set using the new threshold. 3. Ensure the function correctly handles edge cases, such as invalid models or metric names. Performance Requirements - The function should efficiently handle datasets that fit into memory. - You can assume the datasets won’t exceed typical in-memory processing limits for a single machine. Example ```python from sklearn.datasets import make_classification from sklearn.linear_model import LogisticRegression from sklearn.metrics import get_scorer_names X, y = make_classification(n_samples=1000, weights=[0.1, 0.9], random_state=0) X_train, y_train = X[:800], y[:800] X_test, y_test = X[800:], y[800:] model = LogisticRegression() scoring_metric = \'f1\' optimized_score = optimize_decision_threshold(model, X_train, y_train, X_test, y_test, scoring_metric) print(f\\"Optimized score on test data: {optimized_score}\\") ```","solution":"from sklearn.model_selection import cross_val_score from sklearn.metrics import get_scorer, get_scorer_names, make_scorer import numpy as np def optimize_decision_threshold(model, X_train, y_train, X_test, y_test, scoring_metric): Optimizes the decision threshold of a given binary classification model using a specified scoring metric and evaluates it on test data. Parameters: - model (sklearn.base.BaseEstimator): The binary classification model to be optimized. - X_train (numpy.ndarray or pandas.DataFrame): The input features of the training set. - y_train (numpy.ndarray or pandas.Series): The labels of the training set. - X_test (numpy.ndarray or pandas.DataFrame): The input features of the test set. - y_test (numpy.ndarray or pandas.Series): The labels of the test set. - scoring_metric (str): The scoring metric to be optimized. Must be a valid metric name from sklearn.metrics.get_scorer_names(). Returns: - float: The optimized score evaluated on the test set. if model is None or scoring_metric not in get_scorer_names(): raise ValueError(\\"Invalid model or scoring metric\\") model.fit(X_train, y_train) # Get predicted probabilities y_probs = model.predict_proba(X_train)[:, 1] thresholds = np.linspace(0, 1, 101) best_threshold = 0.5 best_score = -1 for threshold in thresholds: y_preds = (y_probs >= threshold).astype(int) scorer = get_scorer(scoring_metric) score = scorer(model, X_train, y_preds) if score > best_score: best_score = score best_threshold = threshold # Apply best threshold to test set y_test_probs = model.predict_proba(X_test)[:, 1] y_test_preds = (y_test_probs >= best_threshold).astype(int) optimized_score = scorer(model, X_test, y_test_preds) return optimized_score"},{"question":"<|Analysis Begin|> The provided documentation covers the `imaplib` module in Python, which is used for handling connections to an IMAP4 server. The module defines three primary classes, `IMAP4`, `IMAP4_SSL`, and `IMAP4_stream`, and provides methods for performing various IMAP4 operations such as logging in, selecting mailboxes, fetching mail, searching, and handling mail operations. The documentation also includes information on utility functions and example usage. Key concepts in the provided documentation include: - Handling IMAP4 connections. - Authentication methods. - Mailbox operations (e.g., create, delete, rename). - Message operations (e.g., append, fetch, copy, store). - Searching and sorting messages. - Error handling using custom exceptions. Given the comprehensive scope of the `imaplib` module, a suitable assessment question can focus on implementing a function that connects to an IMAP server, searches for emails based on specific criteria, and processes the found emails in a certain way (e.g., printing their subjects). <|Analysis End|> <|Question Begin|> Question: Implementing an Email Search and Fetch Function using `imaplib` # Objective Write a Python function named `search_emails` that connects to an IMAP server, logs in using the provided credentials, searches for emails based on the given search criteria, fetches the subjects of the found emails, and returns them as a list of strings. # Function Signature ```python def search_emails(host: str, username: str, password: str, search_criterion: str) -> list: pass ``` # Parameters - `host` (str): The IMAP server hostname. - `username` (str): The username for logging into the IMAP server. - `password` (str): The password for logging into the IMAP server. - `search_criterion` (str): The search criterion used to find emails (e.g., \'FROM \\"example@example.com\\"\', \'SUBJECT \\"Invoice\\"\', etc.). # Returns - (list): A list of email subject strings matching the search criterion. # Constraints 1. You must use the `imaplib` module to handle the connection and operations. 2. Ensure you handle any potential exceptions, such as authentication errors or connection errors. 3. If no emails match the search criterion, return an empty list. 4. Close the connection to the IMAP server once the operation is completed. # Example ```python def search_emails(host: str, username: str, password: str, search_criterion: str) -> list: import imaplib try: # Connect to the IMAP server mail = imaplib.IMAP4(host) mail.login(username, password) mail.select(\\"inbox\\") # Search for emails status, search_data = mail.search(None, search_criterion) if status != \'OK\': return [] email_ids = search_data[0].split() subjects = [] for email_id in email_ids: status, fetch_data = mail.fetch(email_id, \'(BODY[HEADER.FIELDS (SUBJECT)])\') if status != \'OK\': continue subject = fetch_data[0][1].decode(\'utf-8\') subjects.append(subject) mail.close() mail.logout() return subjects except imaplib.IMAP4.error: return [] # Example usage: # print(search_emails(\\"imap.example.com\\", \\"user@example.com\\", \\"password\\", \'FROM \\"example2@example.com\\"\')) ``` # Notes - Ensure the function selects the \\"inbox\\" mailbox before performing the search. - The search criterion should be passed directly to the `search` method without modification. - Handle errors gracefully and ensure that the connection to the IMAP server is properly closed in all cases.","solution":"def search_emails(host: str, username: str, password: str, search_criterion: str) -> list: import imaplib import email try: # Connect to the IMAP server mail = imaplib.IMAP4_SSL(host) # Using SSL for secure connection mail.login(username, password) mail.select(\\"inbox\\") # Search for emails status, search_data = mail.search(None, search_criterion) if status != \'OK\': return [] email_ids = search_data[0].split() subjects = [] for email_id in email_ids: status, fetch_data = mail.fetch(email_id, \'(BODY[HEADER.FIELDS (SUBJECT)])\') if status != \'OK\': continue msg = email.message_from_bytes(fetch_data[0][1]) subjects.append(msg.get(\'Subject\')) mail.close() mail.logout() return subjects except imaplib.IMAP4.error: return [] except Exception as e: print(f\\"An unexpected error occurred: {e}\\") return []"},{"question":"You are tasked with developing a Python program that will perform the following operations using the `fractions`, `decimal`, and `statistics` modules: 1. **Input Parsing**: - Read an input list of numeric values as strings. The list contains integers, floating-point numbers, and fractions in the form of strings. - Example: `[\\"3\\", \\"4.5\\", \\"7/2\\", \\"1.25\\", \\"8\\"]` 2. **Conversion to Decimal**: - Convert each value to a `decimal.Decimal` object for precise arithmetic operations. 3. **Statistical Analysis**: - Calculate the arithmetic mean and standard deviation of the list of decimal values. # Implementation Details 1. Write a function `convert_to_decimal(values: List[str]) -> List[decimal.Decimal]`: - This function should take a list of numeric values as strings and return a list of `decimal.Decimal` objects. - Handle different input formats (integers, floats, and fractions). 2. Write a function `compute_statistics(decimal_values: List[decimal.Decimal]) -> Tuple[decimal.Decimal, decimal.Decimal]`: - This function should compute the arithmetic mean and standard deviation using `statistics.mean` and `statistics.stdev`. - Return the results as a tuple of `decimal.Decimal` objects. # Example Usage ```python from typing import List, Tuple import decimal import statistics def convert_to_decimal(values: List[str]) -> List[decimal.Decimal]: # Implement the function pass def compute_statistics(decimal_values: List[decimal.Decimal]) -> Tuple[decimal.Decimal, decimal.Decimal]: # Implement the function pass # Example values list values = [\\"3\\", \\"4.5\\", \\"7/2\\", \\"1.25\\", \\"8\\"] # Convert to decimal decimal_values = convert_to_decimal(values) # Compute statistics mean, stdev = compute_statistics(decimal_values) print(f\\"Mean: {mean}, Standard Deviation: {stdev}\\") ``` # Constraints - You may assume that the input will always be a list of valid numeric strings. - Use `decimal.Decimal` for all arithmetic operations to ensure precision. - Use the `fractions.Fraction` class as needed to parse fraction strings. # Requirements - `convert_to_decimal` implementation should correctly handle and convert the different numeric formats provided. - `compute_statistics` should correctly compute the mean and standard deviation based on the converted decimal values. - The code should be efficient and handle up to 1000 numeric values.","solution":"from typing import List, Tuple import decimal import statistics from fractions import Fraction def convert_to_decimal(values: List[str]) -> List[decimal.Decimal]: decimals = [] for value in values: if \'/\' in value: # Handle fractional strings frac = Fraction(value) decimals.append(decimal.Decimal(frac.numerator) / decimal.Decimal(frac.denominator)) else: # Handle integer and float strings decimals.append(decimal.Decimal(value)) return decimals def compute_statistics(decimal_values: List[decimal.Decimal]) -> Tuple[decimal.Decimal, decimal.Decimal]: # Compute arithmetic mean mean = statistics.mean(decimal_values) # Compute standard deviation (handle single value list separately as sample stdev isn\'t defined) if len(decimal_values) > 1: stdev = statistics.stdev(decimal_values) else: stdev = decimal.Decimal(0) return (decimal.Decimal(mean), decimal.Decimal(stdev))"},{"question":"# Question: XML Data Extraction and Transformation Using the `xml.dom.pulldom` module, write a function `filter_and_transform(xml_string: str, price_threshold: int) -> str` that processes an XML string containing a list of items with prices. The function should extract items with prices above the given threshold and transform the extracted items into a new XML format. Input - `xml_string` (str): A string containing XML data with multiple items. - `price_threshold` (int): An integer threshold for filtering items based on their price attribute. Output - (str): A string containing the new XML format with the filtered items. Each item should include its original attributes and a new attribute `filtered=\\"true\\"`. Constraints - Assume the XML string is well-formed and valid but it can be arbitrarily large. - You must use the `xml.dom.pulldom` module to parse and process the XML. - Use event-driven processing for filtering and `expandNode()` for transforming the selected nodes. - The new XML format should have the root element `<FilteredItems>`, and each `<item>` element should retain its original attributes with an additional `filtered=\\"true\\"` attribute. Example ```python xml_data = \'\'\' <items> <item price=\\"30\\" name=\\"item1\\"/> <item price=\\"60\\" name=\\"item2\\"/> <item price=\\"70\\" name=\\"item3\\"/> </items> \'\'\' result = filter_and_transform(xml_data, 50) print(result) ``` Output: ```xml <FilteredItems> <item price=\\"60\\" name=\\"item2\\" filtered=\\"true\\"/> <item price=\\"70\\" name=\\"item3\\" filtered=\\"true\\"/> </FilteredItems> ``` # Function Signature ```python def filter_and_transform(xml_string: str, price_threshold: int) -> str: pass ``` # Notes - You should make use of `pulldom.parseString()` to initialize the DOM event stream from the XML string. - Loop through events to filter items based on the price attribute. - For each item that meets the threshold, use `expandNode(node)` to fully expand the item and add the `filtered=\\"true\\"` attribute. - Construct a new XML string with the root `<FilteredItems>` and the filtered items.","solution":"from xml.dom import pulldom from xml.dom.minidom import Document def filter_and_transform(xml_string: str, price_threshold: int) -> str: doc = pulldom.parseString(xml_string) new_doc = Document() root = new_doc.createElement(\\"FilteredItems\\") new_doc.appendChild(root) for event, node in doc: if event == pulldom.START_ELEMENT and node.tagName == \\"item\\": price = int(node.getAttribute(\\"price\\")) if price > price_threshold: doc.expandNode(node) node.setAttribute(\\"filtered\\", \\"true\\") root.appendChild(new_doc.importNode(node, True)) return new_doc.toprettyxml(indent=\\" \\") # Example usage: xml_data = \'\'\' <items> <item price=\\"30\\" name=\\"item1\\"/> <item price=\\"60\\" name=\\"item2\\"/> <item price=\\"70\\" name=\\"item3\\"/> </items> \'\'\' result = filter_and_transform(xml_data, 50) print(result)"},{"question":"**Coding Assessment Question: Configuration Mastery** **Objective:** Demonstrate your understanding of the pandas options API by dynamically configuring and restoring settings for DataFrame display properties. **Problem Statement:** You are given a DataFrame containing random data with 20 rows and 10 columns. Your task is to complete the function `configure_display_options` which will modify various pandas display settings, display the DataFrame with these settings, and then restore the settings to their default values. More specifically, you need to: 1. Set the maximum number of rows displayed (`display.max_rows`) to 10. 2. Set the maximum number of columns displayed (`display.max_columns`) to 5. 3. Set the display precision (`display.precision`) to 4 decimal places. 4. Use a context manager to temporarily apply these settings while displaying the DataFrame. **Input:** - A pandas DataFrame `df` of size (20, 10). **Output:** - No direct output. The function should print the DataFrame with the modified settings and then restore the settings to their default values. **Function Signature:** ```python import pandas as pd import numpy as np def configure_display_options(df: pd.DataFrame) -> None: # Your implementation here pass # Create a sample DataFrame data = np.random.randn(20, 10) df = pd.DataFrame(data, columns=[f\'col_{i}\' for i in range(10)]) # Call the function with the sample DataFrame configure_display_options(df) ``` **Constraints:** - You must use the pandas options API functions (`set_option`, `reset_option`, `option_context`) effectively. - Ensure that any modifications to the display settings are temporary and restored to their defaults after displaying the DataFrame. **Example:** Suppose you create a DataFrame like below and pass it to the function `configure_display_options`: ```python data = np.random.randn(20, 10) df = pd.DataFrame(data, columns=[f\'col_{i}\' for i in range(10)]) ``` When you call `configure_display_options(df)`, the DataFrame should first be displayed according to the modified settings, i.e., showing 10 rows, 5 columns, and values with 4 decimal places. After exiting the context manager, if you display the DataFrame again, it should revert to the default settings. **Notes:** - Proper usage of context managers will be a key evaluation point. - Attention to detail in restoring these settings is crucial for passing the assessment. **Hint:** Refer to the pandas documentation on options API to review functions like `pd.set_option`, `pd.reset_option`, and `pd.option_context`.","solution":"import pandas as pd import numpy as np def configure_display_options(df: pd.DataFrame) -> None: with pd.option_context(\'display.max_rows\', 10, \'display.max_columns\', 5, \'display.precision\', 4): print(df) # Create a sample DataFrame data = np.random.randn(20, 10) df = pd.DataFrame(data, columns=[f\'col_{i}\' for i in range(10)]) # Call the function with the sample DataFrame configure_display_options(df)"},{"question":"**Programming Assessment Question: Navigating TorchScript Constraints** **Objective:** You are required to implement a PyTorch module that computes certain operations on input tensors and is compatible with TorchScript for efficient deployment. **Task Description:** Write a class `ScriptedOperations` which extends `torch.nn.Module` and implement the following: 1. A method `custom_norm` that computes the norm of a tensor. This method should avoid using `torch.norm` due to its unsupported schema in TorchScript. 2. A method `custom_sum` that takes a list of tensors and returns their element-wise sum. Ensure to handle the divergence where necessary. 3. A method `initialize_tensor` using supported tensor initialization methods in TorchScript. **Implementation Details:** - The `custom_norm` method should compute the Frobenius norm of a 2-D tensor without directly using the unsupported `torch.norm`. - The `custom_sum` method should avoid any unsupported operations and ensure it sums up all given tensors element-wise. - The `initialize_tensor` method should create and return a tensor of given shape, using a supported initialization method like `torch.zeros`. **Constraints:** - Do not use any of the unsupported functions/operators listed in the documentation. - Ensure compatibility with TorchScript by limiting operations to those that behave consistently in TorchScript and standard Python. **Input and Output:** - `custom_norm`: Takes a tensor of shape (m, n) and returns a single scalar value. - `custom_sum`: Takes a list of N tensors of the same shape and returns a single tensor of the same shape. - `initialize_tensor`: Takes an integer tuple representing shape, and returns a tensor of that shape initialized to zeros. **Example Usage:** ```python import torch import torch.jit class ScriptedOperations(torch.nn.Module): def __init__(self): super(ScriptedOperations, self).__init__() def custom_norm(self, x): # Implement logic here pass def custom_sum(self, tensors): # Implement logic here pass def initialize_tensor(self, shape): # Implement logic here pass # Example inputs tensor_1 = torch.tensor([[2.0, 3.0], [4.0, 1.0]]) tensor_2 = torch.tensor([[1.0, 0.0], [0.0, 1.0]]) shape = (2, 2) # Create an instance scripted_ops = ScriptedOperations() # Example function calls norm_value = scripted_ops.custom_norm(tensor_1) summed_tensor = scripted_ops.custom_sum([tensor_1, tensor_2]) initialized_tensor = scripted_ops.initialize_tensor(shape) # Convert to TorchScript scripted_model = torch.jit.script(scripted_ops) # Display results print(\\"Norm Value:\\", norm_value) print(\\"Summed Tensor:\\", summed_tensor) print(\\"Initialized Tensor:\\", initialized_tensor) ``` **Note:** Ensure your methods are TorchScript-compatible before submission.","solution":"import torch class ScriptedOperations(torch.nn.Module): def __init__(self): super(ScriptedOperations, self).__init__() def custom_norm(self, x): frobenius_norm = torch.sqrt(torch.sum(x * x)) return frobenius_norm def custom_sum(self, tensors): result = tensors[0] for tensor in tensors[1:]: result = torch.add(result, tensor) return result def initialize_tensor(self, shape): return torch.zeros(shape)"},{"question":"**Question Title**: Unicode Text Processing and Transformation **Objective**: Implement a Python function that reads Unicode text from a file, processes it by normalizing and encoding it, and writes the output to another file. **Description**: You have been provided with a file containing text data written in various languages using Unicode characters. Your task is to write a Python function called `normalize_and_encode` that performs the following operations: 1. Reads the text from the input file in UTF-8 encoding into a Unicode string. 2. Normalizes the text to its canonical form using Unicode Normalization Form C (NFC). 3. Encodes the normalized text into UTF-16 encoding. 4. Writes the encoded text into an output file specified by the user. **Function Signature**: ```python def normalize_and_encode(input_file: str, output_file: str) -> None: pass ``` **Input**: - `input_file`: A string representing the path to the input text file. - `output_file`: A string representing the path to the output text file. **Output**: - The function should not return anything. It should write the processed and encoded text to `output_file`. **Constraints**: - Assume that the input file contains text data that can be read without any errors. - The text file may contain Unicode characters from various languages including but not limited to English, French, Japanese, Hebrew, and Russian. **Example**: Imagine you have an input file `input.txt` containing the text: ``` Hello, world! 😀 Bonjour, le monde! 🌍 こんにちは、世界！🌏 שלום, עולם! 🌎 Привет, мир! 🌍 ``` After running your function as follows: ```python normalize_and_encode(\'input.txt\', \'output.txt\') ``` The output file `output.txt` should contain the normalized and UTF-16 encoded text. **Additional Information**: - You can use the `unicodedata` module to handle Unicode normalization. - The `open` function in Python can be used to read and write files with specific encodings. - Make sure to handle file reading and writing operations correctly, ensuring that file streams are closed properly after operations. Good luck!","solution":"import unicodedata def normalize_and_encode(input_file: str, output_file: str) -> None: Reads Unicode text from the input file, normalizes it using NFC, encodes it in UTF-16, and writes the result to the output file. # Read the text from the input file in UTF-8 encoding with open(input_file, \'r\', encoding=\'utf-8\') as f: text = f.read() # Normalize the text using Unicode Normalization Form C (NFC) normalized_text = unicodedata.normalize(\'NFC\', text) # Encode the normalized text into UTF-16 encoding encoded_text = normalized_text.encode(\'utf-16\') # Write the encoded text into the output file with open(output_file, \'wb\') as f: f.write(encoded_text)"},{"question":"**Question: Logging System with Error Detection and Notification** You are required to implement a logging utility using the Python `syslog` module. Your utility should log messages to the system logger with different severity levels and facilities. Additionally, it should be able to handle an error detection scenario where messages tagged as errors will have a higher priority and should notify the admin via a higher-priority syslog message. **Requirements:** 1. Implement a function `configure_logger(ident, logoption, facility)` that: - Configures the logger using the `openlog()` function. - Parameters: - `ident` (str): A unique identifier prepended to each log message. - `logoption` (int): Bit field representing log options. - `facility` (int): Default facility for messages. 2. Implement a function `log_message(priority, message)` that: - Sends a message to the system logger using the `syslog()` function. - Parameters: - `priority` (int): Priority level of the message. - `message` (str): Log message text. 3. Implement a function `log_error(message)` that: - Logs the error message with `LOG_ERR` priority and notifies the admin with a `LOG_CRIT` priority message. - Parameter: - `message` (str): Error message text. 4. Implement a function `reset_logger()` that: - Resets the logger to its initial state using the `closelog()` function. **Input/Output Example:** ```python # Example usage configure_logger(\'TestApp\', syslog.LOG_PID, syslog.LOG_USER) log_message(syslog.LOG_INFO, \'Process started successfully.\') log_error(\'A critical error occurred.\') # Expected behavior: # Logs \\"Process started successfully.\\" with LOG_INFO priority. # Logs \\"A critical error occurred.\\" with LOG_ERR priority. # Logs \\"Critical Error Notification: A critical error occurred.\\" with LOG_CRIT priority. ``` **Constraints:** - Make sure to handle and test for possible exceptions in your logging utility. - Utilize appropriate syslog priorities and facilities as constants provided by the `syslog` module.","solution":"import syslog def configure_logger(ident, logoption, facility): Configures the logger. Parameters: ident (str): A unique identifier prepended to each log message. logoption (int): Bit field representing log options. facility (int): Default facility for messages. syslog.openlog(ident=ident, logoption=logoption, facility=facility) def log_message(priority, message): Sends a message to the system logger. Parameters: priority (int): Priority level of the message. message (str): Log message text. syslog.syslog(priority, message) def log_error(message): Logs an error message with LOG_ERR priority and notifies admin with LOG_CRIT. Parameter: message (str): Error message text. syslog.syslog(syslog.LOG_ERR, message) syslog.syslog(syslog.LOG_CRIT, f\\"Critical Error Notification: {message}\\") def reset_logger(): Resets the logger to its initial state. syslog.closelog()"},{"question":"# PyTorch Tensor Views and Contiguity Objective You are required to demonstrate your understanding of PyTorch tensor views by implementing a series of tasks. Specifically, you will need to create tensor views, check for contiguity, and enforce contiguity when necessary. Tasks 1. **Create a View**: Given a 2D tensor, create a view with different shape but the same data. 2. **Check Contiguity**: Given a tensor, check whether it is contiguous or not. 3. **Enforce Contiguity**: If a tensor is not contiguous, create a contiguous tensor from it. Input - A 2D tensor `t` with random values. - Desired shape `shape` for the new view. - A tensor `t_check` that needs to be checked for contiguity. Output - A new view of tensor `t` with the desired `shape`. - A boolean indicating whether `t_check` is contiguous. - A contiguous tensor if the input tensor is not contiguous, otherwise the input tensor itself. Constraints - Ensure that the new shape for the view is compatible with the original tensor shape in terms of the number of elements. - Handle the case where creating the view might result in a non-contiguous tensor. Performance Requirements - The solution should avoid unnecessary data copies to maintain efficiency. Example ```python import torch def tensor_view_operations(t, shape, t_check): # Task 1: Create a View view_tensor = t.view(shape) # Task 2: Check Contiguity is_contig = t_check.is_contiguous() # Task 3: Enforce Contiguity if not is_contig: contig_tensor = t_check.contiguous() else: contig_tensor = t_check return view_tensor, is_contig, contig_tensor # Example usage t = torch.rand(4, 4) shape = (2, 8) t_check = t.transpose(0, 1) view_tensor, is_contig, contig_tensor = tensor_view_operations(t, shape, t_check) print(view_tensor) print(is_contig) print(contig_tensor.is_contiguous()) ``` Explanation - The function `tensor_view_operations` performs the three tasks outlined above. - `view_tensor` is a view of the input tensor `t` with the new shape `shape`. - `is_contig` is a boolean that checks if `t_check` is contiguous in memory. - `contig_tensor` is either the same as `t_check` if it is contiguous, or a new contiguous tensor made from `t_check` if it was not.","solution":"import torch def tensor_view_operations(t, shape, t_check): Given a 2D tensor `t`, create a view with a different shape. Check if `t_check` is contiguous, and enforce contiguity if necessary. Args: t (torch.Tensor): A 2D tensor with random values. shape (tuple): Desired shape for the new view. t_check (torch.Tensor): A tensor that needs to be checked for contiguity. Returns: torch.Tensor: New view of tensor `t` with the desired `shape`. bool: Boolean indicating whether `t_check` is contiguous. torch.Tensor: Contiguous tensor if `t_check` is not contiguous, otherwise `t_check` itself. try: # Task 1: Create a View view_tensor = t.view(shape) except RuntimeError as e: return str(e), False, None # Task 2: Check Contiguity is_contig = t_check.is_contiguous() # Task 3: Enforce Contiguity if not is_contig: contig_tensor = t_check.contiguous() else: contig_tensor = t_check return view_tensor, is_contig, contig_tensor"},{"question":"Objective: To test the understanding of the buffer protocol and efficient memory handling in Python. Problem Statement: Implement a function `process_buffer_data(obj: Any) -> List[Any]` that performs the following tasks: 1. **Obtain the Buffer**: Use the Python buffer protocol to obtain buffer details from the given object `obj`. Assume `obj` supports the buffer protocol. 2. **Check Contiguity**: Determine if the buffer is C-style contiguous. 3. **Read Data**: If the buffer is C-contiguous, read and return the data as a list of elements based on the format provided by the buffer. If not, raise a `ValueError`. Expected Input and Output Format: - **Input**: - `obj`: Any object that supports the buffer protocol. - **Output**: - A list of elements read from the buffer if the buffer is C-contiguous. - Raise a `ValueError` if the buffer is not C-contiguous. Constraints: - The function should handle buffers of different types and data structures efficiently. - Assume the buffer could have up to 3 dimensions. Requirements: - Do not use intermediate data structures like `bytes` or `bytearray` to copy data. - Ensure proper release of the buffer. - Handle exceptions and edge cases, such as non-contiguous buffers or unsupported formats. # Example Usage ```python class CustomBuffer: def __init__(self, data): self.data = data def __buffer__(self): import array arr = array.array(\'i\', self.data) # Example array of integers return memoryview(arr) def test_process_buffer_data(): from array import array data = array(\'i\', [1, 2, 3, 4]) custom_buffer = CustomBuffer(data) result = process_buffer_data(custom_buffer) # Should return [1, 2, 3, 4] assert result == [1, 2, 3, 4] buf = memoryview(bytearray(b\\"test\\")) try: process_buffer_data(buf) except ValueError: # Should raise ValueError for non C-contiguous buffer print(\\"Non-C contiguous buffer\\") test_process_buffer_data() ``` You may use the Python C-API functions if needed to facilitate buffer handling. The solution should work seamlessly with Python objects that are compliant with the buffer protocol.","solution":"import numpy as np from typing import Any, List def process_buffer_data(obj: Any) -> List[Any]: Read data from an object\'s buffer using the buffer protocol. If buffer is not C-contiguous, raise ValueError. Args: - obj: Any object that supports the buffer protocol. Returns: - List of elements read from the object\'s buffer. with memoryview(obj) as mv: if not mv.contiguous: raise ValueError(\\"Buffer is not C-contiguous\\") if mv.format == \'B\': return list(mv.tobytes()) # Convert the buffer to a numpy array for easiness in handling # Making use of numpy\'s frombuffer method to avoid copying array = np.frombuffer(mv, dtype=mv.format) return array.tolist()"},{"question":"Objective: Create a Python function that processes text data from a file, filters the data based on specific criteria, and outputs statistical information. This will test your understanding of file I/O, data manipulation, and usage of standard libraries. Problem Statement: You are given a text file containing several lines, each with a string followed by a timestamp and an integer value separated by commas. Here is an example of the file content: ``` error,2023-02-15 14:56:23,5 info,2023-02-15 15:45:10,2 warning,2023-02-16 09:30:55,3 error,2023-02-16 10:15:31,8 ``` Each line represents a log entry with a log level, a timestamp, and a count. Your task is to implement a function `process_logs(file_path: str, start_date: str, end_date: str) -> dict` that reads the file, filters log entries within a specific date range (inclusive), and calculates the total count and average count per log level. Input: - `file_path` (str): Path to the input text file. - `start_date` (str): Start date in the format \\"YYYY-MM-DD\\". - `end_date` (str): End date in the format \\"YYYY-MM-DD\\". Output: - A dictionary where keys are log levels (`str`), and values are tuples `(total_count, average_count)`. Constraints: - The input file is guaranteed to have valid format entries as described. - Log levels are case-sensitive strings. - The function should handle large files efficiently. Example: Given the input file as above and the date range `start_date = \\"2023-02-15\\"` and `end_date = \\"2023-02-15\\"`, the function should return: ```python { \\"error\\": (5, 5.0), \\"info\\": (2, 2.0) } ``` Implementation: You are encouraged to use the following standard libraries: - `datetime` to parse and manipulate dates. - `collections` for efficient counting and grouping of log levels. Additional Notes: - Ensure that the function checks and handles edge cases, such as no logs within the date range. - Implementations will be evaluated on correctness, efficiency, and code readability. ```python import datetime from collections import defaultdict def process_logs(file_path: str, start_date: str, end_date: str) -> dict: try: start_dt = datetime.datetime.strptime(start_date, \\"%Y-%m-%d\\") end_dt = datetime.datetime.strptime(end_date, \\"%Y-%m-%d\\") log_stats = defaultdict(lambda: [0, 0]) # Dictionary with log level as key and [total_count, entry_count] as value with open(file_path, \'r\') as file: for line in file: log_level, timestamp, count = line.strip().split(\',\') log_dt = datetime.datetime.strptime(timestamp, \\"%Y-%m-%d %H:%M:%S\\") count = int(count) if start_dt <= log_dt.date() <= end_dt: log_stats[log_level][0] += count # Add to total count log_stats[log_level][1] += 1 # Increment entry count # Convert counts to the required format (total_count, average_count) result = {level: (total, total / count) for level, (total, count) in log_stats.items()} return result except Exception as e: print(f\\"Error processing logs: {e}\\") return {} # Example usage # result = process_logs(\'path/to/logfile.txt\', \'2023-02-15\', \'2023-02-15\') # print(result) ```","solution":"import datetime from collections import defaultdict def process_logs(file_path: str, start_date: str, end_date: str) -> dict: try: start_dt = datetime.datetime.strptime(start_date, \\"%Y-%m-%d\\") end_dt = datetime.datetime.strptime(end_date, \\"%Y-%m-%d\\") log_stats = defaultdict(lambda: [0, 0]) # Dictionary with log level as key and [total_count, entry_count] as value with open(file_path, \'r\') as file: for line in file: log_level, timestamp, count = line.strip().split(\',\') log_dt = datetime.datetime.strptime(timestamp, \\"%Y-%m-%d %H:%M:%S\\") count = int(count) if start_dt.date() <= log_dt.date() <= end_dt.date(): log_stats[log_level][0] += count # Add to total count log_stats[log_level][1] += 1 # Increment entry count # Convert counts to the required format (total_count, average_count) result = {level: (total, total / count) for level, (total, count) in log_stats.items()} return result except Exception as e: print(f\\"Error processing logs: {e}\\") return {}"},{"question":"You are given Python\'s version information in its individual components: major, minor, micro version, release level, and release serial. Your task is to implement a function that takes these components as input and returns the encoded version number as a single integer, following the encoding scheme described in the documentation. # Function Signature ```python def encode_python_version(major: int, minor: int, micro: int, release_level: str, release_serial: int) -> int: pass ``` # Input * `major` (int): The major version number (e.g., the \\"3\\" in \\"3.4.1a2\\"). * `minor` (int): The minor version number (e.g., the \\"4\\" in \\"3.4.1a2\\"). * `micro` (int): The micro version number (e.g., the \\"1\\" in \\"3.4.1a2\\"). * `release_level` (str): A string representing the release level, can be \\"alpha\\", \\"beta\\", \\"candidate\\", or \\"final\\". * `release_serial` (int): The release serial number (e.g., the \\"2\\" in \\"3.4.1a2\\"). # Output * The function should return an integer representing the encoded version number. # Constraints * The `major`, `minor`, and `micro` will be non-negative integers. * The `release_level` will be one of \\"alpha\\", \\"beta\\", \\"candidate\\", or \\"final\\". * The `release_serial` will be a non-negative integer. # Notes * Use the following map for `release_level` to the corresponding hex value: - \\"alpha\\" -> 0xA - \\"beta\\" -> 0xB - \\"candidate\\" -> 0xC - \\"final\\" -> 0xF # Example ```python assert encode_python_version(3, 4, 1, \\"alpha\\", 2) == 0x030401a2 assert encode_python_version(3, 10, 0, \\"final\\", 0) == 0x030a00f0 ``` # Explanation For the version \\"3.4.1a2\\": * Major version (3) occupies the first 8 bits: `0x03`. * Minor version (4) occupies the next 8 bits: `0x04`. * Micro version (1) occupies the following 8 bits: `0x01`. * Release level (\\"alpha\\" = 0xA) occupies bits 25-28: `0xA`. * Release serial (2) occupies the last 4 bits: `0x2`. Combining these gives us `0x030401a2`. Implement the function to encode such version formats to the corresponding integer value.","solution":"def encode_python_version(major: int, minor: int, micro: int, release_level: str, release_serial: int) -> int: release_level_map = { \\"alpha\\": 0xA, \\"beta\\": 0xB, \\"candidate\\": 0xC, \\"final\\": 0xF } release_level_num = release_level_map[release_level] return (major << 24) | (minor << 16) | (micro << 8) | (release_level_num << 4) | release_serial"},{"question":"Rug Plots and Data Visualization with Seaborn **Objective:** You are tasked with creating an advanced data visualization using Seaborn. This assessment will evaluate your understanding of the Seaborn library, specifically focusing on the implementation of rug plots and combinations with other Seaborn plots. **Problem Statement:** You are provided with the \\"tips\\" and \\"diamonds\\" datasets available in Seaborn. You need to create a single figure that includes the following subplots: 1. **Subplot 1:** A scatter plot of the \'total_bill\' vs. \'tip\' from the \'tips\' dataset, with a rug plot along both axes. 2. **Subplot 2:** The same scatter plot but with hue mapping showing \'time\' and a taller rug plot (height = 0.1). 3. **Subplot 3:** A scatter plot of the \'carat\' vs. \'price\' from the \'diamonds\' dataset with thin lines (linewidth = 1) and alpha blending (alpha = 0.005). Each subplot should have appropriate titles, labels for axes, and a legend where necessary. Arrange these subplots in a single row within the figure. **Input:** There are no input parameters for this task as you will be using predefined datasets from Seaborn. **Output:** A Seaborn figure with the described subplots. **Constraints and Limitations:** - The figure should have three subplots arranged in a single row. - Customize the rug plots as described. - Ensure that plots are clearly labeled and titled. **Performance Requirements:** The code should be efficient and complete the creation and display of the figure within a reasonable time frame. **Example:** ```python import seaborn as sns import matplotlib.pyplot as plt # Set the theme for the plots sns.set_theme() # Load datasets from Seaborn tips = sns.load_dataset(\\"tips\\") diamonds = sns.load_dataset(\\"diamonds\\") # Create subplots fig, axes = plt.subplots(1, 3, figsize=(18, 5)) # Subplot 1: Scatter plot with rug plot along both axes sns.scatterplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", ax=axes[0]) sns.rugplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", ax=axes[0]) axes[0].set_title(\\"Scatter Plot with Rug Plot (Both Axes)\\") # Subplot 2: Scatter plot with hue mapping and taller rug plot sns.scatterplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"time\\", ax=axes[1]) sns.rugplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"time\\", height=0.1, ax=axes[1]) axes[1].set_title(\\"Scatter Plot with Hue Mapping and Taller Rug Plot\\") # Subplot 3: Scatter plot with thin lines and alpha blending sns.scatterplot(data=diamonds, x=\\"carat\\", y=\\"price\\", s=5, ax=axes[2]) sns.rugplot(data=diamonds, x=\\"carat\\", y=\\"price\\", lw=1, alpha=0.005, ax=axes[2]) axes[2].set_title(\\"Scatter Plot with Thin Lines and Alpha Blending\\") # Display the plots plt.tight_layout() plt.show() ``` In this example, you are required to create the subplots in a similar manner and customize them as described. Ensure the final figure is well-structured and visually informative.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_advanced_seaborn_plots(): Creates a figure with three subplots using the Seaborn library and datasets. 1. Scatter plot of \'total_bill\' vs \'tip\' with a rug plot along both axes from \'tips\'. 2. Same scatter plot with \'time\' as hue and a taller rug plot. 3. Scatter plot of \'carat\' vs \'price\' with modified appearance from \'diamonds\'. # Set the theme for the plots sns.set_theme() # Load datasets from Seaborn tips = sns.load_dataset(\\"tips\\") diamonds = sns.load_dataset(\\"diamonds\\") # Create subplots fig, axes = plt.subplots(1, 3, figsize=(18, 5)) # Subplot 1: Scatter plot with rug plot along both axes sns.scatterplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", ax=axes[0]) sns.rugplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", ax=axes[0]) axes[0].set_title(\\"Scatter Plot with Rug Plot (Both Axes)\\") axes[0].set_xlabel(\\"Total Bill ()\\") axes[0].set_ylabel(\\"Tip ()\\") # Subplot 2: Scatter plot with hue mapping and taller rug plot sns.scatterplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"time\\", ax=axes[1]) sns.rugplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"time\\", height=0.1, ax=axes[1]) axes[1].set_title(\\"Scatter Plot with Hue Mapping and Taller Rug Plot\\") axes[1].set_xlabel(\\"Total Bill ()\\") axes[1].set_ylabel(\\"Tip ()\\") # Subplot 3: Scatter plot with thin lines and alpha blending sns.scatterplot(data=diamonds, x=\\"carat\\", y=\\"price\\", s=5, ax=axes[2]) sns.rugplot(data=diamonds, x=\\"carat\\", y=\\"price\\", lw=1, alpha=0.005, ax=axes[2]) axes[2].set_title(\\"Scatter Plot with Thin Lines and Alpha Blending\\") axes[2].set_xlabel(\\"Carat\\") axes[2].set_ylabel(\\"Price ()\\") # Display the plots plt.tight_layout() plt.show()"},{"question":"# Email Message Manipulation The `email.message.Message` class in Python\'s `email` module allows for the creation, modification, and serialization of email messages. This task requires you to use this class to accomplish several tasks involving email message headers and payloads. Task Details You are provided with an email message that may contain multiple headers and an optional payload. Your task is to: 1. **Create a function** to add a header to the provided `Message` object. If the header already exists, replace its value. 2. **Create a function** to extract and return all headers from the message. 3. **Create a function** to set the payload of the message. The payload can be either a text string or a binary object. 4. **Create a function** to convert the entire message to a string. Function Specifications 1. `add_header(msg, header_name, header_value) -> None` - **Input:** - `msg`: A `Message` object. - `header_name`: A string representing the header\'s name. - `header_value`: A string representing the header\'s value. - **Operation:** - If `header_name` exists in `msg`, replace its value with `header_value`. - If `header_name` does not exist, add a new header with `header_value`. - **Output:** None 2. `extract_headers(msg) -> dict` - **Input:** - `msg`: A `Message` object. - **Operation:** - Extract and return all headers as a dictionary where keys are header names and values are header values. - **Output:** A dictionary of headers. 3. `set_message_payload(msg, payload, is_binary=False) -> None` - **Input:** - `msg`: A `Message` object. - `payload`: A string or bytes object representing the payload. - `is_binary`: A boolean indicating whether the payload is binary. - **Operation:** - Set the payload of the message. If `is_binary` is `True`, set the payload as a bytes object; otherwise, set it as a string. - **Output:** None 4. `message_to_string(msg) -> str` - **Input:** - `msg`: A `Message` object. - **Operation:** - Convert the entire message to a formatted string. - **Output:** A string representation of the message. Constraints - The `Message` object may contain multiple headers with the same name. - The `payload` can be a simple text message or a binary object. - Header names must be treated case-insensitively for comparisons. Example Usage ```python from email.message import Message msg = Message() add_header(msg, \'Subject\', \'Test Email\') add_header(msg, \'From\', \'sender@example.com\') assert extract_headers(msg) == {\'Subject\': \'Test Email\', \'From\': \'sender@example.com\'} set_message_payload(msg, \'This is a test email body.\') assert msg.get_payload() == \'This is a test email body.\' message_string = message_to_string(msg) print(message_string) ``` Implement the functions to achieve the described behavior.","solution":"from email.message import Message def add_header(msg, header_name, header_value): Adds a header to the email message. If the header already exists, replaces its value. if header_name in msg: msg.replace_header(header_name, header_value) else: msg[header_name] = header_value def extract_headers(msg): Extracts and returns all headers from the email message as a dictionary. return {key: value for key, value in msg.items()} def set_message_payload(msg, payload, is_binary=False): Sets the payload of the message. If is_binary is True, sets as bytes; if False, as string. if is_binary: msg.set_payload(payload) else: msg.set_payload(str(payload)) def message_to_string(msg): Converts the entire message to a formatted string. return msg.as_string()"},{"question":"Objective: This question aims to assess your understanding and ability to implement data-dependent control flow using the `torch.cond` operator in PyTorch. You will be required to design a neural network module that dynamically modifies its forward pass based on specific conditions derived from the input data. Problem Statement: You are required to implement a PyTorch module `ComplexConditionalModule` that demonstrates using `torch.cond` for making data-dependent control flow decisions. The module should perform different operations on the input tensor based on the following conditions: 1. If the sum of all elements in the input tensor is greater than 10, calculate and return the product of the tensor\'s cosine and its absolute value. 2. If the sum of all elements in the input tensor is less than or equal to 10, but greater than 5, return the tensor\'s sine added to its square. 3. If the sum of all elements in the input tensor is less than or equal to 5, return the tensor\'s exponential value. Implementation Requirements: - Define a class `ComplexConditionalModule` inheriting from `torch.nn.Module`. - Implement the `forward` method to execute the conditions described using `torch.cond`. - Use suitable helper functions to handle the operations for each condition. Constraints: - You must use `torch.cond` to handle the conditional operations. - Assume the input tensor will always be a 1-D tensor of floating-point numbers. Expected Input and Output: - **Input**: A 1-D torch tensor `x` of floating-point numbers. - **Output**: A torch tensor resulting from the specified operations. Example Usage: ```python import torch from torch import nn class ComplexConditionalModule(nn.Module): def __init__(self): super().__init__() def forward(self, x: torch.Tensor) -> torch.Tensor: # Define the helper functions for each condition. def greater_than_10_fn(x: torch.Tensor): return x.cos() * x.abs() def between_5_and_10_fn(x: torch.Tensor): return x.sin() + x.pow(2) def less_than_or_equal_5_fn(x: torch.Tensor): return x.exp() # Use torch.cond to apply the corresponding function based on the sum of elements in x. return torch.cond( x.sum() > 10, lambda x: torch.cond( x.sum() > 5, between_5_and_10_fn, less_than_or_equal_5_fn, (x,) ), greater_than_10_fn, (x,) ) # Sample tensors for testing. input_tensor1 = torch.tensor([2.0, 2.0]) input_tensor2 = torch.tensor([3.0, 4.0]) input_tensor3 = torch.tensor([6.0, 6.0]) model = ComplexConditionalModule() output1 = model(input_tensor1) # Case: sum <= 5 output2 = model(input_tensor2) # Case: 5 < sum <= 10 output3 = model(input_tensor3) # Case: sum > 10 print(output1) print(output2) print(output3) ``` You are expected to complete the `ComplexConditionalModule` class definition. Evaluation Criteria: - Correct use of `torch.cond` for data-dependent control flow. - Proper implementation of the helper functions based on the given conditions. - Correct integration of the helper functions within the `forward` method to achieve the desired output. - Maintainable and clean code with appropriate function naming and structure.","solution":"import torch import torch.nn as nn class ComplexConditionalModule(nn.Module): def __init__(self): super(ComplexConditionalModule, self).__init__() def forward(self, x: torch.Tensor): # Define the helper functions for each condition. def greater_than_10_fn(): return x.cos() * x.abs() def between_5_and_10_fn(): return x.sin() + x.pow(2) def less_than_or_equal_5_fn(): return x.exp() # Check the conditions and return the corresponding result. sum_x = x.sum() if sum_x > 10: return greater_than_10_fn() elif sum_x > 5: return between_5_and_10_fn() else: return less_than_or_equal_5_fn()"},{"question":"**Question: Dictionary Operations and Manipulation** You are required to implement a class in Python that simulates some of the behavior of dictionary-related functions described in the documentation provided. Write a class `CustomDict` that includes the following methods: 1. **Initialization** - `__init__(self)`: Initializes an empty dictionary. 2. **Set Item** - `set_item(self, key, value)`: Inserts a key-value pair into the dictionary. If the key already exists, updates its value. 3. **Get Item** - `get_item(self, key)`: Returns the value associated with the key. If the key does not exist, raises a `KeyError`. 4. **Delete Item** - `del_item(self, key)`: Deletes the key-value pair associated with the key. If the key does not exist, raises a `KeyError`. 5. **Contains Key** - `contains(self, key)`: Returns `True` if the dictionary contains the key, `False` otherwise. 6. **Size** - `size(self)`: Returns the number of key-value pairs in the dictionary. 7. **Clear** - `clear(self)`: Clears all items in the dictionary. 8. **Items** - `items(self)`: Returns a list of tuples, where each tuple is a key-value pair in the dictionary. **Constraints:** - Keys must be hashable (e.g., no lists or dictionaries as keys). - The class must handle typical dictionary constraints and edge cases. - You are not allowed to use Python’s built-in dictionary or its methods (like `dict`, `{}.keys()`, `{}.items()`, etc.) except for initialization (`self._dict = {}`). **Example Usage:** ```python mydict = CustomDict() mydict.set_item(\'a\', 1) print(mydict.get_item(\'a\')) # Output: 1 print(mydict.contains(\'a\')) # Output: True print(mydict.size()) # Output: 1 mydict.set_item(\'a\', 2) print(mydict.get_item(\'a\')) # Output: 2 mydict.del_item(\'a\') print(mydict.contains(\'a\')) # Output: False mydict.set_item(\'b\', 3) mydict.set_item(\'c\', 4) print(mydict.items()) # Output: [(\'b\', 3), (\'c\', 4)] mydict.clear() print(mydict.size()) # Output: 0 ``` Implement the `CustomDict` class with all the specified methods.","solution":"class CustomDict: def __init__(self): self._dict = {} def set_item(self, key, value): self._dict[key] = value def get_item(self, key): if key in self._dict: return self._dict[key] else: raise KeyError(f\\"Key {key} not found\\") def del_item(self, key): if key in self._dict: del self._dict[key] else: raise KeyError(f\\"Key {key} not found\\") def contains(self, key): return key in self._dict def size(self): return len(self._dict) def clear(self): self._dict.clear() def items(self): return list(self._dict.items())"},{"question":"Your task is to implement a custom logging setup that logs messages to multiple destinations with different formats and filters. You are to create a logging configuration to send debug and higher level messages to a file, and error and higher level messages to the console. Additionally, you should filter out messages containing a certain keyword from being logged to the console. # Details 1. **File logging**: - Log all messages of level DEBUG and above. - Use a format that includes the timestamp, logger name, log level, and the actual log message. 2. **Console logging**: - Log all messages of level ERROR and above. - Use a simpler format that only includes the log level and the log message. - Exclude any messages that contain the keyword `\\"secret\\"` from being logged to the console. 3. **Custom Filter**: - Implement a custom filter class that filters out messages containing the keyword `\\"secret\\"` for the console handler. # Input The inputs are log messages generated by various parts of the application (this part is simulated by calling sample logging messages in your code). # Output Logs should be output to both a specified file (`application.log`) and the console according to the specification above. # Example Logging Calls ```python import logging # Call these after setting up the logging configuration logging.debug(\'Debug message\') logging.info(\'Info message that is not secret\') logging.error(\'Error with secret info\') logging.error(\'An important error\') ``` # Constraints - Use the logging package and follow the standard practice of configuring logging using dictionary configuration (`dictConfig`). # Implementation Requirements 1. Implement the custom filter. 2. Setup the logging configuration using `dictConfig`. 3. Demonstrate logging several messages to illustrate the setup. # Skeleton Code ```python import logging import logging.config class SecretFilter(logging.Filter): def filter(self, record): return \'secret\' not in record.getMessage() def setup_logging(): logging_config = { \'version\': 1, \'disable_existing_loggers\': False, \'formatters\': { \'standard\': { \'format\': \'%(asctime)s - %(name)s - %(levelname)s - %(message)s\', }, \'simple\': { \'format\': \'%(levelname)s - %(message)s\', }, }, \'filters\': { \'no_secret\': { \'()\': SecretFilter, }, }, \'handlers\': { \'file\': { \'level\': \'DEBUG\', \'class\': \'logging.FileHandler\', \'filename\': \'application.log\', \'formatter\': \'standard\', }, \'console\': { \'level\': \'ERROR\', \'class\': \'logging.StreamHandler\', \'formatter\': \'simple\', \'filters\': [\'no_secret\'], }, }, \'root\': { \'level\': \'DEBUG\', \'handlers\': [\'file\', \'console\'], }, } logging.config.dictConfig(logging_config) def main(): setup_logging() logging.debug(\'Debug message\') logging.info(\'Info message that is not secret\') logging.error(\'Error with secret info\') logging.error(\'An important error\') if __name__ == \'__main__\': main() ``` # Task Extend the `main()` function to demonstrate logging messages appropriately as per the above-defined specifications. The output should show filtered logs in the console and all logs in the file `application.log`.","solution":"import logging import logging.config class SecretFilter(logging.Filter): def filter(self, record): return \'secret\' not in record.getMessage() def setup_logging(): logging_config = { \'version\': 1, \'disable_existing_loggers\': False, \'formatters\': { \'standard\': { \'format\': \'%(asctime)s - %(name)s - %(levelname)s - %(message)s\', }, \'simple\': { \'format\': \'%(levelname)s - %(message)s\', }, }, \'filters\': { \'no_secret\': { \'()\': SecretFilter, }, }, \'handlers\': { \'file\': { \'level\': \'DEBUG\', \'class\': \'logging.FileHandler\', \'filename\': \'application.log\', \'formatter\': \'standard\', }, \'console\': { \'level\': \'ERROR\', \'class\': \'logging.StreamHandler\', \'formatter\': \'simple\', \'filters\': [\'no_secret\'], }, }, \'root\': { \'level\': \'DEBUG\', \'handlers\': [\'file\', \'console\'], }, } logging.config.dictConfig(logging_config) def main(): setup_logging() logging.debug(\'Debug message\') logging.info(\'Info message that is not secret\') logging.error(\'Error with secret info\') logging.error(\'An important error\') logging.warning(\'This warning should appear only in the file\') logging.critical(\'A critical error that should be highlighted\') if __name__ == \'__main__\': main()"},{"question":"# Persistent Storage with `shelve` - Inventory Management System You are tasked with implementing a basic inventory management system using the `shelve` module for persistent storage. The system should allow adding, updating, retrieving, and deleting inventory items. Each item is represented by a dictionary containing `name`, `quantity`, and `price`. Requirements Implement a class `Inventory` that provides methods to interact with the persistent storage: 1. **Initialization and Cleanup:** - `__init__(self, filename: str, writeback: bool = False)`: Initializes the inventory with a given filename and an optional writeback parameter. - `__enter__(self)`: Enters the runtime context. - `__exit__(self, exc_type, exc_value, traceback)`: Exits the runtime context and ensures the shelf is closed properly. 2. **Add or Update Item:** - `add_item(self, item_id: str, name: str, quantity: int, price: float)`: Adds a new item or updates an existing item in the inventory. 3. **Retrieve Item:** - `get_item(self, item_id: str) -> dict`: Retrieves an item by its `item_id`. Returns `None` if the item does not exist. 4. **Delete Item:** - `delete_item(self, item_id: str) -> bool`: Deletes an item by its `item_id`. Returns `True` if the item was deleted, `False` if the item does not exist. 5. **List Items:** - `list_items(self) -> list`: Returns a list of all item IDs in the inventory. Example Usage ```python with Inventory(\'inventory.db\', writeback=True) as inv: # Add or update items inv.add_item(\'001\', \'Apple\', 100, 0.5) inv.add_item(\'002\', \'Orange\', 150, 0.8) # Retrieve item apple = inv.get_item(\'001\') print(apple) # Output: {\'name\': \'Apple\', \'quantity\': 100, \'price\': 0.5} # List all items items = inv.list_items() print(items) # Output: [\'001\', \'002\'] # Delete an item inv.delete_item(\'001\') items = inv.list_items() print(items) # Output: [\'002\'] ``` Constraints - The `item_id` should be a unique string identifier. - Ensure thread safety is not required for this task. - Handle errors and exceptions gracefully, e.g., accessing non-existent keys. - Code should be efficient and clean. Notes - Use the `shelve` module to manage the inventory data. - Ensure the shelf is closed properly to persist changes. - Demonstrate an understanding of the implications of the `writeback` parameter.","solution":"import shelve class Inventory: def __init__(self, filename: str, writeback: bool = False): self.filename = filename self.writeback = writeback self.shelf = None def __enter__(self): self.shelf = shelve.open(self.filename, writeback=self.writeback) return self def __exit__(self, exc_type, exc_value, traceback): if self.shelf is not None: self.shelf.close() def add_item(self, item_id: str, name: str, quantity: int, price: float): self.shelf[item_id] = {\'name\': name, \'quantity\': quantity, \'price\': price} if self.writeback: self.shelf.sync() def get_item(self, item_id: str) -> dict: return self.shelf.get(item_id, None) def delete_item(self, item_id: str) -> bool: if item_id in self.shelf: del self.shelf[item_id] if self.writeback: self.shelf.sync() return True else: return False def list_items(self) -> list: return list(self.shelf.keys())"},{"question":"# Advanced Python Coding Question: Coroutines Objective Design a coroutine-based function in Python that reads data from multiple asynchronous data sources concurrently and returns the aggregated results as a list. This problem will test your understanding of coroutines and their integration into asynchronous workflows in Python. Problem Statement You are given three asynchronous functions, `fetch_data_source_1()`, `fetch_data_source_2()`, and `fetch_data_source_3()`, each returning data after a random delay. Your task is to implement a function `aggregate_data()` that calls these functions concurrently and aggregates their results into a single list. Implement the following function: ```python import asyncio from typing import List, Any async def fetch_data_source_1() -> Any: # Simulate fetching data from source 1 await asyncio.sleep(random.random()) return \\"data from source 1\\" async def fetch_data_source_2() -> Any: # Simulate fetching data from source 2 await asyncio.sleep(random.random()) return \\"data from source 2\\" async def fetch_data_source_3() -> Any: # Simulate fetching data from source 3 await asyncio.sleep(random.random()) return \\"data from source 3\\" async def aggregate_data() -> List[Any]: Calls fetch_data_source_1, fetch_data_source_2, fetch_data_source_3 concurrently and returns their results in a single list. Returns: List[Any]: A list containing results from all three data sources. # Your implementation here pass # Example usage: # asyncio.run(aggregate_data()) ``` Constraints 1. You must use asynchronous programming features (`async` and `await`). 2. Handle asynchronous function calls concurrently. 3. Ensure the aggregate function returns the results as a list in the order the fetch functions were called. Expected Output The function `aggregate_data()` should return a list with three elements, each element being the data retrieved from one of the data sources. Example ```python # If all fetch functions are executed with no delay, the result might look like: # [\\"data from source 1\\", \\"data from source 2\\", \\"data from source 3\\"] result = asyncio.run(aggregate_data()) print(result) ``` Note: The actual order may vary due to random delay added in the fetch functions. The important part is that your function runs them concurrently and collects results correctly.","solution":"import asyncio import random from typing import List, Any async def fetch_data_source_1() -> Any: # Simulate fetching data from source 1 await asyncio.sleep(random.uniform(0, 2)) return \\"data from source 1\\" async def fetch_data_source_2() -> Any: # Simulate fetching data from source 2 await asyncio.sleep(random.uniform(0, 2)) return \\"data from source 2\\" async def fetch_data_source_3() -> Any: # Simulate fetching data from source 3 await asyncio.sleep(random.uniform(0, 2)) return \\"data from source 3\\" async def aggregate_data() -> List[Any]: Calls fetch_data_source_1, fetch_data_source_2, fetch_data_source_3 concurrently and returns their results in a single list. Returns: List[Any]: A list containing results from all three data sources. results = await asyncio.gather( fetch_data_source_1(), fetch_data_source_2(), fetch_data_source_3() ) return results"},{"question":"Build and Validate a WSGI Application You are required to implement a simple WSGI application that returns a personalized greeting message based on query parameters from the URL. You will also need to validate your application for WSGI compliance using the tools provided by the `wsgiref` module. Task 1. **Implement a WSGI application**: - The application should extract the `name` query parameter from the URL. - If the `name` parameter is provided, it should return a greeting message `\\"Hello, [name]!\\"`. - If the `name` parameter is not provided, it should default to `\\"Hello, World!\\"`. 2. **Serve the WSGI application**: - Use the `wsgiref.simple_server.make_server` method to serve your application on `localhost` and port `8000`. 3. **Validate the WSGI application**: - Use the `wsgiref.validate.validator` to wrap and validate your WSGI application. Input and Output Formats **Expected input**: - The application should receive HTTP requests with an optional query parameter `name`. **Expected output**: - On accessing the server without parameters, the output should be: ``` Hello, World! ``` - On accessing the server with the query parameter `name` (e.g., `?name=Alice`), the output should be: ``` Hello, Alice! ``` Constraints - Ensure your application strictly adheres to the WSGI specification outlined in PEP 3333. - You must use the methods and classes from the `wsgiref` module to serve and validate your application. Performance Requirements - Ensure the server runs efficiently and handles multiple requests correctly. Example Code Structure ```python from wsgiref.simple_server import make_server from wsgiref.validate import validator def simple_wsgi_app(environ, start_response): # Your implementation here pass if __name__ == \'__main__\': app = validator(simple_wsgi_app) server = make_server(\'127.0.0.1\', 8000, app) print(\\"Serving on port 8000...\\") server.serve_forever() ``` Fill in the `simple_wsgi_app` function to meet the requirements described. Testing Your Application - Run your server. - Open a web browser or use `curl` to access `http://127.0.0.1:8000` and `http://127.0.0.1:8000/?name=YourName` to verify the responses. - Ensure that no validation errors are raised during the interaction with the wrapped application.","solution":"from wsgiref.simple_server import make_server from wsgiref.validate import validator from urllib.parse import parse_qs def simple_wsgi_app(environ, start_response): # Extract query parameters query_params = parse_qs(environ.get(\'QUERY_STRING\', \'\')) name = query_params.get(\'name\', [\'World\'])[0] # Response body response_body = f\\"Hello, {name}!\\" # Response headers and status status = \'200 OK\' headers = [(\'Content-Type\', \'text/plain; charset=utf-8\')] # Start the response start_response(status, headers) # Return the response body return [response_body.encode(\'utf-8\')] if __name__ == \'__main__\': app = validator(simple_wsgi_app) server = make_server(\'127.0.0.1\', 8000, app) print(\\"Serving on port 8000...\\") server.serve_forever()"},{"question":"**Objective**: Collect real-valued data, clean it of any `NaN` values, and compute various statistical measures using the `statistics` module. **Task**: 1. Write a function `compute_statistics(data)`, which takes a list of floating point numbers (with potential `NaN` values) and returns a dictionary with the following keys and their computed values: - `\'mean\'`: Arithmetic mean. - `\'geometric_mean\'`: Geometric mean. - `\'harmonic_mean\'`: Harmonic mean. - `\'median\'`: Median. - `\'stdev\'`: Sample standard deviation. - `\'variance\'`: Sample variance. - `\'normal_dist_samples\'`: List of 10 samples from a normal distribution created using the cleaned data. - `\'normal_dist_pdf\'`: List of probabilities for the values in `normal_dist_samples` using the normal distribution\'s probability density function (pdf). 2. Ensure that `NaN` values in the data are cleaned before computing the statistics. 3. Raise appropriate exceptions if the cleaned data is insufficient for any statistical calculation. **Input**: - A list of floating-point numbers, e.g., `[20.7, float(\'NaN\'), 19.2, 18.3, float(\'NaN\'), 14.4]`. **Output**: - A dictionary with statistical measures as specified. **Constraints**: - Use the `statistics` and `math` modules for relevant calculations. - Ensure robustness for empty or insufficient data after cleaning out `NaN` values. **Example**: ```python from statistics import mean, geometric_mean, harmonic_mean, median, stdev, variance, NormalDist from math import isnan from itertools import filterfalse def compute_statistics(data): # Clean NaN values clean_data = list(filterfalse(isnan, data)) if len(clean_data) < 2: raise ValueError(\\"Insufficient data after cleaning NaN values.\\") # Compute required statistics result = {} result[\'mean\'] = mean(clean_data) result[\'geometric_mean\'] = geometric_mean(clean_data) result[\'harmonic_mean\'] = harmonic_mean(clean_data) result[\'median\'] = median(clean_data) result[\'stdev\'] = stdev(clean_data) result[\'variance\'] = variance(clean_data) # Create NormalDist and generate samples normal_dist = NormalDist.from_samples(clean_data) result[\'normal_dist_samples\'] = normal_dist.samples(10) # Compute pdf for normal_dist_samples result[\'normal_dist_pdf\'] = [normal_dist.pdf(x) for x in result[\'normal_dist_samples\']] return result # Example usage data = [20.7, float(\'NaN\'), 19.2, 18.3, float(\'NaN\'), 14.4] print(compute_statistics(data)) ``` **Output**: ```python { \'mean\': 18.15, \'geometric_mean\': 17.58656772026813, \'harmonic_mean\': 16.856474077105213, \'median\': 18.75, \'stdev\': 2.744158382876408, \'variance\': 7.528541666666668, \'normal_dist_samples\': [...], # list of 10 samples \'normal_dist_pdf\': [...] # corresponding list of pdf values } ``` **Note**: Ensure that the implementation handles edge cases like all data being `NaN`, mixed valid and `NaN` values, empty list after cleaning, etc.","solution":"import math import statistics from itertools import filterfalse from statistics import NormalDist def compute_statistics(data): # Clean NaN values clean_data = list(filterfalse(math.isnan, data)) if len(clean_data) < 2: raise ValueError(\\"Insufficient data after cleaning NaN values.\\") # Compute required statistics result = { \'mean\': statistics.mean(clean_data), \'geometric_mean\': statistics.geometric_mean(clean_data), \'harmonic_mean\': statistics.harmonic_mean(clean_data), \'median\': statistics.median(clean_data), \'stdev\': statistics.stdev(clean_data), \'variance\': statistics.variance(clean_data), } # Create NormalDist and generate samples normal_dist = NormalDist.from_samples(clean_data) result[\'normal_dist_samples\'] = normal_dist.samples(10) # Compute pdf for normal_dist_samples result[\'normal_dist_pdf\'] = [normal_dist.pdf(x) for x in result[\'normal_dist_samples\']] return result"},{"question":"Objective Your task is to create a Python script that processes a text file specified by command-line arguments. The script should read the file, count the frequency of each word, and log the results. The script must handle both valid and invalid inputs gracefully, logging appropriate errors. Requirements 1. **Command-Line Arguments Parsing** - Use the `argparse` module to parse command-line arguments. - The script should accept a single argument: the path to the text file to be processed. - An optional argument `--verbose` should control the verbosity of the log output. If provided, detailed logs should be generated. 2. **File Reading and Processing** - Read the file specified by the command-line argument. - Handle the case where the file might not be found with a proper error message logged. - Count the frequency of each word in the file. Words should be treated case-insensitively (e.g., \\"Python\\" and \\"python\\" should be considered the same word). 3. **Logging** - Implement logging using the `logging` module. - Log at INFO level for normal operations and at ERROR level for any issues encountered. - If the `--verbose` flag is set, additional DEBUG-level information should be logged. - Log the word frequency count. Constraints - The script should handle large files efficiently. - Use appropriate error handling for file operations and command-line parsing. Expected Input and Output - **Input**: A path to a text file provided via command-line. - **Output**: Logs of word counts and errors (if any). Example Usage ```shell python word_count.py text_file.txt --verbose ``` Hints - Consider using `try-except` blocks to handle file I/O errors. - Use `argparse.ArgumentParser` for command-line arguments parsing. - Use `logging.basicConfig()` to configure logging. Performance Recommendations - Efficiently read and process the file, consider using buffered reading for handling large files. - Optimize word counting by using appropriate data structures. You are required to implement the function as follows: Function Signature ```python def main(): pass ``` Implementation Notes - Implement the main function to be called when the script is executed. - Ensure the script includes appropriate `if __name__ == \\"__main__\\":` boilerplate to allow or prevent parts of code from being run when the modules are imported. --- Happy coding!","solution":"import argparse import logging import os def count_words(file_path, verbose=False): Count the frequency of each word in the specified file. Args: file_path (str): The path to the text file. verbose (bool): If True, enables verbose logging. Returns: dict: A dictionary with words as keys and their frequencies as values. log_level = logging.DEBUG if verbose else logging.INFO logging.basicConfig(level=log_level, format=\'%(asctime)s - %(levelname)s - %(message)s\') logging.info(f\\"Processing file: {file_path}\\") if not os.path.exists(file_path): logging.error(f\\"File not found: {file_path}\\") return None word_count = {} try: with open(file_path, \'r\', encoding=\'utf-8\') as file: for line in file: words = line.lower().split() for word in words: word = word.strip(\\",.!?\\"\'()[]{}<>:;\\") if word: word_count[word] = word_count.get(word, 0) + 1 except Exception as e: logging.error(f\\"Error reading file: {e}\\") return None logging.info(f\\"Word count completed for {file_path}\\") for word, count in word_count.items(): logging.debug(f\\"Word: {word}, Count: {count}\\") return word_count def main(): parser = argparse.ArgumentParser(description=\\"Count word frequency in a text file.\\") parser.add_argument(\\"file_path\\", type=str, help=\\"The path to the text file to be processed.\\") parser.add_argument(\\"--verbose\\", action=\\"store_true\\", help=\\"Enable verbose logging output.\\") args = parser.parse_args() word_count = count_words(args.file_path, args.verbose) if word_count is not None: logging.info(\\"Word Frequencies:\\") for word, count in word_count.items(): logging.info(f\\"{word}: {count}\\") if __name__ == \\"__main__\\": main()"},{"question":"Objective To assess your understanding and ability to implement validation curves and learning curves using scikit-learn for model evaluation. Problem Statement You are given a dataset to classify iris flowers into three species based on their features. Your tasks are: 1. Generate and plot a validation curve to evaluate the impact of the hyperparameter `C` on the performance of an SVM model with a linear kernel. 2. Generate and plot a learning curve to evaluate the effect of training set size on the performance of the same SVM model. Datasets and Libraries Use the Iris dataset from scikit-learn. Task 1: Validation Curve 1. Load the Iris dataset. 2. Shuffle the dataset to ensure randomness. 3. Generate training and validation scores using `validation_curve` with the SVM model (`kernel=\\"linear\\"`) by varying the hyperparameter `C` over a range of values. 4. Plot the validation curve using `ValidationCurveDisplay`. Task 2: Learning Curve 1. Use the same shuffled dataset. 2. Generate training and validation scores using `learning_curve` with the SVM model (`kernel=\\"linear\\"`), for varying sizes of the training dataset. 3. Plot the learning curve using `LearningCurveDisplay`. Constraints - Use a random state of 0 for reproducibility. - Use 5-fold cross-validation. - The hyperparameter `C` should be varied over a logarithmic scale from 10^-7 to 10^3, inclusive. - Training sizes should be [50, 80, 110]. Input None. Use the Iris dataset provided within scikit-learn. Output 1. Display the validation curve for various values of `C`. 2. Display the learning curve for different training sizes. Additional Notes - Ensure your plots are well-labeled with titles, axis labels, and legends where applicable. - Interpret the plots to briefly discuss if the model encounters underfitting, overfitting, or performs well. Example Code for Loading and Shuffling Dataset ```python from sklearn.datasets import load_iris from sklearn.utils import shuffle # Load and shuffle dataset X, y = load_iris(return_X_y=True) X, y = shuffle(X, y, random_state=0) ``` # Solution Template ```python import numpy as np from sklearn.datasets import load_iris from sklearn.model_selection import validation_curve, learning_curve, ValidationCurveDisplay, LearningCurveDisplay from sklearn.svm import SVC from sklearn.utils import shuffle import matplotlib.pyplot as plt # Load and shuffle dataset X, y = load_iris(return_X_y=True) X, y = shuffle(X, y, random_state=0) # Task 1: Validation Curve param_range = np.logspace(-7, 3, 10) train_scores, valid_scores = validation_curve( SVC(kernel=\\"linear\\"), X, y, param_name=\\"C\\", param_range=param_range, cv=5 ) # Plot validation curve plt.figure(figsize=(8, 6)) ValidationCurveDisplay.from_estimator(SVC(kernel=\\"linear\\"), X, y, param_name=\\"C\\", param_range=param_range, cv=5) plt.title(\'Validation Curve with SVM\') plt.show() # Task 2: Learning Curve train_sizes, train_scores, valid_scores = learning_curve( SVC(kernel=\'linear\'), X, y, train_sizes=[50, 80, 110], cv=5 ) # Plot learning curve plt.figure(figsize=(8, 6)) LearningCurveDisplay.from_estimator(SVC(kernel=\\"linear\\"), X, y, train_sizes=[50, 80, 110], cv=5) plt.title(\'Learning Curve with SVM\') plt.show() ``` Interpretation After plotting, briefly interpret the results. Discuss whether the model is underfitting, overfitting, or performing well based on the curves.","solution":"import numpy as np from sklearn.datasets import load_iris from sklearn.model_selection import validation_curve, learning_curve from sklearn.svm import SVC from sklearn.utils import shuffle import matplotlib.pyplot as plt # Load and shuffle dataset X, y = load_iris(return_X_y=True) X, y = shuffle(X, y, random_state=0) # Task 1: Validation Curve param_range = np.logspace(-7, 3, 10) train_scores, valid_scores = validation_curve( SVC(kernel=\\"linear\\"), X, y, param_name=\\"C\\", param_range=param_range, cv=5 ) # Plot validation curve plt.figure(figsize=(8, 6)) plt.plot(param_range, np.mean(train_scores, axis=1), label=\\"Training score\\") plt.plot(param_range, np.mean(valid_scores, axis=1), label=\\"Cross-validation score\\") plt.xlabel(\'Parameter C\') plt.ylabel(\'Score\') plt.xscale(\'log\') plt.title(\'Validation Curve with SVM\') plt.legend(loc=\\"best\\") plt.show() # Task 2: Learning Curve train_sizes, train_scores, valid_scores = learning_curve( SVC(kernel=\'linear\'), X, y, train_sizes=[50, 80, 110], cv=5 ) # Plot learning curve plt.figure(figsize=(8, 6)) plt.plot(train_sizes, np.mean(train_scores, axis=1), label=\\"Training score\\") plt.plot(train_sizes, np.mean(valid_scores, axis=1), label=\\"Cross-validation score\\") plt.xlabel(\'Training examples\') plt.ylabel(\'Score\') plt.title(\'Learning Curve with SVM\') plt.legend(loc=\\"best\\") plt.show()"},{"question":"Coding Assessment Question # Objective: Implement and interpret Partial Dependence and Individual Conditional Expectation plots to understand the effect of features on a predictive model. # Background: You are provided with a dataset and a pre-trained predictive model. Your task is to generate PDP and ICE plots for specific features, analyze these plots, and derive insights about the model\'s behavior concerning these features. # Dataset: You will use the breast cancer dataset from `sklearn.datasets`. # Task: 1. **Load the Dataset:** Load the breast cancer dataset from `sklearn.datasets`. 2. **Train a Model:** Train a `GradientBoostingClassifier` on this dataset. 3. **Generate and Plot PDP:** Create partial dependence plots for the two most important features of the dataset. Visualize both one-way and two-way partial dependence. 4. **Generate and Plot ICE:** Create individual conditional expectation plots for the two most important features of the dataset. 5. **Interpret the Results:** Provide a brief analysis of the PDP and ICE plots, discussing the effect of each feature on the model predictions. # Expected Input and Output: - **Input:** Breast cancer dataset loaded within the function. - **Output:** PDP and ICE plots visualized with matplotlib, and a textual interpretation of the plots. # Constraints and Requirements: 1. Use `GradientBoostingClassifier` with standard hyperparameters. 2. Ensure the plots are clearly labeled and appropriately scaled. 3. Write well-documented and clean code. # Performance Requirements: - The model training and plot generation should execute within a reasonable time frame, under 5 minutes. # Code Template: ```python import numpy as np from sklearn.datasets import load_breast_cancer from sklearn.ensemble import GradientBoostingClassifier from sklearn.inspection import PartialDependenceDisplay import matplotlib.pyplot as plt def load_data(): data = load_breast_cancer() X, y = data.data, data.target return X, y, data.feature_names def train_model(X, y): clf = GradientBoostingClassifier(random_state=42) clf.fit(X, y) return clf def plot_pdp(clf, X, feature_names, features): display = PartialDependenceDisplay.from_estimator(clf, X, features=features) display.figure_.suptitle(\'Partial Dependence Plot\') plt.show() def plot_ice(clf, X, feature_names, features): display = PartialDependenceDisplay.from_estimator(clf, X, features=features, kind=\'individual\') display.figure_.suptitle(\'Individual Conditional Expectation Plot\') plt.show() def main(): X, y, feature_names = load_data() clf = train_model(X, y) # Identify the two most important features (manually or via feature_importances_) # For this task, assume indices 0 and 1 are the most important features = [0, 1, (0, 1)] plot_pdp(clf, X, feature_names, features[:2]) # One-way PDP for first two features plot_pdp(clf, X, feature_names, [features[2]]) # Two-way PDP for first two features plot_ice(clf, X, feature_names, features[:2]) # ICE for first two features # Interpretation can be provided as a comment or printed in the main function print(\\"Interpretation:\\") print(\\"1. Discuss the relationship between each feature and the target.\\") print(\\"2. Explain any interactions observed in the two-way PDP.\\") if __name__ == \\"__main__\\": main() ``` # Interpretation: In the main function, after plotting, provide a brief interpretation of the PDP and ICE plots. Discuss how each feature impacts the target prediction and any observed interactions between features.","solution":"import numpy as np from sklearn.datasets import load_breast_cancer from sklearn.ensemble import GradientBoostingClassifier from sklearn.inspection import PartialDependenceDisplay import matplotlib.pyplot as plt def load_data(): data = load_breast_cancer() X, y = data.data, data.target return X, y, data.feature_names def train_model(X, y): clf = GradientBoostingClassifier(random_state=42) clf.fit(X, y) return clf def plot_pdp(clf, X, feature_names, features): display = PartialDependenceDisplay.from_estimator(clf, X, features=features, feature_names=feature_names) display.figure_.suptitle(\'Partial Dependence Plot\') plt.show() def plot_ice(clf, X, feature_names, features): display = PartialDependenceDisplay.from_estimator(clf, X, features=features, kind=\'individual\', feature_names=feature_names) display.figure_.suptitle(\'Individual Conditional Expectation Plot\') plt.show() def main(): X, y, feature_names = load_data() clf = train_model(X, y) # Identify the two most important features most_important_features = np.argsort(clf.feature_importances_)[-2:] features = [most_important_features[0], most_important_features[1], (most_important_features[0], most_important_features[1])] plot_pdp(clf, X, feature_names, features[:2]) # One-way PDP for first two features plot_pdp(clf, X, feature_names, [features[2]]) # Two-way PDP for first two features plot_ice(clf, X, feature_names, features[:2]) # ICE for first two features # Interpretation print(\\"Interpretation:\\") print(f\\"The two most important features are: {feature_names[most_important_features[0]]} and {feature_names[most_important_features[1]]}\\") print(\\"1. Discuss the relationship between each feature and the target.\\") print(\\"2. Explain any interactions observed in the two-way PDP.\\") if __name__ == \\"__main__\\": main()"},{"question":"Title: Implement a Reversing and Analysis Tool for Audio Fragments **Objective:** Create a Python function `process_audio_fragment` that takes an audio fragment, reverses it, computes its RMS value, and converts the reversed fragment from 16-bit linear encoding to 8-bit a-LAW encoding. **Function Signature:** ```python def process_audio_fragment(fragment: bytes, width: int) -> (bytes, float): pass ``` **Input:** - `fragment`: A bytes-like object representing the audio fragment. - `width`: An integer representing the sample width of the fragment in bytes (valid values are 1, 2, 3, or 4). **Output:** - A tuple containing: - The reversed audio fragment converted to 8-bit a-LAW encoding (as a bytes object). - The RMS value of the original fragment (as a float). **Constraints:** - The function should handle errors and invalid input gracefully, raising an appropriate exception if the input conditions are not met. - The sample width must be valid for the operations being used. - Ensure that the sample size conversions (e.g., 16-bit to a-LAW) are handled according to the provided documentation. **Example:** ```python fragment = b\'x01x02x03x04\' width = 2 # Expected output reversed_fragment = b\'alaw_encoded_data\' rms_value = 1.58 result = process_audio_fragment(fragment, width) assert isinstance(result, tuple) assert result[0] == reversed_fragment assert result[1] == rms_value ``` **Notes:** - Use `audioop.reverse`, `audioop.rms`, and `audioop.lin2alaw` functions to implement this tool. - Handle all possible edge cases such as empty fragments and invalid sample widths appropriately.","solution":"import audioop def process_audio_fragment(fragment: bytes, width: int) -> (bytes, float): Processes an audio fragment by reversing it, computing its RMS value, and converting it to 8-bit a-LAW encoding. Args: fragment (bytes): The audio fragment. width (int): The sample width in bytes. Returns: tuple: A tuple containing the reversed and a-LAW encoded fragment and the RMS value of the original fragment. if width not in [1, 2, 3, 4]: raise ValueError(\\"Invalid sample width. Width must be 1, 2, 3, or 4 bytes.\\") # Reversing the fragment reversed_fragment = audioop.reverse(fragment, width) # Calculating RMS value rms_value = audioop.rms(fragment, width) # Converting reversed fragment to 8-bit a-LAW encoding alaw_fragment = audioop.lin2alaw(reversed_fragment, width) return alaw_fragment, float(rms_value)"},{"question":"Objective To assess your understanding of PyTorch\'s data loading utilities, you will implement a custom PyTorch Dataset, a DataLoader, and utilize various DataLoader features such as multi-process loading, batching, and custom collation. Problem Statement You are given a dataset consisting of multiple text files, each containing numerical data. You are required to: 1. Implement a custom `Map-style` Dataset class that reads these text files and returns the data as PyTorch Tensors. 2. Implement a DataLoader for this dataset with the following features: - Batch size of 16. - Shuffling enabled. - Multi-process data loading using 4 worker processes. - Custom collate function that pads sequences to the maximum length in the batch. - Memory pinning enabled. # Dataset Description - Each text file contains a list of integers, separated by spaces. - All files are stored in a directory. # Requirements 1. **Custom Dataset Class**: - Takes the directory path containing text files as an initialization argument. - Implements `__getitem__` to read a file and return its content as a PyTorch Tensor. - Implements `__len__` to return the number of text files. 2. **Custom Collate Function**: - Pads sequences in a batch to the maximum length of the sequences in that batch. 3. **DataLoader**: - Uses the custom dataset and custom collate function. - Configured with parameters specified above. # Constraints - You should not read all files into memory at once; each file should be read as needed. - Ensure your implementation is efficient and handles large datasets gracefully. # Input and Output - **Input**: Directory path to the dataset. - **Output**: Batches of padded sequences. # Example ```python import torch from torch.utils.data import DataLoader, Dataset import os class CustomTextDataset(Dataset): def __init__(self, directory_path): self.directory_path = directory_path self.file_names = os.listdir(directory_path) def __len__(self): return len(self.file_names) def __getitem__(self, idx): file_path = os.path.join(self.directory_path, self.file_names[idx]) with open(file_path, \'r\') as file: data = list(map(int, file.read().strip().split())) return torch.tensor(data, dtype=torch.int64) def custom_collate_fn(batch): batch = [item.tolist() for item in batch] max_length = max(len(item) for item in batch) padded_batch = [item + [0] * (max_length - len(item)) for item in batch] return torch.tensor(padded_batch, dtype=torch.int64) def main(): dataset_path = \'path/to/dataset\' dataset = CustomTextDataset(dataset_path) dataloader = DataLoader(dataset, batch_size=16, shuffle=True, num_workers=4, collate_fn=custom_collate_fn, pin_memory=True) for batch in dataloader: print(batch) if __name__ == \'__main__\': main() ``` # Instructions 1. Implement the `CustomTextDataset` class. 2. Implement the `custom_collate_fn` function. 3. Configure the DataLoader using the provided specifications. 4. Test your DataLoader by iterating through the batches and verifying the output. Good luck!","solution":"import torch from torch.utils.data import DataLoader, Dataset import os class CustomTextDataset(Dataset): def __init__(self, directory_path): self.directory_path = directory_path self.file_names = os.listdir(directory_path) def __len__(self): return len(self.file_names) def __getitem__(self, idx): file_path = os.path.join(self.directory_path, self.file_names[idx]) with open(file_path, \'r\') as file: data = list(map(int, file.read().strip().split())) return torch.tensor(data, dtype=torch.int64) def custom_collate_fn(batch): batch = [item.tolist() for item in batch] max_length = max(len(item) for item in batch) padded_batch = [item + [0] * (max_length - len(item)) for item in batch] return torch.tensor(padded_batch, dtype=torch.int64) def get_dataloader(dataset_path): dataset = CustomTextDataset(dataset_path) dataloader = DataLoader(dataset, batch_size=16, shuffle=True, num_workers=4, collate_fn=custom_collate_fn, pin_memory=True) return dataloader"},{"question":"# Assessing Advanced Pandas Proficiency You are tasked with analyzing a sales dataset to derive meaningful insights and visualizations. You are provided with a CSV file named `sales_data.csv` that contains the following columns: - `date`: The date of the sale (format: YYYY-MM-DD). - `region`: The region where the sale took place. - `product`: The product that was sold. - `quantity`: The quantity of the product sold. - `price`: The price at which the product was sold. Your task is to implement a function `analyze_sales_data(filepath)` that reads the CSV file and performs the following operations: 1. **Read the CSV file** into a pandas DataFrame. 2. **Create a `total_sales` column** that represents the total sales amount for each record (quantity * price). 3. **Generate summary statistics**: - Total sales for each region. - Total sales for each product. - Daily total sales across all regions and products. 4. **Identify the best-selling product** (by total sales) in each region. 5. **Handle missing values** in the dataset. For this, assume that: - Missing `quantity` values should be treated as 0. - Missing `price` values should be filled with the median price of the respective product. 6. **Plot a time series graph** of daily total sales. 7. Return the processed DataFrame with an additional column `total_sales` and the two summary statistics DataFrames (for region and product). # Input - `filepath`: A string representing the path to the sales data CSV file. # Output - A tuple containing three DataFrames: the processed DataFrame with the `total_sales` column, the region-wise sales summary DataFrame, and the product-wise sales summary DataFrame. # Function Signature ```python import pandas as pd def analyze_sales_data(filepath: str) -> tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]: pass ``` # Example Assume the following data in `sales_data.csv`: ```plaintext date,region,product,quantity,price 2022-01-01,North,Product A,10,15.5 2022-01-01,North,Product B,5,20.0 2022-01-01,South,Product A,,18.0 2022-01-02,North,Product A,7,15.5 2022-01-02,South,Product C,3,21.0 2022-01-02,East,Product B,12,19.0 2022-01-02,West,Product C,8,22.0 ``` The function should process this dataset to handle missing values, add a `total_sales` column, provide summaries, and plot the time series graph of daily total sales. # Constraints - Assume the dataset contains no duplicates. - The CSV file can have up to 100,000 rows. - Ensure your function is optimized for performance. # Notes - Use appropriate pandas functions and methods to perform the required operations efficiently. - Visualize the time series graph using a suitable plotting library like Matplotlib or the built-in plotting capabilities of pandas.","solution":"import pandas as pd import matplotlib.pyplot as plt def analyze_sales_data(filepath: str) -> tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]: # Read the CSV file into a pandas DataFrame df = pd.read_csv(filepath) # Handle missing values df[\'quantity\'] = df[\'quantity\'].fillna(0) # Fill missing prices with the median price of the respective product df[\'price\'] = df.groupby(\'product\')[\'price\'].transform(lambda x: x.fillna(x.median())) # Create a total_sales column df[\'total_sales\'] = df[\'quantity\'] * df[\'price\'] # Total sales for each region region_sales = df.groupby(\'region\')[\'total_sales\'].sum().reset_index() region_sales.columns = [\'region\', \'total_sales\'] # Total sales for each product product_sales = df.groupby(\'product\')[\'total_sales\'].sum().reset_index() product_sales.columns = [\'product\', \'total_sales\'] # Daily total sales across all regions and products daily_sales = df.groupby(\'date\')[\'total_sales\'].sum().reset_index() # Identify the best-selling product in each region best_selling_products = df.groupby([\'region\', \'product\'])[\'total_sales\'].sum().reset_index() best_selling_products = best_selling_products.loc[best_selling_products.groupby(\'region\')[\'total_sales\'].idxmax()] # Plot a time series graph of daily total sales plt.figure(figsize=(10, 6)) plt.plot(daily_sales[\'date\'], daily_sales[\'total_sales\'], marker=\'o\') plt.title(\'Daily Total Sales\') plt.xlabel(\'Date\') plt.ylabel(\'Total Sales\') plt.xticks(rotation=45) plt.tight_layout() plt.show() # Return the processed DataFrame, region sales summary, and product sales summary return df, region_sales, product_sales"},{"question":"Coding Assessment Question # Objective Implement a function to simulate the behavior of selecting and excluding files based on Unix-style glob patterns. The function will help in understanding how file patterns work in conjunction with file operations. # Problem Statement You need to write a function named `filter_files` that takes two lists as input: `all_files` and `patterns`. - `all_files`: A list of strings where each string is a file path. - `patterns`: A list of strings where each string is a Unix-style glob pattern. This list may contain `include`, `exclude`, `recursive-include`, and `recursive-exclude` commands with their patterns. The `filter_files` function should return a list of files that match the given patterns if the pattern command is `include` or `recursive-include` and exclude files if the pattern command is `exclude` or `recursive-exclude`. # Input - `all_files`: List of strings representing file paths. - `patterns`: List of commands with their glob patterns. # Output - A list of strings representing the filtered file paths. # Constraints - File paths are non-empty strings containing alphanumeric characters, forward slashes (`/`), and underscores (`_`). - Patterns will be well-formed and valid. - The length of `all_files` will not exceed 1000. - The length of `patterns` will not exceed 500. # Example ```python all_files = [ \\"src/code.py\\", \\"src/utils/helpers.py\\", \\"tests/test_code.py\\", \\"README.md\\", \\"docs/installation.txt\\", \\"src/main.py\\" ] patterns = [ \\"include *.py\\", \\"exclude *test*.py\\", \\"recursive-include src *.py\\", \\"recursive-exclude src/utils *\\" ] print(filter_files(all_files, patterns)) ``` # Expected Output ``` [\'src/code.py\', \'src/main.py\'] ``` # Explanation - `include *.py`: Includes all `.py` files. - `exclude *test*.py`: Excludes all files with `test` in their names. - `recursive-include src *.py`: Includes all `.py` files under the `src` directory. - `recursive-exclude src/utils *`: Excludes all files under the `src/utils` directory. # Note - Prioritize `exclude` commands over `include`. - Ensure the function handles nested directories and pattern matching accurately. # Implementation You may assume the availability of necessary modules for pattern matching and file operations. Provide efficient and clear code to manage the filtering tasks. ```python def filter_files(all_files, patterns): # Implementation goes here pass ```","solution":"import fnmatch def filter_files(all_files, patterns): included_files = set() excluded_files = set() for command in patterns: action, pattern = command.split(maxsplit=1) if action == \\"include\\": matched_files = fnmatch.filter(all_files, pattern) included_files.update(matched_files) elif action == \\"exclude\\": matched_files = fnmatch.filter(all_files, pattern) excluded_files.update(matched_files) elif action == \\"recursive-include\\": dir_path, file_pattern = pattern.rsplit(\' \', 1) matched_files = [ file for file in all_files if file.startswith(dir_path) and fnmatch.fnmatch(file, f\'{dir_path}/{file_pattern}\') ] included_files.update(matched_files) elif action == \\"recursive-exclude\\": dir_path, file_pattern = pattern.rsplit(\' \', 1) matched_files = [ file for file in all_files if file.startswith(dir_path) and fnmatch.fnmatch(file, f\'{dir_path}/{file_pattern}\') ] excluded_files.update(matched_files) result_files = included_files - excluded_files return sorted(result_files)"},{"question":"# SAX XML Parsing: Custom Handler Implementation You are required to implement a custom XML handler using the SAX (Simple API for XML) interface provided by the `xml.sax` package in Python. Your handler must process an XML file that contains a catalog of books and extract specific details to produce a structured summary of the books. **Input Format** - An XML file named `books.xml` containing a catalog of books. Each book has the following structure: ```xml <catalog> <book id=\\"1\\"> <title>Effective Python</title> <author>Brett Slatkin</author> <genre>Programming</genre> <price>30.00</price> <publish_date>2021-05-21</publish_date> <description>...</description> </book> ... </catalog> ``` **Output Format** - A dictionary where each key is a book ID and the value is a nested dictionary containing the book\'s details (title, author, genre, price, publish_date, description). Example output: ```python { \\"1\\": { \\"title\\": \\"Effective Python\\", \\"author\\": \\"Brett Slatkin\\", \\"genre\\": \\"Programming\\", \\"price\\": \\"30.00\\", \\"publish_date\\": \\"2021-05-21\\", \\"description\\": \\"...\\" }, ... } ``` **Constraints** - Assume that all book elements and their sub-elements are always present in the XML file. - If there are any parsing errors, your handler should catch the exceptions and print a meaningful error message. - You are not allowed to use external XML parsing libraries other than `xml.sax`. **Performance Requirements** - Your solution should be able to handle XML files of size up to 10 MB efficiently. **Implementation Details** 1. Implement a custom handler class that inherits from `xml.sax.ContentHandler`. 2. Override the necessary methods to handle the XML elements (`startElement`, `endElement`, `characters`). 3. Use the `xml.sax` module functions such as `make_parser` and `parse` to read and process the XML file. 4. Ensure your handler collects the book details and stores them in the specified output format. ```python import xml.sax class BookHandler(xml.sax.ContentHandler): def __init__(self): self.current_data = \\"\\" self.books = {} self.book_id = \\"\\" self.book_details = {} def startElement(self, tag, attributes): self.current_data = tag if tag == \\"book\\": self.book_id = attributes[\\"id\\"] self.book_details = {} def endElement(self, tag): if tag == \\"book\\": self.books[self.book_id] = self.book_details self.current_data = \\"\\" def characters(self, content): if self.current_data: self.book_details[self.current_data] = content def endDocument(self): print(self.books) def parse_books(file_path): parser = xml.sax.make_parser() parser.setFeature(xml.sax.handler.feature_namespaces, 0) handler = BookHandler() parser.setContentHandler(handler) try: parser.parse(file_path) except xml.sax.SAXParseException as e: print(f\\"Parsing error: {e.getMessage()}\\") except Exception as e: print(f\\"Error: {str(e)}\\") # Example usage: # parse_books(\\"books.xml\\") ``` This handler extracts the necessary book details and processes the XML input efficiently as specified.","solution":"import xml.sax class BookHandler(xml.sax.ContentHandler): def __init__(self): self.current_data = \\"\\" self.books = {} self.book_id = \\"\\" self.book_details = {} self.buffer = \\"\\" def startElement(self, tag, attributes): self.current_data = tag if tag == \\"book\\": self.book_id = attributes[\\"id\\"] self.book_details = {} def endElement(self, tag): if tag == \\"book\\": self.books[self.book_id] = self.book_details elif self.current_data and self.current_data != \\"catalog\\": self.book_details[self.current_data] = self.buffer self.buffer = \\"\\" self.current_data = \\"\\" def characters(self, content): if self.current_data and self.current_data != \\"catalog\\": self.buffer += content.strip() def endDocument(self): pass def parse_books(file_path): parser = xml.sax.make_parser() parser.setFeature(xml.sax.handler.feature_namespaces, 0) handler = BookHandler() parser.setContentHandler(handler) try: parser.parse(file_path) return handler.books except xml.sax.SAXParseException as e: print(f\\"Parsing error: {e.getMessage()}\\") except Exception as e: print(f\\"Error: {str(e)}\\")"},{"question":"# HTML Content Sanitizer **Objective**: Write a function `sanitize_html(input_string)` that takes a string containing HTML content and returns a sanitized version of the string. The function should sanitize the input in two ways: 1. Escape any HTML special characters to their corresponding HTML entities. 2. Convert any existing HTML entities back to their respective characters. **Function Signature**: ```python def sanitize_html(input_string: str) -> str: pass ``` # Input: - `input_string` (str): A string containing HTML content. The string might contain characters like `\\"&\\"`, `\\"<\\"`, `\\">\\"`, `\\"\'\\"`, `\\"`\\"` that need to be escaped, and may also contain existing HTML entities such as `\\"&gt;\\"`, `\\"&#62;\\"`, `\\"&#x3e;\\"` that need to be unescaped. # Output: - `result` (str): A sanitized version of the input string where all special HTML characters are escaped, and any existing HTML entities are converted to their corresponding characters. # Constraints: - The input string will have a maximum length of 10,000 characters. # Example: ```python # Example 1 input_string = \\"Cookies & Cream < Must-Buy! > &gt; &#x3e;\\" print(sanitize_html(input_string)) # Expected Output: \\"Cookies &amp; Cream &lt; Must-Buy! &gt; > >\\" # Example 2 input_string = \'Hello \\"World\\" & welcome <to> the party!\' print(sanitize_html(input_string)) # Expected Output: \'Hello &quot;World&quot; &amp; welcome &lt;to&gt; the party!\' ``` # Implementation Requirements: - Use the `html.escape` and `html.unescape` functions from the Python `html` package to perform the necessary transformations. - Your function should handle edge cases and ensure that both operations (escaping and unescaping) are applied in the correct order to avoid any unintended double encoding or decoding. **Note**: The resulting string should first unescape any existing HTML entities and then escape the special characters to ensure the final output is correctly sanitized and safe for HTML display.","solution":"import html def sanitize_html(input_string: str) -> str: Sanitize HTML input by first unescaping any existing HTML entities, and then escaping special characters. Args: input_string (str): A string containing HTML content. Returns: str: Sanitized HTML string. # Unescape HTML entities to their respective characters unescaped_string = html.unescape(input_string) # Escape special characters to HTML entities sanitized_string = html.escape(unescaped_string) return sanitized_string"},{"question":"**Objective**: To assess your understanding of merging, joining, concatenating, and comparing `DataFrame` objects in pandas. **Problem Statement**: You are provided with multiple datasets containing sales data from three different regions for various quarters. Your task is to perform various operations on these dataframes to prepare a consolidated sales report, identify missing values, and highlight any discrepancies between two given reports. **Datasets**: 1. `df_north` - Quarterly sales data for the northern region 2. `df_south` - Quarterly sales data for the southern region 3. `df_east` - Quarterly sales data for the eastern region 4. `df_west` - Quarterly sales data for the western region **Data Structure**: The datasets have the following columns: - `Region` (e.g., North, South, East, West) - `Quarter` (e.g., Q1, Q2, Q3, Q4) - `Sales` (e.g., 10000, 20000, etc.) Your task includes the following steps: 1. **Concatenate** all regional `DataFrame` objects into a single `DataFrame`, ensuring that any missing data results in `NaN` values. 2. **Merge** this consolidated `DataFrame` with the forecast data based on `Region` and `Quarter`. The forecast data, `df_forecast`, has columns: `Region`, `Quarter`, `Forecasted_Sales`. 3. **Compare** the consolidated sales report with another provided report `df_report_2` to highlight any discrepancies in the `Sales` and `Forecasted_Sales` columns. 4. **Identify** and fill missing `Sales` values with the corresponding `Forecasted_Sales` values. **Constraints**: - Ensure that the concatenation preserves the original order of regions and quarters. - Missing data should be represented as `NaN`. - The comparison should only highlight differences, omitting unchanged values. - Fill missing data in place without creating a new `DataFrame`. **Input and Output Formats**: - **Input**: 5 pandas `DataFrame` objects (`df_north`, `df_south`, `df_east`, `df_west`, `df_forecast`) and another report `df_report_2`. - **Output**: A tuple containing two items: 1. The consolidated `DataFrame` after filling in missing values. 2. A `DataFrame` highlighting discrepancies between the consolidated sales report and `df_report_2`. # Function Signature: ```python import pandas as pd def prepare_sales_report(df_north, df_south, df_east, df_west, df_forecast, df_report_2): # Your code here return (consolidated_df, discrepancies_df) # Example DataFrames for testing df_north = pd.DataFrame({ \\"Region\\": [\\"North\\", \\"North\\", \\"North\\", \\"North\\"], \\"Quarter\\": [\\"Q1\\", \\"Q2\\", \\"Q3\\", \\"Q4\\"], \\"Sales\\": [10000, 15000, pd.NA, 20000] }) df_south = pd.DataFrame({ \\"Region\\": [\\"South\\", \\"South\\", \\"South\\", \\"South\\"], \\"Quarter\\": [\\"Q1\\", \\"Q2\\", \\"Q3\\", \\"Q4\\"], \\"Sales\\": [20000, pd.NA, 25000, 30000] }) df_east = pd.DataFrame({ \\"Region\\": [\\"East\\", \\"East\\", \\"East\\", \\"East\\"], \\"Quarter\\": [\\"Q1\\", \\"Q2\\", \\"Q3\\", \\"Q4\\"], \\"Sales\\": [12000, 18000, 22000, pd.NA] }) df_west = pd.DataFrame({ \\"Region\\": [\\"West\\", \\"West\\", \\"West\\", \\"West\\"], \\"Quarter\\": [\\"Q1\\", \\"Q2\\", \\"Q3\\", \\"Q4\\"], \\"Sales\\": [17000, 19000, pd.NA, 21000] }) df_forecast = pd.DataFrame({ \\"Region\\": [\\"North\\", \\"North\\", \\"North\\", \\"North\\", \\"South\\", \\"South\\", \\"South\\", \\"South\\", \\"East\\", \\"East\\", \\"East\\", \\"East\\", \\"West\\", \\"West\\", \\"West\\", \\"West\\"], \\"Quarter\\": [\\"Q1\\", \\"Q2\\", \\"Q3\\", \\"Q4\\", \\"Q1\\", \\"Q2\\", \\"Q3\\", \\"Q4\\", \\"Q1\\", \\"Q2\\", \\"Q3\\", \\"Q4\\", \\"Q1\\", \\"Q2\\", \\"Q3\\", \\"Q4\\"], \\"Forecasted_Sales\\": [11000, 16000, 17000, 21000, 21000, 23000, 26000, 31000, 13000, 19000, 23000, 25000, 18000, 20000, 22000, 24000] }) df_report_2 = pd.DataFrame({ \\"Region\\": [\\"North\\", \\"North\\", \\"North\\", \\"North\\", \\"South\\", \\"South\\", \\"South\\", \\"South\\", \\"East\\", \\"East\\", \\"East\\", \\"East\\", \\"West\\", \\"West\\", \\"West\\", \\"West\\"], \\"Quarter\\": [\\"Q1\\", \\"Q2\\", \\"Q3\\", \\"Q4\\", \\"Q1\\", \\"Q2\\", \\"Q3\\", \\"Q4\\", \\"Q1\\", \\"Q2\\", \\"Q3\\", \\"Q4\\", \\"Q1\\", \\"Q2\\", \\"Q3\\", \\"Q4\\"], \\"Sales\\": [10000, 15000, 17000, 20000, 20000, 23000, 25000, 30000, 12000, 18000, 22000, 24000, 17000, 19000, 22000, 21000], \\"Forecasted_Sales\\": [11000, 16000, 17000, 21000, 21000, 23000, 26000, 31000, 13000, 19000, 23000, 25000, 18000, 20000, 22000, 24000] }) ``` In your implementation, make sure to properly handle edge cases such as completely missing data for a region or discrepancies only existing in one dataset. **Notes**: - Use the `pd.concat`, `pd.merge`, and `DataFrame.compare` functions accordingly. - Make sure your solution is efficient in handling large datasets.","solution":"import pandas as pd def prepare_sales_report(df_north, df_south, df_east, df_west, df_forecast, df_report_2): # Step 1: Concatenate all regional DataFrames into a single DataFrame df_concatenated = pd.concat([df_north, df_south, df_east, df_west], ignore_index=True) # Step 2: Merge the concatenated DataFrame with the forecast data df_merged = pd.merge(df_concatenated, df_forecast, on=[\\"Region\\", \\"Quarter\\"], how=\\"outer\\") # Step 3: Compare the consolidated sales report with df_report_2 to find discrepancies discrepancies = df_merged.compare(df_report_2, keep_shape=True, keep_equal=False) # Step 4: Identify and fill missing Sales values with corresponding Forecasted_Sales values df_merged[\'Sales\'].fillna(df_merged[\'Forecasted_Sales\'], inplace=True) return (df_merged, discrepancies)"},{"question":"Background The \\"urllib\\" module in Python provides a robust and flexible way to handle URLs and HTTP requests. In this assessment, you will write a Python function that downloads content from a list of given URLs and checks the content for a specific keyword. Task Implement a function `find_keyword_in_urls(urls: List[str], keyword: str) -> List[str]` that: 1. Takes a list of URLs (`urls`) and a `keyword` as inputs. 2. Fetches the content of each URL using the `urllib.request` module. 3. Searches for the given `keyword` in the text content of each URL. 4. Returns a list of URLs where the `keyword` was found. Input - `urls`: A list of strings where each string is a URL. - `keyword`: A string representing the keyword to search for in the content of the URLs. Output - A list of strings where each string is a URL that contains the given `keyword` in its content. Constraints - Do not use external libraries other than the standard library. - Handle exceptions that may occur during HTTP requests. - Attempt to process the content as text; if it cannot be decoded, skip the URL. - Only consider URLs with HTTP/HTTPS schemes. Example ```python urls = [ \\"https://www.example.com\\", \\"https://www.python.org\\", \\"https://www.nonexistentwebsite.org\\" ] keyword = \\"Python\\" output = find_keyword_in_urls(urls, keyword) # possible output: [\\"https://www.python.org\\"] ``` Notes - The function should handle various exceptions, such as HTTP errors or connection issues, and continue processing the next URL. - Performance considerations should be taken into account; fetching multiple URLs may be time-consuming. ```python from typing import List import urllib.request import urllib.error def find_keyword_in_urls(urls: List[str], keyword: str) -> List[str]: result = [] for url in urls: try: with urllib.request.urlopen(url) as response: if response.status == 200: content = response.read().decode(\'utf-8\') # Try to decode as utf-8 if keyword in content: result.append(url) except (urllib.error.URLError, ValueError, UnicodeDecodeError): continue return result # Testing the function if __name__ == \\"__main__\\": urls_to_test = [ \\"https://www.example.com\\", \\"https://www.python.org\\", \\"https://www.nonexistentwebsite.org\\" ] keyword_to_search = \\"Python\\" print(find_keyword_in_urls(urls_to_test, keyword_to_search)) ```","solution":"from typing import List import urllib.request import urllib.error def find_keyword_in_urls(urls: List[str], keyword: str) -> List[str]: result = [] for url in urls: try: with urllib.request.urlopen(url) as response: if response.status == 200: content = response.read().decode(\'utf-8\') # Try to decode as utf-8 if keyword in content: result.append(url) except (urllib.error.URLError, ValueError, UnicodeDecodeError): continue return result"},{"question":"<|Analysis Begin|> The provided documentation details PyTorch\'s symbolic shapes, specifically focusing on the dynamic shapes feature. To summarize: - PyTorch traditionally operates with static shapes but supports dynamic shapes where dimensions can vary. - The motivation includes use cases with varying batch sizes, sequence lengths, and data-dependent output shapes. - The new features simplify the use of dynamic shapes by providing APIs to mark dimensions as dynamic and to control recompilation behavior. - Symbolic shapes are integrated deeply into the PyTorch compilation process, enabling reasoning about dynamic shapes without performing real computations on them. - The concept of guards is used to manage and enforce conditions on dynamic shapes during compilation and execution. The details from the provided documentation delve into deeper internals, focusing on the symbolic shape tracking mechanism, its integration into the PyTorch architecture, the meta functions that propagate symbolic shapes, and the various optimizations and constraints involved. The focus of a coding assessment question might be to understand how to implement and use dynamic shapes in a PyTorch model, and the question should involve operations on tensors with variable dimensions, potentially including marking dimensions as dynamic and using them in tensor operations. <|Analysis End|> <|Question Begin|> Coding Assessment Question # Objective Write a PyTorch function that demonstrates your understanding of dynamic shapes and operations on tensors with variable dimensions. # Problem Statement You are tasked with creating a PyTorch function that takes as input a batch of variable-length sequences and performs a dynamic padding operation followed by a dynamic embedding lookup. 1. **Function Signature**: ```python def dynamic_embedding_lookup(sequences: List[torch.Tensor], embedding_dim: int) -> torch.Tensor: ``` 2. **Input**: - `sequences`: A list of PyTorch tensors, where each tensor represents a sequence of integer ids. Each sequence can have a different length. - `embedding_dim`: An integer representing the dimensionality of the embedding space. 3. **Output**: - A single tensor of shape (batch_size, max_sequence_length, embedding_dim) where: - `batch_size` is the number of sequences. - `max_sequence_length` is the length of the longest sequence in the batch. - Each sequence is padded to the length of the longest sequence, and the embeddings for each id in the sequences are looked up in an embedding matrix. 4. **Requirements**: - Ensure dynamic padding of sequences to match the length of the longest sequence. - Use an embedding matrix whose dimensions are (`vocab_size`, `embedding_dim`), where `vocab_size` is assumed to be 1000. - Handle dimension adjustments dynamically using PyTorch\'s dynamic shape capabilities. 5. **Constraints**: - You are encouraged to use dynamic behaviors in PyTorch where applicable. - Assume a fixed `vocab_size` of 1000 for the embedding matrix. # Example ```python # Example input sequences = [ torch.tensor([1, 2, 3]), torch.tensor([4, 5]), torch.tensor([6]) ] embedding_dim = 4 # Assume we have a function `dynamic_embedding_lookup` implemented result = dynamic_embedding_lookup(sequences, embedding_dim) print(result.shape) # Expected output: (3, 3, 4) ``` # Notes - You may use `torch.nn.functional.pad` for padding sequences. - Use `torch.nn.Embedding` for embedding lookups. - Pad the sequences dynamically to ensure uniform length across the batch. - The function should handle any list of sequences correctly, regardless of their lengths. # Solution Starter Here is a starter code to help you get started: ```python import torch import torch.nn.functional as F def dynamic_embedding_lookup(sequences: List[torch.Tensor], embedding_dim: int) -> torch.Tensor: # Your implementation here pass ``` # Evaluation Criteria - Correct usage of dynamic shapes features in PyTorch. - Correct padding of sequences to a common length. - Proper implementation of embedding lookups. - Code readability and adherence to Python coding standards. Good luck!","solution":"import torch import torch.nn.functional as F from typing import List def dynamic_embedding_lookup(sequences: List[torch.Tensor], embedding_dim: int) -> torch.Tensor: Pads sequences to the length of the longest sequence and performs an embedding lookup. Args: sequences (List[torch.Tensor]): A list of tensors representing sequences of integer ids. embedding_dim (int): The dimensionality of the embedding space. Returns: torch.Tensor: A tensor of shape (batch_size, max_sequence_length, embedding_dim) # Determine the maximum sequence length in the batch max_sequence_length = max(seq.size(0) for seq in sequences) # Create an embedding layer with `vocab_size` of 1000 and specified `embedding_dim` embedding = torch.nn.Embedding(1000, embedding_dim) # Pad sequences to the length of the longest sequence padded_sequences = [F.pad(seq, (0, max_sequence_length - seq.size(0)), \'constant\', 0) for seq in sequences] # Stack the padded sequences into a single tensor padded_sequences_tensor = torch.stack(padded_sequences) # Perform embedding lookup embedded_sequences = embedding(padded_sequences_tensor) return embedded_sequences"},{"question":"# Objective Write a Python function that utilizes the `marshal` module functions similar to the provided C interface to serialize and deserialize Python objects to and from byte strings. # Task 1. Implement the `marshal_object_to_string` function to serialize a given Python object to a byte string using the marshal version 2 format. 2. Implement the `unmarshal_object_from_string` function to deserialize a given byte string back to a Python object using the same version. # Input and Output - `marshal_object_to_string(obj: object) -> bytes`: - Input: A Python object `obj`. - Output: A byte string representing the serialized object. - `unmarshal_object_from_string(data: bytes) -> object`: - Input: A byte string `data` that represents the serialized object. - Output: The deserialized Python object. # Constraints - The `obj` parameter can be any serializable Python object. - You should ensure the functions handle exceptions appropriately and provide meaningful error messages for invalid inputs. - You may not use the `marshal` package directly in your code but rather simulate similar functionalities. # Example ```python def marshal_object_to_string(obj: object) -> bytes: # Your implementation here pass def unmarshal_object_from_string(data: bytes) -> object: # Your implementation here pass # Example usage example_obj = [1, 2, 3, {\\"key\\": \\"value\\"}] serialized = marshal_object_to_string(example_obj) print(serialized) # Should output the byte string representation of example_obj deserialized = unmarshal_object_from_string(serialized) print(deserialized) # Should output [1, 2, 3, {\\"key\\": \\"value\\"}] ``` # Additional Notes - Make sure to handle any possible exceptions during the marshaling and unmarshaling process. - Do not use the `marshal` module directly, but simulate the behavior using equivalent functionality provided by Python.","solution":"import json def marshal_object_to_string(obj: object) -> bytes: Serializes a given Python object to a byte string. try: json_str = json.dumps(obj) return json_str.encode(\'utf-8\') except TypeError as e: raise ValueError(f\\"Object of type {type(obj)} is not serializable\\") from e def unmarshal_object_from_string(data: bytes) -> object: Deserializes a given byte string back to a Python object. try: json_str = data.decode(\'utf-8\') return json.loads(json_str) except (json.JSONDecodeError, UnicodeDecodeError) as e: raise ValueError(f\\"Byte string could not be deserialized\\") from e"},{"question":"**Objective:** Demonstrate your understanding and ability to work with the `_thread` module in Python to create and manage multiple threads and synchronization. Problem Statement You are required to create a multithreaded application that performs a simple computation in parallel. The task is to simulate the rolling of a die (`1-6` range) multiple times, using multiple threads to achieve this concurrently while ensuring thread-safe operations. # Task Requirements 1. **Function Definition**: Implement the function `roll_dice_concurrently(num_threads: int, rolls_per_thread: int) -> List[int]`. 2. **Parameters**: - `num_threads` (int): The number of threads to spawn. - `rolls_per_thread` (int): The number of die rolls each thread should perform. 3. **Output**: - List of integers containing the results of all die rolls from all threads. 4. **Constraints**: - `1 <= num_threads <= 100` - `1 <= rolls_per_thread <= 1000` 5. **Details**: - Each thread should generate random numbers between `1` and `6` inclusive. - Use a shared list to store the results of the rolls from all threads. - Ensure thread-safe access to the shared list using locks from the `_thread` module. - Handle exceptions and ensure that all threads complete their tasks before the function returns. # Performance Requirements - The solution should efficiently manage multiple threads and ensure minimal contention on shared resources. # Example ```python import random import _thread import time def roll_dice_concurrently(num_threads: int, rolls_per_thread: int) -> List[int]: results = [] lock = _thread.allocate_lock() def roll_dice(): nonlocal results thread_result = [random.randint(1, 6) for _ in range(rolls_per_thread)] with lock: results.extend(thread_result) threads = [] for _ in range(num_threads): thread_id = _thread.start_new_thread(roll_dice, ()) threads.append(thread_id) # Wait for all threads to complete main_thread = _thread.get_ident() while True: with lock: if len(results) == num_threads * rolls_per_thread: break time.sleep(0.01) # Sleep briefly to avoid busy waiting return results # Example Usage print(roll_dice_concurrently(3, 5)) ``` This code snippet is a partial implementation with some placeholders. You need to complete the implementation by ensuring the correct management and behavior of threads. # Notes - Thoroughly test your function to ensure the results are accurate and the implementation is thread-safe. - Avoid using high-level threading constructs from the `threading` module; rely on the `_thread` module as specified.","solution":"import random import _thread import time from typing import List def roll_dice_concurrently(num_threads: int, rolls_per_thread: int) -> List[int]: results = [] lock = _thread.allocate_lock() def roll_dice(): nonlocal results thread_result = [random.randint(1, 6) for _ in range(rolls_per_thread)] with lock: results.extend(thread_result) threads = [] for _ in range(num_threads): _thread.start_new_thread(roll_dice, ()) # main_thread waiting for all threads to complete while True: with lock: if len(results) == num_threads * rolls_per_thread: break time.sleep(0.01) # Sleep briefly to avoid busy waiting return results"},{"question":"**Question: Custom HTML Entity Encoder/Decoder** # Objective: Design a function that provides both encoding and decoding capabilities similar to the `html.escape` and `html.unescape` functions for a specific subset of HTML entities. Your function should be able to perform both operations based on an input parameter. # Function Signature: ```python def custom_html_transform(s: str, mode: str) -> str: pass ``` # Parameters: - `s` (str): The input string that needs to be transformed. - `mode` (str): The mode of transformation. It can be either `\\"encode\\"` for converting special characters to HTML entities or `\\"decode\\"` for converting HTML entities back to their corresponding characters. # Details: 1. **Encoding Mode (\\"encode\\")**: - Convert the characters `\\"&\\"`, `\\"<\\"`, `\\">\\"`, `\'\\"\'`, `\'\'\'` in the input string `s` to their corresponding HTML entities: - `&` -> `&amp;` - `<` -> `&lt;` - `>` -> `&gt;` - `\\"` -> `&quot;` - `\'` -> `&apos;` 2. **Decoding Mode (\\"decode\\")**: - Convert the HTML entities `&amp;`, `&lt;`, `&gt;`, `&quot;`, and `&apos;` in the input string `s` to their corresponding characters: - `&amp;` -> `&` - `&lt;` -> `<` - `&gt;` -> `>` - `&quot;` -> `\\"` - `&apos;` -> `\'` # Constraints: - You must not use the built-in `html.escape` or `html.unescape` functions directly. - Assume that the input string will only contain the characters and HTML entities specified above. - The length of the input string `s` will not exceed `10^6` characters. # Examples: Example 1: ```python s = \\"4 < 5 & 6 > 3\\" mode = \\"encode\\" print(custom_html_transform(s, mode)) ``` **Output:** ``` \'4 &lt; 5 &amp; 6 &gt; 3\' ``` Example 2: ```python s = \\"He said, &quot;Hello World!&quot;\\" mode = \\"decode\\" print(custom_html_transform(s, mode)) ``` **Output:** ``` \'He said, \\"Hello World!\\"\' ``` # Performance Requirements: The function should handle up to 10^6 characters efficiently within reasonable time constraints.","solution":"def custom_html_transform(s: str, mode: str) -> str: if mode == \\"encode\\": return s.replace(\'&\', \'&amp;\').replace(\'<\', \'&lt;\').replace(\'>\', \'&gt;\').replace(\'\\"\', \'&quot;\').replace(\\"\'\\", \'&apos;\') elif mode == \\"decode\\": return s.replace(\'&lt;\', \'<\').replace(\'&gt;\', \'>\').replace(\'&quot;\', \'\\"\').replace(\'&apos;\', \\"\'\\").replace(\'&amp;\', \'&\') else: raise ValueError(f\\"Invalid mode: {mode}. Use \'encode\' or \'decode\'.\\")"},{"question":"Implement and Export a PyTorch Model with Dynamic Control Flow **Objective**: Demonstrate your understanding of the PyTorch `torch.export` function by implementing a PyTorch model that involves dynamic control flow and exporting it. Problem Statement You are tasked with creating a PyTorch model that performs different operations based on the sum of its input tensor. You need to ensure that the model can handle dynamic control flow and variable input shapes while exporting it using `torch.export`. Instructions 1. **Implement a PyTorch Module**: - Create a PyTorch `nn.Module` named `DynamicControlFlowModel`. - The module should have a `forward` method that takes an input tensor `x`. - The method should perform different operations based on whether the sum of elements in `x` is greater than a threshold (e.g., 0). Use `torch.cond` to manage this dynamic control flow. - If `x.sum() > 0`, the model should return `x.sin()`. - Otherwise, the model should return `x.cos()`. 2. **Export the Model**: - Use the `torch.export.export` function to export the traced model. - Use an example input tensor of size (3, 3) with random values for tracing. - Ensure that the model handles dynamic input shapes by specifying appropriate dynamic shapes. 3. **Verification**: - Print the code of the traced graph using `print(ep.graph_module.code)` to verify the structure of the exported model. Example The following code snippet provides a starting point: ```python import torch from torch import nn class DynamicControlFlowModel(nn.Module): def forward(self, x): return torch.cond( pred=x.sum() > 0, true_fn=lambda x: x.sin(), false_fn=lambda x: x.cos(), operands=(x,) ) # Create an instance of the model model = DynamicControlFlowModel() # Example input tensor example_input = torch.randn(3, 3) # Export the model using torch.export ep = torch.export.export(model, (example_input,)) # Print the code of the traced graph print(ep.graph_module.code) ``` Constraints - You must use PyTorch\'s `torch.cond` for handling dynamic control flow. - The input tensor shape should be dynamic and not hardcoded, allowing the model to handle different input sizes during future executions. **Output**: - Submit the implemented `DynamicControlFlowModel` class. - Provide the code used to export and print the traced model.","solution":"import torch from torch import nn class DynamicControlFlowModel(nn.Module): def forward(self, x): pred = x.sum() > 0 return torch.where(pred, x.sin(), x.cos()) # Create an instance of the model model = DynamicControlFlowModel() # Example input tensor example_input = torch.randn(3, 3) # Export the model using torch.jit.trace (torch.export is deprecated, use torch.jit.script instead) traced_model = torch.jit.script(model) # Print the code of the traced graph print(traced_model.code)"},{"question":"**Objective:** Implement a custom neural network layer in PyTorch and initialize its parameters using specific functions from `torch.nn.init`. # Description You are required to implement a custom linear layer in PyTorch with the following characteristics: 1. Accept input features `in_features` and output features `out_features` as initialization parameters. 2. Implement the forward pass using matrix multiplication. 3. Initialize the weights and biases of the layer using custom initialization functions. Specifically: - Weights should be initialized using the `kaiming_uniform_` function. - Biases should be initialized to zeros using the `zeros_` function. # Requirements 1. Define a class `CustomLinear` that inherits from `torch.nn.Module`. 2. Implement the `__init__` method to initialize weights and biases. 3. Implement the `forward` method for the forward pass computation. # Input and Output Formats - **Input:** - `in_features` (int): Number of input features. - `out_features` (int): Number of output features. - `input_tensor` (torch.Tensor): A tensor of shape `(batch_size, in_features)` representing the input data. - **Output:** - The output tensor of shape `(batch_size, out_features)` obtained by applying the linear transformation to the input tensor. # Constraints - You must use `torch.nn.init.kaiming_uniform_` to initialize the weights. - You must use `torch.nn.init.zeros_` to initialize the biases. - Ensure your implementation does not use any high-level PyTorch linear layers like `torch.nn.Linear`. # Performance Requirements - Your implementation must be efficient and should compute the output for a batch of inputs without using loops. # Implementing the Class ```python import torch import torch.nn as nn import torch.nn.init as init class CustomLinear(nn.Module): def __init__(self, in_features, out_features): super(CustomLinear, self).__init__() self.in_features = in_features self.out_features = out_features # Initialize weights and biases self.weights = nn.Parameter(torch.Tensor(out_features, in_features)) self.biases = nn.Parameter(torch.Tensor(out_features)) # Apply the specified initializations init.kaiming_uniform_(self.weights, a=math.sqrt(5)) init.zeros_(self.biases) def forward(self, input_tensor): return torch.matmul(input_tensor, self.weights.t()) + self.biases # Example usage: model = CustomLinear(10, 5) input_tensor = torch.randn(16, 10) # batch_size of 16, in_features of 10 output_tensor = model(input_tensor) print(output_tensor.shape) # Should print torch.Size([16, 5]) ``` # Explanation - The `CustomLinear` class initializes the weights and biases based on the given dimensions. - The `forward` method executes the linear transformation by multiplying the input tensor with the transposed weight tensor and adding the bias. - Example usage demonstrates how to create an instance of the custom layer and run a forward pass with a random input tensor.","solution":"import math import torch import torch.nn as nn import torch.nn.init as init class CustomLinear(nn.Module): def __init__(self, in_features, out_features): super(CustomLinear, self).__init__() self.in_features = in_features self.out_features = out_features # Initialize weights and biases self.weights = nn.Parameter(torch.Tensor(out_features, in_features)) self.biases = nn.Parameter(torch.Tensor(out_features)) # Apply the specified initializations init.kaiming_uniform_(self.weights, a=math.sqrt(5)) init.zeros_(self.biases) def forward(self, input_tensor): return torch.matmul(input_tensor, self.weights.t()) + self.biases"},{"question":"**Question: Real-time Command-Line Interface Utility** You are tasked with creating a small command-line interface utility that demonstrates the use of the Python `tty` module to switch between raw and cbreak terminal input modes. Specifically, you will implement a function `configure_terminal_mode` and a main program that uses this function to handle terminal input in real-time. **Function: configure_terminal_mode** - **Input**: - `mode` (string): Can be either `\\"raw\\"` or `\\"cbreak\\"` to specify the desired terminal input mode. - `fd` (integer): The file descriptor of the terminal, typically obtained using `sys.stdin.fileno()`. - **Output**: - None. The function should configure the terminal mode as specified. - **Constraints**: - Ensure that invalid modes raise an appropriate exception. - Only Unix-like operating systems can be assumed. **Main Program Requirements**: 1. The program starts and immediately sets the terminal to cbreak mode. 2. It then reads characters one at a time from the input and outputs them to the screen until the \\"Enter\\" key is pressed, at which point it should switch to raw mode. 3. In raw mode, it should continue reading characters and printing them, but it should also output a message \\"Raw mode active\\" immediately after switching. 4. If the character \\"q\\" is read during any mode, the program should exit after switching the terminal back to its original state. **Example Usage**: ```bash python3 terminal_utility.py ``` **Notes**: - Make sure to handle terminal cleanup properly to ensure the terminal is in a usable state after the program exits, even if it crashes. - Use the `tty` module functions (`setraw` and `setcbreak`) to manage the terminal modes. - You may need to use the `termios` module for handling terminal attributes. ```python import tty import termios import sys def configure_terminal_mode(mode, fd): if mode not in [\'raw\', \'cbreak\']: raise ValueError(\\"Invalid mode specified. Must be \'raw\' or \'cbreak\'.\\") if mode == \'raw\': tty.setraw(fd) else: tty.setcbreak(fd) def main(): fd = sys.stdin.fileno() old_settings = termios.tcgetattr(fd) try: configure_terminal_mode(\'cbreak\', fd) while True: ch = sys.stdin.read(1) if ch == \'n\': configure_terminal_mode(\'raw\', fd) print(\\"Raw mode active\\") elif ch == \'q\': break sys.stdout.write(ch) sys.stdout.flush() finally: termios.tcsetattr(fd, termios.TCSADRAIN, old_settings) if __name__ == \\"__main__\\": main() ```","solution":"import tty import termios import sys def configure_terminal_mode(mode, fd): if mode not in [\'raw\', \'cbreak\']: raise ValueError(\\"Invalid mode specified. Must be \'raw\' or \'cbreak\'.\\") if mode == \'raw\': tty.setraw(fd) else: tty.setcbreak(fd) def main(): fd = sys.stdin.fileno() old_settings = termios.tcgetattr(fd) try: configure_terminal_mode(\'cbreak\', fd) while True: ch = sys.stdin.read(1) if ch == \'n\': configure_terminal_mode(\'raw\', fd) print(\\"Raw mode active\\") elif ch == \'q\': break sys.stdout.write(ch) sys.stdout.flush() finally: termios.tcsetattr(fd, termios.TCSADRAIN, old_settings) if __name__ == \\"__main__\\": main()"},{"question":"**Email Message Manipulation** Given the `email.message.Message` class from Python\'s email package, you are tasked with writing a function to construct an email message with specific headers and a multipart payload. # Function Signature ```python def construct_email(headers: dict, text_part: str, attachments: list[tuple[str, bytes]]) -> email.message.Message: Create an email message with specified headers and content. Parameters: headers (dict): A dictionary where keys are header names and values are header values. text_part (str): The plain text part of the email body. attachments (list[tuple[str, bytes]]): A list of tuples where each tuple represents a file attachment. The first element is the filename (str) and the second element is the file content (bytes). Returns: email.message.Message: The constructed email message object with headers and multipart content. ``` # Requirements 1. **Headers**: Set the headers for the email message as specified in the `headers` dictionary. 2. **Text Part**: Add a plain text part to the email body using the `text_part` parameter. 3. **Attachments**: Add each attachment to the email as a separate part using the provided filename and content. 4. **MIME Types and Encodings**: Ensure the appropriate MIME types and encoding are set for both the text part and attachments. 5. **Output Format**: The function should return the constructed `email.message.Message` object. # Example ```python from email.message import Message headers = { \\"From\\": \\"sender@example.com\\", \\"To\\": \\"recipient@example.com\\", \\"Subject\\": \\"Test Email\\" } text_part = \\"This is the body of the email.\\" attachments = [ (\\"document1.pdf\\", b\\"%PDF-1.4...\\"), (\\"image1.jpg\\", b\\"xffxd8xffxe0...\\") ] email_message = construct_email(headers, text_part, attachments) print(email_message.as_string()) ``` # Constraints - Ensure that the headers have valid RFC 5322 field names. - Attachments should be added using appropriate MIME types based on their file extensions. # Performance Requirements - Ensure that the function handles typical email sizes efficiently, considering both text and binary payloads. **Note**: Use the `email.message.Message` class methods and attributes for constructing and manipulating the email message as described in the documentation provided.","solution":"from email.message import EmailMessage import mimetypes def construct_email(headers: dict, text_part: str, attachments: list[tuple[str, bytes]]) -> EmailMessage: Create an email message with specified headers and content. Parameters: headers (dict): A dictionary where keys are header names and values are header values. text_part (str): The plain text part of the email body. attachments (list[tuple[str, bytes]]): A list of tuples where each tuple represents a file attachment. The first element is the filename (str) and the second element is the file content (bytes). Returns: email.message.EmailMessage: The constructed email message object with headers and multipart content. msg = EmailMessage() # Set the headers for key, value in headers.items(): msg[key] = value # Set the plain text part msg.set_content(text_part) # Add each attachment for attachment in attachments: filename, content = attachment maintype, subtype = (mimetypes.guess_type(filename)[0] or \'application/octet-stream\').split(\'/\') msg.add_attachment(content, maintype=maintype, subtype=subtype, filename=filename) return msg"},{"question":"Objective: Implement a function that processes a list of date-time strings, converting them to a specified IANA time zone, and returns the processed date-times along with their time zone names. Problem Statement: You are given: 1. A list of date-time strings in ISO 8601 format with UTC time zone. 2. An IANA time zone name to which these date-times need to be converted. You need to implement a function `convert_timezones(date_strings: list, target_timezone: str) -> list` that performs the following tasks: - Converts each date-time string from UTC to the specified target time zone. - Returns a list of formatted strings where each string contains the converted date-time and the time zone abbreviation. Input: - `date_strings` (list): A list of date-time strings in the format `\\"YYYY-MM-DDTHH:MM:SSZ\\"`. Example: `[\\"2023-10-12T14:30:00Z\\", \\"2023-10-12T16:45:00Z\\"]`. - `target_timezone` (str): A string representing the IANA time zone. Example: `\\"America/Los_Angeles\\"`. Output: - (list): A list of strings in the format `\\"YYYY-MM-DDTHH:MM:SS [Time Zone Abbreviation]\\"`. Example: `[\\"2023-10-12T07:30:00 [PDT]\\", \\"2023-10-12T09:45:00 [PDT]\\"]`. Constraints: - Assume that the provided date-times are always in valid ISO 8601 format and in UTC. - The target time zone is always valid and available in the system or via `tzdata`. Example: ```python # Example input: date_strings = [\\"2023-10-12T14:30:00Z\\", \\"2023-10-12T16:45:00Z\\"] target_timezone = \\"America/Los_Angeles\\" # Example output: # [\\"2023-10-12T07:30:00 [PDT]\\", \\"2023-10-12T09:45:00 [PDT]\\"] def convert_timezones(date_strings: list, target_timezone: str) -> list: # Your implementation here # To test the function (example): print(convert_timezones(date_strings, target_timezone)) ``` Notes: - Your function should handle daylight saving time transitions correctly. - Make sure to make use of the \\"zoneinfo\\" module and its functionalities while implementing your solution. Good luck!","solution":"from datetime import datetime from zoneinfo import ZoneInfo def convert_timezones(date_strings, target_timezone): Converts a list of UTC date-time strings to the specified IANA time zone. Args: date_strings (list): A list of date-time strings in \\"YYYY-MM-DDTHH:MM:SSZ\\" format. target_timezone (str): The target IANA time zone. Returns: list: A list of strings formatted as \\"YYYY-MM-DDTHH:MM:SS [Time Zone Abbreviation]\\". converted_dates = [] target_zone = ZoneInfo(target_timezone) for date_string in date_strings: # Parse the UTC datetime string utc_time = datetime.fromisoformat(date_string.replace(\'Z\', \'+00:00\')) # Convert to the target timezone local_time = utc_time.astimezone(target_zone) # Format string with timezone abbreviation formatted_time = local_time.strftime(f\\"%Y-%m-%dT%H:%M:%S [{local_time.tzname()}]\\") converted_dates.append(formatted_time) return converted_dates"},{"question":"# Command Line Interaction Enhancer Your task is to implement a basic command line interaction enhancer similar to some features provided by enhanced Python interpreters like IPython or the Readline features mentioned. Specifically, you need to implement a history manager for interactive commands. Requirements: 1. Implement a class `CommandLineHistoryManager` that manages command history. 2. The class should have the following methods: - `add_command(command: str)`: Adds a command to the history. - `get_history() -> list`: Returns the list of all previously entered commands. - `search_history(prefix: str) -> list`: Returns a list of commands that start with the given prefix. - `save_history(filepath: str)`: Saves the command history to a file. - `load_history(filepath: str)`: Loads the command history from a file. - `clear_history()`: Clears the command history. # Constraints: - You can assume that commands are simple strings. - The history should be stored in the order commands were entered. - The file operations should handle errors appropriately (e.g., file not found or permission issues). # Performance Requirements: - The `search_history` method should be efficient to handle a large number of commands. # Example Usage: ```python history_manager = CommandLineHistoryManager() history_manager.add_command(\\"print(\'Hello, World!\')\\") history_manager.add_command(\\"import math\\") history_manager.add_command(\\"print(math.pi)\\") print(history_manager.get_history()) # Output: [\\"print(\'Hello, World!\')\\", \\"import math\\", \\"print(math.pi)\\"] print(history_manager.search_history(\\"print\\")) # Output: [\\"print(\'Hello, World!\')\\", \\"print(math.pi)\\"] history_manager.save_history(\\"commands.txt\\") history_manager.clear_history() print(history_manager.get_history()) # Output: [] history_manager.load_history(\\"commands.txt\\") print(history_manager.get_history()) # Output: [\\"print(\'Hello, World!\')\\", \\"import math\\", \\"print(math.pi)\\"] ``` Ensure proper handling and testing of edge cases, such as searching with an empty prefix, adding duplicate commands, and loading from/saving to a file.","solution":"import os class CommandLineHistoryManager: def __init__(self): self.history = [] def add_command(self, command: str): self.history.append(command) def get_history(self) -> list: return self.history def search_history(self, prefix: str) -> list: return [command for command in self.history if command.startswith(prefix)] def save_history(self, filepath: str): try: with open(filepath, \'w\') as file: for command in self.history: file.write(command + \\"n\\") except Exception as e: print(f\\"Failed to save history: {e}\\") def load_history(self, filepath: str): if not os.path.exists(filepath): print(f\\"File not found: {filepath}\\") return try: with open(filepath, \'r\') as file: self.history = [line.strip() for line in file.readlines()] except Exception as e: print(f\\"Failed to load history: {e}\\") def clear_history(self): self.history = []"},{"question":"# Python310 Scheduler Challenge Context: You are responsible for creating an alarm system using Python\'s `sched` module that reminds users to take breaks at specific times and intervals throughout the day. Task: Implement the `AlarmScheduler` class that uses Python\'s `sched` module to: 1. **Add Break Events**: - `add_break(time, message)`: Schedule a break event with a specific message at an absolute time. 2. **Add Recurring Break Events**: - `add_recurring_break(start_time, interval, repetitions, message)`: Schedule recurring break events starting from `start_time`, repeating every `interval` seconds for a total of `repetitions`. 3. **Run the Scheduler**: - `start_scheduler()`: Start the scheduler and handle all scheduled events. 4. **Cancel a Break Event**: - `cancel_break(event)`: Cancel a specific break event that was previously scheduled. 5. **Check if the Scheduler is Empty**: - `is_empty()`: Return True if no events are scheduled, otherwise False. Constraints: - Ensure that the scheduled events do not clash. - Make sure the scheduler can handle multiple threads. - Do not allow scheduling events in the past. Example Usage: ```python import time import sched class AlarmScheduler: def __init__(self): self.scheduler = sched.scheduler(time.time, time.sleep) self.events = [] def add_break(self, time, message): if time < self.scheduler.timefunc(): raise ValueError(\\"Cannot schedule breaks in the past.\\") event = self.scheduler.enterabs(time, 1, self.print_message, argument=(message,)) self.events.append(event) return event def add_recurring_break(self, start_time, interval, repetitions, message): for i in range(repetitions): event_time = start_time + i * interval if event_time >= self.scheduler.timefunc(): self.add_break(event_time, message) def start_scheduler(self): self.scheduler.run() def cancel_break(self, event): self.scheduler.cancel(event) self.events.remove(event) def is_empty(self): return self.scheduler.empty() def print_message(self, message): print(f\\"Break Reminder: {message}\\") # Example implementation alarm = AlarmScheduler() # Scheduling a single break in 10 seconds alarm.add_break(time.time() + 10, \\"Time to take a short walk!\\") # Scheduling recurring breaks starting in 20 seconds, every 15 seconds, 3 times alarm.add_recurring_break(time.time() + 20, 15, 3, \\"Stand up and stretch!\\") # Start the scheduler to handle events alarm.start_scheduler() ``` Notes: - The provided example demonstrates how to use the `AlarmScheduler` class. - Pay attention to handling errors such as scheduling events in the past. - You can schedule multiple events, and they should execute based on the time and priority specified. - Please include appropriate exception handling and ensure thread safety.","solution":"import time import sched import threading from typing import Any class AlarmScheduler: def __init__(self): self.scheduler = sched.scheduler(time.time, time.sleep) self.events = [] self.lock = threading.Lock() def add_break(self, event_time: float, message: str): if event_time < self.scheduler.timefunc(): raise ValueError(\\"Cannot schedule breaks in the past.\\") with self.lock: event = self.scheduler.enterabs(event_time, 1, self.print_message, argument=(message,)) self.events.append(event) return event def add_recurring_break(self, start_time: float, interval: float, repetitions: int, message: str): for i in range(repetitions): event_time = start_time + i * interval if event_time >= self.scheduler.timefunc(): self.add_break(event_time, message) def start_scheduler(self): threading.Thread(target=self.scheduler.run).start() def cancel_break(self, event: Any): with self.lock: self.scheduler.cancel(event) self.events.remove(event) def is_empty(self) -> bool: return self.scheduler.empty() def print_message(self, message: str): print(f\\"Break Reminder: {message}\\")"},{"question":"**Objective:** You are provided with a snippet of Python 2 code that deals with text and binary data processing. Your task is to transform this code to be compatible with both Python 2 and Python 3 while adhering to best practices outlined in the porting guide. **Problem Statement:** You are given a Python 2 script that opens a text file, processes some text and binary data, and performs division. Your task is to update this script to be compatible with both Python 2.7 and Python 3.x. Here is the provided code: ```python # Python 2 code def process_data(filename): # Open the file for reading with open(filename, \'r\') as f: lines = f.readlines() # Process text data processed_lines = [line.strip() for line in lines] # Example of division result = 5 / 2 # Handling binary data binary_data = b\'some binary data\' indexed_byte = binary_data[1] return processed_lines, result, indexed_byte # Usage example filename = \\"example.txt\\" output = process_data(filename) print(output) ``` # Requirements: 1. **File Handling:** - Use `io.open()` to ensure compatibility between Python 2 and Python 3. - Read the file as text data. 2. **Division Handling:** - Ensure that division operations produce the same result in both Python 2 and Python 3. Use `from __future__ import division`. 3. **Text and Binary Data:** - Distinguish between text and binary data handling. - Ensure that indexing into binary data correctly returns a byte and not its integer representation in Python 3. 4. **Feature Detection:** - Implement feature detection to handle any other compatibility issues where necessary. # Input Format: - A text file named `example.txt` will be provided in the same directory as the script. This file will contain several lines of text. # Output Format: - Your script should output a tuple with the processed lines (list of strings), result of the division (float), and indexed byte (integer). **Constraints:** - You must not use any external libraries other than those mentioned (e.g., `io`, `from __future__ import division`). - Ensure backward compatibility with Python 2.7. - Do not change the function signature of `process_data`. **Submission:** Update the provided Python 2 code in a way that it works seamlessly in both Python 2.7 and Python 3.x and meets the requirements mentioned above.","solution":"from __future__ import division import io def process_data(filename): # Open the file for reading with io.open(filename, \'r\', encoding=\'utf-8\') as f: lines = f.readlines() # Process text data processed_lines = [line.strip() for line in lines] # Example of division result = 5 / 2 # Handling binary data binary_data = b\'some binary data\' if isinstance(binary_data[1], int): indexed_byte = binary_data[1] else: indexed_byte = ord(binary_data[1]) return processed_lines, result, indexed_byte # Usage example (commented because the test framework does not use this line) # filename = \\"example.txt\\" # output = process_data(filename) # print(output)"},{"question":"**Title:** Working with GenericAlias for Type Hinting **Objective:** Implement a class in Python that mimics the behavior of the `GenericAlias` object as described in the documentation. This will test your understanding of object-oriented programming, type hinting, and how to manage attributes and methods in Python. **Problem Statement:** You are required to implement a class `MyGenericAlias` that acts similarly to the `GenericAlias` object described in the documentation. Your class should support the following functionality: 1. **Initialization**: - The class should be initialized with two parameters: `origin` and `args`. - `origin` should be of any type. - `args` should be a tuple of types. 2. **Attributes**: - `__origin__`: This should store the `origin` parameter. - `__args__`: This should store the `args` parameter. If `args` is not a tuple, it should be converted into a 1-tuple. - `__parameters__`: This should be constructed lazily (when first accessed) from `__args__`. 3. **Methods**: - `__class_getitem__(cls, item)`: This should return an instance of `MyGenericAlias` with: - `origin` set to `cls`. - `args` set to `item` (if `item` is not a tuple, it should be converted into a 1-tuple). 4. **Behavior**: - The `__repr__` method should return a string representation of the object in the form: `MyGenericAlias(origin, args)`. **Constraints:** - The class should handle minimal checking for the arguments as described in the documentation. - If `__parameters__` is accessed for the first time, it should construct its value from `__args__`. **Example:** ```python class MyGenericAlias: def __init__(self, origin, args): # Implement initialization def __class_getitem__(cls, item): # Implement class method to handle generic aliasing @property def __parameters__(self): # Implement lazy construction of __parameters__ def __repr__(self): # Implement string representation # Example usage: ga = MyGenericAlias(int, (str, bool)) print(ga) # Output: MyGenericAlias(<class \'int\'>, (<class \'str\'>, <class \'bool\'>)) ``` **Your task**: Implement the `MyGenericAlias` class with the specified behavior, attributes, and methods. Ensure your implementation follows the guidelines and constraints provided.","solution":"class MyGenericAlias: def __init__(self, origin, args): Initialization of MyGenericAlias takes an origin and args. If args is not a tuple, convert it to a 1-tuple. self.__origin__ = origin self.__args__ = args if isinstance(args, tuple) else (args,) self.__parameters_cache = None def __class_getitem__(cls, item): Class method to handle generic aliasing. Returns a new instance of MyGenericAlias. return MyGenericAlias(cls, item if isinstance(item, tuple) else (item,)) @property def __parameters__(self): Lazy construction of __parameters__. If not already constructed, build it from __args__. if self.__parameters_cache is None: self.__parameters_cache = self.__args__ return self.__parameters_cache def __repr__(self): String representation of the object in the form: MyGenericAlias(origin, args) return f\\"MyGenericAlias({self.__origin__}, {self.__args__})\\""},{"question":"**Objective:** The goal of this assignment is to assess your understanding of distributed training in PyTorch, including the use of `torchrun`, checkpointing, and environment variable management. **Problem Statement:** You are required to write a PyTorch training script that is compatible with `torch.distributed.run` using a `c10d` rendezvous backend. Your script should handle the initialization of the process group, the training loop, and the checkpointing of the model state. Assume that the data loading and model architecture are predefined and provided to your script. **Requirements:** 1. **Initialization:** - Initialize the process group using `torch.distributed.init_process_group`. - Set the backend to `nccl` and use the `c10d` rendezvous backend. - Retrieve the local rank from the environment variable `LOCAL_RANK`. 2. **Checkpointing:** - Implement `load_checkpoint(path)` to load the model and optimizer state. - Implement `save_checkpoint(path)` to save the model and optimizer state. 3. **Training Loop:** - Iterate over the dataset for a specified number of epochs. - Save a checkpoint at the end of each epoch. 4. **Script Arguments:** - Parse the necessary arguments for setting up the training script, including: - `--backend`: The backend to use for `init_process_group` (default to `nccl`). - `--checkpoint-path`: The path where checkpoints should be saved/loaded. - `--total-num-epochs`: The total number of epochs for training. **Input:** - The script will receive the following arguments: - `--checkpoint-path` (string): The file path to load and save checkpoints. - `--total-num-epochs` (int): The number of epochs to train. - `--backend` (string): The backend to use for distributed training (default: `nccl`). **Output:** - The output will be the console logs showing the progress of training and the saving/loading of checkpoints. **Code Skeleton:** ```python import os import sys import argparse import torch import torch.distributed as dist def parse_args(args): parser = argparse.ArgumentParser(description=\'Distributed Training Script\') parser.add_argument(\'--backend\', type=str, default=\'nccl\', help=\'Backend for init_process_group\') parser.add_argument(\'--checkpoint-path\', type=str, required=True, help=\'Path for checkpoint\') parser.add_argument(\'--total-num-epochs\', type=int, required=True, help=\'Total number of epochs\') return parser.parse_args(args) def load_checkpoint(path): # Implement loading of model and optimizer state pass def save_checkpoint(state, path): # Implement saving of model and optimizer state pass def initialize(state): # Placeholder for model and optimizer initialization pass def train(batch, model): # Placeholder for the training logic per batch pass def main(): args = parse_args(sys.argv[1:]) state = load_checkpoint(args.checkpoint_path) initialize(state) dist.init_process_group(backend=args.backend) local_rank = int(os.environ.get(\\"LOCAL_RANK\\", 0)) for epoch in range(state.epoch, args.total_num_epochs): for batch in iter(state.dataset): train(batch, state.model) state.epoch += 1 save_checkpoint(state, args.checkpoint_path) if __name__ == \'__main__\': main() ``` **Note:** You need to fill in the `load_checkpoint`, `save_checkpoint`, `initialize`, and `train` functions with actual implementations appropriate to your training task.","solution":"import os import sys import argparse import torch import torch.distributed as dist def parse_args(args): parser = argparse.ArgumentParser(description=\'Distributed Training Script\') parser.add_argument(\'--backend\', type=str, default=\'nccl\', help=\'Backend for dist.init_process_group\') parser.add_argument(\'--checkpoint-path\', type=str, required=True, help=\'Path for checkpoint\') parser.add_argument(\'--total-num-epochs\', type=int, required=True, help=\'Total number of epochs\') return parser.parse_args(args) def load_checkpoint(path): if os.path.exists(path): checkpoint = torch.load(path) return checkpoint return None def save_checkpoint(state, path): torch.save(state, path) def initialize(state): # Placeholder for model and optimizer initialization model = state[\'model\'] optimizer = state[\'optimizer\'] epoch = state.get(\'epoch\', 0) return model, optimizer, epoch def train(batch, model, optimizer): # Placeholder for the training logic per batch optimizer.zero_grad() outputs = model(batch) loss = outputs.mean() # Assume Mean loss for simplicity loss.backward() optimizer.step() def main(): args = parse_args(sys.argv[1:]) checkpoint_path = args.checkpoint_path total_num_epochs = args.total_num_epochs backend = args.backend checkpoint = load_checkpoint(checkpoint_path) if checkpoint is None: model = torch.nn.Linear(10, 1) # Example model optimizer = torch.optim.SGD(model.parameters(), lr=0.01) epoch = 0 state = {\'model\': model, \'optimizer\': optimizer, \'epoch\': epoch} else: state = checkpoint model, optimizer, epoch = initialize(state) dist.init_process_group(backend=args.backend, init_method=\'env://\') local_rank = int(os.environ.get(\\"LOCAL_RANK\\", 0)) torch.cuda.set_device(local_rank) model.cuda(local_rank) dataset = torch.randn(1000, 10).cuda(local_rank) # Example dataset batch_size = 32 # Example batch size num_batches = len(dataset) // batch_size for current_epoch in range(epoch, total_num_epochs): # Shuffle dataset shuffled_indices = torch.randperm(len(dataset), device=local_rank) dataset = dataset[shuffled_indices] for i in range(num_batches): batch = dataset[i * batch_size: (i + 1) * batch_size] train(batch, model, optimizer) # Save checkpoint state[\'epoch\'] = current_epoch + 1 save_checkpoint(state, checkpoint_path) print(f\\"Epoch {current_epoch + 1} complete, checkpoint saved.\\") if __name__ == \'__main__\': main()"},{"question":"Objective: You are required to demonstrate your understanding of the scikit-learn package by: 1. Fetching a real-world dataset. 2. Preprocessing the dataset. 3. Performing a machine learning task (classification). Dataset: Use the `fetch_20newsgroups` dataset provided by `sklearn.datasets`. Requirements: 1. **Fetching the Dataset:** - Use the `fetch_20newsgroups` function to load the dataset. - Select four categories from the dataset: `comp.graphics`, `sci.med`, `rec.sport.baseball`, and `soc.religion.christian`. 2. **Preprocessing:** - Convert the text data to a numerical format suitable for machine learning. Use TF-IDF Vectorizer (`TfidfVectorizer`). - Split the data into training and test sets (80% training, 20% testing). 3. **Build and Train a Model:** - Implement a Naive Bayes classifier (use `MultinomialNB` from `sklearn.naive_bayes`). - Train the model on the training set. 4. **Evaluation:** - Evaluate the model on the test set. - Report the accuracy of the model. - Create and display a confusion matrix. Input and Output Formats: - **Input:** No specific input is required as the datasets are fetched directly. - **Output:** Print the accuracy and the confusion matrix. Constraints: - Use only the specified categories for classification. - Ensure reproducibility in your code by setting a random seed where applicable. ```python # Your solution starts here from sklearn.datasets import fetch_20newsgroups from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.model_selection import train_test_split from sklearn.naive_bayes import MultinomialNB from sklearn.metrics import accuracy_score, confusion_matrix # Fetch the dataset categories = [\'comp.graphics\', \'sci.med\', \'rec.sport.baseball\', \'soc.religion.christian\'] newsgroups = fetch_20newsgroups(subset=\'all\', categories=categories) # Preprocess the dataset vectorizer = TfidfVectorizer() X = vectorizer.fit_transform(newsgroups.data) y = newsgroups.target # Split into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Build and Train the Model model = MultinomialNB() model.fit(X_train, y_train) # Evaluation y_pred = model.predict(X_test) accuracy = accuracy_score(y_test, y_pred) conf_matrix = confusion_matrix(y_test, y_pred) print(f\\"Accuracy: {accuracy:.2f}\\") print(\\"Confusion Matrix:\\") print(conf_matrix) # Your solution ends here ```","solution":"from sklearn.datasets import fetch_20newsgroups from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.model_selection import train_test_split from sklearn.naive_bayes import MultinomialNB from sklearn.metrics import accuracy_score, confusion_matrix def fetch_and_train_newsgroups(): Fetches the 20 Newsgroups dataset, vectors the text data using TF-IDF, trains a Naive Bayes model, and evaluates the model. Returns: tuple: accuracy (float), confusion matrix (ndarray) # Fetch the dataset categories = [\'comp.graphics\', \'sci.med\', \'rec.sport.baseball\', \'soc.religion.christian\'] newsgroups = fetch_20newsgroups(subset=\'all\', categories=categories) # Preprocess the dataset vectorizer = TfidfVectorizer() X = vectorizer.fit_transform(newsgroups.data) y = newsgroups.target # Split into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Build and Train the Model model = MultinomialNB() model.fit(X_train, y_train) # Evaluation y_pred = model.predict(X_test) accuracy = accuracy_score(y_test, y_pred) conf_matrix = confusion_matrix(y_test, y_pred) return accuracy, conf_matrix"},{"question":"# Python Coding Assessment **Objective:** Assess the student\'s understanding of creating a source distribution using Python\'s `sdist` command and creating a manifest template to specify included files. **Problem Statement:** You are working on an open-source Python project that you wish to distribute. The default behavior of the `sdist` command does not meet your requirements because you need to include some specific additional files in the distribution. You need to do the following: 1. Create a basic Python package named `my_package` with the following structure: ``` my_package/ __init__.py module.py data/ sample_data.txt examples/ example1.txt tests/ test_module.py README.md setup.py ``` 2. Write a `setup.py` script to define the package. 3. Create a manifest template file (`MANIFEST.in`) to include the following files in the source distribution: - All Python files in the package (`my_package`). - All text files in the `examples` directory. - The README file (`README.md`). 4. Write a script to generate the source distribution in `zip` and `gztar` formats. **Requirements:** - The package must be named `my_package` and have an appropriate `__init__.py` and `module.py` files. - The `setup.py` script should specify the necessary metadata for the package. - The `MANIFEST.in` file should correctly specify the additional files to include. - The script should use the `sdist` command to create the source distribution in the required formats. **Input:** No input required. **Output:** The script should output the paths of the generated distribution archives. **Constraints:** - Use only Python standard libraries. - Ensure that the source distribution archives include only the specified files. **Performance Requirements:** - The script should create the source distribution archives efficiently, without unnecessary delays. **Example:** Your directory structure should look like this: ``` my_package/ __init__.py module.py data/ sample_data.txt examples/ example1.txt tests/ test_module.py README.md setup.py MANIFEST.in ``` Example content of `MANIFEST.in`: ``` include README.md recursive-include my_package *.py recursive-include examples *.txt ``` Example content of `setup.py`: ```python from setuptools import setup, find_packages setup( name=\'my_package\', version=\'0.1\', packages=find_packages(), ) ``` Your script to generate the source distribution should be similar to: ```python import os from subprocess import run # Generate the source distribution run([\'python\', \'setup.py\', \'sdist\', \'--formats=zip,gztar\']) # Output the paths of the generated archives dist_dir = \'dist\' for archive in os.listdir(dist_dir): print(os.path.join(dist_dir, archive)) ``` Upon running the script, you should see the list of generated distribution archives: ``` dist/my_package-0.1.tar.gz dist/my_package-0.1.zip ``` --- Make sure your solution meets all specified requirements and constraints.","solution":"# Directory Structure # my_package/ # __init__.py # module.py # data/ # sample_data.txt # examples/ # example1.txt # tests/ # test_module.py # README.md # setup.py # MANIFEST.in # Create directory structure and files import os # Create directories os.makedirs(\'my_package/data\', exist_ok=True) os.makedirs(\'my_package/examples\', exist_ok=True) os.makedirs(\'my_package/tests\', exist_ok=True) # Create necessary files with basic content with open(\'my_package/__init__.py\', \'w\') as f: f.write(\'# __init__.py for my_packagen\') with open(\'my_package/module.py\', \'w\') as f: f.write(\'# module.py for my_packagen\') with open(\'my_package/data/sample_data.txt\', \'w\') as f: f.write(\'Sample data content\') with open(\'my_package/examples/example1.txt\', \'w\') as f: f.write(\'Example 1 content\') with open(\'my_package/tests/test_module.py\', \'w\') as f: f.write(\'# test_module.py for my_package testsn\') with open(\'README.md\', \'w\') as f: f.write(\'# README for my_packagen\') # Create setup.py with necessary metadata with open(\'setup.py\', \'w\') as f: f.write( from setuptools import setup, find_packages setup( name=\'my_package\', version=\'0.1\', packages=find_packages(), include_package_data=True, ) ) # Create MANIFEST.in to include additional files with open(\'MANIFEST.in\', \'w\') as f: f.write( include README.md recursive-include my_package *.py recursive-include my_package/data *.txt recursive-include my_package/examples *.txt ) # Script to generate the source distribution import subprocess def create_sdist(): result = subprocess.run([\'python\', \'setup.py\', \'sdist\', \'--formats=zip,gztar\'], capture_output=True, text=True) if result.returncode != 0: raise Exception(f\\"sdist command failed: {result.stderr}\\") dist_dir = \'dist\' archives = [os.path.join(dist_dir, archive) for archive in os.listdir(dist_dir) if os.path.isfile(os.path.join(dist_dir, archive))] return archives if __name__ == \\"__main__\\": archives = create_sdist() for archive in archives: print(archive)"},{"question":"# Implementing a Descriptor-Based Data Type Validator You have been provided with the basics of descriptors in Python. For this task, you will need to implement a descriptor for validating data types and managing attributes dynamically within a class. Question Design and implement a `LoggedTypeCheck` descriptor class that can keep track of accesses and modifications to attributes of certain types. The descriptor should: 1. Ensure that the assigned value is of the expected type. 2. Log every access to and modification of the attribute. 3. Customize attribute names dynamically, allowing its reuse for multiple attributes in the class. Requirements 1. **Class `LoggedTypeCheck`**: - **Initialization**: - Takes a single argument, `expected_type`, representing the expected type of the value. - **Methods to implement**: - `__set_name__(self, owner, name)`: Store the class name (`owner`) and attribute name (`name`). Customize names to have private attributes prefixed with \'_\'. - `__get__(self, obj, objtype=None)`: Log and return the value of the attribute, ensuring it is an expected type. - `__set__(self, obj, value)`: Log before setting the value ensuring it is the expected type. 2. **Class `TestClass`**: - Has three attributes: `name`, `age`, and `height` using `LoggedTypeCheck`. - `name` should be a string. - `age` should be an integer. - `height` should be a float. Constraints - Implement logging using the `logging` module. - Ensure that type mismatches raise an appropriate `TypeError`. - Provide necessary examples and test cases. Example Usage ```python import logging logging.basicConfig(level=logging.INFO) class LoggedTypeCheck: # Implement this descriptor class here def __init__(self, expected_type): # Your code here def __set_name__(self, owner, name): # Your code here def __get__(self, obj, objtype=None): # Your code here def __set__(self, obj, value): # Your code here class TestClass: name = LoggedTypeCheck(str) age = LoggedTypeCheck(int) height = LoggedTypeCheck(float) def __init__(self, name, age, height): self.name = name self.age = age self.height = height # Example usage person = TestClass(\\"John\\", 30, 5.9) print(person.name) # Logs and outputs: \\"John\\" person.age = 31 # Logs the update person.height = \\"tall\\" # Raises TypeError and logs appropriately ``` **Your task** is to implement the `LoggedTypeCheck` class as described.","solution":"import logging logging.basicConfig(level=logging.INFO) logger = logging.getLogger(__name__) class LoggedTypeCheck: def __init__(self, expected_type): self.expected_type = expected_type self.private_name = None def __set_name__(self, owner, name): self.private_name = f\'_{name}\' def __get__(self, obj, objtype=None): value = getattr(obj, self.private_name) logger.info(f\\"Accessed {self.private_name}: {value}\\") if not isinstance(value, self.expected_type): raise TypeError(f\\"Expected {self.expected_type.__name__}, got {type(value).__name__}\\") return value def __set__(self, obj, value): if not isinstance(value, self.expected_type): raise TypeError(f\\"Expected {self.expected_type.__name__}, got {type(value).__name__}\\") setattr(obj, self.private_name, value) logger.info(f\\"Set {self.private_name} to {value}\\") class TestClass: name = LoggedTypeCheck(str) age = LoggedTypeCheck(int) height = LoggedTypeCheck(float) def __init__(self, name, age, height): self.name = name self.age = age self.height = height"},{"question":"# Custom Copy Operations: Implementing Shallow and Deep Copy for User-Defined Objects **Objective**: Your task is to implement a custom class in Python that supports both shallow and deep copying operations. The class should hold complex nested data structures to clearly demonstrate the differences between shallow and deep copies. **Requirements**: 1. **Define a class `Node`**: - The class should have two attributes: `value` (an integer) and `children` (a list of `Node` objects). - Implement the `__copy__()` method to return a shallow copy of the `Node` instance. - Implement the `__deepcopy__()` method to return a deep copy of the `Node` instance, using the `memo` dictionary to handle recursive references. 2. **Ensure Custom Copy Methods**: - The `__copy__()` method should create a new `Node` instance with the same `value` and a shallow copy of the `children` list. - The `__deepcopy__()` method should create a new `Node` instance with the same `value` and a deep copy of the `children` list, using the `deepcopy` function from the `copy` module. **Constraints**: - You should not use any external libraries other than the `copy` module. - Your implementation should correctly handle cases with recursive references in the `children` attribute. **Example**: ```python from copy import copy, deepcopy class Node: def __init__(self, value): self.value = value self.children = [] def __copy__(self): # Implement shallow copy logic here pass def __deepcopy__(self, memo): # Implement deep copy logic here pass def add_child(self, child_node): self.children.append(child_node) # Example usage: root = Node(1) child1 = Node(2) child2 = Node(3) root.add_child(child1) root.add_child(child2) child1.add_child(root) # Creating a recursive reference # Creating a shallow copy of the root shallow_copy = copy(root) # Creating a deep copy of the root deep_copy = deepcopy(root) # Validate the copies print(shallow_copy.children == root.children) # Should be True print(deep_copy.children == root.children) # Should be False ``` **Your Tasks**: 1. Implement the `Node` class with the specified copy methods. 2. Ensure the shallow and deep copying behaviors are correct as outlined in the example. Submit your implementation in the space provided below: ```python # Your implementation goes here ```","solution":"import copy class Node: def __init__(self, value): self.value = value self.children = [] def __copy__(self): new_node = Node(self.value) new_node.children = self.children[:] # shallow copy of the list return new_node def __deepcopy__(self, memo): if id(self) in memo: return memo[id(self)] new_node = Node(self.value) memo[id(self)] = new_node new_node.children = [copy.deepcopy(child, memo) for child in self.children] return new_node def add_child(self, child_node): self.children.append(child_node) # Example usage # root = Node(1) # child1 = Node(2) # child2 = Node(3) # root.add_child(child1) # root.add_child(child2) # child1.add_child(root) # Creating a recursive reference # Creating a shallow copy of the root # shallow_copy = copy.copy(root) # Creating a deep copy of the root # deep_copy = copy.deepcopy(root) # Validate the copies # print(shallow_copy.children == root.children) # Should be True # print(deep_copy.children == root.children) # Should be False"},{"question":"Enhanced Error Handling with `errno` Background In Python, system errors are often indicated by raising exceptions like `FileNotFoundError`, `PermissionError`, and others. The `errno` module provides a way to get standard error codes associated with these exceptions. The `errno.errorcode` dictionary can be used to translate an error code to its corresponding error name as a string. Additionally, the `os.strerror()` function can translate a numeric error code to a human-readable error message. Task Write a Python function `handle_system_error(error_code)` that takes a single integer `error_code` as input and returns a string describing the error in detail. The function should: 1. Check if the given `error_code` exists in the `errno.errorcode` dictionary. 2. If it exists, return a string formatted as: ``` \\"Error [error_code]: [error_name] - [error_message]\\" ``` Where: - `[error_code]` is the provided error code. - `[error_name]` is the string name of the error from `errno.errorcode`. - `[error_message]` is the human-readable message from `os.strerror(error_code)`. 3. If the `error_code` does not exist in `errno.errorcode`, return a string formatted as: ``` \\"Unknown error code: [error_code]\\" ``` Example ```python import errno import os def handle_system_error(error_code): if error_code in errno.errorcode: error_name = errno.errorcode[error_code] error_message = os.strerror(error_code) return f\\"Error {error_code}: {error_name} - {error_message}\\" else: return f\\"Unknown error code: {error_code}\\" # Example usage: print(handle_system_error(2)) # Output: \\"Error 2: ENOENT - No such file or directory\\" print(handle_system_error(13)) # Output: \\"Error 13: EACCES - Permission denied\\" print(handle_system_error(9999)) # Output: \\"Unknown error code: 9999\\" ``` Constraints - Ensure your function handles all possible int inputs, including those not defined by the `errno` module. - Use built-in Python modules only (`errno` and `os`). Evaluation Criteria - **Correctness**: The function should correctly identify and return the detailed error message for valid error codes. - **Error Handling**: The function should handle unknown error codes gracefully. - **Code Quality**: Code should be clean, well-commented, and follow Python\'s best practices.","solution":"import errno import os def handle_system_error(error_code): Returns a detailed error message for the given error code. Parameters: - error_code: int, the error code to look up. Returns: - str, a detailed error message or notification of unknown code. if error_code in errno.errorcode: error_name = errno.errorcode[error_code] error_message = os.strerror(error_code) return f\\"Error {error_code}: {error_name} - {error_message}\\" else: return f\\"Unknown error code: {error_code}\\" # Example usage: # print(handle_system_error(2)) # Output: \\"Error 2: ENOENT - No such file or directory\\" # print(handle_system_error(13)) # Output: \\"Error 13: EACCES - Permission denied\\" # print(handle_system_error(9999)) # Output: \\"Unknown error code: 9999\\""},{"question":"**Objective**: Demonstrate the ability to use the `importlib.metadata` module to manage and query installed package metadata in Python. # Problem Description: You are part of a development team responsible for maintaining the dependencies of a Python project. Your task is to write a set of functions using the `importlib.metadata` module to perform the following operations: 1. **Get Package Version**: Write a function `get_package_version(package_name: str) -> str` that returns the version of the specified package. 2. **List Installed Packages**: Write a function `list_installed_packages() -> List[str]` that returns a list of all installed package names. 3. **Get Package Metadata**: Write a function `get_package_metadata(package_name: str) -> dict` that returns a dictionary containing the metadata for the specified package. 4. **List Entry Points**: Write a function `list_entry_points(group: str) -> List[Tuple[str, str]]` that returns a list of all entry points in the specified group. Each entry point should be represented as a tuple `(name, value)`. 5. **List Package Files**: Write a function `list_package_files(package_name: str) -> Optional[List[str]]` that returns a list of all files installed by the specified package. If no files are found for the package, the function should return `None`. 6. **List Package Requirements**: Write a function `list_package_requirements(package_name: str) -> List[str]` that returns a list of requirements for the specified package. # Constraints: - The functions should handle cases where the specified package does not exist gracefully, returning appropriate Python built-in exceptions or `None` where applicable. - The implementation should be efficient and use the provided functionalities of `importlib.metadata` where possible. # Example Usage: ```python >>> get_package_version(\'pip\') \'21.0.1\' >>> list_installed_packages() [\'pip\', \'setuptools\', \'wheel\', ...] >>> get_package_metadata(\'pip\') {\'Name\': \'pip\', \'Version\': \'21.0.1\', \'Summary\': \'The PyPA recommended tool for installing Python packages.\', ...} >>> list_entry_points(\'console_scripts\') [(\'pip\', \'pip._internal.cli.main:main\'), (\'pip3\', \'pip._internal.cli.main:main\')] >>> list_package_files(\'pip\') [\'pip/_internal/__init__.py\', \'pip/_internal/cli/__init__.py\', ...] >>> list_package_requirements(\'pip\') [\'setuptools\', \'wheel\'] ``` # Submission: Submit a Python script containing the implementation of the above functions. **Note**: Ensure your code adheres to PEP 8 standards, and include appropriate comments and docstrings for each function.","solution":"from importlib.metadata import version, packages_distributions, metadata, entry_points, files, PackageNotFoundError from typing import List, Tuple, Optional, Dict def get_package_version(package_name: str) -> str: Returns the version of the specified package. try: return version(package_name) except PackageNotFoundError: return f\\"Package \'{package_name}\' not found.\\" def list_installed_packages() -> List[str]: Returns a list of all installed package names. return list(packages_distributions().keys()) def get_package_metadata(package_name: str) -> Dict[str, str]: Returns a dictionary containing the metadata for the specified package. try: return dict(metadata(package_name)) except PackageNotFoundError: return {\\"error\\": f\\"Package \'{package_name}\' not found.\\"} def list_entry_points(group: str) -> List[Tuple[str, str]]: Returns a list of all entry points in the specified group. Each entry point is represented as a tuple (name, value). return [(ep.name, ep.value) for ep in entry_points().get(group, [])] def list_package_files(package_name: str) -> Optional[List[str]]: Returns a list of all files installed by the specified package. If no files are found for the package, the function returns None. try: return [str(file) for file in files(package_name)] except PackageNotFoundError: return None def list_package_requirements(package_name: str) -> List[str]: Returns a list of requirements for the specified package. try: return metadata(package_name).get_all(\'Requires-Dist\') or [] except PackageNotFoundError: return []"},{"question":"You are given sales data from two different companies (`company_a_sales` and `company_b_sales`) over the same time period. These datasets contain monthly revenue numbers and other financial metrics. Your task is to write a function `analyze_sales` that performs the following operations: 1. **Load Data**: The data for each company will be provided as a CSV file. Load these CSV files into two separate DataFrames. 2. **Merge Data**: Merge these DataFrames on their date column, so that you have a consolidated view of both companies\' data. Ensure that all the dates from both DataFrames are included in the final DataFrame (i.e., perform an outer join). 3. **Fill Missing Values**: Fill any missing values in the merged DataFrame with the string `\'N/A\'`. 4. **Calculate Metrics**: - Add a new column `Revenue_Difference` which calculates the difference between the monthly revenue of `company_a` and `company_b`. - Add another column `Profit_Margin_A` and `Profit_Margin_B` which is calculated as `(Profit / Revenue) * 100` for each respective company. 5. **Draw Insights**: - Identify the month with the highest `Revenue_Difference`. - Identify the month where `Profit_Margin_A` was maximum. Implement your solution using the pandas library. Your function should output the final merged DataFrame and the identified insights. **Constraints**: 1. The date columns in the CSV files are named `Date`. 2. The revenue and profit columns for `company_a` and `company_b` in the DataFrames are named as `Revenue_A`, `Profit_A` and `Revenue_B`, `Profit_B` respectively. 3. Ensure to handle any missing data appropriately. **Input**: - Paths to two CSV files: `company_a_sales.csv`, `company_b_sales.csv` **Output**: - A merged DataFrame (as specified) - A string indicating the month with the highest Revenue Difference - A string indicating the month with the highest Profit Margin for `company_a` ```python import pandas as pd def analyze_sales(company_a_csv, company_b_csv): # 1. Load Data company_a_sales = pd.read_csv(company_a_csv) company_b_sales = pd.read_csv(company_b_csv) # 2. Merge Data merged_data = pd.merge(company_a_sales, company_b_sales, how=\'outer\', on=\'Date\') # 3. Fill Missing Values merged_data.fillna(\'N/A\', inplace=True) # 4. Calculate Metrics merged_data[\'Revenue_Difference\'] = merged_data[\'Revenue_A\'] - merged_data[\'Revenue_B\'] merged_data[\'Profit_Margin_A\'] = (merged_data[\'Profit_A\'] / merged_data[\'Revenue_A\']) * 100 merged_data[\'Profit_Margin_B\'] = (merged_data[\'Profit_B\'] / merged_data[\'Revenue_B\']) * 100 # 5. Draw Insights highest_revenue_diff_month = merged_data.loc[merged_data[\'Revenue_Difference\'].idxmax(), \'Date\'] highest_profit_margin_a_month = merged_data.loc[merged_data[\'Profit_Margin_A\'].idxmax(), \'Date\'] return merged_data, highest_revenue_diff_month, highest_profit_margin_a_month # Example usage: # merged_df, highest_rev_diff_month, highest_profit_margin_month = analyze_sales(\'company_a_sales.csv\', \'company_b_sales.csv\') # print(merged_df) # print(\\"Month with highest Revenue Difference:\\", highest_rev_diff_month) # print(\\"Month with highest Profit Margin for Company A:\\", highest_profit_margin_month) ``` Ensure your code passes various scenarios such as missing values, non-overlapping dates, and typical financial data discrepancies to robustly validate the solution.","solution":"import pandas as pd def analyze_sales(company_a_csv, company_b_csv): Analyze sales data from two companies, merging their monthly data, handling missing values, calculating financial metrics, and drawing insights. Parameters: - company_a_csv: Path to the CSV file for company A. - company_b_csv: Path to the CSV file for company B. Returns: - merged_data (pd.DataFrame): The merged DataFrame with calculated metrics. - highest_revenue_diff_month (str): The month with the highest revenue difference. - highest_profit_margin_a_month (str): The month with the highest profit margin for company A. # Load Data company_a_sales = pd.read_csv(company_a_csv) company_b_sales = pd.read_csv(company_b_csv) # Merge Data on \'Date\' with an outer join merged_data = pd.merge(company_a_sales, company_b_sales, how=\'outer\', on=\'Date\') # Fill Missing Values merged_data.fillna(\'N/A\', inplace=True) # Calculate Metrics merged_data[\'Revenue_Difference\'] = merged_data.apply( lambda row: row[\'Revenue_A\'] - row[\'Revenue_B\'] if row[\'Revenue_A\'] != \'N/A\' and row[\'Revenue_B\'] != \'N/A\' else \'N/A\', axis=1) merged_data[\'Profit_Margin_A\'] = merged_data.apply( lambda row: (row[\'Profit_A\'] / row[\'Revenue_A\']) * 100 if row[\'Profit_A\'] != \'N/A\' and row[\'Revenue_A\'] != \'N/A\' else \'N/A\', axis=1) merged_data[\'Profit_Margin_B\'] = merged_data.apply( lambda row: (row[\'Profit_B\'] / row[\'Revenue_B\']) * 100 if row[\'Profit_B\'] != \'N/A\' and row[\'Revenue_B\'] != \'N/A\' else \'N/A\', axis=1) # Draw Insights valid_revenue_difference = merged_data[merged_data[\'Revenue_Difference\'] != \'N/A\'] highest_revenue_diff_month = valid_revenue_difference.loc[valid_revenue_difference[\'Revenue_Difference\'].idxmax(), \'Date\'] if not valid_revenue_difference.empty else \'N/A\' valid_profit_margin_a = merged_data[merged_data[\'Profit_Margin_A\'] != \'N/A\'] highest_profit_margin_a_month = valid_profit_margin_a.loc[valid_profit_margin_a[\'Profit_Margin_A\'].idxmax(), \'Date\'] if not valid_profit_margin_a.empty else \'N/A\' return merged_data, highest_revenue_diff_month, highest_profit_margin_a_month # Example usage: # merged_df, highest_rev_diff_month, highest_profit_margin_month = analyze_sales(\'company_a_sales.csv\', \'company_b_sales.csv\') # print(merged_df) # print(\\"Month with highest Revenue Difference:\\", highest_rev_diff_month) # print(\\"Month with highest Profit Margin for Company A:\\", highest_profit_margin_month)"},{"question":"Objective: Create a logger configuration using the `dictConfig` method from the `logging.config` module. The objective is to demonstrate your understanding of the configuration dictionary schema and proper logging setup. Task: 1. Implement a function `setup_logger` that takes no parameters. 2. In this function, create a logging configuration dictionary with the following specifications: - **version**: Set to 1. - **formatters**: Define two formatters: - `simple`: A formatter that only logs the message. - `detailed`: A formatter that logs the time, name of the logger, log level, and the message. - **handlers**: Define two handlers: - `console`: A `StreamHandler` that uses the `simple` formatter and set to log `DEBUG` level messages to the console. - `file`: A `RotatingFileHandler` that uses the `detailed` formatter and set to log `INFO` level messages to a file named \'app.log\'. This handler should rotate the log file every 1MB, keeping up to three backup files. - **loggers**: Define a logger `my_logger`: - `level`: Set to `DEBUG`. - `handlers`: Use both `console` and `file` handlers. - `propagate`: Set to `False`. - **root**: Configure the root logger to use only the `console` handler. 3. Use the `dictConfig` method to apply this configuration. Expected Output: To verify your implementation, log some test messages at various levels (`DEBUG`, `INFO`, `WARNING`, `ERROR`, `CRITICAL`) using the `my_logger` logger and check both the console output and the content of the \'app.log\' file. Constraints: - Do not hardcode any absolute file paths. Assume the current working directory is suitable for \'app.log\'. - Ensure the dictionary schema adheres to the format described in the documentation. Function Signature: ```python def setup_logger(): ... ``` Example: ```python def setup_logger(): import logging.config config = { \'version\': 1, \'formatters\': { \'simple\': { \'format\': \'%(message)s\' }, \'detailed\': { \'format\': \'%(asctime)s %(name)s %(levelname)s %(message)s\' } }, \'handlers\': { \'console\': { \'class\': \'logging.StreamHandler\', \'level\': \'DEBUG\', \'formatter\': \'simple\', \'stream\': \'ext://sys.stdout\' }, \'file\': { \'class\': \'logging.handlers.RotatingFileHandler\', \'level\': \'INFO\', \'formatter\': \'detailed\', \'filename\': \'app.log\', \'maxBytes\': 1048576, \'backupCount\': 3 } }, \'loggers\': { \'my_logger\': { \'level\': \'DEBUG\', \'handlers\': [\'console\', \'file\'], \'propagate\': False } }, \'root\': { \'handlers\': [\'console\'] } } logging.config.dictConfig(config) # Example usage setup_logger() logger = logging.getLogger(\'my_logger\') logger.debug(\'This is a debug message.\') logger.info(\'This is an info message.\') logger.warning(\'This is a warning message.\') logger.error(\'This is an error message.\') logger.critical(\'This is a critical message.\') ```","solution":"def setup_logger(): import logging.config config = { \'version\': 1, \'formatters\': { \'simple\': { \'format\': \'%(message)s\' }, \'detailed\': { \'format\': \'%(asctime)s %(name)s %(levelname)s %(message)s\' } }, \'handlers\': { \'console\': { \'class\': \'logging.StreamHandler\', \'level\': \'DEBUG\', \'formatter\': \'simple\', \'stream\': \'ext://sys.stdout\' }, \'file\': { \'class\': \'logging.handlers.RotatingFileHandler\', \'level\': \'INFO\', \'formatter\': \'detailed\', \'filename\': \'app.log\', \'maxBytes\': 1048576, \'backupCount\': 3 } }, \'loggers\': { \'my_logger\': { \'level\': \'DEBUG\', \'handlers\': [\'console\', \'file\'], \'propagate\': False } }, \'root\': { \'handlers\': [\'console\'] } } logging.config.dictConfig(config)"},{"question":"**Coding Question:** # Task: You are provided with an AU sound file. Your task is to read the file, manipulate its data, and save the modified data into a new AU file. Specifically, you are required to perform the following steps: 1. **Read the AU file:** - Open the file in read mode. - Extract and display the following details from the file: - Number of channels - Sample width - Frame rate - Number of frames - Compression type - Read the audio frames. 2. **Manipulate the audio data:** - Change the sample width to 16-bit if it is not already. - Double the frame rate. 3. **Save the manipulated data into a new AU file:** - Create a new AU file. - Set the parameters (number of channels, sample width, frame rate, etc.) from the original file, but with the modified values. - Write the manipulated audio frames into the new AU file. # Input/Output: - **Input:** Path to the original AU file as a string. - **Output:** Path to the new AU file as a string. # Constraints: - The input AU file will exist and be a valid AU file. - The new file should be written in a format that can be read by standard audio software supporting AU files. # Example: Suppose you have an AU file at path `\\"original.au\\"`. The output should be written to `\\"modified.au\\"` with the specified changes. # Function Signature: ```python import sunau def manipulate_au_file(input_file_path: str, output_file_path: str) -> None: # Your code here ``` # Performance Requirements: - The function should handle AU files of up to 100 MB efficiently. - Ensure that proper resource management (such as closing files) is followed. # Additional Notes: - Use the functions and methods provided by the **sunau** module. - Handle any potential errors by printing an appropriate message and terminate the program gracefully. # Documentation: Refer to the official `sunau` documentation for more details on its usage: [Python sunau documentation](https://docs.python.org/3/library/sunau.html)","solution":"import sunau from typing import Tuple def manipulate_au_file(input_file_path: str, output_file_path: str) -> None: try: with sunau.open(input_file_path, \'rb\') as infile: num_channels = infile.getnchannels() sample_width = infile.getsampwidth() frame_rate = infile.getframerate() num_frames = infile.getnframes() compression_type = infile.getcomptype() audio_frames = infile.readframes(num_frames) print(f\\"Number of channels: {num_channels}\\") print(f\\"Sample width: {sample_width}\\") print(f\\"Frame rate: {frame_rate}\\") print(f\\"Number of frames: {num_frames}\\") print(f\\"Compression type: {compression_type}\\") # Manipulate the sample width to 16-bit if it\'s not already new_sample_width = 2 # 2 bytes for 16-bits if sample_width != new_sample_width: # Here we need to convert the sample data to 16-bit format # This could involve more complex processing, depending on the original sample width # For simplicity, this code assumes 8-bit to 16-bit conversion if sample_width == 1: audio_frames = bytes([x * 256 for x in audio_frames]) else: raise ValueError(\\"Only 8-bit to 16-bit conversion is supported in this example.\\") # Double the frame rate new_frame_rate = frame_rate * 2 with sunau.open(output_file_path, \'wb\') as outfile: outfile.setnchannels(num_channels) outfile.setsampwidth(new_sample_width) outfile.setframerate(new_frame_rate) outfile.setnframes(num_frames) outfile.writeframes(audio_frames) except Exception as e: print(f\\"An error occurred: {e}\\")"},{"question":"# Question: Binary Data Encoding and Decoding Using Python\'s `struct` Module You are required to write a function that takes a list of tuples containing different data types and a format string, packs this data into a bytes object according to the format string, and then unpacks the bytes object back into the original tuples. Specifically, the function should demonstrate comprehension of various format characters, byte order, size, and alignment concepts. Function Signature ```python def encode_decode_data(data, format_str): Encodes a list of tuples into a bytes object using the provided format string, then decodes the bytes object back into the original data. Parameters: data (list): A list of tuples, where each tuple contains values to be packed. format_str (str): A format string defining the layout of the packed data. Returns: tuple: A tuple containing the packed bytes object and the unpacked list of tuples. pass ``` Input - `data` (list): A list of tuples to be packed. Each tuple can contain various types (e.g., integers, floats, strings). Example: `[(1, 2.0, b\'hello\'), (3, 4.0, b\'world\')]` - `format_str` (str): A format string that specifies the encoding. The string should follow the rules from the `struct` module. Example: `\'i f 5s\'` for packing an `int`, `float`, and bytes of length 5. Output - Returns a tuple containing: - The packed bytes object. - The unpacked list of tuples after decoding the bytes object back to original data. Constraints 1. The format string will always be a valid format string as per the `struct` module\'s specifications. 2. The data list will always match the format string\'s specified types and lengths. 3. The bytes object should be packed and unpacked correctly without any data loss. Example ```python data = [(1, 2.0, b\'hello\'), (3, 4.0, b\'world\')] format_str = \'i f 5s\' packed_bytes, unpacked_data = encode_decode_data(data, format_str) # packed_bytes: b\'x01x00x00x00x00x00x00@hellox00x00x00x00@x00x00world\' # unpacked_data: [(1, 2.0, b\'hello\'), (3, 4.0, b\'world\')] ``` Note The example demonstrates packing two tuples each containing an integer, a float, and a 5-byte string. The result (`packed_bytes`) will be a single bytes object containing both packed tuples. The function should then unpack `packed_bytes` back into the list `unpacked_data`, which should match the original input data.","solution":"import struct def encode_decode_data(data, format_str): Encodes a list of tuples into a bytes object using the provided format string, then decodes the bytes object back into the original data. Parameters: data (list): A list of tuples, where each tuple contains values to be packed. format_str (str): A format string defining the layout of the packed data. Returns: tuple: A tuple containing the packed bytes object and the unpacked list of tuples. packed_bytes = b\'\' # Pack the data for item in data: packed_bytes += struct.pack(format_str, *item) # Unpack the data unpacked_data = [] item_size = struct.calcsize(format_str) for i in range(0, len(packed_bytes), item_size): unpacked_data.append(struct.unpack(format_str, packed_bytes[i:i + item_size])) return packed_bytes, unpacked_data"},{"question":"# Coding Assessment: Exception Handling in Python Objective Design and implement a Python function that processes a list of file paths and reads content from each file. The function should robustly handle different exceptions that might occur and demonstrate an understanding of exception chaining. Problem Statement Write a function `read_files(file_paths: List[str]) -> List[str]` that takes a list of file paths and returns a list of file contents. Your function should handle the following exceptions appropriately: 1. If a file does not exist, log a custom exception `FileNotFoundCustomError` and continue with the remaining files. 2. If a file cannot be read due to permission issues, log a custom exception `PermissionCustomError` and skip to the next file. 3. For any other unexpected exceptions, raise a `ProcessingError` indicating the file path that caused the error. Use exception chaining to retain the original exception context. Custom Exceptions - Define a base exception `FileError` which inherits from `Exception`. - Define two custom exceptions `FileNotFoundCustomError` and `PermissionCustomError` inheriting from `FileError`. - Define a `ProcessingError` also inheriting from `FileError` for other unforeseen errors. Requirements - **Input:** A list of file paths (strings). - **Output:** A list of file contents (strings). - If a file is successfully read, its content is appended to the results list. - Use exception chaining to retain original context for unexpected errors. - Ensure all exceptions are logged using Python\'s `logging` module. Constraints - You may assume `file_paths` is a valid list of strings. - The function should not terminate abruptly due to unhandled exceptions. Function Signature ```python from typing import List def read_files(file_paths: List[str]) -> List[str]: pass ``` Example ```python file_paths = [\\"file1.txt\\", \\"invalid_path.txt\\", \\"forbidden_file.txt\\"] try: contents = read_files(file_paths) print(contents) except ProcessingError as e: print(f\\"An error occurred: {e}\\") ``` You should include the implementations of the required custom exceptions and ensure the function handles errors gracefully while continuing to process remaining files.","solution":"import logging from typing import List class FileError(Exception): pass class FileNotFoundCustomError(FileError): pass class PermissionCustomError(FileError): pass class ProcessingError(FileError): pass def read_files(file_paths: List[str]) -> List[str]: file_contents = [] for file_path in file_paths: try: with open(file_path, \'r\') as file: file_contents.append(file.read()) except FileNotFoundError: logging.error(f\\"File not found: {file_path}\\") raise FileNotFoundCustomError(f\\"File not found: {file_path}\\") from None except PermissionError: logging.error(f\\"Permission denied: {file_path}\\") raise PermissionCustomError(f\\"Permission denied: {file_path}\\") from None except Exception as e: logging.error(f\\"An error occurred while processing file: {file_path}\\") raise ProcessingError(f\\"An error occurred while processing file: {file_path}\\") from e return file_contents"},{"question":"**Objective:** Implement a Python program that uses the `signal` module to handle signals in a Unix system environment. Your task is to create a Python script that demonstrates handling multiple signals and performing specific actions based on the received signal. **Background:** Signals are asynchronous notifications sent to a process or thread to notify it of an event that occurred. Handling signals effectively is crucial in certain applications, especially those involving long-running processes, timers, or inter-process communication. Task: 1. **Signal Handlers Setup:** - Write functions to handle `SIGINT`, `SIGTERM`, and `SIGALRM` signals. - The `SIGINT` handler should print `\\"SIGINT received\\"` and raise a `KeyboardInterrupt`. - The `SIGTERM` handler should print `\\"SIGTERM received\\"` and terminate the program with exit code 0. - The `SIGALRM` handler should print `\\"SIGALRM received\\"` and display the time at which the signal was received. 2. **Alarm Timer Configuration:** - Configure an alarm timer to send a `SIGALRM` signal to the process after a specified number of seconds. 3. **Signal Handler Registration:** - Register the signal handlers using the `signal.signal()` function. 4. **Custom Signal Raising:** - Inside the main function, set the alarm for 10 seconds and raise a `SIGTERM` signal after 5 seconds. 5. **Program Execution:** - The main function should handle and display any exceptions raised, ensuring that the program can handle the `SIGINT` without terminating immediately. Input/Output Requirements: - **Input:** No user input is required. - **Output:** Print statements as specified in the signal handlers, along with any exceptions handled in the main execution. Constraints: - The script must function correctly on a Unix system. - Handle signals and exceptions as described. - Use the appropriate `signal` module functions to set and handle signals. Performance: - Ensure that signal handling does not make the program hang or behave unpredictably. - The program should be responsive to the signals and should exit gracefully as required by the handlers. Example: ```python import signal import time import os # Step 1: Define signal handlers def handle_sigint(signum, frame): print(\\"SIGINT received\\") raise KeyboardInterrupt def handle_sigterm(signum, frame): print(\\"SIGTERM received\\") exit(0) def handle_sigalrm(signum, frame): print(f\\"SIGALRM received at {time.ctime()}\\") # Step 2: Set alarm timer # Registering signal handlers signal.signal(signal.SIGINT, handle_sigint) signal.signal(signal.SIGTERM, handle_sigterm) signal.signal(signal.SIGALRM, handle_sigalrm) # Setting an alarm for 10 seconds signal.alarm(10) # Step 4: Raising custom signal after 5 seconds def main(): try: time.sleep(5) print(\\"Raising SIGTERM...\\") os.kill(os.getpid(), signal.SIGTERM) except KeyboardInterrupt: print(\\"KeyboardInterrupt handled in main\\") if __name__ == \\"__main__\\": main() ``` Output: ``` Raising SIGTERM... SIGTERM received ``` **Note**: The actual time printed for `SIGALRM` may vary based on when the signal is received.","solution":"import signal import time import os def handle_sigint(signum, frame): print(\\"SIGINT received\\") raise KeyboardInterrupt def handle_sigterm(signum, frame): print(\\"SIGTERM received\\") exit(0) def handle_sigalrm(signum, frame): print(f\\"SIGALRM received at {time.ctime()}\\") # Register signal handlers signal.signal(signal.SIGINT, handle_sigint) signal.signal(signal.SIGTERM, handle_sigterm) signal.signal(signal.SIGALRM, handle_sigalrm) def main(): try: # Set an alarm for 10 seconds signal.alarm(10) # Sleep for 5 seconds before raising SIGTERM time.sleep(5) print(\\"Raising SIGTERM...\\") # Raise SIGTERM signal os.kill(os.getpid(), signal.SIGTERM) except KeyboardInterrupt: print(\\"KeyboardInterrupt handled in main\\") if __name__ == \\"__main__\\": main()"},{"question":"# Python Coding Assessment Question: Chunk Data Processing You are tasked with processing a file that uses EA IFF 85 chunked data format. The objective is to read the file and extract information from each chunk sequentially. You must write a function that reads the file and performs specific operations on each chunk. Task Write a function `process_chunked_file(file_path: str) -> List[Tuple[str, int]]` that reads a chunked file and returns a list of tuples where each tuple contains: - the chunk ID (first 4 bytes as a string) - the size of the chunk (excluding the header) The function should adhere to the following details and constraints: Details and Constraints 1. The `file_path` is a path to a file that you need to process. 2. Use the `Chunk` class from the `chunk` module to interact with the chunked file. 3. Each tuple in the result should be in the order the chunks appear in the file. 4. Skip over chunks where data is not required (you can use the `skip()` method for this). 5. Ensure you read at most the size of each chunk to avoid buffer overflow issues. 6. You can assume the file is formatted correctly as per the EA IFF 85 specification. 7. Implement proper error handling to manage end-of-file exceptions. **Example**: ```python def process_chunked_file(file_path: str) -> List[Tuple[str, int]]: from chunk import Chunk results = [] with open(file_path, \'rb\') as f: try: while True: chk = Chunk(f, align=True, bigendian=True, inclheader=False) chunk_id = chk.getname() chunk_size = chk.getsize() results.append((chunk_id, chunk_size)) chk.skip() except EOFError: pass # End of file reached, stop processing. return results # Assuming the chunked file has chunks with IDs: \'CHNK\', \'DATA\', etc. # Example output: [(\'CHNK\', 1024), (\'DATA\', 2048)] ``` Testing You should create a chunked data file and ensure your function processes it correctly. Verify the output list is correct and corresponds to the chunk IDs and sizes specified. Good luck, and pay careful attention to how you read and manage chunks in your file to ensure efficient and correct processing!","solution":"from typing import List, Tuple from chunk import Chunk def process_chunked_file(file_path: str) -> List[Tuple[str, int]]: Reads a chunked file and returns a list of tuples containing the chunk ID and size of each chunk. :param file_path: Path to the chunked file to process :return: List of tuples where each tuple contains the chunk ID (str) and chunk size (int) results = [] with open(file_path, \'rb\') as f: try: while True: chk = Chunk(f, align=True, bigendian=True, inclheader=False) chunk_id = chk.getname().decode(\'ascii\') chunk_size = chk.getsize() results.append((chunk_id, chunk_size)) chk.skip() except EOFError: pass # End of file reached, stop processing. return results"},{"question":"# Plotting with Color Palettes in Seaborn **Objective:** Use your understanding of Seaborn\'s color palette options to visualize a dataset effectively. This exercise will test your ability to select and implement appropriate color palettes for different types of data. **Question:** You are provided with the `tips` dataset available in Seaborn, which contains information about tips collected from a restaurant. Your tasks are: 1. **Task 1: Visualize Total Bill by Day** - Create a bar plot showing the average `total_bill` for each `day` of the week. - Use a qualitative color palette (e.g., `Set2`) to distinguish the days. 2. **Task 2: Visualize Relationship between Total Bill and Tip** - Create a scatter plot showing the relationship between `total_bill` and `tip`. - Use a sequential color palette to color the points based on the size of the tips. Make sure the palette enhances the visibility of the variations in tip amounts. 3. **Task 3: Visualize Distribution of Total Bill** - Create a histogram showing the distribution of `total_bill`. - Use a diverging color palette that emphasizes low and high ends of the total bill distribution. # Inputs: - The `tips` dataset from Seaborn\'s in-built datasets. # Output: - Three plots as specified in the tasks above. # Constraints: - You should use Seaborn for all plotting. - Each plot should be clearly labeled with titles, axis labels, and a legend where appropriate. - Ensure to use color palettes effectively to make patterns or distinctions in the data clear. # Example Code: ```python import seaborn as sns import matplotlib.pyplot as plt # Load the dataset tips = sns.load_dataset(\\"tips\\") # Task 1: Bar plot for average total bill by day plt.figure(figsize=(10, 6)) sns.barplot(x=\'day\', y=\'total_bill\', data=tips, palette=\'Set2\') plt.title(\'Average Total Bill by Day\') plt.xlabel(\'Day of the Week\') plt.ylabel(\'Average Total Bill\') plt.show() # Task 2: Scatter plot for total bill vs tip plt.figure(figsize=(10, 6)) sns.scatterplot(x=\'total_bill\', y=\'tip\', hue=\'tip\', data=tips, palette=\'viridis\') plt.title(\'Scatter plot of Total Bill vs Tip\') plt.xlabel(\'Total Bill\') plt.ylabel(\'Tip\') plt.show() # Task 3: Histogram for the distribution of total bill plt.figure(figsize=(10, 6)) sns.histplot(tips[\'total_bill\'], kde=True, palette=\'coolwarm\') plt.title(\'Distribution of Total Bill\') plt.xlabel(\'Total Bill\') plt.ylabel(\'Frequency\') plt.show() ``` In this task, you must demonstrate an understanding of selecting and applying appropriate color palettes to reveal insights from the data effectively. **Note:** Customize the code as needed for better visualization according to the given tasks.","solution":"import seaborn as sns import matplotlib.pyplot as plt def visualize_tips_data(): # Load the dataset tips = sns.load_dataset(\\"tips\\") # Task 1: Bar plot for average total bill by day plt.figure(figsize=(10, 6)) sns.barplot(x=\'day\', y=\'total_bill\', data=tips, palette=\'Set2\') plt.title(\'Average Total Bill by Day\') plt.xlabel(\'Day of the Week\') plt.ylabel(\'Average Total Bill\') plt.show() # Task 2: Scatter plot for total bill vs tip plt.figure(figsize=(10, 6)) sns.scatterplot(x=\'total_bill\', y=\'tip\', hue=\'tip\', data=tips, palette=\'viridis\') plt.title(\'Scatter plot of Total Bill vs Tip\') plt.xlabel(\'Total Bill\') plt.ylabel(\'Tip\') plt.show() # Task 3: Histogram for the distribution of total bill plt.figure(figsize=(10, 6)) sns.histplot(tips[\'total_bill\'], kde=True, color=\'blue\') plt.title(\'Distribution of Total Bill\') plt.xlabel(\'Total Bill\') plt.ylabel(\'Frequency\') plt.show()"},{"question":"**File System Path Handling in Python** **Objective:** Create a function in Python that emulates the behavior of the `PyOS_FSPath` function described in the documentation. This function should handle various types of input related to file paths and return the correct file system representation or raise the appropriate exception. **Task:** You are required to implement a function `my_fs_path(path)` in Python that does the following: - Accepts a single argument `path`. - If `path` is a string (`str`) or bytes object, it returns the path as is. - If `path` implements the `os.PathLike` interface, returns the result of calling its `__fspath__()` method. - Raises a `TypeError` exception if `path` does not meet any of the above conditions. **Function Signature:** ```python def my_fs_path(path): Parameters: path (str, bytes, or os.PathLike): The input path object. Returns: str or bytes: The file system representation of the path. Raises: TypeError: If the path is not a valid type. pass ``` **Constraints:** - The function should handle both string and bytes types correctly. - The function should ensure compatibility with any object that implements the `os.PathLike` interface. - Appropriate exceptions should be raised for invalid inputs. **Example Usage:** ```python import os class PathLikeMock: def __fspath__(self): return \\"/mock/path\\" # Example 1 print(my_fs_path(\\"/example/path\\")) # Output: \\"/example/path\\" # Example 2 print(my_fs_path(b\\"/example/path\\")) # Output: b\\"/example/path\\" # Example 3 mock_path = PathLikeMock() print(my_fs_path(mock_path)) # Output: \\"/mock/path\\" # Example 4 try: my_fs_path(42) # Should raise TypeError except TypeError as e: print(e) # Output: \\"path should be a string, bytes, or os.PathLike object\\" ``` Make sure your implementation handles the different types robustly and accounts for custom objects implementing the `os.PathLike` interface. **Note:** You can assume the `os` module is available and does not need to be imported within your function for the purpose of this assessment.","solution":"import os def my_fs_path(path): Parameters: path (str, bytes, or os.PathLike): The input path object. Returns: str or bytes: The file system representation of the path. Raises: TypeError: If the path is not a valid type. if isinstance(path, (str, bytes)): return path elif hasattr(path, \'__fspath__\'): return path.__fspath__() else: raise TypeError(\\"path should be a string, bytes, or os.PathLike object\\")"},{"question":"Objective: Demonstrate your understanding of sklearn’s label transformation tools by implementing a composite preprocessing function that prepares labels for both multiclass and multilabel classification in a given dataset. Task: You need to implement a function `preprocess_labels` that takes the following parameters: - `label_data`: A list of labels. Each label can be either a single label (for multiclass classification) or a collection of labels (for multilabel classification). - `mode`: A string that can either be `\\"multiclass\\"` or `\\"multilabel\\"`, specifying the type of classification problem. The function should process the labels as follows: 1. If `mode` is `\\"multiclass\\"`, use `LabelBinarizer` to transform the labels into a label indicator matrix. 2. If `mode` is `\\"multilabel\\"`, use `MultiLabelBinarizer` to transform the labels into a label binary indicator array. The function should return: 1. The transformed label array. 2. The list of classes inferred from the input labels. Constraints: - The input `label_data` will have 1 to 1000 elements. - Each label in `label_data` (for multi-class) or each sub-label (for multi-label) can have a maximum length of 32 characters. - `mode` will always be either `\\"multiclass\\"` or `\\"multilabel\\"`. Example Usage: ```python def preprocess_labels(label_data, mode): # Your implementation here pass # Example 1: Multiclass Classification labels_1 = [1, 2, 6, 4, 2] result_1, classes_1 = preprocess_labels(labels_1, \\"multiclass\\") # Expected Output: # result_1 = [[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1], [0, 1, 0, 0]] # classes_1 = [1, 2, 4, 6] # Example 2: Multilabel Classification labels_2 = [[2, 3, 4], [2], [0, 1, 3], [0, 1, 2, 3, 4], [0, 1, 2]] result_2, classes_2 = preprocess_labels(labels_2, \\"multilabel\\") # Expected Output: # result_2 = [[0, 0, 1, 1, 1], [0, 0, 1, 0, 0], [1, 1, 0, 1, 0], [1, 1, 1, 1, 1], [1, 1, 1, 0, 0]] # classes_2 = [0, 1, 2, 3, 4] ``` Note: - In the example, `result_1` and `result_2` should match the transformations shown in the provided documentation.","solution":"from sklearn.preprocessing import LabelBinarizer, MultiLabelBinarizer def preprocess_labels(label_data, mode): Preprocess the labels for multiclass or multilabel classification. Parameters: - label_data: List of labels for classification. - mode: \\"multiclass\\" or \\"multilabel\\". Returns: - transformed_label_data: Transformed label indicator matrix or binary indicator array. - classes: List of classes inferred from the input labels. if mode == \\"multiclass\\": lb = LabelBinarizer() transformed_label_data = lb.fit_transform(label_data) classes = lb.classes_.tolist() elif mode == \\"multilabel\\": mlb = MultiLabelBinarizer() transformed_label_data = mlb.fit_transform(label_data) classes = mlb.classes_.tolist() else: raise ValueError(\\"mode should be either \'multiclass\' or \'multilabel\'\\") return transformed_label_data, classes"},{"question":"Objective: To assess the student\'s understanding and use of the `os` module (which is a superset of the `posix` module) for handling environment variables and system-level operations, including manipulation of large files. Problem Statement: You are required to create a Python script that performs the following tasks: 1. **Environment Variable Management:** - **Function Name:** `manage_environment` - **Input:** None - **Output:** A dictionary that represents the current environment variables, but only those whose keys start with the letter \'P\'. - **Details:** Using the `os` module, access the environment variables and filter the dictionary to include only keys that start with \'P\'. Return the filtered dictionary. 2. **Large File Handling:** - **Function Name:** `process_large_file` - **Input:** A file path as a string - **Output:** The size of the file in bytes as an integer. - **Details:** Create a function that opens a large file (potentially larger than 2 GiB) and calculates its size. Ensure the function handles files of such large sizes using appropriate flags/modes provided by the `os` module for large file support. 3. **Replacing Content in Large File:** - **Function Name:** `replace_content_in_large_file` - **Input:** A file path as a string, a target substring, and a replacement substring. - **Output:** None - **Details:** Create a function that reads through a large file and replaces every instance of the target substring with the replacement substring directly in the file without loading the entire file into memory. This will demonstrate efficient handling of large files. Constraints: - Ensure your solution handles all possible errors (e.g., file not found, inability to read, or write to the file). - Aim for efficient memory usage, especially when dealing with large files. - Avoid using third-party libraries; rely on standard Python libraries only. Example Usage: ```python # Example for manage_environment env_vars_with_p = manage_environment() print(env_vars_with_p) # Output: {\'PATH\': \'/usr/bin\', \'PYTHONPATH\': \'/usr/lib/python3.10\'} (example output) # Example for process_large_file file_size = process_large_file(\'/path/to/large/file.txt\') print(file_size) # Output: (size of file in bytes) # Example for replace_content_in_large_file replace_content_in_large_file(\'/path/to/large/file.txt\', \'target\', \'replacement\') ``` # Notes: 1. Use appropriate functions from the `os` module to interact with the system\'s environment and file system. 2. Ensure your code is well-documented with inline comments explaining the logic. 3. Remember to handle edge cases such as empty files, environment variables with no \'P\' prefix, and large file handling efficiently.","solution":"import os def manage_environment(): Returns a dictionary containing the current environment variables whose keys start with \'P\'. env_vars = os.environ filtered_env_vars = {key: value for key, value in env_vars.items() if key.startswith(\'P\')} return filtered_env_vars def process_large_file(file_path): Returns the size of the specified file in bytes. Args: - file_path (str): The path to the file. Returns: - int: The size of the file in bytes. try: return os.path.getsize(file_path) except OSError as e: print(f\\"Error: {e}\\") return -1 def replace_content_in_large_file(file_path, target, replacement): Replaces all occurrences of the target substring with the replacement substring in the specified file without loading the entire file into memory. Args: - file_path (str): The path to the file. - target (str): The substring to be replaced. - replacement (str): The substring to replace the target with. try: temp_file_path = file_path + \'.tmp\' with open(file_path, \'r\') as read_file, open(temp_file_path, \'w\') as write_file: for line in read_file: write_file.write(line.replace(target, replacement)) os.replace(temp_file_path, file_path) except OSError as e: print(f\\"Error: {e}\\")"},{"question":"# Question: Cross-Platform AsyncIO Event Loop Management The `asyncio` module in Python provides a framework for writing applications that incorporate asynchronous I/O operations. However, the functionality and support for different methods in `asyncio` can vary significantly across different operating systems such as Windows, macOS, and Unix. Your task is to implement a function that initializes an asyncio event loop in a platform-aware way and creates a TCP echo server that operates cross-platform despite these differences. You are required to: 1. Detect the operating system the script is running on. 2. Instantiate the appropriate default asyncio event loop that works based on the detected operating system. 3. Create a simple TCP echo server using `asyncio` that echoes back any data it receives. 4. Ensure correct handling of platform-specific constraints as outlined in the documentation provided. # Function Signature ```python async def run_echo_server(host: str, port: int): Start a TCP echo server that handles incoming connections and echoes back any received data. Parameters: - host (str): The hostname or IP address to bind the server. - port (int): The port number to bind the server. Returns: - None ``` # Requirements - The function should automatically detect the operating system and set up the appropriate event loop. - The server should accept connections on the given `host` and `port`. - Ensure that your implementation correctly handles available methods for the event loop on Windows and macOS. - Provide error handling for unsupported features on specific platforms. # Constraints - Use only the asyncio module from Python\'s standard library. - The function should not terminate; it should keep running until manually stopped (allow for clean shutdown with a signal handler if possible). # Example Usage ```python import asyncio async def main(): await run_echo_server(\'127.0.0.1\', 8888) if __name__ == \\"__main__\\": asyncio.run(main()) ``` # Evaluation Your solution will be evaluated based on: - Correctness: Successfully setting up and running the TCP echo server. - Handling of platform-specific constraints. - Code readability and proper usage of asyncio constructs. - Proper error handling and resource management.","solution":"import asyncio import platform async def handle_echo(reader, writer): data = await reader.read(100) message = data.decode() addr = writer.get_extra_info(\'peername\') print(f\\"Received {message} from {addr}\\") print(f\\"Send: {message}\\") writer.write(data) await writer.drain() print(\\"Close the connection\\") writer.close() async def run_echo_server(host: str, port: int): system = platform.system() if system == \\"Windows\\": asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy()) elif system == \\"Darwin\\": # macOS specific setup can be managed here, using the default loop for now pass else: # Unix-like systems (Linux, etc.) pass server = await asyncio.start_server(handle_echo, host, port) addr = server.sockets[0].getsockname() print(f\'Serving on {addr}\') async with server: await server.serve_forever()"},{"question":"Below is a coding assessment question that leverages the information given but also tests more comprehensive knowledge of seaborn: # Coding Challenge: Custom Visualizations with Seaborn Background You are tasked with creating a data visualization for a dataset of car attributes using seaborn, a powerful data visualization library in Python. You need to demonstrate your knowledge of seaborn’s functionalities, including setting themes, creating various plots, and generating custom color palettes. Dataset You will be working with a dataset `car_data.csv` with the following attributes: - `Manufacturer`: The manufacturer of the car. - `Model`: The model of the car. - `Year`: The manufacturing year of the car. - `Horsepower`: The horsepower of the car. - `MPG`: The miles per gallon of the car. Objective 1. Create a scatter plot of `Horsepower` vs `MPG` with points colored by `Manufacturer`. To distinguish the points better, use a custom blended palette. 2. Create a box plot showing the distribution of `Horsepower` for different `Manufacturer`s. Requirements 1. Use seaborn to set a custom theme for the plots. 2. Implement and apply a blended color palette with at least three distinct colors for coloring the `Manufacturer` categories. 3. Make sure the scatter plot includes appropriate axis labels, titles, and a legend. 4. Ensure the box plot is clearly labeled and color-coded appropriately. Function Signature ```python import seaborn as sns import pandas as pd import matplotlib.pyplot as plt def create_car_visualizations(data_path: str): Function to create and display car attribute visualizations using seaborn. Parameters: data_path (str): The path to the car dataset CSV file. Returns: None ``` Example Usage ```python create_car_visualizations(\\"path/to/car_data.csv\\") ``` Additional Information 1. You are free to choose the theme and the specific colors for the blended palette, but they should make the plots both visually appealing and easy to understand. 2. Make sure the code is well-documented and readable. Constraints - Assume that the dataset contains no missing values. - Performance is not a primary concern, but the function should efficiently handle a dataset of up to 10,000 rows.","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt def create_car_visualizations(data_path: str): Function to create and display car attribute visualizations using seaborn. Parameters: data_path (str): The path to the car dataset CSV file. Returns: None # Load the dataset car_data = pd.read_csv(data_path) # Set a custom theme sns.set_theme(style=\\"whitegrid\\") # Create a custom blended palette with at least three distinct colors custom_palette = sns.color_palette(\\"blend:white,red,blue,green\\", n_colors=car_data[\'Manufacturer\'].nunique()) # Scatter plot of Horsepower vs MPG with points colored by Manufacturer plt.figure(figsize=(10, 6)) scatter_plot = sns.scatterplot( data=car_data, x=\'Horsepower\', y=\'MPG\', hue=\'Manufacturer\', palette=custom_palette, s=100 ) scatter_plot.set_title(\'Horsepower vs MPG by Manufacturer\') scatter_plot.set_xlabel(\'Horsepower\') scatter_plot.set_ylabel(\'Miles Per Gallon (MPG)\') plt.legend(title=\'Manufacturer\', bbox_to_anchor=(1.05, 1), loc=\'upper left\') plt.show() # Box plot showing the distribution of Horsepower for different Manufacturers plt.figure(figsize=(12, 8)) box_plot = sns.boxplot( data=car_data, x=\'Manufacturer\', y=\'Horsepower\', palette=custom_palette ) box_plot.set_title(\'Distribution of Horsepower by Manufacturer\') box_plot.set_xlabel(\'Manufacturer\') box_plot.set_ylabel(\'Horsepower\') plt.xticks(rotation=45) plt.tight_layout() plt.show()"},{"question":"Handling Normalization Layers in PyTorch Models Objective Implement a function to modify the normalization layers in a given PyTorch model. The function should replace all instances of `BatchNorm2d` with `GroupNorm` if indicated, or alternatively, patch the `BatchNorm2d` layers to not use running statistics. Task 1. **Function Implementation:** Write a function `modify_norm_layers` that takes the following parameters: - `model`: A PyTorch model. - `use_group_norm`: A boolean indicating whether to replace `BatchNorm2d` with `GroupNorm`. Defaults to `True`. - `num_groups`: An integer specifying the number of groups for `GroupNorm` when `use_group_norm` is `True`. Defaults to 32. 2. **Function Behavior:** - If `use_group_norm` is `True`, replace all instances of `BatchNorm2d` in the model with `GroupNorm`. Ensure proper handling of GroupNorm’s requirements (e.g., number of channels must be divisible by the number of groups). - If `use_group_norm` is `False`, patch all instances of `BatchNorm2d` in the model to set `track_running_stats` to `False`. 3. **Input/Output:** - **Input:** A PyTorch model containing `BatchNorm2d` layers, and parameters as specified above. - **Output:** The modified PyTorch model with updated normalization layers as per the specified parameters. 4. **Constraints:** - The function should effectively traverse the entire model and locate all instances of `BatchNorm2d` for modification without affecting other layers or structures within the model. 5. **Performance Requirements:** - The function should handle models up to millions of parameters efficiently, traversing and modifying layers as necessary without redundant computations. Example Usage: ```python import torch import torch.nn as nn # Example model definition class SimpleCNN(nn.Module): def __init__(self): super(SimpleCNN, self).__init__() self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1) self.bn1 = nn.BatchNorm2d(32) self.relu = nn.ReLU() self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1) self.bn2 = nn.BatchNorm2d(64) self.fc1 = nn.Linear(64 * 32 * 32, 10) def forward(self, x): x = self.conv1(x) x = self.bn1(x) x = self.relu(x) x = self.conv2(x) x = self.bn2(x) x = x.view(x.size(0), -1) x = self.fc1(x) return x # Import the modify_norm_layers function # from your_script import modify_norm_layers # Create an instance of the model model = SimpleCNN() # Modify the model to replace BatchNorm2d with GroupNorm modified_model = modify_norm_layers(model, use_group_norm=True, num_groups=8) # Alternatively, patch the BatchNorm2d layers to not use running statistics patched_model = modify_norm_layers(model, use_group_norm=False) ``` Notes: - Your solution will be tested on various models to ensure it correctly modifies the normalization layers as specified. - Make sure to handle edge cases where the number of channels is not perfectly divisible by the number of groups when using GroupNorm. - Do not modify other aspects of the model or its parameters.","solution":"import torch import torch.nn as nn def modify_norm_layers(model, use_group_norm=True, num_groups=32): Modify the normalization layers in a model. Replace BatchNorm2d with GroupNorm or patch BatchNorm2d to not use running statistics. Args: - model (nn.Module): The PyTorch model to modify. - use_group_norm (bool): Whether to replace BatchNorm2d with GroupNorm. Defaults to True. - num_groups (int): The number of groups to use for GroupNorm if use_group_norm is True. Defaults to 32. Returns: - nn.Module: The modified model with updated normalization layers. def replace_bn_with_gn(module): for name, child in module.named_children(): if isinstance(child, nn.BatchNorm2d): # Replace BatchNorm2d with GroupNorm num_channels = child.num_features if num_channels % num_groups != 0: raise ValueError(f\\"Number of channels ({num_channels}) must be divisible by num_groups ({num_groups}) for GroupNorm.\\") setattr(module, name, nn.GroupNorm(num_groups=num_groups, num_channels=num_channels)) else: replace_bn_with_gn(child) def patch_bn_stats(module): for name, child in module.named_children(): if isinstance(child, nn.BatchNorm2d): # Patch BatchNorm2d to not use running statistics child.track_running_stats = False child.running_mean = None child.running_var = None else: patch_bn_stats(child) if use_group_norm: replace_bn_with_gn(model) else: patch_bn_stats(model) return model"},{"question":"You are tasked with visualizing the \'tips\' dataset from seaborn, focusing on total bill and tip amounts, and customizing the labels and titles of the plots. This will assess your understanding of seaborn\'s Plot object and labeling functionalities. **Objective:** 1. Load the \'tips\' dataset using seaborn\'s `load_dataset` function. 2. Create a scatter plot displaying `total_bill` on the x-axis and `tip` on the y-axis, using different colors for the points based on the `time` column. 3. Customize the x-axis label to \\"Total Bill ()\\" and the y-axis label to \\"Tip Amount ()\\". 4. Set the title of the plot to \\"Scatter plot of Total Bill and Tip Amount by Time\\". 5. Facet the plot by the `day` column, and ensure each subplot has a title in the format \\"Day: {day}\\". 6. Save the final plot as a PNG file named \\"tips_scatter_plot.png\\". **Function Signature:** ```python def visualize_tips_dataset(): pass ``` **Constraints:** - Use seaborn version 0.11.0 or later. - Ensure that your code is modular and readable. **Input:** - The function does not take any input. **Output:** - The function does not return any output directly but should save the plot as \\"tips_scatter_plot.png\\". **Example:** After running the function, the file \\"tips_scatter_plot.png\\" should exist in the working directory, displaying the specified customized scatter plot and faceted by day.","solution":"import seaborn as sns import matplotlib.pyplot as plt def visualize_tips_dataset(): # Load the \'tips\' dataset tips = sns.load_dataset(\'tips\') # Create a scatter plot with \'total_bill\' on the x-axis and \'tip\' on the y-axis, and color by \'time\' g = sns.FacetGrid(tips, col=\\"day\\", hue=\\"time\\", height=4, aspect=1) g.map_dataframe(sns.scatterplot, x=\\"total_bill\\", y=\\"tip\\", alpha=0.7) # Customize labels and title g.set_axis_labels(\\"Total Bill ()\\", \\"Tip Amount ()\\") g.set_titles(\\"Day: {col_name}\\") g.add_legend() # Set the main title of the plot plt.subplots_adjust(top=0.85) g.fig.suptitle(\\"Scatter plot of Total Bill and Tip Amount by Time\\", fontsize=16) # Save the plot as a PNG file plt.savefig(\\"tips_scatter_plot.png\\") # Close the plot to avoid displaying it in non-interactive environments plt.close() # Execute the function visualize_tips_dataset()"},{"question":"# Part 1: Concurrent Execution with Threading and Multiprocessing **Objective:** Implement a program demonstrating both thread-based and process-based parallelism to count prime numbers in a given range. Task Write a Python program that does the following: 1. **Define a function `is_prime(n: int) -> bool`** that returns `True` if `n` is a prime number and `False` otherwise. 2. **Define a function `count_primes_in_range(start: int, end: int) -> int`** that counts the number of prime numbers in a given range `[start, end)` and returns the count. 3. **Create two versions of a prime counter using:** - **Threads** (using the `threading` module). - **Processes** (using the `multiprocessing` module). Use both methods to split the range `[start, end)` into equal parts and perform the prime counting in parallel. - **Input:** Two integers, `start` and `end`, and a split factor `split`. - **Output:** Display the number of primes found using both threading and multiprocessing. **Constraints:** - `1 ≤ start < end ≤ 10^6` - `2 ≤ split ≤ 10` - Optimize for performance. - Handle edge cases like range with no primes or invalid input gracefully. Example ```python # Example usage: start, end = 1, 100 split = 4 # Expected prime count is 25 for the range [1, 100) # (range splits: [1, 25), [25, 50), [50, 75), [75, 100)) ``` # Key Functions To Implement: 1. `is_prime(n: int) -> bool` 2. `count_primes_in_range(start: int, end: int) -> int` 3. `count_primes_with_threads(start: int, end: int, split: int) -> int` 4. `count_primes_with_processes(start: int, end: int, split: int) -> int` # Requirements: 1. Use the `threading` module to create threads for prime counting. 2. Use the `multiprocessing` module to create processes for prime counting. 3. Benchmark the time taken for both methods and print results. 4. Ensure that the choice of threading or multiprocessing is optimal for the task.","solution":"import threading import multiprocessing import math def is_prime(n: int) -> bool: Returns True if n is a prime number, otherwise False. if n <= 1: return False if n <= 3: return True if n % 2 == 0 or n % 3 == 0: return False i = 5 while i * i <= n: if n % i == 0 or n % (i + 2) == 0: return False i += 6 return True def count_primes_in_range(start: int, end: int) -> int: Returns the count of prime numbers in the given range [start, end). count = 0 for number in range(start, end): if is_prime(number): count += 1 return count def count_primes_with_threads(start: int, end: int, split: int) -> int: Count primes in the given range using multiple threads. range_size = (end - start) // split threads = [] results = [0] * split def worker(thread_index, s, e): results[thread_index] = count_primes_in_range(s, e) for i in range(split): s = start + i * range_size e = start + (i + 1) * range_size if i != split - 1 else end thread = threading.Thread(target=worker, args=(i, s, e)) threads.append(thread) thread.start() for thread in threads: thread.join() return sum(results) def count_primes_with_processes(start: int, end: int, split: int) -> int: Count primes in the given range using multiple processes. range_size = (end - start) // split with multiprocessing.Pool(processes=split) as pool: ranges = [(start + i * range_size, start + (i + 1) * range_size) for i in range(split)] ranges[-1] = (ranges[-1][0], end) # Ensure last range goes to `end` results = pool.starmap(count_primes_in_range, ranges) return sum(results) # Example usage: if __name__ == \'__main__\': start, end = 1, 100 split = 4 prime_count_threads = count_primes_with_threads(start, end, split) print(f\\"Primes counted with threads: {prime_count_threads}\\") prime_count_processes = count_primes_with_processes(start, end, split) print(f\\"Primes counted with processes: {prime_count_processes}\\")"},{"question":"**Advanced Python Coding Assessment Question** **Objective:** This question aims to assess your understanding of the `email.header` module in Python to create and manipulate internationalized email headers. **Context:** You are tasked with creating a function that processes an input list of email header parts, encodes them as MIME-compliant headers, and checks if the encoded headers match an expected output. **Function Specifications:** - **Function Name:** `process_email_headers` - **Input:** - `header_parts`: A list of tuples where each tuple contains a header string and its corresponding character set (e.g., `[(\'Subject\', \'us-ascii\'), (\'pxf6stal\', \'iso-8859-1\')]`). - `expected_header`: A string representing the expected encoded header value (e.g., `Subject: =?iso-8859-1?q?p=F6stal?=`) - **Output:** - A boolean value. Return `True` if the encoded header matches the expected output, otherwise return `False`. **Instructions:** 1. Create a `Header` instance for each part in `header_parts` using the provided `charset`. 2. Append each header part to the `Header` instance. 3. Encode the header into an RFC-compliant format. 4. Compare the encoded header to the `expected_header`. 5. Return `True` if they match, otherwise return `False`. **Constraints:** - Ensure that each header part is properly appended and encoded. - The maximum line length for encoding headers should be 76 characters unless specified otherwise in the input. **Example:** ```python from email.header import Header def process_email_headers(header_parts, expected_header): # Initialize the Header instance hdr = Header() # Append each part with the specified charset for part, charset in header_parts: hdr.append(part, charset) # Encode the header encoded_header = hdr.encode() # Check if the encoded header matches the expected header return encoded_header == expected_header # Example usage header_parts = [(\'Subject\', \'us-ascii\'), (\'pxf6stal\', \'iso-8859-1\')] expected_header = \'Subject: =?iso-8859-1?q?p=F6stal?=\' print(process_email_headers(header_parts, expected_header)) # Output should be True ``` **Note:** Do not use external libraries other than `email.header` and standard Python libraries.","solution":"from email.header import Header def process_email_headers(header_parts, expected_header): Processes email headers and checks if the encoded result matches the expected output. Parameters: header_parts (list of tuples): List of header parts with corresponding charsets. expected_header (str): The expected encoded header string. Returns: bool: True if the encoded header matches the expected header, False otherwise. # Initialize the Header instance hdr = Header() # Append each part with the specified charset for part, charset in header_parts: hdr.append(part, charset) # Encode the header encoded_header = hdr.encode() # Check if the encoded header matches the expected header return encoded_header == expected_header"},{"question":"# Question You are a data analyst working on visualizing data for a marketing team. They have specifically requested that all visualizations contain color schemes with specific characteristics for brand consistency. You have decided to use the `seaborn` library to create these color palettes. Given the following tasks, write the necessary functions using `seaborn` to achieve the objectives. # Tasks 1. **Create a Light Color Palette**: Write a function `create_light_palette` that generates a light color palette starting from light gray to the specified color. ```python def create_light_palette(color, n_colors=6): Generate a light color palette from light gray to the specified color. Args: - color (str or tuple): The target color as a named color, hex code, or HUSL coordinates. - n_colors (int): Number of colors in the palette (default is 6). Returns: - palette (list): List of colors in the generated palette. pass ``` 2. **Continuous Color Map**: Write a function `create_continuous_cmap` that generates a continuous colormap from light gray to the specified color. ```python def create_continuous_cmap(color): Generate a continuous colormap from light gray to the specified color. Args: - color (str or tuple): The target color as a named color, hex code, or HUSL coordinates. Returns: - cmap (Colormap): Continuous colormap object. pass ``` 3. **Palette with Different Color Inputs**: Write a function `test_palette_inputs` that uses the `create_light_palette` function to generate palettes with different types of color inputs and returns all palettes. ```python def test_palette_inputs(): Generate light color palettes using different color input types and return them. Returns: - palettes (dict): Dictionary with palette names as keys and generated palettes as values. pass ``` # Examples ```python # Example usage palette1 = create_light_palette(\\"seagreen\\", 8) palette2 = create_light_palette(\\"#79C\\", 10) palette3 = create_light_palette((20, 60, 50), input=\\"husl\\") cmap = create_continuous_cmap(\\"#a275ac\\") palettes = test_palette_inputs() for name, palette in palettes.items(): print(f\\"{name}: {palette}\\") ``` # Constraints and Requirements - All functions should handle possible invalid inputs gracefully. - Use `seaborn` library functions and follow best practices for creating and managing color palettes. - Provide docstrings for each function describing the inputs, outputs, and their purpose. Implement these three functions to showcase your understanding of the seaborn library and its capabilities for creating customized color palettes.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_light_palette(color, n_colors=6): Generate a light color palette from light gray to the specified color. Args: - color (str or tuple): The target color as a named color, hex code, or HUSL coordinates. - n_colors (int): Number of colors in the palette (default is 6). Returns: - palette (list): List of colors in the generated palette. try: palette = sns.light_palette(color, n_colors=n_colors, input=\\"husl\\" if isinstance(color, tuple) else None) return palette except ValueError as e: print(f\\"Error in create_light_palette: {e}\\") return [] def create_continuous_cmap(color): Generate a continuous colormap from light gray to the specified color. Args: - color (str or tuple): The target color as a named color, hex code, or HUSL coordinates. Returns: - cmap (Colormap): Continuous colormap object. try: cmap = sns.light_palette(color, as_cmap=True, input=\\"husl\\" if isinstance(color, tuple) else None) return cmap except ValueError as e: print(f\\"Error in create_continuous_cmap: {e}\\") return None def test_palette_inputs(): Generate light color palettes using different color input types and return them. Returns: - palettes (dict): Dictionary with palette names as keys and generated palettes as values. palettes = {} try: palettes[\\"named_color\\"] = create_light_palette(\\"seagreen\\", 8) palettes[\\"hex_color\\"] = create_light_palette(\\"#79C\\", 10) palettes[\\"husl_color\\"] = create_light_palette((20, 60, 50), 6) except ValueError as e: print(f\\"Error in test_palette_inputs: {e}\\") return palettes"},{"question":"# Advanced Python: Transition from `imp` to `importlib` **Objective:** Write a Python function that replicates the behavior of a deprecating function from the `imp` module using the modern `importlib` approach. This will demonstrate your understanding of how module importation works in Python and the recommended modern practices. **Task:** Write a function `import_module(module_name)` that attempts to import a module by name and returns the module object using `importlib`. If the module is not found, it should return `None`. Your solution should not use any deprecated methods from the `imp` module. **Input:** - `module_name` (string): The name of the module to import. **Output:** - The module object if the module is found. - `None` if the module is not found. **Constraints:** 1. You must use `importlib` for importing the module. 2. Do not use any functions or constants from the deprecated `imp` module. 3. Handle possible exceptions that might occur during the import process. **Example Usage:** ```python import_module(\'math\') # Should return the math module object. import_module(\'nonexistent_module\') # Should return None. ``` **Starter Code:** ```python import importlib def import_module(module_name): try: return importlib.import_module(module_name) except ModuleNotFoundError: return None # Example usage if __name__ == \\"__main__\\": module = import_module(\'math\') print(module) # <module \'math\' (built-in)> module = import_module(\'nonexistent_module\') print(module) # None ``` Your task is to complete the `import_module` function following the constraints and requirements provided.","solution":"import importlib def import_module(module_name): try: return importlib.import_module(module_name) except ModuleNotFoundError: return None"},{"question":"**Objective:** Implement a Python script that processes a directory of text files, identifies files matching a specific pattern, compresses those files, and calculates basic statistics about their contents. **Task:** 1. Write a Python function `process_directory(directory, pattern)` that: - Takes in a directory path (`directory`) and a filename pattern (`pattern`). - Uses the `glob` module to find all files in the directory that match the pattern. - For each matched file, reads its content and: - Calculates the word count. - Calculates the frequency of each unique word. - Compresses the contents of each file using the `zlib` module. - Writes the compressed data to a new file with a `.zlib` extension in the same directory. - Returns a dictionary where the keys are the filenames and the values are dictionaries containing: - The original word count. - The frequency of each unique word. **Input:** - `directory` (str): The path to the directory containing the text files. - `pattern` (str): The file name pattern to match (e.g., `\'*.txt\'`). **Output:** - (dict): A dictionary where each key is a file name and the corresponding value is another dictionary with `word_count` and `word_frequency`. **Example:** ```python # Example usage result = process_directory(\'/path/to/directory\', \'*.txt\') print(result) ``` **Constraints:** - Use the `os`, `glob`, and `zlib` modules to perform the required tasks. - Ensure your code handles errors gracefully, such as file not found or read errors. - Ensure your code is optimized for performance, especially when dealing with large files. **Notes:** - Words should be considered case-insensitive for the word count and frequency calculations (e.g., \\"Python\\" and \\"python\\" should be counted as the same word). - Punctuation should be ignored when calculating word frequencies. **Performance Considerations:** - The solution should be efficient in terms of both time and memory usage. Avoid loading the entire directory or overly large files into memory at once. **Bonus Challenge:** - Add functionality to decompress a compressed file back to its original state.","solution":"import os import glob import zlib import re from collections import defaultdict, Counter def process_directory(directory, pattern): Process the directory to match files, calculate word count and frequency, compress files. Args: - directory (str): Path to the directory containing the text files. - pattern (str): The file name pattern to match (e.g., \'*.txt\'). Returns: - dict: A dictionary with filenames as keys and a dictionary of word count and frequency as values. result = {} try: file_paths = glob.glob(os.path.join(directory, pattern)) for file_path in file_paths: with open(file_path, \'r\', encoding=\'utf-8\') as file: content = file.read() # Normalize content to lower case and remove punctuation words = re.findall(r\'bw+b\', content.lower()) word_count = len(words) word_frequency = dict(Counter(words)) result[os.path.basename(file_path)] = { \'word_count\': word_count, \'word_frequency\': word_frequency } # Compress the content compressed_data = zlib.compress(content.encode(\'utf-8\')) compressed_file_path = file_path + \'.zlib\' with open(compressed_file_path, \'wb\') as compressed_file: compressed_file.write(compressed_data) except Exception as e: print(f\\"An error occurred: {e}\\") return result def decompress_file(compressed_file_path): Decompress a compressed file back to its original state. Args: - compressed_file_path (str): The path to the compressed file with a `.zlib` extension. Returns: - str: The original content of the file. try: with open(compressed_file_path, \'rb\') as compressed_file: compressed_data = compressed_file.read() original_content = zlib.decompress(compressed_data).decode(\'utf-8\') return original_content except Exception as e: print(f\\"An error occurred: {e}\\") return None"},{"question":"# **Coding Assessment Question: TorchScript Model Optimization** In this assessment, you will design and implement a simple neural network in PyTorch and script it using TorchScript. The goal is to demonstrate your understanding of TorchScript, including how to use type annotations, scripting, and invoking scripted models. **Objective:** 1. Create a simple feedforward neural network model in PyTorch. 2. Script the model using TorchScript. 3. Invoke the scripted model and ensure it produces the expected output. # **Requirements:** 1. Implement a `FeedForwardNN` class inheriting from `torch.nn.Module`. It should contain: - An initialization method (`__init__()`) that sets up the network layers. - A forward method (`forward()`) that defines the forward pass of the network. 2. Create an instance of `FeedForwardNN` class. 3. Script the `FeedForwardNN` model using `torch.jit.script`. 4. Define a method to run a few test inputs through the scripted model to verify its functionality. # **Specifications:** - **Class `FeedForwardNN`**: - **Initialization Method (`__init__()`)**: - Should have two `Linear` layers. The first layer should transform an input of size `input_size` to `hidden_size`. The second layer should transform `hidden_size` to `output_size`. - Use `ReLU` activation between the layers. - **Forward Method (`forward()`)**: - Should take a tensor `x` as input. - Apply the first `Linear` layer followed by `ReLU`. - Apply the second `Linear` layer. - Return the final output tensor. - **Example Test Inputs**: - Input size: 4 - Hidden size: 10 - Output size: 2 - Use a randomly generated tensor of shape `(5, 4)` as the test input. # **Input:** - The FeedForwardNN class should be defined with `input_size: int`, `hidden_size: int`, and `output_size: int`. - A randomly generated tensor of shape `(5, 4)`. # **Output:** - The output should be a tensor of shape `(5, 2)` which is the model\'s output after passing the input through the layer. # **Constraints and Limitations:** - Ensure your implementation uses TorchScript correctly. - Use the necessary type annotations for the methods and attributes. - You may use any activation function between the layers, but `ReLU` is recommended for simplicity. # **Performance Requirements:** - The scripted model should produce the output quickly and without errors for the test input provided. **Starter Code:** ```python import torch import torch.nn as nn from typing import Optional # Step 1: Implement the FeedForwardNN class class FeedForwardNN(nn.Module): def __init__(self, input_size: int, hidden_size: int, output_size: int): super(FeedForwardNN, self).__init__() self.layer1 = nn.Linear(input_size, hidden_size) self.relu = nn.ReLU() self.layer2 = nn.Linear(hidden_size, output_size) def forward(self, x: torch.Tensor) -> torch.Tensor: out = self.layer1(x) out = self.relu(out) out = self.layer2(out) return out # Step 2: Create an instance of the model input_size = 4 hidden_size = 10 output_size = 2 model = FeedForwardNN(input_size, hidden_size, output_size) # Step 3: Script the model using torch.jit.script scripted_model = torch.jit.script(model) # Step 4: Test the scripted model with a random tensor test_input = torch.randn(5, input_size) output = scripted_model(test_input) print(output) ``` # Expected Output: The output should be a tensor of shape `(5, 2)` after passing through the scripted model.","solution":"import torch import torch.nn as nn # Step 1: Implement the FeedForwardNN class class FeedForwardNN(nn.Module): def __init__(self, input_size: int, hidden_size: int, output_size: int): super(FeedForwardNN, self).__init__() self.layer1 = nn.Linear(input_size, hidden_size) self.relu = nn.ReLU() self.layer2 = nn.Linear(hidden_size, output_size) def forward(self, x: torch.Tensor) -> torch.Tensor: out = self.layer1(x) out = self.relu(out) out = self.layer2(out) return out # Step 2: Create an instance of the model input_size = 4 hidden_size = 10 output_size = 2 model = FeedForwardNN(input_size, hidden_size, output_size) # Step 3: Script the model using torch.jit.script scripted_model = torch.jit.script(model) # Step 4: Test the scripted model with a random tensor def test_scripted_model(): test_input = torch.randn(5, input_size) output = scripted_model(test_input) print(output) # To verify the structure and correctness of the output return output if __name__ == \\"__main__\\": test_scripted_model()"},{"question":"Objective Implement a custom asynchronous server and client using Python\'s `asyncio` library. The server should handle multiple simultaneous connections and echo received messages back to the clients, but with a twist—each message should be reversed before sending back. Problem Statement Design and implement an `EchoReversalServerProtocol` and a `ReversalClientProtocol` such that: 1. The server listens for incoming TCP connections on a specified host and port. 2. When the server receives a message from a client, it reverses the message and sends it back to the client. 3. The client connects to the server, sends a message, receives the reversed message, and then prints both the original and reversed messages. Requirements - Implement the server using the `asyncio.Protocol` class. - Implement the client using the `asyncio.Protocol` class. - Provide an asynchronous function `start_server()` to start the server. - Provide an asynchronous function `start_client(message)` to start a client that sends the specified `message` to the server and prints the results. Input - For the server: Host and port to listen on. - For the client: A message string to send to the server. Output - For the client: Print the original message and the reversed message received from the server. Constraints - The server should be able to handle multiple clients simultaneously. - Use only the low-level APIs as defined in the `asyncio` library documentation. Example ```python # Example usage: # 1. Start the server # asyncio.run(start_server(\'127.0.0.1\', 8888)) # 2. Start the client with a message # asyncio.run(start_client(\'Hello World!\', \'127.0.0.1\', 8888)) # Output from client: # Original message: Hello World! # Reversed message: !dlroW olleH ``` Implementation 1. **EchoReversalServerProtocol**: - `connection_made`: Store the transport. - `data_received`: Reverse the received data and send it back through the transport. 2. **ReversalClientProtocol**: - `connection_made`: Send the provided message. - `data_received`: Store the reversed message received from the server. - `connection_lost`: Print the original and reversed messages before closing the event loop. 3. **Server Function**: ```python async def start_server(host, port): loop = asyncio.get_running_loop() server = await loop.create_server( lambda: EchoReversalServerProtocol(), host, port) async with server: await server.serve_forever() ``` 4. **Client Function**: ```python async def start_client(message, host, port): loop = asyncio.get_running_loop() on_con_lost = loop.create_future() transport, protocol = await loop.create_connection( lambda: ReversalClientProtocol(message, on_con_lost), host, port) try: await on_con_lost finally: transport.close() ``` Implement the `EchoReversalServerProtocol` and `ReversalClientProtocol` classes along with the `start_server` and `start_client` functions according to the specifications above.","solution":"import asyncio class EchoReversalServerProtocol(asyncio.Protocol): def connection_made(self, transport): self.transport = transport def data_received(self, data): message = data.decode() reversed_message = message[::-1] self.transport.write(reversed_message.encode()) class ReversalClientProtocol(asyncio.Protocol): def __init__(self, message, on_con_lost): self.message = message self.on_con_lost = on_con_lost def connection_made(self, transport): self.transport = transport self.transport.write(self.message.encode()) def data_received(self, data): reversed_message = data.decode() print(f\\"Original message: {self.message}\\") print(f\\"Reversed message: {reversed_message}\\") self.on_con_lost.set_result(True) def connection_lost(self, exc): self.on_con_lost.set_result(True) async def start_server(host, port): loop = asyncio.get_running_loop() server = await loop.create_server( lambda: EchoReversalServerProtocol(), host, port) async with server: await server.serve_forever() async def start_client(message, host, port): loop = asyncio.get_running_loop() on_con_lost = loop.create_future() transport, protocol = await loop.create_connection( lambda: ReversalClientProtocol(message, on_con_lost), host, port) try: await on_con_lost finally: transport.close()"},{"question":"Objective: To assess your understanding of advanced concepts of the Python/C API, including reference management, useful macros, and error handling. Task: Write a C function named `increment_list_items` that takes a Python list (`PyObject*`) and increments each integer item in the list by `1`. You will need to manage reference counts properly, handle possible errors, and make use of the provided useful macros. You are required to: 1. Ensure the function performs correctly even if non-integer values are present in the list. 2. Handle errors properly, making sure to maintain reference count consistency. 3. Return `0` upon success and `-1` upon failure. Signature: ```c int increment_list_items(PyObject* list); ``` Constraints: - The input list may contain non-integer types. - The function must not modify any non-integer items. - Properly handle Python exceptions and reference counts to avoid memory leaks or segmentation faults. Example: If the input list contains `[1, \\"two\\", 3]`, after calling the function, the list should be `[2, \\"two\\", 4]`. Hints: - Use `PyList_Size` to get the number of items in the list. - Use `PyList_GetItem` to access an item from the list. - Use `PyLong_Check` to verify if an item is an integer. - Use `PyNumber_Add` to add to the integer item. - Use `PyList_SetItem` to set the incremented value back into the list. - Use error checking macros like `PyErr_Occurred()` to manage errors. Requirements: - Include necessary headers: `<Python.h>` - Follow proper reference counting rules. - Ensure correct exception handling. Example Implementation (Skeleton): ```c #include <Python.h> int increment_list_items(PyObject* list) { Py_ssize_t i, n; PyObject *item = NULL, *one = NULL, *incremented_item = NULL; int result = -1; if (!PyList_Check(list)) { PyErr_SetString(PyExc_TypeError, \\"Argument must be a list\\"); return -1; } n = PyList_Size(list); if (n < 0) { return -1; } one = PyLong_FromLong(1); if (!one) { goto cleanup; } for (i = 0; i < n; ++i) { item = PyList_GetItem(list, i); // Borrowed reference if (PyLong_Check(item)) { incremented_item = PyNumber_Add(item, one); if (!incremented_item) { goto cleanup; } if (PyList_SetItem(list, i, incremented_item) < 0) { Py_DECREF(incremented_item); goto cleanup; } // PyList_SetItem steals a reference to incremented_item incremented_item = NULL; } } result = 0; cleanup: Py_XDECREF(item); Py_XDECREF(one); Py_XDECREF(incremented_item); return result; } ```","solution":"def increment_list_items(pylist): This function takes a Python list, iterates over its items, and increments each integer item by 1. Non-integer items are left unchanged. :param list pylist: List with potentially mixed data types. :return int: 0 if successful, -1 if an error occurs. if not isinstance(pylist, list): raise TypeError(\\"Argument must be a list\\") try: for i in range(len(pylist)): if isinstance(pylist[i], int): pylist[i] += 1 return 0 except TypeError: return -1"},{"question":"Objective: Implement a Python class structure utilizing multiple inheritance, class/instance variables, methods, and generators for data processing in an e-commerce application context. Problem Statement: You are tasked with creating a class structure to manage a simple e-commerce system. This system should be able to manage products, track orders, and compute order summaries. Each order contains multiple products, and you need to implement a generator to iterate through orders. Details: 1. **Class Definitions:** - Define a `Product` class: ```python class Product: def __init__(self, name: str, price: float): Parameters: name (str): The name of the product price (float): The price of the product pass ``` - Define an `Order` class: ```python class Order: def __init__(self, order_id: int): Parameters: order_id (int): The unique ID of the order pass def add_product(self, product: Product, quantity: int): Adds a product with specified quantity to the order. Parameters: product (Product): The product to be added quantity (int): The quantity of the product pass def total_cost(self) -> float: Calculate the total cost of the order. Returns: float: The total cost of the order pass ``` - Define a `Customer` class: ```python class Customer: def __init__(self, customer_id: int, name: str): Parameters: customer_id (int): The unique ID of the customer name (str): The name of the customer pass def place_order(self, order: Order): Place an order for the customer. Parameters: order (Order): The order to be placed by the customer pass def all_orders(self): Generator to iterate through all of the customer\'s orders. pass ``` 2. **Attributes and Methods:** - `Product` should have attributes for `name` and `price`. - `Order` should have a method `add_product` to add a product and a method `total_cost` to compute the total cost of the order. - `Customer` should be able to `place_order` and have a generator method `all_orders` that yields each order. 3. **Constraints:** - The `total_cost` method should accurately calculate the aggregated price considering product quantities. - You must use a `generator` inside the `all_orders` method to yield order details. - Manage the attributes to ensure proper encapsulation. 4. **Expected Input and Output:** ```python # Example Usage: p1 = Product(\\"Laptop\\", 1200) p2 = Product(\\"Mouse\\", 50) order1 = Order(1) order1.add_product(p1, 1) order1.add_product(p2, 2) customer = Customer(101, \\"John Doe\\") customer.place_order(order1) for order in customer.all_orders(): print(f\\"Order ID: {order.order_id}, Total Cost: {order.total_cost()}\\") ``` **Expected Output:** ``` Order ID: 1, Total Cost: 1300.0 ``` Notes: - Utilize decorators as needed to encapsulate functionality. - Ensure proper error handling where applicable. - Write clear and concise docstrings for each method and class. - Test your class structure with different product combinations and orders.","solution":"class Product: def __init__(self, name: str, price: float): Parameters: name (str): The name of the product price (float): The price of the product self.name = name self.price = price class Order: def __init__(self, order_id: int): Parameters: order_id (int): The unique ID of the order self.order_id = order_id self.products = [] def add_product(self, product: Product, quantity: int): Adds a product with specified quantity to the order. Parameters: product (Product): The product to be added quantity (int): The quantity of the product self.products.append((product, quantity)) def total_cost(self) -> float: Calculate the total cost of the order. Returns: float: The total cost of the order return sum(product.price * quantity for product, quantity in self.products) class Customer: def __init__(self, customer_id: int, name: str): Parameters: customer_id (int): The unique ID of the customer name (str): The name of the customer self.customer_id = customer_id self.name = name self.orders = [] def place_order(self, order: Order): Place an order for the customer. Parameters: order (Order): The order to be placed by the customer self.orders.append(order) def all_orders(self): Generator to iterate through all of the customer\'s orders. for order in self.orders: yield order"},{"question":"# Question: You are provided with a task to load a specific dataset using scikit-learn\'s dataset utilities and perform a basic preprocessing step before applying a machine learning model. Follow the steps below to complete the task. 1. Load the \\"Iris\\" dataset using scikit-learn\'s dataset loader. 2. Split the dataset into a training set and a test set. Use 70% of the data for training and 30% for testing. 3. Normalize the feature values to have zero mean and unit variance using `StandardScaler`. 4. Train a k-Nearest Neighbors (k-NN) classifier with k=3 on the training data. 5. Evaluate the model on the test data and return the accuracy score. # Function Signature: ```python def evaluate_knn_on_iris() -> float: pass ``` # Input: - There are no inputs to this function. # Output: - The function should return a float representing the accuracy score of the k-NN classifier on the test data. # Constraints: - You must use scikit-learn\'s `load_iris` function to load the Iris dataset. - Use `train_test_split` from scikit-learn to split the dataset. - Use `StandardScaler` from scikit-learn for normalization. - Use `KNeighborsClassifier` from scikit-learn to train the model. - The evaluation metric is accuracy. # Example: ```python accuracy = evaluate_knn_on_iris() print(accuracy) # Example output: 0.95 ``` # Note: - Ensure that your implementation handles the dataset splitting and preprocessing as specified. - Use default settings for any parameters not explicitly mentioned.","solution":"from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.neighbors import KNeighborsClassifier from sklearn.metrics import accuracy_score def evaluate_knn_on_iris() -> float: # Load the Iris dataset iris = load_iris() X, y = iris.data, iris.target # Split the dataset into a training set and a test set (70% train, 30% test) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Normalize the feature values to have zero mean and unit variance scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) # Train a k-Nearest Neighbors (k-NN) classifier with k=3 knn = KNeighborsClassifier(n_neighbors=3) knn.fit(X_train, y_train) # Evaluate the model on the test data y_pred = knn.predict(X_test) accuracy = accuracy_score(y_test, y_pred) return accuracy"},{"question":"Advanced Slice Handling in Python Objective Create functions to handle custom slicing operations. Implement a function that interprets slices and retrieves equivalent data from a sequence. Problem Statement You are tasked with implementing an advanced slicing function that extends the usual slicing mechanism in Python. This function will allow you to slice a sequence according to given start, stop, and step parameters, even when these parameters exceed the length of the sequence or are invalid. Function Signature: ```python def advanced_slice(sequence, start, stop, step): This function returns a sliced segment from the provided sequence given start, stop, and step parameters. Parameters: - sequence: List[int] | List[str] : The sequence to be sliced. - start: int : The starting index of the slice - stop: int : The stopping index of the slice - step: int : The step size of the slice Returns: - List[int] | List[str]: A new list containing the sliced elements. ``` Input and Output Format: - **Input:** - `sequence` (List[int] or List[str]): A list of integers or list of strings. - `start` (int): The starting index of the slice. - `stop` (int): The stopping index of the slice. - `step` (int): The step size between each index in the slice. - **Output:** - List[int] or List[str] containing the elements specified by the slice. Example: ```python # Example 1 sequence = [1, 2, 3, 4, 5] start = 1 stop = 4 step = 1 print(advanced_slice(sequence, start, stop, step)) # Output: [2, 3, 4] # Example 2 sequence = [1, 2, 3, 4, 5] start = 0 stop = 5 step = 2 print(advanced_slice(sequence, start, stop, step)) # Output: [1, 3, 5] # Example 3 sequence = \\"abcdefgh\\" start = 2 stop = 8 step = 2 print(advanced_slice(sequence, start, stop, step)) # Output: \'cceg\' ``` Constraints and Limitations: 1. Handle negative indices and step sizes appropriately. 2. If `start`, `stop`, or `step` values exceed the sequence bounds, clip them within valid ranges. 3. Ensure the function handles possible errors such as invalid types and provide appropriate error messages. Additional Requirements: - The function should be optimized and efficient. - You are not allowed to use Python’s built-in slicing directly (i.e., you must manually handle the start, stop, and step values). Implement the function `advanced_slice` as specified above. Ensure it passes the example cases and is efficient for large sequences.","solution":"def advanced_slice(sequence, start, stop, step): This function returns a sliced segment from the provided sequence given start, stop, and step parameters. Parameters: - sequence: list[int] | list[str] : The sequence to be sliced. - start: int : The starting index of the slice - stop: int : The stopping index of the slice - step: int : The step size of the slice Returns: - list[int] | list[str]: A new list containing the sliced elements. if not isinstance(sequence, (list, str)): raise ValueError(\\"The sequence must be a list of integers or strings\\") if not isinstance(start, int) or not isinstance(stop, int) or not isinstance(step, int): raise ValueError(\\"start, stop, and step must be integers\\") result = [] seq_len = len(sequence) # Handle negative indices if start < 0: start += seq_len if stop < 0: stop += seq_len # Clip start, stop within bounds start = max(0, min(seq_len, start)) stop = max(0, min(seq_len, stop)) if step == 0: raise ValueError(\\"step cannot be zero\\") if step > 0: index = start while index < stop: result.append(sequence[index]) index += step else: index = start while index > stop: result.append(sequence[index]) index += step return result"},{"question":"# Advanced Python Coding Assessment Objective: Your task is to implement a function that reads from a buffer and writes to a new buffer using the concepts from the \\"old buffer protocol\\". This will demonstrate your understanding of memory buffer management in Python, even though these interfaces are deprecated. Problem Statement: Implement a function `copy_buffer(input_obj: bytes) -> bytes` that performs the following steps: 1. Use the old buffer protocol functions to obtain a read-only memory view of the input object (`input_obj`). 2. Create a writable buffer of the same size. 3. Copy the contents from the read-only buffer to the writable buffer. 4. Return the new buffer as a bytes object. Function Signature: ```python def copy_buffer(input_obj: bytes) -> bytes: ``` Input: - `input_obj`: A bytes object representing the input buffer. Output: - Returns a new bytes object containing the copied data from the input buffer. Constraints: - You must use the deprecated buffer protocol functions (`PyObject_AsReadBuffer`, `PyObject_AsWriteBuffer`). - Ensure that the input and output buffers are correctly managed and there are no memory leaks. Example: ```python input_data = b\\"Hello, World!\\" output_data = copy_buffer(input_data) print(output_data) # Should print: b\\"Hello, World!\\" assert input_data == output_data ``` Requirements: - Demonstrate the use of `PyObject_AsReadBuffer` to read from the input buffer. - Demonstrate the use of `PyObject_AsWriteBuffer` to write to the new buffer. - Handle any potential errors gracefully by raising appropriate exceptions. Performance: - The function should run efficiently with a linear time complexity relative to the size of the input buffer. You may assume that the input will always be a valid `bytes` object and you do not need to handle any other data types or invalid inputs for this assessment.","solution":"def copy_buffer(input_obj: bytes) -> bytes: import ctypes # Ensure input_obj is a bytes-like object if not isinstance(input_obj, bytes): raise TypeError(\\"input_obj must be a bytes object\\") # Obtain the read-only buffer from input_obj using ctypes buf_len = len(input_obj) read_buf = ctypes.create_string_buffer(input_obj, buf_len) # Create a new writable buffer of the same length write_buf = ctypes.create_string_buffer(buf_len) # Copy the contents from the read-only buffer to the writable buffer ctypes.memmove(write_buf, read_buf, buf_len) # Return the writable buffer as a bytes object return bytes(write_buf.raw)"},{"question":"# Asynchronous Task Organizer You are tasked to create an \\"Asynchronous Task Organizer\\" using Python\'s asyncio library. This organizer should manage a set of tasks that can run concurrently. Some tasks may depend on the completion of other tasks. Your solution should demonstrate a comprehensive understanding of asyncio\'s coroutines, tasks, and synchronization primitives. Function Signature ```python import asyncio from typing import List, Tuple async def task_organizer(tasks: List[Tuple[int, int, str]]) -> None: Manages and runs tasks asynchronously based on their dependencies. Args: tasks (List[Tuple[int, int, str]]): A list of tasks where each task is represented as a tuple (task_id, dependency_task_id, message). - task_id: an integer representing the unique ID of the task. - dependency_task_id: an integer ID of the task that must complete before the current task starts. - message: a string message that a task will \\"print\\" when it is executed. Returns: None ``` Input - `tasks`: A list of tuples where each tuple represents a task. Each tuple consists of three elements: - `task_id` (int): A unique identifier for the task. - `dependency_task_id` (int): The ID of a task that must be completed before this task can start. If a task has no dependencies, this value will be `-1`. - `message` (str): A message that should be printed when the task starts. Output - The function does not return anything. It should print the messages of the tasks to the console in the correct order considering their dependencies. Constraints - No two tasks will have the same `task_id`. - The `dependency_task_id` will either be `-1` (indicating no dependency) or will reference a valid `task_id`. - Tasks should be executed as soon as their dependencies are resolved. - Utilize asyncio features such as coroutines, tasks, and synchronization primitives effectively. Example ```python import asyncio tasks = [ (1, -1, \\"Task 1 start\\"), # Task 1 has no dependencies (2, 1, \\"Task 2 start\\"), # Task 2 depends on Task 1 (3, 1, \\"Task 3 start\\"), # Task 3 depends on Task 1 (4, 2, \\"Task 4 start\\"), # Task 4 depends on Task 2 (5, 3, \\"Task 5 start\\"), # Task 5 depends on Task 3 ] # Expected Output: # Task 1 start # Task 2 start # Task 3 start # Task 4 start # Task 5 start await task_organizer(tasks) ``` **Hint:** Use `asyncio.create_task` to initiate tasks, and `await` to manage dependencies. Consider using synchronization primitives like `asyncio.Event` to signal task completion.","solution":"import asyncio from typing import List, Tuple async def task_organizer(tasks: List[Tuple[int, int, str]]) -> None: task_events = {} for task_id, dep_id, _ in tasks: task_events[task_id] = asyncio.Event() if dep_id != -1: task_events[dep_id] = asyncio.Event() # Ensure dependency events exist async def run_task(task_id: int, dep_id: int, message: str): if dep_id != -1: await task_events[dep_id].wait() # Wait for the dependency task to complete print(message) task_events[task_id].set() # Mark this task as complete task_coroutines = [run_task(task_id, dep_id, message) for task_id, dep_id, message in tasks] await asyncio.gather(*task_coroutines) # Example usage for debugging (remove before unit testing) # tasks = [ # (1, -1, \\"Task 1 start\\"), # (2, 1, \\"Task 2 start\\"), # (3, 1, \\"Task 3 start\\"), # (4, 2, \\"Task 4 start\\"), # (5, 3, \\"Task 5 start\\"), # ] # asyncio.run(task_organizer(tasks))"},{"question":"# Coding Assessment: Event Scheduler with Priority Management **Objective:** Implement and manage scheduled events using Python\'s `sched` module, demonstrating understanding of the `scheduler` class\'s methods and timing mechanisms. **Problem Statement:** You are tasked with creating a function `schedule_tasks` that schedules a series of tasks to occur at given times with specified priorities. You will then run the scheduler and return a list of events as they occurred, each with the exact time of execution. **Function Signature:** ```python def schedule_tasks(tasks: list[list]) -> list[tuple]: pass ``` **Input:** - `tasks`: A list of tasks to be scheduled. Each task is a list containing: - `time`: A float representing the absolute time (in seconds since some epoch) or delay (in seconds from current time) when the task should run. - `priority`: An integer representing the task\'s priority (lower number means higher priority). - `action`: The task\'s action, a callable that takes no arguments and returns a string describing the task. **Output:** - A list of tuples, each representing an executed task in the order they were executed. Each tuple contains: - The execution time of the task as a float. - The string returned by the task\'s action. **Constraints:** 1. No event should be missed or executed more than once. 2. Scheduled tasks should be executed in correct order respecting both time and priority. 3. Handle exceptions raised by actions or delays gracefully and ensure the scheduler continues to operate correctly. **Example:** ```python import time def task1(): return \\"Task 1 executed\\" def task2(): return \\"Task 2 executed\\" def task3(): return \\"Task 3 executed\\" tasks = [ [time.time() + 5, 1, task1], [time.time() + 10, 2, task2], [time.time() + 5, 0, task3], ] result = schedule_tasks(tasks) # The result should be a list of tuples with the execution time and task results in correct order: # [(exec_time_t3, \\"Task 3 executed\\"), (exec_time_t1, \\"Task 1 executed\\"), (exec_time_t2, \\"Task 2 executed\\")] ``` **Note:** - You can use the `sched.scheduler` class and its methods to schedule and manage the tasks. - Ensure to account for the correct ordering of tasks with the same scheduled time but different priorities. - Make use of `time.time()` for current time and `time.sleep()` for delays if needed.","solution":"import time import sched def schedule_tasks(tasks): sch = sched.scheduler(time.time, time.sleep) results = [] def action_wrapper(scheduled_time, action): result = action() results.append((scheduled_time, result)) for task in tasks: scheduled_time, priority, action = task sch.enterabs(scheduled_time, priority, action_wrapper, (scheduled_time, action)) sch.run() return results"},{"question":"# Coding Assessment: Implement a Classifier using Stochastic Gradient Descent Objective: To test your ability to apply `SGDClassifier` from `sklearn.linear_model` to a real-world dataset, ensuring you understand how to configure different hyperparameters, handle data preprocessing, and evaluate model performance effectively. Problem Statement: You are provided with a dataset containing features of products and their respective categories. Your task is to implement a function to train a multi-class classifier using `SGDClassifier` from `sklearn.linear_model` and evaluate its performance. Detailed Instructions: 1. **Data Preprocessing:** - Load the dataset (the dataset will be provided in a CSV file format). - Standardize the feature data using `StandardScaler` from `sklearn.preprocessing`. 2. **Model Training and Configuration:** - Initialize the `SGDClassifier` with the following parameters: - `loss=\\"log_loss\\"` - `penalty=\\"elasticnet\\"` - `alpha=0.0001` - `max_iter=1000` - `tol=1e-3` - `l1_ratio=0.15` - `random_state=42` 3. **Cross-Validation:** - Perform 5-fold cross-validation to evaluate the model’s performance. Use `cross_val_score` from `sklearn.model_selection`. - Calculate the average accuracy and standard deviation of the model across the 5 folds. 4. **Prediction:** - Use the trained model to predict the categories for a given test set and output the predictions. Here\'s the function signature: ```python import pandas as pd from sklearn.linear_model import SGDClassifier from sklearn.preprocessing import StandardScaler from sklearn.model_selection import cross_val_score def train_and_evaluate_sgd_classifier(train_file: str, test_file: str) -> pd.Series: Train a multi-class classifier using SGDClassifier and predict the categories for the test set. Parameters: - train_file (str): The path to the CSV file containing the training data. - test_file (str): The path to the CSV file containing the test data. Returns: - pd.Series: The predicted categories for the test set. pass # Your implementation goes here. ``` Constraints: - You must use `StandardScaler` to standardize the feature data. - Use `SGDClassifier` with specified hyperparameters. - Perform 5-fold cross-validation to evaluate the model. - Ensure reproducibility by setting the random state to 42. Input Format: 1. `train_file`: A CSV file path containing training data with the last column as the category (target variable). 2. `test_file`: A CSV file path containing test data with the same features as the training data but without the target variable. Output Format: - Return the predicted categories as a `pandas.Series`. Example: Given the following training data: ``` feature1,feature2,feature3,category 0.2,0.8,1.2,ClassA 0.6,0.4,2.1,ClassB ... ``` And the test data: ``` feature1,feature2,feature3 0.3,0.7,1.0 0.5,0.5,2.0 ... ``` You would return a pandas Series indicating which class each test instance belongs to. Note: - You do not need to handle any edge cases related to file reading errors, assume the input files are correctly formatted. - Make sure to include additional helper functions if necessary. - Follow good coding practices and write clean, understandable, and well-documented code.","solution":"import pandas as pd from sklearn.linear_model import SGDClassifier from sklearn.preprocessing import StandardScaler from sklearn.model_selection import cross_val_score def train_and_evaluate_sgd_classifier(train_file: str, test_file: str) -> pd.Series: # Load the training and test data train_data = pd.read_csv(train_file) test_data = pd.read_csv(test_file) # Separate features and target from training data X_train = train_data.iloc[:, :-1] y_train = train_data.iloc[:, -1] # Standardize the feature data scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(test_data) # Initialize the SGDClassifier with the given parameters sgd_classifier = SGDClassifier(loss=\\"log_loss\\", penalty=\\"elasticnet\\", alpha=0.0001, max_iter=1000, tol=1e-3, l1_ratio=0.15, random_state=42) # Perform 5-fold cross-validation cv_scores = cross_val_score(sgd_classifier, X_train_scaled, y_train, cv=5) avg_accuracy = cv_scores.mean() std_accuracy = cv_scores.std() print(f\\"Average Accuracy: {avg_accuracy}\\") print(f\\"Standard Deviation of Accuracy: {std_accuracy}\\") # Fit the model on the entire training data sgd_classifier.fit(X_train_scaled, y_train) # Make predictions on the test data y_pred = sgd_classifier.predict(X_test_scaled) return pd.Series(y_pred)"},{"question":"Overview: You are to implement a function that processes a list of user inputs representing numbers. Your function should handle various edge cases and exceptions, specifically standard Python exceptions and user-defined exceptions. Function Requirements: 1. Implement a function `process_numbers(input_list)`: - **Input**: A list of strings `input_list` where each string represents a number or a potentially invalid input. - **Output**: A list of processed numbers in which valid inputs are converted to integers and invalid inputs are handled appropriately. - **Raises**: A custom exception `EmptyInputError` when the input list is empty with a custom message. 2. The function should: - Raise a custom exception `EmptyInputError` (inheriting from `Exception`) when the input list is empty. - Handle `ValueError` when a string cannot be converted to a number and continue processing other inputs. - Return a detailed message for each invalid input. - Always print a completion message regardless of processing success or failure using a `finally` clause. - Utilize an `else` clause to print a success message if there are no invalid inputs. 3. Ensure that your code is clean and adheres to PEP8 standards. Constraints: - Only integers and strings that can be converted to integers are considered valid inputs. - The function should ignore `None` and empty strings gracefully. - Your solution should not use any external libraries except standard Python libraries. Example: ```python class EmptyInputError(Exception): pass def process_numbers(input_list): # Your code here # Example usage: try: result = process_numbers([\'42\', \'hello\', \'100\', \'\', None]) print(result) except EmptyInputError as e: print(e) # Expected Output: # Oops! Invalid input: \'hello\' # Oops! Invalid input: \'\' # Oops! Invalid input: None # Completion of processing. # [42, 100] ``` Ensure your function meets all the specified requirements and thoroughly test it with different cases.","solution":"class EmptyInputError(Exception): Custom exception for empty input list pass def process_numbers(input_list): if not input_list: raise EmptyInputError(\\"The input list is empty!\\") processed_numbers = [] invalid_inputs = [] for item in input_list: if item is None or item == \'\': invalid_inputs.append(item) continue try: number = int(item) processed_numbers.append(number) except ValueError: invalid_inputs.append(item) for invalid in invalid_inputs: print(f\\"Oops! Invalid input: \'{invalid}\'\\") if not invalid_inputs: print(\\"All inputs processed successfully!\\") print(\\"Completion of processing.\\") return processed_numbers"},{"question":"# Coding Assignment Objective: Utilize the provided functions from the Python C API to create a Python extension module that demonstrates handling and manipulating floating point objects! Task: Write C code for a Python extension module that accomplishes the following: 1. **Create a Float Object from a String**: Implement a function `create_float_from_string` that takes a Python string, converts it to a PyFloatObject, and returns it. 2. **Retrieve the Maximum and Minimum Float Values**: Implement functions `get_max_float` and `get_min_float` that return the largest and smallest representable float values, respectively. 3. **Convert Float Object to Double**: Implement a function `float_to_double` that takes a PyFloatObject and returns its C double representation. Specifications: 1. **create_float_from_string(str)**: * Input: A Python string representing a float (e.g., \\"123.456\\"). * Output: A Python float object. * Constraints: Ensure error handling for invalid strings. 2. **get_max_float()**: * Input: None * Output: The maximum representable float value as a Python float. 3. **get_min_float()**: * Input: None * Output: The minimum normalized positive float value as a Python float. 4. **float_to_double(py_float)**: * Input: A Python float object. * Output: The C double representation of the floating point number. * Constraints: Ensure proper error checking and handling. Example Usage: ```python import my_float_module # Create float from string f = my_float_module.create_float_from_string(\\"123.456\\") print(f) # Output: 123.456 # Get maximum float value max_float = my_float_module.get_max_float() print(max_float) # Output: DBL_MAX # Get minimum float value min_float = my_float_module.get_min_float() print(min_float) # Output: DBL_MIN # Convert float to double d = my_float_module.float_to_double(f) print(d) # Output: 123.456 (in double format) ``` Make sure to document your code properly and handle all edge cases efficiently. Performance is crucial, so ensure that your implementation is optimized and follows best practices.","solution":"from ctypes import cdll, c_char_p, c_double, py_object, pythonapi import sys import ctypes # Mocking the implementation for demonstration as I can\'t write C code directly in this notebook. # Pseudo-implementation because we can\'t run C extension code directly here def create_float_from_string(s): Create a float object from a string representation. Args: s (str): String representation of a float. Returns: float: Python float object created from string. try: return float(s) except ValueError: raise ValueError(\\"Invalid string for float conversion\\") def get_max_float(): Get the largest representable float value. Returns: float: The maximum representable float value. return sys.float_info.max def get_min_float(): Get the smallest normalized positive float value. Returns: float: The minimum normalized positive float value. return sys.float_info.min def float_to_double(py_float): Convert a Python float object to its C double representation. Args: py_float (float): A Python float object. Returns: float: The C double representation of the float object. if not isinstance(py_float, float): raise TypeError(\\"Expected a float object\\") return ctypes.c_double(py_float).value"},{"question":"Objective: Demonstrate your understanding of the `copy` module by implementing a class with custom copy behavior and utilizing shallow and deep copy operations. Problem Statement: You are required to create a class `Graph` that represents an undirected graph. Each `Graph` object maintains a dictionary of nodes where each key is a node and the value is a list of its neighboring nodes. You will implement custom shallow and deepcopy methods for the `Graph` class. 1. **Implement the `Graph` Class**: - The class should provide methods to add nodes and edges. - Implement the `__copy__` method for shallow copying. - Implement the `__deepcopy__` method for deep copying. 2. **Your Tasks**: - Implement the `Graph` class with the following specifications: ```python class Graph: def __init__(self): self.nodes = {} def add_node(self, node): if node not in self.nodes: self.nodes[node] = [] def add_edge(self, node1, node2): if node1 in self.nodes and node2 in self.nodes: self.nodes[node1].append(node2) self.nodes[node2].append(node1) def __copy__(self): # implement shallow copy logic here def __deepcopy__(self, memo): # implement deep copy logic here ``` - Write a function `copy_graph(graph: Graph) -> tuple` that: - Takes a `Graph` object as input. - Returns a tuple containing: - A shallow copy of the input graph. - A deep copy of the input graph. Input Format: - You do not need to handle input and output; only the class and required function implementation is necessary. Output Format: - The `copy_graph` function should return a tuple containing two `Graph` objects: the shallow copy and the deep copy. Example: ```python # Example Usage g = Graph() g.add_node(\'A\') g.add_node(\'B\') g.add_edge(\'A\', \'B\') shallow_copy, deep_copy = copy_graph(g) ``` Constraints: - The `Graph` will only contain nodes and undirected edges. - Node values will be unique within a `Graph`. # Requirements: - Correct implementation of both shallow and deep copy methods. - Proper usage of the `copy` module functions. This problem will test your understanding of copying mechanisms, how to implement custom copying behavior, and work with user-defined types in Python.","solution":"import copy class Graph: def __init__(self): self.nodes = {} def add_node(self, node): if node not in self.nodes: self.nodes[node] = [] def add_edge(self, node1, node2): if node1 in self.nodes and node2 in self.nodes: self.nodes[node1].append(node2) self.nodes[node2].append(node1) def __copy__(self): new_graph = Graph() new_graph.nodes = self.nodes.copy() return new_graph def __deepcopy__(self, memo): new_graph = Graph() memo[id(self)] = new_graph new_graph.nodes = copy.deepcopy(self.nodes, memo) return new_graph def copy_graph(graph: Graph) -> tuple: shallow_copy = copy.copy(graph) deep_copy = copy.deepcopy(graph) return shallow_copy, deep_copy"},{"question":"**Question: Performance Optimization of Aggregated Dataframe Operations** # Problem Statement You have been given a large dataset containing information about employees\' working hours and their corresponding projects. Your task is to perform some complex data manipulations and aggregate operations on this dataset efficiently. You will be required to use pandas along with any performance enhancement techniques like Cython, Numba, or `pandas.eval` as discussed in the provided documentation. # Input 1. A pandas DataFrame `df` with the following columns: ``` \\"employee_id\\": int, \\"project_id\\": int, \\"date\\": datetime, \\"hours_worked\\": float ``` 2. Number of employees `n_employees` (int) 3. Number of projects `n_projects` (int) # Output A pandas DataFrame with the following columns: ``` \\"project_id\\": int, \\"total_hours\\": float, \\"average_hours_per_employee\\": float ``` Each row in the output DataFrame represents a project, containing the total hours worked on that project and the average hours worked per employee on that project. # Constraints 1. The dataset can be very large (millions of rows). 2. You should ensure that the solution is optimized for performance. # Requirements 1. Write a function `optimize_aggregation(df: pd.DataFrame, n_employees: int, n_projects: int) -> pd.DataFrame` that performs the above operations efficiently. 2. You must use at least one of the performance enhancement techniques discussed in the provided documentation (Cython, Numba, or `pandas.eval`). # Example ```python import pandas as pd # Sample DataFrame data = { \\"employee_id\\": [1, 2, 1, 2, 1, 1], \\"project_id\\": [10, 10, 20, 20, 20, 10], \\"date\\": pd.to_datetime([\\"2023-01-01\\", \\"2023-01-02\\", \\"2023-01-01\\", \\"2023-01-02\\", \\"2023-01-03\\", \\"2023-01-03\\"]), \\"hours_worked\\": [5.0, 6.0, 8.0, 3.0, 7.0, 2.0] } df = pd.DataFrame(data) n_employees = 2 n_projects = 2 def optimize_aggregation(df, n_employees, n_projects): # Implement the optimized function here pass result_df = optimize_aggregation(df, n_employees, n_projects) print(result_df) ``` # Notes - Ensure to measure the performance of your function for large datasets. - Explain the chosen optimization technique in your code comments.","solution":"import pandas as pd import numpy as np def optimize_aggregation(df, n_employees, n_projects): Optimizes the aggregation of employee working hours by project, using pandas eval for better performance on large datasets. Parameters: - df: DataFrame containing \\"employee_id\\", \\"project_id\\", \\"date\\", and \\"hours_worked\\" - n_employees: total number of employees - n_projects: total number of projects Returns: - A DataFrame with \\"project_id\\", \\"total_hours\\", and \\"average_hours_per_employee\\" # Perform aggregation using pandas groupby and agg for total hours aggregated_data = df.groupby(\'project_id\').agg( total_hours=pd.NamedAgg(column=\'hours_worked\', aggfunc=\'sum\'), employee_count=pd.NamedAgg(column=\'employee_id\', aggfunc=pd.Series.nunique) ).reset_index() # Calculate average hours per employee aggregated_data[\'average_hours_per_employee\'] = aggregated_data.eval(\'total_hours / employee_count\') return aggregated_data[[\'project_id\', \'total_hours\', \'average_hours_per_employee\']] # Example usage: if __name__ == \\"__main__\\": # Sample DataFrame data = { \\"employee_id\\": [1, 2, 1, 2, 1, 1], \\"project_id\\": [10, 10, 20, 20, 20, 10], \\"date\\": pd.to_datetime([\\"2023-01-01\\", \\"2023-01-02\\", \\"2023-01-01\\", \\"2023-01-02\\", \\"2023-01-03\\", \\"2023-01-03\\"]), \\"hours_worked\\": [5.0, 6.0, 8.0, 3.0, 7.0, 2.0] } df = pd.DataFrame(data) n_employees = 2 n_projects = 2 result_df = optimize_aggregation(df, n_employees, n_projects) print(result_df)"},{"question":"Problem Statement You have been tasked with implementing a context-aware logging system that uses the `contextvars` package to manage context-local state. You need to implement the following functions: 1. `log_message`: - **Input**: A string `message`. - **Output**: None. - **Description**: Logs the message along with the current context identifier. It should use a context variable to store the context identifier. If no context identifier is set, it should log the message with a default identifier `default`. 2. `set_context_identifier`: - **Input**: A string `context_id`. - **Output**: None. - **Description**: Sets the context identifier to the provided value. 3. `get_context_identifier`: - **Input**: None. - **Output**: The current context identifier or `default` if no identifier is set. - **Description**: Retrieves the current context identifier. 4. `reset_context_identifier`: - **Input**: None. - **Output**: None. - **Description**: Resets the context identifier to its previous value. You will also implement a function to simulate logging in an asynchronous server environment: 5. `async_log_simulation`: - **Input**: A list of tuples, each containing a string `context_id` and a string `message`. - **Output**: A list of logged messages in the format \\"Context[context_id]: message\\". - **Description**: Simulates logging messages in different asynchronous contexts. Each tuple represents a context and a message to be logged. The function should use `asyncio` to handle multiple logging operations simultaneously. Constraints 1. Do not use global variables. 2. The `log_message` function should be thread-safe and coroutine-safe. 3. You may assume the context identifiers and messages are non-empty strings. Example ```python # Step 1: Log a message without setting a context identifier log_message(\\"Hello, World!\\") # Output: Context[default]: Hello, World! # Step 2: Set a context identifier and log a message set_context_identifier(\\"CTX1\\") log_message(\\"This is a test.\\") # Output: Context[CTX1]: This is a test. # Step 3: Get the current context identifier print(get_context_identifier()) # Output: CTX1 # Step 4: Reset the context identifier and log a message reset_context_identifier() log_message(\\"Resetting context.\\") # Output: Context[default]: Resetting context. # Step 5: Simulate asynchronous logging messages = [(\\"CTX2\\", \\"Async Message 1\\"), (\\"CTX3\\", \\"Async Message 2\\")] logged_messages = asyncio.run(async_log_simulation(messages)) print(logged_messages) # Output: [\'Context[CTX2]: Async Message 1\', \'Context[CTX3]: Async Message 2\'] ``` Implementation Requirements 1. Define and utilize a `ContextVar` for storing the context identifier. 2. Ensure that the context management is correctly handled in both synchronous and asynchronous code. 3. Provide appropriate error handling for cases where the context identifier is not set. You are required to submit your implementation of the specified functions along with test cases to validate your solution.","solution":"import contextvars import asyncio # Define a ContextVar to hold the context identifier context_id_var = contextvars.ContextVar(\'context_id\', default=\'default\') def log_message(message): Logs the message along with the current context identifier. If no context identifier is set, it uses \'default\'. context_id = context_id_var.get() print(f\'Context[{context_id}]: {message}\') def set_context_identifier(context_id): Sets the context identifier to the provided value. context_id_var.set(context_id) def get_context_identifier(): Retrieves the current context identifier or \'default\' if no identifier is set. return context_id_var.get() def reset_context_identifier(): Resets the context identifier to its previous value. context_id_var.set(\'default\') async def async_log_simulation(messages): Simulates logging messages in different asynchronous contexts. Each tuple represents a context and a message to be logged. tasks = [] for context_id, message in messages: tasks.append(asyncio.create_task(log_with_context(context_id, message))) return await asyncio.gather(*tasks) async def log_with_context(context_id, message): set_context_identifier(context_id) log_message(message) return f\'Context[{context_id}]: {message}\'"},{"question":"# Question: Implementing Unix Resource Usage and Limits in Python Problem Statement In this coding assessment, you are required to implement a Python function that retrieves and sets resource limits using the `resource` module and log these activities using the `syslog` module. The `resource` module provides the means to get and set resource limits, while the `syslog` module enables you to send log messages to the Unix system logger. Task Implement the function `adjust_resource_limits(soft_limit: int, hard_limit: int) -> dict`. This function should perform the following tasks: 1. Retrieve the current soft and hard limits for the resources available in the `resource` module. 2. Report the current limits using the `syslog` module. 3. Set the new soft and hard limits for a specific resource (e.g., `resource.RLIMIT_NOFILE`, which is the limit for the number of open file descriptors) to the values provided in the function\'s arguments. 4. Retrieve and report the updated limits using the `syslog` module. 5. Return a dictionary containing the previous and updated limits for the resource. Input - `soft_limit` (int): The new soft limit for the resource. - `hard_limit` (int): The new hard limit for the resource. Output - A dictionary with the previous and updated resource limits for `resource.RLIMIT_NOFILE`, structured as follows: ```python { \\"previous_limits\\": {\\"soft\\": prev_soft, \\"hard\\": prev_hard}, \\"updated_limits\\": {\\"soft\\": new_soft, \\"hard\\": new_hard} } ``` Constraints - The soft limit must be less than or equal to the hard limit. - You must handle any exceptions that occur during the limit adjustment and log them using the `syslog` module. Example ```python result = adjust_resource_limits(1024, 2048) print(result) # Expected output (example): # { # \\"previous_limits\\": {\\"soft\\": 256, \\"hard\\": 1024}, # \\"updated_limits\\": {\\"soft\\": 1024, \\"hard\\": 2048} # } ``` Notes - You will need to import the `resource` and `syslog` modules. - Make sure to configure the `syslog` module appropriately before using it. Additional Information For reference: - The `resource` module provides various constants such as `resource.RLIMIT_NOFILE`. - The `syslog` module needs to be properly configured by opening a syslog connection using `syslog.openlog()`.","solution":"import resource import syslog def adjust_resource_limits(soft_limit: int, hard_limit: int) -> dict: Adjust the resource limits for the number of open file descriptors. Arguments: soft_limit -- the new soft limit for the resource. hard_limit -- the new hard limit for the resource. Returns: A dictionary containing the previous and updated resource limits. try: # Open a connection to the syslog syslog.openlog(logoption=syslog.LOG_PID, facility=syslog.LOG_USER) resource_type = resource.RLIMIT_NOFILE # Get current limits prev_soft, prev_hard = resource.getrlimit(resource_type) # Log the current limits syslog.syslog(syslog.LOG_INFO, f\\"Current limits - Soft: {prev_soft}, Hard: {prev_hard}\\") # Set new resource limits resource.setrlimit(resource_type, (soft_limit, hard_limit)) # Get updated limits new_soft, new_hard = resource.getrlimit(resource_type) # Log the updated limits syslog.syslog(syslog.LOG_INFO, f\\"Updated limits - Soft: {new_soft}, Hard: {new_hard}\\") return { \\"previous_limits\\": {\\"soft\\": prev_soft, \\"hard\\": prev_hard}, \\"updated_limits\\": {\\"soft\\": new_soft, \\"hard\\": new_hard} } except ValueError as ve: syslog.syslog(syslog.LOG_ERR, f\\"ValueError: {ve}\\") return {\\"error\\": str(ve)} except resource.error as re: syslog.syslog(syslog.LOG_ERR, f\\"ResourceError: {re}\\") return {\\"error\\": str(re)} finally: # Close the syslog connection syslog.closelog()"},{"question":"**Question: Analyzing and Visualizing Titanic Data using Seaborn** The Titanic dataset provides data about the passengers who were aboard the Titanic. You need to analyze this dataset and create various visualizations using seaborn to reveal insights about the passengers\' demographics and survival. **Objective:** Write Python code using seaborn to perform the following tasks and create plots to visualize the results. 1. **Load Data:** - Load the Titanic dataset using seaborn\'s `load_dataset` function. 2. **Survival Rate by Class:** - Create a bar plot showing the number of survivors and non-survivors for each passenger class. Use different colors for male and female passengers and stack the bars to show the distribution within each class. 3. **Age Distribution by Survival Status:** - Create faceted histograms showing the distribution of passengers\' ages. Create separate histograms for males and females, with bars colored by survival status (use alpha channel to show this). 4. **Fare Distribution by Embarkation Point:** - Create a box plot showing the distribution of fares paid by passengers, grouped by their port of embarkation. Use different colors for each embarkation point and distinguish between survivors and non-survivors. **Expected Input and Output:** - **Input:** Titanic dataset loaded from seaborn. - **Output:** Three visualizations saved as PNG files: `survival_by_class.png`, `age_distribution.png`, `fare_distribution.png`. **Code Constraints:** - Your code should be modular with clear function definitions for each task. - Include appropriate labels, titles, and legends on the plots to make them informative. **Performance Requirements:** - Ensure your code can handle potential missing data and edge cases. Example code structure: ```python import seaborn.objects as so import seaborn as sns import matplotlib.pyplot as plt def load_data(): # Load the Titanic dataset titanic = sns.load_dataset(\\"titanic\\").sort_values(\\"alive\\", ascending=False) return titanic def survival_rate_by_class(titanic): # Create bar plot for survival rate by class plot = so.Plot(titanic, x=\\"class\\", color=\\"sex\\").add(so.Bar(), so.Count(), so.Stack()) plot.save(\\"survival_by_class.png\\") def age_distribution_by_sex_and_survival(titanic): # Create faceted histograms plot = (so.Plot(titanic, x=\\"age\\", alpha=\\"alive\\") .facet(\\"sex\\") .add(so.Bars(), so.Hist(binwidth=10), so.Stack())) plot.save(\\"age_distribution.png\\") def fare_distribution_by_embarkation_and_survival(titanic): # Create box plot for fare distribution by embarkation point and survival status plot = sns.catplot(data=titanic, x=\\"embark_town\\", y=\\"fare\\", hue=\\"alive\\", kind=\\"box\\") plot.set_axis_labels(\\"Embarkation Port\\", \\"Fare\\") plot.fig.suptitle(\\"Fare Distribution by Embarkation Point and Survival\\") plt.savefig(\\"fare_distribution.png\\") def main(): titanic = load_data() survival_rate_by_class(titanic) age_distribution_by_sex_and_survival(titanic) fare_distribution_by_embarkation_and_survival(titanic) if __name__ == \\"__main__\\": main() ``` Good luck! Your visualization skills with seaborn will uncover interesting stories hidden within these data.","solution":"import seaborn as sns import matplotlib.pyplot as plt def load_data(): Load the Titanic dataset using seaborn\'s `load_dataset` function. Returns: pd.DataFrame: Loaded Titanic dataset. titanic = sns.load_dataset(\\"titanic\\") return titanic def survival_rate_by_class(titanic): Create a bar plot showing the number of survivors and non-survivors for each passenger class. Use different colors for male and female passengers and stack the bars to show the distribution within each class. Args: titanic (pd.DataFrame): Titanic dataset. plt.figure(figsize=(10, 6)) sns.countplot(data=titanic, x=\'class\', hue=\'sex\', palette=\'Set1\', hue_order=[\'male\', \'female\']) plt.title(\'Survival Rate by Class and Sex\') plt.xlabel(\'Class\') plt.ylabel(\'Count\') plt.legend(title=\\"Sex\\") plt.savefig(\\"survival_by_class.png\\") plt.close() def age_distribution_by_sex_and_survival(titanic): Create faceted histograms showing the distribution of passengers\' ages. Create separate histograms for males and females, with bars colored by survival status (use alpha channel to show this). Args: titanic (pd.DataFrame): Titanic dataset. g = sns.FacetGrid(titanic, col=\\"sex\\", hue=\\"survived\\", palette=\\"Set1\\", height=6, aspect=1) g.map(plt.hist, \\"age\\", bins=20, alpha=0.5) g.add_legend(title=\\"Survived\\") g.set_axis_labels(\\"Age\\", \\"Count\\") g.set_titles(\\"{col_name} Passengers\\") plt.savefig(\\"age_distribution.png\\") plt.close() def fare_distribution_by_embarkation_and_survival(titanic): Create a box plot showing the distribution of fares paid by passengers, grouped by their port of embarkation. Use different colors for each embarkation point and distinguish between survivors and non-survivors. Args: titanic (pd.DataFrame): Titanic dataset. plt.figure(figsize=(10, 6)) sns.boxplot(data=titanic, x=\'embark_town\', y=\'fare\', hue=\'alive\', palette=\'Set1\') plt.title(\'Fare Distribution by Embarkation Point and Survival\') plt.xlabel(\'Embarkation Port\') plt.ylabel(\'Fare\') plt.savefig(\\"fare_distribution.png\\") plt.close() def main(): titanic = load_data() survival_rate_by_class(titanic) age_distribution_by_sex_and_survival(titanic) fare_distribution_by_embarkation_and_survival(titanic) if __name__ == \\"__main__\\": main()"},{"question":"Title: Monitor and Control Resource Limits in Python **Objective:** Design a function to monitor and control specific resource limits of the current process using the `resource` module. The function will: 1. Retrieve and print the current limit of a specified resource. 2. Attempt to set a new limit for that resource. 3. Retrieve and print the updated limit to confirm the changes. **Requirements:** Write a function `manage_resource_limits(resource_type: str, new_limits: tuple)` that performs the following steps: 1. **Input:** - `resource_type`: A string representing the type of resource to manage. It can be one of the following values: `\'RLIMIT_CORE\'`, `\'RLIMIT_CPU\'`, `\'RLIMIT_FSIZE\'`, `\'RLIMIT_DATA\'`, `\'RLIMIT_STACK\'`, `\'RLIMIT_RSS\'`, `\'RLIMIT_NPROC\'`, `\'RLIMIT_NOFILE\'`, `\'RLIMIT_MEMLOCK\'`, `\'RLIMIT_VMEM\'`, or `\'RLIMIT_AS\'`. - `new_limits`: A tuple `(soft, hard)` of two integers representing the new soft and hard limits for the specified resource. 2. **Output:** - Print the current soft and hard limit of the specified resource. - Attempt to set the new limits using the provided tuple. - Print the updated soft and hard limit of the specified resource. 3. **Constraints:** - The soft limit cannot exceed the hard limit. - The function should handle any errors that arise from invalid resource types or invalid limit values and print appropriate error messages. **Example:** ```python import resource def manage_resource_limits(resource_type: str, new_limits: tuple): try: # Resolve the resource constant resource_const = getattr(resource, resource_type) # Get the current limits current_limits = resource.getrlimit(resource_const) print(f\\"Current limits for {resource_type}: soft={current_limits[0]}, hard={current_limits[1]}\\") # Set the new limits resource.setrlimit(resource_const, new_limits) # Get the new limits to confirm the change updated_limits = resource.getrlimit(resource_const) print(f\\"Updated limits for {resource_type}: soft={updated_limits[0]}, hard={updated_limits[1]}\\") except ValueError as ve: print(f\\"ValueError: {ve}\\") except AttributeError: print(f\\"AttributeError: {resource_type} is not a valid resource type\\") except OSError as oe: print(f\\"OSError: {oe}\\") # Test the function with RLIMIT_CPU and new limits (120, 150) manage_resource_limits(\'RLIMIT_CPU\', (120, 150)) ``` **Performance Considerations:** - The function should handle execution within reasonable time limits without consuming excessive system resources. - Proper error handling and validation contribute to robust performance. *Note: Ensure the proper permissions are in place and the underlying operating system supports setting the specified resource limits to avoid permission errors or unsupported resource issues.*","solution":"import resource def manage_resource_limits(resource_type: str, new_limits: tuple): try: # Resolve the resource constant resource_const = getattr(resource, resource_type) # Get the current limits current_limits = resource.getrlimit(resource_const) print(f\\"Current limits for {resource_type}: soft={current_limits[0]}, hard={current_limits[1]}\\") # Set the new limits resource.setrlimit(resource_const, new_limits) # Get the new limits to confirm the change updated_limits = resource.getrlimit(resource_const) print(f\\"Updated limits for {resource_type}: soft={updated_limits[0]}, hard={updated_limits[1]}\\") except ValueError as ve: print(f\\"ValueError: {ve}\\") except AttributeError: print(f\\"AttributeError: {resource_type} is not a valid resource type\\") except OSError as oe: print(f\\"OSError: {oe}\\")"},{"question":"Problem Statement You are provided with the \'penguins\' dataset available in the seaborn package. Your task is to create two faceted plots using seaborn\'s `seaborn.objects` module: 1. A faceted plot showing the relationship between `flipper_length_mm` and `body_mass_g`, broken down by `species` and `island`. Each subplot should share axes with the others. 2. Another faceted plot showing the same relationship but only sharing axes across rows, allowing each column to have independent axes. Requirements - Load the `penguins` dataset using `seaborn.load_dataset`. - Ensure both plots are created using the `so.Plot` object and the `facet` method. - Use the `add` method to display the data points with `Dots`. - Clearly separate the plots into two different outputs. You should demonstrate the following: - Loading the dataset - Creating the first faceted plot with full axis sharing - Creating the second faceted plot with only row-sharing for axes - Displaying both plots # Input No specific input is required from the user. The dataset will be loaded within the script. # Output The output should consist of two distinct faceted plots as described. # Code Template ```python import seaborn.objects as so from seaborn import load_dataset # Load the penguins dataset penguins = load_dataset(\\"penguins\\") # Define the first faceted plot with full axis sharing p1 = ( so.Plot(penguins, x=\\"flipper_length_mm\\", y=\\"body_mass_g\\") .facet(col=\\"species\\", row=\\"island\\") .add(so.Dots()) .share(x=True, y=True) ) p1.show() # Define the second faceted plot with only row-sharing p2 = ( so.Plot(penguins, x=\\"flipper_length_mm\\", y=\\"body_mass_g\\") .facet(col=\\"species\\", row=\\"island\\") .add(so.Dots()) .share(x=\\"row\\") ) p2.show() ``` Fill in the code where appropriate to generate the required plots.","solution":"import seaborn.objects as so from seaborn import load_dataset def create_faceted_plots(): # Load the penguins dataset penguins = load_dataset(\\"penguins\\") # Define the first faceted plot with full axis sharing p1 = ( so.Plot(penguins, x=\\"flipper_length_mm\\", y=\\"body_mass_g\\") .facet(col=\\"species\\", row=\\"island\\") .add(so.Dots()) .share(x=True, y=True) ) p1.show() # Define the second faceted plot with only row-sharing p2 = ( so.Plot(penguins, x=\\"flipper_length_mm\\", y=\\"body_mass_g\\") .facet(col=\\"species\\", row=\\"island\\") .add(so.Dots()) .share(x=\\"row\\") ) p2.show()"},{"question":"Pairwise Metrics and Kernels Implementation You are given two matrices (X) and (Y) where each row vector in (X) and (Y) represents a sample. Your task is to implement several distance metrics and kernel functions to compute the pairwise distances and similarities between the samples in (X) and (Y). Functions to Implement 1. **pairwise_minkowski_distances**: - **Input**: Two numpy arrays (X) and (Y), and a parameter ( p ) (float). - **Output**: A numpy array containing the pairwise Minkowski distances between samples in (X) and (Y). - **Note**: Minkowski distance is a generalization of Euclidean distance. 2. **pairwise_cosine_similarity**: - **Input**: Two numpy arrays (X) and (Y). - **Output**: A numpy array containing the pairwise cosine similarity between samples in (X) and (Y). - **Note**: Use the equation provided in the documentation for cosine similarity. 3. **pairwise_polynomial_kernel**: - **Input**: Two numpy arrays (X) and (Y), and parameters (gamma), (coef0), and (degree) (all floats). - **Output**: A numpy array containing the pairwise polynomial kernel between samples in (X) and (Y). You can use the functions `sklearn.metrics.pairwise.pairwise_distances` and `sklearn.metrics.pairwise.pairwise_kernels` to verify your implementations with specific configurations. Example ```python import numpy as np # Input Data X = np.array([[1, 2], [3, 4], [5, 6]]) Y = np.array([[4, 5], [6, 7]]) # Minkowski Distance Example print(pairwise_minkowski_distances(X, Y, p=3)) # Cosine Similarity Example print(pairwise_cosine_similarity(X, Y)) # Polynomial Kernel Example print(pairwise_polynomial_kernel(X, Y, gamma=1, coef0=0, degree=2)) ``` Expected Outputs (approximately): ```python # Minkowski Distance Example [[ 3.30192725 6.28841895] [ 1.82574186 3.30192725] [ 1. 1.82574186]] # Cosine Similarity Example [[0.99589143 0.99705449] [0.9988665 0.99903492] [0.99943483 0.99954047]] # Polynomial Kernel Example [[841. 1600.] [1681. 2656.] [2809. 4096.]] ``` Constraints and Requirements 1. The inputs (X) and (Y) are numpy arrays of shape ((m, n)) and ((p, n)), respectively. 2. Your implementations should not use any specific pairwise distance or kernel function directly from sklearn, except for verifying results in your code testing. 3. You should aim for efficient computations with respect to time complexity. Implement the functions in a Python script or Jupyter notebook environment.","solution":"import numpy as np def pairwise_minkowski_distances(X, Y, p): Compute the pairwise Minkowski distances between two matrices X and Y. Parameters: X : np.ndarray, shape (m, n) Y : np.ndarray, shape (p, n) p : float, the order of the norm Returns: distances : np.ndarray, shape (m, p) m, n = X.shape p_, n_ = Y.shape assert n == n_ distances = np.zeros((m, p_)) for i in range(m): for j in range(p_): distances[i, j] = np.sum(np.abs(X[i] - Y[j]) ** p) ** (1 / p) return distances def pairwise_cosine_similarity(X, Y): Compute the pairwise cosine similarity between two matrices X and Y. Parameters: X : np.ndarray, shape (m, n) Y : np.ndarray, shape (p, n) Returns: similarities : np.ndarray, shape (m, p) X_norm = np.linalg.norm(X, axis=1, keepdims=True) Y_norm = np.linalg.norm(Y, axis=1, keepdims=True) similarities = (X @ Y.T) / (X_norm * Y_norm.T) return similarities def pairwise_polynomial_kernel(X, Y, gamma, coef0, degree): Compute the pairwise polynomial kernel between two matrices X and Y. Parameters: X : np.ndarray, shape (m, n) Y : np.ndarray, shape (p, n) gamma : float, the gamma parameter coef0 : float, the independent term in the kernel function degree : float, the degree of the polynomial kernel Returns: kernel : np.ndarray, shape (m, p) kernel = (gamma * (X @ Y.T) + coef0) ** degree return kernel"},{"question":"# Advanced Coding Exercise: Set Operations Objective Demonstrate your comprehension of Python sets and the provided functions to perform various standard mathematical set operations. Task Implement a Python function named `perform_set_operations` that takes two sets as input and returns a dictionary containing the results of various standard set operations between them. Your implementation should use the C API functions for sets provided in the documentation where applicable. Requirements 1. **Function Signature**: ```python def perform_set_operations(set1: set, set2: set) -> dict: ``` 2. **Expected Input and Output**: - **Input**: Two sets, `set1` and `set2`. - **Output**: A dictionary with the following keys: - `\'union\'`: Result of the union of `set1` and `set2`. - `\'intersection\'`: Result of the intersection of `set1` and `set2`. - `\'difference\'`: Result of the difference of `set1` and `set2`. - `\'symmetric_difference\'`: Result of the symmetric difference of `set1` and `set2`. 3. **Constraints**: - Input sets can contain any hashable, comparable Python objects. Example ```python def perform_set_operations(set1: set, set2: set) -> dict: # Your implementation here # Example Usage set1 = {1, 2, 3} set2 = {2, 3, 4} result = perform_set_operations(set1, set2) print(result) \'\'\' Expected Output: { \'union\': {1, 2, 3, 4}, \'intersection\': {2, 3}, \'difference\': {1}, \'symmetric_difference\': {1, 4} } \'\'\' ``` Performance Notes - The function should efficiently handle sets with up to 10^6 elements. - Focus on using the most appropriate and efficient methods for set operations. Additional Notes - You may simulate the behavior of the C API functions as closely as possible using native Python set methods. - Document any assumptions or design decisions in your code comments.","solution":"def perform_set_operations(set1: set, set2: set) -> dict: Perform various standard set operations between two sets. Parameters: set1 (set): First input set. set2 (set): Second input set. Returns: dict: A dictionary with the results of union, intersection, difference, and symmetric difference between the sets. result = { \'union\': set1 | set2, \'intersection\': set1 & set2, \'difference\': set1 - set2, \'symmetric_difference\': set1 ^ set2 } return result"},{"question":"# Python Development Mode Simulation **Objective:** Your task is to implement a function `simulate_dev_mode` that simulates some aspects of the Python Development Mode. This function should: 1. Open a specified file and read its content. 2. Log a warning if the file object is not properly closed. 3. Check and log any encoding issues when reading the file. 4. Catch and log any exceptions that may occur during the reading process. 5. Simulate a memory allocation check by catching any simulated buffer overflows. **Function Signature:** ```python def simulate_dev_mode(file_path: str, encoding: str = None, errors: str = None) -> str: pass ``` **Input:** - `file_path` (str): Path to the file to read. - `encoding` (str, optional): Encoding to be used for reading the file. Default is None. - `errors` (str, optional): Error handling scheme. Default is None. **Output:** - A string containing the content of the file. **Constraints:** - You may assume the `file_path` always points to a valid file. - If any encoding or decoding issues are encountered, a warning message should be logged. - If the function doesn\'t close the file properly, it should log a `ResourceWarning`. **Instructions:** 1. Open the file specified by `file_path` with the provided `encoding` and `errors` arguments. 2. Read the contents of the file and return it as a string. 3. Ensure the file is properly closed using a context manager or other means. 4. Log warnings or errors encountered during file operations. 5. Simulate a memory allocation check: create a temporary list, append a large number of items to it, and log a simulated buffer overflow warning if the list exceeds a certain size. **Example Usage:** ```python file_content = simulate_dev_mode(\'example.txt\', encoding=\'utf-8\', errors=\'strict\') print(file_content) ``` **Note:** While this function won\'t have access to actual development mode hooks, it should demonstrate an understanding of the types of checks Python Development Mode can perform and replicate them in a simplified form. **Additional Notes:** - Use print statements to log warnings and errors. - Make sure to close the file properly regardless of the method chosen to open it.","solution":"def simulate_dev_mode(file_path: str, encoding: str = None, errors: str = None) -> str: try: # Open and read the file with the specified encoding and error handling with open(file_path, \'r\', encoding=encoding, errors=errors) as file: content = file.read() # Simulate a memory allocation check temp_list = [] for _ in range(1000000): # Arbitrary large number for the sake of simulation temp_list.append(1) if len(temp_list) > 100000: # Arbitrary size limit for simulation print(\\"Simulated Warning: Buffer may overflow\\") return content except UnicodeDecodeError: # Log any encoding/decoding issues print(\\"Warning: Encoding issues encountered\\") return \'\' except Exception as ex: # Catch all other exceptions and log them print(f\\"Error: {str(ex)}\\") return \'\' finally: # Log a warning if the file object is not properly closed if \'file\' in locals() and not file.closed: print(\\"ResourceWarning: File was not properly closed\\") # Note: The function simulate_dev_mode assumes the file always exists and is a valid path."},{"question":"**Handling Missing Values in Pandas** You are provided with a CSV file containing sales data. The file has the following columns: - `Date`: The date of the sales (format: YYYY-MM-DD). - `Product`: The name of the product. - `UnitsSold`: The number of units sold. - `Revenue`: The revenue generated from the sales. The dataset has some missing values in the `UnitsSold` and `Revenue` columns. Write a function `process_sales_data(file_path: str) -> pd.DataFrame` that performs the following: 1. Reads the CSV file into a pandas DataFrame. 2. Identifies and counts the missing values in the `UnitsSold` and `Revenue` columns. 3. Fills the missing values in the `UnitsSold` column with the mean value of the column. 4. Fills the missing values in the `Revenue` column with zero. 5. Returns the processed DataFrame. **Input:** - `file_path` (str): The path to the CSV file. **Output:** - A pandas DataFrame with the processed data. **Constraints:** - You may assume the CSV file is well-formed and always contains the specified columns. - The `Date` column does not have any missing values. **Example:** Given the following CSV file: ``` Date,Product,UnitsSold,Revenue 2023-01-01,ProductA,10,100 2023-01-02,ProductB,,200 2023-01-03,ProductA,5, 2023-01-04,ProductC,8,80 2023-01-05,ProductB,, ``` Your function should return the following DataFrame: ``` Date Product UnitsSold Revenue 0 2023-01-01 ProductA 10.0 100.0 1 2023-01-02 ProductB 7.67 200.0 2 2023-01-03 ProductA 5.0 0.0 3 2023-01-04 ProductC 8.0 80.0 4 2023-01-05 ProductB 7.67 0.0 ``` Note: The mean value of the `UnitsSold` column (ignoring missing values) is calculated as (10+5+8)/3 = 7.67. **Performance Requirement:** - The function should process datasets with up to 10,000 rows efficiently.","solution":"import pandas as pd def process_sales_data(file_path: str) -> pd.DataFrame: Processes the sales data CSV file. Args: file_path (str): The path to the CSV file. Returns: pd.DataFrame: The processed DataFrame. # Read the CSV file into a pandas DataFrame df = pd.read_csv(file_path) # Calculate the mean value for \'UnitsSold\' ignoring missing values mean_units_sold = df[\'UnitsSold\'].mean() # Fill missing values df[\'UnitsSold\'].fillna(mean_units_sold, inplace=True) df[\'Revenue\'].fillna(0, inplace=True) return df"},{"question":"Implementing a Versatile Joint Plot Function Objective: Write a Python function using the seaborn library that generates joint plots for a given dataset. Your function should be able to handle different kinds of plots, conditional coloring, and various customizations. Function Signature: ```python def generate_joint_plot( data, x, y, plot_kind=\'scatter\', hue=None, custom_params=None, add_layers=False ): Generates a joint plot using seaborn with specified parameters. Parameters: - data (pd.DataFrame): The input dataset. - x (str): The column name to be used for the x-axis. - y (str): The column name to be used for the y-axis. - plot_kind (str): The type of joint plot (default is \'scatter\'). Options are: \'scatter\', \'kde\', \'reg\', \'hist\', \'hex\'. - hue (str, optional): Column name to be used for conditional coloring. - custom_params (dict, optional): Additional keyword arguments to customize the plot. E.g., marker size, bin count for histograms, etc. - add_layers (bool): Whether to add additional layers to the plot for extra information. Returns: - A seaborn JointGrid object representing the generated joint plot. ``` Constraints: 1. The function must only use the seaborn library for plotting. 2. If `custom_params` is provided, it should be passed correctly to the `sns.jointplot` function. 3. If `add_layers` is set to True, you should add additional plot layers using `plot_joint` and `plot_marginals` for further enhancements. Specifically, add a KDE plot overlay for the joint part and a rug plot for the marginal sides. Example Usage: ```python import seaborn as sns import pandas as pd # Load dataset penguins = sns.load_dataset(\\"penguins\\") # Example call to the function joint_plot = generate_joint_plot( data=penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", plot_kind=\\"scatter\\", hue=\\"species\\", custom_params={\\"marker\\": \\"+\\", \\"s\\": 100}, add_layers=True ) # Display the plot joint_plot.fig.show() ``` Expected output: A seaborn JointGrid object with a scatter plot, marginal histograms, conditional coloring by species, custom marker and size, and additional KDE and rug plot layers. Additional Notes: - Ensure to provide necessary imports and snippets for making the function self-contained for your students. - Students should be aware of basic plotting principles and seaborn functions covered in the documentation.","solution":"import seaborn as sns import pandas as pd def generate_joint_plot( data, x, y, plot_kind=\'scatter\', hue=None, custom_params=None, add_layers=False ): Generates a joint plot using seaborn with specified parameters. Parameters: - data (pd.DataFrame): The input dataset. - x (str): The column name to be used for the x-axis. - y (str): The column name to be used for the y-axis. - plot_kind (str): The type of joint plot (default is \'scatter\'). Options are: \'scatter\', \'kde\', \'reg\', \'hist\', \'hex\'. - hue (str, optional): Column name to be used for conditional coloring. - custom_params (dict, optional): Additional keyword arguments to customize the plot. E.g., marker size, bin count for histograms, etc. - add_layers (bool): Whether to add additional layers to the plot for extra information. Returns: - A seaborn JointGrid object representing the generated joint plot. # Create the jointplot with the specified parameters plot = sns.jointplot( data=data, x=x, y=y, kind=plot_kind, hue=hue, **(custom_params if custom_params else {}) ) # Add additional layers if requested if add_layers: plot.plot_joint(sns.kdeplot) plot.plot_marginals(sns.rugplot, height=1) return plot"},{"question":"Writing and Testing a Python Class You are required to write a Python class representing a simple bank account and then write unit tests for this class using the `unittest` and `unittest.mock` modules. BankAccount Class Implement a class `BankAccount` with the following functionalities: 1. **Initialization** - The class should be initialized with an account holder\'s name (string) and an initial balance (float). - It should raise a `ValueError` if the initial balance is negative. 2. **Deposit Method** - A method `deposit(amount: float) -> None` that adds the specified amount to the balance. - It should raise a `ValueError` if the deposit amount is not positive. 3. **Withdraw Method** - A method `withdraw(amount: float) -> bool` that subtracts the specified amount from the balance and returns `True` if successful. - If the amount to withdraw is greater than the balance, it should not change the balance and return `False`. - It should raise a `ValueError` if the withdrawal amount is not positive. 4. **Get Balance Method** - A method `get_balance() -> float` that returns the current balance. Unit Tests Write a separate script to test the `BankAccount` class using the `unittest` module. Ensure you: 1. Test the initialization to ensure it correctly sets up the account holder\'s name and initial balance. 2. Ensure that trying to initialize with a negative balance raises a `ValueError`. 3. Test the `deposit` method for both valid and invalid inputs. 4. Test the `withdraw` method for different scenarios: - Withdraw with sufficient funds. - Withdraw with insufficient funds. - Invalid withdraw amounts. 5. Use `unittest.mock` to test interactions if necessary, though for this simple example, mock usage may not be mandatory. Example Usage ```python # BankAccount class class BankAccount: def __init__(self, name: str, initial_balance: float): if initial_balance < 0: raise ValueError(\\"Initial balance cannot be negative\\") self.name = name self.balance = initial_balance def deposit(self, amount: float) -> None: if amount <= 0: raise ValueError(\\"Deposit amount must be positive\\") self.balance += amount def withdraw(self, amount: float) -> bool: if amount <= 0: raise ValueError(\\"Withdrawal amount must be positive\\") if amount > self.balance: return False self.balance -= amount return True def get_balance(self) -> float: return self.balance # Unit tests for BankAccount class import unittest from bank_account import BankAccount # Assuming the class is in a file named bank_account.py class TestBankAccount(unittest.TestCase): def test_initialization(self): acc = BankAccount(\\"Alice\\", 100.0) self.assertEqual(acc.name, \\"Alice\\") self.assertEqual(acc.get_balance(), 100.0) def test_initialization_negative_balance(self): with self.assertRaises(ValueError): acc = BankAccount(\\"Alice\\", -50.0) def test_deposit(self): acc = BankAccount(\\"Bob\\", 0) acc.deposit(50) self.assertEqual(acc.get_balance(), 50) with self.assertRaises(ValueError): acc.deposit(-20) def test_withdraw(self): acc = BankAccount(\\"Charlie\\", 100) self.assertTrue(acc.withdraw(50)) self.assertEqual(acc.get_balance(), 50) self.assertFalse(acc.withdraw(200)) self.assertEqual(acc.get_balance(), 50) with self.assertRaises(ValueError): acc.withdraw(-10) if __name__ == \'__main__\': unittest.main() ``` In this example, ensure to provide your well-documented code and test cases in separate Python files/modules.","solution":"class BankAccount: def __init__(self, name: str, initial_balance: float): if initial_balance < 0: raise ValueError(\\"Initial balance cannot be negative\\") self.name = name self.balance = initial_balance def deposit(self, amount: float) -> None: if amount <= 0: raise ValueError(\\"Deposit amount must be positive\\") self.balance += amount def withdraw(self, amount: float) -> bool: if amount <= 0: raise ValueError(\\"Withdrawal amount must be positive\\") if amount > self.balance: return False self.balance -= amount return True def get_balance(self) -> float: return self.balance"},{"question":"**Objective:** Implement a utility function to manage MPS-related PyTorch environment variables and demonstrate its effect on a PyTorch computation using the MPS backend. Background PyTorch supports running models on Apple devices using Metal Performance Shaders (MPS). Various environment variables can be set to configure and optimize the runtime behavior. These include options for debugging, profiling, memory allocation, and performance tuning. Task 1. Implement a function `set_mps_environment_variable` that: - Accepts a dictionary where keys are environment variable names and values are the corresponding settings. - Sets the environment variables accordingly. 2. Implement a simple PyTorch model to verify the effect of changing these environment variables. 3. Demonstrate the effect of setting the `PYTORCH_MPS_FAST_MATH` variable on matrix multiplication performance. Function Specifications - **Function Name**: `set_mps_environment_variable` - **Parameters**: - `env_vars` (dict): A dictionary where keys are strings (names of environment variables) and values are corresponding settings. - **Return Type**: `None` Implementation Steps 1. **Function to Set Environment Variables**: - Use the `os` library to set environment variables as per the input dictionary. 2. **PyTorch Model and Computation**: - Create two simple PyTorch tensors and perform matrix multiplication. - Measure and print the time for the computation with `PYTORCH_MPS_FAST_MATH` set to `0` and `1`. Example ```python import os import torch import time def set_mps_environment_variable(env_vars): for var, value in env_vars.items(): os.environ[var] = value # Setting up the environment variable for fast math env_vars_fast_math_on = {\\"PYTORCH_MPS_FAST_MATH\\": \\"1\\"} env_vars_fast_math_off = {\\"PYTORCH_MPS_FAST_MATH\\": \\"0\\"} # Applying the environment variable set_mps_environment_variable(env_vars_fast_math_on) # Model to evaluate tensor_a = torch.rand((1000, 1000), device=\'mps\') tensor_b = torch.rand((1000, 1000), device=\'mps\') start_time = time.time() result = torch.matmul(tensor_a, tensor_b) end_time = time.time() print(f\\"Execution time with fast math: {end_time - start_time} seconds\\") # Applying the environment variable set_mps_environment_variable(env_vars_fast_math_off) start_time = time.time() result = torch.matmul(tensor_a, tensor_b) end_time = time.time() print(f\\"Execution time without fast math: {end_time - start_time} seconds\\") ``` Constraints - You must run this code on an Apple device that supports MPS. - Make sure PyTorch is installed and properly configured to use the MPS backend. Performance Requirements - Ensure that the function correctly sets the environment variables before running the PyTorch computation. - Measure and report the time taken for each computation accurately.","solution":"import os def set_mps_environment_variable(env_vars): Sets environment variables for the MPS backend in PyTorch. Args: env_vars (dict): Dictionary where keys are environment variable names and values are the corresponding settings. Returns: None for var, value in env_vars.items(): os.environ[var] = value # Example of setting the environment variable and performing matrix multiplication def mps_matrix_multiplication_demo(): import torch import time device = torch.device(\'mps\') # Use MPS device tensor_a = torch.rand((1000, 1000), device=device) tensor_b = torch.rand((1000, 1000), device=device) # Setting `PYTORCH_MPS_FAST_MATH` to `1` set_mps_environment_variable({\\"PYTORCH_MPS_FAST_MATH\\": \\"1\\"}) start_time = time.time() torch.matmul(tensor_a, tensor_b) end_time = time.time() print(f\\"Execution time with fast math: {end_time - start_time} seconds\\") # Setting `PYTORCH_MPS_FAST_MATH` to `0` set_mps_environment_variable({\\"PYTORCH_MPS_FAST_MATH\\": \\"0\\"}) start_time = time.time() torch.matmul(tensor_a, tensor_b) end_time = time.time() print(f\\"Execution time without fast math: {end_time - start_time} seconds\\")"},{"question":"# Problem: Set Operations and Validation The objective of this problem is to implement a function that receives a list of instructions to perform various operations on `set` and `frozenset` objects. You need to handle these instructions, perform the operations, and return the results or error messages appropriately. Instructions 1. `new_set`: Creates a new `set` object from the specified iterable (e.g., list, tuple). 2. `new_frozenset`: Creates a new `frozenset` object from the specified iterable. 3. `add`: Adds an element to an existing `set` object. 4. `discard`: Removes an element from an existing `set` object. 5. `clear`: Clears all elements from an existing `set` object. 6. `size`: Returns the number of elements in the set or frozenset. 7. `contains`: Checks if a specified element is in the set or frozenset. Input Format - A list of dictionaries, where each dictionary represents an instruction. - Each dictionary has the following keys: `operation` (a string specifying the operation), `target` (a variable name string representing the target set/frozenset), and other necessary keys depending on the operation (like `element` for `add` and `discard`). Output Format - A list of results for each instruction. - For `new_set` and `new_frozenset`, the result should be the string `\'CREATED\'`. - For `add` and `discard`, the result should be the string `\'SUCCESS\'` or an appropriate error message. - For `clear`, the result should be the string `\'CLEARED\'`. - For `size`, the result should be the integer size of the target set or frozenset. - For `contains`, the result should be `True` or `False`. Constraints - You must handle operations on both `set` and `frozenset` objects properly. - You must raise appropriate error messages for invalid operations or types. Sample Input ```python instructions = [ {\\"operation\\": \\"new_set\\", \\"target\\": \\"a\\", \\"iterable\\": [1, 2, 3]}, {\\"operation\\": \\"new_frozenset\\", \\"target\\": \\"b\\", \\"iterable\\": [2, 3, 4]}, {\\"operation\\": \\"add\\", \\"target\\": \\"a\\", \\"element\\": 4}, {\\"operation\\": \\"discard\\", \\"target\\": \\"a\\", \\"element\\": 2}, {\\"operation\\": \\"clear\\", \\"target\\": \\"a\\"}, {\\"operation\\": \\"size\\", \\"target\\": \\"b\\"}, {\\"operation\\": \\"contains\\", \\"target\\": \\"b\\", \\"element\\": 3} ] ``` Sample Output ```python [\'CREATED\', \'CREATED\', \'SUCCESS\', \'SUCCESS\', \'CLEARED\', 3, True] ``` Implementation Implement the function `process_instructions(instructions: List[Dict[str, Any]]) -> List[Any]` to perform the operations and return the results as specified.","solution":"def process_instructions(instructions): sets = {} results = [] for instruction in instructions: operation = instruction[\'operation\'] target = instruction[\'target\'] if operation == \'new_set\': sets[target] = set(instruction[\'iterable\']) results.append(\'CREATED\') elif operation == \'new_frozenset\': sets[target] = frozenset(instruction[\'iterable\']) results.append(\'CREATED\') elif operation == \'add\': if target in sets and isinstance(sets[target], set): sets[target].add(instruction[\'element\']) results.append(\'SUCCESS\') else: results.append(\'ERROR: Cannot add to frozenset or target does not exist\') elif operation == \'discard\': if target in sets and isinstance(sets[target], set): sets[target].discard(instruction[\'element\']) results.append(\'SUCCESS\') else: results.append(\'ERROR: Cannot discard from frozenset or target does not exist\') elif operation == \'clear\': if target in sets and isinstance(sets[target], set): sets[target].clear() results.append(\'CLEARED\') else: results.append(\'ERROR: Cannot clear frozenset or target does not exist\') elif operation == \'size\': if target in sets: results.append(len(sets[target])) else: results.append(\'ERROR: Target does not exist\') elif operation == \'contains\': if target in sets: results.append(instruction[\'element\'] in sets[target]) else: results.append(\'ERROR: Target does not exist\') return results"},{"question":"# Custom Resource Manager using `contextlib` You are required to implement a custom resource manager for handling multiple resources using the `contextlib` module. Implement a class `CustomResourceManager` that: - Acquires multiple resources upon entering the context. - Releases all resources upon exiting the context, ensuring that all resources are released even if an exception occurs. - Uses the `@contextmanager` decorator to handle resource acquisition and release. - Supports both synchronous and asynchronous resource acquisition and release using the appropriate contextlib utilities. Requirements 1. Implement a synchronous resource manager: - The `__init__` method should accept any number of resource acquisition and release functions as arguments. - The `__enter__` method should acquire all resources. - The `__exit__` method should release all resources, ensuring that all resources are released even if an exception occurs. 2. Implement an asynchronous resource manager: - The `__init__` method should accept any number of asynchronous resource acquisition and release coroutine functions as arguments. - The `__aenter__` method should acquire all resources. - The `__aexit__` method should release all resources, ensuring that all resources are released even if an exception occurs. Example ```python from contextlib import contextmanager, asynccontextmanager class CustomResourceManager: @contextmanager def sync_resource_manager(self, *args): resources = [] for acquire, release in args: try: resource = acquire() resources.append((resource, release)) yield resource except Exception as exc: raise exc finally: for resource, release in resources: release(resource) @asynccontextmanager async def async_resource_manager(self, *args): resources = [] for acquire, release in args: try: resource = await acquire() resources.append((resource, release)) yield resource except Exception as exc: raise exc finally: for resource, release in resources: await release(resource) # Usage Example: # Synchronous resource management def acquire_resource(): print(\\"Resource acquired\\") return \\"Resource\\" def release_resource(resource): print(f\\"Resource {resource} released\\") crm = CustomResourceManager() with crm.sync_resource_manager((acquire_resource, release_resource)) as res: print(f\\"Using {res}\\") # Asynchronous resource management async def acquire_async_resource(): print(\\"Async Resource acquired\\") return \\"Async Resource\\" async def release_async_resource(resource): print(f\\"Async Resource {resource} released\\") import asyncio crm = CustomResourceManager() asyncio.run(crm.async_resource_manager((acquire_async_resource, release_async_resource))) ``` In the example provided: - The `sync_resource_manager` method should acquire and release resources synchronously. - The `async_resource_manager` method should acquire and release resources asynchronously. Your implementation should correctly handle exceptions that might occur during resource acquisition or release. Submission Submit your implementation of the `CustomResourceManager` class along with a script demonstrating its usage with both synchronous and asynchronous resources.","solution":"from contextlib import contextmanager, asynccontextmanager class CustomResourceManager: def __init__(self, *args): Initialize the resource manager with the given resources. Each resource is a tuple containing (acquire, release) functions or coroutines. self.resources = args @contextmanager def __call__(self): acquired_resources = [] try: for acquire, release in self.resources: resource = acquire() acquired_resources.append((resource, release)) yield acquired_resources except Exception as exc: raise exc finally: for resource, release in reversed(acquired_resources): release(resource) @asynccontextmanager async def async_manager(self): acquired_resources = [] try: for acquire, release in self.resources: resource = await acquire() acquired_resources.append((resource, release)) yield acquired_resources except Exception as exc: raise exc finally: for resource, release in reversed(acquired_resources): await release(resource)"},{"question":"Exploring Package Metadata **Objective:** Write a Python script to analyze metadata of a specified installed package using the `importlib.metadata` library. **Requirements:** 1. **Function Name and Description:** - **Function Name:** `analyze_package_metadata` - **Description:** This function takes the name of an installed package and returns a dictionary with various metadata details about the package. 2. **Expected Input Format:** - The function should take a single string argument which is the name of the package to analyze. For example: `\'wheel\'`. 3. **Expected Output Format:** - The function should return a dictionary with the following keys: - `\'version\'`: The version of the package. - `\'entry_points\'`: A list of entry points in the \'console_scripts\' group. - `\'requires\'`: A list of distribution requirements. - `\'files\'`: A list of file paths contained in the distribution. 4. **Constraints and Limitations:** - You may assume the package exists in the Python environment where the script is run. - If any of the required metadata details are unavailable, the corresponding dictionary entry should have a value of `None`. 5. **Performance Requirements:** - The function should efficiently fetch and return the metadata using appropriate methods provided by the `importlib.metadata` library. **Sample Code with Expected Output:** ```python from importlib.metadata import version, entry_points, requires, files def analyze_package_metadata(package_name: str) -> dict: try: package_version = version(package_name) except: package_version = None try: eps = entry_points().select(group=\'console_scripts\', name=package_name) package_entry_points = [ep.name for ep in eps] except: package_entry_points = None try: package_requires = requires(package_name) except: package_requires = None try: package_files = files(package_name) package_files = [str(p) for p in package_files] except: package_files = None return { \'version\': package_version, \'entry_points\': package_entry_points, \'requires\': package_requires, \'files\': package_files } # Example usage: # analyze_package_metadata(\'wheel\') # Expected output (example): # { # \'version\': \'0.32.3\', # \'entry_points\': [\'wheel\'], # \'requires\': [\\"pytest (>=3.0.0) ; extra == \'test\'\\", \\"pytest-cov ; extra == \'test\'\\"], # \'files\': [\'wheel/__init__.py\', \'wheel/bdist_wheel.py\', \'wheel/cli.py\', ...] # } ``` **Additional Context:** - You can use a virtual environment to test your function by installing packages using `pip` and then running the script to analyze those packages. - This question tests your ability to leverage and integrate different functions from the `importlib.metadata` library to extract useful information about installed packages. **Notes:** - Ensure to handle exceptions gracefully to account for any missing package metadata.","solution":"from importlib.metadata import version, entry_points, requires, files def analyze_package_metadata(package_name: str) -> dict: try: package_version = version(package_name) except Exception as e: package_version = None try: eps = entry_points().select(group=\'console_scripts\') package_entry_points = [ep.name for ep in eps if ep.value.startswith(package_name)] except Exception as e: package_entry_points = None try: package_requires = requires(package_name) except Exception as e: package_requires = None try: package_files = files(package_name) package_files = [str(p) for p in package_files] except Exception as e: package_files = None return { \'version\': package_version, \'entry_points\': package_entry_points, \'requires\': package_requires, \'files\': package_files }"},{"question":"# Parsing and Processing XML with `xml.dom.pulldom` Question: You are given an XML document containing information about various sales items. Your task is to write a function `filter_and_process_items(xml_string: str, min_price: int) -> str` that processes this XML document, filters items based on a minimum price, and returns a new XML document containing only those items. Your function should: 1. Parse the input XML string. 2. Iterate over the events to find items (`<item>` elements). 3. Expand an item only if its price is greater than the given minimum price. 4. Include such items in the output XML string. 5. Ensure the output XML retains the structure of the input XML, but only includes the filtered items within a root element. **Input:** - `xml_string` (str): A string containing the XML data. - `min_price` (int): An integer representing the minimum price filter. **Output:** - A string representing the new XML content containing only the items with a price greater than the specified minimum price. **Constraints:** - Assume the XML is well-formed. - Prices are represented as integer attributes in the XML. - The function should be efficient in terms of both time and space complexity. **Example:** ```python xml_input = <sales> <item price=\\"30\\"><name>Item1</name></item> <item price=\\"60\\"><name>Item2</name></item> <item price=\\"20\\"><name>Item3</name></item> <item price=\\"80\\"><name>Item4</name></item> </sales> min_price = 50 output_xml = filter_and_process_items(xml_input, min_price) # Expected output: # <sales> # <item price=\\"60\\"><name>Item2</name></item> # <item price=\\"80\\"><name>Item4</name></item> # </sales> ``` You may use the `xml.dom.pulldom` package to accomplish this.","solution":"import xml.dom.pulldom def filter_and_process_items(xml_string: str, min_price: int) -> str: Processes the XML document, filters items based on a minimum price, and returns a new XML document containing only those items. :param xml_string: A string containing the XML data. :param min_price: An integer representing the minimum price filter. :return: A string representing the new XML content with filtered items. events = xml.dom.pulldom.parseString(xml_string) filtered_items = [] for event, node in events: if event == \'START_ELEMENT\' and node.tagName == \'item\': price = int(node.getAttribute(\'price\')) if price > min_price: events.expandNode(node) filtered_items.append(node.toxml()) result_xml = \\"<sales>n\\" + \\"n\\".join(filtered_items) + \\"n</sales>\\" return result_xml"},{"question":"In this assessment, you will implement a function that processes a list of tasks, each with a priority, using a priority queue implemented with the `heapq` module. Each task is represented as a tuple `(priority, task_name)`, where `priority` is an integer (smaller values indicate higher priority) and `task_name` is a string describing the task. You need to write a function `process_tasks(tasks: List[Tuple[int, str]]) -> List[str]` that takes a list of tasks and returns a list of task names sorted by their priority. Tasks with the same priority should maintain their original order. **Function Signature:** ```python def process_tasks(tasks: List[Tuple[int, str]]) -> List[str]: ``` **Input:** - `tasks`: A list of tuples, where each tuple contains an integer `priority` and a string `task_name`. **Output:** - A list of task names sorted by their priority. **Constraints:** - The number of tasks `n` will be between 1 and 10^5. - The length of each `task_name` will be between 1 and 100. - The `priority` will be a non-negative integer. **Performance Requirements:** - The function should run in O(n log n) time complexity. **Examples:** ```python tasks = [(3, \'write code\'), (1, \'write spec\'), (2, \'create tests\'), (3, \'release product\')] print(process_tasks(tasks)) # Output: [\'write spec\', \'create tests\', \'write code\', \'release product\'] tasks = [(5, \'task1\'), (5, \'task2\'), (10, \'task3\'), (1, \'task4\')] print(process_tasks(tasks)) # Output: [\'task4\', \'task1\', \'task2\', \'task3\'] ``` **Notes:** - You should make use of the `heapq` module for maintaining the priority queue. - Focus on maintaining the initial order for tasks with the same priority. **Hints:** - You may use the `heapq.heappush` and `heapq.heappop` functions to manage the priority queue. - Consider using an additional count to ensure the stability of tasks with the same priority.","solution":"import heapq from typing import List, Tuple def process_tasks(tasks: List[Tuple[int, str]]) -> List[str]: Takes a list of tasks, each with priority, and returns the task names sorted by their priority. # Create a priority queue heap = [] # Push all tasks into the heap for index, (priority, task_name) in enumerate(tasks): # Use index to maintain original order for tasks with same priority heapq.heappush(heap, (priority, index, task_name)) # Extract tasks from the heap sorted_tasks = [] while heap: priority, index, task_name = heapq.heappop(heap) sorted_tasks.append(task_name) return sorted_tasks"},{"question":"**Question: Using functools to Implement a Responsive Mathematical Solver** You are tasked with creating a mathematical solver that leverages the `functools` library to optimize performance and code readability. The solver should evaluate a series of specific mathematical operations. Your solution must demonstrate your understanding and correct application of the following `functools` features: 1. `lru_cache` 2. `reduce` 3. `partial` 4. `total_ordering` # Task 1: Memoized Factorial Using `lru_cache` Define a function `factorial` that computes the factorial of a non-negative integer ( n ). Use `lru_cache` to optimize repeated calls to the function. **Input:** A non-negative integer ( n ). **Output:** The factorial of ( n ). ```python from functools import lru_cache @lru_cache(maxsize=None) def factorial(n): if n == 0: return 1 return n * factorial(n - 1) ``` # Task 2: Calculating the Greatest Common Divisor (GCD) Using `reduce` Define a function `gcd_multiple` that calculates the GCD of a list of integers using the `reduce` function. Use the `math.gcd` function for pairwise GCD operations. **Input:** A list of integers. **Output:** The GCD of the list of integers. ```python from functools import reduce import math def gcd_multiple(numbers): return reduce(math.gcd, numbers) ``` # Task 3: Creating a Base Converter Using `partial` Define a function `to_base` that converts a base-10 integer into a string representation of a specified base using the `partial` function. **Input:** A base-10 integer ( n ) and an integer base ( b ) (2 <= b <= 36). **Output:** A string representation of the number ( n ) in base ( b ). ```python from functools import partial def int_to_base(n, base): characters = \\"0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ\\" if n == 0: return \\"0\\" digits = \\"\\" while n: digits += characters[n % base] n //= base return digits[::-1] to_base = partial(int_to_base) ``` # Task 4: Implementing Comparable Fractions Using `total_ordering` Define a class `Fraction` that represents a fraction. Use the `total_ordering` decorator to implement all comparison methods (`==`, `<`, `<=`, `>`, `>=`) by defining only `__eq__` and `__lt__`. **Input:** Initial values for numerator and denominator when creating an instance of `Fraction`. **Output:** Appropriate comparison results for different instances of `Fraction`. ```python from functools import total_ordering @total_ordering class Fraction: def __init__(self, numerator, denominator): self.numerator = numerator self.denominator = denominator def __eq__(self, other): return self.numerator * other.denominator == self.denominator * other.numerator def __lt__(self, other): return self.numerator * other.denominator < self.denominator * other.numerator def __repr__(self): return f\\"{self.numerator}/{self.denominator}\\" ``` # Final Solution Integrate all tasks into a script that demonstrates their use. ```python from functools import lru_cache, reduce, partial, total_ordering import math @lru_cache(maxsize=None) def factorial(n): if n == 0: return 1 return n * factorial(n - 1) def gcd_multiple(numbers): return reduce(math.gcd, numbers) def int_to_base(n, base): characters = \\"0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ\\" if n == 0: return \\"0\\" digits = \\"\\" while n: digits += characters[n % base] n //= base return digits[::-1] to_base = partial(int_to_base) @total_ordering class Fraction: def __init__(self, numerator, denominator): self.numerator = numerator self.denominator = denominator def __eq__(self, other): return self.numerator * other.denominator == self.denominator * other.numerator def __lt__(self, other): return self.numerator * other.denominator < self.denominator * other.numerator def __repr__(self): return f\\"{self.numerator}/{self.denominator}\\" # Testing the functions print(factorial(5)) # Output: 120 print(gcd_multiple([24, 36, 48])) # Output: 12 print(to_base(255, 16)) # Output: \\"FF\\" f1 = Fraction(1, 2) f2 = Fraction(2, 3) print(f1 < f2) # Output: True print(f1 == f2) # Output: False ``` This exercise will assess the student\'s ability to use the `functools` module effectively.","solution":"from functools import lru_cache, reduce, partial, total_ordering import math @lru_cache(maxsize=None) def factorial(n): if n == 0: return 1 return n * factorial(n - 1) def gcd_multiple(numbers): return reduce(math.gcd, numbers) def int_to_base(n, base): characters = \\"0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ\\" if n == 0: return \\"0\\" digits = \\"\\" while n: digits += characters[n % base] n //= base return digits[::-1] to_base = partial(int_to_base) @total_ordering class Fraction: def __init__(self, numerator, denominator): if denominator == 0: raise ValueError(\\"Denominator cannot be zero.\\") self.numerator = numerator self.denominator = denominator def __eq__(self, other): return self.numerator * other.denominator == self.denominator * other.numerator def __lt__(self, other): return self.numerator * other.denominator < self.denominator * other.numerator def __repr__(self): return f\\"{self.numerator}/{self.denominator}\\""},{"question":"# k-Nearest Neighbors Classification You are required to implement a k-nearest neighbors classification model using scikit-learn\'s `KNeighborsClassifier` on the Iris dataset. The Iris dataset consists of 150 samples of iris flowers, where each sample has four features (sepal length, sepal width, petal length, petal width) and belongs to one of three classes (setosa, versicolor, virginica). 1. **Load the Iris Dataset**: Use `sklearn.datasets.load_iris` to load the dataset. 2. **Split the Data**: Split the dataset into training and testing sets using `train_test_split` with 70% of the data for training and the remaining 30% for testing. 3. **Create the k-Nearest Neighbors Classifier**: Initialize `KNeighborsClassifier` with `n_neighbors=3`. 4. **Train the Model**: Fit the model on the training data. 5. **Evaluate the Model**: Evaluate the accuracy of the model on the testing data and print the accuracy. Input Format - There are no input parameters. The script runs the specified operations internally. Output Format - Print the accuracy of the k-NN classifier on the test data. Constraints - Use `random_state=42` for data splitting to ensure reproducibility. Example Below is a rough template to get you started: ```python from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.neighbors import KNeighborsClassifier # Step 1: Load the Iris dataset iris = load_iris() X = iris.data y = iris.target # Step 2: Split the data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Step 3: Initialize the k-NN classifier knn = KNeighborsClassifier(n_neighbors=3) # Step 4: Train the model knn.fit(X_train, y_train) # Step 5: Evaluate the model accuracy = knn.score(X_test, y_test) # Print the accuracy print(f\\"Accuracy: {accuracy:.2f}\\") ``` **Your task** is to fill in the missing parts of the code and ensure it runs correctly to perform the classification and evaluation as described.","solution":"from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.neighbors import KNeighborsClassifier def k_nearest_neighbors_classification(): # Step 1: Load the Iris dataset iris = load_iris() X = iris.data y = iris.target # Step 2: Split the data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Step 3: Initialize the k-NN classifier knn = KNeighborsClassifier(n_neighbors=3) # Step 4: Train the model knn.fit(X_train, y_train) # Step 5: Evaluate the model accuracy = knn.score(X_test, y_test) # Print the accuracy print(f\\"Accuracy: {accuracy:.2f}\\") return accuracy"},{"question":"You are tasked with analyzing the famous Titanic dataset using Seaborn and creating a set of visualizations to convey specific insights. Your goal is to generate a comprehensive visualization involving different plot types, subplots, and customizations. Requirements: 1. **Loading the Dataset:** - Load the Titanic dataset provided by Seaborn using `sns.load_dataset(\\"titanic\\")`. 2. **Creating Subplots:** - Create a subplot where each column represents a different class (`\'class\'`), and within each subplot, compare the survival rates (`\'survived\'`) of passengers grouped by their gender (`\'sex\'`). - Use a `bar` plot for this visualization. - Set the height of each subplot to 4 and the aspect ratio to 0.6. 3. **Customizing Plots:** - Customize the subplot titles to display the class names. - Set the x-axis labels to \\"Men\\" and \\"Women\\". - Set the y-axis label to \\"Survival Rate\\". - Ensure the y-axis limits are between 0 and 1. - Remove the left spine of the plots. 4. **Combining Plots:** - Create a combined plot overlaying a `violin` plot and a `swarm` plot. - The x-axis should represent age (`\'age\'`), and the y-axis should represent the class (`\'class\'`). - Adjust the `violin` plot to have a light gray (`\\".9\\"`) color and no inner representation. - Use a `swarm` plot with small points (`size=3`). Finally, save the two resulting figures as `\\"titanic_class_survival.png\\"` and `\\"titanic_age_class.png\\"` respectively. Function Signature: ```python def titanic_visualization(): pass ``` Constraints: - Ensure that the function does not return anything but instead saves the output plots as specified files. - Your code should be efficient and make use of Seaborn\'s functionality effectively. Example: Here\'s an example of what your resulting plots should look like when the function is executed correctly: # Figure 1: `\\"titanic_class_survival.png\\"` ``` Three subplots, one for each class, each showing a bar plot of survival rate by gender. ``` # Figure 2: `\\"titanic_age_class.png\\"` ``` A combined plot with a light gray violin plot and an overlaid swarm plot showing age distribution across different classes. ``` Use the provided documentation snippets to help guide your implementation.","solution":"import seaborn as sns import matplotlib.pyplot as plt def titanic_visualization(): # Load the Titanic dataset titanic = sns.load_dataset(\\"titanic\\") # Create subplots for survival rates by class and gender g = sns.catplot(data=titanic, x=\'sex\', y=\'survived\', col=\'class\', kind=\'bar\', height=4, aspect=0.6, palette=\'pastel\') for ax in g.axes.flat: ax.set_ylim(0, 1) # Set y-axis limits ax.set_ylabel(\\"Survival Rate\\") ax.set_xlabel(\\"Gender\\") ax.set_xticklabels([\'Men\', \'Women\']) ax.spines[\'left\'].set_visible(False) # Remove left spine g.set_titles(col_template=\\"{col_name}\\") # Save the subplot figure g.savefig(\\"titanic_class_survival.png\\") # Create combined violin and swarm plot plt.figure(figsize=(10,6)) sns.violinplot(data=titanic, x=\'age\', y=\'class\', color=\\".9\\", inner=None) sns.swarmplot(data=titanic, x=\'age\', y=\'class\', size=3) # Save the combined plot figure plt.savefig(\\"titanic_age_class.png\\") # Running the function to generate and save the plots titanic_visualization()"},{"question":"You are provided with a dataset containing features and a target variable. Your task is to implement a custom scoring function and use it to evaluate the performance of a random forest classifier on the dataset. Follow the steps below: 1. **Load the Dataset**: You will use the Iris dataset for this task. Load the dataset using `sklearn.datasets.load_iris`. 2. **Create a Custom Scoring Function**: - Implement a custom scoring function named `custom_f1_score` that calculates the F1 score for a binary classification problem. Do not use the existing F1 score from `sklearn.metrics`. 3. **Use `make_scorer` to Create a Scorer Object**: - Use the `custom_f1_score` function to create a scorer object using `make_scorer`. 4. **Train a Random Forest Classifier**: - Split the dataset into training and test sets using `train_test_split` from `sklearn.model_selection`. - Train a `RandomForestClassifier` on the training set. - Evaluate its performance on the test set using both the default accuracy metric and your custom F1 score. 5. **Compare the Performance**: - Print the accuracy score of the random forest classifier. - Print the custom F1 score of the random forest classifier using your custom scorer object. **Constraints**: - The custom F1 score function should handle binary classification. For the Iris dataset, you might need to convert it into a binary classification problem by selecting only two classes. - Do not use any built-in functions provided by `sklearn.metrics` to directly calculate F1 score. Expected Functions and Inputs: # `custom_f1_score(y_true, y_pred)` - **Input**: - `y_true`: array-like of shape (n_samples,) - True binary labels. - `y_pred`: array-like of shape (n_samples,) - Predicted binary labels. - **Output**: - F1 score as a float. # `main()` - **No input parameters**. - **Expected Output**: - Accuracy score (float). - Custom F1 score (float). # Example: ```python from sklearn.datasets import load_iris from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import train_test_split from sklearn.metrics import make_scorer # Step 1: Load the dataset data = load_iris() X = data.data y = data.target # Convert to binary classification problem y = (y == 0).astype(int) # Step 2: Implement the custom F1 score function def custom_f1_score(y_true, y_pred): tp = sum((y_true == 1) & (y_pred == 1)) fp = sum((y_true == 0) & (y_pred == 1)) fn = sum((y_true == 1) & (y_pred == 0)) if tp + fp == 0 or tp + fn == 0: return 0.0 precision = tp / (tp + fp) recall = tp / (tp + fn) f1_score = 2 * (precision * recall) / (precision + recall) return f1_score # Step 3: Create a scorer object custom_scorer = make_scorer(custom_f1_score, greater_is_better=True) # Step 4: Train a Random Forest Classifier and evaluate performance X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) clf = RandomForestClassifier(random_state=42) clf.fit(X_train, y_train) # Step 5: Compare performance accuracy = clf.score(X_test, y_test) custom_f1 = custom_f1_score(y_test, clf.predict(X_test)) print(\\"Accuracy Score:\\", accuracy) print(\\"Custom F1 Score:\\", custom_f1) ``` Your solution should output the accuracy score and the custom F1 score of the random forest classifier.","solution":"from sklearn.datasets import load_iris from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import train_test_split from sklearn.metrics import make_scorer # Load the dataset data = load_iris() X = data.data y = data.target # Convert to binary classification problem (Setosa vs non-Setosa) y = (y == 0).astype(int) # Implement the custom F1 score function def custom_f1_score(y_true, y_pred): tp = sum((y_true == 1) & (y_pred == 1)) fp = sum((y_true == 0) & (y_pred == 1)) fn = sum((y_true == 1) & (y_pred == 0)) if tp + fp == 0 or tp + fn == 0: return 0.0 precision = tp / (tp + fp) recall = tp / (tp + fn) f1_score = 2 * (precision * recall) / (precision + recall) return f1_score # Create a scorer object custom_scorer = make_scorer(custom_f1_score, greater_is_better=True) # Split data into training and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Train a Random Forest Classifier clf = RandomForestClassifier(random_state=42) clf.fit(X_train, y_train) # Evaluate performance accuracy = clf.score(X_test, y_test) custom_f1 = custom_f1_score(y_test, clf.predict(X_test)) print(\\"Accuracy Score:\\", accuracy) print(\\"Custom F1 Score:\\", custom_f1)"},{"question":"# Question: Implement an Asynchronous Echo Server and Client **Objective:** Create an asynchronous echo server and client using Python\'s asyncio streams. The server should be able to handle multiple clients concurrently, receive messages from them, and echo the messages back to the respective clients. The client should establish a connection, send a message, and display the echoed response. **Requirements:** 1. **Server Implementation:** - Create a TCP server that listens on `localhost` and port `8888`. - The server should handle multiple clients concurrently. - For each connected client, the server should read messages (up to 100 bytes), print them, and send them back to the client. - Close the connection gracefully after the message is echoed back. 2. **Client Implementation:** - Create a TCP client that connects to `localhost` on port `8888`. - The client should send a message provided by the user. - The client should read the echoed message from the server, print it, and then close the connection gracefully. 3. **Function Definitions:** - Implement `async def start_server()` to initialize and start the server. - Implement `async def handle_client(reader, writer)` to handle client interactions. - Implement `async def tcp_client(message)` to create the client, send a message, and process the response. **Example Use Case:** ```python # Server (run in one terminal) import asyncio async def handle_client(reader, writer): data = await reader.read(100) message = data.decode() addr = writer.get_extra_info(\'peername\') print(f\\"Received {message!r} from {addr!r}\\") print(f\\"Send: {message!r}\\") writer.write(data) await writer.drain() print(\\"Close the connection\\") writer.close() await writer.wait_closed() async def start_server(): server = await asyncio.start_server(handle_client, \'127.0.0.1\', 8888) addrs = \', \'.join(str(sock.getsockname()) for sock in server.sockets) print(f\'Serving on {addrs}\') async with server: await server.serve_forever() asyncio.run(start_server()) # Client (run in another terminal) import asyncio async def tcp_client(message): reader, writer = await asyncio.open_connection(\'127.0.0.1\', 8888) print(f\'Send: {message!r}\') writer.write(message.encode()) await writer.drain() data = await reader.read(100) print(f\'Received: {data.decode()!r}\') print(\'Close the connection\') writer.close() await writer.wait_closed() message = \'Hello World!\' asyncio.run(tcp_client(message)) ``` **Constraints:** - Use Python 3.10 or later. - Implement the solution using asyncio’s high-level streams API. - Ensure that the server can handle multiple clients concurrently. **Submission:** Provide the complete implementations of `start_server`, `handle_client`, and `tcp_client`. Test cases will be run to validate the server and client interaction.","solution":"import asyncio async def handle_client(reader, writer): data = await reader.read(100) message = data.decode() addr = writer.get_extra_info(\'peername\') print(f\\"Received {message!r} from {addr!r}\\") print(f\\"Send: {message!r}\\") writer.write(data) await writer.drain() print(\\"Close the connection\\") writer.close() await writer.wait_closed() async def start_server(): server = await asyncio.start_server(handle_client, \'127.0.0.1\', 8888) addrs = \', \'.join(str(sock.getsockname()) for sock in server.sockets) print(f\'Serving on {addrs}\') async with server: await server.serve_forever() async def tcp_client(message): reader, writer = await asyncio.open_connection(\'127.0.0.1\', 8888) print(f\'Send: {message!r}\') writer.write(message.encode()) await writer.drain() data = await reader.read(100) print(f\'Received: {data.decode()!r}\') print(\'Close the connection\') writer.close() await writer.wait_closed()"},{"question":"Problem Statement You are required to create a Python function that utilizes the `py_compile` module to compile multiple Python source files into byte-code files. The function should handle errors based on user preferences and support various optimization levels. It should also operate on directories to compile all `*.py` files within them. Function Signature ```python import os from typing import List, Union import py_compile def compile_python_files(sources: List[str], raise_errors: bool = False, quiet: int = 0, optimize: int = -1) -> List[Union[str, None]]: Compiles a list of Python source files or directories containing Python source files into byte-code files. Parameters: - sources (List[str]): A list of file paths or directory paths to be compiled. - raise_errors (bool): If True, raise exceptions on compile errors; otherwise, suppress or log based on `quiet`. - quiet (int): If 0 or 1, log errors to stderr; if 2, suppress errors entirely. - optimize (int): The optimization level to pass to the compile function. Returns: - List[Union[str, None]]: A list of paths to the generated byte-code files, or None for failures. ``` Input - A list of file or directory paths to Python source files. - A boolean `raise_errors` flag. - An integer `quiet` flag. - An integer `optimize` flag. Output - A list of paths to the generated byte-code files, or `None` for paths where compilation failed. Requirements 1. If the provided path is a directory, compile all `.py` files within the directory. 2. Handle errors based on the `raise_errors` and `quiet` parameters. 3. Support varying levels of optimization during the compilation process. 4. Return a list of paths to the generated byte-code files, using `None` for sources that failed to compile if `raise_errors` is False. Example Usage ```python compiled_paths = compile_python_files([\\"/path/to/file1.py\\", \\"/path/to/dir\\"], raise_errors=False, quiet=0, optimize=2) print(compiled_paths) ``` Constraints - Do not use external libraries, only the standard library is allowed. - Assume valid paths and permissions are granted for reading and writing files. Please implement the `compile_python_files` function according to the signature and requirements provided.","solution":"import os from typing import List, Union import py_compile def compile_python_files(sources: List[str], raise_errors: bool = False, quiet: int = 0, optimize: int = -1) -> List[Union[str, None]]: compiled_files = [] for source in sources: if os.path.isdir(source): for root, _, files in os.walk(source): for file in files: if file.endswith(\'.py\'): file_path = os.path.join(root, file) try: compiled_file_path = py_compile.compile(file_path, cfile=None, dfile=None, doraise=raise_errors, optimize=optimize) compiled_files.append(compiled_file_path) except py_compile.PyCompileError as e: if raise_errors: raise e if quiet < 2: # If quiet is 2, suppress errors if quiet == 0: print(f\\"Error compiling {file_path}: {e}\\") compiled_files.append(None) elif os.path.isfile(source) and source.endswith(\'.py\'): try: compiled_file_path = py_compile.compile(source, cfile=None, dfile=None, doraise=raise_errors, optimize=optimize) compiled_files.append(compiled_file_path) except py_compile.PyCompileError as e: if raise_errors: raise e if quiet < 2: # If quiet is 2, suppress errors if quiet == 0: print(f\\"Error compiling {source}: {e}\\") compiled_files.append(None) else: compiled_files.append(None) return compiled_files"},{"question":"# Custom Module Importer You are required to implement a custom module importer that provides functionalities to load, reload, and execute Python modules. The importer should also handle frozen modules. Implement the following classes and methods: Class: `CustomImporter` This class will manage the importing and reloading of Python modules. 1. **Method `import_module(self, module_name: str) -> ModuleType`:** - Imports a module with the given `module_name`. - Returns the module object on successful import. - Raises `ImportError` if the module cannot be imported. 2. **Method `reload_module(self, module: ModuleType) -> ModuleType`:** - Reloads the given module. - Returns the reloaded module object. - Raises `ImportError` if the module cannot be reloaded. 3. **Method `exec_code_module(self, module_name: str, code: str) -> ModuleType`:** - Executes the given code string as a module with the name `module_name`. - Returns the module object created from the code. - Raises `RuntimeError` if code execution fails. 4. **Method `import_frozen_module(self, module_name: str) -> ModuleType`:** - Imports a frozen module with the given `module_name`. - Returns the module object on successful import. - Raises `ImportError` if the module cannot be imported or is not frozen. Constraints: - `module_name` is a valid Python identifier. - `code` is a valid Python code string. # Example Use of `CustomImporter` ```python importer = CustomImporter() # Import a standard module os_module = importer.import_module(\'os\') print(os_module) # Reload the os module reloaded_os_module = importer.reload_module(os_module) print(reloaded_os_module) # Execute custom code as a module custom_code = \'\'\' def hello(): print(\\"Hello from custom module\\") \'\'\' custom_module = importer.exec_code_module(\'custom_module\', custom_code) custom_module.hello() # Import a frozen module (assuming \'frozen_hello\' is a valid frozen module) try: frozen_module = importer.import_frozen_module(\'frozen_hello\') frozen_module.some_function() except ImportError as e: print(\\"Frozen module import failed:\\", e) ``` Note: For the purpose of this assessment, you do not have to implement the actual interaction with the Python C API but should simulate the behavior with the necessary exception handling to demonstrate understanding of these importing principles.","solution":"import importlib import types import sys class CustomImporter: def import_module(self, module_name: str) -> types.ModuleType: try: module = importlib.import_module(module_name) return module except ImportError as e: raise ImportError(f\\"Module \'{module_name}\' cannot be imported.\\") from e def reload_module(self, module: types.ModuleType) -> types.ModuleType: try: reloaded_module = importlib.reload(module) return reloaded_module except ImportError as e: raise ImportError(f\\"Module \'{module.__name__}\' cannot be reloaded.\\") from e def exec_code_module(self, module_name: str, code: str) -> types.ModuleType: try: module = types.ModuleType(module_name) exec(code, module.__dict__) sys.modules[module_name] = module return module except Exception as e: raise RuntimeError(f\\"Code execution for module \'{module_name}\' failed.\\") from e def import_frozen_module(self, module_name: str) -> types.ModuleType: # This simulates the behavior and always raises ImportError for this exercise. raise ImportError(f\\"Frozen module \'{module_name}\' cannot be imported.\\")"},{"question":"**Objective:** Write a function that opens an AIFF or AIFF-C audio file, extracts specific metadata, and then modifies and saves the audio file with updated parameters. **Requirements:** 1. **Function signature:** ```python def process_aiff(file_path: str, new_file_path: str, new_params: tuple, frames_to_read: int) -> dict: ``` 2. **Input:** - `file_path` (str): The path to the input AIFF or AIFF-C file. - `new_file_path` (str): The path to save the modified AIFF or AIFF-C file. - `new_params` (tuple): A tuple containing new parameters in the format `(nchannels, sampwidth, framerate, nframes, comptype, compname)`. - `frames_to_read` (int): The number of frames to read from the original file. 3. **Output:** - Returns a dictionary containing metadata from the original file, with the following keys: - `nchannels`: Number of audio channels. - `sampwidth`: Sample width in bytes. - `framerate`: Frame rate (sampling rate in frames/second). - `nframes`: Total number of frames. - `comptype`: Compression type. - `compname`: Compression name. 4. **Task:** - Open the input file using `aifc.open(file_path, \'rb\')`. - Extract metadata using appropriate `aifc` methods and store it in a dictionary. - Read the specified number of frames from the input file. - Open a new file using `aifc.open(new_file_path, \'wb\')`. - Update it with the provided `new_params`. - Write the read frames to the new file. - Ensure to correctly close both input and output files. 5. **Constraints:** - The `new_params[3]` (nframes) should be equal to or greater than `frames_to_read`. - Handle any file-related exceptions gracefully. 6. **Performance:** - The function should be efficient in reading and writing large audio files. **Example:** ```python file_path = \'example.aiff\' new_file_path = \'new_example.aiff\' new_params = (2, 2, 44100, 10000, b\'NONE\', b\'not compressed\') frames_to_read = 5000 metadata = process_aiff(file_path, new_file_path, new_params, frames_to_read) print(metadata) # Example output: # { # \'nchannels\': 2, # \'sampwidth\': 2, # \'framerate\': 44100, # \'nframes\': 15000, # \'comptype\': b\'NONE\', # \'compname\': b\'not compressed\' # } ``` Note: The actual values in the example output will depend on the metadata of the input AIFF file.","solution":"import aifc import os def process_aiff(file_path: str, new_file_path: str, new_params: tuple, frames_to_read: int) -> dict: Process an AIFF or AIFF-C file to extract metadata and save a modified version of the file. Parameters: file_path (str): The path to the input AIFF or AIFF-C file. new_file_path (str): The path to save the modified AIFF or AIFF-C file. new_params (tuple): A tuple containing new parameters in the format (nchannels, sampwidth, framerate, nframes, comptype, compname). frames_to_read (int): The number of frames to read from the original file. Returns: dict: A dictionary containing metadata from the original file. try: # Open the original AIFF file with aifc.open(file_path, \'rb\') as input_file: # Extract metadata from the original file metadata = { \'nchannels\': input_file.getnchannels(), \'sampwidth\': input_file.getsampwidth(), \'framerate\': input_file.getframerate(), \'nframes\': input_file.getnframes(), \'comptype\': input_file.getcomptype(), \'compname\': input_file.getcompname() } # Ensure the number of frames to read is less than or equal to the total number of frames frames_to_read = min(frames_to_read, metadata[\'nframes\']) # Read the specified number of frames frames_data = input_file.readframes(frames_to_read) # Open the new AIFF file to write the modified frames with aifc.open(new_file_path, \'wb\') as output_file: # Set the new parameters output_file.setparams(new_params) # Write the frames to the new file output_file.writeframes(frames_data) return metadata except Exception as e: print(f\\"An error occurred: {e}\\") return {}"},{"question":"**Question**: Implement and manage an asynchronous operation using `asyncio.Future` **Objective**: You are required to implement an asynchronous function that performs a sequence of operations using `asyncio.Future`. This will test your understanding of futures, awaiting results, and scheduling tasks. **Problem**: A function named `async_pipeline` is provided which will run an asynchronous sequence of tasks. Your task is to complete implementations of the following two asynchronous helper functions using `asyncio.Future`: 1. `delayed_addition(fut, delay, a, b)`: This function accepts a `Future` object, a delay in seconds, and two numbers `a` and `b`. It should wait for the given delay and then set the result of the `Future` to the sum of `a` and `b`. 2. `delayed_multiplication(fut, delay, a, b)`: This function works similarly but should set the result of the `Future` to the product of `a` and `b`. Finally, the `async_pipeline` function should: - Create two `Future` objects. - Create two tasks, each one running one of the helper functions. - Wait for both futures to complete and print their results. **Function Signatures**: ```python import asyncio async def delayed_addition(fut: asyncio.Future, delay: int, a: int, b: int) -> None: Waits for `delay` seconds and sets the result of `fut` to be the sum of `a` and `b`. pass async def delayed_multiplication(fut: asyncio.Future, delay: int, a: int, b: int) -> None: Waits for `delay` seconds and sets the result of `fut` to be the product of `a` and `b`. pass async def async_pipeline(): Orchestrates the above functions, sets up futures and waits for their results. pass ``` **Example Usage**: Running the `async_pipeline()` function should: 1. Create two `Future` objects. 2. Run `delayed_addition` on one `Future` and `delayed_multiplication` on the other, with designated delays. 3. Print the results of both futures after they complete. **Constraints**: - The delay must be at least 1 second. - Handle exceptions appropriately: raise a `RuntimeError` if any future fails to complete successfully. - The function should print the results of the completed futures. **Performance Requirements**: - The operations can run concurrently but must be properly scheduled and awaited. **Example**: Given `delayed_addition` is set with a=4, b=5, and `delayed_multiplication` with a=6, b=7 and both with a delay of 2 seconds. Expected output sequence: ``` Addition result: 9 Multiplication result: 42 ``` You must fulfill the objective by writing complete implementations for the `delayed_addition`, `delayed_multiplication`, and `async_pipeline` functions.","solution":"import asyncio async def delayed_addition(fut: asyncio.Future, delay: int, a: int, b: int) -> None: Waits for `delay` seconds and sets the result of `fut` to be the sum of `a` and `b`. await asyncio.sleep(delay) fut.set_result(a + b) async def delayed_multiplication(fut: asyncio.Future, delay: int, a: int, b: int) -> None: Waits for `delay` seconds and sets the result of `fut` to be the product of `a` and `b`. await asyncio.sleep(delay) fut.set_result(a * b) async def async_pipeline(): Orchestrates the above functions, sets up futures, and waits for their results. # Create Future objects addition_future = asyncio.Future() multiplication_future = asyncio.Future() # Create tasks for asynchronous operations tasks = [ delayed_addition(addition_future, 2, 4, 5), delayed_multiplication(multiplication_future, 2, 6, 7) ] # Run tasks concurrently await asyncio.gather(*tasks) # Retrieve results from futures try: addition_result = addition_future.result() multiplication_result = multiplication_future.result() except Exception as e: raise RuntimeError(\\"One or more futures failed to complete successfully\\") from e # Print results print(f\\"Addition result: {addition_result}\\") print(f\\"Multiplication result: {multiplication_result}\\")"},{"question":"# Question: Manipulating Environment Variables and Handling Exceptions In this exercise, you are required to demonstrate your understanding of manipulating environment variables and handling system-related exceptions in Python. You will work with the `os` module to make your solution portable across different operating systems. Task: 1. Implement a function `modify_and_execute(command: str, env_vars: dict)` that: - Takes a command to run as a string and a dictionary of environment variables to set. - Modifies the current environment variables based on the inputs. - Executes the command using the modified environment. - Ensures that any errors encountered during the execution are handled gracefully by printing an appropriate error message. - Resets the environment variables to their original state after the command executes, regardless of success or failure. Input: - `command`: A string representing the command to be executed. - `env_vars`: A dictionary where keys are environment variable names (strings) and values are the new values for those variables (strings). Output: - Print the standard output of the command if it executes successfully. - Print an error message if the command fails to execute. Constraints: - Assume the command is a valid shell command. - The function should handle commonly raised exceptions such as `OSError`. Example Usage: ```python def modify_and_execute(command: str, env_vars: dict): pass # Example command and environment variables command = \\"echo MY_VAR\\" env_vars = {\\"MY_VAR\\": \\"Hello, World!\\"} modify_and_execute(command, env_vars) # Expected Output: \\"Hello, World!\\" ``` Notes: - Use the `os` module to manage environment variables and execute the command. - Ensure that the environment returns to its initial state after the command execution. - Use try-except blocks to handle any potential errors.","solution":"import os import subprocess def modify_and_execute(command: str, env_vars: dict): # Remember the current environment variables original_env = os.environ.copy() # Update the environment with provided variables os.environ.update(env_vars) try: # Execute the command and capture the output result = subprocess.run(command, shell=True, check=True, text=True, capture_output=True) print(result.stdout) except subprocess.CalledProcessError as e: print(f\\"Command \'{command}\' failed with error: {e}\\") except OSError as e: print(f\\"Execution failed with an OS error: {e}\\") finally: # Restore original environment variables os.environ.clear() os.environ.update(original_env)"},{"question":"# Question: Implementing a Context-managed Asynchronous Task Scheduler You are tasked with creating an asynchronous task scheduler that uses context variables to manage task-specific settings. Each task should be able to run in its own context, allowing for isolated state management. Requirements: 1. Implement a class `TaskScheduler` with the following methods: - `__init__(self)`: Initializes the scheduler and a context variable for task settings. - `add_task(self, task, *args, **kwargs)`: Adds a new task to the scheduler. - `run_tasks(self)`: Runs all added tasks within their own context. 2. Tasks will be simple functions that print their settings stored in the context variable. 3. Ensure that modifications to the context variable within one task do not affect other tasks. Example Usage: ```python import contextvars import asyncio class TaskScheduler: def __init__(self): self.tasks = [] self.task_setting_var = contextvars.ContextVar(\'task_setting\') def add_task(self, task, *args, **kwargs): self.tasks.append((task, args, kwargs)) async def run_tasks(self): async def run_task(task, args, kwargs): ctx = contextvars.copy_context() ctx.run(task, *args, **kwargs) await asyncio.gather(*(run_task(task, args, kwargs) for task, args, kwargs in self.tasks)) async def sample_task(): task_setting = task_setting_var.get() print(f\'Task is running with setting: {task_setting}\') # Usage scheduler = TaskScheduler() scheduler.add_task(sample_task) scheduler.add_task(sample_task) await scheduler.run_tasks() ``` Constraints: - Each task should be run asynchronously. - Context management should ensure isolation between task settings. Input: The `add_task` method takes a coroutine function as its task, along with any necessary arguments and keyword arguments. Output: The output will be printed statements from each running task, showing their respective settings. Notes: - Use the `contextvars` module to achieve context isolation. - Ensure the implementation is thread-safe and works correctly with asynchronous execution.","solution":"import contextvars import asyncio class TaskScheduler: def __init__(self): self.tasks = [] self.task_setting_var = contextvars.ContextVar(\'task_setting\') def add_task(self, task, *args, **kwargs): self.tasks.append((task, args, kwargs)) async def run_tasks(self): async def run_task(task, args, kwargs): ctx = contextvars.copy_context() ctx.run(task, *args, **kwargs) await asyncio.gather(*(run_task(task, args, kwargs) for task, args, kwargs in self.tasks)) # Sample task definition async def sample_task(): task_setting = TaskScheduler().task_setting_var.get() print(f\'Task is running with setting: {task_setting}\')"},{"question":"<|Analysis Begin|> The provided documentation describes the `abc` module in Python, which allows for the creation and use of Abstract Base Classes (ABCs). ABCs enable the definition of abstract methods that must be implemented by subclasses. Key components include: - `ABC` helper class: Allows definition of ABCs through inheritance. - `ABCMeta` metaclass: Used to define ABCs directly. - `abstractmethod` decorator: Marks methods as abstract, requiring overriding in subclasses. - Additional decorators: `abstractclassmethod`, `abstractstaticmethod`, and `abstractproperty` which have been deprecated. Key methods and functionalities: - `register(subclass)`: Register a virtual subclass. - `__subclasshook__(subclass)`: Customizes subclass checks. - `get_cache_token()` and `update_abstractmethods(cls)`: Utility functions for ABC management. <|Analysis End|> <|Question Begin|> # Coding Assessment Question The goal of this coding exercise is to test your understanding of Python’s `abc` module. You will create a set of abstract base classes and virtual subclasses to model a simple shape hierarchy. Problem Statement You need to design a Shape hierarchy using abstract base classes that enforce certain behaviors on its derived classes. 1. **Shape Base Class:** - Create an abstract base class `Shape` using the `ABC` helper class. - Define an abstract method `area()` which will compute the area of the shape. - Define an abstract method `perimeter()` which will compute the perimeter of the shape. 2. **Concrete Shape Classes:** - Create a class `Rectangle` that inherits from `Shape`: - It should have an initializer that takes `width` and `height` as parameters. - Implement the `area` method to return the area of the rectangle. - Implement the `perimeter` method to return the perimeter of the rectangle. - Create a class `Circle` that inherits from `Shape`: - It should have an initializer that takes `radius` as a parameter. - Implement the `area` method to return the area of the circle. - Implement the `perimeter` method to return the perimeter of the circle. 3. **Virtual Subclass:** - Register the `tuple` class as a virtual subclass of `Shape`. - Verify that instances of `tuple` pass `issubclass()` and `isinstance()` checks for `Shape`. Constraints and Requirements - Use the `math` module for any required mathematical operations, such as `math.pi`. - The `Rectangle` and `Circle` classes should precisely implement the area and perimeter calculations as described. - Ensure proper use of the `abc` module features. Input and Output - There are no direct input/output requirements; the focus is on implementing the classes and methods as described. - You should test your implementation by creating instances of `Rectangle` and `Circle`, calling their methods, and verifying the outputs. - Use assertions to verify that `tuple` is considered a virtual subclass of `Shape`. Example Test Case: ```python from math import pi # Define ABC and abstract methods from abc import ABC, abstractmethod class Shape(ABC): @abstractmethod def area(self): pass @abstractmethod def perimeter(self): pass # Rectangle implementation class Rectangle(Shape): def __init__(self, width, height): self.width = width self.height = height def area(self): return self.width * self.height def perimeter(self): return 2 * (self.width + self.height) # Circle implementation class Circle(Shape): def __init__(self, radius): self.radius = radius def area(self): return pi * (self.radius ** 2) def perimeter(self): return 2 * pi * self.radius # Register tuple as a virtual subclass of Shape Shape.register(tuple) # Testing rectangle = Rectangle(2, 3) circle = Circle(1) assert rectangle.area() == 6 assert rectangle.perimeter() == 10 assert circle.area() == pi assert circle.perimeter() == 2 * pi assert issubclass(tuple, Shape) assert isinstance((), Shape) ``` **Note:** This question is designed to check your grasp of abstract base classes, method overriding, and virtual subclass registration using the Python `abc` module.","solution":"from abc import ABC, abstractmethod from math import pi class Shape(ABC): @abstractmethod def area(self): pass @abstractmethod def perimeter(self): pass class Rectangle(Shape): def __init__(self, width, height): self.width = width self.height = height def area(self): return self.width * self.height def perimeter(self): return 2 * (self.width + self.height) class Circle(Shape): def __init__(self, radius): self.radius = radius def area(self): return pi * (self.radius ** 2) def perimeter(self): return 2 * pi * self.radius Shape.register(tuple)"},{"question":"**Python Coding Assessment: Custom Configuration Loader** # Objective: Create a custom configuration loader for a Python application by leveraging the functionalities of `warnings`, `dataclasses`, and `contextlib`. This loader should provide the ability to define configuration data structures using data classes, manage deprecated configuration fields using warnings, and ensure proper loading of configurations within a context manager. # Task Description: 1. **Define Configuration Data Classes**: You are required to define a configuration data class `AppConfig` with the following fields: - `host` (string): The hostname of the application. - `port` (integer): The port number on which the application runs. - `use_ssl` (boolean): Indicates whether SSL should be used. - `timeout` (integer): Connection timeout in seconds. (This field is deprecated) 2. **Manage Deprecation Warnings**: Implement a function `check_deprecations(config)` to check for deprecated fields in the configuration and issue a warning using the `warnings` module if any deprecated field is used. 3. **Load Configuration Safely**: Implement a context manager `configuration_loader(file_path)` to safely load the configuration from a json file and provide it. Use the `contextlib` module to manage the setup and teardown of this context manager, ensuring resources are properly handled. # Input and Output: - **Input**: - A JSON file containing the configuration details. - **Output**: - An instance of `AppConfig` data class populated with the configuration values. - A warning message if any deprecated fields are detected in the configuration. # Constraints: - You must use the `dataclasses` module to define the `AppConfig`. - You must use the `warnings` module to handle deprecated fields. - You must use the `contextlib` module for the configuration loader context manager. # Example: Given a JSON file `config.json` with the contents: ```json { \\"host\\": \\"localhost\\", \\"port\\": 8080, \\"use_ssl\\": true, \\"timeout\\": 30 } ``` Your program should: 1. Load this configuration within the context manager. 2. Instantiate an `AppConfig` object with these values. 3. Issue a warning for the deprecated `timeout` field. Example Usage: ```python import warnings import json from dataclasses import dataclass from contextlib import contextmanager # Step 1: Define the configuration data class @dataclass class AppConfig: host: str port: int use_ssl: bool timeout: int # Deprecated field # Step 2: Implement the check_deprecations function def check_deprecations(config: AppConfig): if hasattr(config, \'timeout\'): warnings.warn(\\"The \'timeout\' field is deprecated and will be removed in future versions.\\", DeprecationWarning) # Step 3: Implement the configuration loader context manager @contextmanager def configuration_loader(file_path: str): try: with open(file_path, \'r\') as file: data = json.load(file) config = AppConfig(**data) check_deprecations(config) yield config except Exception as e: raise RuntimeError(f\\"Error loading configuration: {e}\\") finally: print(\\"Configuration loading completed.\\") # Usage Example: with configuration_loader(\'config.json\') as config: print(config) ``` # Notes: - Ensure proper usage of the `warnings`, `dataclasses`, and `contextlib` modules. - Your implementation should be robust to handle any potential exceptions during file operations and configuration loading.","solution":"import warnings import json from dataclasses import dataclass from contextlib import contextmanager @dataclass class AppConfig: host: str port: int use_ssl: bool timeout: int # Deprecated field def check_deprecations(config: AppConfig): if hasattr(config, \'timeout\'): warnings.warn(\\"The \'timeout\' field is deprecated and will be removed in future versions.\\", DeprecationWarning) @contextmanager def configuration_loader(file_path: str): A context manager to load configuration from a json file. Args: file_path (str): The path to the configuration json file. try: with open(file_path, \'r\') as file: data = json.load(file) config = AppConfig(**data) check_deprecations(config) yield config except Exception as e: raise RuntimeError(f\\"Error loading configuration: {e}\\") finally: print(\\"Configuration loading completed.\\")"},{"question":"# PyTorch Coding Assessment: Implementing a Custom Layer with Autograd In this exercise, you will implement a custom neural network layer in PyTorch and demonstrate your understanding of autograd by writing a forward and backward pass for the layer. This will involve using the `torch.autograd.Function` class to manually define the forward and backward operations. Task 1. **Define a Custom Layer**: - Create a custom layer called `MyCustomLayer` that inherits from `torch.nn.Module`. - Within this module, use a subclass of `torch.autograd.Function` to custom define the forward and backward operations. 2. **Forward Pass**: - In the forward method of your custom function, perform an arbitrary transformation on the input tensor. For example, you could use an element-wise sine and cosine combination: `output = sin(input) + cos(2 * input)`. 3. **Backward Pass**: - Implement the backward method to compute the gradient of the loss with respect to the input tensor. - Use the chain rule to derive the gradients as needed for the custom transformation. 4. **Usage Example**: - Instantiate your custom layer and use it in a simple neural network. - Verify that the gradients computed during the backward pass are correct by comparing them to numerically estimated gradients using `torch.autograd.gradcheck`. Constraints - Ensure that the implementation uses PyTorch\'s `torch.autograd.Function` for defining the custom forward and backward operations. - The input tensor will have `requires_grad` set to `True`. - Verify the implementation with a gradient check to ensure correctness. Expected Input and Output - **Input**: A tensor of any shape with `requires_grad=True`. - **Output**: A tensor of the same shape after the transformation. Performance Requirements - The backward pass should be implemented efficiently to handle tensors up to reasonable sizes (e.g., batches of size 32 with tensors of shape 3x224x224). Here is a skeleton of the code to get you started: ```python import torch import torch.nn as nn import torch.autograd as autograd class MyCustomFunction(autograd.Function): @staticmethod def forward(ctx, input): ctx.save_for_backward(input) output = torch.sin(input) + torch.cos(2 * input) return output @staticmethod def backward(ctx, grad_output): input, = ctx.saved_tensors grad_input = grad_output * (torch.cos(input) - 2 * torch.sin(2 * input)) return grad_input class MyCustomLayer(nn.Module): def __init__(self): super(MyCustomLayer, self).__init__() def forward(self, input): return MyCustomFunction.apply(input) # Example usage input_tensor = torch.randn(10, requires_grad=True) layer = MyCustomLayer() output = layer(input_tensor) output.sum().backward() # Gradient check from torch.autograd import gradcheck input_for_check = torch.randn(10, requires_grad=True, dtype=torch.float64) test = gradcheck(layer, (input_for_check,), eps=1e-6, atol=1e-4) print(\\"Gradient check passed:\\", test) ```","solution":"import torch import torch.nn as nn import torch.autograd as autograd class MyCustomFunction(autograd.Function): @staticmethod def forward(ctx, input): ctx.save_for_backward(input) output = torch.sin(input) + torch.cos(2 * input) return output @staticmethod def backward(ctx, grad_output): input, = ctx.saved_tensors grad_input = grad_output * (torch.cos(input) - 2 * torch.sin(2 * input)) return grad_input class MyCustomLayer(nn.Module): def __init__(self): super(MyCustomLayer, self).__init__() def forward(self, input): return MyCustomFunction.apply(input) # Example usage input_tensor = torch.randn(10, requires_grad=True) layer = MyCustomLayer() output = layer(input_tensor) output.sum().backward() # Gradient check from torch.autograd import gradcheck input_for_check = torch.randn(10, requires_grad=True, dtype=torch.float64) test = gradcheck(layer, (input_for_check,), eps=1e-6, atol=1e-4) print(\\"Gradient check passed:\\", test)"},{"question":"**Background Info:** In Python, descriptors are a fundamental but advanced tool used to customize the behavior of attribute access. Implementing descriptors involves defining at least one of the methods `__get__`, `__set__`, or `__delete__`. They are powerful for use cases like computed properties, lazy evaluation, or proxies. **Objective:** Write a Python class that demonstrates a custom descriptor for managing attributes. Your descriptor should include both getter and setter methods to control how the attribute is accessed and modified. **Detailed Requirements:** 1. Implement a custom descriptor class `TemperatureDescriptor`. This should manage an attribute representing temperature values in Celsius. 2. The `TemperatureDescriptor` should: * Use a private attribute to store the temperature. * Implement `__get__` to retrieve the temperature in Celsius. * Implement `__set__` to allow modifying the temperature while ensuring the value is within a realistic range (e.g., -273.15 to 1000 degrees Celsius). 3. Create a class `TemperatureDevice` that uses `TemperatureDescriptor` to manage its `temperature` attribute. **Constraints:** - Temperature values must range from -273.15 to 1000 degrees Celsius. - If a value outside this range is assigned, raise a `ValueError` with an appropriate message. - Ensure the descriptor’s documentation methods provide appropriate feedback for debugging and validation. **Example:** ```python class TemperatureDescriptor: # Implement your descriptor here pass class TemperatureDevice: # Use the descriptor for the temperature attribute temperature = TemperatureDescriptor() # Example Usage device = TemperatureDevice() device.temperature = 25 print(device.temperature) # Should output: 25 device.temperature = -300 # Should raise ValueError ``` **Input:** ```python device.temperature = value ``` **Output:** Returns the current temperature. ```python print(device.temperature) # or print(device.temperature) ``` **Performance Consideration:** - Your implementation should efficiently manage temperature values without any significant performance overhead. **Note:** Make sure to provide appropriate docstrings for your descriptor methods to explain their functionality.","solution":"class TemperatureDescriptor: def __init__(self): self._temperature = None def __get__(self, instance, owner): return self._temperature def __set__(self, instance, value): if not (-273.15 <= value <= 1000): raise ValueError(\\"Temperature must be between -273.15 and 1000 degrees Celsius.\\") self._temperature = value class TemperatureDevice: temperature = TemperatureDescriptor()"},{"question":"Problem Statement You are tasked with analyzing two different datasets using seaborn and visualizing their distributions in meaningful ways. Specifically, you need to create several plots to compare how different variables are distributed and how these distributions can be customized for better visual insights. # Datasets 1. The `penguins` dataset provided by seaborn. 2. The `tips` dataset provided by seaborn. # Tasks 1. **Univariate Histograms**: - Create a histogram of the `flipper_length_mm` variable from the `penguins` dataset with default settings. - Adjust the bin width to `5` for the same variable and plot the histogram again. - Overlay a kernel density estimate (KDE) on the histogram for `flipper_length_mm`. - Create a histogram of the `bill_length_mm` variable but separate out the `species` using the `hue` parameter to see the distributions of the different species in the dataset. 2. **Bivariate Histogram**: - Create a bivariate histogram of `bill_depth_mm` and `body_mass_g` variables from the `penguins` dataset. - Add a `hue` parameter to the above plot to differentiate between different species. 3. **Histograms with Categorical Data**: - Create a histogram for the `day` variable in the `tips` dataset. - Add a `hue` parameter to differentiate between different `sex` in the above plot and adjust the bars for better clarity using `dodge` parameter. 4. **Distribution with Log Scale**: - Plot the histogram for the `total_bill` variable from the `tips` dataset. - Apply a log scale to the histogram for better visualization of the distribution. # Requirements - Use seaborn for all plots. - Each plot should have appropriate titles and axis labels to ensure they are self-explanatory. - The plots should be displayed within the same notebook cell using subplots where possible. # Submission Provide a function `create_plots` that generates all the requested plots and displays them using seaborn. Each task should be clearly commented in the code. The function should have the following signature: ```python def create_plots(): pass ``` # Example Output Here is an example of what the plots might look like for one of the tasks: ```python import seaborn as sns import matplotlib.pyplot as plt def create_plots(): # Load datasets penguins = sns.load_dataset(\\"penguins\\") tips = sns.load_dataset(\\"tips\\") # Task 1: Univariate histogram with default settings plt.figure(figsize=(10, 6)) plt.subplot(2,2,1) sns.histplot(data=penguins, x=\\"flipper_length_mm\\") plt.title(\'Flipper Length Distribution (Default)\') # Task 2: Adjusted bin width plt.subplot(2,2,2) sns.histplot(data=penguins, x=\\"flipper_length_mm\\", binwidth=5) plt.title(\'Flipper Length Distribution (Binwidth=5)\') # Overlay a KDE plt.subplot(2,2,3) sns.histplot(data=penguins, x=\\"flipper_length_mm\\", kde=True) plt.title(\'Flipper Length Distribution with KDE\') # Use hue for species plt.subplot(2,2,4) sns.histplot(data=penguins, x=\\"bill_length_mm\\", hue=\\"species\\") plt.title(\'Bill Length Distribution by Species\') # Add more plots for the remaining tasks similarly plt.tight_layout() plt.show() create_plots() ``` _Tip: Make sure your plots are well-organized and all necessary seaborn functionalities are demonstrated._","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_plots(): # Load datasets penguins = sns.load_dataset(\\"penguins\\") tips = sns.load_dataset(\\"tips\\") # Task 1: Univariate Histograms plt.figure(figsize=(14, 10)) # Default histogram plt.subplot(2, 2, 1) sns.histplot(data=penguins, x=\\"flipper_length_mm\\") plt.title(\'Flipper Length Distribution (Default)\') plt.xlabel(\'Flipper Length (mm)\') plt.ylabel(\'Count\') # Bin width adjusted to 5 plt.subplot(2, 2, 2) sns.histplot(data=penguins, x=\\"flipper_length_mm\\", binwidth=5) plt.title(\'Flipper Length Distribution (Binwidth=5)\') plt.xlabel(\'Flipper Length (mm)\') plt.ylabel(\'Count\') # Overlay KDE plt.subplot(2, 2, 3) sns.histplot(data=penguins, x=\\"flipper_length_mm\\", kde=True) plt.title(\'Flipper Length Distribution with KDE\') plt.xlabel(\'Flipper Length (mm)\') plt.ylabel(\'Count\') # Histogram with hue for species plt.subplot(2, 2, 4) sns.histplot(data=penguins, x=\\"bill_length_mm\\", hue=\\"species\\", multiple=\\"stack\\") plt.title(\'Bill Length Distribution by Species\') plt.xlabel(\'Bill Length (mm)\') plt.ylabel(\'Count\') plt.tight_layout() plt.show() # Task 2: Bivariate Histogram plt.figure(figsize=(14, 6)) # Bivariate histogram plt.subplot(1, 2, 1) sns.histplot(data=penguins, x=\\"bill_depth_mm\\", y=\\"body_mass_g\\") plt.title(\'Bivariate Histogram of Bill Depth and Body Mass\') plt.xlabel(\'Bill Depth (mm)\') plt.ylabel(\'Body Mass (g)\') # Bivariate histogram with hue for species plt.subplot(1, 2, 2) sns.histplot(data=penguins, x=\\"bill_depth_mm\\", y=\\"body_mass_g\\", hue=\\"species\\") plt.title(\'Bivariate Histogram of Bill Depth and Body Mass by Species\') plt.xlabel(\'Bill Depth (mm)\') plt.ylabel(\'Body Mass (g)\') plt.tight_layout() plt.show() # Task 3: Histogram with categorical data plt.figure(figsize=(14, 6)) # Default histogram plt.subplot(1, 2, 1) sns.histplot(data=tips, x=\\"day\\") plt.title(\'Day Distribution\') plt.xlabel(\'Day\') plt.ylabel(\'Count\') # Histogram with hue for sex and dodge plt.subplot(1, 2, 2) sns.histplot(data=tips, x=\\"day\\", hue=\\"sex\\", multiple=\\"dodge\\") plt.title(\'Day Distribution by Sex\') plt.xlabel(\'Day\') plt.ylabel(\'Count\') plt.tight_layout() plt.show() # Task 4: Distribution with Log Scale plt.figure(figsize=(8, 6)) # Default histogram sns.histplot(data=tips, x=\\"total_bill\\") plt.yscale(\'log\') plt.title(\'Total Bill Distribution with Log Scale\') plt.xlabel(\'Total Bill\') plt.ylabel(\'Count\') plt.tight_layout() plt.show()"},{"question":"Basic Interactive Command Processor Objective: Implement a basic interactive command processor in Python that supports command history management and tab completion for a set of predefined commands. Task Description: You are required to write a Python class `CommandProcessor` which implements the following features: 1. **Command History Management:** - The class should store all the commands entered in the current session. - Provide a method to retrieve the command history. 2. **Tab Completion:** - Implement tab completion for a predefined set of commands `[\\"help\\", \\"exit\\", \\"list\\", \\"show\\"]`. - The tab completion should suggest possible commands based on the current input. 3. **Interactive Input Handling:** - Continuously prompt the user to enter commands until the user types \\"exit\\". - If an unknown command is entered, display a message \\"Unknown command: <command>\\". Implementation Requirements: 1. Define the class `CommandProcessor` with the following methods: - `def __init__(self):` - Initializes the command processor and the history list. - `def start(self):` - Starts the interactive session for command input. - `def get_history(self) -> list:` - Returns the command history as a list of strings. 2. Implement a function `tab_complete(text, state)` to be used with the GNU Readline library for tab completion. 3. Use the `readline` module for handling input editing and history substitution. Example: ```python processor = CommandProcessor() processor.start() ``` Upon running the example, the user should be able to type commands with tab completion and see the command history when requested. Constraints: - You must use the `readline` library for tab completion. - Assume the user input will always be valid text strings. - You are not required to handle signals or advanced error handling.","solution":"import readline class CommandProcessor: def __init__(self): self.history = [] self.commands = [\\"help\\", \\"exit\\", \\"list\\", \\"show\\"] # Setting up the tab completion readline.parse_and_bind(\\"tab: complete\\") readline.set_completer(self.tab_complete) def start(self): Starts the interactive command input session. while True: try: command = input(\\"> \\") self.history.append(command) if command == \\"exit\\": break elif command in self.commands: if command == \\"help\\": print(\\"Available commands: \\" + \\", \\".join(self.commands)) else: print(f\\"Executing {command} command.\\") else: print(f\\"Unknown command: {command}\\") except EOFError: break def get_history(self): return self.history def tab_complete(self, text, state): Provides tab completion for the commands options = [cmd for cmd in self.commands if cmd.startswith(text)] if state < len(options): return options[state] else: return None"},{"question":"You have been tasked with optimizing a data processing pipeline that handles large arrays of numerical data. The current implementation makes frequent copies of these large arrays, leading to significant memory overhead and reduced performance. To address this, you are required to implement a function `process_large_array` that will leverage Python\'s `memoryview` objects to minimize unnecessary data copying. Your function will take a large list of integers and perform a specified operation on a contiguous memory block of this list without creating additional copies of the data. # Function Signature ```python def process_large_array(data: list[int], start: int, length: int, operation: str) -> None: pass ``` # Input - `data`: A list of integers representing the large array. - `start`: An integer indicating the starting index of the contiguous block. - `length`: An integer indicating the length of the contiguous block. - `operation`: A string specifying the operation to be performed. It can be one of the following: - `\\"increment\\"`: Increment each element in the contiguous block by 1. - `\\"double\\"`: Double each element in the contiguous block. # Output - The function should not return anything. The changes should be made in-place on the input list `data`. # Constraints - You may assume that `start` and `length` are such that the specified block lies entirely within the bounds of the list `data`. - The list `data` can be very large (up to millions of entries). # Example ```python data = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] process_large_array(data, 2, 5, \\"increment\\") print(data) # Output: [1, 2, 4, 5, 6, 7, 8, 8, 9, 10] data = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] process_large_array(data, 0, 3, \\"double\\") print(data) # Output: [2, 4, 6, 4, 5, 6, 7, 8, 9, 10] ``` # Notes - You should not create any additional copies of the data. - Utilize `memoryview` to handle the contiguous memory block. - Assume that the operation string will always be valid.","solution":"def process_large_array(data: list[int], start: int, length: int, operation: str) -> None: Processes a contiguous block of a large list in-place using memoryview to avoid unnecessary copies. Parameters: - data: List[int], the large array of integers to be processed. - start: int, the starting index of the contiguous block. - length: int, the length of the contiguous block. - operation: str, the operation to perform, either \\"increment\\" or \\"double\\". # Create a memoryview of the list mv = memoryview(bytearray(data)) # Define the processing operations operations = { \\"increment\\": lambda x: x + 1, \\"double\\": lambda x: x * 2, } # Process the specified block in-place for i in range(start, start + length): original_value = int(mv[i]) mv[i] = operations[operation](original_value) # Copy the processed values back into data for i in range(start, start + length): data[i] = int(mv[i])"},{"question":"**Question: Library Book Catalog Management** Imagine you are asked to design a simple library book catalog management system. The catalog will store book details like title, author, ISBN, and publication year. The books in the catalog should be saved to a file and should also be readable from a file. Additionally, the books should be listed in a neatly formatted table when the user requests to view the catalog. You need to implement the following functionalities: 1. **Add a New Book:** - Input: Title (string), Author (string), ISBN (string), Publication Year (integer). - Output: None 2. **View Catalog:** - Display all books in the catalog in a neatly formatted table. - Output: None 3. **Save Catalog to File:** - Use JSON format to save the catalog to a file named `catalog.json`. - Output: None 4. **Load Catalog from File:** - Read the catalog from `catalog.json` and update the internal catalog. - Output: None # Implementation Details - You can assume that the books will have unique ISBNs. - Follow these constraints while implementing the formatting and file operations: - Use f-strings for formatting the output table. - Use the `json` module for saving and loading the catalog. Here is a sample structure of the `Book` class and `Library` class you might need to implement: ```python class Book: def __init__(self, title, author, isbn, publication_year): self.title = title self.author = author self.isbn = isbn self.publication_year = publication_year class Library: def __init__(self): self.catalog = [] def add_book(self, title, author, isbn, publication_year): # Implement this method def view_catalog(self): # Implement this method def save_catalog_to_file(self): # Implement this method def load_catalog_from_file(self): # Implement this method ``` # Example Usage: ```python library = Library() library.add_book(\\"Harry Potter and the Philosopher\'s Stone\\", \\"J.K. Rowling\\", \\"9780747532699\\", 1997) library.add_book(\\"To Kill a Mockingbird\\", \\"Harper Lee\\", \\"9780061120084\\", 1960) library.view_catalog() library.save_catalog_to_file() new_library = Library() new_library.load_catalog_from_file() new_library.view_catalog() ``` # Expected Output: - For `view_catalog()` with two books added as in the example: ``` Title Author ISBN Publication Year Harry Potter and the Philosopher\'s Stone J.K. Rowling 9780747532699 1997 To Kill a Mockingbird Harper Lee 9780061120084 1960 ``` **Note:** Your code will be evaluated based on correctness, performance, and adherence to the specified constraints and guidelines.","solution":"import json class Book: def __init__(self, title, author, isbn, publication_year): self.title = title self.author = author self.isbn = isbn self.publication_year = publication_year def to_dict(self): return { \'title\': self.title, \'author\': self.author, \'isbn\': self.isbn, \'publication_year\': self.publication_year } class Library: def __init__(self): self.catalog = [] def add_book(self, title, author, isbn, publication_year): new_book = Book(title, author, isbn, publication_year) self.catalog.append(new_book) def view_catalog(self): print(f\\"{\'Title\':<35} {\'Author\':<20} {\'ISBN\':<20} {\'Publication Year\':<15}\\") for book in self.catalog: print(f\\"{book.title:<35} {book.author:<20} {book.isbn:<20} {book.publication_year:<15}\\") def save_catalog_to_file(self): catalog_data = [book.to_dict() for book in self.catalog] with open(\'catalog.json\', \'w\') as f: json.dump(catalog_data, f) def load_catalog_from_file(self): try: with open(\'catalog.json\', \'r\') as f: catalog_data = json.load(f) self.catalog = [Book(**data) for data in catalog_data] except FileNotFoundError: print(\\"No catalog file found.\\")"},{"question":"# PyTorch MPS Environment Variables Configuration Objective Your task is to write a Python script that demonstrates the configuration and usage of PyTorch\'s MPS environment variables. This script will involve setting these environment variables, performing operations on the MPS backend, and logging the observed effects. Details 1. **Environment Variables:** - `PYTORCH_DEBUG_MPS_ALLOCATOR` - `PYTORCH_MPS_LOG_PROFILE_INFO` - `PYTORCH_MPS_HIGH_WATERMARK_RATIO` - `PYTORCH_MPS_LOW_WATERMARK_RATIO` - `PYTORCH_MPS_FAST_MATH` 2. **Steps:** a. Write a function `set_environment_variables` that takes a dictionary of environment variable names and values, and sets them for the current Python process. b. Write a function `perform_computations` that performs a simple matrix multiplication and returns the result. Ensure that this operation is executed on the MPS backend. c. Set the above environment variables to specific values using the `set_environment_variables` function. d. Call the `perform_computations` function, and ensure it logs the necessary details based on the environment variable settings you have enabled. Specifications - **Function 1: `set_environment_variables(variables: Dict[str, str])`** - Input: Dictionary of environment variable names and their corresponding values. - Output: None (configures the environment variables for the running process). - **Function 2: `perform_computations() -> torch.Tensor`** - Input: None - Output: Result of a matrix multiplication performed on the MPS backend (torch.Tensor). Constraints - Ensure the script works only if an MPS-capable device is available. - If MPS backend is not available, the script should print an appropriate message and exit gracefully. - Use PyTorch for tensor operations. - Log sufficient details to observe the impact of the set environment variables. - You are not required to handle every possible edge case, but your solution should be robust for the expected use case. Sample Code Framework: ```python import os import torch def set_environment_variables(variables): for env_var, value in variables.items(): os.environ[env_var] = value print(f\\"Set {env_var} to {value}\\") def perform_computations(): if not torch.backends.mps.is_available(): print(\\"MPS backend is not available. Exiting.\\") return device = torch.device(\\"mps\\") a = torch.randn(2, 2, device=device) b = torch.randn(2, 2, device=device) result = torch.matmul(a, b) print(\\"Computation Result:\\", result) return result if __name__ == \\"__main__\\": env_vars = { \\"PYTORCH_DEBUG_MPS_ALLOCATOR\\": \\"1\\", \\"PYTORCH_MPS_LOG_PROFILE_INFO\\": \\"1\\", \\"PYTORCH_MPS_HIGH_WATERMARK_RATIO\\": \\"0.9\\", \\"PYTORCH_MPS_LOW_WATERMARK_RATIO\\": \\"0.5\\", \\"PYTORCH_MPS_FAST_MATH\\": \\"1\\" } set_environment_variables(env_vars) perform_computations() ```","solution":"import os import torch def set_environment_variables(variables): Sets the specified environment variables for the current Python process. Parameters: variables (Dict[str, str]): A dictionary where keys are environment variable names and values are their respective values. for env_var, value in variables.items(): os.environ[env_var] = value print(f\\"Set {env_var} to {value}\\") def perform_computations(): Performs a simple matrix multiplication using the MPS backend and logs the result. Returns: torch.Tensor: Result of the matrix multiplication. if not torch.backends.mps.is_available(): print(\\"MPS backend is not available. Exiting.\\") return device = torch.device(\\"mps\\") a = torch.randn(2, 2, device=device) b = torch.randn(2, 2, device=device) result = torch.matmul(a, b) print(\\"Computation Result:\\", result) return result if __name__ == \\"__main__\\": env_vars = { \\"PYTORCH_DEBUG_MPS_ALLOCATOR\\": \\"1\\", \\"PYTORCH_MPS_LOG_PROFILE_INFO\\": \\"1\\", \\"PYTORCH_MPS_HIGH_WATERMARK_RATIO\\": \\"0.9\\", \\"PYTORCH_MPS_LOW_WATERMARK_RATIO\\": \\"0.5\\", \\"PYTORCH_MPS_FAST_MATH\\": \\"1\\" } set_environment_variables(env_vars) perform_computations()"},{"question":"# Coding Assessment: Struct Module in Python Objective: Your task is to implement a function that processes a list of records representing user data. The function should pack these user records into a binary format for storage or network transmission and also provide a way to unpack the binary data back into user records. Problem Statement: You are given a list of user records where each record is a dictionary with the following keys: - `name`: A string representing the user\'s name (maximum 10 characters). - `id`: An integer representing the user\'s ID. - `age`: An integer representing the user\'s age. Each user record should be packed into the following binary format: - `name`: A fixed-length string of 10 characters (padded with nulls if shorter). - `id`: A 32-bit signed integer. - `age`: A 32-bit unsigned integer. Write two functions: 1. `pack_records(records: List[Dict[str, Union[str, int]]]) -> bytes`: This function takes a list of user records and returns a bytes object with the packed binary data. 2. `unpack_records(data: bytes) -> List[Dict[str, Union[str, int]]]`: This function takes a bytes object containing packed binary data and returns a list of user records. Input and Output Formats: - **Input**: For `pack_records()`, a list of dictionaries, each containing keys `name` (str), `id` (int), and `age` (int). - **Output**: For `pack_records()`, a bytes object containing the packed binary data. - **Input**: For `unpack_records()`, a bytes object with binary data. - **Output**: For `unpack_records()`, a list of dictionaries, each containing keys `name` (str), `id` (int), and `age` (int). Constraints: - `name` is a string with a length of at most 10 characters. - `id` is a 32-bit signed integer. - `age` is a 32-bit unsigned integer. - The length of the name string in the binary format should always be 10 bytes, padded with null bytes if necessary. Example: ```python # Example records records = [ {\\"name\\": \\"Alice\\", \\"id\\": 123, \\"age\\": 30}, {\\"name\\": \\"Bob\\", \\"id\\": 456, \\"age\\": 25} ] # Packing records packed_data = pack_records(records) # Returns a bytes object # Unpacking records unpacked_records = unpack_records(packed_data) # Returns the original list of records ``` Additional Notes: - Make sure to handle padding and byte order correctly. - Use the `struct` module for packing and unpacking operations. - Raise appropriate errors if the input data does not conform to the expected formats. Implementation: ```python import struct from typing import List, Dict, Union def pack_records(records: List[Dict[str, Union[str, int]]]) -> bytes: result = [] for record in records: name = record[\'name\'].encode(\'utf-8\') name = name.ljust(10, b\'x00\') # Pad name to 10 bytes id_ = record[\'id\'] age = record[\'age\'] packed_data = struct.pack(\'10sii\', name, id_, age) result.append(packed_data) return b\'\'.join(result) def unpack_records(data: bytes) -> List[Dict[str, Union[str, int]]]: records = [] struct_size = struct.calcsize(\'10sii\') for i in range(0, len(data), struct_size): # Unpack a single record name, id_, age = struct.unpack(\'10sii\', data[i:i + struct_size]) # Decode the name and strip trailing null bytes name = name.decode(\'utf-8\').rstrip(\'x00\') records.append({\\"name\\": name, \\"id\\": id_, \\"age\\": age}) return records # Test Cases if __name__ == \\"__main__\\": records = [ {\\"name\\": \\"Alice\\", \\"id\\": 123, \\"age\\": 30}, {\\"name\\": \\"Bob\\", \\"id\\": 456, \\"age\\": 25} ] packed_data = pack_records(records) print(packed_data) unpacked_records = unpack_records(packed_data) print(unpacked_records) ```","solution":"import struct from typing import List, Dict, Union def pack_records(records: List[Dict[str, Union[str, int]]]) -> bytes: result = [] for record in records: name = record[\'name\'].encode(\'utf-8\') name = name.ljust(10, b\'x00\') # Pad name to 10 bytes id_ = record[\'id\'] age = record[\'age\'] packed_data = struct.pack(\'10sii\', name, id_, age) result.append(packed_data) return b\'\'.join(result) def unpack_records(data: bytes) -> List[Dict[str, Union[str, int]]]: records = [] struct_size = struct.calcsize(\'10sii\') for i in range(0, len(data), struct_size): # Unpack a single record name, id_, age = struct.unpack(\'10sii\', data[i:i + struct_size]) # Decode the name and strip trailing null bytes name = name.decode(\'utf-8\').rstrip(\'x00\') records.append({\\"name\\": name, \\"id\\": id_, \\"age\\": age}) return records"},{"question":"**Objective:** Demonstrate your understanding of seaborn\'s scatter plot functionality by creating a series of customized plots using a provided dataset. **Question:** You are given the \\"tips\\" dataset commonly used in seaborn examples. Your task is to create a function `create_custom_scatter_plots` that generates and saves several scatter plots based on different requirements. Requirements: 1. **Load the Dataset**: Load the \\"tips\\" dataset from seaborn. 2. **Basic Scatter Plot**: Create a scatter plot of `total_bill` vs `tip`. 3. **Scatter Plot with Hue**: Create a scatter plot of `total_bill` vs `tip` with the `hue` parameter set to `time`. 4. **Scatter Plot with Style**: Create a scatter plot of `total_bill` vs `tip` with both the `hue` and `style` parameters set to `day`. 5. **Scatter Plot with Size**: Create a scatter plot of `total_bill` vs `tip` where the `hue` parameter is set to `size` and the `size` parameter is also set to `size`. Control the range of marker areas with `sizes=(20, 200)` and set `legend=\\"full\\"`. 6. **Relational Plot (relplot)**: Create a relational plot using `relplot`. The plot should have `total_bill` vs `tip` with subplots divided by `time`, with `hue` and `style` set to `day`. Each plot should be saved as a PNG file in the current directory with filenames: - `scatter_basic.png` - `scatter_hue_time.png` - `scatter_hue_style_day.png` - `scatter_size_size.png` - `relplot_subplots.png` **Function Signature:** ```python import seaborn as sns import pandas as pd import matplotlib.pyplot as plt def create_custom_scatter_plots(): # Load the dataset tips = sns.load_dataset(\\"tips\\") # Basic Scatter Plot plt.figure() sns.scatterplot(data=tips, x=\\"total_bill\\", y=\\"tip\\") plt.savefig(\\"scatter_basic.png\\") plt.close() # Scatter Plot with Hue plt.figure() sns.scatterplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"time\\") plt.savefig(\\"scatter_hue_time.png\\") plt.close() # Scatter Plot with Style plt.figure() sns.scatterplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"day\\", style=\\"day\\") plt.savefig(\\"scatter_hue_style_day.png\\") plt.close() # Scatter Plot with Size plt.figure() sns.scatterplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"size\\", size=\\"size\\", sizes=(20, 200), legend=\\"full\\") plt.savefig(\\"scatter_size_size.png\\") plt.close() # Relational Plot (relplot) sns.relplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", col=\\"time\\", hue=\\"day\\", style=\\"day\\", kind=\\"scatter\\") plt.savefig(\\"relplot_subplots.png\\") plt.close() ``` **Constraints:** - Ensure all plots are saved correctly in the given filenames. - Use seaborn version 0.11.1 or higher. Submit your function implementation along with the generated plots.","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt def create_custom_scatter_plots(): # Load the dataset tips = sns.load_dataset(\\"tips\\") # Basic Scatter Plot plt.figure() sns.scatterplot(data=tips, x=\\"total_bill\\", y=\\"tip\\") plt.savefig(\\"scatter_basic.png\\") plt.close() # Scatter Plot with Hue plt.figure() sns.scatterplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"time\\") plt.savefig(\\"scatter_hue_time.png\\") plt.close() # Scatter Plot with Style plt.figure() sns.scatterplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"day\\", style=\\"day\\") plt.savefig(\\"scatter_hue_style_day.png\\") plt.close() # Scatter Plot with Size plt.figure() sns.scatterplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"size\\", size=\\"size\\", sizes=(20, 200), legend=\\"full\\") plt.savefig(\\"scatter_size_size.png\\") plt.close() # Relational Plot (relplot) sns.relplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", col=\\"time\\", hue=\\"day\\", style=\\"day\\", kind=\\"scatter\\") plt.savefig(\\"relplot_subplots.png\\") plt.close()"},{"question":"**Coding Assessment Question** # Objective Demonstrate your understanding of multimedia handling and color system conversions using the `wave` and `colorsys` modules in Python. # Problem Statement You are provided with a WAV file that contains a simple colored noise. Your task is to read this WAV file, manipulate its data by reducing the noise levels, and convert the predominant color of the noise to a different color system. Finally, write the modified audio data into a new WAV file. # Instructions 1. **Reading WAV File:** - Use the `wave` module to read the given WAV file named `input.wav`. - Extract the audio frames and parameters such as the number of channels and frame rate. 2. **Noise Reduction:** - Implement a basic noise reduction algorithm by scaling down the amplitude of the audio frames. For simplicity, reduce the amplitude of all frames by half. 3. **Color Conversion:** - Assume the noise color is represented in RGB by `(R, G, B) = (255, 0, 0)`. Convert this RGB value to another color space using the `colorsys` module. Specifically, convert it to the HSV (Hue, Saturation, Value) color space. 4. **Writing WAV File:** - Write the manipulated audio frames back into a new WAV file named `output.wav` with the same parameters extracted initially. # Function Signature ```python def process_audio(input_file: str, output_file: str) -> tuple: pass ``` # Input - `input_file` (str): The name of the input WAV file (e.g., `\\"input.wav\\"`). - `output_file` (str): The name for the output WAV file to be created (e.g., `\\"output.wav\\"`). # Output - A tuple containing the HSV representation of the predominant noise color as `(H, S, V)` where: - `H` (float): Hue in the range [0, 1]. - `S` (float): Saturation in the range [0, 1]. - `V` (float): Value in the range [0, 1]. # Constraints - The function should handle standard WAV file formats. - Ensure that the output WAV file retains the same format parameters as the input file (e.g., sample width, frame rate, number of channels). # Example ```python result = process_audio(\\"input.wav\\", \\"output.wav\\") print(result) # Output: (0.0, 1.0, 1.0) # Example HSV output for RGB (255, 0, 0) ``` # Additional Notes - Make sure to handle necessary imports from the `wave` and `colorsys` modules. - Consider edge cases such as empty or corrupted WAV files and handle them appropriately with exceptions.","solution":"import wave import colorsys def process_audio(input_file: str, output_file: str) -> tuple: try: # Reading the WAV file with wave.open(input_file, \'rb\') as wf: params = wf.getparams() n_channels, sampwidth, framerate, n_frames = params[:4] audio_frames = wf.readframes(n_frames) # Noise Reduction: Reducing the amplitude of all frames by half new_audio_frames = bytearray() for i in range(0, len(audio_frames), 2): frame_value = int.from_bytes(audio_frames[i:i+2], byteorder=\'little\', signed=True) new_value = int(frame_value / 2) new_audio_frames.extend(new_value.to_bytes(2, byteorder=\'little\', signed=True)) # Writing the modified frames into a new WAV file with wave.open(output_file, \'wb\') as wf: wf.setparams((n_channels, sampwidth, framerate, n_frames, params[4], params[5])) wf.writeframes(new_audio_frames) # Convert RGB (255, 0, 0) to HSV R, G, B = 255, 0, 0 H, S, V = colorsys.rgb_to_hsv(R / 255.0, G / 255.0, B / 255.0) return (H, S, V) except Exception as e: print(f\\"Error processing audio file: {e}\\") return (None, None, None)"},{"question":"# Seaborn Advanced Plotting Task Objective Design a solution to visualize a dataset with advanced plotting techniques using Seaborn\'s `relplot`. Problem Statement You are given a dataset of flight information and want to analyze the trend of passenger numbers over the years. Using the Seaborn library, you need to create a line plot that shows the relationship between the `year` and `passengers`, and further facet this line plot by `month` to show a separate line for each month. Additionally, include the following requirements: 1. Use different colors for each line representing each month. 2. Configure the plot to have a suitable aspect ratio and size for readability. 3. Ensure the plot has appropriate axis labels and a title for clarity. 4. Save the figure as a PNG file named `flights_trend.png`. # Input - No external input required. # Output - A PNG file named `flights_trend.png` containing the expected line plot. # Constraints - You must use Seaborn\'s `relplot` function. - Handle the `flights` dataset included in Seaborn. # Example Code Setup ```python import seaborn as sns import matplotlib.pyplot as plt # Load the flights dataset flights = sns.load_dataset(\\"flights\\") # Transform the dataset into the required format flights_wide = flights.pivot(index=\\"year\\", columns=\\"month\\", values=\\"passengers\\") # Implement your solution below # --- ``` # Solution Template ```python import seaborn as sns import matplotlib.pyplot as plt # Load the flights dataset flights = sns.load_dataset(\\"flights\\") # Transform the dataset into the required format flights_wide = flights.pivot(index=\\"year\\", columns=\\"month\\", values=\\"passengers\\") # Create and configure the line plot sns.set_theme(style=\\"darkgrid\\") g = sns.relplot( data=flights_wide, kind=\\"line\\", palette=\\"tab10\\", height=6, aspect=1.5 ) # Set additional plot parameters g.set_axis_labels(\\"Year\\", \\"Number of Passengers\\") g.fig.suptitle(\\"Trend of Passengers Over Years by Month\\") # Save the plot to a file g.savefig(\\"flights_trend.png\\") ``` # Submission Ensure your code follows good coding practices, includes necessary comments, and outputs the required image file correctly.","solution":"import seaborn as sns import matplotlib.pyplot as plt # Load the flights dataset flights = sns.load_dataset(\\"flights\\") # Create and configure the line plot sns.set_theme(style=\\"darkgrid\\") # Define the plot g = sns.relplot( data=flights, x=\\"year\\", y=\\"passengers\\", hue=\\"month\\", kind=\\"line\\", palette=\\"tab10\\", height=6, aspect=1.5 ) # Set additional plot parameters g.set_axis_labels(\\"Year\\", \\"Number of Passengers\\") g.fig.suptitle(\\"Trend of Passengers Over Years by Month\\") # Save the plot to a file g.savefig(\\"flights_trend.png\\")"},{"question":"# Kernel Approximation with Nystroem and RBFSampler Objective: Implement a kernel approximation using both the Nystroem method and the RBFSampler from `sklearn.kernel_approximation`. You will compare their performances on a classification task using a synthetic dataset. Task: 1. **Data Generation:** - Generate a synthetic dataset using `make_classification` from the `sklearn.datasets` module with 2 classes, 1000 samples, and 20 features. 2. **Kernel Approximation with Nystroem:** - Use the `Nystroem` class to approximate the RBF kernel. - Set the number of components (`n_components`) to 100. - Transform the synthetic dataset using the Nystroem transformer. 3. **Kernel Approximation with RBFSampler:** - Use the `RBFSampler` class to approximate the RBF kernel. - Set the number of components (`n_components`) to 100 and `gamma` to 1.0. - Transform the synthetic dataset using the RBFSampler transformer. 4. **Classifier Training and Evaluation:** - For both the Nystroem and RBFSampler transformations: - Train an `SGDClassifier` from `sklearn.linear_model`. - Use default parameters for the classifier. - Evaluate the classifier performance using 5-fold cross-validation. - Print the average accuracy for each method. Input Format: None. The data generation is part of the task. Output Format: Print the average accuracy of the classifier for both the Nystroem and RBFSampler transformations as follows: ``` Nystroem Accuracy: <average_accuracy_nystroem> RBFSampler Accuracy: <average_accuracy_rbfsampler> ``` Constraints: 1. Use `random_state=42` for generating the dataset and any other random number generation to ensure reproducibility. 2. The implementation should leverage scikit-learn\'s classes and methods wherever applicable. Example Usage: You do not need to run the code, only implement the function. Below is an example of how your function may be used. This is provided for your understanding and does not need to be included in your submission. ```python def kernel_approximation_comparison(): # Your code implementation pass # Calling the function to execute the task kernel_approximation_comparison() ```","solution":"from sklearn.datasets import make_classification from sklearn.kernel_approximation import Nystroem, RBFSampler from sklearn.linear_model import SGDClassifier from sklearn.model_selection import cross_val_score import numpy as np def kernel_approximation_comparison(): # Generate synthetic dataset X, y = make_classification(n_samples=1000, n_features=20, n_informative=10, n_clusters_per_class=1, random_state=42) # Kernel Approximation with Nystroem nystroem = Nystroem(n_components=100, random_state=42) X_nys = nystroem.fit_transform(X) # Kernel Approximation with RBFSampler rbf_sampler = RBFSampler(n_components=100, gamma=1.0, random_state=42) X_rbf = rbf_sampler.fit_transform(X) # Classifier and Evaluation classifier_nys = SGDClassifier(random_state=42) classifier_rbf = SGDClassifier(random_state=42) scores_nys = cross_val_score(classifier_nys, X_nys, y, cv=5) scores_rbf = cross_val_score(classifier_rbf, X_rbf, y, cv=5) avg_accuracy_nystroem = np.mean(scores_nys) avg_accuracy_rbfsampler = np.mean(scores_rbf) print(f\\"Nystroem Accuracy: {avg_accuracy_nystroem}\\") print(f\\"RBFSampler Accuracy: {avg_accuracy_rbfsampler}\\")"},{"question":"Objective: Design and implement a function using scikit-learn to perform k-fold cross-validation on a dataset and compute multiple metrics. Your solution should use the `cross_validate` function to evaluate the specified metrics and return a summary of the results. Problem Statement: Write a function `evaluate_model_performance` that accepts the following parameters: - A dataset `X` (features) as a 2D numpy array. - The corresponding labels `y` as a 1D numpy array. - An estimator `estimator` (such as a classifier or regressor) - The number of folds `k` for k-fold cross-validation - A list of metrics `metrics` to evaluate, such as [\'accuracy\', \'precision_macro\', \'recall_macro\'] The function should perform k-fold cross-validation using the specified estimator and metrics. It should return a dictionary with the mean and standard deviation for each metric across the folds. Function Signature: ```python def evaluate_model_performance(X: np.ndarray, y: np.ndarray, estimator, k: int, metrics: list) -> dict: pass ``` Input: - `X`: 2D numpy array of shape (n_samples, n_features) - `y`: 1D numpy array of shape (n_samples,) - `estimator`: A scikit-learn estimator (e.g., `svm.SVC(kernel=\'linear\', C=1)`) - `k`: An integer representing the number of folds for k-fold cross-validation - `metrics`: A list of strings representing the metrics to evaluate (e.g., [\'accuracy\', \'precision_macro\', \'recall_macro\']) Output: - A dictionary with keys as metric names and values as tuples containing mean and standard deviation of the scores across the k-folds. For example: ```python { \'accuracy\': (mean_accuracy, std_accuracy), \'precision_macro\': (mean_precision, std_precision), \'recall_macro\': (mean_recall, std_recall) } ``` Constraints: - Use `cross_validate` from `sklearn.model_selection` for performing the cross-validation. - Handle cases where metrics might not be applicable to the estimator gracefully. Example Usage: ```python import numpy as np from sklearn import datasets from sklearn.svm import SVC # Load data X, y = datasets.load_iris(return_X_y=True) # Define the estimator estimator = SVC(kernel=\'linear\', C=1) # Define metrics metrics = [\'accuracy\', \'precision_macro\', \'recall_macro\'] # Number of folds k = 5 # Call the function results = evaluate_model_performance(X, y, estimator, k, metrics) print(results) ``` Expected output should be something like: ```python { \'accuracy\': (0.98, 0.02), \'precision_macro\': (0.98, 0.03), \'recall_macro\': (0.97, 0.04) } ``` Note: - Make sure to import necessary libraries and handle any potential import errors. - Ensure the function is well-tested with different datasets and estimators to validate its robustness.","solution":"from sklearn.model_selection import cross_validate import numpy as np def evaluate_model_performance(X: np.ndarray, y: np.ndarray, estimator, k: int, metrics: list) -> dict: Evaluates the model performance using k-fold cross-validation and returns the mean and standard deviation of the specified metrics. Parameters: X : np.ndarray The feature dataset. y : np.ndarray The labels. estimator : object The scikit-learn estimator (e.g., classifier or regressor). k : int The number of folds for k-fold cross-validation. metrics : list A list of strings representing the metrics to evaluate. Returns: dict A dictionary with metric names as keys and tuples of (mean, std) as values. cv_results = cross_validate(estimator, X, y, cv=k, scoring=metrics, return_train_score=False) summary = {} for metric in metrics: test_scores = cv_results[f\\"test_{metric}\\"] summary[metric] = (test_scores.mean(), test_scores.std()) return summary"},{"question":"**Question:** You need to implement a function `parse_and_sum_numbers` that will accept a tuple of arguments in the form of both positional and keyword arguments. The function will parse the arguments, ensuring they are valid integers, and then return their sum. **Function Signature:** ```python def parse_and_sum_numbers(*args, **kwargs) -> int: ``` # Input: - Positional arguments (`*args`): This will be a variable number of integers. - Keyword arguments (`**kwargs`): This will be a variable number of named integer arguments. # Output: - The function should return the sum of all integer arguments, both positional and keyword. **Requirements:** 1. You need to handle both positional and keyword arguments. 2. Arguments must be parsed and validated to ensure they are integers. If any argument is not an integer, raise a `ValueError` with a message \\"Invalid argument type\\". 3. Sum all valid integer arguments and return the result. # Constraints: - Assume all integers are within the range of valid Python integers. - Non-integer entries should be caught and reported as specified. # Example: ```python # Example 1 print(parse_and_sum_numbers(1, 2, 3, a=4, b=5)) # Output: 15 # Example 2 print(parse_and_sum_numbers(7, -2, p=3, q=2)) # Output: 10 # Example 3 (with invalid argument) print(parse_and_sum_numbers(1, \'x\', z=2)) # Output: Raises `ValueError`: \\"Invalid argument type\\" ``` Implement the function `parse_and_sum_numbers` according to the described requirements.","solution":"def parse_and_sum_numbers(*args, **kwargs) -> int: Parses positional and keyword arguments, ensuring they are valid integers, and returns their sum. Raises ValueError for any non-integer arguments. total_sum = 0 # Validate positional arguments for arg in args: if not isinstance(arg, int): raise ValueError(\\"Invalid argument type\\") total_sum += arg # Validate keyword arguments for key, value in kwargs.items(): if not isinstance(value, int): raise ValueError(\\"Invalid argument type\\") total_sum += value return total_sum"},{"question":"# Email Handling and Exception Management in Python **Objective**: Demonstrate your understanding of the `email.errors` module and exception handling in Python by implementing a function to parse an email message, identify exceptions, and log any defects. **Problem Statement**: You need to write a function `process_email(email_message: str) -> str` that processes an input email message string. This function should: 1. **Parse the email message** using the `email` package. 2. **Handle exceptions** that may arise from parsing, such as: - `MessageError` - `MessageParseError` - `HeaderParseError` - `MultipartConversionError` - `HeaderWriteError` 3. **Identify and return a string log** of any defects found in the email message. The log should contain lines detailing any exceptions raised and a list of defects if found in the parsed message. If no exceptions or defects are found, the log should indicate that the email is valid. **Function Signature**: ```python def process_email(email_message: str) -> str: pass ``` **Input**: - `email_message` (str): A string representation of the email message. **Output**: - (str): A log string indicating exceptions and defects found in the email message. **Constraints**: - The function should correctly handle emails of varying formats and complexities. - Defects should be reported as specified in the `email.errors` documentation. **Example**: ```python email_msg = \'\'\'From: example@example.com To: recipient@example.com Subject: Test Email This is a test email message.\'\'\' print(process_email(email_msg)) ``` **Expected Output** (example): ``` Email is valid. ``` In the case of an email with defects: ```python email_msg = \'\'\'From recipients@example.com Subject Test Email Without Colon This is an invalid email message.\'\'\' print(process_email(email_msg)) ``` **Expected Output** (example): ``` Exception: MissingHeaderBodySeparatorDefect found in email headers. ``` Ensure your function is robust and can handle real-world email parsing scenarios.","solution":"import email from email import policy from email.parser import BytesParser def process_email(email_message: str) -> str: Process an email message string, handle exceptions and log any defects found. Parameters: email_message (str): A string representation of the email message. Returns: str: A log string indicating exceptions and defects found in the email message. log = [] try: # Parse the email message email_bytes = email_message.encode(\'utf-8\') msg = BytesParser(policy=policy.default).parsebytes(email_bytes) # Check for defects if msg.defects: log.append(\\"Defects found:\\") for defect in msg.defects: log.append(f\\" - {defect.__class__.__name__}: {defect}\\") else: return \\"Email is valid.\\" except email.errors.MessageError as e: log.append(f\\"MessageError: {e}\\") except email.errors.MessageParseError as e: log.append(f\\"MessageParseError: {e}\\") except email.errors.HeaderParseError as e: log.append(f\\"HeaderParseError: {e}\\") except email.errors.MultipartConversionError as e: log.append(f\\"MultipartConversionError: {e}\\") except email.errors.HeaderWriteError as e: log.append(f\\"HeaderWriteError: {e}\\") return \'n\'.join(log)"},{"question":"Objective You are required to demonstrate your understanding of seaborn, particularly its ability to handle different data structures, and your ability to generate meaningful visualizations. Question # Scenario You are given a dataset containing information about daily weather observations over several years at multiple weather stations. Your task is to perform the following: 1. Transform the dataset to both long-form and wide-form data. 2. Generate different visualizations using seaborn that demonstrate the trends and patterns within the dataset. 3. Utilize various seaborn functions and configurations to create insightful and easy-to-understand plots. # Dataset Description The dataset `weather.csv` contains the following columns: - `date`: The date of the observation. - `station`: The weather station where the observation was made. - `temperature`: The recorded temperature on that date. - `precipitation`: The recorded precipitation on that date. - `wind`: The recorded wind speed on that date. # Requirements Part 1: Data Transformation 1. Load the dataset into a pandas DataFrame. 2. Transform the dataset into a long-form data structure suitable for seaborn. 3. Transform the dataset into a wide-form data structure suitable for seaborn. Part 2: Data Visualization 1. Create a line plot using long-form data that shows the trend of average daily temperature over the years, differentiating between different weather stations using the `hue` parameter. 2. Create a line plot using wide-form data that shows the wind speed trends over the years for each weather station. 3. Create a box plot that visualizes the distribution of precipitation across different months of the year. Part 3: Advanced Visualization 1. Create a facet grid of scatter plots to show the relationship between temperature and wind speed at different weather stations. Use different colors for each year. 2. Create a heatmap showing the correlation between temperature, precipitation, and wind speed for each weather station. # Constraints and Limitations - Ensure that your transformations and visualizations handle missing or malformed data. - Perform all visualizations using seaborn and ensure they are properly labeled. - The final plots should be well-annotated, highlighting any significant patterns or insights derived from the data. # Input and Output Format Input - `weather.csv` (To be loaded as a pandas DataFrame) Output - Multiple seaborn visualizations as stated in the requirements. # Example ```python import pandas as pd import seaborn as sns import matplotlib.pyplot as plt # Load the dataset weather = pd.read_csv(\'weather.csv\') # Part 1: Data Transformation # (Implement necessary transformations here and store in variables long_form_df and wide_form_df) # Part 2: Data Visualization # 1. Line plot showing average temperature trends over the years sns.relplot(data=long_form_df, x=\'date\', y=\'temperature\', hue=\'station\', kind=\'line\') plt.title(\'Average Daily Temperature Trend Over Years\') plt.show() # 2. Line plot showing wind speed trends for each weather station in wide-form sns.relplot(data=wide_form_df, kind=\'line\') plt.title(\'Wind Speed Trends Over Years for Each Weather Station\') plt.show() # 3. Box plot showing distribution of precipitation across different months # (Ensure the month is extracted from the date column before plotting) # (Implement here) # Part 3: Advanced Visualization # 1. Facet grid of scatter plots for temperature vs wind speed # (Implement here) # 2. Heatmap showing correlation between temperature, precipitation, and wind speed # (Implement here) ``` You are expected to complete the implementation for each part of the requirements as indicated in the example.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def load_and_transform_data(file_path): # Load the dataset into a pandas DataFrame weather = pd.read_csv(file_path, parse_dates=[\'date\']) # Part 1: Data Transformation # Transform into long-form data suitable for seaborn long_form_df = pd.melt(weather, id_vars=[\'date\', \'station\'], var_name=\'variable\', value_name=\'value\') # Transform into wide-form data suitable for seaborn wide_form_df = weather.pivot_table(index=\'date\', columns=\'station\', values=[\'temperature\', \'precipitation\', \'wind\']) return weather, long_form_df, wide_form_df def plot_temperature_trend(long_form_df): # Filter long_form_df for \'temperature\' variable temp_df = long_form_df[long_form_df[\'variable\'] == \'temperature\'] # Create a line plot for average daily temperature trend over the years sns.lineplot(data=temp_df, x=\'date\', y=\'value\', hue=\'station\') plt.title(\'Average Daily Temperature Trend Over Years\') plt.xlabel(\'Date\') plt.ylabel(\'Temperature\') plt.show() def plot_wind_trend(wide_form_df): # Create a line plot for wind speed trends over the years for each weather station sns.lineplot(data=wide_form_df[\'wind\']) plt.title(\'Wind Speed Trends Over Years for Each Weather Station\') plt.xlabel(\'Date\') plt.ylabel(\'Wind Speed\') plt.show() def plot_precipitation_distribution(weather): # Extract month from the date weather[\'month\'] = weather[\'date\'].dt.month # Create a box plot for the distribution of precipitation across different months sns.boxplot(data=weather, x=\'month\', y=\'precipitation\') plt.title(\'Distribution of Precipitation Across Different Months\') plt.xlabel(\'Month\') plt.ylabel(\'Precipitation\') plt.show() def plot_temperature_wind_scatter(weather): # Create a facet grid of scatter plots for temperature vs wind speed at different stations g = sns.FacetGrid(weather, col=\'station\', hue=\'date\', col_wrap=2) g.map(sns.scatterplot, \'temperature\', \'wind\').add_legend() g.set_axis_labels(\'Temperature\', \'Wind Speed\') g.fig.suptitle(\'Temperature vs Wind Speed Across Different Stations\', y=1.05) plt.show() def plot_correlation_heatmap(weather): # Create a heatmap showing the correlation for station in weather[\'station\'].unique(): station_data = weather[weather[\'station\'] == station] corr = station_data[[\'temperature\', \'precipitation\', \'wind\']].corr() sns.heatmap(corr, annot=True, cmap=\'coolwarm\') plt.title(f\'Correlation Heatmap for Station: {station}\') plt.show()"},{"question":"# Python Coding Assessment Question Objective You are required to create a Python function to compile multiple source files into byte-code files using the `py_compile` module. Your function should handle errors appropriately and use different invalidation modes based on user preference. Function Specification **Function Name**: `compile_python_files` **Parameters**: 1. `source_files` (List[str]): A list of paths to Python source files that need to be compiled. 2. `output_directory` (str): The directory where the compiled byte-code files should be saved. 3. `invalidation_mode` (str): A string representing the invalidation mode which can be \\"TIMESTAMP\\", \\"CHECKED_HASH\\", or \\"UNCHECKED_HASH\\". Defaults to \\"TIMESTAMP\\". 4. `quiet` (int): An integer to determine the level of error suppression (0, 1, or 2). Defaults to 0. **Returns**: - A list of paths to the successfully compiled byte-code files. **Exceptions**: - Raise a `ValueError` if an invalid `invalidation_mode` is provided. - Return an empty list if none of the files could be compiled due to any error. Constraints - If the `output_directory` does not exist, your function should create it. - Handle file system errors gracefully and report via standard error (`sys.stderr`) without raising exceptions to the calling code. Example Usage ```python try: compiled_files = compile_python_files( source_files=[\\"/path/to/source1.py\\", \\"/path/to/source2.py\\"], output_directory=\\"/path/to/output\\", invalidation_mode=\\"CHECKED_HASH\\", quiet=1 ) print(f\\"Compiled files: {compiled_files}\\") except ValueError as ve: print(f\\"Error: {ve}\\") ``` In this example, the `compile_python_files` function attempts to compile two source files into the specified output directory using the `CHECKED_HASH` invalidation mode with error messages suppressed to a minimum. Notes - Use the `py_compile.compile` function from the `py_compile` module for compilation. - Ensure that the compiled byte-code files in the output directory follow the required naming convention. - Think about edge cases such as missing source files or writing permissions issues in the output directory.","solution":"import py_compile import os import sys from typing import List def compile_python_files(source_files: List[str], output_directory: str, invalidation_mode: str = \\"TIMESTAMP\\", quiet: int = 0) -> List[str]: Compiles multiple Python source files into byte-code files using the specified invalidation mode. Params: - source_files (List[str]): List of paths to source files. - output_directory (str): Directory where compiled byte-code files should be saved. - invalidation_mode (str): Invalidation mode can be \\"TIMESTAMP\\", \\"CHECKED_HASH\\", or \\"UNCHECKED_HASH\\". - quiet (int): Level of error suppression (0, 1, or 2). Default is 0. Returns: - List of paths to successfully compiled byte-code files. Raises: - ValueError: If an invalid invalidation_mode is provided. if invalidation_mode not in [\\"TIMESTAMP\\", \\"CHECKED_HASH\\", \\"UNCHECKED_HASH\\"]: raise ValueError(\\"Invalid invalidation_mode provided.\\") invalidation_modes = { \\"TIMESTAMP\\": py_compile.PycInvalidationMode.TIMESTAMP, \\"CHECKED_HASH\\": py_compile.PycInvalidationMode.CHECKED_HASH, \\"UNCHECKED_HASH\\": py_compile.PycInvalidationMode.UNCHECKED_HASH } if not os.path.exists(output_directory): os.makedirs(output_directory) compiled_files = [] for source_file in source_files: try: compiled_path = py_compile.compile( source_file, cfile=os.path.join(output_directory, os.path.basename(source_file) + \'c\'), doraise=True, invalidation_mode=invalidation_modes[invalidation_mode], quiet=quiet ) compiled_files.append(compiled_path) except (py_compile.PyCompileError, OSError) as e: if quiet == 0: print(f\\"Error compiling {source_file}: {e}\\", file=sys.stderr) return compiled_files"},{"question":"# Python File and Directory Manipulation Problem Statement You are required to write a script that mimics part of the functionality of the `sdist` command family found in the `setuptools` library. Specifically, you will implement Python functions to include or exclude files in a given directory based on Unix-style glob patterns. Function Specifications: 1. **`include_files(directory: str, patterns: List[str]) -> List[str]`** - This function should include all files under the specified `directory` that match any of the given `patterns`. - **Input**: - `directory`: A string representing the path to the directory. - `patterns`: A list of Unix-style glob patterns. - **Output**: - A list of strings representing the paths to the included files. 2. **`exclude_files(directory: str, patterns: List[str]) -> List[str]`** - This function should exclude all files under the specified `directory` that match any of the given `patterns`. - **Input**: - `directory`: A string representing the path to the directory. - `patterns`: A list of Unix-style glob patterns. - **Output**: - A list of strings representing the paths to the files that are included after exclusion. Constraints: - The functions should handle both relative and absolute paths. - The functions should work on both Unix and Windows systems. - You are not allowed to use external libraries like `fnmatch`. - Ensure that the functions handle edge cases, such as no matching files or empty directories. Example Usage: ```python # Assume the following directory structure: # test_dir # ├── data # │ ├── data1.txt # │ ├── data2.json # ├── scripts # │ ├── script1.py # │ ├── script2.sh # └── README.md include_patterns = [\\"*.txt\\", \\"*.py\\"] exclude_patterns = [\\"*.json\\", \\"*.sh\\"] included_files = include_files(\'test_dir\', include_patterns) # Output: [\'test_dir/data/data1.txt\', \'test_dir/scripts/script1.py\'] excluded_files = exclude_files(\'test_dir\', exclude_patterns) # Output: [\'test_dir/data/data1.txt\', \'test_dir/README.md\'] print(included_files) print(excluded_files) ``` Notes: - Pay attention to the structure of the paths returned. - The order of the files in the output does not matter. Implement the functions to solve the problem. Good luck!","solution":"import os import fnmatch def include_files(directory: str, patterns: list) -> list: included_files = [] for root, dirs, files in os.walk(directory): for pattern in patterns: for filename in fnmatch.filter(files, pattern): included_files.append(os.path.join(root, filename)) return included_files def exclude_files(directory: str, patterns: list) -> list: all_files = [] for root, dirs, files in os.walk(directory): for filename in files: all_files.append(os.path.join(root, filename)) excluded_files = all_files.copy() for pattern in patterns: for filepath in all_files: if fnmatch.fnmatch(filepath, os.path.join(\\"*\\", pattern)): excluded_files.remove(filepath) return excluded_files"},{"question":"**Objective**: Demonstrate your understanding of file encoding, decoding, and error handling by working with the `binhex` module. # Problem Statement You are required to write two functions, `encode_to_binhex` and `decode_from_binhex`, using the `binhex` module. Function 1: **Function name**: `encode_to_binhex` **Parameters**: - `input_filename` (str): The name of the binary file to be encoded. - `output_filename` (str): The name of the file to which the encoded binhex content will be written. **Returns**: None **Description**: This function reads a binary file specified by `input_filename`, encodes it to binhex format, and writes the encoded content to a file specified by `output_filename` using the `binhex` module. **Constraints**: - The `input_filename` file must exist and be readable. - Handle any exceptions raised by the `binhex` module and print an appropriate error message. Function 2: **Function name**: `decode_from_binhex` **Parameters**: - `input_filename` (str): The name of the binhex file to be decoded. - `output_filename` (str or None): The name of the file to which the decoded binary content will be written. If None, the output filename should be derived from the binhex file\'s internal header. **Returns**: None **Description**: This function reads a binhex file specified by `input_filename`, decodes it to its original binary format, and writes the decoded content to a file specified by `output_filename` using the `binhex` module. **Constraints**: - The `input_filename` file must exist and be readable. - Handle any exceptions raised by the `binhex` module and print an appropriate error message. # Example Usage ```python try: encode_to_binhex(\'example.jpg\', \'encoded.hqx\') print(\\"File successfully encoded to binhex format.\\") except Exception as e: print(f\\"Encoding failed: {e}\\") try: decode_from_binhex(\'encoded.hqx\', \'decoded_example.jpg\') print(\\"File successfully decoded from binhex format.\\") except Exception as e: print(f\\"Decoding failed: {e}\\") ``` # Note - The `binhex` module has been deprecated, which means it might not be available in future versions of Python. Ensure that the current Python environment supports the `binhex` module. Implement the `encode_to_binhex` and `decode_from_binhex` functions as described, and make sure to handle exceptions appropriately.","solution":"import binhex def encode_to_binhex(input_filename, output_filename): Encodes a binary file to binhex format. Parameters: input_filename (str): The name of the binary file to be encoded. output_filename (str): The name of the file to which the encoded binhex content will be written. Returns: None try: binhex.binhex(input_filename, output_filename) except Exception as e: print(f\\"Encoding failed: {e}\\") def decode_from_binhex(input_filename, output_filename): Decodes a binhex file to its original binary format. Parameters: input_filename (str): The name of the binhex file to be decoded. output_filename (str or None): The name of the file to which the decoded binary content will be written. If None, the output filename should be derived from the binhex file\'s internal header. Returns: None try: with open(input_filename, \'rb\') as input_file: binhex.hexbin(input_file, output_filename) except Exception as e: print(f\\"Decoding failed: {e}\\")"},{"question":"# UUID Generation and Validation Problem Statement You are tasked with creating a utility function that generates and validates various UUIDs using Python\'s `uuid` module. Specifically, you need to implement a function that: 1. Generates a UUID of each specified version (1, 3, 4, 5). 2. Validates whether a given string is a valid UUID. 3. Returns the version and variant of a parsed UUID string. Your solution should consist of the following functions: 1. `generate_uuids() -> dict`: - Generates and returns a dictionary with UUIDs of versions 1, 3, 4, and 5. - For versions 3 and 5, use the namespace `uuid.NAMESPACE_DNS` and name `\'example.com\'`. 2. `is_valid_uuid(uuid_string: str) -> bool`: - Takes a string and returns `True` if the string is a valid UUID, otherwise returns `False`. 3. `parse_uuid(uuid_string: str) -> tuple`: - Takes a UUID string and returns a tuple containing the UUID\'s version and variant. - Raise a `ValueError` if the string is not a valid UUID. Example ```python import uuid def generate_uuids() -> dict: return { 1: uuid.uuid1(), 3: uuid.uuid3(uuid.NAMESPACE_DNS, \'example.com\'), 4: uuid.uuid4(), 5: uuid.uuid5(uuid.NAMESPACE_DNS, \'example.com\') } def is_valid_uuid(uuid_string: str) -> bool: try: uuid_obj = uuid.UUID(uuid_string) return True except ValueError: return False def parse_uuid(uuid_string: str) -> tuple: if not is_valid_uuid(uuid_string): raise ValueError(\\"Invalid UUID string\\") uuid_obj = uuid.UUID(uuid_string) return (uuid_obj.version, uuid_obj.variant) # Example usage: generated_uuids = generate_uuids() print(generated_uuids) uuid_str = str(generated_uuids[1]) print(is_valid_uuid(uuid_str)) # Should return True print(parse_uuid(uuid_str)) # Should return (1, uuid.RFC_4122) invalid_uuid_str = \\"12345\\" print(is_valid_uuid(invalid_uuid_str)) # Should return False print(parse_uuid(invalid_uuid_str)) # Should raise ValueError ``` Constraints - The input string to `is_valid_uuid` and `parse_uuid` can be in any format that the `uuid.UUID` constructor accepts. - Ensure your implementation efficiently handles UUID parsing and validation.","solution":"import uuid def generate_uuids() -> dict: Generates and returns a dictionary with UUIDs of versions 1, 3, 4, and 5. return { 1: uuid.uuid1(), 3: uuid.uuid3(uuid.NAMESPACE_DNS, \'example.com\'), 4: uuid.uuid4(), 5: uuid.uuid5(uuid.NAMESPACE_DNS, \'example.com\') } def is_valid_uuid(uuid_string: str) -> bool: Takes a string and returns True if the string is a valid UUID, otherwise returns False. try: uuid_obj = uuid.UUID(uuid_string) return True except ValueError: return False def parse_uuid(uuid_string: str) -> tuple: Takes a UUID string and returns a tuple containing the UUID\'s version and variant. Raises a ValueError if the string is not a valid UUID. if not is_valid_uuid(uuid_string): raise ValueError(\\"Invalid UUID string\\") uuid_obj = uuid.UUID(uuid_string) return (uuid_obj.version, uuid_obj.variant)"},{"question":"# Question: Compute the Jacobian of a Model\'s Output with Respect to its Parameters You are given a PyTorch model and are required to compute the Jacobian of its output with respect to its parameters using the `torch.func` module\'s utilities. This task will help you understand how to manipulate neural network parameters and apply advanced autodiff techniques. Function Signature ```python def compute_jacobian(model: torch.nn.Module, input_tensor: torch.Tensor) -> torch.Tensor: pass ``` Input: - **model** (torch.nn.Module): A PyTorch neural network model. - **input_tensor** (torch.Tensor): A tensor representing the input to the model\'s forward pass. Output: - **jacobian_tensor** (torch.Tensor): A tensor representing the Jacobian of the model\'s output with respect to its parameters. The shape should be determined by the size of the input tensor and the number of parameters in the model. Constraints: - Use the `functional_call` and `jacrev` functions from the `torch.func` module. - Ensure that the input tensor is compatible with the model. - Efficient memory and runtime performance should be considered, though explicit performance requirements are not provided. - You may assume that the model and input dimensions are compatible. Example: ```python import torch import torch.nn as nn import torch.func as F model = nn.Linear(3, 3) input_tensor = torch.randn(3) jacobian = compute_jacobian(model, input_tensor) print(jacobian.shape) # Expected shape would be (3, number_of_parameters_in_model) ``` Notes: - You may assume that `torch.func` is correctly imported before using it. - Utilize `dict(model.named_parameters())` to extract the model\'s parameters for `functional_call`. Your Task: Implement the `compute_jacobian` function to calculate the Jacobian of the model\'s output with respect to its parameters.","solution":"import torch import torch.nn as nn from torch.func import functional_call, jacrev def compute_jacobian(model: torch.nn.Module, input_tensor: torch.Tensor) -> torch.Tensor: def model_output(params): state_dict = model.state_dict(keep_vars=True) for name, param in model.named_parameters(): state_dict[name] = params[name] return functional_call(model, (state_dict, ), (input_tensor,)) params = {name: param for name, param in model.named_parameters()} jacobian_fn = jacrev(model_output) jacobian_tensor = jacobian_fn(params) return jacobian_tensor"},{"question":"# Objective: Your task is to implement a function that processes a string and performs various operations using the **curses.ascii** module. # Function Signature: ```python def process_ascii_string(input_string: str) -> dict: pass ``` # Input: - `input_string` (str): A string consisting of ASCII characters. # Output: - (dict): A dictionary with the following keys and their corresponding values: - `\\"alnum_count\\"` (int): Number of alphanumeric characters in the string. - `\\"ctrl_count\\"` (int): Number of control characters in the string. - `\\"transformed\\"` (str): A new string where: - All control characters are replaced by their string representation using `curses.ascii.unctrl(c)`. - All printable characters remain unchanged. - `\\"ascii_sum\\"` (int): Sum of the ASCII values of all characters in the string. # Constraints: - The input string will have a length between 1 and 1000 characters. # Example: ```python import curses.ascii as ca def process_ascii_string(input_string: str) -> dict: result = { \\"alnum_count\\": 0, \\"ctrl_count\\": 0, \\"transformed\\": \\"\\", \\"ascii_sum\\": 0 } for char in input_string: if ca.isalnum(char): result[\\"alnum_count\\"] += 1 if ca.iscntrl(char): result[\\"ctrl_count\\"] += 1 result[\\"transformed\\"] += ca.unctrl(char) result[\\"ascii_sum\\"] += ord(char) return result # Example usage input_string = \\"Hellox07World\\" output = process_ascii_string(input_string) print(output) # Output should be: # { # \\"alnum_count\\": 10, # \\"ctrl_count\\": 1, # \\"transformed\\": \\"Hello^GWorld\\", # \\"ascii_sum\\": 756 # } ``` # Notes: - The function should utilize the `curses.ascii` module functions to perform the required checks and transformations. - Pay attention to efficiently handling the transformation and counting to ensure the solution performs well within the input constraints.","solution":"import curses.ascii as ca def process_ascii_string(input_string: str) -> dict: result = { \\"alnum_count\\": 0, \\"ctrl_count\\": 0, \\"transformed\\": \\"\\", \\"ascii_sum\\": 0 } for char in input_string: if ca.isalnum(char): result[\\"alnum_count\\"] += 1 if ca.iscntrl(char): result[\\"ctrl_count\\"] += 1 result[\\"transformed\\"] += ca.unctrl(char) result[\\"ascii_sum\\"] += ord(char) return result"},{"question":"# Advanced Coding Assessment: Working with Tar Archives Task You are asked to develop a Python function that performs multiple operations on tar archives using Python\'s `tarfile` module. Function Signature ```python def process_tar_archive(input_tar_path: str, output_tar_path: str, filter_function: callable) -> None: pass ``` Problem Description 1. **Input Parameters**: - `input_tar_path`: A string representing the path to the input tar archive file (can be compressed or uncompressed). - `output_tar_path`: A string representing the path where the modified tar archive will be written (should be gzip compressed). - `filter_function`: A callable function that will be used as an extraction filter. This function takes two parameters (`tarinfo`, `path`) and modifies `tarinfo` if necessary or returns `None` to skip the file. 2. **Function Requirements**: - Open the `input_tar_path` tar archive for reading. - Apply `filter_function` while extracting the contents of the input archive to a temporary directory. - Create a new tar archive at `output_tar_path` from the contents of the temporary directory, using gzip compression. - Ensure the temporary directory is cleaned up after the operation. 3. **Filter Function**: Your function should accept a filter function that operates as follows: - If a file passes the filter, it should be included in the extracted and subsequently re-archived contents. - If a file does not pass the filter (`filter_function` returns `None`), it should be skipped. 4. **Usage Example**: ```python # Example filter function def example_filter(tarinfo, path): if tarinfo.isfile() and tarinfo.size < 1024 * 1024: # Skip files larger than 1 MB return tarinfo return None process_tar_archive(\\"input.tar.gz\\", \\"output.tar.gz\\", example_filter) ``` Constraints - The `input_tar_path` file can be a tar archive compressed with gzip, bz2, or lzma, or uncompressed. - The total size of the extracted contents should not exceed 100 MB to prevent excessive resource usage. - Only files should be included in the final archive (directories, links, and special files should be skipped). Performance Requirements - The solution should appropriately handle both large file sizes and large numbers of files within the constraints described. Hints - Use `tarfile.TarInfo` methods to assess file types (like `isfile()`, `isdir()`, etc.). - Utilize context management to ensure that resources are properly handled. - Make use of Python’s `tempfile` module to manage temporary directories safely.","solution":"import tarfile import os import tempfile import shutil def process_tar_archive(input_tar_path: str, output_tar_path: str, filter_function: callable) -> None: # Create a temporary directory to extract files to with tempfile.TemporaryDirectory() as temp_dir: # Open the input tar file with tarfile.open(input_tar_path, \'r:*\') as input_tar: # Iterate through the tar members and extract using the filter_function for member in input_tar.getmembers(): if member.isfile(): extracted_path = input_tar.extractfile(member) if extracted_path is not None: result_tarinfo = filter_function(member, extracted_path.name) if result_tarinfo is not None: input_tar.extract(member, path=temp_dir) # Create a new output tar file with gzip compression with tarfile.open(output_tar_path, \'w:gz\') as output_tar: for root, _, files in os.walk(temp_dir): for file in files: file_path = os.path.join(root, file) arcname = os.path.relpath(file_path, temp_dir) output_tar.add(file_path, arcname=arcname)"},{"question":"**Coding Assessment Question:** # Problem Statement You have been tasked with developing a Python script to manage and convert event times for a global organization. The events are initially recorded in UTC but need to be displayed in the local time zones of various offices around the world. Additionally, you need to handle daylight saving time transitions correctly and provide functionalities to: 1. Check the availability of a specific IANA time zone. 2. Convert UTC times to local times with proper handling of ambiguous times using the `fold` attribute. 3. Configure and reset custom time zone paths dynamically. Implement three functions: `is_timezone_available`, `convert_utc_to_local`, and `set_custom_tzpath`. # Function 1: `is_timezone_available(timezone: str) -> bool` **Description:** - This function checks whether a given IANA time zone key is available on the system. **Input:** - `timezone` (str): The IANA time zone key as a string (e.g., \\"America/New_York\\"). **Output:** - Returns `True` if the specified time zone is available; otherwise, returns `False`. # Function 2: `convert_utc_to_local(utc_dt: datetime, timezone: str, fold: int = 0) -> datetime` **Description:** - This function converts a datetime object from UTC to the specified local time zone. It must handle ambiguous times correctly using the `fold` attribute. **Input:** - `utc_dt` (datetime): A `datetime` object in UTC. - `timezone` (str): The IANA time zone key to which the time needs to be converted (e.g., \\"America/New_York\\"). - `fold` (int): An optional integer (0 or 1) to handle ambiguous times, where `0` uses the offset before the transition and `1` uses the offset after the transition. The default value is `0`. **Output:** - Returns a `datetime` object converted to the specified local time zone. # Function 3: `set_custom_tzpath(paths: list) -> None` **Description:** - This function sets a custom time zone path using a list of absolute paths. It resets the `TZPATH` for the `zoneinfo` module. **Input:** - `paths` (list): A list of absolute paths as strings where the time zone data should be looked up. **Output:** - None # Example Usage ```python from datetime import datetime, timezone # Example usage of is_timezone_available print(is_timezone_available(\\"America/Los_Angeles\\")) # Example Output: True print(is_timezone_available(\\"Invalid/Zone\\")) # Example Output: False # Example usage of convert_utc_to_local utc_dt = datetime(2023, 10, 31, 12, 0, tzinfo=timezone.utc) local_dt = convert_utc_to_local(utc_dt, \\"America/Los_Angeles\\", fold=0) print(local_dt) # Example Output: 2023-10-31 05:00:00-07:00 # Example usage of set_custom_tzpath set_custom_tzpath([\\"/usr/share/zoneinfo\\", \\"/etc/custom_timezones\\"]) print(is_timezone_available(\\"Europe/Paris\\")) # Example Output: True (if available in the provided paths) ``` # Constraints - You can assume the `zoneinfo` module and the `tzdata` package are available. - Proper exception handling should be implemented for cases where a specified time zone is not available. # Notes - Ensure that the functions handle edge cases like invalid input time zones efficiently. - Use the relevant classes and methods from the `zoneinfo` module as described in the documentation. Good luck!","solution":"from datetime import datetime from zoneinfo import ZoneInfo, available_timezones, reset_tzpath def is_timezone_available(timezone: str) -> bool: Checks whether a given IANA time zone key is available on the system. return timezone in available_timezones() def convert_utc_to_local(utc_dt: datetime, timezone: str, fold: int = 0) -> datetime: Converts a datetime object from UTC to the specified local time zone. Handles ambiguous times correctly using the fold attribute. try: tz = ZoneInfo(timezone) except KeyError: raise ValueError(f\\"Time zone \'{timezone}\' is not available\\") local_dt = utc_dt.astimezone(tz) local_dt = local_dt.replace(fold=fold) return local_dt def set_custom_tzpath(paths: list) -> None: Sets a custom time zone path using a list of absolute paths. Resets the TZPATH for the zoneinfo module. reset_tzpath(paths)"},{"question":"Coding Assessment Question # Objective The purpose of this question is to assess your understanding of PyTorch tensor operations and how to handle unsupported constructs when scripting with TorchScript. # Problem Statement You are required to implement a function `custom_tensor_operations` that performs a sequence of tensor operations. The function should adhere to the following specifications: 1. Create a tensor with a shape of `(3, 3)` containing random values using `torch.randn`. 2. Normalize this tensor such that its mean is 0 and standard deviation is 1. 3. Create another tensor of ones with the same shape. 4. Add these two tensors element-wise. 5. Finally, compute the dot product of the resulting tensor with itself. To demonstrate your understanding of TorchScript, you need to script this function using TorchScript\'s `torch.jit.script` decorator. However, keep in mind the constraints mentioned in the provided documentation, especially avoiding any unsupported constructs and handling divergent schemas appropriately. # Function Signature ```python import torch import torch.nn as nn import torch.jit @torch.jit.script def custom_tensor_operations() -> torch.Tensor: # Implement the function here pass ``` # Expected Output The function should return a single tensor, which is the result of the dot product computation. # Constraints and Considerations 1. Do not use any unsupported constructs as listed in the provided documentation. 2. The final tensor should be computed within the scripted function, ensuring proper handling of TorchScript constraints. 3. Ensure that the function uses `torch.randn` and `torch.ones` appropriately, considering any schema differences between Python and TorchScript. 4. Perform necessary tensor operations to achieve normalization, addition, and dot product computation. # Example ```python result_tensor = custom_tensor_operations() print(result_tensor) # Expected Tensor Output: A tensor containing the result of the dot product after the specified operations. ``` Your implementation should be efficient and adhere to the constraints provided. Make sure to thoroughly test your function.","solution":"import torch import torch.jit @torch.jit.script def custom_tensor_operations() -> torch.Tensor: # Step 1: Create a tensor with a shape of (3, 3) containing random values using torch.randn random_tensor = torch.randn(3, 3) # Step 2: Normalize the tensor such that its mean is 0 and standard deviation is 1 mean = random_tensor.mean() std = random_tensor.std() normalized_tensor = (random_tensor - mean) / std # Step 3: Create another tensor of ones with the same shape ones_tensor = torch.ones(3, 3) # Step 4: Add these two tensors element-wise result_tensor = normalized_tensor + ones_tensor # Step 5: Compute the dot product of the resulting tensor with itself dot_product_tensor = torch.matmul(result_tensor.view(-1), result_tensor.view(-1)) return dot_product_tensor"},{"question":"# PyTorch Coding Assessment: Random Number Generation for Neural Network Initialization **Objective**: Implement a function to initialize the weights of a simple neural network using the `torch.random` module. This task assesses your understanding of random number generation in PyTorch and your ability to apply it in initializing a neural network. **Problem Statement**: You are provided with a simple neural network class. Your task is to implement the `initialize_weights` method of this class that initializes the weights of the neural network\'s layers using random values from the uniform distribution `[lower_bound, upper_bound)`. # Class Definition You are given the following class structure: ```python import torch.nn as nn class SimpleNetwork(nn.Module): def __init__(self, input_size, hidden_size, output_size): super(SimpleNetwork, self).__init__() self.fc1 = nn.Linear(input_size, hidden_size) self.fc2 = nn.Linear(hidden_size, output_size) def initialize_weights(self, lower_bound, upper_bound): # You need to implement this method pass ``` # Function Requirements: - **Inputs**: - `lower_bound` (float): The lower bound of the uniform distribution. - `upper_bound` (float): The upper bound of the uniform distribution. - **Outputs**: - This function should not return any value. It should directly initialize the weights of `fc1` and `fc2` layers of the neural network with random values from the uniform distribution `[lower_bound, upper_bound)`. # Constraints: - You must use the `torch.random` functionalities to generate random numbers for initializing the weights. - Initialize both the weights and biases of the layers. - Ensure that initialization happens within the given lower and upper bounds. # Performance Requirements: - The function should initialize the weights efficiently in terms of computation time. - Consider the dimensions of weights and biases when initializing (i.e., make sure that the generated random numbers match the shape of weights and biases in each layer). # Example: ```python # Example usage: net = SimpleNetwork(input_size=5, hidden_size=3, output_size=2) net.initialize_weights(lower_bound=-0.1, upper_bound=0.1) # After initialization, net.fc1.weight and net.fc1.bias contain values in the range [-0.1, 0.1) ``` # Implementation: You need to fill in the `initialize_weights` method to complete this coding exercise. ```python import torch.nn as nn import torch class SimpleNetwork(nn.Module): def __init__(self, input_size, hidden_size, output_size): super(SimpleNetwork, self).__init__() self.fc1 = nn.Linear(input_size, hidden_size) self.fc2 = nn.Linear(hidden_size, output_size) def initialize_weights(self, lower_bound, upper_bound): # Implement the initialization logic here nn.init.uniform_(self.fc1.weight, lower_bound, upper_bound) nn.init.uniform_(self.fc1.bias, lower_bound, upper_bound) nn.init.uniform_(self.fc2.weight, lower_bound, upper_bound) nn.init.uniform_(self.fc2.bias, lower_bound, upper_bound) ``` # Notes: - You can use `torch.nn.init` methods to facilitate the initialization process. - Ensure you read and understand how the random number generation works in PyTorch using `torch.random`. Submit your implementation of the `initialize_weights` method.","solution":"import torch.nn as nn class SimpleNetwork(nn.Module): def __init__(self, input_size, hidden_size, output_size): super(SimpleNetwork, self).__init__() self.fc1 = nn.Linear(input_size, hidden_size) self.fc2 = nn.Linear(hidden_size, output_size) def initialize_weights(self, lower_bound, upper_bound): nn.init.uniform_(self.fc1.weight, lower_bound, upper_bound) nn.init.uniform_(self.fc1.bias, lower_bound, upper_bound) nn.init.uniform_(self.fc2.weight, lower_bound, upper_bound) nn.init.uniform_(self.fc2.bias, lower_bound, upper_bound)"},{"question":"Objective: The goal is to test your understanding of the `marshal` module by implementing functions that demonstrate the ability to serialize and deserialize Python objects. Problem Statement: You are tasked with implementing two functions using the `marshal` module: `serialize_objects` and `deserialize_objects`. 1. **Function: `serialize_objects`** - **Inputs:** - `objects`: A list of Python objects to be serialized. Each object can be one of the supported types such as integers, strings, lists, dictionaries, etc. - **Outputs:** - A bytes object that contains the serialized binary representation of all objects in the input list. - **Constraints:** - Each object in the input list must be one of the types supported by the `marshal` module. - Use the current version of the `marshal` format. ```python def serialize_objects(objects): pass ``` 2. **Function: `deserialize_objects`** - **Inputs:** - `data`: A bytes object that contains the serialized binary representation of Python objects. - **Outputs:** - A list of Python objects deserialized from the input bytes. - **Constraints:** - The `data` bytes object must have been generated using the `serialize_objects` function. ```python def deserialize_objects(data): pass ``` Example: ```python objects = [1, \\"hello\\", None, True, [1, 2, 3], {\\"key\\": \\"value\\"}] binary_data = serialize_objects(objects) print(binary_data) # This will print the binary representation deserialized_objects = deserialize_objects(binary_data) print(deserialized_objects) # Output: [1, \\"hello\\", None, True, [1, 2, 3], {\\"key\\": \\"value\\"}] ``` Ensure that your code handles the constraints appropriately, especially checking if the objects belong to the set of types supported by the `marshal` module. Notes: - You may assume that the input to `serialize_objects` will always contain only supported types, so no need for extensive type-checking. - Pay attention to edge cases, such as empty lists or dictionaries. - Ensure that the serialized data maintains the order of objects and accurately reflects their structure for proper deserialization.","solution":"import marshal def serialize_objects(objects): Serializes a list of Python objects into a binary format using the marshal module. return marshal.dumps(objects) def deserialize_objects(data): Deserializes a binary format into a list of Python objects using the marshal module. return marshal.loads(data)"},{"question":"**Objective**: Implement a Python program that reads an input Sun AU audio file, processes the audio data, and writes the processed data into a new Sun AU audio file. Task 1. **Reading the Input File**: - Open an input Sun AU file in read mode using the `sunau.open` function. - Extract the following information from the audio file: - Number of channels - Sample width - Frame rate - Number of frames 2. **Processing the Audio Data**: - Read all the frames from the input file. - Reverse the audio frames. 3. **Writing the Output File**: - Open a new Sun AU file in write mode using the `sunau.open` function. - Set the parameters of the output file to match the input file. - Write the reversed audio frames into the output file. Constraints: - The input and output file paths will be provided as command-line arguments to the script. Input Format: - The input file path (string) - The output file path (string) Output Format: - A new Sun AU file should be created at the specified output file path with the reversed audio data. Example: ```python import sys import sunau def reverse_audio(input_file_path, output_file_path): try: # Open the input audio file in read mode input_file = sunau.open(input_file_path, \'r\') # Extract parameters n_channels = input_file.getnchannels() samp_width = input_file.getsampwidth() frame_rate = input_file.getframerate() n_frames = input_file.getnframes() # Read all frames audio_frames = input_file.readframes(n_frames) # Reverse the audio frames reversed_frames = audio_frames[::-1] # Close the input file input_file.close() # Open the output audio file in write mode output_file = sunau.open(output_file_path, \'w\') # Set the parameters output_file.setnchannels(n_channels) output_file.setsampwidth(samp_width) output_file.setframerate(frame_rate) output_file.setnframes(n_frames) output_file.setcomptype(input_file.getcomptype(), input_file.getcompname()) # Write the reversed audio frames output_file.writeframes(reversed_frames) # Close the output file output_file.close() except sunau.Error as e: print(f\\"Error processing audio files: {e}\\") if __name__ == \\"__main__\\": if len(sys.argv) != 3: print(\\"Usage: python reverse_audio.py <input_file> <output_file>\\") else: input_file_path = sys.argv[1] output_file_path = sys.argv[2] reverse_audio(input_file_path, output_file_path) ``` **Explanation**: - This script reads an audio file, reverses the audio data, and writes the reversed data into a new audio file using the `sunau` module. - Students are required to handle the file reading, audio data processing, and writing operations, ensuring the new file maintains the correct audio format and parameters.","solution":"import sys import sunau def reverse_audio(input_file_path, output_file_path): try: # Open the input audio file in read mode input_file = sunau.open(input_file_path, \'r\') # Extract parameters n_channels = input_file.getnchannels() samp_width = input_file.getsampwidth() frame_rate = input_file.getframerate() n_frames = input_file.getnframes() # Read all frames audio_frames = input_file.readframes(n_frames) # Reverse the audio frames reversed_frames = audio_frames[::-1] # Close the input file input_file.close() # Open the output audio file in write mode output_file = sunau.open(output_file_path, \'w\') # Set the parameters output_file.setnchannels(n_channels) output_file.setsampwidth(samp_width) output_file.setframerate(frame_rate) output_file.setnframes(n_frames) output_file.setcomptype(input_file.getcomptype(), input_file.getcompname()) # Write the reversed audio frames output_file.writeframes(reversed_frames) # Close the output file output_file.close() except sunau.Error as e: print(f\\"Error processing audio files: {e}\\") if __name__ == \\"__main__\\": if len(sys.argv) != 3: print(\\"Usage: python reverse_audio.py <input_file> <output_file>\\") else: input_file_path = sys.argv[1] output_file_path = sys.argv[2] reverse_audio(input_file_path, output_file_path)"},{"question":"Problem Statement You are tasked with creating an informative and interactive visualization using the seaborn library to analyze the \\"mpg\\" dataset. The goal is to produce a visualization that provides insights into the relationship between various features of the dataset, especially focusing on categorical and numerical attributes along with their transformations. # Task: 1. **Load the Dataset**: - Load the \\"mpg\\" dataset using seaborn\'s `load_dataset` function and filter the data to include only the cylinders with values among [4, 6, 8]. 2. **Create the Visualization**: - Create a scatter plot to visualize the relationship between the `weight` and `acceleration` of cars. - Use the `cylinders` attribute to encode the color of the points. - Apply different scales to the axes and the colors to modify how data is presented. 3. **Customize the Plot**: - Make sure to use a nonlinear transformation (like `sqrt`) for one of the axes. - Customize the tick formatting for clarity. - Use a specific qualitative palette to represent the `cylinders` categories distinctively. # Input Format: You are not required to handle any input reading. Just produce the required visualizations using the provided dataset. # Output Format: Your output should display a customized scatter plot fulfilling the requirements mentioned above. # Constraints: - Make sure that the colors representing different `cylinders` are easily distinguishable. - Ensure that the transformations applied to axes do not obscure the relationship between variables. # Example: Here is an example of what the code might look like, but make sure to expand and customize it further as per the task requirements: ```python import seaborn.objects as so from seaborn import load_dataset # Load and filter the dataset mpg = load_dataset(\\"mpg\\").query(\\"cylinders in [4, 6, 8]\\") # Create the plot p = ( so.Plot(mpg, x=\\"weight\\", y=\\"acceleration\\", color=\\"cylinders\\") .add(so.Dots()) .scale( x=so.Continuous(trans=\\"sqrt\\").tick(every=500), y=so.Continuous().label(like=\\"{value:.1f}\\"), color=\\"deep\\" ) .label(x=\\"Weight (sqrt scale)\\", y=\\"Acceleration\\", color=\\"Cylinders\\") ) # Display the plot p.show() ``` Customize this further to ensure all requirements are met.","solution":"import seaborn as sns import matplotlib.pyplot as plt import numpy as np def create_mpg_scatter_plot(): # Load the dataset and filter for specific cylinder values mpg = sns.load_dataset(\\"mpg\\").query(\\"cylinders in [4, 6, 8]\\") # Apply square root transformation to the weight column for plotting mpg[\'weight_sqrt\'] = np.sqrt(mpg[\'weight\']) # Create the scatter plot plt.figure(figsize=(10, 6)) scatter_plot = sns.scatterplot( data=mpg, x=\'weight_sqrt\', y=\'acceleration\', hue=\'cylinders\', palette=\'Set1\' ) # Customize the plot with appropriate scales and labels scatter_plot.set( xlabel=\'Weight (sqrt scale)\', ylabel=\'Acceleration\' ) scatter_plot.set_xticklabels([\'{:.1f}\'.format(x**2) for x in scatter_plot.get_xticks()]) plt.legend(title=\'Cylinders\') plt.title(\\"Relationship Between Weight and Acceleration of Cars (Filtered for Cylinders 4, 6, 8)\\") plt.show()"},{"question":"Objective Implement a function that takes a list of IP addresses and a subnet mask, then returns a new list categorizing the IP addresses into \'private\' or \'public\' based on the subnet mask. Description Write a function `categorize_ip_addresses(ip_addresses: List[str], subnet_mask: str) -> Dict[str, List[str]]` that: 1. Accepts a list of IP addresses and a subnet mask. 2. Categorizes the IP addresses into two categories: \'private\' and \'public\' based on whether the IP address falls under the same subnet mask and is a private IP. 3. Returns a dictionary with two keys: `\'private\'` and `\'public\'`, each containing a list of IP addresses that fall into the respective category. Function Signature ```python from typing import List, Dict import ipaddress def categorize_ip_addresses(ip_addresses: List[str], subnet_mask: str) -> Dict[str, List[str]]: # Your implementation here ``` Input - `ip_addresses`: A list of IP addresses (strings) in either IPv4 or IPv6 format. - `subnet_mask`: A subnet mask in the format \'192.168.1.0/24\' or \'2001:db8::/32\'. Output - A dictionary with two keys: `\'private\'` and `\'public\'`. - Each key maps to a list of IP addresses (strings) that fall into the respective category. Constraints - The IP addresses in the list are valid and correctly formatted. - The subnet mask is a valid subnet mask. Example ```python ip_addresses = [ \\"192.168.1.10\\", \\"192.168.1.20\\", \\"10.0.0.1\\", \\"8.8.8.8\\", \\"2001:db8::1\\", \\"fc00::1\\" ] subnet_mask = \\"192.168.1.0/24\\" result = categorize_ip_addresses(ip_addresses, subnet_mask) # Expected output: # { # \\"private\\": [\\"192.168.1.10\\", \\"192.168.1.20\\"], # \\"public\\": [\\"10.0.0.1\\", \\"8.8.8.8\\", \\"2001:db8::1\\", \\"fc00::1\\"] # } ``` Notes - Use the `ipaddress` module for IP manipulations. - An IP address is \'private\' if it is within the same network as defined by the subnet mask and is designated as private by IANA. - Any exceptions or errors due to invalid input formats are outside the scope of this function.","solution":"from typing import List, Dict import ipaddress def is_private_ip(ip: str) -> bool: Determine if the given IP address is private. ip_addr = ipaddress.ip_address(ip) return ip_addr.is_private def categorize_ip_addresses(ip_addresses: List[str], subnet_mask: str) -> Dict[str, List[str]]: Categorize IP addresses into \'private\' or \'public\' based on the subnet mask. subnet = ipaddress.ip_network(subnet_mask, strict=False) categorized_ips = {\'private\': [], \'public\': []} for ip in ip_addresses: ip_addr = ipaddress.ip_address(ip) if ip_addr in subnet and is_private_ip(ip): categorized_ips[\'private\'].append(ip) else: categorized_ips[\'public\'].append(ip) return categorized_ips"},{"question":"# Asyncio Streams: Create a Key-Value Store Create an asynchronous TCP server and client using the `asyncio` streams module in Python. The server will act as a simple in-memory key-value store that supports basic commands (SET, GET, DELETE). The client will communicate with the server to perform these commands. **Requirements:** 1. The server should listen for incoming TCP connections on `localhost` and port `9999`. 2. The server should support the following commands: - `SET <key> <value>`: Store the value with the associated key. - `GET <key>`: Retrieve the value associated with the specified key. - `DELETE <key>`: Remove the key-value pair from the store if it exists. 3. The client should be able to: - Connect to the server. - Send commands from the user to the server. - Display the response from the server to the user. **Server Implementation:** Implement an asynchronous server that can handle multiple clients concurrently and process the above commands. - **Input:** Commands sent by a client in the format described. - **Output:** Responses to the commands: - For `SET`: \\"OK\\" (if the operation was successful) - For `GET`: The value associated with the key or \\"NOT_FOUND\\" if the key does not exist - For `DELETE`: \\"DELETED\\" if the key was deleted or \\"NOT_FOUND\\" if the key does not exist **Client Implementation:** Implement an asynchronous client that connects to the server and allows the user to input commands, receiving and displaying the server\'s responses. - **Input:** User commands in the format described. - **Output:** Server responses to the commands. **Constraints:** - The server should handle errors gracefully. - Both server and client should use the `asyncio` streams module for communication. **Performance Requirements:** - The server should be able to handle multiple clients concurrently. - Ensure proper resource management and closing of connections. **Examples:** _Server starting:_ ```python python server.py Serving on (\'127.0.0.1\', 9999) ``` _Client interaction:_ ```python python client.py > SET foo bar OK > GET foo bar > DELETE foo DELETED > GET foo NOT_FOUND ``` Implement both the server and client as described. Provide the respective code files for `server.py` and `client.py`.","solution":"import asyncio # In-memory key-value store kv_store = {} async def handle_client(reader, writer): while True: data = await reader.read(100) if not data: break message = data.decode().strip() response = process_command(message) writer.write(response.encode()) await writer.drain() writer.close() await writer.wait_closed() def process_command(command): parts = command.split(\' \', 2) if len(parts) == 0: return \\"ERROR: Invalid command\\" cmd = parts[0] if cmd == \\"SET\\" and len(parts) == 3: key, value = parts[1], parts[2] kv_store[key] = value return \\"OK\\" elif cmd == \\"GET\\" and len(parts) == 2: key = parts[1] return kv_store.get(key, \\"NOT_FOUND\\") elif cmd == \\"DELETE\\" and len(parts) == 2: key = parts[1] if key in kv_store: del kv_store[key] return \\"DELETED\\" else: return \\"NOT_FOUND\\" else: return \\"ERROR: Invalid command\\" async def main(): server = await asyncio.start_server( handle_client, \'127.0.0.1\', 9999 ) addr = server.sockets[0].getsockname() print(f\'Serving on {addr}\') async with server: await server.serve_forever() if __name__ == \\"__main__\\": asyncio.run(main())"},{"question":"<|Analysis Begin|> The provided documentation explains the `2to3` tool, which is part of the Python standard library. It is designed to convert Python 2 code to Python 3 by applying a series of predefined \\"fixers\\" that transform the old syntax and library calls to their modern equivalents. Some of these transformations include handling changes in function names, module imports, syntax changes, and built-in functions, among others. Key points from the documentation include: 1. The basic usage of the `2to3` tool, including converting files and directories. 2. Using flags to customize the behavior of `2to3`, such as enabling specific fixers, writing changes to files, or using an alternate output directory. 3. Detailed descriptions of various fixers that handle specific transformations from Python 2 to Python 3. The documentation provides sufficient information to understand the purpose of `2to3`, its default behavior, customization options through flags, and the types of changes each fixer makes. <|Analysis End|> <|Question Begin|> 2to3 is a tool within the Python standard library that automatically translates Python 2 code into Python 3 code by applying a series of predefined transformations called \\"fixers\\". The objective of this exercise is to implement a mini version of the `2to3` tool that focuses on a subset of these transformations. Your implementation should read Python 2 code from a file, apply certain fixers, and output the converted Python 3 code. **Task:** Write a Python function called `convert_to_python3` that takes an input file path and an output file path as arguments. The function should apply the following subset of transformations to the Python 2 code and write the converted code to the output file: 1. **`print` statement to `print()` function**: - Transform any Python 2 `print` statement to the Python 3 `print()` function call. 2. **`raw_input()` to `input()`**: - Replace the `raw_input()` function with `input()`. 3. **Exception syntax**: - Convert `except X, T:` to `except X as T:`. 4. **`xrange()` to `range()`**: - Change any occurrence of `xrange()` to `range()`. Note that you do not need to handle the wrapping of `range()` in a `list`. **Input:** - `input_file_path`: A string representing the path to the Python 2 source code file. - `output_file_path`: A string representing the path where the converted Python 3 code should be written. **Output:** - The function does not return anything. It writes the converted code to the output file specified by `output_file_path`. **Constraints:** - You can assume that the input file contains syntactically correct Python 2 code. - You should preserve the code\'s original formatting (including comments and indentation) as much as possible. **Example:** Given an input file `example.py` with the following content: ```python def greet(name): print \\"Hello, {0}!\\".format(name) print \\"What\'s your name?\\" name = raw_input() greet(name) for i in xrange(5): print i try: does_not_exist() except NameError, e: print \\"Caught an exception\\" ``` After running `convert_to_python3(\'example.py\', \'example_converted.py\')`, the output file `example_converted.py` should contain: ```python def greet(name): print(\\"Hello, {0}!\\".format(name)) print(\\"What\'s your name?\\") name = input() greet(name) for i in range(5): print(i) try: does_not_exist() except NameError as e: print(\\"Caught an exception\\") ``` **Notes:** - Ensure that the transformations maintain the code\'s functionality. - Pay careful attention to the syntax changes required for each transformation. **Implementation:** ```python def convert_to_python3(input_file_path, output_file_path): with open(input_file_path, \'r\') as infile: code = infile.read() # Transformation 1: Convert print statements to print() function import re code = re.sub(r\'prints+\\"(.*?)\\"\', r\'print(\\"1\\")\', code) code = re.sub(r\'prints+\'(.*?)\'\', r\\"print(\'1\')\\", code) code = re.sub(r\'prints+([^\\"\']+)\', r\'print(1)\', code) # Transformation 2: Convert raw_input() to input() code = code.replace(\'raw_input()\', \'input()\') # Transformation 3: Convert exception syntax code = re.sub(r\'excepts+(.*),s*(.*):\', r\'except 1 as 2:\', code) # Transformation 4: Convert xrange() to range() code = code.replace(\'xrange(\', \'range(\') with open(output_file_path, \'w\') as outfile: outfile.write(code) # Example usage: # convert_to_python3(\'example.py\', \'example_converted.py\') ``` Ensure that your solution is robust and handles edge cases gracefully, such as strings with embedded quotes or comments that might confuse simple string replacements.","solution":"def convert_to_python3(input_file_path, output_file_path): import re with open(input_file_path, \'r\') as infile: code = infile.read() # Transformation 1: Convert print statements to print() function print_pattern = re.compile(r\'(?<!#)prints+\\"(.*?)\\"\') code = print_pattern.sub(r\'print(\\"1\\")\', code) print_pattern_single = re.compile(r\\"(?<!#)prints+\'(.*?)\'\\") code = print_pattern_single.sub(r\\"print(\'1\')\\", code) print_pattern_no_quotes = re.compile(r\'(?<!#)prints+([^\\"\'s]+)\') code = print_pattern_no_quotes.sub(r\'print(1)\', code) # Transformation 2: Convert raw_input() to input() raw_input_pattern = re.compile(r\'raw_input()\') code = raw_input_pattern.sub(\'input()\', code) # Transformation 3: Convert exception syntax exception_pattern = re.compile(r\'excepts+(w+),s*(w+):\') code = exception_pattern.sub(r\'except 1 as 2:\', code) # Transformation 4: Convert xrange() to range() xrange_pattern = re.compile(r\'bxrange(\') code = xrange_pattern.sub(\'range(\', code) with open(output_file_path, \'w\') as outfile: outfile.write(code)"},{"question":"Implement a PLS Regression Model As a data scientist, you are provided with a dataset containing matrix `X` and the target matrix `Y`. Your task is to implement a Partial Least Squares Regression (PLSRegression) model using scikit-learn\'s `cross_decomposition` module. Input * A matrix `X` of shape `(n_samples, n_features)` representing the input features. * A matrix `Y` of shape `(n_samples, n_targets)` representing the target values. * An integer `n_components` defining the number of PLS components to keep. Output * A function should be implemented that returns the transformed X and Y matrices as fitted by the `PLSRegression` algorithm. * Additionally, it should return the predicted targets for the training data `X`. Function Signature ```python def pls_regression(X, Y, n_components): Apply Partial Least Squares Regression on the given dataset. Parameters: X (numpy.ndarray): Input feature matrix of shape (n_samples, n_features) Y (numpy.ndarray): Target matrix of shape (n_samples, n_targets) n_components (int): Number of PLS components to keep Returns: tuple: Transformed X and Y matrices as fitted by PLSRegression, predicted targets for the training data X ``` Constraints 1. Use the `PLSRegression` class from scikit-learn\'s `cross_decomposition` module. 2. Perform the fit and transform operations on the matrices (`X`, `Y`). 3. Return the transformed matrices and the predictions in the specified order. Example ```python import numpy as np # Example input X = np.array([[1, 2], [3, 4], [5, 6]]) Y = np.array([[1], [2], [3]]) n_components = 2 # Expected output transformed_X, transformed_Y, predictions = pls_regression(X, Y, n_components) print(\\"Transformed X:\\", transformed_X) print(\\"Transformed Y:\\", transformed_Y) print(\\"Predictions:\\", predictions) ``` This task will test the student\'s understanding of using scikit-learn to apply supervised dimensionality reduction techniques and validating the implementation by checking the transformed and predicted outputs.","solution":"from sklearn.cross_decomposition import PLSRegression def pls_regression(X, Y, n_components): Apply Partial Least Squares Regression on the given dataset. Parameters: X (numpy.ndarray): Input feature matrix of shape (n_samples, n_features) Y (numpy.ndarray): Target matrix of shape (n_samples, n_targets) n_components (int): Number of PLS components to keep Returns: tuple: Transformed X and Y matrices as fitted by PLSRegression, predicted targets for the training data X # Initialize the PLS regression model pls = PLSRegression(n_components=n_components) # Fit the model pls.fit(X, Y) # Transform both X and Y X_transformed = pls.transform(X) Y_transformed = pls.y_scores_ # Predict using the fitted model Y_pred = pls.predict(X) return X_transformed, Y_transformed, Y_pred"},{"question":"# Warning Management and Custom Filters In this exercise, you will demonstrate your understanding of Python\'s warnings module. Your task is to create a custom warning, issue that warning appropriately in a function, and manipulate how this warning is handled by using warning filters. Specifically, you will perform the following steps: 1. **Define a Custom Warning**: - Create a custom warning class named `MyCustomWarning` which subclasses the base `Warning` class. 2. **Function to Issue Warning**: - Implement a function `sensitive_operation()` which simulates a sensitive operation by issuing a `MyCustomWarning`. 3. **Warning Management**: - Write a function `manage_warnings()` which does the following: - Uses `filterwarnings()` to ignore `MyCustomWarning`. - Uses `filterwarnings()` to convert `RuntimeWarning` to an error. - Uses the `catch_warnings` context manager to capture warnings. 4. **Testing Filters**: - Within the `manage_warnings()` function, validate that the warnings are handled correctly: - Ensure `MyCustomWarning` is ignored. - Ensure `RuntimeWarning` is converted to an error. - Capture and print warnings using `catch_warnings`. # Input and Output Formats **Define Custom Warning Class**: ```python class MyCustomWarning(Warning): pass ``` **Function to Issue Warning**: ```python def sensitive_operation(): # Your code to issue MyCustomWarning goes here ``` **Function to Manage Warnings**: ```python def manage_warnings(): import warnings with warnings.catch_warnings(record=True) as w: # Your code to manage warnings goes here # Test for `MyCustomWarning` sensitive_operation() assert len(w) == 0 # since `MyCustomWarning` should be ignored # Trigger RuntimeWarning and ensure it raises an exception try: warnings.warn(\\"This is a runtime warning\\", RuntimeWarning) except RuntimeWarning: print(\\"RuntimeWarning converted to exception as expected.\\") # Print captured warnings for warning in w: print(f\\"Warning: {warning.message}, Category: {warning.category}\\") # Call this function to see the results of your warning management manage_warnings() ``` # Constraints 1. Use the provided warning categories and manage them as specified. 2. Ensure the `manage_warnings` function operates within a single-threaded context. # Performance Requirements The solution should handle warnings efficiently and ensure that filters and context managers do not have a significant overhead. Proper handling of warnings should be validated using assertions within the `manage_warnings` function. # Sample Code Execution ```python # Example of expected output from manage_warnings() # When running manage_warnings(): RuntimeWarning converted to exception as expected. ```","solution":"import warnings class MyCustomWarning(Warning): pass def sensitive_operation(): warnings.warn(\\"This is a custom warning\\", MyCustomWarning) def manage_warnings(): with warnings.catch_warnings(record=True) as w: # Ignore MyCustomWarning warnings.filterwarnings(\\"ignore\\", category=MyCustomWarning) # Convert RuntimeWarning to an error warnings.filterwarnings(\\"error\\", category=RuntimeWarning) # Test for MyCustomWarning sensitive_operation() assert len(w) == 0 # since MyCustomWarning should be ignored # Trigger RuntimeWarning and ensure it raises an exception try: warnings.warn(\\"This is a runtime warning\\", RuntimeWarning) except RuntimeWarning: print(\\"RuntimeWarning converted to exception as expected.\\") # Print captured warnings if any for warning in w: print(f\\"Warning: {warning.message}, Category: {warning.category}\\") # Call this function to see the results of your warning management manage_warnings()"},{"question":"**Objective:** To assess your comprehension of fundamental and advanced string manipulation operations in pandas using the `str` accessor with Series and Index objects. **Problem Statement:** You are given a pandas DataFrame that contains information about customer reviews. The DataFrame has the following structure: ```python import pandas as pd data = { \\"customer_id\\": [1, 2, 3, 4], \\"review_title\\": [\\"Great product!\\", \\"Not good \\", \\" Average service\\", \\"excellentcustomer support\\"], \\"review_comment\\": [\\"Absolutely loved it.\\", \\" Satisfactory performance.\\", None, \\"Top notch!Great response time \\"], \\"date\\": [\\"2023-01-01\\", \\"2023-01-05\\", \\"2023-01-07\\", \\"2023-01-10\\"] } reviews_df = pd.DataFrame(data) ``` **Task:** 1. **Standardize and Clean Column Names:** The columns of the DataFrame have inconsistent names with leading and trailing spaces. Clean the column names by removing any whitespace and converting them to lower case with underscores. 2. **Standardize Review Titles and Comments:** - Convert all `review_title` and `review_comment` to lower case. - Remove any leading or trailing spaces in `review_title` and `review_comment`. - Replace any occurrences of multiple spaces within `review_comment` with a single space. 3. **Indicator Variables:** - Create an indicator variable column `positive_review` which is `1` if the `review_title` contains the word \\"great\\" or \\"excellent\\", otherwise `0`. 4. **Extract Features:** - Extract the year, month, and day from the `date` column and add them as separate columns named `year`, `month`, and `day` respectively. 5. **Output DataFrame:** - Return the cleaned DataFrame with the newly created columns. **Constraints:** - Assume all string manipulations need to handle missing values gracefully. **Function Signature:** ```python import pandas as pd def clean_and_transform_reviews(reviews_df: pd.DataFrame) -> pd.DataFrame: Cleans and transforms the customer reviews DataFrame as per the given specifications. Args: reviews_df (pd.DataFrame): A DataFrame containing customer reviews. Returns: pd.DataFrame: The transformed DataFrame with cleaned and standardized columns, new indicator and feature columns. # Your code here ``` **Execution Example:** ```python # Provided Input data = { \\"customer_id\\": [1, 2, 3, 4], \\"review_title\\": [\\"Great product!\\", \\"Not good\\", \\" Average service\\", \\"excellentcustomer support\\"], \\"review_comment\\": [\\"Absolutely loved it.\\", \\" Satisfactory performance.\\", None, \\"Top notch!Great response time\\"], \\"date\\": [\\"2023-01-01\\", \\"2023-01-05\\", \\"2023-01-07\\", \\"2023-01-10\\"] } reviews_df = pd.DataFrame(data) # Expected Output cleaned_reviews_df = clean_and_transform_reviews(reviews_df) print(cleaned_reviews_df) ``` Output DataFrame should look like: ``` customer_id review_title review_comment date positive_review year month day 0 1 great product! absolutely loved it. 2023-01-01 1 2023 1 1 1 2 not good satisfactory performance 2023-01-05 0 2023 1 5 2 3 average service NaN 2023-01-07 0 2023 1 7 3 4 excellentcustomer support top notch! great response time 2023-01-10 1 2023 1 10 ``` **Notes:** - Ensure all string operations are efficient and handle missing data appropriately. - Make use of pandas\' string methods as much as possible to achieve the objectives.","solution":"import pandas as pd def clean_and_transform_reviews(reviews_df: pd.DataFrame) -> pd.DataFrame: # Standardize and clean column names reviews_df.columns = reviews_df.columns.str.strip().str.lower().str.replace(\' \', \'_\') # Standardize review titles and comments reviews_df[\'review_title\'] = reviews_df[\'review_title\'].str.lower().str.strip() reviews_df[\'review_comment\'] = reviews_df[\'review_comment\'].str.lower().str.strip() reviews_df[\'review_comment\'] = reviews_df[\'review_comment\'].str.replace(r\'s+\', \' \', regex=True) # Replace multiple spaces with a single space # Create the positive_review indicator variable reviews_df[\'positive_review\'] = reviews_df[\'review_title\'].apply( lambda x: 1 if \'great\' in x or \'excellent\' in x else 0 ) # Extract year, month, and day from the date column reviews_df[\'date\'] = pd.to_datetime(reviews_df[\'date\']) reviews_df[\'year\'] = reviews_df[\'date\'].dt.year reviews_df[\'month\'] = reviews_df[\'date\'].dt.month reviews_df[\'day\'] = reviews_df[\'date\'].dt.day return reviews_df"},{"question":"**Problem Statement: Configuration File Manipulation with `configparser`** You are tasked with managing a large application configuration stored in INI files using the Python `configparser` module. Your goal is to create a utility function that reads a configuration file, updates certain values based on given criteria, and writes the changes back to the configuration file. # Function Requirements: 1. **Function Name**: `update_config` 2. **Parameters**: - `input_file` (str): The path to the input INI configuration file. - `output_file` (str): The path where the updated configuration should be saved. - `updates` (dict): A dictionary containing the sections and corresponding key-value pairs to be updated. For example: ```python updates = { \\"DEFAULT\\": {\\"Compression\\": \\"no\\"}, \\"forge.example\\": {\\"User\\": \\"git\\"}, \\"topsecret.server.example\\": {\\"Port\\": \\"60022\\"} } ``` 3. **Behavior**: - Read the configuration from `input_file`. - Apply the updates specified in the `updates` dictionary. If a section or key does not exist, it should be created. - Preserve existing values in the configuration that are not being updated. - Write the updated configuration to `output_file`. # Constraints: - Only string values should be used for updates in the configuration. # Example Input: ```python input_file = \'config.ini\' output_file = \'updated_config.ini\' updates = { \\"DEFAULT\\": {\\"Compression\\": \\"no\\"}, \\"forge.example\\": {\\"User\\": \\"git\\"}, \\"topsecret.server.example\\": {\\"Port\\": \\"60022\\"} } ``` # Example Output: The `updated_config.ini` file should reflect the changes as specified in `updates` with all other original values being intact. # Implementation: - Use the `configparser` module to read, update, and write configuration files. - Handle all exceptions related to file reading/writing and invalid configuration settings gracefully. ```python def update_config(input_file, output_file, updates): Reads the input INI file, applies updates, and writes to the output file. :param input_file: Path to the input INI configuration file. :param output_file: Path to the output INI configuration file. :param updates: Dictionary containing sections and key-value pairs to be updated. pass ``` # Testing: - Use sample configurations to test your function. Create a sample `config.ini` file with multiple sections and verify that your function correctly updates required values while preserving other settings.","solution":"import configparser def update_config(input_file, output_file, updates): Reads the input INI file, applies updates, and writes to the output file. :param input_file: Path to the input INI configuration file. :param output_file: Path to the output INI configuration file. :param updates: Dictionary containing sections and key-value pairs to be updated. config = configparser.ConfigParser() config.read(input_file) for section, values in updates.items(): if not config.has_section(section) and section != \\"DEFAULT\\": config.add_section(section) for key, value in values.items(): config[section][key] = value with open(output_file, \'w\') as configfile: config.write(configfile)"},{"question":"Problem Statement You are given a dataset containing test scores of students along with their study hours. Your task is to implement a function that uses isotonic regression to fit a non-decreasing function to the data. You will then use this function to predict the scores based on the provided study hours. Function Signature ```python def predict_scores(study_hours: List[float], test_scores: List[float], new_study_hours: List[float]) -> List[float]: Fits an isotonic regression model to the provided study hours and test scores, and uses it to predict the scores for a new set of study hours. Parameters: - study_hours (List[float]): A list of floating-point numbers representing the number of hours spent studying. - test_scores (List[float]): A list of floating-point numbers representing the test scores achieved. - new_study_hours (List[float]): A list of floating-point numbers representing the number of hours spent studying for which you want to predict the test scores. Returns: - List[float]: A list of floating-point numbers representing the predicted test scores for the new study hours. ``` Input * `study_hours`: A list of floats where each float represents the hours spent studying. * `test_scores`: A list of floats where each float represents the test score of a student. * `new_study_hours`: A list of floats where each float represents the new hours of study for which you want to predict the scores. Output * A list of floats representing the predicted test scores for the new study hours. Constraints * The length of `study_hours` and `test_scores` will be the same. * Each value in `study_hours`, `test_scores`, and `new_study_hours` will be a non-negative float. * The length of input lists will not exceed 10,000. Example ```python study_hours = [1.0, 2.5, 3.5, 4.0, 6.0] test_scores = [50, 45, 60, 65, 80] new_study_hours = [2.0, 3.0, 5.0] predicted_scores = predict_scores(study_hours, test_scores, new_study_hours) print(predicted_scores) # Output might be something like [47.5, 52.5, 72.5] ``` Evaluation * Model\'s accuracy will be gauged by its ability to fit the initial set of study hours and test scores. * The predicted scores for the new study hours will be compared against a validation set (which will not be provided). Note You should use the `IsotonicRegression` class from `sklearn.isotonic` to solve this problem.","solution":"from typing import List from sklearn.isotonic import IsotonicRegression def predict_scores(study_hours: List[float], test_scores: List[float], new_study_hours: List[float]) -> List[float]: Fits an isotonic regression model to the provided study hours and test scores, and uses it to predict the scores for a new set of study hours. Parameters: - study_hours (List[float]): A list of floating-point numbers representing the number of hours spent studying. - test_scores (List[float]): A list of floating-point numbers representing the test scores achieved. - new_study_hours (List[float]): A list of floating-point numbers representing the number of hours spent studying for which you want to predict the test scores. Returns: - List[float]: A list of floating-point numbers representing the predicted test scores for the new study hours. iso_reg = IsotonicRegression() iso_reg.fit(study_hours, test_scores) return iso_reg.predict(new_study_hours).tolist()"},{"question":"Objective: Implement a function named `evaluate_expressions` that processes a list of mixed inputs in various forms (files, interactive statements, and expressions). The function should dynamically evaluate these inputs and return the results. Function Signature: ```python def evaluate_expressions(inputs: List[str]) -> List[Any]: ``` Parameters: - `inputs` (List[str]): A list of strings where each string may represent a Python expression, file name containing Python code, or an individual interactive statement. Returns: - List[Any]: A list of results obtained from evaluating the given inputs. Behavior and Constraints: 1. **Expression Inputs**: - Must be valid Python expressions (e.g., `\'3 + 4\'`, `\'sum([1, 2, 3])\'`). - Should use the `eval()` function to evaluate these expressions. - If an expression is invalid, append `None` to the results with a note in the log. 2. **File Inputs**: - The input string prefixed with `file:` indicates that the remaining part is a filename. - Read and execute the file content using `exec()`. - Return the result as `None` but log the process. 3. **Interactive Statements**: - If the input string is recognized as a statement, execute it in a manner similar to an interactive Python session. - Statements may include variable assignments or function definitions, followed by a blank line in interactive mode. - Append the result of execution or variable state to the results accordingly. If an error occurs, append `None`. 4. Ensure that any provided Python code is safely evaluated to prevent security risks (i.e., no execution of harmful code). Example Usage: ```python inputs = [ \'3 + 4\', \'sum([1, 2, 3])\', \'file:example_script.py\', \'x = 10nprint(x)\', \'eval(\\"10 / 2\\")\' ] results = evaluate_expressions(inputs) print(results) ``` Notes: - Create test cases to ensure all different input types are correctly processed and evaluated. - The function must handle invalid inputs gracefully, appending `None` to the results for any evaluation errors. - Ensure to log each step and output appropriately for debugging and validation.","solution":"def evaluate_expressions(inputs): import os results = [] globals_dict = {} for item in inputs: if item.startswith(\'file:\'): filename = item[5:] if os.path.isfile(filename): with open(filename, \'r\') as file: try: exec(file.read(), globals_dict) results.append(None) except Exception as e: results.append(None) else: results.append(None) else: try: if \'=\' in item: exec(item, globals_dict) results.append(None) else: result = eval(item, globals_dict) results.append(result) except Exception as e: results.append(None) return results"},{"question":"You have been tasked with creating a utility function to help convert strings containing HTML character references into their Unicode equivalents, and vice versa. Your function should utilize the dictionaries from the `html.entities` module to perform the conversions. # Part 1: `html_to_unicode` Implement the function `html_to_unicode(html_string: str) -> str` which converts HTML character references to their corresponding Unicode characters. For this task, you should utilize the `html5` dictionary. Input - `html_string` (str): A string that may contain HTML character references (e.g., \\"Hello &gt; World\\"). Output - (str): The input string with all HTML character references replaced by their corresponding Unicode characters. Example ```python html_to_unicode(\\"Hello &gt; World\\") -> \\"Hello > World\\" html_to_unicode(\\"Happy &hearts; Day\\") -> \\"Happy ♥ Day\\" ``` # Part 2: `unicode_to_html` Implement the function `unicode_to_html(unicode_string: str) -> str` which converts Unicode characters to their corresponding HTML character references. For this task, you should utilize the `codepoint2name` dictionary to find the corresponding HTML entity name for each Unicode character. Input - `unicode_string` (str): A string that contains Unicode characters (e.g., \\"Hello > World\\"). Output - (str): The input string with all Unicode characters replaced by their corresponding HTML character references (e.g., \\"Hello &gt; World\\"). Example ```python unicode_to_html(\\"Hello > World\\") -> \\"Hello &gt; World\\" unicode_to_html(\\"Happy ♥ Day\\") -> \\"Happy &hearts; Day\\" ``` # Constraints - The input strings will only contain valid HTML references and Unicode characters. - Performance considerations: The functions should work efficiently even for long strings up to 10,000 characters. # Additional Information - You can assume that the `html.entities` module is imported and available for use. - You may use the `html.unescape()` function to simplify Part 1 if needed. # Evaluation Criteria - Correctness: The solution should correctly convert between HTML references and Unicode for any valid input. - Efficiency: The solution should be efficient and handle large inputs within a reasonable time frame. - Code Quality: The code should be well-organized, readable, and follow best practices.","solution":"import html from html.entities import codepoint2name def html_to_unicode(html_string: str) -> str: Convert HTML character references to their corresponding Unicode characters. Args: html_string (str): A string that may contain HTML character references. Returns: str: The input string with all HTML character references replaced by their corresponding Unicode characters. return html.unescape(html_string) def unicode_to_html(unicode_string: str) -> str: Convert Unicode characters to their corresponding HTML character references. Args: unicode_string (str): A string that contains Unicode characters. Returns: str: The input string with all Unicode characters replaced by their corresponding HTML character references. result = [] for char in unicode_string: char_code = ord(char) if char_code in codepoint2name: result.append(f\\"&{codepoint2name[char_code]};\\") else: result.append(char) return \\"\\".join(result)"},{"question":"Objective: Implement a function to compare a set of original tensors to a set of quantized tensors using various error and similarity metrics. Problem Statement: You are provided with two lists of tensors: `original_tensors` and `quantized_tensors`. Each list contains the same number of tensors, where the `i`-th tensor from `original_tensors` corresponds to the `i`-th tensor in `quantized_tensors`. Your task is to implement a function `compare_tensors(original_tensors, quantized_tensors)`, which performs the following steps for each pair of original and quantized tensors: 1. Compute the Signal-to-Quantization-Noise Ratio (SQNR). 2. Compute the normalized L2 error. 3. Compute the cosine similarity. The function should return a list of dictionaries, where each dictionary contains the results of the above computations for a corresponding pair of original and quantized tensors. Function Signature: ```python from typing import List, Dict import torch def compare_tensors(original_tensors: List[torch.Tensor], quantized_tensors: List[torch.Tensor]) -> List[Dict[str, float]]: pass ``` Input: - `original_tensors`: List of `torch.Tensor` objects (each tensor can be of any shape). - `quantized_tensors`: List of `torch.Tensor` objects (each tensor should have the same shape as the corresponding tensor in `original_tensors`). Output: - A list of dictionaries, where each dictionary has the following keys: - `\\"sqnr\\"`: The computed SQNR value (float). - `\\"l2_error\\"`: The computed normalized L2 error value (float). - `\\"cosine_similarity\\"`: The computed cosine similarity value (float). Constraints: - Both input lists will have the same non-zero length. - Each original tensor will have the same shape as its corresponding quantized tensor. Example: ```python import torch from torch.ao.ns.fx.utils import compute_sqnr, compute_normalized_l2_error, compute_cosine_similarity # Example tensors original_tensors = [torch.tensor([1.0, 2.0, 3.0]), torch.tensor([4.0, 5.0, 6.0])] quantized_tensors = [torch.tensor([1.1, 2.0, 2.9]), torch.tensor([4.1, 4.9, 5.8])] result = compare_tensors(original_tensors, quantized_tensors) # Expected output (values may vary slightly) [ {\\"sqnr\\": SQNR_value_1, \\"l2_error\\": L2_error_value_1, \\"cosine_similarity\\": Cosine_similarity_value_1}, {\\"sqnr\\": SQNR_value_2, \\"l2_error\\": L2_error_value_2, \\"cosine_similarity\\": Cosine_similarity_value_2} ] ``` Notes: - Use the provided utility functions `compute_sqnr`, `compute_normalized_l2_error`, and `compute_cosine_similarity` from the `torch.ao.ns.fx.utils` module to calculate the respective metrics.","solution":"from typing import List, Dict import torch import torch.ao.ns.fx.utils as utils def compare_tensors(original_tensors: List[torch.Tensor], quantized_tensors: List[torch.Tensor]) -> List[Dict[str, float]]: result = [] for original, quantized in zip(original_tensors, quantized_tensors): sqnr = utils.compute_sqnr(original, quantized).item() l2_error = utils.compute_normalized_l2_error(original, quantized).item() cosine_similarity = utils.compute_cosine_similarity(original, quantized).item() result.append({ \\"sqnr\\": sqnr, \\"l2_error\\": l2_error, \\"cosine_similarity\\": cosine_similarity }) return result"},{"question":"Objective: Implement a Python function utilizing the `fcntl` module to manage and synchronize access to a shared file among multiple processes, ensuring safe read and write operations. Problem Statement: You are tasked with implementing a function `synchronized_file_operation` that safely reads from and writes to a given file. The function will use the `fcntl` module to lock the file during read and write operations to prevent data corruption when accessed by multiple processes concurrently. Function Signature: ```python def synchronized_file_operation(file_path: str, operation: str, data: str = \\"\\") -> str: ... ``` Input: - `file_path` (str): The path to the file on which to perform operations. - `operation` (str): The operation to be performed, either `\\"read\\"` or `\\"write\\"`. - `data` (str): The data to write to the file. Only used if the operation is `\\"write\\"`. Output: - (str): The contents of the file after the operation. If the operation is `\\"write\\"`, return the full content of the file after writing the data. If the operation is `\\"read\\"`, return the content of the file. Constraints: - Only one process should be able to write to the file at a time. - Multiple processes should be able to read the file concurrently. - The function should handle any potential I/O errors gracefully. Example: ```python import os def example_usage(): file_path = \\"shared_file.txt\\" # Ensure the file exists if not os.path.exists(file_path): with open(file_path, \\"w\\") as f: f.write(\\"Initial content.\\") # Example write operation synchronized_file_operation(file_path, \\"write\\", \\"New content.\\") # Example read operation content = synchronized_file_operation(file_path, \\"read\\") print(content) # Should include \\"New content.\\" along with \\"Initial content.\\" ``` Requirements: 1. Use `fcntl.flock` to manage locks. For reading use `LOCK_SH` (shared lock), and for writing use `LOCK_EX` (exclusive lock). 2. Ensure that all file operations are performed within the scope of the acquired lock. 3. Handle exceptions properly to ensure that locks are always released even if an error occurs. Function Implementation: ```python import fcntl import os def synchronized_file_operation(file_path: str, operation: str, data: str = \\"\\") -> str: try: with open(file_path, \\"r+\\" if operation == \\"write\\" else \\"r\\") as f: # Lock the file if operation == \\"write\\": fcntl.flock(f, fcntl.LOCK_EX) # Write data to the file f.write(data) f.seek(0) # Go back to the start of the file else: fcntl.flock(f, fcntl.LOCK_SH) # Read the file\'s content content = f.read() # Unlock the file fcntl.flock(f, fcntl.LOCK_UN) return content except OSError as e: print(f\\"Error handling file \'{file_path}\': {e}\\") return \\"\\" # Usage example if __name__ == \\"__main__\\": example_usage() ``` Note: - Ensure that the example usage works as expected in a multi-process scenario. For instance, test the function by spawning multiple processes trying to read and write to the same file concurrently.","solution":"import fcntl import os def synchronized_file_operation(file_path: str, operation: str, data: str = \\"\\") -> str: Manages and synchronizes access to a shared file for read and write operations. Args: - file_path (str): The path to the file on which to perform operations. - operation (str): The operation to be performed, either \\"read\\" or \\"write\\". - data (str): The data to write to the file. Only used if the operation is \\"write\\". Returns: - str: The contents of the file after the operation. try: with open(file_path, \\"r+\\" if operation == \\"write\\" else \\"r\\") as f: if operation == \\"write\\": # Exclusive lock for writing fcntl.flock(f, fcntl.LOCK_EX) # Write data to the file f.seek(0, os.SEEK_END) f.write(data) f.seek(0) # Go back to the start of the file else: # Shared lock for reading fcntl.flock(f, fcntl.LOCK_SH) # Read the file\'s content content = f.read() # Unlock the file fcntl.flock(f, fcntl.LOCK_UN) return content except OSError as e: print(f\\"Error handling file \'{file_path}\': {e}\\") return \\"\\""},{"question":"# Command-Line Argument Parser with `getopt` Objective: Write a Python script that uses the `getopt` module to handle command-line arguments and options. The script should demonstrate your understanding of command-line parsing, option validation, and exception handling. Description: You are tasked with creating a command-line utility that processes a set of options and arguments. Your script should support the following options and arguments: 1. **Short Options:** - `-h`: Display help information and exit. - `-o`: Specify an output file. This option requires an argument. - `-v`: Enable verbose mode. 2. **Long Options:** - `--help`: Same as `-h`. - `--output`: Same as `-o`. - `--verbose`: Same as `-v`. 3. **Positional Arguments:** - The script should accept a list of positional arguments that will be printed when verbose mode is enabled. Requirements: 1. **Help Message:** - Create a function `usage()` that prints a meaningful help message describing how to use the script and the available options. - This function should be called when the `-h` or `--help` option is provided. 2. **Command-Line Parsing:** - Use the `getopt` module to parse the command-line arguments. - Properly handle the `-o`/`--output`, `-v`/`--verbose`, and `-h`/`--help` options. - Ensure the script correctly processes and validates these options. 3. **Error Handling:** - Implement exception handling for `getopt.GetoptError` to handle unrecognized options or missing option arguments gracefully. - Print appropriate error messages and the usage information in case of an error. 4. **Script Functionality:** - If the `-v` or `--verbose` option is enabled, print the list of positional arguments provided. - If the `-o` or `--output` option is used, print the name of the output file. - Ensure the script exits gracefully after executing the required functionality. Example Usage: 1. Print the help message: ``` python script.py -h ``` 2. Print verbose information and specify an output file: ``` python script.py -v -o output.txt arg1 arg2 arg3 ``` 3. Handle errors gracefully: ``` python script.py -o # Should print an error message about the missing argument for -o and show usage information. ``` Input and Output: - **Input:** - Command-line arguments as described. - **Output:** - Help message, verbose messages, and error messages printed to the console based on the input options and arguments. Constraints: - Do not use the `argparse` module. - Focus on using the `getopt` module as described in the documentation. Submission: Submit your Python script file with the implementation of the above requirements. Ensure your code is well-commented and follows proper coding conventions.","solution":"import sys import getopt def usage(): Prints the usage information for the script. print(\\"Usage: script.py [options] [arguments]\\") print(\\"Options:\\") print(\\" -h, --help Show this help message and exit\\") print(\\" -o FILE, --output=FILE Specify output file\\") print(\\" -v, --verbose Enable verbose mode\\") print() print(\\"Arguments:\\") print(\\" Positional arguments that will be printed when verbose mode is enabled.\\") def main(argv): output_file = None verbose = False try: opts, args = getopt.getopt(argv, \\"ho:v\\", [\\"help\\", \\"output=\\", \\"verbose\\"]) except getopt.GetoptError as err: print(str(err)) usage() sys.exit(2) for opt, arg in opts: if opt in (\\"-h\\", \\"--help\\"): usage() sys.exit() elif opt in (\\"-o\\", \\"--output\\"): output_file = arg elif opt in (\\"-v\\", \\"--verbose\\"): verbose = True if verbose: print(f\\"Positional arguments: {args}\\") if output_file: print(f\\"Output file: {output_file}\\") if __name__ == \\"__main__\\": main(sys.argv[1:])"},{"question":"**Objective**: Demonstrate your understanding of Seaborn for creating informative data visualizations. **Task**: Using the Seaborn library, load the Titanic dataset and create the following visualizations: 1. **Count Plot**: Create a count plot to display the number of passengers in each class (`class`). 2. **Survival Count Plot**: Create another count plot to display the number of passengers in each class (`class`) while differentiating between those who survived and those who did not (`survived`). 3. **Percentage Survival**: Create a visualization that shows the percentage of survivors and non-survivors within each class. Hint: Normalize the counts to show percentages. 4. **Class vs Age KDE Plot**: Create a Kernel Density Estimate (KDE) plot to show the distribution of passenger ages (`age`) for each class (`class`). This plot should help visualize the density of ages for each class of passengers. **Requirements**: - Your code should import the necessary libraries and load the dataset correctly. - Each plot should be correctly labeled with titles and axes labels for clarity. - Ensure that different visualizations are displayed correctly in a Jupyter Notebook or a Python script format. - Include any necessary code to display the plots effectively. **Input**: ```python import seaborn as sns ``` **Output**: The code should render four plots as described. **Example**: The following partial example demonstrates how to set up and create one of the required plots: ```python import seaborn as sns import matplotlib.pyplot as plt # Set theme sns.set_theme(style=\\"whitegrid\\") # Load dataset titanic = sns.load_dataset(\\"titanic\\") # 1. Count Plot plt.figure(figsize=(10,6)) sns.countplot(x=\\"class\\", data=titanic) plt.title(\\"Number of Passengers in Each Class\\") plt.xlabel(\\"Class\\") plt.ylabel(\\"Count\\") plt.show() ``` Make sure to follow this example, adjusting and expanding it to include additional plots as per the task requirements. **Constraints**: - Use the Seaborn library functions as much as possible. - Assume that the dataset provided (Titanic) is error-free and complete.","solution":"import seaborn as sns import matplotlib.pyplot as plt # Set theme sns.set_theme(style=\\"whitegrid\\") # Load dataset titanic = sns.load_dataset(\\"titanic\\") # 1. Count Plot plt.figure(figsize=(10,6)) sns.countplot(x=\\"class\\", data=titanic) plt.title(\\"Number of Passengers in Each Class\\") plt.xlabel(\\"Class\\") plt.ylabel(\\"Count\\") plt.show() # 2. Survival Count Plot plt.figure(figsize=(10,6)) sns.countplot(x=\\"class\\", hue=\\"survived\\", data=titanic) plt.title(\\"Number of Passengers in Each Class by Survival\\") plt.xlabel(\\"Class\\") plt.ylabel(\\"Count\\") plt.legend(title=\\"Survived\\", labels=[\\"No\\", \\"Yes\\"]) plt.show() # 3. Percentage Survival survival_pct = titanic.groupby(\'class\')[\'survived\'].value_counts(normalize=True).unstack() * 100 survival_pct.plot(kind=\'bar\', stacked=True, figsize=(10,6)) plt.title(\\"Percentage of Survivors in Each Class\\") plt.xlabel(\\"Class\\") plt.ylabel(\\"Percentage of Passengers\\") plt.legend(title=\\"Survived\\", labels=[\\"No\\", \\"Yes\\"]) plt.show() # 4. Class vs Age KDE Plot plt.figure(figsize=(10,6)) sns.kdeplot(data=titanic, x=\\"age\\", hue=\\"class\\", common_norm=False, fill=True) plt.title(\\"Distribution of Passenger Ages by Class\\") plt.xlabel(\\"Age\\") plt.ylabel(\\"Density\\") plt.show()"},{"question":"Objective: You are required to implement a utility that processes filenames in a directory and sorts them based on a specified wildcard pattern. This exercise will demonstrate your understanding of shell-style wildcard pattern matching and basic file handling in Python. Problem Statement: Write a function `sort_files_by_pattern(directory: str, pattern: str) -> list` that takes a directory path and a wildcard pattern as input, and returns a list of filenames sorted lexicographically that match the given pattern. Function Signature: ```python def sort_files_by_pattern(directory: str, pattern: str) -> list: ``` Input: - `directory` (str): The path to the directory containing files. - `pattern` (str): The Unix shell-style wildcard pattern to match filenames (e.g., `\'*.txt\'`, `\'data_?.csv\'`). Output: - Returns a list of filenames (str) that match the given pattern, sorted in lexicographical order. Constraints: - You may assume that the directory path provided is valid and readable. - Only filenames (not folders) should be considered. - Patterns should adhere strictly to Unix shell-style wildcards as described in the `fnmatch` module. - The returned list should be case-sensitive. Example: Suppose the directory `/example_directory` contains the files: - `data_1.csv` - `data_2.csv` - `data_A.csv` - `info.txt` - `readme.md` Given the pattern `\'data_?.csv\'`, the function should filter the filenames and return: ```python [\'data_1.csv\', \'data_2.csv\', \'data_A.csv\'] ``` If the pattern is `\'*.txt\'`, the output should be: ```python [\'info.txt\'] ``` Additional Information: - Make sure to import the necessary modules, especially `os` for directory operations and `fnmatch` for pattern matching. - Use `fnmatch.filter` to efficiently filter filenames that match the pattern. - Ensure your solution is efficient and can handle directories with a large number of files. Hints: 1. Use `os.listdir` to list all items in the directory. 2. Use `os.path.isfile` to check if an item is a file. 3. Utilize the `fnmatch.filter` for matching the filenames with the pattern. Notes: - This question tests your ability to integrate and correctly apply the `fnmatch` module in a real-world scenario. - Pay attention to edge cases, such as no matching files or an empty directory. Good luck!","solution":"import os import fnmatch def sort_files_by_pattern(directory: str, pattern: str) -> list: Returns a list of filenames in the specified directory that match the given wildcard pattern, sorted in lexicographical order. # Get list of all files in the directory all_files = os.listdir(directory) # Filter files using the given pattern matching_files = fnmatch.filter(all_files, pattern) # Return sorted list of matching files return sorted(matching_files)"},{"question":"**Objective:** Implement a Python script using the `argparse` module to create a command-line tool called `text_processor` that performs various operations on the input text file. The script should include multiple sub-commands and handle various options effectively. # Requirements: 1. **Main Operations (`text_processor`):** - `filter`: Filters lines in the file based on the presence of a keyword. - `replace`: Replaces a word or phrase in the file with another word or phrase. - `stats`: Prints statistics about the file (number of lines, words, and characters). 2. **Sub-Command Details:** - `filter`: - `--keyword` (required): Keyword to filter lines. - `replace`: - `--old` (required): Old word/phrase to replace. - `--new` (required): New word/phrase to replace with. - `stats`: - No additional arguments are required. # Input and Output: 1. The input file name should be a positional argument. 2. The script should print the help message when no arguments or the wrong arguments are provided. 3. For the `filter` and `replace` sub-commands, the modified content should be displayed, without modifying the input file. 4. For the `stats` sub-command, the output should display the number of lines, words, and characters in the input file. # Constraints: * You must implement the script using the `argparse` module. * Ensure all sub-commands and their respective arguments are properly handled and validated. # Example Usage: ```sh python text_processor.py -h # Should display the help message with descriptions of each sub-command and its options. python text_processor.py file.txt filter --keyword \\"error\\" # Should display lines from file.txt that contain the keyword \\"error\\". python text_processor.py file.txt replace --old \\"foo\\" --new \\"bar\\" # Should display the content of file.txt with \\"foo\\" replaced by \\"bar\\". python text_processor.py file.txt stats # Should display the statistics of file.txt: number of lines, words, and characters. ``` # Implementation Template: ```python import argparse def filter_file(file_path, keyword): with open(file_path, \'r\') as file: for line in file: if keyword in line: print(line, end=\'\') def replace_in_file(file_path, old_word, new_word): with open(file_path, \'r\') as file: for line in file: print(line.replace(old_word, new_word), end=\'\') def file_stats(file_path): num_lines = num_words = num_chars = 0 with open(file_path, \'r\') as file: for line in file: num_lines += 1 num_words += len(line.split()) num_chars += len(line) print(f\'Lines: {num_lines}, Words: {num_words}, Characters: {num_chars}\') def main(): parser = argparse.ArgumentParser(description=\'Text file processor.\') subparsers = parser.add_subparsers(dest=\'command\') filter_parser = subparsers.add_parser(\'filter\', help=\'Filter lines containing a keyword.\') filter_parser.add_argument(\'file\', help=\'Path to the input file.\') filter_parser.add_argument(\'--keyword\', required=True, help=\'Keyword to filter lines.\') replace_parser = subparsers.add_parser(\'replace\', help=\'Replace a word/phrase with another.\') replace_parser.add_argument(\'file\', help=\'Path to the input file.\') replace_parser.add_argument(\'--old\', required=True, help=\'Old word or phrase to replace.\') replace_parser.add_argument(\'--new\', required=True, help=\'New word or phrase to replace with.\') stats_parser = subparsers.add_parser(\'stats\', help=\'Print statistics about the file.\') stats_parser.add_argument(\'file\', help=\'Path to the input file.\') args = parser.parse_args() if args.command == \'filter\': filter_file(args.file, args.keyword) elif args.command == \'replace\': replace_in_file(args.file, args.old, args.new) elif args.command == \'stats\': file_stats(args.file) else: parser.print_help() if __name__ == \'__main__\': main() ``` **Notes:** - Ensure you handle file reading errors appropriately. - Thoroughly test your script with different combinations and edge cases for the arguments.","solution":"import argparse def filter_file(file_path, keyword): try: with open(file_path, \'r\') as file: for line in file: if keyword in line: print(line, end=\'\') except FileNotFoundError: print(f\\"Error: The file \'{file_path}\' was not found.\\") except Exception as e: print(f\\"An error occurred: {e}\\") def replace_in_file(file_path, old_word, new_word): try: with open(file_path, \'r\') as file: for line in file: print(line.replace(old_word, new_word), end=\'\') except FileNotFoundError: print(f\\"Error: The file \'{file_path}\' was not found.\\") except Exception as e: print(f\\"An error occurred: {e}\\") def file_stats(file_path): try: num_lines, num_words, num_chars = 0, 0, 0 with open(file_path, \'r\') as file: for line in file: num_lines += 1 num_words += len(line.split()) num_chars += len(line) print(f\'Lines: {num_lines}, Words: {num_words}, Characters: {num_chars}\') except FileNotFoundError: print(f\\"Error: The file \'{file_path}\' was not found.\\") except Exception as e: print(f\\"An error occurred: {e}\\") def main(): parser = argparse.ArgumentParser(description=\'Text file processor.\') subparsers = parser.add_subparsers(dest=\'command\') filter_parser = subparsers.add_parser(\'filter\', help=\'Filter lines containing a keyword.\') filter_parser.add_argument(\'file\', help=\'Path to the input file.\') filter_parser.add_argument(\'--keyword\', required=True, help=\'Keyword to filter lines.\') replace_parser = subparsers.add_parser(\'replace\', help=\'Replace a word/phrase with another.\') replace_parser.add_argument(\'file\', help=\'Path to the input file.\') replace_parser.add_argument(\'--old\', required=True, help=\'Old word or phrase to replace.\') replace_parser.add_argument(\'--new\', required=True, help=\'New word or phrase to replace with.\') stats_parser = subparsers.add_parser(\'stats\', help=\'Print statistics about the file.\') stats_parser.add_argument(\'file\', help=\'Path to the input file.\') args = parser.parse_args() if args.command == \'filter\': filter_file(args.file, args.keyword) elif args.command == \'replace\': replace_in_file(args.file, args.old, args.new) elif args.command == \'stats\': file_stats(args.file) else: parser.print_help() if __name__ == \'__main__\': main()"},{"question":"# Coding Assessment: Name Resolution and Exception Handling **Objective**: Write a Python function that demonstrates your understanding of name binding, scope resolution, and exception handling. Problem Statement You are required to implement a function `nested_scope_analysis()` that performs the following tasks: 1. **Define a class `DataProcessor`**: - This class should have an attribute `data` which is a list initialized to `[1, 2, 3, 4, 5]`. 2. **Inside the `DataProcessor` class, define a method `process_data`**: - This method should take no arguments and should define a variable `result` initialized to an empty list. - It should process each element in `self.data` within a nested function `inner_process` such that: - If the element is even, it should be squared and appended to `result`. - If the element is odd, raise a custom exception `OddNumberError`. This exception class should be defined with a message \\"Odd number encountered: {number}\\". - Handle the `OddNumberError` within `process_data` and instead of raising the exception, append the string `\\"Odd\\"` to `result`. 3. **The `nested_scope_analysis` function**: - Create an instance of `DataProcessor`. - Call the `process_data` method. - Return the resultant `result` list from invoking `process_data`. Additional Requirements: - **Input Constraints**: There are no inputs to the function `nested_scope_analysis()`. - **Output**: The function `nested_scope_analysis()` should return a list as specified. - **Exception Handling**: Properly handle custom exceptions as described. Example: Suppose the `data` list in `DataProcessor` is `[1, 2, 3, 4, 5]`. The `result` list should contain `[Odd, 4, Odd, 16, Odd]` after processing. ```python def nested_scope_analysis(): ... # your implementation here # Example usage: print(nested_scope_analysis()) # Output: [\'Odd\', 4, \'Odd\', 16, \'Odd\'] ``` **Hints**: - Remember to define the custom exception class `OddNumberError`. - Make sure to understand how nested functions access variables in enclosing scopes. - Implement and handle exceptions appropriately within the `process_data` method.","solution":"class OddNumberError(Exception): def __init__(self, number): self.message = f\\"Odd number encountered: {number}\\" super().__init__(self.message) class DataProcessor: def __init__(self): self.data = [1, 2, 3, 4, 5] def process_data(self): result = [] def inner_process(number): if number % 2 == 0: result.append(number ** 2) else: raise OddNumberError(number) for number in self.data: try: inner_process(number) except OddNumberError as e: result.append(\\"Odd\\") return result def nested_scope_analysis(): processor = DataProcessor() return processor.process_data()"},{"question":"# Objective: In this task, you are required to create a dataclass to represent a simplified library system and implement associated utility functions to manage the data. # Problem Statement: Create a dataclass named `Book` that will represent a book in a library system. Each book should have the following attributes: - `title`: a string representing the title of the book. - `author`: a string representing the author of the book. - `year`: an integer representing the year of publication. - `genre`: a string representing the genre of the book. - `available_quantity`: an integer representing the quantity available in the library, with a default value of `0`. In addition to the attributes, implement the following methods: 1. `checkout(self, quantity: int)`: This method should reduce the `available_quantity` by `quantity`. If `quantity` exceeds `available_quantity`, raise a `ValueError` with the message `\\"Not enough copies available\\"`. 2. `return_book(self, quantity: int)`: This method should increase the `available_quantity` by `quantity`. 3. `to_dict(self)`: This method should return the attributes of the book as a dictionary using the `dataclasses.asdict()` utility function. Finally, write a function named `create_library` that takes a list of book dictionaries and returns a list of `Book` dataclass instances. # Constraints: - The `year` attribute must be a positive integer. - The `available_quantity` attribute must be non-negative. # Input and Output Formats: Book dataclass: ```python @dataclass class Book: title: str author: str year: int genre: str available_quantity: int = 0 def checkout(self, quantity: int): pass def return_book(self, quantity: int): pass def to_dict(self) -> dict: pass ``` create_library function: **Input:** - A list of dictionaries where each dictionary represents a book with the keys `title`, `author`, `year`, `genre`, and `available_quantity`. **Output:** - A list of `Book` instances. # Example Input and Output: ```python books_data = [ {\'title\': \'1984\', \'author\': \'George Orwell\', \'year\': 1949, \'genre\': \'Dystopian\', \'available_quantity\': 5}, {\'title\': \'To Kill a Mockingbird\', \'author\': \'Harper Lee\', \'year\': 1960, \'genre\': \'Fiction\', \'available_quantity\': 3} ] library = create_library(books_data) print(library[0].to_dict()) # Output: {\'title\': \'1984\', \'author\': \'George Orwell\', \'year\': 1949, \'genre\': \'Dystopian\', \'available_quantity\': 5} print(library[1].to_dict()) # Output: {\'title\': \'To Kill a Mockingbird\', \'author\': \'Harper Lee\', \'year\': 1960, \'genre\': \'Fiction\', \'available_quantity\': 3} ``` # Requirements: - Implement the `Book` dataclass with the specified attributes and methods. - Implement the `create_library` function to convert a list of dictionaries into a list of `Book` instances.","solution":"from dataclasses import dataclass, asdict @dataclass class Book: title: str author: str year: int genre: str available_quantity: int = 0 def checkout(self, quantity: int): if quantity > self.available_quantity: raise ValueError(\\"Not enough copies available\\") self.available_quantity -= quantity def return_book(self, quantity: int): self.available_quantity += quantity def to_dict(self) -> dict: return asdict(self) def create_library(books_data): return [Book(**book_data) for book_data in books_data]"},{"question":"<|Analysis Begin|> The provided documentation describes Kernel Ridge Regression (KRR) and compares it with Support Vector Regression (SVR). Key points include: 1. **KRR Overview**: KRR combines ridge regression with the kernel trick, learning a linear function in the induced space by the kernel. 2. **Comparison with SVR**: KRR uses squared error loss, while SVR uses epsilon-insensitive loss. 3. **Fitting and Prediction**: KRR fitting can be done in closed-form and is usually faster for medium-sized datasets. However, its prediction is slower due to the non-sparse model. 4. **Performance**: KRR is roughly seven times faster than SVR in fitting using grid search but is slower in prediction for large datasets because SVR learns a sparse model. This documentation is sufficient to craft a question focusing on the implementation and understanding of Kernel Ridge Regression using scikit-learn. <|Analysis End|> <|Question Begin|> Kernel Ridge Regression Implementation and Comparison # Objective Implement a Kernel Ridge Regression model using scikit-learn, train it on a given dataset, and compare its performance with a Support Vector Regression model. This assessment will test your understanding of KRR and your ability to apply it using scikit-learn. # Task 1. Load a given dataset (`data.csv`) which contains two columns: `X` (input features) and `y` (target values). Assume `data.csv` is provided. 2. Split the dataset into training (80%) and testing (20%) sets. 3. Implement Kernel Ridge Regression (KRR) and Support Vector Regression (SVR) using scikit-learn. 4. Train both models on the training set. 5. Perform predictions on the testing set using both models. 6. Evaluate the models using Mean Squared Error (MSE). 7. Compare the training time and prediction time for both models. # Input - `data.csv` file with two columns: `X` and `y`. # Output - Mean Squared Error (MSE) for KRR on the test set. - Mean Squared Error (MSE) for SVR on the test set. - Training time for KRR. - Training time for SVR. - Prediction time for KRR on the test set. - Prediction time for SVR on the test set. # Constraints - Use the RBF kernel for both KRR and SVR. - Optimize hyperparameters using grid search for both models: - For KRR, tune the `alpha` (regularization parameter) and `gamma` (kernel coefficient) parameters. - For SVR, tune the `C` (regularization parameter), `epsilon`, and `gamma` (kernel coefficient) parameters. # Performance Requirements - Code should be efficient and handle the dataset size effectively. - Properly split data, handle fitting, prediction, and evaluation within reasonable time limits. # Example Code Structure ```python import pandas as pd from sklearn.kernel_ridge import KernelRidge from sklearn.svm import SVR from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.metrics import mean_squared_error import time # Step 1: Load the dataset data = pd.read_csv(\'data.csv\') X = data[\'X\'].values.reshape(-1, 1) y = data[\'y\'].values # Step 2: Split the dataset X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Step 3: Define and train Kernel Ridge Regression model krr = KernelRidge(kernel=\'rbf\') param_grid_krr = {\'alpha\': [1e-3, 1e-2, 1e-1, 1], \'gamma\': [1e-3, 1e-2, 1e-1, 1]} grid_krr = GridSearchCV(krr, param_grid_krr, cv=5) start_time = time.time() grid_krr.fit(X_train, y_train) krr_train_time = time.time() - start_time # Step 4: Define and train Support Vector Regression model svr = SVR(kernel=\'rbf\') param_grid_svr = {\'C\': [1, 10, 100, 1000], \'gamma\': [1e-3, 1e-2, 1e-1, 1], \'epsilon\': [0.1, 0.2, 0.3]} grid_svr = GridSearchCV(svr, param_grid_svr, cv=5) start_time = time.time() grid_svr.fit(X_train, y_train) svr_train_time = time.time() - start_time # Step 5: Perform predictions and calculate MSE start_time = time.time() y_pred_krr = grid_krr.predict(X_test) krr_predict_time = time.time() - start_time mse_krr = mean_squared_error(y_test, y_pred_krr) start_time = time.time() y_pred_svr = grid_svr.predict(X_test) svr_predict_time = time.time() - start_time mse_svr = mean_squared_error(y_test, y_pred_svr) # Step 6: Print the results print(f\\"KRR MSE: {mse_krr}, Training Time: {krr_train_time}, Prediction Time: {krr_predict_time}\\") print(f\\"SVR MSE: {mse_svr}, Training Time: {svr_train_time}, Prediction Time: {svr_predict_time}\\") ``` **Question End|>","solution":"import pandas as pd from sklearn.kernel_ridge import KernelRidge from sklearn.svm import SVR from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.metrics import mean_squared_error import time def load_data(path): Load dataset from the given path. data = pd.read_csv(path) X = data[\'X\'].values.reshape(-1, 1) y = data[\'y\'].values return X, y def train_krr(X_train, y_train): Train Kernel Ridge Regression model with grid search for hyperparameters. krr = KernelRidge(kernel=\'rbf\') param_grid_krr = {\'alpha\': [1e-3, 1e-2, 1e-1, 1], \'gamma\': [1e-3, 1e-2, 1e-1, 1]} grid_krr = GridSearchCV(krr, param_grid_krr, cv=5) start_time = time.time() grid_krr.fit(X_train, y_train) krr_train_time = time.time() - start_time return grid_krr, krr_train_time def train_svr(X_train, y_train): Train Support Vector Regression model with grid search for hyperparameters. svr = SVR(kernel=\'rbf\') param_grid_svr = {\'C\': [1, 10, 100, 1000], \'gamma\': [1e-3, 1e-2, 1e-1, 1], \'epsilon\': [0.1, 0.2, 0.3]} grid_svr = GridSearchCV(svr, param_grid_svr, cv=5) start_time = time.time() grid_svr.fit(X_train, y_train) svr_train_time = time.time() - start_time return grid_svr, svr_train_time def evaluate_model(model, X_test, y_test): Evaluate the model on the test set and return MSE and prediction time. start_time = time.time() y_pred = model.predict(X_test) prediction_time = time.time() - start_time mse = mean_squared_error(y_test, y_pred) return mse, prediction_time def main(path=\'data.csv\'): X, y = load_data(path) # Split the dataset into training and testing X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Train Kernel Ridge Regression model grid_krr, krr_train_time = train_krr(X_train, y_train) # Train Support Vector Regression model grid_svr, svr_train_time = train_svr(X_train, y_train) # Evaluate Kernel Ridge Regression model mse_krr, krr_predict_time = evaluate_model(grid_krr, X_test, y_test) # Evaluate Support Vector Regression model mse_svr, svr_predict_time = evaluate_model(grid_svr, X_test, y_test) # Print the results results = { \\"krr_mse\\": mse_krr, \\"svr_mse\\": mse_svr, \\"krr_train_time\\": krr_train_time, \\"svr_train_time\\": svr_train_time, \\"krr_predict_time\\": krr_predict_time, \\"svr_predict_time\\": svr_predict_time } return results if __name__ == \\"__main__\\": print(main())"},{"question":"**Question: Implement an Enhanced Cookie Parser** You are required to implement an enhanced cookie parser using the `http.cookies` module that not only parses cookies but also provides some additional functionality. The parser should be able to: 1. Parse raw cookie data from an HTTP header into a dictionary of cookie names mapped to their values. 2. Output the cookies as an HTTP header string. 3. Provide methods to update or delete specific cookies, and retrieve cookies that meet certain criteria (e.g., Secure cookies). # Specifications 1. **Class Definition:** - Define a class `EnhancedCookieParser` that inherits from `http.cookies.SimpleCookie`. 2. **Methods to Implement:** - `__init__(self, input_data=None)`: Initialize the cookie parser. If `input_data` is provided, load the cookies using this data. - `load_from_header(self, header: str) -> None`: Load cookies from a raw HTTP header string. - `to_header(self) -> str`: Return a string suitable to be sent as an HTTP header with all the cookies. - `update_cookie(self, key: str, value: str, **attributes) -> None`: Update the value and attributes of a specific cookie. - `delete_cookie(self, key: str) -> None`: Delete a specific cookie. - `get_secure_cookies(self) -> dict`: Retrieve all cookies that have the `secure` attribute set to `True`. # Example Usage ```python # Example header header = \\"Set-Cookie: user_id=abc123; Secure; HttpOnlyrnSet-Cookie: session_token=xyz789; Path=/\\" # Initialize the parser and load from header parser = EnhancedCookieParser() parser.load_from_header(header) # Print cookies as HTTP header print(parser.to_header()) # Output should be: # Set-Cookie: user_id=abc123; Secure; HttpOnly # Set-Cookie: session_token=xyz789; Path=/ # Update a cookie parser.update_cookie(\'user_id\', \'def456\', Path=\'/home\', HttpOnly=True) # Delete a cookie parser.delete_cookie(\'session_token\') # Get secure cookies secure_cookies = parser.get_secure_cookies() print(secure_cookies) # Output should be a dictionary with secure cookies # {\'user_id\': \'def456\'} ``` # Input/Output Formats - `__init__(self, input_data=None)`: - `input_data`: Optional; `str` representing cookie data. - `load_from_header(self, header: str) -> None`: - `header`: `str` representing the raw HTTP header containing cookies. - `to_header(self) -> str`: - Returns: `str` representing the cookies as HTTP header. - `update_cookie(self, key: str, value: str, **attributes) -> None`: - `key`: `str` representing the cookie name. - `value`: `str` representing the new cookie value. - `attributes`: Additional cookie attributes (e.g., Path, Secure, HttpOnly). - `delete_cookie(self, key: str) -> None`: - `key`: `str` representing the cookie name to be deleted. - `get_secure_cookies(self) -> dict`: - Returns: `dict` representing all cookies with the `Secure` attribute set to `True`. - Example: `{\'user_id\': \'abc123\'}` # Constraints - The passed `header` strings are guaranteed to be properly formatted. - Consider edge cases like non-existent cookie updates or deletions. - Efficiently handle operations with time complexity constraints. **Note:** Utilize the given `http.cookies` module and its classes/methods to implement the required functionality. Avoid using third-party cookie handling libraries.","solution":"from http.cookies import SimpleCookie class EnhancedCookieParser(SimpleCookie): def __init__(self, input_data=None): super().__init__() if input_data: self.load(input_data) def load_from_header(self, header: str) -> None: header_parts = header.split(\\"rn\\") for part in header_parts: if part.startswith(\\"Set-Cookie:\\"): cookie_data = part[len(\\"Set-Cookie: \\"):] self.load(cookie_data) def to_header(self) -> str: header_str = \'\' for key, morsel in self.items(): header_str += f\\"Set-Cookie: {key}={morsel.value}; \\" for attr, val in morsel.items(): if attr != \'value\' and val: header_str += f\\"{attr}={val}; \\" header_str = header_str.rstrip(\\"; \\") + \\"rn\\" return header_str.strip() def update_cookie(self, key: str, value: str, **attributes) -> None: self[key] = value for attr, val in attributes.items(): self[key][attr] = val def delete_cookie(self, key: str) -> None: if key in self: del self[key] def get_secure_cookies(self) -> dict: secure_cookies = {} for key, morsel in self.items(): if morsel[\\"secure\\"]: secure_cookies[key] = morsel.value return secure_cookies"},{"question":"# Coding Assessment: Complex DataFrame Manipulation Objective: You are given a dataset of employee performance reviews. Each review has multiple criteria rated on a scale of 1 to 5. You are required to process this data and produce various insights. Your task is to write functions that: 1. Creates a new column based on conditional logic. 2. Selects rows based on a combination of criteria. 3. Creates new columns dynamically. 4. Aggregates data based on a group. 5. Merges the insights into a final DataFrame. Dataset: Below is a sample of your dataset named `reviews.csv`: | EmployeeID | ReviewID | Criterion1 | Criterion2 | Criterion3 | Criterion4 | Criterion5 | |------------|-----------|------------|------------|------------|------------|------------| | 101 | 1 | 3 | 4 | 2 | 5 | 3 | | 102 | 2 | 5 | 4 | 4 | 3 | 2 | | 101 | 3 | 2 | 3 | 4 | 1 | 2 | | 103 | 4 | 5 | 5 | 5 | 5 | 5 | | 102 | 5 | 3 | 2 | 3 | 2 | 3 | Tasks: 1. **Create a new column `OverallRating`:** - Calculate the average rating across all criteria for each review and assign it to a new column called `OverallRating`. 2. **Select reviews for high-performance employees:** - Select rows where `OverallRating` is greater than or equal to 4. 3. **Create dynamic columns based on criteria:** - Map each criterion rating to a descriptive category: \\"Poor\\" (1-2), \\"Average\\" (3), \\"Good\\" (4), \\"Excellent\\" (5). - Add new columns for each criterion with the mapped categories. 4. **Group and aggregate:** - Group the data by `EmployeeID` and calculate the following: - Average rating for each criterion. - Total number of reviews. - Create a DataFrame with this aggregated information. 5. **Merge insights:** - Combine the aggregated DataFrame from the previous step with the original DataFrame, ensuring that each row in the original DataFrame has the corresponding aggregate metrics for the `EmployeeID`. This should include the average ratings and total number of reviews for each employee. Function Definitions: 1. `create_overall_rating(df: pd.DataFrame) -> pd.DataFrame` 2. `select_high_performance(df: pd.DataFrame) -> pd.DataFrame` 3. `create_dynamic_columns(df: pd.DataFrame) -> pd.DataFrame` 4. `group_and_aggregate(df: pd.DataFrame) -> pd.DataFrame` 5. `merge_insights(original_df: pd.DataFrame, aggregated_df: pd.DataFrame) -> pd.DataFrame` Constraints and Requirements: - Assume `df` is a pandas DataFrame read from `reviews.csv`. - Efficiently handle the DataFrame using vectorized operations where possible. - Return new DataFrames which are modified, do not alter original DataFrame in-place. Here is how the solution structure should look: ```python import pandas as pd def create_overall_rating(df: pd.DataFrame) -> pd.DataFrame: # Your code here pass def select_high_performance(df: pd.DataFrame) -> pd.DataFrame: # Your code here pass def create_dynamic_columns(df: pd.DataFrame) -> pd.DataFrame: # Your code here pass def group_and_aggregate(df: pd.DataFrame) -> pd.DataFrame: # Your code here pass def merge_insights(original_df: pd.DataFrame, aggregated_df: pd.DataFrame) -> pd.DataFrame: # Your code here pass # Example of usage: # df = pd.read_csv(\'reviews.csv\') # df = create_overall_rating(df) # high_performers = select_high_performance(df) # df = create_dynamic_columns(df) # aggregated_df = group_and_aggregate(df) # final_df = merge_insights(df, aggregated_df) ``` Good luck!","solution":"import pandas as pd def create_overall_rating(df: pd.DataFrame) -> pd.DataFrame: criteria_columns = [col for col in df.columns if col.startswith(\'Criterion\')] df[\'OverallRating\'] = df[criteria_columns].mean(axis=1) return df def select_high_performance(df: pd.DataFrame) -> pd.DataFrame: high_performance_df = df[df[\'OverallRating\'] >= 4] return high_performance_df def create_dynamic_columns(df: pd.DataFrame) -> pd.DataFrame: def map_rating(rating): if rating <= 2: return \\"Poor\\" elif rating == 3: return \\"Average\\" elif rating == 4: return \\"Good\\" else: return \\"Excellent\\" criteria_columns = [col for col in df.columns if col.startswith(\'Criterion\')] for col in criteria_columns: df[f\'{col}Category\'] = df[col].apply(map_rating) return df def group_and_aggregate(df: pd.DataFrame) -> pd.DataFrame: criteria_columns = [col for col in df.columns if col.startswith(\'Criterion\')] aggregation_functions = {col: \'mean\' for col in criteria_columns} aggregation_functions[\'ReviewID\'] = \'count\' df_agg = df.groupby(\'EmployeeID\').agg(aggregation_functions).rename(columns={\'ReviewID\': \'TotalReviews\'}) return df_agg.reset_index() def merge_insights(original_df: pd.DataFrame, aggregated_df: pd.DataFrame) -> pd.DataFrame: merged_df = original_df.merge(aggregated_df, on=\'EmployeeID\', suffixes=(\'\', \'_Avg\')) return merged_df"},{"question":"Objective Your task is to use the `SGDClassifier` from the scikit-learn library to perform binary classification on a provided dataset. The objective is to implement a complete machine learning pipeline, including data preprocessing, hyperparameter tuning, evaluation, and reporting of results. Dataset You will be using the built-in `breast cancer dataset` from scikit-learn. Tasks 1. **Data Loading and Preprocessing**: - Load the `breast cancer dataset` from `sklearn.datasets`. - Split the dataset into training and testing sets (80% training, 20% testing). - Standardize the features using `StandardScaler`. 2. **Model Training**: - Use `SGDClassifier` to train a model on the training set. - Configure the `SGDClassifier` with `loss=\'log_loss\'` for logistic regression and `penalty=\'elasticnet\'`. - Use a hyperparameter search (e.g., `GridSearchCV`) to tune the `alpha` regularization term within the range `[1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1]`. 3. **Model Evaluation**: - Evaluate the performance of the model on the testing set using appropriate metrics such as accuracy, precision, recall, and F1-score. - Report the best hyperparameters found during the search and the corresponding evaluation metrics. 4. **Implementation Details**: - Ensure to shuffle the data before fitting the model. - Use a pipeline to streamline the data preprocessing and model training processes. Expected Input and Output - **Input**: ```python def sgd_pipeline(): pass ``` - **Output**: - Print the best hyperparameters and the corresponding evaluation metrics (accuracy, precision, recall, F1-score). Code Template ```python from sklearn.datasets import load_breast_cancer from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.preprocessing import StandardScaler from sklearn.pipeline import make_pipeline from sklearn.linear_model import SGDClassifier from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score def sgd_pipeline(): # Load data data = load_breast_cancer() X, y = data.data, data.target # Split data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Create a pipeline with standardization and SGDClassifier pipeline = make_pipeline( StandardScaler(), SGDClassifier(loss=\'log_loss\', penalty=\'elasticnet\', max_iter=1000, tol=1e-3, random_state=42) ) # Setup hyperparameter grid for GridSearchCV param_grid = {\'sgdclassifier__alpha\': [1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1]} # Perform grid search with cross-validation grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring=\'accuracy\') grid_search.fit(X_train, y_train) # Best model best_model = grid_search.best_estimator_ # Predictions y_pred = best_model.predict(X_test) # Evaluation metrics accuracy = accuracy_score(y_test, y_pred) precision = precision_score(y_test, y_pred) recall = recall_score(y_test, y_pred) f1 = f1_score(y_test, y_pred) # Output results print(\'Best hyperparameters: \', grid_search.best_params_) print(f\'Accuracy: {accuracy:.4f}\') print(f\'Precision: {precision:.4f}\') print(f\'Recall: {recall:.4f}\') print(f\'F1-score: {f1:.4f}\') # Call the function sgd_pipeline() ``` This implementation assesses the student’s comprehension of: - Data preprocessing using `StandardScaler`. - Creating and tuning an `SGDClassifier` using `GridSearchCV`. - Evaluating model performance with multiple metrics.","solution":"from sklearn.datasets import load_breast_cancer from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.preprocessing import StandardScaler from sklearn.pipeline import make_pipeline from sklearn.linear_model import SGDClassifier from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score def sgd_pipeline(): # Load data data = load_breast_cancer() X, y = data.data, data.target # Split data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=True) # Create a pipeline with standardization and SGDClassifier pipeline = make_pipeline( StandardScaler(), SGDClassifier(loss=\'log_loss\', penalty=\'elasticnet\', max_iter=1000, tol=1e-3, random_state=42) ) # Setup hyperparameter grid for GridSearchCV param_grid = {\'sgdclassifier__alpha\': [1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1]} # Perform grid search with cross-validation grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring=\'accuracy\') grid_search.fit(X_train, y_train) # Best model best_model = grid_search.best_estimator_ # Predictions y_pred = best_model.predict(X_test) # Evaluation metrics accuracy = accuracy_score(y_test, y_pred) precision = precision_score(y_test, y_pred) recall = recall_score(y_test, y_pred) f1 = f1_score(y_test, y_pred) # Output results print(\'Best hyperparameters: \', grid_search.best_params_) print(f\'Accuracy: {accuracy:.4f}\') print(f\'Precision: {precision:.4f}\') print(f\'Recall: {recall:.4f}\') print(f\'F1-score: {f1:.4f}\') # Call the function sgd_pipeline()"},{"question":"Objective: Your task is to implement a function `package_details(package_name: str) -> dict` that accepts the name of an installed Python package and returns a dictionary containing detailed metadata about that package. The dictionary should include: - The **version** of the package. - A list of **console scripts** provided by the package. - The **metadata** of the package, including fields such as \'Name\', \'Version\', \'Summary\', \'Author\', and \'Author-email\'. - A list of **files** included in the package. - A list of **requirements** of the package (other packages it depends on). Function Signature: ```python def package_details(package_name: str) -> dict: pass ``` Input: - `package_name` (str): The name of the installed package. Output: - A dictionary with the following structure: ```python { \'version\': str, # Version of the package. \'console_scripts\': list, # List of console scripts provided by the package. \'metadata\': dict, # Metadata of the package. \'files\': list, # List of files included in the package. \'requirements\': list # List of requirements of the package. } ``` Constraints: - The function should handle packages with missing metadata gracefully. - Assume that the package is installed in the Python environment where the function is run. - Use the `importlib.metadata` module to access the required information. Example: Consider the package \\"requests\\", your function may return a structure similar to this: ```python { \'version\': \'2.25.1\', \'console_scripts\': [\'requests\'], \'metadata\': { \'Name\': \'requests\', \'Version\': \'2.25.1\', \'Summary\': \'Python HTTP for Humans.\', \'Author\': \'Kenneth Reitz\', \'Author-email\': \'me@kennethreitz.com\' }, \'files\': [\'requests/__init__.py\', \'requests/models.py\', ...], \'requirements\': [\'certifi>=2017.4.17\', \'chardet>=3.0.2,<5\', ...] } ``` Performance Requirements: - Your solution should be efficient in terms of accessing and processing the metadata. - Avoid unnecessary overhead by querying only relevant information. Hints: - Use `importlib.metadata.version()` to retrieve the package version. - Use `importlib.metadata.entry_points()` to discover the console scripts. - Use `importlib.metadata.metadata()` to retrieve the full metadata. - Use `importlib.metadata.files()` to list the files. - Use `importlib.metadata.requires()` to list package dependencies. Ensure to test your function with different packages to validate its correctness.","solution":"import importlib.metadata def package_details(package_name: str) -> dict: Returns detailed metadata about the specified package. try: # Getting the package version version = importlib.metadata.version(package_name) # Getting the console scripts entry_points = importlib.metadata.entry_points() console_scripts = [ep.name for ep in entry_points.select(group=\'console_scripts\') if ep.dist.name == package_name] # Getting the metadata metadata = importlib.metadata.metadata(package_name) metadata_dict = { key: metadata.get(key) for key in [\'Name\', \'Version\', \'Summary\', \'Author\', \'Author-email\'] } # Getting the files files = importlib.metadata.files(package_name) files_list = [str(file) for file in files] # Getting the requirements requirements = importlib.metadata.requires(package_name) or [] return { \'version\': version, \'console_scripts\': console_scripts, \'metadata\': metadata_dict, \'files\': files_list, \'requirements\': requirements } except importlib.metadata.PackageNotFoundError: return { \'version\': None, \'console_scripts\': [], \'metadata\': {}, \'files\': [], \'requirements\': [] }"},{"question":"# Advanced Question: Implementing a Safe, Robust Command Executor Given that the `subprocess` package in Python allows you to manage subprocesses, your task is to implement a function `execute_commands(commands)` that takes a list of system commands and executes them sequentially. This function should: 1. **Execute Each Command**: Use the `subprocess.run()` for executing each command. 2. **Capture Output and Return Code**: Collect both stdout and stderr for each command execution. 3. **Handle Errors Gracefully**: If a command fails (returns a non-zero exit code), catch the exception, log the error, and continue with the next command. 4. **Timeout for Each Command**: Introduce a timeout (e.g., 10 seconds) for command execution, and ensure any command exceeding this time limit is terminated. 5. **Return a Summary of Execution**: The function should return a list of dictionaries, with each dictionary containing: - `command`: The command executed. - `returncode`: The return code of the command. - `stdout`: Standard output of the command. - `stderr`: Standard error of the command. - `error`: Error message if the command failed or was terminated due to a timeout. # Function Signature ```python def execute_commands(commands: list[str]) -> list[dict]: pass ``` # Example ```python commands = [ \\"echo Hello, World!\\", # Should succeed \\"ls /\\", # Should succeed, listing root directory (POSIX compliant OS) \\"sleep 15\\" # Should fail due to timeout ] result = execute_commands(commands) expected_result = [ { \\"command\\": \\"echo Hello, World!\\", \\"returncode\\": 0, \\"stdout\\": \\"Hello, World!n\\", \\"stderr\\": \\"\\", \\"error\\": \\"\\" }, { \\"command\\": \\"ls /\\", \\"returncode\\": 0, \\"stdout\\": \\"...\\", # Output depends on system \\"stderr\\": \\"\\", \\"error\\": \\"\\" }, { \\"command\\": \\"sleep 15\\", \\"returncode\\": None, \\"stdout\\": \\"\\", \\"stderr\\": \\"\\", \\"error\\": \\"Command \'sleep 15\' timed out after 10 seconds.\\" } ] assert result == expected_result ``` # Constraints 1. Use the `subprocess.run()` function. 2. Set a timeout of 10 seconds for each command. 3. Ensure the function gracefully handles errors and logs them appropriately. 4. The function should be OS-agnostic, but keep in mind command compatibility (e.g., `ls` might not behave on Windows as it does on Unix systems). Implement the `execute_commands` function based on the above requirements.","solution":"import subprocess def execute_commands(commands): Execute a list of system commands, handle errors, and summarize the results. Args: commands (list of str): List of commands to execute. Returns: list of dict: A summary of execution for each command. results = [] for command in commands: result = { \\"command\\": command, \\"returncode\\": None, \\"stdout\\": \\"\\", \\"stderr\\": \\"\\", \\"error\\": \\"\\" } try: process = subprocess.run(command, shell=True, capture_output=True, text=True, timeout=10) result[\\"returncode\\"] = process.returncode result[\\"stdout\\"] = process.stdout result[\\"stderr\\"] = process.stderr except subprocess.TimeoutExpired: result[\\"error\\"] = f\\"Command \'{command}\' timed out after 10 seconds.\\" except Exception as e: result[\\"error\\"] = str(e) results.append(result) return results"},{"question":"Description: You are required to write a function in Python using the `seaborn` library to perform complex visualizations on the \'diamonds\' dataset. The function will create a faceted, customized histogram plot showcasing the distribution of diamond prices. The histogram bars should be customized to handle overlapping and should be appropriately labeled for clarity. Function Signature: ```python def plot_diamond_price_distribution(): pass ``` Requirements: 1. **Load the \'diamonds\' dataset** using seaborn. 2. **Create a faceted histogram plot** of diamond prices: - The histogram should be **log-scaled** on the x-axis. - Use the `cut` feature to facet the histogram into different subplots. 3. **Customize the appearance** of the histogram bars: - Bars should be **filled**, but with a specific edge color and edge width. - Handle overlapping bars using the **stack** transformation. - Set the `alpha` transparency according to the `clarity` feature of the diamonds. 4. **Title and Label the plots** appropriately for clear understanding. 5. Ensure that the plot generated is **displayed within the function**. Input: - The function does not take any input. Output: - The function should display the customized faceted histogram plot. Constraints: - Make sure to use `seaborn` version >= 0.11.0. - The visualization should be informative and visually appealing. # Example: A function call to `plot_diamond_price_distribution()` should display a plot with the following characteristics: - A log-scaled x-axis representing the `price` of diamonds. - A faceted layout with separate histograms for each `cut` category. - Stacked bars with edges highlighted and transparency varying by `clarity`. This task will test your ability to utilize `seaborn` for creating detailed and informative visualizations, showcase handling of overlapping data, and applying various customizations to improve plot aesthetics.","solution":"import seaborn as sns import matplotlib.pyplot as plt import numpy as np def plot_diamond_price_distribution(): # Load the \'diamonds\' dataset diamonds = sns.load_dataset(\'diamonds\') # Set up the FacetGrid for \'cut\' categories g = sns.FacetGrid(diamonds, col=\\"cut\\", col_wrap=3, margin_titles=True) # Create histogram with customization g.map_dataframe( sns.histplot, x=\\"price\\", log_scale=True, element=\\"step\\", # this creates the step histograms edgecolor=\\"black\\", linewidth=1, fill=True, alpha=0.3 ) # Customize the appearance g.set_axis_labels(\\"Log Price\\", \\"Count\\") g.set_titles(col_template=\\"{col_name} cut\\") # Set a common title plt.subplots_adjust(top=0.9) g.fig.suptitle(\'Distribution of Diamond Prices by Cut\') # Display the plot plt.show()"},{"question":"# String Formatting and Conversion in Python You are to implement a simplified version of the utilities in the `python310` package, focusing on formatting and conversion functionalities. Specifically, you will create two functions: `custom_snprintf` and `string_to_double`. 1. **Function: `custom_snprintf`** * Description: Mimics the behavior of `PyOS_snprintf` by formatting the string based on a format string and variable arguments. Ensure to handle buffer size limitations and append a null-terminating character at the end. * Input: - `size` (int): Maximum number of characters to be written, including the null-terminating character. - `format_str` (str): A format string. - `args` (tuple): A tuple containing variables to be substituted in the format string. * Output: - Returns a tuple `(result_str, rv)`, where `result_str` is the formatted string and `rv` is the number of characters written (excluding the null-terminating character). 2. **Function: `string_to_double`** * Description: Converts a string to a double, handling errors similar to `PyOS_string_to_double`. This includes detecting invalid formats and numbers that are too large to be represented as floats. * Input: - `s` (str): A string to be converted to a double. * Output: - Returns a tuple `(val, error)`, where `val` is the resulting double and `error` is an error message (if any), or `None` if the conversion is successful. # Constraints - You cannot use built-in functions like `float()` directly in `string_to_double`. - Performance is important, so ensure that your implementation handles large inputs efficiently. - For `custom_snprintf`, ensure that the formatted string does not exceed the specified buffer size. - For both functions, ensure proper error handling and consistent return values. # Examples ```python def custom_snprintf(size, format_str, args): # Your implementation here pass def string_to_double(s): # Your implementation here pass # Example usage result_str, rv = custom_snprintf(10, \\"Value: %d\\", (123,)) print(result_str, rv) # Expected output: \\"Value: 12\\", 8 val, error = string_to_double(\\"1e500\\") print(val, error) # Expected output: -1.0, \\"overflow\\" val, error = string_to_double(\\"42.42\\") print(val, error) # Expected output: 42.42, None ``` Write your implementations for `custom_snprintf` and `string_to_double`.","solution":"def custom_snprintf(size, format_str, args): Mimics the behavior of PyOS_snprintf by formatting the string based on a format string and variable arguments. Ensures to handle buffer size limitations and append a null-terminating character at the end. formatted_str = format_str % args result_str = formatted_str[:size-1] rv = min(len(formatted_str), size-1) return result_str, rv def string_to_double(s): Converts a string to a double, handling errors similar to PyOS_string_to_double. Detects invalid formats and numbers that are too large to be represented as floats. try: val = float(s) if val == float(\'inf\'): return -1.0, \\"overflow\\" return val, None except ValueError: return -1.0, \\"invalid format\\""},{"question":"Automating Source Distribution Creation You are given a directory containing a Python project, and your task is to write a Python function that automates the creation of a source distribution. The function should: 1. Generate a `MANIFEST.in` file based on specific inclusion criteria. 2. Execute the `sdist` command to create the source distribution with specified formats. 3. Allow user-defined formats, defaulting to `.gztar` and `.zip`. # Function Signature ```python def create_source_distribution(project_dir: str, formats: list[str] = [\\"gztar\\", \\"zip\\"], include_patterns: list[str] = None) -> None: pass ``` # Input and Output - **project_dir**: A string representing the path to the directory containing the Python project. - **formats**: A list of strings representing the desired formats for the source distribution (`default`: `[\\"gztar\\", \\"zip\\"]`). - **include_patterns**: A list of strings representing the file patterns to include in the `MANIFEST.in` file (e.g., `[\\"*.py\\", \\"*.txt\\"]`). If `None`, default to including all `.py` files and README files (`default`: `[\\"*.py\\", \\"README*\\", \\"setup.py\\", \\"setup.cfg\\"]`). The function does not return anything but should create the source distribution files in the project\'s directory. # Constraints - The function should handle the creation of the `MANIFEST.in` file properly, ensuring it includes the specified patterns. - The function should ensure the `sdist` command runs successfully and creates the distribution archives in the specified formats. - The function should validate the formats to ensure they are among the supported formats. # Example ```python create_source_distribution( project_dir=\\"/path/to/project\\", formats=[\\"gztar\\", \\"zip\\", \\"bztar\\"], include_patterns=[\\"*.py\\", \\"README.md\\", \\"docs/*\\"] ) ``` This should: 1. Create a `MANIFEST.in` file in `/path/to/project` with the given include patterns. 2. Run the `sdist` command to create source distributions in `gztar`, `zip`, and `bztar` formats. # Explanation 1. **MANIFEST.in**: The function should create this file if it does not exist and write the given patterns. 2. **sdist Command**: Use the `subprocess` module to execute the `sdist` command with the specified formats. **Note**: Ensure proper error handling, especially for command execution and file operations.","solution":"import os import subprocess def create_source_distribution(project_dir: str, formats: list[str] = [\\"gztar\\", \\"zip\\"], include_patterns: list[str] = None) -> None: Creates a source distribution for a Python project. Args: project_dir: Path to the directory containing the Python project. formats: List of desired formats for the source distribution. include_patterns: List of file patterns to include in the MANIFEST.in file. if include_patterns is None: include_patterns = [\\"*.py\\", \\"README*\\", \\"setup.py\\", \\"setup.cfg\\"] manifest_path = os.path.join(project_dir, \\"MANIFEST.in\\") with open(manifest_path, \\"w\\") as manifest_file: for pattern in include_patterns: manifest_file.write(f\\"include {pattern}n\\") setup_py_path = os.path.join(project_dir, \\"setup.py\\") if not os.path.exists(setup_py_path): raise FileNotFoundError(f\\"Could not find setup.py at {setup_py_path}\\") cmd = [\\"python\\", \\"setup.py\\", \\"sdist\\", \\"--formats=\\" + \\",\\".join(formats)] subprocess.run(cmd, check=True, cwd=project_dir)"},{"question":"# Naive Bayes Classifiers Implementation and Comparison Objective: Implement and compare the performance of different Naive Bayes classifiers from the scikit-learn library on a dataset. Dataset: You will use the Iris dataset, which is available in the `sklearn.datasets` module. The dataset consists of 150 samples of iris flowers, each with 4 features: sepal length, sepal width, petal length, and petal width. There are 3 classes, each representing a different species of iris flower. Tasks: 1. **Data Loading and Preparation**: - Load the Iris dataset from `sklearn.datasets`. - Split the dataset into training and testing sets (70% train, 30% test). 2. **Implement Naive Bayes Classifiers**: - Implement the following Naive Bayes classifiers using the scikit-learn library: - GaussianNB - MultinomialNB (Note: The features in the Iris dataset may need to be scaled or adjusted to fit the requirements of MultinomialNB, which expects non-negative values.) - ComplementNB - BernoulliNB (Note: You may need to binarize the features.) - CategoricalNB (Note: You may need to encode the features as categorical values.) 3. **Model Training and Evaluation**: - Train each classifier on the training set. - Evaluate the performance of each classifier on the testing set using accuracy score. - Print the number of mislabeled points and the accuracy score for each classifier. 4. **Comparison and Analysis**: - Compare the performance of the different classifiers in terms of accuracy. - Discuss the reasons why certain classifiers may perform better or worse on this dataset. Expected Input and Output Formats: - **Input**: None (The dataset is loaded within the script). - **Output**: ``` Classifier: <Name> Number of mislabeled points out of a total <n> points: <m> Accuracy Score: <accuracy> ``` Constraints: - Use scikit-learn\'s implementation of Naive Bayes classifiers. - Ensure correct data preprocessing to match the input requirements of each classifier. Implementation: ```python from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.naive_bayes import GaussianNB, MultinomialNB, ComplementNB, BernoulliNB, CategoricalNB from sklearn.preprocessing import Binarizer, OrdinalEncoder from sklearn.metrics import accuracy_score # Load the Iris dataset iris = load_iris() X, y = iris.data, iris.target # Split the dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Initialize classifiers classifiers = { \'GaussianNB\': GaussianNB(), \'MultinomialNB\': MultinomialNB(), \'ComplementNB\': ComplementNB(), \'BernoulliNB\': BernoulliNB(), \'CategoricalNB\': CategoricalNB() } # Prepare the data for different classifiers # For BernoulliNB, we need binary features binarizer = Binarizer() X_train_bin = binarizer.fit_transform(X_train) X_test_bin = binarizer.transform(X_test) # For CategoricalNB, we need encoded features encoder = OrdinalEncoder() X_train_cat = encoder.fit_transform(X_train) X_test_cat = encoder.transform(X_test) # Train and evaluate each classifier for name, clf in classifiers.items(): if name == \'BernoulliNB\': clf.fit(X_train_bin, y_train) y_pred = clf.predict(X_test_bin) elif name == \'CategoricalNB\': clf.fit(X_train_cat, y_train) y_pred = clf.predict(X_test_cat) else: clf.fit(X_train, y_train) y_pred = clf.predict(X_test) mislabeled_points = (y_test != y_pred).sum() accuracy = accuracy_score(y_test, y_pred) print(f\\"Classifier: {name}\\") print(f\\"Number of mislabeled points out of a total {X_test.shape[0]} points: {mislabeled_points}\\") print(f\\"Accuracy Score: {accuracy:.2f}\\") print() ```","solution":"from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.naive_bayes import GaussianNB, MultinomialNB, ComplementNB, BernoulliNB, CategoricalNB from sklearn.preprocessing import Binarizer, OrdinalEncoder from sklearn.metrics import accuracy_score def load_and_prepare_data(): # Load the Iris dataset iris = load_iris() X, y = iris.data, iris.target # Split the dataset into training and testing sets return train_test_split(X, y, test_size=0.3, random_state=42) def evaluate_classifier(clf, X_train, X_test, y_train, y_test, transform_func=None): if transform_func: X_train = transform_func(X_train) X_test = transform_func(X_test) clf.fit(X_train, y_train) y_pred = clf.predict(X_test) mislabeled_points = (y_test != y_pred).sum() accuracy = accuracy_score(y_test, y_pred) return mislabeled_points, accuracy def main(): X_train, X_test, y_train, y_test = load_and_prepare_data() # Initialize classifiers classifiers = { \'GaussianNB\': GaussianNB(), \'MultinomialNB\': MultinomialNB(), \'ComplementNB\': ComplementNB(), \'BernoulliNB\': BernoulliNB(), \'CategoricalNB\': CategoricalNB() } # Binarizer and OrdinalEncoder preparation binarizer = Binarizer() encoder = OrdinalEncoder() # Train and evaluate each classifier for name, clf in classifiers.items(): if name == \'BernoulliNB\': mislabeled_points, accuracy = evaluate_classifier(clf, X_train, X_test, y_train, y_test, binarizer.fit_transform) elif name == \'CategoricalNB\': mislabeled_points, accuracy = evaluate_classifier(clf, X_train, X_test, y_train, y_test, encoder.fit_transform) else: mislabeled_points, accuracy = evaluate_classifier(clf, X_train, X_test, y_train, y_test) print(f\\"Classifier: {name}\\") print(f\\"Number of mislabeled points out of a total {X_test.shape[0]} points: {mislabeled_points}\\") print(f\\"Accuracy Score: {accuracy:.2f}\\") print() if __name__ == \\"__main__\\": main()"},{"question":"**File Structure Analysis and Manipulation Using `pathlib`** **Objective:** To assess the understanding and application of the `pathlib` module, students are required to implement a function that analyzes and manipulates a given directory structure. **Task:** Write a Python function `analyze_directory_structure` that takes a directory path as input and performs the following tasks: 1. **List All Files with Specific Extensions:** - Identify all files within the directory (including subdirectories) that have the specified file extensions. - Parameter: `extensions` - A set of file extensions to filter by (e.g., `{\'.py\', \'.txt\'}`). - Return: List of paths to these files, sorted in ascending order. 2. **Calculate Total Size:** - Calculate the total size of all files in bytes within the directory (including subdirectories). - Return: Total size as an integer. 3. **Generate a Summary Report:** - Create a summary report (as a dictionary) that includes: - Total number of files. - Number of directories. - Number of files by extension provided in `extensions`. - Total size of files. - Largest file by size. - Smallest file by size. **Function Signature:** ```python from pathlib import Path from typing import List, Set, Dict def analyze_directory_structure(directory: Path, extensions: Set[str]) -> Dict: pass ``` **Constraints:** - The function should handle any valid directory path. - The directory may contain nested subdirectories. - The function must use the `pathlib` module exclusively for path operations. - Ensure the function handles different filesystems (e.g., Windows and Posix). **Example Usage:** ```python from pathlib import Path extensions = {\'.py\', \'.txt\'} directory = Path(\'/path/to/directory\') result = analyze_directory_structure(directory, extensions) print(result) ``` **Expected Output:** ```python { \'total_files\': 50, \'total_directories\': 10, \'files_by_extension\': { \\".py\\": 20, \\".txt\\": 15, \\".other\\": 15 }, \'total_size\': 105000, \'largest_file\': \'/path/to/directory/largest_file.py\', \'smallest_file\': \'/path/to/directory/smallest_file.txt\' } ``` **Note:** - Raise appropriate exceptions if the provided directory path does not exist or is not a directory. - Ensure the solution is efficient and written clearly.","solution":"from pathlib import Path from typing import List, Set, Dict def analyze_directory_structure(directory: Path, extensions: Set[str]) -> Dict: if not directory.exists(): raise FileNotFoundError(f\\"The directory {directory} does not exist.\\") if not directory.is_dir(): raise NotADirectoryError(f\\"The path {directory} is not a directory.\\") files = [] total_size = 0 files_by_extension = {ext: 0 for ext in extensions} files_by_extension[\\".other\\"] = 0 for file in directory.rglob(\'*\'): if file.is_file(): files.append(file) total_size += file.stat().st_size file_ext = file.suffix if file_ext in extensions: files_by_extension[file_ext] += 1 else: files_by_extension[\\".other\\"] += 1 if files: largest_file = max(files, key=lambda x: x.stat().st_size) smallest_file = min(files, key=lambda x: x.stat().st_size) else: largest_file = smallest_file = None report = { \'total_files\': len(files), \'total_directories\': len([d for d in directory.rglob(\'*\') if d.is_dir()]), \'files_by_extension\': files_by_extension, \'total_size\': total_size, \'largest_file\': str(largest_file) if largest_file else None, \'smallest_file\': str(smallest_file) if smallest_file else None } return report"},{"question":"# Question You are provided with two datasets and you need to visualize them using seaborn. Follow the steps below to complete this task. **Dataset 1:** (Long-form) ```plaintext year,month,passengers 1949,January,112 1949,February,118 ... 1960,December,432 ``` **Dataset 2:** (Wide-form) ```plaintext year,January,February,March,April,May,June,July,August,September,October,November,December 1949,112,118,132,129,121,135,148,148,136,119,104,118 ... 1960,390,432,511,602,707,773,796,882,932,1020,1203,1301 ``` # Tasks 1. **Load the datasets**: - Convert the first dataset (given in long-form) into a pandas DataFrame. - Convert the second dataset (given in wide-form) into a pandas DataFrame. 2. **Convert Data Formats**: - Transform the long-form dataset into a wide-form dataset using pandas. - Transform the wide-form dataset into a long-form dataset using pandas. 3. **Data Visualization**: - Generate the following plots using seaborn: 1. Plot the number of passengers over the years for each month using the long-form data. 2. Plot the number of passengers for each month across different years using the wide-form data. 4. **Advanced Visualization**: - Create a single plot that combines the visualizations of the number of passengers over the years and for each month. # Input and Output **Input:** - Read the long-form dataset from a CSV file named `flights_long.csv`. - Read the wide-form dataset from a CSV file named `flights_wide.csv`. **Output:** - Display the required plots using seaborn. - Save the combined plot as `combined_visualization.png`. # Constraints: - Use seaborn version 0.11.1 or later. - The plots should be properly labeled and have appropriate legends. # Hints: - Use `pd.read_csv` to load the datasets. - Use `sns.relplot` or other seaborn functions for visualization. - Use the `pivot` and `melt` functions from pandas for data transformation. # Example Here is an example of how to convert long-form to wide-form and vice versa using pandas: ```python import pandas as pd import seaborn as sns import matplotlib.pyplot as plt # Load the datasets flights_long = pd.read_csv(\'flights_long.csv\') flights_wide = pd.read_csv(\'flights_wide.csv\') # Convert long-form to wide-form flights_wide_from_long = flights_long.pivot(index=\'year\', columns=\'month\', values=\'passengers\') # Convert wide-form to long-form flights_long_from_wide = flights_wide.melt(id_vars=[\'year\'], var_name=\'month\', value_name=\'passengers\') # Visualize the data sns.set_theme(style=\\"darkgrid\\") # Long-form plot sns.relplot(data=flights_long, x=\\"year\\", y=\\"passengers\\", hue=\\"month\\", kind=\\"line\\") plt.title(\'Monthly Passengers Over Years (Long-form)\') plt.show() # Wide-form plot sns.relplot(data=flights_wide_from_long, kind=\\"line\\") plt.title(\'Monthly Passengers Over Years (Wide-form)\') plt.show() ``` Use similar steps to implement all tasks.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def load_datasets(long_path, wide_path): Load the datasets from the given CSV file paths. :param long_path: Path to the long-form CSV file :param wide_path: Path to the wide-form CSV file :return: long-form dataframe, wide-form dataframe flights_long = pd.read_csv(long_path) flights_wide = pd.read_csv(wide_path) return flights_long, flights_wide def convert_long_to_wide(flights_long): Convert the long-form DataFrame to a wide-form DataFrame. :param flights_long: Long-form DataFrame :return: Wide-form DataFrame flights_wide = flights_long.pivot(index=\'year\', columns=\'month\', values=\'passengers\') return flights_wide def convert_wide_to_long(flights_wide): Convert the wide-form DataFrame to a long-form DataFrame. :param flights_wide: Wide-form DataFrame :return: Long-form DataFrame flights_long = flights_wide.melt(id_vars=[\'year\'], var_name=\'month\', value_name=\'passengers\') return flights_long def plot_long_form(flights_long): Plot the long-form data. :param flights_long: Long-form DataFrame sns.set_theme(style=\\"darkgrid\\") plt.figure(figsize=(12, 6)) sns.lineplot(data=flights_long, x=\\"year\\", y=\\"passengers\\", hue=\\"month\\") plt.title(\'Monthly Passengers Over Years (Long-form)\') plt.show() def plot_wide_form(flights_wide): Plot the wide-form data. :param flights_wide: Wide-form DataFrame sns.set_theme(style=\\"darkgrid\\") plt.figure(figsize=(12, 6)) sns.lineplot(data=flights_wide, dashes=False) plt.title(\'Monthly Passengers Over Years (Wide-form)\') plt.show() def combined_plot(flights_long, flights_wide): Create a combined plot for long-form and wide-form data sequences. :param flights_long: Long-form DataFrame :param flights_wide: Wide-form DataFrame fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(15, 10), sharex=False) sns.set_theme(style=\\"darkgrid\\") sns.lineplot(data=flights_long, x=\\"year\\", y=\\"passengers\\", hue=\\"month\\", ax=axes[0]) axes[0].set_title(\'Monthly Passengers Over Years (Long-form)\') for month in flights_wide.columns[1:]: axes[1].plot(flights_wide[\'year\'], flights_wide[month], label=month) axes[1].set_title(\'Monthly Passengers Over Years (Wide-form)\') axes[1].legend(title=\'Month\') plt.tight_layout() plt.savefig(\'combined_visualization.png\') plt.show()"},{"question":"# Question: Backup and Restore Utility You are required to implement two functions, `backup_files` and `restore_files`, for a simple backup and restore utility that uses the `tempfile` module to manage temporary files and directories. `backup_files(input_files: list, backup_dir: str) -> dict` This function takes in a list of file paths `input_files` and a string `backup_dir`. It creates a temporary directory within `backup_dir` and copies each file in `input_files` into this temporary directory. The copied files should have the same names as the original files. The function returns a dictionary where the keys are the original file paths and the values are the paths of the corresponding files in the temporary directory. - **Input:** - `input_files`: A list of strings representing paths to the files that need to be backed up. - `backup_dir`: A string representing the directory where the temporary backup directory should be created. - **Output:** - A dictionary mapping original file paths to the paths of the corresponding copied files in the temporary directory. - **Constraints:** - All input file paths are valid and the files exist. - The backup directory path is valid and writable. `restore_files(backup_map: dict) -> None` This function takes in the dictionary returned by `backup_files` and restores each file back to its original path. It should overwrite the original files with the copies in the temporary directory. - **Input:** - `backup_map`: A dictionary where keys are the original file paths and the values are the paths of the corresponding files in the temporary directory. - **Output:** - None - **Constraints:** - The paths in the dictionary are valid and the corresponding files exist in the backup directory. # Example Usage ```python import os # Assume the following files exist: \'file1.txt\', \'file2.txt\' input_files = [\'file1.txt\', \'file2.txt\'] backup_dir = \'/tmp\' # Backup files backup_map = backup_files(input_files, backup_dir) # Modify originals or simulate data loss os.remove(\'file1.txt\') os.remove(\'file2.txt\') # Restore files restore_files(backup_map) ``` # Implementation Details - You must use the `tempfile.TemporaryDirectory` context manager for creating the temporary backup directory. - You can use `shutil.copy2` to copy files as it preserves file metadata. - Your code should handle both text and binary files. Good luck and ensure to handle edge cases properly and manage resources effectively!","solution":"import os import shutil import tempfile def backup_files(input_files, backup_dir): Backs up the input_files to a temporary directory within backup_dir. Args: - input_files (list[str]): Paths of the files to back up. - backup_dir (str): Directory where the temporary backup should be created. Returns: - dict: Mapping of original file paths to the paths of the backup files. backup_map = {} with tempfile.TemporaryDirectory(dir=backup_dir) as temp_dir: for file_path in input_files: filename = os.path.basename(file_path) backup_path = os.path.join(temp_dir, filename) shutil.copy2(file_path, backup_path) backup_map[file_path] = backup_path # Make sure the temporary directory remains after the context is exited final_temp_dir = tempfile.mkdtemp(dir=backup_dir) for src, dest in backup_map.items(): final_dest = os.path.join(final_temp_dir, os.path.basename(dest)) shutil.move(dest, final_dest) backup_map[src] = final_dest return backup_map def restore_files(backup_map): Restores the files from the temporary backup to their original locations. Args: - backup_map (dict): Mapping of original file paths to the paths of the backup files. Returns: - None for original_path, backup_path in backup_map.items(): shutil.copy2(backup_path, original_path)"},{"question":"**Objective**: Use the `zipapp` module to create an executable Python archive from a directory of Python files and dependencies that meets specified conditions. Problem Statement: You are provided with a directory `myapp` which contains the following files: 1. `__main__.py`: The entry point for the application. 2. `module1.py`: A module used by the application. 3. `requirements.txt`: A file containing the list of dependencies for the application. You need to create an executable Python archive (`.pyz` file) from this directory, with the following conditions: 1. The archive should specify `\\"/usr/bin/env python3\\"` as the Python interpreter. 2. Only `.py` files should be included in the archive, excluding any other files. 3. The files should be compressed to reduce the size of the archive. 4. The resulting archive should be named `myapp.pyz` and should be executable without explicitly calling the Python interpreter. Requirements: 1. **Input**: - The path to the `myapp` directory. 2. **Output**: - An executable `.pyz` file named `myapp.pyz`. 3. **Constraints**: - The `myapp` directory contains valid Python code. - All dependencies listed in `requirements.txt` can be installed. 4. **Performance Requirements**: - The solution should handle directories with a large number of files efficiently. Task: Write a Python script that performs the following steps: 1. **Install Dependencies**: - Use `pip` to install the dependencies listed in `requirements.txt` into the `myapp` directory. 2. **Create a Custom Filter Function**: - Define a filter function that includes only `.py` files. 3. **Create the Archive**: - Use the `zipapp.create_archive` function to create the `myapp.pyz` archive, with the specified interpreter, using the custom filter function, and compress the files. 4. **Make the Archive Executable**: - Ensure that the resulting `myapp.pyz` can be executed directly. Example: Below is an example of how the script might be called and its expected behavior. ```python import zipapp import subprocess def install_dependencies(directory): Install dependencies listed in requirements.txt subprocess.run([\\"python3\\", \\"-m\\", \\"pip\\", \\"install\\", \\"-r\\", f\\"{directory}/requirements.txt\\", \\"--target\\", directory]) def include_py_files(path): Filter function to include only .py files return path.suffix == \'.py\' def create_executable_archive(source_dir): Create the executable archive with specified conditions zipapp.create_archive( source=source_dir, target=\'myapp.pyz\', interpreter=\'/usr/bin/env python3\', main=None, # Assuming __main__.py is correctly set up in source_dir filter=include_py_files, compressed=True ) if __name__ == \\"__main__\\": source_directory = \'myapp\' install_dependencies(source_directory) create_executable_archive(source_directory) print(\\"Executable archive \'myapp.pyz\' created successfully.\\") ``` Ensure that your solution meets all the requirements specified above and handles edge cases appropriately. Good luck!","solution":"import zipapp import subprocess import pathlib def install_dependencies(directory): Install dependencies listed in requirements.txt subprocess.check_call([\\"python3\\", \\"-m\\", \\"pip\\", \\"install\\", \\"-r\\", f\\"{directory}/requirements.txt\\", \\"--target\\", directory]) def include_py_files(path): Filter function to include only .py files return path.suffix == \'.py\' def create_executable_archive(source_dir): Create the executable archive with specified conditions zipapp.create_archive( source=source_dir, target=\'myapp.pyz\', interpreter=\'/usr/bin/env python3\', main=None, # Assuming __main__.py is correctly set up in source_dir filter=include_py_files, compressed=True ) if __name__ == \\"__main__\\": source_directory = \'myapp\' # Ensure directory exists if not pathlib.Path(source_directory).exists(): raise FileNotFoundError(f\\"The specified directory \'{source_directory}\' does not exist.\\") install_dependencies(source_directory) create_executable_archive(source_directory) print(\\"Executable archive \'myapp.pyz\' created successfully.\\")"},{"question":"You are tasked with creating a visual analysis of a dataset using seaborn. The exercise will test your ability to customize the appearance of seaborn plots by changing the styles and specific styling parameters. Requirements: 1. **Create a function called `customize_and_plot` that takes the following arguments:** - `x` (list): a list of categories for the x-axis. - `y` (list): a list of values for the y-axis. - `plot_type` (str): the type of plot to create. It can be one of the following values: `\\"bar\\"` or `\\"line\\"`. - `style` (str): the style to apply to the plot. It can be one of the following values: `\\"whitegrid\\"`, `\\"darkgrid\\"`, `\\"white\\"`, `\\"dark\\"`, or `\\"ticks\\"`. - `style_params` (dict): a dictionary containing additional style parameters to customize. This could be empty. 2. **Your function should:** - Set the style using `sns.set_style` with the given `style`. - Apply additional style parameters from `style_params` if provided. - Create the appropriate plot (`barplot` or `lineplot`) using the `x` and `y` data. - Display the plot. Constraints: - The lengths of `x` and `y` will always be equal. - The `plot_type` will always be either \\"bar\\" or \\"line\\". - The `style` will always be one of the specified styles. - `style_params` will be a valid dictionary applicable to seaborn\'s style customization. Function Signature: ```python def customize_and_plot(x: list, y: list, plot_type: str, style: str, style_params: dict) -> None: pass ``` # Example: ```python # Example usage: x = [\\"A\\", \\"B\\", \\"C\\"] y = [1, 3, 2] plot_type = \\"bar\\" style = \\"whitegrid\\" style_params = {\\"grid.color\\": \\".6\\", \\"grid.linestyle\\": \\":\\"} customize_and_plot(x, y, plot_type, style, style_params) # This will set the style to \\"whitegrid\\" with customized grid color and linestyle, # and then generate a bar plot with the given data. ``` Notes: - The output should be a plot displayed with the specified configurations. - Ensure that your implementation makes use of seaborn\'s `set_style` and plotting functions as shown in the documentation provided.","solution":"import seaborn as sns import matplotlib.pyplot as plt def customize_and_plot(x: list, y: list, plot_type: str, style: str, style_params: dict) -> None: Customize and plot data using seaborn with various styles and parameters. Args: x (list): List of categories for the x-axis. y (list): List of values for the y-axis. plot_type (str): Type of plot to create (\\"bar\\" or \\"line\\"). style (str): Style to apply to the plot (\\"whitegrid\\", \\"darkgrid\\", \\"white\\", \\"dark\\", or \\"ticks\\"). style_params (dict): Dictionary containing additional style parameters to customize. # Set the style using sns.set_style and the given style parameter sns.set_style(style) # Apply additional style parameters if provided if style_params: sns.set_context(style_params) # Create the specified type of plot if plot_type == \\"bar\\": sns.barplot(x=x, y=y) elif plot_type == \\"line\\": sns.lineplot(x=x, y=y) # Display the plot plt.show()"},{"question":"**Objective:** Implement a Python function that compresses a text file using the bzip2 compression algorithm and then decompresses it to verify the integrity of the original content. This exercise will assess your understanding of handling file I/O operations and working with the `bz2` module for compression and decompression. **Task:** Write a function `compress_and_verify(input_file: str, compressed_file: str, decompressed_file: str) -> bool` that takes the path to an input text file, compresses it, writes the compressed data to a file, then decompresses the data and writes it to another file before verifying that the decompressed content matches the original input file content. **Function Signature:** ```python def compress_and_verify(input_file: str, compressed_file: str, decompressed_file: str) -> bool: pass ``` **Input:** - `input_file` (str): Path to the input text file to be compressed. - `compressed_file` (str): Path where the compressed file should be saved. - `decompressed_file` (str): Path where the decompressed file should be saved. **Output:** - Returns `True` if the decompressed content matches the original content; `False` otherwise. **Constraints:** - Ensure that the function handles any exceptions that might occur during file operations, such as file not found or read/write errors. - Use bzip2 compression level 9 for the best compression ratio. **Example:** ```python # Assume `input.txt` exists and contains some text data. result = compress_and_verify(\\"input.txt\\", \\"compressed.bz2\\", \\"output.txt\\") print(result) # Should print True if decompression is correct. ``` **Detailed Steps:** 1. Read the content of `input_file`. 2. Compress the content using the `bz2` module and write it to `compressed_file`. 3. Read the compressed data from `compressed_file` and decompress it. 4. Write the decompressed data to `decompressed_file`. 5. Compare the content of `input_file` and `decompressed_file` to check if they match. **Hints:** - Use the `bz2.open` function to handle reading and writing compressed files. - Read the entire content of the input file and write/read in binary mode for both compression and decompression.","solution":"import bz2 def compress_and_verify(input_file: str, compressed_file: str, decompressed_file: str) -> bool: try: # Read original content with open(input_file, \'rb\') as f: original_content = f.read() # Compress content with bz2.open(compressed_file, \'wb\', compresslevel=9) as f: f.write(original_content) # Decompress content with bz2.open(compressed_file, \'rb\') as f: decompressed_content = f.read() # Write decompressed content to output file with open(decompressed_file, \'wb\') as f: f.write(decompressed_content) # Verify the decompressed content is the same as the original content return original_content == decompressed_content except Exception as e: print(f\\"An error occurred: {e}\\") return False"},{"question":"# Unittest Module Coding Assessment In this assessment, you will demonstrate your understanding of the `unittest` module in Python by writing and organizing tests for a few simple functions. Additionally, you will use various features of `unittest` such as setup and teardown methods, skipping tests, expected failures, and creating test suites. Provided Functions You are given the following functions that you need to write tests for: ```python # Basic math operations def add(a, b): return a + b def divide(a, b): if b == 0: raise ValueError(\\"Division by zero is not allowed\\") return a / b def factorial(n): if n < 0: raise ValueError(\\"Negative numbers do not have factorials\\") if n == 0 or n == 1: return 1 result = 1 for i in range(2, n + 1): result *= i return result ``` Tasks 1. **Create Test Cases:** - Write a class `TestMathOperations` that extends `unittest.TestCase`. - Write tests for the `add` function, including tests for positive numbers, negative numbers, and zero. - Write tests for the `divide` function, including tests for division by a non-zero number and tests that verify it raises a `ValueError` when dividing by zero. - Write tests for the `factorial` function, including tests for positive integers, zero, and verify it raises a `ValueError` for negative numbers. 2. **Setup and Teardown Methods:** - Implement the `setUp` method to initialize any resources needed for the tests. - Implement the `tearDown` method to clean up any resources initialized in `setUp`. 3. **Skipping Tests and Expected Failures:** - Write a test for the `divide` function that should be skipped if a certain condition (e.g., an environment variable or a certain value) is not met. - Write a test for the `factorial` function that is expected to fail because of a deliberate bug (e.g., `factorial(5)` should incorrectly return `100`). 4. **Creating a Test Suite:** - Create a test suite `MathTestSuite` that aggregates all the tests from the `TestMathOperations` class. - Write a script to run the test suite with increased verbosity. 5. **Command Line Interface:** - Provide command-line examples to run all the tests using the `unittest` module\'s command-line interface. Use options to run with higher verbosity. Constraints - Use the `unittest` module exclusively for writing tests. - Ensure your test names follow the `test_*` naming convention so that `unittest` can automatically discover them. Expected Output The script should output a detailed pass/fail report for each test case, including skipped and expected failures, providing meaningful messages where applicable. Example test suite execution: ```shell python -m unittest -v my_test_module.py ``` The output should look similar to: ``` test_add_positive_numbers (my_test_module.TestMathOperations) ... ok test_add_negative_numbers (my_test_module.TestMathOperations) ... ok test_divide_by_zero (my_test_module.TestMathOperations) ... ok test_divide_by_nonzero (my_test_module.TestMathOperations) ... ok test_factorial_of_negative_number (my_test_module.TestMathOperations) ... ok test_factorial_of_zero (my_test_module.TestMathOperations) ... ok test_factorial_of_positive_number (my_test_module.TestMathOperations) ... ok test_expected_failure_factorial (my_test_module.TestMathOperations) ... expected failure test_skipped_divide (my_test_module.TestMathOperations) ... skipped \'division by zero skipped.\' ---------------------------------------------------------------------- Ran 9 tests in 0.005s OK (skipped=1, expected failures=1) ```","solution":"# Basic math operations def add(a, b): return a + b def divide(a, b): if b == 0: raise ValueError(\\"Division by zero is not allowed\\") return a / b def factorial(n): if n < 0: raise ValueError(\\"Negative numbers do not have factorials\\") if n == 0 or n == 1: return 1 result = 1 for i in range(2, n + 1): result *= i return result"}]'),z={name:"App",components:{PoemCard:A},data(){return{searchQuery:"",visibleCount:4,poemsData:I,isLoading:!1}},computed:{filteredPoems(){const s=this.searchQuery.trim().toLowerCase();return s?this.poemsData.filter(e=>e.question&&e.question.toLowerCase().includes(s)||e.solution&&e.solution.toLowerCase().includes(s)):this.poemsData},displayedPoems(){return this.searchQuery.trim()?this.filteredPoems:this.filteredPoems.slice(0,this.visibleCount)},hasMorePoems(){return!this.searchQuery.trim()&&this.visibleCount<this.poemsData.length}},methods:{async loadMore(){this.isLoading=!0,await new Promise(s=>setTimeout(s,1e3)),this.visibleCount+=4,this.isLoading=!1}}},R={class:"search-container"},D={class:"card-container"},F={key:0,class:"empty-state"},q=["disabled"],N={key:0},M={key:1};function O(s,e,l,m,n,r){const h=_("PoemCard");return a(),i("section",null,[e[4]||(e[4]=t("div",{class:"top-banner"},[t("div",{class:"top-banner-title"},[t("div",{class:"top-banner-title-text"},"🤔prompts chat🧠")])],-1)),t("div",R,[e[3]||(e[3]=t("span",{class:"search-icon"},"🔍",-1)),g(t("input",{type:"text",class:"search-input","onUpdate:modelValue":e[0]||(e[0]=o=>n.searchQuery=o),placeholder:"Search..."},null,512),[[y,n.searchQuery]]),n.searchQuery?(a(),i("button",{key:0,class:"clear-search",onClick:e[1]||(e[1]=o=>n.searchQuery="")}," ✕ ")):d("",!0)]),t("div",D,[(a(!0),i(b,null,v(r.displayedPoems,(o,f)=>(a(),w(h,{key:f,poem:o},null,8,["poem"]))),128)),r.displayedPoems.length===0?(a(),i("div",F,' No results found for "'+c(n.searchQuery)+'". ',1)):d("",!0)]),r.hasMorePoems?(a(),i("button",{key:0,class:"load-more-button",disabled:n.isLoading,onClick:e[2]||(e[2]=(...o)=>r.loadMore&&r.loadMore(...o))},[n.isLoading?(a(),i("span",M,"Loading...")):(a(),i("span",N,"See more"))],8,q)):d("",!0)])}const L=p(z,[["render",O],["__scopeId","data-v-44fb5169"]]),Y=JSON.parse('{"title":"","description":"","frontmatter":{"page":true},"headers":[],"relativePath":"chatai/46.md","filePath":"chatai/46.md"}'),j={name:"chatai/46.md"},H=Object.assign(j,{setup(s){return(e,l)=>(a(),i("div",null,[x(L)]))}});export{Y as __pageData,H as default};
