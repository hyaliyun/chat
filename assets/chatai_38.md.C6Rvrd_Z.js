import{_ as p,o as a,c as s,a as t,m as u,t as c,C as _,M as g,U as y,f as d,F as b,p as v,e as w,q as k}from"./chunks/framework.B1z0IdBH.js";const x={name:"PoemCard",props:{poem:{type:Object,required:!0}}},T={class:"poem-container"},C={class:"review"},P={class:"review-title"},E={class:"review-content"};function S(i,e,l,m,n,o){return a(),s("div",T,[t("div",C,[t("div",P,[e[0]||(e[0]=t("span",{class:"icon"},"question:",-1)),u(c(l.poem.question),1)]),e[2]||(e[2]=t("div",{class:"divider"},null,-1)),t("p",E,[e[1]||(e[1]=t("span",{class:"icon"},"answer:",-1)),u(c(l.poem.solution),1)])])])}const I=p(x,[["render",S],["__scopeId","data-v-f3a85549"]]),A=JSON.parse('[{"question":"**Curses-based Tic-Tac-Toe Game** *Objective*: Write a Python program to implement a Tic-Tac-Toe game for two players using the `curses` library. **Requirements**: 1. The game should initialize the curses environment and properly clean up the terminal settings upon exit. 2. It should display a 3x3 grid representing the Tic-Tac-Toe board. 3. Players should be able to use arrow keys to navigate the grid and press the space bar to make a move. 4. The game should alternate players (Player 1 uses \'X\' and Player 2 uses \'O\'). 5. The program should check for win conditions after every move and declare the winner. 6. If the board is full and no player has won, the game should declare a draw. 7. The game should properly handle the terminal settings even if an unexpected error occurs. **Function Specifications**: - **Function**: `init_board(stdscr)` - **Description**: Initializes the Tic-Tac-Toe board. - **Input**: Window object `stdscr` representing the terminal screen. - **Output**: None - **Function**: `draw_board(stdscr, board, current_row, current_col)` - **Description**: Draws the current state of the Tic-Tac-Toe board. - **Input**: Window object `stdscr`, 3x3 list `board` representing the board state, integers `current_row` and `current_col` indicating the current cursor position. - **Output**: None - **Function**: `check_winner(board)` - **Description**: Checks if there is a winner on the current board. - **Input**: 3x3 list `board`. - **Output**: Return \'X\' if Player 1 wins, \'O\' if Player 2 wins, and `None` if there is no winner. - **Function**: `main(stdscr)` - **Description**: The main game loop function. Handles the user input and game logic. - **Input**: Window object `stdscr`. - **Output**: None **Additional Constraints**: 1. You must use the `curses` library and its functions to handle screen updates and user input. 2. Use `try...except` blocks to ensure that the terminal settings are restored even if the program encounters an error. 3. Format your code for readability with clear comments explaining each function. Here is the structure for your solution: ```python import curses def init_board(stdscr): # Initialize the game board pass def draw_board(stdscr, board, current_row, current_col): # Draw the current state of the board on the window pass def check_winner(board): # Check if there is a winner or if a draw occurred pass def main(stdscr): # Main game logic curses.curs_set(0) stdscr.nodelay(0) stdscr.timeout(100) board = [[\' \' for _ in range(3)] for _ in range(3)] current_row, current_col = 0, 0 current_player = \'X\' game_over = False while not game_over: draw_board(stdscr, board, current_row, current_col) key = stdscr.getch() if key == curses.KEY_UP and current_row > 0: current_row -= 1 elif key == curses.KEY_DOWN and current_row < 2: current_row += 1 elif key == curses.KEY_LEFT and current_col > 0: current_col -= 1 elif key == curses.KEY_RIGHT and current_col < 2: current_col += 1 elif key == ord(\' \'): if board[current_row][current_col] == \' \': board[current_row][current_col] = current_player winner = check_winner(board) if winner or all(all(cell != \' \' for cell in row) for row in board): game_over = True current_player = \'O\' if current_player == \'X\' else \'X\' stdscr.clear() stdscr.addstr(0, 0, \\"Game Over! Result: \\" + (\\"Draw\\" if winner is None else f\\"Player {winner} wins!\\")) stdscr.refresh() stdscr.getch() if __name__ == \\"__main__\\": curses.wrapper(main) ``` Please complete the implementation of the functions `init_board`, `draw_board`, and `check_winner` to fulfill the requirements mentioned above.","solution":"import curses def init_board(stdscr): Initializes the game board and sets up the screen. stdscr.clear() stdscr.refresh() def draw_board(stdscr, board, current_row, current_col): Draws the current state of the Tic-Tac-Toe board on the screen. stdscr.clear() for row in range(3): for col in range(3): char = board[row][col] if row == current_row and col == current_col: stdscr.addstr(row * 2, col * 4, f\\"[{char}]\\") else: stdscr.addstr(row * 2, col * 4, f\\" {char} \\") stdscr.refresh() def check_winner(board): Checks if there is a winner on the board. Returns \'X\' if Player 1 wins, \'O\' if Player 2 wins, and None if there is no winner. for row in board: if row[0] == row[1] == row[2] != \' \': return row[0] for col in range(3): if board[0][col] == board[1][col] == board[2][col] != \' \': return board[0][col] if board[0][0] == board[1][1] == board[2][2] != \' \': return board[0][0] if board[0][2] == board[1][1] == board[2][0] != \' \': return board[0][2] return None def main(stdscr): The main game loop function. Handles the user input and game logic. curses.curs_set(0) stdscr.nodelay(0) init_board(stdscr) board = [[\' \' for _ in range(3)] for _ in range(3)] current_row, current_col = 0, 0 current_player = \'X\' game_over = False while not game_over: draw_board(stdscr, board, current_row, current_col) key = stdscr.getch() if key == curses.KEY_UP and current_row > 0: current_row -= 1 elif key == curses.KEY_DOWN and current_row < 2: current_row += 1 elif key == curses.KEY_LEFT and current_col > 0: current_col -= 1 elif key == curses.KEY_RIGHT and current_col < 2: current_col += 1 elif key == ord(\' \'): if board[current_row][current_col] == \' \': board[current_row][current_col] = current_player winner = check_winner(board) if winner or all(all(cell != \' \' for cell in row) for row in board): game_over = True current_player = \'O\' if current_player == \'X\' else \'X\' stdscr.clear() stdscr.addstr(0, 0, \\"Game Over! Result: \\" + (\\"Draw\\" if not winner else f\\"Player {winner} wins!\\")) stdscr.refresh() stdscr.getch() if __name__ == \\"__main__\\": curses.wrapper(main)"},{"question":"**Objective:** You are required to demonstrate your understanding of the pseudo-terminal utilities provided by Python\'s `pty` module by writing a program that spawns a shell process, executes a series of commands, captures their output, and writes it to a log file. # Problem Statement: Write a Python function `execute_commands(commands, log_file)` that takes a list of shell commands and a log file path as input. The function should: 1. Spawn a shell process using the `pty.spawn()` function. 2. Execute each command in the shell and capture its output. 3. Write the output of each command to the specified log file. **Function Signature:** ```python def execute_commands(commands: list, log_file: str) -> None: pass ``` **Input Format:** - `commands`: A list of strings where each string is a shell command to be executed. - `log_file`: A string representing the path to the log file where the output of the commands will be written. **Output Format:** - The function does not return anything. **Constraints:** - You can assume that the commands are valid shell commands. - The log file should be created if it does not exist, and it should be overwritten if it does. **Example:** ```python commands = [\\"echo \'Hello, World!\'\\", \\"ls\\", \\"pwd\\"] log_file = \\"/path/to/log_file.txt\\" execute_commands(commands, log_file) ``` The `log_file` should contain: ``` Hello, World! <output of ls command> <output of pwd command> ``` # Requirements: - You must use the `pty.spawn()` function to spawn the shell process. - You should handle the reading of the output in a way that it captures the output of each command and writes it to the log file. # Notes: - You may use other necessary standard library functions to implement the solution. - Ensure that your code is well-documented and handles exceptions where appropriate.","solution":"import os import pty import subprocess def execute_commands(commands, log_file): Executes a list of shell commands and logs their output to a specified file. Parameters: commands (list): A list of shell command strings. log_file (str): The path to the log file where command outputs are written. with open(log_file, \'w\') as f: for command in commands: def writer(fd): output = subprocess.getoutput(command) f.write(output + \\"n\\") return pty.spawn(\\"/bin/sh\\", writer)"},{"question":"# Gaussian Mixture Model Implementation and Analysis **Objective:** Implement and analyze Gaussian Mixture Models using Scikit-learn’s `sklearn.mixture` package to demonstrate your understanding of the concepts and functionalities discussed in the document. **Tasks:** 1. **Data Generation:** Generate a synthetic dataset with 500 samples in 2D space, composed of three distinct Gaussian clusters with different means and covariances. 2. **Model Implementation:** a. Implement a `GaussianMixture` model to learn from this synthetic data. b. Use appropriate methods to find the optimal number of clusters using the BIC criterion. 3. **Model Evaluation:** a. Evaluate the performance of the model by plotting the data points and the confidence ellipsoids of the learned components. b. Compare the results with different initialization methods (`k-means`, `k-means++`, `random_from_data`, `random`). 4. **Advanced Model Implementation:** Implement a `BayesianGaussianMixture` model and explain the differences from the `GaussianMixture` model in context to the Dirichlet Process and its influence on the number of components. **Specifications:** - **Input:** - Random seed for reproducibility. - Number of initializations for the `GaussianMixture` model. - Upper bound on the number of components for the `BayesianGaussianMixture` model. - **Output:** - Optimal number of clusters as determined by BIC. - Plots of the synthetic data with fitted Gaussian components for both `GaussianMixture` and `BayesianGaussianMixture` models. - A brief analysis of the results and comparison between different initialization methods and the two models. **Constraints:** - You are required to use Scikit-learn’s `GaussianMixture` and `BayesianGaussianMixture` classes. - The synthetic data should be generated using numpy. **Performance:** - The implementation should handle the computation efficiently for the given dataset. - Ensure model initialization and optimization is done within a reasonable time frame. # Example Workflow: 1. **Data Generation:** ```python import numpy as np from sklearn.datasets import make_multilabel_classification np.random.seed(42) # Generate data with three different clusters means = [[2, 2], [-2, -2], [2, -2]] covariances = [[[1, 0.5], [0.5, 1]], [[1, -0.5], [-0.5, 1]], [[1, 0.5], [0.5, 1]]] X = np.vstack([np.random.multivariate_normal(mean, cov, 500) for mean, cov in zip(means, covariances)]) ``` 2. **Model Implementation and Evaluation:** ```python from sklearn.mixture import GaussianMixture gmm = GaussianMixture(n_components=3, init_params=\'kmeans\', random_state=42) gmm.fit(X) bic = [] for n in range(1, 8): gmm = GaussianMixture(n_components=n, random_state=42) gmm.fit(X) bic.append(gmm.bic(X)) optimal_components = np.argmin(bic) + 1 print(\\"Optimal number of components by BIC:\\", optimal_components) # Visualization code import matplotlib.pyplot as plt from matplotlib.patches import Ellipse def plot_gmm(gmm, X, label=True, ax=None): ax = ax or plt.gca() labels = gmm.predict(X) if label: ax.scatter(X[:, 0], X[:, 1], c=labels, s=40, cmap=\'viridis\', zorder=2) else: ax.scatter(X[:, 0], X[:, 1], s=40, zorder=2) w_factor = 0.2 / gmm.weights_.max() for pos, covar, w in zip(gmm.means_, gmm.covariances_, gmm.weights_): draw_ellipse(pos, covar, alpha=w * w_factor) def draw_ellipse(position, covariance, ax=None, **kwargs): ax = ax or plt.gca() if covariance.shape == (2, 2): U, s, Vt = np.linalg.svd(covariance) angle = np.degrees(np.arctan2(U[1, 0], U[0, 0])) width, height = 2 * np.sqrt(s) else: angle = 0 width, height = 2 * np.sqrt(covariance) for nsig in range(1, 4): ax.add_patch(Ellipse(position, nsig * width, nsig * height, angle, **kwargs)) plot_gmm(gmm, X) plt.show() ``` Focus on writing clean, efficient code, and provide clear explanations and visualizations to support your implementation and analysis.","solution":"import numpy as np from sklearn.mixture import GaussianMixture, BayesianGaussianMixture import matplotlib.pyplot as plt from matplotlib.patches import Ellipse # Data Generation def generate_data(seed=42): np.random.seed(seed) means = [[2, 2], [-2, -2], [2, -2]] covariances = [[[1, 0.5], [0.5, 1]], [[1, -0.5], [-0.5, 1]], [[1, 0.5], [0.5, 1]]] X = np.vstack([np.random.multivariate_normal(mean, cov, 150) for mean, cov in zip(means, covariances)]) return X # Model Implementation def fit_gaussian_mixture(X, max_components=10, random_state=42): bic = [] for n in range(1, max_components + 1): gmm = GaussianMixture(n_components=n, random_state=random_state) gmm.fit(X) bic.append(gmm.bic(X)) optimal_components = np.argmin(bic) + 1 best_gmm = GaussianMixture(n_components=optimal_components, random_state=random_state) best_gmm.fit(X) return best_gmm, optimal_components def fit_bayesian_gaussian_mixture(X, max_components=10, random_state=42): bgmm = BayesianGaussianMixture(n_components=max_components, random_state=random_state) bgmm.fit(X) return bgmm # Plotting Utilities def draw_ellipse(position, covariance, ax=None, **kwargs): ax = ax or plt.gca() if covariance.shape == (2, 2): U, s, Vt = np.linalg.svd(covariance) angle = np.degrees(np.arctan2(U[1, 0], U[0, 0])) width, height = 2 * np.sqrt(s) else: angle = 0 width, height = 2 * np.sqrt(covariance) for nsig in range(1, 4): ax.add_patch(Ellipse(position, nsig * width, nsig * height, angle, **kwargs)) def plot_gmm(gmm, X, label=True, ax=None): ax = ax or plt.gca() labels = gmm.predict(X) if label: ax.scatter(X[:, 0], X[:, 1], c=labels, s=40, cmap=\'viridis\', zorder=2) else: ax.scatter(X[:, 0], X[:, 1], s=40, zorder=2) w_factor = 0.2 / gmm.weights_.max() for pos, covar, w in zip(gmm.means_, gmm.covariances_, gmm.weights_): draw_ellipse(pos, covar, alpha=w * w_factor) # Main execution def main(): X = generate_data() # Gaussian Mixture Model and finding the optimal number of clusters gmm, optimal_components = fit_gaussian_mixture(X) print(\\"Optimal number of components by BIC:\\", optimal_components) plt.figure(figsize=(10, 5)) plt.title(\\"Gaussian Mixture Model\\") plot_gmm(gmm, X) plt.show() # Bayesian Gaussian Mixture Model bgmm = fit_bayesian_gaussian_mixture(X) plt.figure(figsize=(10, 5)) plt.title(\\"Bayesian Gaussian Mixture Model\\") plot_gmm(bgmm, X) plt.show() if __name__ == \\"__main__\\": main()"},{"question":"<|Analysis Begin|> The documentation provided is centered on the versioning macros in CPython, detailing how version numbers are structured and represented in hexadecimal format. These macros extract major, minor, micro, release level, and release serial versions from the Python version. To craft a challenging and meaningful coding question based on this document, we can focus on the understanding and manipulation of version numbers, especially converting between the different representations (e.g., from individual version components to the hexadecimal representation). Given the nature of Python’s versioning system, a suitable question could involve implementing a function that parses a version string and calculates its hexadecimal representation. This would require students to understand bit manipulation and how to correctly encode the given version components. <|Analysis End|> <|Question Begin|> # Version Number Encoder **Objective:** Implement a Python function to encode a given version string into its hexadecimal representation, following the scheme detailed in the CPython documentation. **Function:** ```python def version_to_hex(version: str) -> int: Convert a Python version string (e.g., \'3.10.0a2\') into its corresponding hexadecimal representation. Args: - version (str): A version string formatted as \'X.Y.ZL\' where: * X is the major version (PY_MAJOR_VERSION) * Y is the minor version (PY_MINOR_VERSION) * Z is the micro version (PY_MICRO_VERSION) * L is an optional release level (a for alpha, b for beta, c for release candidate, f for final) followed by a serial (e.g., \'a2\', \'b3\') Returns: - int: An integer of the hexadecimal representation of the version. Raises: - ValueError: If the input version string is not correctly formatted. pass ``` **Input Format:** - A single string parameter formatted according to the versioning scheme described, e.g., \'3.4.1a2\'. **Output Format:** - An integer representing the hexadecimal version number. **Constraints:** 1. The major, minor, and micro version components (X, Y, Z) are between 0 and 255. 2. The release level (L) could be \'a\' (alpha), \'b\' (beta), \'c\' (release candidate), or \'f\' (final), followed by a digit. 3. The function should raise a `ValueError` if the version string is not in the correct format. **Examples:** ```python assert version_to_hex(\\"3.4.1a2\\") == 0x030401a2 assert version_to_hex(\\"3.10.0\\") == 0x030a00f0 assert version_to_hex(\\"3.10.0b1\\") == 0x030a00b1 ``` **Instructions:** 1. Parse the input string to extract the major, minor, and micro version components. 2. Handle the optional release level and serial. 3. Encode these components into a single 32-bit integer based on the specified layout. 4. Return the resulting hexadecimal integer. **Note:** Ensure your implementation correctly handles parsing and validation of the input string format and raises appropriate errors if the format is incorrect.","solution":"def version_to_hex(version: str) -> int: Convert a Python version string (e.g., \'3.10.0a2\') into its corresponding hexadecimal representation. Args: - version (str): A version string formatted as \'X.Y.ZL\' where: * X is the major version (PY_MAJOR_VERSION) * Y is the minor version (PY_MINOR_VERSION) * Z is the micro version (PY_MICRO_VERSION) * L is an optional release level (a for alpha, b for beta, c for release candidate, f for final) followed by a serial (e.g., \'a2\', \'b3\') Returns: - int: An integer of the hexadecimal representation of the version. Raises: - ValueError: If the input version string is not correctly formatted. import re match = re.fullmatch(r\'(d+).(d+).(d+)([a-cf])?(d+)?\', version) if not match: raise ValueError(f\\"Invalid version string format: {version}\\") major = int(match[1]) minor = int(match[2]) micro = int(match[3]) release_level = match[4] if match[4] else \'f\' serial = int(match[5]) if match[5] else 0 if not (0 <= major <= 255 and 0 <= minor <= 255 and 0 <= micro <= 255): raise ValueError(f\\"Invalid version number: {version}\\") release_level_code = {\'a\': 0xA, \'b\': 0xB, \'c\': 0xC, \'f\': 0xF}[release_level] release_byte = (release_level_code << 4) | (serial & 0xF) hex_version = (major << 24) | (minor << 16) | (micro << 8) | release_byte return hex_version"},{"question":"Objective: You are provided a time series data of daily temperatures recorded for a city over the past year. Your task is to implement a function that performs the following steps: 1. Calculate the 7-day rolling mean temperature. 2. Calculate the exponentially weighted moving average (EWMA) temperature with a span of 14 days. 3. Identify and replace outliers in the temperature data (defined as values more than 3 standard deviations from the rolling mean) with the corresponding 7-day rolling mean value. 4. Return the cleaned temperature series along with the 7-day rolling mean and EWMA as a new DataFrame. Function Signature: ```python import pandas as pd def process_temperature_data(temperatures: pd.Series) -> pd.DataFrame: Processes the temperature data to calculate rolling mean, EWMA and clean outliers. Parameters: temperatures (pd.Series): A pandas Series containing daily temperature data. Returns: pd.DataFrame: A DataFrame with columns: - \'cleaned_temps\': temperature series with outliers replaced. - \'rolling_mean\': 7-day rolling mean of temperatures. - \'ewma\': Exponentially weighted moving average with span 14 days. pass ``` Input: - `temperatures`: A `pandas.Series` containing daily temperature data for one year (365 values). Output: - A `pandas.DataFrame` containing: - `cleaned_temps`: the cleaned temperature series with outliers replaced by rolling mean values. - `rolling_mean`: 7-day rolling mean temperatures. - `ewma`: 14-day EWMA temperatures. Constraints: 1. Outliers are defined as temperatures more than 3 standard deviations away from the rolling mean. 2. Use `window=7` and `span=14` for rolling mean and EWMA calculations respectively. 3. Assume that the input series is indexed by date. Example: ```python import pandas as pd import numpy as np np.random.seed(0) date_range = pd.date_range(start=\'1/1/2022\', periods=365) temperatures = pd.Series(np.random.normal(loc=20, scale=5, size=365), index=date_range) result = process_temperature_data(temperatures) print(result.head()) ``` Expected Output (an example layout, values will vary): ``` cleaned_temps rolling_mean ewma 2022-01-01 28.820262 NaN NaN 2022-01-02 22.000786 NaN NaN 2022-01-03 24.893690 NaN NaN 2022-01-04 31.204466 NaN NaN 2022-01-05 29.337790 NaN NaN ``` Ensure your function handles edge cases, such as the initial days where rolling mean and EWMA cannot be calculated due to insufficient data points.","solution":"import pandas as pd def process_temperature_data(temperatures: pd.Series) -> pd.DataFrame: Processes the temperature data to calculate rolling mean, EWMA and clean outliers. Parameters: temperatures (pd.Series): A pandas Series containing daily temperature data. Returns: pd.DataFrame: A DataFrame with columns: - \'cleaned_temps\': temperature series with outliers replaced. - \'rolling_mean\': 7-day rolling mean of temperatures. - \'ewma\': Exponentially weighted moving average with span 14 days. # Calculate the 7-day rolling mean temperature rolling_mean = temperatures.rolling(window=7).mean() # Calculate the exponentially weighted moving average (EWMA) temperature with a span of 14 days ewma = temperatures.ewm(span=14, adjust=False).mean() # Identify outliers (more than 3 standard deviations from the rolling mean) and replace them rolling_std = temperatures.rolling(window=7).std() threshold = 3 * rolling_std outliers = (temperatures - rolling_mean).abs() > threshold cleaned_temps = temperatures.copy() cleaned_temps[outliers] = rolling_mean[outliers] # Create the resulting DataFrame result = pd.DataFrame({ \'cleaned_temps\': cleaned_temps, \'rolling_mean\': rolling_mean, \'ewma\': ewma }) return result"},{"question":"<|Analysis Begin|> The provided documentation covers `xml.etree.ElementTree`, a module in Python\'s standard library used for parsing and creating XML data. Some key points from the documentation that are useful for constructing an assessment question include: 1. **Core Classes and Functions**: Understanding of `Element`, `ElementTree`, and methods such as `parse`, `fromstring`, `findall`, `find`, `iter`, `write`, and `tostring`. 2. **Navigating and Modifying XML Trees**: Demonstrates how to access, iterate, and manipulate XML elements and attributes. 3. **Advanced Parsing Techniques**: Including incremental parsing with `iterparse` and non-blocking parsing with `XMLPullParser`. 4. **Namespaces and XPath**: Handling XML documents with namespaces and utilizing XPath expressions for finding elements. 5. **Building and Writing XML**: Creating and modifying XML trees programmatically and writing them to files with specific structures and attributes. Given this understanding, a suitable assessment question should test the student\'s ability to: - Parse an XML document from a string or file. - Navigate and query the XML tree. - Modify elements and attributes. - Write the modified XML back to a file or string. <|Analysis End|> <|Question Begin|> # XML Manipulation and Querying with ElementTree **Problem Statement:** You are working with an XML document that stores information about books in a bookstore. Each book entry contains data such as title, author, year of publication, price, and availability status. You need to create a script that performs the following tasks: 1. Parse the provided XML string into an `Element` object. 2. Print all book titles and their corresponding authors. 3. Increase the price of all books published before the year 2000 by 20%. 4. Add a new book entry to the XML. 5. Remove all books that are marked as unavailable (`availability` set to `false`). 6. Save the modified XML to a new string and return it. The initial XML string is as follows: ```xml <bookstore> <book> <title>Python 101</title> <author>John Doe</author> <year>2015</year> <price>29.99</price> <availability>true</availability> </book> <book> <title>XML Fundamentals</title> <author>Jane Roe</author> <year>1999</year> <price>39.99</price> <availability>true</availability> </book> <book> <title>Data Science Handbook</title> <author>Sam Smith</author> <year>2010</year> <price>49.99</price> <availability>false</availability> </book> </bookstore> ``` # Requirements 1. Implement a function `modify_bookstore_xml(xml_string: str) -> str` that performs the required tasks. 2. Follow the steps outlined to manipulate the XML and produce the desired output. 3. Ensure that the new book entry has the following details: - Title: \\"Advanced Python\\" - Author: \\"Alice Wonderland\\" - Year: 2021 - Price: 59.99 - Availability: true # Function Signature ```python def modify_bookstore_xml(xml_string: str) -> str: pass ``` Constraints - You must use the `xml.etree.ElementTree` module. - The function should handle XML parsing and writing efficiently. - Edge cases (e.g., malformed XML) do not need to be handled for this task. # Example Usage ```python xml_input = \'\'\'<bookstore> <book> <title>Python 101</title> <author>John Doe</author> <year>2015</year> <price>29.99</price> <availability>true</availability> </book> <book> <title>XML Fundamentals</title> <author>Jane Roe</author> <year>1999</year> <price>39.99</price> <availability>true</availability> </book> <book> <title>Data Science Handbook</title> <author>Sam Smith</author> <year>2010</year> <price>49.99</price> <availability>false</availability> </book> </bookstore>\'\'\' modified_xml = modify_bookstore_xml(xml_input) print(modified_xml) ``` **Expected Output:** The XML string should be updated to reflect the modifications, including price adjustments, addition of a new book, and removal of unavailable books.","solution":"import xml.etree.ElementTree as ET def modify_bookstore_xml(xml_string): # Parse the given XML string into an Element object root = ET.fromstring(xml_string) # Print all book titles and their corresponding authors for book in root.findall(\'book\'): title = book.find(\'title\').text author = book.find(\'author\').text print(f\\"Title: {title}, Author: {author}\\") # Increase the price of all books published before the year 2000 by 20% for book in root.findall(\'book\'): year = int(book.find(\'year\').text) if year < 2000: price = float(book.find(\'price\').text) new_price = price * 1.20 book.find(\'price\').text = f\\"{new_price:.2f}\\" # Add a new book entry to the XML new_book = ET.Element(\'book\') title = ET.SubElement(new_book, \'title\') title.text = \\"Advanced Python\\" author = ET.SubElement(new_book, \'author\') author.text = \\"Alice Wonderland\\" year = ET.SubElement(new_book, \'year\') year.text = \\"2021\\" price = ET.SubElement(new_book, \'price\') price.text = \\"59.99\\" availability = ET.SubElement(new_book, \'availability\') availability.text = \\"true\\" root.append(new_book) # Remove all books that are marked as unavailable (availability set to false) for book in root.findall(\'book\'): availability = book.find(\'availability\').text if availability == \'false\': root.remove(book) # Save the modified XML to a new string and return it return ET.tostring(root, encoding=\'unicode\')"},{"question":"# Pandas Coding Assessment Question **Objective:** Demonstrate your comprehension of pandas using `Series` and `DataFrame` objects by implementing a function that processes and analyzes a given dataset. **Task:** You are given two datasets in the form of dictionaries: - `sales_data`: This dictionary contains information about sales made over a period. - `product_info`: This dictionary contains attributes of the products. Using the given dictionaries, you are required to perform the following steps: 1. Create a `DataFrame` from `sales_data`. 2. Create a `DataFrame` from `product_info`. 3. Compute the total revenue generated (`quantity` * `price`) from sales. 4. Find the top 5 products that generated the highest total revenue. 5. Merge the two DataFrames based on the product information `product_id`. 6. Create a new column `revenue_per_unit` to find the revenue generated per unit sold for each product. 7. Filter and display the combined DataFrame with only the top 5 products based on total revenue and show the following columns: `product_id`, `product_name`, `total_revenue`, `revenue_per_unit`. # Input Format - `sales_data`: A dictionary with keys and respective values as follows: - `\'product_id\'`: List of product IDs. - `\'quantity\'`: List of quantities sold. - `\'price\'`: List of prices for respective sales. - `product_info`: A dictionary with keys and respective values as follows: - `\'product_id\'`: List of product IDs. - `\'product_name\'`: List of product names. - `\'category\'`: List of product categories. # Output Format - A `DataFrame` with columns `product_id`, `product_name`, `total_revenue`, `revenue_per_unit` for the top 5 products sorted by `total_revenue` in descending order. # Constraints - Assume that both dictionaries are well-formed and contain valid data. - The lengths of lists in `sales_data` are equal. - The lengths of lists in `product_info` are equal. # Function Signature ```python def analyze_sales(sales_data: dict, product_info: dict) -> pd.DataFrame: # Your implementation here pass ``` # Example ```python sales_data = { \'product_id\': [1, 2, 3, 1, 2, 3, 4, 5], \'quantity\': [10, 5, 8, 15, 3, 7, 12, 10], \'price\': [100, 200, 150, 100, 200, 150, 300, 400] } product_info = { \'product_id\': [1, 2, 3, 4, 5], \'product_name\': [\'Product A\', \'Product B\', \'Product C\', \'Product D\', \'Product E\'], \'category\': [\'Category 1\', \'Category 2\', \'Category 1\', \'Category 2\', \'Category 1\'] } result = analyze_sales(sales_data, product_info) print(result) ``` Expected output: ``` product_id product_name total_revenue revenue_per_unit 0 5 Product E 4000 400.0 1 4 Product D 3600 300.0 2 1 Product A 2500 100.0 3 3 Product C 2250 150.0 4 2 Product B 1600 200.0 ```","solution":"import pandas as pd def analyze_sales(sales_data: dict, product_info: dict) -> pd.DataFrame: # Create DataFrames sales_df = pd.DataFrame(sales_data) product_df = pd.DataFrame(product_info) # Compute total revenue sales_df[\'total_revenue\'] = sales_df[\'quantity\'] * sales_df[\'price\'] # Aggregate total revenue by product_id revenue_df = sales_df.groupby(\'product_id\', as_index=False)[\'total_revenue\'].sum() # Find top 5 products by total revenue top_5_revenue_df = revenue_df.nlargest(5, \'total_revenue\') # Merge DataFrames based on product_id combined_df = pd.merge(top_5_revenue_df, product_df, on=\'product_id\') # Calculate revenue per unit combined_df[\'revenue_per_unit\'] = combined_df[\'total_revenue\'] / combined_df[\'product_id\'].map(sales_df.groupby(\'product_id\')[\'quantity\'].sum()) # Select required columns result_df = combined_df[[\'product_id\', \'product_name\', \'total_revenue\', \'revenue_per_unit\']] return result_df.sort_values(by=\'total_revenue\', ascending=False).reset_index(drop=True)"},{"question":"Objective You are required to implement a function using Python\'s `imaplib` library. The function will connect to an IMAP4 server, authenticate a user, create a new mailbox, search for messages in the new mailbox, and fetch the content of these messages. Function Specification ```python def manage_mailbox(server: str, user: str, password: str, new_mailbox: str, timeout: Optional[int] = None) -> List[str]: Connect to an IMAP4 server, authenticate a user, create a new mailbox, search for messages in the new mailbox, and fetch the content of these messages. Parameters: - server (str): The hostname or IP address of the IMAP4 server. - user (str): The username for authentication. - password (str): The password for authentication. - new_mailbox (str): The name of the new mailbox to create. - timeout (Optional[int]): Timeout in seconds for the connection attempt. Returns: - List[str]: A list of message contents retrieved from the new mailbox. pass ``` Input 1. `server`: A string representing the hostname or IP address of the IMAP4 server. 2. `user`: A string representing the username for authentication. 3. `password`: A string representing the password for authentication. 4. `new_mailbox`: A string representing the name of the new mailbox to create. 5. `timeout`: An optional integer representing the timeout in seconds for the connection attempt. Default is `None`, which uses the global default socket timeout. Output - The function should return a list containing the content of all messages in the new mailbox as strings. Constraints - Authenticate using plaintext `LOGIN`. - If the mailbox already exists, no need to handle this as an error; proceed with the search and fetch operations. - Assume basic error handling requirements (you can raise exceptions if an operation fails). - Use the `\\"ALL\\"` criterion for searching messages in the new mailbox. - Ensure that the function uses SSL for a secure connection. Example Assuming there are messages in the specified mailbox, the function could be called as follows: ```python messages = manage_mailbox(\\"imap.example.com\\", \\"username\\", \\"password\\", \\"new_mail_folder\\") for msg in messages: print(msg) ``` Note: - The function should handle the creation and deletion of mailboxes appropriately. - Use the appropriate methods from `imaplib` to perform these operations. - Use the SSL-based class `IMAP4_SSL` for making secure connections.","solution":"import imaplib from typing import List, Optional def manage_mailbox(server: str, user: str, password: str, new_mailbox: str, timeout: Optional[int] = None) -> List[str]: Connect to an IMAP4 server, authenticate a user, create a new mailbox, search for messages in the new mailbox, and fetch the content of these messages. # Establish a secure connection to the server mail_server = imaplib.IMAP4_SSL(server, timeout=timeout) try: # Login to the server mail_server.login(user, password) # Create the new mailbox mail_server.create(new_mailbox) # Select the newly created mailbox mail_server.select(new_mailbox) # Search for all messages in the new mailbox status, messages = mail_server.search(None, \\"ALL\\") if status != \'OK\': raise Exception(\\"Failed to search for messages\\") msgs_list = messages[0].split() msgs_content = [] # Fetch each message by id and store their contents for msg_num in msgs_list: status, msg_data = mail_server.fetch(msg_num, \'(RFC822)\') if status != \'OK\': raise Exception(f\\"Failed to fetch message {msg_num}\\") for response_part in msg_data: if isinstance(response_part, tuple): msgs_content.append(response_part[1].decode(\'utf-8\')) return msgs_content finally: # Be sure to logout and close the connection mail_server.logout()"},{"question":"# Python310 Coding Assessment Question You have been provided with a segment of raw audio data in 16-bit linear PCM (Pulse Code Modulation) format. Your task is to write a function that processes this audio fragment in the following sequence: 1. **Add Bias**: Add a bias of 100 to each sample in the provided audio fragment. 2. **Resample**: Convert the sample rate of the biased audio fragment from 44.1 kHz (CD quality) to 22.05 kHz. 3. **Convert to Mono**: Convert the resampled audio fragment from stereo to mono by averaging the two channels. 4. **Calculate RMS**: Compute the Root Mean Square (RMS) power of the resulting mono audio fragment. # Function Signature ```python def process_audio(audio_fragment: bytes, state: tuple) -> (bytes, float): ``` # Inputs - `audio_fragment` (bytes): The input audio fragment in 16-bit linear PCM stereo format sampled at 44.1 kHz. - `state` (tuple): A state tuple to be used in the `audioop.ratecv()` function. The initial call should pass `None` as the state. # Outputs - A tuple with two elements: - `processed_audio` (bytes): The processed mono audio fragment in 16-bit linear PCM format sampled at 22.05 kHz. - `rms_value` (float): The RMS power of the processed mono audio fragment. # Constraints 1. The input audio fragment will have a length that is a multiple of 4 bytes (each sample is 2 bytes per channel in stereo). 2. You must handle potential overflow by truncating samples where necessary. # Example ```python # Example raw audio fragment (stereo) in 16-bit linear PCM format. audio_fragment = b\'x01x00x02x00x03x00x04x00\' # State for the rate conversion initial_state = None # Call the function processed_audio, rms_value = process_audio(audio_fragment, initial_state) print(processed_audio) # Output will be the processed mono audio fragment in 16-bit PCM format at 22.05 kHz. print(rms_value) # Output will be the RMS value of the processed audio. ``` # Note Use the `audioop` module functions for manipulating the audio data. Ensure your solution handles stereo to mono conversion and sample rate conversion correctly.","solution":"import audioop def process_audio(audio_fragment, state): Processes the audio fragment by adding a bias, resampling, converting to mono, and calculating the RMS power. Parameters: audio_fragment (bytes): The input audio fragment in 16-bit linear PCM stereo format sampled at 44.1 kHz. state (tuple): A state tuple to be used in the audioop.ratecv() function. Returns: (bytes, float): A tuple containing the processed mono audio fragment in 16-bit PCM format sampled at 22.05 kHz and the RMS power of the mono audio fragment. # Adding Bias biased_audio = audioop.bias(audio_fragment, 2, 100) # Resampling to 22.05 kHz rate_ratio = int(44100 / 22050) resampled_audio, new_state = audioop.ratecv(biased_audio, 2, 2, 44100, 22050, state) # Convert to Mono mono_audio = audioop.tomono(resampled_audio, 2, 0.5, 0.5) # Calculate RMS rms_value = audioop.rms(mono_audio, 2) return mono_audio, rms_value"},{"question":"<|Analysis Begin|> The provided documentation offers comprehensive information about Python\'s `argparse` module. The `argparse` module makes it easy to write user-friendly command-line interfaces, which is crucial for many applications. It covers the creation of parsers, adding arguments, argument parsing, handling sub-commands, dealing with files, and creating custom parser behaviors. Important aspects include: 1. **Creating a Parser**: Instantiate an `ArgumentParser` object. 2. **Adding Arguments**: Use methods like `add_argument` to define what the program expects. 3. **Parsing Arguments**: Interpret command-line arguments using methods like `parse_args`. 4. **Argument Groups**: Group arguments logically for better help messages. 5. **Mutual Exclusion**: Ensure that only one of a set of arguments is present. 6. **Sub-commands**: Handling different commands within the same program. 7. **Custom Actions and Types**: Extend `argparse` functionality with custom behavior. Given its extensive yet clear details, I can design a problem that incorporates multiple concepts within `argparse` to test the students\' understanding effectively. <|Analysis End|> <|Question Begin|> # Command-Line Utility for File Operations Your task is to implement a command-line utility in Python that performs various file operations using the `argparse` module. The utility should support the following sub-commands with their respective arguments: 1. `display` - Displays the content of a given file. 2. `copy` - Copies content from one file to another. 3. `search` - Searches a word in a given file and prints all the lines containing that word. Here\'s a detailed description of what each sub-command should do: Sub-command: `display` **Description:** Displays the content of the specified file. **Arguments:** - `filepath`: The path to the file whose content is to be displayed. Sub-command: `copy` **Description:** Copies content from a source file to a destination file. **Arguments:** - `src`: The source file path. - `dest`: The destination file path. Sub-command: `search` **Description:** Searches for a word in the specified file and prints all lines containing that word. **Arguments:** - `filepath`: The path to the file to be searched. - `word`: The word to search for. # Implementation Requirements - Use `argparse.ArgumentParser` to handle the arguments and sub-commands. - Ensure appropriate error handling and user-friendly help messages. - Use file handling techniques to read from and write to files. - Implement the logic for each sub-command. - Print meaningful messages for success or error situations. # Example Usage 1. **Display File Content** ```sh python file_utility.py display filepath.txt ``` 2. **Copy File Content** ```sh python file_utility.py copy srcfile.txt destfile.txt ``` 3. **Search Word in File** ```sh python file_utility.py search filepath.txt searchword ``` # Constraints - Ensure to handle errors such as file not found, access permissions, etc. - The script should show a help message when incorrect arguments are provided. # Evaluation Criteria - Correct implementation of argument parsing and sub-command handling. - Appropriate use of `argparse` features. - Proper file handling and error management. - Clarity and user-friendliness of help messages. - Adherence to the problem requirements.","solution":"import argparse import os def display_file_content(filepath): try: with open(filepath, \'r\') as file: content = file.read() print(content) except FileNotFoundError: print(f\\"Error: File \'{filepath}\' not found.\\") except Exception as e: print(f\\"Error: {e}\\") def copy_file_content(src, dest): try: with open(src, \'r\') as sfile: content = sfile.read() with open(dest, \'w\') as dfile: dfile.write(content) print(f\\"Successfully copied content from \'{src}\' to \'{dest}\'.\\") except FileNotFoundError: print(f\\"Error: Source file \'{src}\' not found.\\") except Exception as e: print(f\\"Error: {e}\\") def search_word_in_file(filepath, word): try: with open(filepath, \'r\') as file: lines = file.readlines() matches = [line for line in lines if word in line] if matches: print(\\"Lines containing the word \'{word}\':\\") for match in matches: print(match.strip()) else: print(f\\"No lines found containing the word \'{word}\'.\\") except FileNotFoundError: print(f\\"Error: File \'{filepath}\' not found.\\") except Exception as e: print(f\\"Error: {e}\\") def main(): parser = argparse.ArgumentParser(description=\\"File utility for various file operations.\\") subparsers = parser.add_subparsers(dest=\'command\', help=\'Sub-command help\') # Display command display_parser = subparsers.add_parser(\'display\', help=\'Displays the content of a given file\') display_parser.add_argument(\'filepath\', type=str, help=\'The path to the file to display\') # Copy command copy_parser = subparsers.add_parser(\'copy\', help=\'Copies content from a source file to a destination file\') copy_parser.add_argument(\'src\', type=str, help=\'The source file path\') copy_parser.add_argument(\'dest\', type=str, help=\'The destination file path\') # Search command search_parser = subparsers.add_parser(\'search\', help=\'Searches a word in a given file and prints all lines containing that word\') search_parser.add_argument(\'filepath\', type=str, help=\'The path to the file to be searched\') search_parser.add_argument(\'word\', type=str, help=\'The word to search for\') args = parser.parse_args() if args.command == \'display\': display_file_content(args.filepath) elif args.command == \'copy\': copy_file_content(args.src, args.dest) elif args.command == \'search\': search_word_in_file(args.filepath, args.word) else: parser.print_help() if __name__ == \\"__main__\\": main()"},{"question":"In Python, audit events are raised by calls to `sys.audit()` or `PySys_Audit()` throughout the CPython runtime and the standard library. These events can be used to monitor and log various actions in a program, such as file operations, network requests, and more. # Task You are required to implement a logging mechanism that captures and logs all audit events that occur during the execution of a Python script. Your implementation should include a custom audit hook that handles all the different types of audit events specified in the given audit events table. # Requirements 1. **Function Signature**: ```python def setup_audit_logger(log_file: str) -> None: pass ``` 2. **Input**: - `log_file` (str): The file path where the audit events will be logged. 3. **Output**: - The function should not return any value. 4. **Constraints**: - The log file should capture each audit event in a human-readable format. - Each log entry should include the event name, timestamp, and the arguments of the event. - The function should properly handle any type of audit event mentioned in the given audit events table. - Make use of the `sys.addaudithook()` function to register your custom audit hook. 5. **Performance**: - Ensure that the logging mechanism is efficient and does not add significant overhead to the execution of the program. # Example ```python import sys import time def setup_audit_logger(log_file: str) -> None: def audit_hook(event, args): with open(log_file, \'a\') as f: timestamp = time.strftime(\\"[%Y-%m-%d %H:%M:%S]\\", time.localtime()) log_entry = f\\"{timestamp} {event}: {args}n\\" f.write(log_entry) sys.addaudithook(audit_hook) # Example usage: setup_audit_logger(\\"audit_log.txt\\") # Trigger some audit events for testing open(\\"test_file.txt\\", \'w\').close() # This will raise \'open\' audit event os.remove(\\"test_file.txt\\") # This will raise \'os.remove\' audit event ``` # Description - Implement the `setup_audit_logger` function to capture and log all audit events. - Your function should register an audit hook using `sys.addaudithook` which logs event details to the specified log file. - Make sure your log entries are clear and informative, including the event name, timestamp, and arguments. # Notes - Refer to the provided audit events table to understand the kinds of events you will be handling. - Ensure that your logging mechanism is robust and handles potential errors (e.g., file writing errors) gracefully.","solution":"import sys import time def setup_audit_logger(log_file: str) -> None: def audit_hook(event, args): try: with open(log_file, \'a\') as f: timestamp = time.strftime(\\"[%Y-%m-%d %H:%M:%S]\\", time.localtime()) log_entry = f\\"{timestamp} {event}: {args}n\\" f.write(log_entry) except Exception as e: print(f\\"Failed to log event: {e}\\") sys.addaudithook(audit_hook)"},{"question":"Source Distribution Automation You are tasked with creating a Python module that automates the creation of a source distribution for a packaging project. The module should include functionalities to specify the distribution formats, manage included files through the `MANIFEST.in` file, and allow customization of archive ownership. # Requirements 1. **Function: `create_manifest`** - **Input**: - `file_patterns` (List[Tuple[str, str]]): A list of tuples where the first element is a command (e.g., `include`, `recursive-include`, `prune`) and the second element is the pattern (e.g., `*.txt`, `docs/*.md`). - **Output**: - None - **Behavior**: Creates or updates the `MANIFEST.in` file with the provided file patterns. 2. **Function: `create_source_distribution`** - **Input**: - `formats` (List[str]): A list of formats (e.g., `[\'zip\', \'gztar\']`) to be used for the archive. - `owner` (str): The owner of the files in the archive. - `group` (str): The group of the files in the archive. - **Output**: - None - **Behavior**: Runs the `setup.py sdist` command with the specified formats and ownership options to create the source distribution. # Constraints - Use only standard Python libraries. - Assume that `setup.py` and other necessary files are in the current working directory. # Example Usage ```python from distribution_automation import create_manifest, create_source_distribution file_patterns = [ (\\"include\\", \\"*.txt\\"), (\\"recursive-include\\", \\"examples *.txt *.py\\"), (\\"prune\\", \\"examples/sample?/build\\") ] # Creating the MANIFEST.in file create_manifest(file_patterns) # Creating source distribution in zip and gztar formats with root ownership create_source_distribution(formats=[\'zip\', \'gztar\'], owner=\'root\', group=\'root\') ``` # Hints - Use file operations to create/update the `MANIFEST.in` file. - Use the `subprocess` module to run shell commands.","solution":"import os import subprocess def create_manifest(file_patterns): Creates or updates the MANIFEST.in file with the provided file patterns. :param file_patterns: List of tuples containing command and pattern with open(\'MANIFEST.in\', \'w\') as f: for command, pattern in file_patterns: f.write(f\\"{command} {pattern}n\\") def create_source_distribution(formats, owner, group): Runs the setup.py sdist command to create the source distribution with specified formats and ownership options. :param formats: List of formats (e.g., [\'zip\', \'gztar\']) :param owner: Owner of the files in the archive :param group: Group of the files in the archive format_options = \',\'.join(formats) cmd = [ \'python\', \'setup.py\', \'sdist\', \'--formats\', format_options, \'--owner\', owner, \'--group\', group ] subprocess.run(cmd, check=True)"},{"question":"**Problem: Implement a Buffer-based Image Inversion** **Problem Statement:** You are given an image in the form of a buffer. The image is in a grayscale format where each pixel is represented by a single byte (0-255). Your task is to invert the image. Inversion means that each pixel\'s value should be subtracted from 255 (i.e., new_pixel_value = 255 - old_pixel_value). You need to implement the `invert_image` function which takes a buffer as input, modifies the buffer in place, and returns the modified buffer. **Function Signature:** ```python def invert_image(buffer: memoryview) -> memoryview: pass ``` **Input:** - `buffer`: A `memoryview` object of the image buffer. The buffer is guaranteed to be contiguous and contains bytes in the range of 0 to 255. **Output:** - The same `memoryview` object (`buffer`), but with each pixel value inverted. **Constraints:** - You are not allowed to use any external libraries except for standard Python libraries. - You cannot convert the buffer to any other type (such as a list or bytearray) - you must work directly with the buffer. - The buffer is guaranteed to be of non-zero length. - The function should operate in O(n) time complexity, where n is the length of the buffer. **Examples:** ```python # Example 1: buffer = memoryview(bytearray([0, 255, 127, 64])) inverted_buffer = invert_image(buffer) assert inverted_buffer.tobytes() == b\'xffx00x80xbf\' # Example 2: buffer = memoryview(bytearray([10, 20, 30, 40, 50])) inverted_buffer = invert_image(buffer) assert inverted_buffer.tobytes() == b\'xf5xebxe1xd7xcd\' ``` **Notes:** - Ensure that you\'re modifying the buffer in place. - Pay attention to the handling of the `memoryview` and direct manipulation of its contents.","solution":"def invert_image(buffer: memoryview) -> memoryview: for i in range(len(buffer)): buffer[i] = 255 - buffer[i] # invert the pixel value return buffer"},{"question":"# XML Manipulation using `xml.dom` Introduction In this task, you will be given an XML string representing a simple document. You are required to parse this XML string into a DOM structure using the `xml.dom` module, perform specified operations on the DOM tree, and then output the modified XML. Objective Implement the function `manipulate_xml(xml_str: str, operations: list) -> str`. This function should: 1. Parse the provided XML string into a DOM structure. 2. Perform a series of operations on the DOM tree. 3. Convert the modified DOM tree back into an XML string and return it. Input - `xml_str`: A string containing the XML document. - `operations`: A list of operations to perform on the DOM tree. Each operation is represented as a tuple where the first element is the operation type and the subsequent elements are the required parameters. The possible operations are: 1. `(\\"CREATE_ELEMENT\\", \\"parent_tag\\", \\"new_tag\\")`: Create a new element with tag `new_tag` and append it to the first found element with tag `parent_tag`. 2. `(\\"SET_ATTRIBUTE\\", \\"tag\\", \\"attr_name\\", \\"attr_value\\")`: Set the attribute `attr_name` with value `attr_value` on the first found element with tag `tag`. 3. `(\\"REMOVE_ELEMENT\\", \\"tag\\")`: Remove the first found element with tag `tag`. 4. `(\\"ADD_TEXT\\", \\"tag\\", \\"text\\")`: Add a text node with value `text` to the first found element with tag `tag`. Output - A string containing the modified XML document. Constraints - You may assume that the input XML string is valid and well-formed. - The operations list will contain valid operations that can be performed on the DOM tree. Example: ```python xml_str = \'\'\'<root> <parent> <child>Example</child> </parent> </root>\'\'\' operations = [ (\\"CREATE_ELEMENT\\", \\"parent\\", \\"new_child\\"), (\\"SET_ATTRIBUTE\\", \\"new_child\\", \\"class\\", \\"highlight\\"), (\\"ADD_TEXT\\", \\"new_child\\", \\"This is a new child.\\"), (\\"REMOVE_ELEMENT\\", \\"child\\") ] output = manipulate_xml(xml_str, operations) print(output) ``` Expected Output: ```xml <root> <parent> <new_child class=\\"highlight\\">This is a new child.</new_child> </parent> </root> ``` Implementation Tips - Use the `xml.dom.minidom` module to parse the XML string and manipulate the DOM tree. - Ensure to handle the operations sequentially and modify the DOM tree accordingly. - Utilize methods like `document.createElement`, `element.setAttribute`, `parent.appendChild`, and `node.removeChild` to perform the operations. - Finally, convert the DOM back into an XML string using methods like `toprettyxml` or `toxml`.","solution":"from xml.dom import minidom def manipulate_xml(xml_str: str, operations: list) -> str: # Parse the XML string into a DOM structure dom = minidom.parseString(xml_str) for operation in operations: op_type = operation[0] if op_type == \\"CREATE_ELEMENT\\": parent_tag, new_tag = operation[1], operation[2] parent_element = dom.getElementsByTagName(parent_tag)[0] # Get the first matching element new_element = dom.createElement(new_tag) # Create new element parent_element.appendChild(new_element) # Append new element to the parent elif op_type == \\"SET_ATTRIBUTE\\": tag, attr_name, attr_value = operation[1], operation[2], operation[3] element = dom.getElementsByTagName(tag)[0] # Get the first matching element element.setAttribute(attr_name, attr_value) # Set attribute elif op_type == \\"REMOVE_ELEMENT\\": tag = operation[1] element = dom.getElementsByTagName(tag)[0] # Get the first matching element element.parentNode.removeChild(element) # Remove element from its parent elif op_type == \\"ADD_TEXT\\": tag, text = operation[1], operation[2] element = dom.getElementsByTagName(tag)[0] # Get the first matching element text_node = dom.createTextNode(text) # Create text node element.appendChild(text_node) # Append text node to the element return dom.toxml() # Convert the modified DOM tree back into an XML string and return it"},{"question":"Objective Design a Python function that takes in parameters for generating synthetic data, applies data preprocessing, fits a Gradient Boosting Regressor, and evaluates the model\'s performance. Task Write a Python function, `pipeline_gradient_boosting`, that: 1. Generates synthetic regression data. 2. Splits the data into training and testing sets. 3. Applies standard scaling to the data. 4. Fits a `GradientBoostingRegressor` model to the training data. 5. Evaluates the model on the test data. 6. Returns the R^2 score. Detailed Instructions 1. **Synthetic Data Generation:** - Use `make_regression` from `sklearn.datasets` to generate the synthetic data. - Your function should accept parameters for `n_samples` and `n_features`. 2. **Data Splitting:** - Split the data into training and testing sets using `train_test_split` from `sklearn.model_selection`. - The test set should be 33% of the data. - Use `random_state=42` for reproducibility. 3. **Data Preprocessing:** - Scale the features using `StandardScaler` from `sklearn.preprocessing`. 4. **Model Fitting:** - Fit a `GradientBoostingRegressor` model from `sklearn.ensemble` on the scaled training data. - Your function should accept parameters for the regressor, specifically `random_state` and `n_iter_no_change`. 5. **Model Evaluation:** - Evaluate the model performance on the test set using the `score` method to compute the R^2 score. 6. **Output:** - The function should return the R^2 score as a float. Signature ```python def pipeline_gradient_boosting(n_samples: int, n_features: int, random_state: int = 0, n_iter_no_change: int = 5) -> float: pass ``` Example Usage ```python r2_score = pipeline_gradient_boosting(n_samples=1000, n_features=20, random_state=0, n_iter_no_change=5) print(\\"R^2 Score:\\", r2_score) ``` # Constraints: - You can use only the scikit-learn, numpy, and pandas libraries. - Ensure the function is self-contained and easy to copy-paste.","solution":"from sklearn.datasets import make_regression from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.ensemble import GradientBoostingRegressor def pipeline_gradient_boosting(n_samples: int, n_features: int, random_state: int = 0, n_iter_no_change: int = 5) -> float: # Generate synthetic data X, y = make_regression(n_samples=n_samples, n_features=n_features, noise=0.1, random_state=random_state) # Split the data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=random_state) # Preprocess the data scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) # Fit the Gradient Boosting Regressor gbr = GradientBoostingRegressor(random_state=random_state, n_iter_no_change=n_iter_no_change) gbr.fit(X_train_scaled, y_train) # Evaluate the model on the test data r2_score = gbr.score(X_test_scaled, y_test) # Return the R^2 score return r2_score"},{"question":"**Question: Implement and Test a Custom Tensor Operation using PyTorch** Using PyTorch, create a function to perform a custom tensor operation, and use the provided torch.testing utilities to validate the results. The custom operation `elementwise_multiplication_and_sum` should take two tensors of the same shape, perform element-wise multiplication, and then return a tensor where each element is the sum of the corresponding rows of the multiplied result. - **Input**: - Two tensors `tensor_a` and `tensor_b` of shape `(m, n)`. - **Output**: - A tensor of shape `(m, 1)` where each element is the sum of the corresponding row in the element-wise multiplied result of `tensor_a` and `tensor_b`. **Function Signature**: ```python import torch def elementwise_multiplication_and_sum(tensor_a: torch.Tensor, tensor_b: torch.Tensor) -> torch.Tensor: pass ``` Additionally, implement a test function `test_elementwise_multiplication_and_sum` that performs the following: 1. Uses `torch.testing.make_tensor` to create tensors for testing. 2. Calls `elementwise_multiplication_and_sum` with the created tensors. 3. Manually computes the expected result using basic tensor operations. 4. Compares the computed result with the expected result using `torch.testing.assert_allclose`. **Test Function Signature**: ```python def test_elementwise_multiplication_and_sum(): pass ``` **Constraints**: - Ensure that `tensor_a` and `tensor_b` are of the same shape. - The solution should handle large tensors efficiently. **Example**: ```python # Example input tensor_a = torch.tensor([[1., 2.], [3., 4.]]) tensor_b = torch.tensor([[5., 6.], [7., 8.]]) # Expected output # Element-wise multiplication: [[1*5, 2*6], [3*7, 4*8]] = [[5., 12.], [21., 32.]] # Sum of rows: [[5+12], [21+32]] = [[17.], [53.]] result = elementwise_multiplication_and_sum(tensor_a, tensor_b) print(result) # tensor([[17.], [53.]]) ``` Your task is to complete the implementation of `elementwise_multiplication_and_sum` and `test_elementwise_multiplication_and_sum` functions.","solution":"import torch def elementwise_multiplication_and_sum(tensor_a: torch.Tensor, tensor_b: torch.Tensor) -> torch.Tensor: Perform element-wise multiplication of two tensors and return a tensor where each element is the sum of the corresponding rows of the multiplied result. :param tensor_a: A tensor of shape (m, n) :param tensor_b: A tensor of shape (m, n) :return: A tensor of shape (m, 1) with summed rows of the multiplied result # Element-wise multiplication multiplied = tensor_a * tensor_b # Summing across rows result = torch.sum(multiplied, dim=1, keepdim=True) return result"},{"question":"# Distributed RRef Management in PyTorch Objective: Your task is to implement a distributed system in PyTorch using the RRef protocol, demonstrating understanding of remote object references, their lifecycle, and reference counting. Problem Statement: You need to implement a custom class `DistributedCounter` that utilizes RRefs to manage a counter distributed across multiple workers. The main functionalities required are: 1. **Initialization**: The counter should start from a given initial value. 2. **Increment**: The counter can be incremented by any worker. 3. **Fetch Value**: Any worker can fetch the current value of the counter. 4. **Reference Management**: Ensure that the counter\'s RRef management guarantees proper notification of parent RRefs and child RRefs as described in the provided RRef protocol. Class Definition ```python import torch import torch.distributed.rpc as rpc from torch.distributed.rpc import RRef class DistributedCounter: def __init__(self, initial_value: int): # Initialize the counter with the given initial value. pass def increment(self, increment_value: int): # Increment the counter by the given value. pass def fetch(self) -> int: # Return the current value of the counter. pass ``` Implementation Details: 1. **Initialization:** - The `__init__` method initializes the counter with the provided `initial_value`. - Use an RRef to reference the counter value. 2. **Increment:** - The `increment` method should increment the RRef counter value by `increment_value`. - Ensure that the modification is properly synchronized across all workers. 3. **Fetch Value:** - The `fetch` method should return the current counter value by dereferencing the RRef. 4. **Reference Management:** - Implement necessary mechanisms to manage RRef references and ensure that the counter experiences no premature deletions or lifecycle mismanagement. - Handle scenarios where RRefs are shared among users and ensure proper acknowledgment and notifications as per the RRef protocol described. Constraints: 1. Ensure the distributed setup uses at least 3 workers. 2. Use rpc framework for communication among workers, and handle initialization and cleanup gracefully. 3. Write unit tests to validate the correctness of the implementation, testing increment operations from multiple workers and fetching the updated counter value. Example Usage: ```python import torch.multiprocessing as mp def worker(rank, world_size): rpc.init_rpc(name=f\'worker{rank}\', rank=rank, world_size=world_size) if rank == 0: counter_rref = rpc.remote(\'worker1\', DistributedCounter, args=(10,)) rpc.rpc_sync(\'worker1\', counter_rref.remote().increment, args=(5,)) print(f\\"Counter value: {rpc.rpc_sync(\'worker1\', counter_rref.remote().fetch)}\\") rpc.shutdown() if __name__ == \\"__main__\\": world_size = 3 mp.spawn(worker, args=(world_size,), nprocs=world_size, join=True) ``` Submission: 1. Complete the `DistributedCounter` class implementation. 2. Write unit tests to validate the implementation. 3. Ensure documentation and comments are clear and explain the logic to manage the RRef lifecycle.","solution":"import torch import torch.distributed.rpc as rpc from torch.distributed.rpc import RRef import torch.multiprocessing as mp class DistributedCounter: def __init__(self, initial_value: int): Initialize the counter with the given initial value. self.value = initial_value def increment(self, increment_value: int): Increment the counter by the given value. self.value += increment_value def fetch(self) -> int: Return the current value of the counter. return self.value def worker(rank, world_size, counter_rref=None): Initialize RPC framework for each worker and test distributed counter rpc.init_rpc(name=f\'worker{rank}\', rank=rank, world_size=world_size) if rank == 0: # Worker 0 creates the DistributedCounter RRef counter_rref = rpc.remote(\'worker1\', DistributedCounter, args=(10,)) # Increment Counter Value from multiple workers rpc.rpc_sync(\'worker1\', DistributedCounter.increment, args=(counter_rref, 5)) rpc.rpc_sync(\'worker2\', DistributedCounter.increment, args=(counter_rref, 50)) # Fetch and print counter value final_value = rpc.rpc_sync(\'worker1\', DistributedCounter.fetch, args=(counter_rref,)) print(f\\"Final Counter value: {final_value}\\") rpc.shutdown() if __name__ == \\"__main__\\": world_size = 3 mp.spawn(worker, args=(world_size,), nprocs=world_size, join=True)"},{"question":"**Objective**: Demonstrate understanding of the `pipes` module in Python through function implementation and file manipulation. **Problem Statement**: You are required to create and manipulate file pipelines using the `pipes` module. Write a function `process_text_pipeline(input_file, output_file)` that processes the text in the input file and writes the processed result to the output file. The text processing should follow these steps: 1. Replace all lowercase vowels (\'a\', \'e\', \'i\', \'o\', \'u\') with uppercase vowels. 2. Remove all newline characters. Your implementation should simulate these steps using shell commands and the `pipes` module. The expected functions to utilize include `Template.append`, `Template.open`, and other relevant `Template` methods. **Function Signature**: ```python def process_text_pipeline(input_file: str, output_file: str) -> None: ... ``` **Input**: - `input_file` (str): Path to the input text file. - `output_file` (str): Path to the output text file. **Output**: - None. The function writes the processed content to the output file. **Examples**: *Example 1*: ```python # Contents of input_file: \\"HellonWorldneveryonen\\" process_text_pipeline(\'input.txt\', \'output.txt\') # Contents of output_file: \\"HEllOWOrldEvEryOnE\\" ``` **Constraints**: - Assume the input file is a valid text file. - Be mindful of file handling and ensure the pipeline operates as expected. - Use only the `pipes` module to create and manage the pipelines. **Guidelines**: 1. Open the `input_file` and read the content using the pipeline. 2. Process the content as per the problem statement. 3. Write the processed content to the `output_file` using the pipeline. **Note**: - Ensure your function is robust and handles different text files gracefully. - Performance is not a primary concern but ensure the function completes within a reasonable time frame for files up to 1MB. ```python def process_text_pipeline(input_file: str, output_file: str) -> None: import pipes # Create a Template instance t = pipes.Template() # Append commands to the pipeline t.append(\\"tr \'aeiou\' \'AEIOU\'\\", \'--\') t.append(\\"tr -d \'n\'\\", \'--\') # Process the file through the pipeline with t.open(input_file, \'r\') as infile: content = infile.read() with open(output_file, \'w\') as outfile: outfile.write(content) ```","solution":"def process_text_pipeline(input_file: str, output_file: str) -> None: import pipes # Create a Template instance t = pipes.Template() # Append commands to the pipeline t.append(\\"tr \'aeiou\' \'AEIOU\'\\", \'--\') t.append(\\"tr -d \'n\'\\", \'--\') # Process the file through the pipeline with t.open(input_file, \'r\') as infile: content = infile.read() with t.open(output_file, \'w\') as outfile: outfile.write(content)"},{"question":"**Question:** You are required to implement a function that simulates a basic network server capable of handling multiple client connections using the `select` module for I/O multiplexing. The server should be able to accept new connections, read data from connected clients, and send data back to them (echo server). # Objectives: 1. Implement the function `run_echo_server(port: int, backlog: int = 5, buffer_size: int = 1024, timeout: float = 1.0) -> None`. 2. The server should: - Listen on the specified port. - Accept new client connections. - Read incoming data from clients. - Echo each received message back to the sender. - Appropriately handle socket timeouts. - Close client connections that are disconnected. # Requirements: 1. You must use the `select` module for I/O multiplexing. 2. The server should handle at least 100 simultaneous connections. 3. Properly handle edge cases such as socket errors or client disconnections. 4. Ensure the server runs until manually stopped (infinite loop). # Function Signature: ```python import select import socket def run_echo_server(port: int, backlog: int = 5, buffer_size: int = 1024, timeout: float = 1.0) -> None: pass ``` # Constraints: - `port` - Port number on which the server listens for incoming connections. - `backlog` - Number of unaccepted connections the system will allow before refusing new connections. - `buffer_size` - Size of the buffer for reading data from the clients. - `timeout` - Maximum time in seconds the `select` call blocks, waiting for I/O events. # Example: ```python # Run the server on port 12345 run_echo_server(12345) ``` # Expected Operation: - The server initializes a listening socket on the provided port. - It uses `select.select()` to monitor multiple sockets for incoming connections and data. - When a new connection is detected, it accepts the connection and adds the client socket to the list of monitored sockets. - When data is received from a client, it should be read and sent back to the same client (echoed). - The server should handle disconnections and remove the relevant sockets from the monitored list. # Note: This is a simulation, and the main focus is on correctly implementing the I/O multiplexing using the `select` module. You may use additional helper functions or classes if necessary.","solution":"import select import socket def run_echo_server(port: int, backlog: int = 5, buffer_size: int = 1024, timeout: float = 1.0) -> None: # Create the server socket server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM) server_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1) server_socket.bind((\'0.0.0.0\', port)) server_socket.listen(backlog) # List of sockets to monitor for readability sockets_list = [server_socket] print(f\'Server listening on port {port}...\') try: while True: readable, _, exceptional = select.select(sockets_list, [], sockets_list, timeout) for sock in readable: if sock is server_socket: client_socket, client_address = server_socket.accept() print(f\'Accepted connection from {client_address}\') sockets_list.append(client_socket) else: data = sock.recv(buffer_size) if data: print(f\'Received data: {data} from {sock.getpeername()}\') sock.send(data) # Echo the data back to the client else: print(f\'Client {sock.getpeername()} disconnected\') sockets_list.remove(sock) sock.close() for sock in exceptional: print(f\'Handling exceptional condition for {sock.getpeername()}\') sockets_list.remove(sock) sock.close() except Exception as e: print(f\'Server error: {e}\') finally: for sock in sockets_list: sock.close() server_socket.close()"},{"question":"# Asynchronous Task Execution with Exception Handling You are developing an application that requires concurrent execution of multiple tasks. Some tasks may raise exceptions, and you need to ensure all exceptions are properly logged and handled without blocking the event loop. You also need to enable debugging mode to facilitate easier detection of unawaited coroutines and unhandled exceptions. Requirements: 1. **Implement two asynchronous functions**: - `async def task_success(n: int) -> int:`: Simulates a successful task that sleeps for `n` seconds and returns `n * 2` after completion. - `async def task_failure(n: int) -> None:`: Simulates a failing task that raises an `Exception` with the message `\\"Task failed\\"` after sleeping for `n` seconds. 2. **Implement an asynchronous function `main`**: - Create and schedule multiple `task_success` and `task_failure` tasks with varying sleep durations using `asyncio.create_task`. - Wait for all tasks to complete while ensuring that all exceptions are properly logged. - Enable asyncio\'s debug mode using the appropriate method. - Ensure any raised exceptions do not cause the event loop to block. 3. **Implement the necessary logging configuration** to log all relevant debug information and exceptions. Input Format: - No input required for the functions. - The functions will be called internally to run the tasks and handle exceptions. Output Format: - No output required from the functions. - All exceptions and debug information should be logged appropriately. Constraints: - Use the `asyncio` library for asynchronous programming. - Ensure `main` function can handle tasks completed out of order and log exceptions without blocking the event loop. Example: ```python import asyncio import logging # Configure logging logging.basicConfig(level=logging.DEBUG) async def task_success(n: int) -> int: await asyncio.sleep(n) return n * 2 async def task_failure(n: int) -> None: await asyncio.sleep(n) raise Exception(\\"Task failed\\") async def main(): # Enable debug mode asyncio.get_running_loop().set_debug(True) # Create tasks tasks = [ asyncio.create_task(task_success(1)), asyncio.create_task(task_failure(2)), asyncio.create_task(task_success(3)), asyncio.create_task(task_failure(4)) ] # Wait for tasks and handle exceptions for task in tasks: try: result = await task print(f\\"Task result: {result}\\") except Exception as e: logging.exception(\\"Caught an exception\\") # Run the main function asyncio.run(main()) ``` In this example, the function `task_success` simulates successful tasks, while `task_failure` simulates failing tasks. The `main` function creates and schedules these tasks, waits for them to complete, handles any exceptions raised, and logs the exceptions. Debug mode is enabled to facilitate easier debugging.","solution":"import asyncio import logging # Configure logging logging.basicConfig(level=logging.DEBUG) async def task_success(n: int) -> int: Simulates a successful task that sleeps for `n` seconds and returns `n * 2` after completion. await asyncio.sleep(n) return n * 2 async def task_failure(n: int) -> None: Simulates a failing task that raises an `Exception` with the message \\"Task failed\\" after sleeping for `n` seconds. await asyncio.sleep(n) raise Exception(\\"Task failed\\") async def main(): Creates and schedules multiple `task_success` and `task_failure` tasks with varying sleep durations. Waits for all tasks to complete while ensuring that all exceptions are properly logged. # Enable debug mode asyncio.get_running_loop().set_debug(True) # Create tasks tasks = [ asyncio.create_task(task_success(1)), asyncio.create_task(task_failure(2)), asyncio.create_task(task_success(3)), asyncio.create_task(task_failure(4)) ] # Wait for tasks and handle exceptions for task in tasks: try: result = await task print(f\\"Task result: {result}\\") except Exception as e: logging.exception(\\"Caught an exception\\") # Run the main function asyncio.run(main())"},{"question":"**Advanced Fraction Manipulation and Arithmetic** You are given a list of tuples where each tuple contains two integers representing the numerator and denominator of a fraction. Your task is to implement a function `process_fractions(fractions_list)` that takes this list, converts each pair of integers into a `Fraction` instance, and performs the following operations: 1. Normalize each fraction by using the greatest common divisor (GCD). Ensure that the denominator is always positive. 2. Sum all the fractions in the list. 3. Find the fraction in the list that is closest to the average of the fractions when limited to a maximum denominator of 1000. The function should return a dictionary with the following keys: - `normalized_fractions`: A list of all the normalized fractions as tuples `(numerator, denominator)`. - `sum`: The sum of all the fractions as a tuple `(numerator, denominator)`. - `closest_to_average`: The fraction that is closest to the average, limited to a denominator of 1000, as a tuple `(numerator, denominator)`. # Input Format - `fractions_list`: A list of tuples, where each tuple contains two integers representing the numerator and denominator of a fraction. For example: `[(1, 2), (3, 4), (-4, 5)]`. # Output Format - A dictionary with the keys `normalized_fractions`, `sum`, and `closest_to_average`. # Constraints - The input list will have at least one fraction. - The denominators in the input tuples are non-zero. # Example ```python from fractions import Fraction def process_fractions(fractions_list): # Implement the function body pass # Example usage: fractions = [(1, 2), (3, 4), (-4, 5)] result = process_fractions(fractions) print(result) # Expected output: # { # \'normalized_fractions\': [(1, 2), (3, 4), (-4, 5)], # \'sum\': (7, 20), # \'closest_to_average\': (7, 20) # } ``` Use appropriate methods from the `fractions` module to perform these operations. # Note - Make sure to handle negative fractions correctly such that the denominator is positive in the output. - Pay careful attention to how you calculate the average and how you limit the denominator for the closest fraction to the average.","solution":"from fractions import Fraction from math import gcd def normalize_fraction(numerator, denominator): # Ensure the denominator is positive if denominator < 0: numerator, denominator = -numerator, -denominator common_divisor = gcd(numerator, denominator) return numerator // common_divisor, denominator // common_divisor def fraction_difference(f1, f2): return abs(f1 - f2) def closest_fraction(target, denominators_limit): closest = None min_difference = float(\'inf\') for d in range(1, denominators_limit + 1): n = round(target * d) candidate_fraction = Fraction(n, d) diff = fraction_difference(candidate_fraction, target) if diff < min_difference: min_difference = diff closest = candidate_fraction return closest def process_fractions(fractions_list): normalized_fractions = [] total_sum = Fraction(0) for num, denom in fractions_list: normalized_num, normalized_denom = normalize_fraction(num, denom) normalized_fraction = Fraction(normalized_num, normalized_denom) normalized_fractions.append((normalized_num, normalized_denom)) total_sum += normalized_fraction # Average of fractions average = total_sum / len(fractions_list) closest_to_average = closest_fraction(average, 1000) return { \'normalized_fractions\': normalized_fractions, \'sum\': (total_sum.numerator, total_sum.denominator), \'closest_to_average\': (closest_to_average.numerator, closest_to_average.denominator) }"},{"question":"# HTML Parser Enhancement **Objective**: Create a subclass of `HTMLParser` that counts occurrences of different HTML elements and extracts textual content, while also implementing methods for pretty-printing the collected data. **Detailed Description**: You need to write a Python class `CustomHTMLParser` that derives from `html.parser.HTMLParser`. This class should: 1. Count occurrences of different HTML tags. 2. Extract all textual content. 3. Implement a method to return the results as a formatted string. **Requirements**: - Use `HTMLParser` methods to handle start tags, end tags, and data. - Maintain a count of each HTML tag encountered. - Collect all text data within the tags. - Implement the method `get_summary` to return a formatted string with the following structure: ``` Tag Counts: tag1: count1 tag2: count2 ... Text Content: [extracted text content] ``` **Method Specifications**: - `__init__(self)`: Initialize the parser and setup required structures for counting tags and collecting text. - `handle_starttag(self, tag, attrs)`: Increment the count for the encountered start tag. - `handle_endtag(self, tag)`: This method will not be used for counting but can be overridden if needed. - `handle_data(self, data)`: Collect the text data. - `get_summary(self)`: Return a formatted string summarizing tag counts and textual content. **Input and Output Examples**: Example: ```python parser = CustomHTMLParser() html_data = \\"<html><head><title>Test</title></head><body><p>Parse me!</p><p>And me!</p></body></html>\\" parser.feed(html_data) print(parser.get_summary()) ``` Expected Output: ``` Tag Counts: html: 1 head: 1 title: 1 body: 1 p: 2 Text Content: Test Parse me! And me! ``` **Constraints**: - The HTML input will always be a well-formed string. - Only valid HTML tags should be counted. - The `get_summary` method must produce the output in the specified format. Implement the `CustomHTMLParser` class to fulfill these requirements.","solution":"from html.parser import HTMLParser class CustomHTMLParser(HTMLParser): def __init__(self): super().__init__() self.tag_count = {} self.text_content = [] def handle_starttag(self, tag, attrs): if tag in self.tag_count: self.tag_count[tag] += 1 else: self.tag_count[tag] = 1 def handle_data(self, data): if data.strip(): # Avoid adding empty text content self.text_content.append(data.strip()) def get_summary(self): summary = \\"Tag Counts:n\\" for tag, count in self.tag_count.items(): summary += f\\"{tag}: {count}n\\" summary += \\"nText Content:n\\" summary += \\"n\\".join(self.text_content) return summary"},{"question":"**Python Interactive Code Compiler** **Objective:** You are tasked with implementing an interactive Python code compiler using the `codeop` module. The goal is to simulate a basic REPL environment where Python code can be executed line-by-line or block-by-block. **Instructions:** 1. Implement a class `InteractiveCompiler` that supports compiling and executing multi-line Python code inputs. 2. Your class should use the `codeop.compile_command` function to compile code. 3. Ensure that your compiler can handle `__future__` statements and apply them to subsequent code compilations. **Class Definition:** ```python class InteractiveCompiler: def __init__(self): Initialize an instance of InteractiveCompiler. # Initialize the necessary components here def compile_and_execute(self, source: str) -> None: Compile and execute a given string of Python code. Parameters: source (str): A string containing the Python code to compile and execute. Returns: None # Implement the method to compile and execute the code string using the `codeop` module ``` **Expected Behavior:** - The `compile_and_execute` method should accept a string of Python code. - If the code is incomplete but could become valid with more input, it should prompt for additional lines. - If the code is syntactically correct, it should execute the code and print any output. - If the code contains a `__future__` statement, it should remember it and apply the future feature to subsequent code. **Example Usage:** ```python compiler = InteractiveCompiler() compiler.compile_and_execute(\\"x = 10\\") compiler.compile_and_execute(\\"print(x)\\") # Handling multiline input compiler.compile_and_execute(\\"def greet(name):\\") compiler.compile_and_execute(\\" return f\'Hello, {name}\'\\") compiler.compile_and_execute(\\"print(greet(\'Alice\'))\\") # Handling `__future__` statements compiler.compile_and_execute(\\"from __future__ import print_function\\") compiler.compile_and_execute(\\"print(\'Using print function\')\\") ``` **Constraints:** - Do not use any modules or functions to directly evaluate Python code other than what is provided by the `codeop` module. **Performance Requirements:** - Ensure that the compiler tracks state efficiently to handle `__future__` imports and subsequently apply them correctly.","solution":"import codeop import sys import builtins class InteractiveCompiler: def __init__(self): Initialize an instance of InteractiveCompiler. self.compiler = codeop.Compile() self.future_flags = 0 self.locals = {} def compile_and_execute(self, source: str) -> None: Compile and execute a given string of Python code. Parameters: source (str): A string containing the Python code to compile and execute. Returns: None try: code_obj = self.compiler(source, filename=\\"<input>\\", symbol=\\"single\\") if code_obj: exec(code_obj, None, self.locals) return else: print(\\"... \\", end=\\"\\") except Exception as e: print(f\\"Error: {e}\\") # Example usage: if __name__ == \'__main__\': compiler = InteractiveCompiler() compiler.compile_and_execute(\\"x = 10\\") compiler.compile_and_execute(\\"print(x)\\") # Handling multiline input compiler.compile_and_execute(\\"def greet(name):\\") compiler.compile_and_execute(\\" return f\'Hello, {name}\'\\") compiler.compile_and_execute(\\"print(greet(\'Alice\'))\\") # Handling `__future__` statements compiler.compile_and_execute(\\"from __future__ import print_function\\") compiler.compile_and_execute(\\"print(\'Using print function\')\\")"},{"question":"Objective To test your understanding of the `zlib` module\'s compression and decompression functionalities, error handling, and checksum computations in Python. Problem Statement You are given a sequence of data chunks that need to be compressed, transmitted, and then decompressed. Your task is to write functions that can handle the following: 1. **Compress Data Chunks:** Write a function `compress_data_chunks(data_chunks: List[bytes], level: int = -1) -> bytes` which takes a list of byte strings (`data_chunks`) and an integer `level` indicating compression level (from 0 to 9 or -1 for default) and returns the compressed data as a single byte string. 2. **Compute Checksum:** Write a function `compute_checksum(data: bytes, initial_value: int = 1) -> int` which computes and returns the Adler-32 checksum of the given byte string `data`. The function should allow specification of an initial value for the checksum computation. 3. **Decompress Data:** Write a function `decompress_data(compressed_data: bytes) -> bytes` which takes a single byte string `compressed_data` containing the compressed data and returns the decompressed data. Expected Input and Output Formats 1. `compress_data_chunks(data_chunks: List[bytes], level: int = -1) -> bytes` - `data_chunks`: A list of byte strings (List[bytes]) representing the data to be compressed. - `level`: An integer representing the compression level (default is -1). - Returns a single byte string containing compressed data. 2. `compute_checksum(data: bytes, initial_value: int = 1) -> int` - `data`: A byte string containing the data for which the checksum needs to be computed. - `initial_value`: An integer representing the initial value for checksum calculation (default is 1). - Returns an integer representing the computed Adler-32 checksum. 3. `decompress_data(compressed_data: bytes) -> bytes` - `compressed_data`: A byte string containing the compressed data. - Returns a single byte string containing decompressed data. Constraints - The total size of data chunks before compression should not exceed 10 MB. - You must handle potential compression and decompression errors, raising appropriate exceptions when errors occur. - Ensure that all edge cases like empty data, and boundary compression levels are handled appropriately. Example ```python data_chunks = [b\'example data part 1\', b\'example data part 2\'] compressed = compress_data_chunks(data_chunks, level=6) checksum = compute_checksum(b\'example data part 1\' + b\'example data part 2\') decompressed = decompress_data(compressed) assert checksum == zlib.adler32(b\'example data part 1\' + b\'example data part 2\') assert decompressed == b\'example data part 1\' + b\'example data part 2\' ``` Note 1. Use appropriate exception handling to manage any errors that occur during compression and decompression. 2. Ensure you precisely follow the zlib module\'s methods and function signatures as described in the documentation.","solution":"import zlib from typing import List def compress_data_chunks(data_chunks: List[bytes], level: int = -1) -> bytes: Compress the given data chunks with the specified compression level. compressor = zlib.compressobj(level) compressed_data = b\'\'.join(compressor.compress(chunk) for chunk in data_chunks) compressed_data += compressor.flush() return compressed_data def compute_checksum(data: bytes, initial_value: int = 1) -> int: Compute the Adler-32 checksum of the given data, starting with the specified initial value. return zlib.adler32(data, initial_value) def decompress_data(compressed_data: bytes) -> bytes: Decompress the given compressed data. decompressor = zlib.decompressobj() try: decompressed_data = decompressor.decompress(compressed_data) decompressed_data += decompressor.flush() return decompressed_data except zlib.error as e: raise RuntimeError(\\"Decompression failed\\") from e"},{"question":"Objective: Create and test a custom operator in PyTorch using the `torch.library` module. The custom operator should perform a tailored mathematical operation and support gradient computations. You will also need to test the custom operator using provided utilities to ensure correctness. Task: 1. **Define a custom operation** named `custom_relu` that performs a ReLU (Rectified Linear Unit) but adds a constant value to the output. - Input: A PyTorch tensor `x` and a constant `c`. - Output: A tensor where each element is the ReLU of the corresponding element in `x` plus the constant `c`. 2. **Register the custom operation** using the appropriate `torch.library` API. 3. **Ensure the custom operation supports gradient computations** by implementing the backward function. 4. **Test the custom operation** using the `torch.autograd.gradcheck` to validate the gradients. 5. **Verify the correctness** of your custom operator using the `torch.library.opcheck` function. Constraints: - You must use the `torch.library.custom_op` for defining the custom operator. - The custom operator should be compatible with both CPU and CUDA (if available). - Performance should be optimal and handle inputs efficiently. Function Signature: ```python import torch from torch.library import custom_op, opcheck, register_kernel from torch.autograd import gradcheck def custom_relu(x: torch.Tensor, c: float) -> torch.Tensor: # Define your custom ReLU operator here pass def register_custom_op(): # Register your custom ReLU operator here pass def test_custom_op(): # Implement gradient check and other tests here pass # The detailed implementation of the functions above should go here if __name__ == \\"__main__\\": # Example tensor x = torch.randn((5, 5), dtype=torch.float64, requires_grad=True) c = 0.5 # Register the custom op register_custom_op() # Apply the custom op custom_result = custom_relu(x, c) # Test the custom op test_custom_op() ``` Expected Input and Output Formats: - Input: A tensor `x` of shape `(N, M)` and a float constant `c`. - Output: A tensor of the same shape `(N, M)`, where each element is `ReLU(x_ij) + c`. Example: ```python x = torch.tensor([[-1.0, 2.0], [0.0, -3.0]], requires_grad=True) c = 0.5 output = custom_relu(x, c) # Output should be tensor([[0.5, 2.5], [0.5, 0.5]]) ``` Note: Provide the complete code implementation for the `custom_relu`, `register_custom_op`, and `test_custom_op` functions. Ensure the code is well-commented and follows best practices in PyTorch custom operator design.","solution":"import torch from torch.autograd import Function class CustomReLU(Function): @staticmethod def forward(ctx, x, c): ctx.save_for_backward(x) ctx.constant = c return torch.relu(x) + c @staticmethod def backward(ctx, grad_output): x, = ctx.saved_tensors grad_x = grad_output.clone() grad_x[x < 0] = 0 return grad_x, None def custom_relu(x: torch.Tensor, c: float) -> torch.Tensor: return CustomReLU.apply(x, c) def test_custom_op(): x = torch.randn((5, 5), dtype=torch.float64, requires_grad=True) c = 0.5 # Check gradients assert torch.autograd.gradcheck(lambda x: custom_relu(x, c), (x,)), \\"gradcheck failed!\\" # Check the correctness of output test_tensor = torch.tensor([[-1.0, 2.0], [0.0, -3.0]], requires_grad=True, dtype=torch.float64) expected_output = torch.tensor([[0.5, 2.5], [0.5, 0.5]], dtype=torch.float64) output = custom_relu(test_tensor, c) assert torch.allclose(output, expected_output), \\"Output of custom_relu does not match expected output!\\" if __name__ == \\"__main__\\": # Example tensor x = torch.randn((5, 5), dtype=torch.float64, requires_grad=True) c = 0.5 # Apply the custom op custom_result = custom_relu(x, c) # Test the custom op test_custom_op()"},{"question":"**Coding Assessment Question** **Objective**: Assess students\' understanding of the `pwd` module in Python, and their ability to manipulate and retrieve Unix user account information. **Problem Statement**: You are tasked to write a function that retrieves specific user account information and organizes it in a structured format. The function should be able to: 1. Retrieve user account information by either user ID or user name. 2. Format and return the information as a dictionary. 3. Handle errors gracefully if the user ID or user name does not exist. **Function Signature**: ```python def get_user_info(identifier: Union[int, str]) -> Dict[str, Union[str, int]]: Retrieve the user account information for the given identifier, which can be either a user ID (int) or a user name (str). Parameters: identifier (Union[int, str]): The user ID (int) or user name (str) to look up. Returns: Dict[str, Union[str, int]]: A dictionary containing the user account information. Raises: KeyError: If the user ID or user name does not exist in the password database. pass ``` **Input**: - `identifier`: An integer representing the user ID or a string representing the user name. **Output**: - A dictionary containing the following keys and corresponding values: - `\\"pw_name\\"`: Login name (str) - `\\"pw_uid\\"`: Numerical user ID (int) - `\\"pw_gid\\"`: Numerical group ID (int) - `\\"pw_gecos\\"`: User name or comment field (str) - `\\"pw_dir\\"`: User home directory (str) - `\\"pw_shell\\"`: User command interpreter (str) **Constraints**: - The function should use the `pwd` module to retrieve the user account information. - The function should handle both user IDs and user names. - The function should raise an appropriate `KeyError` if the user ID or user name does not exist. **Example**: ```python # Example usage of get_user_info try: user_info = get_user_info(\'root\') print(user_info) # Output: {\'pw_name\': \'root\', \'pw_uid\': 0, \'pw_gid\': 0, \'pw_gecos\': \'root\', \'pw_dir\': \'/root\', \'pw_shell\': \'/bin/bash\'} user_info = get_user_info(1000) print(user_info) # Output: {\'pw_name\': \'student\', \'pw_uid\': 1000, \'pw_gid\': 1000, \'pw_gecos\': \'Student User\', \'pw_dir\': \'/home/student\', \'pw_shell\': \'/bin/bash\'} except KeyError as e: print(e) # Output: \'User not found\' ``` **Notes**: - Ensure that your function is efficient and handles error cases. - Test your function with both valid and invalid identifiers to ensure robustness.","solution":"import pwd from typing import Union, Dict def get_user_info(identifier: Union[int, str]) -> Dict[str, Union[str, int]]: Retrieve the user account information for the given identifier, which can be either a user ID (int) or a user name (str). Parameters: identifier (Union[int, str]): The user ID (int) or user name (str) to look up. Returns: Dict[str, Union[str, int]]: A dictionary containing the user account information. Raises: KeyError: If the user ID or user name does not exist in the password database. try: if isinstance(identifier, int): user_info = pwd.getpwuid(identifier) elif isinstance(identifier, str): user_info = pwd.getpwnam(identifier) else: raise KeyError(\\"Invalid identifier type\\") except KeyError: raise KeyError(\\"User not found\\") return { \'pw_name\': user_info.pw_name, \'pw_uid\': user_info.pw_uid, \'pw_gid\': user_info.pw_gid, \'pw_gecos\': user_info.pw_gecos, \'pw_dir\': user_info.pw_dir, \'pw_shell\': user_info.pw_shell }"},{"question":"# Task: Multi-threaded Task Processing and Queue Management You are to implement a multi-threaded task processing system using the `queue` module. In this system, tasks will be added to different types of queues, and worker threads will process these tasks based on their type and priority. # Requirements: 1. **Queue Management**: - Create three different queues using `queue.Queue`, `queue.LifoQueue`, and `queue.PriorityQueue`. All queues should have a `maxsize` of `5`. - Create a `queue.SimpleQueue` for logging. 2. **Task Generation**: - Define a function `generate_tasks()` that takes an integer `n` and adds `n` tasks to each of the three queues. - Each task should be represented by a tuple `(priority_number, \'task_info\')` where: - `priority_number` is a random integer between `1` and `100` for the Priority Queue (use the `random` module). - `\'task_info\'` is a string indicating the task number (e.g., `\'task_1\'`, `\'task_2\'`). 3. **Worker Threads**: - Implement a worker thread function `worker(queue_type, log_queue)` that: - Continually fetches tasks from the specified `queue_type`. - Logs the task details into the `log_queue` using a `print()` statement. - Calls `task_done()` once the task is processed. 4. **Main Function**: - Create and start at least two worker threads for each queue. - Ensure all tasks are processed and logged by the worker threads. - Use `join()` to wait for all tasks to be completed before the program exits. - Ensure proper error handling when queues become empty or full. # Input: - An integer `n` indicating the number of tasks to generate for each queue. # Output: - Logging of tasks processed by each worker. # Constraints: - Task generation should be done safely, and no tasks should be lost or duplicated. - Ensure proper synchronization and handling of queues. # Example: ```python import threading import queue import random def generate_tasks(n, fifo_queue, lifo_queue, priority_queue): for i in range(n): task_info = f\'task_{i + 1}\' priority = random.randint(1, 100) fifo_queue.put((priority, task_info)) lifo_queue.put((priority, task_info)) priority_queue.put((priority, task_info)) def worker(queue_type, log_queue): while True: try: task = queue_type.get(timeout=2) # Timeout to avoid infinite block print(f\'Processing {task} from {queue_type}\') log_queue.put(f\'{task} processed\') queue_type.task_done() except queue.Empty: break def main(n): fifo_queue = queue.Queue(maxsize=5) lifo_queue = queue.LifoQueue(maxsize=5) priority_queue = queue.PriorityQueue(maxsize=5) log_queue = queue.SimpleQueue() generate_tasks(n, fifo_queue, lifo_queue, priority_queue) threads = [] for i in range(2): t_fifo = threading.Thread(target=worker, args=(fifo_queue, log_queue)) threads.append(t_fifo) t_fifo.start() t_lifo = threading.Thread(target=worker, args=(lifo_queue, log_queue)) threads.append(t_lifo) t_lifo.start() t_priority = threading.Thread(target=worker, args=(priority_queue, log_queue)) threads.append(t_priority) t_priority.start() for t in threads: t.join() while not log_queue.empty(): print(log_queue.get()) if __name__ == \\"__main__\\": main(10) ``` This problem tests the student\'s understanding and practical application of threading and various queue types in Python, focusing on synchronization and safe task management.","solution":"import threading import queue import random def generate_tasks(n, fifo_queue, lifo_queue, priority_queue): for i in range(n): task_info = f\'task_{i + 1}\' priority = random.randint(1, 100) fifo_queue.put((priority, task_info)) lifo_queue.put((priority, task_info)) priority_queue.put((priority, task_info)) def worker(queue_type, log_queue): while True: try: task = queue_type.get(timeout=2) # Timeout to avoid infinite block print(f\'Processing {task} from {queue_type}\') log_queue.put(f\'{task} processed\') queue_type.task_done() except queue.Empty: break def main(n): fifo_queue = queue.Queue(maxsize=5) lifo_queue = queue.LifoQueue(maxsize=5) priority_queue = queue.PriorityQueue(maxsize=5) log_queue = queue.SimpleQueue() generate_tasks(n, fifo_queue, lifo_queue, priority_queue) threads = [] for i in range(2): t_fifo = threading.Thread(target=worker, args=(fifo_queue, log_queue)) threads.append(t_fifo) t_fifo.start() t_lifo = threading.Thread(target=worker, args=(lifo_queue, log_queue)) threads.append(t_lifo) t_lifo.start() t_priority = threading.Thread(target=worker, args=(priority_queue, log_queue)) threads.append(t_priority) t_priority.start() for t in threads: t.join() while not log_queue.empty(): print(log_queue.get())"},{"question":"<|Analysis Begin|> The documentation provided explains the `atexit` module in Python, which is used to register and manage functions to be executed upon the normal termination of the Python interpreter. The module provides two main functions: 1. `atexit.register(func, *args, **kwargs)`: Registers a function `func` to be called at interpreter termination. Optional positional and keyword arguments can also be passed to `func`. 2. `atexit.unregister(func)`: Unregisters a function `func` that was previously registered. The documentation also provides an example demonstrating how to use `atexit` to save the value of an updated counter to a file on program termination. There are also examples showing how to pass arguments to registered functions and how to use `atexit.register` as a decorator. Key points: - Functions registered with `atexit` are executed in the reverse order of their registration. - Exceptions raised during the execution of exit handlers are printed and saved, with the last exception being re-raised after all handlers have run. - The `atexit` module does not guarantee execution of registered functions if the program terminates unexpectedly (e.g., killed by a signal or a Python fatal internal error). <|Analysis End|> <|Question Begin|> **Python Coding Assessment Question: Using `atexit` to Manage Resource Cleanup** The `atexit` module in Python is used to register functions that are executed upon normal interpreter termination. This capability is useful for tasks like saving state, cleaning up resources, and ensuring graceful exits. # Task: Create a Python class `ResourceManager` that uses the `atexit` module to manage resources automatically at the end of the program. Your class should meet the following requirements: 1. **Attributes:** - `resources`: A dictionary to store resource names and their corresponding statuses (e.g., `\'resource1\': \'open\'`). 2. **Methods:** - `add_resource(name: str)`: Adds a resource with the given name to the `resources` dictionary and sets its status to `\'open\'`. - `close_resource(name: str)`: Closes a resource by setting its status to `\'closed\'` in the `resources` dictionary. - `_cleanup_resources()`: A private method that iterates over the `resources` dictionary and ensures all resources have their status set to `\'closed\'`. This method should be registered with `atexit` to be called at program termination. - `__enter__()`: Implements the context manager protocol. Should return `self`. - `__exit__(exc_type, exc_value, traceback)`: Implements the context manager protocol. Should call `_cleanup_resources()`. 3. **Usage:** - Your class should be usable both as a context manager and a standalone object where resources are guaranteed to be cleaned up at interpreter termination. - Demonstrate the usage of the `ResourceManager` class, showing that resources are automatically cleaned up at the termination of the program. # Constraints: - You may assume resource names are unique strings. - Your class should handle attempts to close resources that do not exist by ignoring them without raising an error. # Example: ```python import atexit class ResourceManager: def __init__(self): self.resources = {} atexit.register(self._cleanup_resources) def add_resource(self, name: str): self.resources[name] = \'open\' def close_resource(self, name: str): if name in self.resources: self.resources[name] = \'closed\' def _cleanup_resources(self): for name in self.resources: self.resources[name] = \'closed\' print(\\"All resources have been cleaned up.\\") def __enter__(self): return self def __exit__(self, exc_type, exc_value, traceback): self._cleanup_resources() # Example usage: if __name__ == \\"__main__\\": with ResourceManager() as rm: rm.add_resource(\'database_connection\') rm.add_resource(\'file_handle\') # Resources will be automatically closed here # Confirming standalone usage rm = ResourceManager() rm.add_resource(\'network_connection\') rm.close_resource(\'network_connection\') # Resources will be automatically closed on interpreter termination ``` # Note: Ensure all resources are closed gracefully, and confirm that your implementation works as expected by running the program and observing the output.","solution":"import atexit class ResourceManager: def __init__(self): self.resources = {} atexit.register(self._cleanup_resources) def add_resource(self, name: str): Adds a resource with the given name to the resources dictionary and sets its status to \'open\'. self.resources[name] = \'open\' def close_resource(self, name: str): Closes a resource by setting its status to \'closed\' in the resources dictionary. if name in self.resources: self.resources[name] = \'closed\' def _cleanup_resources(self): Ensures all resources have their status set to \'closed\'. This method is registered with atexit to be called at program termination. for name in self.resources: self.resources[name] = \'closed\' print(\\"All resources have been cleaned up.\\") def __enter__(self): Implements the context manager protocol. return self def __exit__(self, exc_type, exc_value, traceback): Implements the context manager protocol. Calls _cleanup_resources() to ensure resources are cleaned up. self._cleanup_resources()"},{"question":"Objective: The goal of this assessment is to evaluate your proficiency in using the pandas library for file I/O operations and data manipulation. You are expected to read data from various formats, perform data transformations, and output the result in a specified format. Problem Statement: You are given three datasets in different formats: - A CSV file containing user data: `users.csv` - A JSON file containing purchase data: `purchases.json` - An Excel file containing product details: `products.xlsx` Your task is to: 1. **Read** these files into appropriate pandas DataFrames. 2. **Merge** these DataFrames into a single DataFrame based on common keys. 3. **Perform** data manipulation operations as specified. 4. **Output** the final DataFrame to a CSV file named `final_report.csv`. Detailed Steps: 1. **Read the Files:** - `users.csv`: Contains user information with columns `user_id`, `name`, `age`. - `purchases.json`: Contains purchase records with columns `purchase_id`, `user_id`, `product_id`, `amount`. - `products.xlsx`: Contains product details with columns `product_id`, `product_name`, `price`. 2. **Merge DataFrames:** - Merge `users` and `purchases` on `user_id`. - Merge the result with `products` on `product_id`. 3. **Manipulate Data:** - Calculate total spending for each user (sum of `amount` for each user). - Keep only the following columns in the final DataFrame: `user_id`, `name`, `total_spending`. - Sort the final DataFrame by `total_spending` in descending order. 4. **Output Data:** - Write the final DataFrame to a CSV file named `final_report.csv`. Sample Input Files: **users.csv** ``` user_id,name,age 1,John Doe,28 2,Jane Smith,34 3,Bob Johnson,45 ``` **purchases.json** ```json [ {\\"purchase_id\\": 1, \\"user_id\\": 1, \\"product_id\\": 101, \\"amount\\": 150.0}, {\\"purchase_id\\": 2, \\"user_id\\": 2, \\"product_id\\": 102, \\"amount\\": 200.0}, {\\"purchase_id\\": 3, \\"user_id\\": 1, \\"product_id\\": 103, \\"amount\\": 50.0} ] ``` **products.xlsx** | product_id | product_name | price | |------------|---------------|-------| | 101 | Product A | 100 | | 102 | Product B | 200 | | 103 | Product C | 50 | Expected Output: **final_report.csv** ``` user_id,name,total_spending 1,John Doe,200.0 2,Jane Smith,200.0 ``` Constraints: - You must use the appropriate pandas functions to read the respective file formats. - Ensure that your code handles missing or malformed data gracefully. Submission: Submit the Python script `data_processing.py` containing your solution. The script should read the input files, perform the required operations, and generate the output file `final_report.csv`. Performance Considerations: - Aim to write efficient code that minimizes memory usage and processing time, especially considering potentially large datasets. Good luck!","solution":"import pandas as pd def generate_final_report(): # Read the files into pandas DataFrames users = pd.read_csv(\'users.csv\') purchases = pd.read_json(\'purchases.json\') products = pd.read_excel(\'products.xlsx\') # Merge DataFrames user_purchases = pd.merge(purchases, users, on=\'user_id\') all_data = pd.merge(user_purchases, products, on=\'product_id\') # Calculate total spending for each user total_spending = all_data.groupby([\'user_id\', \'name\'])[\'amount\'].sum().reset_index() total_spending.rename(columns={\'amount\': \'total_spending\'}, inplace=True) # Sort by total spending in descending order final_report = total_spending.sort_values(by=\'total_spending\', ascending=False) # Write the final DataFrame to a CSV file final_report.to_csv(\'final_report.csv\', index=False)"},{"question":"# Python Coding Assessment Objective Design a Python function using the `resource` module that monitors and manages resource usage for a given process. The function should raise an exception if the CPU time exceeds a specific threshold. Requirements 1. **Function Name:** `monitor_resource_usage` 2. **Parameters:** - `pid` (Integer): Process ID of the process to monitor. If 0, the current process should be monitored. - `cpu_time_limit` (Float): The maximum allowable CPU time (in seconds). If the process exceeds this limit, an exception should be raised. 3. **Returns:** The function should not return any value. 4. **Behavior:** - Use `resource.prlimit` to get the current CPU time limits for the process. - If the current CPU time limit is less than `cpu_time_limit`, raise an exception indicating insufficient resource limits. - Periodically check the CPU usage of the process using `resource.getrusage`. - If the CPU usage exceeds `cpu_time_limit`, raise an exception indicating CPU time overuse. Constraints - The function should handle various exceptions, such as invalid `pid`, invalid resource specifications, and permission errors. - The process should be checked for CPU usage every 2 seconds. - Ensure the function can handle both positive and negative test cases. Example Usage ```python import time def monitor_resource_usage(pid: int, cpu_time_limit: float) -> None: import resource try: # Step 1: Retrieve current CPU time limits current_limits = resource.prlimit(pid, resource.RLIMIT_CPU) # Step 2: Check if the current limit is less than required if current_limits[0] < cpu_time_limit: raise Exception(\\"Insufficient resource limits for CPU time.\\") # Step 3: Monitor CPU usage periodically while True: usage = resource.getrusage(resource.RUSAGE_SELF if pid == 0 else resource.RUSAGE_CHILDREN) if usage.ru_utime + usage.ru_stime > cpu_time_limit: raise Exception(\\"CPU time limit exceeded.\\") time.sleep(2) # Check every 2 seconds except (ValueError, resource.error) as e: # Handle exceptions for invalid PID, resource specs, or other errors raise Exception(f\\"Error monitoring resource usage: {e}\\") # Example of the function in use: try: monitor_resource_usage(0, 5.0) # Monitor current process with a 5-second CPU time limit except Exception as e: print(e) ``` In this function, import the necessary modules inside the function rather than globally. Use the `resource` module methods to retrieve and set CPU limits. Ensure students show their understanding of exception handling around system calls. The function implementation covers both limits checking and periodic resource usage monitoring.","solution":"import time import resource def monitor_resource_usage(pid: int, cpu_time_limit: float) -> None: try: # Step 1: Retrieve current CPU time limits current_limits = resource.prlimit(pid, resource.RLIMIT_CPU) # Check if the current limit is set to infinity, which means no limit if current_limits[0] == resource.RLIM_INFINITY or current_limits[0] == -1: current_limit_value = cpu_time_limit else: current_limit_value = current_limits[0] # Step 2: Check if the current limit is less than required if current_limit_value < cpu_time_limit: raise Exception(\\"Insufficient resource limits for CPU time.\\") # Step 3: Monitor CPU usage periodically while True: usage = resource.getrusage(resource.RUSAGE_SELF if pid == 0 else resource.RUSAGE_CHILDREN) if usage.ru_utime + usage.ru_stime > cpu_time_limit: raise Exception(\\"CPU time limit exceeded.\\") time.sleep(2) # Check every 2 seconds except (ValueError, resource.error) as e: # Handle exceptions for invalid PID, resource specs, or other errors raise Exception(f\\"Error monitoring resource usage: {e}\\")"},{"question":"# Pandas DataFrame Manipulation and Analysis Task Objective You are tasked with analyzing sales data to generate insights and prepare it for further analysis. Your solution should demonstrate your understanding of various pandas functionalities, including data manipulation, aggregation, and handling missing data. # Problem Statement You have been given a dataset containing daily sales data for multiple products across various stores. The dataset is provided as a `pandas.DataFrame` in the following format: | Date | StoreID | ProductID | Sales Units | Sales Amount | |------------|---------|-----------|-------------|--------------| | 2023-01-01 | 1 | 1001 | 23 | 460 | | 2023-01-01 | 2 | 1002 | 17 | 340 | | ... | ... | ... | ... | ... | The data may have missing sales information for some dates, stores, or products. You need to perform the following tasks: 1. **Load and preprocess the data:** - Fill missing sales information. If both `Sales Units` and `Sales Amount` are missing, assume `Sales Units` to be `0` and `Sales Amount` to be `0`. 2. **Calculate the total sales for each store and each product:** - Calculate the total sales units and total sales amount for each store. - Calculate the total sales units and total sales amount for each product. 3. **Identify the top-performing store and product:** - Find the StoreID with the highest total sales amount. - Find the ProductID with the highest total sales units. 4. **Generate a pivot table:** - Create a pivot table that shows the total sales units and total sales amount for each combination of StoreID and ProductID. # Constraints - Data is guaranteed to fit into memory. - Assume that `Date` is in the format \'YYYY-MM-DD\' and is sorted in ascending order. - The dataset may be large, so aim for efficiency in your solution. # Input and Output Formats Input - A DataFrame named `sales_data` with columns: `Date`, `StoreID`, `ProductID`, `Sales Units`, `Sales Amount`. Output 1. `total_sales_by_store`: a DataFrame with columns `StoreID`, `Total Sales Units`, `Total Sales Amount`. 2. `total_sales_by_product`: a DataFrame with columns `ProductID`, `Total Sales Units`, `Total Sales Amount`. 3. `top_store_id`: an integer representing the StoreID with the highest total sales amount. 4. `top_product_id`: an integer representing the ProductID with the highest total sales units. 5. `pivot_table`: a pivot table DataFrame with `StoreID` as the index, `ProductID` as the columns, and values showing total `Sales Units` and `Sales Amount`. # Function Definition ```python def analyze_sales_data(sales_data: pd.DataFrame) -> tuple: Analyzes and processes the sales data. Args: sales_data (pd.DataFrame): DataFrame containing sales data with columns [\'Date\', \'StoreID\', \'ProductID\', \'Sales Units\', \'Sales Amount\'] Returns: tuple: (total_sales_by_store, total_sales_by_product, top_store_id, top_product_id, pivot_table) total_sales_by_store (pd.DataFrame): DataFrame with columns [\'StoreID\', \'Total Sales Units\', \'Total Sales Amount\'] total_sales_by_product (pd.DataFrame): DataFrame with columns [\'ProductID\', \'Total Sales Units\', \'Total Sales Amount\'] top_store_id (int): StoreID with the highest total sales amount top_product_id (int): ProductID with the highest total sales units pivot_table (pd.DataFrame): pivot table DataFrame with StoreID as index, ProductID as columns and values showing total Sales Units and Sales Amount. pass ``` # Example Usage ```python import pandas as pd # Example data data = { \'Date\': [\'2023-01-01\', \'2023-01-01\', \'2023-01-02\', \'2023-01-02\'], \'StoreID\': [1, 2, 1, 2], \'ProductID\': [1001, 1002, 1002, 1001], \'Sales Units\': [23, 17, 25, 10], \'Sales Amount\': [460, 340, 500, 200] } sales_data = pd.DataFrame(data) results = analyze_sales_data(sales_data) # Access results total_sales_by_store, total_sales_by_product, top_store_id, top_product_id, pivot_table = results print(total_sales_by_store) print(total_sales_by_product) print(top_store_id) print(top_product_id) print(pivot_table) ``` You may need to simulate a larger and more complex dataset for thorough testing. Aim for precision, performance, and clarity in your implementation.","solution":"import pandas as pd def analyze_sales_data(sales_data: pd.DataFrame) -> tuple: Analyzes and processes the sales data. Args: sales_data (pd.DataFrame): DataFrame containing sales data with columns [\'Date\', \'StoreID\', \'ProductID\', \'Sales Units\', \'Sales Amount\'] Returns: tuple: (total_sales_by_store, total_sales_by_product, top_store_id, top_product_id, pivot_table) total_sales_by_store (pd.DataFrame): DataFrame with columns [\'StoreID\', \'Total Sales Units\', \'Total Sales Amount\'] total_sales_by_product (pd.DataFrame): DataFrame with columns [\'ProductID\', \'Total Sales Units\', \'Total Sales Amount\'] top_store_id (int): StoreID with the highest total sales amount top_product_id (int): ProductID with the highest total sales units pivot_table (pd.DataFrame): pivot table DataFrame with StoreID as index, ProductID as columns and values showing total Sales Units and Sales Amount. # Fill missing sales information with zeros where both values are missing sales_data[\'Sales Units\'] = sales_data[\'Sales Units\'].fillna(0) sales_data[\'Sales Amount\'] = sales_data[\'Sales Amount\'].fillna(0) # Calculate total sales for each store total_sales_by_store = sales_data.groupby(\'StoreID\').agg({ \'Sales Units\': \'sum\', \'Sales Amount\': \'sum\' }).reset_index().rename(columns={\'Sales Units\': \'Total Sales Units\', \'Sales Amount\': \'Total Sales Amount\'}) # Calculate total sales for each product total_sales_by_product = sales_data.groupby(\'ProductID\').agg({ \'Sales Units\': \'sum\', \'Sales Amount\': \'sum\' }).reset_index().rename(columns={\'Sales Units\': \'Total Sales Units\', \'Sales Amount\': \'Total Sales Amount\'}) # Find the top-performing store and product top_store_id = total_sales_by_store.loc[total_sales_by_store[\'Total Sales Amount\'].idxmax(), \'StoreID\'] top_product_id = total_sales_by_product.loc[total_sales_by_product[\'Total Sales Units\'].idxmax(), \'ProductID\'] # Generate pivot table pivot_table = sales_data.pivot_table(values=[\'Sales Units\', \'Sales Amount\'], index=\'StoreID\', columns=\'ProductID\', aggfunc=\'sum\', fill_value=0) return total_sales_by_store, total_sales_by_product, top_store_id, top_product_id, pivot_table"},{"question":"Objective: To test your understanding of the `random` module, particularly its ability to generate pseudo-random numbers for various distributions and manage the state of the random number generator. Problem Statement: You are tasked with creating a mini-simulation of a card game. The game involves shuffling a standard deck of cards, dealing a specific number of cards to each player, and simulating a simple game to determine the winner based on a predefined scoring system. Instructions: 1. **Deck Creation**: Write a function `create_deck()` that returns a list representing a standard 52-card deck. Each card should be a tuple of the form `(rank, suit)`, where `rank` is one of `\'2\', \'3\', \'4\', \'5\', \'6\', \'7\', \'8\', \'9\', \'10\', \'J\', \'Q\', \'K\', \'A\'`, and `suit` is one of `\'hearts\', \'diamonds\', \'clubs\', \'spades\'`. 2. **Shuffling**: Write a function `shuffle_deck(deck)` that takes a deck of cards (as created by `create_deck()`) and shuffles it in place using the `random.shuffle()` function. 3. **Dealing Cards**: Write a function `deal_cards(deck, num_players, cards_per_player)` that deals cards to a specified number of players. This function should: - Distribute `cards_per_player` cards to each player. - Return a list of lists, where each inner list represents the cards dealt to a player. - Ensure that no card is dealt to more than one player. 4. **Scoring System**: Implement a function `score_hand(hand)` that computes the score of a hand of cards. For simplicity, use the following scoring system: - Number cards (\'2\' to \'10\') are worth their face value in points. - Face cards (J, Q, K) are each worth 10 points. - Aces (A) are worth 11 points. 5. **Winner Determination**: Write a function `determine_winner(players_hands)` that takes the hands dealt to each player and determines the winner based on the highest score. Return the index of the winning player (0-based indexing). 6. **Simulation Execution**: Write a function `simulate_game(num_players, cards_per_player)` that: - Creates and shuffles the deck. - Deals the cards to the players. - Calculates the scores for each player\'s hand. - Determines and returns the winner. Constraints: - `num_players` should be between 2 and 10 (inclusive). - `cards_per_player` should be between 1 and 5 (inclusive). - Assume that the total number of cards needed will not exceed the number of cards in the deck. Example: ```python deck = create_deck() shuffle_deck(deck) players_hands = deal_cards(deck, 4, 5) scores = [score_hand(hand) for hand in players_hands] winner = determine_winner(players_hands) print(f\\"Player hands: {players_hands}\\") print(f\\"Scores: {scores}\\") print(f\\"Winner: Player {winner}\\") ``` Expected Output: The output will vary because of the random nature of the shuffle, but it should follow this pattern: ``` Player hands: [[(\'3\', \'hearts\'), (\'J\', \'diamonds\'), ...], ...] Scores: [28, 31, 25, 30] Winner: Player 1 ``` Note: Make sure to handle edge cases such as: - The deck not having enough cards to deal (should raise an appropriate error). - Cards being dealt uniquely, i.e., no card should appear in multiple hands. Write your Python code for this simulation below.","solution":"import random def create_deck(): Returns a standard 52-card deck. ranks = [\'2\', \'3\', \'4\', \'5\', \'6\', \'7\', \'8\', \'9\', \'10\', \'J\', \'Q\', \'K\', \'A\'] suits = [\'hearts\', \'diamonds\', \'clubs\', \'spades\'] deck = [(rank, suit) for rank in ranks for suit in suits] return deck def shuffle_deck(deck): Shuffles the deck of cards in place. random.shuffle(deck) def deal_cards(deck, num_players, cards_per_player): Deals cards to specified number of players. Returns a list of lists representing the hands of each player. if num_players * cards_per_player > len(deck): raise ValueError(\\"Not enough cards to deal.\\") players_hands = [] for _ in range(num_players): hand = [deck.pop() for _ in range(cards_per_player)] players_hands.append(hand) return players_hands def score_hand(hand): Computes the score of a hand of cards based on predefined scoring rules. score = 0 for card in hand: rank = card[0] if rank in [\'J\', \'Q\', \'K\']: score += 10 elif rank == \'A\': score += 11 else: score += int(rank) return score def determine_winner(players_hands): Determines the winner based on the highest score. Returns the index of the winning player (0-based indexing). scores = [score_hand(hand) for hand in players_hands] highest_score = max(scores) winner = scores.index(highest_score) return winner def simulate_game(num_players, cards_per_player): Simulates a game and returns the winner. if not (2 <= num_players <= 10): raise ValueError(\\"Number of players must be between 2 and 10.\\") if not (1 <= cards_per_player <= 5): raise ValueError(\\"Number of cards per player must be between 1 and 5.\\") deck = create_deck() shuffle_deck(deck) players_hands = deal_cards(deck, num_players, cards_per_player) winner = determine_winner(players_hands) return winner, players_hands"},{"question":"Objective You are tasked with building a text classification system capable of handling large-scale text data that doesn\'t fit into memory using the concepts of out-of-core learning, feature extraction, and incremental learning. The goal is to create a model that can incrementally learn from data and make predictions on new text data. Problem Statement Implement a class called `IncrementalTextClassifier` that will: 1. **Stream data** from a file. 2. **Extract features** using `HashingVectorizer`. 3. **Train an incremental classifier** using `SGDClassifier`. Your implementation should be able to read data in chunks, process each chunk to extract features, incrementally train the classifier, and finally, make predictions on new data. Class Definition ```python from sklearn.feature_extraction.text import HashingVectorizer from sklearn.linear_model import SGDClassifier import numpy as np class IncrementalTextClassifier: def __init__(self): self.vectorizer = HashingVectorizer(alternate_sign=False) self.classifier = SGDClassifier() def stream_data(self, file_path, chunk_size=1000): Generator that yields lines from a file. :param file_path: Path to the text file. :param chunk_size: Number of lines to read at a time. :yield: List of lines. # Implement this method to read lines from file in chunks of `chunk_size` pass def extract_features(self, data): Extracts features using HashingVectorizer. :param data: List of text data. :return: Transformed feature matrix. # Implement this method to transform `data` into a feature matrix pass def incremental_train(self, file_path, labels, chunk_size=1000, classes=None): Incrementally trains the SGDClassifier on chunks of data. :param file_path: Path to the text file. :param labels: List of labels corresponding to the data. :param chunk_size: Number of lines to read at a time. :param classes: List of possible class labels. # Implement this method to stream data in chunks, extract features, and train incrementally pass def predict(self, data): Predicts the class labels for the given data. :param data: List of text data. :return: Predictions for the data. # Implement this method to predict class labels for `data` pass ``` Input and Output Formats - `stream_data(file_path, chunk_size)`: Reads chunks of text data from the file. Yields a list of text data for each chunk. - **Input**: `file_path` (str), `chunk_size` (int) - **Output**: yield `data_chunk` (List[str]) - `extract_features(data)`: Transforms the list of text data into a feature matrix using `HashingVectorizer`. - **Input**: `data` (List[str]) - **Output**: `X` (array-like, shape [n_samples, n_features]) - `incremental_train(file_path, labels, chunk_size, classes)`: Reads data in chunks, extracts features, and trains the classifier incrementally. - **Input**: `file_path` (str), `labels` (List[int]), `chunk_size` (int), `classes` (List[int]) - **Output**: None - `predict(data)`: Predicts the class labels for the given text data. - **Input**: `data` (List[str]) - **Output**: `predictions` (array-like, shape [n_samples]) Constraints - You should handle files with very large data, so your implementation must not load the entire file into memory at once. - You can assume the labels are provided separately and correctly correspond to the chunks read in sequence. - The `incremental_train` method should handle unseen classes properly by initializing the classifier with all possible classes using the `classes` parameter. Performance Requirements - Your implementation should efficiently handle streaming and incremental training. - Aim for minimal memory footprint by processing data in chunks of size specified by `chunk_size`.","solution":"from sklearn.feature_extraction.text import HashingVectorizer from sklearn.linear_model import SGDClassifier import numpy as np class IncrementalTextClassifier: def __init__(self): self.vectorizer = HashingVectorizer(alternate_sign=False) self.classifier = SGDClassifier() def stream_data(self, file_path, chunk_size=1000): Generator that yields lines from a file. :param file_path: Path to the text file. :param chunk_size: Number of lines to read at a time. :yield: List of lines. with open(file_path, \'r\') as file: chunk = [] for line in file: chunk.append(line.strip()) if len(chunk) >= chunk_size: yield chunk chunk = [] if chunk: yield chunk def extract_features(self, data): Extracts features using HashingVectorizer. :param data: List of text data. :return: Transformed feature matrix. return self.vectorizer.transform(data) def incremental_train(self, file_path, labels, chunk_size=1000, classes=None): Incrementally trains the SGDClassifier on chunks of data. :param file_path: Path to the text file. :param labels: List of labels corresponding to the data. :param chunk_size: Number of lines to read at a time. :param classes: List of possible class labels. label_index = 0 for i, chunk in enumerate(self.stream_data(file_path, chunk_size)): X = self.extract_features(chunk) y = labels[label_index:label_index + len(chunk)] label_index += len(chunk) if i == 0 and classes is not None: self.classifier.partial_fit(X, y, classes=classes) else: self.classifier.partial_fit(X, y) def predict(self, data): Predicts the class labels for the given data. :param data: List of text data. :return: Predictions for the data. X = self.vectorizer.transform(data) return self.classifier.predict(X)"},{"question":"**Objective:** Write a function that demonstrates your understanding of Python sorting techniques, including the use of key functions and sort stability, to perform multi-level sorting on a list of objects. **Problem Statement:** You are given a list of `Book` objects. Each `Book` object has the following attributes: - `title` (string): The title of the book. - `author` (string): The author of the book. - `year` (int): The year the book was published. - `rating` (float): The rating of the book out of 5.0. Implement a function `sort_books()` that takes a list of `Book` objects and sorts them based on the following criteria: 1. Primary sort: By `rating` in descending order. 2. Secondary sort: By `year` in ascending order. 3. Tertiary sort: By `author` in ascending alphabetical order. 4. Final sort: By `title` in ascending alphabetical order. The function should return a new list of sorted books. Use the `sorted()` function along with appropriate key functions to achieve this. **Input Format:** - A list of `Book` objects where each object has the attributes `title`, `author`, `year`, and `rating`. **Output Format:** - A new list of `Book` objects sorted based on the specified criteria. **Constraints:** - All `title` and `author` strings contain only alphabetical characters and spaces. - All `year` values are valid integers from the 19th century onwards. - All `rating` values are floats ranging from 0.0 to 5.0. - Ensure the sorting is stable where required. ```python class Book: def __init__(self, title, author, year, rating): self.title = title self.author = author self.year = year self.rating = rating def __repr__(self): return f\\"Book(title={self.title}, author={self.author}, year={self.year}, rating={self.rating})\\" def sort_books(books): Sorts a list of Book objects based on the specified criteria. :param books: List of Book objects to be sorted. :return: A new list of sorted Book objects. # Your implementation here pass # Example usage books = [ Book(\\"A Tale of Two Cities\\", \\"Charles Dickens\\", 1859, 4.2), Book(\\"To Kill a Mockingbird\\", \\"Harper Lee\\", 1960, 4.3), Book(\\"1984\\", \\"George Orwell\\", 1949, 4.5), Book(\\"The Great Gatsby\\", \\"F. Scott Fitzgerald\\", 1925, 3.9) ] sorted_books = sort_books(books) for book in sorted_books: print(book) ``` **Notes:** - You are encouraged to use the `operator` module\'s functions for key extraction where appropriate. - Maintain the order of books with the same primary key criterion by using sort stability.","solution":"class Book: def __init__(self, title, author, year, rating): self.title = title self.author = author self.year = year self.rating = rating def __repr__(self): return f\\"Book(title={self.title}, author={self.author}, year={self.year}, rating={self.rating})\\" def sort_books(books): Sorts a list of Book objects based on the specified criteria. :param books: List of Book objects to be sorted. :return: A new list of sorted Book objects. return sorted(books, key=lambda book: (-book.rating, book.year, book.author, book.title))"},{"question":"**Question:** Implement a Python function `fetch_weather_data` that retrieves weather information from a given weather API. The function should handle various aspects covered in the `urllib` package, such as GET requests, POST requests, managing headers, and handling exceptions. Specifically, your function should: 1. Perform a GET request to retrieve current weather data. 2. Perform a POST request to upload some user-generated weather data. 3. Handle HTTP headers to specify the `User-Agent` and other relevant metadata. 4. Appropriately handle various potential errors including networking issues and HTTP errors with proper exception messages. 5. Handle basic authentication if needed. Below are the expected behaviors and formats: # Function Signature ```python def fetch_weather_data(api_url: str, endpoint: str, data: dict, headers: dict, auth: tuple = None) -> tuple: pass ``` # Parameters - **api_url**: The base URL of the weather API. - **endpoint**: The specific endpoint for requesting weather data. - **data**: A dictionary containing data to be sent in the POST request payload. - **headers**: A dictionary containing any headers that must be included in the request. - **auth**: A tuple containing username and password for basic authentication. Default is `None`. # Returns - A tuple containing: - The response data from the GET request (in the form of a dictionary). - The response status from the POST request (status code). # Constraints - Use the `urllib` package for all networking functionalities. - Ensure proper error handling for any issues (e.g., network connectivity problems, API not found, incorrect data). - Headers must include at least a `User-Agent`. # Example Usage ```python api_url = \\"https://api.weather.com\\" endpoint = \\"/v3/weather/conditions\\" headers = {\\"User-Agent\\": \\"WeatherApp/1.0\\"} data = {\\"location\\": \\"London\\"} response_get, post_status = fetch_weather_data(api_url, endpoint, data, headers) print(response_get) print(post_status) ``` # Implementation Details - The GET request should be made to `{api_url}{endpoint}` and fetch the current weather conditions. - The POST request should upload `data` to {api_url}{endpoint}/submit`. - Handle URLError and HTTPError with appropriate messages. - If `auth` is provided, use Basic Authentication. # Notes - You can assume that the weather API and the endpoints exist. - Focus on error handling robustness.","solution":"import urllib.request import urllib.parse import json def fetch_weather_data(api_url: str, endpoint: str, data: dict, headers: dict, auth: tuple = None) -> tuple: Fetches weather data from a given API and uploads user-generated data. Parameters: api_url (str): The base URL of the weather API. endpoint (str): The specific endpoint for requesting weather data. data (dict): A dictionary containing data to be sent in the POST request payload. headers (dict): A dictionary containing any headers that must be included in the request. auth (tuple): A tuple containing username and password for basic authentication. Default is None. Returns: tuple: Contains the response data from the GET request (dict) and the response status from the POST request (status code). get_response_data = None post_response_status = None # Handle authentication if auth: password_mgr = urllib.request.HTTPPasswordMgrWithDefaultRealm() password_mgr.add_password(None, api_url, auth[0], auth[1]) handler = urllib.request.HTTPBasicAuthHandler(password_mgr) opener = urllib.request.build_opener(handler) urllib.request.install_opener(opener) # Perform GET request try: req = urllib.request.Request(url=f\\"{api_url}{endpoint}\\", headers=headers) with urllib.request.urlopen(req) as response: get_response_data = json.loads(response.read().decode()) except urllib.error.HTTPError as e: print(f\\"HTTP error occurred: {e.code}\\") except urllib.error.URLError as e: print(f\\"URL error occurred: {e.reason}\\") except Exception as e: print(f\\"An error occurred: {str(e)}\\") # Perform POST request try: post_endpoint = f\\"{api_url}{endpoint}/submit\\" post_data = urllib.parse.urlencode(data).encode(\\"utf-8\\") post_req = urllib.request.Request( url=post_endpoint, data=post_data, headers=headers, method=\\"POST\\" ) with urllib.request.urlopen(post_req) as response: post_response_status = response.getcode() except urllib.error.HTTPError as e: print(f\\"HTTP error occurred: {e.code}\\") except urllib.error.URLError as e: print(f\\"URL error occurred: {e.reason}\\") except Exception as e: print(f\\"An error occurred: {str(e)}\\") return get_response_data, post_response_status"},{"question":"**Title:** Concurrent Execution with concurrent.futures **Objective:** Implement a Python function that utilizes the concurrent.futures module to perform concurrent execution of tasks. This problem will test your understanding of managing parallel tasks using ThreadPoolExecutor or ProcessPoolExecutor. **Problem Statement:** Write a function `fetch_data_concurrently(urls, workers)` that: 1. Takes a list of URLs (`urls`) and an integer (`workers`) representing the number of worker threads to use. 2. Fetches the content from each URL concurrently using the specified number of worker threads. 3. Returns a dictionary where the keys are the URLs and the values are the fetched content. To fetch the content, you can use the `requests` library, so ensure it is installed in your environment (`pip install requests`). **Function Signature:** ```python def fetch_data_concurrently(urls: List[str], workers: int) -> Dict[str, str]: pass ``` **Input:** - `urls`: A list of strings where each string is a valid URL. (1 ≤ len(urls) ≤ 100) - `workers`: An integer representing the number of worker threads to use. (1 ≤ workers ≤ 10) **Output:** - A dictionary where the keys are the URLs from the input list and the values are the content fetched from each URL as strings. **Example:** ```python urls = [ \\"http://example.com\\", \\"http://example.org\\", \\"http://example.net\\" ] workers = 3 result = fetch_data_concurrently(urls, workers) print(result) ``` The output will be: ```python { \\"http://example.com\\": \\"<html>...</html>\\", \\"http://example.org\\": \\"<html>...</html>\\", \\"http://example.net\\": \\"<html>...</html>\\" } ``` **Constraints:** - Use ThreadPoolExecutor from the concurrent.futures module. - Handle any exceptions that occur during fetching and ensure the function completes execution for all URLs. - You may assume all URLs are valid and reachable within the scope of this problem. **Hints:** - Consider using `executor.map` or `executor.submit` for concurrent task submission. - Handle cases where fetching content may fail by logging or ignoring the error pragmatically. **Performance Requirement:** The function should efficiently manage concurrent requests and ensure that all URLs are processed within a reasonable time frame based on the number of worker threads specified.","solution":"from concurrent.futures import ThreadPoolExecutor, as_completed from typing import List, Dict import requests def fetch_content(url): try: response = requests.get(url) response.raise_for_status() # Raise an error for bad status codes return response.text except requests.RequestException as e: return str(e) def fetch_data_concurrently(urls: List[str], workers: int) -> Dict[str, str]: results = {} with ThreadPoolExecutor(max_workers=workers) as executor: future_to_url = {executor.submit(fetch_content, url): url for url in urls} for future in as_completed(future_to_url): url = future_to_url[future] try: results[url] = future.result() except Exception as e: results[url] = str(e) return results"},{"question":"# Question: Tensor Shape Manipulation with PyTorch You are given a 3D tensor of dimensions `(N, C, H, W)`, where `N` is the batch size, `C` is the number of channels, `H` is the height, and `W` is the width. Your task is to write a function `reshape_tensor` that takes the tensor as input and returns a reshaped version of it based on new dimensions provided. Function Signature ```python def reshape_tensor(input_tensor: torch.Tensor, new_shape: tuple) -> torch.Tensor: pass ``` Input - `input_tensor`: a 4D PyTorch tensor of shape `(N, C, H, W)`. - `new_shape`: a tuple containing the new dimensions for the tensor. Output - A 4D PyTorch tensor reshaped according to `new_shape`. Constraints - The total number of elements in the `new_shape` tuple must match the total number of elements in the `input_tensor`. - If the new shape does not conform to the original number of elements, raise a `ValueError`. Example ```python import torch # Create a tensor of shape (2, 3, 4, 5) input_tensor = torch.ones(2, 3, 4, 5) # Define new shape new_shape = (6, 4, 5, 1) # Reshape tensor reshaped_tensor = reshape_tensor(input_tensor, new_shape) # The reshaped tensor should have the shape (6, 4, 5, 1) print(reshaped_tensor.size()) # Output: torch.Size([6, 4, 5, 1]) ``` Notes - Use the `.reshape()` method of the tensor to change its shape. - Ensure to include necessary checks for matching element counts and handle exceptions properly. Good luck!","solution":"import torch def reshape_tensor(input_tensor: torch.Tensor, new_shape: tuple) -> torch.Tensor: Reshapes the input tensor to the desired new shape. Args: - input_tensor (torch.Tensor): The 4D tensor to reshape of shape (N, C, H, W). - new_shape (tuple): A tuple containing the new dimensions. Returns: - torch.Tensor: The reshaped tensor. Raises: - ValueError: If the total number of elements in the new shape does not match the total number of elements in the input tensor. total_elements_input = input_tensor.numel() total_elements_new_shape = torch.tensor(new_shape).prod().item() if total_elements_input != total_elements_new_shape: raise ValueError(\\"The total number of elements in the new shape must match the total number of elements in the input tensor.\\") return input_tensor.reshape(new_shape)"},{"question":"Employee Data Analysis You are given a dataset containing information about employees in a company. Your task is to write a Python function using pandas to process and analyze this data. Input Format 1. A dictionary `employee_data` with the following keys: - \'Name\': List of employee names. - \'Age\': List of employee ages. - \'Department\': List of departments where they work. - \'Salary\': List of employee salaries. 2. A dictionary `department_data` with the following keys: - \'Department\': List of departments. - \'Manager\': List of department managers. Example: ```python employee_data = { \'Name\': [\'Alice\', \'Bob\', \'Charlie\', \'David\', \'Eva\'], \'Age\': [25, 30, 35, 40, 29], \'Department\': [\'HR\', \'Engineering\', \'HR\', \'Engineering\', \'Marketing\'], \'Salary\': [50000, 80000, 52000, 85000, 60000] } department_data = { \'Department\': [\'HR\', \'Engineering\', \'Marketing\', \'Sales\'], \'Manager\': [\'John\', \'Doe\', \'Smith\', \'Jane\'] } ``` Output Format A pandas DataFrame that contains the following columns: - \'Name\': Employee name. - \'Age\': Employee age. - \'Department\': Employee department. - \'Salary\': Employee salary. - \'Manager\': Department manager. Task Implement the function `analyze_employee_data(employee_data: dict, department_data: dict) -> pd.DataFrame` to generate the required DataFrame. Constraints - All lists in the dictionaries are of the same length. - The department names are unique within each dictionary but can have different lengths. Function Signature ```python import pandas as pd def analyze_employee_data(employee_data: dict, department_data: dict) -> pd.DataFrame: # Your code here ``` Example ```python employee_data = { \'Name\': [\'Alice\', \'Bob\', \'Charlie\', \'David\', \'Eva\'], \'Age\': [25, 30, 35, 40, 29], \'Department\': [\'HR\', \'Engineering\', \'HR\', \'Engineering\', \'Marketing\'], \'Salary\': [50000, 80000, 52000, 85000, 60000] } department_data = { \'Department\': [\'HR\', \'Engineering\', \'Marketing\', \'Sales\'], \'Manager\': [\'John\', \'Doe\', \'Smith\', \'Jane\'] } result_df = analyze_employee_data(employee_data, department_data) print(result_df) ``` Expected output (DataFrame): ``` Name Age Department Salary Manager 0 Alice 25 HR 50000 John 1 Bob 30 Engineering 80000 Doe 2 Charlie 35 HR 52000 John 3 David 40 Engineering 85000 Doe 4 Eva 29 Marketing 60000 Smith ```","solution":"import pandas as pd def analyze_employee_data(employee_data: dict, department_data: dict) -> pd.DataFrame: # Create DataFrame from the input dictionaries employee_df = pd.DataFrame(employee_data) department_df = pd.DataFrame(department_data) # Merge the dataframes on the \'Department\' column merged_df = pd.merge(employee_df, department_df, on=\'Department\', how=\'left\') return merged_df"},{"question":"Objective Implement a custom Python list-like class that internally uses the functions provided in the `PyListObject` C API. Your class should mimic the behavior of Python\'s built-in list, but you should use the C API functions described in the provided documentation to handle the list operations. Class Definition Implement a class called `CustomList` with the following methods: 1. **`__init__(self, initial_data: list = None):`** Initializes a new `CustomList`. If `initial_data` is provided, the list should be initialized with its elements. Otherwise, the list should be empty. 2. **`append(self, item):`** Appends an `item` to the end of the list. 3. **`insert(self, index: int, item):`** Inserts an `item` at the specified `index` in the list. 4. **`get_item(self, index: int) -> object:`** Returns the item at the specified `index`. If the index is out of bounds, raise an `IndexError`. 5. **`set_item(self, index: int, item):`** Sets the item at the specified `index` with the given `item`. If the index is out of bounds, raise an `IndexError`. 6. **`get_slice(self, start: int, end: int) -> list:`** Returns a new list containing the items from `start` to `end`. 7. **`set_slice(self, start: int, end: int, new_list: list):`** Replaces the elements in the slice from `start` to `end` with those from `new_list`. 8. **`reverse(self):`** Reverses the list in place. 9. **`sort(self):`** Sorts the list in place. 10. **`to_tuple(self) -> tuple:`** Returns a tuple containing the elements of the list. Constraints - You must use the functions from the provided `PyListObject` documentation to manipulate the list. - Manage memory properly to avoid memory leaks. - Assume that all operations will be valid if they conform to the constraints mentioned in the function definitions. Expected Usage and Input/Output ```python # Create an empty custom list cl = CustomList() # Append items to the list cl.append(1) cl.append(2) cl.append(3) # Insert an item at a specific index cl.insert(1, 4) # List: [1, 4, 2, 3] # Get an item item = cl.get_item(2) # item should be 2 # Set an item cl.set_item(2, 10) # List: [1, 4, 10, 3] # Get a slice sublist = cl.get_slice(1, 3) # sublist should be [4, 10] # Set a slice cl.set_slice(1, 3, [5, 6]) # List: [1, 5, 6, 3] # Reverse the list cl.reverse() # List: [3, 6, 5, 1] # Sort the list cl.sort() # List: [1, 3, 5, 6] # Convert to tuple tuple_list = cl.to_tuple() # tuple_list should be (1, 3, 5, 6) ``` Implement the `CustomList` class according to the specifications above.","solution":"class CustomList: def __init__(self, initial_data=None): self.data = initial_data if initial_data is not None else [] def append(self, item): self.data.append(item) def insert(self, index, item): self.data.insert(index, item) def get_item(self, index): if index < 0 or index >= len(self.data): raise IndexError(\'list index out of range\') return self.data[index] def set_item(self, index, item): if index < 0 or index >= len(self.data): raise IndexError(\'list index out of range\') self.data[index] = item def get_slice(self, start, end): return self.data[start:end] def set_slice(self, start, end, new_list): self.data[start:end] = new_list def reverse(self): self.data.reverse() def sort(self): self.data.sort() def to_tuple(self): return tuple(self.data)"},{"question":"Python\'s `imp` module provided mechanisms to interact with the import statement, but it has been deprecated since Python 3.4 in favor of the `importlib` module. Your task is to write a function that emulates the behavior of `imp.find_module()` and `imp.load_module()` using `importlib`. **Objective:** Implement a function `custom_import(module_name: str) -> Any` that: 1. Finds and loads the specified module using `importlib`. 2. Returns the loaded module object. # Detailed Requirements 1. **Function Signature:** ```python def custom_import(module_name: str) -> Any: ``` 2. **Input:** - `module_name` (str): The name of the module to import. 3. **Output:** - Returns the imported module object. 4. **Constraints:** - You cannot use the `imp` module. - Use `importlib` functions such as `import_module`, `find_spec`, and others as required. - Ensure that any file objects opened during the import process are properly closed. 5. **Behavior:** - The function should raise an `ImportError` if the module cannot be found. - Handle both regular modules and package modules. 6. **Performance Requirements:** - The function should perform the import without polling the filesystem directly. # Examples ```python # Example 1 module = custom_import(\'math\') print(module.sqrt(16)) # Should output: 4.0 # Example 2 module = custom_import(\'os\') print(module.name) # Should output the name of the operating system depending on the environment # Example 3 try: module = custom_import(\'non_existent_module\') except ImportError: print(\\"Module not found\\") # Should output: Module not found ``` Good luck! Your implementation should demonstrate a solid understanding of the Python import system and best practices for module management.","solution":"import importlib import importlib.util def custom_import(module_name: str): Finds and loads the specified module using importlib. Args: - module_name (str): The name of the module to import. Returns: - The imported module object. Raises: - ImportError: If the module cannot be found. module_spec = importlib.util.find_spec(module_name) if module_spec is None: raise ImportError(f\\"No module named \'{module_name}\'\\") module = importlib.util.module_from_spec(module_spec) module_spec.loader.exec_module(module) return module"},{"question":"# Question: Implement a Custom Linear Layer using PyTorch Tensors You are tasked with implementing a custom linear (fully connected) layer from scratch using PyTorch tensors. This will help you understand how tensors and basic tensor operations work in PyTorch, which is crucial for creating and training neural networks. Requirements 1. **Class Definition**: - Define a class `CustomLinearLayer` that initializes with the input size (`in_features`) and output size (`out_features`). - Optionally, include a bias term. 2. **Initialization**: - The layer should initialize its weights and biases (if applicable) as `torch.Tensor` with dimensions [output_size, input_size] for weights and [output_size] for biases. - Use a uniform distribution to initialize the weights and biases. 3. **Forward Pass**: - Implement a `forward` method that takes a `torch.Tensor` as input and returns the result of the linear transformation. 4. **Gradient Calculation**: - Implement a `backward` method if you want to challenge yourself further. This method should calculate the gradient with respect to the weights and biases given the gradient with respect to the output. Input and Output - **Input**: 1. Input size: Integer (`in_features`) 2. Output size: Integer (`out_features`) 3. A tensor `x` of shape `[batch_size, in_features]` - **Output**: - A tensor of shape `[batch_size, out_features]` which is the result of the linear transformation. Constraints - Do not use existing modules like `torch.nn.Linear`. Everything should be built using basic PyTorch tensor operations. Example ```python import torch class CustomLinearLayer: def __init__(self, in_features, out_features, bias=True): self.in_features = in_features self.out_features = out_features self.weights = torch.Tensor(out_features, in_features).uniform_(-0.1, 0.1) self.bias = torch.Tensor(out_features).uniform_(-0.1, 0.1) if bias else None def forward(self, x): output = torch.matmul(x, self.weights.T) if self.bias is not None: output += self.bias return output # Example Usage layer = CustomLinearLayer(in_features=4, out_features=3) input_tensor = torch.Tensor([[1.0, 2.0, 3.0, 4.0]]) output_tensor = layer.forward(input_tensor) print(output_tensor) ``` In this problem, you need to implement the `CustomLinearLayer` class to complete the functionality described above. This will help you understand the basic mechanics of neural network layers, tensor initialization, and manipulation using PyTorch.","solution":"import torch class CustomLinearLayer: def __init__(self, in_features, out_features, bias=True): self.in_features = in_features self.out_features = out_features self.weights = torch.Tensor(out_features, in_features).uniform_(-0.1, 0.1) self.bias = torch.Tensor(out_features).uniform_(-0.1, 0.1) if bias else None def forward(self, x): output = torch.matmul(x, self.weights.T) if self.bias is not None: output += self.bias return output"},{"question":"Objective To assess your understanding of loading real-world datasets using scikit-learn\'s `datasets` module and implementing a basic machine learning pipeline. Question Implement a function `load_and_train(dataset_name: str) -> float` that performs the following tasks: 1. Loads a specified real-world dataset from `sklearn.datasets`. 2. Splits the dataset into training and testing sets. 3. Trains a simple machine learning model on the training set. 4. Evaluates the model on the testing set and returns the accuracy score. Function Signature ```python def load_and_train(dataset_name: str) -> float: pass ``` Input - `dataset_name`: A string specifying the name of the dataset to be loaded. Valid options are `\'iris\'`, `\'wine\'`, and `\'breast_cancer\'`. Output - Returns the accuracy score of the trained model on the testing set as a float. Constraints - You must use a `RandomForestClassifier` from `sklearn.ensemble`. - Split the dataset into 80% training and 20% testing. - Use `random_state=42` for reproducibility. Example ```python accuracy = load_and_train(\'iris\') print(accuracy) # Output will be the accuracy score (float) of the model on the testing set. ``` Notes - You can use `train_test_split` from `sklearn.model_selection`. - Ensure that your function can handle invalid dataset names by raising a `ValueError` with the message \\"Invalid dataset name\\". Hints - Use `sklearn.datasets.load_iris`, `sklearn.datasets.load_wine`, and `sklearn.datasets.load_breast_cancer` to load the respective datasets. - The `RandomForestClassifier` can be used without hyperparameter tuning for this simple implementation.","solution":"from sklearn.datasets import load_iris, load_wine, load_breast_cancer from sklearn.model_selection import train_test_split from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import accuracy_score def load_and_train(dataset_name: str) -> float: Loads a specified real-world dataset, trains a RandomForestClassifier, and evaluates it. :param dataset_name: str: Name of the dataset to load (\'iris\', \'wine\', \'breast_cancer\') :return: float: Accuracy score of the model on the test set. if dataset_name == \'iris\': data = load_iris() elif dataset_name == \'wine\': data = load_wine() elif dataset_name == \'breast_cancer\': data = load_breast_cancer() else: raise ValueError(\\"Invalid dataset name\\") X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.2, random_state=42) model = RandomForestClassifier(random_state=42) model.fit(X_train, y_train) y_pred = model.predict(X_test) return accuracy_score(y_test, y_pred)"},{"question":"# Python Descriptor with Custom Validation Objective Your task is to implement a Python class using descriptors to manage and validate the attributes of another class. You will create a descriptor class that performs custom validation on an attribute. This will help to assess your understanding of Python descriptors, encapsulation, and data management. Requirements 1. Implement a descriptor class `PositiveInteger` that will manage an attribute ensuring: - Only positive integers are allowed. - Any attempt to set a non-positive integer or non-integer value should raise an appropriate error (`ValueError` for non-positive, `TypeError` for non-integer). 2. Use this descriptor in a class `Product` to manage a `quantity` attribute. 3. Provide the following functionalities: - The `quantity` attribute should only accept positive integers. - Implement an `__init__` method to initialize the `quantity` attribute. - Ensure that attempts to set invalid values for `quantity` raise an appropriate error. Descriptor Class Specification - Define a `PositiveInteger` class with the following methods: ```python class PositiveInteger: def __init__(self): self.private_name = None def __set_name__(self, owner, name): self.private_name = f\'_{name}\' def __get__(self, obj, objtype=None): return getattr(obj, self.private_name) def __set__(self, obj, value): if not isinstance(value, int): raise TypeError(f\\"Expected int, got {type(value).__name__}\\") if value <= 0: raise ValueError(\\"Expected value to be a positive integer\\") setattr(obj, self.private_name, value) ``` Product Class Specification - Define a `Product` class with the following structure: ```python class Product: quantity = PositiveInteger() def __init__(self, quantity): self.quantity = quantity ``` Constraints - Do not modify the provided descriptor class `PositiveInteger` structure. - Use the descriptor strictly for the `quantity` attribute. Example ```python try: p = Product(10) print(p.quantity) # Expected output: 10 p.quantity = 5 print(p.quantity) # Expected output: 5 p.quantity = -3 # Expected to raise ValueError except Exception as e: print(e) try: p = Product(\'five\') # Expected to raise TypeError except Exception as e: print(e) ``` Performance - The implementation should ensure minimal performance overhead while managing attribute access and validation.","solution":"class PositiveInteger: def __init__(self): self.private_name = None def __set_name__(self, owner, name): self.private_name = f\'_{name}\' def __get__(self, obj, objtype=None): return getattr(obj, self.private_name) def __set__(self, obj, value): if not isinstance(value, int): raise TypeError(f\\"Expected int, got {type(value).__name__}\\") if value <= 0: raise ValueError(\\"Expected value to be a positive integer\\") setattr(obj, self.private_name, value) class Product: quantity = PositiveInteger() def __init__(self, quantity): self.quantity = quantity"},{"question":"**Question: Data Visualization with Seaborn Objects** You are provided with a dataset containing information about penguins. Using the seaborn.objects interface, your task is to create a visually informative plot that includes a scatter plot with a regression line, faceted by the `sex` attribute. Additionally, the sizes of the points should represent the `body_mass_g` attribute, and the colors should represent different `species`. # Data The dataset `penguins` is preloaded and contains the following columns: - `species`: The species of the penguin. - `bill_length_mm`: The bill length of the penguin in millimeters. - `bill_depth_mm`: The bill depth of the penguin in millimeters. - `flipper_length_mm`: The flipper length of the penguin in millimeters. - `body_mass_g`: The body mass of the penguin in grams. - `sex`: The sex of the penguin. # Requirements 1. **Scatter Plot**: Create a scatter plot with the x-axis representing `bill_length_mm` and the y-axis representing `bill_depth_mm`. The points should be colored by `species`. 2. **Point Size**: The size of each point should be proportional to the `body_mass_g` attribute. 3. **Regression Line**: Add a regression line to the scatter plot. 4. **Faceting**: Facet the plot by the `sex` attribute. 5. **Plot Appearance**: Customize the plot to have appropriate labels, titles, and a clear visual distinction in colors. # Function Signature ```python def create_penguin_plot(penguins): # Your code here ``` # Example Output The function should display a faceted scatter plot with the specifications mentioned above when called. **Note**: You may assume that all necessary libraries (seaborn, pandas, matplotlib) are already imported, and the dataset `penguins` is loaded as a pandas DataFrame.","solution":"import seaborn.objects as so import seaborn as sns import matplotlib.pyplot as plt def create_penguin_plot(penguins): Creates a faceted scatter plot of penguins data with the following specifications: - x-axis: bill_length_mm - y-axis: bill_depth_mm - Point color: species - Point size: body_mass_g - Regression line: yes - Faceted by: sex # Create the point plot with regression line and faceting plot = ( so.Plot( penguins, x=\'bill_length_mm\', y=\'bill_depth_mm\', color=\'species\', pointsize=\'body_mass_g\' ) .facet(\\"sex\\") .add(so.Dot()) .add(so.Line(), so.PolyFit()) .label(title=\'Penguin Characteristics by Sex\', x=\'Bill Length (mm)\', y=\'Bill Depth (mm)\') ) plot.show()"},{"question":"# Objective: Assess your ability to utilize seaborn\'s advanced plotting interface in conjunction with pandas for data transformation and visualization. Problem Statement: You are provided with a dataset representing brain network activity measurements over various time points, networs, and hemispheres. Your task is to load, transform, and visualize this dataset using seaborn\'s object-oriented interface. Steps to Follow: 1. **Data Loading and Transformation:** - Load the provided dataset `brain_networks` with the header spanning three rows, and use the first column as the index. - Rename the index to `timepoint`. - Perform a series of transformations: stack the data, group by `timepoint`, `network`, and `hemi`, compute the mean, unstack by `network`, reset the index, and filter the data to include only `timepoint` values less than 100. 2. **Visualization:** - Create a plot using seaborn\'s `so.Plot` interface to visualize how different network activities evolve across time for each hemisphere. - Specifically, use column categories `5`, `8`, `12`, and `15` for the x-axis and `6`, `13`, and `16` for the y-axis. - Add paths to the plot to show the trajectories of these network activities. - Customize the paths with a linewidth of 1 and an alpha of 0.8, and color them based on the hemisphere (`hemi`). Expected Input: - None. The dataset is to be loaded from seaborn\'s datasets directly. Expected Output: - A seaborn plot complying with the problem statement specifications. Coding Requirements: Below is a template to help you get started. Fill in the missing parts. ```python import seaborn.objects as so from seaborn import load_dataset def brain_network_activity_plot(): # Load and transform the dataset networks = ( load_dataset(\\"brain_networks\\", header=[0, 1, 2], index_col=0) .rename_axis(\\"timepoint\\") .stack([0, 1, 2]) .groupby([\\"timepoint\\", \\"network\\", \\"hemi\\"]) .mean() .unstack(\\"network\\") .reset_index() .query(\\"timepoint < 100\\") ) # Create the plot p = ( so.Plot(networks) .pair( x=[\\"5\\", \\"8\\", \\"12\\", \\"15\\"], y=[\\"6\\", \\"13\\", \\"16\\"], ) .layout(size=(8, 5)) .share(x=True, y=True) ) # Add paths to the plot p.add(so.Paths(linewidth=1, alpha=.8), color=\\"hemi\\") # Return the plot object (or you can use p.show() to display) return p # Example usage (uncomment the line below to display): # brain_network_activity_plot().show() ``` Ensure your code is efficient and correctly implements all requirements. Constraints: - Only use seaborn and pandas for visualization and data manipulation. - Ensure the plot is visually clear and accurately represents the data transformations outlined. Performance Requirements: - Aim for concise and readable code. - Handle the dataset transformations efficiently.","solution":"import seaborn.objects as so from seaborn import load_dataset import pandas as pd def brain_network_activity_plot(): # Load and transform the dataset networks = ( load_dataset(\\"brain_networks\\", header=[0, 1, 2], index_col=0) .rename_axis(\\"timepoint\\") .stack([0, 1, 2]) .groupby([\\"timepoint\\", \\"network\\", \\"hemi\\"]).mean() .unstack(\\"network\\") .reset_index() .query(\\"timepoint < 100\\") ) # Create the plot p = ( so.Plot(networks) .pair(x=[\\"5\\", \\"8\\", \\"12\\", \\"15\\"], y=[\\"6\\", \\"13\\", \\"16\\"]) .layout(size=(8, 5)) .share(x=True, y=True) ) # Add paths to the plot p.add(so.Paths(linewidth=1, alpha=.8), color=\\"hemi\\") # Return the plot object return p # Example usage (uncomment the line below to display the plot): # brain_network_activity_plot().show()"},{"question":"Objective: In this exercise, you will demonstrate your understanding of partial dependence plots (PDP) and individual conditional expectation (ICE) plots using the Scikit-learn library. Specifically, you will create and interpret these plots for a given dataset. Problem Statement: You are provided with the well-known Diabetes dataset from Scikit-learn, which consists of 442 samples and 10 feature variables. Your task is to: 1. Load the dataset and split it into training and testing sets. 2. Train a `GradientBoostingRegressor` model on the training data. 3. Generate and plot one-way and two-way partial dependence plots for selected features. 4. Generate and plot individual conditional expectation (ICE) plots. 5. Provide a brief interpretation of the results obtained from the plots. Instructions: 1. **Data Loading and Preprocessing**: - Load the diabetes dataset using `sklearn.datasets.load_diabetes`. - Split the data into training and testing sets with a test size of 20% using `sklearn.model_selection.train_test_split`. 2. **Model Training**: - Train a `GradientBoostingRegressor` model using the training data. - Use 100 estimators, a learning rate of 0.1, and a maximum depth of 3 for the model. 3. **Partial Dependence Plot (PDP)**: - Generate one-way PDPs for at least two features of your choice. - Generate a two-way PDP for a pair of features to understand their interactions. - Utilize `PartialDependenceDisplay.from_estimator` to create the plots. 4. **Individual Conditional Expectation (ICE) Plot**: - Generate ICE plots for the same features used in the one-way PDPs. - Utilize `PartialDependenceDisplay.from_estimator` with `kind=\'individual\'` to create the plots. 5. **Interpretation**: - Provide brief comments on the insights obtained from the PDP and ICE plots about the relationship between the features and the target variable. Expected Input and Output Format: - **Input**: The data loading and model parameters are predefined. No additional input is required. - **Output**: - A plot showing one-way and two-way PDPs. - A plot showing ICE plots. - Textual explanation interpreting the plots. Constraints: - Use only the `GradientBoostingRegressor` model for training. - Focus on the features `BMI` (feature index 2) and `s5` (feature index 8) for generating the plots. Performance Requirements: - Ensure that your code runs efficiently within a reasonable time frame, considering the computational complexity of generating PDP and ICE plots. You should write your code within a single function `pdp_ice_analysis()` that performs all the steps mentioned above, and then call this function to display the plots and interpretations. ```python import numpy as np import matplotlib.pyplot as plt from sklearn.datasets import load_diabetes from sklearn.model_selection import train_test_split from sklearn.ensemble import GradientBoostingRegressor from sklearn.inspection import PartialDependenceDisplay def pdp_ice_analysis(): # Load the diabetes dataset diabetes = load_diabetes() X, y = diabetes.data, diabetes.target # Split the data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Train the GradientBoostingRegressor model model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42) model.fit(X_train, y_train) # Generate one-way PDPs for features \'BMI\' (index 2) and \'s5\' (index 8) features = [2, 8, (2, 8)] # Create Partial Dependence Display (PDP) fig, ax = plt.subplots(figsize=(15, 10)) PartialDependenceDisplay.from_estimator(model, X_train, features, ax=ax) plt.show() # Generate Individual Conditional Expectation (ICE) plots for the same features fig, ax = plt.subplots(figsize=(15, 10)) PartialDependenceDisplay.from_estimator(model, X_train, [2, 8], kind=\'individual\', ax=ax) plt.show() # Interpretation (write your textual interpretation here) interpretation = Interpretation: - One-way PDPs show the average effect of \'BMI\' and \'s5\' on the target variable. The influence of \'BMI\' seems to be positively correlated with the target variable, indicating that higher BMI values are associated with higher diabetes progression. - The two-way PDP indicates potential interaction effects between \'BMI\' and \'s5\'. - ICE plots provide additional information by showing individual sample dependencies. The variability in individual lines indicates heterogeneity in the relationships between \'BMI\', \'s5\', and the target variable. print(interpretation) pdp_ice_analysis() ```","solution":"import numpy as np import matplotlib.pyplot as plt from sklearn.datasets import load_diabetes from sklearn.model_selection import train_test_split from sklearn.ensemble import GradientBoostingRegressor from sklearn.inspection import PartialDependenceDisplay def pdp_ice_analysis(): # Load the diabetes dataset diabetes = load_diabetes() X, y = diabetes.data, diabetes.target # Split the data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Train the GradientBoostingRegressor model model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42) model.fit(X_train, y_train) # Generate one-way PDPs for features \'BMI\' (index 2) and \'s5\' (index 8) features = [2, 8, (2, 8)] # Create Partial Dependence Display (PDP) fig, ax = plt.subplots(figsize=(15, 10)) PartialDependenceDisplay.from_estimator(model, X_train, features, ax=ax) plt.show() # Generate Individual Conditional Expectation (ICE) plots for the same features fig, ax = plt.subplots(figsize=(15, 10)) PartialDependenceDisplay.from_estimator(model, X_train, [2, 8], kind=\'individual\', ax=ax) plt.show() # Interpretation (write your textual interpretation here) interpretation = Interpretation: - One-way PDPs show the average effect of \'BMI\' and \'s5\' on the target variable. The influence of \'BMI\' seems to be positively correlated with the target variable, indicating that higher BMI values are associated with higher diabetes progression. - The two-way PDP indicates potential interaction effects between \'BMI\' and \'s5\'. - ICE plots provide additional information by showing individual sample dependencies. The variability in individual lines indicates heterogeneity in the relationships between \'BMI\', \'s5\', and the target variable. print(interpretation) pdp_ice_analysis()"},{"question":"**Question**: Create a **Resource Management Utility** script in Python that performs several tasks involving file handling, directory manipulation, and environment variable management using the `os` module. Your task is to implement the following functions: 1. **List Directory Contents**: Write a function `list_directory_contents(directory)` that lists all files and directories in the specified directory. The function should handle potential errors gracefully if the directory does not exist. 2. **Create Environment Variable**: Write a function `set_environment_variable(key, value)` that sets an environment variable. The function should confirm that the variable has been set by returning the value of the newly set environment variable. 3. **Process Info**: Write a function `get_process_info()` that returns the current process id (PID), the parent process id (PPID), and the current working directory. 4. **Move Files**: Write a function `move_file(src, dest)` that moves a file from the source path to the destination path. The function should check if the source file exists and if the destination directory is valid before attempting to move the file. 5. **Delete Environment Variable**: Write a function `delete_environment_variable(key)` that removes a specified environment variable. The function should handle cases where the environment variable does not exist. 6. **Current System Load**: Write a function `get_system_load()` that returns the system load average over the last 1, 5, and 15 minutes. This function should handle systems where this information is not available. # Function Definitions ```python import os def list_directory_contents(directory): List all files and directories in the specified directory. :param directory: str - directory path :return: List[str] - List of names in the directory try: contents = os.listdir(directory) return contents except FileNotFoundError: return f\\"Error: The directory \'{directory}\' does not exist.\\" except NotADirectoryError: return f\\"Error: \'{directory}\' is not a directory.\\" def set_environment_variable(key, value): Set an environment variable. :param key: str - environment variable key :param value: str - environment variable value :return: str - confirmation of setting the variable os.environ[key] = value return os.environ[key] def get_process_info(): Get current process id, parent process id, and current working directory. :return: dict - dictionary containing PID, PPID, and CWD pid = os.getpid() ppid = os.getppid() cwd = os.getcwd() return {\\"PID\\": pid, \\"PPID\\": ppid, \\"CWD\\": cwd} def move_file(src, dest): Move a file from source path to destination path. :param src: str - source file path :param dest: str - destination file path :return: str - success or error message try: if not os.path.isfile(src): return f\\"Error: The source file \'{src}\' does not exist.\\" os.rename(src, dest) return f\\"File moved from \'{src}\' to \'{dest}\' successfully.\\" except FileNotFoundError: return f\\"Error: Destination directory does not exist.\\" except PermissionError: return f\\"Error: Permission denied to move the file.\\" except Exception as e: return f\\"Error: {e}\\" def delete_environment_variable(key): Delete a specified environment variable. :param key: str - environment variable key :return: str - success or error message try: del os.environ[key] return f\\"Environment variable \'{key}\' deleted successfully.\\" except KeyError: return f\\"Error: Environment variable \'{key}\' does not exist.\\" def get_system_load(): Get system load average over the last 1, 5, and 15 minutes. :return: Tuple[float, float, float] - load averages try: return os.getloadavg() except AttributeError: return \\"Error: System load average information is not available on this platform.\\" ``` # Usage Example ```python print(list_directory_contents(\\".\\")) # List contents of the current directory print(set_environment_variable(\\"MY_VAR\\", \\"1234\\")) # Set an environment variable print(get_process_info()) # Get information about the current process print(move_file(\\"test.txt\\", \\"/new_location/test.txt\\")) # Move a file print(delete_environment_variable(\\"MY_VAR\\")) # Delete an environment variable print(get_system_load()) # Get system load averages ``` # Constraints - Ensure all functions handle errors gracefully. - Ensure compatibility on both Unix and Windows platforms where applicable. - `move_file()` should validate if the source file exists before attempting to move. - Assume reasonable default alternatives if certain information (e.g., system load average) is not available on the system. Implement the above functions in a script, and provide a usage example demonstrating these functionalities.","solution":"import os def list_directory_contents(directory): List all files and directories in the specified directory. :param directory: str - directory path :return: List[str] - List of names in the directory try: contents = os.listdir(directory) return contents except FileNotFoundError: return f\\"Error: The directory \'{directory}\' does not exist.\\" except NotADirectoryError: return f\\"Error: \'{directory}\' is not a directory.\\" def set_environment_variable(key, value): Set an environment variable. :param key: str - environment variable key :param value: str - environment variable value :return: str - confirmation of setting the variable os.environ[key] = value return os.environ[key] def get_process_info(): Get current process id, parent process id, and current working directory. :return: dict - dictionary containing PID, PPID, and CWD pid = os.getpid() ppid = os.getppid() cwd = os.getcwd() return {\\"PID\\": pid, \\"PPID\\": ppid, \\"CWD\\": cwd} def move_file(src, dest): Move a file from source path to destination path. :param src: str - source file path :param dest: str - destination file path :return: str - success or error message try: if not os.path.isfile(src): return f\\"Error: The source file \'{src}\' does not exist.\\" os.rename(src, dest) return f\\"File moved from \'{src}\' to \'{dest}\' successfully.\\" except FileNotFoundError: return f\\"Error: Destination directory does not exist.\\" except PermissionError: return f\\"Error: Permission denied to move the file.\\" except Exception as e: return f\\"Error: {e}\\" def delete_environment_variable(key): Delete a specified environment variable. :param key: str - environment variable key :return: str - success or error message try: del os.environ[key] return f\\"Environment variable \'{key}\' deleted successfully.\\" except KeyError: return f\\"Error: Environment variable \'{key}\' does not exist.\\" def get_system_load(): Get system load average over the last 1, 5, and 15 minutes. :return: Tuple[float, float, float] - load averages try: return os.getloadavg() except AttributeError: return \\"Error: System load average information is not available on this platform.\\""},{"question":"# Python Coding Assessment Question Temporary Storage Management using the `tempfile` Module **Objective:** Implement a Python function to create temporary files and directories using the `tempfile` module and perform operations efficiently while ensuring proper cleanup of these temporary resources. **Problem Statement:** Write a function `temp_file_operations` that performs the following steps: 1. Creates a temporary directory. 2. Creates a specified number of temporary files within that directory. 3. Writes unique data to each temporary file. 4. Reads data from each temporary file and stores it in a dictionary where the key is the file name and the value is the data read from the file. 5. Ensures that all temporary files and the directory are properly cleaned up after the function completes, even if an error occurs. **Function Signature:** ```python def temp_file_operations(file_count: int) -> dict: pass ``` **Parameters:** - `file_count` (int): The number of temporary files to create. **Returns:** - `dict`: A dictionary containing the temporary file names (key) and their respective data (value). **Constraints:** - Each temporary file should be written with the text \\"Data for file {i}\\" where `{i}` is the file number (starting from 1 to `file_count`). - Ensure that the temporary files and directory are cleaned up, regardless of any exceptions that might occur. **Example Usage:** ```python result = temp_file_operations(3) print(result) ``` **Expected Output:** The expected output is a dictionary containing three entries with keys corresponding to temporary file names and values as the written data. ```python { \'/path/to/temporary/directory/tmpfile1\': \'Data for file 1\', \'/path/to/temporary/directory/tmpfile2\': \'Data for file 2\', \'/path/to/temporary/directory/tmpfile3\': \'Data for file 3\' } ``` Note: The actual paths will vary depending on the system and run-time. **Implementation Notes:** - Use the `tempfile.TemporaryDirectory` context manager to create and manage the temporary directory. - Within this temporary directory, use `tempfile.NamedTemporaryFile` to create temporary files allowing their names to be accessed. - Handle file write and read operations properly, ensuring all resources are cleaned up even if errors occur.","solution":"import tempfile import os def temp_file_operations(file_count: int) -> dict: result = {} try: with tempfile.TemporaryDirectory() as temp_dir: for i in range(1, file_count + 1): temp_file = tempfile.NamedTemporaryFile(delete=False, dir=temp_dir) file_name = temp_file.name data = f\\"Data for file {i}\\" with open(file_name, \'w\') as f: f.write(data) with open(file_name, \'r\') as f: read_data = f.read() result[file_name] = read_data finally: # Ensure cleanup of created temporary files. for file_path in result.keys(): if os.path.exists(file_path): os.remove(file_path) return result"},{"question":"**XML Manipulation and Querying with `xml.etree.ElementTree`** You are tasked with writing a Python function that processes an XML document containing information about a collection of books. Each book has a title, author, genre, price, and publish date. Here is a sample XML document structure: ```xml <catalog> <book id=\\"bk101\\"> <author>Gambardella, Matthew</author> <title>XML Developer\'s Guide</title> <genre>Computer</genre> <price>44.95</price> <publish_date>2000-10-01</publish_date> </book> <book id=\\"bk102\\"> <author>Ralls, Kim</author> <title>Midnight Rain</title> <genre>Fantasy</genre> <price>5.95</price> <publish_date>2000-12-16</publish_date> </book> <!-- more book entries --> </catalog> ``` # Requirements: 1. **Function Name**: `process_books` 2. **Input**: A string containing XML data structured as shown above. 3. **Output**: A dictionary with the following structure: ```python { \\"total_books\\": <total number of books>, \\"expensive_books\\": [<list of titles of books with price > 20>], \\"authors\\": [<list of authors>] } ``` 4. **Constraints**: - The XML string will not contain more than 1000 book entries. - The price will always be a numeric value. 5. **Implementation details**: - You must use the `xml.etree.ElementTree` module for all XML parsing and handling. - You must query elements using XPath where appropriate. - Ensure that your function performs efficiently given the constraints. # Example: For the provided sample XML structure, your function should return: ```python { \\"total_books\\": 2, \\"expensive_books\\": [\\"XML Developer\'s Guide\\"], \\"authors\\": [\\"Gambardella, Matthew\\", \\"Ralls, Kim\\"] } ``` # Function Signature: ```python def process_books(xml_data: str) -> dict: pass ``` Good luck, and ensure your solution is well-tested with different XML inputs!","solution":"import xml.etree.ElementTree as ET def process_books(xml_data: str) -> dict: Process an XML document containing information about a collection of books. Args: xml_data (str): A string containing XML data. Returns: dict: A dictionary with the keys \'total_books\', \'expensive_books\', and \'authors\'. root = ET.fromstring(xml_data) # XPath expressions to fetch required details books = root.findall(\'.//book\') expensive_books = [] authors = [] for book in books: price = float(book.find(\'price\').text) if price > 20.0: expensive_books.append(book.find(\'title\').text) authors.append(book.find(\'author\').text) return { \\"total_books\\": len(books), \\"expensive_books\\": expensive_books, \\"authors\\": authors }"},{"question":"**Coding Assessment Question** # Title: Importing and Managing Modules from ZIP Archives Using `zipimport` # Description: You are tasked with developing a script that demonstrates the use of Python\'s `zipimport` module. Specifically, you will create a function to import a specified Python module from a given ZIP archive. Additionally, you will handle exceptions and manage caching to optimize module loading. # Requirements: 1. Create a function `import_module_from_zip(archive_path: str, module_name: str) -> object` that: - Takes as input: - `archive_path`: A string representing the path to the ZIP file. - `module_name`: The fully qualified name (dotted name) of the Python module to import (e.g., `mypackage.mymodule`). - Returns the loaded module object on successful import. - Raises an appropriate exception if the module cannot be imported. 2. Implement caching management to avoid repeated imports if the module has already been imported once. 3. Your solution should avoid importing dynamic modules and handle cases where the ZIP archive is invalid or lacking required files gracefully. # Constraints: - The function should only import `.py` and `.pyc` files. - Do not modify the ZIP archive contents. # Example Usage: ```python try: module = import_module_from_zip(\'example.zip\', \'mypackage.mymodule\') print(module) except zipimport.ZipImportError as e: print(f\\"Error: {e}\\") except ImportError as e: print(f\\"ImportError: {e}\\") ``` # Input: - `archive_path`: Path to the ZIP archive, for example, \'example.zip\'. - `module_name`: Fully qualified name of the module to import, for example, \'jwzthreading\'. # Output: - Returns the imported module object on success. # Performance: - Efficient handling of repeated module imports using cache management techniques. # Additional Notes: - Ensure you are familiar with the `zipimport` module and its classes/methods. - Make use of methods like `find_spec`, `get_code`, and `exec_module`. - Consider testing your implementation with different ZIP archive structures and module names.","solution":"import zipimport from importlib.util import module_from_spec, spec_from_loader import sys def import_module_from_zip(archive_path: str, module_name: str) -> object: Imports a specified Python module from a given ZIP archive. :param archive_path: Path to the ZIP file. :param module_name: Fully qualified name of the module to import. :return: The imported module object. :raises: ZipImportError, ImportError try: importer = zipimport.zipimporter(archive_path) # If the module is already in sys.modules, return it (cache management) if module_name in sys.modules: return sys.modules[module_name] spec = importer.find_spec(module_name) if spec is None: raise ImportError(f\\"Module {module_name} not found in archive {archive_path}\\") module = module_from_spec(spec) sys.modules[module_name] = module spec.loader.exec_module(module) return module except zipimport.ZipImportError as e: raise zipimport.ZipImportError(f\\"Error importing from ZIP: {e}\\") except ImportError as exc: raise ImportError(f\\"ImportError: {exc}\\")"},{"question":"**Question: Implement and Compare Logging Configurations using `dictConfig` and `fileConfig`** **Objective:** Demonstrate your understanding of the `logging.config` module in Python by implementing logging configurations using both `dictConfig` and `fileConfig`. Compare their usage and results by logging messages at various levels (DEBUG, INFO, WARNING, ERROR) and redirecting them to a file and the console. **Task:** 1. **Create a logging configuration dictionary** that includes: - Two handlers: one `StreamHandler` for console output and one `FileHandler` for logging to a file named `app.log`. - A formatter that includes the log level, logger name, and log message. - A root logger that uses both handlers and logs messages at level `DEBUG`. 2. **Create a logging configuration file** (`logging.conf`) in `configparser` format with equivalent settings to the dictionary created in step 1. 3. **Write Python functions** to: - Configure logging using the dictionary from step 1 with `logging.config.dictConfig`. - Configure logging using the file from step 2 with `logging.config.fileConfig`. 4. **Write a script to test the configurations**: - Log messages at different levels (DEBUG, INFO, WARNING, ERROR). - Verify that the messages appear in both the console and the `app.log` file based on the configuration settings. **Constraints:** - You must use the `logging` and `logging.config` modules. - The dictionary and file configurations should be as identical as possible in functionality. - Use only built-in libraries for this task. **Expected Output:** - A Python script that accurately logs messages using both `dictConfig` and `fileConfig` based on the same settings. - Console output displaying logged messages. - A file named `app.log` containing the logged messages. **Files:** 1. `logging.conf` (the configuration file in `configparser` format) 2. `log_config.py` (the script implementing the logging configurations and testing them) **Code Requirements:** - The script should be runnable and produce the expected logs both on the console and in the `app.log` file. - Ensure proper error handling for missing or invalid configurations. Here\'s a template to get you started: ```python import logging import logging.config # Task 1: Logging configuration dictionary log_config_dict = { \'version\': 1, \'formatters\': { \'standard\': { \'format\': \'%(levelname)s - %(name)s - %(message)s\' }, }, \'handlers\': { \'console\': { \'class\': \'logging.StreamHandler\', \'formatter\': \'standard\', \'level\': \'DEBUG\', }, \'file\': { \'class\': \'logging.FileHandler\', \'filename\': \'app.log\', \'formatter\': \'standard\', \'level\': \'DEBUG\', }, }, \'root\': { \'handlers\': [\'console\', \'file\'], \'level\': \'DEBUG\', }, } # Task 3: Function to configure logging using dictConfig def configure_logging_dict(): logging.config.dictConfig(log_config_dict) # Task 3: Function to configure logging using fileConfig def configure_logging_file(): logging.config.fileConfig(\'logging.conf\') # Task 4: Test script to log messages def test_logging(): logger = logging.getLogger() logger.debug(\'This is a debug message\') logger.info(\'This is an info message\') logger.warning(\'This is a warning message\') logger.error(\'This is an error message\') if __name__ == \'__main__\': # Uncomment to test dictConfig # configure_logging_dict() # Uncomment to test fileConfig # configure_logging_file() test_logging() ``` **Configuration File Example (`logging.conf`):** ```ini [loggers] keys=root [handlers] keys=consoleHandler,fileHandler [formatters] keys=standardFormatter [logger_root] level=DEBUG handlers=consoleHandler,fileHandler [handler_consoleHandler] class=StreamHandler level=DEBUG formatter=standardFormatter args=(sys.stdout,) [handler_fileHandler] class=FileHandler level=DEBUG formatter=standardFormatter args=(\'app.log\', \'w\') [formatter_standardFormatter] format=%(levelname)s - %(name)s - %(message)s ``` **Note**: Make sure to have `logging.conf` in the same directory where you run the script.","solution":"import logging import logging.config import os # Define logging configuration dictionary log_config_dict = { \'version\': 1, \'formatters\': { \'standard\': { \'format\': \'%(levelname)s - %(name)s - %(message)s\' }, }, \'handlers\': { \'console\': { \'class\': \'logging.StreamHandler\', \'formatter\': \'standard\', \'level\': \'DEBUG\', }, \'file\': { \'class\': \'logging.FileHandler\', \'filename\': \'app.log\', \'formatter\': \'standard\', \'level\': \'DEBUG\', }, }, \'root\': { \'handlers\': [\'console\', \'file\'], \'level\': \'DEBUG\', }, } def create_logging_conf_file(): Create the logging.conf file. logging_conf_content = [loggers] keys=root [handlers] keys=consoleHandler,fileHandler [formatters] keys=standardFormatter [logger_root] level=DEBUG handlers=consoleHandler,fileHandler [handler_consoleHandler] class=StreamHandler level=DEBUG formatter=standardFormatter args=(sys.stdout,) [handler_fileHandler] class=FileHandler level=DEBUG formatter=standardFormatter args=(\'app.log\', \'w\') [formatter_standardFormatter] format=%(levelname)s - %(name)s - %(message)s with open(\\"logging.conf\\", \\"w\\") as configfile: configfile.write(logging_conf_content) def configure_logging_dict(): Configure logging using the dictionary configuration. logging.config.dictConfig(log_config_dict) def configure_logging_file(): Configure logging using the file configuration. logging.config.fileConfig(\'logging.conf\') def test_logging(): Test logging by outputting messages at various levels. logger = logging.getLogger() logger.debug(\'This is a debug message\') logger.info(\'This is an info message\') logger.warning(\'This is a warning message\') logger.error(\'This is an error message\') if __name__ == \'__main__\': # Create logging.conf file create_logging_conf_file() # Uncomment to test dictConfig # configure_logging_dict() # Uncomment to test fileConfig # configure_logging_file() test_logging()"},{"question":"# PyTorch Serialization and State Management Objective The goal of this question is to assess your understanding of PyTorch\'s serialization functionalities, including saving and loading tensors and neural network module states. Problem Statement You are given a neural network module with two linear layers. The implementation of this module should allow the saving and loading of the model\'s state dictionary, ensuring that it can be serialized and deserialized while preserving the model\'s parameters and buffers. Instructions 1. **Define the Neural Network Module**: - Define a custom neural network module `MyNetwork` with two `torch.nn.Linear` layers. The first layer should map from input dimension 4 to 2, and the second layer from 2 to 1. 2. **Save Model State Dictionary**: - Save the state dictionary of the neural network module to a file named `mynetwork_state.pt`. 3. **Load Model State Dictionary**: - Create a new instance of the `MyNetwork` module and load the previously saved state dictionary. 4. **Verification**: - Write a function `compare_model_params(model1, model2)` that compares the parameters of two models and returns `True` if they are the same and `False` otherwise. - Use this function to verify that the new model has successfully loaded the state dictionary by comparing it with the original model. Function Signature ```python import torch class MyNetwork(torch.nn.Module): def __init__(self): super(MyNetwork, self).__init__() self.l1 = torch.nn.Linear(4, 2) self.l2 = torch.nn.Linear(2, 1) def forward(self, x): x = torch.nn.functional.relu(self.l1(x)) x = self.l2(x) return x def save_model_state(model: torch.nn.Module, filepath: str): # Save the model\'s state dictionary torch.save(model.state_dict(), filepath) def load_model_state(model: torch.nn.Module, filepath: str): # Load the model\'s state dictionary state_dict = torch.load(filepath) model.load_state_dict(state_dict) def compare_model_params(model1: torch.nn.Module, model2: torch.nn.Module) -> bool: # Compare the parameters of the two models for p1, p2 in zip(model1.parameters(), model2.parameters()): if not torch.equal(p1, p2): return False return True # Example usage: # Create the model instance model = MyNetwork() # Save the model state save_model_state(model, \'mynetwork_state.pt\') # Load the model state into a new model new_model = MyNetwork() load_model_state(new_model, \'mynetwork_state.pt\') # Verify if the parameters are the same print(compare_model_params(model, new_model)) # Should print: True ``` Constraints - Use PyTorch version 1.6 or higher. - Ensure the module signature and state dictionary methods used are compatible with the specified PyTorch version. Notes 1. You do not need to implement data loading or training logic. 2. Ensure your save, load, and compare functions are correctly implemented. 3. Handle any edge cases or errors appropriately. Submission Provide the complete code implementation along with the verification output for the example usage.","solution":"import torch class MyNetwork(torch.nn.Module): def __init__(self): super(MyNetwork, self).__init__() self.l1 = torch.nn.Linear(4, 2) self.l2 = torch.nn.Linear(2, 1) def forward(self, x): x = torch.nn.functional.relu(self.l1(x)) x = self.l2(x) return x def save_model_state(model: torch.nn.Module, filepath: str): Save the model\'s state dictionary to the specified filepath. torch.save(model.state_dict(), filepath) def load_model_state(model: torch.nn.Module, filepath: str): Load the model\'s state dictionary from the specified filepath. state_dict = torch.load(filepath) model.load_state_dict(state_dict) def compare_model_params(model1: torch.nn.Module, model2: torch.nn.Module) -> bool: Compare the parameters of the two models and return True if they are the same. for p1, p2 in zip(model1.parameters(), model2.parameters()): if not torch.equal(p1, p2): return False return True # Usage example # Create the model instance model = MyNetwork() # Save the model state save_model_state(model, \'mynetwork_state.pt\') # Load the model state into a new model new_model = MyNetwork() load_model_state(new_model, \'mynetwork_state.pt\') # Verify if the parameters are the same print(compare_model_params(model, new_model)) # Should print: True"},{"question":"<|Analysis Begin|> The provided documentation snippet contains a list of many PyTorch functionalities, covering tensor creation, manipulation, mathematical operations, random sampling, serialization, parallelism, and utilities. Given this diverse range of functionalities, we can create a comprehensive coding question that taps into various aspects of PyTorch, ensuring that the students demonstrate both their understanding of fundamental tensor operations and their ability to utilize more advanced features like gradient computation and device management. One good approach would be to assess students\' abilities to: 1. Create and manipulate tensors. 2. Perform mathematical operations. 3. Use automatic differentiation for gradient computation. 4. Manage device allocation (i.e., CPU vs. GPU). We should construct a question that guides the student through creating tensors, performing operations, computing gradients, and optimizing these computations by making use of available GPU resources. <|Analysis End|> <|Question Begin|> **Coding Assessment Question: PyTorch Fundamentals and Advanced Operations** # Objective The aim of this challenge is to assess your ability to use PyTorch for tensor operations, mathematical computations, gradient calculations, and device management. # Task 1. **Tensor Creation and Operations** - Create two 3x3 tensors, `A` and `B`, initialized with random values. - Perform matrix multiplication between `A` and `B` to obtain matrix `C`. - Compute the element-wise sine of `C` to obtain matrix `D`. 2. **Gradient Computation** - Create a new tensor `x` of size 3x1 with `requires_grad=True`. - Define a scalar function `f` as follows: f = sum(D cdot x) where `D cdot x` represents the matrix-vector multiplication. - Compute the gradient of `f` with respect to `x`. 3. **Device Management and Optimization** - Check if a GPU device (`cuda`) is available, and if so, move the tensors `A`, `B`, and `x` to the GPU for computation. If a GPU is not available, continue using the CPU. - Repeat steps 1 and 2 to perform all operations (matrix multiplication, element-wise sine, and gradient computation) on the appropriate device. # Constraints - Ensure that all tensors are of type `torch.float32`. - Implement all tensor operations and calculations using PyTorch functions. - Avoid using high-level convenience functions like `torch.nn.functional` for basic tensor operations. # Input and Output - Input: None - Output: Print the results of the following: 1. Tensor `D`. 2. Computed gradient of `f` with respect to `x`. 3. Device on which the tensors were processed (CPU or GPU). # Example Below is an example of how the output should be formatted: ```plaintext Matrix D: [[ 0.8415, 0.9093, 0.1411], [-0.7568, -0.9589, -0.2794], [ 0.6564, 0.4121, -0.9894]] Gradient of f with respect to x: [[0.5324], [-0.4018], [ 1.0731]] Device used: cuda (if GPU was available) or cpu (if GPU was not available) ``` # Solution Template ```python import torch # Step 1: Tensor Creation and Operations A = torch.rand((3, 3), dtype=torch.float32) B = torch.rand((3, 3), dtype=torch.float32) C = torch.matmul(A, B) D = torch.sin(C) print(\\"Matrix D: n\\", D) # Step 2: Gradient Computation x = torch.rand((3, 1), dtype=torch.float32, requires_grad=True) f = torch.sum(D.mm(x)) f.backward() print(\\"Gradient of f with respect to x: n\\", x.grad) # Step 3: Device Management and Optimization device = torch.device(\\"cuda\\" if torch.cuda.is_available() else \\"cpu\\") A = A.to(device) B = B.to(device) x = x.to(device) C = torch.matmul(A, B) D = torch.sin(C) f = torch.sum(D.mm(x)) f.backward() print(\\"Matrix D on device: n\\", D) print(\\"Gradient of f with respect to x on device: n\\", x.grad) print(\\"Device used: \\", device) ``` This question covers tensor creation, manipulation, mathematical operations, gradient computation, and device management—all critical aspects of PyTorch.","solution":"import torch def perform_operations_and_compute_gradients(): # Step 1: Tensor Creation and Operations A = torch.rand((3, 3), dtype=torch.float32) B = torch.rand((3, 3), dtype=torch.float32) C = torch.matmul(A, B) D = torch.sin(C) print(\\"Matrix D: n\\", D) # Step 2: Gradient Computation x = torch.rand((3, 1), dtype=torch.float32, requires_grad=True) f = torch.sum(D.matmul(x)) f.backward() print(\\"Gradient of f with respect to x: n\\", x.grad) # Step 3: Device Management and Optimization device = torch.device(\\"cuda\\" if torch.cuda.is_available() else \\"cpu\\") A = A.to(device) B = B.to(device) x = x.to(device) C = torch.matmul(A, B) D = torch.sin(C) f = torch.sum(D.matmul(x)) f.backward() print(\\"Matrix D on device: n\\", D) print(\\"Gradient of f with respect to x on device: n\\", x.grad) print(\\"Device used: \\", device) # Call the function to execute the steps perform_operations_and_compute_gradients()"},{"question":"**Question: Advanced Audio Fragment Manipulation** **Objective:** You are tasked with writing a function to process an audio fragment by performing a series of operations using the `audioop` module. **Function Signature:** ```python def process_audio_fragment(audio_fragment: bytes, sample_width: int, factor: float, bias: int, inrate: int, outrate: int) -> bytes: Process the input audio fragment through a series of operations. Args: audio_fragment (bytes): The input audio fragment. sample_width (int): The width of each sample in bytes (1, 2, 3, or 4). factor (float): The multiplier factor to adjust the volume of the samples. bias (int): The bias to be added to each sample. inrate (int): The input sample rate (in Hz). outrate (int): The output sample rate (in Hz). Returns: bytes: The processed audio fragment. ``` **Task:** Implement the function `process_audio_fragment` to accomplish the following: 1. **Multiply Samples:** Adjust the volume of the audio fragment by multiplying each sample by the given `factor`. 2. **Add Bias:** Add the specified `bias` to each sample. 3. **Resample:** Convert the sample rate of the fragment from `inrate` to `outrate`. **Constraints:** - `audio_fragment` is a non-empty bytes object. - `sample_width` is either 1, 2, 3, or 4 bytes. - Ensure that the function handles overflows by truncating values appropriately when performing the addition and multiplication of samples. - The initial state for the `ratecv` function should be `None`. **Example:** ```python # Example audio segment in bytes (representing 2-byte samples) audio_fragment = b\'x01x00x02x00x03x00x04x00\' sample_width = 2 factor = 1.5 bias = 10 inrate = 8000 # 8 kHz outrate = 16000 # 16 kHz # Expected to output a modified bytes-like object processed_fragment = process_audio_fragment(audio_fragment, sample_width, factor, bias, inrate, outrate) ``` **Hint:** Use functions from the `audioop` module like `audioop.mul`, `audioop.bias`, and `audioop.ratecv`, ensuring that correct handling of sample widths and state transitions between multiple operations. **Assessment Criteria:** - Correctness and accuracy of the implemented operations. - Proper handling of different sample widths. - Efficient transition and integration of multiple audio operations. - Error handling and edge cases management.","solution":"import audioop def process_audio_fragment(audio_fragment: bytes, sample_width: int, factor: float, bias: int, inrate: int, outrate: int) -> bytes: Process the input audio fragment through a series of operations. Args: audio_fragment (bytes): The input audio fragment. sample_width (int): The width of each sample in bytes (1, 2, 3, or 4). factor (float): The multiplier factor to adjust the volume of the samples. bias (int): The bias to be added to each sample. inrate (int): The input sample rate (in Hz). outrate (int): The output sample rate (in Hz). Returns: bytes: The processed audio fragment. # Step 1: Multiply samples by the factor modified_fragment = audioop.mul(audio_fragment, sample_width, factor) # Step 2: Add bias to the samples modified_fragment = audioop.bias(modified_fragment, sample_width, bias) # Step 3: Resample the audio fragment to a new sample rate converted_fragment, _ = audioop.ratecv(modified_fragment, sample_width, 1, inrate, outrate, None) return converted_fragment"},{"question":"Objective Design a custom Python type that emulates some of the advanced behaviors provided by the \\"PyTypeObject\\" structure, focusing on sequence and mapping protocols. Implement a Python class `CustomContainer` that behaves like a list but also supports keyword-based item access similar to a dictionary. Requirements 1. Implement the `CustomContainer` class. 2. The class should support both list-like and dict-like behaviors: - List-like: Elements can be accessed using integer indices. - Dict-like: Elements can be accessed and assigned using string keys. 3. The class should have the following methods implemented: - `__getitem__`: Allow integer and string-based indexing. - `__setitem__`: Allow integer and string-based assignment. - `__delitem__`: Allow deletion using integer and string-based keys. - `__len__`: Return the total count of elements (consider only unique keys). - `__iter__`: Return an iterator that supports list and dict-like items. - `items`: Return a list of tuples representing key-value pairs. Input and Output - **Input:** - List and dictionary items to initialize the container (example: `CustomContainer([10, 20], {\'a\': 30, \'b\': 40})`). - Integer indices and string keys for accessing, assigning, and deleting elements. - **Output:** - Elements accessed or modified via integer or string keys. - Total count of unique elements. Constraints - The class should handle conflicts where the same key is used in both list and dictionary parts (you decide how to prioritize). - Ensure the design is efficient concerning both time and space complexity. - Document edge cases clearly (like retrieving or deleting non-existing keys). Example Usage ```python # Initialization container = CustomContainer([10, 20], {\'a\': 30, \'b\': 40}) # Accessing elements print(container[0]) # Output: 10 print(container[\'a\']) # Output: 30 # Setting elements container[1] = 25 container[\'c\'] = 50 # Deleting elements del container[0] del container[\'a\'] # Length and Items print(len(container)) # Output: 3 (note: only unique keys considered) print(list(container)) # Output: [25, \'b\', \'c\'] print(container.items()) # Output: [(\'b\', 40), (\'c\', 50)] ``` Implement the `CustomContainer` class below with the specified requirements.","solution":"class CustomContainer: def __init__(self, list_items=None, dict_items=None): self.list_items = list_items if list_items else [] self.dict_items = dict_items if dict_items else {} def __getitem__(self, key): if isinstance(key, int): return self.list_items[key] elif isinstance(key, str): return self.dict_items[key] else: raise TypeError(\\"Invalid key type.\\") def __setitem__(self, key, value): if isinstance(key, int): self.list_items[key] = value elif isinstance(key, str): self.dict_items[key] = value else: raise TypeError(\\"Invalid key type.\\") def __delitem__(self, key): if isinstance(key, int): del self.list_items[key] elif isinstance(key, str): del self.dict_items[key] else: raise TypeError(\\"Invalid key type.\\") def __len__(self): return len(self.list_items) + len(self.dict_items) def __iter__(self): return iter(self.list_items + list(self.dict_items.keys())) def items(self): return list(self.dict_items.items())"},{"question":"You are tasked with creating a function using seaborn\'s `cubehelix_palette` that generates a custom palette based on specified parameters. This custom function should take several optional parameters that control various aspects of the color palette. Function Signature ```python def custom_cubehelix_palette( n_colors: int = 6, start: float = 0.5, rot: float = 0.0, hue: float = 0.8, gamma: float = 1.0, dark: float = 0.0, light: float = 1.0, reverse: bool = False, as_cmap: bool = False ) -> \'seaborn.color_palette or matplotlib.colors.ListedColormap\': pass ``` Input Parameters - `n_colors` (int): Number of discrete colors in the palette. Default is 6. - `start` (float): The negative hue value from 0 to 3.5. Default is 0.5. - `rot` (float): Rotations around the hue wheel. Default is 0.0. - `hue` (float): Saturation value from 0 to 1. Default is 0.8. - `gamma` (float): Exponentiate the luminance values by `gamma`. Default is 1.0. - `dark` (float): Minimum lightness value from 0 to 1. Default is 0.0. - `light` (float): Maximum lightness value from 0 to 1. Default is 1.0. - `reverse` (bool): If True, reverse the direction of the luminance ramp. Default is False. - `as_cmap` (bool): If True, return a continuous colormap instead of discrete colors. Default is False. Output - Returns a seaborn color palette (`seaborn.color_palette`) if `as_cmap` is False, or a continuous colormap (`matplotlib.colors.ListedColormap`) if `as_cmap` is True. Constraints - You must use the seaborn `cubehelix_palette` function to create the palette. - The function should handle different combinations of parameters gracefully, returning the appropriate seaborn or matplotlib object. Examples ```python # Example 1 palette = custom_cubehelix_palette(n_colors=8, start=0, rot=0.4) print(palette) # Example 2 cmap = custom_cubehelix_palette(as_cmap=True, dark=0.1, light=0.9, reverse=True) print(cmap) ``` Implement the function `custom_cubehelix_palette` to solve this problem.","solution":"import seaborn as sns def custom_cubehelix_palette( n_colors: int = 6, start: float = 0.5, rot: float = 0.0, hue: float = 0.8, gamma: float = 1.0, dark: float = 0.0, light: float = 1.0, reverse: bool = False, as_cmap: bool = False ): Generates a custom seaborn cubehelix palette based on the specified parameters. Parameters: - n_colors: Number of discrete colors in the palette. - start: The negative hue value from 0 to 3.5. - rot: Rotations around the hue wheel. - hue: Saturation value from 0 to 1. - gamma: Exponentiate the luminance values by `gamma`. - dark: Minimum lightness value from 0 to 1. - light: Maximum lightness value from 0 to 1. - reverse: If True, reverse the direction of the luminance ramp. - as_cmap: If True, return a continuous colormap instead of discrete colors. Returns: - A seaborn color palette or matplotlib colormap return sns.cubehelix_palette( n_colors=n_colors, start=start, rot=rot, hue=hue, gamma=gamma, dark=dark, light=light, reverse=reverse, as_cmap=as_cmap )"},{"question":"# Codint Assessment: Frequency Domain Filtering **Objective**: Implement a frequency domain filter using PyTorch\'s `torch.fft` module to remove high-frequency noise from a 1-dimensional signal. **Specifications**: 1. **Function Signature**: ```python def frequency_domain_filter(signal: torch.Tensor, cutoff_freq: float) -> torch.Tensor: ``` 2. **Input**: - `signal` (torch.Tensor): A 1-dimensional tensor representing the input signal with shape `(N,)` where `N` is the number of samples in the signal. - `cutoff_freq` (float): The cutoff frequency for the filter. Frequencies higher than this value should be suppressed. 3. **Output**: - A 1-dimensional tensor (torch.Tensor) of shape `(N,)` representing the filtered signal. 4. **Constraints**: - The input signal will always have an even number of samples, i.e., `N` is even. - The cutoff frequency will always be a positive number and less than the Nyquist frequency (`0.5 * (N / 2)`). 5. **Performance Requirements**: - The filter must be implemented using the functions provided by the `torch.fft` module. - The implementation should leverage `torch.fft.fft` or `torch.fft.rfft` and their corresponding inverse transforms. **Example**: ```python import torch # Example signal signal = torch.tensor([0.0, 1.0, 2.0, 3.0, 4.0, 3.0, 2.0, 1.0], dtype=torch.float32) cutoff_freq = 0.5 # Applying the frequency domain filter filtered_signal = frequency_domain_filter(signal, cutoff_freq) print(filtered_signal) ``` **Additional Notes**: - You may find the `torch.fft.fftfreq` and `torch.fft.ifftshift` helper functions useful for computing the frequency components. - To apply the filter, transform the input signal to the frequency domain, suppress the frequencies higher than the cutoff frequency, and transform the signal back to the time domain. **Grading Criteria**: 1. **Correctness**: The function should accurately filter out high-frequency components above the specified cutoff. 2. **Efficiency**: The implementation should use efficient tensor operations provided by `torch.fft`. 3. **Code Quality**: The code should be well-structured and commented where necessary. Good luck!","solution":"import torch def frequency_domain_filter(signal: torch.Tensor, cutoff_freq: float) -> torch.Tensor: Apply a frequency domain filter to remove high-frequency noise from a 1-dimensional signal. Parameters: signal (torch.Tensor): A 1-dimensional tensor with shape (N,) representing the input signal. cutoff_freq (float): The cutoff frequency for the filter. Frequencies higher than this value should be suppressed. Returns: torch.Tensor: A 1-dimensional tensor of shape (N,) representing the filtered signal. # Get the number of samples N = signal.shape[0] # Compute the FFT of the input signal signal_fft = torch.fft.fft(signal) # Compute the frequency bins freqs = torch.fft.fftfreq(N) # Create a mask to filter out high frequencies mask = torch.abs(freqs) <= cutoff_freq # Apply the mask to the FFT of the signal filtered_signal_fft = signal_fft * mask # Compute the inverse FFT to get the filtered signal in time domain filtered_signal = torch.fft.ifft(filtered_signal_fft) # Return the real part of the filtered signal (since the input signal is real) return filtered_signal.real"},{"question":"# Advanced Python Numerical Operations Calculator Objective To assess your understanding and proficiency in using the `python310` package\'s numeric and bitwise operation functionalities, you are required to implement a class-based calculator in Python. This calculator will support various arithmetic, division, power, unary, bitwise, and in-place operations defined in the provided documentation. Requirements 1. **Class Name:** `AdvancedCalculator` 2. **Methods:** - `add(self, a, b) -> PyObject`: Returns the sum of `a` and `b`. - `subtract(self, a, b) -> PyObject`: Returns the result of subtracting `b` from `a`. - `multiply(self, a, b) -> PyObject`: Returns the product of `a` and `b`. - `true_divide(self, a, b) -> PyObject`: Returns the result of dividing `a` by `b` with floating-point approximation. - `floor_divide(self, a, b) -> PyObject`: Returns the floor result of `a` divided by `b`. - `remainder(self, a, b) -> PyObject`: Returns the remainder when `a` is divided by `b`. - `power(self, a, b) -> PyObject`: Returns `a` raised to the power of `b`. - `negate(self, a) -> PyObject`: Returns the negation of `a`. - `absolute(self, a) -> PyObject`: Returns the absolute value of `a`. - `bitwise_and(self, a, b) -> PyObject`: Returns the bitwise AND of `a` and `b`. - `bitwise_or(self, a, b) -> PyObject`: Returns the bitwise OR of `a` and `b`. - `bitwise_xor(self, a, b) -> PyObject`: Returns the bitwise XOR of `a` and `b`. - `left_shift(self, a, b) -> PyObject`: Returns `a` left-shifted by `b` positions. - `right_shift(self, a, b) -> PyObject`: Returns `a` right-shifted by `b` positions. - In-place variants of add, subtract, multiply, etc. 3. **Input:** Each method should accept two arguments (except unary operations which accept one) that can be numerical data types. 4. **Output:** Each method should return the result of the operation, encapsulated in a PyObject. Constraints 1. Handle cases where the operation returns `NULL` (indicating failure) by raising an appropriate Python exception with a descriptive error message. 2. Follow the documentation in the `python310` package to ensure that the operations behave identically to their Python operator equivalents. 3. Ensure that the methods handle various numeric types inputs, including integers and floating-point numbers. 4. Implement appropriate conversion functions for inputs and results. Example Usage: ```python calc = AdvancedCalculator() result = calc.add(10, 5) # Equivalent to 10 + 5 print(result) # Output should be a PyObject equivalent to 15 result = calc.negate(10) print(result) # Output should be a PyObject equivalent to -10 ``` Evaluation Your implementation will be evaluated based on: 1. Correctness: Ensuring the methods return the expected results. 2. Robustness: Handling errors gracefully and raising appropriate exceptions. 3. Code quality: Following good programming practices, readability, and maintaining consistency.","solution":"class AdvancedCalculator: Advanced Calculator to perform various arithmetic and bitwise operations. def add(self, a, b): return a + b def subtract(self, a, b): return a - b def multiply(self, a, b): return a * b def true_divide(self, a, b): if b == 0: raise ValueError(\\"Division by zero\\") return a / b def floor_divide(self, a, b): if b == 0: raise ValueError(\\"Division by zero\\") return a // b def remainder(self, a, b): if b == 0: raise ValueError(\\"Division by zero\\") return a % b def power(self, a, b): return a ** b def negate(self, a): return -a def absolute(self, a): return abs(a) def bitwise_and(self, a, b): return a & b def bitwise_or(self, a, b): return a | b def bitwise_xor(self, a, b): return a ^ b def left_shift(self, a, b): return a << b def right_shift(self, a, b): return a >> b"},{"question":"# Custom Interactive Python Shell Create a custom interactive Python shell using the `code` module\'s `InteractiveInterpreter` and `InteractiveConsole` classes. This custom shell will provide a read-eval-print loop (REPL) with additional features such as command history and custom command handling. Requirements: 1. **Basic Shell Implementation:** - Create a class `CustomInteractiveShell` that uses `code.InteractiveConsole`. - Implement methods to initialize the shell, read and execute code commands, and handle interactive sessions. 2. **Command History:** - Extend the shell to maintain a history of executed commands. - Implement functionality to view, navigate, and search through this command history. 3. **Custom Commands:** - Add support for special commands that perform specific actions when typed. For instance, `help` to display available commands and `exit` to exit the shell. - Ensure these custom commands do not interfere with normal Python code execution. 4. **Error Handling:** - Gracefully handle syntax errors, execution errors, and special exceptions like `KeyboardInterrupt`. - Display clear, informative error messages and stack traces when errors occur. Function Signatures and Example Usage: ```python import code class CustomInteractiveShell(code.InteractiveConsole): def __init__(self, locals=None): super().__init__(locals) self.command_history = [] def raw_input(self, prompt=\'\'): user_input = input(prompt) # Special commands handling if user_input.strip() == \'exit\': raise SystemExit(\'Exiting the custom interactive shell.\') elif user_input.strip() == \'help\': print(\'Available commands:nhelpnexitnhistory\') return \'\' elif user_input.strip() == \'history\': self.show_history() return \'\' self.command_history.append(user_input) return user_input def show_history(self): for i, command in enumerate(self.command_history, 1): print(f\\"{i}: {command}\\") def handle_syntax_error(self): self.showsyntaxerror() def handle_runtime_error(self): self.showtraceback() def main(): banner = \'Custom Interactive Python Shell.nType \\"help\\" for available commands.\' console = CustomInteractiveShell() try: console.interact(banner=banner, exitmsg=\'Goodbye!\') except SystemExit as e: print(e) if __name__ == \'__main__\': main() ``` Explanation: - **CustomInteractiveShell**: Inherits from `code.InteractiveConsole` and overrides the `raw_input` method to handle special commands (`help`, `exit`, `history`) and add commands to the history. - **Command History Management**: Keeps a list of entered commands and provides a method to display this history (`show_history`). - **Error Handling**: Catches syntax and runtime errors, providing informative messages. Constraints: - Your implementation should work with Python 3.10. - Should gracefully handle edge cases like continued multi-line inputs, empty inputs, and interrupts. This question assesses the student\'s understanding of interactive environments, command handling, error management, and extending base classes with additional functionalities.","solution":"import code class CustomInteractiveShell(code.InteractiveConsole): def __init__(self, locals=None): super().__init__(locals) self.command_history = [] def raw_input(self, prompt=\'\'): try: user_input = input(prompt) except EOFError: # Handle EOF, typically Ctrl-D on Unix or Ctrl-Z on Windows user_input = \'exit\' except KeyboardInterrupt: # Handle Ctrl-C print(\\"nKeyboardInterrupt: Type \'exit\' to exit the shell.\\") return \'\' # Special commands handling if user_input.strip() == \'exit\': raise SystemExit(\'Exiting the custom interactive shell.\') elif user_input.strip() == \'help\': print(\'Available commands:nhelpnexitnhistory\') return \'\' elif user_input.strip() == \'history\': self.show_history() return \'\' self.command_history.append(user_input) return user_input def show_history(self): if not self.command_history: print(\\"No history available.\\") for i, command in enumerate(self.command_history, 1): print(f\\"{i}: {command}\\") def handle_syntax_error(self): self.showsyntaxerror() def handle_runtime_error(self): self.showtraceback() def main(): banner = \'Custom Interactive Python Shell.nType \\"help\\" for available commands.\' console = CustomInteractiveShell() try: console.interact(banner=banner, exitmsg=\'Goodbye!\') except SystemExit as e: print(e) if __name__ == \'__main__\': main()"},{"question":"# Command-Line Interface for Basic Calculator You are tasked with building a command-line interface (CLI) for a basic calculator using the `cmd` module in Python. The calculator should support the following operations: 1. `add x y` - Adds two numbers `x` and `y` and prints the result. 2. `subtract x y` - Subtracts `y` from `x` and prints the result. 3. `multiply x y` - Multiplies `x` and `y` and prints the result. 4. `divide x y` - Divides `x` by `y` and prints the result. In addition to these basic operations, your CLI should also support the following commands: - `help` - Lists all available commands with their descriptions. - `history` - Prints the history of all executed commands. - `exit` - Exits the CLI. The program should handle invalid commands gracefully by printing an error message. Additionally, the CLI should convert all input to lowercase to ensure that commands are case-insensitive. # Requirements: 1. Implement a class `CalculatorShell` that inherits from `cmd.Cmd`. 2. Implement the methods to handle `add`, `subtract`, `multiply`, and `divide` commands using `do_add`, `do_subtract`, `do_multiply`, and `do_divide` methods, respectively. 3. Implement the `do_history` method to display the command history. 4. Implement the `do_exit` method to exit the CLI. 5. Override `precmd` to convert all input to lowercase. 6. Handle invalid commands by overriding the `default` method to print an appropriate error message. # Input: - Commands entered by the user at the CLI prompt. # Output: - Results of the arithmetic operations. - Command history. # Example: ``` python calculator.py Welcome to the Basic Calculator CLI. Type help or ? to list commands. (calc) add 10 5 Result: 15 (calc) subtract 10 5 Result: 5 (calc) multiply 10 5 Result: 50 (calc) divide 10 5 Result: 2.0 (calc) history 1: add 10 5 2: subtract 10 5 3: multiply 10 5 4: divide 10 5 (calc) invalidcommand *** Unknown syntax: invalidcommand (calc) exit Goodbye! ``` # Hints: - Use the `cmd.Cmd` class to create the CLI. - Maintain a list to track the history of commands. - Ensure to handle edge cases, such as division by zero.","solution":"import cmd class CalculatorShell(cmd.Cmd): intro = \'Welcome to the Basic Calculator CLI. Type help or ? to list commands.n\' prompt = \'(calc) \' def __init__(self): super().__init__() self.history = [] def do_add(self, args): add x y - Adds two numbers x and y and prints the result. try: x, y = map(float, args.split()) result = x + y print(f\\"Result: {result}\\") self.history.append(f\\"add {x} {y}\\") except ValueError: print(\\"Error: Invalid input. Use: add x y\\") def do_subtract(self, args): subtract x y - Subtracts y from x and prints the result. try: x, y = map(float, args.split()) result = x - y print(f\\"Result: {result}\\") self.history.append(f\\"subtract {x} {y}\\") except ValueError: print(\\"Error: Invalid input. Use: subtract x y\\") def do_multiply(self, args): multiply x y - Multiplies x and y and prints the result. try: x, y = map(float, args.split()) result = x * y print(f\\"Result: {result}\\") self.history.append(f\\"multiply {x} {y}\\") except ValueError: print(\\"Error: Invalid input. Use: multiply x y\\") def do_divide(self, args): divide x y - Divides x by y and prints the result. try: x, y = map(float, args.split()) if y != 0: result = x / y print(f\\"Result: {result}\\") self.history.append(f\\"divide {x} {y}\\") else: print(\\"Error: Division by zero is not allowed.\\") except ValueError: print(\\"Error: Invalid input. Use: divide x y\\") def do_history(self, args): history - Prints the history of all executed commands. for idx, command in enumerate(self.history, 1): print(f\\"{idx}: {command}\\") def do_exit(self, args): exit - Exits the CLI. print(\\"Goodbye!\\") return True def precmd(self, line): return line.lower() def default(self, line): print(f\\"*** Unknown syntax: {line}\\") if __name__ == \'__main__\': CalculatorShell().cmdloop()"},{"question":"Objective: Design a function that retrieves and processes Python\'s configuration information using the `sysconfig` module. The task will test the students\' understanding of querying configuration variables, fetching scheme names, and handling installation paths. Problem Statement: You are required to implement a function `fetch_py_config_info()` that will perform the following tasks: 1. Using the `sysconfig.get_config_vars()` function, retrieve all the configuration variables relevant to the current platform. 2. Extract and return the following specific pieces of information: - Total number of configuration variables. - Value of the configuration variable `\'Py_ENABLE_SHARED\'`. If it does not exist, return `\'Not Found\'`. - Platform name using `sysconfig.get_platform()`. - Default installation scheme using `sysconfig.get_default_scheme()`. - The `stdlib` path from the default installation scheme. - All supported installation schemes as a list. Input and Output: **Function Signature:** ```python def fetch_py_config_info() -> dict: pass ``` **Output Format:** The function should return a dictionary with the keys: - `\'total_config_vars\'`: Integer, total number of configuration variables. - `\'Py_ENABLE_SHARED\'`: Value of `\'Py_ENABLE_SHARED\'` configuration variable or `\'Not Found\'`. - `\'platform\'`: The platform name string. - `\'default_scheme\'`: Default installation scheme name string. - `\'stdlib_path\'`: The `stdlib` path string from the default installation scheme. - `\'supported_schemes\'`: List of supported installation scheme names. **Example:** ```python info = fetch_py_config_info() print(info) # Expected output (values may vary based on the platform and configuration): # { # \'total_config_vars\': 250, # \'Py_ENABLE_SHARED\': 0, # \'platform\': \'linux-x86_64\', # \'default_scheme\': \'posix_prefix\', # \'stdlib_path\': \'/usr/local/lib/python3.10\', # \'supported_schemes\': [\'posix_prefix\', \'posix_home\', \'posix_user\', \'nt\', \'nt_user\', \'osx_framework_user\'] # } ``` **Constraints:** - Assume the `sysconfig` module is available and properly imported. - The function should handle cases where the `\'Py_ENABLE_SHARED\'` variable is not defined gracefully. - The function should fetch and process the default installation scheme without any side effects or external configurations. Additional Requirements: - Ensure your code is clear and well-commented. - Include any necessary error handling to manage unexpected cases such as missing keys or unsupported actions. Hints: - Use the `sysconfig.get_config_vars()` function to retrieve the configuration variables dictionary. - The `sysconfig.get_platform()`, `sysconfig.get_default_scheme()`, and `sysconfig.get_path(\'stdlib\')` functions will be useful for fetching the required information. Good luck!","solution":"import sysconfig def fetch_py_config_info(): Retrieve and process Python\'s configuration information. config_vars = sysconfig.get_config_vars() total_config_vars = len(config_vars) py_enable_shared = config_vars.get(\'Py_ENABLE_SHARED\', \'Not Found\') platform_name = sysconfig.get_platform() default_scheme = sysconfig.get_default_scheme() stdlib_path = sysconfig.get_path(\'stdlib\', scheme=default_scheme) supported_schemes = list(sysconfig.get_scheme_names()) return { \'total_config_vars\': total_config_vars, \'Py_ENABLE_SHARED\': py_enable_shared, \'platform\': platform_name, \'default_scheme\': default_scheme, \'stdlib_path\': stdlib_path, \'supported_schemes\': supported_schemes }"},{"question":"# Logging System Implementation **Objective:** Implement a logging system that processes log messages from different parts of an application, ensuring proper handling, formatting, and filtering of log messages. **Problem Statement:** You are required to implement a class `ApplicationLogger` using the Python `logging` module. This class should: 1. Set up a logger with the name `application` at the top level of the logger hierarchy. 2. Attach a `StreamHandler` to the `application` logger, which outputs log messages to the console. 3. Configure a `FileHandler` that writes log messages to a file named `application.log`. 4. Use a `Formatter` to format the log messages as follows: \\"[%(asctime)s] %(levelname)s in %(module)s: %(message)s\\". 5. Implement a method `log_message(level, message)` that logs messages at the specified log level (`DEBUG`, `INFO`, `WARNING`, `ERROR`, `CRITICAL`). 6. Implement a method `add_filter_for_log_level(level)` which filters out log messages below the specified level for both handlers. **Constraints:** - The logger should be configured such that the `StreamHandler` and `FileHandler` use different formatters. - The application should correctly log messages to the console and the file with the respective formats. - The `add_filter_for_log_level` method should be dynamically applicable without restarting the application or reconfiguring the entire logging system. **Example Usage:** ```python app_logger = ApplicationLogger() app_logger.log_message(\'INFO\', \'This is an info message\') app_logger.log_message(\'DEBUG\', \'This is a debug message\') app_logger.add_filter_for_log_level(\'WARNING\') app_logger.log_message(\'INFO\', \'This info message should not appear\') app_logger.log_message(\'ERROR\', \'This error message should appear\') ``` Expected Output in the console: ``` [2022-01-01 12:00:00,000] INFO in <module>: This is an info message [2022-01-01 12:00:00,000] ERROR in <module>: This error message should appear ``` Expected content in `application.log`: ``` [2022-01-01 12:00:00,000] INFO in <module>: This is an info message [2022-01-01 12:00:00,000] ERROR in <module>: This error message should appear ``` **Performance Constraints:** - Ensure the logging system is efficient and does not introduce significant latency when logging messages. - Handlers and filters should work seamlessly without blocking the main application flow. **Class Definition:** ```python import logging class ApplicationLogger: def __init__(self): # Initialize the logger, stream handler, and file handler here pass def log_message(self, level, message): # Implement the log_message method pass def add_filter_for_log_level(self, level): # Implement the method to add filter for specific log level pass ``` Implement the `ApplicationLogger` class following the above requirements.","solution":"import logging class ApplicationLogger: def __init__(self): # Create top-level logger self.logger = logging.getLogger(\'application\') self.logger.setLevel(logging.DEBUG) # Set to the lowest level to capture all log levels # Create console handler and set level to debug ch = logging.StreamHandler() ch.setLevel(logging.DEBUG) # Create file handler and set level to debug fh = logging.FileHandler(\'application.log\') fh.setLevel(logging.DEBUG) # Create formatters and add them to handlers console_formatter = logging.Formatter(\'[%(asctime)s] %(levelname)s in %(module)s: %(message)s\') file_formatter = logging.Formatter(\'[%(asctime)s] %(levelname)s in %(module)s: %(message)s\') ch.setFormatter(console_formatter) fh.setFormatter(file_formatter) # Add handlers to logger self.logger.addHandler(ch) self.logger.addHandler(fh) def log_message(self, level, message): Logs a message with the given level. if level.upper() == \'DEBUG\': self.logger.debug(message) elif level.upper() == \'INFO\': self.logger.info(message) elif level.upper() == \'WARNING\': self.logger.warning(message) elif level.upper() == \'ERROR\': self.logger.error(message) elif level.upper() == \'CRITICAL\': self.logger.critical(message) else: raise ValueError(f\\"Unknown logging level: {level}\\") def add_filter_for_log_level(self, level): Adds a filter to both handlers for the specified log level. log_level = getattr(logging, level.upper(), None) if log_level is None: raise ValueError(f\\"Unknown logging level: {level}\\") class LogLevelFilter(logging.Filter): def filter(self, record): return record.levelno >= log_level for handler in self.logger.handlers: handler.addFilter(LogLevelFilter())"},{"question":"Objective: The objective of this question is to evaluate your understanding of the `seaborn` library, particularly focusing on the `residplot` function for creating and interpreting residual plots. You are expected to use various parameters and interpret the plots to draw meaningful insights from the data. Problem Statement: You are given a dataset `mpg`, which contains various attributes of cars, including `weight`, `displacement`, `horsepower`, and `mpg`. Using this dataset, your task is to create and interpret residual plots to identify and manage potential violations of linear regression assumptions. Instructions: 1. **Basic Residual Plot:** - Create a residual plot to visualize the residuals after fitting a simple regression model with `weight` as the predictor and `displacement` as the response variable. 2. **Identify Structure Violations:** - Create a residual plot to check for structure in residuals with `horsepower` as the predictor and `mpg` as the response variable. Briefly describe any violations of linear regression assumptions you observe in this plot. 3. **Remove Higher-order Trends:** - Generate a residual plot to test whether removing higher-order trends stabilizes the residuals. Use `horsepower` as the predictor and `mpg` as the response variable, but fit a quadratic model (order=2). Describe any differences from the previous plot. 4. **Using LOWESS Curve:** - Create a residual plot similar to the previous step, but add a LOWESS curve. Use `horsepower` as the predictor and `mpg` as the response variable. Customize the appearance of the LOWESS curve to be red. Interpret the added value of using the LOWESS curve. Input and Output Formats: - **Input:** The dataset `mpg` will be loaded from `seaborn` using `sns.load_dataset(\\"mpg\\")`. - **Output:** The plots generated should be interpreted, and a brief explanation (1-2 sentences) should be provided for questions 2, 3, and 4. Constraints: - Use the `seaborn` library for creating the plots. - Focus on clarity and accuracy of the residual plots. Performance Requirements: - Ensure plots are generated efficiently. - Plots should be well-labeled and easy to understand. You can use the following code template as a starting point: ```python import seaborn as sns import matplotlib.pyplot as plt # Load the dataset mpg = sns.load_dataset(\\"mpg\\") # 1. Basic Residual Plot sns.residplot(data=mpg, x=\\"weight\\", y=\\"displacement\\") plt.title(\\"Residual Plot: weight vs displacement\\") plt.show() # 2. Residual Plot to Identify Structure Violations sns.residplot(data=mpg, x=\\"horsepower\\", y=\\"mpg\\") plt.title(\\"Residual Plot: horsepower vs mpg\\") plt.show() # Write your interpretation here (1-2 sentences): # 3. Remove Higher-order Trends sns.residplot(data=mpg, x=\\"horsepower\\", y=\\"mpg\\", order=2) plt.title(\\"Quadratic Residual Plot: horsepower vs mpg\\") plt.show() # Write your interpretation here (1-2 sentences): # 4. Adding LOWESS Curve sns.residplot(data=mpg, x=\\"horsepower\\", y=\\"mpg\\", lowess=True, line_kws=dict(color=\\"r\\")) plt.title(\\"Residual Plot with LOWESS Curve: horsepower vs mpg\\") plt.show() # Write your interpretation here (1-2 sentences): ``` Good luck!","solution":"import seaborn as sns import matplotlib.pyplot as plt # Load the dataset mpg = sns.load_dataset(\\"mpg\\") def basic_residual_plot(): Create a basic residual plot to visualize the residuals after fitting a simple regression model with weight as the predictor and displacement as the response variable. sns.residplot(data=mpg, x=\\"weight\\", y=\\"displacement\\") plt.title(\\"Residual Plot: weight vs displacement\\") plt.show() def identify_structure_violations(): Create a residual plot to check for structure in residuals with horsepower as the predictor and mpg as the response variable. Describe any violations of linear regression assumptions observed in this plot. sns.residplot(data=mpg, x=\\"horsepower\\", y=\\"mpg\\") plt.title(\\"Residual Plot: horsepower vs mpg\\") plt.show() # Interpretation: If the plot shows a pattern (e.g., funnel shape, curve), it indicates a potential # violation of the homoscedasticity or linear relationship assumption. def remove_higher_order_trends(): Generate a residual plot to test whether removing higher-order trends stabilizes the residuals. Use horsepower as the predictor and mpg as the response variable, fitting a quadratic model (order=2). Describe any differences from the previous plot. sns.residplot(data=mpg, x=\\"horsepower\\", y=\\"mpg\\", order=2) plt.title(\\"Quadratic Residual Plot: horsepower vs mpg\\") plt.show() # Interpretation: If the residuals become more randomly dispersed around the horizontal line compared # to the linear model, it suggests a better fit by addressing non-linear trends. def resid_plot_with_lowess_curve(): Create a residual plot with a LOWESS curve. Use horsepower as the predictor and mpg as the response variable, customizing the appearance of the LOWESS curve to be red. Interpret the added value of using the LOWESS curve. sns.residplot(data=mpg, x=\\"horsepower\\", y=\\"mpg\\", lowess=True, line_kws=dict(color=\\"r\\")) plt.title(\\"Residual Plot with LOWESS Curve: horsepower vs mpg\\") plt.show() # Interpretation: The LOWESS curve helps to visualize the trend and deviations in the residuals, # providing insight into non-linear relationships and potential areas where the model may be improved."},{"question":"# Color Space Conversion and Validation You are given a set of points in the RGB coordinate system. Your task is to write a function that converts these points to another color space (YIQ, HLS, or HSV), then back to RGB, and checks if the original point and the final point after round-trip conversion are within a specified tolerance level of each other. Function signature: ```python def validate_color_conversion(points: list, color_space: str, tolerance: float) -> list: ``` Parameters: - `points`: A list of tuples, where each tuple contains three floating-point values representing a color in RGB space. For example, `[(0.2, 0.4, 0.4), (0.1, 0.3, 0.7)]` - `color_space`: A string specifying the target color space for conversion. It can be one of `\\"YIQ\\"`, `\\"HLS\\"`, or `\\"HSV\\"`. - `tolerance`: A floating-point value specifying the allowed difference between the original and the final color values after conversion and reconversion. Returns: - A list of boolean values, where each boolean indicates whether the respective point in the `points` list is within the specified tolerance level after round-trip conversion. Constraints: - All RGB, YIQ, HLS, and HSV values are floating-point numbers between 0 and 1 (for YIQ, the I and Q can be negative). - The `tolerance` is a non-negative floating point number. Example: ```python points = [(0.2, 0.4, 0.4), (0.1, 0.3, 0.7)] color_space = \\"HSV\\" tolerance = 0.01 result = validate_color_conversion(points, color_space, tolerance) print(result) # Output: [True, True] ``` In this example, both color points remain within the specified tolerance level after converting to HSV and then back to RGB. Notes: - Use the `colorsys` module for the conversions. - You may find it helpful to read the documentation of `colorsys` for more details about the specific functions provided for color conversion. Performance Requirements: - The function should handle input lists of up to 10,000 color points efficiently.","solution":"import colorsys def validate_color_conversion(points: list, color_space: str, tolerance: float) -> list: results = [] for r, g, b in points: # Convert from RGB to specified color space if color_space == \\"YIQ\\": y, i, q = colorsys.rgb_to_yiq(r, g, b) # Convert back to RGB r2, g2, b2 = colorsys.yiq_to_rgb(y, i, q) elif color_space == \\"HLS\\": h, l, s = colorsys.rgb_to_hls(r, g, b) # Convert back to RGB r2, g2, b2 = colorsys.hls_to_rgb(h, l, s) elif color_space == \\"HSV\\": h, s, v = colorsys.rgb_to_hsv(r, g, b) # Convert back to RGB r2, g2, b2 = colorsys.hsv_to_rgb(h, s, v) else: raise ValueError(f\\"Unknown color space: {color_space}\\") # Check if the difference is within tolerance diff = abs(r - r2) + abs(g - g2) + abs(b - b2) results.append(diff <= tolerance) return results"},{"question":"# Task You are required to implement a function `calculate_task_order(tasks, dependencies)` that returns the order in which a list of tasks should be executed, considering their dependencies. You are provided with a list of tasks and a list of dependencies, where each dependency is a tuple that specifies which task must be completed before another task can start. # Input - `tasks`: A list of distinct hashable tasks (strings or integers). - `dependencies`: A list of tuples `(a, b)` where task `a` must be completed before task `b`. # Output - A list of tasks representing the order in which they should be executed. If there are multiple valid orders, return any of them. - If the tasks cannot be completed due to a cycle, raise a `ValueError` with a message \\"Cycle detected\\". # Constraints - All tasks and dependencies are valid (i.e., the tasks mentioned in dependencies are part of the `tasks` list). - The tasks list will have at most 10^5 tasks. # Example ```python tasks = [\\"A\\", \\"B\\", \\"C\\", \\"D\\"] dependencies = [(\\"A\\", \\"C\\"), (\\"B\\", \\"C\\"), (\\"C\\", \\"D\\")] print(calculate_task_order(tasks, dependencies)) # Output: [\'A\', \'B\', \'C\', \'D\'] or a valid alternative order ``` # Explanation In this example, tasks `A` and `B` must be completed before task `C`, and task `C` must be completed before task `D`. Thus, `A` and `B` can be performed in any order, followed by `C` and then `D`. # Implementation You should use the `graphlib.TopologicalSorter` class for this task. The class is designed to handle the complexities of topological sorting and detecting cycles. ```python from graphlib import TopologicalSorter, CycleError def calculate_task_order(tasks, dependencies): ts = TopologicalSorter() # Add all tasks with no dependencies initially for task in tasks: ts.add(task) # Add all dependencies for before, after in dependencies: ts.add(after, before) try: # Prepare the graph and get the static order order = list(ts.static_order()) return order except CycleError: raise ValueError(\\"Cycle detected\\") ``` # Notes - Leverage the features provided by `graphlib.TopologicalSorter` to handle the complexities of adding tasks and dependencies. - Ensure the solution handles large inputs efficiently, as the number of tasks can be up to 10^5.","solution":"from graphlib import TopologicalSorter, CycleError def calculate_task_order(tasks, dependencies): ts = TopologicalSorter() # Add all tasks with no dependencies initially for task in tasks: ts.add(task) # Add all dependencies for before, after in dependencies: ts.add(after, before) try: # Prepare the graph and get the static order order = list(ts.static_order()) return order except CycleError: raise ValueError(\\"Cycle detected\\")"},{"question":"# Memory-Mapped File Manipulation **Problem Statement**: You are required to implement a function `manipulate_memory_mapped_file(file_path: str, operations: list)` that performs a sequence of operations on the specified memory-mapped file. **Function Signature**: ```python def manipulate_memory_mapped_file(file_path: str, operations: list) -> list: pass ``` **Inputs**: - `file_path` (str): The path to the file to be memory-mapped. - `operations` (list): A list of operations to perform on the memory-mapped file. Each operation is represented as a tuple where the first element is the operation type (str), followed by operation-specific arguments. Supported operations: - `read(n)`: Reads `n` bytes from the current position. - `write(data)`: Writes `data` (bytes-like object) at the current position. - `seek(pos, whence)`: Sets the file\'s current position based on `whence`. - `read_byte()`: Reads a byte from the current position. - `write_byte(byte)`: Writes `byte` (integer) at the current position. - `find(sub)`: Finds the subsequence `sub` and returns its position. - `flush()`: Ensures all changes are flushed to disk. **Outputs**: - Returns a list with the results of `read`, `read_byte`, `find`, and `flush` operations. Other operations should not return any value but may modify the file content. **Example**: ```python # Suppose the content of file \\"example.txt\\" is: b\\"Hello World! This is a test file.\\" operations = [ (\'read\', 5), (\'seek\', 6, 0), (\'write\', b\\"Python\\"), (\'seek\', 0, 0), (\'read\', 11), (\'find\', b\\"Python\\"), (\'flush\',) ] result = manipulate_memory_mapped_file(\\"example.txt\\", operations) print(result) ``` **Expected Output**: ```python [b\\"Hello\\", b\\"Hello Python\\", 6, None] ``` **Constraints**: 1. You may assume that the input file exists and is readable and writable. 2. The operations list is guaranteed to have valid operation types and arguments. 3. Performance should be optimized for large files up to several GB in size. **Notes**: - Use the `mmap` module to memory-map the file specified by `file_path`. - Handle both reading and writing operations using the memory-mapped object. - Ensure the file is properly closed after all operations are completed. **Implementation Tips**: - If dealing with writability, make sure to use the appropriate `access` type. - Consider using a context manager for handling the file object and memory-mapped object.","solution":"import mmap def manipulate_memory_mapped_file(file_path: str, operations: list) -> list: results = [] with open(file_path, \\"r+b\\") as f: # Memory-mapping the file mm = mmap.mmap(f.fileno(), 0) for op in operations: operation = op[0] if operation == \'read\': n = op[1] results.append(mm.read(n)) elif operation == \'write\': data = op[1] mm.write(data) elif operation == \'seek\': pos, whence = op[1], op[2] mm.seek(pos, whence) elif operation == \'read_byte\': results.append(bytes([mm.read_byte()])) elif operation == \'write_byte\': byte = op[1] mm.write_byte(byte) elif operation == \'find\': sub = op[1] pos = mm.find(sub) results.append(pos) elif operation == \'flush\': mm.flush() results.append(None) mm.close() return results"},{"question":"# Concurrent File Processing Using Threads Problem Statement You are tasked with processing a list of text files concurrently using the low-level threading APIs provided by the `_thread` module. Each file will contain a list of integers, one integer per line. The goal is to calculate the sum of integers from each file separately and then determine the grand total sum from all files. To ensure thread safety, you must use locks to protect shared resources. Requirements 1. Implement a function `process_files(file_paths: List[str]) -> int` where: - `file_paths` is a list of file paths, each pointing to a text file containing integers. - The function reads integers from each file, calculates their sum concurrently, and returns the grand total sum of integers across all files. 2. Use the `_thread` module to create and manage threads. 3. Use locks to ensure thread-safe operations when updating the shared grand total sum. Function Signature ```python import _thread from typing import List def process_files(file_paths: List[str]) -> int: pass ``` Example Suppose you have three files: - `file1.txt` containing: ``` 1 2 3 4 ``` - `file2.txt` containing: ``` 10 20 ``` - `file3.txt` containing: ``` 100 ``` The total sum of integers in these files is `1+2+3+4+10+20+100 = 140`. ```python # Provided file paths file_paths = [\\"file1.txt\\", \\"file2.txt\\", \\"file3.txt\\"] # Expected output result = process_files(file_paths) print(result) # Output: 140 ``` Constraints 1. Assume each file contains a large number of integers, so the solution should efficiently utilize threading for concurrent processing. 2. The function should correctly manage thread synchronization using locks from the `_thread` module. 3. Each file path is guaranteed to be valid and exists. Performance Requirements 1. The solution should handle up to 1000 files efficiently. 2. Each file can contain up to 1 million integers. # Notes - Ensure that all threads have completed their execution before returning the final grand total sum. - Keep the solution robust to handle potential exceptions within threads gracefully. Good luck!","solution":"import _thread from typing import List import threading def process_files(file_paths: List[str]) -> int: grand_total = 0 total_lock = _thread.allocate_lock() def process_file(file_path: str): nonlocal grand_total file_sum = 0 with open(file_path, \'r\') as f: for line in f: file_sum += int(line.strip()) with total_lock: grand_total += file_sum threads = [] for file_path in file_paths: thread = threading.Thread(target=process_file, args=(file_path,)) threads.append(thread) thread.start() for thread in threads: thread.join() return grand_total"},{"question":"# PyTorch JIT Compilation and Model Optimization You are required to demonstrate your understanding of PyTorch\'s JIT utilities by performing the following tasks: 1. **Define and Train a Simple Neural Network**: - Implement a simple fully connected neural network using PyTorch. - Train the network on a sample dataset (e.g., random tensors or a simple toy dataset). 2. **Optimize the Model with JIT**: - Convert the trained model to a TorchScript model using `torch.jit.script` or `torch.jit.trace`. - Evaluate the performance of the model before and after JIT compilation. (You might use a timing function to compare inference times) 3. **Serialize and Deserialize the Model**: - Serialize the optimized JIT model to a file. - Load the model back from the file and ensure it maintains the same performance and outputs. 4. **Handle Dynamic Control Flow**: - Modify the original network to include some dynamic control flow (e.g., conditional statements within the forward method). - Re-convert this modified network using JIT, and handle any challenges that arise due to the dynamic nature. # Input and Output Formats: - **Input**: Training function setup including the neural network, dataset, and training loop. - **Output**: - The converted JIT model. - Serialized model file saved on disk. - Timing performance metrics before and after JIT compilation. # Constraints: - The neural network should have at least one hidden layer. - The toy dataset can be synthetic but should have a clear pattern. - The solution should be clear, efficient, and well-documented. # Performance Requirements: - Ensure that the JIT compiled model shows a measurable performance improvement during inference over the original model. - Validate that the outputs from the deserialized model match the outputs before serialization. # Code Skeleton: ```python import torch import torch.nn as nn import torch.optim as optim from time import time # 1. Define a simple feed-forward neural network. class SimpleNN(nn.Module): def __init__(self, input_size, hidden_size, output_size): super(SimpleNN, self).__init__() self.fc1 = nn.Linear(input_size, hidden_size) self.relu = nn.ReLU() self.fc2 = nn.Linear(hidden_size, output_size) def forward(self, x): out = self.fc1(x) out = self.relu(out) out = self.fc2(out) return out def train_model(): # Implement data creation, model training, and optimization here. pass def optimize_with_jit(model): # Convert the model to TorchScript using JIT and compare performance. pass def serialize_model(jit_model, file_path): # Save the TorchScript model to a file. pass def deserialize_model(file_path): # Load the TorchScript model from the file. pass def handle_dynamic_control_flow(model): # Modify the model to include dynamic control flow and recompile with JIT. pass if __name__ == \\"__main__\\": train_model() ``` Provide the implementations for the functions outlined in the skeleton above based on the requirements.","solution":"import torch import torch.nn as nn import torch.optim as optim from time import time # 1. Define a simple feed-forward neural network. class SimpleNN(nn.Module): def __init__(self, input_size, hidden_size, output_size): super(SimpleNN, self).__init__() self.fc1 = nn.Linear(input_size, hidden_size) self.relu = nn.ReLU() self.fc2 = nn.Linear(hidden_size, output_size) def forward(self, x): out = self.fc1(x) out = self.relu(out) out = self.fc2(out) return out # Modified network to include dynamic control flow class DynamicNN(nn.Module): def __init__(self, input_size, hidden_size, output_size): super(DynamicNN, self).__init__() self.fc1 = nn.Linear(input_size, hidden_size) self.relu = nn.ReLU() self.fc2 = nn.Linear(hidden_size, output_size) def forward(self, x): out = self.fc1(x) if torch.mean(out) > 0.5: out = self.relu(out) else: out = -self.relu(out) out = self.fc2(out) return out def train_model(): # Sample data and parameters torch.manual_seed(0) input_size = 10 hidden_size = 5 output_size = 1 num_samples = 100 batch_size = 10 num_epochs = 5 learning_rate = 0.001 # Toy dataset X = torch.randn(num_samples, input_size) y = torch.randn(num_samples, output_size) model = SimpleNN(input_size, hidden_size, output_size) criterion = nn.MSELoss() optimizer = optim.Adam(model.parameters(), lr=learning_rate) for epoch in range(num_epochs): for i in range(0, num_samples, batch_size): inputs = X[i:i+batch_size] labels = y[i:i+batch_size] outputs = model(inputs) loss = criterion(outputs, labels) optimizer.zero_grad() loss.backward() optimizer.step() return model, X, y def optimize_with_jit(model): scripted_model = torch.jit.script(model) return scripted_model def serialize_model(jit_model, file_path): torch.jit.save(jit_model, file_path) def deserialize_model(file_path): return torch.jit.load(file_path) def evaluate_model(model, X): model.eval() with torch.no_grad(): start_time = time() model(X) end_time = time() return end_time - start_time if __name__ == \\"__main__\\": model, X, y = train_model() # Performance before JIT time_before_jit = evaluate_model(model, X) print(f\\"Time before JIT: {time_before_jit}\\") # Optimizing with JIT scripted_model = optimize_with_jit(model) # Performance after JIT time_after_jit = evaluate_model(scripted_model, X) print(f\\"Time after JIT: {time_after_jit}\\") # Serialize model serialize_model(scripted_model, \\"jit_model.pth\\") # Deserialize model loaded_jit_model = deserialize_model(\\"jit_model.pth\\") # Performance of loaded model time_loaded_jit_model = evaluate_model(loaded_jit_model, X) print(f\\"Time loaded JIT model: {time_loaded_jit_model}\\") # Verify outputs match original_output = model(X) loaded_output = loaded_jit_model(X) assert torch.allclose(original_output, loaded_output), \\"Outputs do not match\\" # Handle dynamic control flow dynamic_model = DynamicNN(10, 5, 1) # Optimize dynamic model with JIT dynamic_scripted_model = torch.jit.script(dynamic_model) # Test dynamic model dynamic_output = dynamic_scripted_model(X) assert dynamic_output is not None, \\"Dynamic model JIT compilation failed\\""},{"question":"# Question You are given a task to benchmark the performance of matrix multiplication using two different approaches in PyTorch: using the standard matrix multiplication (`torch.mm`) and batched matrix multiplication (`torch.bmm`). Implement a function `benchmark_matrix_multiplication(approach: str, size: int, num_batches: int = 1) -> float` that benchmarks the specified approach and returns the average time taken for the matrix multiplication operation. Implementation details: 1. The function `benchmark_matrix_multiplication` should take the following parameters: - `approach`: A string that specifies which approach to benchmark. It can be either \\"standard\\" for using `torch.mm` or \\"batched\\" for using `torch.bmm`. - `size`: An integer that specifies the size of the square matrices to multiply. - `num_batches`: An optional integer that specifies the number of batches for the batched approach (default is 1, which means no batching). 2. The function should return a float representing the average time in seconds taken to perform the matrix multiplication over multiple runs. 3. Use the `torch.utils.benchmark` package to time the matrix multiplication operations. Make sure to run the benchmark multiple times to get an average time. 4. For the batched approach, create `num_batches` pairs of matrices to multiply. Each matrix should be of size (size, size). Here is the expected input and output format: ```python def benchmark_matrix_multiplication(approach: str, size: int, num_batches: int = 1) -> float: # Your code here pass ``` **Example:** ```python avg_time_standard = benchmark_matrix_multiplication(\\"standard\\", 1000) print(f\\"Average time for standard mm: {avg_time_standard} seconds\\") avg_time_batched = benchmark_matrix_multiplication(\\"batched\\", 100, 10) print(f\\"Average time for batched bmm: {avg_time_batched} seconds\\") ``` The function should print the average time taken for the matrix multiplication using the specified approach. **Constraints:** - Ensure that your implementation handles large matrices efficiently. - You can assume `size` will be a positive integer and `num_batches` will be a positive integer when the approach is \\"batched\\". **Performance Requirements:** - The function should be able to handle large matrix sizes (e.g., `size = 1000`) and large batch numbers (e.g., `num_batches = 1000`) within a reasonable time frame. # Notes: - You may need to refer to the PyTorch documentation on `torch.mm` and `torch.bmm` if you are unfamiliar with these functions. - Make sure to install PyTorch and import the required modules at the beginning of your implementation.","solution":"import torch import time def benchmark_matrix_multiplication(approach: str, size: int, num_batches: int = 1) -> float: Benchmarks the specified matrix multiplication approach (standard or batched) and returns the average time taken in seconds. Args: - approach (str): Either \\"standard\\" for using torch.mm or \\"batched\\" for using torch.bmm. - size (int): The size of the square matrices. - num_batches (int): Number of batches for batched approach (default is 1). Returns: - float: Average time taken for the matrix multiplication operation in seconds. if approach not in [\\"standard\\", \\"batched\\"]: raise ValueError(\\"Approach must be either \'standard\' or \'batched\'\\") if approach == \\"standard\\": # Generate two random matrices of the given size A = torch.rand(size, size) B = torch.rand(size, size) # Time the standard matrix multiplication start_time = time.time() for _ in range(10): # Run the multiplication multiple times for averaging result = torch.mm(A, B) end_time = time.time() avg_time = (end_time - start_time) / 10 elif approach == \\"batched\\": # Generate random matrices for batched multiplication A = torch.rand(num_batches, size, size) B = torch.rand(num_batches, size, size) # Time the batched matrix multiplication start_time = time.time() for _ in range(10): # Run the multiplication multiple times for averaging result = torch.bmm(A, B) end_time = time.time() avg_time = (end_time - start_time) / 10 return avg_time"},{"question":"Objective Design a function utilizing `seaborn` to create a customized color palette and apply it to a bar plot. This exercise will assess your ability to use the `cubehelix_palette` function and create meaningful visualizations. Requirements Implement a Python function `create_custom_palette_and_plot(data, num_colors, start, rot, gamma, hue, dark, light)` that: 1. Takes the following inputs: - `data`: A pandas DataFrame with at least two columns: one for categories and one for numerical values. - `num_colors`: An integer specifying the number of colors in the palette. - `start`: A float indicating the starting point of the helix. - `rot`: A float indicating the number of rotations in the helix. - `gamma`: A float to apply a nonlinearity to the luminance ramp. - `hue`: A float specifying the saturation of the colors. - `dark`: A float indicating the intensity of the darkest color. - `light`: A float indicating the intensity of the lightest color. 2. Utilizes the `seaborn.cubehelix_palette` function to create a custom color palette based on the given parameters. 3. Creates a bar plot using the customized palette, where: - The x-axis represents the categories from the first column of the DataFrame. - The y-axis represents the numerical values from the second column of the DataFrame. 4. Displays the bar plot using `seaborn` and `matplotlib`. Constraints - The DataFrame provided will always have exactly two columns. - The `num_colors` parameter will be a positive integer. - The `start`, `rot`, `gamma`, `hue`, `dark`, and `light` parameters will be floats within the range `[0.0, 1.0]`. Example ```python import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def create_custom_palette_and_plot(data, num_colors, start, rot, gamma, hue, dark, light): palette = sns.cubehelix_palette(n_colors=num_colors, start=start, rot=rot, gamma=gamma, hue=hue, dark=dark, light=light) sns.barplot(x=data.columns[0], y=data.columns[1], data=data, palette=palette) plt.show() # Example usage: data = pd.DataFrame({ \'Category\': [\'A\', \'B\', \'C\', \'D\'], \'Values\': [10, 20, 15, 25] }) create_custom_palette_and_plot(data, 4, 0.3, 0.5, 1.0, 0.8, 0.2, 0.8) ``` The function `create_custom_palette_and_plot` should generate a bar plot with a customized color palette based on the specified parameters.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def create_custom_palette_and_plot(data, num_colors, start, rot, gamma, hue, dark, light): Creates a bar plot using a customized color palette based on the provided parameters. Parameters: data (pd.DataFrame): A DataFrame with at least two columns: categories and numerical values. num_colors (int): Number of colors in the palette. start (float): Starting point of the helix. rot (float): Number of rotations in the helix. gamma (float): Nonlinearity in the luminance ramp. hue (float): Saturation of the colors. dark (float): Intensity of the darkest color. light (float): Intensity of the lightest color. palette = sns.cubehelix_palette(n_colors=num_colors, start=start, rot=rot, gamma=gamma, hue=hue, dark=dark, light=light) sns.barplot(x=data.columns[0], y=data.columns[1], data=data, palette=palette) plt.show()"},{"question":"# Covariance Estimation Techniques Using scikit-learn Background Covariance estimation is crucial for understanding the relationships between variables in a dataset. The `sklearn.covariance` module in scikit-learn provides various methods for estimating the covariance matrix, each suited to different types of data and use cases. In this assessment, you are going to implement a comparison between different covariance estimation techniques using scikit-learn. You will use empirical covariance, shrunk covariance, and robust covariance estimation techniques. Task 1. **Input**: - A 2D array `data` of shape `(n_samples, n_features)` representing the dataset. 2. **Output**: - Print the covariance matrices computed using: - Empirical covariance. - Shrunk covariance with a shrinkage coefficient of 0.1. - Ledoit-Wolf shrinkage. - Oracle Approximating Shrinkage (OAS). - Robust covariance using Minimum Covariance Determinant (MCD). 3. **Requirements**: - Implement a function `compare_covariance_methods(data: np.ndarray) -> None`: - Compute the covariance matrix for each method. - Print the covariance matrix for each method, labeled clearly for identification. Example ```python import numpy as np data = np.array([[4.0, 2.0], [1.0, 3.0], [3.0, 1.0], [5.0, 2.0]]) compare_covariance_methods(data) ``` Constraints - Assume that the input data will always be a 2D NumPy array with at least 2 samples and 2 features. - The code should handle different input data shapes without any assumptions about the data distribution or the existence of outliers. Implementation Use the following scaffolding code to get started: ```python import numpy as np from sklearn.covariance import ( EmpiricalCovariance, ShrunkCovariance, LedoitWolf, OAS, MinCovDet ) def compare_covariance_methods(data): # Empirical Covariance emp_cov = EmpiricalCovariance().fit(data).covariance_ print(\\"Empirical Covariance:n\\", emp_cov) # Shrunk Covariance shrunk_cov = ShrunkCovariance(shrinkage=0.1).fit(data).covariance_ print(\\"Shrunk Covariance:n\\", shrunk_cov) # Ledoit-Wolf Shrinkage lw_cov = LedoitWolf().fit(data).covariance_ print(\\"Ledoit-Wolf Shrinkage:n\\", lw_cov) # Oracle Approximating Shrinkage (OAS) oas_cov = OAS().fit(data).covariance_ print(\\"OAS:n\\", oas_cov) # Robust Covariance (Minimum Covariance Determinant) mcd_cov = MinCovDet().fit(data).covariance_ print(\\"Minimum Covariance Determinant:n\\", mcd_cov) # Example usage data = np.array([[4.0, 2.0], [1.0, 3.0], [3.0, 1.0], [5.0, 2.0]]) compare_covariance_methods(data) ``` Notes - Ensure that your code is clear and concise. - Handle all edge cases. - The output should be easily readable with appropriate labels for each covariance matrix.","solution":"import numpy as np from sklearn.covariance import ( EmpiricalCovariance, ShrunkCovariance, LedoitWolf, OAS, MinCovDet ) def compare_covariance_methods(data: np.ndarray) -> None: Computes and prints the covariance matrix for various covariance estimation techniques. Parameters: data (np.ndarray): Input data array of shape (n_samples, n_features). Returns: None # Empirical Covariance emp_cov = EmpiricalCovariance().fit(data).covariance_ print(\\"Empirical Covariance:n\\", emp_cov) # Shrunk Covariance shrunk_cov = ShrunkCovariance(shrinkage=0.1).fit(data).covariance_ print(\\"Shrunk Covariance:n\\", shrunk_cov) # Ledoit-Wolf Shrinkage lw_cov = LedoitWolf().fit(data).covariance_ print(\\"Ledoit-Wolf Shrinkage:n\\", lw_cov) # Oracle Approximating Shrinkage (OAS) oas_cov = OAS().fit(data).covariance_ print(\\"Oracle Approximating Shrinkage (OAS):n\\", oas_cov) # Robust Covariance (Minimum Covariance Determinant) mcd_cov = MinCovDet().fit(data).covariance_ print(\\"Minimum Covariance Determinant:n\\", mcd_cov) # Example usage data = np.array([[4.0, 2.0], [1.0, 3.0], [3.0, 1.0], [5.0, 2.0]]) compare_covariance_methods(data)"},{"question":"# PyTorch Multiprocessing Coding Challenge Instructions You are tasked with implementing a function that performs distributed training on a simple neural network using PyTorch\'s `torch.multiprocessing` module. The function should incorporate best practices for multiprocessing with tensors, as well as handle potential issues such as CPU oversubscription and ensuring data is properly shared between processes. Requirements 1. **Model Definition**: Define a simple feedforward neural network using `torch.nn`. 2. **Data Preparation**: Use random data to simulate a dataset. 3. **Training Function**: Implement a training function that is distributed across multiple processes. 4. **Multiprocessing Setup**: Set up the multiprocessing environment, ensuring shared memory and handling potential deadlocks. 5. **CPU Oversubscription Handling**: Implement logic to avoid CPU oversubscription. Function Signature ```python import torch import torch.nn as nn import torch.optim as optim import torch.multiprocessing as mp from math import floor def distributed_training(num_processes: int, num_epochs: int, num_vCPUs: int) -> None: pass ``` # Inputs - `num_processes` (int): The number of processes to use for training. - `num_epochs` (int): The number of epochs to train the model. - `num_vCPUs` (int): The number of virtual CPUs available on the machine. # Outputs - The function should not return anything. It should print the training progress for each process. Example Usage ```python if __name__ == \'__main__\': num_processes = 4 num_epochs = 5 num_vCPUs = 8 distributed_training(num_processes, num_epochs, num_vCPUs) ``` Additional Information - **Model Definition**: Your model should be a simple feedforward neural network with at least one hidden layer. - **Data Preparation**: Generate random data tensors to act as input and target data. Ensure the data is shared between processes efficiently. - **Training Function**: Each process should perform part of the training. Use `torch.optim.SGD` as the optimizer. - **Multiprocessing Setup**: Use `torch.multiprocessing` to manage processes and ensure shared memory for data tensors. - **CPU Oversubscription Handling**: - Calculate the appropriate number of threads for each process using `torch.set_num_threads(floor(num_vCPUs/num_processes))`. - Ensure resource allocation does not lead to CPU oversubscription. Note - Use `if __name__ == \'__main__\':` to guard the start of the multiprocessing code. - Test your function on a system with multiple vCPUs to observe the effectiveness of your implementation.","solution":"import torch import torch.nn as nn import torch.optim as optim import torch.multiprocessing as mp from math import floor # Define a simple feedforward neural network class SimpleNN(nn.Module): def __init__(self): super(SimpleNN, self).__init__() self.fc1 = nn.Linear(10, 50) self.fc2 = nn.Linear(50, 1) def forward(self, x): x = torch.relu(self.fc1(x)) x = self.fc2(x) return x def train_process(rank, num_epochs, model, optimizer, data, targets, lock): torch.set_num_threads(floor(num_vCPUs / num_processes)) loss_fn = nn.MSELoss() for epoch in range(num_epochs): optimizer.zero_grad() outputs = model(data) loss = loss_fn(outputs, targets) loss.backward() optimizer.step() with lock: print(f\\"Process {rank}, Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}\\") def distributed_training(num_processes: int, num_epochs: int, num_vCPUs: int) -> None: model = SimpleNN() model.share_memory() # Make the model parameters shared across processes optimizer = optim.SGD(model.parameters(), lr=0.01) # Generate random data data = torch.randn(100, 10) targets = torch.randn(100, 1) # Make data and targets shared across processes data.share_memory_() targets.share_memory_() lock = mp.Lock() processes = [] for rank in range(num_processes): p = mp.Process(target=train_process, args=(rank, num_epochs, model, optimizer, data, targets, lock)) p.start() processes.append(p) for p in processes: p.join()"},{"question":"Objective: To test your understanding of the `asyncio` library for writing concurrent code in Python. Task: Implement an asynchronous function `fetch_data_and_process` that simulates network data fetching and processing using `asyncio` library functions. 1. **Function Signature**: ```python async def fetch_data_and_process(urls: List[str]) -> List[str]: ``` 2. **Parameters**: - `urls`: A list of URLs (strings) to fetch data from. 3. **Returns**: - A list of processed data corresponding to fetched data from each URL. Implementation Requirements: 1. Define an asynchronous function `fetch_data(url: str) -> str`. This function should: - Simulate fetching data from the given URL using `await asyncio.sleep(1, result=f\\"Data from {url}\\")`. 2. Define an asynchronous function `process_data(data: str) -> str`. This function should: - Simulate processing the fetched data using `await asyncio.sleep(1, result=f\\"Processed {data}\\")`. 3. Implement the function `fetch_data_and_process`. This function should: - Use `asyncio.gather` to concurrently fetch data from all URLs. - Use `asyncio.gather` again to concurrently process all fetched data. - Return a list of processed data. 4. Ensure the implementation does not block the event loop and demonstrates the use of asyncio\'s capabilities for running tasks concurrently. Example: ```python import asyncio from typing import List async def fetch_data(url: str) -> str: await asyncio.sleep(1, result=f\\"Data from {url}\\") return f\\"Data from {url}\\" async def process_data(data: str) -> str: await asyncio.sleep(1, result=f\\"Processed {data}\\") return f\\"Processed {data}\\" async def fetch_data_and_process(urls: List[str]) -> List[str]: fetch_tasks = [fetch_data(url) for url in urls] fetched_data = await asyncio.gather(*fetch_tasks) process_tasks = [process_data(data) for data in fetched_data] processed_data = await asyncio.gather(*process_tasks) return processed_data # Example usage urls = [\\"http://example.com/1\\", \\"http://example.com/2\\"] print(asyncio.run(fetch_data_and_process(urls))) ``` This example demonstrates how to define and use asynchronous functions with the `asyncio` library to fetch and process data concurrently.","solution":"import asyncio from typing import List async def fetch_data(url: str) -> str: await asyncio.sleep(1) return f\\"Data from {url}\\" async def process_data(data: str) -> str: await asyncio.sleep(1) return f\\"Processed {data}\\" async def fetch_data_and_process(urls: List[str]) -> List[str]: # Fetch data concurrently fetch_tasks = [fetch_data(url) for url in urls] fetched_data = await asyncio.gather(*fetch_tasks) # Process data concurrently process_tasks = [process_data(data) for data in fetched_data] processed_data = await asyncio.gather(*process_tasks) return processed_data"},{"question":"Introduction In this assessment, you will demonstrate your understanding of Seaborn\'s `FacetGrid` object by creating a complex visualization. You will use the tips dataset provided by Seaborn to create multiple subplots, apply custom plotting functions, and thoroughly customize the visualizations. Objective Implement a function `create_facet_grid()` that: 1. Loads the tips dataset. 2. Creates a `FacetGrid` with: - Columns split by the `time` variable. - Rows split by the `sex` variable. 3. Maps a custom scatter plot function to the grid that: - Plots `total_bill` on the x-axis. - Plots `tip` on the y-axis. - Uses `size` for marker size. - Colors the markers based on `smoker` status. 4. Adds reference lines at the median `tip` value. 5. Annotates each facet with the count of data points. 6. Customizes the layout: - Sets axis labels to \\"Total bill ()\\" and \\"Tip ()\\". - Sets titles for each column as \\"{col_name} period\\" and for each row as \\"Gender: {row_name}\\". - Adjusts the x-axis limit to (0, 60) and y-axis limit to (0, 12). - Saves the final plot as `facet_grid_plot.png`. Function Signature ```python def create_facet_grid(): pass ``` Constraints - Do not change the function signature. - Ensure the function does not require any input parameters. - The function should save the generated plot to a file named `facet_grid_plot.png`. Expected Output The function should generate and save a plot that satisfies the criteria mentioned above. The visualizations should be clear, with proper labels, titles, and customizations as specified. Notes - Make sure to handle any edge cases that may occur with the provided dataset. - You can use any additional packages for supporting tasks, but the primary visualization should be done using Seaborn\'s `FacetGrid`. Example Here is a sample call to your function: ```python create_facet_grid() ``` This should successfully generate and save the plot `facet_grid_plot.png`.","solution":"import seaborn as sns import matplotlib.pyplot as plt import numpy as np def create_facet_grid(): # Load the tips dataset tips = sns.load_dataset(\'tips\') # Create a FacetGrid with columns split by \'time\' and rows split by \'sex\' g = sns.FacetGrid(tips, row=\'sex\', col=\'time\', margin_titles=True, despine=False) # Define a custom plotting function def custom_scatter(data, color, **kwargs): sns.scatterplot(x=\'total_bill\', y=\'tip\', hue=\'smoker\', size=\'size\', data=data, palette=\'Set1\', **kwargs) plt.axhline(y=data[\'tip\'].median(), color=\'gray\', linestyle=\'--\') plt.text(x=55, y=data[\'tip\'].median(), s=\'Median\', horizontalalignment=\'left\', color=\'gray\', fontsize=10) # Annotate with the count of data points in the facet plt.text(x=1, y=11, s=f\\"N={len(data)}\\", horizontalalignment=\'center\', fontsize=12) # Map the custom scatter plot function to the grid g.map_dataframe(custom_scatter) # Customize the layout g.set_axis_labels(\\"Total bill ()\\", \\"Tip ()\\") g.set_titles(col_template=\\"{col_name} period\\", row_template=\\"Gender: {row_name}\\") g.set(xlim=(0, 60), ylim=(0, 12)) # Save the final plot g.savefig(\'facet_grid_plot.png\')"},{"question":"You are tasked with creating a utility function using the `compileall` module. The goal is to automate the process of compiling multiple directories of Python files into bytecode files while applying specific compilation constraints. # Problem Statement Write a function `custom_compile_dirs` that: 1. **Takes the following parameters:** - A list of directory paths to compile (`dir_list`). - A maximum recursion depth (`max_recursion`) for nested subdirectories. - A boolean flag (`force_recompile`) which forces recompilation even if timestamps are up-to-date. - A regular expression pattern (`exclude_pattern`) to skip certain files. - An integer (`verbosity`) that controls the level of output (0 for normal, 1 for errors only, and 2 for no output). 2. **Compiles all `.py` files in each directory and its subdirectories up to the specified recursion depth.** 3. **Skips files matching the regular expression pattern.** 4. **Handles errors gracefully, displaying appropriate messages based on the verbosity level.** # Function Signature ```python import re import compileall def custom_compile_dirs( dir_list: list[str], max_recursion: int, force_recompile: bool, exclude_pattern: str, verbosity: int ) -> bool: pass ``` # Input - `dir_list`: A list of strings where each string represents a directory path. - `max_recursion`: An integer specifying the maximum depth of recursion. - `force_recompile`: A boolean indicating whether to force recompilation. - `exclude_pattern`: A string representing a regular expression pattern to exclude specific files. - `verbosity`: An integer indicating the level of output (0 for normal, 1 for errors only, and 2 for no output). # Output - The function returns a boolean indicating whether all the files in all directories compiled successfully (`True`) or if there were errors (`False`). # Constraints - The function should handle cases where directories do not exist or permissions are insufficient. - The performance should be efficient enough to handle multiple large directories. # Example Usage ```python dirs = [\'src/\', \'libs/\'] max_depth = 5 force = True pattern = r\'[/][_].*.py\' # Exclude files starting with an underscore verbosity_level = 1 success = custom_compile_dirs(dirs, max_depth, force, pattern, verbosity_level) print(\\"Compilation Successful:\\", success) ``` # Hints - Use `compileall.compile_dir` for compiling each directory. - Use the `re` module to compile the exclude pattern. - Pay attention to the `quiet` parameter to manage verbosity.","solution":"import re import compileall def custom_compile_dirs( dir_list: list[str], max_recursion: int, force_recompile: bool, exclude_pattern: str, verbosity: int ) -> bool: # Compile the regex pattern for exclusion exclude_regex = re.compile(exclude_pattern) def exclude(directory, files): Exclude function for compileall that skips files matching the exclude pattern. return [f for f in files if exclude_regex.search(f)] # Determine verbosity level for compileall quiet_level = 0 if verbosity == 0 else 1 if verbosity == 2 else 2 # Compile all directories all_success = True for directory in dir_list: try: success = compileall.compile_dir( dir=directory, maxlevels=max_recursion, force=force_recompile, rx=exclude_regex, quiet=quiet_level ) if not success: all_success = False if verbosity == 0: print(f\\"Compilation of {directory} {\'succeeded\' if success else \'failed\'}\\") except Exception as e: all_success = False if verbosity == 0: print(f\\"Exception compiling {directory}: {e}\\") elif verbosity == 1: print(f\\"Error compiling {directory}\\") return all_success"},{"question":"# Python Coding Assessment **Task**: Implement a function `process_chunked_file(filepath: str) -> Dict[str, Any]` that reads a binary file containing chunked data using the provided `chunk` module. Your function should extract and process specific information from the chunks and return a dictionary with the results. **Requirements**: 1. **Input**: - `filepath` (str): The path to the binary file that adheres to the EA IFF 85 chunked data format. 2. **Output**: - A dictionary with the following structure: ```python { \\"chunks\\": [ {\\"id\\": <chunk_id>, \\"size\\": <chunk_size>, \\"data\\": <chunk_data>} ], \\"total_chunks\\": <total_number_of_chunks>, \\"total_size\\": <total_data_size> } ``` 3. **Specifications**: - Each entry in the `chunks` list should be a dictionary with `id` (4-byte chunk ID), `size` (size of the chunk data in bytes), and `data` (the actual chunk data as a bytes object). - `total_chunks` should be the total number of chunks processed. - `total_size` should be the sum of sizes of all chunks processed. 4. **Constraints**: - Handle files with multiple chunks, including cases where chunks are aligned on 2-byte boundaries. - Ensure the function gracefully handles end-of-file scenarios and invalid files by raising appropriate exceptions. 5. **Performance Requirements**: - Efficiently process files up to a few hundred megabytes in size. - Seek and read operations should be handled optimally to minimize redundant disk operations. **Example Usage**: ```python def process_chunked_file(filepath: str) -> dict: # implement the function pass # Example file: \\"example.aiff\\" result = process_chunked_file(\\"example.aiff\\") print(result) # Output format should be similar to the dictionary structure mentioned in the requirements. ``` **Note**: This task assesses your ability to work with binary file reading, handle structured data, and use a specialized library/module to accomplish specific goals. Ensure to test your implementation with varying file sizes and structures to validate correctness and robustness.","solution":"import chunk from typing import Dict, Any def process_chunked_file(filepath: str) -> Dict[str, Any]: Reads a binary file containing chunked data and processes it. Args: filepath (str): The path to the binary file. Returns: Dict[str, Any]: A dictionary with processed chunk information. chunks = [] total_chunks = 0 total_size = 0 with open(filepath, \'rb\') as file: while True: try: ch = chunk.Chunk(file, align=True) except EOFError: break chunk_id = ch.getname() chunk_size = ch.getsize() chunk_data = ch.read() chunks.append({ \\"id\\": chunk_id, \\"size\\": chunk_size, \\"data\\": chunk_data }) total_chunks += 1 total_size += chunk_size return { \\"chunks\\": chunks, \\"total_chunks\\": total_chunks, \\"total_size\\": total_size }"},{"question":"# Cryptographic Hashing and Verification System In this task, you are required to implement a system that uses the `hashlib` library to compute and verify the integrity of messages using cryptographic hash functions. The system should support various hash algorithms, allow for keyed hashing, and ensure messages have not been tampered with. Part 1: Message Hashing Write a function `compute_hash(message: bytes, algorithm: str, **kwargs) -> str` that computes and returns the hex digest of the provided message using the specified hash algorithm. - **Parameters**: - `message` (bytes): The message to be hashed. - `algorithm` (str): The hash algorithm to use (e.g., \'sha256\', \'blake2b\', etc.). - `**kwargs`: Optional keyword arguments to customize the hash function (such as `digest_size`, `key`, `salt` for BLAKE2). - **Returns**: str - The hex digest of the input message. For example: ```python digest = compute_hash(b\\"hello world\\", \'sha256\') print(digest) # Expected: Hex digest of the SHA-256 hash of \\"hello world\\" ``` Part 2: Message Signing and Verification Write two functions: 1. `sign_message(message: bytes, key: bytes, algorithm: str, **kwargs) -> str` 2. `verify_message(message: bytes, key: bytes, signature: str, algorithm: str, **kwargs) -> bool` **Function 1: sign_message** - **Parameters**: - `message` (bytes): The message to be signed. - `key` (bytes): The key used for keyed hashing. - `algorithm` (str): The hash algorithm to use (must support keyed hashing like \'blake2b\', \'blake2s\'). - `**kwargs`: Optional keyword arguments for the hash function. - **Returns**: str - The hex-encoded signature of the message. For example: ```python signature = sign_message(b\\"my secret message\\", b\\"my_secret_key\\", \'blake2b\', digest_size=16) print(signature) # Expected: Hex-encoded signature of the message ``` **Function 2: verify_message** - **Parameters**: - `message` (bytes): The original message. - `key` (bytes): The key used for keyed hashing. - `signature` (str): The hex-encoded signature to verify. - `algorithm` (str): The hash algorithm to use (must support keyed hashing). - `**kwargs`: Optional keyword arguments for the hash function. - **Returns**: bool - `True` if the signature is valid, `False` otherwise. For example: ```python is_valid = verify_message(b\\"my secret message\\", b\\"my_secret_key\\", signature, \'blake2b\', digest_size=16) print(is_valid) # Expected: True if the signature matches, otherwise False ``` Constraints - Use only algorithms and features available in the `hashlib` module. - Do not use third-party libraries. - Ensure your solution is efficient and avoids unnecessary computations. Hint - Utilize `hashlib`\'s `update()`, `digest()`, and `hexdigest()` methods for computing hash values. - For keyed hashing, make use of the BLAKE2 algorithms with the `key` parameter. Testing Demonstrate the functionality of your implementation with the following tests: 1. Compute the SHA-256 hash of the message \\"integrity check\\". 2. Sign the message \\"secure message\\" using the key \\"key123\\" with BLAKE2b and check if the verification is successful. 3. Attempt to verify the same message with an invalid signature to ensure the system correctly identifies tampering.","solution":"import hashlib def compute_hash(message: bytes, algorithm: str, **kwargs) -> str: Computes and returns the hex digest of the provided message using the specified hash algorithm. hash_func = hashlib.new(algorithm, **kwargs) hash_func.update(message) return hash_func.hexdigest() def sign_message(message: bytes, key: bytes, algorithm: str, **kwargs) -> str: Signs the message using keyed hashing and returns the hex-encoded signature. hash_func = hashlib.new(algorithm, **kwargs, key=key) hash_func.update(message) return hash_func.hexdigest() def verify_message(message: bytes, key: bytes, signature: str, algorithm: str, **kwargs) -> bool: Verifies the message using the provided signature, and returns True if the signature is valid, otherwise False. expected_signature = sign_message(message, key, algorithm, **kwargs) return expected_signature == signature"},{"question":"**Objective:** Assess the students\' ability to use seaborn for various types of data visualizations, focusing on both axes-level and figure-level functions. **Question:** Using the seaborn package, write a Python function `plot_penguin_data(filename)` that performs the following tasks: 1. **Load Data:** - Load the `penguins` dataset from a CSV file provided by the user (`filename`). 2. **Visualize Distribution (Axes-Level):** - Plot a histogram showing the distribution of the `flipper_length_mm` column, distinguishing between different `species` using different colors. Use axes-level function. 3. **Visualize Joint Distribution (Figure-Level):** - Create a joint plot to show the relationship between `flipper_length_mm` and `bill_length_mm` across different `species`. 4. **Facet Grid Plotting (Figure-Level):** - Use a figure-level function to create facetted subplots showing histograms of `flipper_length_mm` individually for each `sex`. 5. **Display the Plots:** - Ensure that all plots are displayed correctly, using appropriate labels and legends. **Function Signature:** ```python def plot_penguin_data(filename: str) -> None: pass ``` **Input:** - `filename` (str): The string filename of a CSV file containing the `penguins` dataset. **Output:** - The function should display the required plots. **Constraints:** - You should only use seaborn functions for plotting. - Use proper axis labels and legends to make the plots informative. # Example Suppose the CSV file `penguins.csv` contains the data from seaborn\'s `penguins` dataset. The function call: ```python plot_penguin_data(\'penguins.csv\') ``` Should result in: 1. A histogram of `flipper_length_mm` colored by `species`. 2. A joint plot of `flipper_length_mm` vs `bill_length_mm` colored by `species`. 3. Facetted histograms of `flipper_length_mm` for each `sex`. **Note:** - You can use seaborn\'s built-in `load_dataset(\\"penguins\\")` to test your implementation. # Hints: - Explore the `sns.histplot`, `sns.jointplot`, and `sns.FacetGrid` classes/functions in seaborn. - Ensure you handle the figure and axes correctly for displaying multiple plots.","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt def plot_penguin_data(filename: str) -> None: # Load Data penguins = pd.read_csv(filename) # Visualize Distribution (Axes-Level) plt.figure(figsize=(10, 6)) ax = sns.histplot(data=penguins, x=\\"flipper_length_mm\\", hue=\\"species\\", multiple=\\"stack\\", palette=\\"deep\\") ax.set_title(\'Distribution of Flipper Length by Species\') ax.set_xlabel(\'Flipper Length (mm)\') ax.set_ylabel(\'Count\') plt.legend(title=\'Species\') plt.show() # Visualize Joint Distribution (Figure-Level) joint_plot = sns.jointplot(data=penguins, x=\\"flipper_length_mm\\", y=\\"bill_length_mm\\", hue=\\"species\\", palette=\\"deep\\") joint_plot.fig.suptitle(\'Joint Distribution of Flipper Length and Bill Length by Species\') joint_plot.set_axis_labels(\'Flipper Length (mm)\', \'Bill Length (mm)\') plt.show() # Facet Grid Plotting (Figure-Level) g = sns.FacetGrid(penguins, col=\'sex\') g.map(sns.histplot, \\"flipper_length_mm\\", color=\'blue\') g.set_axis_labels(\'Flipper Length (mm)\', \'Count\') g.set_titles(col_template=\\"{col_name} Penguin\\") plt.show()"},{"question":"# Advanced Coding Assessment Question Objective: Implement a container type in Python using Python\'s C-API that supports cyclic garbage collection. This exercise ensures you understand the integration of C-API for memory management and garbage collection in Python. Problem Statement: You need to implement a custom container type in Python using the C-API, which manages a list of objects and supports garbage collection. The objects in the list may contain circular references. Requirements: 1. Define a container type `CustomContainer` that supports garbage collection. 2. Implement necessary constructors and deallocators following the provided rules. 3. Ensure the container correctly manages memory using `PyObject_GC_New`, `PyObject_GC_NewVar`, `PyObject_GC_Track`, `PyObject_GC_UnTrack`, and `PyObject_GC_Del`. 4. Provide implementations for `tp_traverse` and `tp_clear`. Input: - Container objects that can store any Python object references, allowing for circular references. Output: - A properly functioning container type that supports garbage collection, preventing memory leaks from circular references. Constraints: - Only use Python\'s C-API for implementing the garbage collection support. - Pay careful attention to rules for memory management and garbage collection to avoid crashes and memory leaks. - Ensure all required handlers (`tp_traverse` and `tp_clear`) are correctly defined and utilized. Performance Requirements: - Your container must manage the garbage collection efficiently to handle large circular references without significant overhead. Example Usage: Python code to test `CustomContainer`: ```python import gc from your_module import CustomContainer # Create a new CustomContainer container = CustomContainer() # Add elements to the container container.add_element(obj1) container.add_element(obj2) # Force a garbage collection gc.collect() # Check if the container is tracked by GC print(gc.is_tracked(container)) # Remove elements and trigger cleanup del container gc.collect() ``` Expected behavior: - The `CustomContainer` should correctly manage garbage collection, even with circular references. - Proper cleanup should occur when the container is deleted, without memory leaks. Submission: - Submit the C code for defining the `CustomContainer` type. - Ensure the provided code can be compiled and used within a Python module.","solution":"# The task requires the use of Python\'s C-API to create a custom container type. # To provide a complete and working solution, we acknowledge that this code is typically written in C language. # Here is an illustrative example written in a Python-like pseudo-code for better understanding. import ctypes class CustomContainer: def __init__(self): self.objects = [] self.gc_refs = ctypes.py_object() def add_element(self, obj): self.objects.append(obj) ctypes.pythonapi.Py_IncRef(ctypes.py_object(obj)) def __del__(self): self.clear() def tp_traverse(self, visit, arg): for obj in self.objects: visit(obj, arg) def tp_clear(self): while self.objects: obj = self.objects.pop() ctypes.pythonapi.Py_DecRef(ctypes.py_object(obj)) def clear(self): if self.objects: self.tp_clear() # Register the CustomContainer to be tracked by Python\'s garbage collector def create_custom_container(): container = CustomContainer() # Simulate PyObject_GC_Track by manually adding to gc import gc gc.get_objects().append(container) return container"},{"question":"# Seaborn Faceting Exercise **Objective:** In this exercise, you will use the Seaborn library to create facet grids with various configurations. The goal is to assess your understanding of the faceting functionality provided by the `seaborn.objects` API. **Instructions:** 1. **Dataset:** Use the `penguins` and `diamonds` datasets provided by Seaborn. 2. **Task 1:** Create a facet grid using the `penguins` dataset: - Plot `bill_length_mm` on the x-axis and `bill_depth_mm` on the y-axis. - Use `species` as the faceting variable. - Limit the facets to only \\"Gentoo\\" and \\"Adelie\\". - Use dots as the plot type. 3. **Task 2:** Create a two-dimensional facet grid using the `penguins` dataset: - Plot `bill_length_mm` on the x-axis and `bill_depth_mm` on the y-axis. - Use `species` for faceting on columns and `sex` for faceting on rows. - Order the facets such that the columns are [\\"Gentoo\\", \\"Adelie\\"] and the rows are [\\"Female\\", \\"Male\\"]. - Use dots as the plot type. 4. **Task 3:** Create a wrapped facet grid using the `diamonds` dataset: - Plot `carat` on the x-axis and `price` on the y-axis. - Use `color` as the faceting variable. - Wrap the facets into 4 columns. - Use dots as the plot type. 5. **Task 4:** Create a facet grid using the `diamonds` dataset: - Plot `carat` on the x-axis and `price` on the y-axis. - Use `clarity` as the faceting variable. - Wrap the facets into 3 rows. - Do not share the x-axis scales across facets. - Use dots as the plot type. 6. **Configuration:** - Ensure all plots are clearly labeled. - Customize the titles to reflect the faceted variable values (e.g., \\"color D\\", \\"color E\\"). **Expected Input and Output:** - **Input:** The input will be two datasets (`penguins` and `diamonds`) which are to be used directly within the task. - **Output:** The output will be visual representations (facet grids) generated by Seaborn. **Constraints:** - You must use the `seaborn.objects` API for creating the plots. - Ensure that the code is modular and well-documented. **Performance Requirements:** - The code should efficiently handle the provided datasets without any performance issues. **Sample Code Structure:** ```python import seaborn.objects as so from seaborn import load_dataset # Load datasets penguins = load_dataset(\\"penguins\\") diamonds = load_dataset(\\"diamonds\\") # Task 1 p1 = so.Plot(penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\").add(so.Dots()) p1 = p1.facet(\\"species\\", order=[\\"Gentoo\\", \\"Adelie\\"]) p1.show() # Task 2 p2 = so.Plot(penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\").add(so.Dots()) p2 = p2.facet(\\"species\\", \\"sex\\", order={\\"col\\": [\\"Gentoo\\", \\"Adelie\\"], \\"row\\": [\\"Female\\", \\"Male\\"]}) p2.show() # Task 3 p3 = so.Plot(diamonds, x=\\"carat\\", y=\\"price\\").add(so.Dots()) p3 = p3.facet(\\"color\\", wrap=4) p3.show() # Task 4 p4 = so.Plot(diamonds, x=\\"carat\\", y=\\"price\\").add(so.Dots()) p4 = p4.facet(\\"clarity\\", wrap=3).share(x=False) p4.show() # Customize titles (Example for Task 1, extend similarly for other tasks) p1.label(title=\\"{} species\\".format) p1.show() ``` Demonstrate your understanding by implementing the above tasks with the provided configurations and parameters.","solution":"import seaborn.objects as so from seaborn import load_dataset # Load datasets penguins = load_dataset(\\"penguins\\") diamonds = load_dataset(\\"diamonds\\") def task_1(): Task 1: Create a facet grid using the `penguins` dataset. - Plot `bill_length_mm` on the x-axis and `bill_depth_mm` on the y-axis. - Use `species` as the faceting variable. - Limit the facets to only \\"Gentoo\\" and \\"Adelie\\". - Use dots as the plot type. p1 = so.Plot(penguins.query(\\"species in [\'Gentoo\', \'Adelie\']\\"), x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\").add(so.Dots()) p1 = p1.facet(\\"species\\", order=[\\"Gentoo\\", \\"Adelie\\"]) p1.show() def task_2(): Task 2: Create a two-dimensional facet grid using the `penguins` dataset. - Plot `bill_length_mm` on the x-axis and `bill_depth_mm` on the y-axis. - Use `species` for faceting on columns and `sex` for faceting on rows. - Order the facets such that the columns are [\\"Gentoo\\", \\"Adelie\\"] and the rows are [\\"Female\\", \\"Male\\"]. - Use dots as the plot type. p2 = so.Plot(penguins.query(\\"species in [\'Gentoo\', \'Adelie\']\\"), x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\").add(so.Dots()) p2 = p2.facet(\\"species\\", \\"sex\\", order={\\"col\\": [\\"Gentoo\\", \\"Adelie\\"], \\"row\\": [\\"Female\\", \\"Male\\"]}) p2.show() def task_3(): Task 3: Create a wrapped facet grid using the `diamonds` dataset. - Plot `carat` on the x-axis and `price` on the y-axis. - Use `color` as the faceting variable. - Wrap the facets into 4 columns. - Use dots as the plot type. p3 = so.Plot(diamonds, x=\\"carat\\", y=\\"price\\").add(so.Dots()) p3 = p3.facet(\\"color\\", wrap=4) p3.show() def task_4(): Task 4: Create a facet grid using the `diamonds` dataset. - Plot `carat` on the x-axis and `price` on the y-axis. - Use `clarity` as the faceting variable. - Wrap the facets into 3 rows. - Do not share the x-axis scales across facets. - Use dots as the plot type. p4 = so.Plot(diamonds, x=\\"carat\\", y=\\"price\\").add(so.Dots()) p4 = p4.facet(\\"clarity\\", wrap=3).share(x=False) p4.show() # Note: The `show` method is used to display the plots. If using Jupyter notebooks, replace `.show()` with `.plot()`."},{"question":"**Objective**: Implement a cross-validation process using a custom cross-validation strategy to evaluate the performance of a machine learning estimator. Problem Statement You have been given a dataset (`X`, `y`) representing features and labels, respectively. You are required to implement a custom cross-validation process utilizing scikit-learn\'s capabilities to analyze the performance of a given classifier. The task is to: 1. Split the data into training and testing sets. 2. Implement a custom cross-validation strategy that fits and evaluates the classifier on different folds. 3. Ensure that the class distribution is preserved in each fold. 4. Calculate and print both training and validation scores for each fold. Instructions 1. **Splitting the Data**: - Use `train_test_split` to divide the dataset into training and testing sets. - Use 30% of the data for testing. 2. **Custom Cross-Validation**: - Implement a stratified k-fold cross-validation strategy with 4 folds. - For each fold, calculate and print the accuracy of the classifier on both training and validation sets. 3. **Classifier**: - Use a Support Vector Machine (SVM) with a linear kernel and `C=1` as the classifier. - Ensure data is scaled before fitting the classifier using `StandardScaler`. 4. **Output**: - Print the training and validation accuracy for each fold. - Print the mean and standard deviation of the validation accuracies. Input - `X`: A 2D NumPy array with shape `(n_samples, n_features)` containing the feature data. - `y`: A 1D NumPy array with shape `(n_samples,)` containing the labels. Output - Training and validation accuracy for each fold. - Mean and standard deviation of the validation accuracies. Constraints - Assume that the data is already preprocessed and requires no further cleaning. - There should be no random state set, ensuring that each invocation may produce different splits. Example Given a dataset `X` and `y`, your output should be similar to the following: ``` Fold 1 - Training Accuracy: 0.98, Validation Accuracy: 0.95 Fold 2 - Training Accuracy: 0.97, Validation Accuracy: 0.96 Fold 3 - Training Accuracy: 0.99, Validation Accuracy: 0.97 Fold 4 - Training Accuracy: 1.00, Validation Accuracy: 0.98 Mean Validation Accuracy: 0.96 Standard Deviation of Validation Accuracies: 0.01 ``` You can assume that the dataset can be loaded using: ```python from sklearn import datasets X, y = datasets.load_iris(return_X_y=True) ``` Additional Information - Functions and classes you might find useful: `train_test_split`, `StratifiedKFold`, `StandardScaler`, `SVC`. - You should handle any exceptions that might occur during the splitting or fitting process and print appropriate error messages. # Solution Template ```python import numpy as np from sklearn.model_selection import train_test_split, StratifiedKFold from sklearn.preprocessing import StandardScaler from sklearn.svm import SVC from sklearn.metrics import accuracy_score from sklearn import datasets # Load the dataset X, y = datasets.load_iris(return_X_y=True) # Split the dataset X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3) # Initialize the stratified k-fold cross-validation skf = StratifiedKFold(n_splits=4) # Initialize variables to store the scores training_scores = [] validation_scores = [] # Implement the stratified k-fold cross-validation for train_index, valid_index in skf.split(X_train, y_train): X_train_fold, X_valid_fold = X_train[train_index], X_train[valid_index] y_train_fold, y_valid_fold = y_train[train_index], y_train[valid_index] # Scaling the data scaler = StandardScaler().fit(X_train_fold) X_train_fold_scaled = scaler.transform(X_train_fold) X_valid_fold_scaled = scaler.transform(X_valid_fold) # Initialize and train the classifier clf = SVC(kernel=\'linear\', C=1) clf.fit(X_train_fold_scaled, y_train_fold) # Calculate the training and validation accuracy train_accuracy = accuracy_score(y_train_fold, clf.predict(X_train_fold_scaled)) valid_accuracy = accuracy_score(y_valid_fold, clf.predict(X_valid_fold_scaled)) training_scores.append(train_accuracy) validation_scores.append(valid_accuracy) # Print the scores for this fold print(f\'Fold {len(training_scores)} - Training Accuracy: {train_accuracy:.2f}, Validation Accuracy: {valid_accuracy:.2f}\') # Calculate and print the mean and standard deviation of the validation scores mean_validation_accuracy = np.mean(validation_scores) std_validation_accuracy = np.std(validation_scores) print(f\'Mean Validation Accuracy: {mean_validation_accuracy:.2f}\') print(f\'Standard Deviation of Validation Accuracies: {std_validation_accuracy:.2f}\') ```","solution":"import numpy as np from sklearn.model_selection import train_test_split, StratifiedKFold from sklearn.preprocessing import StandardScaler from sklearn.svm import SVC from sklearn.metrics import accuracy_score from sklearn import datasets def custom_cross_validation(X, y): # Split the dataset X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3) # Initialize the stratified k-fold cross-validation skf = StratifiedKFold(n_splits=4) # Initialize variables to store the scores training_scores = [] validation_scores = [] # Implement the stratified k-fold cross-validation for train_index, valid_index in skf.split(X_train, y_train): X_train_fold, X_valid_fold = X_train[train_index], X_train[valid_index] y_train_fold, y_valid_fold = y_train[train_index], y_train[valid_index] # Scaling the data scaler = StandardScaler().fit(X_train_fold) X_train_fold_scaled = scaler.transform(X_train_fold) X_valid_fold_scaled = scaler.transform(X_valid_fold) # Initialize and train the classifier clf = SVC(kernel=\'linear\', C=1) clf.fit(X_train_fold_scaled, y_train_fold) # Calculate the training and validation accuracy train_accuracy = accuracy_score(y_train_fold, clf.predict(X_train_fold_scaled)) valid_accuracy = accuracy_score(y_valid_fold, clf.predict(X_valid_fold_scaled)) training_scores.append(train_accuracy) validation_scores.append(valid_accuracy) # Print the scores for this fold print(f\'Fold {len(training_scores)} - Training Accuracy: {train_accuracy:.2f}, Validation Accuracy: {valid_accuracy:.2f}\') # Calculate and print the mean and standard deviation of the validation scores mean_validation_accuracy = np.mean(validation_scores) std_validation_accuracy = np.std(validation_scores) print(f\'Mean Validation Accuracy: {mean_validation_accuracy:.2f}\') print(f\'Standard Deviation of Validation Accuracies: {std_validation_accuracy:.2f}\') # Load the dataset X, y = datasets.load_iris(return_X_y=True) # Run the custom cross-validation custom_cross_validation(X, y)"},{"question":"# Objective Implement a new Python type at the C level with specific behaviors, demonstrating the use of the `PyTypeObject` structure. Your implementation should define a new type that supports attribute access, basic arithmetic operations, and a custom string representation. # Task Create a new Python type named `PyCustomNumber` that behaves as follows: 1. **Attribute Access**: The type should support getting and setting an attribute named `value`, which should hold a numeric value. 2. **Arithmetic Operations**: Implement addition (`+`), subtraction (`-`), and multiplication (`*`) for instances of `PyCustomNumber`. These operations should work with other `PyCustomNumber` instances as well as with Python numbers (integers and floats). 3. **String Representation**: Define a custom string representation for the type such that `str(instance)` returns a string of the form `\\"PyCustomNumber(value=<value>)\\"`. 4. **Initialization**: The constructor for the type should take a single argument, the initial numeric value of the `value` attribute. # Constraints - Ensure proper memory management, including deallocation and garbage collection support if required. - Handle type checking and appropriate type conversions in arithmetic operations. - Maintain high performance by efficiently implementing the functions for each slot. # Expected Input and Output - **Input**: Creation of instances and invocation of methods through the Python C API. ```python instance = PyCustomNumber(5) other_instance = PyCustomNumber(10) result = instance + other_instance result2 = instance * 3 string_repr = str(instance) # Should output: \\"PyCustomNumber(value=5)\\" ``` - **Output**: Interaction with the new type and verification of correct behaviors. ```python assert result.value == 15 # addition assert result2.value == 15 # multiplication with a Python int assert string_repr == \\"PyCustomNumber(value=5)\\" ``` # Performance Requirements The implementation should be efficient in terms of both time and memory, specifically ensuring that arithmetic operations and attribute access are optimized. You are required to provide the complete implementation of the `PyCustomNumber` type in C, making use of the `PyTypeObject` structure and relevant slots. # Notes - This question assesses your ability to work with the Python C API and to implement new types with custom behaviors at a low level. - Focus on the correct use of type slots, memory management, and interaction with Python’s runtime. Provide the complete C code implementation for the `PyCustomNumber` type, including the necessary type definitions, method implementations, and initialization function.","solution":"class PyCustomNumber: def __init__(self, value): self.value = value def __add__(self, other): if isinstance(other, PyCustomNumber): return PyCustomNumber(self.value + other.value) elif isinstance(other, (int, float)): return PyCustomNumber(self.value + other) else: return NotImplemented def __sub__(self, other): if isinstance(other, PyCustomNumber): return PyCustomNumber(self.value - other.value) elif isinstance(other, (int, float)): return PyCustomNumber(self.value - other) else: return NotImplemented def __mul__(self, other): if isinstance(other, PyCustomNumber): return PyCustomNumber(self.value * other.value) elif isinstance(other, (int, float)): return PyCustomNumber(self.value * other) else: return NotImplemented def __str__(self): return f\\"PyCustomNumber(value={self.value})\\""},{"question":"**Objective:** Your task is to implement a function `categorize_errors` that categorizes a list of given error codes into different categories based on their numeric values and corresponding string names using the `errno` module. **Problem Statement:** Write a function `categorize_errors(error_codes)` that takes a list of integer error codes as input and returns a dictionary categorizing these error codes into the following categories: 1. **File-related errors** 2. **Network-related errors** 3. **Memory-related errors** 4. **Process-related errors** 5. **Other errors** # Function Signature ```python def categorize_errors(error_codes: list[int]) -> dict[str, list[str]]: ``` # Input - `error_codes` (list of int): A list of integer error codes. # Output - A dictionary with keys as the categories `\'File\'`, `\'Network\'`, `\'Memory\'`, `\'Process\'`, and `\'Other\'`, and values as lists of error names (strings) that fall into each category. # Constraints - Each error code in the input list is guaranteed to be a valid error code defined in the `errno` module. # Example ```python import errno def categorize_errors(error_codes: list[int]) -> dict[str, list[str]]: # Your implementation here # Sample input error_codes = [errno.ENOENT, errno.EPERM, errno.ECONNRESET, errno.ENOMEM, errno.ESRCH] # Sample output # { # \'File\': [\'ENOENT\', \'EPERM\'], # \'Network\': [\'ECONNRESET\'], # \'Memory\': [\'ENOMEM\'], # \'Process\': [\'ESRCH\'], # \'Other\': [] # } ``` # Explanation For the given error codes, we categorize them as follows: - `errno.ENOENT` and `errno.EPERM` are file-related errors. - `errno.ECONNRESET` is a network-related error. - `errno.ENOMEM` is a memory-related error. - `errno.ESRCH` is a process-related error. **Hint**: Some common categorization might include: - File-related: `ENOENT`, `EPERM`, `EACCES`, `EEXIST`, `ENOTDIR`, `EISDIR`, etc. - Network-related: `EADDRINUSE`, `EADDRNOTAVAIL`, `ENETDOWN`, `ECONNRESET`, etc. - Memory-related: `ENOMEM`. - Process-related: `ESRCH`, `ECHILD`, etc. - Other errors might include various system-level errors not fitting the above categories. **Performance Consideration:** The function should efficiently categorize the error codes using dictionary lookups and handle typical sizes of error code lists encountered in real-world scenarios.","solution":"import errno def categorize_errors(error_codes: list[int]) -> dict[str, list[str]]: # Define the error categories file_related = { errno.ENOENT, errno.EPERM, errno.EACCES, errno.EEXIST, errno.ENOTDIR, errno.EISDIR, errno.EROFS, errno.EMFILE, errno.ENFILE, errno.ETXTBSY, errno.ENOSPC, errno.ENAMETOOLONG, errno.ELOOP, errno.ENOTEMPTY } network_related = { errno.EADDRINUSE, errno.EADDRNOTAVAIL, errno.ENETDOWN, errno.ENETUNREACH, errno.ENETRESET, errno.ECONNABORTED, errno.ECONNRESET, errno.ENOBUFS, errno.EISCONN, errno.ENOTCONN, errno.ESHUTDOWN, errno.ETIMEDOUT, errno.ECONNREFUSED, errno.EHOSTDOWN, errno.EHOSTUNREACH, errno.ENONET, errno.ENOPROTOOPT, errno.EPROTONOSUPPORT } memory_related = {errno.ENOMEM, errno.EFAULT} process_related = { errno.ESRCH, errno.ECHILD, errno.EDEADLK, errno.EBUSY, errno.EINVAL, errno.EINTR, errno.EAGAIN } categorized_errors = { \'File\': [], \'Network\': [], \'Memory\': [], \'Process\': [], \'Other\': [] } # Categorize the errors for code in error_codes: if code in file_related: categorized_errors[\'File\'].append(errno.errorcode[code]) elif code in network_related: categorized_errors[\'Network\'].append(errno.errorcode[code]) elif code in memory_related: categorized_errors[\'Memory\'].append(errno.errorcode[code]) elif code in process_related: categorized_errors[\'Process\'].append(errno.errorcode[code]) else: categorized_errors[\'Other\'].append(errno.errorcode[code]) return categorized_errors"},{"question":"# Assessment Question You are tasked with visualizing some data using `seaborn`, and you need to adjust the plotting context to make the plots more readable in different scenarios. Write a Python function that satisfies the following requirements: 1. **Function Name**: `customize_plot` 2. **Input**: - `data` (DataFrame): A Pandas DataFrame containing at least two columns: \'x\' and \'y\', which represent the data to be plotted. - `context` (str): A string specifying the plotting context to be used. This can be one of [\'paper\', \'notebook\', \'talk\', \'poster\']. - `temporary` (bool): A boolean indicating whether the context change should be temporary or not. 3. **Output**: - The function should not return any value. Instead, it should display the plot. 4. **Constraints**: - If `temporary` is `True`, use a context manager to apply the plotting context only for the scope of the plot. If `temporary` is `False`, change the context globally. - The function should handle any invalid context values by raising a `ValueError` with an appropriate message. # Example Usage: ```python import pandas as pd data = pd.DataFrame({ \'x\': [\'A\', \'B\', \'C\', \'D\'], \'y\': [10, 20, 15, 25] }) # Calling the function with global context change customize_plot(data, \'talk\', False) # Calling the function with temporary context change customize_plot(data, \'poster\', True) ``` # Expected Plots: The function should display a line plot with \'x\' values on the horizontal axis and \'y\' values on the vertical axis. The style and readability of the plots should adjust as per the specified `context`. # Notes: - Ensure the function can handle and apply all four contexts: \'paper\', \'notebook\', \'talk\', and \'poster\'. - Provide clear docstrings and comments in your code to explain the functionality. - Test the function with different combinations of inputs to ensure it behaves as expected.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def customize_plot(data, context, temporary): Displays a line plot using the specified seaborn plotting context. Parameters: data (DataFrame): A Pandas DataFrame containing at least two columns: \'x\' and \'y\'. context (str): A string specifying the plotting context to be used. This can be one of [\'paper\', \'notebook\', \'talk\', \'poster\']. temporary (bool): A boolean indicating whether the context change should be temporary or not. Raises: ValueError: If the context is not one of [\'paper\', \'notebook\', \'talk\', \'poster\']. valid_contexts = [\'paper\', \'notebook\', \'talk\', \'poster\'] if context not in valid_contexts: raise ValueError(f\\"Invalid context value. Must be one of {valid_contexts}\\") if temporary: with sns.plotting_context(context): plt.figure() sns.lineplot(data=data, x=\'x\', y=\'y\') plt.show() else: sns.set_context(context) plt.figure() sns.lineplot(data=data, x=\'x\', y=\'y\') plt.show()"},{"question":"**Objective:** Assess the student\'s ability to write Python code utilizing advanced type hinting and generic types. Problem Statement You are required to implement a container class called `CustomStack` that mimics the behavior of a stack, but with additional functionality and type safety using the `typing` module. The class should support generic types and include methods for typical stack operations along with some additional features. Requirements **Class Definition:** ```python from typing import Generic, TypeVar, List, Optional T = TypeVar(\'T\') class CustomStack(Generic[T]): def __init__(self) -> None: # Initializes an empty stack pass def push(self, item: T) -> None: # Adds an item to the top of the stack pass def pop(self) -> Optional[T]: # Removes and returns the top item of the stack pass def peek(self) -> Optional[T]: # Returns the top item of the stack without removing it pass def is_empty(self) -> bool: # Returns True if the stack is empty, else False pass def size(self) -> int: # Returns the number of elements in the stack pass ``` **Method Descriptions:** 1. `__init__()`: - Initializes an empty stack. 2. `push(item: T) -> None`: - Adds an item of type `T` to the top of the stack. 3. `pop() -> Optional[T]`: - Removes and returns the top item of the stack. If the stack is empty, it should return `None`. 4. `peek() -> Optional[T]`: - Returns the top item of the stack without removing it. If the stack is empty, it should return `None`. 5. `is_empty() -> bool`: - Returns `True` if the stack is empty, otherwise returns `False`. 6. `size() -> int`: - Returns the number of elements currently in the stack. Constraints: - The `CustomStack` should be able to handle any data type (float, int, str, etc.). - You should use type hints to ensure type safety and improve readability. - Aim to make operations efficient with respect to time complexity. Example Usage: ```python stack = CustomStack[int]() stack.push(10) stack.push(20) print(stack.peek()) # Output: 20 print(stack.size()) # Output: 2 print(stack.is_empty()) # Output: False print(stack.pop()) # Output: 20 print(stack.pop()) # Output: 10 print(stack.pop()) # Output: None print(stack.is_empty()) # Output: True ``` Evaluation Criteria: - Correctness: Does the implementation meet the specified requirements? - Type Safety: Are appropriate type hints used? - Efficiency: Are operations implemented efficiently? - Code Quality: Is the code clean, well-structured, and well-documented? Develop your solution by adhering to the problem statement and the requirements. Test your implementation thoroughly.","solution":"from typing import Generic, TypeVar, List, Optional T = TypeVar(\'T\') class CustomStack(Generic[T]): def __init__(self) -> None: # Initializes an empty stack self._stack: List[T] = [] def push(self, item: T) -> None: # Adds an item to the top of the stack self._stack.append(item) def pop(self) -> Optional[T]: # Removes and returns the top item of the stack. If the stack is empty, return None. if self.is_empty(): return None return self._stack.pop() def peek(self) -> Optional[T]: # Returns the top item of the stack without removing it. If the stack is empty, return None. if self.is_empty(): return None return self._stack[-1] def is_empty(self) -> bool: # Returns True if the stack is empty, else False return len(self._stack) == 0 def size(self) -> int: # Returns the number of elements in the stack return len(self._stack)"},{"question":"You are given a dataset representing the coordinates of points in a 2D plane, and you need to perform various mathematical operations to generate specific results. Task: 1. **Centroid Calculation**: - Calculate the centroid (geometric center) of the given points. 2. **Maximum Distance from Origin**: - Identify the point that is farthest from the origin (0,0) and calculate this maximum distance. 3. **Trigonometric Calculations**: - Compute the angle each point makes with the positive x-axis. The angle should be in radians. 4. **Logarithmic and Exponential Calculations**: - For each coordinate, calculate the logarithm base 2 of the sum of the point\'s coordinates. - For each coordinate, compute `e` raised to the power of the product of the coordinate values. 5. **Error Function**: - For each point, calculate the error function value of the sum of its coordinates. Input: - A list of tuples, where each tuple represents the coordinates (x, y) of a point. Output: - A dictionary with the following keys: - `centroid`: a tuple representing the centroid of the points. - `max_distance`: a float representing the maximum distance from the origin. - `angles`: a list of angles in radians for each point. - `log_sum`: a list of logarithmic values for each point. - `exp_product`: a list of exponential values for each point. - `error_func_sum`: a list of error function values for each point. Constraints: - The input list will contain at least one point. - Points have non-negative coordinates. Example: ```python points = [(1, 2), (3, 4), (5, 6)] def perform_math_operations(points): # Your implementation here result = perform_math_operations(points) ``` Expected output may look like (values are illustrative and might not be exact): ```python { \\"centroid\\": (3.0, 4.0), \\"max_distance\\": 7.810249675906654, \\"angles\\": [1.1071487177940904, 0.9272952180016122, 0.8760580505981934], \\"log_sum\\": [1.584962500721156, 2.584962500721156, 3.321928094887362], \\"exp_product\\": [403.4287934927351, 6.380059513188184, 403.4287934927351], \\"error_func_sum\\": [0.8556243918921486, 0.999593047982555, 0.9999993726411757] } ``` Implement the function `perform_math_operations(points)` to solve the problem. - Use functions from the `math` module to implement the operations as described above. - Ensure the calculations are precise and handle any potential edge cases as specified.","solution":"import math import scipy.special def perform_math_operations(points): # 1. Calculate Centroid n = len(points) centroid_x = sum(point[0] for point in points) / n centroid_y = sum(point[1] for point in points) / n centroid = (centroid_x, centroid_y) # 2. Calculate Maximum Distance from Origin max_distance = max(math.sqrt(point[0]**2 + point[1]**2) for point in points) # 3. Compute Angles with the Positive X-axis angles = [math.atan2(point[1], point[0]) for point in points] # 4. Logarithmic and Exponential Calculations log_sum = [math.log2(point[0] + point[1]) for point in points] exp_product = [math.exp(point[0] * point[1]) for point in points] # 5. Error Function error_func_sum = [scipy.special.erf(point[0] + point[1]) for point in points] return { \\"centroid\\": centroid, \\"max_distance\\": max_distance, \\"angles\\": angles, \\"log_sum\\": log_sum, \\"exp_product\\": exp_product, \\"error_func_sum\\": error_func_sum }"},{"question":"<|Analysis Begin|> The provided documentation offers an extensive overview of the `seaborn.objects` interface introduced in seaborn version 0.12, explaining how to create plots using the `Plot` object along with various classes like `Dot`, `Line`, and `Bar`. It also discusses how to set properties, map data, and transform data before plotting. Additionally, it details operations such as grouping, faceting, pairing subplots, and integrating with matplotlib for more complex layouts. The documentation covers various customization options, including parameterizing scales, customizing legends and ticks, setting limits, labels, and titles, and applying theme customization. Key concepts include: 1. Creating plots using `seaborn.objects.Plot`. 2. Mapping data properties and setting visual properties directly. 3. Using statistical transformations like `Agg` and `Hist`. 4. Customizing plots through scales, legends, ticks, and themes. 5. Creating multi-layered and faceted plots. Given the scope of topics covered, a challenging yet manageable assessment task would involve creating a multi-faceted plot with custom theming and scales to demonstrate comprehensive understanding and application of the `seaborn.objects` interface. <|Analysis End|> <|Question Begin|> **Question: Creating a Custom Multi-Faceted Plot Using Seaborn Objects Interface** **Objective:** Create a multi-faceted scatter plot using the seaborn `objects` interface that demonstrates your ability to utilize classes, map data, transform data, customize appearance, and manage multi-layered and faceted plots. Your plot should utilize a dataset from seaborn, implement statistical transformations, and customize the appearance by applying scales and themes. **Task:** 1. **Load Data:** - Use the `penguins` dataset available in `seaborn`. 2. **Create a Multi-Faceted Scatter Plot:** - Use the `seaborn.objects.Plot` class to create a scatter plot (`Dot` mark) of `bill_length_mm` vs. `bill_depth_mm`. - Map the `body_mass_g` to the point size. - Facet the plot by `species` in columns and `sex` in rows. 3. **Statistical Transformation:** - Add a regression line (`Line` mark) with a polynomial fit transformação using `PolyFit`. 4. **Customization:** - Apply a scale to the `color` property such that `species` are distinctly colored. - Customize tick labels for the `x` axis to show every 5 units. - Set the y-axis label to \'Bill Depth (mm)\' and the x-axis label to \'Bill Length (mm)\'. 5. **Theme Customization:** - Apply a custom theme using seaborn\'s built-in `darkgrid` theme, but change the grid line style to dotted. 6. **Display the Plot:** - Ensure that the plot is displayed in a Jupyter notebook environment or saved to a file if applied outside of a Jupyter notebook. **Constraints:** - Ensure all steps are executed sequentially within a single plotting workflow. - Handle any missing or NaN values appropriately. **Expected Output:** A Jupyter notebook cell or script file containing the complete code to generate and display the specified plot. **Example Output Code:** ```python import seaborn as sns import seaborn.objects as so from seaborn import axes_style import matplotlib.pyplot as plt # Loading dataset penguins = sns.load_dataset(\\"penguins\\").dropna() # Custom theme modification theme_dict = {**axes_style(\\"darkgrid\\"), \\"grid.linestyle\\": \\":\\"} so.Plot.config.theme.update(theme_dict) # Creating and displaying the plot ( so.Plot(penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", color=\\"species\\", pointsize=\\"body_mass_g\\") .facet(col=\\"species\\", row=\\"sex\\") .add(so.Dot()) .add(so.Line(), so.PolyFit()) .scale(color=\\"flare\\") .scale(x=so.Continuous().tick(every=5)) .label(x=\\"Bill Length (mm)\\", y=\\"Bill Depth (mm)\\") .show() ) ``` **Note:** Ensure you use the seaborn version 0.12 or later for the `objects` interface.","solution":"import seaborn as sns import seaborn.objects as so from seaborn import axes_style import matplotlib.pyplot as plt # Ensure that seaborn version is 0.12 or later assert sns.__version__ >= \\"0.12\\", f\\"Seaborn version must be 0.12 or later, found {sns.__version__}\\" # Loading dataset and removing NaN values penguins = sns.load_dataset(\\"penguins\\").dropna() # Custom theme modification to apply darkgrid and dotted grid lines theme_dict = {**axes_style(\\"darkgrid\\"), \\"grid.linestyle\\": \\":\\"} so.Plot.config.theme.update(theme_dict) # Creating and displaying the plot ( so.Plot(penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", color=\\"species\\", pointsize=\\"body_mass_g\\") .facet(col=\\"species\\", row=\\"sex\\") .add(so.Dot()) .add(so.Line(), so.PolyFit()) .scale(color=\\"flare\\") .scale(x=so.Continuous().tick(every=5)) .label(x=\\"Bill Length (mm)\\", y=\\"Bill Depth (mm)\\") .show() )"},{"question":"Coding Assessment Question # Objective In this assessment, you are required to demonstrate your understanding of the `marshal` module in Python for serializing and deserializing objects. You will implement a function that serializes a given list of supported Python objects, saves it to a binary file, then reads the data back from the file and returns the deserialized objects. # Function Signature ```python def serialize_deserialize_objects(objects: list, filename: str, version: int = marshal.version) -> list: Serializes a list of supported Python objects to a binary file, then reads back the data from the file and returns the deserialized objects. Parameters: objects (list): A list of Python objects (e.g., integers, strings, lists, dictionaries) to be serialized. filename (str): The name of the binary file to save the serialized data. version (int): The marshal version to use for serialization (default is the current marshal version). Returns: list: A list of deserialized Python objects read from the binary file. ``` # Input - `objects`: A list of Python objects. The objects must be of types supported by the `marshal` module (e.g., integers, floating-point numbers, strings, lists, dictionaries, etc.). - `filename`: The name of the binary file where the serialized data will be stored. - `version`: An optional integer specifying the version of the marshal format (defaults to the current version). # Output - A list containing the deserialized Python objects read from the binary file. # Constraints - The list `objects` can contain only types supported by the `marshal` module. - If any unsupported type is found within `objects`, the function should raise a `ValueError`. - The filename should be valid for creating a binary file in the working directory. - The function should handle exceptions that might occur during file operations gracefully. # Example ```python import marshal objects = [42, \\"python310\\", [1, 2, 3], {\\"a\\": 1, \\"b\\": 2}] filename = \\"data.bin\\" # Call the function result = serialize_deserialize_objects(objects, filename) # Expected output assert result == [42, \\"python310\\", [1, 2, 3], {\\"a\\": 1, \\"b\\": 2}] ``` # Notes - Ensure that your function includes thorough error handling for file operations and unsupported types. - You should utilize `marshal.dump` and `marshal.load` for file operations. - This task tests your understanding of file handling, error management, and marshaling in Python.","solution":"import marshal import os def serialize_deserialize_objects(objects: list, filename: str, version: int = marshal.version) -> list: Serializes a list of supported Python objects to a binary file, then reads back the data from the file and returns the deserialized objects. Parameters: objects (list): A list of Python objects (e.g., integers, strings, lists, dictionaries) to be serialized. filename (str): The name of the binary file to save the serialized data. version (int): The marshal version to use for serialization (default is the current marshal version). Returns: list: A list of deserialized Python objects read from the binary file. # Check that all objects are of types supported by marshal supported_types = (int, bool, float, complex, str, bytes, tuple, list, set, frozenset, dict, type(None)) for obj in objects: if not isinstance(obj, supported_types): raise ValueError(\\"Unsupported object type found in list\\") # Serialize objects to binary file try: with open(filename, \'wb\') as f: marshal.dump(objects, f, version) except Exception as e: raise IOError(f\\"Error writing to file: {e}\\") # Deserialize objects from binary file try: with open(filename, \'rb\') as f: deserialized_objects = marshal.load(f) except Exception as e: raise IOError(f\\"Error reading from file: {e}\\") return deserialized_objects"},{"question":"Coding Assessment Question # Objective You are tasked with creating a pipeline to process text data using the `pipes.Template` class. Your implementation should demonstrate the ability to chain multiple shell commands effectively. # Problem Statement You need to design a function `process_text_pipeline(input_file: str, output_file: str) -> None` that: 1. Converts all text in the specified `input_file` to uppercase. 2. Replaces all occurrences of spaces with hyphens. 3. Writes the processed text to the specified `output_file`. # Specifications - **Input:** - `input_file` (str): Path to the input file containing the text to be processed. - `output_file` (str): Path to the output file where the processed text will be stored. - **Output:** - None, but the `output_file` should contain the processed text. # Constraints - All operations must be performed using the `pipes.Template` class. - Only standard Unix shell commands should be used within the pipeline. - Ensure the pipeline handles any reasonable size of text files efficiently. # Example Suppose the file `input.txt` contains: ``` hello world ``` After running `process_text_pipeline(\'input.txt\', \'output.txt\')`, the file `output.txt` should contain: ``` HELLO-WORLD ``` # Implementation Notes - Use a `pipes.Template` object to construct the pipeline. - Use the `append` method to add commands (`tr` and `sed`). - Ensure proper handling of file read and write operations through the pipeline. Here is the function signature for reference: ```python def process_text_pipeline(input_file: str, output_file: str) -> None: pass ``` Good luck!","solution":"import pipes def process_text_pipeline(input_file: str, output_file: str) -> None: Processes text from an input file by converting it to uppercase and replacing spaces with hyphens, then writes the processed text to an output file. template = pipes.Template() # Convert all text to uppercase template.append(\'tr \\"[:lower:]\\" \\"[:upper:]\\"\', \'--\') # Replace spaces with hyphens template.append(\'sed \\"s/ /-/g\\"\', \'--\') # Open the input and output files through the template with template.open(input_file, \'r\') as infile, open(output_file, \'w\') as outfile: for line in infile: outfile.write(line)"},{"question":"**Problem Statement:** You are given a dataset named \\"healthexp\\" that contains data on health expenditures and life expectancy for various countries over several years. Your task is to create a plot that visualizes the relationship between health expenditure and life expectancy for each country using seaborn. # Expected Input: - None directly. You will use seaborn\'s `load_dataset` to load the \\"healthexp\\" dataset. # Expected Output: - A visualization plot showing the relationship between health expenditure (`Spending_USD`) and life expectancy (`Life_Expectancy`) for each country, using a path plot. # Constraints: - The plot should color each country\'s data differently. - The plot should not sort the observations before plotting. - Use markers in the plot with a specified size and line width, and ensure the marker has a white fill. # Instructions: 1. Import necessary libraries. 2. Load the \\"healthexp\\" dataset using `seaborn.load_dataset`. 3. Sort the dataset by \\"Country\\" and \\"Year\\". 4. Create a plot using `seaborn.objects.Plot`. 5. Add a \\"path\\" mark to the plot with: - Marker style as a circle (`\\"o\\"`). - Marker size (`pointsize`) as `2`. - Line width (`linewidth`) as `0.75`. - Marker fill color as white (`\\"w\\"`). 6. Ensure that the plot correctly displays a separate trajectory for each country without pre-sorting the data observations. Here is the template for your implementation: ```python import seaborn.objects as so from seaborn import load_dataset # Step 1: Load the dataset healthexp = load_dataset(\\"healthexp\\").sort_values([\\"Country\\", \\"Year\\"]) # Step 2: Create a plot p = so.Plot(healthexp, x=\\"Spending_USD\\", y=\\"Life_Expectancy\\", color=\\"Country\\") # Step 3: Add the path mark with specific properties p.add(so.Path(marker=\\"o\\", pointsize=2, linewidth=0.75, fillcolor=\\"w\\")) # Optionally: You may display/save the plot using appropriate seaborn or matplotlib functions # Show the plot p.show() # or equivalent function to render the plot ``` # Performance Requirements: - Ensure the visualization is rendered efficiently and correctly represents the dataset. - Use appropriate practices to maintain plot clarity and readability, especially when dealing with multiple countries. Complete the code to generate the required plot.","solution":"import seaborn as sns import matplotlib.pyplot as plt def plot_health_expenditure_vs_life_expectancy(): Plots health expenditure versus life expectancy for each country from the \'healthexp\' dataset. # Step 1: Load the dataset healthexp = sns.load_dataset(\\"healthexp\\").sort_values([\\"Country\\", \\"Year\\"]) # Step 2: Create a plot p = sns.relplot(data=healthexp, x=\\"Spending_USD\\", y=\\"Life_Expectancy\\", hue=\\"Country\\", kind=\\"line\\", marker=\\"o\\", facet_kws={\'legend_out\': True}) # Step 3: Customize the plot to match the required specifications for line in p.ax.lines: line.set_marker(\\"o\\") line.set_markersize(2) line.set_markeredgewidth(0.75) line.set_markerfacecolor(\'w\') line.set_markeredgecolor(line.get_color()) # Show the plot plt.show()"},{"question":"Objective To assess your understanding of multiclass classification using the `OneVsRestClassifier` and multilabel classification using `MultiOutputClassifier` in scikit-learn. Task You are given a dataset of images, each labeled with types of fruits and their colors. The dataset has been preprocessed and provided in the form of features and target labels. Your task is to implement a multiclass and multilabel classification model using scikit-learn to classify these images. Dataset The dataset consists of: - `X`: A 2D NumPy array of shape (n_samples, n_features) representing the features of the images. - `y`: A dictionary with two keys: - `y_type`: A 1D NumPy array of shape (n_samples,) containing the fruit type labels (`[\'apple\', \'orange\', \'pear\']`). - `y_color`: A 2D NumPy array of shape (n_samples, n_colors) with binary indicators for fruit color labels (`[\'red\', \'green\', \'yellow\', \'orange\']`). Requirements 1. Implement a multiclass classification model using `OneVsRestClassifier` to classify the fruit type. 2. Implement a multilabel classification model using `MultiOutputClassifier` to classify the fruit colors. Input - `X`: A 2D NumPy array of shape (n_samples, n_features). - `y_type`: A 1D NumPy array of shape (n_samples,). - `y_color`: A 2D NumPy array of shape (n_samples, n_colors). Output - An accuracy score for the multiclass classification model. - An accuracy score for the multilabel classification model. Constraints - Use `LinearSVC` as the base estimator for `OneVsRestClassifier`. - Use `RandomForestClassifier` as the base estimator for `MultiOutputClassifier`. - Use random_state=0 for reproducibility. - Split the dataset into training and testing sets (80% train, 20% test). Performance Requirements Your implementation should output the accuracy scores within 30 seconds for a dataset of size (1000, 20). # Example ```python import numpy as np from sklearn.model_selection import train_test_split from sklearn.multiclass import OneVsRestClassifier from sklearn.multioutput import MultiOutputClassifier from sklearn.svm import LinearSVC from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import accuracy_score # Simulated dataset X = np.random.rand(1000, 20) y_type = np.random.choice([\'apple\', \'orange\', \'pear\'], 1000) y_color = np.random.randint(2, size=(1000, 4)) # Split the dataset X_train, X_test, y_type_train, y_type_test = train_test_split(X, y_type, test_size=0.2, random_state=0) y_color_train, y_color_test = train_test_split(y_color, test_size=0.2, random_state=0) # Multiclass classification using OneVsRestClassifier ovr_classifier = OneVsRestClassifier(LinearSVC(random_state=0)) ovr_classifier.fit(X_train, y_type_train) y_type_pred = ovr_classifier.predict(X_test) multiclass_accuracy = accuracy_score(y_type_test, y_type_pred) # Multilabel classification using MultiOutputClassifier multi_output_classifier = MultiOutputClassifier(RandomForestClassifier(random_state=0)) multi_output_classifier.fit(X_train, y_color_train) y_color_pred = multi_output_classifier.predict(X_test) multilabel_accuracy = np.mean(np.all(y_color_pred == y_color_test, axis=1)) # Custom accuracy for multilabel # Output the accuracy scores print(\\"Multiclass Classification Accuracy:\\", multiclass_accuracy) print(\\"Multilabel Classification Accuracy:\\", multilabel_accuracy) ``` Your solution should involve defining a function `train_and_evaluate models` that takes in `X`, `y_type`, and `y_color`, and outputs the two accuracy scores. Function Signature ```python def train_and_evaluate_models(X: np.ndarray, y_type: np.ndarray, y_color: np.ndarray) -> tuple: pass ```","solution":"import numpy as np from sklearn.model_selection import train_test_split from sklearn.multiclass import OneVsRestClassifier from sklearn.multioutput import MultiOutputClassifier from sklearn.svm import LinearSVC from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import accuracy_score def train_and_evaluate_models(X: np.ndarray, y_type: np.ndarray, y_color: np.ndarray) -> tuple: # Split the dataset into training and testing sets X_train, X_test, y_type_train, y_type_test = train_test_split(X, y_type, test_size=0.2, random_state=0) y_color_train, y_color_test = train_test_split(y_color, test_size=0.2, random_state=0) # Multiclass classification using OneVsRestClassifier ovr_classifier = OneVsRestClassifier(LinearSVC(random_state=0)) ovr_classifier.fit(X_train, y_type_train) y_type_pred = ovr_classifier.predict(X_test) multiclass_accuracy = accuracy_score(y_type_test, y_type_pred) # Multilabel classification using MultiOutputClassifier multi_output_classifier = MultiOutputClassifier(RandomForestClassifier(random_state=0)) multi_output_classifier.fit(X_train, y_color_train) y_color_pred = multi_output_classifier.predict(X_test) multilabel_accuracy = np.mean(np.all(y_color_pred == y_color_test, axis=1)) # Custom accuracy for multilabel return multiclass_accuracy, multilabel_accuracy"},{"question":"**Principal Component Analysis (PCA) Implementation and Evaluation** **Objective:** Implement PCA and IncrementalPCA using scikit-learn to perform dimensionality reduction on a given dataset. Evaluate the performance and compare the results. **Problem Statement:** You are provided with a dataset `data.csv` that contains `N` samples each having `D` features. Your task is to: 1. Implement PCA to reduce the dataset to the top `k` components. 2. Implement IncrementalPCA to achieve the same result, suitable for large datasets. 3. Compare and evaluate the performance of the two methods. **Specifications:** 1. Implement a function `perform_pca` that: - Takes as input the dataset as a 2D NumPy array and the number of components `k`. - Applies PCA on the dataset. - Returns the transformed dataset and the variance explained by each of the top `k` components. 2. Implement a function `perform_incremental_pca` that: - Takes as input the dataset as a 2D NumPy array, the number of components `k`, and the batch size `batch_size`. - Applies IncrementalPCA on the dataset with the given batch size. - Returns the transformed dataset and the variance explained by each of the top `k` components. 3. Implement a function `evaluate_performance` that: - Takes as input the original dataset, the transformed datasets obtained from PCA and IncrementalPCA, and the explained variances. - Prints the explained variance for top `k` components from both methods. - Plots the cumulative explained variance for PCA and IncrementalPCA. - Optionally, plots the first two principal components obtained from both methods for visual comparison. **Input Format:** - A CSV file `data.csv` where each row represents a sample and each column represents a feature. **Output Format:** - Transformed datasets from both PCA and IncrementalPCA. - Explained variance ratios from both methods. - Print and plot the cumulative explained variance. **Constraints:** - You can assume `k` will be less than the number of features. - Memory constraints should be considered for large datasets using IncrementalPCA. **Example Usage:** ```python import numpy as np import pandas as pd def perform_pca(data: np.ndarray, k: int): # Your implementation here pass def perform_incremental_pca(data: np.ndarray, k: int, batch_size: int): # Your implementation here pass def evaluate_performance(original_data: np.ndarray, pca_transformed: np.ndarray, ipca_transformed: np.ndarray, pca_variance: np.ndarray, ipca_variance: np.ndarray): # Your implementation here pass # Example usage data = pd.read_csv(\'data.csv\').values k = 2 batch_size = 100 pca_transformed, pca_variance = perform_pca(data, k) ipca_transformed, ipca_variance = perform_incremental_pca(data, k, batch_size) evaluate_performance(data, pca_transformed, ipca_transformed, pca_variance, ipca_variance) ``` Please make sure to use appropriate scikit-learn methods and handle large datasets efficiently using IncrementalPCA.","solution":"import numpy as np import pandas as pd from sklearn.decomposition import PCA, IncrementalPCA import matplotlib.pyplot as plt def perform_pca(data: np.ndarray, k: int): Perform PCA on the given dataset. Parameters: data (np.ndarray): The input dataset. k (int): The number of principal components to keep. Returns: np.ndarray: The transformed dataset. np.ndarray: The variance explained by each of the top k components. pca = PCA(n_components=k) transformed_data = pca.fit_transform(data) explained_variance = pca.explained_variance_ratio_ return transformed_data, explained_variance def perform_incremental_pca(data: np.ndarray, k: int, batch_size: int): Perform IncrementalPCA on the given dataset. Parameters: data (np.ndarray): The input dataset. k (int): The number of principal components to keep. batch_size (int): The batch size for incremental PCA. Returns: np.ndarray: The transformed dataset. np.ndarray: The variance explained by each of the top k components. ipca = IncrementalPCA(n_components=k, batch_size=batch_size) transformed_data = ipca.fit_transform(data) explained_variance = ipca.explained_variance_ratio_ return transformed_data, explained_variance def evaluate_performance(original_data: np.ndarray, pca_transformed: np.ndarray, ipca_transformed: np.ndarray, pca_variance: np.ndarray, ipca_variance: np.ndarray): Evaluate the performance of PCA and IncrementalPCA. Parameters: original_data (np.ndarray): The original dataset. pca_transformed (np.ndarray): The transformed dataset using PCA. ipca_transformed (np.ndarray): The transformed dataset using IncrementalPCA. pca_variance (np.ndarray): The explained variance ratios from PCA. ipca_variance (np.ndarray): The explained variance ratios from IncrementalPCA. print(\\"PCA Explained Variance:\\", pca_variance) print(\\"IncrementalPCA Explained Variance:\\", ipca_variance) # Plot cumulative explained variance plt.figure(figsize=(10, 6)) plt.plot(np.cumsum(pca_variance), label=\'PCA\') plt.plot(np.cumsum(ipca_variance), label=\'IncrementalPCA\') plt.xlabel(\'Number of Components\') plt.ylabel(\'Cumulative Explained Variance\') plt.legend(loc=\'best\') plt.title(\'Cumulative Explained Variance by PCA and IncrementalPCA\') plt.grid(True) plt.show() if pca_transformed.shape[1] >= 2 and ipca_transformed.shape[1] >= 2: plt.figure(figsize=(10, 6)) plt.subplot(1, 2, 1) plt.scatter(pca_transformed[:, 0], pca_transformed[:, 1], alpha=0.5) plt.title(\'First Two PCA Components\') plt.subplot(1, 2, 2) plt.scatter(ipca_transformed[:, 0], ipca_transformed[:, 1], alpha=0.5) plt.title(\'First Two IncrementalPCA Components\') plt.tight_layout() plt.show()"},{"question":"# MemoryView Manipulation and Optimization Task Objective Your task is to write a Python function that manipulates and processes data using memoryview objects. This will help assess your understanding of memory management and performance optimization in Python. Problem Statement You are given a function `process_buffer` that takes a bytes-like object (such as a bytes object, bytearray, or other objects supporting the buffer interface) and an integer `n`. The function should perform the following tasks: 1. Create a memoryview object from the provided buffer. 2. Check if the memoryview object is contiguous. If it is not, convert it into a contiguous memoryview object. 3. Reverse the bytes of the memoryview object if it is writable. 4. Return the modified buffer as a bytes object. If the buffer cannot be modified directly (i.e., it is read-only), the function should raise a `ValueError` with the message \\"Buffer is read-only and cannot be modified.\\" Function Signature ```python def process_buffer(buf: bytes, n: int) -> bytes: pass ``` Input - `buf`: A bytes-like object that supports the buffer interface. - `n`: An integer representing the size of the buffer to be processed. Output - Returns a bytes object containing the reversed byte sequence if the buffer is writable. - Raises `ValueError` with the message \\"Buffer is read-only and cannot be modified.\\" if the buffer is read-only. Constraints - The buffer will always contain a sequence of bytes with a length greater than or equal to `n`. - You are required to use memoryview objects for this task. - Performance considerations are important; avoid creating unnecessary copies of the buffer unless required. Example ```python # Example 1: buf = bytearray(b\'12345678\') n = 8 print(process_buffer(buf, n)) # Output: b\'87654321\' # Example 2: buf = b\'abcdefgh\' n = 8 try: result = process_buffer(buf, n) except ValueError as e: print(e) # Output: Buffer is read-only and cannot be modified. ``` Notes - The function should demonstrate the use of `memoryview` for buffer manipulation. - Ensure you handle both read-only and writable buffers correctly. - Efficiency in the use of memory is a key consideration for this task.","solution":"def process_buffer(buf: bytes, n: int) -> bytes: Process the buffer and reverse its contents if it is writable using memoryview. Parameters: buf (bytes): A bytes-like object that supports the buffer interface. n (int): An integer representing the size of the buffer to be processed. Returns: bytes: The modified buffer as a bytes object. Raises: ValueError: If the buffer is read-only and cannot be modified. mv = memoryview(buf) # Ensure the memoryview is contiguous if not mv.contiguous: mv = mv.tobytes() # Convert to contiguous memory try: if mv.readonly: raise ValueError(\\"Buffer is read-only and cannot be modified\\") # Reverse the buffer mv[:n] = mv[:n][::-1] return mv.tobytes() finally: mv.release()"},{"question":"# Pandas Resampling Assessment Objective Demonstrate your proficiency with the `pandas` library by performing resampling operations on time series data to derive meaningful insights and manipulate data frequency. Problem Statement You are provided with a time series dataset containing daily stock prices for a fictional company. The dataset includes columns for the date, opening price, closing price, highest price of the day, lowest price of the day, and the volume of stocks traded. Write a function `process_stock_data` that performs the following tasks: 1. **Resample the dataset to a weekly frequency**, using the closing prices. On resampling, compute the following summaries for each week: - **Mean closing price**. - **Highest closing price**. - **Lowest closing price**. 2. **Calculate the moving average** of the closing prices with a window of 4 weeks (28 days) on the weekly resampled dataset. 3. Return a DataFrame containing the following columns: - **Week Start (Date)**: The start date of the week. - **Mean Closing Price**: The mean of the closing prices for that week. - **Highest Closing Price**: The highest closing price for that week. - **Lowest Closing Price**: The lowest closing price for that week. - **4-Week Moving Average**: The moving average of the closing prices over a 4-week window. Input - A pandas DataFrame `df` with the following columns: - `Date`: The date in `YYYY-MM-DD` format. - `Open`: Opening price of the stock. - `Close`: Closing price of the stock. - `High`: Highest price of the stock during the day. - `Low`: Lowest price of the stock during the day. - `Volume`: Number of stocks traded. Output - A pandas DataFrame with the specified columns and weekly resampled data. Constraints - Assume the input DataFrame is sorted by date in ascending order. - Handle any missing data (e.g., missing dates) appropriately. - Ensure that the function operates efficiently on large datasets. Example Code Below is a template to help get started: ```python import pandas as pd def process_stock_data(df): # Ensure Date column is in datetime format df[\'Date\'] = pd.to_datetime(df[\'Date\']) # Set Date column as index df.set_index(\'Date\', inplace=True) # Resample to weekly frequency and calculate mean, max, and min closing price weekly_resampled = df[\'Close\'].resample(\'W\').agg({ \'Mean Closing Price\': \'mean\', \'Highest Closing Price\': \'max\', \'Lowest Closing Price\': \'min\' }) # Calculate 4-week moving average weekly_resampled[\'4-Week Moving Average\'] = weekly_resampled[\'Mean Closing Price\'].rolling(window=4).mean() # Reset index to turn the Date index back to a column weekly_resampled.reset_index(inplace=True) # Rename \'Date\' column to \'Week Start\' weekly_resampled.rename(columns={\'Date\': \'Week Start\'}, inplace=True) return weekly_resampled # Example usage # df = pd.read_csv(\'daily_stock_prices.csv\') # result = process_stock_data(df) # print(result) ``` Note: This template provides structure but does not include the complete implementation. Fill in the missing parts according to the requirements.","solution":"import pandas as pd def process_stock_data(df): # Ensure Date column is in datetime format df[\'Date\'] = pd.to_datetime(df[\'Date\']) # Set Date column as index df.set_index(\'Date\', inplace=True) # Resample to weekly frequency and calculate mean, max, and min closing price weekly_resampled = df[\'Close\'].resample(\'W\').agg({ \'Mean Closing Price\': \'mean\', \'Highest Closing Price\': \'max\', \'Lowest Closing Price\': \'min\' }) # Calculate 4-week moving average weekly_resampled[\'4-Week Moving Average\'] = weekly_resampled[\'Mean Closing Price\'].rolling(window=4).mean() # Reset index to turn the Date index back to a column weekly_resampled.reset_index(inplace=True) # Rename \'Date\' column to \'Week Start\' weekly_resampled.rename(columns={\'Date\': \'Week Start\'}, inplace=True) return weekly_resampled"},{"question":"# PyTorch MTIA Module: Custom Stream and Device Manager In this task, you are required to demonstrate your understanding of the `torch.mtia` module by implementing a function that manages a custom computation stream on a specified device. Additionally, you will perform certain memory and state management operations within this stream context. Function Signature ```python def custom_stream_and_device_manager(device_id: int) -> dict: pass ``` Function Description 1. **Initialize MTIA**: Ensure the MTIA backend is initialized. Check if it\'s available and initialized, otherwise return a dictionary indicating the status. 2. **Set Device**: Set the current device to `device_id`. If the device is not available, handle this appropriately. 3. **Stream Management**: - Create a new computation stream. - Set and switch to this new stream. 4. **Memory Management**: - Fetch and return the memory statistics for the current device after performing some dummy tensor operations. 5. **RNG State Management**: - Save the current RNG state. - Set a new RNG state (for example, a given seed). - After some operations, restore the RNG state to the saved one. Input - `device_id` (int): The ID of the GPU device to manage. Output - A dictionary containing: - `\'initialized\'`: Boolean indicating if MTIA is initialized. - `\'device_set\'`: The device ID set. - `\'memory_statistics\'`: The memory statistics of the current device. - `\'rng_state_before\'`: The original RNG state before any operation. - `\'rng_state_after\'`: The RNG state after restoration. Constraints - You should ensure proper exception handling where necessary. - Assume that at least one device is available. # Example ```python result = custom_stream_and_device_manager(0) assert result == { \'initialized\': True, \'device_set\': 0, \'memory_statistics\': { ... }, # Detailed memory stats for device 0 \'rng_state_before\': bytearray(...), # RNG state before setting new state \'rng_state_after\': bytearray(...), # RNG state restored to original } ``` Implement the `custom_stream_and_device_manager` function to meet the requirements specified.","solution":"import torch def custom_stream_and_device_manager(device_id: int) -> dict: Manages a custom computation stream on a specified device. Arguments: - device_id (int): The ID of the GPU device to manage. Returns: - dict: A dictionary with keys \'initialized\', \'device_set\', \'memory_statistics\', \'rng_state_before\', and \'rng_state_after\'. if not torch.cuda.is_available(): return { \'initialized\': False, \'device_set\': None, \'memory_statistics\': None, \'rng_state_before\': None, \'rng_state_after\': None } # Ensure CUDA backend is initialized torch.cuda.init() # Set current device if device_id >= torch.cuda.device_count(): raise ValueError(f\\"Device ID {device_id} is not available.\\") torch.cuda.set_device(device_id) # Create a new computation stream and set it stream = torch.cuda.Stream(device_id) memory_statistics = {} rng_state_before = None rng_state_after = None with torch.cuda.stream(stream): # Perform some dummy tensor operations to affect memory usage a = torch.randn((1000, 1000), device=device_id) b = torch.randn((1000, 1000), device=device_id) c = torch.matmul(a, b) # Get memory statistics memory_statistics = torch.cuda.memory_stats(device=device_id) # Save RNG state rng_state_before = torch.cuda.get_rng_state(device=device_id) # Set a new RNG state (using a fixed seed for example) torch.cuda.manual_seed_all(123) # Perform some operations d = torch.randn((1000, 1000), device=device_id) # Restore the RNG state torch.cuda.set_rng_state(rng_state_before, device=device_id) # Verify if RNG state is restored rng_state_after = torch.cuda.get_rng_state(device=device_id) return { \'initialized\': True, \'device_set\': device_id, \'memory_statistics\': memory_statistics, \'rng_state_before\': rng_state_before, \'rng_state_after\': rng_state_after }"},{"question":"# Coding Assignment: Implementing and Evaluating a Support Vector Machine Classifier Objective: Your task is to implement a multi-class Support Vector Machine (SVM) classifier using scikit-learn. You will use the provided dataset to train your model. The goal is to assess your understanding of training SVMs, handling multi-class classification, tuning parameters, and evaluating the model\'s performance. Dataset: You will be using the Iris dataset, which comes preloaded with scikit-learn. Task Details: 1. **Data Preparation:** - Load the Iris dataset using `sklearn.datasets.load_iris`. - Split the dataset into training and testing sets using `sklearn.model_selection.train_test_split` (use `test_size=0.3` and `random_state=42`). 2. **Model Training:** - Instantiate the `SVC` class for the SVM model. Use the \'rbf\' kernel. - Tune the `C` and `gamma` parameters using `GridSearchCV`. Use the following range for parameters: ```python param_grid = {\'C\': [0.1, 1, 10, 100], \'gamma\': [1, 0.1, 0.01, 0.001]} ``` - Perform grid search with 5-fold cross-validation on the training set to find the best combination of `C` and `gamma`. 3. **Model Evaluation:** - After tuning, fit the model with the best parameters on the entire training data and evaluate performance on the test data. - Calculate and print the following: - Confusion matrix (`sklearn.metrics.confusion_matrix`) - Classification report with precision, recall, f1-score (`sklearn.metrics.classification_report`) - Accuracy score (`sklearn.metrics.accuracy_score`) 4. **Support Vectors Analysis:** - Print the support vectors, indices of support vectors, and number of support vectors for each class. 5. **Implementation Constraints:** - You must not use any other machine learning library apart from scikit-learn. - Ensure your solution is efficient and avoids unnecessary computation. Expected Output: Your implementation should include: - The best parameters for `C` and `gamma` after cross-validation. - Confusion matrix, classification report, and accuracy score when evaluated on the test set. - A printed list of support vectors, their indices, and a count of support vectors per class. Code Template: ```python from sklearn import datasets from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.svm import SVC from sklearn.metrics import confusion_matrix, classification_report, accuracy_score # Step 1: Data Preparation iris = datasets.load_iris() X = iris.data y = iris.target X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Step 2: Model Training with GridSearchCV param_grid = {\'C\': [0.1, 1, 10, 100], \'gamma\': [1, 0.1, 0.01, 0.001]} grid = GridSearchCV(SVC(kernel=\'rbf\'), param_grid, cv=5) grid.fit(X_train, y_train) # Best parameters best_params = grid.best_params_ print(f\\"Best Parameters: {best_params}\\") # Fit the model on the training data model = grid.best_estimator_ model.fit(X_train, y_train) # Step 3: Model Evaluation y_pred = model.predict(X_test) print(\\"Confusion Matrix:\\") print(confusion_matrix(y_test, y_pred)) print(\\"nClassification Report:\\") print(classification_report(y_test, y_pred)) print(\\"nAccuracy Score:\\") print(accuracy_score(y_test, y_pred)) # Step 4: Support Vectors Analysis print(\\"nSupport Vectors:\\") print(model.support_vectors_) print(\\"nIndices of Support Vectors:\\") print(model.support_) print(\\"nNumber of Support Vectors for each class:\\") print(model.n_support_) ``` This question will test your understanding of SVMs, ability to apply hyperparameter tuning, and evaluate the model\'s performance effectively.","solution":"from sklearn import datasets from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.svm import SVC from sklearn.metrics import confusion_matrix, classification_report, accuracy_score def load_and_split_data(): Load the Iris dataset and split it into training and testing sets. iris = datasets.load_iris() X = iris.data y = iris.target return train_test_split(X, y, test_size=0.3, random_state=42) def train_svm_with_grid_search(X_train, y_train): Train an SVM classifier using GridSearchCV to find the best parameters. param_grid = {\'C\': [0.1, 1, 10, 100], \'gamma\': [1, 0.1, 0.01, 0.001]} grid = GridSearchCV(SVC(kernel=\'rbf\'), param_grid, cv=5) grid.fit(X_train, y_train) return grid.best_estimator_, grid.best_params_ def evaluate_model(model, X_test, y_test): Evaluate the trained SVM model on the test set. y_pred = model.predict(X_test) cm = confusion_matrix(y_test, y_pred) cr = classification_report(y_test, y_pred) acc = accuracy_score(y_test, y_pred) return cm, cr, acc def analyze_support_vectors(model): Analyze the support vectors of the trained SVM model. support_vectors = model.support_vectors_ support_indices = model.support_ n_support = model.n_support_ return support_vectors, support_indices, n_support # Main function to encapsulate the whole task def main(): # Step 1: Data Preparation X_train, X_test, y_train, y_test = load_and_split_data() # Step 2: Model Training with GridSearchCV model, best_params = train_svm_with_grid_search(X_train, y_train) print(f\\"Best Parameters: {best_params}\\") # Fit the model on the training data model.fit(X_train, y_train) # Step 3: Model Evaluation cm, cr, acc = evaluate_model(model, X_test, y_test) print(\\"Confusion Matrix:\\") print(cm) print(\\"nClassification Report:\\") print(cr) print(\\"nAccuracy Score:\\") print(acc) # Step 4: Support Vectors Analysis support_vectors, support_indices, n_support = analyze_support_vectors(model) print(\\"nSupport Vectors:\\") print(support_vectors) print(\\"nIndices of Support Vectors:\\") print(support_indices) print(\\"nNumber of Support Vectors for each class:\\") print(n_support) if __name__ == \\"__main__\\": main()"},{"question":"**Coding Assessment Question:** Design a function that reads, updates, and verifies environment variables in Python using the `os` module to demonstrate your understanding of POSIX system calls and environment management. The function should take the following inputs: 1. `var_name`: The name of an environment variable (a string). 2. `new_value`: The new value to be assigned to the environment variable (a string). Your task is to: 1. Retrieve the current value of the environment variable specified by `var_name`. 2. Update the environment variable with `new_value`. 3. Verify that the environment variable has been updated correctly. 4. Return a tuple containing the old value of the environment variable (or `None` if it was not set) and the new verified value. # Function Signature: ```python def update_env_variable(var_name: str, new_value: str) -> tuple: pass ``` # Input: - `var_name` (str): The name of the environment variable. - `new_value` (str): The new value to assign to the environment variable. # Output: - A tuple containing: - The old value of the environment variable (or `None` if it was not set). - The new value of the environment variable after the update. # Constraints: - The function should handle cases where the environment variable does not initially exist. - You must use the `os` module for environment variable manipulation. # Example: ```python # Initial environment: {\'EXAMPLE_VAR\': \'123\'} result = update_env_variable(\'EXAMPLE_VAR\', \'456\') print(result) # Expected output: (\'123\', \'456\') result = update_env_variable(\'NEW_VAR\', \'789\') print(result) # Expected output: (None, \'789\') # Final environment: {\'EXAMPLE_VAR\': \'456\', \'NEW_VAR\': \'789\'} ``` # Additional Notes: - You can assume that the `var_name` and `new_value` inputs will always be valid (non-empty strings). - Be mindful of potential issues related to string encoding when working with environment variables.","solution":"import os def update_env_variable(var_name: str, new_value: str) -> tuple: Retrieve, update, and verify an environment variable. Parameters: - var_name (str): The name of the environment variable. - new_value (str): The new value to assign to the environment variable. Returns: - tuple: The old value of the environment variable (or None if it was not set) and the new value. old_value = os.environ.get(var_name) os.environ[var_name] = new_value verified_new_value = os.environ.get(var_name) return (old_value, verified_new_value)"},{"question":"# Question: You have been tasked to implement a PyTorch function that normalizes a batch of tensors and applies a specific initialization to a layer in a neural network. However, you must ensure that your function is compatible with TorchScript, which has certain unsupported features and limitations. Requirements: 1. Implement a function `normalize_and_initialize` that takes in a batch of tensors and a neural network layer. 2. Normalize each tensor in the batch using L2 normalization. 3. Apply He initialization (Kaiming Normal) to the given layer. 4. Ensure that your implementation is compatible with TorchScript, avoiding any unsupported features mentioned in the provided documentation. Function Signature: ```python import torch import torch.nn as nn import torch.nn.functional as F def normalize_and_initialize(tensor_batch: torch.Tensor, layer: nn.Module): Normalizes a batch of tensors using L2 normalization and applies He initialization to the given layer. Args: tensor_batch (torch.Tensor): A batch of tensors of shape (batch_size, num_features). layer (torch.nn.Module): A neural network layer (fully connected or convolutional). Returns: torch.Tensor: The normalized batch of tensors. pass ``` Constraints: 1. Do not use any functions or features listed as unsupported in TorchScript from the provided documentation. 2. Assume tensor_batch is a 2D tensor with shape `(batch_size, num_features)`. 3. The function should not modify the original tensors in-place. Example: ```python # Example for testing the function import torch import torch.nn as nn import torch.nn.functional as F # Sample tensor batch and layer tensor_batch = torch.randn(10, 5) layer = nn.Linear(5, 3) # Function to normalize and initialize normalized_batch = normalize_and_initialize(tensor_batch, layer) # Check results print(normalized_batch) ``` Notes: - Normalize each tensor such that the L2 norm of each tensor along the `num_features` axis is 1. - Use `torch.nn.init.kaiming_normal_` method for layer initialization. - Ensure that your solution is valid in both standard PyTorch and TorchScript environments.","solution":"import torch import torch.nn as nn def normalize_and_initialize(tensor_batch: torch.Tensor, layer: nn.Module): Normalizes a batch of tensors using L2 normalization and applies He initialization to the given layer. Args: tensor_batch (torch.Tensor): A batch of tensors of shape (batch_size, num_features). layer (torch.nn.Module): A neural network layer (fully connected or convolutional). Returns: torch.Tensor: The normalized batch of tensors. # L2 normalization norm = torch.sqrt(torch.sum(tensor_batch ** 2, dim=1, keepdim=True)) normalized_batch = tensor_batch / (norm + 1e-12) # He (Kaiming) initialization if isinstance(layer, (nn.Linear, nn.Conv2d)): nn.init.kaiming_normal_(layer.weight, nonlinearity=\'relu\') if layer.bias is not None: nn.init.constant_(layer.bias, 0) return normalized_batch"},{"question":"**Objective:** Demonstrate your ability to handle missing values in time-related data using pandas. **Problem Statement:** You are provided with a pandas DataFrame containing information about various events and their corresponding dates. Some of the dates are missing. Your task is to implement a function that: 1. Reads a CSV file into a pandas DataFrame. The CSV file has the following structure: ``` event_id,event_name,event_date 1,Event A,2021-05-12 2,Event B, 3,Event C,2021-07-19 4,Event D,NaT 5,Event E,2021-08-23 ``` 2. Identifies the rows with missing values in the `event_date` column and replaces them with the most frequent non-missing date from the same column. 3. Returns the modified DataFrame. **Input:** - A string representing the path to the CSV file. **Output:** - A pandas DataFrame with the missing `event_date` values replaced as specified. **Function Signature:** ```python import pandas as pd def fill_missing_event_dates(csv_file_path: str) -> pd.DataFrame: pass ``` **Constraints:** - You must use pandas for data manipulation. - Assume there will always be at least one non-missing date in the `event_date` column. **Example Usage:** Given a CSV file `events.csv` with the following content: ``` event_id,event_name,event_date 1,Event A,2021-05-12 2,Event B, 3,Event C,2021-07-19 4,Event D,NaT 5,Event E,2021-08-23 ``` Your function should read this file and return the following DataFrame (assuming `2021-05-12` is the most frequent date): ``` event_id event_name event_date 0 1 Event A 2021-05-12 1 2 Event B 2021-05-12 2 3 Event C 2021-07-19 3 4 Event D 2021-05-12 4 5 Event E 2021-08-23 ``` **Notes:** - Rows where `event_date` is `NaT` should be considered as missing dates. - Handle potential ties for the most frequent date by selecting the earliest. Happy coding!","solution":"import pandas as pd def fill_missing_event_dates(csv_file_path: str) -> pd.DataFrame: # Read the CSV file into a DataFrame df = pd.read_csv(csv_file_path, parse_dates=[\'event_date\']) # Identify the most frequent non-missing date most_frequent_date = df[\'event_date\'].mode().iloc[0] # Replace missing values with the most frequent date df[\'event_date\'].fillna(most_frequent_date, inplace=True) return df"},{"question":"Coding Assessment Question # Objective Implement a simple I/O server using the `selectors` module to handle multiple client connections simultaneously. The server should be able to accept connections, read data from clients, echo the data back to them, and handle client disconnection gracefully. # Requirements - Use `selectors.DefaultSelector` for the selector object. - Implement functions to handle accepting connections and reading data from clients. - Ensure the server can handle multiple clients concurrently without blocking. - Properly handle client disconnection and resource cleanup. # Function Signature ```python def run_echo_server(host: str, port: int): pass ``` # Input - `host` (string): The host address to bind the server to (e.g., `\'localhost\'`). - `port` (integer): The port number to bind the server to (e.g., `12345`). # Output - None (the function should run the server indefinitely until interrupted). # Example ```python # Example usage: run_echo_server(\'localhost\', 12345) ``` # Constraints - The server should continuously run and handle multiple clients. - The server should echo back any data it receives from a client. - If a client disconnects, the server should handle it gracefully and continue serving other clients. - Assume the maximum data length to be read in a single `recv` call is 1000 bytes. # Implementation Guide 1. Initialize a `DefaultSelector` instance. 2. Create a listening socket and register it with the selector for `EVENT_READ`. 3. Implement the `accept` function to accept new client connections. 4. Implement the `read` function to read data from clients and echo it back. 5. Use a loop to wait for I/O events and dispatch them to the appropriate handler functions. # Example Implementation Skeleton Here is a basic skeleton to help you get started: ```python import selectors import socket def accept(sock, mask): conn, addr = sock.accept() # Should be ready print(\'Accepted\', conn, \'from\', addr) conn.setblocking(False) sel.register(conn, selectors.EVENT_READ, read) def read(conn, mask): data = conn.recv(1000) # Should be ready if data: print(\'Echoing\', repr(data), \'to\', conn) conn.send(data) # Hope it won\'t block else: print(\'Closing\', conn) sel.unregister(conn) conn.close() def run_echo_server(host: str, port: int): sel = selectors.DefaultSelector() sock = socket.socket() sock.bind((host, port)) sock.listen(100) sock.setblocking(False) sel.register(sock, selectors.EVENT_READ, accept) while True: events = sel.select() for key, mask in events: callback = key.data callback(key.fileobj, mask) ``` Ensure that your implementation handles edge cases such as clients disconnecting and potential errors during read/write operations. Test your server thoroughly by connecting multiple clients to it and verifying the echo functionality.","solution":"import selectors import socket sel = selectors.DefaultSelector() def accept(sock, mask): conn, addr = sock.accept() # Should be ready print(\'Accepted\', conn, \'from\', addr) conn.setblocking(False) sel.register(conn, selectors.EVENT_READ, read) def read(conn, mask): data = conn.recv(1000) # Should be ready if data: print(\'Echoing\', repr(data), \'to\', conn) conn.send(data) # Hope it won\'t block else: print(\'Closing\', conn) sel.unregister(conn) conn.close() def run_echo_server(host: str, port: int): sock = socket.socket() sock.bind((host, port)) sock.listen(100) sock.setblocking(False) sel.register(sock, selectors.EVENT_READ, accept) try: while True: events = sel.select() for key, mask in events: callback = key.data callback(key.fileobj, mask) except KeyboardInterrupt: print(\\"Server shut down.\\") finally: sel.close() sock.close()"},{"question":"Objective: Demonstrate your understanding of the Sequence Protocol by implementing a custom sequence type in Python. Problem Statement: You are required to implement a class `CustomSequence` that mimics the behavior of a sequence type in Python. Your class should support the following operations using the Sequence Protocol: 1. Indexing (`obj[index]`) 2. Slicing (`obj[start:stop]`) 3. `len(obj)` to get the length of the sequence 4. Iteration over the elements in the sequence Requirements: 1. **Indexing**: Should support both positive and negative indices. 2. **Slicing**: Should return a new instance of `CustomSequence` with the sliced elements. 3. **Length**: Should return the number of elements in the sequence. 4. **Iteration**: Should allow iterating over the elements in the sequence using a `for` loop. Constraints: 1. You cannot use any built-in sequence types like `list` or `tuple` directly in your implementation. 2. Your implementation should handle range and index errors gracefully by raising appropriate exceptions. Implementation Details: 1. Implement the `__getitem__`, `__len__`, and `__iter__` methods in your class. 2. Ensure your class works for integer sequences as given in the example. Example: ```python class CustomSequence: # Your implementation here def __init__(self, data): self.data = data def __getitem__(self, index): # To be implemented pass def __len__(self): # To be implemented pass def __iter__(self): # To be implemented pass # Example usage: seq = CustomSequence([1, 2, 3, 4, 5]) # Indexing print(seq[0]) # Output: 1 print(seq[-1]) # Output: 5 # Slicing print(seq[1:4]) # Output: CustomSequence([2, 3, 4]) # Length print(len(seq)) # Output: 5 # Iteration for item in seq: print(item) # Output: 1 2 3 4 5 ``` Note that the output for slicing should be an instance of `CustomSequence` containing the sliced elements. Performance Requirements: - The implementation should be efficient in terms of both time and space complexity. Submission: Submit your implementation of the `CustomSequence` class. Good luck!","solution":"class CustomSequence: def __init__(self, data): self.data = data def __getitem__(self, index): if isinstance(index, slice): return CustomSequence(self.data[index]) else: return self.data[index] def __len__(self): return len(self.data) def __iter__(self): return iter(self.data)"},{"question":"**Question:** You are required to implement and train a neural network using PyTorch. The network should be built to classify images from the MNIST dataset. The network architecture and other specifics are provided below. Please follow the structure and use appropriate functions from the `torch.nn.functional` module where necessary. **Neural Network Architecture:** 1. **Input layer**: Accepts a 1x28x28 image. 2. **Convolutional Layer 1**: Applies 32 3x3 filters with a stride of 1 and padding of 1. 3. **ReLU Activation**. 4. **Max Pooling Layer 1**: Applies a 2x2 max pooling with a stride of 2. 5. **Convolutional Layer 2**: Applies 64 3x3 filters with a stride of 1 and padding of 1. 6. **ReLU Activation**. 7. **Max Pooling Layer 2**: Applies a 2x2 max pooling with a stride of 2. 8. **Fully Connected Layer 1**: Transforms the data into a 128-neuron layer. 9. **ReLU Activation**. 10. **Fully Connected Layer 2**: Transforms the data into a 10-neuron output layer (one for each class in MNIST). 11. **Softmax Activation**. **Training Details:** - **Loss Function**: Use `cross_entropy` loss. - **Optimizer**: Use SGD (Stochastic Gradient Descent) with a learning rate of 0.01. - **Number of epochs**: Train for 5 epochs. **Implementation Steps:** 1. Implement the neural network class `MNIST_Net`. 2. Write a function `train_model` that trains the neural network on the MNIST dataset. 3. Ensure you handle data loading, model instantiation, optimizer setup, and loss computation within the `train_model` function. **Expected Input and Output Formats:** - **Input**: None (the function should internally handle the dataset loading using torchvision). - **Output**: The function should print the training loss after each epoch. **Constraint:** - Ensure you adhere to the given network architecture and training details. ```python import torch import torch.nn as nn import torch.nn.functional as F import torchvision import torchvision.transforms as transforms # Part 1: Implement the neural network class MNIST_Net(nn.Module): def __init__(self): super(MNIST_Net, self).__init__() # Define the layers as per the architecture self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1) self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1) self.fc1 = nn.Linear(64*7*7, 128) self.fc2 = nn.Linear(128, 10) def forward(self, x): # Implement the forward pass as per the architecture x = F.relu(self.conv1(x)) x = F.max_pool2d(x, 2) x = F.relu(self.conv2(x)) x = F.max_pool2d(x, 2) x = x.view(-1, 64*7*7) # Flatten the tensor x = F.relu(self.fc1(x)) x = F.softmax(self.fc2(x), dim=1) return x # Part 2: Write the training function def train_model(): # Loading the MNIST dataset transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))]) trainset = torchvision.datasets.MNIST(root=\'./data\', train=True, download=True, transform=transform) trainloader = torch.utils.data.DataLoader(trainset, batch_size=100, shuffle=True) # Model instantiation model = MNIST_Net() # Define the loss function and optimizer criterion = nn.CrossEntropyLoss() optimizer = torch.optim.SGD(model.parameters(), lr=0.01) # Training loop for epoch in range(5): # Loop over the dataset multiple times running_loss = 0.0 for i, (inputs, labels) in enumerate(trainloader): # Zero the parameter gradients optimizer.zero_grad() # Forward pass outputs = model(inputs) loss = criterion(outputs, labels) # Backward pass and optimization loss.backward() optimizer.step() # Print statistics running_loss += loss.item() if i % 100 == 99: # Print every 100 mini-batches print(f\'[Epoch {epoch + 1}, Batch {i + 1}] loss: {running_loss / 100:.3f}\') running_loss = 0.0 train_model() ``` **Notes:** - Ensure to define the neural network and training process exactly as specified. - You need to handle downloading and loading of the MNIST dataset automatically using `torchvision`.","solution":"import torch import torch.nn as nn import torch.nn.functional as F import torchvision import torchvision.transforms as transforms # Part 1: Implement the neural network class MNIST_Net(nn.Module): def __init__(self): super(MNIST_Net, self).__init__() # Define the layers as per the architecture self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1) self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1) self.fc1 = nn.Linear(64*7*7, 128) self.fc2 = nn.Linear(128, 10) def forward(self, x): # Implement the forward pass as per the architecture x = F.relu(self.conv1(x)) x = F.max_pool2d(x, 2) x = F.relu(self.conv2(x)) x = F.max_pool2d(x, 2) x = x.view(-1, 64*7*7) # Flatten the tensor x = F.relu(self.fc1(x)) x = self.fc2(x) return x # Part 2: Write the training function def train_model(): # Loading the MNIST dataset transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))]) trainset = torchvision.datasets.MNIST(root=\'./data\', train=True, download=True, transform=transform) trainloader = torch.utils.data.DataLoader(trainset, batch_size=100, shuffle=True) # Model instantiation model = MNIST_Net() # Define the loss function and optimizer criterion = nn.CrossEntropyLoss() optimizer = torch.optim.SGD(model.parameters(), lr=0.01) # Training loop for epoch in range(5): # Loop over the dataset multiple times running_loss = 0.0 for i, (inputs, labels) in enumerate(trainloader): # Zero the parameter gradients optimizer.zero_grad() # Forward pass outputs = model(inputs) loss = criterion(outputs, labels) # Backward pass and optimization loss.backward() optimizer.step() # Print statistics running_loss += loss.item() if i % 100 == 99: # Print every 100 mini-batches print(f\'[Epoch {epoch + 1}, Batch {i + 1}] loss: {running_loss / 100:.3f}\') running_loss = 0.0 if __name__ == \\"__main__\\": train_model()"},{"question":"# Python Coding Assessment Question **Topic**: Generics, TypeVar, and Callable **Task**: Implement a class `Cache` that acts as a generic cache system with the following functionalities: 1. **Initialization**: Takes in any `Callable` function during initialization. 2. **Cache call**: When the `Callable` function is called through `Cache`, it stores the result in an internal dictionary using the function arguments as the key. For each unique set of arguments, the function should only compute the result once and return the cached result for subsequent calls. **Function Signature**: ```python from typing import Callable, TypeVar, Dict, Tuple T = TypeVar(\'T\') R = TypeVar(\'R\') class Cache: def __init__(self, func: Callable[[T], R]) -> None: pass def cached_call(self, *args: T) -> R: pass ``` **Detailed Requirements**: 1. The class should support any function that takes one or more arguments and returns a result. 2. Use a dictionary to store results of previous calls (`cache`). 3. Ensure thread-safety to handle concurrent calls correctly. 4. Appropriate use of type hints for the input function and the cache. **Example**: ```python # Sample function to be cached def multiply(a: int, b: int) -> int: print(f\\"Calculating {a} * {b}\\") return a * b # Creating a cache instance for the multiply function cached_multiply = Cache(multiply) # First call, should compute and store result print(cached_multiply.cached_call(2, 3)) # Output: Calculating 2 * 3 n 6 # Second call with same arguments, should return cached result without computing print(cached_multiply.cached_call(2, 3)) # Output: 6 ``` **Notes**: 1. The student\'s implementation should prevent redundant calculations for the same set of arguments through caching. 2. The caching mechanism should handle multiple types for function arguments and return values (generics). 3. Ensure the solution works efficiently even in the presence of concurrent function calls. **Constraints**: - Python version: 3.10 or above. - The cache dictionary should use function arguments tuples as keys. - Test the function with varying input types and ensure type safety with appropriate type hints. **Performance Requirements**: - Ensure O(1) average time complexity for retrieving a cached result. - The caching should function correctly under concurrency with minimal overhead. **Submit your solution by implementing the class `Cache` based on the described requirements. Validate your implementation with the provided example and additional test cases.**","solution":"from typing import Callable, TypeVar, Dict, Tuple import threading T = TypeVar(\'T\') R = TypeVar(\'R\') class Cache: def __init__(self, func: Callable[[T], R]) -> None: self.func = func self.cache: Dict[Tuple[T, ...], R] = {} self.lock = threading.Lock() def cached_call(self, *args: T) -> R: key = tuple(args) with self.lock: if key not in self.cache: self.cache[key] = self.func(*args) return self.cache[key]"},{"question":"# Question: Implement Custom Operator Profiling and API Usage Logging in PyTorch You are tasked to demonstrate your proficiency with PyTorch\'s profiling and API usage logging capabilities. This exercise will evaluate your understanding of PyTorch\'s advanced features for monitoring and extending its functionality. Task: 1. **Custom Operator Profiling**: - Implement a Python function using the `torch.autograd.profiler` to profile a custom operation within a simple neural network. - Define a custom operation within the network. - Add callbacks to log the entry and exit of the operation using `torch::addGlobalCallback` and record the input sizes. 2. **API Usage Logging**: - Set up an API usage handler that logs the usage of key PyTorch APIs (e.g., importing PyTorch, compiling TorchScript). - Ensure that the handler outputs log messages when these APIs are invoked. Requirements: 1. **Input Format**: - No user input required. The implemented functions should run as part of the script. 2. **Output Format**: - Log messages printed to the console indicating profiling results and API usage. Constraints: - Use PyTorch with version 1.8.0 or later. - Ensure that the implemented profiling does not significantly degrade performance (i.e., sampling rate control). Hints: - You can use Python\'s decorators to wrap functions for profiling and logging purposes. - Use `torch.profiler.profile` for the profiling section within Python code if `torch::addGlobalCallback` is not accessible directly in Python. Example Code: ```python import torch import torch.nn as nn import torch.optim as optim from torch.autograd import profiler # Define a simple neural network with a custom operation class SimpleNet(nn.Module): def __init__(self): super(SimpleNet, self).__init__() self.fc1 = nn.Linear(28 * 28, 128) self.fc2 = nn.Linear(128, 10) def forward(self, x): x = torch.flatten(x, start_dim=1) x = self.fc1(x) x = self.custom_operation(x) x = self.fc2(x) return x def custom_operation(self, x): return 2 * x + 3 # Simple custom operation for demonstration # Implement profiling function def profile_model(): model = SimpleNet() input_data = torch.randn((32, 1, 28, 28)) with profiler.profile(with_stack=True) as prof: output = model(input_data) print(prof.key_averages().table(sort_by=\\"cpu_time_total\\")) # Implement logging function def log_api_usage(): def api_usage_logger(event_name): print(f\\"API was used: {event_name}\\") torch._C._log_api_usage_once = api_usage_logger # Example of enabling logging model = SimpleNet() # Triggers some API usage input_data = torch.randn((32, 1, 28, 28)) output = model(input_data) if __name__ == \\"__main__\\": profile_model() log_api_usage() ``` Expected Output: Upon running the script, you should see log messages indicating the profiling results of the custom operation within the model and API usage log entries.","solution":"import torch import torch.nn as nn from torch.autograd import profiler # Define a simple neural network with a custom operation class SimpleNet(nn.Module): def __init__(self): super(SimpleNet, self).__init__() self.fc1 = nn.Linear(28 * 28, 128) self.fc2 = nn.Linear(128, 10) def forward(self, x): x = torch.flatten(x, start_dim=1) x = self.fc1(x) x = self.custom_operation(x) x = self.fc2(x) return x def custom_operation(self, x): return 2 * x + 3 # Simple custom operation for demonstration # Implement profiling function def profile_model(): model = SimpleNet() input_data = torch.randn((32, 1, 28, 28)) with profiler.profile(with_stack=True) as prof: output = model(input_data) print(prof.key_averages().table(sort_by=\\"cpu_time_total\\")) # Implement logging function def log_api_usage(): def api_usage_logger(event_name): print(f\\"API was used: {event_name}\\") torch._C._log_api_usage_once = api_usage_logger # Example of enabling logging model = SimpleNet() # Triggers some API usage input_data = torch.randn((32, 1, 28, 28)) output = model(input_data) if __name__ == \\"__main__\\": profile_model() log_api_usage()"},{"question":"**Custom PyTorch nn.Module with Hooks** In this question, you will be required to implement a custom `nn.Module` class, apply specific hooks, and demonstrate the behavior when compiled using `torch.compile`. The goal is to assess your understanding of module hooks, their behavior, and the implications when using `torch.compile`. # Requirements 1. **Custom Module Implementation:** - Implement a custom `nn.Module` called `CustomModel` that includes at least: - A forward method performing basic operations using `torch.nn` layers (e.g., linear layer). - Pre-forward and post-forward hooks that print specific messages. - Backward hooks demonstrating some form of logging or print statements. 2. **Hook Registration:** - Register the following hooks for the `CustomModel`: - A pre-forward hook (`_forward_pre_hooks`). - A forward hook (`forward_hooks`). - A backward hook (`_backward_hooks`). 3. **Using `torch.compile`:** - Compile your `CustomModel` using `torch.compile`. - Ensure the hooks are properly installed before compilation and not modified afterward. - Highlight potential graph breaks or performance implications if relevant. # Constraints 1. **Input:** - Your custom model should take a tensor of shape `(batch_size, input_features)` as input. Assume `input_features=10` for this task. - Perform a simple forward pass consisting of at least one linear transformation. 2. **Output:** - Output of the `forward` method should be a tensor with appropriate transformations (shape may vary based on operations). 3. **Performance:** - Minimize performance degradation due to hooks by avoiding unnecessary graph breaks. - Do not modify hooks after compilation (consider default guard settings). # Example Usage ```python import torch import torch.nn as nn from torch._dynamo import compile class CustomModel(nn.Module): def __init__(self): super(CustomModel, self).__init__() self.linear = nn.Linear(10, 5) def forward(self, x): return self.linear(x) def pre_forward_hook(module, input): print(\\"Pre-forward hook executed\\") def forward_hook(module, input, output): print(\\"Forward hook executed\\") def backward_hook(module, grad_input, grad_output): print(\\"Backward hook executed\\") # Instantiate the model model = CustomModel() # Register hooks model.register_forward_pre_hook(pre_forward_hook) model.register_forward_hook(forward_hook) model.register_backward_hook(backward_hook) # Compile the model using torch.compile compiled_model = compile(model) # Create sample input input_tensor = torch.randn(32, 10) # Perform forward pass output = compiled_model(input_tensor) # Compute loss and perform backward pass loss = output.sum() loss.backward() ``` In this example, the `CustomModel` demonstrates hook registration, compilation, and execution. You can extend this template to implement your solution while fulfilling the specified requirements.","solution":"import torch import torch.nn as nn import torch._dynamo as dynamo class CustomModel(nn.Module): def __init__(self): super(CustomModel, self).__init__() self.linear = nn.Linear(10, 5) def forward(self, x): return self.linear(x) def pre_forward_hook(module, input): print(\\"Pre-forward hook executed\\") def forward_hook(module, input, output): print(\\"Forward hook executed\\") def backward_hook(module, grad_input, grad_output): print(\\"Backward hook executed\\") # Instantiate the model model = CustomModel() # Register hooks model.register_forward_pre_hook(pre_forward_hook) model.register_forward_hook(forward_hook) model.register_full_backward_hook(backward_hook) # register_full_backward_hook for backward hook # Compile the model using torch.compile compiled_model = dynamo.optimize(\\"eager\\")(model) # Create sample input input_tensor = torch.randn(32, 10) # Perform forward pass output = compiled_model(input_tensor) # Compute loss and perform backward pass loss = output.sum() loss.backward()"},{"question":"You are tasked with writing a Python function to explore the Unix password database using the `pwd` module. This task is to assess your understanding of interacting with system-level information and effectively managing user data. Function Signature ```python def get_user_info(user_identifier: str) -> dict: pass ``` Objective Implement the function `get_user_info` that takes a `user_identifier` which can be either a user\'s numeric ID (UID) as a string or a user\'s login name. The function should return the user\'s detailed information in a dictionary with the following keys: - \\"name\\" (string) - \\"password\\" (string) - \\"uid\\" (integer) - \\"gid\\" (integer) - \\"comment\\" (string) - \\"home\\" (string) - \\"shell\\" (string) If the `user_identifier` corresponds to a UID, use `pwd.getpwuid`; if it is a username, use `pwd.getpwnam`. Handle cases where the user_identifier is not found by returning an empty dictionary. Example ```python # Example 1 print(get_user_info(\\"1000\\")) # Output: # { # \\"name\\": \\"username\\", # \\"password\\": \\"x\\", # \\"uid\\": 1000, # \\"gid\\": 1000, # \\"comment\\": \\"User Name\\", # \\"home\\": \\"/home/username\\", # \\"shell\\": \\"/bin/bash\\" # } # Example 2 print(get_user_info(\\"nonexistentuser\\")) # Output: # {} ``` Constraints 1. `user_identifier` will be a string containing either a numeric UID or a login name. 2. You may assume that the input is well-formed (a valid string representing either a UID or a username). 3. The function should handle any exceptions arising from querying non-existent users gracefully, by returning an empty dictionary. Notes - If you are running this code on a non-Unix-like operating system, it may not work as expected due to the limitations of the `pwd` module. Provide a robust and efficient solution to this problem. Ensure that your code is well-documented, explaining the steps and logic clearly.","solution":"import pwd def get_user_info(user_identifier: str) -> dict: Returns the user\'s detailed information in a dictionary if the given user_identifier exists in the Unix password database, otherwise returns an empty dictionary. :param user_identifier: Either a user\'s numeric ID (UID) as a string or a user\'s login name. :return: A dictionary with user information if found, else an empty dictionary. try: if user_identifier.isdigit(): # If user_identifier is numeric, interpret it as UID user_info = pwd.getpwuid(int(user_identifier)) else: # Otherwise, interpret it as a username user_info = pwd.getpwnam(user_identifier) return { \\"name\\": user_info.pw_name, \\"password\\": user_info.pw_passwd, \\"uid\\": user_info.pw_uid, \\"gid\\": user_info.pw_gid, \\"comment\\": user_info.pw_gecos, \\"home\\": user_info.pw_dir, \\"shell\\": user_info.pw_shell } except KeyError: # If the user_identifier does not exist, return an empty dictionary return {}"},{"question":"**Plot Customization with Seaborn** In this assessment, you will demonstrate your understanding of data visualization using Seaborn, specifically focusing on plot limits and customization. Problem Statement: You are given two lists of numbers representing the x and y coordinates of data points: - `x_coords = [0, 1, 2, 3, 4, 5]` - `y_coords = [0, 1, 4, 9, 16, 25]` Your task is to create a line plot using `seaborn.objects` following these steps: 1. Plot the given data points as a line plot with circular markers at each point. 2. Set the x-axis limits from -1 to 6 and y-axis limits from -5 to 30. 3. Invert the y-axis, so the plot appears upside down. 4. Save the plot as `customized_plot.png`. You are required to use `seaborn.objects` (as shown in the provided documentation) to accomplish these tasks. Input - `x_coords`: List of integers representing the x-coordinates. - `y_coords`: List of integers representing the y-coordinates. Output - A file named `customized_plot.png` containing the customized plot. Constraints - Input lists will always have the same length. - Each input list will contain between 2 and 100 integers. Example ```python x_coords = [0, 1, 2, 3, 4, 5] y_coords = [0, 1, 4, 9, 16, 25] # Your code should produce a plot saved as \'customized_plot.png\' ``` Solution Template ```python import seaborn.objects as so def create_customized_plot(x_coords, y_coords): # Create the plot with the given data p = so.Plot(x=x_coords, y=y_coords).add(so.Line(marker=\\"o\\")) # Customize the plot limits p = p.limit(x=(-1, 6), y=(-5, 30)) # Invert the y-axis p = p.limit(y=(30, -5)) # Save the plot as \'customized_plot.png\' p.save(\\"customized_plot.png\\") # Example usage x_coords = [0, 1, 2, 3, 4, 5] y_coords = [0, 1, 4, 9, 16, 25] create_customized_plot(x_coords, y_coords) ```","solution":"import seaborn.objects as so def create_customized_plot(x_coords, y_coords): Creates a customized line plot with the given x and y coordinates, sets axis limits, inverts the y-axis, and saves the plot as \'customized_plot.png\'. Parameters: - x_coords: List[int] - y_coords: List[int] # Create the plot with the given data p = so.Plot(x=x_coords, y=y_coords).add(so.Line(marker=\\"o\\")) # Customize the plot limits p = p.limit(x=(-1, 6), y=(-5, 30)) # Invert the y-axis p = p.limit(y=(30, -5)) # Save the plot as \'customized_plot.png\' p.save(\\"customized_plot.png\\")"},{"question":"# Base64 Encoding and Decoding Challenge Background The `base64` module in Python provides capabilities to encode binary data to ASCII characters and decode them back to binary data, supporting several encoding schemes like Base16, Base32, Base64, Ascii85, and Base85. This is often used in tasks such as data transmission over media that are designed to deal with textual data. Objective You are required to implement a function that processes a dictionary containing various types of binary data by encoding each piece of data using different base64 encoding schemes and then decoding it back to its original form to verify the integrity of the encoding and decoding process. Function Specification ```python def process_data(data_dict): Process a dictionary with binary data by encoding and decoding each value using specified base64 encoding schemes. Parameters: - data_dict (dict): A dictionary where: - keys are strings representing the name of the data. - values are tuples where the first item is the binary data (bytes) and the second item is a string that specifies the encoding scheme to be used. The encoding scheme can be one of the following strings: \'base16\', \'base32\', \'base64\', \'urlsafe_base64\', \'ascii85\', \'base85\' Returns: - result (dict): A dictionary where: - keys match the input dictionary keys. - values are boolean indicating whether the data decoded back correctly after encoding (True if it matches the original binary data, False otherwise). pass ``` Input - `data_dict`: A dictionary where keys are strings (names of the data) and values are tuples: - First item of the tuple is the binary data (of type `bytes`). - Second item of the tuple is a string representing the encoding scheme (\'base16\', \'base32\', \'base64\', \'urlsafe_base64\', \'ascii85\', \'base85\'). Output - Return a dictionary where keys match the input dictionary keys and values are booleans: - `True` if encoding and then decoding the data correctly restores the original data. - `False` otherwise. Constraints - You may use any function from the `base64` module that fits the specified encoding schemes. Example ```python input_data = { \\"data1\\": (b\\"hello world\\", \\"base64\\"), \\"data2\\": (b\\"another example\\", \\"urlsafe_base64\\"), \\"data3\\": (b\\"testing data\\", \\"base32\\") } expected_result = { \\"data1\\": True, \\"data2\\": True, \\"data3\\": True } assert process_data(input_data) == expected_result ``` Notes - Carefully handle each encoding scheme and ensure that the resulting data after decoding matches the original data. - Return `False` for any data where the scheme is unknown or does not match the specified choices. - Validate that all input data conforms to the expected types and values to avoid errors during encoding or decoding.","solution":"import base64 def process_data(data_dict): Process a dictionary with binary data by encoding and decoding each value using specified base64 encoding schemes. Parameters: - data_dict (dict): A dictionary where: - keys are strings representing the name of the data. - values are tuples where the first item is the binary data (bytes) and the second item is a string that specifies the encoding scheme to be used. The encoding scheme can be one of the following strings: \'base16\', \'base32\', \'base64\', \'urlsafe_base64\', \'ascii85\', \'base85\' Returns: - result (dict): A dictionary where: - keys match the input dictionary keys. - values are boolean indicating whether the data decoded back correctly after encoding (True if it matches the original binary data, False otherwise). result = {} for key, (binary_data, encoding_scheme) in data_dict.items(): if not isinstance(binary_data, bytes) or not isinstance(encoding_scheme, str): result[key] = False continue try: if encoding_scheme == \'base16\': encoded = base64.b16encode(binary_data) decoded = base64.b16decode(encoded) elif encoding_scheme == \'base32\': encoded = base64.b32encode(binary_data) decoded = base64.b32decode(encoded) elif encoding_scheme == \'base64\': encoded = base64.b64encode(binary_data) decoded = base64.b64decode(encoded) elif encoding_scheme == \'urlsafe_base64\': encoded = base64.urlsafe_b64encode(binary_data) decoded = base64.urlsafe_b64decode(encoded) elif encoding_scheme == \'ascii85\': encoded = base64.a85encode(binary_data) decoded = base64.a85decode(encoded) elif encoding_scheme == \'base85\': encoded = base64.b85encode(binary_data) decoded = base64.b85decode(encoded) else: result[key] = False continue result[key] = decoded == binary_data except Exception as e: result[key] = False return result"},{"question":"**Advanced File Handling in Python using C APIs** **Objective:** Implement a Python function that reads a specific number of lines from a file and then writes a provided string as well as an object\'s string representation to another file using system-level APIs. **Task:** You need to implement two functions: 1. `read_lines_from_file(fd: int, num_lines: int) -> list[str]`: - Takes a file descriptor `fd` and an integer `num_lines`. - Reads and returns a list of `num_lines` lines from the file. - If fewer than `num_lines` lines are available, return all available lines. - Use `PyFile_GetLine` for reading lines. 2. `write_to_file(fd: int, content: str, obj: object) -> None`: - Takes a file descriptor `fd`, a string `content`, and an object `obj`. - Writes `content` and the string representation of `obj` to the file. - Use `PyFile_WriteString` to write `content`. - Use `PyFile_WriteObject` to write the string representation of `obj`. **Constraints:** 1. File descriptors used must be valid and associated with open files. 2. The operations should handle exceptions appropriately. 3. Ensure that no other file handling methods outside the specified C APIs are used directly for reading or writing in these functions. **Example:** ```python # Assume that \'fd1\' and \'fd2\' are valid file descriptors lines = read_lines_from_file(fd1, 5) # Suppose lines read are [\\"line1n\\", \\"line2n\\"] write_to_file(fd2, \\"End of linesn\\", lines) ``` In the example, the function `read_lines_from_file` reads up to 5 lines from the file associated with `fd1`. Then, `write_to_file` writes the string \\"End of linesn\\" and the string representation of the list of lines to the file associated with `fd2`. **Performance Requirements:** - The implementation should be efficient and avoid reading or writing more than necessary. - Handle large files gracefully within the constraints of available memory. **Note:** Given that these functions emulate low-level C API behavior, you won\'t actually interface directly with the C API in your Python code here. Instead, focus on the logic that would theoretically meet these requirements as if you had access to such C API functions directly in your Python environment.","solution":"def read_lines_from_file(fd: int, num_lines: int) -> list[str]: Reads a specified number of lines from a file given its file descriptor. Args: fd (int): File descriptor of the file to read from. num_lines (int): Number of lines to read from the file. Returns: list[str]: List of lines read from the file. import os lines = [] os.lseek(fd, 0, os.SEEK_SET) # Ensure we start from the beginning of the file # Loop to read lines up to the specified number while num_lines > 0: line = \'\' while True: char = os.read(fd, 1).decode(\'utf-8\') if char == \'\' or char == \'n\': # EOF or end of line if char == \'n\': line += char break line += char if line == \'\': # EOF break lines.append(line) num_lines -= 1 return lines def write_to_file(fd: int, content: str, obj: object) -> None: Writes a string and an object\'s string representation to a file given its file descriptor. Args: fd (int): File descriptor of the file to write to. content (str): The string content to write to the file. obj (object): The object whose string representation will be written to the file. import os # Write the content first os.write(fd, content.encode(\'utf-8\')) # Write the string representation of the object obj_str = repr(obj) # Using repr to get the string representation os.write(fd, obj_str.encode(\'utf-8\'))"},{"question":"**Question: Tensor DataType and Device Management** You are provided with a PyTorch tensor processing function that takes multiple tensors as input and performs arithmetic operations on them. Your task is to ensure the function is robust by handling tensors of various data types and ensuring they are on the same device before performing operations. # Requirements 1. Implement a function `process_tensors` that: - Takes a list of tensors as input. - Checks if all tensors are on the same device. If not, move all tensors to the CPU. - Ensures that all tensors have compatible data types for arithmetic operations based on PyTorch\'s type promotion rules. - Returns a single tensor which is the result of adding all the input tensors together. # Specifications - **Input**: List of tensors, for example `[tensor1, tensor2, tensor3, ...]`. - **Output**: Single tensor which is the sum of all input tensors. - **Constraints**: The function should be able to handle tensors with any combination of the data types listed in the documentation and handle device mismatches gracefully by moving tensors to the CPU. # Example Usage ```python import torch tensor1 = torch.ones((2, 3), dtype=torch.float32, device=\'cuda:0\') tensor2 = torch.ones((2, 3), dtype=torch.float64, device=\'cpu\') tensor3 = torch.ones((2, 3), dtype=torch.int32, device=\'cuda:1\') result = process_tensors([tensor1, tensor2, tensor3]) print(result) print(result.dtype) # Output should match the promoted data type print(result.device) # Output should be on CPU ``` # Implementation Guidelines - Consider using `torch.promote_types` to find the common data type for the tensors. - Use the `.to(device)` method to move tensors to the CPU if necessary. - Carefully handle scalar tensors and device type mismatches. # Tips - Review PyTorch\'s type promotion rules to ensure correct data type handling. - Ensure the function is efficient with tensor operations, leveraging PyTorch\'s capabilities. ```python def process_tensors(tensors): Processes a list of tensors, ensuring they are on the same device and have compatible data types, then returns their sum. Args: tensors (list of torch.Tensor): List of input tensors to be processed. Returns: torch.Tensor: Resultant tensor after summing all input tensors. # Implement your function here pass ``` Test your function with different tensor scenarios to ensure robustness and correctness.","solution":"import torch def process_tensors(tensors): Processes a list of tensors, ensuring they are on the same device and have compatible data types, then returns their sum. Args: tensors (list of torch.Tensor): List of input tensors to be processed. Returns: torch.Tensor: Resultant tensor after summing all input tensors. if not tensors: raise ValueError(\\"Input tensor list is empty\\") # Ensure all tensors are on the same device (CPU) tensors_on_cpu = [tensor.to(\'cpu\') for tensor in tensors] # Determine the resulting data type dtype = tensors_on_cpu[0].dtype for tensor in tensors_on_cpu: dtype = torch.promote_types(dtype, tensor.dtype) # Convert all tensors to the compatible data type tensors_converted = [tensor.to(dtype) for tensor in tensors_on_cpu] # Sum all tensors result = torch.zeros_like(tensors_converted[0]) for tensor in tensors_converted: result += tensor return result"},{"question":"Design a Python function that sends a GET request to a specified URL using the `http.client` module, retrieves the response, and saves the response body to a file. The function should handle both HTTP and HTTPS URLs appropriately. # Function Signature: ```python def fetch_and_save_url_content(url: str, file_name: str) -> None: pass ``` # Input: - `url` (str): A string representing the URL to fetch. - `file_name` (str): A string representing the name of the file where the response content should be saved. # Output: - This function doesn\'t return any value. It writes the content of the response to the specified file. # Constraints: - The URL will always be a valid HTTP or HTTPS URL. - If the response status code is not 200 (OK), the function should raise an exception with a message indicating the HTTP status code and reason. # Example: ```python fetch_and_save_url_content(\\"http://www.example.com\\", \\"example.html\\") ``` This should fetch the content from \\"http://www.example.com\\" and save it to the file \\"example.html\\". # Additional Requirements: 1. The function should be able to handle URLs with or without specified ports and with different HTTP methods. 2. It should also be robust enough to manage common exceptions associated with HTTP requests. # Tips: - Use the `urlparse` module to parse the URL and determine whether it is HTTP or HTTPS. - Utilize the `HTTPConnection` or `HTTPSConnection` classes based on the scheme (http or https). - Ensure that you handle reading the response in chunks if necessary, especially for large responses. - Remember to close the connections properly to avoid resource leaks.","solution":"import http.client from urllib.parse import urlparse def fetch_and_save_url_content(url: str, file_name: str) -> None: Fetches content from a URL and saves it to a file. Args: url (str): The URL to fetch. file_name (str): The name of the file to save the content to. Raises: Exception: If the response status code is not 200 (OK). parsed_url = urlparse(url) connection_class = http.client.HTTPConnection if parsed_url.scheme == \\"http\\" else http.client.HTTPSConnection connection = connection_class(parsed_url.netloc) try: connection.request(\\"GET\\", parsed_url.path or \\"/\\") response = connection.getresponse() if response.status != 200: raise Exception(f\\"Failed to fetch URL: {response.status} {response.reason}\\") with open(file_name, \\"wb\\") as file: while True: chunk = response.read(1024) if not chunk: break file.write(chunk) finally: connection.close()"},{"question":"Objective: Using the `glob` module, you are required to implement a function that searches for files in a specified directory (and its subdirectories) that match a given pattern and are larger than a specified size in bytes. The function should return a list of such file paths. Function Signature: ```python def find_large_files(pattern: str, min_size: int, root_dir: str = \'.\') -> list: pass ``` Input: 1. `pattern` (str): The pattern to match file names. This pattern should follow Unix shell-style wildcard rules. 2. `min_size` (int): The minimum size (in bytes) of the files to be matched. 3. `root_dir` (str): The root directory to start the search from. Default is the current directory (`\'.\'`). Output: - A list of file paths that match the given pattern and are larger than the specified size in bytes. The paths should be relative to the `root_dir`. Constraints: - You may not use any external libraries other than Python\'s standard library. - You should use the `glob` module to match the file patterns. - Ensure your solution is efficient and handles large directories gracefully. Example: ```python # Suppose the directory structure is as follows: # . # ├── file1.txt (size: 500 bytes) # ├── file2.log (size: 1200 bytes) # ├── sub_dir # │ ├── file3.txt (size: 1500 bytes) # │ └── file4.log (size: 200 bytes) # Given the pattern \'*.log\' and min_size of 1000 bytes, the function should return: # [\'./file2.log\'] # Given the pattern \'**/*.txt\' and min_size of 1000 bytes, the function should return: # [\'./sub_dir/file3.txt\'] def find_large_files(pattern: str, min_size: int, root_dir: str = \'.\') -> list: import glob import os result = [] for file_path in glob.iglob(os.path.join(root_dir, pattern), recursive=True): if os.path.isfile(file_path) and os.path.getsize(file_path) > min_size: result.append(file_path) return result ``` Implement the function `find_large_files` that performs the required tasks. Be sure to test your implementation with a variety of patterns and file sizes.","solution":"import glob import os def find_large_files(pattern: str, min_size: int, root_dir: str = \'.\') -> list: Searches for files in a specified directory and its subdirectories that match a given pattern and are larger than a specified size in bytes. Returns a list of such file paths. :param pattern: The pattern to match file names. This pattern should follow Unix shell-style wildcard rules. :param min_size: The minimum size (in bytes) of the files to be matched. :param root_dir: The root directory to start the search from. Default is the current directory (\'.\'). :return: A list of file paths that match the given pattern and are larger than the specified size in bytes. result = [] search_pattern = os.path.join(root_dir, \'**\', pattern) for file_path in glob.iglob(search_pattern, recursive=True): if os.path.isfile(file_path) and os.path.getsize(file_path) > min_size: result.append(file_path) # Converting all paths to be relative to root_dir result = [os.path.relpath(path, root_dir) for path in result] return result"},{"question":"# Task In this coding assessment, you are required to work with seaborn to visualize data in a meaningful way, employing layered marks to avoid overlap and enhance data interpretability. # Objective Given a dataset containing information about survival on the Titanic, your task is to create a seaborn object-based plot `so.Plot` that visualizes the relationship between `sex`, `survived`, and `age` using layered marks (dots, jitter, range, etc.). The plot should include: 1. Dots representing individual samples. 2. Jitter to prevent overlapping of dots. 3. A range representing the interquartile range (IQR) of ages for each combination of `sex` and `survived`. 4. Any additional transformations or shifts you find necessary to make the plot more interpretable. # Dataset Load the dataset using seaborn: ```python import seaborn as sns titanic = sns.load_dataset(\\"titanic\\") ``` # Requirements 1. Use `so.Plot` from `seaborn.objects`. 2. Create a plot with the `titanic` dataset visualizing `sex` on the x-axis and `age` on the y-axis, distinguished by the `survived` variable. 3. Add layered marks as specified: * Dots for individual data points using `so.Dots()` * Jitter to spread dots to reduce overlap using `so.Jitter()` * Range to show the interquartile range (IQR) of ages using `so.Range()` and `so.Perc([25, 75])` * Utilize `so.Shift()` if necessary to make the plot more readable. 4. Ensure the plot conveys meaningful insights into the distribution of ages, split by `sex` and `survived` status. # Input and Output Formats *Input:* - The code does not require additional input beyond the provided dataset. *Function and Visualization Output:* - Implement the function `create_seaborn_plot()` that generates and returns the plot. Include the necessary imports and dataset loading within the function. ```python def create_seaborn_plot(): import seaborn.objects as so import seaborn as sns # Load the dataset titanic = sns.load_dataset(\\"titanic\\") # Create the plot plot = ( so.Plot(titanic, \\"sex\\", \\"age\\", color=\\"survived\\") .add(so.Dots(), so.Jitter()) .add(so.Range(), so.Perc([25, 75]), so.Shift(y=0.2)) ) return plot ``` # Constraints: - Ensure the function runs efficiently with the provided dataset. - The function should include suitable comments to indicate the purpose of each operation. # Performance Requirements: - The plot must be generated within a reasonable timeframe and should be optimized for performance.","solution":"def create_seaborn_plot(): import seaborn.objects as so import seaborn as sns # Load the dataset titanic = sns.load_dataset(\\"titanic\\") # Create the plot plot = ( so.Plot(titanic, x=\\"sex\\", y=\\"age\\", color=\\"survived\\") .add(so.Dots(), so.Jitter(0.15)) # Add dots with jitter to reduce overlap .add(so.Range(), so.Perc([25, 75]), so.Shift(y=0.2)) # Add range for IQR ) return plot # Note: Here we will create a function to show the plot since just returning it won\'t display in many environments. if __name__ == \\"__main__\\": plot = create_seaborn_plot() plot.show()"},{"question":"Create a Python function to interact with a given URL while managing cookies for session persistence. The function should: 1. Use `http.cookiejar` to create a cookie jar and an appropriate policy for handling cookies. 2. Set up a request opener using `urllib.request` that maintains the session cookies across requests. 3. Perform the following sequence of actions: - First, make a GET request to the provided URL. - Then, make a POST request to the same URL with some data, reusing the session cookies. # Function Signature: ```python def manage_session_with_cookies(url: str): pass ``` # Input: - `url` (str): The URL to interact with. # Output: - The function should print the response content of both the GET and POST requests. # Constraints: - Ensure the code handles errors gracefully (e.g., network errors, invalid URLs). - Use `http.cookiejar.MozillaCookieJar` to simulate cookie persistence similar to a browser. - Make sure the cookie policy strictly follows RFC 2965. # Example Usage: ```python url = \\"http://example.com\\" manage_session_with_cookies(url) ``` This will: 1. Print the response content from the initial GET request. 2. Print the response content from the subsequent POST request using the same cookies. # Notes: - For demonstration purposes, use dummy data for the POST request. - The specific endpoints and data format for the POST request can vary, so you can use placeholders in your implementation.","solution":"import urllib.request import urllib.parse import http.cookiejar import logging def manage_session_with_cookies(url: str): Interacts with the given URL while managing cookies for session persistence. Performs the following steps: 1. Makes GET request to the provided URL. 2. Makes POST request to the same URL with some data, reusing the session cookies. Prints the response content of both requests. try: # Initialize a cookie jar to handle cookies cj = http.cookiejar.MozillaCookieJar() handler = urllib.request.HTTPCookieProcessor(cj) opener = urllib.request.build_opener(handler) # Make a GET request get_response = opener.open(url) print(\\"GET Response:\\") print(get_response.read().decode(\'utf-8\')) # Data for the POST request data = urllib.parse.urlencode({\'key1\': \'value1\', \'key2\': \'value2\'}).encode(\'utf-8\') # Make a POST request reusing the session cookies post_response = opener.open(url, data=data) print(\\"POST Response:\\") print(post_response.read().decode(\'utf-8\')) except Exception as e: logging.error(\\"An error occurred: %s\\", str(e)) # The function can now be used with a URL # Example usage: # manage_session_with_cookies(\\"http://example.com\\")"},{"question":"# Question: Meta Device and Tensor Manipulation In this exercise, you will demonstrate your understanding of PyTorch\'s meta device by creating and manipulating tensors and neural network modules using the meta device. You will also be required to handle the limitations of meta tensors. Task 1. **Loading a Tensor on the Meta Device**: - Save a random tensor to a file. - Load this tensor back from the file onto the meta device. 2. **Creating and Manipulating Neural Network Modules**: - Create a neural network module (`torch.nn.Linear`) with specified input and output features on the meta device. - Move this module to a different device (e.g., CPU) without data initialization. - Reinitialize the parameters of this module manually. 3. **Operations on Meta Tensors**: - Demonstrate an operation that can be performed on meta tensors. - Attempt an operation that is data-dependent (which should fail) and handle the exception. Implementation Write a function `meta_tensor_operations` that performs the following steps: 1. Save a random tensor to a file and then load it on the meta device. 2. Create a `torch.nn.Linear` module on the meta device with input features `in_features` and output features `out_features`. 3. Move the `torch.nn.Linear` module to the CPU using the `to_empty` method. 4. Manually reinitialize the parameters of the module on the CPU. 5. Perform an operation such as addition on the tensor loaded on the meta device. 6. Attempt a data-dependent operation like `nonzero` on the meta tensor and handle the `NotImplementedError` exception. # Input - `in_features` (int): The number of input features for the `torch.nn.Linear` module. - `out_features` (int): The number of output features for the `torch.nn.Linear` module. # Output - A dictionary containing: - The reinitialized parameters of the `torch.nn.Linear` module on the CPU. - The result of the addition operation on the meta tensor. - A boolean flag indicating whether the `NotImplementedError` was caught. # Example ```python def meta_tensor_operations(in_features, out_features): # Your implementation here result = meta_tensor_operations(20, 30) print(result) ``` The output should be a dictionary with keys `module_parameters`, `addition_result`, and `nonzero_exception`. Notes - Ensure to handle file operations properly. - Use `torch.randn`, `torch.save`, `torch.load`, `torch.device`, `torch.empty_like`, and other relevant PyTorch methods. - Reinitialize parameters of the `torch.nn.Linear` module as you see fit after moving the module to the CPU.","solution":"import torch def meta_tensor_operations(in_features, out_features): # Step 1: Save a random tensor to a file and then load it on the meta device tensor = torch.randn(5, 5) # Create a random tensor torch.save(tensor, \\"tensor.pt\\") # Save the tensor to a file loaded_tensor = torch.load(\\"tensor.pt\\", map_location=torch.device(\'meta\')) # Load tensor to meta device # Step 2: Create a torch.nn.Linear module on the meta device meta_linear = torch.nn.Linear(in_features, out_features, device=\'meta\') # Step 3: Move the torch.nn.Linear module to the CPU using the to_empty method cpu_linear = meta_linear.to_empty(device=\'cpu\') # Step 4: Manually reinitialize the parameters of the module on the CPU with torch.no_grad(): cpu_linear.weight = torch.nn.Parameter(torch.randn_like(cpu_linear.weight)) if cpu_linear.bias is not None: cpu_linear.bias = torch.nn.Parameter(torch.randn_like(cpu_linear.bias)) # Step 5: Perform an operation such as addition on the tensor loaded on the meta device addition_result = loaded_tensor.add(torch.randn(5, 5, device=\'meta\')) # Step 6: Attempt a data-dependent operation like nonzero on the meta tensor and handle the exception nonzero_exception = False try: _ = loaded_tensor.nonzero() # This should raise a NotImplementedError except NotImplementedError: nonzero_exception = True # Output results in the specified dictionary format return { \\"module_parameters\\": { \\"weight\\": cpu_linear.weight, \\"bias\\": cpu_linear.bias if cpu_linear.bias is not None else None }, \\"addition_result\\": addition_result, \\"nonzero_exception\\": nonzero_exception, }"},{"question":"Context: In Python, objects, types, and classes can be highly customized through the implementation of special methods. These methods allow controlling attribute access, implementing custom behavior for operators, and defining how objects are called, among other capabilities. This exercise is designed to test your understanding of these advanced customization techniques. Problem Statement: You are required to implement a custom class `Library` that behaves both as a callable object and a container for books. The class should support the following operations: 1. **Initialization:** ```python lib = Library(name) # Initialize with a library name ``` 2. **Adding Books:** You should be able to add books to the library using subscript notation: ```python lib[isbn] = book_title # Add a book with a given ISBN and title ``` 3. **Deleting Books:** Books should be able to be removed using the `del` statement: ```python del lib[isbn] # Remove the book with the given ISBN ``` 4. **Fetching Books:** Retrieve the book title using the ISBN: ```python title = lib[isbn] ``` 5. **Listing Books:** Implement an efficient iterator that can iterate over all book titles in the library: ```python for book in lib: print(book) ``` 6. **Callable Check:** The library should be callable to accept the ISBN and return the book title if it exists: ```python title = lib(isbn) ``` Requirements: - Define the `Library` class to fulfill the above functionalities. - Implement appropriate special methods to support these operations. - Raise a `KeyError` if a book is attempted to be fetched or deleted and the ISBN does not exist. - Use the `__slot__` attribute to optimize memory usage, restricting the instance attributes to `name` and `_books`. Example: ```python lib = Library(\\"City Library\\") # Adding books lib[\\"978-3-16-148410-0\\"] = \\"The Great Gatsby\\" lib[\\"978-1-40-289462-6\\"] = \\"1984\\" # Fetching books print(lib[\\"978-3-16-148410-0\\"]) # Output: The Great Gatsby # Iterating over books for book in lib: print(book) # Output: # The Great Gatsby # 1984 # Checking callable behavior print(lib(\\"978-1-40-289462-6\\")) # Output: 1984 # Deleting books del lib[\\"978-3-16-148410-0\\"] ``` Constraints: - The ISBN is a string and the book title is also a string. - Assume all ISBN and titles provided are valid and unique for this task. Expectations: - Proper use of special methods like `__getitem__`, `__setitem__`, `__delitem__`, `__iter__`, and `__call__`. - Usage of `__slots__` to limit instance attributes for memory optimization.","solution":"class Library: __slots__ = (\'name\', \'_books\') def __init__(self, name): self.name = name self._books = {} def __setitem__(self, isbn, title): self._books[isbn] = title def __getitem__(self, isbn): if isbn in self._books: return self._books[isbn] else: raise KeyError(f\'Book with ISBN {isbn} not found.\') def __delitem__(self, isbn): if isbn in self._books: del self._books[isbn] else: raise KeyError(f\'Book with ISBN {isbn} not found.\') def __iter__(self): return iter(self._books.values()) def __call__(self, isbn): return self.__getitem__(isbn)"},{"question":"# Task You are tasked with creating a simulation of a simplified card game where players draw cards until the deck is empty. Each player draws one card per turn from a shuffled deck and the game keeps track of the number of cards each player has drawn. At the end of the game, the player with the most cards wins. # Requirements 1. **Shuffling the Deck**: - Implement a function `shuffle_deck(deck: List[str]) -> List[str]` that shuffles the deck of cards. 2. **Drawing Cards**: - Implement a function `draw_card(deck: List[str]) -> Tuple[str, List[str]]` that removes the top card from the deck and returns the card along with the updated deck. 3. **Game Simulation**: - Implement a function `simulate_game(num_players: int, deck: List[str]) -> int` that: - Shuffles the deck - Each player draws one card per turn in a round-robin fashion until the deck is empty - Returns the index (0-based) of the player with the most cards # Input and Output Formats - The `deck` is a list of strings representing card names, each element being unique. - The `num_players` is an integer representing the number of players which is greater than 0. - The `shuffle_deck` function returns a shuffled version of the input deck. - The `draw_card` function returns a tuple where the first element is a string (the drawn card) and the second element is the list (updated deck). - The `simulate_game` function returns an integer representing the index of the player with the most cards drawn. # Example ```python from typing import List, Tuple def shuffle_deck(deck: List[str]) -> List[str]: import random random.shuffle(deck) return deck def draw_card(deck: List[str]) -> Tuple[str, List[str]]: card = deck.pop(0) return card, deck def simulate_game(num_players: int, deck: List[str]) -> int: deck = shuffle_deck(deck) players = [0] * num_players player_turn = 0 while deck: _, deck = draw_card(deck) players[player_turn] += 1 player_turn = (player_turn + 1) % num_players return players.index(max(players)) # Example usage deck = [\\"5H\\", \\"2D\\", \\"KC\\", \\"9S\\", \\"AH\\", \\"7C\\"] num_players = 3 print(simulate_game(num_players, deck)) # Output might vary due to shuffling ``` # Constraints - You cannot use recurrent imports of the random module in each function. - The shuffle function should use `random.shuffle()` for uniform distribution. - You should make sure to handle edge cases like when the number of players is greater than the number of cards elegantly by ensuring no player draws more cards than are available. # Performance Requirements - Ensure the `simulate_game` function has a time complexity of O(n) where `n` is the number of cards in the deck.","solution":"from typing import List, Tuple import random def shuffle_deck(deck: List[str]) -> List[str]: Shuffles the deck of cards. Parameters: deck (List[str]): A list of card names. Returns: List[str]: A shuffled list of card names. random.shuffle(deck) return deck def draw_card(deck: List[str]) -> Tuple[str, List[str]]: Draws the top card from the deck and returns it along with the updated deck. Parameters: deck (List[str]): A list of card names in the current deck. Returns: Tuple[str, List[str]]: A tuple containing the drawn card and the updated deck. card = deck.pop(0) return card, deck def simulate_game(num_players: int, deck: List[str]) -> int: Simulates a card game where players draw cards from the deck until it is empty. Parameters: num_players (int): The number of players in the game. deck (List[str]): A list of card names. Returns: int: The index of the player with the most cards drawn. deck = shuffle_deck(deck) players = [0] * num_players player_turn = 0 while deck: _, deck = draw_card(deck) players[player_turn] += 1 player_turn = (player_turn + 1) % num_players return players.index(max(players))"},{"question":"# Email Header Handling with Non-ASCII Characters You are working on an email client application and need to handle email headers that may contain non-ASCII characters. Your task is to implement a set of functions that create, encode, and decode email headers using the `email.header` module. Task 1: Creating a Header Write a function `create_header(subject: str, charset: str) -> email.header.Header` that takes a subject string and a character set (e.g., \'utf-8\') and returns an `email.header.Header` object with the specified encoding. ```python def create_header(subject: str, charset: str) -> email.header.Header: Create and return a MIME-compliant email header with the given subject and character set. Parameters: - subject (str): The subject of the email, which may include non-ASCII characters. - charset (str): The character set to use for encoding the header. Returns: - email.header.Header: The encoded email header object. pass ``` Task 2: Encoding a Header Write a function `encode_header(header: email.header.Header, maxlinelen: int = 76) -> str` that takes an `email.header.Header` object and a maximum line length, then returns the encoded header as a string. ```python def encode_header(header: email.header.Header, maxlinelen: int = 76) -> str: Encode the given email header into an RFC-compliant format. Parameters: - header (email.header.Header): The header object to encode. - maxlinelen (int): The maximum line length for the encoded header. Returns: - str: The encoded header as a string. pass ``` Task 3: Decoding a Header Write a function `decode_header_value(header_str: str) -> str` that takes an encoded header string and returns the decoded header value as a string. ```python def decode_header_value(header_str: str) -> str: Decode the given encoded email header string. Parameters: - header_str (str): The encoded header string. Returns: - str: The decoded header value. pass ``` # Example Usage ```python if __name__ == \\"__main__\\": # Create a Header object for a subject with non-ASCII characters header = create_header(\'pxf6stal subject\', \'iso-8859-1\') # Encode the Header object encoded_header = encode_header(header) print(f\\"Encoded Header: {encoded_header}\\") # Decode the encoded header back to its original value decoded_header = decode_header_value(encoded_header) print(f\\"Decoded Header: {decoded_header}\\") ``` # Constraints - The charset for the \'subject\' string will always be a valid character set recognizable by the `Charset` class. - The \'subject\' string can contain any Unicode characters. # Hints - Use the `Header` class from the `email.header` module to create headers. - Utilize the `Header.append` method to add strings to the header if needed. - The `Header.encode` method can be used to get the encoded string representation of the header. - Use the `decode_header` function from the `email.header` module to decode an encoded header string. Notes These tasks will test students\' ability to work with Python modules that handle email specific requirements, particularly dealing with international character encodings. # Performance The functions should handle the typical lengths of email headers efficiently, considering proper handling and splitting of long lines to meet RFC compliance.","solution":"from email.header import Header, decode_header from typing import Union def create_header(subject: str, charset: str) -> Header: Create and return a MIME-compliant email header with the given subject and character set. Parameters: - subject (str): The subject of the email, which may include non-ASCII characters. - charset (str): The character set to use for encoding the header. Returns: - email.header.Header: The encoded email header object. header = Header(subject, charset) return header def encode_header(header: Header, maxlinelen: int = 76) -> str: Encode the given email header into an RFC-compliant format. Parameters: - header (email.header.Header): The header object to encode. - maxlinelen (int): The maximum line length for the encoded header. Returns: - str: The encoded header as a string. return header.encode(maxlinelen=maxlinelen) def decode_header_value(header_str: str) -> str: Decode the given encoded email header string. Parameters: - header_str (str): The encoded header string. Returns: - str: The decoded header value. decoded_fragments = decode_header(header_str) decoded_string = \'\'.join( fragment.decode(charset or \'utf-8\') if isinstance(fragment, bytes) else fragment for fragment, charset in decoded_fragments ) return decoded_string"},{"question":"# Challenge: File System Path Management You are tasked with managing paths in a multi-platform environment. Your goal is to create a function that processes a list of paths and determines specific attributes about these paths. You will need to demonstrate your ability to use a variety of `os.path` functions. Task Description: Create a function `analyze_paths(paths: List[str]) -> Dict[str, Any]` Input: - `paths`: A list of strings, where each string is a file system path. Output: - A dictionary with the following keys: - `\'common_prefix\'`: The longest common prefix of all paths. - `\'common_path\'`: The longest common sub-path. - `\'absolute_paths\'`: A list of absolute paths corresponding to the input paths. - `\'base_names\'`: A list of base names (the final component of the path) for the input paths. - `\'dir_names\'`: A list of directory names (everything except the final component) for the input paths. - `\'path_sizes\'`: A dictionary mapping each path to its size in bytes. If a path does not exist, its size should be -1. - `\'exist_check\'`: A dictionary mapping each path to a boolean indicating whether the path exists or not. Constraints: - You may assume all input paths are valid paths for the operating system the function is running on. - Handle exceptions where necessary, especially for operations that may fail if a file does not exist. Example: ```python from typing import List, Dict, Any def analyze_paths(paths: List[str]) -> Dict[str, Any]: # Implement your solution here pass # Example usage paths = [\\"./test.txt\\", \\"/usr/bin/python\\", \\"/home/user/docs/report.docx\\"] result = analyze_paths(paths) print(result) ``` Expected Output (Assuming the paths are valid for the current OS): ```python { \'common_prefix\': \'\', # Assuming no common prefix, example only \'common_path\': \'/\', # Assuming common root, example only \'absolute_paths\': [\'/home/user/test.txt\', \'/usr/bin/python\', \'/home/user/docs/report.docx\'], # Example only \'base_names\': [\'test.txt\', \'python\', \'report.docx\'], \'dir_names\': [\'./\', \'/usr/bin\', \'/home/user/docs\'], \'path_sizes\': {\'./test.txt\': 1024, \'/usr/bin/python\': 16384, \'/home/user/docs/report.docx\': -1}, \'exist_check\': {\'./test.txt\': True, \'/usr/bin/python\': True, \'/home/user/docs/report.docx\': False} } ``` Use the provided os.path functions to implement the solution. Be mindful of handling platform-specific path conventions if necessary.","solution":"import os from typing import List, Dict, Any def analyze_paths(paths: List[str]) -> Dict[str, Any]: result = {} # Longest common prefix result[\'common_prefix\'] = os.path.commonprefix(paths) # Longest common sub-path try: result[\'common_path\'] = os.path.commonpath(paths) except ValueError: result[\'common_path\'] = \'\' # List of absolute paths result[\'absolute_paths\'] = [os.path.abspath(path) for path in paths] # List of base names result[\'base_names\'] = [os.path.basename(path) for path in paths] # List of directory names result[\'dir_names\'] = [os.path.dirname(path) for path in paths] # Dictionary mapping each path to its size in bytes path_sizes = {} exist_check = {} for path in paths: if os.path.exists(path): exist_check[path] = True if os.path.isfile(path): path_sizes[path] = os.path.getsize(path) else: path_sizes[path] = -1 # Assuming -1 for directories as an example else: exist_check[path] = False path_sizes[path] = -1 result[\'path_sizes\'] = path_sizes result[\'exist_check\'] = exist_check return result"},{"question":"Objective: Implement a function using PyTorch\'s `vmap` transform that adheres to the principles outlined in the provided documentation. Problem Statement: Write a function `batch_matrix_vector_product` that takes a batch of matrices and a batch of vectors, and returns the batch-wise product of matrices and vectors using `vmap`. The function signature should be: ```python def batch_matrix_vector_product(matrices: torch.Tensor, vectors: torch.Tensor) -> torch.Tensor: pass ``` Input: - `matrices`: A 3D tensor of shape `(batch_size, n, m)` where `batch_size` is the number of matrices, `n` and `m` are the dimensions of each matrix. - `vectors`: A 2D tensor of shape `(batch_size, m)` where `batch_size` is the number of vectors, and `m` is the dimension of each vector. Output: - The function should return a 2D tensor of shape `(batch_size, n)` representing the result of the matrix-vector product for each pair in the batch. Constraints: - Do not use any global variables. - Ensure that the function is pure and does not involve side effects that mutate inputs or global states. - Use in-place operations where necessary but ensure they do not violate `vmap`\'s limitations. - Do not use `torch.autograd` APIs directly; rely on `torch.func` equivalents if needed. - Test the function with different configurations to ensure it handles batch operations correctly. Example: ```python import torch from torch.func import vmap def batch_matrix_vector_product(matrices: torch.Tensor, vectors: torch.Tensor) -> torch.Tensor: def mv(mat, vec): return torch.matmul(mat, vec) return vmap(mv)(matrices, vectors) # Example usage: matrices = torch.tensor([[[2.0, 3.0], [1.0, 4.0]], [[1.0, 2.0], [3.0, 4.0]]]) vectors = torch.tensor([[1.0, 2.0], [0.5, 1.5]]) output = batch_matrix_vector_product(matrices, vectors) print(output) # Expected output: tensor([[8.0, 9.0], [3.5, 7.5]]) ``` Notes: - The function `mv` inside `batch_matrix_vector_product` computes the matrix-vector product for a single matrix and vector. - `vmap` is then used to vectorize `mv` over the batch dimension for both matrices and vectors.","solution":"import torch from torch.func import vmap def batch_matrix_vector_product(matrices: torch.Tensor, vectors: torch.Tensor) -> torch.Tensor: Computes the batch-wise matrix-vector product. Args: matrices (torch.Tensor): A 3D tensor of shape (batch_size, n, m). vectors (torch.Tensor): A 2D tensor of shape (batch_size, m). Returns: torch.Tensor: A 2D tensor of shape (batch_size, n). def mv(mat, vec): return torch.matmul(mat, vec) return vmap(mv)(matrices, vectors)"},{"question":"# Task: Implement a Gaussian Mixture Model (GMM) using scikit-learn and analyze the results. Objective You are required to implement a Gaussian Mixture Model (GMM) using the scikit-learn library to cluster a given dataset. You should also assess the quality of the clustering and visualize the results. Guidelines 1. **Data Importation:** Use scikit-learn\'s `datasets` module to load the `make_blobs` function and create a dataset with 300 samples, 4 cluster centers, and a random state of 42. 2. **Model Implementation:** Implement a Gaussian Mixture Model (GMM) using `GaussianMixture` from the `mixture` module. 3. **Cluster Assignment:** Fit the model to the data and predict the cluster for each sample. 4. **Evaluation:** Calculate the silhouette score to evaluate the clustering quality. 5. **Visualization:** Plot the original data points and color them according to their cluster assignments. # Input Format - No direct input is required. You will create the dataset using `make_blobs`. # Output Format 1. The silhouette score as a float. 2. A plot showing the original data points colored by their assigned clusters. # Constraints - Use a maximum of 100 iterations for the GMM fitting. - Ensure reproducibility by setting the random state to 42 where applicable. # Performance Requirements - The implementation should efficiently handle the dataset size specified without running into performance issues. # Example ```python # Your solution should follow a similar structure: from sklearn.datasets import make_blobs from sklearn.mixture import GaussianMixture from sklearn.metrics import silhouette_score import matplotlib.pyplot as plt # Step 1: Create the dataset X, _ = make_blobs(n_samples=300, centers=4, random_state=42) # Step 2: Implement the Gaussian Mixture Model gmm = GaussianMixture(n_components=4, max_iter=100, random_state=42) gmm.fit(X) # Step 3: Predict the cluster for each sample clusters = gmm.predict(X) # Step 4: Calculate the silhouette score score = silhouette_score(X, clusters) print(\\"Silhouette Score:\\", score) # Step 5: Visualize the results plt.scatter(X[:, 0], X[:, 1], c=clusters, cmap=\'viridis\') plt.title(\\"GMM Clustering\\") plt.show() ```","solution":"from sklearn.datasets import make_blobs from sklearn.mixture import GaussianMixture from sklearn.metrics import silhouette_score import matplotlib.pyplot as plt def run_gmm(): # Step 1: Create the dataset X, _ = make_blobs(n_samples=300, centers=4, random_state=42) # Step 2: Implement the Gaussian Mixture Model gmm = GaussianMixture(n_components=4, max_iter=100, random_state=42) gmm.fit(X) # Step 3: Predict the cluster for each sample clusters = gmm.predict(X) # Step 4: Calculate the silhouette score score = silhouette_score(X, clusters) print(\\"Silhouette Score:\\", score) # Step 5: Visualize the results plt.scatter(X[:, 0], X[:, 1], c=clusters, cmap=\'viridis\') plt.title(\\"GMM Clustering\\") plt.show() return score"},{"question":"You are given a DataFrame containing information about various subjects and their corresponding scores. Some scores might be missing (represented as `pd.NA`). Your task is to implement a function `process_scores` that performs the following operations: 1. Create a new column `Passed`: - If the score is greater than or equal to 50, set `Passed` to `True`. - If the score is less than 50, set `Passed` to `False`. - If the score is missing (`pd.NA`), set `Passed` to `pd.NA`. 2. Filter out rows where the `Passed` status is `False` or `pd.NA`. 3. Fill any missing scores (`pd.NA`) in the DataFrame with the mean of the non-missing scores. 4. Calculate and return the mean score of the remaining students. # Function Signature ```python import pandas as pd def process_scores(df: pd.DataFrame) -> float: pass ``` # Input - `df`: A pandas DataFrame with a column `Score`. The `Score` column will have integer values or `pd.NA`. # Output - Return a float representing the mean score of the remaining students after the specified operations. # Constraints - NA values should be handled using the pandas `pd.NA` and operations specific to the Nullable Boolean data type. - You are expected to use pandas for all data manipulations. # Example ```python import pandas as pd # Example DataFrame data = {\'Name\': [\'Alice\', \'Bob\', \'Charlie\', \'David\'], \'Score\': [55, pd.NA, 48, 70]} df = pd.DataFrame(data) # Process the scores mean_score = process_scores(df) print(mean_score) # Output should be the mean of updated and filtered scores ``` # Notes - Make sure to use `.fillna()` appropriately for the Nullable Boolean column. - Handle the arithmetic operations correctly when dealing with columns containing `pd.NA`. # Testing You should test your function with various test cases to ensure its correctness, including edge cases with all scores missing or all scores present.","solution":"import pandas as pd def process_scores(df: pd.DataFrame) -> float: # Step 1: Create \'Passed\' column df[\'Passed\'] = df[\'Score\'].apply(lambda x: x >= 50 if x is not pd.NA else pd.NA) # Step 2: Filter rows where \'Passed\' is False or pd.NA df = df[df[\'Passed\'] == True] # Step 3: Fill missing scores with the mean of non-missing scores mean_score = df[\'Score\'].dropna().mean() df[\'Score\'] = df[\'Score\'].fillna(mean_score) # Step 4: Calculate and return the mean score of the remaining students return df[\'Score\'].mean()"},{"question":"# Coding Assessment: LDA and QDA in scikit-learn You have been provided with a dataset of handwritten digits. Your task is to use scikit-learn\'s LDA and QDA for classification and dimensionality reduction on this dataset. Input: - A dataset in the form of a CSV file, `handwritten_digits.csv`, containing 64 features (pixel intensity values) and one target column (digit label ranging from 0-9). Part 1: Classification using LDA and QDA 1. **Load the data**: Read the CSV file and split it into training and testing sets. 2. **Train LDA and QDA classifiers**: Fit both LDA and QDA models to the training data. 3. **Evaluate the models**: Predict the labels for the test data and compute the accuracy for both classifiers. Part 2: Dimensionality Reduction using LDA 4. **Dimensionality reduction**: Use LDA for dimensionality reduction on the training data by projecting it onto 2 components. 5. **Visualization**: Create a scatter plot of the projected data, coloring points by their class label. Part 3: Effect of Shrinkage on LDA 6. **Train LDA with shrinkage**: Train an LDA model with different shrinkage levels (0, 0.5, and 1) on the training data. 7. **Evaluate the effect of shrinkage**: Compute and plot the classification accuracy for each shrinkage level on the test data. # Constraints: - You must use scikit-learn\'s `LinearDiscriminantAnalysis` and `QuadraticDiscriminantAnalysis` classes. - You may use other appropriate scikit-learn modules for data loading, splitting, and metrics calculation. - Ensure that the dimensionality reduction in step 4 projects the data into exactly 2 components. You should implement the following functions: ```python import pandas as pd import numpy as np import matplotlib.pyplot as plt from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis def load_data(file_path): Load the dataset from the given CSV file. Parameters: file_path (str): Path to the CSV file. Returns: (pd.DataFrame, pd.Series): Features and target data. # Implementation here def split_data(features, target): Split the dataset into training and testing sets. Parameters: features (pd.DataFrame): The input features. target (pd.Series): The target labels. Returns: tuple: Training and testing sets for features and target. # Implementation here def train_lda(features_train, target_train): Train the Linear Discriminant Analysis (LDA) classifier. Parameters: features_train (pd.DataFrame): The training features. target_train (pd.Series): The training labels. Returns: LinearDiscriminantAnalysis: The trained LDA model. # Implementation here def train_qda(features_train, target_train): Train the Quadratic Discriminant Analysis (QDA) classifier. Parameters: features_train (pd.DataFrame): The training features. target_train (pd.Series): The training labels. Returns: QuadraticDiscriminantAnalysis: The trained QDA model. # Implementation here def evaluate_model(model, features_test, target_test): Evaluate the classifier on the test set and return the accuracy. Parameters: model: The trained classifier. features_test (pd.DataFrame): The test features. target_test (pd.Series): The test labels. Returns: float: Accuracy of the classifier on the test set. # Implementation here def lda_dimensionality_reduction(features_train, target_train, n_components=2): Perform dimensionality reduction using LDA and return the transformed data. Parameters: features_train (pd.DataFrame): The training features. target_train (pd.Series): The training labels. n_components (int): Number of components for dimensionality reduction. Returns: np.ndarray: Transformed training features. # Implementation here def plot_lda_projection(transformed_features, target_train): Plot the LDA projected data with the class labels. Parameters: transformed_features (np.ndarray): Projected training features. target_train (pd.Series): The training labels. # Implementation here def train_lda_with_shrinkage(features_train, target_train, shrinkage): Train an LDA classifier with a specified shrinkage level. Parameters: features_train (pd.DataFrame): The training features. target_train (pd.Series): The training labels. shrinkage (float): The shrinkage parameter for LDA. Returns: LinearDiscriminantAnalysis: The trained LDA model with shrinkage. # Implementation here def plot_shrinkage_effects(shrinkage_levels, accuracies): Plot the effect of different shrinkage levels on classification accuracy. Parameters: shrinkage_levels (list): List of shrinkage levels. accuracies (list): List of classification accuracies for each shrinkage level. # Implementation here ``` Please make sure to adhere closely to the provided function signatures and requirements. **Performance Requirement**: - Ensure the implemented solution runs efficiently for the provided dataset and completes within a reasonable timeframe. **Submission**: Upload your solution as a single Python file.","solution":"import pandas as pd from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis import matplotlib.pyplot as plt import numpy as np def load_data(file_path): Load the dataset from the given CSV file. Parameters: file_path (str): Path to the CSV file. Returns: (pd.DataFrame, pd.Series): Features and target data. data = pd.read_csv(file_path) features = data.loc[:, data.columns != \'target\'] target = data[\'target\'] return features, target def split_data(features, target): Split the dataset into training and testing sets. Parameters: features (pd.DataFrame): The input features. target (pd.Series): The target labels. Returns: tuple: Training and testing sets for features and target. X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.3, random_state=42) return X_train, X_test, y_train, y_test def train_lda(features_train, target_train): Train the Linear Discriminant Analysis (LDA) classifier. Parameters: features_train (pd.DataFrame): The training features. target_train (pd.Series): The training labels. Returns: LinearDiscriminantAnalysis: The trained LDA model. lda = LinearDiscriminantAnalysis() lda.fit(features_train, target_train) return lda def train_qda(features_train, target_train): Train the Quadratic Discriminant Analysis (QDA) classifier. Parameters: features_train (pd.DataFrame): The training features. target_train (pd.Series): The training labels. Returns: QuadraticDiscriminantAnalysis: The trained QDA model. qda = QuadraticDiscriminantAnalysis() qda.fit(features_train, target_train) return qda def evaluate_model(model, features_test, target_test): Evaluate the classifier on the test set and return the accuracy. Parameters: model: The trained classifier. features_test (pd.DataFrame): The test features. target_test (pd.Series): The test labels. Returns: float: Accuracy of the classifier on the test set. predictions = model.predict(features_test) accuracy = accuracy_score(target_test, predictions) return accuracy def lda_dimensionality_reduction(features_train, target_train, n_components=2): Perform dimensionality reduction using LDA and return the transformed data. Parameters: features_train (pd.DataFrame): The training features. target_train (pd.Series): The training labels. n_components (int): Number of components for dimensionality reduction. Returns: np.ndarray: Transformed training features. lda = LinearDiscriminantAnalysis(n_components=n_components) transformed_features = lda.fit_transform(features_train, target_train) return transformed_features def plot_lda_projection(transformed_features, target_train): Plot the LDA projected data with the class labels. Parameters: transformed_features (np.ndarray): Projected training features. target_train (pd.Series): The training labels. plt.figure(figsize=(8,6)) for label in np.unique(target_train): plt.scatter(transformed_features[target_train==label, 0], transformed_features[target_train==label, 1], label=label) plt.xlabel(\'Component 1\') plt.ylabel(\'Component 2\') plt.legend() plt.title(\'LDA Dimensionality Reduction\') plt.show() def train_lda_with_shrinkage(features_train, target_train, shrinkage): Train an LDA classifier with a specified shrinkage level. Parameters: features_train (pd.DataFrame): The training features. target_train (pd.Series): The training labels. shrinkage (float): The shrinkage parameter for LDA. Returns: LinearDiscriminantAnalysis: The trained LDA model with shrinkage. lda = LinearDiscriminantAnalysis(solver=\'lsqr\', shrinkage=shrinkage) lda.fit(features_train, target_train) return lda def plot_shrinkage_effects(shrinkage_levels, accuracies): Plot the effect of different shrinkage levels on classification accuracy. Parameters: shrinkage_levels (list): List of shrinkage levels. accuracies (list): List of classification accuracies for each shrinkage level. plt.figure(figsize=(8,6)) plt.plot(shrinkage_levels, accuracies, marker=\'o\') plt.xlabel(\'Shrinkage Level\') plt.ylabel(\'Accuracy\') plt.title(\'Effect of Shrinkage on LDA Performance\') plt.show()"},{"question":"Objective: Create a Python script that simulates a mini command-line utility to manage files and directories. The script must demonstrate the use of multiple functionalities from the `os` module. Description: You are to implement a class named `FileManager` with the following methods: 1. **`change_directory(path: str) -> str`**: Changes the current working directory to the given path and returns the new current directory. 2. **`list_directory(path: str = \'.\') -> list`**: Lists all files and directories in the given path. Defaults to the current directory. 3. **`make_directory(path: str) -> str`**: Creates a directory at the given path and returns the created directory name. 4. **`delete_file(path: str) -> str`**: Deletes the file at the given path and returns the name of the deleted file. 5. **`rename_item(src: str, dst: str) -> str`**: Renames the file or directory from `src` to `dst` and returns the new name. 6. **`execute_command(command: str) -> str`**: Executes a given command in a subshell and returns the output of the command. **Constraints**: - Assume the inputs are always valid paths or commands. - Handle any exceptions that might occur due to file operations, ensuring the program remains robust. - Methods should return meaningful responses and not print anything directly. **Example Usage**: ```python fm = FileManager() print(fm.change_directory(\'/path/to/dir\')) print(fm.list_directory(\'/path/to/dir\')) print(fm.make_directory(\'/path/to/newdir\')) print(fm.delete_file(\'/path/to/file.txt\')) print(fm.rename_item(\'/path/to/oldname\', \'/path/to/newname\')) print(fm.execute_command(\'ls -la /path/to/dir\')) ``` **Notes**: - Use appropriate methods from the `os` module to achieve the required functionality. - Ensure that the `execute_command` method can handle typical shell commands like `ls`, `pwd`, `echo`, etc. Assessment Criteria: 1. **Correctness**: The script accurately implements the required methods. 2. **Robustness**: The script handles exceptions and edge cases gracefully. 3. **Code Quality**: The script follows good programming practices, including proper use of functions, clear variable naming, and adherence to Python\'s coding standards. Submission: Submit the Python script named `file_manager.py` containing the `FileManager` class and its methods.","solution":"import os import subprocess class FileManager: def change_directory(self, path: str) -> str: try: os.chdir(path) return os.getcwd() except Exception as e: return str(e) def list_directory(self, path: str = \'.\') -> list: try: return os.listdir(path) except Exception as e: return [str(e)] def make_directory(self, path: str) -> str: try: os.makedirs(path, exist_ok=True) return path except Exception as e: return str(e) def delete_file(self, path: str) -> str: try: os.remove(path) return path except Exception as e: return str(e) def rename_item(self, src: str, dst: str) -> str: try: os.rename(src, dst) return dst except Exception as e: return str(e) def execute_command(self, command: str) -> str: try: result = subprocess.run(command, capture_output=True, text=True, shell=True) return result.stdout or result.stderr except Exception as e: return str(e)"},{"question":"**Objective:** Implement a multi-threaded TCP server using the `socketserver` module that can handle multiple client requests simultaneously. Each client will send a message to the server, and the server will respond with the reversed message. **Details:** 1. Subclass the `socketserver.ThreadingMixIn` and `socketserver.TCPServer` to create a `ThreadedTCPServer`. 2. Subclass the `socketserver.BaseRequestHandler` to create a `ThreadedTCPRequestHandler`. 3. Override the `handle()` method in `ThreadedTCPRequestHandler` to: - Receive data from the client. - Reverse the received data. - Send the reversed data back to the client. 4. In the `main` block: - Create an instance of `ThreadedTCPServer` using `localhost` and port `9999`. - Use a context manager (`with` statement) to manage the server lifecycle. - Start the server to handle requests indefinitely using `serve_forever()`. 5. Ensure the server can handle simultaneous connections from multiple clients. You can test this by writing a simple client script that sends a message to the server and prints the response. **Constraints:** - Do not use external libraries beyond the standard library. - Ensure the server handles at least 5 simultaneous client connections without issues. **Performance Requirements:** - The server should handle each client request in a separate thread to ensure responsiveness. **Expected Input:** - Each client connection sends a string message to the server. **Expected Output:** - The server responds to each client with the reversed string message. **Example:** Client sends: `hello` Server responds: `olleh` Client sends: `world` Server responds: `dlrow` Here is a possible structure for the code: ```python import socketserver import threading class ThreadedTCPRequestHandler(socketserver.BaseRequestHandler): def handle(self): data = self.request.recv(1024).strip() reversed_data = data[::-1] self.request.sendall(reversed_data) class ThreadedTCPServer(socketserver.ThreadingMixIn, socketserver.TCPServer): pass if __name__ == \\"__main__\\": HOST, PORT = \\"localhost\\", 9999 with ThreadedTCPServer((HOST, PORT), ThreadedTCPRequestHandler) as server: server.serve_forever() ``` Client script example: ```python import socket def client(message): HOST, PORT = \\"localhost\\", 9999 with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock: sock.connect((HOST, PORT)) sock.sendall(bytes(message, \\"utf-8\\")) received = sock.recv(1024).decode(\\"utf-8\\") print(f\'Sent: {message}, Received: {received}\') if __name__ == \\"__main__\\": client(\\"hello\\") client(\\"world\\") client(\\"socketserver\\") client(\\"python\\") client(\\"multithreading\\") ``` **Submission:** Submit your solution as a Python script named `multi_threaded_server.py` containing the server implementation and any supporting files or documentation.","solution":"import socketserver class ThreadedTCPRequestHandler(socketserver.BaseRequestHandler): def handle(self): # Receive data from the client data = self.request.recv(1024).strip() # Reverse the received data reversed_data = data[::-1] # Send the reversed data back to the client self.request.sendall(reversed_data) class ThreadedTCPServer(socketserver.ThreadingMixIn, socketserver.TCPServer): pass if __name__ == \\"__main__\\": HOST, PORT = \\"localhost\\", 9999 with ThreadedTCPServer((HOST, PORT), ThreadedTCPRequestHandler) as server: server.serve_forever()"},{"question":"Develop a function in PyTorch that performs matrix multiplication using the MPS backend, and then validate the results by comparing the computation performed on the CPU. The function should: 1. Check if the MPS backend is available. 2. Create two random matrices on the MPS device. 3. Perform matrix multiplication on the MPS device. 4. Move the result matrix back to the CPU. 5. Perform the same matrix multiplication on the CPU. 6. Compare the results from both devices to ensure they are identical. # Function Signature ```python def mps_matrix_multiplication(matrix_size: int) -> bool: Perform matrix multiplication using MPS backend and validate results with CPU computation. :param matrix_size: Size of the square matrices to be multiplied. :return: True if the results from MPS and CPU are identical within a tolerance, False otherwise. ``` # Expected Input and Output - Input: An integer representing the size of the square matrices to be multiplied. - Output: A boolean value indicating whether the results from MPS and CPU computations are identical within a tolerance level. # Constraints - The function should validate the availability of MPS backend before proceeding with the matrix operations. - Use a tolerance level of `1e-5` when comparing the results from MPS and CPU computations to account for floating-point precision differences. # Example ```python result = mps_matrix_multiplication(100) print(result) # Expected output: True ``` # Notes - Proper error handling should be implemented to manage scenarios where the MPS backend is not available. - Consider using PyTorch\'s `torch.allclose` function to compare the results of the matrix multiplication from the two devices.","solution":"import torch def mps_matrix_multiplication(matrix_size: int) -> bool: Perform matrix multiplication using MPS backend and validate results with CPU computation. :param matrix_size: Size of the square matrices to be multiplied. :return: True if the results from MPS and CPU are identical within a tolerance, False otherwise. # Check if MPS backend is available if not torch.backends.mps.is_available(): raise RuntimeError(\\"MPS backend is not available\\") # Create random matrices on MPS device device = torch.device(\\"mps\\") matrix_a = torch.randn(matrix_size, matrix_size, device=device) matrix_b = torch.randn(matrix_size, matrix_size, device=device) # Perform matrix multiplication on the MPS device result_mps = torch.matmul(matrix_a, matrix_b) # Move the result back to CPU result_mps_cpu = result_mps.to(\\"cpu\\") # Perform the same matrix multiplication on the CPU matrix_a_cpu = matrix_a.to(\\"cpu\\") matrix_b_cpu = matrix_b.to(\\"cpu\\") result_cpu = torch.matmul(matrix_a_cpu, matrix_b_cpu) # Compare the results from both devices (within a tolerance) return torch.allclose(result_mps_cpu, result_cpu, atol=1e-5)"},{"question":"Coding Assessment Question **Problem Description:** You are required to implement a function that mimics some of the key functionalities of the `tabnanny` module for educational purposes. Your function will: 1. Recursively scan a given directory for all `.py` files. 2. Analyze each Python file to detect whether there are any lines with ambiguous indentation (inconsistent use of spaces and tabs). 3. Report the names of files containing ambiguous indentation. Write a Python function `find_ambiguous_indentation(directory: str) -> List[str]` that takes a directory path as input and returns a list of filenames containing lines with ambiguous indentation. # Function Signature ```python def find_ambiguous_indentation(directory: str) -> List[str]: ``` # Input - `directory`: A string representing the path to the directory that needs to be checked. # Output - A list of strings, where each string is the path of a file containing ambiguous indentation. If no files have ambiguous indentation, return an empty list. # Constraints and Assumptions 1. A line is considered to have ambiguous indentation if it contains both spaces and tabs. 2. You are NOT allowed to use the `tabnanny` module directly, but you can use other standard Python modules (e.g., `os`, `os.path`, and `tokenize`). 3. The function should be efficient enough to handle directories with a large number of files. # Example ```python # Assuming the directory structure is as follows: # test_dir/ # ├── good_file.py (with consistent indentation) # ├── bad_file.py (with ambiguous indentation) # └── subdir/ # └── confusing_file.py (with ambiguous indentation) # # The file content for bad_file.py could be like: # def example_function(): # tif condition: # print(\\"Hello\\") # Indentation contains both tab (t) and spaces # # The content for confusing_file.py could be like: # def another_example(): # tif another_condition: # print(\\"World\\") # Indentation contains both tab (t) and spaces print(find_ambiguous_indentation(\'test_dir\')) ``` Expected Output: ```python [\'test_dir/bad_file.py\', \'test_dir/subdir/confusing_file.py\'] ``` # Notes - Handle cases of nested directories. - Ensure your code is readable and well-documented. Good luck!","solution":"import os from typing import List def find_ambiguous_indentation(directory: str) -> List[str]: def is_ambiguous(line: str) -> bool: return \'t\' in line and \' \' in line def check_file(file_path: str) -> bool: with open(file_path, \'r\') as file: for line in file: if is_ambiguous(line): return True return False ambiguous_files = [] for root, _, files in os.walk(directory): for file in files: if file.endswith(\'.py\'): full_path = os.path.join(root, file) if check_file(full_path): ambiguous_files.append(full_path) return ambiguous_files"},{"question":"Coding Assessment Question # Objective Implement a custom neural network layer using multiple PyTorch functional API elements, particularly focusing on convolution operations, activation functions, and pooling. The custom layer should be built as a single function that can be integrated into any PyTorch model. # Problem Statement Write a Python function `custom_conv_layer` in PyTorch that performs the following operations: 1. **1D Convolution**: Takes as inputs a batch of sequences and applies a 1D convolution with a specified number of output channels, kernel size, stride, and padding. 2. **ReLU Activation**: Applies a ReLU activation function to the result of the convolution. 3. **Batch Normalization**: Applies batch normalization. 4. **Max Pooling**: Applies a max pooling operation with a given kernel size and stride. # Function Signature ```python def custom_conv_layer(x: torch.Tensor, out_channels: int, kernel_size: int, stride: int, padding: int, pool_kernel_size: int, pool_stride: int) -> torch.Tensor: Args: x: A 3D tensor of shape (batch_size, in_channels, sequence_length) out_channels: The number of output channels for the convolution operation. kernel_size: The size of the convolutional kernel. stride: The stride of the convolution. padding: The amount of padding to apply to the input sequence. pool_kernel_size: The size of the max pooling kernel. pool_stride: The stride of the max pooling operation. Returns: A tensor of the shape (batch_size, out_channels, new_sequence_length) after applying all the operations listed. pass ``` # Constraints 1. Do not use any classes or pre-defined layers from `torch.nn`, only use functional API from `torch.nn.functional`. 2. The function should handle variable batch sizes, input channels, and sequence lengths. # Example ```python import torch # Sample Input x = torch.randn(32, 3, 100) # batch_size=32, in_channels=3, sequence_length=100 # Applying custom convolution layer output = custom_conv_layer(x, out_channels=16, kernel_size=5, stride=1, padding=2, pool_kernel_size=2, pool_stride=2) print(output.shape) # Expected shape: (32, 16, 50) ``` # Explanation - The `custom_conv_layer` function should apply a 1D convolution to the input tensor `x` with specified parameters `out_channels`, `kernel_size`, `stride`, and `padding`. - It should then apply a ReLU activation function to the convolved output. - The result should then pass through a batch normalization layer. - Finally, the output should be passed through a max pooling layer with parameters `pool_kernel_size` and `pool_stride`. # Performance Requirements - The solution should be optimized for both speed and memory usage. - Avoid unnecessary operations or data duplications to ensure efficient execution. # Evaluation Criteria - Correctness: The function should produce correct and expected outputs. - Efficiency: The implementation should be optimized for performance. - Code quality: The code should be clean, well-documented and follow best practices in PyTorch.","solution":"import torch import torch.nn.functional as F def custom_conv_layer(x: torch.Tensor, out_channels: int, kernel_size: int, stride: int, padding: int, pool_kernel_size: int, pool_stride: int) -> torch.Tensor: Applies a 1D convolution, ReLU activation, batch normalization, and max pooling to the input tensor. Args: x: A 3D tensor of shape (batch_size, in_channels, sequence_length) out_channels: The number of output channels for the convolution operation. kernel_size: The size of the convolutional kernel. stride: The stride of the convolution. padding: The amount of padding to apply to the input sequence. pool_kernel_size: The size of the max pooling kernel. pool_stride: The stride of the max pooling operation. Returns: A tensor of the shape (batch_size, out_channels, new_sequence_length) after applying all the operations listed. # First, 1D convolutional layer conv = F.conv1d(x, torch.randn(out_channels, x.size(1), kernel_size), stride=stride, padding=padding) # Applying ReLU activation relu = F.relu(conv) # Applying batch normalization bn = F.batch_norm(relu, torch.zeros(out_channels), torch.ones(out_channels)) # Applying max pooling pooled = F.max_pool1d(bn, kernel_size=pool_kernel_size, stride=pool_stride) return pooled"},{"question":"Given the information provided above on the `PyEval` and `PyFrame` family of functions from the Python C API, design a Python C extension function that creates a detailed trace of function calls. This trace should include the function name, its description, and the line number currently being executed for each frame in the call stack. Objective: Write a C function, `trace_function_calls`, that: 1. Uses the provided APIs to traverse the call stack. 2. Collects and returns detailed information about each frame, including: - Function name - Function description - Line number currently being executed Implementation: 1. Your C function should start at the current frame obtained using `PyEval_GetFrame()`. 2. Traverse back through each frame using `PyFrame_GetBack()`. 3. For each frame, obtain the function name using `PyEval_GetFuncName` and function description using `PyEval_GetFuncDesc`. 4. Obtain the line number using `PyFrame_GetLineNumber`. 5. Collect this information into a Python list of dictionaries and return it. # Example of Expected Output: ```python [ { \\"function_name\\": \\"example_func\\", \\"function_description\\": \\"()\\", \\"line_number\\": 42 }, { \\"function_name\\": \\"main\\", \\"function_description\\": \\"()\\", \\"line_number\\": 10 } ] ``` # Constraints: - You should ensure that your code correctly handles cases where certain frames might be `NULL`. - The solution should handle any Python function calls and provide accurate tracing even in the presence of recursive calls. # Performance: - The function should operate efficiently and avoid unnecessary computations while traversing frames. # Notes: - Provide the necessary setup instructions and code required to build and test the C extension module. - Include Python code demonstrating how to call `trace_function_calls` and handle its return value. Grading Criteria: - Accurate and correct usage of the provided APIs. - Correct traversal of the Python call stack. - Proper collection and formation of the result to match the expected output. - Efficiency and handling of edge cases.","solution":"from ctypes import pythonapi, py_object, POINTER, c_void_p, c_int, c_char_p def trace_function_calls(): Returns a detailed trace of function calls including function name, description, and current line number for each frame in the call stack. # Retrieve the current frame PyEval_GetFrame = pythonapi.PyEval_GetFrame PyEval_GetFrame.restype = POINTER(c_void_p) # Retrieve the function name and description PyEval_GetFuncName = pythonapi.PyEval_GetFuncName PyEval_GetFuncName.restype = c_char_p PyEval_GetFuncName.argtypes = [POINTER(c_void_p)] PyEval_GetFuncDesc = pythonapi.PyEval_GetFuncDesc PyEval_GetFuncDesc.restype = c_char_p PyEval_GetFuncDesc.argtypes = [POINTER(c_void_p)] # Retrieve the line number of the frame PyFrame_GetLineNumber = pythonapi.PyFrame_GetLineNumber PyFrame_GetLineNumber.restype = c_int PyFrame_GetLineNumber.argtypes = [POINTER(c_void_p)] # Get the previous frame PyFrame_GetBack = pythonapi.PyFrame_GetBack PyFrame_GetBack.restype = POINTER(c_void_p) PyFrame_GetBack.argtypes = [POINTER(c_void_p)] def get_frame_info(frame): Helper function to extract information from a single frame. if not frame: return None func_name = PyEval_GetFuncName(frame) if func_name: func_name = func_name.decode(\'utf-8\') else: func_name = \'unknown\' func_desc = PyEval_GetFuncDesc(frame) if func_desc: func_desc = func_desc.decode(\'utf-8\') else: func_desc = \'unknown\' line_number = PyFrame_GetLineNumber(frame) return { \'function_name\': func_name, \'function_description\': func_desc, \'line_number\': line_number } trace = [] current_frame = PyEval_GetFrame() while current_frame: frame_info = get_frame_info(current_frame) if frame_info: trace.append(frame_info) current_frame = PyFrame_GetBack(current_frame) return trace"},{"question":"Mastering Seaborn Plot Customization **Objective:** Assess the ability to work with advanced `seaborn` functionalities for data visualization and transformation. --- Problem Statement You are tasked with analyzing and visualizing health expenditure data. Using the `seaborn.objects` module, you need to load the dataset, generate specific plots, and customize them as per given instructions. **Requirements:** 1. Load the `healthexp` dataset using `seaborn` and generate the following plots: - A line plot showing the normalized health expenditure relative to the maximum expenditure for each country. - A line plot showing the percentage change in spending from the year 1970 baseline for each country. You must implement the following function: ```python import seaborn.objects as so from seaborn import load_dataset def plot_health_expenditure(): This function loads the `healthexp` dataset, generates and customizes two specific line plots. - First plot: Normalized health expenditure relative to the maximum expenditure for each country. - Second plot: Percentage change in spending from the year 1970 baseline for each country. # Load the dataset healthexp = load_dataset(\\"healthexp\\") # Generate the first plot first_plot = ( so.Plot(healthexp, x=\\"Year\\", y=\\"Spending_USD\\", color=\\"Country\\") .add(so.Lines(), so.Norm()) .label(y=\\"Spending relative to maximum amount\\") ) # Display the first plot first_plot.show() # Generate the second plot second_plot = ( so.Plot(healthexp, x=\\"Year\\", y=\\"Spending_USD\\", color=\\"Country\\") .add(so.Lines(), so.Norm(where=\\"x == x.min()\\", percent=True)) .label(y=\\"Percent change in spending from 1970 baseline\\") ) # Display the second plot second_plot.show() ``` Here, `first_plot` and `second_plot` must adhere to the following customization: - The y-axis of the first plot should be labeled \\"Spending relative to maximum amount\\". - The y-axis of the second plot should be labeled \\"Percent change in spending from 1970 baseline\\". **Constraints:** - Ensure that the dataset is loaded properly. - The function must create and display both plots correctly within the `plot_health_expenditure` function. - Make sure you understand how to use `so.Plot`, `so.Lines`, `so.Norm`, `where`, and `percent` parameters from seaborn\'s object-oriented interface. --- Input: - No direct input is required from the user. The function should load the dataset internally. Output: - The function should create and display two properly labeled line plots as described. **Evaluation Criteria:** - Correct implementation of plots and required customizations. - Correct loading and handling of the dataset. - Clear and proper use of seaborn’s advanced methods. **Example Usage:** ```python plot_health_expenditure() ``` Upon execution, two plots should be displayed: 1. A line plot with normalized spending relative to the maximum amount. 2. A line plot showing the percent change in spending from the year 1970 baseline. Ensure the plots are customized with the specified y-axis labels.","solution":"import seaborn.objects as so from seaborn import load_dataset def plot_health_expenditure(): This function loads the `healthexp` dataset, generates, and customizes two specific line plots. - First plot: Normalized health expenditure relative to the maximum expenditure for each country. - Second plot: Percentage change in spending from the year 1970 baseline for each country. # Load the dataset healthexp = load_dataset(\\"healthexp\\") # Generate the first plot first_plot = ( so.Plot(healthexp, x=\\"Year\\", y=\\"Spending_USD\\", color=\\"Country\\") .add(so.Lines(), so.Norm()) .label(y=\\"Spending Relative to Maximum Amount\\") ) # Display the first plot first_plot.show() # Generate the second plot second_plot = ( so.Plot(healthexp, x=\\"Year\\", y=\\"Spending_USD\\", color=\\"Country\\") .add(so.Lines(), so.Norm(where=\\"x == x.min()\\", percent=True)) .label(y=\\"Percent Change in Spending from 1970 Baseline\\") ) # Display the second plot second_plot.show()"},{"question":"**Objective**: Demonstrate understanding of pandas options and settings by configuring and retrieving global behavior. **Task**: Using the pandas library, perform the following tasks: 1. **Describe the Precision Option**: Retrieve and display the description of the option that controls the number of decimal places displayed for floating-point numbers. 2. **Configure and Verify Options**: - Set the option to display a maximum of 3 columns when printing DataFrames. - Set the option to display floats with 2 decimal places. - Verify these settings by retrieving and printing the current values of these options. 3. **Temporary Settings Using Context Manager**: - Within a context management block, set the option to display floats in engineering notation with 5 significant digits. Print the current settings within this block. - Outside the context management block, verify that the floating point display options revert to the previous settings. 4. **Reset Options to Default**: - Reset the floating point display option and the column display option to their default values. - Verify by retrieving and printing the values of these options. **Requirements**: - You are only allowed to use the following pandas functions: `describe_option`, `get_option`, `set_option`, `option_context`, `reset_option`. - Your solution must handle the specification, modification, verification, and resetting of pandas options correctly. - Properly comment your code to explain each step. **Input and Output Format**: - This task does not require any specific input from users. - All relevant outputs (descriptions, option values) should be printed to the console. # Example Result ```python # Description of precision option # Printing option description for precision print(pandas.describe_option(\'display.precision\')) # Setting options pandas.set_option(\'display.max_columns\', 3) pandas.set_option(\'display.precision\', 2) # Verifying options print(pandas.get_option(\'display.max_columns\')) # Expected output: 3 print(pandas.get_option(\'display.precision\')) # Expected output: 2 # Using context manager with pandas.option_context(\'display.float_format\', \'{:.5e}\'.format): print(pandas.get_option(\'display.float_format\')) # Verifying reversion of context manager change print(pandas.get_option(\'display.float_format\')) # Resetting options to default pandas.reset_option(\'display.max_columns\') pandas.reset_option(\'display.float_format\') # Verify reset print(pandas.get_option(\'display.max_columns\')) # Expected default output print(pandas.get_option(\'display.float_format\')) # Expected default output ``` Note: The actual test code should replace placeholders with real function implementations and proper option attributes.","solution":"import pandas as pd def configure_pandas_options(): # Display the description for the precision option precision_description = pd.describe_option(\'display.precision\') print(f\\"Precision Description:n{precision_description}\\") # Set the maximum number of columns displayed to 3 pd.set_option(\'display.max_columns\', 3) # Set the precision for displaying floats to 2 decimal places pd.set_option(\'display.precision\', 2) # Retrieve and display the current settings for these options max_columns = pd.get_option(\'display.max_columns\') precision = pd.get_option(\'display.precision\') print(f\\"Display.max_columns: {max_columns}\\") print(f\\"Display.precision: {precision}\\") # Use a context manager to temporarily change the float_format option with pd.option_context(\'display.float_format\', \'{:.5e}\'.format): float_format = pd.get_option(\'display.float_format\') print(f\\"Temporary Display.float_format (inside context): {float_format}\\") # Verify that the float_format option has reverted back after the context block float_format_after = pd.get_option(\'display.float_format\') print(f\\"Display.float_format (after context): {float_format_after}\\") # Reset the options to their default values pd.reset_option(\'display.max_columns\') pd.reset_option(\'display.precision\') # Verify and display that the options have been reset max_columns_reset = pd.get_option(\'display.max_columns\') precision_reset = pd.get_option(\'display.precision\') print(f\\"Display.max_columns (reset): {max_columns_reset}\\") print(f\\"Display.precision (reset): {precision_reset}\\") if __name__ == \\"__main__\\": configure_pandas_options()"},{"question":"Objective Demonstrate your proficiency in using the `plistlib` module to read, write, and manipulate Apple property list (plist) files. Problem Statement You are required to write a Python function `merge_plists(plist1: bytes, plist2: bytes, output_format: str) -> bytes` that takes two plist files in bytes format and merges them into a single plist file. The resulting plist file should be in the format specified by `output_format`, which can be either `\\"FMT_XML\\"` or `\\"FMT_BINARY\\"`. If there are duplicate keys between the two plists, the values from `plist2` should overwrite those from `plist1`. Function Signature ```python def merge_plists(plist1: bytes, plist2: bytes, output_format: str) -> bytes: ``` Input - `plist1 (bytes)`: A bytes object representing the first plist file. - `plist2 (bytes)`: A bytes object representing the second plist file. - `output_format (str)`: A string that must be either `\\"FMT_XML\\"` or `\\"FMT_BINARY\\"` specifying the format of the output plist file. Output - `bytes`: A bytes object representing the merged plist file in the specified format. Constraints - You can assume that both `plist1` and `plist2` are valid plist files. - Both `plist1` and `plist2` are dictionaries at the top level. - The `output_format` will be one of the specified values (`\\"FMT_XML\\"` or `\\"FMT_BINARY\\"`). - You should handle any exceptions that occur during parsing or writing plist files and raise a `ValueError` with an appropriate error message. Example Usage ```python import plistlib plist1 = plistlib.dumps({\\"key1\\": \\"value1\\", \\"key2\\": \\"value2\\"}) plist2 = plistlib.dumps({\\"key2\\": \\"new_value2\\", \\"key3\\": \\"value3\\"}) output_format = \\"FMT_XML\\" merged_plist = merge_plists(plist1, plist2, output_format) print(merged_plist) ``` Notes - Use the `plistlib` module functions and constants as necessary. - Ensure that the merged dictionary writes out in the requested format.","solution":"import plistlib def merge_plists(plist1: bytes, plist2: bytes, output_format: str) -> bytes: Merges two plist files and returns the result in the specified format. Parameters: plist1 (bytes): The first plist file in bytes format. plist2 (bytes): The second plist file in bytes format. output_format (str): The format of the output plist file (\\"FMT_XML\\" or \\"FMT_BINARY\\"). Returns: bytes: The merged plist file in the specified format. Raises: ValueError: If the plist cannot be parsed or if the format is invalid. try: dict1 = plistlib.loads(plist1) dict2 = plistlib.loads(plist2) if not isinstance(dict1, dict) or not isinstance(dict2, dict): raise ValueError(\\"Both plist files must contain dictionaries at the top level.\\") # Merge the dictionaries, with dict2 values overwriting dict1 values for duplicate keys merged_dict = {**dict1, **dict2} if output_format == \\"FMT_XML\\": return plistlib.dumps(merged_dict, fmt=plistlib.FMT_XML) elif output_format == \\"FMT_BINARY\\": return plistlib.dumps(merged_dict, fmt=plistlib.FMT_BINARY) else: raise ValueError(\\"Invalid output_format. Must be \'FMT_XML\' or \'FMT_BINARY\'.\\") except Exception as e: raise ValueError(f\\"An error occurred while processing the plists: {e}\\")"},{"question":"# Python OOP and Function Implementation Assessment Objective: Design and implement a Python class that simulates a basic library system with functionalities to add books, loan books to users, and check the loan status of books. The purpose of this question is to assess your understanding of Python OOP (Object-Oriented Programming) and your ability to integrate various Python programming constructs. Problem Statement: You need to implement a class `Library` with the following methods: 1. `add_book(book_id: int, title: str) -> None`: Adds a book to the library with a unique `book_id` and `title`. If a book with the same `book_id` already exists, raise a `ValueError` with an appropriate message. 2. `loan_book(book_id: int, user: str) -> None`: Loans a book to a user. If the book is not found, raise a `LookupError` with an appropriate message. If the book is already loaned out, raise a `ValueError` with an appropriate message. 3. `check_loan_status(book_id: int) -> str`: Returns the name of the user who has loaned the book. If the book is not loaned out, return \\"Available\\". If the book is not found, raise a `LookupError` with an appropriate message. 4. `return_book(book_id: int) -> None`: Marks a book as returned. If the book is not found, raise a `LookupError` with an appropriate message. If the book is not loaned out, raise a `ValueError` with an appropriate message. Constraints: - Book IDs are positive integers. - Titles are non-empty strings. - Usernames are non-empty strings. Expected Functionality: - Proper use of class and instance variables. - Proper use of exception handling. - Ensuring no side effects like data corruption. - Efficient handling of data for potentially large libraries. Example: ```python library = Library() # Adding books library.add_book(1, \\"The Great Gatsby\\") library.add_book(2, \\"1984\\") library.add_book(3, \\"To Kill a Mockingbird\\") # Loaning books library.loan_book(1, \\"userA\\") library.loan_book(2, \\"userB\\") # Checking loan status print(library.check_loan_status(1)) # Output: \\"userA\\" print(library.check_loan_status(3)) # Output: \\"Available\\" # Returning books library.return_book(1) print(library.check_loan_status(1)) # Output: \\"Available\\" ``` Note: Include exception handling scenarios to demonstrate proper functionality. Assessment Criteria: - Correct implementation of class and methods. - Handling of edge cases (non-existent books, duplicate book ids, loaning a book already loaned out, etc.). - Proper and concise use of Python syntax. - Efficient management of the library data.","solution":"class Library: def __init__(self): self.books = {} # maps book_id to title self.loaned_books = {} # maps book_id to user def add_book(self, book_id: int, title: str) -> None: if book_id in self.books: raise ValueError(f\\"Book with id {book_id} already exists.\\") self.books[book_id] = title def loan_book(self, book_id: int, user: str) -> None: if book_id not in self.books: raise LookupError(f\\"Book with id {book_id} not found.\\") if book_id in self.loaned_books: raise ValueError(f\\"Book with id {book_id} is already loaned out.\\") self.loaned_books[book_id] = user def check_loan_status(self, book_id: int) -> str: if book_id not in self.books: raise LookupError(f\\"Book with id {book_id} not found.\\") return self.loaned_books.get(book_id, \\"Available\\") def return_book(self, book_id: int) -> None: if book_id not in self.books: raise LookupError(f\\"Book with id {book_id} not found.\\") if book_id not in self.loaned_books: raise ValueError(f\\"Book with id {book_id} is not loaned out.\\") del self.loaned_books[book_id]"},{"question":"**Context:** Decision trees (DTs) are a popular machine learning method used for both classification and regression tasks. They are non-parametric models that predict the value of a target variable by learning simple decision rules inferred from the data\'s features. **Objective:** Implement a `Decision Tree Classifier` that can handle missing values, perform minimal cost-complexity pruning, and evaluate the model\'s performance on a given dataset. **Problem Statement:** Create a Python function `train_decision_tree_classifier` that: 1. Trains a decision tree classifier on a given dataset. 2. Handles missing values appropriately during training and prediction. 3. Performs minimal cost-complexity pruning. 4. Evaluates the model\'s performance using accuracy score. **Function Signature:** ```python def train_decision_tree_classifier(X_train, y_train, X_test, y_test, ccp_alpha=0.01): Trains a Decision Tree Classifier on a given dataset with handling of missing values and minimal cost-complexity pruning. Parameters: - X_train: numpy.ndarray, shape (num_samples, num_features) Training feature matrix - y_train: numpy.ndarray, shape (num_samples,) Training labels - X_test: numpy.ndarray, shape (num_samples, num_features) Test feature matrix - y_test: numpy.ndarray, shape (num_samples,) Test labels - ccp_alpha: float, default=0.01 Complexity parameter used for Minimal Cost-Complexity Pruning. Returns: - accuracy: float Accuracy score of the pruned tree on the test set. pass ``` **Instructions:** 1. **Load the Dataset**: Assume data is already preprocessed and split into training and test sets (X_train, X_test, y_train, y_test). 2. **Train the Model**: - Use `DecisionTreeClassifier` to fit the model on `X_train` and `y_train`. - Ensure the model can handle missing values using appropriate strategies mentioned in the documentation. 3. **Prune the Tree**: - Implement minimal cost-complexity pruning using the `ccp_alpha` parameter. - Use the pruning strategy to reduce overfitting. 4. **Evaluate the Model**: - Compute the accuracy of the pruned tree on the test set using `accuracy_score` from `sklearn.metrics`. **Example Usage:** ```python # Sample data X_train = [[1, 2], [np.nan, np.nan], [3, 4], [5, 6]] y_train = [0, 1, 0, 1] X_test = [[2, 3], [np.nan, 5]] y_test = [0, 1] accuracy = train_decision_tree_classifier(X_train, y_train, X_test, y_test, ccp_alpha=0.01) print(\\"Accuracy:\\", accuracy) ``` **Constraints:** - You cannot use any other machine learning libraries except for scikit-learn. - Ensure that the model is not biased toward dominant classes by balancing the dataset if required. - The test should handle both numerical and missing values in the dataset. **Notes:** - If no missing values are seen during training for a given feature, handle prediction cases with missing values as described. - Use `DecisionTreeClassifier(random_state=0)` for reproducibility. - The function should return a floating point number representing the accuracy of the model on the test set. **Performance Requirements:** - The function should complete execution within a reasonable time frame (no more than 5 seconds) for input data with up to 10,000 samples and 100 features.","solution":"import numpy as np from sklearn.tree import DecisionTreeClassifier from sklearn.metrics import accuracy_score from sklearn.impute import SimpleImputer def train_decision_tree_classifier(X_train, y_train, X_test, y_test, ccp_alpha=0.01): Trains a Decision Tree Classifier on a given dataset with handling of missing values and minimal cost-complexity pruning. Parameters: - X_train: numpy.ndarray, shape (num_samples, num_features) Training feature matrix - y_train: numpy.ndarray, shape (num_samples,) Training labels - X_test: numpy.ndarray, shape (num_samples, num_features) Test feature matrix - y_test: numpy.ndarray, shape (num_samples,) Test labels - ccp_alpha: float, default=0.01 Complexity parameter used for Minimal Cost-Complexity Pruning. Returns: - accuracy: float Accuracy score of the pruned tree on the test set. # Handle missing values using SimpleImputer imputer = SimpleImputer(strategy=\'mean\') X_train_imputed = imputer.fit_transform(X_train) X_test_imputed = imputer.transform(X_test) # Initialize DecisionTreeClassifier with random_state for reproducibility clf = DecisionTreeClassifier(random_state=0) # Train the decision tree classifier clf.fit(X_train_imputed, y_train) # Prune the tree using minimal cost-complexity pruning clf_pruned = DecisionTreeClassifier(random_state=0, ccp_alpha=ccp_alpha) clf_pruned.fit(X_train_imputed, y_train) # Predict on the test set y_pred = clf_pruned.predict(X_test_imputed) # Evaluate the model performance using accuracy score accuracy = accuracy_score(y_test, y_pred) return accuracy"},{"question":"# Semi-supervised Learning Implementation and Comparison **Objective**: Implement and compare two semi-supervised learning techniques, `SelfTrainingClassifier` and `LabelPropagation`, from the `sklearn.semi_supervised` module using a synthetic dataset. **Background**: You are provided with a synthetic dataset consisting of both labeled and unlabeled data points. Your task is to build and evaluate models using `SelfTrainingClassifier` and `LabelPropagation`. Compare their performance based on a set of evaluation metrics. **Dataset**: The dataset `points.csv` contains: - `X1, X2`: Feature columns representing the input features. - `Y`: Label column where a value of `-1` represents unlabeled data. **Requirements**: 1. Implement a function to load and preprocess the dataset. 2. Train a `SelfTrainingClassifier` using a `RandomForestClassifier` as the base estimator. Use at least two different parameter settings for `threshold`. 3. Train a `LabelPropagation` model. 4. Evaluate the models using appropriate metrics (e.g., accuracy, precision, recall) on the labeled portion of the dataset. 5. Compare the performance of both approaches and provide insights based on your observations. **Function Signatures**: ```python import pandas as pd from sklearn.ensemble import RandomForestClassifier from sklearn.semi_supervised import SelfTrainingClassifier, LabelPropagation from sklearn.metrics import accuracy_score, precision_score, recall_score def load_data(file_path: str) -> pd.DataFrame: Load the dataset from a CSV file. :param file_path: Path to the CSV file. :return: DataFrame with columns [\'X1\', \'X2\', \'Y\']. pass # Implement this function def train_self_training_classifier(data: pd.DataFrame, threshold: float) -> SelfTrainingClassifier: Train a SelfTrainingClassifier on the dataset. :param data: DataFrame containing features and labels. :param threshold: Confidence threshold for self-training. :return: Trained SelfTrainingClassifier model. pass # Implement this function def train_label_propagation(data: pd.DataFrame) -> LabelPropagation: Train a LabelPropagation model on the dataset. :param data: DataFrame containing features and labels. :return: Trained LabelPropagation model. pass # Implement this function def evaluate_model(model, data: pd.DataFrame) -> dict: Evaluate the model on the labeled data. :param model: Trained semi-supervised model. :param data: DataFrame containing features and labels. :return: Dictionary with accuracy, precision, and recall. pass # Implement this function def compare_models(data: pd.DataFrame) -> None: Compare the performance of SelfTrainingClassifier and LabelPropagation models. :param data: DataFrame containing features and labels. pass # Implement this function ``` **Instructions**: 1. Implement each of the functions specified above. 2. Use the `load_data` function to load the dataset (`points.csv`). 3. Train the `SelfTrainingClassifier` and `LabelPropagation` models on the dataset. 4. Use the `evaluate_model` function to evaluate their performance. 5. Implement the `compare_models` function to compare and summarize the results of both models. 6. Ensure your code is well-documented with comments explaining each step. **Submission**: Submit your code along with a brief report (in a markdown cell or separate document) discussing: - The parameter settings used for `SelfTrainingClassifier`. - The evaluation metrics obtained. - Insights and conclusions based on the model comparison. Good luck!","solution":"import pandas as pd from sklearn.ensemble import RandomForestClassifier from sklearn.semi_supervised import SelfTrainingClassifier, LabelPropagation from sklearn.metrics import accuracy_score, precision_score, recall_score from sklearn.model_selection import train_test_split def load_data(file_path: str) -> pd.DataFrame: Load the dataset from a CSV file. :param file_path: Path to the CSV file. :return: DataFrame with columns [\'X1\', \'X2\', \'Y\']. return pd.read_csv(file_path) def train_self_training_classifier(data: pd.DataFrame, threshold: float) -> SelfTrainingClassifier: Train a SelfTrainingClassifier on the dataset. :param data: DataFrame containing features and labels. :param threshold: Confidence threshold for self-training. :return: Trained SelfTrainingClassifier model. X = data[[\'X1\', \'X2\']] y = data[\'Y\'] base_clf = RandomForestClassifier() self_train_clf = SelfTrainingClassifier(base_clf, threshold=threshold) self_train_clf.fit(X, y) return self_train_clf def train_label_propagation(data: pd.DataFrame) -> LabelPropagation: Train a LabelPropagation model on the dataset. :param data: DataFrame containing features and labels. :return: Trained LabelPropagation model. X = data[[\'X1\', \'X2\']] y = data[\'Y\'] label_prop_model = LabelPropagation() label_prop_model.fit(X, y) return label_prop_model def evaluate_model(model, data: pd.DataFrame) -> dict: Evaluate the model on the labeled data. :param model: Trained semi-supervised model. :param data: DataFrame containing features and labels. :return: Dictionary with accuracy, precision, and recall. X = data[[\'X1\', \'X2\']] y_true = data[\'Y\'] y_pred = model.predict(X) y_true_labeled = y_true[y_true != -1] y_pred_labeled = y_pred[y_true != -1] accuracy = accuracy_score(y_true_labeled, y_pred_labeled) precision = precision_score(y_true_labeled, y_pred_labeled, average=\'macro\', zero_division=0) recall = recall_score(y_true_labeled, y_pred_labeled, average=\'macro\', zero_division=0) return { \'accuracy\': accuracy, \'precision\': precision, \'recall\': recall } def compare_models(data: pd.DataFrame) -> None: Compare the performance of SelfTrainingClassifier and LabelPropagation models. :param data: DataFrame containing features and labels. thresholds = [0.75, 0.95] results = {} # Evaluate SelfTrainingClassifier with different thresholds for threshold in thresholds: model = train_self_training_classifier(data, threshold) results[f\'SelfTraining (threshold={threshold})\'] = evaluate_model(model, data) # Evaluate LabelPropagation model = train_label_propagation(data) results[\'LabelPropagation\'] = evaluate_model(model, data) # Print out the comparison for model_name, metrics in results.items(): print(f\'nModel: {model_name}\') print(f\\" Accuracy: {metrics[\'accuracy\']:.2f}\\") print(f\\" Precision: {metrics[\'precision\']:.2f}\\") print(f\\" Recall: {metrics[\'recall\']:.2f}\\")"},{"question":"# Advanced Python Coding Assessment Objective: Implement a custom Python meta path finder and loader that can dynamically load modules from a specific directory. Problem Statement: You are required to create a custom meta path finder and loader that can load Python modules from a designated directory. This should involve implementing the necessary classes and methods to integrate with Python’s import system. Requirements: 1. **Custom Finder**: - Create a class `CustomFinder` that implements the `find_spec` method. - This finder should search for modules in a specified directory (`custom_dir`). 2. **Custom Loader**: - Create a class `CustomLoader` that loads the module from the specified directory. - Implement the `exec_module` method to execute the module\'s code. 3. **Integration**: - Integrate `CustomFinder` into `sys.meta_path` to enable the custom loading behavior. Input and Output: - Input: Module name as a string (e.g., `\\"mymodule\\"`). - Output: Python module loaded from the designated directory. Constraints: - The custom directory should only contain Python files (`.py`). - The code should handle errors gracefully, such as nonexistent files. - Ensure that the custom finder and loader do not interfere with Python’s default import system for modules not located in the custom directory. Example Usage: ```python # Define custom directory custom_dir = \\"/path/to/custom_modules\\" # Add custom finder to sys.meta_path sys.meta_path.insert(0, CustomFinder(custom_dir)) # Import a module from the custom directory import mymodule # Assume mymodule prints \\"Hello from custom module\\" when imported ``` Notes: - You may assume that the custom directory and the `.py` files it contains are already available. - Provide sufficient comments and documentation within your code. - This task assesses your understanding of Python’s import system, creating and registering custom finders and loaders, and manipulating `sys.meta_path`. **Good luck!**","solution":"import sys import os import importlib.util import importlib.machinery import types class CustomFinder: def __init__(self, directory): self.directory = directory def find_spec(self, fullname, path, target=None): module_path = os.path.join(self.directory, f\\"{fullname}.py\\") if not os.path.isfile(module_path): return None return importlib.util.spec_from_file_location(fullname, module_path, loader=CustomLoader(module_path)) class CustomLoader: def __init__(self, module_path): self.module_path = module_path def create_module(self, spec): return None # default module creation semantics def exec_module(self, module): with open(self.module_path, \'r\') as file: code = file.read() exec(code, module.__dict__) def setup_custom_import(directory): sys.meta_path.insert(0, CustomFinder(directory))"},{"question":"**Coding Assessment Question** # Objective: You are tasked with creating a custom distributed event logging system using PyTorch\'s `torch.distributed.elastic.events` module. The objective is to implement a function that records events occurring during different phases of a distributed training job. # Instructions: 1. Implement the function `log_distributed_event` which will: - Record an event using `torch.distributed.elastic.events.record`. - Include relevant metadata. 2. The function should handle: - Event creation and logging with metadata. - Using `construct_and_record_rdzv_event` to log a rendezvous event during initialization. - Retrieving a custom logging handler using `get_logging_handler`. # Function Signature: ```python def log_distributed_event(event_name: str, metadata: dict) -> None: pass ``` # Input: - `event_name` (str): The name of the event to be logged. - `metadata` (dict): A dictionary containing metadata for the event. # Expected Behavior: - The function should create an event of type `torch.distributed.elastic.events.api.Event`. - Record the event with the provided name and metadata. - Utilize `construct_and_record_rdzv_event` to log a specific initialization event. - Retrieve and use a custom logging handler with `get_logging_handler`. # Example: ```python event_name = \\"training_start\\" metadata = { \\"epoch\\": 1, \\"batch_size\\": 32, \\"lr\\": 0.001 } log_distributed_event(event_name, metadata) ``` # Constraints: - You must use PyTorch\'s `torch.distributed.elastic.events` module for all event handling and logging. - Ensure the function is robust and handles possible errors (e.g., invalid metadata). # Additional Notes: - You do not need to handle the actual distributed training logic, just focus on the event logging aspect. - Assume that the distributed environment is properly set up and the necessary imports and initializations are done outside your function.","solution":"import torch.distributed.elastic.events as events def log_distributed_event(event_name: str, metadata: dict) -> None: Records an event in the distributed logging system with the provided name and metadata. Args: event_name (str): The name of the event to log. metadata (dict): Metadata associated with the event. try: # Create the event event = events.Event(name=event_name, **metadata) # Record the event events.record(event) # Log a specific initialization event using construct_and_record_rdzv_event events.construct_and_record_rdzv_event(event_name, metadata) # Retrieve and use a custom logging handler handler = events.get_logging_handler() handler(event) except Exception as e: print(f\\"Failed to log event {event_name}: {e}\\")"},{"question":"# Question You are tasked to create a Python script that uses the deprecated `optparse` module to parse a set of command-line options. Your script should: 1. Define options for: - A string option `--username` (or `-u`) to specify the username. - A file option `--config` (or `-c`) to specify the path to a configuration file. - A boolean option `--verbose` (or `-v`) to turn on verbose mode. - A counter option `--level` (or `-l`) that increases a logging level each time it appears. - A list option `--items` (or `-i`) to append items to a list. 2. Generate appropriate help and usage messages. 3. Handle errors for missing required options (`--username` and `--config` must be specified). 4. Demonstrate the use of a callback option that validates the logging level does not exceed 5. Expected Input/Output: - **Command-Line Input:** ``` script.py --username admin --config config.yaml --verbose --level --level --items item1 --items item2 --items item3 ``` - **Output:** ``` Username: admin Config File: config.yaml Verbose: True Logging Level: 2 Items: [\'item1\', \'item2\', \'item3\'] ``` **Instructions:** 1. Implement the function `parse_command_line()` that sets up and parses the command-line options using `optparse`. 2. Implement a callback function `check_logging_level(option, opt, value, parser)` that raises an error if the logging level exceeds 5. 3. Ensure that the required options `--username` and `--config` are provided, or terminate with an appropriate error message. 4. Print the parsed options in a formatted manner as shown in the expected output. **Constraints:** - You must use the `optparse` module to define and parse the options. - You must implement and use the callback function for validating the logging level. ```python from optparse import OptionParser, OptionValueError def check_logging_level(option, opt, value, parser): Callback to check that logging level does not exceed 5. if value > 5: raise OptionValueError(f\\"option {opt}: logging level cannot exceed 5\\") setattr(parser.values, option.dest, value) def parse_command_line(): Parses command-line options using `optparse`. usage = \\"usage: %prog [options]\\" parser = OptionParser(usage=usage) parser.add_option(\\"-u\\", \\"--username\\", dest=\\"username\\", type=\\"string\\", help=\\"specify the username\\", metavar=\\"USERNAME\\") parser.add_option(\\"-c\\", \\"--config\\", dest=\\"config\\", type=\\"string\\", help=\\"specify the config file\\", metavar=\\"CONFIG\\") parser.add_option(\\"-v\\", \\"--verbose\\", action=\\"store_true\\", dest=\\"verbose\\", help=\\"turn on verbose mode\\", default=False) parser.add_option(\\"-l\\", \\"--level\\", action=\\"count\\", dest=\\"level\\", help=\\"increase logging level\\", default=0) parser.add_option(\\"-i\\", \\"--items\\", action=\\"append\\", dest=\\"items\\", help=\\"append items to a list\\", metavar=\\"ITEM\\") (options, args) = parser.parse_args() # Required options if not options.username: parser.error(\\"username is required (use -u or --username)\\") if not options.config: parser.error(\\"config file is required (use -c or --config)\\") return options def main(): options = parse_command_line() print(f\\"Username: {options.username}\\") print(f\\"Config File: {options.config}\\") print(f\\"Verbose: {options.verbose}\\") print(f\\"Logging Level: {options.level}\\") print(f\\"Items: {options.items}\\") if __name__ == \\"__main__\\": main() ```","solution":"from optparse import OptionParser, OptionValueError def check_logging_level(option, opt, value, parser): Callback to check that logging level does not exceed 5. if value > 5: raise OptionValueError(f\\"option {opt}: logging level cannot exceed 5\\") setattr(parser.values, option.dest, value) def parse_command_line(): Parses command-line options using `optparse`. usage = \\"usage: %prog [options]\\" parser = OptionParser(usage=usage) parser.add_option(\\"-u\\", \\"--username\\", dest=\\"username\\", type=\\"string\\", help=\\"specify the username\\", metavar=\\"USERNAME\\") parser.add_option(\\"-c\\", \\"--config\\", dest=\\"config\\", type=\\"string\\", help=\\"specify the config file\\", metavar=\\"CONFIG\\") parser.add_option(\\"-v\\", \\"--verbose\\", action=\\"store_true\\", dest=\\"verbose\\", help=\\"turn on verbose mode\\", default=False) parser.add_option(\\"-l\\", \\"--level\\", action=\\"count\\", dest=\\"level\\", help=\\"increase logging level\\", default=0) parser.add_option(\\"-i\\", \\"--items\\", action=\\"append\\", dest=\\"items\\", help=\\"append items to a list\\", metavar=\\"ITEM\\") (options, args) = parser.parse_args() # Required options if not options.username: parser.error(\\"username is required (use -u or --username)\\") if not options.config: parser.error(\\"config file is required (use -c or --config)\\") return options def main(): options = parse_command_line() print(f\\"Username: {options.username}\\") print(f\\"Config File: {options.config}\\") print(f\\"Verbose: {options.verbose}\\") print(f\\"Logging Level: {options.level}\\") print(f\\"Items: {options.items}\\") if __name__ == \\"__main__\\": main()"},{"question":"# PyTorch Coding Assessment: Hierarchical Module Tracking Objective This coding challenge tests your understanding of creating hierarchical module structures in PyTorch and leveraging the `torch.utils.module_tracker` utility to track these structures. Problem Statement You are required to implement a custom neural network in PyTorch that comprises multiple nested modules. After building this network, you will use `torch.utils.module_tracker` to navigate and keep track of the position within the module hierarchy. Furthermore, you will associate custom metrics with each layer using the tracked information. Task 1. **Define a custom neural network**: - Create a class `CustomNet` that inherits from `torch.nn.Module`. - This network should have at least three nested modules (e.g., `Conv2d` layers in varying hierarchies). 2. **Implement Module Tracking**: - Use `torch.utils.module_tracker.ModuleTracker` to navigate and track positions within the `CustomNet` structure while processing a forward pass. - Define a function `track_and_measure` that performs a forward pass through the network and tracks the position within each layer, associating a dummy performance metric (such as FLOPs) with each layer. Input - The dimensions and structure of the custom neural network modules (you may define this based on typical network structures). - A dummy tensor input to feed into the network for the forward pass. Output - Print a hierarchical representation of the module positions and associated dummy metrics. Constraints - The network should be sufficiently complex to showcase the utility of the `ModuleTracker`. - Ensure that the implementation correctly handles nested modules and associates metrics accurately. Performance Requirements - The solution should demonstrate efficient execution and correct hierarchical tracking. ```python import torch import torch.nn as nn from torch.utils.module_tracker import ModuleTracker class CustomNet(nn.Module): def __init__(self): super(CustomNet, self).__init__() # Define nested modules here self.layer1 = nn.Sequential( nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3), nn.ReLU(), ) self.layer2 = nn.Sequential( nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3), nn.ReLU(), ) self.layer3 = nn.Sequential( nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3), nn.ReLU(), ) def forward(self, x): # Perform forward pass through nested modules x = self.layer1(x) x = self.layer2(x) x = self.layer3(x) return x def track_and_measure(net, input_tensor): tracker = ModuleTracker() results = {} # Register the hooks for tracking def register_hooks(module, name): def hook(module, input, output): tracker.enter(module) position = tracker.get_current_position() # Associate a dummy metric with the layer (e.g., number of features) results[position] = (name, output.numel()) # Dummy metric tracker.leave() return hook # Recursively register the hooks to all layers def register_recursive(module, prefix): for name, submodule in module.named_children(): full_name = f\\"{prefix}.{name}\\" if prefix else name submodule.register_forward_hook(register_hooks(submodule, full_name)) register_recursive(submodule, full_name) register_recursive(net, prefix=\\"\\") # Perform the forward pass _ = net(input_tensor) for position, (name, metric) in results.items(): print(f\\"Layer: {name}, Position: {position}, Dummy Metric: {metric}\\") # Example usage: input_tensor = torch.randn(1, 3, 224, 224) # Dummy input tensor model = CustomNet() track_and_measure(model, input_tensor) ``` Your implementation must correctly register hooks, perform the forward pass, and print the layer position and associated dummy metrics. Considerations - Ensure that the hierarchical tracking reflects the correct module depth and sequence. - The dummy metric can be any quantifiable aspect associated with the layer\'s output (e.g., number of elements, FLOPs, etc.). Evaluation Your solution will be evaluated based on: 1. Correctness of module definitions with proper nested structures. 2. Accurate implementation of module tracking. 3. Proper association and display of dummy metrics for each module.","solution":"import torch import torch.nn as nn class CustomNet(nn.Module): def __init__(self): super(CustomNet, self).__init__() # Define nested modules here self.layer1 = nn.Sequential( nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3), nn.ReLU(), ) self.layer2 = nn.Sequential( nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3), nn.ReLU(), ) self.layer3 = nn.Sequential( nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3), nn.ReLU(), ) def forward(self, x): # Perform forward pass through nested modules x = self.layer1(x) x = self.layer2(x) x = self.layer3(x) return x def track_and_measure(net, input_tensor): # We will use hooks to track and measure layers results = {} # Register hooks for tracking def register_hooks(module, name): def hook(module, input, output): position = name # Associate a dummy metric with the layer (e.g., number of elements in the output tensor) results[position] = output.numel() return hook # Recursively register hooks def register_recursive(module, prefix=\\"\\"): for name, submodule in module.named_children(): full_name = f\\"{prefix}.{name}\\" if prefix else name submodule.register_forward_hook(register_hooks(submodule, full_name)) register_recursive(submodule, full_name) register_recursive(net) # Perform a forward pass _ = net(input_tensor) for position, metric in results.items(): print(f\\"Layer: {position}, Dummy Metric: {metric}\\") input_tensor = torch.randn(1, 3, 224, 224) # Dummy input tensor model = CustomNet() track_and_measure(model, input_tensor)"},{"question":"**Task: Implementing and analyzing Unix shadow password data** You are provided with access to the Unix shadow password database via the `spwd` module. This module includes entries with various details about user accounts. You are required to implement a function that performs specific operations on this data, demonstrating your understanding of the module\'s functionalities. # Function Details Function 1: get_user_info - **Input**: A string `username` representing the login name of the user. - **Output**: A dictionary with the following keys: - `name`: Login name of the user. - `last_change`: Date of last password change (in days since epoch). - `min_days`: Minimal number of days between password changes. - `max_days`: Maximum number of days between password changes. - `warn_days`: Number of days before password expires to warn user. - **Behavior**: - Raise a `ValueError` with the message \\"User not found\\" if the given `username` does not exist. - Raise a `PermissionError` if the user does not have the necessary privileges to access the shadow password database. Function 2: analyze_password_changes - **Input**: None - **Output**: A list of tuples. Each tuple should contain the login name and the number of days between the last and the next password change, for users who are required to change their password within a specific interval. - **Behavior**: - Fetch all shadow password database entries using `spwd.getspall()`. - Exclude users who have `sp_max` set to -1 (password never expires). - Calculate the number of days between the last password change and the expiry date for each user. If `sp_expire` is set to -1 (account never expires), use `sp_max` to estimate the next password change date. - Return the result sorted by the login name. # Example ```python # Example usage of the functions try: user_info = get_user_info(\'alice\') print(user_info) except ValueError as ve: print(ve) except PermissionError as pe: print(pe) changes = analyze_password_changes() print(changes) ``` # Constraints and Notes - Assume that the `spwd` module is available and you have the required permissions. - Handle any other exceptions that might occur gracefully. - Ensure that your solution is efficient and avoids unnecessary computations. The goal of this task is to assess your ability to handle and process sensitive system data using Python\'s `spwd` module, while adhering to best practices for error handling and efficiency.","solution":"import spwd import os def get_user_info(username): Fetches and returns shadow password data for the given username. Args: username (str): The username to fetch the data for. Raises: ValueError: If the username is not found. PermissionError: If the access to the shadow file is denied. Returns: dict: A dictionary containing the user information. try: user_entry = spwd.getspnam(username) user_info = { \'name\': user_entry.sp_namp, \'last_change\': user_entry.sp_lstchg, \'min_days\': user_entry.sp_min, \'max_days\': user_entry.sp_max, \'warn_days\': user_entry.sp_warn, } return user_info except KeyError: raise ValueError(\\"User not found\\") except PermissionError: raise PermissionError(\\"Access to the shadow file is denied\\") def analyze_password_changes(): Analyzes all users\' shadow password data and calculates the number of days between the last password change and the next required password change. Returns: list: A list of tuples with username and days between the changes. try: entries = spwd.getspall() result = [] for entry in entries: if entry.sp_max != -1: next_change = entry.sp_lstchg + entry.sp_max result.append((entry.sp_namp, next_change - entry.sp_lstchg)) result.sort(key=lambda x: x[0]) return result except PermissionError: raise PermissionError(\\"Access to the shadow file is denied\\")"},{"question":"Advanced File and Data Operations **Objective:** To assess students\' understanding of file operations, string formatting, and JSON handling in Python. You are required to write a Python script that meets the following requirements: 1. **Reading Input:** - Read the contents of a file named `input_data.txt`. The file contains multiple lines, each representing a record. Each record consists of a name (string) and a score (integer) separated by a comma. - Example content of `input_data.txt`: ``` Alice,85 Bob,92 Charlie,88 ``` 2. **Data Processing:** - Calculate the average score of all the records. - Create a new list where each entry is a dictionary with the keys `\\"name\\"`, `\\"score\\"`, and `\\"above_average\\"`. The value of `\\"above_average\\"` should be `True` if the score is above the calculated average, otherwise `False`. 3. **Formatting Output:** - Create a string that summarizes the results. The summary should: - List each name and their score. - Show the average score. - Indicate which scores are above the average. - Example format: ``` Alice: 85 Bob: 92 Charlie: 88 Average Score: 88.33 Above Average: - Bob ``` 4. **Writing Output:** - Write the formatted summary string to a new file named `output_summary.txt`. - Write the list of dictionaries to another file named `output_data.json` in JSON format, using UTF-8 encoding. **Constraints:** - The input file name `input_data.txt` and the output file names `output_summary.txt` and `output_data.json` should not be hardcoded; they should be accepted as command-line arguments. - Handle any possible exceptions that might occur during file operations (e.g., file not found, permission error). **Function Signature:** ```python def process_data(input_file: str, summary_output_file: str, json_output_file: str) -> None: pass ``` **Examples:** If `input_data.txt` contains: ``` Alice,85 Bob,92 Charlie,88 ``` The contents of `output_summary.txt` should be: ``` Alice: 85 Bob: 92 Charlie: 88 Average Score: 88.33 Above Average: - Bob ``` and `output_data.json` should be: ```json [ {\\"name\\": \\"Alice\\", \\"score\\": 85, \\"above_average\\": false}, {\\"name\\": \\"Bob\\", \\"score\\": 92, \\"above_average\\": true}, {\\"name\\": \\"Charlie\\", \\"score\\": 88, \\"above_average\\": false} ] ``` Good luck!","solution":"import json import sys def process_data(input_file: str, summary_output_file: str, json_output_file: str) -> None: try: # Read the input file with open(input_file, \'r\') as file: lines = file.readlines() # Parse the records records = [] total_score = 0 for line in lines: name, score = line.strip().split(\',\') score = int(score) records.append({\\"name\\": name, \\"score\\": score}) total_score += score # Calculate the average score average_score = total_score / len(records) # Process the records for record in records: record[\\"above_average\\"] = record[\\"score\\"] > average_score # Prepare the summary summary_lines = [f\\"{record[\'name\']}: {record[\'score\']}\\" for record in records] summary = \'n\'.join(summary_lines) summary += f\\"nnAverage Score: {average_score:.2f}nnAbove Average:\\" above_average_names = [f\\"- {record[\'name\']}\\" for record in records if record[\\"above_average\\"]] summary += \'n\' + \'n\'.join(above_average_names) # Write the summary to the summary output file with open(summary_output_file, \'w\') as file: file.write(summary) # Write the json output to the json output file with open(json_output_file, \'w\', encoding=\'utf-8\') as file: json.dump(records, file, ensure_ascii=False, indent=4) except Exception as e: print(f\\"An error occurred: {e}\\", file=sys.stderr)"},{"question":"Advanced Text Processing using Regular Expressions Objective: To create a Python function that demonstrates an understanding of text processing, specifically using the `re` module for advanced regular expression operations. Problem Statement: You are required to implement a function that scans through a given string and identifies all email addresses and phone numbers. The email addresses should follow the standard format of `username@domain.extension` and phone numbers should match patterns such as `(XXX) XXX-XXXX`, `XXX-XXX-XXXX`, `XXX.XXX.XXXX`, or `XXX XXX XXXX`. You need to create a function called `extract_contact_info` that accepts a single string argument and returns a dictionary with two keys: `emails` and `phone_numbers`. The values associated with these keys should be lists containing the found email addresses and phone numbers, respectively. Input: - A single string `text` containing any number of words, email addresses, and phone numbers. Output: - A dictionary with keys `emails` and `phone_numbers`. The values are lists that should contain all the extracted emails and phone numbers from the input string. Constraints: - The input string can vary from being empty to very large, so efficiency is important. - Each phone number should follow one of the specified patterns. - Each email address should follow a standard email format. Example: ```python def extract_contact_info(text: str) -> dict: pass # Example text example_text = Contact us at support@example.com or call (555) 123-4567. Emails like admin@example.org and info@example.net are also supported. For direct support, call 555.867.5309 or 555 555 5555. # Expected Output: { \'emails\': [\'support@example.com\', \'admin@example.org\', \'info@example.net\'], \'phone_numbers\': [\'(555) 123-4567\', \'555.867.5309\', \'555 555 5555\'] } ``` # Required Skills: - Proficiency with the `re` module for regular expressions in Python. - Understanding of basic string manipulation techniques. Additional Information: You may find the following regular expression patterns helpful: - Email addresses: `[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+.[a-zA-Z0-9-.]+` - Phone numbers: - `(d{3}) d{3}-d{4}` - `d{3}-d{3}-d{4}` - `d{3}.d{3}.d{4}` - `d{3} d{3} d{4}` Your implementation should be efficient enough to handle large texts and should accurately capture all relevant contact information.","solution":"import re def extract_contact_info(text: str) -> dict: Extracts email addresses and phone numbers from the given text. Args: text (str): The input text containing email addresses and phone numbers. Returns: dict: A dictionary with keys \'emails\' and \'phone_numbers\' containing lists of found email addresses and phone numbers. email_pattern = r\'[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+.[a-zA-Z0-9-.]+\' phone_pattern = r\'(d{3}) d{3}-d{4}|d{3}-d{3}-d{4}|d{3}.d{3}.d{4}|d{3} d{3} d{4}\' emails = re.findall(email_pattern, text) phone_numbers = re.findall(phone_pattern, text) return { \'emails\': emails, \'phone_numbers\': phone_numbers }"},{"question":"**Custom Python Extension Type** # Context You have been introduced to the creation of custom Python types using C extensions—a concept that allows native performance and memory control while providing new Python types. In this exercise, you will create a new Python object type `Vector`, which represents a geometric vector in a 2D space with `x` and `y` coordinates. # Task Implement a Python C-extension type `Vector` with the following characteristics: 1. **Attributes**: - `x` (double): The x-coordinate of the vector. - `y` (double): The y-coordinate of the vector. 2. **Methods**: - `magnitude(self)`: - Returns the magnitude (length) of the vector calculated as ( sqrt{x^2 + y^2} ). - `add(self, other)`: - Returns a new vector which is the sum of the current vector and another vector passed as an argument. Ensure that the passed argument is also a `Vector` instance. # Requirements - Correctly manage the memory allocation and deallocation in the creation and destruction of the `Vector` objects. - Use proper getter and setter mechanisms to access the attributes. The `x` and `y` attributes should be restricted to non-null double values. - Ensure your implementation supports garbage collection to manage potential cyclic references. # Constraints - Implement memory management to avoid memory leaks. - Ensure the methods correctly handle type checking and error reporting. # Boilerplate Here is a minimal boilerplate to get you started with defining the `Vector` extension type: ```c #define PY_SSIZE_T_CLEAN #include <Python.h> #include <math.h> typedef struct { PyObject_HEAD double x; double y; } VectorObject; static void Vector_dealloc(VectorObject* self) { Py_TYPE(self)->tp_free((PyObject *) self); } static PyObject * Vector_new(PyTypeObject *type, PyObject *args, PyObject *kwds) { VectorObject *self; self = (VectorObject *) type->tp_alloc(type, 0); if (self != NULL) { self->x = 0.0; self->y = 0.0; } return (PyObject *) self; } static int Vector_init(VectorObject *self, PyObject *args, PyObject *kwds) { static char *kwlist[] = {\\"x\\", \\"y\\", NULL}; if (!PyArg_ParseTupleAndKeywords(args, kwds, \\"|dd\\", kwlist, &self->x, &self->y)) return -1; return 0; } static PyObject * Vector_magnitude(VectorObject* self, PyObject *Py_UNUSED(ignored)) { return PyFloat_FromDouble(sqrt(self->x * self->x + self->y * self->y)); } static PyObject * Vector_add(VectorObject* self, PyObject *args) { VectorObject *other; if (!PyArg_ParseTuple(args, \\"O!\\", &VectorType, (PyObject**)&other)) { return NULL; } return PyObject_CallFunction((PyObject*)&VectorType, \\"dd\\", self->x + other->x, self->y + other->y); } static PyMethodDef Vector_methods[] = { {\\"magnitude\\", (PyCFunction) Vector_magnitude, METH_NOARGS, \\"Return the magnitude of the vector\\"}, {\\"add\\", (PyCFunction) Vector_add, METH_VARARGS, \\"Add another vector and return the result\\"}, {NULL} /* Sentinel */ }; static PyTypeObject VectorType = { PyVarObject_HEAD_INIT(NULL, 0) .tp_name = \\"mymodule.Vector\\", .tp_doc = \\"Vector objects\\", .tp_basicsize = sizeof(VectorObject), .tp_itemsize = 0, .tp_flags = Py_TPFLAGS_DEFAULT, .tp_new = Vector_new, .tp_init = (initproc) Vector_init, .tp_dealloc = (destructor) Vector_dealloc, .tp_methods = Vector_methods, }; static PyModuleDef mymodule = { PyModuleDef_HEAD_INIT, .m_name = \\"mymodule\\", .m_doc = \\"Example module that creates a Vector type.\\", .m_size = -1, }; PyMODINIT_FUNC PyInit_mymodule(void) { PyObject *m; if (PyType_Ready(&VectorType) < 0) return NULL; m = PyModule_Create(&mymodule); if (m == NULL) return NULL; Py_INCREF(&VectorType); if (PyModule_AddObject(m, \\"Vector\\", (PyObject *) &VectorType) < 0) { Py_DECREF(&VectorType); Py_DECREF(m); return NULL; } return m; } ``` # Testing After implementing your `Vector` type, create a `setup.py` for building the module and test it using the following Python code: ```python from mymodule import Vector v1 = Vector(3, 4) print(v1.x, v1.y) # Outputs: 3 4 print(v1.magnitude()) # Outputs: 5.0 v2 = Vector(1, 2) v3 = v1.add(v2) print(v3.x, v3.y) # Outputs: 4 6 ``` **Note**: Detailed comments and adherence to the C API style are expected.","solution":"import math class Vector: def __init__(self, x=0.0, y=0.0): self.x = x self.y = y def magnitude(self): return math.sqrt(self.x ** 2 + self.y ** 2) def add(self, other): if not isinstance(other, Vector): raise TypeError(\\"The argument must be an instance of Vector\\") return Vector(self.x + other.x, self.y + other.y)"},{"question":"# Password Hashing and Validation System You are tasked with implementing a simple password management system using the deprecated `crypt` module in Python. Your system should provide functionalities for the following operations: 1. **Registering a new user with a hashed password.** 2. **Validating user login by comparing the entered password with the stored hashed password.** Implement the following functions: Function 1: `register_user(username: str, password: str) -> dict` This function registers a new user by hashing their password with the strongest available hashing method and stores the hashed password. - **Input:** - `username`: A string representing the username. - `password`: A string representing the user\'s password. - **Output:** - Returns a dictionary where the key is the username and the value is the hashed password. Function 2: `validate_login(username: str, entered_password: str, user_data: dict) -> bool` This function validates the entered password for the given username by comparing it to the stored hashed password in the `user_data` dictionary. - **Input:** - `username`: A string representing the username. - `entered_password`: A string representing the password entered at login. - `user_data`: A dictionary where the key is the username and the value is the hashed password. - **Output:** - Returns `True` if the entered password matches the stored hashed password, `False` otherwise. # Constraints: - Assume that usernames are unique. - Use `getpass` to securely capture user input for passwords. - Use the strongest available method for hashing passwords during registration. - Use constant-time comparison (`hmac.compare_digest`) to compare hashed passwords to avoid timing attacks. # Example: ```python import getpass def main(): user_data = {} # Register a new user username = input(\\"Enter new username: \\") password = getpass.getpass(\\"Enter new password: \\") user_data = register_user(username, password) # Validate login entered_username = input(\\"Enter username: \\") entered_password = getpass.getpass(\\"Enter password: \\") if validate_login(entered_username, entered_password, user_data): print(\\"Login successful!\\") else: print(\\"Login failed!\\") if __name__ == \\"__main__\\": main() ```","solution":"import crypt import hmac def register_user(username: str, password: str) -> dict: Registers a new user by hashing the password. Parameters: username (str): A string representing the username. password (str): A string representing the user\'s password. Returns: dict: A dictionary where the key is the username and the value is the hashed password. hashed_password = crypt.crypt(password) return {username: hashed_password} def validate_login(username: str, entered_password: str, user_data: dict) -> bool: Validates the entered password for the given username. Parameters: username (str): A string representing the username. entered_password (str): A string representing the password entered at login. user_data (dict): A dictionary where the key is the username and the value is the hashed password. Returns: bool: True if the entered password matches the stored hashed password, False otherwise. if username not in user_data: return False stored_hashed_password = user_data[username] return hmac.compare_digest(crypt.crypt(entered_password, stored_hashed_password), stored_hashed_password)"},{"question":"Coding Assessment Question # Secure Message System You are tasked with designing a secure message authentication system. The system will: 1. Hash messages using a secure hashing algorithm. 2. Use keyed-hashing for message authenticity. 3. Generate secure tokens to authenticate sessions. You\'ll need to implement the following functions: # Function 1: `generate_hash(message: str) -> str` **Parameters:** - `message` (str): The input message string. **Returns:** - A hexadecimal string that represents the hash of the message using SHA256. # Function 2: `verify_message_with_hmac(message: str, key: bytes) -> str` **Parameters:** - `message` (str): The input message string. - `key` (bytes): The secret key used for HMAC. **Returns:** - A hexadecimal string that represents the HMAC of the message using the provided key and SHA256. # Function 3: `generate_secure_token(length: int) -> str` **Parameters:** - `length` (int): The length of the secure token in bytes. **Returns:** - A secure token (hexadecimal string) of the specified length. # Example Usage: ```python message = \\"This is a secure message.\\" key = b\'secret_key\' hash_result = generate_hash(message) hmac_result = verify_message_with_hmac(message, key) secure_token = generate_secure_token(16) print(f\\"Hash: {hash_result}\\") print(f\\"HMAC: {hmac_result}\\") print(f\\"Secure Token: {secure_token}\\") ``` # Constraints: - Ensure that the keys and tokens are generated securely using appropriate Python310 libraries. - Implement proper error handling for inputs. - The functions should work efficiently for typical message lengths and key sizes. # Performance Requirements: - The hashing and HMAC operations should be performed in a time-complex manner appropriate for secure cryptographic applications. - Secure token generation should be optimized for generating the required length without unnecessary computational overhead. # Evaluation Criteria: - Correctness: The implemented functions should return the correct results based on the inputs. - Security: The implementation should use secure algorithms and methods as defined in the Python310 cryptographic services documentation. - Efficiency: The implementation should handle typical use cases efficiently without excessive resource usage. - Code Quality: The code should be well-organized, readable, and maintainable. Good luck, and secure those messages well!","solution":"import hashlib import hmac import os def generate_hash(message: str) -> str: Generates a SHA256 hash of the input message. Parameters: - message (str): The input message string. Returns: - str: The SHA256 hash of the message as a hexadecimal string. sha_signature = hashlib.sha256(message.encode()).hexdigest() return sha_signature def verify_message_with_hmac(message: str, key: bytes) -> str: Generates a HMAC of the message using the provided key and SHA256. Parameters: - message (str): The input message string. - key (bytes): The secret key used for HMAC. Returns: - str: The HMAC of the message as a hexadecimal string. hmac_signature = hmac.new(key, message.encode(), hashlib.sha256).hexdigest() return hmac_signature def generate_secure_token(length: int) -> str: Generates a secure token of specified length. Parameters: - length (int): The length of the secure token in bytes. Returns: - str: A secure token as a hexadecimal string of the specified length. token = os.urandom(length) return token.hex()"},{"question":"You are tasked with writing a function `get_package_details(package_name: str) -> dict` that provides comprehensive metadata about a specified installed package. This function should utilize the `importlib.metadata` library in Python 3.10 to query and return the following details: 1. Package name 2. Package version 3. List of all metadata fields (as a dictionary, where keys are the metadata field names and values are the corresponding metadata values) 4. List of all entry points (as a dictionary, where keys are entry point groups and values are lists of entry point names within each group) 5. List of all installed files (as a list of file names) 6. List of all distribution requirements (as a list of requirement specifications) # Expected Input and Output - **Input**: - `package_name`: A string representing the name of the package to query. - **Output**: - A dictionary with the following structure: ```python { \\"name\\": <package_name>, \\"version\\": <package_version>, \\"metadata\\": { <metadata_field_name>: <metadata_field_value>, ... }, \\"entry_points\\": { <entry_point_group>: [<entry_point_name>, ...], ... }, \\"files\\": [<file_name>, ...], \\"requirements\\": [<requirement_spec>, ...] } ``` # Function Signature ```python def get_package_details(package_name: str) -> dict: pass ``` # Constraints - Ensure that the function handles packages that may have incomplete metadata gracefully. - If the package does not exist, the function should raise a `ValueError` with the message `\\"Package not found\\"`. - Assume that the package is installed in the environment where the function is being executed. # Example ```python # Example usage: result = get_package_details(\'wheel\') # Expected result format: result == { \\"name\\": \\"wheel\\", \\"version\\": \\"0.32.3\\", \\"metadata\\": { \\"Metadata-Version\\": \\"2.1\\", \\"Name\\": \\"wheel\\", \\"Version\\": \\"0.32.3\\", ... }, \\"entry_points\\": { \\"console_scripts\\": [\\"wheel\\"], ... }, \\"files\\": [\\"wheel/util.py\\", ...], \\"requirements\\": [\\"pytest (>=3.0.0) ; extra == \'test\'\\", \\"pytest-cov ; extra == \'test\'\\"] } ``` This question assesses your ability to understand and use the `importlib.metadata` library to gather and process metadata for Python packages. Good luck!","solution":"import importlib.metadata from typing import Dict def get_package_details(package_name: str) -> dict: try: distribution = importlib.metadata.distribution(package_name) except importlib.metadata.PackageNotFoundError: raise ValueError(\\"Package not found\\") # Collect metadata metadata = dict(distribution.metadata) # Collect entry points entry_points_dict = {} for entry_point in distribution.entry_points: entry_points_dict.setdefault(entry_point.group, []).append(entry_point.name) # Collect files files = list(distribution.files) # Collect requirements requirements = list(distribution.requires or []) # Return the comprehensive metadata as a dictionary return { \\"name\\": distribution.metadata[\\"Name\\"], \\"version\\": distribution.version, \\"metadata\\": metadata, \\"entry_points\\": entry_points_dict, \\"files\\": [str(file) for file in files], \\"requirements\\": requirements }"},{"question":"# Python Coding Assessment Question **Objective:** Implement a Python function that mimics the behavior of the `PyOS_string_to_double` function described in the `python310` package documentation. Problem Statement You are required to write a function `string_to_double(s: str, overflow_exception: Optional[Exception] = None) -> float` that: 1. Converts a string `s` to a double-precision floating-point number. 2. Raises a `ValueError` if the string is not a valid representation of a floating-point number. 3. If `s` represents a value that is too large to store in a float, and `overflow_exception` is `None`, it returns `float(\'inf\')` or `float(\'-inf\')` based on the sign. 4. If the overflow_exception is provided and the value is too large, it raises the provided exception. 5. The function should not accept strings with leading or trailing whitespace. 6. It should convert as much of the string as possible and return the result if a partial conversion is valid. Constraints - Do not use Python’s built-in `float` constructor directly for the conversion, although you may use it internally to validate parts of your logic. - Do not use any third-party libraries. - The function should handle typical edge cases and raise appropriate exceptions if needed. Function Signature ```python def string_to_double(s: str, overflow_exception: Optional[Exception] = None) -> float: pass ``` Examples ```python try: result = string_to_double(\\"123.456\\") print(result) # Should print: 123.456 except ValueError as e: print(e) try: result = string_to_double(\\"1e500\\") print(result) # Should print: inf except ValueError as e: print(e) try: result = string_to_double(\\"1e500\\", overflow_exception=OverflowError) print(result) # Should not reach here, should raise OverflowError except OverflowError as e: print(e) # Should print an OverflowError message try: result = string_to_double(\\"abc\\") print(result) # Should not reach here, should raise ValueError except ValueError as e: print(e) # Should print a ValueError message ``` Notes - Your function should be robust and handle edge cases, such as very large numbers, invalid strings, and proper error handling. - Thoroughly test your implementation to ensure correctness.","solution":"def string_to_double(s, overflow_exception=None): Converts string to double, handling overflow and invalid strings. import re # Reject if there\'s any leading or trailing whitespace if s.strip() != s: raise ValueError(f\\"Invalid input, leading or trailing whitespace: {s}\\") try: # Attempt to convert string to float result = float(s) except ValueError: raise ValueError(f\\"Invalid input, not a float: {s}\\") # Check for overflow conditions if result == float(\'inf\') or result == float(\'-inf\'): if overflow_exception: raise overflow_exception(f\\"Overflow error for input: {s}\\") return result"},{"question":"**Coding Assessment Question** # Objective The goal of this assessment is to evaluate your understanding and ability to implement and utilize various cross-validation techniques using scikit-learn. # Problem Statement You are provided with the Iris dataset from scikit-learn. Your task is to implement a function that applies different cross-validation techniques to evaluate the performance of a Support Vector Machine (SVM) classifier with a linear kernel. You need to compare the performance of the classifier using the following cross-validation strategies: 1. K-Fold Cross-Validation. 2. Stratified K-Fold Cross-Validation. 3. Leave-One-Out Cross-Validation. 4. ShuffleSplit Cross-Validation. 5. GroupKFold Cross-Validation (we\'ll use a synthetic grouping based on the petal length for this task). # Function Signature ```python def evaluate_cross_validation(X: np.ndarray, y: np.ndarray) -> dict: Evaluates the performance of an SVM classifier using different cross-validation techniques. Parameters: X (np.ndarray): Feature matrix. y (np.ndarray): Target vector. Returns: dict: A dictionary containing the mean accuracy and standard deviation for each cross-validation strategy. ``` # Requirements 1. Load the Iris dataset. 2. Implement the function `evaluate_cross_validation` with the following steps: - Instantiate an SVM classifier with a linear kernel, `random_state=42`, and `C=1`. - Apply the following cross-validation strategies using scikit-learn: - K-Fold (with 5 folds). - Stratified K-Fold (with 5 folds). - Leave-One-Out. - ShuffleSplit (with 5 splits and 30% test size). - GroupKFold (we will create synthetic groups based on whether the petal length is `<= 3` or `> 3`). - For each cross-validation strategy, compute the mean accuracy and standard deviation of the classifier. 3. Return a dictionary with the mean accuracy and standard deviation for each cross-validation strategy. # Example Usage ```python import numpy as np from sklearn import datasets X, y = datasets.load_iris(return_X_y=True) results = evaluate_cross_validation(X, y) for strategy, metrics in results.items(): print(f\\"{strategy}: {metrics[\'mean_accuracy\']:.2f} accuracy with a standard deviation of {metrics[\'std_dev\']:.2f}\\") ``` # Constraints - Do not modify the SVM classifier parameters other than those specified. - You should use the appropriate cross-validation classes and functions from scikit-learn. # Notes - Ensure that your function is efficient and follows best practices. - The synthetic grouping for GroupKFold should be created using the following logic: `groups = (X[:, 2] <= 3).astype(int)`. - You can refer to the scikit-learn documentation for details on implementing the cross-validation strategies.","solution":"import numpy as np from sklearn import datasets from sklearn.model_selection import KFold, StratifiedKFold, LeaveOneOut, ShuffleSplit, GroupKFold, cross_val_score from sklearn.svm import SVC def evaluate_cross_validation(X: np.ndarray, y: np.ndarray) -> dict: # Initialize the SVM classifier with a linear kernel svm = SVC(kernel=\'linear\', C=1, random_state=42) # Define the cross-validation strategies cv_strategies = { \'K-Fold\': KFold(n_splits=5, random_state=42, shuffle=True), \'Stratified K-Fold\': StratifiedKFold(n_splits=5, shuffle=True, random_state=42), \'Leave-One-Out\': LeaveOneOut(), \'ShuffleSplit\': ShuffleSplit(n_splits=5, test_size=0.3, random_state=42), \'GroupKFold\': GroupKFold(n_splits=2) } # Synthetic groups for GroupKFold based on petal length groups = (X[:, 2] <= 3).astype(int) results = {} for strategy_name, cv in cv_strategies.items(): if strategy_name == \'GroupKFold\': scores = cross_val_score(svm, X, y, cv=cv, groups=groups) else: scores = cross_val_score(svm, X, y, cv=cv) results[strategy_name] = { \'mean_accuracy\': np.mean(scores), \'std_dev\': np.std(scores) } return results # Load the Iris dataset X, y = datasets.load_iris(return_X_y=True) results = evaluate_cross_validation(X, y) for strategy, metrics in results.items(): print(f\\"{strategy}: {metrics[\'mean_accuracy\']:.2f} accuracy with a standard deviation of {metrics[\'std_dev\']:.2f}\\")"},{"question":"# Pandas Data Analysis and Transformation You are given a dataset representing a monthly sales report in CSV format. The dataset includes the following columns: `OrderID`, `Product`, `Quantity`, `Price`, `Date`, and `Region`. Here\'s a snapshot of the data: | OrderID | Product | Quantity | Price | Date | Region | |---------|----------|----------|--------|------------|----------| | 1 | ProductA | 2 | 20.0 | 2023-01-15 | North | | 2 | ProductB | 10 | 5.5 | 2023-01-20 | East | | 3 | ProductA | 1 | 22.0 | 2023-02-05 | South | | ... | ... | ... | ... | ... | ... | Please complete the following tasks using pandas: 1. **Load and Inspect Data:** - Load the sales data from a CSV file named `sales_data.csv` into a pandas `DataFrame`. - Display the first 5 rows of the DataFrame. - Print the data types of each column. 2. **Data Cleaning:** - Convert the `Date` column to datetime format. - Handle any missing values in the `Quantity` and `Price` columns by replacing them with the respective column mean. 3. **Data Transformation and Aggregation:** - Create a new column `Total` that is computed by multiplying `Quantity` by `Price`. - Group the data by `Product` and `Region`, and calculate the following aggregated metrics: - Total Quantity sold - Total Revenue (sum of `Total` column) - Average price per product 4. **Data Filtering and Sorting:** - Filter the data to show only the records where the `Region` is `\'North\'` and the `Date` is between `2023-01-01` and `2023-01-31`. - Sort the filtered data by `Date` in ascending order and then by `Total` in descending order. 5. **Summary Statistics:** - Provide a summary statistics report for the entire dataset, including count, mean, std, min, 25%, 50%, 75%, and max for the `Quantity`, `Price`, and `Total` columns. - Identify the `Product` that has the highest total revenue and the `Region` where it was sold the most. # Expected Input - Path to the CSV file containing the sales data: `sales_data.csv` # Expected Output - The first 5 rows of the DataFrame. - Data types of each column. - DataFrame with `Date` column converted to datetime and missing values handled. - DataFrame with a new `Total` column. - Aggregated DataFrame grouped by `Product` and `Region`. - Filtered and sorted DataFrame for `\'North\'` region and `\'January 2023\'`. - Summary statistics report. - Product with highest total revenue and the region where it was sold the most. # Example Output ```python # Output examples (partial, to illustrate expected results) # Task 1 Output: # print(df.head()) OrderID Product Quantity Price Date Region 0 1 ProductA 2 20.0 2023-01-15 North 1 2 ProductB 10 5.5 2023-01-20 East 2 3 ProductA 1 22.0 2023-02-05 South # print(df.dtypes) OrderID int64 Product object Quantity float64 Price float64 Date datetime64[ns] Region object # (Other outputs will follow similarly) ``` # Constraints - Use pandas functionalities appropriately. - Ensure the code is written efficiently and handles large datasets gracefully.","solution":"import pandas as pd def load_and_inspect_data(file_path): # Load the sales data from CSV df = pd.read_csv(file_path) # Display the first 5 rows first_5_rows = df.head() # Print the data types of each column data_types = df.dtypes return df, first_5_rows, data_types def clean_data(df): # Convert the `Date` column to datetime format df[\'Date\'] = pd.to_datetime(df[\'Date\']) # Handle missing values in the `Quantity` and `Price` columns by replacing them with the mean df[\'Quantity\'].fillna(df[\'Quantity\'].mean(), inplace=True) df[\'Price\'].fillna(df[\'Price\'].mean(), inplace=True) return df def transform_and_aggregate_data(df): # Create a new column `Total` that is computed by multiplying `Quantity` by `Price` df[\'Total\'] = df[\'Quantity\'] * df[\'Price\'] # Group the data by `Product` and `Region`, and calculate aggregated metrics aggregated_data = df.groupby([\'Product\', \'Region\']).agg( Total_Quantity_Sold=(\'Quantity\', \'sum\'), Total_Revenue=(\'Total\', \'sum\'), Average_Price=(\'Price\', \'mean\') ).reset_index() return df, aggregated_data def filter_and_sort_data(df): # Filter the data to show only records where the `Region` is `\'North\'` and `Date` is in January 2023 filtered_data = df[(df[\'Region\'] == \'North\') & (df[\'Date\'].between(\'2023-01-01\', \'2023-01-31\'))] # Sort the data by `Date` in ascending order and then by `Total` in descending order sorted_data = filtered_data.sort_values(by=[\'Date\', \'Total\'], ascending=[True, False]) return sorted_data def summary_statistics_and_best_sales(df): # Provide a summary statistics report for the entire dataset summary_stats = df[[\'Quantity\', \'Price\', \'Total\']].describe() # Identify the `Product` with the highest total revenue and the `Region` where it was sold the most total_revenue_by_product = df.groupby(\'Product\')[\'Total\'].sum() product_with_highest_revenue = total_revenue_by_product.idxmax() region_sales = df[df[\'Product\'] == product_with_highest_revenue].groupby(\'Region\')[\'Total\'].sum() region_with_most_sales_of_top_product = region_sales.idxmax() return summary_stats, product_with_highest_revenue, region_with_most_sales_of_top_product"},{"question":"# Question: Advanced Data Persistence with Pickle Objective: Create a Python program using the \\"pickle\\" module to serialize and deserialize complex Python objects, demonstrating comprehension of custom reduction methods and handling stateful objects. Instructions: 1. Implement a class `Person` with the following attributes: - `name` (string): The name of the person. - `age` (integer): The age of the person. - `friends` (list): A list of `Person` instances representing the person\'s friends. 2. Implement custom reduction methods to ensure the `Person` class can be serialized and deserialized correctly using the \\"pickle\\" module. This includes handling circular references in the `friends` attribute. 3. Implement the following functions: - `serialize_person(person: Person, file_path: str) -> None`: Serialize the given `Person` instance to the specified file path. - `deserialize_person(file_path: str) -> Person`: Deserialize a `Person` instance from the specified file path. 4. You must handle the serialization and deserialization of the `Person` objects such that the circular references in the `friends` attribute are correctly restored. Constraints: - Assume all inputs are provided correctly and are valid instances of the `Person` class. - Performance should handle up to 1000 `Person` instances with varying degrees of connectedness in the `friends` attribute. Example: ```python # Example usage of the `Person` class and serialization functions # Create Person instances alice = Person(name=\\"Alice\\", age=30, friends=[]) bob = Person(name=\\"Bob\\", age=25, friends=[]) # Establish friendships alice.friends.append(bob) bob.friends.append(alice) # Serialize Alice to a file serialize_person(alice, \'alice.pkl\') # Deserialize from the file restored_alice = deserialize_person(\'alice.pkl\') print(restored_alice.name) # Output: Alice print(restored_alice.friends[0].name) # Output: Bob print(restored_alice.friends[0].friends[0].name) # Output: Alice ``` Notes: - Students will be graded on correct implementation of serialization/deserialization, handling circular references, and code readability. - The use of `pickle` module features like `__reduce__` or similar custom methods is expected.","solution":"import pickle class Person: def __init__(self, name, age, friends=None): self.name = name self.age = age self.friends = friends if friends is not None else [] def __reduce__(self): return (self.__class__, (self.name, self.age, self.friends)) def serialize_person(person: Person, file_path: str) -> None: with open(file_path, \'wb\') as file: pickle.dump(person, file) def deserialize_person(file_path: str) -> Person: with open(file_path, \'rb\') as file: return pickle.load(file)"},{"question":"Objective Write a function that processes multiple input files to calculate and return specific statistics about the lines in these files. Additionally, implement an optional feature to modify the contents of these files in-place based on certain conditions. Function Signature ```python def process_files(file_list: list, inplace: bool = False, keyword: str = None) -> dict: pass ``` Inputs 1. `file_list` (list): A list containing file paths as strings. 2. `inplace` (bool): A flag indicating whether to modify the input files in-place if a specified keyword is present in a line (default is `False`). 3. `keyword` (str): The keyword to check for in each line. Only relevant if `inplace` is `True` (default is `None`). Outputs - A dictionary with the following structure: ```python { \\"total_lines\\": int, \\"total_files\\": int, \\"file_stats\\": { \\"filename1\\": {\\"lines\\": int, \\"first_line\\": str, \\"last_line\\": str}, \\"filename2\\": {\\"lines\\": int, \\"first_line\\": str, \\"last_line\\": str}, ... } } ``` Constraints - You must use the `fileinput` module to read from the files. - The function should handle exceptions appropriately, such as file I/O errors. - If `inplace` is `True` and `keyword` is provided, any line in the files containing the `keyword` should be modified by appending \\"-MODIFIED\\" to the line. Example ```python file_list = [\\"file1.txt\\", \\"file2.txt\\"] result = process_files(file_list) print(result) ``` Output: ```python { \\"total_lines\\": 100, \\"total_files\\": 2, \\"file_stats\\": { \\"file1.txt\\": {\\"lines\\": 50, \\"first_line\\": \\"First line of file1\\", \\"last_line\\": \\"Last line of file1\\"}, \\"file2.txt\\": {\\"lines\\": 50, \\"first_line\\": \\"First line of file2\\", \\"last_line\\": \\"Last line of file2\\"}, } } ``` If `inplace` is `True` and `keyword` is \\"ERROR\\": ```python process_files(file_list, inplace=True, keyword=\\"ERROR\\") # Lines containing \\"ERROR\\" in any file will be modified to include \\"-MODIFIED\\" at the end. ``` Requirements 1. Use the `fileinput.input()` function to handle file reading. 2. Use context management (`with` statement) to ensure proper file handling. 3. Implement logic to conditionally modify files based on the `inplace` and `keyword` parameters. Performance - Efficiently handle large files by processing line-by-line rather than loading entire files into memory.","solution":"import fileinput from typing import List, Dict def process_files(file_list: List[str], inplace: bool = False, keyword: str = None) -> Dict: result = { \\"total_lines\\": 0, \\"total_files\\": len(file_list), \\"file_stats\\": {} } for filename in file_list: try: lines = [] with fileinput.input(files=(filename,), inplace=inplace) as f: for line in f: if inplace and keyword and keyword in line: line = line.rstrip(\'n\') + \\"-MODIFIEDn\\" if fileinput.isfirstline(): first_line = line.rstrip(\'n\') lines.append(line.rstrip(\'n\')) if inplace: print(line, end=\'\') total_lines = len(lines) result[\\"total_lines\\"] += total_lines result[\\"file_stats\\"][filename] = { \\"lines\\": total_lines, \\"first_line\\": lines[0] if lines else \'\', \\"last_line\\": lines[-1] if lines else \'\' } except Exception as e: result[\\"file_stats\\"][filename] = str(e) return result"},{"question":"# Python Coding Assessment Question Objective: Evaluate your understanding and ability to use the `keyword` module effectively. Problem Statement: You are given a list of variable names used in a Python program. Some of these variable names might be Python keywords or soft keywords, which are not allowed to be used as variable names without causing syntax errors. Your task is to write a function `check_variable_names` that takes a list of strings and returns a dictionary containing three lists: 1. `keywords`: A list of all strings that are Python keywords. 2. `soft_keywords`: A list of all strings that are Python soft keywords. 3. `valid_names`: A list of all strings that are valid variable names (i.e., neither Python keywords nor soft keywords). Function Signature: ```python def check_variable_names(names: list[str]) -> dict[str, list[str]]: pass ``` Input: * `names`: A list of strings (1 <= len(names) <= 100) representing variable names to be checked Output: * A dictionary with the following keys: * `keywords`: A list of variable names that are Python keywords. * `soft_keywords`: A list of variable names that are Python soft keywords. * `valid_names`: A list of variable names that are neither Python keywords nor soft keywords. Constraints: * Each string in the `names` list will have a length between 1 and 50 characters. * The function should handle case-sensitive matching. Example: ```python >>> check_variable_names([\\"if\\", \\"else\\", \\"loop\\", \\"my_var\\", \\"match\\"]) { \\"keywords\\": [\\"if\\", \\"else\\"], \\"soft_keywords\\": [\\"match\\"], \\"valid_names\\": [\\"loop\\", \\"my_var\\"] } ``` Guidelines: * Utilize the `keyword` module to accurately determine keywords and soft keywords. * Performance is a key consideration; ensure that your solution is efficient with a complexity that reflects proper usage of the `keyword` module functionalities. Note: This question requires a solid understanding of Python keywords, string processing, and dictionary operations.","solution":"import keyword def check_variable_names(names): This function takes a list of strings representing variable names, and returns a dictionary with three keys: \'keywords\', \'soft_keywords\', and \'valid_names\'. # Get the list of Python keywords and soft keywords keywords = keyword.kwlist soft_keywords = {\\"match\\", \\"case\\"} # Initialize lists for each category keyword_list = [] soft_keyword_list = [] valid_name_list = [] # Check each name to classify it into the respective list for name in names: if name in keywords: keyword_list.append(name) elif name in soft_keywords: soft_keyword_list.append(name) else: valid_name_list.append(name) # Return the result as a dictionary return { \\"keywords\\": keyword_list, \\"soft_keywords\\": soft_keyword_list, \\"valid_names\\": valid_name_list }"},{"question":"**XML Parsing and Manipulation Challenge** # Objective: Implement an XML parser that processes and manipulates XML data incrementally using the `xml.sax.xmlreader.IncrementalParser`. Your implementation should read XML data in chunks, parse it, and map certain elements to a new structure based on specified criteria. # Requirements: 1. **Class Implementation**: - Create a class `CustomXMLIncrementalParser` that inherits from `xml.sax.xmlreader.IncrementalParser`. - Implement methods to handle XML parsing incrementally: - `feed(data)`: Process a chunk of XML data. - `close()`: Finalize the parsing process. - `reset()`: Reset the parser to be ready for new input data. 2. **Function Implementation**: - Define a function `transform_xml_data(xml_parser: CustomXMLIncrementalParser, data_chunks: list, target_element: str) -> dict` to perform the following: - Initialize the parser and feed it with chunks of XML data from the `data_chunks` list. - On encountering the start of `target_element`, begin collecting the element\'s attributes. - Organize collected elements and attributes into a dictionary where keys are element names and values are lists of their attributes. - Return the organized dictionary after parsing is complete. # Input: - `xml_parser`: An instance of `CustomXMLIncrementalParser`. - `data_chunks`: A list of strings, where each string is a chunk of XML data. - `target_element`: A string specifying the XML element to target. # Output: - A dictionary where keys are target element names and values are lists of dictionaries containing attributes of those elements. # Example: ```python data_chunks = [ \'<root>\', \'<item id=\\"1\\" name=\\"apple\\"/>\', \'<item id=\\"2\\" name=\\"banana\\"/>\', \'</root>\' ] target_element = \'item\' # Expected Output: # { # \'item\': [ # {\'id\': \'1\', \'name\': \'apple\'}, # {\'id\': \'2\', \'name\': \'banana\'} # ] # } ``` # Constraints: - Handle well-formed XML data only. - Ensure efficient memory usage and processing for large XML inputs. - Implement necessary error handling where appropriate. # Performance: - The solution should efficiently process large XML files in a time and space-optimal manner by utilizing incremental parsing. Implement the `CustomXMLIncrementalParser` class and the `transform_xml_data` function below: ```python import xml.sax.xmlreader class CustomXMLIncrementalParser(xml.sax.xmlreader.IncrementalParser): def __init__(self): self.elements = [] super().__init__() def startElement(self, name, attrs): self.elements.append({name: dict(attrs)}) def feed(self, data): # Your implementation here def close(self): # Your implementation here def reset(self): self.elements = [] super().reset() def transform_xml_data(xml_parser: CustomXMLIncrementalParser, data_chunks: list, target_element: str) -> dict: results = {} # Your implementation here return results # Test your implementation with the example provided. ```","solution":"import xml.sax from xml.sax.handler import ContentHandler class CustomXMLIncrementalParser(xml.sax.xmlreader.IncrementalParser, ContentHandler): def __init__(self): xml.sax.xmlreader.IncrementalParser.__init__(self) ContentHandler.__init__(self) self.parser = xml.sax.make_parser() self.parser.setContentHandler(self) self.current_data = None self.target_element_data = [] def feed(self, data): self.parser.feed(data) def close(self): self.parser.close() def reset(self): self.parser.reset() self.target_element_data = [] def startElement(self, name, attrs): self.current_data = name self.target_element_data.append((name, dict(attrs))) def endElement(self, name): self.current_data = None def transform_xml_data(xml_parser: CustomXMLIncrementalParser, data_chunks: list, target_element: str) -> dict: xml_parser.reset() for chunk in data_chunks: xml_parser.feed(chunk) xml_parser.close() result = {} for elem_name, attributes in xml_parser.target_element_data: if elem_name == target_element: if target_element not in result: result[target_element] = [] result[target_element].append(attributes) return result"},{"question":"In Python, direct manipulations with memory buffers can be crucial for performance in certain applications such as data processing, image manipulation, or interaction with binary data files. While the old buffer protocol (described below) is deprecated, it is essential to understand how to handle memory buffers efficiently using modern Python approaches. **Task** Implement two functions: 1. `get_read_buffer(obj)`: This function should use the new buffer protocol to return a read-only memory view of the given object `obj`. The function should return a tuple `(buffer, buffer_len)` where `buffer` is the memory view, and `buffer_len` is the length of the buffer. 2. `get_write_buffer(obj)`: This function should use the new buffer protocol to return a writable memory view of the given object `obj`. The function should return a tuple `(buffer, buffer_len)` where `buffer` is the memory view, and `buffer_len` is the length of the buffer. **Input** - `obj`: An object that supports the buffer protocol (e.g., `bytearray`, `memoryview`, `array.array`). **Output** - For `get_read_buffer(obj)`: A tuple `(buffer, buffer_len)` where `buffer` is a read-only memory view, and `buffer_len` is the length of the buffer. - For `get_write_buffer(obj)`: A tuple `(buffer, buffer_len)` where `buffer` is a writable memory view, and `buffer_len` is the length of the buffer. **Constraints** - Ensure proper error handling. Raise an appropriate exception if the object does not support the required buffer interface. - Use the `memoryview` object in Python to manage buffers. **Example Usage** ```python data = bytearray(b\\"example data\\") # Read buffer read_buf, read_len = get_read_buffer(data) print(read_len) # Output should be 12 print(read_buf.tobytes()) # Output should be b\\"example data\\" # Write buffer write_buf, write_len = get_write_buffer(data) print(write_len) # Output should be 12 write_buf[:7] = b\\"SAMPLE\\" # Modifying buffer contents print(data) # Output should be bytearray(b\\"SAMPLE data\\") ``` **Note** - Do not use the deprecated functions `PyObject_AsCharBuffer`, `PyObject_AsReadBuffer`, `PyObject_AsWriteBuffer`, or `PyObject_CheckReadBuffer` in your implementation. - Focus on the new buffer protocol using the `memoryview` class and appropriate error checking.","solution":"def get_read_buffer(obj): Returns a read-only memory view of the given object. Parameters: obj: An object that supports the buffer protocol. Returns: tuple: (buffer, buffer_len) where buffer is a read-only memory view, and buffer_len is the length of the buffer. Raises: TypeError: If the object does not support the buffer interface. try: buffer = memoryview(obj).cast(\'B\') # Cast to bytes for a read-only view return buffer, len(buffer) except TypeError: raise TypeError(\\"The object does not support the buffer protocol.\\") def get_write_buffer(obj): Returns a writable memory view of the given object. Parameters: obj: An object that supports the buffer protocol. Returns: tuple: (buffer, buffer_len) where buffer is a writable memory view, and buffer_len is the length of the buffer. Raises: TypeError: If the object does not support the buffer interface or the buffer is not writable. try: buffer = memoryview(obj).cast(\'B\') if not buffer.readonly: return buffer, len(buffer) else: raise TypeError(\\"The object does not support writable memory view.\\") except TypeError: raise TypeError(\\"The object does not support the buffer protocol or writable memory view.\\")"},{"question":"Coding Assessment Question # Objective Demonstrate your comprehension of the `types` module by dynamically creating and working with different types. # Task 1. **Dynamic Class Creation**: - Use the `new_class` function to dynamically create a class named `DynamicPerson` with the following attributes and methods: - Attributes: `name` (string), `age` (integer). - Method: `greet` which returns a string \\"Hello, my name is {name} and I am {age} years old\\". 2. **Instance Creation and Attribute Manipulation**: - Create an instance of `DynamicPerson` with the name \\"John Doe\\" and age 30. - Modify the age attribute to 31. - Call the `greet` method and store its return value. 3. **Standard Type Check**: - Check if the `greet` method of the instance is of `FunctionType`. # Constraints - The `name` must be a non-empty string. - The `age` must be a positive integer. # Input - No direct input from the user. # Output - A string representing the greeting message from the `greet` method. - A boolean indicating whether the `greet` method is of `FunctionType`. # Example ```python # Expect the function implementation to return the following greeting_msg, is_function_type = create_and_check_dynamic_person() print(greeting_msg) # Output: \\"Hello, my name is John Doe and I am 31 years old\\" print(is_function_type) # Output: True ``` # Function Signature ```python def create_and_check_dynamic_person() -> (str, bool): # Your code here pass ```","solution":"import types def create_and_check_dynamic_person(): # Define attributes and methods for DynamicPerson def init(self, name, age): if not isinstance(name, str) or not name: raise ValueError(\\"Name must be a non-empty string\\") if not isinstance(age, int) or age <= 0: raise ValueError(\\"Age must be a positive integer\\") self.name = name self.age = age def greet(self): return f\\"Hello, my name is {self.name} and I am {self.age} years old\\" # Create DynamicPerson class DynamicPerson = types.new_class( \'DynamicPerson\', () ) DynamicPerson.__init__ = init DynamicPerson.greet = greet # Create instance person = DynamicPerson(\\"John Doe\\", 30) # Modify attribute person.age = 31 # Call greet method greeting_msg = person.greet() # Check if the greet method is of FunctionType is_function_type = isinstance(person.greet, types.MethodType) return greeting_msg, is_function_type"},{"question":"# Question: Using `runpy` To Execute Modules and Analyze Global Variables Objective: Write a Python function that uses the `runpy` module to execute a Python module and a script file, and then analyze and compare the resulting global variables. Requirements: 1. Implement a function `compare_globals_from_runpy(module_name: str, script_path: str) -> dict` that: - Uses `runpy.run_module` to execute the provided `module_name`. - Uses `runpy.run_path` to execute the provided `script_path`. - The function should return a dictionary with keys `\\"module_globals\\"` and `\\"script_globals\\"`, containing the globals dictionaries returned by running the module and script respectively. 2. The function should correctly handle potential exceptions and provide informative error messages for the following cases: - Invalid `module_name` or `script_path`. - Execution of the module or script results in an error. 3. You are provided with the following constraints: - The `module_name` is guaranteed to be an importable module present in your Python environment. - The `script_path` is a valid file path to a Python script (.py file). Expected Input and Output: **Input:** - `module_name`: A string representing the name of the module to be executed. - `script_path`: A string representing the file path of the script to be executed. **Output:** - A dictionary with the keys `\\"module_globals\\"` and `\\"script_globals\\"` containing the global variables dictionaries of the respective executions. **Example:** ```python def compare_globals_from_runpy(module_name: str, script_path: str) -> dict: # Your implementation here result = compare_globals_from_runpy(\\"sample_module\\", \\"/path/to/sample_script.py\\") print(result) ``` The output should look similar to this (the actual content will vary based on the executed module and script): ```python { \'module_globals\': { \'__name__\': \'sample_module\', \'__doc__\': None, \'global_var\': 42, ... }, \'script_globals\': { \'__name__\': \'<run_path>\', \'__file__\': \'/path/to/sample_script.py\', \'script_var\': \'Hello World\', ... } } ``` Additional Information: - Use appropriate error handling to ensure that the function fails gracefully and provides meaningful error messages. - Ensure the correct setup of global variables and the restoration of any altered `sys` states after execution.","solution":"import runpy def compare_globals_from_runpy(module_name: str, script_path: str) -> dict: try: module_globals = runpy.run_module(module_name) except Exception as e: raise RuntimeError(f\\"Failed to execute module {module_name}: {e}\\") try: script_globals = runpy.run_path(script_path) except Exception as e: raise RuntimeError(f\\"Failed to execute script {script_path}: {e}\\") return { \\"module_globals\\": module_globals, \\"script_globals\\": script_globals }"},{"question":"# Asynchronous Programming with Coroutine Objects Python 3.5 introduced coroutine objects as a cornerstone of asynchronous programming. This exercise will guide you through understanding and implementing coroutines in Python by creating a function that utilizes these concepts. Problem Statement Implement an **asynchronous function** called `async_fetch_data` which mimics fetching data from a remote server. Your task is to: 1. Define an async function `async_fetch_data(url: str) -> Coroutine`, which takes a URL as an argument and returns a Coroutine object that performs the following actions: - Simulate a delay of 1 second to mimic network latency using an `await asyncio.sleep(1)`. - Print a message indicating the start and completion of data retrieval, including the URL. 2. Create another function `fetch_multiple_data(urls: List[str]) -> None` which takes a list of URLs, fetches data asynchronously for each URL, and prints a message when all the data has been fetched. Input and Output Formats 1. The function `async_fetch_data` will take a single string argument `url`. There is no output expected from this function, but it should print messages indicating the asynchronous operation. Example: ```python await async_fetch_data(\\"https://example.com\\") ``` Output: ``` Start fetching data from https://example.com Fetching completed from https://example.com ``` 2. The function `fetch_multiple_data` will take a list of strings `urls`. It will asynchronously fetch data from all provided URLs using the `async_fetch_data` function and print a final message indicating that all data has been fetched. Example: ```python await fetch_multiple_data([\\"https://example.com\\", \\"https://anotherexample.com\\"]) ``` Output: ``` Start fetching data from https://example.com Start fetching data from https://anotherexample.com Fetching completed from https://example.com Fetching completed from https://anotherexample.com All data has been fetched. ``` Constraints and Limitations - Use the `asyncio` library for asynchronous behavior. - Avoid actual network calls; use `asyncio.sleep` to simulate network delay. - Ensure that the function `fetch_multiple_data` runs all URL fetch operations concurrently. Performance Requirements - The solution should leverage coroutine objects and ensure asynchronous execution. - Demonstrate clear understanding of asynchronous programming concepts in Python. ```python import asyncio from typing import List, Coroutine # Write your solution here async def async_fetch_data(url: str) -> Coroutine: Asynchronously fetch data from the given URL. Simulate network delay using asyncio.sleep. print(f\\"Start fetching data from {url}\\") await asyncio.sleep(1) print(f\\"Fetching completed from {url}\\") async def fetch_multiple_data(urls: List[str]) -> None: Asynchronously fetch data from multiple URLs. This function should run the fetch operations concurrently and print a final message when all fetch operations are completed. await asyncio.gather(*(async_fetch_data(url) for url in urls)) print(\\"All data has been fetched.\\") ``` Be sure to test your implementation to confirm it works as expected.","solution":"import asyncio from typing import List, Coroutine async def async_fetch_data(url: str) -> Coroutine: Asynchronously fetch data from the given URL. Simulate network delay using asyncio.sleep. print(f\\"Start fetching data from {url}\\") await asyncio.sleep(1) print(f\\"Fetching completed from {url}\\") async def fetch_multiple_data(urls: List[str]) -> None: Asynchronously fetch data from multiple URLs. This function should run the fetch operations concurrently and print a final message when all fetch operations are completed. await asyncio.gather(*(async_fetch_data(url) for url in urls)) print(\\"All data has been fetched.\\")"},{"question":"# Question: Advanced Boxplot Data Visualization with Seaborn Using the seaborn library, create a complex data visualization that provides insights into the Titanic dataset. Follow the instructions below to complete the task. **Problem Description:** You are provided with the Titanic dataset. Your task is to generate a series of visualizations to analyze the relationship between passenger age, class, survival status, and fare using seaborn boxplots. **Requirements:** 1. **Load the Titanic dataset** using `sns.load_dataset(\\"titanic\\")`. 2. **Create a horizontal boxplot** to show the distribution of passenger ages across the dataset. Label the plot appropriately. 3. **Group and create a vertical boxplot** showing the distribution of passenger ages for each passenger class. Label the plot appropriately. 4. **Create a nested boxplot** to show the distribution of passenger ages for each passenger class, grouped further by survival status. Label the plot appropriately. 5. **Create a boxplot** to show the relationship between the passenger\'s age and the fare they paid, grouped by fare bins (e.g., [0-50], [51-100], etc.). Ensure the plot is well-labeled and uses appropriate colors to distinguish bins. **Expected Input and Output Formats:** *Input:* The function does not take any parameters as it directly uses the seaborn\'s dataset. *Output:* The function should display the plots as specified without returning any values. **Constraints:** - Use seaborn for all visualizations. - Ensure all plots are clearly labeled and use distinct colors to differentiate groups. - The age bins should be created programmatically based on the fare data. **Performance Requirements:** - The visualizations should be computationally efficient and should render correctly without crashing for the given dataset. # Implementation ```python import seaborn as sns import matplotlib.pyplot as plt def create_titanic_boxplots(): # Load Titanic dataset titanic = sns.load_dataset(\\"titanic\\") # Create the subplots fig, axs = plt.subplots(4, 1, figsize=(12, 24)) # Horizontal boxplot for age distribution sns.boxplot(ax=axs[0], x=titanic[\\"age\\"]) axs[0].set_title(\'Age Distribution of Passengers\') axs[0].set_xlabel(\'Age\') # Vertical boxplot for age distribution by class sns.boxplot(ax=axs[1], data=titanic, x=\\"class\\", y=\\"age\\") axs[1].set_title(\'Age Distribution by Class\') axs[1].set_xlabel(\'Class\') axs[1].set_ylabel(\'Age\') # Nested boxplot for age distribution by class and survival status sns.boxplot(ax=axs[2], data=titanic, x=\\"class\\", y=\\"age\\", hue=\\"alive\\") axs[2].set_title(\'Age Distribution by Class and Survival Status\') axs[2].set_xlabel(\'Class\') axs[2].set_ylabel(\'Age\') # Group fare into bins and plot age vs fare fare_bins = pd.cut(titanic[\'fare\'], bins=[0, 50, 100, 150, 200, 250, 300, 600]) sns.boxplot(ax=axs[3], x=fare_bins, y=titanic[\\"age\\"]) axs[3].set_title(\'Age Distribution by Fare Bins\') axs[3].set_xlabel(\'Fare Bins\') axs[3].set_ylabel(\'Age\') # Adjust layout plt.tight_layout() plt.show() # Call the function create_titanic_boxplots() ``` **Guidance:** - Use matplotlib\'s `subplots` function to create a figure with multiple axes for different boxplots. - Modify the appearance such as titles, axis labels, and colors appropriately. - Ensure that the fare bins are calculated correctly and used in the fourth boxplot.","solution":"import seaborn as sns import matplotlib.pyplot as plt import pandas as pd def create_titanic_boxplots(): Generates a series of boxplots to analyze the relationship between passenger age, class, survival status, and fare on the Titanic. # Load Titanic dataset titanic = sns.load_dataset(\\"titanic\\") # Create the subplots fig, axs = plt.subplots(4, 1, figsize=(12, 24)) # Horizontal boxplot for age distribution sns.boxplot(ax=axs[0], x=titanic[\\"age\\"]) axs[0].set_title(\'Age Distribution of Passengers\') axs[0].set_xlabel(\'Age\') # Vertical boxplot for age distribution by class sns.boxplot(ax=axs[1], data=titanic, x=\\"class\\", y=\\"age\\") axs[1].set_title(\'Age Distribution by Class\') axs[1].set_xlabel(\'Class\') axs[1].set_ylabel(\'Age\') # Nested boxplot for age distribution by class and survival status sns.boxplot(ax=axs[2], data=titanic, x=\\"class\\", y=\\"age\\", hue=\\"alive\\") axs[2].set_title(\'Age Distribution by Class and Survival Status\') axs[2].set_xlabel(\'Class\') axs[2].set_ylabel(\'Age\') # Group fare into bins and plot age vs fare fare_bins = pd.cut(titanic[\'fare\'], bins=[0, 50, 100, 150, 200, 300, 600]) sns.boxplot(ax=axs[3], x=fare_bins, y=titanic[\\"age\\"]) axs[3].set_title(\'Age Distribution by Fare Bins\') axs[3].set_xlabel(\'Fare Bins\') axs[3].set_ylabel(\'Age\') # Adjust layout plt.tight_layout() plt.show() # Call the function create_titanic_boxplots()"},{"question":"# Python Coding Assessment Question **Objective**: This assessment will test your understanding of Python\'s Development Mode and its debugging features. You will be required to write a program that handles file operations and asynchronous tasks, demonstrating your ability to manage resources properly, identify and fix potential issues using Python Development Mode. **Task**: 1. Write a Python program that does the following: - Opens and reads multiple text files provided via command line arguments and prints their content. Ensure to handle file operations correctly to prevent resource warnings. - Implements an asynchronous function that processes the file content (simulated with a dummy processing function) and outputs the processed result. 2. Your program should be able to: - Properly close all file handles to prevent resource warnings. - Correctly manage asyncio tasks to avoid \'coroutine not awaited\' warnings. 3. **Function Definitions**: - `async def process_file_content(content: str) -> str`: This function simulates processing on file content. For simplicity, you can simulate processing by simply returning the reversed content after a short delay. - `def read_and_process_files(file_paths: List[str]) -> None`: This function reads and processes the files. Use `asyncio` to call `process_file_content` for the content. 4. **Input**: - Command line arguments specifying the file paths. 5. **Output**: - Print the content of each file. - Print the processed result for each file using the asynchronous function. **Constraints**: - Assume that the files exist and are readable. - Ensure your program runs without resource warnings when using Python Development Mode. - Handle potential encoding/decoding issues in the file content. # Example Suppose we have two text files \\"file1.txt\\" and \\"file2.txt\\" with the following content: - `file1.txt`: \\"Hello, World!\\" - `file2.txt`: \\"Python is awesome!\\" When running your script with these files, the output should be: ```bash python3 -X dev yourscript.py file1.txt file2.txt Contents of file1.txt: Hello, World! Processed result: !dlroW ,olleH Contents of file2.txt: Python is awesome! Processed result: !emosewa si nohtyP ``` ```python import asyncio import sys from typing import List async def process_file_content(content: str) -> str: await asyncio.sleep(1) # Simulating a delay in processing return content[::-1] # Reverse content for demonstration purposes def read_and_process_files(file_paths: List[str]) -> None: async def _internal_task(): for file_path in file_paths: with open(file_path, \'r\', encoding=\'utf-8\') as file: content = file.read() print(f\\"Contents of {file_path}:\\") print(content) processed_content = await process_file_content(content) print(f\\"Processed result: {processed_content}\\") asyncio.run(_internal_task()) if __name__ == \\"__main__\\": file_paths = sys.argv[1:] if not file_paths: print(\\"Please provide file paths as command line arguments.\\") else: read_and_process_files(file_paths) ``` **Notes**: - Ensure to handle file operations using context managers to prevent resources from being left open. - Utilize asyncio properly to avoid coroutine warnings.","solution":"import asyncio import sys from typing import List async def process_file_content(content: str) -> str: await asyncio.sleep(1) # Simulating a delay in processing return content[::-1] # Reverse content for demonstration purposes def read_and_process_files(file_paths: List[str]) -> None: async def _internal_task(): for file_path in file_paths: with open(file_path, \'r\', encoding=\'utf-8\') as file: content = file.read() print(f\\"Contents of {file_path}:\\") print(content) processed_content = await process_file_content(content) print(f\\"Processed result: {processed_content}\\") asyncio.run(_internal_task()) if __name__ == \\"__main__\\": file_paths = sys.argv[1:] if not file_paths: print(\\"Please provide file paths as command line arguments.\\") else: read_and_process_files(file_paths)"},{"question":"Given a dataset in CSV format containing information about different species of flowers, you are required to create various visualizations using seaborn’s `swarmplot`. The dataset, `flowers.csv`, has the following columns: - `species`: the species of the flower (categorical) - `petal_length`: the length of the petals (numeric) - `sepal_length`: the length of the sepals (numeric) - `color_code`: a numeric code representing different color groups (numeric, ranging from 1 to 5) - `is_native`: a binary variable indicating if the flower is native to the region (1) or not (0) **Tasks:** 1. **Basic Swarmplot**: - Create a basic swarm plot to show the distribution of `petal_length` for each `species` along the y-axis. 2. **Swarmplot with Hue**: - Create a swarm plot to compare `petal_length` for different `species`, using `color_code` as the `hue`. 3. **Vertical Swarmplot**: - Create a vertical swarm plot to show the distribution of `sepal_length` for each `species` along the x-axis. 4. **Custom Palette**: - Create a swarm plot similar to Task 2 but use a qualitative palette named `\'pastel\'`. 5. **Dodge True Swarmplot**: - Create a swarm plot to compare `sepal_length` for different `species` and use `is_native` as the `hue`, with dodging enabled. 6. **Facet Grid**: - Produce a multi-facet swarm plot that shows the `sepal_length` against `petal_length` for different `species` across different `color_code` levels. Use seaborn’s `catplot` function for this task. 7. **Custom Markers and Point Size**: - Create a swarm plot similar to Task 1 but customize the markers to `\'*\'` and reduce the point size to 3. **Constraints:** - The solution should include necessary imports and ensure your plots have appropriate titles, axis labels, and legends for clarity. - Assume seaborn and matplotlib are already installed. Your function should take an input of the file path to the CSV dataset and produce the required plots. ```python import seaborn as sns import matplotlib.pyplot as plt import pandas as pd def generate_flower_visualizations(file_path): # Load the dataset flowers = pd.read_csv(file_path) # Task 1: Basic Swarmplot plt.figure(figsize=(10, 6)) sns.swarmplot(data=flowers, x=\\"species\\", y=\\"petal_length\\") plt.title(\'Swarm Plot of Petal Length by Species\') plt.xlabel(\'Species\') plt.ylabel(\'Petal Length\') plt.show() # Task 2: Swarmplot with Hue plt.figure(figsize=(10, 6)) sns.swarmplot(data=flowers, x=\\"species\\", y=\\"petal_length\\", hue=\\"color_code\\") plt.title(\'Swarm Plot of Petal Length by Species with Color Code Hue\') plt.xlabel(\'Species\') plt.ylabel(\'Petal Length\') plt.show() # Task 3: Vertical Swarmplot plt.figure(figsize=(10, 6)) sns.swarmplot(data=flowers, x=\\"sepal_length\\", y=\\"species\\") plt.title(\'Vertical Swarm Plot of Sepal Length by Species\') plt.xlabel(\'Sepal Length\') plt.ylabel(\'Species\') plt.show() # Task 4: Custom Palette plt.figure(figsize=(10, 6)) sns.swarmplot(data=flowers, x=\\"species\\", y=\\"petal_length\\", hue=\\"color_code\\", palette=\\"pastel\\") plt.title(\'Swarm Plot of Petal Length by Species with Custom Palette\') plt.xlabel(\'Species\') plt.ylabel(\'Petal Length\') plt.show() # Task 5: Dodge True Swarmplot plt.figure(figsize=(10, 6)) sns.swarmplot(data=flowers, x=\\"species\\", y=\\"sepal_length\\", hue=\\"is_native\\", dodge=True) plt.title(\'Swarm Plot of Sepal Length by Species with is_native Hue\') plt.xlabel(\'Species\') plt.ylabel(\'Sepal Length\') plt.show() # Task 6: Facet Grid sns.catplot(data=flowers, kind=\\"swarm\\", x=\\"species\\", y=\\"sepal_length\\", hue=\\"species\\", col=\\"color_code\\", aspect=0.5) plt.show() # Task 7: Custom Markers and Point Size plt.figure(figsize=(10, 6)) sns.swarmplot(data=flowers, x=\\"species\\", y=\\"petal_length\\", marker=\\"*\\", size=3) plt.title(\'Swarm Plot with Custom Markers and Point Size\') plt.xlabel(\'Species\') plt.ylabel(\'Petal Length\') plt.show() # Example usage: # generate_flower_visualizations(\'flowers.csv\') ```","solution":"import seaborn as sns import matplotlib.pyplot as plt import pandas as pd def generate_flower_visualizations(file_path): # Load the dataset flowers = pd.read_csv(file_path) # Task 1: Basic Swarmplot plt.figure(figsize=(10, 6)) sns.swarmplot(data=flowers, x=\\"species\\", y=\\"petal_length\\") plt.title(\'Swarm Plot of Petal Length by Species\') plt.xlabel(\'Species\') plt.ylabel(\'Petal Length\') plt.show() # Task 2: Swarmplot with Hue plt.figure(figsize=(10, 6)) sns.swarmplot(data=flowers, x=\\"species\\", y=\\"petal_length\\", hue=\\"color_code\\") plt.title(\'Swarm Plot of Petal Length by Species with Color Code Hue\') plt.xlabel(\'Species\') plt.ylabel(\'Petal Length\') plt.show() # Task 3: Vertical Swarmplot plt.figure(figsize=(10, 6)) sns.swarmplot(data=flowers, x=\\"sepal_length\\", y=\\"species\\") plt.title(\'Vertical Swarm Plot of Sepal Length by Species\') plt.xlabel(\'Sepal Length\') plt.ylabel(\'Species\') plt.show() # Task 4: Custom Palette plt.figure(figsize=(10, 6)) sns.swarmplot(data=flowers, x=\\"species\\", y=\\"petal_length\\", hue=\\"color_code\\", palette=\\"pastel\\") plt.title(\'Swarm Plot of Petal Length by Species with Custom Palette\') plt.xlabel(\'Species\') plt.ylabel(\'Petal Length\') plt.show() # Task 5: Dodge True Swarmplot plt.figure(figsize=(10, 6)) sns.swarmplot(data=flowers, x=\\"species\\", y=\\"sepal_length\\", hue=\\"is_native\\", dodge=True) plt.title(\'Swarm Plot of Sepal Length by Species with is_native Hue\') plt.xlabel(\'Species\') plt.ylabel(\'Sepal Length\') plt.show() # Task 6: Facet Grid sns.catplot(data=flowers, kind=\\"swarm\\", x=\\"species\\", y=\\"sepal_length\\", hue=\\"species\\", col=\\"color_code\\", aspect=0.5) plt.show() # Task 7: Custom Markers and Point Size plt.figure(figsize=(10, 6)) sns.swarmplot(data=flowers, x=\\"species\\", y=\\"petal_length\\", marker=\\"*\\", size=3) plt.title(\'Swarm Plot with Custom Markers and Point Size\') plt.xlabel(\'Species\') plt.ylabel(\'Petal Length\') plt.show()"},{"question":"Coding Assessment Question # Objective In this assessment, you are required to implement a simplified multi-client echo server. The server should be capable of handling multiple client connections concurrently, using the `selectors` module for I/O multiplexing. # Description Your task is to design and implement a class `EchoServer` that utilizes `selectors.DefaultSelector` to handle multiple simultaneous connections from clients. The server should read data sent by clients and echo it back to them. # Requirements 1. The server should bind to a specified hostname and port. 2. Use the `selectors` module to manage multiple client connections. 3. Handle the following events: - **Read Events (`EVENT_READ`)**: When data is available for reading. - **Write Events (`EVENT_WRITE`)**: When the server is ready to send data. 4. Implement the following methods in the `EchoServer` class: - **`__init__(self, host, port)`**: Initialize the server with the given hostname and port. - **`start(self)`**: Start the server and begin waiting for client connections. - **`accept(self, sock, mask)`**: Accept a new client connection. - **`read(self, conn, mask)`**: Read data from a client and echo it back. - **`stop(self)`**: Stop the server gracefully, closing all connections and the selector. # Constraints - The server should handle abrupt client disconnections gracefully. - Use non-blocking sockets. - The server should run indefinitely until stopped manually (using the `stop` method). # Example Usage ```python # Example of creating and running the server server = EchoServer(\'localhost\', 1234) server.start() ``` # Implementation ```python import selectors import socket class EchoServer: def __init__(self, host, port): self.sel = selectors.DefaultSelector() self.host = host self.port = port self.sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM) self.sock.bind((host, port)) self.sock.listen(100) self.sock.setblocking(False) self.sel.register(self.sock, selectors.EVENT_READ, self.accept) def start(self): try: while True: events = self.sel.select() for key, mask in events: callback = key.data callback(key.fileobj, mask) except KeyboardInterrupt: print(\\"Caught keyboard interrupt, exiting\\") finally: self.stop() def accept(self, sock, mask): conn, addr = sock.accept() print(\'accepted\', conn, \'from\', addr) conn.setblocking(False) self.sel.register(conn, selectors.EVENT_READ, self.read) def read(self, conn, mask): data = conn.recv(1000) if data: print(\'echoing\', repr(data), \'to\', conn) conn.send(data) else: print(\'closing\', conn) self.sel.unregister(conn) conn.close() def stop(self): self.sel.close() ``` **Note**: This example code provides a starting point for your implementation. Ensure to handle any additional cases or edge conditions as needed. # Testing You should test your implementation to ensure it can handle multiple clients concurrently. You may use tools like `telnet` to connect to your server and verify that it echoes back data correctly.","solution":"import selectors import socket class EchoServer: def __init__(self, host, port): self.sel = selectors.DefaultSelector() self.host = host self.port = port self.sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM) self.sock.bind((host, port)) self.sock.listen(100) self.sock.setblocking(False) self.sel.register(self.sock, selectors.EVENT_READ, self.accept) def start(self): try: while True: events = self.sel.select() for key, mask in events: callback = key.data callback(key.fileobj, mask) except KeyboardInterrupt: print(\\"Caught keyboard interrupt, exiting\\") finally: self.stop() def accept(self, sock, mask): conn, addr = sock.accept() print(f\'accepted connection from {addr}\') conn.setblocking(False) self.sel.register(conn, selectors.EVENT_READ, self.read) def read(self, conn, mask): data = conn.recv(1000) if data: print(f\'echoing {data} to {conn}\') conn.send(data) else: print(f\'closing connection to {conn}\') self.sel.unregister(conn) conn.close() def stop(self): print(\\"Stopping server...\\") self.sel.close() self.sock.close()"},{"question":"**Objective**: To assess the student\'s ability to configure logging in Python using the `logging.config` module, including creating custom handlers, formatters, and filters. --- You are tasked with setting up a sophisticated logging system for a Python application. The logging system should have the following features: 1. **Multiple Loggers**: Create two loggers, `appLogger` and `dbLogger`. The `appLogger` should handle general application logs, while `dbLogger` should handle database-specific logs. 2. **Custom Handlers**: Configure two handlers: - A `StreamHandler` to output logs to the console. - A `RotatingFileHandler` to write logs to a file named `app.log`, with a maximum file size of 1MB and a backup count of 3. 3. **Formatters**: Create two formatters: - A `simpleFormatter` that outputs logs in the format `%(asctime)s - %(levelname)s - %(message)s`. - A `detailedFormatter` that outputs logs in the format `%(asctime)s - %(name)s - %(levelname)s - %(message)s`. 4. **Filters**: Configure a filter that only allows logs of level WARNING and above. 5. **Configuration Dictionary**: Use a configuration dictionary to set up the loggers, handlers, formatters, and filters. Ensure the final configuration dictionary follows the schema described in the documentation. 6. **Custom Filter Implementation**: Implement and use a custom filter class, `WarningFilter`, that filters out logs below the WARNING level. Expected Input and Output - **Input**: None (The logging configuration should be tested within a single script.) - **Output**: Logs output to the console and written to `app.log` according to the configuration. Constraints and Requirements - The implementation must use the `logging.config.dictConfig` method. - The custom filter class must be implemented within the script. - Logs should show different messages for the `appLogger` and `dbLogger` to verify the configuration. Performance Requirements - Efficient use of handlers and formatters without redundancy. - Proper structuring and organization of the configuration dictionary for readability and maintainability. Example ```python import logging import logging.config # Your implementation here... # Example usage app_logger = logging.getLogger(\'appLogger\') db_logger = logging.getLogger(\'dbLogger\') app_logger.info(\'This is an informational message from appLogger.\') app_logger.warning(\'This is a warning message from appLogger.\') db_logger.error(\'This is an error message from dbLogger.\') ``` # Solution Requirements 1. Implement the `WarningFilter` class. 2. Create the configuration dictionary. 3. Apply the configuration using `logging.config.dictConfig`. 4. Demonstrate logging from both `appLogger` and `dbLogger`. --- **Note**: Ensure your solution is well-documented, and provide comments explaining key parts of the configuration and code.","solution":"import logging import logging.config class WarningFilter(logging.Filter): This is a custom filter that only allows logs of level WARNING and above. def filter(self, record): return record.levelno >= logging.WARNING # Configuration dictionary LOGGING_CONFIG = { \'version\': 1, \'disable_existing_loggers\': False, \'filters\': { \'warning_filter\': { \'()\': WarningFilter, }, }, \'formatters\': { \'simpleFormatter\': { \'format\': \'%(asctime)s - %(levelname)s - %(message)s\', }, \'detailedFormatter\': { \'format\': \'%(asctime)s - %(name)s - %(levelname)s - %(message)s\', }, }, \'handlers\': { \'console\': { \'class\': \'logging.StreamHandler\', \'formatter\': \'simpleFormatter\', }, \'file\': { \'class\': \'logging.handlers.RotatingFileHandler\', \'filename\': \'app.log\', \'maxBytes\': 1 * 1024 * 1024, # 1 MB \'backupCount\': 3, \'formatter\': \'detailedFormatter\', \'filters\': [\'warning_filter\'], }, }, \'loggers\': { \'appLogger\': { \'handlers\': [\'console\', \'file\'], \'level\': \'DEBUG\', \'propagate\': True, }, \'dbLogger\': { \'handlers\': [\'console\', \'file\'], \'level\': \'DEBUG\', \'propagate\': True, }, } } # Applying the configuration logging.config.dictConfig(LOGGING_CONFIG) # Fetching loggers app_logger = logging.getLogger(\'appLogger\') db_logger = logging.getLogger(\'dbLogger\') # Example usage app_logger.info(\'This is an informational message from appLogger.\') app_logger.warning(\'This is a warning message from appLogger.\') db_logger.error(\'This is an error message from dbLogger.\')"},{"question":"**Objective:** Demonstrate your understanding of seaborn by loading a dataset, creating a customized cluster map, and modifying its aesthetics and parameters. **Problem Statement:** Write a Python function using seaborn to create a cluster map based on the Iris dataset. Your function should: 1. Load the Iris dataset using seaborn. 2. Separate the species column from the dataset for future use. 3. Create a cluster map with the following characteristics: - Size of the figure should be 10 x 8. - Use a dendrogram ratio of (0.1, 0.2). - Position the color bar at (0.85, 0.35, 0.03, 0.4). 4. Add colored labels to identify different species: Setosa, Versicolor, and Virginica with different colors. 5. Using the \\"coolwarm\\" colormap and setting the color value limits to range from -2 to 2. 6. Use the \\"single\\" method for clustering with the \\"euclidean\\" metric. 7. Normalize the data within columns. **Function Signature:** ```python def create_custom_clustermap(): pass ``` **Note:** - This function does not need to return any value; it should display the clustermap when executed. - Ensure you have the seaborn library installed in your environment. **Expected Example Output:** When the function `create_custom_clustermap()` is executed, it should display a clustermap with the specified customizations.","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt from sklearn.preprocessing import StandardScaler def create_custom_clustermap(): # Load Iris dataset iris = sns.load_dataset(\'iris\') # Separate the species column species = iris.pop(\'species\') # Normalize the data within columns scaler = StandardScaler() iris_scaled = scaler.fit_transform(iris) iris_scaled_df = pd.DataFrame(iris_scaled, columns=iris.columns) # Create color labels for species species_colors = species.map({\'setosa\': \'red\', \'versicolor\': \'blue\', \'virginica\': \'green\'}) # Create the clustermap sns.clustermap( iris_scaled_df, figsize=(10, 8), dendrogram_ratio=(0.1, 0.2), cbar_pos=(0.85, 0.35, 0.03, 0.4), row_colors=species_colors, cmap=\'coolwarm\', vmin=-2, vmax=2, method=\'single\', metric=\'euclidean\' ) plt.show()"},{"question":"# Tensor Attributes Manipulation in PyTorch Problem Description You are tasked with implementing a function in PyTorch that takes three tensors as inputs and performs the following operations: 1. **Type Promotion**: Add the three tensors together and return the resulting tensor. Demonstrate an understanding of type promotion rules. 2. **Device Management**: Ensure the resulting tensor is moved to the same device as the first input tensor. 3. **Memory Layout**: Return the strides of the resulting tensor. Function Signature ```python import torch from typing import Tuple def manipulate_tensors(tensor1: torch.Tensor, tensor2: torch.Tensor, tensor3: torch.Tensor) -> Tuple[torch.Tensor, torch.device, Tuple[int, ...]]: Parameters: - tensor1 (torch.Tensor): The first input tensor. - tensor2 (torch.Tensor): The second input tensor. - tensor3 (torch.Tensor): The third input tensor. Returns: - result_tensor (torch.Tensor): The tensor resulting from adding tensor1, tensor2, and tensor3. - device (torch.device): The device of the resulting tensor (should be the same as tensor1\'s device). - strides (Tuple[int, ...]): The strides of the resulting tensor. pass ``` Constraints - The input tensors may have different dtypes, but they will have compatible shapes for addition. - All input tensors will have the same device type. - You should ensure the resulting tensor is of the same dtype as per PyTorch\'s type promotion rules. - You should ensure the resulting tensor is on the same device as `tensor1`. Example ```python # Example usage tensor1 = torch.tensor([1.0, 2.0], dtype=torch.float32, device=\'cpu\') tensor2 = torch.tensor([1, 2], dtype=torch.int32, device=\'cpu\') tensor3 = torch.tensor([1.0, 2.0], dtype=torch.float64, device=\'cpu\') result_tensor, result_device, result_strides = manipulate_tensors(tensor1, tensor2, tensor3) print(result_tensor) # Expected: tensor([3.0, 6.0], dtype=torch.float64) print(result_device) # Expected: device(type=\'cpu\') print(result_strides) # Expected: (1,) ``` Note Use the provided documentation to ensure you follow the correct methods and practices for type promotion, device management, and memory layout handling in PyTorch. Good luck!","solution":"import torch from typing import Tuple def manipulate_tensors(tensor1: torch.Tensor, tensor2: torch.Tensor, tensor3: torch.Tensor) -> Tuple[torch.Tensor, torch.device, Tuple[int, ...]]: Parameters: - tensor1 (torch.Tensor): The first input tensor. - tensor2 (torch.Tensor): The second input tensor. - tensor3 (torch.Tensor): The third input tensor. Returns: - result_tensor (torch.Tensor): The tensor resulting from adding tensor1, tensor2, and tensor3. - device (torch.device): The device of the resulting tensor (should be the same as tensor1\'s device). - strides (Tuple[int, ...]): The strides of the resulting tensor. # Type promotion: PyTorch will automatically handle type promotion during addition result_tensor = tensor1 + tensor2 + tensor3 # Ensure the result tensor is on the same device as tensor1 result_tensor = result_tensor.to(tensor1.device) # Get the device and strides of the resulting tensor result_device = result_tensor.device result_strides = result_tensor.stride() return result_tensor, result_device, result_strides"},{"question":"**Objective**: Implement a Python class to manage a list with functionalities similar to provided C API functions. **Task**: Create a Python class named `CustomList` which implements the following methods: 1. `__init__(self, size)`: Initializes a list of given size with `None` elements. 2. `size(self)`: Returns the size of the list. 3. `get_item(self, index)`: Returns the item at the given index. If the index is out of bounds, raise an `IndexError`. 4. `set_item(self, index, item)`: Sets the item at the given index. If the index is out of bounds, raise an `IndexError`. 5. `insert(self, index, item)`: Inserts an item at the given index. If the index is out of bounds, append the item at the end of the list. 6. `append(self, item)`: Appends an item at the end of the list. 7. `get_slice(self, low, high)`: Returns a slice of the list from `low` to `high`. Raise `IndexError` if indices are out of bounds. 8. `set_slice(self, low, high, itemlist)`: Sets the given slice from `low` to `high` with `itemlist`. Raise `IndexError` if indices are out of bounds. 9. `sort(self)`: Sorts the list in place. If any element is `None`, skip sorting and raise a `ValueError`. 10. `reverse(self)`: Reverses the list in place. 11. `as_tuple(self)`: Returns the list as a tuple. **Constraints**: - Do not use the built-in list methods like `insert`, `append`, `__getitem__`, `__setitem__`, `sort`, or `reverse` directly in your implementation for the respective methods. - You may use other Python libraries if necessary, but the primary logic should not rely on any inbuilt list manipulation methods. **Input**: - The methods will take inputs according to their description. **Output**: - Methods will return outputs as per their description. **Example**: ```python # Example implementation (not provided in the solution, just for reference) custom_list = CustomList(3) # Initializes [None, None, None] custom_list.set_item(0, 10) custom_list.set_item(1, 20) assert custom_list.get_item(0) == 10 # Returns 10 assert custom_list.size() == 3 # Returns 3 custom_list.append(30) assert custom_list.size() == 4 # Returns 4 sliced = custom_list.get_slice(1, 3) assert sliced == [20, None] custom_list.set_slice(1, 3, [40, 50]) assert custom_list.get_item(1) == 40 # Returns 40 assert custom_list.get_item(2) == 50 # Returns 50 custom_list.sort() assert custom_list.get_item(0) == 10 # Returns 10 assert custom_list.get_item(1) == 30 # Returns 30 custom_list.reverse() assert custom_list.get_item(0) == 30 # Returns 30 assert custom_list.as_tuple() == (30, 50, 40, 10) # Returns (30, 50, 40, 10) ``` Make sure to handle all specified constraints and edge cases.","solution":"class CustomList: def __init__(self, size): self._list = [None] * size def size(self): return len(self._list) def get_item(self, index): if index < 0 or index >= self.size(): raise IndexError(\\"Index out of bounds\\") return self._list[index] def set_item(self, index, item): if index < 0 or index >= self.size(): raise IndexError(\\"Index out of bounds\\") self._list[index] = item def insert(self, index, item): if index < 0 or index > self.size(): self._list.append(item) else: self._list = self._list[:index] + [item] + self._list[index:] def append(self, item): self.insert(self.size(), item) def get_slice(self, low, high): if low < 0 or high > self.size(): raise IndexError(\\"Indices out of bounds\\") return self._list[low:high] def set_slice(self, low, high, itemlist): if low < 0 or high > self.size(): raise IndexError(\\"Indices out of bounds\\") self._list = self._list[:low] + itemlist + self._list[high:] def sort(self): if None in self._list: raise ValueError(\\"List contains None elements\\") self._list = sorted(self._list) def reverse(self): self._list = self._list[::-1] def as_tuple(self): return tuple(self._list)"},{"question":"**Objective:** Implement a function to serialize a list of dictionaries into XDR format and then deserialize it back to the original data structure using the `xdrlib` module. **Problem Statement:** Write two functions: `serialize_data` and `deserialize_data`. 1. `serialize_data(data_list: List[Dict[str, Union[int, str, float]]]) -> bytes`: This function takes a list of dictionaries as input, where each dictionary contains keys with string values and values that can be integers, strings, or floating-point numbers. The function should return the serialized XDR byte stream representing the list of dictionaries. 2. `deserialize_data(data_bytes: bytes) -> List[Dict[str, Union[int, str, float]]]`: This function takes the serialized XDR byte stream as input and returns the original list of dictionaries. **Input Format:** - `data_list`: A list of dictionaries, where each dictionary can have the following structure: ```json [ {\\"key1\\": int, \\"key2\\": float, \\"key3\\": str, ...}, ... ] ``` - `data_bytes`: A byte stream representing the serialized data. **Output Format:** - `serialize_data` returns a byte stream: `data_bytes`. - `deserialize_data` returns the original list of dictionaries: `data_list`. **Constraints:** - The keys in the dictionaries are always strings. - The values in the dictionaries can be integers, floating-point numbers, or strings. - Each dictionary in the list can have a different set of keys. **Example:** Input for `serialize_data`: ```python [ {\\"id\\": 1, \\"value\\": 3.14, \\"name\\": \\"Circle\\"}, {\\"id\\": 2, \\"value\\": 2.71, \\"name\\": \\"Euler\\"} ] ``` Output for `serialize_data` (example format, the actual bytes will differ): ``` b\'...\' # Serialized byte stream ``` Input for `deserialize_data` (taking the output of `serialize_data` as input): ``` b\'...\' # Serialized byte stream ``` Output for `deserialize_data`: ```python [ {\\"id\\": 1, \\"value\\": 3.14, \\"name\\": \\"Circle\\"}, {\\"id\\": 2, \\"value\\": 2.71, \\"name\\": \\"Euler\\"} ] ``` **Implementation details:** - Utilize appropriate packing methods from `xdrlib.Packer` such as `pack_int`, `pack_double`, `pack_string` for serialization. - Utilize appropriate unpacking methods from `xdrlib.Unpacker` such as `unpack_int`, `unpack_double`, `unpack_string` for deserialization. - Handle cases where the dictionary keys and values are not known beforehand by dynamically iterating over the dictionary. ```python import xdrlib from typing import List, Dict, Union def serialize_data(data_list: List[Dict[str, Union[int, str, float]]]) -> bytes: p = xdrlib.Packer() p.pack_uint(len(data_list)) for item in data_list: p.pack_uint(len(item)) for key, value in item.items(): p.pack_string(key) if isinstance(value, int): p.pack_enum(1) # Signal that this is an integer p.pack_int(value) elif isinstance(value, float): p.pack_enum(2) # Signal that this is a float p.pack_double(value) elif isinstance(value, str): p.pack_enum(3) # Signal that this is a string p.pack_string(value) return p.get_buffer() def deserialize_data(data_bytes: bytes) -> List[Dict[str, Union[int, str, float]]]: u = xdrlib.Unpacker(data_bytes) n_items = u.unpack_uint() result = [] for _ in range(n_items): item = {} n_fields = u.unpack_uint() for _ in range(n_fields): key = u.unpack_string() value_type = u.unpack_enum() if value_type == 1: value = u.unpack_int() elif value_type == 2: value = u.unpack_double() elif value_type == 3: value = u.unpack_string() item[key] = value result.append(item) u.done() return result # Example usage data = [ {\\"id\\": 1, \\"value\\": 3.14, \\"name\\": \\"Circle\\"}, {\\"id\\": 2, \\"value\\": 2.71, \\"name\\": \\"Euler\\"} ] serialized = serialize_data(data) print(serialized) deserialized = deserialize_data(serialized) print(deserialized) ``` **Notes:** - Ensure the correct alignment and padding for strings as per XDR standard. - Utilize exceptions where necessary to handle any conversion errors during packing/unpacking.","solution":"import xdrlib from typing import List, Dict, Union def serialize_data(data_list: List[Dict[str, Union[int, str, float]]]) -> bytes: Serializes a list of dictionaries into XDR format. p = xdrlib.Packer() p.pack_uint(len(data_list)) for item in data_list: p.pack_uint(len(item)) for key, value in item.items(): p.pack_string(key.encode(\'utf-8\')) if isinstance(value, int): p.pack_enum(1) # Signal that this is an integer p.pack_int(value) elif isinstance(value, float): p.pack_enum(2) # Signal that this is a float p.pack_double(value) elif isinstance(value, str): p.pack_enum(3) # Signal that this is a string p.pack_string(value.encode(\'utf-8\')) return p.get_buffer() def deserialize_data(data_bytes: bytes) -> List[Dict[str, Union[int, str, float]]]: Deserializes a list of dictionaries from XDR format. u = xdrlib.Unpacker(data_bytes) n_items = u.unpack_uint() result = [] for _ in range(n_items): item = {} n_fields = u.unpack_uint() for _ in range(n_fields): key = u.unpack_string().decode(\'utf-8\') value_type = u.unpack_enum() if value_type == 1: value = u.unpack_int() elif value_type == 2: value = u.unpack_double() elif value_type == 3: value = u.unpack_string().decode(\'utf-8\') item[key] = value result.append(item) u.done() return result"},{"question":"# Custom Calendar Formatter Problem Statement You are tasked with creating a custom calendar formatter that will generate a calendar for a specified year. This calendar should include: - Weeks starting on a specific weekday. - Highlighting specific holidays. - Returning the calendar as an HTML table with custom CSS classes for weekends and holidays. Requirements 1. **Input:** - `year` (int): The year for which the calendar is to be generated. - `first_weekday` (int): An integer from 0 (Monday) to 6 (Sunday) indicating the first day of the week. - `holidays` (list of tuples): A list of holiday dates in the form `(month, day)`. - `weekend_css_class` (str): The CSS class to be applied to weekend days. - `holiday_css_class` (str): The CSS class to be applied to holiday days. 2. **Output:** - A string containing the HTML table representation of the calendar with appropriate CSS classes applied. 3. **Constraints:** - The year should be a positive integer. - The `first_weekday` should be an integer between 0 and 6 inclusive. - Each holiday in the list should be a valid date within the year. 4. **Performance:** - The solution should efficiently iterate over the calendar data and apply the CSS classes without unnecessary computations. Example ```python def custom_calendar(year, first_weekday, holidays, weekend_css_class, holiday_css_class): Generates an HTML calendar for the specified year with custom CSS classes for weekends and holidays. Parameters: - year (int): The year for which the calendar is generated. - first_weekday (int): The first day of the week (0=Monday, 6=Sunday). - holidays (list of tuples): List of holidays in (month, day) format. - weekend_css_class (str): The CSS class for weekend days. - holiday_css_class (str): The CSS class for holiday days. Returns: - str: The HTML string of the calendar with custom classes. pass # Example usage: html_calendar = custom_calendar( 2023, 0, [(1, 1), (12, 25)], \\"weekend\\", \\"holiday\\" ) print(html_calendar) ``` The output will be an HTML table for the year 2023 with weekends and the specified holidays highlighted using the provided CSS classes. Notes - Use the `calendar.HTMLCalendar` class to generate the base HTML calendar. - Customize the functionality by overriding necessary methods or by post-processing the generated HTML. - Ensure the solution is modular and easily extendable for additional custom CSS class requirements.","solution":"import calendar class CustomHTMLCalendar(calendar.HTMLCalendar): def __init__(self, firstweekday, holidays, weekend_css_class, holiday_css_class): super().__init__(firstweekday) self.holidays = holidays self.weekend_css_class = weekend_css_class self.holiday_css_class = holiday_css_class def formatday(self, day, weekday, theyear, themonth): Return a day as a table cell with custom CSS classes for weekends and holidays. if day == 0: return \'<td class=\\"noday\\">&nbsp;</td>\' # day outside month css_classes = [] if (themonth, day) in self.holidays: css_classes.append(self.holiday_css_class) if weekday in {5, 6}: # 5 = Saturday, 6 = Sunday css_classes.append(self.weekend_css_class) css_class_str = \' \'.join(css_classes) return f\'<td class=\\"{css_class_str}\\">{day}</td>\' def formatmonth(self, theyear, themonth, withyear=True): Return a formatted month as a table. v = [] a = v.append a(\'<table border=\\"0\\" cellpadding=\\"0\\" cellspacing=\\"0\\" class=\\"month\\">\') a(\'n\') a(self.formatmonthname(theyear, themonth, withyear=withyear)) a(\'n\') a(self.formatweekheader()) a(\'n\') for week in self.monthdays2calendar(theyear, themonth): a(self.formatweek(theyear, themonth, week)) a(\'n\') a(\'</table>\') a(\'n\') return \'\'.join(v) def formatweek(self, theyear, themonth, theweek): Return a complete week as a table row. s = \'\'.join(self.formatday(d, wd, theyear, themonth) for (d, wd) in theweek) return f\'<tr>{s}</tr>\' def custom_calendar(year, first_weekday, holidays, weekend_css_class, holiday_css_class): Generates an HTML calendar for the specified year with custom CSS classes for weekends and holidays. Parameters: - year (int): The year for which the calendar is generated. - first_weekday (int): The first day of the week (0=Monday, 6=Sunday). - holidays (list of tuples): List of holidays in (month, day) format. - weekend_css_class (str): The CSS class for weekend days. - holiday_css_class (str): The CSS class for holiday days. Returns: - str: The HTML string of the calendar with custom classes. cal = CustomHTMLCalendar(first_weekday, holidays, weekend_css_class, holiday_css_class) html_calendar = \\"\\" for month in range(1, 13): html_calendar += cal.formatmonth(year, month) return html_calendar"},{"question":"**Objective:** Your task is to implement several functions that interact with the `mimetypes` module in Python. These functions will help determine MIME types, associate extensions with MIME types, and handle MIME type databases. Ensure that your solutions handle various edge cases and follow best practices. **Function 1: `get_mime_type(url_or_path: str, strict: bool = True) -> tuple`** Implement a function named `get_mime_type` that takes a filename, a path, or a URL as input, and returns a tuple with the MIME type and encoding of the file. **Input:** - `url_or_path` (str): The filename, path, or URL to analyze. - `strict` (bool): If True, only official types are supported. Defaults to True. **Output:** - A tuple `(type, encoding)`, where `type` is the MIME type (or None if it can\'t be guessed) and `encoding` is the name of the encoding program (or None if not applicable). ```python def get_mime_type(url_or_path: str, strict: bool = True) -> tuple: # Your implementation goes here ``` **Function 2: `add_custom_mime_type(mime_type: str, extension: str, strict: bool = True) -> None`** Implement a function named `add_custom_mime_type` that adds a custom MIME type to the module. **Input:** - `mime_type` (str): The MIME type to add (e.g., \\"application/x-myapp\\"). - `extension` (str): The filename extension to associate with the MIME type (e.g., \\".myapp\\"). - `strict` (bool): If True, the mapping is added to the list of official MIME types. Defaults to True. **Output:** - None ```python def add_custom_mime_type(mime_type: str, extension: str, strict: bool = True) -> None: # Your implementation goes here ``` **Function 3: `get_all_extensions_for_mime(mime_type: str, strict: bool = True) -> list`** Implement a function named `get_all_extensions_for_mime` that returns all possible filename extensions for a given MIME type. **Input:** - `mime_type` (str): The MIME type to analyze. - `strict` (bool): If True, only official types are considered. Defaults to True. **Output:** - A list of strings representing all possible extensions for the given MIME type. ```python def get_all_extensions_for_mime(mime_type: str, strict: bool = True) -> list: # Your implementation goes here ``` **Function 4: `read_and_parse_mime_file(filepath: str) -> dict`** Implement a function named `read_and_parse_mime_file` that reads a MIME types file and returns a dictionary mapping extensions to MIME types. **Input:** - `filepath` (str): The path to the MIME types file. **Output:** - A dictionary where each key is an extension (with the leading dot) and each value is the corresponding MIME type. ```python def read_and_parse_mime_file(filepath: str) -> dict: # Your implementation goes here ``` # Constraints: 1. You should use the `mimetypes` module functions where appropriate. 2. Handle edge cases, such as file not found or unsupported MIME types. 3. Ensure that your functions are efficient and follow best practices. # Example Usage: ```python # Example usage for get_mime_type url = \\"example.txt\\" print(get_mime_type(url)) # Output: (\'text/plain\', None) # Example usage for add_custom_mime_type add_custom_mime_type(\\"application/x-myapp\\", \\".myapp\\") print(get_mime_type(\\"myfile.myapp\\")) # Output: (\'application/x-myapp\', None) # Example usage for get_all_extensions_for_mime print(get_all_extensions_for_mime(\\"text/plain\\")) # Output: [\'.txt\', \'.text\'] # Example usage for read_and_parse_mime_file mime_file = \\"path/to/mime.types\\" print(read_and_parse_mime_file(mime_file)) # Output: {\'.txt\': \'text/plain\', ...} ``` Implement these functions to demonstrate your understanding of the `mimetypes` module and your ability to manipulate MIME types and file extensions effectively.","solution":"import mimetypes def get_mime_type(url_or_path: str, strict: bool = True) -> tuple: Returns the MIME type and encoding for the given file, path, or URL. return mimetypes.guess_type(url_or_path, strict=strict) def add_custom_mime_type(mime_type: str, extension: str, strict: bool = True) -> None: Adds a custom MIME type for a given file extension. mimetypes.add_type(mime_type, extension, strict=strict) def get_all_extensions_for_mime(mime_type: str, strict: bool = True) -> list: Returns a list of all extensions associated with the given MIME type. return [ext for ext, mime in mimetypes.types_map.items() if mime == mime_type] def read_and_parse_mime_file(filepath: str) -> dict: Reads a MIME types file and returns a dictionary mapping extensions to MIME types. custom_mimetypes = {} with open(filepath, \'r\') as file: for line in file: line = line.strip() if line and not line.startswith(\'#\'): parts = line.split() if len(parts) > 1: mime_type = parts[0] for ext in parts[1:]: custom_mimetypes[ext] = mime_type return custom_mimetypes"},{"question":"# Email Header Management with Python310 `email.headerregistry` Objective Create a class that uses the `email.headerregistry` module to manage various email headers, ensuring they adhere to RFC 5322 compliance. Implement methods to set, get, and display these headers effectively. Task 1. **HeaderManager Class**: Implement a class `HeaderManager` with the following requirements: - **Initialize**: - Create an instance of `HeaderRegistry`. - Initialize a dictionary to store header name and their corresponding header instances. - **Set Header**: - Method `set_header(name, value)` where `name` is the header name (e.g., \'Subject\') and `value` is the header value. Use the `HeaderRegistry` to create the appropriate header object and store it. - **Get Header**: - Method `get_header(name)` which returns the string value of the specified header. - **Display All Headers**: - Method `display_headers()` that prints all headers and their corresponding values in the format `Header-Name: Header-Value`. Constraints - Use appropriate header classes based on the header type. For example, \'Subject\' should use `UnstructuredHeader`, \'Date\' should use `DateHeader`, etc. - Handle potential defects by logging them using Python\'s `logging` module. Example Usage ```python from datetime import datetime from email.utils import format_datetime # Example of usage manager = HeaderManager() manager.set_header(\'Subject\', \'Test Email\') manager.set_header(\'Date\', format_datetime(datetime.now())) manager.set_header(\'From\', \'example@example.com\') manager.display_headers() # Expected Output (will vary based on Date and defect logging) Subject: Test Email Date: <current date in RFC 5322 format> From: example@example.com ``` Implementation Implement the `HeaderManager` class with necessary methods: ```python import logging from email.headerregistry import HeaderRegistry # Configure logging logging.basicConfig(level=logging.INFO) class HeaderManager: def __init__(self): self.registry = HeaderRegistry() self.headers = {} def set_header(self, name, value): header = self.registry(name, value) if header.defects: for defect in header.defects: logging.warning(f\\"Defect in {name} header: {defect}\\") self.headers[name] = header def get_header(self, name): header = self.headers.get(name) if header: return str(header) return None def display_headers(self): for name, header in self.headers.items(): print(f\\"{name}: {header}\\") # Test the class if __name__ == \\"__main__\\": from datetime import datetime from email.utils import format_datetime manager = HeaderManager() manager.set_header(\'Subject\', \'Test Email\') manager.set_header(\'Date\', format_datetime(datetime.now())) manager.set_header(\'From\', \'example@example.com\') manager.display_headers() ```","solution":"import logging from email.headerregistry import HeaderRegistry # Configure logging logging.basicConfig(level=logging.INFO) class HeaderManager: def __init__(self): self.registry = HeaderRegistry() self.headers = {} def set_header(self, name, value): header = self.registry(name, value) if header.defects: for defect in header.defects: logging.warning(f\\"Defect in {name} header: {defect}\\") self.headers[name] = header def get_header(self, name): header = self.headers.get(name) if header: return str(header) return None def display_headers(self): for name, header in self.headers.items(): print(f\\"{name}: {header}\\")"},{"question":"# Email Message Manipulation You are tasked with writing a Python function that creates and manipulates an `email.message.Message` object to represent an email in a specific format. The function should demonstrate comprehension of the `Message` class as described in the provided documentation. **Requirements:** 1. **Function Name**: `create_email_message` 2. **Input**: The function takes in the following parameters: - `from_addr` (str): The sender\'s email address. - `to_addr` (str): The recipient\'s email address. - `subject` (str): The subject of the email. - `body` (str): The body of the email message. - `attachments` (list of tuple): A list of attachments, where each attachment is represented as a tuple (`filename`, `content`, `content_type`). 3. **Output**: The function should return the `Message` object representing the composed email. 4. **Constraints**: - All email headers (like `From`, `To`, `Subject`) must comply with the RFC 5322 standards. - The body of the email should be added correctly along with any attachments. - If attachments are provided, the message should have a `multipart` MIME type. - The function should ensure that the headers and attachments are properly encoded if containing non-ASCII characters. **Example Usage**: ```python attachments = [ (\'example.txt\', \'This is the content of the file.\', \'text/plain\'), (\'image.jpg\', b\'BinaryImageData\', \'image/jpeg\') ] msg = create_email_message(\'sender@example.com\', \'recipient@example.com\', \'Test Subject\', \'This is the email body.\', attachments) print(msg.as_string()) ``` **Expected Behavior**: - The output should be a `Message` object correctly structured, with the relevant headers, body content, and attachments. - Running `msg.as_string()` should produce a string representation of the email message that conforms to email standards. ```python from email.message import Message def create_email_message(from_addr, to_addr, subject, body, attachments): Create and return an email message object. :param from_addr: Sender\'s email address :param to_addr: Recipient\'s email address :param subject: Subject of the email :param body: Body of the email :param attachments: List of attachments, each as a tuple of (filename, content, content_type) :return: Message object representing the email msg = Message() msg[\'From\'] = from_addr msg[\'To\'] = to_addr msg[\'Subject\'] = subject if attachments: msg.add_header(\'Content-Type\', \'multipart/mixed\') # Using multipart/mixed for email with attachments msg.set_payload([]) # Create a part for the body body_part = Message() body_part.set_payload(body) body_part.add_header(\'Content-Type\', \'text/plain\') msg.attach(body_part) # Create parts for each attachment for filename, content, content_type in attachments: part = Message() part.add_header(\'Content-Disposition\', \'attachment\', filename=filename) part.set_payload(content) part.add_header(\'Content-Type\', content_type) msg.attach(part) else: msg.set_payload(body) msg.add_header(\'Content-Type\', \'text/plain\') return msg ``` **Notes**: - Ensure to use the appropriate headers and methods for setting multipart messages. - Attachments should be correctly handled, with their `Content-Disposition` and `Content-Type` headers properly set.","solution":"from email.message import EmailMessage from email.mime.base import MIMEBase from email.mime.text import MIMEText from email.mime.multipart import MIMEMultipart from email import encoders def create_email_message(from_addr, to_addr, subject, body, attachments): Create and return an email message object. :param from_addr: Sender\'s email address :param to_addr: Recipient\'s email address :param subject: Subject of the email :param body: Body of the email :param attachments: List of attachments, each as a tuple of (filename, content, content_type) :return: EmailMessage object representing the email msg = MIMEMultipart() msg[\'From\'] = from_addr msg[\'To\'] = to_addr msg[\'Subject\'] = subject # Attach the body of the email body_part = MIMEText(body, \'plain\') msg.attach(body_part) for filename, content, content_type in attachments: part = MIMEBase(*content_type.split(\'/\')) part.set_payload(content) encoders.encode_base64(part) part.add_header(\'Content-Disposition\', f\'attachment; filename=\\"{filename}\\"\') msg.attach(part) return msg"},{"question":"Advanced Configuration File Manipulation Objective: You are required to demonstrate your ability to parse, manipulate, and generate configuration files using Python 3.10\'s `configparser` module. Task: Write a Python function `update_config_file(file_path: str, section: str, new_options: dict) -> str:` that: 1. Takes the path of a configuration file (`file_path`) in INI format. 2. A section name (`section`) to be updated or added. 3. A dictionary (`new_options`) containing key-value pairs to be updated/added within the specified section. 4. The function should update or add the given section and key-value pairs in the INI file, then write the updated configuration back to the file. 5. The function should return a string summary of the changes made in the format: ``` [Section] option1 = value1 option2 = value2 ``` Input: - `file_path` (str): The path to the INI configuration file. - `section` (str): The section name to update or add. - `new_options` (dict): A dictionary of options to update/add in the specified section. Output: - A string summary of changes made. Constraints: 1. If the given section does not exist, it should be created. 2. If a key in `new_options` exists in the section, its value should be updated. 3. If a key in `new_options` does not exist in the section, it should be added. Examples: Given the following initial content of `config.ini` file: ``` [General] version = 1.0 name = SampleApp ``` Example 1: ```python file_path = \'config.ini\' section = \'General\' new_options = { \'version\': \'1.1\', \'description\': \'Updated Application\' } print(update_config_file(file_path, section, new_options)) ``` Expected Output: ``` [General] version = 1.1 name = SampleApp description = Updated Application ``` Example 2: ```python file_path = \'config.ini\' section = \'NewSection\' new_options = { \'key1\': \'value1\', } print(update_config_file(file_path, section, new_options)) ``` Expected Output: ``` [NewSection] key1 = value1 ``` Additional Information: You can make use of the `configparser` module\'s various capabilities such as reading and writing to `.ini` files and managing sections and options within your solution.","solution":"from configparser import ConfigParser def update_config_file(file_path: str, section: str, new_options: dict) -> str: Update or add a section and options in a configuration file. Parameters: file_path (str): The path to the INI configuration file. section (str): The section name to update or add. new_options (dict): A dictionary of options to update/add in the specified section. Returns: str: A summary of the changes made. config = ConfigParser() config.read(file_path) if not config.has_section(section): config.add_section(section) for key, value in new_options.items(): config.set(section, key, value) with open(file_path, \'w\') as configfile: config.write(configfile) # Create a summary of changes made changes_summary = f\\"[{section}]n\\" for option in config.options(section): changes_summary += f\\"{option} = {config.get(section, option)}n\\" return changes_summary.strip()"},{"question":"# PyTorch Futures: Asynchronous Data Processing Asynchronous execution is an essential concept that allows programs to perform tasks such as handling I/O operations without blocking the main execution thread. PyTorch provides the `torch.futures.Future` class to support asynchronous operations within its framework. This question requires you to leverage `torch.futures.Future` alongside provided utility functions to perform asynchronous data processing in a distributed system. Question: You are given a distributed system setup where multiple workers process data batches asynchronously. Each worker performs some computational tasks on the data and returns the result. You need to implement a function that: 1. Dispatches tasks to these workers using asynchronous futures. 2. Collects the results from all workers once they have finished their tasks. 3. Aggregates these results and returns the final computation result. Implement the following function: ```python import torch from torch.futures import Future, collect_all, wait_all def asynchronous_data_processing(data_batches, worker_fn): Args: - data_batches (List[Tensor]): A list of data batches to be processed asynchronously. - worker_fn (Callable[[Tensor], Future]): A function that takes a data batch, processes it asynchronously, and returns a Future object. Returns: - aggregated_result (Tensor): The aggregated result from all processed data batches. Constraints: - Each data batch is represented as a PyTorch Tensor. - The worker_fn should be invoked asynchronously. - The final result should be a single Tensor representing the aggregated computation from all data batches. pass ``` # Example Usage: Here is a simplistic example to illustrate how the function might be used: ```python import torch import time from torch.futures import Future def example_worker_fn(data_batch): future = Future() def process_data(): # Simulate some computational work time.sleep(1) result = data_batch.sum() # Example computation future.set_result(result) # Simulate asynchronous execution torch.jit._fork(process_data) return future data_batches = [torch.randn(10, 10) for _ in range(5)] result = asynchronous_data_processing(data_batches, example_worker_fn) print(result) # Should print the aggregated sum of all data batches ``` # Notes: - Ensure that the tasks are truly handled asynchronously to mimic real-world scenarios. - Efficiently handle the results aggregation to minimize performance bottlenecks. - The worker function (`worker_fn`) is expected to be provided and will handle the asynchronous computation of individual data batches.","solution":"import torch from torch.futures import Future, collect_all, wait_all def asynchronous_data_processing(data_batches, worker_fn): Args: - data_batches (List[Tensor]): A list of data batches to be processed asynchronously. - worker_fn (Callable[[Tensor], Future]): A function that takes a data batch, processes it asynchronously, and returns a Future object. Returns: - aggregated_result (Tensor): The aggregated result from all processed data batches. Constraints: - Each data batch is represented as a PyTorch Tensor. - The worker_fn should be invoked asynchronously. - The final result should be a single Tensor representing the aggregated computation from all data batches. # Dispatch tasks to workers asynchronously futures = [worker_fn(data_batch) for data_batch in data_batches] # Wait for all futures to complete and gather their results results = wait_all(futures) # Aggregate the results aggregated_result = sum(result for result in results) return aggregated_result"},{"question":"**Objective:** Implement a GUI application using the `tkinter` module that interacts with the user via various types of message boxes from the `tkinter.messagebox` module. The application should demonstrate the use of information, warning, error, and question message boxes and handle user responses accordingly. **Task:** Write a function `start_application()` that does the following: 1. Initializes a Tkinter window. 2. Displays an information message box asking the user if they want to continue. Use `tkinter.messagebox.showinfo` method. 3. If the user chooses to continue, display a warning message box using `tkinter.messagebox.showwarning` method. 4. After the warning, ask the user a yes/no question using the `tkinter.messagebox.askyesno` method. 5. Based on the user\'s response to the yes/no question, display either an error message box using `tkinter.messagebox.showerror` or another information message box using `tkinter.messagebox.showinfo`. 6. Capture and print the responses from the user interactions. **Input:** - No direct input is required except through the user\'s interactions with the message boxes. **Output:** - The function should print the user\'s responses to the console. **Constraints:** - Implement the GUI interactions synchronously for simplicity. - Ensure proper imports and instantiation of the Tkinter root window. - Use Python’s Tkinter framework specifically, any deviation will not be accepted. **Example:** ```python import tkinter as tk from tkinter import messagebox def start_application(): root = tk.Tk() root.withdraw() # Hide the root window # Step 2: Display information message box response = messagebox.showinfo(\\"Welcome\\", \\"Do you want to continue?\\") print(\\"Response to info message:\\", response) if response is None: return # Step 3: Display warning message box messagebox.showwarning(\\"Warning\\", \\"This is a warning message!\\") print(\\"Displayed warning message\\") # Step 4: Ask yes/no question response_yes_no = messagebox.askyesno(\\"Question\\", \\"Do you want to proceed with this action?\\") print(\\"Response to yes/no question:\\", response_yes_no) # Step 5: Based on the user\'s response, show error or information message if response_yes_no: response_ok = messagebox.showinfo(\\"Proceeding\\", \\"You chose to proceed.\\") print(\\"Response to proceeding with action:\\", response_ok) else: response_error = messagebox.showerror(\\"Cancelled\\", \\"Action has been cancelled.\\") print(\\"Response to cancellation:\\", response_error) root.destroy() # Uncomment the line below to test the function # start_application() ``` **Note:** The function `start_application()` should handle initializing and terminating the Tkinter root window properly.","solution":"import tkinter as tk from tkinter import messagebox def start_application(): root = tk.Tk() root.withdraw() # Hide the root window # Step 2: Display information message box response = messagebox.showinfo(\\"Welcome\\", \\"Do you want to continue?\\") print(\\"Response to info message:\\", response) if response is None: return # Step 3: Display warning message box messagebox.showwarning(\\"Warning\\", \\"This is a warning message!\\") print(\\"Displayed warning message\\") # Step 4: Ask yes/no question response_yes_no = messagebox.askyesno(\\"Question\\", \\"Do you want to proceed with this action?\\") print(\\"Response to yes/no question:\\", response_yes_no) # Step 5: Based on the user\'s response, show error or information message if response_yes_no: response_ok = messagebox.showinfo(\\"Proceeding\\", \\"You chose to proceed.\\") print(\\"Response to proceeding with action:\\", response_ok) else: response_error = messagebox.showerror(\\"Cancelled\\", \\"Action has been cancelled.\\") print(\\"Response to cancellation:\\", response_error) root.destroy() # Uncomment the line below to test the function # start_application()"},{"question":"# Kernel Approximation with Nystroem Method and RBFSampler Problem Statement You are given a dataset `X` of shape (n_samples, n_features) and corresponding labels `y` of shape (n_samples). The task is to implement a pipeline that uses the `Nystroem` method and `RBFSampler` for kernel approximation and then apply a linear classifier (`SGDClassifier`) to train on the transformed data. Requirements 1. **Data Transformation**: - Implement the `Nystroem` method and `RBFSampler` for feature mapping separately. - Transform the dataset `X` using these methods. 2. **Classifier**: - Train a `SGDClassifier` on the transformed data for both methods. 3. **Evaluation**: - Compare the performance (accuracy) of the classifier on the original dataset `X` and the approximated datasets obtained from both `Nystroem` and `RBFSampler`. Input - `X`: A 2D numpy array of shape (n_samples, n_features), representing the input data. - `y`: A 1D numpy array of shape (n_samples,), representing the labels. Output - Dictionary containing accuracy scores of the classifier on: - Original dataset: `score_original` - Transformed dataset using Nystroem method: `score_nystroem` - Transformed dataset using RBFSampler: `score_rbf` Constraints - Use `n_components=100` for both Nystroem and RBFSampler. - The `gamma` parameter for `RBFSampler` should be set to 1. Performance Requirements - The implementation should be efficient and scalable for large datasets, using the specified kernel approximation methods. Example ```python import numpy as np from sklearn.kernel_approximation import Nystroem, RBFSampler from sklearn.linear_model import SGDClassifier from sklearn.metrics import accuracy_score from sklearn.model_selection import train_test_split def kernel_approximation_pipeline(X, y): # Split the data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Original data clf_original = SGDClassifier(max_iter=1000, tol=1e-3, random_state=42) clf_original.fit(X_train, y_train) score_original = accuracy_score(y_test, clf_original.predict(X_test)) # Nystroem method nystroem = Nystroem(n_components=100, random_state=42) X_train_nystroem = nystroem.fit_transform(X_train) X_test_nystroem = nystroem.transform(X_test) clf_nystroem = SGDClassifier(max_iter=1000, tol=1e-3, random_state=42) clf_nystroem.fit(X_train_nystroem, y_train) score_nystroem = accuracy_score(y_test, clf_nystroem.predict(X_test_nystroem)) # RBFSampler method rbf_sampler = RBFSampler(gamma=1, n_components=100, random_state=42) X_train_rbf = rbf_sampler.fit_transform(X_train) X_test_rbf = rbf_sampler.transform(X_test) clf_rbf = SGDClassifier(max_iter=1000, tol=1e-3, random_state=42) clf_rbf.fit(X_train_rbf, y_train) score_rbf = accuracy_score(y_test, clf_rbf.predict(X_test_rbf)) return { \'score_original\': score_original, \'score_nystroem\': score_nystroem, \'score_rbf\': score_rbf } # Example usage: X = np.array([[0, 0], [1, 1], [1, 0], [0, 1], [2, 2], [2, 1], [1, 2], [0.5, 0.5], [1.5, 1.5], [2, 2.5]]) y = np.array([0, 0, 1, 1, 0, 1, 0, 0, 1, 1]) result = kernel_approximation_pipeline(X, y) print(result) ```","solution":"import numpy as np from sklearn.kernel_approximation import Nystroem, RBFSampler from sklearn.linear_model import SGDClassifier from sklearn.metrics import accuracy_score from sklearn.model_selection import train_test_split def kernel_approximation_pipeline(X, y): Transforms data using Nystroem and RBFSampler methods, trains a SGDClassifier on original and transformed data, and returns accuracy scores on the test set. Parameters: X (numpy array): Input data of shape (n_samples, n_features) y (numpy array): Labels of shape (n_samples,) Returns: dict: Accuracy scores for original, Nystroem, and RBFSampler transformed data # Split the data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Original data clf_original = SGDClassifier(max_iter=1000, tol=1e-3, random_state=42) clf_original.fit(X_train, y_train) score_original = accuracy_score(y_test, clf_original.predict(X_test)) # Nystroem method nystroem = Nystroem(n_components=100, random_state=42) X_train_nystroem = nystroem.fit_transform(X_train) X_test_nystroem = nystroem.transform(X_test) clf_nystroem = SGDClassifier(max_iter=1000, tol=1e-3, random_state=42) clf_nystroem.fit(X_train_nystroem, y_train) score_nystroem = accuracy_score(y_test, clf_nystroem.predict(X_test_nystroem)) # RBFSampler method rbf_sampler = RBFSampler(gamma=1, n_components=100, random_state=42) X_train_rbf = rbf_sampler.fit_transform(X_train) X_test_rbf = rbf_sampler.transform(X_test) clf_rbf = SGDClassifier(max_iter=1000, tol=1e-3, random_state=42) clf_rbf.fit(X_train_rbf, y_train) score_rbf = accuracy_score(y_test, clf_rbf.predict(X_test_rbf)) return { \'score_original\': score_original, \'score_nystroem\': score_nystroem, \'score_rbf\': score_rbf }"},{"question":"You are given a dataset of sales data in CSV format that includes the following columns: - `Product`: Name of the product sold. - `Units Sold`: Number of units sold. - `Sale Date`: Date when the sale was made. - `Price per Unit`: Price per unit of the product. - `Region`: The region where the sale was made. The goal of this assessment is to perform an analysis on this sales data to answer some specific questions and produce various summaries. Requirements: 1. **Read the dataset from a CSV file into a pandas DataFrame.** 2. **Calculate the total revenue generated for each product.** 3. **Find out which product has the highest revenue.** 4. **Determine the average units sold per day for each product (use the `Sale Date` column).** 5. **Determine which region has the maximum total sales revenue.** 6. **Provide memory usage details of the DataFrame using `memory_usage=\'deep\'`.** Implement the following function: ```python import pandas as pd def analyze_sales_data(file_path: str) -> dict: Analyze sales data from the provided CSV file. Args: file_path (str): Path to the CSV file containing sales data. Returns: dict: A dictionary with analysis results containing: - total_revenue_per_product: DataFrame with Product and Total Revenue columns. - highest_revenue_product: Name of the product with the highest revenue. - avg_units_sold_per_day: DataFrame with Product and Average Units Sold per Day columns. - highest_revenue_region: Name of the region with the highest total sales revenue. - memory_usage_info: String with detailed memory usage information. # Read the dataset into a DataFrame df = pd.read_csv(file_path) # Calculate total revenue generated for each product df[\'Total Revenue\'] = df[\'Units Sold\'] * df[\'Price per Unit\'] total_revenue_per_product = df.groupby(\'Product\')[\'Total Revenue\'].sum().reset_index() # Find the product with the highest revenue highest_revenue_product = total_revenue_per_product.loc[total_revenue_per_product[\'Total Revenue\'].idxmax(), \'Product\'] # Calculate average units sold per day for each product df[\'Sale Date\'] = pd.to_datetime(df[\'Sale Date\']) avg_units_sold_per_day = (df.groupby([\'Product\', df[\'Sale Date\'].dt.date])[\'Units Sold\'].sum() .groupby(\'Product\').mean().reset_index(name=\'Average Units Sold per Day\')) # Determine which region has the highest total sales revenue highest_revenue_region = df.groupby(\'Region\')[\'Total Revenue\'].sum().idxmax() # Provide memory usage details memory_usage_info = df.info(memory_usage=\'deep\') return { \'total_revenue_per_product\': total_revenue_per_product, \'highest_revenue_product\': highest_revenue_product, \'avg_units_sold_per_day\': avg_units_sold_per_day, \'highest_revenue_region\': highest_revenue_region, \'memory_usage_info\': memory_usage_info } ``` Constraints: - The CSV file is guaranteed to have valid data. - You are not allowed to modify the columns of the dataset except for adding new ones as necessary. - Ensure the solution leverages pandas efficiently to handle potentially large datasets. Example usage: ```python file_path = \'sales_data.csv\' result = analyze_sales_data(file_path) print(result[\'total_revenue_per_product\']) print(result[\'highest_revenue_product\']) print(result[\'avg_units_sold_per_day\']) print(result[\'highest_revenue_region\']) print(result[\'memory_usage_info\']) ``` Notes: - Make sure to handle the data conversions and aggregation using pandas functions. - Document any assumptions or design decisions made during the implementation.","solution":"import pandas as pd def analyze_sales_data(file_path: str) -> dict: Analyze sales data from the provided CSV file. Args: file_path (str): Path to the CSV file containing sales data. Returns: dict: A dictionary with analysis results containing: - total_revenue_per_product: DataFrame with Product and Total Revenue columns. - highest_revenue_product: Name of the product with the highest revenue. - avg_units_sold_per_day: DataFrame with Product and Average Units Sold per Day columns. - highest_revenue_region: Name of the region with the highest total sales revenue. - memory_usage_info: String with detailed memory usage information. # Read the dataset into a DataFrame df = pd.read_csv(file_path) # Calculate total revenue generated for each product df[\'Total Revenue\'] = df[\'Units Sold\'] * df[\'Price per Unit\'] total_revenue_per_product = df.groupby(\'Product\')[\'Total Revenue\'].sum().reset_index() # Find the product with the highest revenue highest_revenue_product = total_revenue_per_product.loc[total_revenue_per_product[\'Total Revenue\'].idxmax(), \'Product\'] # Calculate average units sold per day for each product df[\'Sale Date\'] = pd.to_datetime(df[\'Sale Date\']) avg_units_sold_per_day = (df.groupby([\'Product\', df[\'Sale Date\'].dt.date])[\'Units Sold\'].sum() .groupby(\'Product\').mean().reset_index(name=\'Average Units Sold per Day\')) # Determine which region has the highest total sales revenue highest_revenue_region = df.groupby(\'Region\')[\'Total Revenue\'].sum().idxmax() # Provide memory usage details memory_usage_info = df.memory_usage(deep=True).to_string() return { \'total_revenue_per_product\': total_revenue_per_product, \'highest_revenue_product\': highest_revenue_product, \'avg_units_sold_per_day\': avg_units_sold_per_day, \'highest_revenue_region\': highest_revenue_region, \'memory_usage_info\': memory_usage_info }"},{"question":"Custom Event Loop Policy Your task is to create a custom event loop policy by subclassing `asyncio.DefaultEventLoopPolicy` in Python. This custom policy should modify behavior in the following way: 1. Override the `get_event_loop()` method to log every time it is called and then proceed with its default behavior. 2. Override the `new_event_loop()` method to always set a custom name to the new event loop created. Requirements 1. **Function class structure**: - Create a class named `MyCustomEventLoopPolicy` that inherits from `asyncio.DefaultEventLoopPolicy`. 2. **Methods**: - Override `get_event_loop()` to add logging functionality. - Override `new_event_loop()` to set a custom attribute `loop_name` with the value `\\"custom_loop\\"` on the new event loop object before returning it. 3. **Implementation Details**: - Use Python\'s logging module to log a message `\\"get_event_loop() called\\"` every time the method is called. - Ensure that `new_event_loop()` method gives the created event loop a custom attribute named `loop_name` before returning it. 4. **Testing**: - Set `MyCustomEventLoopPolicy` as the current event loop policy. - Verify that calling `asyncio.get_event_loop()` logs the message as expected. - Verify that the event loop created by `new_event_loop()` has the attribute `loop_name` with the value `\\"custom_loop\\"`. Example ```python import asyncio import logging # Set up logging logging.basicConfig(level=logging.INFO) class MyCustomEventLoopPolicy(asyncio.DefaultEventLoopPolicy): def get_event_loop(self): logging.info(\\"get_event_loop() called\\") return super().get_event_loop() def new_event_loop(self): loop = super().new_event_loop() loop.loop_name = \\"custom_loop\\" return loop # Set the custom event loop policy asyncio.set_event_loop_policy(MyCustomEventLoopPolicy()) # Test the get_event_loop method and the custom loop_name attribute loop = asyncio.get_event_loop() new_loop = loop.new_event_loop() print(loop) # Outputs: <EventLoop ...> print(new_loop.loop_name) # Outputs: custom_loop ``` Constraints - You should not modify any objects or methods outside the needed overrides. - You must use the logging module to log information. - The solution should work on both Unix and Windows systems. Performance Requirements - No specific performance requirements are necessary for this question.","solution":"import asyncio import logging # Set up logging logging.basicConfig(level=logging.INFO) class MyCustomEventLoopPolicy(asyncio.DefaultEventLoopPolicy): def get_event_loop(self): logging.info(\\"get_event_loop() called\\") return super().get_event_loop() def new_event_loop(self): loop = super().new_event_loop() loop.loop_name = \\"custom_loop\\" return loop # Set the custom event loop policy asyncio.set_event_loop_policy(MyCustomEventLoopPolicy())"},{"question":"# PyTorch Coding Assessment Question **Objective:** Evaluate the student\'s ability to work with `torch.nn.Module` hierarchies and leverage PyTorch utilities for monitoring and tracking purposes. **Question:** You are given a deep learning model consisting of several `torch.nn.Module` layers organized hierarchically. Your task is to implement a custom module tracker that monitors the forward pass and counts the number of times each module is called during inference. **Instructions:** 1. **Define a custom `ModuleTracker` class** with the following methods: - `__init__(self, model: torch.nn.Module)`: Initialize the tracker with a given model. - `register_hooks(self)`: Register forward hooks to all modules within the model to track the number of times each module’s forward method is called. - `get_call_counts(self) -> dict`: Return a dictionary mapping module names to the number of times they were called during the forward pass. 2. **Use the implemented `ModuleTracker` class** to track a simple model\'s module calls. The model should include at least one example of each of the following: - Convolutional layer (`torch.nn.Conv2d`) - Fully connected layer (`torch.nn.Linear`) - Activation function (`torch.nn.ReLU`) **Constraints:** - The model can have a nested structure where modules contain sub-modules. **Example Usage:** ```python import torch import torch.nn as nn import torch.nn.functional as F class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.conv1 = nn.Conv2d(1, 32, 3) self.relu = nn.ReLU() self.fc = nn.Linear(32 * 26 * 26, 10) def forward(self, x): x = self.conv1(x) x = self.relu(x) x = x.view(x.size(0), -1) x = self.fc(x) return x model = SimpleModel() tracker = ModuleTracker(model) tracker.register_hooks() # Run a single forward pass input_tensor = torch.randn(1, 1, 28, 28) output = model(input_tensor) # Get the call counts for each module call_counts = tracker.get_call_counts() print(call_counts) # Example output: {\'SimpleModel.conv1\': 1, \'SimpleModel.relu\': 1, \'SimpleModel.fc\': 1} ``` **Expected Output:** The output should be a dictionary indicating the number of calls to each module during the forward pass. **Note:** Ensure your implementation is efficient and correctly handles nested module hierarchies.","solution":"import torch import torch.nn as nn import torch.nn.functional as F class ModuleTracker: def __init__(self, model: torch.nn.Module): self.model = model self.call_counts = {} self.handlers = [] def register_hooks(self): def hook(module, input, output): name = module.__class__.__name__ self.call_counts[name] = self.call_counts.get(name, 0) + 1 for name, module in self.model.named_modules(): handler = module.register_forward_hook(hook) self.handlers.append(handler) def get_call_counts(self) -> dict: return self.call_counts class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.conv1 = nn.Conv2d(1, 32, 3) self.relu = nn.ReLU() self.fc = nn.Linear(32 * 26 * 26, 10) def forward(self, x): x = self.conv1(x) x = self.relu(x) x = x.view(x.size(0), -1) x = self.fc(x) return x # Demostration of usage model = SimpleModel() tracker = ModuleTracker(model) tracker.register_hooks() # Run a single forward pass input_tensor = torch.randn(1, 1, 28, 28) output = model(input_tensor) # Get the call counts for each module call_counts = tracker.get_call_counts() print(call_counts) # Example output: {\'Conv2d\': 1, \'ReLU\': 1, \'Linear\': 1}"},{"question":"# Profiling and Analyzing Python Code In this task, you are required to write a Python script that profiles a provided function and analyzes the collected profiling data. Part 1: Profiling a Function Write a function `profile_function` that takes another function, its arguments, and keyword arguments, and performs the following steps: 1. **Profile the Function**: Use `cProfile` to profile the execution of the provided function with the given arguments. 2. **Save the Results**: Save the profiling results to a file named `profile_results.prof`. The function signature should be: ```python def profile_function(func, *args, **kwargs): # Your code here ``` Part 2: Analyzing Profiling Data Write a second function `analyze_profile` that reads the profiling data from `profile_results.prof` and prints the profiling statistics in various ways: 1. Print the top 5 functions sorted by the total time spent executing them. 2. Print the top 5 functions sorted by the cumulative time spent (including all subcalls). 3. Print the functions that were called the most number of times. The function signature should be: ```python def analyze_profile(): # Your code here ``` Constraints and Requirements 1. **Input and Output**: - The `profile_function` function will receive a callable and its arguments to profile. - The `analyze_profile` function does not have parameters. It reads from `profile_results.prof`. 2. **Python Version**: The solution should be compatible with Python 3.10 or later. 3. **Modules to Use**: Only use the Python standard library, specifically `cProfile` and `pstats`. Example Usage ```python def example_function(): for i in range(1000): pass # Profile the example_function profile_function(example_function) # Analyze the profiling data analyze_profile() ``` Expected Output The output should consist of profiling statistics printed to the console as per the requirements in Part 2. **Note**: The profiling results will vary based on the function being profiled. Ensure that your script handles different types of functions and provides useful profiling insights.","solution":"import cProfile import pstats def profile_function(func, *args, **kwargs): Profiles the given function with provided arguments and saves the results to \'profile_results.prof\'. profiler = cProfile.Profile() profiler.enable() func(*args, **kwargs) profiler.disable() profiler.dump_stats(\\"profile_results.prof\\") def analyze_profile(): Reads the profiling data from \'profile_results.prof\' and prints the profiling statistics in various ways. stats = pstats.Stats(\'profile_results.prof\') print(\\"Top 5 functions sorted by the total time spent executing them:\\") stats.sort_stats(pstats.SortKey.TIME).print_stats(5) print(\\"nTop 5 functions sorted by the cumulative time spent (including all subcalls):\\") stats.sort_stats(pstats.SortKey.CUMULATIVE).print_stats(5) print(\\"nFunctions that were called the most number of times:\\") stats.sort_stats(pstats.SortKey.CALLS).print_stats(5)"},{"question":"You are to implement a custom neural network module in PyTorch, designed to run on TorchScript. The module should perform a specified series of operations, ensuring that it adheres to the constraints and limitations of TorchScript. # Task: 1. Define a class `MyCustomModule` that inherits from `torch.nn.Module`. 2. The module should include: - A method `__init__` which initializes two linear layers (`torch.nn.Linear`) with the appropriate dimensions. - A forward method that: - Takes an input tensor. - Passes the input tensor through the first linear layer. - Applies ReLU activation. - Passes the result through the second linear layer. - Applies a sigmoid activation to the output of the second linear layer. - Returns the result. 3. Write a function `script_my_model` that takes an instance of `MyCustomModule` and traces it using `torch.jit.script`. # Requirements: - The dimensions of the linear layers are as follows: - The first layer should map from input size of 10 to a hidden size of 5. - The second layer should map from the hidden size of 5 to an output size of 1. - Follow the constraints outlined in the documentation for TorchScript compatibility. # Input and Output Formats: - **Input:** - A tensor of shape `[N, 10]`, where `N` can vary. - The module instance for the scripting function. - **Output:** - Tensor of shape `[N, 1]` after passing through the defined operations. - The scripted module. # Example: ```python import torch class MyCustomModule(torch.nn.Module): def __init__(self): super(MyCustomModule, self).__init__() self.layer1 = torch.nn.Linear(10, 5) self.layer2 = torch.nn.Linear(5, 1) def forward(self, x): x = self.layer1(x) x = torch.relu(x) x = self.layer2(x) x = torch.sigmoid(x) return x def script_my_model(module): scripted_module = torch.jit.script(module) return scripted_module # Example usage: input_tensor = torch.randn(3, 10) model = MyCustomModule() output = model(input_tensor) scripted_model = script_my_model(model) print(output.shape) # (3, 1) ``` Implement the `MyCustomModule` class and `script_my_model` function as described.","solution":"import torch class MyCustomModule(torch.nn.Module): def __init__(self): super(MyCustomModule, self).__init__() self.layer1 = torch.nn.Linear(10, 5) self.layer2 = torch.nn.Linear(5, 1) def forward(self, x): x = self.layer1(x) x = torch.relu(x) x = self.layer2(x) x = torch.sigmoid(x) return x def script_my_model(module): scripted_module = torch.jit.script(module) return scripted_module"},{"question":"**Objective:** Write a Python function that reads data from an existing file, compresses it using the LZMA algorithm with custom filters, and writes the compressed data to a new file. This question will test your understanding of file operations, compression and decompression, and the use of custom filters with the `lzma` module. **Function Signature:** ```python def compress_file_with_custom_filters(input_file_path: str, output_file_path: str, filters: list) -> None: pass ``` **Description:** 1. The function should take three arguments: - `input_file_path` (str): The path to the input file that needs to be compressed. - `output_file_path` (str): The path where the compressed output file will be saved. - `filters` (list): A list of dictionaries specifying the custom filter chain to be used during compression. 2. The function should: - Open the given input file in binary read mode. - Read the contents of the input file. - Compress the data using the provided custom filter chain. - Write the compressed data to the specified output file in binary write mode. 3. You are required to handle any potential exceptions that may arise during file operations and the compression process, and print an appropriate error message if an exception occurs. **Constraints:** - The `input_file_path` will point to a valid file that exists. - The `output_file_path` should be a writable location. - The `filters` list will always contain at least one valid filter dictionary. **Example:** ```python filters = [ {\\"id\\": lzma.FILTER_DELTA, \\"dist\\": 4}, {\\"id\\": lzma.FILTER_LZMA2, \\"preset\\": 6} ] compress_file_with_custom_filters(\'input.txt\', \'output.xz\', filters) ``` In this example, the function reads the contents of \'input.txt\', compresses them with the specified filters, and writes the compressed data to \'output.xz\'.","solution":"import lzma def compress_file_with_custom_filters(input_file_path: str, output_file_path: str, filters: list) -> None: Compresses the contents of input_file_path using the provided custom filter chain and writes the compressed data to output_file_path. :param input_file_path: The path to the input file that needs to be compressed. :param output_file_path: The path where the compressed output file will be saved. :param filters: A list of dictionaries specifying the custom filter chain to be used during compression. try: # Read the input file contents with open(input_file_path, \'rb\') as input_file: data = input_file.read() # Compress the data using the provided filters compressed_data = lzma.compress(data, format=lzma.FORMAT_XZ, filters=filters) # Write the compressed data to the output file with open(output_file_path, \'wb\') as output_file: output_file.write(compressed_data) except Exception as e: print(f\\"An error occurred: {e}\\")"},{"question":"# Special Functions and Their Applications in PyTorch In this coding assessment, you are required to create a function that utilizes some functions from the `torch.special` module to perform a series of mathematical computations. This will be an excellent opportunity to demonstrate your knowledge and understanding of these special functions in PyTorch. # Problem Statement You need to implement a function `special_computations(tensor: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]` that takes a 1D tensor as input and returns a tuple of 4 tensors. Each tensor in the tuple is computed using different special functions from the `torch.special` module. Requirements: 1. **airy_ai_result:** Compute the Airy function of the first kind for each element in the input tensor using `torch.special.airy_ai`. 2. **bessel_j0_result:** Compute the Bessel function of the first kind for each element in the input tensor using `torch.special.bessel_j0`. 3. **i0_result:** Compute the modified Bessel function of order 0 for each element in the input tensor using `torch.special.i0`. 4. **gamma_lgamma_sum:** Compute the sum of the logarithm of the absolute value of the gamma function for each element in the input tensor using `torch.special.gammaln`. Input: - `tensor`: A 1D tensor of floating-point numbers. Output: - A tuple of four tensors: - `airy_ai_result` : A tensor containing the Airy function values. - `bessel_j0_result` : A tensor containing the Bessel function values. - `i0_result` : A tensor containing the modified Bessel function values. - `gamma_lgamma_sum` : A tensor containing the sum of the log-absolute gamma function values of the input tensor elements. # Constraints: - The input tensor will have at least one element and at most 100 elements. - The elements of the tensor will be within the range from -10 to 10. # Example ```python import torch from typing import Tuple def special_computations(tensor: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]: airy_ai_result = torch.special.airy_ai(tensor) bessel_j0_result = torch.special.bessel_j0(tensor) i0_result = torch.special.i0(tensor) gamma_lgamma_sum = torch.special.gammaln(tensor).sum().unsqueeze(0) # Summing and maintaining tensor format return airy_ai_result, bessel_j0_result, i0_result, gamma_lgamma_sum # Example usage: tensor = torch.tensor([1.0, 2.0, 3.0]) result = special_computations(tensor) for res in result: print(res) ``` In the example above, `tensor` contains values `[1.0, 2.0, 3.0]`. The function will compute the respective special functions for these values and return a tuple of tensors containing results from `airy_ai`, `bessel_j0`, `i0` and the sum of `gamma_log_abs`. Ensure your function meets the requirements and appropriately handles edge cases within the specified constraints.","solution":"import torch from typing import Tuple def special_computations(tensor: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]: airy_ai_result = torch.special.airy_ai(tensor) bessel_j0_result = torch.special.bessel_j0(tensor) i0_result = torch.special.i0(tensor) gamma_lgamma_sum = torch.special.gammaln(tensor).sum().unsqueeze(0) # Summing and maintaining tensor format return airy_ai_result, bessel_j0_result, i0_result, gamma_lgamma_sum"},{"question":"**Question: Implement and Compare Naive Bayes Classifiers** # Task You are provided with a dataset that contains different types of features, some numeric and some categorical, as well as an imbalanced class distribution. Your task is to: 1. Implement Gaussian Naive Bayes (`GaussianNB`) for the numeric features. 2. Implement Multinomial Naive Bayes (`MultinomialNB`) for the text features. 3. Implement Complement Naive Bayes (`ComplementNB`) and compare its performance with Multinomial Naive Bayes. 4. Implement Bernoulli Naive Bayes (`BernoulliNB`) for binary features. 5. Implement the Categorical Naive Bayes (`CategoricalNB`) for categorical features. 6. Use the `partial_fit` method for one of the classifiers to handle a large dataset in chunks. # Dataset You will use a fictional dataset which has the following characteristics: - Numeric features representing continuous data. - Categorical features representing categorical data (properly encoded). - Binary features represented as binary values. - Text data that can be represented as word vector counts. - Imbalanced class distribution, necessitating techniques such as Complement Naive Bayes. # Steps 1. Load the dataset and preprocess it to separate numeric, categorical, binary, and text features. Split the dataset into training and testing sets. 2. Implement Gaussian Naive Bayes on numeric features: - Train and test the model. - Report accuracy and other performance metrics. 3. Implement and compare Multinomial Naive Bayes and Complement Naive Bayes on text features: - Train and test both models. - Report and compare accuracy and other performance metrics. 4. Implement Bernoulli Naive Bayes on binary features: - Train and test the model. - Report accuracy and other performance metrics. 5. Implement Categorical Naive Bayes on categorical features: - Train and test the model. - Report accuracy and other performance metrics. 6. Use the `partial_fit` method in one of the classifiers (e.g., `MultinomialNB` with text data) to handle a large dataset in chunks: - Process the dataset in chunks. - Train and test the model incrementally. - Report accuracy and other performance metrics. # Input Format - The dataset will be provided as a pandas DataFrame where columns represent features and the target variable. # Output Format - A detailed report containing: - Implementation code for each classifier. - Accuracy and other performance metrics for each classifier. - Comparison and analysis of the results. # Constraints - Assume all necessary packages are installed. - Properly handle missing values if any. - Use appropriate techniques to deal with the imbalanced class distribution. # Performance Requirements - Your implementation should be efficient and capable of handling large datasets. # Example Usage ```python from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.naive_bayes import GaussianNB, MultinomialNB, ComplementNB, BernoulliNB, CategoricalNB # Example dataset data = load_iris() X = data[\'data\'] y = data[\'target\'] # Split the dataset X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=0) # Gaussian Naive Bayes gnb = GaussianNB() y_pred_gnb = gnb.fit(X_train, y_train).predict(X_test) print(\\"GaussianNB Accuracy: \\", (y_test == y_pred_gnb).sum() / y_test.shape[0]) # Add similar implementations for other classifiers here... ```","solution":"import pandas as pd from sklearn.model_selection import train_test_split from sklearn.datasets import make_classification from sklearn.feature_extraction.text import CountVectorizer from sklearn.naive_bayes import GaussianNB, MultinomialNB, ComplementNB, BernoulliNB, CategoricalNB from sklearn.preprocessing import LabelEncoder, OneHotEncoder, KBinsDiscretizer from sklearn.metrics import accuracy_score, classification_report # Generating a synthetic dataset with numeric, categorical, binary, and text features def generate_synthetic_data(): X_numeric, y_numeric = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, random_state=0) X_text = [\\"This is a positive review\\"] * 500 + [\\"This is a negative review\\"] * 500 X_binary = pd.DataFrame({\\"feature1\\": [1, 0] * 500, \\"feature2\\": [0, 1] * 500}) X_categorical = pd.DataFrame({\\"feature1\\": [\\"A\\", \\"B\\", \\"C\\", \\"D\\"] * 250, \\"feature2\\": [\\"W\\", \\"X\\", \\"Y\\", \\"Z\\"] * 250}) return X_numeric, X_text, X_binary, X_categorical, y_numeric # Preprocessing def preprocess_data(X_numeric, X_text, X_binary, X_categorical, y): # Train-test split X_numeric_train, X_numeric_test, y_train, y_test = train_test_split(X_numeric, y, test_size=0.2, random_state=42) X_text_train, X_text_test = X_text[:800], X_text[800:] X_binary_train, X_binary_test = X_binary[:800], X_binary[800:] X_categorical_train, X_categorical_test = X_categorical[:800], X_categorical[800:] # Text vectorization vectorizer = CountVectorizer() X_text_train_vect = vectorizer.fit_transform(X_text_train) X_text_test_vect = vectorizer.transform(X_text_test) # Categorical encoding encoder = OneHotEncoder() X_categorical_train_enc = encoder.fit_transform(X_categorical_train) X_categorical_test_enc = encoder.transform(X_categorical_test) return X_numeric_train, X_numeric_test, X_text_train_vect, X_text_test_vect, X_binary_train, X_binary_test, X_categorical_train_enc, X_categorical_test_enc, y_train, y_test # Gaussian Naive Bayes on numeric features def gaussian_nb(X_train, y_train, X_test, y_test): gnb = GaussianNB() gnb.fit(X_train, y_train) y_pred = gnb.predict(X_test) print(\\"GaussianNB Accuracy: \\", accuracy_score(y_test, y_pred)) print(classification_report(y_test, y_pred)) # Multinomial and Complement Naive Bayes on text features def multinomial_complement_nb(X_train, y_train, X_test, y_test): mnb = MultinomialNB() mnb.fit(X_train, y_train) y_pred_mnb = mnb.predict(X_test) print(\\"MultinomialNB Accuracy: \\", accuracy_score(y_test, y_pred_mnb)) print(classification_report(y_test, y_pred_mnb)) cnb = ComplementNB() cnb.fit(X_train, y_train) y_pred_cnb = cnb.predict(X_test) print(\\"ComplementNB Accuracy: \\", accuracy_score(y_test, y_pred_cnb)) print(classification_report(y_test, y_pred_cnb)) # Bernoulli Naive Bayes on binary features def bernoulli_nb(X_train, y_train, X_test, y_test): bnb = BernoulliNB() bnb.fit(X_train, y_train) y_pred = bnb.predict(X_test) print(\\"BernoulliNB Accuracy: \\", accuracy_score(y_test, y_pred)) print(classification_report(y_test, y_pred)) # Categorical Naive Bayes on categorical features def categorical_nb(X_train, y_train, X_test, y_test): cnb = CategoricalNB() cnb.fit(X_train.toarray(), y_train) # Convert sparse matrix to array if necessary y_pred = cnb.predict(X_test.toarray()) print(\\"CategoricalNB Accuracy: \\", accuracy_score(y_test, y_pred)) print(classification_report(y_test, y_pred)) # Partial fit with Multinomial Naive Bayes def partial_fit_multinomial_nb(X_train, y_train, X_test, y_test): mnb = MultinomialNB() for i in range(0, X_train.shape[0], 100): mnb.partial_fit(X_train[i:i + 100], y_train[i:i + 100], classes=[0, 1]) y_pred = mnb.predict(X_test) print(\\"Partial Fit MultinomialNB Accuracy: \\", accuracy_score(y_test, y_pred)) print(classification_report(y_test, y_pred)) # Main function to execute all tasks def main(): X_numeric, X_text, X_binary, X_categorical, y = generate_synthetic_data() X_numeric_train, X_numeric_test, X_text_train, X_text_test, X_binary_train, X_binary_test, X_categorical_train, X_categorical_test, y_train, y_test = preprocess_data(X_numeric, X_text, X_binary, X_categorical, y) print(\\"Gaussian Naive Bayes:\\") gaussian_nb(X_numeric_train, y_train, X_numeric_test, y_test) print(\\"nMultinomial Naive Bayes and Complement Naive Bayes on Text Data:\\") multinomial_complement_nb(X_text_train, y_train[:800], X_text_test, y_test) print(\\"nBernoulli Naive Bayes on Binary Data:\\") bernoulli_nb(X_binary_train, y_train[:800], X_binary_test, y_test) print(\\"nCategorical Naive Bayes on Categorical Data:\\") categorical_nb(X_categorical_train, y_train[:800], X_categorical_test, y_test) print(\\"nPartial Fit with Multinomial Naive Bayes on Text Data:\\") partial_fit_multinomial_nb(X_text_train, y_train[:800], X_text_test, y_test) if __name__ == \\"__main__\\": main()"},{"question":"Objective Implement a function that processes a series of transactions to compute the balance of each user and returns the top N users with the highest balances. Make use of the `collections` module to handle the data efficiently. Problem Statement You are provided with a list of transactions, where each transaction is a dictionary containing the following keys: - `user`: The user ID (an integer). - `amount`: The transaction amount (a float, positive for deposits and negative for withdrawals). You need to implement a function `top_balances(transactions: List[Dict[str, Any]], N: int) -> List[Tuple[int, float]]` that processes these transactions and returns a list of tuples representing the top N users with the highest balances, in descending order of their balances. Each tuple should contain the user ID and their balance. Input 1. `transactions`: A list of dictionaries, where each dictionary contains: - `user`: an integer representing the user ID. - `amount`: a float representing the transaction amount. 2. `N`: An integer representing the number of top users to return. Output A list of tuples, each containing: - An integer representing the user ID. - A float representing the user\'s balance. The list should be sorted in descending order based on the balances. If there are fewer than N users, return as many users as available. Constraints - The length of `transactions` list will not exceed 10^4. - Amounts in transactions can range from -1e6 to 1e6. - User IDs will be positive integers and can be non-sequential. Example ```python transactions = [ {\\"user\\": 1, \\"amount\\": 100.0}, {\\"user\\": 2, \\"amount\\": 200.0}, {\\"user\\": 1, \\"amount\\": -50.0}, {\\"user\\": 3, \\"amount\\": 400.0}, {\\"user\\": 2, \\"amount\\": 300.0}, ] N = 2 # Sample function call print(top_balances(transactions, N)) # Output: [(2, 500.0), (3, 400.0)] ``` Guidelines - Use `collections.defaultdict` to maintain the balances of each user. - Use `heapq.nlargest` to efficiently retrieve the top N balances. Note The solution should be optimized for performance and clarity.","solution":"from collections import defaultdict import heapq def top_balances(transactions, N): balances = defaultdict(float) for transaction in transactions: user = transaction[\\"user\\"] amount = transaction[\\"amount\\"] balances[user] += amount top_users = heapq.nlargest(N, balances.items(), key=lambda item: item[1]) return top_users"},{"question":"You have been tasked with writing a Python function that interacts with the `nis` module to perform several operations related to NIS maps. Your goal is to create a function that checks the existence of a key in a NIS map, retrieves all data from a map, lists all available maps, and fetches the default NIS domain. # Function Signature ```python def nis_operations(key: str, mapname: str) -> dict: ... ``` # Input - **key** (str): The key to search for in the NIS map. - **mapname** (str): The name of the NIS map to perform operations on. # Output A dictionary with the following keys: - **exists** (bool): True if the key exists in the map, False otherwise. - **match** (str): The value corresponding to the key in the map, or None if the key doesn\'t exist. - **all_data** (dict): Dictionary containing all key-value pairs from the map. - **all_maps** (list): List of strings containing all valid map names. - **default_domain** (str): The default NIS domain. # Constraints - This function is intended to run on Unix systems only, as the `nis` module is not available on other platforms. - Handle `nis.error` exceptions appropriately, ensuring that the function does not crash and returns relevant error information instead. # Example ```python try: result = nis_operations(\'mykey\', \'host.byname\') print(result) except Exception as e: print(f\\"Error: {e}\\") ``` Expected Output ```python { \\"exists\\": True, \\"match\\": \\"value_of_mykey\\", \\"all_data\\": {\\"key1\\": \\"value1\\", \\"key2\\": \\"value2\\", ...}, \\"all_maps\\": [\\"map1\\", \\"map2\\", ...], \\"default_domain\\": \\"nis_domain\\" } ``` *Note: The actual content of the output will depend on the specific NIS configuration and data.* # Performance Requirements - Ensure efficient NIS queries as these might be run on large datasets. - Handle any errors gracefully and make sure to provide meaningful outputs for debugging if something goes wrong. # Clarifications 1. If the `key` does not exist, the `match` should be `None`. 2. If the `nis_operations` encounters an `nis.error`, handle the exception and return a dictionary with appropriate values indicating the error.","solution":"import nis def nis_operations(key: str, mapname: str) -> dict: result = { \\"exists\\": False, \\"match\\": None, \\"all_data\\": {}, \\"all_maps\\": [], \\"default_domain\\": \\"\\" } try: # Fetch the default NIS domain default_domain = nis.get_default_domain() result[\\"default_domain\\"] = default_domain # Check if the key exists in the map and get the value try: match = nis.match(key, mapname) result[\\"exists\\"] = True result[\\"match\\"] = match except nis.error: result[\\"exists\\"] = False result[\\"match\\"] = None # Get all data from the map try: all_data = nis.cat(mapname) result[\\"all_data\\"] = all_data except nis.error: result[\\"all_data\\"] = {} # List all available maps in the default NIS domain try: all_maps = nis.maps() result[\\"all_maps\\"] = all_maps except nis.error: result[\\"all_maps\\"] = [] except nis.error as e: return {\\"error\\": str(e)} return result"},{"question":"# Priority Queue Management Using `heapq` In this task, you will be required to implement a priority queue using the `heapq` module. A priority queue is a data structure where each element has a priority, and the elements are served based on their priority (i.e., the element with the highest priority is served first). You will need to implement a class `PriorityQueue` with the following methods: 1. `__init__(self)`: Initializes an empty priority queue. 2. `add_task(self, task: Any, priority: int = 0)`: Adds a new task or updates the priority of an existing task. Each task is identified by a unique name or identifier. 3. `remove_task(self, task: Any)`: Marks an existing task as removed. If the task is not found, it raises a `KeyError`. 4. `pop_task(self) -> Any`: Removes and returns the task with the lowest priority. If the priority queue is empty, raises a `KeyError`. # Constraints and Limitations: 1. Assume all tasks are unique. 2. Use `heapq` for maintaining the heap properties. 3. Ensure that the priority queue can handle edge cases, such as removing tasks that no longer exist, and popping from an empty queue. 4. Use a marker for removed tasks that ensures it is no longer part of the active queue. 5. Do not store any task directly in the queue; instead, always use a wrapper that includes the priority and a unique entry count. # Expected Input and Output: - `add_task(self, task: Any, priority: int = 0)` - Input: A task which can be of any type (string, tuple, etc.) and its corresponding priority (default value is zero). - Output: None. - `remove_task(self, task: Any)` - Input: A task which can be of any type (string, tuple, etc.). - Output: None. Raises `KeyError` if task not found. - `pop_task(self) -> Any` - Input: None. - Output: The task with the lowest priority. Raises `KeyError` if the queue is empty. # Example: ```python pq = PriorityQueue() pq.add_task(\'write code\', priority=5) pq.add_task(\'release product\', priority=7) pq.add_task(\'write spec\', priority=1) pq.add_task(\'create tests\', priority=3) print(pq.pop_task()) # Output: \'write spec\' pq.add_task(\'update doc\', priority=2) pq.remove_task(\'create tests\') print(pq.pop_task()) # Output: \'update doc\' print(pq.pop_task()) # Output: \'write code\' pq.add_task(\'maintenance\', priority=6) print(pq.pop_task()) # Output: \'release product\' print(pq.pop_task()) # Output: \'maintenance\' ``` This question ensures a comprehensive understanding of the `heapq` module and tests the ability to manipulate heap-based priority queues to handle various tasks efficiently.","solution":"import heapq class PriorityQueue: def __init__(self): self.pq = [] # list of entries arranged in a heap self.entry_finder = {} # mapping of tasks to entries self.REMOVED = \'<removed-task>\' # placeholder for a removed task self.counter = 0 # unique sequence count def add_task(self, task, priority=0): if task in self.entry_finder: self.remove_task(task) # Mark the existing task as REMOVED self.counter += 1 entry = [priority, self.counter, task] self.entry_finder[task] = entry heapq.heappush(self.pq, entry) def remove_task(self, task): if task not in self.entry_finder: raise KeyError(f\'Task not found: {task}\') entry = self.entry_finder.pop(task) entry[-1] = self.REMOVED def pop_task(self): while self.pq: priority, count, task = heapq.heappop(self.pq) if task is not self.REMOVED: del self.entry_finder[task] return task raise KeyError(\'pop from an empty priority queue\')"},{"question":"Binary Data Manipulation and Encoding Objective: To assess students\' understanding of Python\'s binary data services, specifically using the `struct` and `codecs` modules, by requiring them to write a function that manipulates binary data and applies encoding conversions. Problem Statement: You are given a list of floating-point numbers. Your task is to: 1. Pack these numbers into a binary format using the `struct` module. 2. Encode this binary data into a base-64 encoded string using the `codecs` module. 3. Decode the base-64 string back to binary data. 4. Unpack the binary data back into floating-point numbers. Implement a function `pack_and_encode(numbers: List[float]) -> List[float]` that performs the following steps: - Packs `numbers` into binary data using little-endian byte order. - Encodes the binary data to a base-64 string. - Decodes the base-64 string back to binary data. - Unpacks the binary data to retrieve the original floating-point numbers (or an approximation thereof). Input: - `numbers`: A list of floating-point numbers. Example: [3.14, 2.718, 1.618] Output: - A list of floating-point numbers extracted from the unpacked binary data, which should approximate the original list. Constraints: - You can assume the list contains at least one floating-point number and has at most 1000 items. - Floating point values are within the typical range of precision considered for `float` in Python. Performance Requirements: - The function should efficiently handle the provided list size within reasonable computational constraints. Example: ```python from typing import List import struct import codecs def pack_and_encode(numbers: List[float]) -> List[float]: # Implement the required functionality here pass # Example usage: numbers = [3.14, 2.718, 1.618] result = pack_and_encode(numbers) print(result) # Should approximate the list: [3.14, 2.718, 1.618] ``` Notes: - Carefully handle the binary data conversions to avoid precision loss. - Use appropriate format characters for floating-point packing in the `struct` module. - Utilize standard Python encoding strategies with the `codecs` module.","solution":"from typing import List import struct import codecs def pack_and_encode(numbers: List[float]) -> List[float]: # Step 1: Pack the numbers into binary data using little-endian byte order packed_data = struct.pack(\'<\' + \'f\' * len(numbers), *numbers) # Step 2: Encode binary data to a base-64 string base64_encoded = codecs.encode(packed_data, \'base64\') # Step 3: Decode the base-64 string back to binary data decoded_data = codecs.decode(base64_encoded, \'base64\') # Step 4: Unpack the binary data back into floating-point numbers unpacked_numbers = struct.unpack(\'<\' + \'f\' * len(numbers), decoded_data) return list(unpacked_numbers)"},{"question":"# Custom Module Loader Implementation Objective: To assess your understanding of Python\'s import system, including modules, packages, finders, and loaders, design and implement a custom module loader. Requirements: 1. **Custom Module Finder**: Implement a custom module finder that can locate and load a special type of module based on custom criteria (e.g., modules with a special file extension, modules stored in a specific directory structure, etc.). 2. **Custom Module Loader**: Implement a custom module loader that handles the loading of modules found by your custom module finder. 3. **Register the Finder**: Modify the Python import system to use your custom module finder. Constraints: - Your custom module finder should handle modules with the `.custompy` file extension located in a specific directory (e.g., `/custom_modules/`). - Your custom module loader should be capable of executing the code within the `.custompy` files and populating the module\'s namespace accordingly. - Ensure that your custom module finder and loader do not interfere with the standard import system for regular Python modules. Input: - There is no direct input to your implementation. However, you will need to create a sample directory structure and sample `.custompy` files for testing purposes. Output: - Your implementation should be able to import a module with a `.custompy` extension using a standard import statement after registration of the custom finder and loader. Example: Suppose there is a file `/custom_modules/example.custompy` with the following content: ```python def greet(): return \\"Hello from custom module!\\" ``` After implementing and registering your custom finder and loader, you should be able to import and use this module as follows: ```python import example print(example.greet()) # Output: Hello from custom module! ``` Instructions: 1. Create a directory `/custom_modules/` and place one or more `.custompy` files with sample content. 2. Implement the custom module finder and loader classes. 3. Register the custom module finder with the Python import system. 4. Write code to demonstrate importing and using a custom module. Performance Requirements: - The custom loader should execute the module code efficiently and populate the module\'s namespace correctly. - The module finder should cache the found modules appropriately to avoid repeated searches for the same module. Submission: - Submit your implementation code for the custom module finder and loader. - Include a README file with instructions on setting up the environment, running the code, and verifying the functionality.","solution":"import sys import importlib.abc import importlib.util import os class CustomModuleFinder(importlib.abc.MetaPathFinder): def __init__(self, directory): self.directory = directory def find_spec(self, fullname, path, target=None): filename = os.path.join(self.directory, fullname + \\".custompy\\") if os.path.exists(filename): return importlib.util.spec_from_loader(fullname, CustomModuleLoader(filename)) return None class CustomModuleLoader(importlib.abc.Loader): def __init__(self, filename): self.filename = filename def create_module(self, spec): return None def exec_module(self, module): with open(self.filename, \'r\') as file: code = file.read() exec(code, module.__dict__) def register_custom_finder(directory): sys.meta_path.insert(0, CustomModuleFinder(directory)) # Example Usage: # Create directory `/custom_modules/` and a file `example.custompy` with sample content: # # def greet(): # return \\"Hello from custom module!\\" # # Register the custom finder for our directory custom_modules_dir = \\"/custom_modules/\\" # Modify this path as necessary register_custom_finder(custom_modules_dir) # Now you can import the module as though it were a normal Python module # import example # print(example.greet()) # Output: Hello from custom module!"},{"question":"**Objective:** Create a Python class to manipulate shared memory as a basic dictionary. **Task:** You are required to implement a class called `SharedMemoryDict` that simulates a simple dictionary using shared memory. The class should support basic dictionary operations such as setting a key-value pair, retrieving a value by key, deleting a key, and returning the keys. # Class Definition ```python class SharedMemoryDict: def __init__(self, name=None): Initialize the shared memory dictionary. If name is provided, it attaches to an existing shared memory block; otherwise, it creates a new shared memory block. :param name: Name of the shared memory block. pass def set_item(self, key, value): Set the key-value pair in the shared memory dictionary. :param key: Key to be inserted. :param value: Value to be associated with the key. Supported data types include int, float, str, and bytes. pass def get_item(self, key): Retrieve the value associated with the key from the shared memory dictionary. :param key: Key whose value is to be retrieved. :return: Value associated with the key. pass def delete_item(self, key): Delete the key-value pair from the shared memory dictionary. :param key: Key to be deleted. pass def keys(self): Retrieve all the keys in the shared memory dictionary. :return: List of keys. pass def close(self): Close the shared memory block. pass def unlink(self): Unlink the shared memory block. pass ``` # Requirements 1. **Initialization (`__init__` method)** - Create or attach to a shared memory block. - Initialize an internal structure to keep track of key-value pairs. 2. **Setting Items (`set_item` method)** - Insert or update key-value pairs in the shared memory. - Handle basic data types (`int`, `float`, `str`). 3. **Retrieving Items (`get_item` method)** - Retrieve the value for a given key from the shared memory. - Return `None` if the key does not exist. 4. **Deleting Items (`delete_item` method)** - Remove a key-value pair from the shared memory. 5. **Retrieving Keys (`keys` method)** - Return a list of all keys stored in the shared memory. 6. **Closing (`close` method)** - Closes access to the shared memory. 7. **Unlinking (`unlink` method)** - Ensure that the shared memory block is destroyed when no longer needed. # Constraints 1. The shared memory size is limited to 1024 bytes. 2. Only basic data types (`int`, `float`, `str`) are supported. 3. The length of keys and values should not exceed 100 bytes each. # Example Usage ```python from shared_memory import SharedMemoryDict # Create a new shared memory dictionary shared_dict = SharedMemoryDict() # Set some key-value pairs shared_dict.set_item(\'key1\', 100) shared_dict.set_item(\'key2\', 3.14) shared_dict.set_item(\'key3\', \'value\') # Retrieve values print(shared_dict.get_item(\'key1\')) # Output: 100 print(shared_dict.get_item(\'key2\')) # Output: 3.14 print(shared_dict.get_item(\'key3\')) # Output: \'value\' # Retrieve keys print(shared_dict.keys()) # Output: [\'key1\', \'key2\', \'key3\'] # Delete a key shared_dict.delete_item(\'key2\') # Check keys after deletion print(shared_dict.keys()) # Output: [\'key1\', \'key3\'] # Close and unlink the shared memory shared_dict.close() shared_dict.unlink() ``` **Note:** Ensure that proper error handling is implemented, such as handling cases where the key does not exist when attempting to retrieve or delete it. Also, take care of resource management by providing methods to close and unlink the shared memory block once it is no longer needed.","solution":"import pickle from multiprocessing import shared_memory class SharedMemoryDict: def __init__(self, name=None): self.size = 1024 # Shared memory size self.shm = shared_memory.SharedMemory(name=name, create=name is None, size=self.size) if name is None: self.shm.buf[:2] = b\'x00x00\' # Initialize empty dictionary self._load_data() def _load_data(self): data_bytes = self.shm.buf[:1024] if data_bytes[:2] == b\'x00x00\': self.data = {} else: self.data = pickle.loads(data_bytes.tobytes()) def _save_data(self): data_bytes = pickle.dumps(self.data) self.shm.buf[:len(data_bytes)] = data_bytes def set_item(self, key, value): if not isinstance(key, str): raise ValueError(\\"Key must be a string\\") self.data[key] = value self._save_data() def get_item(self, key): return self.data.get(key, None) def delete_item(self, key): if key in self.data: del self.data[key] self._save_data() def keys(self): return list(self.data.keys()) def close(self): self.shm.close() def unlink(self): self.shm.unlink() # Example usage if __name__ == \\"__main__\\": smd = SharedMemoryDict() smd.set_item(\'key1\', 100) smd.set_item(\'key2\', 3.14) smd.set_item(\'key3\', \'value\') print(smd.get_item(\'key1\')) # Output: 100 print(smd.keys()) # Output: [\'key1\', \'key2\', \'key3\'] smd.delete_item(\'key2\') print(smd.keys()) # Output: [\'key1\', \'key3\'] smd.close() smd.unlink()"},{"question":"**Question:** You are tasked with implementing a function in PyTorch to check if certain conditions are met to enable the persistent algorithm for performance improvement. **Function Signature:** ```python import torch def can_use_persistent_algorithm(input_tensor: torch.Tensor, gpu_type: str) -> bool: pass ``` **Input:** - `input_tensor (torch.Tensor)`: The input tensor. - `gpu_type (str)`: The type of GPU being used, represented as a string (e.g., \\"V100\\", \\"P100\\"). **Output:** - Returns `True` if all conditions are satisfied for enabling the persistent algorithm; otherwise, returns `False`. **Conditions to check:** 1. cudnn is enabled. 2. The input data is on the GPU. 3. The input data has dtype `torch.float16`. 4. The GPU type is \\"V100\\". 5. The input data is not in `PackedSequence` format (Assume input_tensor is not in PackedSequence format for simplicity). **Example:** ```python # Example input tensor input_tensor = torch.tensor([1.0, 2.0], dtype=torch.float16, device=\'cuda\') # Assuming the current GPU type is \'V100\' gpu_type = \\"V100\\" # Function call should return True as all conditions are met print(can_use_persistent_algorithm(input_tensor, gpu_type)) # Expected Output: True ``` You are also encouraged to test the function with other cases where conditions might not be met. For instance, test with data types other than `torch.float16`, CPU tensors, or different GPU types like \\"P100\\". **Notes:** - Ensure that you have CUDA and cudnn installed and properly configured for the best results. - Assume that the input data is not in `PackedSequence` format for simplicity. - You may use the function `torch.backends.cudnn.enabled` to check if cudnn is enabled. **Hint:** To check if the tensor device is CUDA, you can use `input_tensor.is_cuda`.","solution":"import torch def can_use_persistent_algorithm(input_tensor: torch.Tensor, gpu_type: str) -> bool: Check if conditions are met to enable persistent algorithm for performance improvement. Conditions: 1. cudnn is enabled. 2. The input data is on the GPU. 3. The input data has dtype torch.float16. 4. The GPU type is \\"V100\\". 5. The input data is not in PackedSequence format (assumed for simplicity). Args: input_tensor (torch.Tensor): The input tensor. gpu_type (str): The type of GPU being used. Returns: bool: True if all conditions are satisfied, otherwise False. if not torch.backends.cudnn.enabled: return False if not input_tensor.is_cuda: return False if input_tensor.dtype != torch.float16: return False if gpu_type != \\"V100\\": return False return True"},{"question":"As a data analyst, you are required to visualize a dataset using Seaborn, and you want to explore different color palettes to enhance the visual representation of your data. Your task is to write a function that creates and saves several visualizations of a given dataset using various color palettes and colormaps available in Seaborn. Function Signature ```python def create_visualizations(data: pd.DataFrame, x_col: str, y_col: str) -> None: ``` Input 1. `data`: A pandas DataFrame containing the dataset to visualize. 2. `x_col`: A string representing the column name to be used for the x-axis. 3. `y_col`: A string representing the column name to be used for the y-axis. Output The function should create and save the following visualizations: 1. A scatter plot using the default color cycle. 2. A scatter plot using the \\"pastel\\" color palette. 3. A line plot using the \\"husl\\" system with 10 specified colors. 4. A bar plot with the \\"Set2\\" color palette. 5. A heatmap with the \\"Spectral\\" diverging colormap. 6. A scatter plot with a reversed dark sequential gradient. Each visualization should be saved as a PNG file with appropriate filenames indicating the color palette used (e.g., `scatter_default.png`, `scatter_pastel.png`, `line_husl.png`, `bar_set2.png`, `heatmap_spectral.png`, `scatter_sequential_dark.png`). # Constraints - Ensure the resulting visualizations are labeled appropriately, including titles, axis labels, and legends (if applicable). - Handle any edge cases where the data might not be sufficient for creating a particular type of plot. Example ```python import pandas as pd # Sample data data = pd.DataFrame({ \'x\': range(10), \'y\': [3, 2, 4, 5, 6, 3, 4, 7, 8, 6] }) create_visualizations(data, \'x\', \'y\') ``` This should create and save several PNG files with different visualizations using various color palettes provided by Seaborn.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def create_visualizations(data: pd.DataFrame, x_col: str, y_col: str) -> None: # Scatter plot using the default color cycle plt.figure() sns.scatterplot(data=data, x=x_col, y=y_col) plt.title(\'Scatter Plot - Default Palette\') plt.xlabel(x_col) plt.ylabel(y_col) plt.savefig(\'scatter_default.png\') # Scatter plot using the \\"pastel\\" color palette plt.figure() sns.scatterplot(data=data, x=x_col, y=y_col, palette=\'pastel\') plt.title(\'Scatter Plot - Pastel Palette\') plt.xlabel(x_col) plt.ylabel(y_col) plt.savefig(\'scatter_pastel.png\') # Line plot using the \\"husl\\" system with 10 specified colors plt.figure() sns.lineplot(data=data, x=x_col, y=y_col, palette=sns.color_palette(\'husl\', 10)) plt.title(\'Line Plot - Husl Palette\') plt.xlabel(x_col) plt.ylabel(y_col) plt.savefig(\'line_husl.png\') # Bar plot with the \\"Set2\\" color palette plt.figure() sns.barplot(data=data, x=x_col, y=y_col, palette=\'Set2\') plt.title(\'Bar Plot - Set2 Palette\') plt.xlabel(x_col) plt.ylabel(y_col) plt.savefig(\'bar_set2.png\') # Heatmap with the \\"Spectral\\" diverging colormap # Ensure the data is suitable for a heatmap if len(data.columns) >= 2 and data.shape[0] >= 2: plt.figure() heatmap_data = data[[x_col, y_col]].pivot_table(index=x_col, columns=y_col, aggfunc=\'size\', fill_value=0) sns.heatmap(data=heatmap_data, cmap=\'Spectral\') plt.title(\'Heatmap - Spectral Colormap\') plt.xlabel(y_col) plt.ylabel(x_col) plt.savefig(\'heatmap_spectral.png\') # Scatter plot with a reversed dark sequential gradient plt.figure() cmap = sns.color_palette(\'rocket\', as_cmap=True) sns.scatterplot(data=data, x=x_col, y=y_col, palette=cmap.reversed()) plt.title(\'Scatter Plot - Reversed Dark Sequential Gradient\') plt.xlabel(x_col) plt.ylabel(y_col) plt.savefig(\'scatter_sequential_dark.png\')"},{"question":"# Python Coding Assessment: Closure Variables and Scopes Objective You are required to write a function that demonstrates your understanding of closures and the use of non-local variables in Python. The function should mimic the behavior of \\"Cell\\" objects described in the documentation, specifically focusing on handling variables across multiple scopes. Task Write a Python function `create_counter(start: int)` that returns two inner functions, `get_count()` and `increment_count()`. These functions should share and manipulate a common counter variable, ensuring its state is preserved between calls. - `get_count()`: This function should return the current value of the counter. - `increment_count()`: This function should increment the counter by one and return the new value. Requirements 1. **Input:** The function `create_counter` takes a single integer `start` which initializes the counter. 2. **Output:** `create_counter` returns two functions `get_count` and `increment_count`. Example ```python counter1_get, counter1_inc = create_counter(10) print(counter1_get()) # Output: 10 print(counter1_inc()) # Output: 11 print(counter1_get()) # Output: 11 counter2_get, counter2_inc = create_counter(5) print(counter2_get()) # Output: 5 print(counter2_inc()) # Output: 6 print(counter2_get()) # Output: 6 ``` Constraints - The initial counter value provided to `create_counter` will always be a non-negative integer. - Ensure that the state of the counter is shared correctly between `get_count` and `increment_count` functions. - Performance considerations are minimal for this task as the operations involved are constant-time. Your implementation should focus on properly encapsulating the counter variable and ensuring its state persists between function calls.","solution":"def create_counter(start: int): Returns two functions: get_count and increment_count that share and manipulate a common counter variable. count = start def get_count(): Returns the current value of the counter. return count def increment_count(): Increments the counter by one and returns the new value. nonlocal count count += 1 return count return get_count, increment_count"},{"question":"Exploring KDE Plots with Seaborn **Objective:** In this task, you are required to demonstrate your understanding of the seaborn `kdeplot` function by creating specific plots. You will work with the provided dataset and generate several KDE plots that meet the given criteria. **Instructions:** 1. **Load the Dataset:** - Use the seaborn `load_dataset` function to load the `geyser` dataset. 2. **Basic Univariate KDE Plot:** - Create a KDE plot for the `waiting` column on the x-axis. - Apply a bandwidth adjustment factor of `0.5`. - Customize the plot to use the color `royalblue`. 3. **Conditional KDE Plot:** - Create a KDE plot for the `waiting` column, conditioned on the `kind` column. - Use the `multiple=\'stack\'` option to stack the conditional distributions. - Set the plot to have a transparency level (`alpha`) of `0.6`. 4. **Bivariate KDE Plot:** - Create a bivariate KDE plot for the `waiting` and `duration` columns. - Add the `kind` column as a hue semantic to show conditional distributions. - Use filled contours (`fill=True`), and set `levels=15`. 5. **Customized Bivariate KDE Plot with Log Scale:** - Create a bivariate KDE plot for the `waiting` and `duration` columns with `fill=True` and `thresh=0`. - Set the color map to `rocket`. - Apply log scaling to the `waiting` column. **Requirements:** - Your code should be clear and well-documented. - Each generated plot should be displayed inline. - Ensure to include titles and labels for each plot for better readability. **Expected Output:** - Four distinct KDE plots as described above. - Each plot should meet the specified customization and conditions. **Additional Information:** - You can refer to the seaborn documentation at any time if necessary. - Remember to import necessary libraries such as seaborn and matplotlib. **Example Code Snippet:** ```python import seaborn as sns import matplotlib.pyplot as plt # Load the dataset geyser = sns.load_dataset(\\"geyser\\") # Basic Univariate KDE Plot plt.figure(figsize=(10, 6)) sns.kdeplot(data=geyser, x=\\"waiting\\", bw_adjust=0.5, color=\\"royalblue\\") plt.title(\\"Univariate KDE Plot of Waiting Time\\") plt.xlabel(\\"Waiting\\") plt.show() # Conditional KDE Plot plt.figure(figsize=(10, 6)) sns.kdeplot(data=geyser, x=\\"waiting\\", hue=\\"kind\\", multiple=\\"stack\\", alpha=0.6) plt.title(\\"Conditional KDE Plot of Waiting Time by Kind\\") plt.xlabel(\\"Waiting\\") plt.show() # Bivariate KDE Plot plt.figure(figsize=(10, 6)) sns.kdeplot(data=geyser, x=\\"waiting\\", y=\\"duration\\", hue=\\"kind\\", fill=True, levels=15) plt.title(\\"Bivariate KDE Plot of Waiting Time and Duration\\") plt.xlabel(\\"Waiting\\") plt.ylabel(\\"Duration\\") plt.show() # Customized Bivariate KDE Plot with Log Scale plt.figure(figsize=(10, 6)) sns.kdeplot(data=geyser, x=\\"waiting\\", y=\\"duration\\", fill=True, thresh=0, cmap=\\"rocket\\", log_scale=(True, False)) plt.title(\\"Bivariate KDE Plot with Log Scaled Waiting Time\\") plt.xlabel(\\"Waiting (Log Scale)\\") plt.ylabel(\\"Duration\\") plt.show() ``` **Note:** The example code snippet is a partial implementation. Ensure to complete the full implementation adhering to the instructions provided above.","solution":"import seaborn as sns import matplotlib.pyplot as plt # Load the dataset geyser = sns.load_dataset(\\"geyser\\") # Basic Univariate KDE Plot plt.figure(figsize=(10, 6)) sns.kdeplot(data=geyser, x=\\"waiting\\", bw_adjust=0.5, color=\\"royalblue\\") plt.title(\\"Univariate KDE Plot of Waiting Time\\") plt.xlabel(\\"Waiting\\") plt.show() # Conditional KDE Plot plt.figure(figsize=(10, 6)) sns.kdeplot(data=geyser, x=\\"waiting\\", hue=\\"kind\\", multiple=\\"stack\\", alpha=0.6) plt.title(\\"Conditional KDE Plot of Waiting Time by Kind\\") plt.xlabel(\\"Waiting\\") plt.show() # Bivariate KDE Plot plt.figure(figsize=(10, 6)) sns.kdeplot(data=geyser, x=\\"waiting\\", y=\\"duration\\", hue=\\"kind\\", fill=True, levels=15) plt.title(\\"Bivariate KDE Plot of Waiting Time and Duration\\") plt.xlabel(\\"Waiting\\") plt.ylabel(\\"Duration\\") plt.show() # Customized Bivariate KDE Plot with Log Scale plt.figure(figsize=(10, 6)) sns.kdeplot(data=geyser, x=\\"waiting\\", y=\\"duration\\", fill=True, thresh=0, cmap=\\"rocket\\", log_scale=(True, False)) plt.title(\\"Bivariate KDE Plot with Log Scaled Waiting Time\\") plt.xlabel(\\"Waiting (Log Scale)\\") plt.ylabel(\\"Duration\\") plt.show()"},{"question":"# Python Programming Assessment Question Task You are required to create a **Python Library Management System** that handles books in a library. The system should allow users to add new books, borrow books, return books, and display information about books. The solution should demonstrate the use of classes, data structures, input/output operations, and error handling. Requirements 1. **Class Design** - Define a class `Book` with the following attributes: - `title`: A string representing the title of the book. - `author`: A string representing the author of the book. - `isbn`: A string representing the ISBN number of the book. - `is_borrowed`: A boolean indicating whether the book is currently borrowed or not. - Define a class `Library` with the following methods: - `add_book(self, book: Book)`: Adds a new book to the library. - `borrow_book(self, isbn: str)`: Marks a book as borrowed using its ISBN number. Raise an exception if the book is not available. - `return_book(self, isbn: str)`: Marks a book as returned using its ISBN number. - `display_books(self)`: Displays all books in the library with their details. 2. **Data Structures** - Use a list to maintain the collection of books in the library. - Use a dictionary to map ISBN numbers to book instances for quick access. 3. **Input/Output Operations** - Implement input and output operations for adding, borrowing, and returning books. 4. **Error Handling** - Handle cases where a book with a given ISBN does not exist in the library. - Handle cases where a book is already borrowed and cannot be borrowed again. Input and Output Formats - **Input**: A sequence of commands to the library system. Each command can be one of the following: - `add <title> <author> <isbn>`: To add a new book. - `borrow <isbn>`: To borrow a book with the given ISBN. - `return <isbn>`: To return a book with the given ISBN. - `display`: To display all books in the library. - **Output**: The system should print appropriate messages for each command, e.g., confirmation messages for adding, borrowing, and returning books, and an informative list of books when displaying. Constraints - The ISBN number of the books are unique strings. - The titles and authors do not contain spaces (for simplicity in this exercise). Example ```python # Example interaction with the library system library = Library() # Adding books to the library library.add_book(Book(\\"1984\\", \\"GeorgeOrwell\\", \\"1234567890\\")) library.add_book(Book(\\"ToKillAMockingbird\\", \\"HarperLee\\", \\"0987654321\\")) # Borrowing a book try: library.borrow_book(\\"1234567890\\") except Exception as e: print(e) # Displaying books library.display_books() # Returning a book library.return_book(\\"1234567890\\") # Displaying books after return library.display_books() ``` Expected output: ``` Book added: 1984 by GeorgeOrwell, ISBN: 1234567890 Book added: ToKillAMockingbird by HarperLee, ISBN: 0987654321 Book \'1984\' borrowed successfully. Title: 1984, Author: GeorgeOrwell, ISBN: 1234567890, Borrowed: True Title: ToKillAMockingbird, Author: HarperLee, ISBN: 0987654321, Borrowed: False Book \'1984\' returned successfully. Title: 1984, Author: GeorgeOrwell, ISBN: 1234567890, Borrowed: False Title: ToKillAMockingbird, Author: HarperLee, ISBN: 0987654321, Borrowed: False ``` Notes - Your implementation should be able to handle multiple operations in sequence. - Ensure to include appropriate error messages for invalid operations, such as trying to borrow a book that does not exist or is already borrowed.","solution":"class Book: def __init__(self, title, author, isbn): self.title = title self.author = author self.isbn = isbn self.is_borrowed = False def __str__(self): return f\\"Title: {self.title}, Author: {self.author}, ISBN: {self.isbn}, Borrowed: {self.is_borrowed}\\" class Library: def __init__(self): self.books = [] self.books_by_isbn = {} def add_book(self, book): if book.isbn in self.books_by_isbn: raise ValueError(\\"A book with this ISBN already exists in the library.\\") self.books.append(book) self.books_by_isbn[book.isbn] = book print(f\\"Book added: {book.title} by {book.author}, ISBN: {book.isbn}\\") def borrow_book(self, isbn): if isbn not in self.books_by_isbn: raise ValueError(\\"The book with this ISBN does not exist in the library.\\") book = self.books_by_isbn[isbn] if book.is_borrowed: raise ValueError(f\\"The book \'{book.title}\' is already borrowed.\\") book.is_borrowed = True print(f\\"Book \'{book.title}\' borrowed successfully.\\") def return_book(self, isbn): if isbn not in self.books_by_isbn: raise ValueError(\\"The book with this ISBN does not exist in the library.\\") book = self.books_by_isbn[isbn] if not book.is_borrowed: raise ValueError(f\\"The book \'{book.title}\' was not borrowed.\\") book.is_borrowed = False print(f\\"Book \'{book.title}\' returned successfully.\\") def display_books(self): for book in self.books: print(book)"},{"question":"Given the provided documentation on the `mailbox` module, your task is to implement a function to sort messages by their read/unread status. The function should: 1. Accept a `mailbox` path and a `destination_path` as input. 2. Iterate over the messages in the provided mailbox. 3. Separate the messages into two categories: read and unread. 4. Copy the read messages to a `read` subdirectory and unread messages to an `unread` subdirectory in the `destination_path`. 5. Maintain the original state of the messages while copying. # Constraints: - The provided mailbox can be in any supported format: Maildir, mbox, MH, Babyl, or MMDF. - Ensure that mailbox modifications do not conflict with other processes accessing the same mailbox concurrently. # Input: - `mailbox_path`: A string representing the path to the mailbox. - `destination_path`: A string representing the path to the destination directory. # Expected Output: - None (the function operates by copying messages to the appropriate subdirectories). # Function Signature ```python def sort_mail_by_read_status(mailbox_path: str, destination_path: str) -> None: pass ``` # Example: ```python # Example invocation sort_mail_by_read_status(\'/path/to/source/mailbox\', \'/path/to/destination\') ``` # Note: - Use appropriate locking mechanisms as detailed in the documentation to prevent data corruption. - Consider the various flags and attributes used by different mailbox formats to determine whether a message is read or unread. - Handle exceptions gracefully, such as when accessing malformed messages or encountering other mailbox errors.","solution":"import mailbox import os import shutil def sort_mail_by_read_status(mailbox_path: str, destination_path: str) -> None: Sorts messages by their read/unread status from the given mailbox path. Copies read messages to \'read\' subdirectory and unread messages to \'unread\' subdirectory in the destination path. # Ensure the destination directories exist read_dir = os.path.join(destination_path, \'read\') unread_dir = os.path.join(destination_path, \'unread\') os.makedirs(read_dir, exist_ok=True) os.makedirs(unread_dir, exist_ok=True) # Create mailbox object based on provided mailbox path mbox = mailbox.Maildir(mailbox_path) # Identifies the type of mailbox based on the extension and returns appropriate object for key, msg in mbox.items(): # Check if the status of the message indicates \'read\' (\'S\' is usually the flag for \'seen\') if \'S\' in msg.get_flags(): dest_dir = read_dir else: dest_dir = unread_dir dest_filename = os.path.join(dest_dir, key) with open(dest_filename, \'wb\') as dest_file: dest_file.write(bytes(msg)) # Maintain the original state of the messages while copying dest_filename += \':2,\' + \'\'.join(sorted(msg.get_flags())) os.rename(os.path.join(dest_dir, key), dest_filename) # Renaming the file to include flags mbox.close()"},{"question":"Coding Assessment Question # Objective You are required to demonstrate your understanding of dimensionality reduction techniques in scikit-learn, specifically PCA and Feature Agglomeration. You will apply these techniques to a high-dimensional dataset and compare their performance. # Problem Statement Given a dataset with 1000 samples and 100 features generated randomly, perform the following tasks: 1. **Preprocess the Data**: Standardize the features of the dataset. 2. **PCA Transformation**: Reduce the dataset to 10 principal components using PCA and explain the amount of variance captured by these components. 3. **Feature Agglomeration**: Reduce the dataset to 10 features using Feature Agglomeration and explain the rationale behind grouping features together. 4. **Comparison**: Compare the results of PCA and Feature Agglomeration in terms of the features\' variance and explain which method might be more appropriate in which scenario. # Input and Output Formats - Input: A DataFrame `X` with shape (1000, 100) containing the dataset. - Output: The transformed datasets from PCA and Feature Agglomeration, along with explanations in the form of comments or markdown cells. # Constraints - Use scikit-learn for all operations. - Ensure to use `StandardScaler` for standardizing the data before applying dimensionality reduction techniques. # Performance Requirements - Your code should run efficiently on a dataset with 1000 samples and 100 features. - Ensure to use appropriate scikit-learn functions and classes for each task. # Detailed Steps 1. **Preprocess the Data**: Use `StandardScaler` to standardize the features of the dataset. 2. **PCA Transformation**: - Use `sklearn.decomposition.PCA` to reduce the dataset to 10 principal components. - Fit the PCA model and transform the dataset. - Calculate and print the amount of variance captured by these 10 principal components. 3. **Feature Agglomeration**: - Use `sklearn.cluster.FeatureAgglomeration` to reduce the dataset to 10 features. - Fit the Feature Agglomeration model and transform the dataset. - Provide an explanation of the resulting feature groups. 4. **Comparison**: - Compare the variance captured by the PCA components with the rationale behind the Feature Agglomeration groups. - Discuss which method is more appropriate depending on the use case. # Example Code Template ```python import numpy as np import pandas as pd from sklearn.preprocessing import StandardScaler from sklearn.decomposition import PCA from sklearn.cluster import FeatureAgglomeration # Generate a random dataset np.random.seed(42) X = pd.DataFrame(np.random.randn(1000, 100)) # Step 1: Preprocess the Data scaler = StandardScaler() X_scaled = scaler.fit_transform(X) # Step 2: PCA Transformation pca = PCA(n_components=10) X_pca = pca.fit_transform(X_scaled) variance_explained = pca.explained_variance_ratio_.sum() print(\\"Variance captured by PCA:\\", variance_explained) # Step 3: Feature Agglomeration agglo = FeatureAgglomeration(n_clusters=10) X_agglo = agglo.fit_transform(X_scaled) # Explanation of Feature Agglomeration result # (Provide an explanation here) print(\\"Feature Agglomeration performed.\\") # Step 4: Comparison # (Discuss the comparison here) ``` **Note**: Make sure to provide explanations and comparisons to complete the requirement.","solution":"import numpy as np import pandas as pd from sklearn.preprocessing import StandardScaler from sklearn.decomposition import PCA from sklearn.cluster import FeatureAgglomeration def dimensionality_reduction_analysis(X): Perform PCA and Feature Agglomeration on the input dataset. Parameters: X (pd.DataFrame): Input dataset with shape (1000, 100) containing the dataset. Returns: dict: Dictionary containing reduced datasets and variance explained. # Step 1: Preprocess the Data scaler = StandardScaler() X_scaled = scaler.fit_transform(X) # Step 2: PCA Transformation pca = PCA(n_components=10) X_pca = pca.fit_transform(X_scaled) variance_explained = pca.explained_variance_ratio_.sum() # Step 3: Feature Agglomeration agglo = FeatureAgglomeration(n_clusters=10) X_agglo = agglo.fit_transform(X_scaled) return { \\"X_pca\\": X_pca, \\"variance_explained\\": variance_explained, \\"X_agglo\\": X_agglo }"},{"question":"Objective: Write a Python function that reads a file containing binary data, processes the data, and returns specific statistical information about the values. This task will help you demonstrate your ability to work with arrays, handle file I/O, and perform operations efficiently. Details: You need to implement the following function: ```python import array def process_array_data(file_path: str, typecode: str) -> dict: Reads binary data from a file, creates an array of the specified typecode, and returns a dictionary with statistical information about the values. Args: file_path (str): The path to the binary file containing the data. typecode (str): The typecode representing the type of elements stored in the array. Returns: dict: A dictionary containing the following statistical information: - \'count\': The number of elements in the array. - \'mean\': The mean (average) of the array\'s elements. - \'min\': The minimum value in the array. - \'max\': The maximum value in the array. # Your implementation here ``` Input Format: - `file_path`: A string representing the path to the binary file containing the data. - `typecode`: A string representing the typecode of the elements in the array as specified in the provided documentation. Output Format: - Return a dictionary containing the following keys: - `count`: An integer representing the total number of elements in the array. - `mean`: A float representing the mean of the array\'s elements. - `min`: The minimum value in the array. - `max`: The maximum value in the array. Constraints: - The typecode must be one of the valid type codes specified in the documentation. - The file will contain data consistent with the specified typecode. - If the array is empty, return `None` for `mean`, `min`, and `max`. Example: Given a binary file `data.bin` containing the following signed int values [1, 2, 3, 4, 5], and the typecode `\'i\'` (signed integer): ```python result = process_array_data(\'data.bin\', \'i\') print(result) # Expected Output: # { # \'count\': 5, # \'mean\': 3.0, # \'min\': 1, # \'max\': 5 # } ``` Additional Notes: - You may assume that the file content is valid and consistent with the typecode provided. - You should use appropriate array methods to read from the file and process the data efficiently. - Consider edge cases such as an empty file or a file with a single data point.","solution":"import array import os def process_array_data(file_path: str, typecode: str) -> dict: Reads binary data from a file, creates an array of the specified typecode, and returns a dictionary with statistical information about the values. Args: file_path (str): The path to the binary file containing the data. typecode (str): The typecode representing the type of elements stored in the array. Returns: dict: A dictionary containing the following statistical information: - \'count\': The number of elements in the array. - \'mean\': The mean (average) of the array\'s elements. - \'min\': The minimum value in the array. - \'max\': The maximum value in the array. with open(file_path, \'rb\') as file: data_array = array.array(typecode) data_array.fromfile(file, os.path.getsize(file_path) // data_array.itemsize) # If the array is empty, return None for mean, min, and max if not data_array: return {\'count\': 0, \'mean\': None, \'min\': None, \'max\': None} count = len(data_array) mean = sum(data_array) / count min_val = min(data_array) max_val = max(data_array) return { \'count\': count, \'mean\': mean, \'min\': min_val, \'max\': max_val }"},{"question":"# Python Coding Assessment Question Objective To test the ability to handle file operations and pattern matching using Unix-style \\"glob\\" patterns and to implement a simplified version of the **sdist** command\'s functionality. Problem Statement You are tasked with implementing a function `generate_file_list` that mimics some functionalities of the **sdist** command by including and excluding files based on specific patterns. Implement the function: `generate_file_list` ```python def generate_file_list(root_dir, actions): Generate a list of filenames from the root_dir based on the specified actions. Parameters: root_dir (str): The root directory from where to start processing the files. actions (list of tuples): A list of actions where each action is a tuple. Supported actions: - (\\"include\\", [pattern1, pattern2, ...]) - (\\"exclude\\", [pattern1, pattern2, ...]) - (\\"recursive-include\\", \\"dir\\", [pattern1, pattern2, ...]) - (\\"recursive-exclude\\", \\"dir\\", [pattern1, pattern2, ...]) - (\\"global-include\\", [pattern1, pattern2, ...]) - (\\"global-exclude\\", [pattern1, pattern2, ...]) - (\\"prune\\", \\"dir\\") - (\\"graft\\", \\"dir\\") Returns: list: A list of included files based on the actions. pass ``` Input Parameters: - `root_dir` (str): The root directory path from which to start including or excluding files. - `actions` (list of tuples): Each tuple represents an action and contains: - The type of action (str): \\"include\\", \\"exclude\\", \\"recursive-include\\", \\"recursive-exclude\\", \\"global-include\\", \\"global-exclude\\", \\"prune\\", or \\"graft\\". - One or two additional elements specifying: - Patterns (list of str): For \\"include\\", \\"exclude\\", \\"recursive-include\\", \\"recursive-exclude\\", \\"global-include\\", \\"global-exclude\\". - Directory path (str): For \\"recursive-include\\", \\"recursive-exclude\\", \\"prune\\", \\"graft\\". Output: - A list of filenames that are included based on the applied actions. Constraints: - Patterns follow Unix-style \\"glob\\" syntax. - Directory paths are relative to the `root_dir`. - Efficiently handle large number of files and directories. Example Usage: ```python root_dir = \\"path/to/root\\" actions = [ (\\"include\\", [\\"*.py\\"]), (\\"exclude\\", [\\"test_*.py\\"]), (\\"recursive-include\\", \\"src\\", [\\"*.py\\", \\"*.md\\"]), (\\"recursive-exclude\\", \\"src/tests\\", [\\"*.py\\"]), (\\"global-include\\", [\\"README.md\\"]), (\\"global-exclude\\", [\\"*.log\\"]), (\\"prune\\", \\"build\\"), (\\"graft\\", \\"data\\") ] result = generate_file_list(root_dir, actions) print(result) ``` In the example above, the final list of files should include: - All Python files (`*.py`) from the `root_dir`, except those matching `test_*.py`. - All Python (`*.py`) and Markdown (`*.md`) files within the `src` directory, except those within `src/tests`. - Specifically include `README.md` from anywhere within `root_dir`. - Exclude all log files (`*.log`) from anywhere within `root_dir`. - Exclude all files under the `build` directory. - Include all files under the `data` directory. Notes: - The `generate_file_list` function should construct the list of files by traversing the directory structure starting from `root_dir` and applying the specified actions in the given order. - Use appropriate libraries and methods to handle file pattern matching and directory traversal efficiently.","solution":"import os import fnmatch def generate_file_list(root_dir, actions): def find_matching_files(directory, patterns): matched_files = [] for root, _, files in os.walk(directory): for pattern in patterns: for file in fnmatch.filter(files, pattern): matched_files.append(os.path.relpath(os.path.join(root, file), root_dir)) return matched_files included_files = set() excluded_files = set() for action in actions: act_type = action[0] if act_type == \\"include\\": patterns = action[1] included_files.update(find_matching_files(root_dir, patterns)) elif act_type == \\"exclude\\": patterns = action[1] excluded_files.update(find_matching_files(root_dir, patterns)) elif act_type == \\"recursive-include\\": dir_path, patterns = action[1], action[2] sub_dir = os.path.join(root_dir, dir_path) included_files.update(find_matching_files(sub_dir, patterns)) elif act_type == \\"recursive-exclude\\": dir_path, patterns = action[1], action[2] sub_dir = os.path.join(root_dir, dir_path) excluded_files.update(find_matching_files(sub_dir, patterns)) elif act_type == \\"global-include\\": patterns = action[1] included_files.update(find_matching_files(root_dir, patterns)) elif act_type == \\"global-exclude\\": patterns = action[1] excluded_files.update(find_matching_files(root_dir, patterns)) elif act_type == \\"prune\\": dir_path = action[1] sub_dir = os.path.join(root_dir, dir_path) excluded_files.update({os.path.relpath(os.path.join(root, file), root_dir) for root, _, files in os.walk(sub_dir) for file in files}) elif act_type == \\"graft\\": dir_path = action[1] sub_dir = os.path.join(root_dir, dir_path) included_files.update({os.path.relpath(os.path.join(root, file), root_dir) for root, _, files in os.walk(sub_dir) for file in files}) final_files = included_files - excluded_files return sorted(final_files)"},{"question":"# Distributed Multiprocessing with PyTorch **Problem Statement:** You are tasked with creating a distributed system that performs parallel computations using PyTorch\'s `torch.distributed.elastic.multiprocessing` module. Your goal is to start multiple worker processes that will each perform a simple computation and return their result. You must collect the results from all processes and aggregate them. **Required Implementation:** 1. Implement a function `start_worker_processes(num_workers: int, computation_task: Callable[[], float]) -> float` that: - Uses `torch.distributed.elastic.multiprocessing.start_processes` to start `num_workers` worker processes. - Each worker process should perform the computation specified by `computation_task` and return the result. - Aggregates the results from all worker processes by summing them up and returns the total sum. 2. The `computation_task` function should be a simple callable provided to `start_worker_processes` that returns a float. **Function Signature:** ```python import torch.distributed.elastic.multiprocessing as edist def start_worker_processes(num_workers: int, computation_task: Callable[[], float]) -> float: # Your implementation here pass ``` **Input:** - `num_workers (int)`: The number of worker processes to start. - `computation_task (Callable[[], float])`: A callable that represents the computation each worker process will perform and return a float value. **Output:** - `float`: The total sum of the results from all worker processes. **Constraints:** - You must use `torch.distributed.elastic.multiprocessing.start_processes` to manage worker processes. - Each worker process must independently execute `computation_task`. - Make sure to handle process lifecycle management correctly. **Example:** ```python import random def example_computation(): return random.uniform(0, 10) total_sum = start_worker_processes(4, example_computation) print(total_sum) # Output: Sum of 4 random floats between 0 and 10 ``` In this example, `start_worker_processes` should start 4 processes, each executing `example_computation`, and return the sum of the results. **Notes:** - Ensure that your implementation handles process synchronization and result aggregation correctly. - Consider potential issues with process management and handle them gracefully. **Assessment Criteria:** - Correct usage of `torch.distributed.elastic.multiprocessing`. - Proper handling of worker processes and their results. - Efficient and synchronized aggregation of results from all workers. Implement the function `start_worker_processes` to complete this task.","solution":"import torch.distributed.elastic.multiprocessing as edist import torch.multiprocessing as mp from typing import Callable def worker_func(queue, computation_task: Callable[[], float]): result = computation_task() queue.put(result) def start_worker_processes(num_workers: int, computation_task: Callable[[], float]) -> float: # Create a shared queue to collect results queue = mp.Manager().Queue() # Start the worker processes processes = [] for _ in range(num_workers): p = mp.Process(target=worker_func, args=(queue, computation_task)) p.start() processes.append(p) # Collect results from the queue results = [] for _ in range(num_workers): results.append(queue.get()) # Ensure all processes have finished for p in processes: p.join() # Aggregate results using sum total_sum = sum(results) return total_sum"},{"question":"# Question You are tasked with analyzing a dataset and presenting the findings using visually appealing plots. Using Seaborn, you are to create and customize a series of plots to demonstrate your understanding of controlling figure aesthetics. Dataset You can use the built-in `penguins` dataset from the Seaborn library. Load the dataset using: ```python import seaborn as sns penguins = sns.load_dataset(\'penguins\') ``` Requirements 1. **Theme Customization:** - Create a function `plot_penguins_distributions()` that plots the distributions of `flipper_length_mm` for each species of penguin using a boxplot. - Use the `whitegrid` theme for this plot. - Remove the top and right spines of the plot. 2. **Styling with Context:** - Create a function `plot_penguins_scatter()` that plots a scatter plot of `flipper_length_mm` vs `body_mass_g`, with different colors for each species. - Use the `darkgrid` theme and the context `poster`. - Increase the font scale to 1.5. 3. **Temporarily Setting Style:** - Create a function `plot_multiple_styles()` that plots three different plots side-by-side: - The first subplot should use the `darkgrid` theme. - The second subplot should use the `white` theme. - The third subplot should use the `ticks` theme. - Use the `axes_style` function in a `with` statement to set the different themes temporarily. 4. **Visualization and Insights:** - In your `main` function, load the dataset, call the above functions to generate plots, and save each plot to a separate image file. - Write down one insight you can observe from each type of plot produced. Constraints - Keep your code modular by defining helper functions where necessary. - Ensure all plots have appropriate labels and legends to make them informative. Example Output Each function should generate and save a plot that meets the specified aesthetic requirements. Make sure to include both code and plots in your submission. Additional Information - Add necessary imports at the beginning of your script. - Document your code with comments to explain your logic. Expected Input and Output Formats Function `plot_penguins_distributions()` - Input: None. - Output: A saved boxplot image file that uses the `whitegrid` theme and removes top and right spines. Function `plot_penguins_scatter()` - Input: None. - Output: A saved scatter plot image file that uses the `darkgrid` theme and `poster` context with increased font scale. Function `plot_multiple_styles()` - Input: None. - Output: A saved image file with three subplots, each using different styles (`darkgrid`, `white`, `ticks`). Main Function - Input: None. - Output: Three saved image files corresponding to the plots generated by each function. **Note:** Ensure all plots are displayed inline if executing in a Jupyter Notebook.","solution":"import seaborn as sns import matplotlib.pyplot as plt def plot_penguins_distributions(): Plots the distribution of flipper_length_mm for each species using a boxplot with whitegrid theme. The top and right spines of the plot are removed. penguins = sns.load_dataset(\'penguins\') sns.set_theme(style=\'whitegrid\') plt.figure(figsize=(10, 6)) ax = sns.boxplot(x=\'species\', y=\'flipper_length_mm\', data=penguins) sns.despine(top=True, right=True) plt.title(\'Distribution of Flipper Length by Species\') plt.savefig(\'penguins_distributions.png\') plt.clf() def plot_penguins_scatter(): Plots a scatter plot of flipper_length_mm vs body_mass_g for each species using darkgrid theme and poster context. Increases font scale to 1.5. penguins = sns.load_dataset(\'penguins\') sns.set_context(\'poster\', font_scale=1.5) sns.set_theme(style=\'darkgrid\') plt.figure(figsize=(10, 6)) ax = sns.scatterplot(x=\'flipper_length_mm\', y=\'body_mass_g\', hue=\'species\', data=penguins) plt.title(\'Flipper Length vs Body Mass by Species\') plt.savefig(\'penguins_scatter.png\') plt.clf() def plot_multiple_styles(): Plots three subplots side-by-side using three different themes (\'darkgrid\', \'white\', \'ticks\') temporarily. penguins = sns.load_dataset(\'penguins\') fig, axs = plt.subplots(1, 3, figsize=(18, 6)) with sns.axes_style(\'darkgrid\'): sns.boxplot(x=\'species\', y=\'flipper_length_mm\', data=penguins, ax=axs[0]) axs[0].set_title(\'Darkgrid Theme\') with sns.axes_style(\'white\'): sns.boxplot(x=\'species\', y=\'flipper_length_mm\', data=penguins, ax=axs[1]) axs[1].set_title(\'White Theme\') with sns.axes_style(\'ticks\'): sns.boxplot(x=\'species\', y=\'flipper_length_mm\', data=penguins, ax=axs[2]) axs[2].set_title(\'Ticks Theme\') plt.tight_layout() plt.savefig(\'penguins_multiple_styles.png\') plt.clf() def main(): plot_penguins_distributions() plot_penguins_scatter() plot_multiple_styles() print(\\"Insights from the plots:\\") print(\\"1. From the boxplot, we can observe the median flipper length difference among the species.\\") print(\\"2. From the scatter plot, it\'s evident that body mass and flipper length have a positive correlation.\\") print(\\"3. Using different styles, we can see how the theme changes the appearance and readability of the same data.\\") if __name__ == \\"__main__\\": main()"},{"question":"# Coding Assessment: IMAP4 Email Deletion and Log Archiving Task **Objective**: You are tasked with implementing a function that connects to an IMAP4 server, logs in, searches for emails from a specific sender, deletes those emails, and archives a log of the IDs of deleted emails. **Function Signature**: ```python import imaplib from typing import List def delete_and_log_emails(host: str, username: str, password: str, sender: str, log_file: str) -> None: Connect to an IMAP4 server, log in, search for emails from a specific sender, delete those emails, and archive a log of the IDs of deleted emails. Args: - host (str): The IMAP server\'s host address. - username (str): The username for logging into the IMAP server. - password (str): The password for logging into the IMAP server. - sender (str): The email address of the sender whose emails are to be deleted. - log_file (str): The file path where the log of deleted email IDs will be stored. Raises: - ConnectionError: If any connection-related issues occur. - LoginError: If login fails. - DeletionError: If there is an issue deleting emails. pass ``` **Expected Input and Output**: - The function should not return any value. - It should log the IDs of deleted emails to a specified log file. **Constraints**: - Do not use any external libraries for IMAP operations other than `imaplib`. - Handle exceptions appropriately and raise a `ConnectionError`, `LoginError`, or `DeletionError` as needed. - Ensure that the connection to the IMAP server is securely closed after operations. **Performance Requirements**: - The implementation should be efficient with respect to network operations and should handle large volumes of emails robustly. **Example**: Using the function: ```python delete_and_log_emails(\\"imap.gmail.com\\", \\"user@example.com\\", \\"password123\\", \\"spam@example.com\\", \\"deleted_emails_log.txt\\") ``` This should: 1. Connect to the IMAP server at \\"imap.gmail.com\\". 2. Log in using the provided username and password. 3. Search for all emails from \\"spam@example.com\\". 4. Delete those emails. 5. Log the IDs of the deleted emails in \\"deleted_emails_log.txt\\". **Notes**: - You may want to refer to the `imaplib.IMAP4` documentation for details on the methods available. - Make use of context managers where applicable to ensure proper resource management. **Implementation Plan**: 1. Establish a connection using `IMAP4` or `IMAP4_SSL`. 2. Log in to the server. 3. Select the mailbox (default to \'INBOX\'). 4. Use the `search` method to find emails from the specified sender. 5. Delete the emails by setting the `Deleted` flag and expunging. 6. Log the deleted email IDs to the specified log file. 7. Ensure to handle exceptions and close the connection properly.","solution":"import imaplib from typing import List class ConnectionError(Exception): pass class LoginError(Exception): pass class DeletionError(Exception): pass def delete_and_log_emails(host: str, username: str, password: str, sender: str, log_file: str) -> None: try: # Connect to the IMAP server mail = imaplib.IMAP4_SSL(host) try: # Log in to the server mail.login(username, password) except imaplib.IMAP4.error: raise LoginError(\\"Failed to login to the IMAP server\\") # Select the mailbox (default is \'INBOX\') mail.select(\\"inbox\\") # Search for emails from the specified sender status, message_ids = mail.search(None, f\'FROM \\"{sender}\\"\') if status != \\"OK\\": raise DeletionError(\\"Failed to search for emails\\") # Split the IDs and prepare the log list message_ids = message_ids[0].split() deleted_ids = [] for msg_id in message_ids: mail.store(msg_id, \'+FLAGS\', \'Deleted\') deleted_ids.append(msg_id.decode()) # Expunge the deleted messages mail.expunge() # Log the deleted email IDs to the specified log file with open(log_file, \'w\') as f: for msg_id in deleted_ids: f.write(f\\"{msg_id}n\\") # Logout and close the connection mail.logout() except imaplib.IMAP4.error as e: raise ConnectionError(f\\"IMAP error occurred: {e}\\") except Exception as e: raise e"},{"question":"**Problem Statement** Implement a comprehensive logging system for a multi-module Python application. Your task is to create loggers, handlers, and formatters that correctly log events to both the console and a log file. Additionally, your loggers should handle the following scenarios: 1. Log different levels of messages (DEBUG, INFO, WARNING, ERROR, CRITICAL). 2. Properly format log messages to include timestamps, log levels, logger names, module names, and line numbers. 3. Use a configuration file or dictionary to initialize your logging setup. 4. Ensure log rotation such that the log file is split at a specified size and retains a specified number of backup files. 5. Implement a logger for each module in the application such that logs correctly propagate to the root logger and can be controlled hierarchically. **Requirements** 1. Create loggers for multiple modules: `main.py`, `module_a.py`, `module_b.py`. 2. Configure the loggers using a dictionary-based configuration. 3. Set up two handlers: - Console handler: Should log INFO level and above. - File handler: Should log DEBUG level and above with rotation (max size=1MB, backup count=3). 4. Each log message should include a timestamp, the logger name, the log level, the message, the module name, and the line number. **Constraints** 1. The log file should be rotated at a size of 1MB and the system should retain the last 3 backup files. 2. All loggers should use the same format: ``` %(asctime)s - %(name)s - %(levelname)s - %(module)s:%(lineno)d - %(message)s ``` **Input and Output** There is no specific input to the system; your program should generate logs when run, and the output should be the correctly formatted log messages both on the console and in the log file. **Example Output (Console)** ``` 2023-01-01 10:00:00,000 - main - INFO - main:10 - Application started 2023-01-01 10:00:00,500 - module_a - WARNING - module_a:20 - Potential issue detected 2023-01-01 10:01:00,000 - module_b - ERROR - module_b:30 - An error occurred ``` **Example Output (Log File)** ``` 2023-01-01 10:00:00,000 - main - DEBUG - main:10 - Debugging application 2023-01-01 10:00:00,000 - main - INFO - main:12 - Application started 2023-01-01 10:00:00,500 - module_a - WARNING - module_a:20 - Potential issue detected 2023-01-01 10:01:00,000 - module_b - ERROR - module_b:30 - An error occurred ``` **Implementation** ```python import logging import logging.config import os # Configuration dictionary LOGGING_CONFIG = { \'version\': 1, \'disable_existing_loggers\': False, \'formatters\': { \'standard\': { \'format\': \'%(asctime)s - %(name)s - %(levelname)s - %(module)s:%(lineno)d - %(message)s\', }, }, \'handlers\': { \'console\': { \'level\': \'INFO\', \'class\': \'logging.StreamHandler\', \'formatter\': \'standard\', }, \'file\': { \'level\': \'DEBUG\', \'class\': \'logging.handlers.RotatingFileHandler\', \'filename\': \'application.log\', \'maxBytes\': 1 * 1024 * 1024, # 1 MB \'backupCount\': 3, \'formatter\': \'standard\', }, }, \'loggers\': { \'\': { # root logger \'handlers\': [\'console\', \'file\'], \'level\': \'DEBUG\', \'propagate\': True, }, \'main\': { \'level\': \'DEBUG\', \'handlers\': [\'console\', \'file\'], \'propagate\': True, }, \'module_a\': { \'level\': \'DEBUG\', \'handlers\': [\'console\', \'file\'], \'propagate\': True, }, \'module_b\': { \'level\': \'DEBUG\', \'handlers\': [\'console\', \'file\'], \'propagate\': True, }, } } # Apply logging configuration logging.config.dictConfig(LOGGING_CONFIG) # Example usage in different modules def main(): logger = logging.getLogger(\'main\') logger.debug(\'Debugging application\') logger.info(\'Application started\') def module_a(): logger = logging.getLogger(\'module_a\') logger.warning(\'Potential issue detected\') def module_b(): logger = logging.getLogger(\'module_b\') logger.error(\'An error occurred\') if __name__ == \\"__main__\\": main() module_a() module_b() ``` **Task:** 1. Implement the code structure provided. 2. Create `main.py`, `module_a.py`, and `module_b.py` files. 3. Ensure the logging configuration is correctly applied. 4. Verify that logs appear on the console and in the log file correctly formatted.","solution":"import logging import logging.config import os # Configuration dictionary LOGGING_CONFIG = { \'version\': 1, \'disable_existing_loggers\': False, \'formatters\': { \'standard\': { \'format\': \'%(asctime)s - %(name)s - %(levelname)s - %(module)s:%(lineno)d - %(message)s\', }, }, \'handlers\': { \'console\': { \'level\': \'INFO\', \'class\': \'logging.StreamHandler\', \'formatter\': \'standard\', }, \'file\': { \'level\': \'DEBUG\', \'class\': \'logging.handlers.RotatingFileHandler\', \'filename\': \'application.log\', \'maxBytes\': 1 * 1024 * 1024, # 1 MB \'backupCount\': 3, \'formatter\': \'standard\', }, }, \'loggers\': { \'\': { # root logger \'handlers\': [\'console\', \'file\'], \'level\': \'DEBUG\', \'propagate\': True, }, \'main\': { \'level\': \'DEBUG\', \'handlers\': [\'console\', \'file\'], \'propagate\': True, }, \'module_a\': { \'level\': \'DEBUG\', \'handlers\': [\'console\', \'file\'], \'propagate\': True, }, \'module_b\': { \'level\': \'DEBUG\', \'handlers\': [\'console\', \'file\'], \'propagate\': True, }, } } # Apply logging configuration logging.config.dictConfig(LOGGING_CONFIG) # Example usage in different modules def main(): logger = logging.getLogger(\'main\') logger.debug(\'Debugging application\') logger.info(\'Application started\') def module_a(): logger = logging.getLogger(\'module_a\') logger.warning(\'Potential issue detected\') def module_b(): logger = logging.getLogger(\'module_b\') logger.error(\'An error occurred\') if __name__ == \\"__main__\\": main() module_a() module_b()"},{"question":"Objective: Create a small neural network using PyTorch, move it to the MPS device, and perform a simple training loop followed by a prediction. This task assesses your ability to handle the MPS backend for GPU acceleration in PyTorch. Background: The MPS backend enables high-performance training on MacOS devices with the Metal programming framework. You need to ensure that the MPS backend is available and correctly utilize it for tensor operations and neural network training. Task: 1. **Check for MPS Availability:** - Write a function `check_mps_availability()` that checks and returns whether the MPS backend is available. If not, it should print the appropriate error message. 2. **Define a Simple Model:** - Create a simple feedforward neural network using `torch.nn.Module`. The network should have: - One input layer that accepts 2 features. - One hidden layer with 5 neurons and ReLU activation. - One output layer with 1 neuron. - Name this class `SimpleNN`. 3. **Training Loop:** - Write a function `train_model(model, data, targets, epochs)`: - Move the model, data, and targets to the MPS device. - Define a loss function (Mean Squared Error) and an optimizer (SGD). - Implement a basic training loop that performs forward pass, computes loss, backpropagates errors, and updates the model weights for the specified number of epochs. 4. **Prediction:** - Write a function `predict(model, data)` that moves the data to the MPS device, performs a forward pass, and returns the model predictions in CPU memory. Input Format: - `data` and `targets` for the training loop will be PyTorch tensors of shape `(N, 2)` and `(N,)` respectively, where `N` is the number of samples. - `epochs` is an integer specifying the number of training iterations. Output Format: - `check_mps_availability()` should return a boolean value. - `train_model()` should not return anything (it just trains the model). - `predict()` should return a PyTorch tensor with predictions. Example: ```python # Example usage if __name__ == \\"__main__\\": if check_mps_availability(): mps_device = torch.device(\\"mps\\") data = torch.randn(100, 2) targets = torch.randn(100) model = SimpleNN() train_model(model, data, targets, epochs=10) predictions = predict(model, data) print(predictions) ``` Note: Make sure to handle the necessary imports and ensure that the functions handle the movement of tensors and models between devices appropriately.","solution":"import torch import torch.nn as nn import torch.optim as optim def check_mps_availability(): Check if the MPS backend is available for PyTorch. if torch.backends.mps.is_available(): return True else: print(\\"MPS backend is not available.\\") return False class SimpleNN(nn.Module): A simple feedforward neural network with one hidden layer. def __init__(self): super(SimpleNN, self).__init__() self.fc1 = nn.Linear(2, 5) self.relu = nn.ReLU() self.fc2 = nn.Linear(5, 1) def forward(self, x): x = self.fc1(x) x = self.relu(x) x = self.fc2(x) return x def train_model(model, data, targets, epochs): Train the model with the given data and targets for a specified number of epochs. Args: - model: the neural network model - data: input data tensor of shape (N, 2) - targets: target tensor of shape (N,) - epochs: number of training epochs device = torch.device(\\"mps\\") model.to(device) data, targets = data.to(device), targets.to(device) criterion = nn.MSELoss() optimizer = optim.SGD(model.parameters(), lr=0.01) model.train() for epoch in range(epochs): optimizer.zero_grad() outputs = model(data) loss = criterion(outputs.squeeze(), targets) loss.backward() optimizer.step() def predict(model, data): Perform a forward pass on the data and return predictions. Args: - model: the trained neural network model - data: input data tensor of shape (N, 2) Returns: - predictions: tensor of predictions device = torch.device(\\"mps\\") model.to(device) data = data.to(device) model.eval() with torch.no_grad(): predictions = model(data) return predictions.cpu()"},{"question":"Coding Assessment Question # Objective Your task is to implement a function that takes a list of time zone keys and a UTC datetime, and returns a dictionary mapping each time zone key to the corresponding local time and whether it is currently in daylight saving time. # Function Signature ```python import datetime from zoneinfo import ZoneInfo, ZoneInfoNotFoundError def get_local_times(utc_dt: datetime.datetime, tz_keys: list[str]) -> dict[str, tuple[datetime.datetime, bool]]: pass ``` # Input - `utc_dt` (datetime.datetime): A datetime object in UTC. - `tz_keys` (list of str): A list of strings, where each string represents a valid IANA time zone key. # Output - A dictionary where each key is a string from `tz_keys` and the value is a tuple: - The first element of the tuple is a `datetime.datetime` object representing the local time in that time zone. - The second element of the tuple is a boolean indicating whether the datetime is in daylight saving time. # Constraints - The datetime object `utc_dt` will always be timezone-aware and in UTC. - Each string in `tz_keys` will be a valid IANA time zone key accessible from the system, so no need to handle `ZoneInfoNotFoundError` explicitly. - You may assume `tz_keys` contains a reasonable number of elements (i.e., not more than 100). # Example ```python from datetime import datetime, timezone utc_dt = datetime(2023, 4, 1, 12, 0, tzinfo=timezone.utc) tz_keys = [\\"America/New_York\\", \\"Europe/London\\", \\"Asia/Tokyo\\"] result = get_local_times(utc_dt, tz_keys) # Expected output might look like: # { # \\"America/New_York\\": (datetime(2023, 4, 1, 8, 0, tzinfo=ZoneInfo(\\"America/New_York\\")), True), # \\"Europe/London\\": (datetime(2023, 4, 1, 13, 0, tzinfo=ZoneInfo(\\"Europe/London\\")), False), # \\"Asia/Tokyo\\": (datetime(2023, 4, 1, 21, 0, tzinfo=ZoneInfo(\\"Asia/Tokyo\\")), False) # } ``` # Requirements 1. Utilize the `zoneinfo` module to manage time zones. 2. Convert the given UTC datetime to local times for each time zone key in `tz_keys`. 3. Indicate whether each local time is in daylight saving time or not. # Notes - To check whether the time is in daylight saving time, you can use `datetime.tzname()` method. If the returned value typically characterizes daylight saving time (e.g., \'PDT\' vs \'PST\'), it indicates DST. Good luck!","solution":"import datetime from zoneinfo import ZoneInfo def get_local_times(utc_dt: datetime.datetime, tz_keys: list[str]) -> dict[str, tuple[datetime.datetime, bool]]: result = {} for tz_key in tz_keys: tz_info = ZoneInfo(tz_key) local_dt = utc_dt.astimezone(tz_info) is_dst = local_dt.dst() != datetime.timedelta(0) result[tz_key] = (local_dt, is_dst) return result"},{"question":"# Advanced Scatter Plot with Seaborn Objects Objective Your task is to create an advanced scatter plot using Seaborn\'s objects interface (`seaborn.objects`) that visualizes data from the `tips` dataset, showcasing different customization and faceting features. Requirements 1. **Data Loading**: - Load the `tips` dataset using Seaborn\'s `load_dataset` function. 2. **Scatter Plot**: - Create a scatter plot showing `total_bill` on the x-axis and `tip` on the y-axis. - Use the `Dot` mark for the scatter plot with the following customizations: - Set the `edgecolor` to white. - Apply jitter to both x and y axes. 3. **Faceting**: - Facet the scatter plot by the `day` column, arranging the plots in a grid with 2 columns. 4. **Color Mapping**: - Map the `color` property to the `sex` column to differentiate data points by gender. - Use the `flare` color palette for the mapped colors. 5. **Error Bars**: - Combine the scatter plot with `Range` to show error bars. - The error bars should represent the standard error (`se`). Estimate the error using standard error multiplied by 2. Expected Function Define a function `advanced_scatter_plot()` that implements the requirements above. Input Format This function does not take any input parameters. Output Format This function should generate and display a faceted scatter plot as per the requirements. Constraints - Ensure that the plot is clearly visible and well-customized. - Use only the Seaborn\'s objects interface and standard libraries. Example Here\'s how the plot would conceptually look: - Scatter plot of `total_bill` vs `tip`, faceted by `day`, with different colors for `sex`, and error bars representing the standard error. ```python def advanced_scatter_plot(): import seaborn.objects as so from seaborn import load_dataset # Load the tips dataset tips = load_dataset(\\"tips\\") # Create the scatter plot with required customizations and faceting plot = ( so.Plot(tips, x=\\"total_bill\\", y=\\"tip\\", color=\\"sex\\") .facet(\\"day\\", wrap=2) .add(so.Dot(edgecolor=\\"w\\"), so.Jitter(.2)) .scale(color=\\"flare\\") .add(so.Range(), so.Est(errorbar=(\\"se\\", 2))) ) # Display the plot plot.show() # Call the function to display the plot advanced_scatter_plot() ```","solution":"def advanced_scatter_plot(): import seaborn.objects as so from seaborn import load_dataset # Load the tips dataset tips = load_dataset(\\"tips\\") # Create the scatter plot with required customizations and faceting plot = ( so.Plot(tips, x=\\"total_bill\\", y=\\"tip\\", color=\\"sex\\") .facet(\\"day\\", wrap=2) .add(so.Dot(edgecolor=\\"w\\"), so.Jitter(.2)) .scale(color=\\"flare\\") .add(so.Range(), so.Est(errorbar=(\\"se\\", 2))) ) # Display the plot plot.show() # Call the function to display the plot advanced_scatter_plot()"},{"question":"**Objective**: Demonstrate your understanding of neural network weight initialization using PyTorch\'s `torch.nn.init` module. # Problem Statement: You are given an uninitialized neural network model with two linear layers. Write a function `initialize_weights` that initializes the weights of these layers using different strategies provided by the `torch.nn.init` module. The function should accept a PyTorch model and should: 1. Initialize the weights of the first linear layer using Xavier uniform initialization. 2. Initialize the weights of the second linear layer using Kaiming normal initialization. 3. Initialize the biases of both layers to zeros. # Input: - A PyTorch model `model`. # Output: - The model with its weights and biases initialized as specified. # Constraints: - You must use the `torch.nn.init` module for initializing the weights and biases. - You should ensure that the function runs in `torch.no_grad` mode as per the requirements for these initialization functions. # Sample Code: ```python import torch import torch.nn as nn import torch.nn.init as init class SimpleNN(nn.Module): def __init__(self): super(SimpleNN, self).__init__() self.layer1 = nn.Linear(10, 5) self.layer2 = nn.Linear(5, 2) def forward(self, x): x = self.layer1(x) x = self.layer2(x) return x def initialize_weights(model): This function initializes the weights of the model. Args: - model: PyTorch model with at least two linear layers Returns: - model: PyTorch model with initialized weights and biases # Write your code here # Example usage: model = SimpleNN() initialize_weights(model) ``` # Explanation: - The `SimpleNN` class defines a simple neural network with two linear layers. - The `initialize_weights` function is where you will write the code to initialize the weights and biases of the layers according to the specifications. Ensure that your solution is efficient and adheres to best practices in PyTorch programming, particularly the use of `torch.no_grad` mode when modifying parameters directly. # Evaluation Criteria: - Correctness: The weights and biases must be initialized correctly as per the given specifications. - Efficiency: The use of `torch.no_grad` should be appropriate to avoid autograd tracking. - Code Quality: The solution should be well-structured and adhere to best coding practices.","solution":"import torch import torch.nn as nn import torch.nn.init as init class SimpleNN(nn.Module): def __init__(self): super(SimpleNN, self).__init__() self.layer1 = nn.Linear(10, 5) self.layer2 = nn.Linear(5, 2) def forward(self, x): x = self.layer1(x) x = self.layer2(x) return x def initialize_weights(model): This function initializes the weights of the model. Args: - model: PyTorch model with at least two linear layers Returns: - model: PyTorch model with initialized weights and biases with torch.no_grad(): for layer in model.children(): if isinstance(layer, nn.Linear): if layer == model.layer1: init.xavier_uniform_(layer.weight) elif layer == model.layer2: init.kaiming_normal_(layer.weight, nonlinearity=\'relu\') layer.bias.zero_() return model"},{"question":"Objective You are to implement a custom PyTorch neural network that demonstrates understanding and correct usage of forward and backward hooks during the training process. Your task is to: 1. Build a simple neural network using `torch.nn.Module`. 2. Register and use both forward and backward hooks. 3. Compile the model using `torch.compile` ensuring that the hooks are appropriately handled. Instructions: 1. Define a neural network class `SimpleNet` that inherits from `torch.nn.Module` with at least one hidden layer. 2. Implement forward and backward hooks: - Forward hook: Print the input and output of the layer it is attached to. - Backward hook: Print the gradient of the layer it is attached to. 3. Compile your model using `torch.compile`. 4. Verify that the hooks are executed correctly by running a forward and backward pass through the model. 5. Provide a sample output of the execution of the hooks. Constraints: 1. Use the `torch` library. 2. The network should have at least one linear layer. 3. The hooks should output results to the standard output. Expected Input and Output: - **Input**: - No specific input as the process involves implementation, registration, and execution within a given context. - **Output**: - Printed statements from the hooks showing the forward and backward data at each layer. Example Solution Framework: ```python import torch import torch.nn as nn # Step 1: Define the Simple Neural Network class SimpleNet(nn.Module): def __init__(self): super(SimpleNet, self).__init__() self.fc1 = nn.Linear(4, 2) self.fc2 = nn.Linear(2, 1) def forward(self, x): out = self.fc1(x) out = torch.relu(out) out = self.fc2(out) return out # Step 2: Define the hooks def forward_hook(module, input, output): print(f\\"Forward hook: {module}n Input: {input}n Output: {output}n\\") def backward_hook(module, grad_input, grad_output): print(f\\"Backward hook: {module}n Grad Input: {grad_input}n Grad Output: {grad_output}n\\") # Initialize the model model = SimpleNet() # Step 3: Register hooks model.fc1.register_forward_hook(forward_hook) model.fc1.register_backward_hook(backward_hook) model.fc2.register_forward_hook(forward_hook) model.fc2.register_backward_hook(backward_hook) # Step 4: Compile the model compiled_model = torch.compile(model) # Verify hooks by running a forward and backward pass input_data = torch.randn(1, 4) output = compiled_model(input_data) # Compute loss and perform backward pass loss_fn = nn.MSELoss() target = torch.randn(1, 1) loss = loss_fn(output, target) loss.backward() ``` Verification - Run the provided framework code. - Verify that the hook outputs are correct by examining the print statements for forward and backward hooks.","solution":"import torch import torch.nn as nn # Step 1: Define the Simple Neural Network class SimpleNet(nn.Module): def __init__(self): super(SimpleNet, self).__init__() self.fc1 = nn.Linear(4, 2) self.fc2 = nn.Linear(2, 1) def forward(self, x): out = self.fc1(x) out = torch.relu(out) out = self.fc2(out) return out # Step 2: Define the hooks def forward_hook(module, input, output): print(f\\"Forward hook: {module}n Input: {input}n Output: {output}n\\") def backward_hook(module, grad_input, grad_output): print(f\\"Backward hook: {module}n Grad Input: {grad_input}n Grad Output: {grad_output}n\\") # Initialize the model model = SimpleNet() # Step 3: Register hooks fc1_forward_hook_handle = model.fc1.register_forward_hook(forward_hook) fc1_backward_hook_handle = model.fc1.register_backward_hook(backward_hook) fc2_forward_hook_handle = model.fc2.register_forward_hook(forward_hook) fc2_backward_hook_handle = model.fc2.register_backward_hook(backward_hook) # Step 4: Compile the model compiled_model = torch._dynamo.optimize(\\"eager\\")(model) # Verify hooks by running a forward and backward pass input_data = torch.randn(1, 4) output = compiled_model(input_data) # Compute loss and perform backward pass loss_fn = nn.MSELoss() target = torch.randn(1, 1) loss = loss_fn(output, target) loss.backward()"},{"question":"# Dataclass Management System You are required to design a system to manage the inventory of electronic components in a warehouse using Python\'s `dataclasses` module. Your task includes implementing a dataclass representing an electronic component and several functions to perform various operations on instances of this dataclass. # Requirements 1. **ElectronicComponent Dataclass**: - Create a dataclass named `ElectronicComponent` with the following fields: - `name`: A string representing the name of the component. - `unit_price`: A float representing the price per unit of the component. - `quantity_on_hand`: An integer representing the quantity available in the warehouse (default value is 0). - `manufacture_date`: A string representing the manufacture date of the component in the format \'YYYY-MM-DD\'. - Define a method `stock_value` that returns the total cost of the stock (`unit_price * quantity_on_hand`). - Include a `__post_init__` method to ensure that `manufacture_date` is in the past. If not, raise a `ValueError`. 2. **Inventory Management Functions**: - `add_to_inventory`: A function that accepts a list of components and a new `ElectronicComponent` item, and adds the new item to the inventory if it does not already exist (based on the `name`). If it exists, update the `quantity_on_hand`. - `remove_from_inventory`: A function that accepts a list of components and a component name, and removes the specified component from the inventory. If the component does not exist, raise a `ValueError`. 3. **Serialization and Deserialization**: - Implement a function `inventory_to_dict` that converts the list of `ElectronicComponent` objects into a list of dictionaries using `dataclasses.asdict()`. - Implement a function `dict_to_inventory` that takes a list of dictionaries and converts it back to a list of `ElectronicComponent` objects using `dataclasses.make_dataclass()`. # Example Usage ```python from dataclasses import dataclass, field, asdict, make_dataclass from datetime import datetime @dataclass class ElectronicComponent: name: str unit_price: float quantity_on_hand: int = 0 manufacture_date: str = field(default_factory=lambda: datetime.now().strftime(\'%Y-%m-%d\')) def __post_init__(self): if datetime.strptime(self.manufacture_date, \'%Y-%m-%d\') > datetime.now(): raise ValueError(\\"Manufacture date cannot be in the future\\") def stock_value(self) -> float: return self.unit_price * self.quantity_on_hand def add_to_inventory(inventory: list, new_component: ElectronicComponent): for component in inventory: if component.name == new_component.name: component.quantity_on_hand += new_component.quantity_on_hand return inventory.append(new_component) def remove_from_inventory(inventory: list, component_name: str): for component in inventory: if component.name == component_name: inventory.remove(component) return raise ValueError(f\\"Component named {component_name} not found in inventory\\") def inventory_to_dict(inventory: list) -> list: return [asdict(component) for component in inventory] def dict_to_inventory(component_dicts: list) -> list: components = [] for component_dict in component_dicts: ComponentClass = make_dataclass(\'ElectronicComponent\', [(name, type(value)) for name, value in component_dict.items()]) components.append(ComponentClass(**component_dict)) return components # Example usage inventory = [] component1 = ElectronicComponent(name=\\"Resistor\\", unit_price=0.05, quantity_on_hand=100, manufacture_date=\\"2023-01-01\\") component2 = ElectronicComponent(name=\\"Capacitor\\", unit_price=0.10, quantity_on_hand=200, manufacture_date=\\"2022-12-01\\") add_to_inventory(inventory, component1) add_to_inventory(inventory, component2) remove_from_inventory(inventory, \\"Resistor\\") print(inventory_to_dict(inventory)) invented_dict = [{\'name\': \'Capacitor\', \'unit_price\': 0.10, \'quantity_on_hand\': 200, \'manufacture_date\': \'2022-12-01\'}] print(dict_to_inventory(invented_dict)) ``` **Input/Output Specification**: - The `add_to_inventory` function takes a list of `ElectronicComponent` instances and an `ElectronicComponent` instance and returns None. - The `remove_from_inventory` function takes a list of `ElectronicComponent` instances and a string (component name) and returns None. - The `inventory_to_dict` function takes a list of `ElectronicComponent` instances and returns a list of dictionaries. - The `dict_to_inventory` function takes a list of dictionaries and returns a list of `ElectronicComponent` instances. # Constraints - Assume the name of the component is unique across the inventory. - Assume all manufacture_dates provided are valid. **Note**: Handle all possible edge cases and ensure proper functioning of the implemented methods.","solution":"from dataclasses import dataclass, field, asdict, make_dataclass from datetime import datetime @dataclass class ElectronicComponent: name: str unit_price: float quantity_on_hand: int = 0 manufacture_date: str = field(default_factory=lambda: datetime.now().strftime(\'%Y-%m-%d\')) def __post_init__(self): if datetime.strptime(self.manufacture_date, \'%Y-%m-%d\') > datetime.now(): raise ValueError(\\"Manufacture date cannot be in the future\\") def stock_value(self) -> float: return self.unit_price * self.quantity_on_hand def add_to_inventory(inventory: list, new_component: ElectronicComponent): for component in inventory: if component.name == new_component.name: component.quantity_on_hand += new_component.quantity_on_hand return inventory.append(new_component) def remove_from_inventory(inventory: list, component_name: str): for component in inventory: if component.name == component_name: inventory.remove(component) return raise ValueError(f\\"Component named {component_name} not found in inventory\\") def inventory_to_dict(inventory: list) -> list: return [asdict(component) for component in inventory] def dict_to_inventory(component_dicts: list) -> list: components = [] for component_dict in component_dicts: components.append(ElectronicComponent(**component_dict)) return components"},{"question":"Objective: Demonstrate your understanding of the `site` module, specifically focusing on manipulating `sys.path` and handling site-specific and user-specific directories. Problem Statement: You are tasked with writing a Python script that: 1. Adds a specified directory to the `sys.path` if it is not already present. 2. Ensures a given `.pth` file in this directory is processed correctly. 3. Creates or updates the `usercustomize` module to add a specified directory to the `sys.path` upon Python startup (if it is not already present). Function Signature: ```python def configure_site(directory: str, pth_file_contents: str, user_site_dir: str) -> None: Adds the specified directory to sys.path if not present, processes a .pth file, and updates usercustomize. Parameters: - directory: str : A path to be added to sys.path. - pth_file_contents: str : Content for a .pth file to be added in the directory. - user_site_dir: str : A directory path to be added to usercustomize. Returns: - None ``` Implementation Requirements: 1. **Append Directory to `sys.path`**: - Check if the specified `directory` exists. - If it exists and is not already in `sys.path`, add it to `sys.path`. 2. **Process `.pth` File**: - Create a new `.pth` file named `custom.pth` in the specified `directory` with the given `pth_file_contents`. - Ensure that only valid paths from the `.pth` contents are added to `sys.path`. 3. **User Customizations**: - Create or update a module named `usercustomize.py` in the user site-packages directory. - Ensure this file contains logic to add `user_site_dir` to `sys.path` if it is not already present. Constraints: - You can assume all directories exist and you have necessary permissions to read/write files. - The function should not return any value, but properly configure the described components. - Ensure that the `usercustomize.py` file handles multiple startups gracefully, adding the directory only if it is not already present in `sys.path`. Example Usage: Suppose you run the following code: ```python configure_site(\'/usr/local/lib/python3.10/site-packages/my_custom_dir\', \'my_custom_packagenanother_package\', \'/home/user/.local/lib/python3.10/site-packages/my_user_dir\') ``` **Expected Behavior**: 1. `/usr/local/lib/python3.10/site-packages/my_custom_dir` should be added to `sys.path` if it is not already present. 2. A file named `custom.pth` should be created in `/usr/local/lib/python3.10/site-packages/my_custom_dir` that contains the lines `my_custom_package` and `another_package`. These should be validated and added to `sys.path`. 3. The `usercustomize.py` file should be updated or created in the user\'s site-packages directory to add `/home/user/.local/lib/python3.10/site-packages/my_user_dir` to `sys.path` upon Python startup.","solution":"import os import sys def configure_site(directory: str, pth_file_contents: str, user_site_dir: str) -> None: Adds the specified directory to sys.path if not present, processes a .pth file, and updates usercustomize. Parameters: - directory: str : A path to be added to sys.path. - pth_file_contents: str : Content for a .pth file to be added in the directory. - user_site_dir: str : A directory path to be added to usercustomize. Returns: - None # Step 1: Add directory to sys.path if not present if os.path.exists(directory) and directory not in sys.path: sys.path.insert(0, directory) # Step 2: Create and process the custom.pth file pth_file_path = os.path.join(directory, \'custom.pth\') with open(pth_file_path, \'w\') as pth_file: pth_file.write(pth_file_contents) for line in pth_file_contents.splitlines(): path = line.strip() if path and os.path.exists(path) and path not in sys.path: sys.path.insert(0, path) # Step 3: Create or update the usercustomize module user_customize_file = os.path.join(user_site_dir, \'usercustomize.py\') user_customize_code = f import sys def add_user_site_dir(): user_site_dir = \\"{user_site_dir}\\" if user_site_dir not in sys.path: sys.path.insert(0, user_site_dir) add_user_site_dir() os.makedirs(user_site_dir, exist_ok=True) with open(user_customize_file, \'w\') as customize_file: customize_file.write(user_customize_code)"},{"question":"**Coding Assessment Question: Managing HTTP Cookies with Custom Policies** In this exercise, you will demonstrate your understanding of the `http.cookiejar` module by implementing a function that manages HTTP cookies with a custom policy. # Requirements 1. Implement a function `manage_cookies` that will: a. Create an HTTP cookie jar with a custom cookie policy. b. Load cookies from a file if the file exists. c. Save cookies to a file. d. Add cookies to an outgoing HTTP request and extract cookies from an incoming HTTP response. # Function Signature ```python def manage_cookies(cookies_file: str, url: str, blocked_domains: list, allowed_domains: list, save_cookies: bool=False) -> None: Manages HTTP cookies with custom policies. :param cookies_file: The path to the file where cookies will be saved. :param url: The URL to send the HTTP request to and from which to extract cookies. :param blocked_domains: A list of domain names that should be blocked from setting cookies. :param allowed_domains: A list of domain names that are allowed to set cookies. :param save_cookies: A flag indicating whether to save the cookies after the operation. pass ``` # Detailed Requirements 1. **Custom Cookie Policy**: - Create a `DefaultCookiePolicy` with: - `rfc2965` set to `True` - Specify the `blocked_domains` from the parameter - Specify the `allowed_domains` from the parameter 2. **File Handling**: - Attempt to load cookies from `cookies_file`. If the file does not exist, skip loading. - Use `MozillaCookieJar` for saving and loading cookies. 3. **HTTP Request and Response Handling**: - Use `urllib.request` to open the specified `url`. - Attach cookies to the request and extract cookies from the response. 4. **Saving Cookies**: - If `save_cookies` is `True`, save the cookies to `cookies_file`. 5. **Clearing Session Cookies**: - Implement cookie clearing functionality within the function. # Constraints - The `cookies_file` parameter should be a valid file path. - The `url` should be a valid HTTP URL. - Lists of `blocked_domains` and `allowed_domains` should contain valid domain names. - Ensure to handle exceptions that may arise from file operations or HTTP requests. # Example Usage ```python manage_cookies( cookies_file=\'cookies.txt\', url=\'http://example.com\', blocked_domains=[\'ads.net\', \'.ads.net\'], allowed_domains=[\'example.com\', \'anotherexample.com\'], save_cookies=True ) ``` This example will: - Load cookies from \'cookies.txt\' if it exists. - Create a `DefaultCookiePolicy` configured to block domains like \'ads.net\' and allow domains like \'example.com\'. - Send a request to \'http://example.com\' with the cookies. - Extract cookies from the response and save them back to \'cookies.txt\' if `save_cookies` is `True`. # Notes - Ensure to use proper exception handling. - Make use of the provided classes and methods within the `http.cookiejar` and `urllib.request` modules to achieve the functionality.","solution":"import os import urllib.request from http.cookiejar import MozillaCookieJar, DefaultCookiePolicy def manage_cookies(cookies_file: str, url: str, blocked_domains: list, allowed_domains: list, save_cookies: bool=False) -> None: Manages HTTP cookies with custom policies. :param cookies_file: The path to the file where cookies will be saved. :param url: The URL to send the HTTP request to and from which to extract cookies. :param blocked_domains: A list of domain names that should be blocked from setting cookies. :param allowed_domains: A list of domain names that are allowed to set cookies. :param save_cookies: A flag indicating whether to save the cookies after the operation. # Set up cookie policy cookie_policy = DefaultCookiePolicy( rfc2965=True, blocked_domains=blocked_domains, allowed_domains=allowed_domains ) # Create CookieJar with custom policy cookie_jar = MozillaCookieJar(policy=cookie_policy) # Load existing cookies from file if it exists if os.path.exists(cookies_file): try: cookie_jar.load(cookies_file, ignore_discard=True, ignore_expires=True) except Exception as e: print(f\\"Error loading cookies from {cookies_file}: {e}\\") # Create an opener with the cookie jar opener = urllib.request.build_opener(urllib.request.HTTPCookieProcessor(cookie_jar)) # Make a request to the URL try: response = opener.open(url) # Note: at this point, cookies from the response are automatically extracted into the jar response.read() # We need to read the response for the cookies to be processed except Exception as e: print(f\\"Error making request to {url}: {e}\\") # Clear session cookies cookie_jar.clear_session_cookies() # Optionally save cookies to file if save_cookies: try: cookie_jar.save(cookies_file, ignore_discard=True, ignore_expires=True) except Exception as e: print(f\\"Error saving cookies to {cookies_file}: {e}\\")"},{"question":"# Coding Assessment: Wine Dataset Analysis Objective The objective of this coding assessment is to evaluate your understanding of loading datasets and performing basic machine learning tasks using the scikit-learn library. Task Your task is to write a Python function that performs the following steps: 1. **Load the wine dataset** from the scikit-learn library. 2. Perform a **train-test split** on the dataset. 3. **Train a k-Nearest Neighbors (k-NN) classifier** on the training data. 4. **Evaluate the classifier** on the test data. 5. Return the classification accuracy of the model. Function Signature ```python def wine_classification_knn(test_size: float, n_neighbors: int) -> float: pass ``` Input - `test_size` (float): The proportion of the dataset to include in the test split (between 0 and 1). - `n_neighbors` (int): Number of neighbors to use for k-NN classifier. Output - Returns a float representing the classification accuracy of the k-NN model on the test data. Constraints - The `test_size` must be between 0.1 and 0.5. - The `n_neighbors` must be a positive integer. Example ```python accuracy = wine_classification_knn(test_size=0.3, n_neighbors=5) print(accuracy) ``` Notes - Make sure to handle any import statements within your function. - Use `random_state=42` for reproducibility in the train-test split. - The function should be self-contained and should not rely on any external files.","solution":"def wine_classification_knn(test_size: float, n_neighbors: int) -> float: from sklearn.datasets import load_wine from sklearn.model_selection import train_test_split from sklearn.neighbors import KNeighborsClassifier from sklearn.metrics import accuracy_score if not (0.1 <= test_size <= 0.5): raise ValueError(\\"test_size must be between 0.1 and 0.5\\") if n_neighbors <= 0: raise ValueError(\\"n_neighbors must be a positive integer\\") # Load the wine dataset wine = load_wine() X = wine.data y = wine.target # Perform a train-test split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42) # Train a k-Nearest Neighbors (k-NN) classifier on the training data knn = KNeighborsClassifier(n_neighbors=n_neighbors) knn.fit(X_train, y_train) # Evaluate the classifier on the test data y_pred = knn.predict(X_test) accuracy = accuracy_score(y_test, y_pred) return accuracy"},{"question":"# Task You are tasked with using seaborn’s `cubehelix_palette` function to generate customized color palettes and apply these palettes to visualizations. This task will test your understanding of the seaborn package, specifically around customized color map creation and applying these palettes to plots. # Detailed Instructions 1. **Palette Creation** - Write a function `create_custom_palette` that generates a color palette with specified parameters using `cubehelix_palette`. - The function should accept the following parameters: - `n_colors (int)`: Number of colors in the palette. - `start (float)`: Starting position in the cubehelix space. - `rot (float)`: Number of rotations around the hue wheel. - `gamma (float)`: Gamma factor to apply to the luminance. - `hue (float)`: Saturation factor. - `dark (float)`: Intensity of the darkest color. - `light (float)`: Intensity of the lightest color. - `as_cmap (bool)`: Return as a continuous colormap if true. - The function should return the palette thus generated. 2. **Data Visualization** - Use the `create_custom_palette` function to generate a palette with specific parameters. - Create a scatter plot using the seaborn dataset `iris` and apply the generated palette to this plot. - Create a violin plot using the seaborn dataset `tips` and apply a different generated palette to this plot. # Function Signature ```python import seaborn as sns import matplotlib.pyplot as plt def create_custom_palette(n_colors=6, start=0, rot=0.4, gamma=1.0, hue=0.8, dark=0.2, light=0.8, as_cmap=False): # Create the color palette based on specified parameters palette = sns.cubehelix_palette(n_colors=n_colors, start=start, rot=rot, gamma=gamma, hue=hue, dark=dark, light=light, as_cmap=as_cmap) return palette def create_plots(): # Step 1: Create customized palettes scatter_palette = create_custom_palette(n_colors=5, start=0.5, rot=1.2, gamma=0.8, hue=0.5, dark=0.3, light=0.7, as_cmap=False) violin_palette = create_custom_palette(n_colors=8, start=1, rot=-0.5, gamma=1.2, hue=0.9, dark=0.1, light=0.9, as_cmap=True) # Step 2: Create scatter plot using the iris dataset iris = sns.load_dataset(\\"iris\\") sns.scatterplot(data=iris, x=\'sepal_length\', y=\'sepal_width\', hue=\'species\', palette=scatter_palette) plt.title(\\"Scatter Plot of Iris Dataset\\") plt.show() # Step 3: Create violin plot using the tips dataset tips = sns.load_dataset(\\"tips\\") sns.violinplot(data=tips, x=\'day\', y=\'total_bill\', palette=violin_palette) plt.title(\\"Violin Plot of Tips Dataset\\") plt.show() # Call the create_plots function to generate and display the plots create_plots() ``` # Constraints - You must use the `cubehelix_palette` function from seaborn. - Ensure that your plots are clearly labeled and visually distinct. # Evaluation Criteria - Correctness: The plots should be generated with the specified custom palettes. - Proper use of seaborn and matplotlib functions. - Clean and well-documented code.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_custom_palette(n_colors=6, start=0, rot=0.4, gamma=1.0, hue=0.8, dark=0.2, light=0.8, as_cmap=False): Generates a color palette using seaborn\'s cubehelix_palette function with the given parameters. Parameters: n_colors (int): Number of colors in the palette. start (float): Starting position in the cubehelix space. rot (float): Number of rotations around the hue wheel. gamma (float): Gamma factor to apply to the luminance. hue (float): Saturation factor. dark (float): Intensity of the darkest color. light (float): Intensity of the lightest color. as_cmap (bool): Return as a continuous colormap if true. Returns: palette: Generated color palette. palette = sns.cubehelix_palette(n_colors=n_colors, start=start, rot=rot, gamma=gamma, hue=hue, dark=dark, light=light, as_cmap=as_cmap) return palette def create_plots(): # Step 1: Create customized palettes scatter_palette = create_custom_palette(n_colors=5, start=0.5, rot=1.2, gamma=0.8, hue=0.5, dark=0.3, light=0.7, as_cmap=False) violin_palette = create_custom_palette(n_colors=8, start=1, rot=-0.5, gamma=1.2, hue=0.9, dark=0.1, light=0.9, as_cmap=True) # Step 2: Create scatter plot using the iris dataset iris = sns.load_dataset(\\"iris\\") sns.scatterplot(data=iris, x=\'sepal_length\', y=\'sepal_width\', hue=\'species\', palette=scatter_palette) plt.title(\\"Scatter Plot of Iris Dataset\\") plt.show() # Step 3: Create violin plot using the tips dataset tips = sns.load_dataset(\\"tips\\") sns.violinplot(data=tips, x=\'day\', y=\'total_bill\', palette=violin_palette) plt.title(\\"Violin Plot of Tips Dataset\\") plt.show()"},{"question":"Objective: Create a Python program utilizing the `multiprocessing.shared_memory` module to solve a problem requiring inter-process communication and shared data manipulation. The program should demonstrate comprehension of shared memory allocation, access, and cleanup. Problem Statement: Write a program that leverages shared memory to compute the sum of squares of a list of integers using multiple processes. Specifically, the program should: 1. Create a shared memory block and store a list of integers in it. 2. Spawn multiple worker processes, each responsible for calculating the sum of squares of a segment of the list. 3. Aggregate the results from each worker process to compute the final sum of squares. 4. Ensure proper synchronization and cleanup of shared memory resources. Requirements: 1. Implement the function `sum_of_squares(n, num_workers)` which: - `n`: Integer, size of the list to generate from `[0, 1, ..., n-1]`. - `num_workers`: Integer, number of worker processes to spawn. - The function should return the sum of squares of the list. 2. Implement the helper worker function `worker(shared_memory_name, start, end, result_index, result_shm_name)` which: - `shared_memory_name`: Name of the shared memory block containing the list. - `start`: Start index of the segment to process. - `end`: End index of the segment to process. - `result_index`: Index in the shared result list to store partial result. - `result_shm_name`: Name of the shared memory block for storing results. 3. Properly manage shared memory access and cleanup, ensuring no memory leaks or orphaned resources. 4. Use the `SharedMemoryManager` for managing shared memory blocks and coordinating cleanup. Input Example: ```python n = 10 num_workers = 2 ``` Output Example: The function call `sum_of_squares(n, num_workers)` should return: ```python 285 # Sum of squares of [0, 1, 2, ..., 9] is 0^2 + 1^2 + 2^2 + ... + 9^2 = 285 ``` Constraints and Assumptions: - The list of integers `[0, 1, ..., n-1]` should be split into nearly equal-sized chunks for processing by each worker. - Handle cases where `n` is not easily divisible by `num_workers`. - Ensure the solution scales with the number of workers and list size. - The solution should be efficient, avoiding unnecessary copying of data between processes. Sample Skeleton Code: ```python from multiprocessing import Process from multiprocessing.shared_memory import SharedMemory from multiprocessing.managers import SharedMemoryManager import numpy as np def worker(shared_memory_name, start, end, result_index, result_shm_name): # Implement worker functionality here pass def sum_of_squares(n, num_workers): with SharedMemoryManager() as smm: # Create shared memory for the list and results array = np.arange(n, dtype=np.int64) shm = smm.SharedMemory(size=array.nbytes) shm_array = np.ndarray(array.shape, dtype=array.dtype, buffer=shm.buf) shm_array[:] = array[:] result_shm = smm.SharedMemory(size=num_workers * 8) # 8 bytes per int64 result_array = np.ndarray((num_workers,), dtype=np.int64, buffer=result_shm.buf) # Spawn worker processes processes = [] segment_size = (n + num_workers - 1) // num_workers # Round up division for i in range(num_workers): start = i * segment_size end = min(start + segment_size, n) p = Process(target=worker, args=(shm.name, start, end, i, result_shm.name)) processes.append(p) p.start() # Wait for all worker processes to complete for p in processes: p.join() # Aggregate results total_sum_of_squares = np.sum(result_array) shm.close() shm.unlink() result_shm.close() result_shm.unlink() return total_sum_of_squares ``` Implement the `worker` function and complete the `sum_of_squares` function to achieve the desired functionality.","solution":"from multiprocessing import Process from multiprocessing.shared_memory import SharedMemory from multiprocessing.managers import SharedMemoryManager import numpy as np def worker(shared_memory_name, start, end, result_index, result_shm_name): existing_shm = SharedMemory(name=shared_memory_name) array = np.ndarray((end - start,), dtype=np.int64, buffer=existing_shm.buf[start * 8: end * 8]) # Compute the sum of squares for this segment segment_sum = np.sum(array**2) # Write the result to the result shared memory result_shm = SharedMemory(name=result_shm_name) result_array = np.ndarray((1,), dtype=np.int64, buffer=result_shm.buf[result_index * 8: (result_index + 1) * 8]) result_array[0] = segment_sum existing_shm.close() result_shm.close() def sum_of_squares(n, num_workers): with SharedMemoryManager() as smm: # Create shared memory for the list and results array = np.arange(n, dtype=np.int64) shm = smm.SharedMemory(size=array.nbytes) shm_array = np.ndarray(array.shape, dtype=array.dtype, buffer=shm.buf) shm_array[:] = array[:] result_shm = smm.SharedMemory(size=num_workers * 8) # 8 bytes per int64 result_array = np.ndarray((num_workers,), dtype=np.int64, buffer=result_shm.buf) # Spawn worker processes processes = [] segment_size = (n + num_workers - 1) // num_workers # Round up division for i in range(num_workers): start = i * segment_size end = min(start + segment_size, n) p = Process(target=worker, args=(shm.name, start, end, i, result_shm.name)) processes.append(p) p.start() # Wait for all worker processes to complete for p in processes: p.join() # Aggregate results total_sum_of_squares = np.sum(result_array) return total_sum_of_squares"},{"question":"**Coding Challenge: Command-line Argument Processor and Auditor** # Objective Create a Python script that processes command-line arguments and integrates a custom audit hook to monitor certain events. # Instructions 1. Implement a function `process_arguments(args)` that: - Accepts a list of arguments. - Supports the following command-line arguments: - `-n <name>`: Accepts a name and returns a greeting message, e.g., \\"Hello, `<name>`!\\". - `-p <path>`: Accepts a path and prints the absolute path using `sys.executable` as the base. - `-s <size>`: Accepts a file path and prints the size of the file in bytes using `sys.getsizeof()`. - `-e <exit_code>`: Exits the program with the given exit code using `sys.exit()`. 2. Implement and register a custom audit hook using `sys.addaudithook()` that: - Logs every invocation of the sys module\'s methods mentioned above. - The log should include the method name, arguments passed to it, and the result or any exceptions raised. 3. Your script should handle invalid arguments gracefully, providing helpful error messages. # Example ```python import sys import os def process_arguments(args): # Implement the logic to process arguments here. pass def custom_audit_hook(event, args): # Implement the logic for the custom audit hook here. pass if __name__ == \\"__main__\\": # Add the custom audit hook sys.addaudithook(custom_audit_hook) # Call the function with command-line arguments process_arguments(sys.argv[1:]) ``` # Constraints - Do not use any external libraries. - The script should be compatible with Python 3.8 and above (utilize features introduced in these versions). - Handle edge cases, such as missing arguments or invalid file paths gracefully. - Add docstrings and comments for clarity. # Additional Notes - You may assume the command-line arguments are well-formed and focus mainly on functionality. - Make sure to test your script thoroughly with different sets of command-line arguments.","solution":"import sys import os def process_arguments(args): This function processes command-line arguments and performs specified actions based on flags. if len(args) == 0: print(\\"No arguments provided.\\") return i = 0 while i < len(args): if args[i] == \'-n\' and i + 1 < len(args): name = args[i + 1] print(f\\"Hello, {name}!\\") i += 2 elif args[i] == \'-p\' and i + 1 < len(args): path = args[i + 1] print(os.path.abspath(path)) i += 2 elif args[i] == \'-s\' and i + 1 < len(args): file_path = args[i + 1] try: size = os.path.getsize(file_path) print(f\\"Size of {file_path} is {size} bytes\\") except OSError as e: print(f\\"Error getting size of {file_path}: {str(e)}\\") i += 2 elif args[i] == \'-e\' and i + 1 < len(args): try: exit_code = int(args[i + 1]) sys.exit(exit_code) except ValueError: print(\\"Invalid exit code provided.\\") i += 2 else: print(f\\"Invalid argument: {args[i]}\\") i += 1 def custom_audit_hook(event, args): Custom audit hook to log events triggered by specific sys functions. if event.startswith(\'sys\'): print(f\\"Audit event: {event}, Arguments: {args}\\") if __name__ == \\"__main__\\": sys.addaudithook(custom_audit_hook) process_arguments(sys.argv[1:])"},{"question":"**Coding Assessment Question** # Objective Create a custom container type in Python that supports cyclic garbage collection using the C-API provided by Python. # Requirements 1. **Container Type Definition:** - Define a new container type `MyContainer` which can hold references to other objects. This type should support the garbage collector. 2. **Memory Allocation:** - Use `PyObject_GC_New` or `PyObject_GC_NewVar` for allocating memory for the `MyContainer` instances. - Properly invoke `PyObject_GC_Track` after initializing the container’s fields. 3. **Container Deallocation:** - Ensure the deallocation process follows the GC protocol by calling `PyObject_GC_UnTrack` before invalidating fields and using `PyObject_GC_Del` to free memory. 4. **GC Handlers:** - Implement the `tp_traverse` handler to support GC traversal. Use `Py_VISIT` to visit all contained objects. - Implement the `tp_clear` handler to clear references that might create reference cycles. 5. **Type Registration:** - Register the new container type so that it is recognized by the Python interpreter and GC system. 6. **GC Control Functions:** - Implement functions to enable, disable, and query the state of the GC. # Input and Output - There is no specific input and output required for the container type itself. Your implementation will be tested via automated scripts that instantiate the container, store references, and check for correct GC behavior. - Ensure your functions handle multiple objects and cases where circular references might exist. # Constraints - Follow the C-API functions as mentioned in the documentation. - Handle memory management and GC interaction properly to prevent memory leaks or crashes. - Ensure the container type is robust and can be used safely in a multi-threaded environment if necessary. # Performance - The solution should be optimized for performance, ensuring minimal overhead during GC operations. # Example (Conceptual) Here is a conceptual skeleton to get you started. Details will vary based on your actual implementation. ```c #include <Python.h> typedef struct { PyObject_HEAD PyObject* contained_object; } MyContainer; static int MyContainer_traverse(MyContainer *self, visitproc visit, void *arg) { Py_VISIT(self->contained_object); return 0; } static int MyContainer_clear(MyContainer *self) { Py_CLEAR(self->contained_object); return 0; } static void MyContainer_dealloc(MyContainer* self) { PyObject_GC_UnTrack(self); MyContainer_clear(self); PyObject_GC_Del(self); } static PyTypeObject MyContainerType = { PyVarObject_HEAD_INIT(NULL, 0) .tp_name = \\"my_module.MyContainer\\", .tp_basicsize = sizeof(MyContainer), .tp_flags = Py_TPFLAGS_DEFAULT | Py_TPFLAGS_HAVE_GC, .tp_traverse = (traverseproc)MyContainer_traverse, .tp_clear = (inquiry)MyContainer_clear, .tp_dealloc = (destructor)MyContainer_dealloc, }; static int MyContainer_init(PyObject *self, PyObject *args, PyObject *kwds) { // Initialize container attributes return 0; } static PyObject* MyContainer_new(PyTypeObject *type, PyObject *args, PyObject *kwds) { MyContainer *self; self = (MyContainer *)PyObject_GC_New(MyContainer, type); if (self != NULL) { self->contained_object = NULL; PyObject_GC_Track(self); } return (PyObject *)self; } // Module initialization static PyModuleDef MyModule = { PyModuleDef_HEAD_INIT, .m_name = \\"my_module\\", .m_size = -1, }; PyMODINIT_FUNC PyInit_my_module(void) { PyObject *module; if (PyType_Ready(&MyContainerType) < 0) return NULL; module = PyModule_Create(&MyModule); if (module == NULL) return NULL; Py_INCREF(&MyContainerType); if (PyModule_AddObject(module, \\"MyContainer\\", (PyObject *)&MyContainerType) < 0) { Py_DECREF(&MyContainerType); Py_DECREF(module); return NULL; } return module; } ``` Complete the implementation of `MyContainer` with proper reference handling and garbage collection mechanics. Good Luck!","solution":"def add(a, b): Returns the sum of a and b. return a + b"},{"question":"# Question: Seaborn Visualization of Health Expenditure Data You are provided with a health expenditure dataset where you need to create a customized visualization using Seaborn\'s `objects` interface. The dataset contains the following columns: - Year: Year of the data. - Country: Name of the country. - Spending_USD: Health expenditure in USD for that year. # Tasks: 1. **Data Loading and Transformation:** - Load the dataset using `seaborn.load_dataset(\'healthexp\')`. - Transform the dataset to have columns: `Year`, `Country`, and `Spending_USD`. 2. **Visualization:** - Create a faceted area plot by country for the health expenditure over the years. - Set the color of the areas to represent different countries. - Add a line on top of the area to denote the spending more clearly. # Input: - The dataset loaded using `seaborn.load_dataset(\'healthexp\')`. # Output: - A faceted area plot by country using Seaborn\'s `objects` interface. # Constraints: 1. The data transformation must handle any missing values by interpolating. 2. The visualization must use facets to create separate plots for each country, with a maximum of 3 facets per row. # Example Implementation: The following example illustrates the basic structure of your solution. ```python import seaborn.objects as so from seaborn import load_dataset # Load and transform the dataset healthexp = ( load_dataset(\\"healthexp\\") .pivot(index=\\"Year\\", columns=\\"Country\\", values=\\"Spending_USD\\") .interpolate() .stack() .rename(\\"Spending_USD\\") .reset_index() .sort_values(\\"Country\\") ) # Create the faceted area plot by country p = so.Plot(healthexp, \\"Year\\", \\"Spending_USD\\").facet(\\"Country\\", wrap=3) p.add(so.Area(), color=\\"Country\\").add(so.Line()) # Display the plot p.show() ``` Make sure your implementation meets the requirements and constraints specified above.","solution":"import seaborn.objects as so from seaborn import load_dataset # Load and transform the dataset healthexp = ( load_dataset(\\"healthexp\\") .pivot(index=\\"Year\\", columns=\\"Country\\", values=\\"Spending_USD\\") .interpolate() .stack() .rename(\\"Spending_USD\\") .reset_index() .sort_values(\\"Country\\") ) # Create the faceted area plot by country p = so.Plot(healthexp, \\"Year\\", \\"Spending_USD\\").facet(\\"Country\\", wrap=3) p.add(so.Area(), color=\\"Country\\").add(so.Line()) # Display the plot p.show()"},{"question":"**Problem Statement:** You are tasked with creating a function called `customize_plot` that generates a line plot with specified x and y data. You must allow customization of the plot\'s context, font scale, and additional parameters. The objective is to demonstrate your understanding of seaborn\'s `set_context` function. # Function Signature: ```python def customize_plot(x: list, y: list, context: str = \\"notebook\\", font_scale: float = 1.0, **rc_params): pass ``` # Input: - `x` (list): A list of numerical values for the x-axis. - `y` (list): A list of numerical values for the y-axis. - `context` (str): A string specifying the context to set for the plot (e.g., \\"paper\\", \\"notebook\\", \\"talk\\", \\"poster\\"). Default is \\"notebook\\". - `font_scale` (float): A float value to scale the font size of the elements in the plot. Default value is 1.0. - `**rc_params`: Additional keyword arguments for modifying specific parameters of the plotting context. # Output: The function should not return anything. It should display a line plot with the specified parameters using seaborn. # Example: ```python x = [0, 1, 2, 3, 4] y = [10, 15, 8, 12, 14] # Generate a plot with \\"talk\\" context and larger font customize_plot(x, y, context=\\"talk\\", font_scale=1.5, lines_linewidth=2.5) # Generate a plot with \\"paper\\" context and thicker lines customize_plot(x, y, context=\\"paper\\", rc={\\"lines.linewidth\\": 4}) ``` # Constraints: 1. Ensure that `x` and `y` are lists of equal length. 2. The function should handle invalid contexts gracefully by falling back to the default \\"notebook\\" context. 3. Additional parameters should be valid key-value pairs suitable for seaborn\'s `rc` configuration. # Tips: 1. Use `sns.set_context()` to set the context and configuration of the plot. 2. Use `sns.lineplot()` to create and display the line plot using the provided x and y data. 3. Handle edge cases such as invalid context strings or parameter keys that do not exist in `rc`.","solution":"import seaborn as sns import matplotlib.pyplot as plt def customize_plot(x: list, y: list, context: str = \\"notebook\\", font_scale: float = 1.0, **rc_params): Generates a line plot with specified x and y data, allowing customization of the plot\'s context, font scale, and additional parameters. Parameters: - x (list): Numerical values for the x-axis. - y (list): Numerical values for the y-axis. - context (str): Context to set for the plot (e.g., \\"paper\\", \\"notebook\\", \\"talk\\", \\"poster\\"). Default is \\"notebook\\". - font_scale (float): Float value to scale the font size of the elements in the plot. Default is 1.0. - **rc_params: Additional keyword arguments for modifying specific parameters of the plotting context. Returns: - None. The function displays a line plot with the specified parameters. try: sns.set_context(context, font_scale=font_scale, rc=rc_params) except ValueError: # If an invalid context is provided, fall back to the default \\"notebook\\" context sns.set_context(\\"notebook\\", font_scale=font_scale, rc=rc_params) sns.lineplot(x=x, y=y) plt.show()"},{"question":"You have been given access to a low-level C API that manages \\"cell\\" objects. These objects are used to store variables referenced by multiple scopes. Your task is to simulate part of this API in pure Python to understand the internal working and management of these \\"cell\\" objects without using any C extensions. Objective Implement a `Cell` class in Python to simulate the behavior of the cell objects described. Your class should include the following methods: 1. **`__init__(self, value=None)`**: Initialize a new cell object with the given value. If no value is provided, initialize the cell with `None`. 2. **`get(self)`**: Return the current value contained in the cell. 3. **`set(self, value)`**: Set the cell\'s value to the provided value. 4. **`is_cell(obj)`**: Static method that returns True if the provided object is an instance of the `Cell` class. Constraints - The class must not use any external libraries. - The class should mimic the reference and dereference behavior described, but you don’t need to manage reference counting as Python does this automatically. - Ensure that setting a value in the cell overwrites any previous value stored. Example Usage ```python cell = Cell(10) # Initialize a cell with value 10 print(cell.get()) # Output should be 10 cell.set(20) # Update the cell\'s value to 20 print(cell.get()) # Output should be 20 print(Cell.is_cell(cell)) # Output should be True print(Cell.is_cell(100)) # Output should be False ``` Implementation ```python class Cell: def __init__(self, value=None): # Create and return a new cell object containing the value `value`. self.value = value def get(self): # Return the contents of the cell. return self.value def set(self, value): # Set the contents of the cell object to `value`. self.value = value @staticmethod def is_cell(obj): # Return true if `obj` is a cell object. return isinstance(obj, Cell) ``` Implement the `Cell` class according to the above specification and test your implementation with the given example usage.","solution":"class Cell: def __init__(self, value=None): Initialize a new cell object with the given value. If no value is provided, initialize the cell with None. self.value = value def get(self): Return the current value contained in the cell. return self.value def set(self, value): Set the cell\'s value to the provided value. self.value = value @staticmethod def is_cell(obj): Static method that returns True if the provided object is an instance of the Cell class. return isinstance(obj, Cell)"},{"question":"# Decision Tree Assessment **Objective**: The objective of this assignment is to evaluate your understanding of decision trees using the scikit-learn library. You will implement a multi-output decision tree regressor, handle missing values in the dataset, and perform pruning to avoid overfitting. The task will involve data preprocessing, model training, evaluation, and visualization. **Problem Statement**: You are provided with a dataset containing features and two target variables. Your task is to build, visualize, and evaluate a decision tree regressor that predicts both target variables simultaneously. You should also handle any missing values in the dataset and employ pruning techniques to optimize the model. **Dataset**: - You\'ll use the `load_linnerud` dataset from scikit-learn, which contains physiological measurements. Load the dataset using `sklearn.datasets.load_linnerud()`. This dataset includes: - `data`: A 2D array where each row is a sample, and columns are features (n_samples, n_features). - `target`: A 2D array where each row corresponds to the target values (n_samples, n_outputs). **Requirements**: 1. **Data Preparation**: - Split the dataset into training and testing sets using an 80-20 split. 2. **Model Training**: - Train a `DecisionTreeRegressor` on the training set. - Use appropriate handling for any missing values in the dataset. 3. **Model Evaluation**: - Evaluate the model on the test set using Mean Squared Error (MSE) for both output variables. 4. **Visualization**: - Visualize the trained decision tree using `plot_tree`. 5. **Pruning**: - Apply minimal cost-complexity pruning to the decision tree to reduce overfitting. - Evaluate the pruned tree model on the test set and compare the performance before and after pruning. **Expected Input and Output**: 1. **Input**: - No direct input is needed as the data will be loaded from `sklearn.datasets.load_linnerud()`. 2. **Output**: - Print the MSE for both output variables before and after pruning. - Display the plot of the trained decision tree before and after pruning. **Code Structure**: ```python from sklearn.datasets import load_linnerud from sklearn.model_selection import train_test_split from sklearn.tree import DecisionTreeRegressor, plot_tree from sklearn.metrics import mean_squared_error import matplotlib.pyplot as plt # Load dataset data = load_linnerud() X = data.data y = data.target # Split data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Create and train the model regressor = DecisionTreeRegressor(random_state=42) # Handling missing values: Implement if necessary # Fit the model regressor.fit(X_train, y_train) # Predict and evaluate y_pred = regressor.predict(X_test) mse_before_pruning = mean_squared_error(y_test, y_pred, multioutput=\'raw_values\') print(\\"MSE before pruning:\\", mse_before_pruning) # Visualize the tree plt.figure(figsize=(15, 10)) plot_tree(regressor, filled=True) plt.title(\\"Decision Tree before Pruning\\") plt.show() # Apply pruning path = regressor.cost_complexity_pruning_path(X_train, y_train) ccp_alphas = path.ccp_alphas # Train on different alphas and choose the best # Pruning Implementation # Refit the pruned model, evaluate, and display tree # Visualize the pruned tree # Predict and evaluate pruned tree # Print MSE after pruning ``` **Constraints**: - Use `random_state=42` for reproducibility. - Ensure that your tree visualizations are clear and properly labeled. *Note*: Handle any missing values appropriately, even if the dataset provided has none, your implementation should be robust enough to handle such scenarios generally. **Performance Parameters**: - Code correctness: Ensure the model creation, training, evaluation, and visualization are implemented correctly. - Handling of missing values and pruning technique. - Clarity and labeling of visualizations. **Bonus**: For extra credit, explain how different hyperparameters (`max_depth`, `min_samples_split`, etc.) can affect the performance of the decision tree and how you would tune them.","solution":"from sklearn.datasets import load_linnerud from sklearn.model_selection import train_test_split from sklearn.tree import DecisionTreeRegressor, plot_tree from sklearn.metrics import mean_squared_error import matplotlib.pyplot as plt import numpy as np def decision_tree_regression_with_pruning(): # Load dataset data = load_linnerud() X = data.data y = data.target # Split data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Handle missing values (if any) - here use mean imputation as an example from sklearn.impute import SimpleImputer imputer = SimpleImputer(strategy=\'mean\') X_train = imputer.fit_transform(X_train) X_test = imputer.transform(X_test) # Create and train the model regressor = DecisionTreeRegressor(random_state=42) regressor.fit(X_train, y_train) # Predict and evaluate y_pred = regressor.predict(X_test) mse_before_pruning = mean_squared_error(y_test, y_pred, multioutput=\'raw_values\') print(\\"MSE before pruning:\\", mse_before_pruning) # Visualize the tree plt.figure(figsize=(15, 10)) plot_tree(regressor, filled=True) plt.title(\\"Decision Tree before Pruning\\") plt.show() # Apply minimal cost-complexity pruning path = regressor.cost_complexity_pruning_path(X_train, y_train) ccp_alphas = path.ccp_alphas[:-1] # Exclude the maximum value which prunes all leaves models = [] for ccp_alpha in ccp_alphas: model = DecisionTreeRegressor(random_state=42, ccp_alpha=ccp_alpha) model.fit(X_train, y_train) models.append(model) # Find the best pruned model using cross-validation or a specific metric mse_values = [mean_squared_error(y_test, model.predict(X_test), multioutput=\'raw_values\') for model in models] best_alpha_index = np.argmin(np.mean(mse_values, axis=1)) best_model = models[best_alpha_index] mse_after_pruning = mse_values[best_alpha_index] print(\\"MSE after pruning:\\", mse_after_pruning) # Visualize the pruned tree plt.figure(figsize=(15, 10)) plot_tree(best_model, filled=True) plt.title(\\"Decision Tree after Pruning\\") plt.show() return mse_before_pruning, mse_after_pruning, regressor, best_model mse_before_pruning, mse_after_pruning, regressor, best_model = decision_tree_regression_with_pruning()"},{"question":"You are provided with a dataset containing information about various planets discovered using different methods. Your task is to create a series of visualizations to analyze the distribution of some key attributes in this dataset using the `seaborn` library. This will test your understanding of fundamental and advanced concepts of the `sns.histplot` function. # Dataset The `planets` dataset can be loaded using the following code: ```python import seaborn as sns planets = sns.load_dataset(\\"planets\\") ``` # Instructions 1. Load the `planets` dataset and inspect its structure to understand the available columns and data types. 2. Generate a histogram of the `distance` attribute of the planets dataset, ensuring the following: - Use a logarithmic scale for the x-axis to handle skewness in the data. - Add a Kernel Density Estimate (KDE) to the histogram to visualize the distribution\'s shape better. 3. Create a comparative histogram for the `distance` attribute, grouped by the `method` used to discover the planets. Ensure the following: - Use different colors for each method using the `hue` parameter. - Normalize the histograms so that the heights represent densities, making it easier to compare different groups. - Use step elements to make overlapping distributions more distinguishable. 4. Generate a bivariate histogram to visualize the relationship between the `year` and `distance` attributes. Ensure the following: - Apply logarithmic scaling to the `distance` axis. - Add a colorbar to annotate the colormap. - Make cells with no observations transparent. 5. [Advanced] Create a custom function `plot_histogram` that: - Takes parameters for the dataset, x and y attributes, hue attribute, log_scale options for x and y axes, normalization method, element type, and whether to add a colorbar. - Returns a seaborn plot based on the specified parameters. 6. Use your `plot_histogram` function to recreate the previous three histograms according to specifications. # Constraints - You must use the `seaborn` library, specifically the `sns.histplot` function. - Ensure your visualizations are clear and legible. - Follow best practices for data visualization, including appropriate axis labels and titles. # Expected Output Your submission should include: - The code for generating the above visualizations. - The generated plots as visual verification of your code\'s correctness. - The implementation of the `plot_histogram` function and its usage examples. # Example ```python import seaborn as sns import matplotlib.pyplot as plt # Load the dataset planets = sns.load_dataset(\\"planets\\") # 1. Histogram of the distance attribute plt.figure(figsize=(10, 6)) sns.histplot(data=planets, x=\\"distance\\", log_scale=True, kde=True) plt.title(\'Histogram of Planet Distances\') plt.xlabel(\'Distance (log scale)\') plt.ylabel(\'Count\') plt.show() # 2. Comparative histogram plt.figure(figsize=(10, 6)) sns.histplot(data=planets, x=\\"distance\\", hue=\\"method\\", element=\\"step\\", stat=\\"density\\", log_scale=True) plt.title(\'Comparative Histogram of Planet Distances by Discovery Method\') plt.xlabel(\'Distance (log scale)\') plt.ylabel(\'Density\') plt.show() # 3. Bivariate histogram plt.figure(figsize=(10, 6)) sns.histplot(data=planets, x=\\"year\\", y=\\"distance\\", log_scale=(False, True), cbar=True, cbar_kws=dict(shrink=.75)) plt.title(\'Bivariate Histogram of Year of Discovery vs Distance\') plt.xlabel(\'Year of Discovery\') plt.ylabel(\'Distance (log scale)\') plt.show() # 4. Custom plot_histogram function def plot_histogram(data, x, y=None, hue=None, log_scale=(False, False), stat=\'count\', element=\'bars\', cbar=False): plt.figure(figsize=(10, 6)) sns.histplot(data=data, x=x, y=y, hue=hue, log_scale=log_scale, stat=stat, element=element, cbar=cbar, cbar_kws=dict(shrink=.75)) plt.title(f\'Histogram of {x}\' + (f\' vs {y}\' if y else \'\')) plt.xlabel(f\'{x}\' + (\' (log scale)\' if log_scale[0] else \'\')) plt.ylabel(f\'{y}\' + (\' (log scale)\' if log_scale[1] else \'\') if y else \'Count\') plt.show() # Usage examples plot_histogram(data=planets, x=\\"distance\\", log_scale=(True, False), kde=True) plot_histogram(data=planets, x=\\"distance\\", hue=\\"method\\", element=\\"step\\", stat=\\"density\\", log_scale=(True, False)) plot_histogram(data=planets, x=\\"year\\", y=\\"distance\\", log_scale=(False, True), cbar=True) ```","solution":"import seaborn as sns import matplotlib.pyplot as plt # Load the dataset planets = sns.load_dataset(\\"planets\\") # 1. Histogram of the distance attribute plt.figure(figsize=(10, 6)) sns.histplot(data=planets, x=\\"distance\\", log_scale=True, kde=True) plt.title(\'Histogram of Planet Distances\') plt.xlabel(\'Distance (log scale)\') plt.ylabel(\'Count\') plt.show() # 2. Comparative histogram plt.figure(figsize=(10, 6)) sns.histplot(data=planets, x=\\"distance\\", hue=\\"method\\", element=\\"step\\", stat=\\"density\\", log_scale=True) plt.title(\'Comparative Histogram of Planet Distances by Discovery Method\') plt.xlabel(\'Distance (log scale)\') plt.ylabel(\'Density\') plt.show() # 3. Bivariate histogram plt.figure(figsize=(10, 6)) sns.histplot(data=planets, x=\\"year\\", y=\\"distance\\", log_scale=(False, True), cbar=True, cbar_kws=dict(shrink=.75)) plt.title(\'Bivariate Histogram of Year of Discovery vs Distance\') plt.xlabel(\'Year of Discovery\') plt.ylabel(\'Distance (log scale)\') plt.show() # 4. Custom plot_histogram function def plot_histogram(data, x, y=None, hue=None, log_scale=(False, False), stat=\'count\', element=\'bars\', cbar=False): plt.figure(figsize=(10, 6)) sns.histplot(data=data, x=x, y=y, hue=hue, log_scale=log_scale, stat=stat, element=element, cbar=cbar, cbar_kws=dict(shrink=.75)) plt.title(f\'Histogram of {x}\' + (f\' vs {y}\' if y else \'\')) plt.xlabel(f\'{x}\' + (\' (log scale)\' if log_scale[0] else \'\')) plt.ylabel(f\'{y}\' + (\' (log scale)\' if log_scale[1] else \'\') if y else \'Count\') plt.show() # Usage examples plot_histogram(data=planets, x=\\"distance\\", log_scale=(True, False), stat=\\"count\\", element=\\"bars\\") plot_histogram(data=planets, x=\\"distance\\", hue=\\"method\\", element=\\"step\\", stat=\\"density\\", log_scale=(True, False)) plot_histogram(data=planets, x=\\"year\\", y=\\"distance\\", log_scale=(False, True), cbar=True)"},{"question":"**Question:** You are required to write a Python function `introspect_object` that takes an object as input and returns a detailed dictionary containing various introspection details about the object. This will demonstrate your understanding of the `inspect` module and its functionalities. # Function Signature: ```python def introspect_object(obj: object) -> dict: pass ``` # Input: - An object `obj` which can be of any type (module, class, method, function, etc.). # Output: - A dictionary with the following keys and their corresponding values: - `\'type\'`: The type of the object (e.g., `\'module\'`, `\'class\'`, `\'function\'`, etc.). - `\'members\'`: A dictionary of all members of the object, where keys are member names and values are their types. - `\'source_code\'` (if the object is a function, method, class, module, traceback, frame, or code object): The source code of the object. - `\'call_signatures\'` (if the object is callable): The call signature of the object as a string. - `\'doc_string\'`: The documentation string of the object. - `\'attributes\'`: A dictionary of other relevant attributes of the object based on its type. # Constraints: - Use the `inspect` module to perform all introspections. - Handle cases where getting some information might fail (e.g., some objects might not have source code available) gracefully by setting the value to `None`. # Example: ```python import math def example_function(a, b=42): An example function. return a + b class ExampleClass: An example class. def method(self): An example method. pass # Example usage: result = introspect_object(example_function) print(result) # Expected output: # { # \'type\': \'function\', # \'members\': { \'__annotations__\': \'dict\', \'__call__\': \'method-wrapper\', ... }, # \'source_code\': \'def example_function(a, b=42):n An example function.n return a + bn\', # \'call_signatures\': \'(a, b=42)\', # \'doc_string\': \'An example function.\', # \'attributes\': { \'__name__\': \'example_function\', \'__globals__\': \'dict\', ... } # } ``` Your implementation should cover various types of objects and extract relevant introspection details sensibly. **Hint:** Use the `inspect` module\'s functions like `ismodule()`, `isclass()`, `ismethod()`, `isfunction()`, `getmembers()`, `getdoc()`, `getsource()`, `signature()`, and so on to gather information about the object.","solution":"import inspect def introspect_object(obj: object) -> dict: result = {} result[\'type\'] = type(obj).__name__ result[\'members\'] = {name: type(member).__name__ for name, member in inspect.getmembers(obj)} try: if inspect.ismodule(obj) or inspect.isclass(obj) or inspect.isfunction(obj) or inspect.ismethod(obj) or inspect.iscode(obj) or inspect.isframe(obj) or inspect.istraceback(obj): result[\'source_code\'] = inspect.getsource(obj) else: result[\'source_code\'] = None except (TypeError, OSError): result[\'source_code\'] = None if callable(obj): try: result[\'call_signatures\'] = str(inspect.signature(obj)) except (TypeError, ValueError): result[\'call_signatures\'] = None else: result[\'call_signatures\'] = None result[\'doc_string\'] = inspect.getdoc(obj) result[\'attributes\'] = {name: getattr(obj, name) for name in dir(obj) if not name.startswith(\'_\')} return result"},{"question":"**Advanced Python Coding Assessment Question:** # Problem Statement You are given the task of writing a networked application in Python, leveraging the `asyncio` module. Your application needs to function correctly across both Windows and macOS, taking into account specific limitations and capabilities detailed below. # Requirements 1. **Unix Domain Sockets**: Create a server that listens for incoming connections. Note that Unix Domain Sockets (`AF_UNIX`) are only supported on Unix-based systems (e.g., macOS). On Windows, you should fall back to using TCP/IP sockets. 2. **Event Loop Selection**: On Windows, ensure that the appropriate event loop is chosen: - Use `ProactorEventLoop` for subprocess support. - Only fall back to `SelectorEventLoop` if there’s an explicit reason why `ProactorEventLoop` cannot be used. 3. **Subprocess Management**: - On Windows, ensure subprocess management works by leveraging `ProactorEventLoop`. If running on macOS, subprocess management should work normally as it fully supports subprocess capabilities. 4. **Character Device Support**: On macOS versions 10.6, 10.7, and 10.8, ensure the event loop can handle character devices. Use `SelectSelector` or `PollSelector` for compatibility. # Function Definition Implement a function `start_server_and_subprocess()` that: 1. Initializes the correct event loop based on the platform and requirements. 2. Sets up a server that listens either using Unix Domain Sockets (if possible) or TCP/IP. 3. Manages subprocesses effectively on both platforms. ```python import asyncio import platform import sys async def start_server_and_subprocess(): # Step 1: Initialize the correct event loop based on the platform # Step 2: Set up a server with appropriate socket based on platform # Step 3: Manage subprocesses within your established event loop pass ``` # Constraints - **Input/Output**: There are no direct inputs or outputs for this function. The objective is to ensure the function performs the set-up correctly on the platform it runs, which can be verified via testing. - **Performance**: - The server should handle multiple client connections concurrently. - Subprocess management should not block or significantly delay the main event loop\'s operations. # Additional Information You can use libraries such as `selectors`, `os` (for subprocesses), and any part of the `asyncio` module as needed. Consider edge cases like unsupported features (as documented) and handle exceptions accordingly to ensure graceful degradation of functionalities when an unsupported feature is encountered. # Example This is a conceptual example to illustrate expected behavior: ```python # On a Unix-based system: loop = asyncio.get_event_loop() server = await asyncio.start_unix_server(handle_client, path=\'/tmp/socket\') # On a Windows system: try: loop = asyncio.ProactorEventLoop() asyncio.set_event_loop(loop) server = await asyncio.start_server(handle_client, \'127.0.0.1\', 8888) except NotImplementedError as e: print(f\\"Warning: {e}. Falling back to SelectorEventLoop.\\") loop = asyncio.SelectorEventLoop() asyncio.set_event_loop(loop) server = await asyncio.start_server(handle_client, \'127.0.0.1\', 8888) ``` --- Ensure that your implementation adheres strictly to the requirements and handles all documented limitations effectively.","solution":"import asyncio import os import sys import platform async def handle_client(reader, writer): try: while True: data = await reader.read(100) if not data: break writer.write(data) await writer.drain() except asyncio.CancelledError: pass finally: writer.close() await writer.wait_closed() async def start_server_and_subprocess(): system_platform = platform.system() # Choosing the appropriate event loop based on platform if system_platform == \'Windows\': try: loop = asyncio.ProactorEventLoop() asyncio.set_event_loop(loop) except NotImplementedError: print(\\"ProactorEventLoop not available. Falling back to SelectorEventLoop.\\") loop = asyncio.SelectorEventLoop() asyncio.set_event_loop(loop) else: loop = asyncio.get_event_loop() async def start_server(): if system_platform != \'Windows\': # Using Unix domain socket on Unix-based systems server = await asyncio.start_unix_server(handle_client, path=\'/tmp/socket\') else: # Using TCP/IP sockets on Windows server = await asyncio.start_server(handle_client, \'127.0.0.1\', 8888) async with server: await server.serve_forever() async def start_subprocess(): # Dummy subprocess, e.g., echoing \\"Hello\\" on the terminal proc = await asyncio.create_subprocess_exec(sys.executable, \'-c\', \'print(\\"Hello from subprocess!\\")\') await proc.communicate() await asyncio.gather(start_server(), start_subprocess()) if __name__ == \\"__main__\\": asyncio.run(start_server_and_subprocess())"},{"question":"You have been provided with a dataset about the different species of penguins (`\\"penguins\\"`), which you can load using `sns.load_dataset(\\"penguins\\")`. Your task is to create a function `plot_penguin_ecdf` that generates the following ECDF plots: 1. **Basic ECDF Plot**: Plot an ECDF of the penguin\'s flipper length (in mm) along the x-axis. 2. **ECDF Plot with Hue**: Plot the ECDF of `bill_length_mm`, and separate the lines by different penguin species using the `hue` parameter. 3. **Complementary ECDF Plot**: Plot the complementary ECDF (1 - CDF) of the `bill_depth_mm` for different species using the `hue` parameter. 4. **ECDF with Count Stat**: Plot the ECDF of `body_mass_g` displayed with absolute counts. # Function Signature ```python import seaborn as sns import matplotlib.pyplot as plt def plot_penguin_ecdf(): # Your code here pass ``` # Implementation Details 1. **Basic ECDF Plot**: - Plot the empirical cumulative distribution function of `flipper_length_mm`. 2. **ECDF Plot with Hue**: - Plot the ECDF of `bill_length_mm`. - Use the `hue` parameter to color code the distribution by `species`. 3. **Complementary ECDF Plot**: - Plot the complementary ECDF of `bill_depth_mm`. - Use the `hue` parameter to separate the lines by `species`. 4. **ECDF with Count Stat**: - Plot the ECDF of `body_mass_g`. - Set the `stat` parameter to `count` to display absolute counts instead of proportions. # Constraints - Ensure proper labels and titles for each plot. - Use `sns.load_dataset(\\"penguins\\")` to get the dataset. # Example Output The function should render four ECDF plots as described above. ```python plot_penguin_ecdf() # Should output four different plots ``` # Notes - You should use `seaborn` for plotting and ensure the plots are clear and informative. - Handle edge cases where the dataset might not be loaded properly or have missing values.","solution":"import seaborn as sns import matplotlib.pyplot as plt def plot_penguin_ecdf(): # Load the penguins dataset penguins = sns.load_dataset(\\"penguins\\") # Handle missing values by dropping them penguins = penguins.dropna() # Basic ECDF Plot plt.figure(figsize=(12, 8)) plt.subplot(2, 2, 1) sns.ecdfplot(data=penguins, x=\\"flipper_length_mm\\") plt.title(\\"ECDF of Flipper Length (mm)\\") plt.xlabel(\\"Flipper Length (mm)\\") plt.ylabel(\\"ECDF\\") # ECDF Plot with Hue plt.subplot(2, 2, 2) sns.ecdfplot(data=penguins, x=\\"bill_length_mm\\", hue=\\"species\\") plt.title(\\"ECDF of Bill Length (mm) by Species\\") plt.xlabel(\\"Bill Length (mm)\\") plt.ylabel(\\"ECDF\\") # Complementary ECDF Plot plt.subplot(2, 2, 3) sns.ecdfplot(data=penguins, x=\\"bill_depth_mm\\", hue=\\"species\\", complementary=True) plt.title(\\"Complementary ECDF of Bill Depth (mm) by Species\\") plt.xlabel(\\"Bill Depth (mm)\\") plt.ylabel(\\"1 - ECDF\\") # ECDF with Count Stat plt.subplot(2, 2, 4) sns.ecdfplot(data=penguins, x=\\"body_mass_g\\", stat=\\"count\\") plt.title(\\"ECDF of Body Mass (g) with Absolute Counts\\") plt.xlabel(\\"Body Mass (g)\\") plt.ylabel(\\"Count\\") # Adjust layout plt.tight_layout() plt.show()"},{"question":"<|Analysis Begin|> The provided documentation focuses on the use of the `seaborn.objects` interface to create plots with the `so.Plot` object. It specifically demonstrates two examples: 1. Plotting health expenditure over years for various countries and normalizing values relative to the maximum value within each group. 2. Plotting the percentage change in health spending from a baseline year (1970). Key concepts from the documentation that can be targeted for the assessment include: - Loading a dataset using `seaborn.load_dataset` - Creating a plot with `so.Plot` - Adding layers to a plot with `so.Lines` - Applying normalization using `so.Norm` - Labeling axes with `.label` An effective question will challenge students to use these concepts comprehensively, ensuring they understand both basic plotting and the use of normalization transformations. <|Analysis End|> <|Question Begin|> # Coding Assessment Question You are provided with a dataset on health expenditure across different countries over several years. Your task is to analyze and visualize this data using Seaborn\'s objects interface. Your visualization should help in understanding how health expenditure has changed over time across different countries. Requirements 1. **Load the dataset**: Use Seaborn to load the `healthexp` dataset. 2. **Normalization Plot**: - Create a plot that shows how health spending (in USD) has changed relative to the maximum spending observed for each country over the years. - Normalize the spending values such that each value represents the percentage of the maximum spending for that country. - Label the y-axis as \\"Spending relative to maximum amount\\". 3. **Percent Change Plot**: - Create another plot that shows the percent change in health spending from the year 1970 for each country. - Normalize the spending values using the year 1970 as the baseline. - Label the y-axis as \\"Percent change in spending from 1970 baseline\\". Input - No input parameters are required for this task. Output - Two Seaborn plots generated using the `so.Plot` interface, following the described requirements. Constraints - You should use the Seaborn library and its objects interface (`seaborn.objects`) for plotting. - Ensure that plots utilize the provided dataset and apply the necessary transformations as described. Example The following is an example of how to start with the dataset loading and using `so.Plot`. You need to complete the rest according to the requirements. ```python import seaborn.objects as so from seaborn import load_dataset # Load the dataset healthexp = load_dataset(\\"healthexp\\") # Task 1: Normalization plot ( so.Plot(healthexp, x=\\"Year\\", y=\\"Spending_USD\\", color=\\"Country\\") .add(so.Lines(), so.Norm()) .label(y=\\"Spending relative to maximum amount\\") ) # Task 2: Percent change plot ( so.Plot(healthexp, x=\\"Year\\", y=\\"Spending_USD\\", color=\\"Country\\") .add(so.Lines(), so.Norm(where=\\"x == x.min()\\", percent=True)) .label(y=\\"Percent change in spending from 1970 baseline\\") ) ``` Make sure to validate the correctness of your plots according to the requirements specified.","solution":"import seaborn.objects as so from seaborn import load_dataset def create_normalized_plot(): # Load the dataset healthexp = load_dataset(\\"healthexp\\") # Normalization plot plot1 = ( so.Plot(healthexp, x=\\"Year\\", y=\\"Spending_USD\\", color=\\"Country\\") .add(so.Lines(), so.Norm()) .label(y=\\"Spending relative to maximum amount\\") ) return plot1 def create_percent_change_plot(): # Load the dataset healthexp = load_dataset(\\"healthexp\\") # Percent change plot plot2 = ( so.Plot(healthexp, x=\\"Year\\", y=\\"Spending_USD\\", color=\\"Country\\") .add(so.Lines(), so.Norm(where=\\"x == 1970\\", percent=True)) .label(y=\\"Percent change in spending from 1970 baseline\\") ) return plot2"},{"question":"**Objective:** To assess your understanding of advanced visualization techniques using the seaborn library\'s `PairGrid` for creating complex multi-plot grids. # Question: You are given a dataset of penguin measurements. Your task is to create a comprehensive visualization using seaborn\'s `PairGrid` class. Follow these steps to accomplish this: 1. **Load the Dataset:** Load the penguins dataset from seaborn\'s built-in datasets. 2. **Create a `PairGrid`:** Set up a PairGrid with the following specifications: - Use \\"species\\" as the hue parameter to color the plots by species. - Use the following variables: `body_mass_g`, `bill_length_mm`, `bill_depth_mm`, `flipper_length_mm`. 3. **Map the Functions:** - On the diagonal, plot histograms with stacks and steps. - On the off-diagonal, plot scatter plots. 4. **Customize the Visuals:** - Ensure the plot uses an appropriate color palette. - Add a legend to the plot to distinguish between species. 5. **Save the Plot:** Save the resulting plot into a file named `penguins_pairgrid.png`. # Input and Output: - **Input:** There is no direct input from the user. - **Output:** The final visualization saved as `penguins_pairgrid.png`. # Constraints: - Use only seaborn and matplotlib libraries for the visualization. - Ensure the code is efficient and adheres to clean coding practices. # Example: Your code should look like this: ```python import seaborn as sns import matplotlib.pyplot as plt # Load the dataset penguins = sns.load_dataset(\\"penguins\\") # Create the PairGrid variables = [\\"body_mass_g\\", \\"bill_length_mm\\", \\"bill_depth_mm\\", \\"flipper_length_mm\\"] g = sns.PairGrid(penguins, hue=\\"species\\", vars=variables) # Map the functions g.map_diag(sns.histplot, multiple=\\"stack\\", element=\\"step\\") g.map_offdiag(sns.scatterplot) # Customize the plot g.add_legend() # Save the plot plt.savefig(\\"penguins_pairgrid.png\\") plt.show() ``` Take care to ensure all requirements are met for full credit.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_penguins_pairgrid(): Creates and saves a PairGrid plot for the penguins dataset. # Load the dataset penguins = sns.load_dataset(\\"penguins\\") # Create the PairGrid variables = [\\"body_mass_g\\", \\"bill_length_mm\\", \\"bill_depth_mm\\", \\"flipper_length_mm\\"] g = sns.PairGrid(penguins, hue=\\"species\\", vars=variables) # Map the functions g.map_diag(sns.histplot, multiple=\\"stack\\", element=\\"step\\") g.map_offdiag(sns.scatterplot) # Customize the plot g.add_legend() # Save the plot plt.savefig(\\"penguins_pairgrid.png\\") plt.show()"},{"question":"Objective: Your task is to create a function that reads and parses EA IFF 85 chunks from a binary file and returns a list of dictionaries, each representing a chunk with its ID and data. Description: Write a function `parse_chunks(file_path: str) -> List[Dict[str, Union[str, bytes]]]` that processes a binary file containing multiple EA IFF 85 chunks. Each chunk should be parsed into a dictionary with the following keys: - `\\"ID\\"`: The 4-byte ID of the chunk as a string. - `\\"Data\\"`: The data bytes of the chunk. The function should read through the entire file, handling all chunks until the end of the file is reached. Implementation Details: 1. Use the `chunk.Chunk` class from the provided module to handle the chunk reading. 2. The function should be able to handle both big-endian and little-endian formats. 3. Ensure that the chunk reading respects the padding requirement for aligned chunks. Example: Let\'s assume you have a binary file `example.iff` with the following chunks: - ID: \\"FORM\\", Data: b\'x00x01x02x03\' - ID: \\"DATA\\", Data: b\'x04x05x06x07\' Your function should return: ```python [ {\\"ID\\": \\"FORM\\", \\"Data\\": b\'x00x01x02x03\'}, {\\"ID\\": \\"DATA\\", \\"Data\\": b\'x04x05x06x07\'} ] ``` Function Signature: ```python def parse_chunks(file_path: str) -> List[Dict[str, Union[str, bytes]]]: ``` Constraints: - The file size can be up to 10MB. - Each chunk size will fit into the memory. Notes: - The function should handle any exceptions that may arise from opening or reading the file. - Include proper type annotations for your function. Good luck!","solution":"import struct from typing import List, Dict, Union def parse_chunks(file_path: str) -> List[Dict[str, Union[str, bytes]]]: chunks = [] try: with open(file_path, \'rb\') as file: while True: chunk_header = file.read(8) if len(chunk_header) < 8: break chunk_id, chunk_size = struct.unpack(\'>4sI\', chunk_header) chunk_id = chunk_id.decode(\'ascii\') chunk_data = file.read(chunk_size) if len(chunk_data) < chunk_size: break chunks.append({\\"ID\\": chunk_id, \\"Data\\": chunk_data}) # If the chunk size is odd, there\'s a padding byte if chunk_size % 2 == 1: file.read(1) except Exception as e: print(f\\"Error reading file: {e}\\") return chunks"},{"question":"# Abstract Shape Hierarchy You are required to design a shape hierarchy using the `abc` module in Python. Your task is to create an abstract base class `Shape`, with several concrete subclasses representing different shapes. You must also include a method to compute the perimeter of each shape. **Requirements:** 1. Define an abstract base class `Shape` using `ABCMeta` (or by inheriting from `ABC`). 2. The `Shape` class should have at least one abstract method `perimeter()` that computes and returns the perimeter of the shape. 3. Implement at least three concrete subclasses (`Circle`, `Rectangle`, and `Triangle`) that inherit from `Shape` and provide their own implementation of the `perimeter()` method. 4. Ensure each concrete subclass correctly overrides the `perimeter()` method. 5. Include appropriate class-level and instance-level methods and attributes to store the necessary dimensions (e.g., radius for `Circle`, lengths of sides for `Rectangle` and `Triangle`). **Input Format:** - Each concrete class should have a constructor that accepts its respective dimensions as parameters. **Output Format:** - The `perimeter()` method should return a float representing the calculated perimeter of the shape. **Constraints:** - For the `Circle` class, the radius should be a positive real number. - For the `Rectangle` class, the lengths of all sides should be positive real numbers. - For the `Triangle` class, the lengths of all sides should be positive real numbers and should satisfy the triangle inequality theorem. **Example Implementation:** ```python from abc import ABC, abstractmethod class Shape(ABC): @abstractmethod def perimeter(self): pass class Circle(Shape): def __init__(self, radius): self.radius = radius def perimeter(self): import math return 2 * math.pi * self.radius class Rectangle(Shape): def __init__(self, length, width): self.length = length self.width = width def perimeter(self): return 2 * (self.length + self.width) class Triangle(Shape): def __init__(self, side1, side2, side3): self.side1 = side1 self.side2 = side2 self.side3 = side3 def perimeter(self): return self.side1 + self.side2 + self.side3 # Example usage: # circle = Circle(5) # print(circle.perimeter()) # Outputs: 31.41592653589793 # rect = Rectangle(4, 6) # print(rect.perimeter()) # Outputs: 20 # tri = Triangle(3, 4, 5) # print(tri.perimeter()) # Outputs: 12 ``` **Note**: Ensure thorough testing of each class to verify correct implementation of the `perimeter()` method.","solution":"from abc import ABC, abstractmethod import math class Shape(ABC): @abstractmethod def perimeter(self): pass class Circle(Shape): def __init__(self, radius): if radius <= 0: raise ValueError(\\"Radius must be a positive value\\") self.radius = radius def perimeter(self): return 2 * math.pi * self.radius class Rectangle(Shape): def __init__(self, length, width): if length <= 0 or width <= 0: raise ValueError(\\"Length and width must be positive values\\") self.length = length self.width = width def perimeter(self): return 2 * (self.length + self.width) class Triangle(Shape): def __init__(self, side1, side2, side3): if side1 <= 0 or side2 <= 0 or side3 <= 0: raise ValueError(\\"Sides must be positive values\\") if side1 + side2 <= side3 or side1 + side3 <= side2 or side2 + side3 <= side1: raise ValueError(\\"The sides do not satisfy the triangle inequality theorem\\") self.side1 = side1 self.side2 = side2 self.side3 = side3 def perimeter(self): return self.side1 + self.side2 + self.side3"},{"question":"# PyTorch MTIA Advanced Device and Stream Management **Objective**: Implement a function in PyTorch that demonstrates the ability to manage devices and streams, and efficiently handle memory operations within the MTIA backend. **Question**: You are given the task to manage multiple devices and streams in a high-performance computation using PyTorch\'s MTIA backend. Your goal is to implement a function `device_memory_management` that: 1. **Initializes** the MTIA backend. 2. **Checks** if MTIA is available and initialized; if not, it should raise an appropriate error. 3. **Sets** the MTIA device based on the given device index. 4. **Creates** a stream for asynchronous operations and sets it as the current stream. 5. **Records** memory statistics before and after a dummy tensor operation. 6. **Synchronizes** the stream to ensure all operations are completed. 7. **Manages** the emptying of the memory cache if the memory usage exceeds a given threshold. **Function Signature**: ```python def device_memory_management(device_index: int, memory_threshold: int) -> dict: Manages MTIA device, streams, and memory. Parameters: - device_index (int): The index of the device to use. - memory_threshold (int): The memory threshold in bytes to trigger cache emptying. Returns: - dict: A dictionary containing initial and final memory stats, with the structure: { \\"initial_memory\\": int, \\"final_memory\\": int } Raises: - DeferredMtiaCallError: If MTIA initialization fails. - RuntimeError: If MTIA is not available or initialized. pass ``` **Constraints**: - Ensure that all PyTorch MTIA functions are properly called. - Handle exceptions and errors gracefully. - Optimize for performance and memory efficiency. **Example**: ```python try: stats = device_memory_management(0, 1000000) print(stats) except (DeferredMtiaCallError, RuntimeError) as e: print(f\\"Error: {str(e)}\\") ``` **Notes**: - Utilize functions like `torch.mtia.current_device`, `torch.mtia.set_device`, `torch.mtia.init`, `torch.mtia.is_available`, `torch.mtia.is_initialized`, `torch.mtia.memory_stats`, `torch.mtia.empty_cache`, `torch.mtia.synchronize`, `torch.mtia.stream`, and `torch.mtia.Stream`. - You may use dummy tensor operations to trigger memory usage. **Grading Criteria**: - Correctness: The function should handle device and stream management accurately. - Memory Management: Properly track and manage memory before and after operations. - Error Handling: Graceful handling of errors and exceptions. - Code Quality: Clear, well-documented, and efficient implementation.","solution":"import torch def device_memory_management(device_index: int, memory_threshold: int) -> dict: Manages MTIA device, streams, and memory. Parameters: - device_index (int): The index of the device to use. - memory_threshold (int): The memory threshold in bytes to trigger cache emptying. Returns: - dict: A dictionary containing initial and final memory stats, with the structure: { \\"initial_memory\\": int, \\"final_memory\\": int } Raises: - DeferredMtiaCallError: If MTIA initialization fails. - RuntimeError: If MTIA is not available or initialized. # Ensure the MTIA backend is initialized if not torch.mtia.is_available(): raise RuntimeError(\\"MTIA is not available.\\") if not torch.mtia.is_initialized(): torch.mtia.init() if not torch.mtia.is_initialized(): raise RuntimeError(\\"Failed to initialize MTIA.\\") # Set the MTIA device torch.mtia.set_device(device_index) # Create a stream for asynchronous operations stream = torch.mtia.Stream() with torch.mtia.stream(stream): # Record initial memory statistics initial_memory_stats = torch.mtia.memory_stats() initial_memory = initial_memory_stats[\'allocated_bytes.all.current\'] # Dummy tensor operation to engage memory tensor = torch.randn(1024, 1024, device=\'mtia\') # Record final memory statistics final_memory_stats = torch.mtia.memory_stats() final_memory = final_memory_stats[\'allocated_bytes.all.current\'] # Synchronize the stream to ensure all operations are completed torch.mtia.synchronize() # Check if memory usage exceeds the threshold and empty cache if necessary if final_memory - initial_memory > memory_threshold: torch.mtia.empty_cache() return { \\"initial_memory\\": initial_memory, \\"final_memory\\": final_memory }"},{"question":"**Question: Implementing a Custom Python Object Type** You are required to implement a custom object type named `CustomList` which mimics the basic functionality of a Python list but with additional constraints and methods described below. # Requirements: 1. **Initialization & Attributes:** - The `CustomList` should be able to store list elements and have an attribute `max_size` which determines the maximum number of elements the list can hold. - The `max_size` should be specified during the object creation and cannot be changed afterwards. 2. **Methods:** - `append(element)`: Adds `element` to the list if it does not exceed `max_size`. If it does, raise an `OverflowError` with a message \\"CustomList is full\\". - `pop()`: Removes and returns the last item from the list. If the list is empty, raise an `IndexError` with a message \\"CustomList is empty\\". - `__len__`: Returns the number of elements in the list. - `__str__`: Returns a string representation of the list elements in the format `CustomList([element1, element2, ...])`. - `clear()`: Removes all elements from the list. - `is_empty()`: Returns `True` if the list is empty, `False` otherwise. - `is_full()`: Returns `True` if the list is full, `False` otherwise. # Input and Output Formats: - The initialization will receive a single integer parameter for `max_size`. - Each method corresponds to typical list operations and should be called based on specific inputs or function calls. - When errors are raised (`OverflowError` or `IndexError`), they should include the specified error messages. # Example Usage: ```python # Creating an object of CustomList with max_size 3 cl = CustomList(3) # Adding elements cl.append(1) cl.append(2) # Checking the length print(len(cl)) # Output: 2 # Adding more elements cl.append(3) # Trying to add more elements beyond max_size try: cl.append(4) except OverflowError as e: print(e) # Output: CustomList is full # Popping an element print(cl.pop()) # Output: 3 # Checking if list is empty print(cl.is_empty()) # Output: False # Clearing the list cl.clear() # Checking if list is empty after clearing print(cl.is_empty()) # Output: True ``` # Constraints: - The elements added to `CustomList` can be of any data type. - The `max_size` will be a positive integer. # Implement the `CustomList` class below: ```python class CustomList: def __init__(self, max_size: int): # Your implementation here def append(self, element): # Your implementation here def pop(self): # Your implementation here def __len__(self): # Your implementation here def __str__(self): # Your implementation here def clear(self): # Your implementation here def is_empty(self): # Your implementation here def is_full(self): # Your implementation here ``` Implementing this custom list type allows you to demonstrate your understanding of class construction, memory allocation, and method creation in Python.","solution":"class CustomList: def __init__(self, max_size: int): if max_size <= 0: raise ValueError(\\"max_size must be a positive integer\\") self.max_size = max_size self.elements = [] def append(self, element): if len(self.elements) >= self.max_size: raise OverflowError(\\"CustomList is full\\") self.elements.append(element) def pop(self): if not self.elements: raise IndexError(\\"CustomList is empty\\") return self.elements.pop() def __len__(self): return len(self.elements) def __str__(self): return f\\"CustomList({self.elements})\\" def clear(self): self.elements.clear() def is_empty(self): return len(self.elements) == 0 def is_full(self): return len(self.elements) >= self.max_size"},{"question":"Problem Statement You are working on a system to manage the book inventory of a library. Each book in the library has the following properties: - `title`: The title of the book (a string). - `author`: The name of the author(s) (a string). Multiple authors are separated by commas. - `isbn`: ISBN of the book (a string). - `publication_year`: The year the book was published (an integer). - `copies_available`: The number of copies available in the library (an integer, default is zero). - `genres`: The genres associated with the book (a list of strings). Tasks: 1. Create a dataclass `Book` that accurately represents the above properties. 2. Implement a method `age` within the `Book` class that calculates the book\'s age based on the current year (assume the current year can be passed to the method). 3. Prevent any modification of the `title`, `author`, and `isbn` properties after a book has been created. 4. Include a `__post_init__` method that ensures `author` names are stored in a standardized format with proper capitalization (e.g., \\"john doe, jane smith\\" should be converted to \\"John Doe, Jane Smith\\"). 5. Use `default_factory` to ensure that each book has its own list of genres. 6. Create a function `search_books_by_genre` that takes a list of Book instances and a genre string and returns a list of books that belong to the specified genre. Input and Output Format **Class Definition:** ```python from typing import List from dataclasses import dataclass, field, InitVar @dataclass(frozen=True) class Book: title: str author: str isbn: str publication_year: int copies_available: int = 0 genres: List[str] = field(default_factory=list) def age(self, current_year: int) -> int: # Method implementation def __post_init__(self): # Post-init method implementation # Function to search books by genre def search_books_by_genre(books: List[Book], genre: str) -> List[Book]: # Function implementation ``` **Input Constraints:** - The `publication_year` must be a positive integer. - The `copies_available` must be a non-negative integer. **Output:** - The `age` method should return an integer representing the book\'s age. - The `__post_init__` method should ensure authors\' names are properly capitalized. - The `search_books_by_genre` function should return a list of `Book` instances filtered by the specified genre. **Example Usage:** ```python book1 = Book(title=\\"Python 101\\", author=\\"john doe\\", isbn=\\"1234567890\\", publication_year=2020, genres=[\\"Programming\\", \\"Python\\"]) book2 = Book(title=\\"Advanced Python\\", author=\\"jane smith\\", isbn=\\"0987654321\\", publication_year=2018, genres=[\\"Programming\\", \\"Python\\"]) book3 = Book(title=\\"Sci-Fi Novel\\", author=\\"anne brown\\", isbn=\\"5555555555\\", publication_year=2015, genres=[\\"Science Fiction\\"]) print(book1.age(2023)) # Output: 3 books = [book1, book2, book3] filtered_books = search_books_by_genre(books, \\"Python\\") for book in filtered_books: print(book.title) # Output should include: # Python 101 # Advanced Python ``` Note: Ensure to handle edge cases and produce clean and readable code for implementation.","solution":"from typing import List from dataclasses import dataclass, field @dataclass(frozen=True) class Book: title: str author: str isbn: str publication_year: int copies_available: int = 0 genres: List[str] = field(default_factory=list) def age(self, current_year: int) -> int: if current_year < self.publication_year: raise ValueError(\\"Current year cannot be less than publication year\\") return current_year - self.publication_year def __post_init__(self): # Ensure authors\' names are properly capitalized object.__setattr__(self, \'author\', \', \'.join(name.strip().title() for name in self.author.split(\',\'))) def search_books_by_genre(books: List[Book], genre: str) -> List[Book]: return [book for book in books if genre in book.genres]"},{"question":"# Python Coding Assessment Question In this assessment, you will be asked to implement a function that processes a text file containing a series of commands and data and generates specific output based on these commands. Your solution should demonstrate your understanding of Python syntax, statements, and error handling. Problem Statement: You are given a text file named `commands.txt` which contains multiple lines. Each line can be one of the following commands: 1. `ADD <number>` - Add the specified number to a running total. 2. `SUBTRACT <number>` - Subtract the specified number from the running total. 3. `MULTIPLY <number>` - Multiply the running total by the specified number. 4. `DIVIDE <number>` - Divide the running total by the specified number. 5. `PRINT` - Print the current value of the running total. 6. `EXIT` - Stop processing commands. Your task is to implement a function `process_commands(file_path: str) -> None` that reads these commands from the file, processes them sequentially, and handles any potential errors (e.g., division by zero, invalid commands). The function does not need to return any value but should print outputs as specified by the `PRINT` command. The initial value of the running total should be 0. Example: Suppose the file `commands.txt` contains the following lines: ``` ADD 5 SUBTRACT 3 MULTIPLY 4 DIVIDE 2 PRINT ADD 10 PRINT DIVIDE 0 EXIT ``` The function `process_commands` should produce the following output: ``` 4.0 14.0 Error: Division by zero ``` Constraints: - You can assume the file will always exist and be readable. - Commands may contain extra spaces which should be trimmed. - The function should handle invalid commands gracefully by printing an appropriate error message. - You should use exception handling to manage errors such as division by zero. Implementation: ```python def process_commands(file_path: str) -> None: total = 0 with open(file_path, \'r\') as file: for line in file: line = line.strip() if not line: continue parts = line.split() if len(parts) == 0: continue command = parts[0].upper() if command == \'ADD\' and len(parts) == 2: try: total += float(parts[1]) except ValueError: print(\\"Error: Invalid number for ADD command\\") elif command == \'SUBTRACT\' and len(parts) == 2: try: total -= float(parts[1]) except ValueError: print(\\"Error: Invalid number for SUBTRACT command\\") elif command == \'MULTIPLY\' and len(parts) == 2: try: total *= float(parts[1]) except ValueError: print(\\"Error: Invalid number for MULTIPLY command\\") elif command == \'DIVIDE\' and len(parts) == 2: try: number = float(parts[1]) if number == 0: print(\\"Error: Division by zero\\") else: total /= number except ValueError: print(\\"Error: Invalid number for DIVIDE command\\") elif command == \'PRINT\': print(total) elif command == \'EXIT\': break else: print(f\\"Error: Invalid command \'{line}\'\\") ``` Notes: - Ensure to test your function thoroughly to handle various edge cases. - This question assesses your ability to process file input, handle string parsing, manage errors, and implement core Python statements and expressions effectively.","solution":"def process_commands(file_path: str) -> None: total = 0.0 with open(file_path, \'r\') as file: for line in file: line = line.strip() if not line: continue parts = line.split() if len(parts) == 0: continue command = parts[0].upper() if command == \'ADD\' and len(parts) == 2: try: total += float(parts[1]) except ValueError: print(\\"Error: Invalid number for ADD command\\") elif command == \'SUBTRACT\' and len(parts) == 2: try: total -= float(parts[1]) except ValueError: print(\\"Error: Invalid number for SUBTRACT command\\") elif command == \'MULTIPLY\' and len(parts) == 2: try: total *= float(parts[1]) except ValueError: print(\\"Error: Invalid number for MULTIPLY command\\") elif command == \'DIVIDE\' and len(parts) == 2: try: number = float(parts[1]) if number == 0: print(\\"Error: Division by zero\\") else: total /= number except ValueError: print(\\"Error: Invalid number for DIVIDE command\\") elif command == \'PRINT\': print(total) elif command == \'EXIT\': break else: print(f\\"Error: Invalid command \'{line}\'\\")"},{"question":"# Configuration and Initialization in Python **Objective:** Implement and customize Python initialization using the `PyConfig` and `PyWideStringList` structures. This task will assess your ability to manipulate configuration settings, manage wide string lists, and handle initialization statuses with error handling mechanisms. **Task:** Write a Python program that: 1. Initializes a `PyConfig` structure with custom configurations. 2. Manages a `PyWideStringList` that includes at least one command-line argument. 3. Preinitializes and initializes Python using the configurations set in `PyConfig`. 4. Handles any initialization errors using `PyStatus`. **Input and Output Formats:** - **Input:** No separate input will be taken; you will set all configurations within the code. - **Output:** Print a success message if the initialization is successful or an error message if it fails. **Constraints:** 1. Modify the `sys.path` to include an additional directory. 2. Customize the `program_name`, `home`, and `executable` parameters in `PyConfig`. 3. Ensure appropriate error checking and handling using `PyStatus`. **Example:** ```python # Example Configuration def init_python_configuration(): # Import necessary ctypes from ctypes import c_wchar_p, c_int, POINTER, byref, Structure, wstring_at class PyWideStringList(Structure): _fields_ = [(\\"length\\", c_int), (\\"items\\", POINTER(c_wchar_p))] class PyStatus(Structure): _fields_ = [(\\"exitcode\\", c_int), (\\"err_msg\\", c_wchar_p), (\\"func\\", c_wchar_p)] # Functions from python310 for initialization and error handling ... # Step 1: Initialize PyConfig with custom settings config = ... ... # Set custom program name, home, and executable ... # Step 2: Example of setting PyWideStringList ... # Step 3: Preinitialize and Initialize Python with PyConfig ... # Step 4: Handle initialization status and print appropriate messages status = ... if ... : print(f\\"Initialization Error: {status.err_msg}\\") else: print(\\"Python Initialized Successfully!\\") # Initialize the configuration init_python_configuration() ``` Your implementation should follow the structure and logic outlined above, ensuring appropriate manipulation and initialization of Python configurations.","solution":"import sys import ctypes class PyConfig(ctypes.Structure): _fields_ = [ (\\"program_name\\", ctypes.c_wchar_p), (\\"home\\", ctypes.c_wchar_p), (\\"executable\\", ctypes.c_wchar_p), ] class PyWideStringList(ctypes.Structure): _fields_ = [ (\\"length\\", ctypes.c_int), (\\"items\\", ctypes.POINTER(ctypes.c_wchar_p)), ] class PyStatus(ctypes.Structure): _fields_ = [ (\\"exitcode\\", ctypes.c_int), (\\"err_msg\\", ctypes.c_wchar_p), (\\"func\\", ctypes.c_wchar_p), ] def PyConfig_InitPythonConfig(config): config.program_name = \\"custom_program\\" config.home = \\"custom_home\\" config.executable = \\"custom_executable\\" def PyWideStringList_Init(list_struct): list_struct.length = 1 list_struct.items = (ctypes.c_wchar_p * 1)(\\"custom_argument\\") def check_status(status): if status.exitcode != 0: return f\\"Initialization Error: {status.err_msg}\\" return \\"Python Initialized Successfully!\\" def init_python_configuration(): from sys import path # Step 1: Initialize PyConfig with custom settings config = PyConfig() PyConfig_InitPythonConfig(config) # Step 2: Example of setting PyWideStringList wide_string_list = PyWideStringList() PyWideStringList_Init(wide_string_list) # Modify sys.path to include an additional directory additional_directory = \\"/custom/path/\\" path.append(additional_directory) # Simulate the preinitialization and initialization status = PyStatus() status.exitcode = 0 # Simulating a successful init # Step 4: Handle initialization status and print appropriate messages message = check_status(status) print(message) # Initialize the configuration init_python_configuration()"},{"question":"**CSV Manipulation and Output** **Problem Statement:** You are provided with a CSV file titled `students.csv`. The file contains student information, including their First Name, Last Name, Age, and GPA (Grade Point Average). Your task is to perform the following operations: 1. Read the content of the `students.csv` file. 2. Sort the students by their GPA in descending order. 3. Write the sorted data into a new CSV file named `sorted_students.csv`. The `students.csv` file has the following columns: ``` First Name, Last Name, Age, GPA ``` # Input: The input to your function is the path to the `students.csv` file. # Output: Generate a file `sorted_students.csv` with the same columns as `students.csv`, but with rows sorted by GPA in descending order. Ensure the output file retains the header from the original CSV file. # Function Signature: Your function should have the following signature: ```python def sort_students_by_gpa(input_file: str) -> None: ``` # Constraints: - Assume the `students.csv` file is correctly formatted. - The GPA is a float between 0.0 and 4.0. - Use the `csv` module for reading and writing CSV files. - Handle any potential exception cases where the file cannot be read or written. # Example: Given `students.csv`: ``` First Name, Last Name, Age, GPA Alice, Johnson, 20, 3.9 Bob, Smith, 21, 3.6 Charlie, Lee, 19, 3.7 ``` Output `sorted_students.csv`: ``` First Name, Last Name, Age, GPA Alice, Johnson, 20, 3.9 Charlie, Lee, 19, 3.7 Bob, Smith, 21, 3.6 ``` # Notes: - Ensure that the function reads the content of `students.csv` and correctly sorts the rows by GPA. - Write the sorted content into a new file named `sorted_students.csv`. **Hint:** Consider using `csv.DictReader` for reading the CSV file into dictionaries, which will allow easy access to column names and values, and `csv.DictWriter` for writing dictionaries back to the new CSV file.","solution":"import csv def sort_students_by_gpa(input_file: str) -> None: Sort students by GPA in descending order and write to a new file. students = [] # Read the content of the input file with open(input_file, mode=\'r\', newline=\'\') as infile: reader = csv.DictReader(infile) header = reader.fieldnames students = list(reader) # Sort the list of students by GPA in descending order students_sorted = sorted(students, key=lambda x: float(x[\'GPA\']), reverse=True) # Write the sorted data into the new file with open(\'sorted_students.csv\', mode=\'w\', newline=\'\') as outfile: writer = csv.DictWriter(outfile, fieldnames=header) writer.writeheader() writer.writerows(students_sorted)"},{"question":"Objective: You are required to demonstrate your understanding of the seaborn library by performing data manipulation and visualization tasks using different data structures. Problem Statement: 1. Load the famous `tips` dataset using seaborn. 2. Convert the dataset into wide-form by pivoting it using `day` as the columns, `time` as the index, and the mean `total_bill` as values. 3. Generate a line plot using seaborn\'s `relplot` with the converted wide-form data. 4. Convert the dataset back into long-form. 5. Create a scatter plot using seaborn\'s `relplot` with the long-form data, showing the relationship between `total_bill` and `tip`, differentiated by `day`. Requirements: - The dataset used should be the `tips` dataset from seaborn. - The wide-form data should be displayed as a table. - The wide-form data plot needs to have appropriate labels. - The scatter plot should have point size (`size`) based on the `size` variable in the `tips` dataset. Expected Input and Output Formats: - **Input format:** No input required, but you must assume the necessary libraries are imported. - **Output format:** Two plots displaying the required visualizations and the wide-form data table printed. Constraints: - Ensure all visualizations are clear and properly labeled. - Use seaborn and pandas libraries. Code Template: ```python # Required imports import seaborn as sns import pandas as pd # 1. Load the tips dataset tips = sns.load_dataset(\\"tips\\") # 2. Convert the dataset into wide-form tips_wide = tips.pivot_table(index=\'time\', columns=\'day\', values=\'total_bill\', aggfunc=\'mean\') # Display the wide-form data print(\\"Wide-form data:n\\", tips_wide.head()) # 3. Generate a line plot with the wide-form data sns.relplot(data=tips_wide, kind=\'line\') plt.xlabel(\'Time\') plt.ylabel(\'Total Bill Mean\') plt.title(\'Total Bill Mean by Day and Time\') plt.show() # 4. Convert the dataset back into long-form tips_long = pd.melt(tips_wide.reset_index(), id_vars=\'time\', var_name=\'day\', value_name=\'total_bill\') # 5. Create a scatter plot with the long-form data sns.relplot(data=tips, x=\'total_bill\', y=\'tip\', hue=\'day\', size=\'size\', sizes=(10, 200)) plt.xlabel(\'Total Bill\') plt.ylabel(\'Tip\') plt.title(\'Scatter plot of Total Bill vs Tip by Day\') plt.show() ```","solution":"# Required imports import seaborn as sns import pandas as pd import matplotlib.pyplot as plt # 1. Load the tips dataset tips = sns.load_dataset(\\"tips\\") # 2. Convert the dataset into wide-form tips_wide = tips.pivot_table(index=\'time\', columns=\'day\', values=\'total_bill\', aggfunc=\'mean\') # Display the wide-form data print(\\"Wide-form data:n\\", tips_wide.head()) # 3. Generate a line plot with the wide-form data sns.relplot(data=tips_wide, kind=\'line\') plt.xlabel(\'Time\') plt.ylabel(\'Total Bill Mean\') plt.title(\'Total Bill Mean by Day and Time\') plt.show() # 4. Convert the dataset back into long-form tips_long = pd.melt(tips_wide.reset_index(), id_vars=\'time\', var_name=\'day\', value_name=\'total_bill\') # 5. Create a scatter plot with the long-form data sns.relplot(data=tips, x=\'total_bill\', y=\'tip\', hue=\'day\', size=\'size\', sizes=(10, 200)) plt.xlabel(\'Total Bill\') plt.ylabel(\'Tip\') plt.title(\'Scatter plot of Total Bill vs Tip by Day\') plt.show()"},{"question":"Implement Scaled Dot-Product Attention in PyTorch In this assessment, you will implement the core component of many advanced neural network architectures: the Scaled Dot-Product Attention. Background Attention mechanisms allow neural networks to focus on specific parts of the input data, which is essential for tasks like machine translation, text generation, and more. The scaled dot-product attention is a fundamental type of attention. Given queries ( Q ), keys ( K ), and values ( V ), scaled dot-product attention calculates the attention scores as follows: [ text{Attention}(Q, K, V) = text{softmax}left(frac{QK^T}{sqrt{d_k}}right)V ] where ( d_k ) is the dimensionality of the keys. Task 1. **Implement** the `ScaledDotProductAttention` class in PyTorch. 2. Your implementation must include a method for the forward pass. 3. You are required to use PyTorch tensor operations and neural network modules. 4. You should handle the potential dimension mismatch and apply the softmax function in a numerically stable way. Specifications - **Input**: - `queries` (Tensor of shape `(batch_size, seq_length_query, d_k)`): The query vectors. - `keys` (Tensor of shape `(batch_size, seq_length_key, d_k)`): The key vectors. - `values` (Tensor of shape `(batch_size, seq_length_value, d_v)`): The value vectors. - **Output**: - A tensor of shape `(batch_size, seq_length_query, d_v)` which is the result of the scaled dot-product attention mechanism. - **Constraints**: - You can assume that `seq_length_key` = `seq_length_value`. - **Performance Requirements**: - The implementation should be efficient in tensor operations to handle large sizes of input data. Example ```python import torch import torch.nn.functional as F class ScaledDotProductAttention(torch.nn.Module): def __init__(self): super(ScaledDotProductAttention, self).__init__() def forward(self, queries, keys, values): d_k = queries.size(-1) # Calculate the dot product between queries and keys scores = torch.matmul(queries, keys.transpose(-2, -1)) / torch.sqrt(torch.tensor(d_k, dtype=torch.float32)) # Apply softmax to scores attention_weights = F.softmax(scores, dim=-1) # Multiply the attention weights with the values output = torch.matmul(attention_weights, values) return output # Testing the implementation batch_size, seq_length, d_k, d_v = 2, 3, 4, 5 queries = torch.rand(batch_size, seq_length, d_k) keys = torch.rand(batch_size, seq_length, d_k) values = torch.rand(batch_size, seq_length, d_v) attention = ScaledDotProductAttention() output = attention(queries, keys, values) print(output.shape) # Should print: torch.Size([2, 3, 5]) ``` This example provides a template that you can use to test your implementation. Ensure your code follows this structure and meets the specified requirements. Good luck!","solution":"import torch import torch.nn.functional as F class ScaledDotProductAttention(torch.nn.Module): def __init__(self): super(ScaledDotProductAttention, self).__init__() def forward(self, queries, keys, values): d_k = queries.size(-1) # Calculate the dot product between queries and keys scores = torch.matmul(queries, keys.transpose(-2, -1)) / torch.sqrt(torch.tensor(d_k, dtype=torch.float32)) # Apply softmax to scores in a numerically stable way attention_weights = F.softmax(scores, dim=-1) # Multiply the attention weights with the values output = torch.matmul(attention_weights, values) return output # Example instantiation and forward pass batch_size, seq_length_q, seq_length_k, d_k, d_v = 2, 3, 3, 4, 5 queries = torch.rand(batch_size, seq_length_q, d_k) keys = torch.rand(batch_size, seq_length_k, d_k) values = torch.rand(batch_size, seq_length_k, d_v) attention = ScaledDotProductAttention() output = attention(queries, keys, values) print(output.shape) # Should print: torch.Size([2, 3, 5])"},{"question":"Implement a Custom Integer Parser in Python Objective: Demonstrate your understanding of integer object creation, conversion, and error handling in Python. Description: You are required to implement a custom parser that converts various string representations of integers into Python integer objects. The parser should handle different bases (2 to 36) and manage potential errors, such as invalid characters or overflow conditions. Function Signature: ```python def parse_integer(input_str: str, base: int = 10) -> int: pass ``` Input: - `input_str` (str): The string representation of the number, which might include leading spaces and single underscores. - `base` (int): The base in which the number is represented, defaults to 10. Must be between 2 and 36, inclusive. Output: - Returns the corresponding integer value if the `input_str` is valid. - Raises a `ValueError` if the `input_str` is invalid for the given base or if any other error occurs (such as overflow). Constraints: - The `input_str` should be non-empty and must represent a valid integer for the specified base. - The function should handle large integers and manage memory efficiently. - Leading spaces and single underscores are permitted and need to be removed in the final integer conversion. Performance Requirements: - The function must work efficiently for typical input sizes commonly expected in competitive coding scenarios. - Error handling must be robust and correctly identify and report issues. Example Usage: ```python try: print(parse_integer(\\" 1234 \\", 10)) # Output: 1234 print(parse_integer(\\"1111\\", 2)) # Output: 15 print(parse_integer(\\"1A3F\\", 16)) # Output: 6719 print(parse_integer(\\"10000000000\\", 2)) # Output: 1024 print(parse_integer(\\"1_000\\", 10)) # Output: 1000 print(parse_integer(\\"1 000\\")) # Raises ValueError: invalid literal for base 10 except ValueError as e: print(e) ``` Bonus: - Implement `parse_integer` without using built-in Python conversion functions like `int()` for an added challenge. Note: Please focus on structuring your code for clarity, handling edge cases, and implementing efficient and reusable error-checking mechanisms.","solution":"def parse_integer(input_str: str, base: int = 10) -> int: Parses a string representation of an integer into a Python integer object for the given base. Args: input_str (str): The string representation of the number. base (int): The base in which the number is represented (between 2 and 36 inclusive). Returns: int: The corresponding integer value if input_str is valid. Raises: ValueError: If the input_str is invalid for the given base or any other error occurs. if not (2 <= base <= 36): raise ValueError(\\"Base must be between 2 and 36\\") try: sanitized_str = input_str.replace(\'_\', \'\').strip() result = int(sanitized_str, base) # Using built-in int function since it\'s more reliable for bases 2-36. except ValueError as e: raise ValueError(f\\"Invalid input for base {base}: {input_str}\\") from e return result"},{"question":"Objective The objective of this task is to implement a function that processes a given datetime series and adjusts the dates using different pandas date offsets. This will test your understanding of working with date offsets, properties, and methods in pandas. Problem Description You are provided with a pandas DataFrame that contains a column of dates, and a specific date offset type (as a string) that should be applied to these dates. Your task is to implement a function `adjust_dates` that adjusts the dates in the DataFrame according to the specified date offset, and also adds columns indicating whether the adjusted dates correspond to specific time boundaries (e.g., the start of the month, end of the quarter, etc.). Function Signature ```python import pandas as pd from pandas.tseries.offsets import ( BusinessDay, BusinessHour, CustomBusinessDay, MonthEnd, MonthBegin, YearEnd, YearBegin, Day, Hour, Minute, Second ) def adjust_dates(df: pd.DataFrame, date_col: str, offset_type: str) -> pd.DataFrame: Adjusts the dates in the given DataFrame according to the specified date offset. Parameters: df (pd.DataFrame): A DataFrame containing at least one column of datetime type. date_col (str): The name of the column containing datetime values. offset_type (str): The type of date offset to apply. Possible values include \'BusinessDay\', \'BusinessHour\', \'CustomBusinessDay\', \'MonthEnd\', \'MonthBegin\', \'YearEnd\', \'YearBegin\', \'Day\', \'Hour\', \'Minute\', \'Second\'. Returns: pd.DataFrame: A new DataFrame with an additional column \'adjusted_date\' for the adjusted dates and boolean columns indicating the time boundaries of the adjusted dates. ``` Input - `df`: A pandas DataFrame containing at least one column with datetime values. - `date_col`: A string representing the name of the datetime column in the DataFrame. - `offset_type`: A string specifying the type of date offset to be used. Possible values include: - \'BusinessDay\' - \'BusinessHour\' - \'CustomBusinessDay\' - \'MonthEnd\' - \'MonthBegin\' - \'YearEnd\' - \'YearBegin\' - \'Day\' - \'Hour\' - \'Minute\' - \'Second\' Output - A pandas DataFrame, which includes: - An additional column `adjusted_date` representing the adjusted dates. - Additional boolean columns indicating if the `adjusted_date`: - Is at the start of the month (`is_month_start`) - Is at the end of the month (`is_month_end`) - Is at the start of the quarter (`is_quarter_start`) - Is at the end of the quarter (`is_quarter_end`) - Is at the start of the year (`is_year_start`) - Is at the end of the year (`is_year_end`) Constraints - The input DataFrame must contain the specified date column with valid datetime values. - The `offset_type` must be a valid string representing one of the above-mentioned date offsets. - The function must handle any edge cases, such as missing or null values in the date column. Example ```python # Example input DataFrame data = { \'dates\': [\'2023-01-01\', \'2023-02-15\', \'2023-03-30\', \'2023-12-31\'] } df = pd.DataFrame(data) df[\'dates\'] = pd.to_datetime(df[\'dates\']) # Applying the function result = adjust_dates(df, \'dates\', \'MonthEnd\') # Expected output # The resulting DataFrame will have the original `dates` column, a new `adjusted_date` column, # and the boolean columns as specified in the output section describing the time boundaries. print(result) ``` Remember to handle the import statements necessary to use the pandas library and the various date offsets.","solution":"import pandas as pd from pandas.tseries.offsets import ( BusinessDay, BusinessHour, CustomBusinessDay, MonthEnd, MonthBegin, YearEnd, YearBegin, Day, Hour, Minute, Second ) def adjust_dates(df: pd.DataFrame, date_col: str, offset_type: str) -> pd.DataFrame: Adjusts the dates in the given DataFrame according to the specified date offset. Parameters: df (pd.DataFrame): A DataFrame containing at least one column of datetime type. date_col (str): The name of the column containing datetime values. offset_type (str): The type of date offset to apply. Possible values include \'BusinessDay\', \'BusinessHour\', \'CustomBusinessDay\', \'MonthEnd\', \'MonthBegin\', \'YearEnd\', \'YearBegin\', \'Day\', \'Hour\', \'Minute\', \'Second\'. Returns: pd.DataFrame: A new DataFrame with an additional column \'adjusted_date\' for the adjusted dates and boolean columns indicating the time boundaries of the adjusted dates. # Offset mapping offset_map = { \'BusinessDay\': BusinessDay(), \'BusinessHour\': BusinessHour(), \'CustomBusinessDay\': CustomBusinessDay(), \'MonthEnd\': MonthEnd(), \'MonthBegin\': MonthBegin(), \'YearEnd\': YearEnd(), \'YearBegin\': YearBegin(), \'Day\': Day(), \'Hour\': Hour(), \'Minute\': Minute(), \'Second\': Second() } # Check if provided offset type is valid if offset_type not in offset_map: raise ValueError(\\"Invalid offset type provided.\\") # Apply the date offset df[\'adjusted_date\'] = df[date_col] + offset_map[offset_type] # Create boolean columns for different boundaries df[\'is_month_start\'] = df[\'adjusted_date\'].dt.is_month_start df[\'is_month_end\'] = df[\'adjusted_date\'].dt.is_month_end df[\'is_quarter_start\'] = df[\'adjusted_date\'].dt.is_quarter_start df[\'is_quarter_end\'] = df[\'adjusted_date\'].dt.is_quarter_end df[\'is_year_start\'] = df[\'adjusted_date\'].dt.is_year_start df[\'is_year_end\'] = df[\'adjusted_date\'].dt.is_year_end return df"},{"question":"**Coding Assessment Question** # Task You are to implement a Python module that simulates a simplified downloader which concurrently fetches data from multiple sources and processes it. Use the `asyncio` library for this purpose. The downloader will have the following functionalities: 1. **Fetch Data Concurrently**: Multiple data sources (simulated with URLs) need to be fetched concurrently. Use asynchronous I/O for fetching data. 2. **Processing Data**: Each fetched piece of data will be processed asynchronously. 3. **Synchronized Logging**: Log the progress and results in a thread-safe manner using asynchronous synchronization primitives. # Requirements 1. **Function Implementations**: - `async def fetch_data(url: str) -> str`: Simulates fetching data from a given URL. - `async def process_data(data: str) -> str`: Processes the fetched data. - `async def main(urls: List[str]) -> List[str]`: Main function which manages the fetching and processing of data from a list of URLs and returns the processed data. 2. **Input and Output Formats**: - The input to `main` is a list of URLs (strings). - The output of `main` is a list of processed data strings. 3. **Constraints**: - Use `asyncio.Queue` to manage the URLs to be fetched and processed. - Use `asyncio` synchronization primitives such as locks for thread-safe logging. 4. **Performance Requirements**: - Ensure that data fetching and processing are performed efficiently using concurrency. - The solution should be scalable to handle a large number of URLs. # Example ```python import asyncio async def fetch_data(url: str) -> str: # Simulate a network request to fetch data from the URL. await asyncio.sleep(1) # Simulate network delay return f\\"Data from {url}\\" async def process_data(data: str) -> str: # Simulate data processing await asyncio.sleep(0.5) # Simulate processing delay return f\\"Processed {data}\\" async def main(urls: List[str]) -> List[str]: async def worker(): while True: url = await queue.get() if url is None: break data = await fetch_data(url) processed_data = await process_data(data) # Log the fetching and processing stages async with log_lock: print(f\\"Fetched: {data}\\") print(f\\"Processed: {processed_data}\\") results.append(processed_data) queue.task_done() queue = asyncio.Queue() log_lock = asyncio.Lock() results = [] # Populate the queue with URLs for url in urls: await queue.put(url) # Create worker tasks to process the queue tasks = [asyncio.create_task(worker()) for _ in range(5)] # Wait until the queue is fully processed await queue.join() # Stop the worker tasks for _ in range(5): await queue.put(None) for task in tasks: await task return results # Example usage if __name__ == \\"__main__\\": urls = [f\\"http://example.com/{i}\\" for i in range(10)] processed_data = asyncio.run(main(urls)) print(\\"Final processed data:\\", processed_data) ``` # Notes - Ensure to use proper async/await syntax for concurrency. - Handle exceptions and edge cases gracefully. - Write unit tests to cover different scenarios and validate your implementation.","solution":"import asyncio from typing import List async def fetch_data(url: str) -> str: Simulates fetching data from a given URL. await asyncio.sleep(1) # Simulate network delay return f\\"Data from {url}\\" async def process_data(data: str) -> str: Simulates processing fetched data. await asyncio.sleep(0.5) # Simulate processing delay return f\\"Processed {data}\\" async def main(urls: List[str]) -> List[str]: Manages the fetching and processing of data from a list of URLs. async def worker(): while True: url = await queue.get() if url is None: break try: data = await fetch_data(url) processed_data = await process_data(data) # Log the fetching and processing stages async with log_lock: print(f\\"Fetched: {data}\\") print(f\\"Processed: {processed_data}\\") results.append(processed_data) finally: queue.task_done() queue = asyncio.Queue() log_lock = asyncio.Lock() results = [] # Populate the queue with URLs for url in urls: await queue.put(url) # Create worker tasks to process the queue tasks = [asyncio.create_task(worker()) for _ in range(5)] # Wait until the queue is fully processed await queue.join() # Stop the worker tasks for _ in range(5): await queue.put(None) for task in tasks: await task return results"},{"question":"Objective Create a PyTorch program to profile the execution of a custom neural network, understand the performance bottlenecks, and optimize its kernel launch efficiency. Problem Statement 1. Implement a custom PyTorch neural network model. 2. Perform a forward-backward pass through the network with both training and inference modes profiled using `torch.profiler`. 3. Identify any graph breaks and gaps between kernel launches from the profiling traces. 4. Optimize the model to minimize these gaps, using techniques such as batch size adjustment or fusion of small operators. Requirements 1. **Custom Neural Network**: - Implement a simple multi-layer perceptron (MLP) model using PyTorch. - The network should include at least three linear layers with activation functions in between. 2. **Profiling**: - Use `torch.profiler.profile()` to profile both training and inference passes. - Export the profiling traces to JSON files named `train_trace.json` and `infer_trace.json`. 3. **Analysis**: - Load and view the profiling traces using Chrome (chrome://tracing). - Identify any significant gaps between GPU kernels and explain potential causes. - Identify any graph breaks in the model and explain their impact on performance. 4. **Optimization**: - Modify the model or its usage to reduce gaps between GPU kernel launches and improve overall performance. - Provide an explanation of the optimization strategies employed. Constraints - Use CUDA for any operations that support GPU acceleration. - The custom neural network should be implemented from scratch without using any pre-trained models. Input - No direct input from the user. The program should generate dummy data for profiling. Output - Profiling trace files (`train_trace.json`, `infer_trace.json`). - A report outlining: - Identified performance bottlenecks (gaps between GPU kernels, graph breaks). - Applied optimization strategies and their effects on performance. ```python import torch import torch.nn as nn import torch.optim as optim class SimpleMLP(nn.Module): def __init__(self): super(SimpleMLP, self).__init__() self.fc1 = nn.Linear(784, 256) self.relu1 = nn.ReLU() self.fc2 = nn.Linear(256, 128) self.relu2 = nn.ReLU() self.fc3 = nn.Linear(128, 10) def forward(self, x): x = self.fc1(x) x = self.relu1(x) x = self.fc2(x) x = self.relu2(x) x = self.fc3(x) return x # Initialize the model, loss, and optimizer model = SimpleMLP().cuda() criterion = nn.CrossEntropyLoss() optimizer = optim.Adam(model.parameters()) # Generate dummy data inputs = torch.randn((64, 784), device=\'cuda\') targets = torch.randint(0, 10, (64,), device=\'cuda\') def train_step(inputs, targets): model.train() optimizer.zero_grad() outputs = model(inputs) loss = criterion(outputs, targets) loss.backward() optimizer.step() def infer_step(inputs): model.eval() with torch.no_grad(): outputs = model(inputs) return outputs # Warm-up run train_step(inputs, targets) infer_step(inputs) # Profile training step with torch.profiler.profile() as train_prof: train_step(inputs, targets) train_prof.export_chrome_trace(\\"train_trace.json\\") # Profile inference step with torch.profiler.profile() as infer_prof: infer_step(inputs) infer_prof.export_chrome_trace(\\"infer_trace.json\\") # Analysis and optimization of model # [Your code to analyze traces and optimize model goes here] ``` Deliverables 1. The implemented model and profiling code. 2. The generated profiling trace files (`train_trace.json`, `infer_trace.json`). 3. A written report explaining performance bottlenecks and applied optimizations.","solution":"import torch import torch.nn as nn import torch.optim as optim from torch.profiler import profile, record_function, ProfilerActivity class SimpleMLP(nn.Module): def __init__(self): super(SimpleMLP, self).__init__() self.fc1 = nn.Linear(784, 256) self.relu1 = nn.ReLU() self.fc2 = nn.Linear(256, 128) self.relu2 = nn.ReLU() self.fc3 = nn.Linear(128, 10) def forward(self, x): x = self.fc1(x) x = self.relu1(x) x = self.fc2(x) x = self.relu2(x) x = self.fc3(x) return x # Initialize the model, loss, and optimizer model = SimpleMLP().cuda() criterion = nn.CrossEntropyLoss() optimizer = optim.Adam(model.parameters()) # Generate dummy data inputs = torch.randn((64, 784), device=\'cuda\') targets = torch.randint(0, 10, (64,), device=\'cuda\') def train_step(inputs, targets): model.train() optimizer.zero_grad() outputs = model(inputs) loss = criterion(outputs, targets) loss.backward() optimizer.step() def infer_step(inputs): model.eval() with torch.no_grad(): outputs = model(inputs) return outputs # Warm-up run train_step(inputs, targets) infer_step(inputs) # Profile training step with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA], record_shapes=True) as train_prof: with record_function(\\"model_inference\\"): train_step(inputs, targets) train_prof.export_chrome_trace(\\"train_trace.json\\") # Profile inference step with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA], record_shapes=True) as infer_prof: with record_function(\\"model_inference\\"): infer_step(inputs) infer_prof.export_chrome_trace(\\"infer_trace.json\\") # Analysis and optimization of model # [Based on profiling, we can modify the model or usage such as increasing batch size, operator fusion, etc.]"},{"question":"Objective: Implement a class that simulates a text editor history. The history should record text changes and be able to serialize and deserialize the history using the `pickle` module. This will test the candidate\'s ability to handle object serialization, especially custom classes, and immutable states. Functionality: 1. Implement a class `TextEditorHistory` which will have the following properties and methods: - `init(text: str)` - Initialize with the given text. - `add_change(change: str)` - Adds a change to the text. - `undo()` - Undoes the last change. - `redo()` - Redoes the last undone change. - `get_text() -> str` - Returns the current state of the text. 2. Serialize and deserialize `TextEditorHistory` instances using the `pickle` module. Steps: 1. The class should maintain a log of text changes (both done and undone) and be able to perform undo and redo operations. 2. Use `TextEditorHistory.__getstate__` and `TextEditorHistory.__setstate__` methods to control how the instance\'s state is pickled and unpickled respectively. 3. Demonstrate the functionality by: - Creating an instance of `TextEditorHistory`. - Adding a few changes. - Serializing the instance to a file. - Deserializing from the file. - Verifying that the deserialized instance maintains the correct state post restoration. Constraints: 1. Ensure that the entire log of changes and the current state of the text are correctly serialized and deserialized. 2. Provide a proper demonstration of the use case in the main function. Example: ```python import pickle class TextEditorHistory: def __init__(self, text: str): self._original_text = text self._changes = [] self._undos = [] def add_change(self, change: str): self._changes.append(change) self._undos.clear() # Clear the redo stack def undo(self): if self._changes: change = self._changes.pop() self._undos.append(change) def redo(self): if self._undos: change = self._undos.pop() self._changes.append(change) def get_text(self) -> str: text = self._original_text for change in self._changes: text += change return text def __getstate__(self): # Customize the state to be pickled state = self.__dict__.copy() return state def __setstate__(self, state): # Restore state from unpicking self.__dict__.update(state) # Demonstration if __name__ == \\"__main__\\": history = TextEditorHistory(\\"Hello\\") history.add_change(\\", world\\") history.add_change(\\"!\\") print(\\"Before serialization:\\", history.get_text()) # Serialize with open(\'history.pkl\', \'wb\') as f: pickle.dump(history, f) # Deserialize with open(\'history.pkl\', \'rb\') as f: loaded_history = pickle.load(f) print(\\"After deserialization:\\", loaded_history.get_text()) # Demonstrate undo and redo loaded_history.undo() print(\\"After undo:\\", loaded_history.get_text()) loaded_history.redo() print(\\"After redo:\\", loaded_history.get_text()) ``` Expected Output: ``` Before serialization: Hello, world! After deserialization: Hello, world! After undo: Hello After redo: Hello, world! ``` The question is designed to assess the understanding and application of the `pickle` module, particularly in handling custom class serialization that involves managing internal state and dependencies correctly.","solution":"import pickle class TextEditorHistory: def __init__(self, text: str): self._original_text = text self._changes = [] self._undos = [] def add_change(self, change: str): self._changes.append(change) self._undos.clear() # Clear the redo stack def undo(self): if self._changes: change = self._changes.pop() self._undos.append(change) def redo(self): if self._undos: change = self._undos.pop() self._changes.append(change) def get_text(self) -> str: text = self._original_text for change in self._changes: text += change return text def __getstate__(self): # Customize the state to be pickled state = self.__dict__.copy() return state def __setstate__(self, state): # Restore state from unpicking self.__dict__.update(state) # Demonstration if __name__ == \\"__main__\\": history = TextEditorHistory(\\"Hello\\") history.add_change(\\", world\\") history.add_change(\\"!\\") print(\\"Before serialization:\\", history.get_text()) # Serialize with open(\'history.pkl\', \'wb\') as f: pickle.dump(history, f) # Deserialize with open(\'history.pkl\', \'rb\') as f: loaded_history = pickle.load(f) print(\\"After deserialization:\\", loaded_history.get_text()) # Demonstrate undo and redo loaded_history.undo() print(\\"After undo:\\", loaded_history.get_text()) loaded_history.redo() print(\\"After redo:\\", loaded_history.get_text())"},{"question":"Objective The objective of this task is to evaluate your understanding and proficiency with PyTorch, specifically using Intel GPUs. You are required to implement a training routine for a deep learning model and optimize its performance using `torch.compile`. Problem Statement 1. **Tensor Operations**: Write a function that performs basic tensor operations on an Intel GPU. 2. **Training with Intel GPU**: Implement a training routine for a simple convolutional neural network (CNN) to classify the MNIST dataset using Intel GPUs. Ensure that your routine includes the following: - Model initialization. - Data loading and preprocessing. - Training loop with loss calculation and optimization. - Model evaluation. - Saving the model state. 3. **Optimization with `torch.compile`**: Modify your training routine to use `torch.compile` and compare the training times for 5 epochs with and without `torch.compile`. Input and Output Formats 1. **Tensor Operations Function** - Input: None - Output: The result of basic tensor operations performed on an Intel GPU. 2. **Training Function** - Input: None - Output: Prints the training and validation accuracy after each epoch. 3. **Optimization Function** - Input: None - Output: Prints the training time for 5 epochs with and without `torch.compile`. Constraints - Ensure Intel GPU is available using `torch.xpu.is_available()`. - Use FP32 data type for training. - Implement proper error handling if Intel GPU is not available. Example Usage ```python def basic_tensor_operations(): import torch # Sample tensor operations tensor = torch.tensor([1.0, 2.0, 3.0], device=\\"xpu\\") result = tensor * 2 print(result) def train_model(): import torch import torch.nn as nn import torch.optim as optim import torchvision import torchvision.transforms as transforms # Define a simple CNN class SimpleCNN(nn.Module): def __init__(self): super(SimpleCNN, self).__init__() self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1) self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1) self.fc1 = nn.Linear(64*28*28, 128) self.fc2 = nn.Linear(128, 10) def forward(self, x): x = torch.relu(self.conv1(x)) x = torch.relu(self.conv2(x)) x = x.view(-1, 64*28*28) x = torch.relu(self.fc1(x)) x = self.fc2(x) return x # Check for Intel GPU if not torch.xpu.is_available(): raise RuntimeError(\\"Intel GPU is not available\\") # Data loading and preprocessing transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))]) train_dataset = torchvision.datasets.MNIST(root=\'./data\', train=True, transform=transform, download=True) train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=64, shuffle=True) test_dataset = torchvision.datasets.MNIST(root=\'./data\', train=False, transform=transform, download=True) test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=64, shuffle=False) # Model, loss function, and optimizer model = SimpleCNN().to(\\"xpu\\") criterion = nn.CrossEntropyLoss().to(\\"xpu\\") optimizer = optim.Adam(model.parameters(), lr=0.001) # Training loop for epoch in range(5): model.train() running_loss = 0.0 for data, target in train_loader: data, target = data.to(\\"xpu\\"), target.to(\\"xpu\\") optimizer.zero_grad() outputs = model(data) loss = criterion(outputs, target) loss.backward() optimizer.step() running_loss += loss.item() print(f\'Epoch [{epoch + 1}/5] Loss: {running_loss / len(train_loader)}\') # Evaluate model.eval() correct = 0 total = 0 with torch.no_grad(): for data, target in test_loader: data, target = data.to(\\"xpu\\"), target.to(\\"xpu\\") outputs = model(data) _, predicted = torch.max(outputs.data, 1) total += target.size(0) correct += (predicted == target).sum().item() print(f\'Accuracy: {100 * correct / total}\') # Save the model torch.save(model.state_dict(), \'mnist_cnn.pth\') def optimize_with_torch_compile(): import torch import time # Define the same model and training routine as above # ... # Train without torch.compile start_time = time.time() train_model() end_time = time.time() print(f\'Training time without torch.compile: {end_time - start_time} seconds\') # Train with torch.compile model = torch.compile(SimpleCNN()).to(\\"xpu\\") # Rest of the training routine # ... start_time = time.time() train_model() end_time = time.time() print(f\'Training time with torch.compile: {end_time - start_time} seconds\') # Run functions basic_tensor_operations() train_model() optimize_with_torch_compile() ``` Note - You are required to implement `basic_tensor_operations()`, `train_model()`, and `optimize_with_torch_compile()` functions based on the given example usage and provided constraints. - Ensure your code is well-documented and handles any potential errors gracefully.","solution":"import time import torch import torch.nn as nn import torch.optim as optim import torchvision import torchvision.transforms as transforms # Function for basic tensor operations on an Intel GPU def basic_tensor_operations(): if not torch.xpu.is_available(): raise RuntimeError(\\"Intel GPU is not available\\") tensor = torch.tensor([1.0, 2.0, 3.0], device=\\"xpu\\") result = tensor * 2 return result # Simple CNN model definition class SimpleCNN(nn.Module): def __init__(self): super(SimpleCNN, self).__init__() self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1) self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1) self.fc1 = nn.Linear(64 * 28 * 28, 128) self.fc2 = nn.Linear(128, 10) def forward(self, x): x = torch.relu(self.conv1(x)) x = torch.relu(self.conv2(x)) x = x.view(-1, 64 * 28 * 28) x = torch.relu(self.fc1(x)) x = self.fc2(x) return x # Function to train the model def train_model(): if not torch.xpu.is_available(): raise RuntimeError(\\"Intel GPU is not available\\") transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))]) train_dataset = torchvision.datasets.MNIST(root=\'./data\', train=True, transform=transform, download=True) train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=64, shuffle=True) test_dataset = torchvision.datasets.MNIST(root=\'./data\', train=False, transform=transform, download=True) test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=64, shuffle=False) model = SimpleCNN().to(\\"xpu\\") criterion = nn.CrossEntropyLoss().to(\\"xpu\\") optimizer = optim.Adam(model.parameters(), lr=0.001) for epoch in range(5): model.train() running_loss = 0.0 for data, target in train_loader: data, target = data.to(\\"xpu\\"), target.to(\\"xpu\\") optimizer.zero_grad() outputs = model(data) loss = criterion(outputs, target) loss.backward() optimizer.step() running_loss += loss.item() print(f\'Epoch [{epoch + 1}/5] Loss: {running_loss / len(train_loader):.4f}\') model.eval() correct = 0 total = 0 with torch.no_grad(): for data, target in test_loader: data, target = data.to(\\"xpu\\"), target.to(\\"xpu\\") outputs = model(data) _, predicted = torch.max(outputs, 1) total += target.size(0) correct += (predicted == target).sum().item() print(f\'Accuracy: {100 * correct / total:.2f}%\') torch.save(model.state_dict(), \'mnist_cnn.pth\') # Function to optimize training using torch.compile and compare training times def optimize_with_torch_compile(): if not torch.xpu.is_available(): raise RuntimeError(\\"Intel GPU is not available\\") # Training without torch.compile start_time = time.time() train_model() end_time = time.time() print(f\'Training time without torch.compile: {end_time - start_time:.2f} seconds\') # Define the model and compile it model = SimpleCNN() compiled_model = torch.compile(model).to(\\"xpu\\") # Redefine the training with compiled model def train_compiled_model(): transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))]) train_dataset = torchvision.datasets.MNIST(root=\'./data\', train=True, transform=transform, download=True) train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=64, shuffle=True) test_dataset = torchvision.datasets.MNIST(root=\'./data\', train=False, transform=transform, download=True) test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=64, shuffle=False) criterion = nn.CrossEntropyLoss().to(\\"xpu\\") optimizer = optim.Adam(compiled_model.parameters(), lr=0.001) for epoch in range(5): compiled_model.train() running_loss = 0.0 for data, target in train_loader: data, target = data.to(\\"xpu\\"), target.to(\\"xpu\\") optimizer.zero_grad() outputs = compiled_model(data) loss = criterion(outputs, target) loss.backward() optimizer.step() running_loss += loss.item() print(f\'Epoch [{epoch + 1}/5] Loss: {running_loss / len(train_loader):.4f}\') compiled_model.eval() correct = 0 total = 0 with torch.no_grad(): for data, target in test_loader: data, target = data.to(\\"xpu\\"), target.to(\\"xpu\\") outputs = compiled_model(data) _, predicted = torch.max(outputs, 1) total += target.size(0) correct += (predicted == target).sum().item() print(f\'Accuracy: {100 * correct / total:.2f}%\') torch.save(compiled_model.state_dict(), \'mnist_cnn_compiled.pth\') # Training with torch.compile start_time = time.time() train_compiled_model() end_time = time.time() print(f\'Training time with torch.compile: {end_time - start_time:.2f} seconds\') # Run the functions if __name__ == \\"__main__\\": basic_tensor_operations() train_model() optimize_with_torch_compile()"},{"question":"# Python Coding Assessment Question **Objective:** Implement a class in Python that mimics the behavior of a given iterator but allows injecting additional values during iteration and handles exceptions gracefully. **Problem Statement:** You are required to implement a class `CustomIterator` that takes an iterable and has the following functionalities: 1. Iterate through the given iterable. 2. Allow sending a value into the iterator which will be returned as the next value. 3. If an exception is raised during iteration, it should be caught and printed with a custom message (`\\"Exception occurred during iteration\\"`). **Function Signature:** ```python class CustomIterator: def __init__(self, iterable: iter): pass def __iter__(self): pass def __next__(self): pass def send(self, value): pass # Example: # Given iterable data = [1, 2, 3, 4] iterator = CustomIterator(data) print(next(iterator)) # Output: 1 print(iterator.send(99)) # Output: 99 print(next(iterator)) # Output: 2 print(next(iterator)) # Output: 3 # Manually raising exception to test robustness try: raise ValueError except: print(next(iterator)) # Output: \\"Exception occurred during iteration\\" print(next(iterator)) # Output: 4 try: print(next(iterator)) # Output: StopIteration except StopIteration: print(\\"Iteration complete\\") ``` **Constraints and Limitations:** 1. The iterable passed to `CustomIterator` can be any Python iterable object. 2. The `send` method allows injecting a value that will be returned in the next iteration. 3. If an exception occurs, handle it gracefully by printing `\\"Exception occurred during iteration\\"`. **Assumptions:** 1. The primary focus is on correctly implementing the behavior and understanding of Python\'s iterator protocol. 2. Proper error handling and maintaining the state of the iterator for continuous iteration are essential.","solution":"class CustomIterator: def __init__(self, iterable): self.iterator = iter(iterable) self.sent_value = None self.has_sent_value = False def __iter__(self): return self def __next__(self): try: if self.has_sent_value: self.has_sent_value = False return self.sent_value return next(self.iterator) except Exception as e: print(\\"Exception occurred during iteration\\") raise e def send(self, value): self.sent_value = value self.has_sent_value = True return self.__next__()"},{"question":"Objective: Your task is to implement an out-of-core learning system using scikit-learn to handle large datasets. You will read data from a file in chunks, extract features using the Hashing Vectorizer, and apply an incremental learning algorithm for classification. You will also plot the accuracy of the classifier over time. Requirements: 1. **Input:** - A CSV file `data.csv` where each row represents a text document. The first column is the label (target) and the second column is the document (text data). - Chunk size: Number of rows to read in each iteration. 2. **Output:** - Plot showing the accuracy of the classifier over the number of processed examples. 3. **Steps:** - Stream the data from the CSV file in chunks. - Use the `HashingVectorizer` from `sklearn.feature_extraction.text` for feature extraction. - Use an incremental learning algorithm (e.g., `SGDClassifier`) from `sklearn.linear_model`. - Maintain and update the accuracy of the classifier over time after processing each chunk. Constraints: - The size of the entire dataset is unknown and cannot be loaded into memory at once. - Assume the dataset contains multiple classes. - Use a mini-batch size of 1000 for the SGDClassifier. - The CSV file `data.csv` is structured as follows: ``` label,text 0,\\"This is the first document.\\" 1,\\"This document is the second document.\\" ... ``` Performance Requirements: - Ensure efficient memory usage while streaming and processing the data. Implementation: ```python import pandas as pd import matplotlib.pyplot as plt from sklearn.feature_extraction.text import HashingVectorizer from sklearn.linear_model import SGDClassifier from sklearn.metrics import accuracy_score def stream_data(file_path, chunk_size=1000): for chunk in pd.read_csv(file_path, chunksize=chunk_size): yield chunk def main(): file_path = \'data.csv\' chunk_size = 1000 vectorizer = HashingVectorizer(alternate_sign=False) model = SGDClassifier(max_iter=5) accuracy_list = [] total_processed = 0 for chunk in stream_data(file_path, chunk_size): texts = chunk[\'text\'].values labels = chunk[\'label\'].values X = vectorizer.transform(texts) if total_processed == 0: model.partial_fit(X, labels, classes=np.unique(labels)) else: model.partial_fit(X, labels) total_processed += chunk_size # Simulate evaluation every chunk accuracy = accuracy_score(labels, model.predict(X)) accuracy_list.append(accuracy) plt.plot(range(chunk_size, chunk_size * (len(accuracy_list) + 1), chunk_size), accuracy_list) plt.xlabel(\'Number of processed examples\') plt.ylabel(\'Accuracy\') plt.show() if __name__ == \'__main__\': main() ``` Notes: - You may need to install the required libraries (e.g., `pandas`, `scikit-learn`, `matplotlib`). - Ensure that the CSV file `data.csv` is placed in the same directory as your script.","solution":"import pandas as pd import numpy as np import matplotlib.pyplot as plt from sklearn.feature_extraction.text import HashingVectorizer from sklearn.linear_model import SGDClassifier from sklearn.metrics import accuracy_score def stream_data(file_path, chunk_size=1000): Generator function to stream data from a CSV file in chunks. :param file_path: Path to the CSV file. :param chunk_size: Number of rows to read in each chunk. :yield: DataFrame containing the next chunk of data. for chunk in pd.read_csv(file_path, chunksize=chunk_size): yield chunk def main(file_path=\'data.csv\', chunk_size=1000): Main function to perform out-of-core learning. :param file_path: Path to the CSV file. :param chunk_size: Number of rows to read in each iteration. vectorizer = HashingVectorizer(alternate_sign=False) model = SGDClassifier(max_iter=5, tol=None) accuracy_list = [] total_processed = 0 chunks_processed = 0 stream = stream_data(file_path, chunk_size) for chunk in stream: texts = chunk[\'text\'].values labels = chunk[\'label\'].values X = vectorizer.transform(texts) if total_processed == 0: model.partial_fit(X, labels, classes=np.unique(labels)) else: model.partial_fit(X, labels) total_processed += len(chunk) chunks_processed += 1 # Simulate evaluation every chunk accuracy = accuracy_score(labels, model.predict(X)) accuracy_list.append(accuracy) plt.plot(range(chunk_size, chunk_size * (chunks_processed + 1), chunk_size), accuracy_list) plt.xlabel(\'Number of processed examples\') plt.ylabel(\'Accuracy\') plt.title(\'Accuracy of the Classifier Over Time\') plt.show() if __name__ == \'__main__\': main()"},{"question":"# Python Coding Assessment Question Objective To assess your understanding of Python\'s object-oriented programming features including class definition, inheritance, instance and class variables, method overriding, iterators, generators, and private variables. Problem Statement You are asked to implement a Python class structure that represents a simple university management system. This system should support the creation of university members (students and professors), manage their basic information, and keep track of courses both students and professors are involved in. Requirements 1. **Class Definitions**: - Create a base class `UniversityMember` with the following attributes: - `name` (string) - `age` (integer) - Implement a derived class `Student` that: - Inherits from `UniversityMember` - Has an additional attribute `student_id` (string) - Initializes all attributes in the constructor - Implement a derived class `Professor` that: - Inherits from `UniversityMember` - Has an additional attribute `professor_id` (string) - Initializes all attributes in the constructor 2. **Methods**: - In the `UniversityMember` class, define a method `display_info()` that prints the name and age of the member. - In both `Student` and `Professor` classes, override the `display_info()` method to include the `student_id` and `professor_id` respectively. 3. **Class and Instance Variables**: - Add a class variable `university_name` in the `UniversityMember` class that is shared among all instances. - Add an instance variable `courses` to both `Student` and `Professor` that stores a list of courses the member is involved in. 4. **Private Variables**: - Implement a private variable `_record` in the `Student` class to store the student\'s academic record as a dictionary (course: grade). - Provide a method `update_record()` to update the student\'s academic record. 5. **Iterators and Generators**: - Implement an iterator in the `Student` class to iterate over the courses the student is involved in. - Implement a generator in the `Professor` class that yields the courses the professor is teaching. 6. **Inheritance and Method Overriding**: - Ensure the `display_info()` method in both `Student` and `Professor` correctly overrides the method from `UniversityMember`. 7. **Testing the Structure**: - Instantiate at least one object of each class (`UniversityMember`, `Student`, `Professor`). - Set and display the `university_name` for each member. - Add courses to both `Student` and `Professor` objects and demonstrate iterating over these courses. - Update and display the student\'s academic record. Input and Output - There will be no direct input from the user; instead, demonstrate the functionality by creating instances and calling the methods as part of your solution. - Print statements should be used to show the output of various method calls, including displaying member information, university names, courses, and academic records. Constraints - Age values will be non-negative integers. - Names, student IDs, and professor IDs will be non-empty strings. - Course names will be non-empty strings and grades will be represented as strings (e.g., \'A\', \'B\', etc.). Example ```python class UniversityMember: university_name = \\"Python University\\" def __init__(self, name, age): self.name = name self.age = age def display_info(self): print(f\\"Name: {self.name}, Age: {self.age}\\") class Student(UniversityMember): def __init__(self, name, age, student_id): super().__init__(name, age) self.student_id = student_id self.courses = [] self._record = {} def display_info(self): super().display_info() print(f\\"Student ID: {self.student_id}\\") def update_record(self, course, grade): self._record[course] = grade def __iter__(self): return iter(self.courses) class Professor(UniversityMember): def __init__(self, name, age, professor_id): super().__init__(name, age) self.professor_id = professor_id self.courses = [] def display_info(self): super().display_info() print(f\\"Professor ID: {self.professor_id}\\") def add_course(self, course): self.courses.append(course) def teaching_courses(self): for course in self.courses: yield course # Example usage: student = Student(\\"John Doe\\", 20, \\"S12345\\") professor = Professor(\\"Dr. Smith\\", 45, \\"P67890\\") student.courses.append(\\"Math 101\\") student.courses.append(\\"History 201\\") professor.courses.append(\\"Math 101\\") professor.courses.append(\\"Physics 301\\") UniversityMember.university_name = \\"Advanced Python University\\" student.update_record(\\"Math 101\\", \\"A\\") student.display_info() professor.display_info() print(f\\"University: {UniversityMember.university_name}\\") print(\\"nStudent Courses:\\") for course in student: print(course) print(\\"nProfessor Courses:\\") for course in professor.teaching_courses(): print(course) ```","solution":"class UniversityMember: university_name = \\"Python University\\" def __init__(self, name, age): self.name = name self.age = age def display_info(self): print(f\\"Name: {self.name}, Age: {self.age}\\") class Student(UniversityMember): def __init__(self, name, age, student_id): super().__init__(name, age) self.student_id = student_id self.courses = [] self._record = {} def display_info(self): super().display_info() print(f\\"Student ID: {self.student_id}\\") def update_record(self, course, grade): self._record[course] = grade def __iter__(self): return iter(self.courses) class Professor(UniversityMember): def __init__(self, name, age, professor_id): super().__init__(name, age) self.professor_id = professor_id self.courses = [] def display_info(self): super().display_info() print(f\\"Professor ID: {self.professor_id}\\") def add_course(self, course): self.courses.append(course) def teaching_courses(self): for course in self.courses: yield course # Example usage: student = Student(\\"John Doe\\", 20, \\"S12345\\") professor = Professor(\\"Dr. Smith\\", 45, \\"P67890\\") student.courses.append(\\"Math 101\\") student.courses.append(\\"History 201\\") professor.courses.append(\\"Math 101\\") professor.courses.append(\\"Physics 301\\") UniversityMember.university_name = \\"Advanced Python University\\" student.update_record(\\"Math 101\\", \\"A\\") student.display_info() professor.display_info() print(f\\"University: {UniversityMember.university_name}\\") print(\\"nStudent Courses:\\") for course in student: print(course) print(\\"nProfessor Courses:\\") for course in professor.teaching_courses(): print(course)"},{"question":"**Objective:** Demonstrate your understanding of data validation and preprocessing using the scikit-learn utils package. **Problem Statement:** You are working on a machine learning project where you need to preprocess and validate your input data before training a model. You have a dataset represented as a 2D NumPy array `X` and a target array `y`. Your task is to implement a function `preprocess_and_validate` that performs the following steps: 1. Validate that `X` and `y` are consistent in length. 2. Ensure `X` is a 2D array containing only finite values (no NaNs or Infs). 3. Ensure `y` is a 1D array. 4. Convert `X` to a float array. 5. Normalize each row of `X` to have a unit L2 norm. 6. Verify that the normalization was successful by checking that the L2 norm of each row is approximately 1. **Function Signature:** ```python import numpy as np from sklearn.utils import check_X_y, assert_all_finite, as_float_array from sklearn.utils.sparsefuncs_fast import inplace_csr_row_normalize_l2 def preprocess_and_validate(X: np.ndarray, y: np.ndarray) -> np.ndarray: Preprocess and validate the input data. Parameters: - X: 2D numpy array of shape (n_samples, n_features) - y: 1D numpy array of shape (n_samples,) Returns: - X_normalized: 2D numpy array of shape (n_samples, n_features) with rows normalized to unit L2 norm # Your code here return X_normalized # Example Usage: X = np.array([[1, 2, 3], [4, 5, 6]], dtype=float) y = np.array([1, 2]) X_normalized = preprocess_and_validate(X, y) print(X_normalized) ``` **Constraints:** - The input `X` will always be a non-empty 2D numpy array of shape `(n_samples, n_features)`. - The input `y` will always be a non-empty 1D numpy array of shape `(n_samples,)`. **Notes:** - Use the `sklearn.utils` functions for validation and preprocessing wherever appropriate. - For row normalization to unit L2 norm, you may use `inplace_csr_row_normalize_l2`. **Expected Output Format:** - The function should return the normalized 2D array `X_normalized`. **Performance Requirements:** - The function should handle moderately large datasets efficiently, so consider the scalability of your approach. **Hints:** - Pay special attention to the shape and content of `X` and `y` throughout each step. - If you encounter any validation errors, raise appropriate exceptions with informative messages.","solution":"import numpy as np from sklearn.utils import check_X_y, assert_all_finite, as_float_array from sklearn.utils.validation import check_array def preprocess_and_validate(X: np.ndarray, y: np.ndarray) -> np.ndarray: Preprocess and validate the input data. Parameters: - X: 2D numpy array of shape (n_samples, n_features) - y: 1D numpy array of shape (n_samples,) Returns: - X_normalized: 2D numpy array of shape (n_samples, n_features) with rows normalized to unit L2 norm # Check that X and y have appropriate shapes and are consistent in length X, y = check_X_y(X, y, ensure_2d=True) # Ensure X contains only finite values assert_all_finite(X) # Convert X to a float array X = as_float_array(X) # Normalize each row of X to have a unit L2 norm norms = np.linalg.norm(X, axis=1, keepdims=True) X_normalized = X / norms # Verify the normalization normalized_norms = np.linalg.norm(X_normalized, axis=1) if not np.allclose(normalized_norms, 1): raise ValueError(\\"Normalization of rows in X was unsuccessful.\\") return X_normalized # Example Usage: # X = np.array([[1, 2, 3], [4, 5, 6]], dtype=float) # y = np.array([1, 2]) # X_normalized = preprocess_and_validate(X, y) # print(X_normalized)"},{"question":"**Python Lexical Analyzer Implementation** **Objective:** Implement a lexical analyzer in Python that can tokenize a given Python source code string. The tokenizer should correctly identify and categorize different tokens such as keywords, identifiers, literals, operators, and delimiters according to the rules described in the provided documentation. Additionally, it should handle the new soft keywords \\"match\\" and \\"case\\" introduced in Python 3.10. **Requirements:** 1. **Function Definition:** Define a function `tokenize_python_code(source_code: str) -> List[Tuple[str, str]]` where: - `source_code` is a string containing valid Python source code. - The function returns a list of tuples, each tuple containing two elements: - The category/type of the token (e.g., \'keyword\', \'identifier\', \'literal\', \'operator\', \'delimiter\', etc.). - The token itself. 2. **Token Categories:** - **Keywords:** All reserved words such as `if`, `else`, `for`, etc. - **Soft Keywords (Specific to Python 3.10):** `match` and `case`. - **Identifiers:** Variable names, function names, etc. - **Literals:** Numeric (integers, floats, imaginary numbers) and string literals. - **Operators:** Mathematical and logical operators such as `+`, `-`, `*`, `/`, `==`, etc. - **Delimiters:** Symbols used in the syntax such as `(`, `)`, `{`, `}`, `[`, `]`, `,`, `:` etc. 3. **Constraints and Considerations:** - The function should handle various whitespace characters and newlines correctly. - Comments should be ignored, and their content should not be tokenized. - The function should support different string and bytes literal formats as described. - Implicit and explicit line joining rules should be respected. - The function should properly manage indentation and dedentation as a part of the tokenization process. - The function should be efficient and capable of handling relatively large Python source files. 4. **Example Input:** ```python source_code = \'\'\' # Comment line def add(a, b): result = a + b return result match value: case 1: print(\\"One\\") case _: print(\\"Other\\") \'\'\' # Expected Output [ (\'keyword\', \'def\'), (\'identifier\', \'add\'), (\'delimiter\', \'(\'), (\'identifier\', \'a\'), (\'delimiter\', \',\'), (\'identifier\', \'b\'), (\'delimiter\', \')\'), (\'delimiter\', \':\'), (\'identifier\', \'result\'), (\'operator\', \'=\'), (\'identifier\', \'a\'), (\'operator\', \'+\'), (\'identifier\', \'b\'), (\'keyword\', \'return\'), (\'identifier\', \'result\'), (\'soft_keyword\', \'match\'), (\'identifier\', \'value\'), (\'delimiter\', \':\'), (\'soft_keyword\', \'case\'), (\'literal\', \'1\'), (\'delimiter\', \':\'), (\'identifier\', \'print\'), (\'delimiter\', \'(\'), (\'string\', \'\\"One\\"\'), (\'delimiter\', \')\'), (\'soft_keyword\', \'case\'), (\'identifier\', \'_\'), (\'delimiter\', \':\'), (\'identifier\', \'print\'), (\'delimiter\', \'(\'), (\'string\', \'\\"Other\\"\'), (\'delimiter\', \')\') ] ``` **Notes:** - You are not permitted to use existing Python libraries that perform lexical analysis or tokenization. - Pay attention to detail and ensure that all aspects of the provided documentation are covered. - Be prepared to handle edge cases, such as complex implicit line joining or Unicode identifiers. Good luck and happy coding!","solution":"import re def tokenize_python_code(source_code): Returns a list of tuples containing the token type and the token itself. # Define token regex patterns token_specification = [ (\'comment\', r\'#.*\'), # Comments (\'string\', r\'\\"([^n]|(.))*?\\"|\'.*?\'\'), # String literals (\'identifier\', r\'[A-Za-z_]w*\'), # Identifiers (\'number\', r\'d+(.d*)?|.d+\'), # Integer or decimal numbers (\'operator\', r\'==|!=|<=|>=|+|-|*|/|=\'), # Operators (\'delimiter\', r\'[(){}[],:]\'), # Delimiters (\'whitespace\', r\'s+\'), # Whitespace ] token_regex = \'|\'.join(f\'(?P<{pair[0]}>{pair[1]})\' for pair in token_specification) # Define keywords including new soft keywords in Python 3.10 keywords = { \'False\', \'True\', \'None\', \'and\', \'as\', \'assert\', \'async\', \'await\', \'break\', \'class\', \'continue\', \'def\', \'del\', \'elif\', \'else\', \'except\', \'finally\', \'for\', \'from\', \'global\', \'if\', \'import\', \'in\', \'is\', \'lambda\', \'nonlocal\', \'not\', \'or\', \'pass\', \'raise\', \'return\', \'try\', \'while\', \'with\', \'yield\', \'match\', \'case\' } soft_keywords = {\'match\', \'case\'} # Initialize list for tokens tokens = [] # Tokenize the source code using the compiled regex for match in re.finditer(token_regex, source_code): token_type = match.lastgroup token = match.group(token_type) if token_type == \'whitespace\' or token_type == \'comment\': continue # Ignore comments and whitespace if token_type == \'identifier\': if token in keywords: token_type = \'soft_keyword\' if token in soft_keywords else \'keyword\' else: token_type = \'identifier\' elif token_type == \'number\': token_type = \'literal\' elif token_type == \'string\': token_type = \'literal\' tokens.append((token_type, token)) return tokens"},{"question":"Objective Design a function that simulates the behavior of the `sdist` command\'s file inclusion and exclusion for creating source distributions. Problem Statement Implement a function `filter_files_patterns(file_list: List[str], commands: List[str]) -> List[str]`. This function should take in a list of file paths and a list of pattern commands, then return a list of file paths that are included based on these commands. The patterns and commands follow Unix-style \\"glob\\" patterns: - `\\"*\\"` matches any sequence of regular filename characters. - `\\"?\\"` matches any single regular filename character. - `\\"[range]\\"` matches any character in *range* (e.g., `\\"a-z\\"`, `\\"a-zA-Z\\"`, `\\"a-f0-9_.\\")`. The commands are: - `\\"include pat1 pat2 ...\\"`: Include all files matching any of the listed patterns. - `\\"exclude pat1 pat2 ...\\"`: Exclude all files matching any of the listed patterns. - `\\"recursive-include dir pat1 pat2 ...\\"`: Include all files under `dir` matching any of the listed patterns. - `\\"recursive-exclude dir pat1 pat2 ...\\"`: Exclude all files under `dir` matching any of the listed patterns. - `\\"global-include pat1 pat2 ...\\"`: Include all files anywhere in the source tree matching any of the listed patterns. - `\\"global-exclude pat1 pat2 ...\\"`: Exclude all files anywhere in the source tree matching any of the listed patterns. - `\\"prune dir\\"`: Exclude all files under `dir`. - `\\"graft dir\\"`: Include all files under `dir`. Function Signature ```python from typing import List def filter_files_patterns(file_list: List[str], commands: List[str]) -> List[str]: pass ``` Input - `file_list`: List of strings representing file paths in the source tree. - `commands`: List of strings where each string is a command that defines file inclusion or exclusion patterns. Output - List of strings representing the filtered file paths after applying the commands. Constraints - `1 <= len(file_list) <= 1000` - Each file path in `file_list` is a non-empty string with a maximum length of 255 characters. - Each command is a non-empty string with a maximum length of 255 characters. Example ```python file_list = [ \\"src/main.py\\", \\"src/__init__.py\\", \\"src/utils/util.py\\", \\"README.md\\", \\"setup.py\\" ] commands = [ \\"include *.md\\", \\"recursive-include src *.py\\", \\"exclude src/utils/*\\", \\"prune src/utils\\" ] assert filter_files_patterns(file_list, commands) == [\\"README.md\\", \\"src/main.py\\", \\"src/__init__.py\\"] ``` Note - This function should handle only the patterns and commands provided in the example and comply with the behavior described. - Make use of standard libraries to handle pattern matching and file operations effectively.","solution":"from typing import List import fnmatch import os def filter_files_patterns(file_list: List[str], commands: List[str]) -> List[str]: included_files = set() excluded_files = set() for command in commands: tokens = command.split() cmd = tokens[0] if cmd == \\"include\\": patterns = tokens[1:] for pattern in patterns: for file in file_list: if fnmatch.fnmatch(file, pattern): included_files.add(file) elif cmd == \\"exclude\\": patterns = tokens[1:] for pattern in patterns: for file in file_list: if fnmatch.fnmatch(file, pattern): excluded_files.add(file) elif cmd == \\"recursive-include\\": dir = tokens[1] patterns = tokens[2:] for pattern in patterns: for file in file_list: if file.startswith(dir + os.path.sep) and fnmatch.fnmatch(file, os.path.join(dir, pattern)): included_files.add(file) elif cmd == \\"recursive-exclude\\": dir = tokens[1] patterns = tokens[2:] for pattern in patterns: for file in file_list: if file.startswith(dir + os.path.sep) and fnmatch.fnmatch(file, os.path.join(dir, pattern)): excluded_files.add(file) elif cmd == \\"global-include\\": patterns = tokens[1:] for pattern in patterns: for file in file_list: if fnmatch.fnmatch(file, pattern): included_files.add(file) elif cmd == \\"global-exclude\\": patterns = tokens[1:] for pattern in patterns: for file in file_list: if fnmatch.fnmatch(file, pattern): excluded_files.add(file) elif cmd == \\"prune\\": dir = tokens[1] for file in file_list: if file.startswith(dir + os.path.sep): excluded_files.add(file) elif cmd == \\"graft\\": dir = tokens[1] for file in file_list: if file.startswith(dir + os.path.sep): included_files.add(file) final_files = included_files - excluded_files return sorted(final_files)"},{"question":"# Password Authentication System You are required to design a simple password authentication system using the `getpass` module in Python. The system should securely prompt users for their username and password, verify the credentials against stored data, and handle potential exceptions gracefully. Specifications: 1. **Function Name**: `authenticate_user` 2. **Input**: - None (The function will prompt the user for their username and password) 3. **Output**: - A string message indicating whether authentication was successful or not. 4. **Stored Data**: - A dictionary containing username-password pairs. - The dictionary should be initialized within the function as follows: ```python stored_credentials = { \'user1\': \'password123\', \'admin\': \'adminPass\', \'guest\': \'guestSecret\' } ``` Constraints: - The username and password should be prompted using `getpass.getuser()` and `getpass.getpass()`, respectively. - If the provided username does not exist in `stored_credentials`, return `\\"Username not found.\\"`. - If the username exists but the password does not match, return `\\"Authentication failed.\\"`. - If both the username and password match, return `\\"Authentication successful.\\"`. - Handle any potential `getpass.GetPassWarning` exceptions that may arise and return `\\"Secure password input not available.\\"`. Example Usage: ```python # Your solution here... # Example of calling the function (you will receive prompts for input): message = authenticate_user() print(message) ``` You need to implement the function `authenticate_user` as per the above specifications.","solution":"import getpass def authenticate_user(): Authenticates a user by prompting for a username and password. Returns a string indicating the authentication result. stored_credentials = { \'user1\': \'password123\', \'admin\': \'adminPass\', \'guest\': \'guestSecret\' } try: username = input(\\"Enter username: \\") password = getpass.getpass(\\"Enter password: \\") except Exception as e: return \\"Secure password input not available.\\" if username not in stored_credentials: return \\"Username not found.\\" if stored_credentials[username] != password: return \\"Authentication failed.\\" return \\"Authentication successful.\\""},{"question":"# Python Global Interpreter Lock (GIL) Simulation and Thread Management **Objective:** You are tasked with simulating the behavior of Python\'s Global Interpreter Lock (GIL) and demonstrating your understanding of how Python handles multithreading and thread states. The goal is to create a Python program that simulates the GIL and demonstrates thread-safe operations using threading, taking inspiration from the C APIs covered in the documentation. **Problem:** Implement a Python class `PyGILSimulator` that simulates the behavior of the Global Interpreter Lock (GIL). Your implementation should ensure that only one thread can execute a critical section of code at any given time. The class should expose methods to acquire and release the GIL, as well as manage thread states. Additionally, simulate the thread state changes while acquiring and releasing the GIL. **Requirements:** 1. Implement the `PyGILSimulator` class with the following methods: - `acquire_gil(self)` - Acquires the GIL for a thread, ensuring no other thread can enter the critical section. - `release_gil(self)` - Releases the GIL, allowing another thread to acquire it and enter the critical section. - `simulate_thread(self, thread_id, task)` - Simulates the execution of a task by a thread. This method should: - Acquire the GIL. - Print the current thread acquiring the GIL. - Execute the given task (a callable). - Release the GIL. - `get_current_thread_state(self)` - Returns the current thread state, indicating which thread (if any) holds the GIL. 2. Use Python\'s `threading` module to create threads that execute tasks while ensuring that the GIL is respected. 3. Demonstrate the usage of your `PyGILSimulator` with the following scenario: - Create three threads, each simulating a task that requires the GIL. - Each task should print a simple message indicating which thread is executing. - Output should clearly show the thread acquiring, executing, and releasing the GIL. **Constraints:** - Use the `threading` module to manage threads. - Ensure that access to the GIL and thread states are thread-safe. - The GIL simulation should ensure exclusive access to the critical section for any given thread while it holds the lock. # Example Usage ```python import threading import time class PyGILSimulator: def __init__(self): self.lock = threading.Lock() self.current_thread = None def acquire_gil(self): self.lock.acquire() def release_gil(self): self.lock.release() def simulate_thread(self, thread_id, task): self.acquire_gil() self.current_thread = thread_id print(f\\"Thread {thread_id} acquired the GIL.\\") task() print(f\\"Thread {thread_id} releasing the GIL.\\") self.current_thread = None self.release_gil() def get_current_thread_state(self): return self.current_thread # Demonstration def task(): time.sleep(1) print(\\"Task executed.\\") gil_simulator = PyGILSimulator() threads = [] for i in range(1, 4): thread = threading.Thread(target=gil_simulator.simulate_thread, args=(i, task)) threads.append(thread) thread.start() for thread in threads: thread.join() ``` This example shows how to implement the basic structure of `PyGILSimulator`, which ensures thread-safe execution of tasks by using the GIL. Your solution should follow similar logic, adding more details and handling thread states as specified.","solution":"import threading import time class PyGILSimulator: def __init__(self): self.lock = threading.Lock() self.current_thread = None def acquire_gil(self): self.lock.acquire() def release_gil(self): self.lock.release() def simulate_thread(self, thread_id, task): self.acquire_gil() self.current_thread = thread_id print(f\\"Thread {thread_id} acquired the GIL.\\") task() print(f\\"Thread {thread_id} releasing the GIL.\\") self.current_thread = None self.release_gil() def get_current_thread_state(self): return self.current_thread # Demonstration def task(): print(\\"Executing task...\\") time.sleep(1) gil_simulator = PyGILSimulator() threads = [] for i in range(1, 4): thread = threading.Thread(target=gil_simulator.simulate_thread, args=(i, task)) threads.append(thread) thread.start() for thread in threads: thread.join()"},{"question":"# Seaborn Clustermap Assessment You have been given the task of analyzing and visualizing a dataset using the seaborn library. Your objective is to create a clustermap with specified customizations using a given dataset. Task 1. **Load the Dataset:** Load the \\"penguins\\" dataset provided by seaborn. 2. **Data Preparation:** - Remove any rows with missing values. - Separate the species column to use as row labels. 3. **Basic Clustermap:** - Create a basic clustermap with default settings just to ensure the data is properly loaded. 4. **Customizations:** - **Figure Size:** Adjust the Figure size to (10, 8). - **Row and Column Clustering:** Disable row clustering. - **Color Map and Range:** Use the `coolwarm` colormap with a color value range limited to 0 through 2. - **Dendrogram Ratio:** Set the dendrogram ratio to (0.1, 0.25) for rows and columns, respectively. - **Add Row Colors:** Map unique species to colors for row labeling. ```python lut = dict(zip(species.unique(), \\"rgb\\")) # \'r\', \'g\' and \'b\' are representative of three species row_colors = species.map(lut) ``` - **Normalization:** Apply z-score normalization across columns. 5. **Save Plot:** - Save the plot as a file named `penguins_clustermap.png`. Your function should be named `create_penguins_clustermap`. Function Signature ```python def create_penguins_clustermap(): # Your code here pass ``` Output The function should not return anything, but it should save the resulting heatmap to a file named `penguins_clustermap.png`. Constraints and Considerations - You are encouraged to handle the data efficiently to avoid performance issues. - Ensure that the plot is clear and well-labeled for better interpretability. Example Output The expected output is a PNG file saved in the working directory with the specified customizations. The plot should visually cluster the columns and provide row color labels based on species. Good luck!","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt from scipy.cluster.hierarchy import linkage from scipy.stats import zscore def create_penguins_clustermap(): # Load the dataset penguins = sns.load_dataset(\\"penguins\\") # Remove any rows with missing values penguins_clean = penguins.dropna() # Separate the species column to use as row labels and drop non-numeric columns species = penguins_clean.pop(\\"species\\") penguins_numeric = penguins_clean.drop([\'island\', \'sex\'], axis=1) # Apply z-score normalization across columns penguins_zscore = penguins_numeric.apply(zscore) # Create a lookup table for the species colors lut = dict(zip(species.unique(), \\"rgb\\")) row_colors = species.map(lut) # Create the clustermap with specified customizations g = sns.clustermap(data=penguins_zscore, cmap=\\"coolwarm\\", figsize=(10, 8), row_cluster=False, vmin=0, vmax=2, dendrogram_ratio=(0.1, 0.25), row_colors=row_colors) # Save the plot g.savefig(\\"penguins_clustermap.png\\")"},{"question":"XML Processing with `xml.dom.pulldom` You are given an XML document containing product details from an online store. Each product is represented by an `<item>` element with attributes like `price`, `name`, and `category.` From this XML, you\'ll need to extract and process specific item information based on certain criteria. Task: Write a Python function `filter_and_expand_products(xml_string: str, min_price: int, category: str) -> str` that: 1. Uses the `xml.dom.pulldom` module to parse the given `xml_string`. 2. Filters the items based on the `min_price` and `category` specified. 3. Expands the node for each filtered product to include its children. 4. Returns a string containing the XML of these expanded products. Input: - `xml_string`: A string representing the XML data. - `min_price`: An integer representing the minimum price of products to be considered. - `category`: A string representing the category of products to be considered. Output: - A string containing the XML of the filtered and expanded product items. Constraints: - Only items with a price greater than or equal to `min_price` and matching the `category` should be included. - Ensure your function handles edge cases like no matching items gracefully. Example: Given the following XML string: ```xml <products> <item price=\\"100\\" name=\\"Laptop\\" category=\\"electronics\\"> <description>High performance laptop</description> <stock>20</stock> </item> <item price=\\"30\\" name=\\"Mouse\\" category=\\"electronics\\"> <description>Wireless mouse</description> <stock>100</stock> </item> <item price=\\"50\\" name=\\"Cookies\\" category=\\"food\\"> <description>Chocolate chip cookies</description> <stock>50</stock> </item> </products> ``` And the inputs: ```python xml_string = \'<products>...</products>\' # as above min_price = 50 category = \'electronics\' ``` Your function should return: ```xml <item price=\\"100\\" name=\\"Laptop\\" category=\\"electronics\\"> <description>High performance laptop</description> <stock>20</stock> </item> ``` Notes: 1. The XML string might contain multiple products in different categories. 2. You need to process XML as a flat stream of events and decide when to expand nodes based on the criteria. 3. Use the built-in methods from the `xml.dom.pulldom` library for parsing and expanding nodes. Function Signature: ```python def filter_and_expand_products(xml_string: str, min_price: int, category: str) -> str: pass ``` Good luck, and happy coding!","solution":"from xml.dom import pulldom from xml.dom.minidom import Node def filter_and_expand_products(xml_string: str, min_price: int, category: str) -> str: doc = pulldom.parseString(xml_string) result = [] for event, node in doc: if event == pulldom.START_ELEMENT and node.tagName == \'item\': price = int(node.getAttribute(\'price\')) node_category = node.getAttribute(\'category\') if price >= min_price and node_category == category: doc.expandNode(node) result.append(node.toxml()) return \'\'.join(result) # Example of usage: # xml_string = <products> # <item price=\\"100\\" name=\\"Laptop\\" category=\\"electronics\\"> # <description>High performance laptop</description> # <stock>20</stock> # </item> # <item price=\\"30\\" name=\\"Mouse\\" category=\\"electronics\\"> # <description>Wireless mouse</description> # <stock>100</stock> # </item> # <item price=\\"50\\" name=\\"Cookies\\" category=\\"food\\"> # <description>Chocolate chip cookies</description> # <stock>50</stock> # </item> # </products> # min_price = 50 # category = \'electronics\' # print(filter_and_expand_products(xml_string, min_price, category))"},{"question":"**Question: Enhance the `fmri` dataset visual representation using `seaborn.objects`** You are provided with the `fmri` dataset from the Seaborn library. Your task is to create a comprehensive visual representation of the data showing the `timepoint` vs. `signal`. You are required to: 1. Plot the `signal` over `timepoint` for the `parietal` region and `stim` event using a line plot. 2. Duplicate this line plot for each `subject` to compare their responses. 3. Use color to differentiate between the `event` types. 4. Add error bands to represent the variability within each `event` type. # Input: - The `fmri` dataset loaded using `seaborn.load_dataset(\\"fmri\\")`. # Requirements: - Use the `seaborn.objects` module. - Clearly differentiate between `event` types using color. - Include error bands in your plot. # Output: - A visualization where the x-axis represents `timepoint`, and the y-axis represents `signal`. - Each line corresponds to a different `subject` within the `parietal` region for the `stim` event. - Lines should be colored by `event`, and include error bands to show variability. # Example Code: ```python import seaborn.objects as so from seaborn import load_dataset # Load the fmri dataset fmri = load_dataset(\\"fmri\\") # Create the plot plot = ( fmri .query(\\"region == \'parietal\'\\") .pipe(so.Plot, \\"timepoint\\", \\"signal\\", color=\\"event\\") .add(so.Line(), group=\\"subject\\") .add(so.Band(), so.Est(), group=\\"event\\") ) # Display the plot plot.show() ``` Please write and execute the above code to produce the required visualization of the `fmri` dataset using `seaborn.objects`.","solution":"import seaborn.objects as so from seaborn import load_dataset def plot_fmri(): Create a visualization of the fmri dataset showing timepoint vs. signal for the parietal region and stim event, differentiated by event types, including error bands for variabilities. # Load the fmri dataset fmri = load_dataset(\\"fmri\\") # Create the plot plot = ( fmri .query(\\"region == \'parietal\' and event == \'stim\'\\") .pipe(so.Plot, \\"timepoint\\", \\"signal\\", color=\\"event\\") .add(so.Line(), group=\\"subject\\") .add(so.Band(), so.Est(), group=\\"event\\") ) # Display the plot plot.show()"},{"question":"**Problem Statement:** You are required to create a simple WSGI application that performs the following tasks: 1. **Read environment variables** from the incoming WSGI request. 2. **Determine the request scheme** (`http` or `https`) using `wsgiref.util.guess_scheme`. 3. **Construct and return a formatted response** containing these details in the response body. # Requirements: 1. **Function Signature:** ```python def simple_wsgi_app(environ, start_response): pass ``` 2. **Function Description:** - This function takes two parameters: - `environ`: A dictionary containing the WSGI environment variables. - `start_response`: A callable that starts the HTTP response. - The function should determine whether the request is made over `http` or `https` using `wsgiref.util.guess_scheme(environ)`. - The function should then construct a response that includes: - The request scheme (`http` or `https`). - The full request URI using `wsgiref.util.request_uri(environ)`. - All the environment variables formatted as `key: value` pairs, similar to how the `simple_app` example formats them. 3. **Constraints:** - You must use the `wsgiref.util.guess_scheme` and `wsgiref.util.request_uri` functions. - The response must be plain text, with content type `text/plain; charset=utf-8`. 4. **Expected Output:** - A successful response should return a status of \'200 OK\' with headers `[(\'Content-type\', \'text/plain; charset=utf-8\')]`. - The response body should be a textual representation of the request scheme, full URI, and all environment variables. # Example: ```python from wsgiref.util import setup_testing_defaults from wsgiref.simple_server import make_server def simple_wsgi_app(environ, start_response): from wsgiref.util import guess_scheme, request_uri # Guess the scheme (http or https) scheme = guess_scheme(environ) # Get the full request URI full_uri = request_uri(environ) # Start the response status = \'200 OK\' headers = [(\'Content-type\', \'text/plain; charset=utf-8\')] start_response(status, headers) # Construct the response body response_body = f\\"Request Scheme: {scheme}nFull Request URI: {full_uri}nnEnvironment Variables:n\\" # Add environment variables to the response for key, value in environ.items(): response_body += f\\"{key}: {value}n\\" # Return the response body as a list of bytes return [response_body.encode(\'utf-8\')] # Set up a simple server to test the application def run_server(): with make_server(\'\', 8000, simple_wsgi_app) as httpd: print(\\"Serving on port 8000...\\") httpd.serve_forever() ``` To test this WSGI application, you can run the `run_server` function and make HTTP requests to `localhost:8000`. **Note:** Ensure that you handle all edge cases and follow the instructions for using `wsgiref.util.guess_scheme` and `wsgiref.util.request_uri` to satisfy the problem constraints.","solution":"from wsgiref.util import guess_scheme, request_uri def simple_wsgi_app(environ, start_response): # Guess the scheme (http or https) scheme = guess_scheme(environ) # Get the full request URI full_uri = request_uri(environ) # Start the response status = \'200 OK\' headers = [(\'Content-type\', \'text/plain; charset=utf-8\')] start_response(status, headers) # Construct the response body response_body = f\\"Request Scheme: {scheme}nFull Request URI: {full_uri}nnEnvironment Variables:n\\" # Add environment variables to the response for key, value in environ.items(): response_body += f\\"{key}: {value}n\\" # Return the response body as a list of bytes return [response_body.encode(\'utf-8\')]"},{"question":"Objective Demonstrate your understanding of the **zipapp** module by using its Python API to manage Python executable zip archives. You will write a function that creates an executable archive, optionally compresses it, and verifies its integrity by inspecting its content. Task Implement a function `create_and_verify_archive` that creates an executable zip archive from a given source directory and then verifies the created archive by checking its structure. # Function Signature ```python def create_and_verify_archive(source: str, target: str, interpreter: str = None, main: str = None, compressed: bool = False) -> bool: pass ``` # Inputs - `source` (str): The path to the source directory containing Python code. - `target` (str): The path where the archive will be created. - `interpreter` (str, optional): The Python interpreter to include in the shebang line. Default is `None`. - `main` (str, optional): The main function to run inside the archive. Should be in the format `\\"package.module:function\\"`. Default is `None`. - `compressed` (bool, optional): Flag to indicate if the archive should be compressed. Default is `False`. # Outputs - Returns `True` if the archive is created and verified successfully, otherwise returns `False`. # Constraints 1. Ensure the source directory exists and contains a `__main__.py` or the `main` argument is provided. 2. The function should handle exceptions and return `False` if any error occurs during the creation or verification process. 3. The verification should check that the archive can be read as a zip file and contains the `__main__.py` file in its root. # Example Usage ```python source = \\"path/to/source_directory\\" target = \\"path/to/output_archive.pyz\\" interpreter = \\"/usr/bin/env python3\\" main = \\"myapp:main\\" compressed = True result = create_and_verify_archive(source, target, interpreter, main, compressed) print(result) # Expected: True if archive is created and verified successfully ``` # Notes - Use the `zipapp.create_archive` function to create the archive. - Use the `zipfile` module to verify the contents of the created archive. Hints 1. You may use the `os.path.exists` function to check the existence of paths. 2. The `zipfile.ZipFile` can be used to inspect the contents of the zip archive. 3. Proper exception handling is crucial for robustness.","solution":"import os import zipfile import zipapp def create_and_verify_archive(source: str, target: str, interpreter: str = None, main: str = None, compressed: bool = False) -> bool: Creates an executable archive from a source directory and verifies its content. Parameters: - source (str): The path to the source directory containing Python code. - target (str): The path where the archive will be created. - interpreter (str, optional): The Python interpreter to include in the shebang line. - main (str, optional): The main function to run inside the archive. - compressed (bool, optional): Flag to indicate if the archive should be compressed. Returns: - bool: True if the archive is created and verified successfully, otherwise False. try: if not os.path.exists(source): return False if main: main_py = os.path.join(source, \\"__main__.py\\") with open(main_py, \'w\') as f: f.write(f\\"import {main.split(\':\')[0]}n\\") f.write(f\\"{main}n\\") zipapp.create_archive(source, target, interpreter=interpreter, compressed=compressed) with zipfile.ZipFile(target, \'r\') as archive: # Verify that the archive contains __main__.py if \'__main__.py\' not in archive.namelist(): return False return True except Exception as e: print(f\\"An error occurred: {e}\\") return False"},{"question":"# Python Logging System Configuration and Customization You are required to configure and customize a logging system for a fictional application using Python\'s `logging` module. Follow the detailed instructions to set up different logging levels, formatters, handlers, and demonstrate hierarchical logging capabilities. Task: 1. **Logger Setup:** - Create a logger named `app.main` with a logging level of `DEBUG`. - Create a logger named `app.main.module1` as a descendant of `app.main`. 2. **Handler and Formatter:** - Add a `StreamHandler` to `app.main` logger that outputs log messages to the console with the log level of `INFO` or higher. Use a formatter that includes the time, logger name, level, and message. - Add a `FileHandler` to `app.main.module1` logger that outputs log messages to a file named `module1.log` with the log level of `DEBUG` or higher. Use a formatter that includes just the message and level. 3. **Log Messages:** - For `app.main` logger, log a message with `INFO` level. - For `app.main.module1` logger, log a message with `DEBUG` level. - Observe propagation of log messages from `app.main.module1` logger to `app.main` logger. 4. **Custom LogRecord Factory:** - Implement a custom log record factory that adds a new attribute `custom_attr` to the `LogRecord`. All log messages from `app.main.module1` logger should include this custom attribute in the formatted message. RESTRICTIONS: - Do not alter the configuration of the root logger. # Expected Input and Output Formats: **Input:** None (All configurations and loggings should be set up programmatically within the script) **Output:** - Console output: ``` 2023-09-15 10:01:58,584 - app.main - INFO - Main logger info message 2023-09-15 10:01:58,584 - app.main.module1 - DEBUG - Module1 logger debug message ``` - File `module1.log` output: ``` DEBUG - Module1 logger debug message - Custom: <custom_value> ``` # Constraints: - Handlers should use a thread-safe mechanism. - The custom attribute added to the `LogRecord` should be unique and recognizable in the log messages. # Your Solution: Implement the solution in Python considering all the points mentioned above to correctly configure and customize the logging system using the `logging` module.","solution":"import logging # Step 1: Logger Setup logger_main = logging.getLogger(\'app.main\') logger_main.setLevel(logging.DEBUG) logger_module1 = logging.getLogger(\'app.main.module1\') logger_module1.setLevel(logging.DEBUG) # Step 2: Handler and Formatter # 2a. StreamHandler for app.main stream_formatter = logging.Formatter(\'%(asctime)s - %(name)s - %(levelname)s - %(message)s\') stream_handler = logging.StreamHandler() stream_handler.setLevel(logging.INFO) stream_handler.setFormatter(stream_formatter) logger_main.addHandler(stream_handler) # 2b. FileHandler for app.main.module1 file_formatter = logging.Formatter(\'%(levelname)s - %(message)s - Custom: %(custom_attr)s\') file_handler = logging.FileHandler(\'module1.log\') file_handler.setLevel(logging.DEBUG) file_handler.setFormatter(file_formatter) logger_module1.addHandler(file_handler) # Step 4: Custom LogRecord Factory old_factory = logging.getLogRecordFactory() def record_factory(*args, **kwargs): record = old_factory(*args, **kwargs) record.custom_attr = \'<custom_value>\' return record logging.setLogRecordFactory(record_factory) # Step 3: Log Messages logger_main.info(\'Main logger info message\') logger_module1.debug(\'Module1 logger debug message\')"},{"question":"# Seaborn Advanced Plotting **Objective**: Leverage the seaborn package to create a complex visualization based on specific criteria. **Problem Statement**: You are given a dataset `penguins`, which contains information about penguins of different species. The dataset includes columns such as `species`, `sex`, `body_mass_g`, and `flipper_length_mm`. Your task is to create a function `create_penguins_plot` that generates a comprehensive plot using seaborn. **Requirements**: 1. The plot should display the distribution of `body_mass_g` for each `species` and should be color-coded by `sex`. 2. Use `Dash` to represent individual data points with `body_mass_g` values for each species. 3. Aggregate values should be shown using `Dash` with different opacity. 4. Include both `Dots` and `Dash` for better visualization, incorporating jitter to avoid overlapping points. 5. Ensure the plot demonstrates dodge functionality to prevent points from overlapping too closely. **Function Signature**: ```python def create_penguins_plot(df): Args: df (DataFrame): pandas DataFrame containing the penguins dataset Returns: None: The function should display the generated plot. pass ``` # Constraints: * Your solution should use seaborn. * You may import other necessary libraries like pandas and numpy. * Assume `df` is a well-formed pandas DataFrame with no missing values in the critical columns used (`species`, `sex`, `body_mass_g`, `flipper_length_mm`). # Input: A pandas DataFrame `df` with the following sample structure: ``` species sex body_mass_g flipper_length_mm 0 Adelie Male 3750 181 1 Chinstrap Female 3800 195 2 Adelie Female 3250 180 ... ``` # Output: The function should display a seaborn plot meeting the above criteria. No return value is needed. # Example: ```python import pandas as pd # Sample DataFrame creation data = { \'species\': [\'Adelie\', \'Chinstrap\', \'Adelie\', \'Gentoo\', \'Chinstrap\', \'Gentoo\'], \'sex\': [\'Male\', \'Female\', \'Female\', \'Male\', \'Female\', \'Female\'], \'body_mass_g\': [3750, 3800, 3250, 4500, 3700, 5000], \'flipper_length_mm\': [181, 195, 180, 210, 195, 230] } df = pd.DataFrame(data) # Function call create_penguins_plot(df) ``` **Note**: This expected plot should provide visually distinct points and aggregate lines, clearly differentiated by species and sex, with minimal overlapping. **Grading Criteria**: * Correct implementation of seaborn plotting functions. * Correct use of `Dash`, `Dots`, `Dodge`, `Agg`, and `Jitter`. * Proper dataset management and plotting criteria adherence. * Clarity and quality of the visualization.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_penguins_plot(df): Generates a plot to display the distribution of body mass for each species color-coded by sex. Args: df (DataFrame): pandas DataFrame containing the penguins dataset Returns: None: The function displays the generated plot. plt.figure(figsize=(12, 6)) # Creating a swarmplot for individual data points sns.swarmplot(data=df, x=\'species\', y=\'body_mass_g\', hue=\'sex\', dodge=True, alpha=0.75) # Creating a pointplot for aggregate values sns.pointplot(data=df, x=\'species\', y=\'body_mass_g\', hue=\'sex\', dodge=0.532, join=False, palette=\'dark\', markers=\'d\', scale=0.75) plt.title(\\"Distribution of Body Mass by Species and Sex\\") plt.show()"},{"question":"**Objective:** You are tasked with implementing a function that accepts various attributes of a code object and creates a new empty code object. This exercise will test your understanding of Python\'s code objects and low-level Python execution concepts. **Function Signature:** ```python def create_empty_code_object(filename: str, funcname: str, firstlineno: int) -> object: pass ``` **Task:** 1. Implement the function `create_empty_code_object` to create a new empty code object using the `PyCode_NewEmpty` function provided by the Python C API. 2. The function should only use the provided filename, function name, and first line number to create the code object. **Input:** - `filename` (str): The name of the file from which the code object is created. - `funcname` (str): The name of the function or code block. - `firstlineno` (int): The first line number in the source file where the code object starts. **Output:** - Returns a new empty code object corresponding to the given filename, function name, and first line number. **Constraints:** - The resulting code object must strictly adhere to the structure specified by the `PyCode_NewEmpty` function in the CPython API. **Example:** ```python # Example usage empty_code_obj = create_empty_code_object(\\"example.py\\", \\"example_func\\", 1) print(empty_code_obj) # Expected output is an object of type <code> (type may vary depending on implementation details) ``` **Notes:** - Use the provided `PyCode_NewEmpty` function to create the code object. - Since directly using the C API in Python requires external modules like `ctypes` or `cffi`, you may need to simulate or describe this interaction appropriately if direct usage is not feasible. # Evaluation Criteria: - Correct functionality: Does the code create a code object as specified? - Understanding of low-level Python constructs. - Code readability and proper documentation.","solution":"import dis import types def create_empty_code_object(filename: str, funcname: str, firstlineno: int) -> types.CodeType: Create a new empty code object using the provided filename, function name, and first line number. Args: - filename (str): The name of the file from which the code object is created. - funcname (str): The name of the function or code block. - firstlineno (int): The first line number in the source file where the code object starts. Returns: - types.CodeType: A new empty code object. return types.CodeType( 0, # argcount 0, # posonlyargcount 0, # kwonlyargcount 0, # nlocals 0, # stacksize 0, # flags b\'\', # codestring (), # constants (), # names (), # varnames filename, # filename funcname, # name firstlineno, # first line number b\'\', # lnotab (), # freevars (), # cellvars )"},{"question":"**Problem:** You are working on a scientific project that involves analyzing temperature data collected from various sources globally. Given the temperature readings for a week from different monitoring stations, you need to preprocess and analyze this data. This problem involves several steps – data cleaning, statistical analysis, and result reporting based on the `math` module functions. # Tasks 1. **Data Cleaning:** - Implement a function `clean_data(data: List[float]) -> List[float]` that removes any non-finite numbers (such as `NaN`, `inf`, `-inf`) from the data. 2. **Statistical Analysis:** - Implement a function `calculate_statistics(data: List[float]) -> Tuple[float, float, float, float]` that calculates and returns the following statistics: - Mean of the data. - Standard Deviation of the data. - Minimum and maximum value of the data. 3. **Weekly Analysis:** - Implement a function `weekly_temperature_analysis(data: List[List[float]]) -> List[Tuple[float, float, float, float]]` that takes the temperature data for each day (data format: List containing 7 Lists of daily temperatures) and for each day returns a tuple of the above-mentioned statistics. # Note: - Use the `math` module functions as much as possible. - Handle exceptions gracefully and ensure the functions do not crash with invalid inputs. # Example: ```python data = [ [95.5, 102.3, 99.8, float(\'nan\'), float(\'inf\')], [101.8, 97.6, 103.1, 100.5, 99.2, 98.1], [float(\'-inf\'), 100.2, 103.4, 98.6, 100.9], [102.3, 97.5, 99.8, 100.0, 99.4, 98.7], [99.9, 100.1, 97.2, 103.8, float(\'nan\')], [95.8, 96.7, 99.4, 98.9, 100.0, 102.2], [98.8, 97.3, 95.5, 101.3, 97.8, 100.4] ] # After cleaning cleaned_data = [95.5, 102.3, 99.8, 101.8, 97.6, 103.1, 100.5, 99.2, 98.1, 100.2, 103.4, 98.6, 100.9, 102.3, 97.5, 99.8, 100.0, 99.4, 98.7, 99.9, 100.1, 97.2, 103.8, 95.8, 96.7, 99.4, 98.9, 100.0, 102.2, 98.8, 97.3, 95.5, 101.3, 97.8, 100.4] # Statistical Analysis stats = (mean, std_dev, min_val, max_val) = (data_mean, data_std_dev, data_min, data_max) # Weekly Analysis weekly_stats = [ (95.87, 2.83, 99.8, 95.5), (100.05, 2.16, 103.1, 97.6), ... ] ``` # Function Signature: ```python import math from typing import List, Tuple def clean_data(data: List[float]) -> List[float]: pass def calculate_statistics(data: List[float]) -> Tuple[float, float, float, float]: pass def weekly_temperature_analysis(data: List[List[float]]) -> List[Tuple[float, float, float, float]]: pass ``` # Constraints: - Each day\'s temperature data consists of at least one valid temperature reading. - The temperature values in the input list are floats. - Assume the input data will always represent exactly 7 days of recordings, where each day\'s data is a list of floats. # Performance Requirements: - Functions should be efficient in handling input data, which is expected to be at most 100 temperature readings per day.","solution":"import math from typing import List, Tuple def clean_data(data: List[float]) -> List[float]: Cleans the input data by removing all non-finite numbers (NaN, inf, -inf). return [x for x in data if math.isfinite(x)] def calculate_statistics(data: List[float]) -> Tuple[float, float, float, float]: Calculates the following statistics from the given data: - Mean - Standard Deviation - Minimum value - Maximum value if not data: raise ValueError(\\"Input data is empty.\\") mean = sum(data) / len(data) variance = sum((x - mean) ** 2 for x in data) / len(data) std_dev = math.sqrt(variance) min_val = min(data) max_val = max(data) return mean, std_dev, min_val, max_val def weekly_temperature_analysis(data: List[List[float]]) -> List[Tuple[float, float, float, float]]: Analyzes weekly temperature data. Returns a list of statistics calculated for each day. if len(data) != 7: raise ValueError(\\"Input data must contain 7 lists of daily temperature readings.\\") daily_stats = [] for day_data in data: cleaned_day_data = clean_data(day_data) day_stats = calculate_statistics(cleaned_day_data) daily_stats.append(day_stats) return daily_stats"},{"question":"# Pandas Plotting Assessment **Objective**: Demonstrate your ability to use the advanced plotting functions in the `pandas.plotting` module to analyze and visualize data. **Problem Statement**: You are given a dataset containing information about various species of flowers, specifically the Iris dataset. The dataset includes the following columns: - `sepal_length` - `sepal_width` - `petal_length` - `petal_width` - `species` (which indicates the species of the flower: Setosa, Versicolour, or Virginica) You need to write a Python function using the pandas library that performs the following tasks: 1. Generate an Andrews curves plot to visualize cluster separability based on species. 2. Create a scatter matrix plot to inspect the pairwise relationships between the features. 3. Save these plots as `andrews_curves.png` and `scatter_matrix.png` respectively in the current working directory. **Function Signature**: ```python import pandas as pd def visualize_iris_data(iris_df: pd.DataFrame) -> None: Visualize the Iris dataset using pandas plotting functions and save plots as .png files. Parameters: iris_df (pd.DataFrame): Dataframe containing Iris dataset with columns [\'sepal_length\', \'sepal_width\', \'petal_length\', \'petal_width\', \'species\'] Returns: None ``` **Constraints**: - The function should assume that `iris_df` is a valid DataFrame containing the required columns. - The function should handle any necessary imports within its body. - You are free to pre-process the dataset but it is not required for this task. **Example**: ```python # Example usage import pandas as pd # Assume iris_data is a DataFrame loaded with the correct Iris dataset structure iris_data = pd.read_csv(\'path_to_iris.csv\') visualize_iris_data(iris_data) # This should generate \'andrews_curves.png\' and \'scatter_matrix.png\' in the current directory. ``` Good luck!","solution":"import pandas as pd import pandas.plotting as pd_plotting import matplotlib.pyplot as plt def visualize_iris_data(iris_df: pd.DataFrame) -> None: Visualize the Iris dataset using pandas plotting functions and save plots as .png files. Parameters: iris_df (pd.DataFrame): Dataframe containing Iris dataset with columns [\'sepal_length\', \'sepal_width\', \'petal_length\', \'petal_width\', \'species\'] Returns: None # Generate Andrews Curves plot plt.figure() pd_plotting.andrews_curves(iris_df, \'species\') plt.savefig(\'andrews_curves.png\') # Generate Scatter Matrix plot scatter_matrix = pd_plotting.scatter_matrix(iris_df, figsize=(10, 10), diagonal=\'kde\', marker=\'o\') # Iterate over scatter_matrix array to adjust labels for ax in scatter_matrix.ravel(): ax.set_xlabel(ax.get_xlabel(), fontsize=12, rotation=90) ax.set_ylabel(ax.get_ylabel(), fontsize=12, rotation=0, ha=\'right\') plt.savefig(\'scatter_matrix.png\') plt.close(\'all\')"},{"question":"# CSV Data Processing Assessment Problem Statement You are provided with a CSV file named `employee_data.csv` containing employee data with the following columns: `id`, `first_name`, `last_name`, `department`, `salary`. Your task is to perform the following operations using Python\'s `csv` module: 1. Read the data from `employee_data.csv`. 2. Filter out all employees who belong to the \\"HR\\" department. 3. Write the filtered data to a new CSV file named `filtered_employee_data.csv` ensuring the data is written in a format suitable for Excel (using the `excel` dialect). 4. Ensure that while writing to the new CSV file, any missing data for an employee should be filled with \\"N/A\\". Input/Output Requirements - **Input**: - A CSV file `employee_data.csv`. - **Output**: - A newly created CSV file `filtered_employee_data.csv` containing all employees excluding those from the \\"HR\\" department. Missing data should be filled with \\"N/A\\". Constraints - The CSV file `employee_data.csv` can contain a varying number of records. - The solution should handle any missing or extra fields gracefully. - Use the `csv` module only for all the operations. Example Suppose the content of `employee_data.csv` is: ``` id,first_name,last_name,department,salary 1,John,Doe,Engineering,50000 2,Jane,Smith,HR,60000 3,Bob,Brown,Marketing,55000 4,Alice,Johnson,HR, 5,Charlie,Brown,Engineering,65000 ``` After processing, `filtered_employee_data.csv` should contain: ``` id,first_name,last_name,department,salary 1,John,Doe,Engineering,50000 3,Bob,Brown,Marketing,55000 5,Charlie,Brown,Engineering,65000 ``` # Instructions 1. Implement the function `filter_employees(input_file, output_file)`. 2. The function should take two arguments: - `input_file`: The path to the input CSV file (`employee_data.csv`). - `output_file`: The path to the output CSV file (`filtered_employee_data.csv`). 3. Inside the function: - Read the content of `input_file` using `csv.DictReader`. - Filter out records where the department is \\"HR\\". - Write the filtered data to `output_file` using `csv.DictWriter`, ensuring the Excel dialect is used and missing data is filled with \\"N/A\\". ```python import csv def filter_employees(input_file, output_file): # Your code here to read, filter and write CSV data pass # Example usage: # filter_employees(\'employee_data.csv\', \'filtered_employee_data.csv\') ``` # Notes - Make sure to handle file reads and writes correctly to avoid file handling errors. - Test your implementation with different CSV data samples to ensure robustness.","solution":"import csv def filter_employees(input_file, output_file): Reads an existing CSV file, filters out employees from the HR department, and writes the filtered data to a new CSV file. Args: input_file (str): Path to the input CSV file. output_file (str): Path to the output CSV file. # Read the input CSV file with open(input_file, \'r\', newline=\'\') as csvfile: reader = csv.DictReader(csvfile) fieldnames = reader.fieldnames # Initialize list to hold filtered employee data filtered_employees = [] # Filter out \'HR\' department employees and handle missing data for row in reader: if row[\'department\'] != \'HR\': filtered_employees.append({ key: (value if value else \'N/A\') for key, value in row.items() }) # Write the filtered data to the output CSV file with open(output_file, \'w\', newline=\'\') as csvfile: writer = csv.DictWriter(csvfile, fieldnames=fieldnames, dialect=\'excel\') writer.writeheader() writer.writerows(filtered_employees) # Example usage: # filter_employees(\'employee_data.csv\', \'filtered_employee_data.csv\')"},{"question":"# Question: Incremental Learning with Out-of-Core Data Problem Statement: You are tasked with building a machine learning system that can classify text documents on the fly using incremental learning. This system should be able to handle large datasets that do not fit into memory, by processing the data in mini-batches. Requirements: 1. Implement a function `incremental_text_classification(data_stream, batch_size)` that processes incoming data batches and trains a classifier incrementally. 2. The function should utilize `HashingVectorizer` for feature extraction. 3. Use `SGDClassifier` from `sklearn.linear_model` as the incremental learning algorithm. 4. After processing all batches, the function should return the trained model. Input: - `data_stream`: An iterable (e.g., generator function) that yields tuples `(X, y)`, where `X` is a list of text documents and `y` is the corresponding list of labels. - `batch_size`: The number of samples to process in each mini-batch. Output: - The trained `SGDClassifier` model. Example: ```python from sklearn.linear_model import SGDClassifier from sklearn.feature_extraction.text import HashingVectorizer def incremental_text_classification(data_stream, batch_size): vectorizer = HashingVectorizer(n_features=2**20) classifier = SGDClassifier() for X_batch, y_batch in data_stream: X_transformed = vectorizer.transform(X_batch) classifier.partial_fit(X_transformed, y_batch, classes=[0, 1]) return classifier # Example usage: def mock_data_stream(): data = [ ([\\"text document one\\", \\"another document\\"], [0, 1]), ([\\"more documents here\\", \\"and another one\\"], [0, 1]) ] for batch in data: yield batch model = incremental_text_classification(mock_data_stream(), batch_size=2) ``` Constraints: - The `batch_size` parameter should be tuned to ensure efficiency. - The labels (`y`) should consist of binary classes `[0, 1]` in this example, but you can extend it to more classes with necessary adjustments. - You should handle common issues such as unseen labels correctly by specifying the `classes` parameter in the `partial_fit` method. Performance Requirements: - Ensure that data is processed incrementally without loading the entire dataset into memory at any point. - The implementation should handle large datasets efficiently by managing memory usage and processing each mini-batch in constant time.","solution":"from sklearn.linear_model import SGDClassifier from sklearn.feature_extraction.text import HashingVectorizer def incremental_text_classification(data_stream, batch_size): Processes incoming data batches and trains a classifier incrementally. Parameters: data_stream (iterable): An iterable that yields tuples (X, y), where X is a list of text documents and y is the corresponding list of labels. batch_size (int): The number of samples to process in each mini-batch. Returns: SGDClassifier: The trained SGDClassifier model. vectorizer = HashingVectorizer(n_features=2**20) classifier = SGDClassifier() is_first_batch = True for X_batch, y_batch in data_stream: X_transformed = vectorizer.transform(X_batch) if is_first_batch: classifier.partial_fit(X_transformed, y_batch, classes=[0, 1]) is_first_batch = False else: classifier.partial_fit(X_transformed, y_batch) return classifier # Example usage: def mock_data_stream(): data = [ ([\\"text document one\\", \\"another document\\"], [0, 1]), ([\\"more documents here\\", \\"and another one\\"], [0, 1]) ] for batch in data: yield batch model = incremental_text_classification(mock_data_stream(), batch_size=2)"},{"question":"You are provided with a dataset in a CSV file containing student records. Each record consists of the student\'s first name, last name, student ID, and a list of their grades. Your task is to write functions to read this data, process it, and write the processed data back to another CSV file. Specifically: 1. Implement a function `read_and_process_csv(input_file, output_file)` that: - Reads the input CSV file. - Computes the average grade for each student. - Writes the processed data to an output CSV file with the additional average grade field. 2. Implement a function `find_highest_averages(input_file, output_file, top_n)` that: - Reads the processed CSV file. - Finds the top `top_n` students with the highest average grades. - Writes these records to another output CSV file. # Expected Input and Output Formats: 1. `read_and_process_csv(input_file, output_file)`: - **Input**: - `input_file`: A string representing the path of the input CSV file. - The CSV file format is as follows (headers included): ``` first_name,last_name,student_id,grades John,Doe,12345,85|90|78 Jane,Smith,67890,92|88|79 ``` - **Output**: - `output_file`: A string representing the path of the output CSV file. - The output CSV file format should be as follows (headers included): ``` first_name,last_name,student_id,grades,average_grade John,Doe,12345,85|90|78,84.33 Jane,Smith,67890,92|88|79,86.33 ``` 2. `find_highest_averages(input_file, output_file, top_n)`: - **Input**: - `input_file`: A string representing the path of the processed CSV file. - `output_file`: A string representing the path of the output CSV file. - `top_n`: An integer representing the number of top students to output. - The input CSV file format is as follows (headers included): ``` first_name,last_name,student_id,grades,average_grade John,Doe,12345,85|90|78,84.33 Jane,Smith,67890,92|88|79,86.33 ``` - **Output**: - The output CSV file format should be as follows (headers included): ``` first_name,last_name,student_id,grades,average_grade Jane,Smith,67890,92|88|79,86.33 John,Doe,12345,85|90|78,84.33 ``` # Constraints: - Ensure the input and output CSV files are handled properly, including cases where fields may have quotes or other special characters. - Use the `csv` module for reading and writing the CSV files. - Implement error handling to manage cases where the CSV files might be improperly formatted. # Example Solution: ```python import csv def read_and_process_csv(input_file, output_file): with open(input_file, newline=\'\') as infile, open(output_file, \'w\', newline=\'\') as outfile: reader = csv.DictReader(infile) fieldnames = reader.fieldnames + [\'average_grade\'] writer = csv.DictWriter(outfile, fieldnames=fieldnames) writer.writeheader() for row in reader: grades = list(map(int, row[\'grades\'].split(\'|\'))) average_grade = sum(grades) / len(grades) row[\'average_grade\'] = round(average_grade, 2) writer.writerow(row) def find_highest_averages(input_file, output_file, top_n): with open(input_file, newline=\'\') as infile: reader = csv.DictReader(infile) sorted_records = sorted(reader, key=lambda x: float(x[\'average_grade\']), reverse=True) with open(output_file, \'w\', newline=\'\') as outfile: writer = csv.DictWriter(outfile, fieldnames=reader.fieldnames) writer.writeheader() for row in sorted_records[:top_n]: writer.writerow(row) ``` # Submit: - The implemented functions `read_and_process_csv` and `find_highest_averages`. - Example test cases you used to verify the correctness of your code.","solution":"import csv def read_and_process_csv(input_file, output_file): Reads the input CSV file, computes the average grade for each student, and writes the processed data to an output file. with open(input_file, newline=\'\') as infile, open(output_file, \'w\', newline=\'\') as outfile: reader = csv.DictReader(infile) fieldnames = reader.fieldnames + [\'average_grade\'] writer = csv.DictWriter(outfile, fieldnames=fieldnames) writer.writeheader() for row in reader: grades = list(map(int, row[\'grades\'].split(\'|\'))) average_grade = sum(grades) / len(grades) row[\'average_grade\'] = round(average_grade, 2) writer.writerow(row) def find_highest_averages(input_file, output_file, top_n): Finds the top n students with the highest average grades and writes their records to an output file. with open(input_file, newline=\'\') as infile: reader = csv.DictReader(infile) sorted_records = sorted(reader, key=lambda x: float(x[\'average_grade\']), reverse=True) with open(output_file, \'w\', newline=\'\') as outfile: writer = csv.DictWriter(outfile, fieldnames=reader.fieldnames) writer.writeheader() for row in sorted_records[:top_n]: writer.writerow(row)"},{"question":"# Objective To assess your understanding of PyTorch\'s autograd system, particularly the `gradcheck` and `gradgradcheck` functions, you are required to implement a function that calculates both first-order and second-order gradients of a given function, and then verify their correctness using `gradcheck` and `gradgradcheck`. # Problem Statement Implement a function in PyTorch that: 1. Defines a parametrized function `f(x)`. 2. Calculates the first-order and second-order derivatives of `f` using PyTorch\'s autograd. 3. Verifies the correctness of these derivatives using `gradcheck` and `gradgradcheck`. # Function Signature ```python import torch from torch.autograd import gradcheck, gradgradcheck def verify_gradients(): Defines a function f(x), computes its gradients, and verifies them using torch.autograd.gradcheck and torch.autograd.gradgradcheck. Returns: bool: True if all gradient checks pass, False otherwise. pass ``` # Expectation 1. **Function Definition**: Define a function `f(x)` where `x` is a tensor. 2. **Gradient Calculation**: Calculate the first-order gradient `df/dx` and the second-order gradient `d2f/dx2`. 3. **Gradient Verification**: - Use `gradcheck` to verify the first-order gradient. - Use `gradgradcheck` to verify the second-order gradient. # Example For a quadratic function `f(x) = x^3 + 2x^2 + 3x + 4`, you should: 1. Define this function in PyTorch. 2. Compute its first and second-order gradients. 3. Verify these gradients using `gradcheck` and `gradgradcheck`. # Constraints - The tensor `x` should be initialized with random values. - Use a small epsilon value for numerical gradient approximation (e.g., `eps=1e-6`) - Ensure that gradients are computed with respect to input `x`. # Notes - Pay attention to the type and device of tensors to avoid common pitfalls in gradient calculations. - Ensure that `requires_grad` is set to `True` for tensors involved in gradient computation to track operations correctly. # Evaluation Criteria - Correct implementation of the function. - Accurate computation of first-order and second-order gradients. - Successful verification of gradients using `gradcheck` and `gradgradcheck`. # Additional Information Refer to the PyTorch documentation for `torch.autograd.gradcheck` and `torch.autograd.gradgradcheck` for more details on using these functions: https://pytorch.org/docs/stable/autograd.html#torch.autograd.gradcheck","solution":"import torch from torch.autograd import gradcheck, gradgradcheck def verify_gradients(): Defines a function f(x), computes its gradients, and verifies them using torch.autograd.gradcheck and torch.autograd.gradgradcheck. Returns: bool: True if all gradient checks pass, False otherwise. # Define a function f(x) = x^3 + 2x^2 + 3x + 4 def f(x): return x**3 + 2*x**2 + 3*x + 4 # Initialize a tensor with requires_grad=True x = torch.randn(1, dtype=torch.double, requires_grad=True) # Perform gradcheck which checks the first-order gradients first_order_gradcheck = gradcheck(f, (x,), eps=1e-6, atol=1e-4) # Perform gradgradcheck which checks the second-order gradients second_order_gradcheck = gradgradcheck(f, (x,), eps=1e-6, atol=1e-4) # Return True if both checks pass, otherwise False return first_order_gradcheck and second_order_gradcheck"},{"question":"**Objective**: This assessment evaluates your understanding of TorchDynamo, specifically in tracing functions, handling dynamic shapes, and interpreting generated FX graphs. You will be required to implement a function, trace it using `torch.compile`, and confirm the implementation against different inputs, including symbolic shapes. # Task Description 1. **Function Implementation**: - Implement a function `custom_operation` that takes two tensors `a` and `b` as inputs. - The function should perform the following operations: - Compute the element-wise absolute difference between `a` and `b`. - Multiply the result by the symbolic value: `shape[0] + 2`. - Return the sum of the modified result. ```python import torch @torch.compile def custom_operation(a: torch.Tensor, b: torch.Tensor) -> torch.Tensor: # Your implementation here pass ``` 2. **Testing and Tracing**: - Create two test tensors `a` and `b` each of shape `(N, 100)` where `N` can be a dynamic value (use random values for tensor elements). - Execute the function `custom_operation` with the test tensors. - Extract and print the traced FX graph. - Change the dynamic dimension `N` of the tensors and re-execute the function to observe changes in the generated graph. 3. **Bonus**: - Implement a mechanism to detect if a graph break occurred during tracing and print relevant debug information. # Input and Output Format - **Input**: Function `custom_operation` inputs are two PyTorch tensors. - **Output**: - The function returns a modified tensor. - Print the extracted FX graph. - Print information about graph breaks if any. # Constraints - You should use PyTorch version 1.10.0 or higher. - Avoid using any non-PyTorch libraries for tensor manipulations. # Example ```python import torch @torch.compile def custom_operation(a: torch.Tensor, b: torch.Tensor) -> torch.Tensor: abs_diff = torch.abs(a - b) result = (a.shape[0] + 2) * abs_diff return result.sum() # Test with dynamic dimensions a = torch.randn(10, 100) b = torch.randn(10, 100) print(custom_operation(a, b)) # Extract and print the traced FX graph graph = custom_operation.graph print(graph) # Modify `N` and re-test a = torch.randn(20, 100) b = torch.randn(20, 100) print(custom_operation(a, b)) ``` Your goal is to ensure the function works as described, traces the operations accurately, and handles symbolic dimensions properly. The printed FX graph should reflect the operations and shape handling, and any graph break information should be printed if present.","solution":"import torch @torch.jit.script def custom_operation(a: torch.Tensor, b: torch.Tensor) -> torch.Tensor: abs_diff = torch.abs(a - b) result = (a.shape[0] + 2) * abs_diff return result.sum() # Function to trace and print FX graph def trace_and_print_graph(custom_operation, a, b): traced = torch.jit.trace(custom_operation, (a, b)) print(traced.graph) # Detect graph breaks def detect_graph_breaks(custom_operation, a, b): with torch.jit.fuser(\\"fuser1\\"): traced = torch.jit.trace(custom_operation, (a, b)) graph_str = str(traced.graph) if \\"prim::\\" in graph_str: print(\\"Graph break detected.\\") else: print(\\"No graph break detected.\\") print(graph_str)"},{"question":"Custom Color Palettes with Seaborn **Objective:** Demonstrate your understanding of the seaborn library and the `sns.blend_palette` function by creating and visualizing custom color palettes. **Task:** 1. Import the seaborn library and set the theme using `sns.set_theme()`. 2. Create three different color palettes using the `sns.blend_palette` function as follows: - A discrete palette blending two basic colors (e.g., blue and red). - A discrete palette using at least three colors in different formats (e.g., hex codes, color names). - A continuous colormap using at least three colors. 3. Use matplotlib to visualize the discrete palettes as bar plots and the continuous colormap as a heatmap. **Specifications:** - **Function Name:** `create_and_visualize_palettes()` - **Inputs:** None. - **Outputs:** None (the function will generate visual output directly). **Constraints:** - You must use the seaborn library for creating the palettes. - Use matplotlib for visualization. - Ensure plots have appropriate titles indicating the type of palette. **Example Output:** You should provide visualizations similar to the following: - A bar plot showing the discrete palette created from two basic colors. - A bar plot showing the discrete palette created from multiple color formats. - A heatmap displaying the continuous colormap. ```python import seaborn as sns import matplotlib.pyplot as plt def create_and_visualize_palettes(): # Step 1: Set the seaborn theme sns.set_theme() # Step 2: Create the color palettes palette1 = sns.blend_palette([\\"blue\\", \\"red\\"]) palette2 = sns.blend_palette([\\"#45a872\\", \\".8\\", \\"xkcd:golden\\"]) cmap = sns.blend_palette([\\"#bdc\\", \\"#7b9\\", \\"#47a\\"], as_cmap=True) # Step 3: Visualize the palettes # Discrete Palette 1 Plot plt.figure(figsize=(6, 1)) sns.palplot(palette1) plt.title(\'Discrete Palette: Blue to Red\') # Discrete Palette 2 Plot plt.figure(figsize=(6, 1)) sns.palplot(palette2) plt.title(\'Discrete Palette: Multiple Formats\') # Continuous Colormap Heatmap plt.figure(figsize=(6, 6)) data = [[0, 1], [1, 0]] sns.heatmap(data, cmap=cmap, annot=True, cbar=False) plt.title(\'Continuous Colormap\') plt.show() # Call the function to test the implementation create_and_visualize_palettes() ```","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_and_visualize_palettes(): # Step 1: Set the seaborn theme sns.set_theme() # Step 2: Create the color palettes palette1 = sns.blend_palette([\\"blue\\", \\"red\\"]) palette2 = sns.blend_palette([\\"#45a872\\", \\".8\\", \\"xkcd:golden\\"]) cmap = sns.blend_palette([\\"#bdc\\", \\"#7b9\\", \\"#47a\\"], as_cmap=True) # Step 3: Visualize the palettes # Discrete Palette 1 Plot plt.figure(figsize=(6, 1)) sns.palplot(palette1) plt.title(\'Discrete Palette: Blue to Red\') # Discrete Palette 2 Plot plt.figure(figsize=(6, 1)) sns.palplot(palette2) plt.title(\'Discrete Palette: Multiple Formats\') # Continuous Colormap Heatmap plt.figure(figsize=(6, 6)) data = [[0, 1], [1, 0]] sns.heatmap(data, cmap=cmap, annot=True, cbar=False) plt.title(\'Continuous Colormap\') plt.show()"},{"question":"<|Analysis Begin|> The provided documentation focuses on the usage of seaborn for visualizing regression fits and estimating relationships between variables through various functionalities such as `regplot`, `lmplot`, and other utilities. The following key points can be derived from the documentation: 1. **Simple Linear Regression**: Using `regplot` and `lmplot` to visualize a simple linear fit between two variables. 2. **Jitter and Estimator Options**: Adding jitter for discrete variables and using estimators to display central tendencies. 3. **Polynomial and Robust Regression**: Handling higher-order relationships and outliers through polynomial and robust regression. 4. **Logistic Regression**: Fitting logistic regression for binary outcomes. 5. **Non-parametric Regression**: Utilizing `lowess` smoother for non-parametric regression without assumptions. 6. **Residual Plot**: Using `residplot` for evaluating the fit of regression models. 7. **Conditional Relationships**: Displaying conditional relationships using `hue` and faceting. 8. **Complex Plotting Contexts**: Integrating regression plots within `jointplot` and `pairplot`. Given these functionalities, we can design a comprehensive question that requires students to implement and understand various regression models using seaborn, including both simple and advanced techniques. <|Analysis End|> <|Question Begin|> # Coding Assessment Question: Regression Visualization with Seaborn **Objective**: Demonstrate your understanding of seaborn\'s capabilities for visualizing regression fits, handling various kinds of regression models, and creating insightful visualizations. Task: You are provided with a dataset that contains multiple variables. Your task is to implement a series of functions to visualize regression fits among these variables using seaborn. Functions to Implement: 1. **Simple Linear Regression Plot**: ```python def simple_linear_regression_plot(data, x, y): Plots a simple linear regression between two variables. Args: - data (pd.DataFrame): The dataset containing the variables. - x (str): The name of the independent variable. - y (str): The name of the dependent variable. Returns: - None: Displays a seaborn regression plot with a confidence interval. ``` 2. **Polynomial Regression Plot**: ```python def polynomial_regression_plot(data, x, y, order): Plots a polynomial regression of a specified order between two variables. Args: - data (pd.DataFrame): The dataset containing the variables. - x (str): The name of the independent variable. - y (str): The name of the dependent variable. - order (int): The order of the polynomial regression. Returns: - None: Displays a seaborn polynomial regression plot without a confidence interval. ``` 3. **Logistic Regression Plot**: ```python def logistic_regression_plot(data, x, y): Plots a logistic regression fit when the dependent variable is binary. Args: - data (pd.DataFrame): The dataset containing the variables. - x (str): The name of the independent variable. - y (str): The name of the binary dependent variable. Returns: - None: Displays a seaborn logistic regression plot with jitter applied. ``` 4. **Conditional Regression Plot**: ```python def conditional_regression_plot(data, x, y, hue): Plots regression fits conditioned on a categorical variable. Args: - data (pd.DataFrame): The dataset containing the variables. - x (str): The name of the independent variable. - y (str): The name of the dependent variable. - hue (str): The name of the categorical variable to condition on. Returns: - None: Displays a seaborn regression plot conditioned on the `hue` variable. ``` # Dataset: You can use any dataset available in seaborn\'s built-in datasets (e.g., `tips`, `anscombe`, `iris`, etc.) for testing your functions. # Example Usage: ```python import seaborn as sns import pandas as pd import numpy as np # Load dataset data = sns.load_dataset(\\"tips\\") # Simple linear regression plot simple_linear_regression_plot(data, x=\\"total_bill\\", y=\\"tip\\") # Polynomial regression plot polynomial_regression_plot(data, x=\\"total_bill\\", y=\\"tip\\", order=2) # Logistic regression plot data[\\"big_tip\\"] = (data.tip / data.total_bill) > .15 logistic_regression_plot(data, x=\\"total_bill\\", y=\\"big_tip\\") # Conditional regression plot conditional_regression_plot(data, x=\\"total_bill\\", y=\\"tip\\", hue=\\"smoker\\") ``` # Requirements: - Ensure all necessary libraries (seaborn, pandas, matplotlib) are imported. - Properly handle and display plots. - Comment your code to explain the implementation steps. # Constraints: - Assume the dataset provided is always clean and properly formatted. - Focus on delivering clear and insightful visualizations. **Note**: Do not forget to reset any modified data (e.g., adding new columns) if reusing the dataset in multiple functions to avoid data leakage or unintended impacts.","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt def simple_linear_regression_plot(data, x, y): Plots a simple linear regression between two variables. Args: - data (pd.DataFrame): The dataset containing the variables. - x (str): The name of the independent variable. - y (str): The name of the dependent variable. Returns: - None: Displays a seaborn regression plot with a confidence interval. sns.regplot(x=x, y=y, data=data) plt.show() def polynomial_regression_plot(data, x, y, order): Plots a polynomial regression of a specified order between two variables. Args: - data (pd.DataFrame): The dataset containing the variables. - x (str): The name of the independent variable. - y (str): The name of the dependent variable. - order (int): The order of the polynomial regression. Returns: - None: Displays a seaborn polynomial regression plot without a confidence interval. sns.regplot(x=x, y=y, data=data, order=order, ci=None) plt.show() def logistic_regression_plot(data, x, y): Plots a logistic regression fit when the dependent variable is binary. Args: - data (pd.DataFrame): The dataset containing the variables. - x (str): The name of the independent variable. - y (str): The name of the binary dependent variable. Returns: - None: Displays a seaborn logistic regression plot with jitter applied. sns.regplot(x=x, y=y, data=data, logistic=True, y_jitter=0.03) plt.show() def conditional_regression_plot(data, x, y, hue): Plots regression fits conditioned on a categorical variable. Args: - data (pd.DataFrame): The dataset containing the variables. - x (str): The name of the independent variable. - y (str): The name of the dependent variable. - hue (str): The name of the categorical variable to condition on. Returns: - None: Displays a seaborn regression plot conditioned on the `hue` variable. sns.lmplot(x=x, y=y, hue=hue, data=data) plt.show()"},{"question":"# Seaborn Plot Customization and Analysis Task **Objective**: Demonstrate your understanding of seaborn by creating a multi-faceted plot with customized contexts, fonts, and line widths. Additionally, you will analyze and interpret the data presented in the plot. **Problem Statement**: You are given a dataset `data.csv` containing daily temperature and humidity readings in a city over one year. The dataset has the following columns: - `date`: The date of the reading (format: YYYY-MM-DD). - `temperature`: The temperature on that day in Celsius. - `humidity`: The humidity percentage on that day. Your task is to: 1. Read the dataset using pandas. 2. Create three subplots: - A line plot showing the temperature trend over the year. - A line plot showing the humidity trend over the year. - A scatter plot of temperature vs. humidity. Customize these plots using seaborn to have: - Different contexts (`paper`, `notebook`, `talk`) for each subplot. - Custom font scales and line widths. - A suitable title and labels for each plot. Finally, provide a brief analysis (in comments within the code) of any visible trends or patterns you observe in the data. **Input**: 1. `data.csv` (This file should be present in your working directory and contains the columns `date`, `temperature`, and `humidity`). **Output**: 1. A multi-faceted plot as described above. 2. Comments with your analysis of the trends or patterns. **Constraints**: - Use seaborn for all plotting tasks. - Each subplot should have a different context. - Ensure the plot is clear and well-labeled. ```python # Here is the structure for your implementation import pandas as pd import seaborn as sns import matplotlib.pyplot as plt # Load the dataset data = pd.read_csv(\\"data.csv\\") # Convert the \'date\' column to datetime format data[\'date\'] = pd.to_datetime(data[\'date\']) # Create subplots fig, axs = plt.subplots(3, 1, figsize=(10, 15)) fig.tight_layout(pad=5.0) # Plot 1: Temperature trend over the year sns.set_context(\\"paper\\", font_scale=1.2) sns.lineplot(ax=axs[0], x=\'date\', y=\'temperature\', data=data) axs[0].set_title(\\"Temperature Trend Over the Year\\") axs[0].set_xlabel(\\"Date\\") axs[0].set_ylabel(\\"Temperature (°C)\\") # Plot 2: Humidity trend over the year sns.set_context(\\"notebook\\", font_scale=1.5) sns.lineplot(ax=axs[1], x=\'date\', y=\'humidity\', data=data, linewidth=2) axs[1].set_title(\\"Humidity Trend Over the Year\\") axs[1].set_xlabel(\\"Date\\") axs[1].set_ylabel(\\"Humidity (%)\\") # Plot 3: Scatter plot of Temperature vs Humidity sns.set_context(\\"talk\\") sns.scatterplot(ax=axs[2], x=\'temperature\', y=\'humidity\', data=data) axs[2].set_title(\\"Temperature vs. Humidity\\") axs[2].set_xlabel(\\"Temperature (°C)\\") axs[2].set_ylabel(\\"Humidity (%)\\") plt.show() # Comments: # 1. Briefly analyze the temperature trends you observe. # 2. Briefly analyze the humidity trends you observe. # 3. Describe any correlation you observe between temperature and humidity. ```","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def load_and_preprocess_data(filename): data = pd.read_csv(filename) data[\'date\'] = pd.to_datetime(data[\'date\']) return data def create_plots(data): fig, axs = plt.subplots(3, 1, figsize=(10, 15)) fig.tight_layout(pad=5.0) # Plot 1: Temperature trend over the year sns.set_context(\\"paper\\", font_scale=1.2) sns.lineplot(ax=axs[0], x=\'date\', y=\'temperature\', data=data) axs[0].set_title(\\"Temperature Trend Over the Year\\") axs[0].set_xlabel(\\"Date\\") axs[0].set_ylabel(\\"Temperature (°C)\\") # Plot 2: Humidity trend over the year sns.set_context(\\"notebook\\", font_scale=1.5) sns.lineplot(ax=axs[1], x=\'date\', y=\'humidity\', data=data, linewidth=2) axs[1].set_title(\\"Humidity Trend Over the Year\\") axs[1].set_xlabel(\\"Date\\") axs[1].set_ylabel(\\"Humidity (%)\\") # Plot 3: Scatter plot of Temperature vs Humidity sns.set_context(\\"talk\\") sns.scatterplot(ax=axs[2], x=\'temperature\', y=\'humidity\', data=data) axs[2].set_title(\\"Temperature vs. Humidity\\") axs[2].set_xlabel(\\"Temperature (°C)\\") axs[2].set_ylabel(\\"Humidity (%)\\") plt.show() # Analysis: # 1. The temperature trend over the year shows seasonal variations with # higher temperatures during the summer months and lower temperatures # during the winter months. # 2. The humidity trend over the year shows fluctuations but does not have # as clear a seasonal pattern as temperature. # 3. The scatter plot of temperature vs. humidity indicates a weak inverse # correlation; on some days with higher temperatures, the humidity tends # to be lower, although this pattern is not universally consistent. # Example usage: # data = load_and_preprocess_data(\\"data.csv\\") # create_plots(data)"},{"question":"Implementing a Custom Sequence Type Objective: In this assessment, you are required to implement a custom sequence type in Python that behaves similarly to a Python list but with some additional functionalities. This will demonstrate your understanding of the Sequence Protocol, Mapping Protocol, and Iterator Protocol in Python. Question: 1. Implement a custom sequence class `CustomSequence` that supports the following methods: - `__init__(self, iterable)`: Initializes the sequence with elements from the provided iterable. - `__getitem__(self, index)`: Retrieves the item at the specified index. - `__setitem__(self, index, value)`: Sets the item at the specified index to the given value. - `__delitem__(self, index)`: Deletes the item at the specified index. - `__len__(self)`: Returns the length of the sequence. - `__iter__(self)`: Returns an iterator for iterating through the sequence. - `__reversed__(self)`: Returns an iterator that iterates through the sequence in reverse order. - `__contains__(self, item)`: Checks if the sequence contains the specified item. - `__str__(self)`: Returns a string representation of the sequence. 2. Additionally, implement a method `append(self, value)` that appends a value to the end of the sequence. 3. Ensure your class properly handles edge cases, such as accessing indices that are out of range. Example Usage: ```python seq = CustomSequence([1, 2, 3, 4]) print(seq) # Output: CustomSequence([1, 2, 3, 4]) seq.append(5) print(seq) # Output: CustomSequence([1, 2, 3, 4, 5]) print(seq[2]) # Output: 3 seq[2] = 10 print(seq) # Output: CustomSequence([1, 2, 10, 4, 5]) del seq[2] print(seq) # Output: CustomSequence([1, 2, 4, 5]) print(len(seq)) # Output: 4 for item in seq: print(item) # Output: # 1 # 2 # 4 # 5 print(3 in seq) # Output: False print(4 in seq) # Output: True for item in reversed(seq): print(item) # Output: # 5 # 4 # 2 # 1 ``` Constraints: - The class should mimic the behavior of a list as closely as possible. - Aim for optimal performance while accessing and modifying elements. - Handle edge cases appropriately (e.g., index out of range errors). Implement the `CustomSequence` class below: ```python class CustomSequence: def __init__(self, iterable): # Your implementation here def __getitem__(self, index): # Your implementation here def __setitem__(self, index, value): # Your implementation here def __delitem__(self, index): # Your implementation here def __len__(self): # Your implementation here def __iter__(self): # Your implementation here def __reversed__(self): # Your implementation here def __contains__(self, item): # Your implementation here def append(self, value): # Your implementation here def __str__(self): # Your implementation here ```","solution":"class CustomSequence: def __init__(self, iterable): self._data = list(iterable) def __getitem__(self, index): if index >= len(self._data) or index < -len(self._data): raise IndexError(\'list index out of range\') return self._data[index] def __setitem__(self, index, value): if index >= len(self._data) or index < -len(self._data): raise IndexError(\'list index out of range\') self._data[index] = value def __delitem__(self, index): if index >= len(self._data) or index < -len(self._data): raise IndexError(\'list index out of range\') del self._data[index] def __len__(self): return len(self._data) def __iter__(self): return iter(self._data) def __reversed__(self): return reversed(self._data) def __contains__(self, item): return item in self._data def append(self, value): self._data.append(value) def __str__(self): return f\\"CustomSequence({self._data})\\""},{"question":"You are tasked with creating a Python function using the `tkinter.colorchooser` module to facilitate color selection for a GUI application. # Problem Write a function called `select_color` that: - Opens a color chooser dialog. - Allows the user to pick a color. - Returns the chosen color in a hexadecimal format (e.g., `#rrggbb`). # Function Signature ```python def select_color() -> str: pass ``` # Input - There are no input parameters for this function. The function should open a modal dialog for the user to interact with. # Output - The function should return a string representing the chosen color in hexadecimal format (e.g., `#rrggbb`). - If the user cancels the operation or closes the dialog without choosing a color, the function should return the string `\\"None\\"`. # Constraints - You should use the `tkinter.colorchooser.askcolor` method to implement the color selection. - Ensure that the function handles user cancellation appropriately. # Example ```python # When you run the function, a color picker dialog should appear. color = select_color() print(color) # Output will be the color in hexadecimal format if a color is chosen, otherwise \\"None\\". ``` # Note - There is no need to write code to create an additional GUI window; focus exclusively on the color chooser functionality. - Ensure your environment includes `tkinter` for proper execution of the function.","solution":"import tkinter as tk from tkinter import colorchooser def select_color() -> str: Opens a color chooser dialog and returns the selected color in hexadecimal format. If the user cancels or closes the dialog, returns \\"None\\". # Initialize tkinter root, but do not create a full GUI root = tk.Tk() root.withdraw() # Hide the root window # Open the color chooser dialog color = colorchooser.askcolor()[1] # Close the root window root.destroy() # If the user cancels, color is None, otherwise it contains hex color return color if color is not None else \\"None\\""},{"question":"Overview In this assessment, you are required to create a Python script using the `argparse` module. The script should process various command-line arguments and options according to specific requirements. This task will test your understanding of command-line parsing, argument handling, and help message generation. Problem Statement You need to implement a Python script that processes a list of integers. The script should provide options to calculate the sum, product, or average of the integers, along with additional functionalities. # Requirements 1. Create an `ArgumentParser` object with a description. 2. Add a positional argument `integers` to capture a list of integers. 3. Add optional arguments to perform the following operations: - `--sum`: Calculate the sum of the integers. - `--product`: Calculate the product of the integers. - `--average`: Calculate the average of the integers. 4. Add a mutually exclusive group for the above operations to ensure only one operation can be performed at a time. 5. Add a `--file` option to read integers from a file instead of the command line. If this option is used, the integers should be read from the file and the file path should be provided as the argument. 6. The script should handle improper arguments gracefully and display appropriate help messages. # Constraints - The file specified with the `--file` option should contain integers, one per line. # Input and Output Input The input will be provided as command-line arguments. Examples: 1. Directly from the command line: ``` python script.py 1 2 3 4 --sum ``` 2. From an input file: ``` python script.py --file numbers.txt --average ``` Output For the above input examples, the output should be: 1. `10` 2. The average value of integers in `numbers.txt`. # Code Structure - Define an `ArgumentParser` to parse command-line arguments. - Define a function `process_integers` to handle different operations based on the parsed arguments. - Read integers from the file if `--file` option is used. - Perform the calculations based on specified options and print the result. # Example Code ```python import argparse import sys def process_integers(args): if args.file: with open(args.file, \'r\') as file: integers = [int(line.strip()) for line in file] else: integers = args.integers if args.sum: result = sum(integers) elif args.product: result = 1 for i in integers: result *= i elif args.average: result = sum(integers) / len(integers) else: result = None print(result) def main(): parser = argparse.ArgumentParser(description=\'Process a list of integers.\') parser.add_argument(\'integers\', metavar=\'N\', type=int, nargs=\'*\', help=\'an integer for processing\') parser.add_argument(\'--file\', type=str, help=\'file containing integers, one per line\') group = parser.add_mutually_exclusive_group(required=True) group.add_argument(\'--sum\', action=\'store_true\', help=\'calculate the sum of the integers\') group.add_argument(\'--product\', action=\'store_true\', help=\'calculate the product of the integers\') group.add_argument(\'--average\', action=\'store_true\', help=\'calculate the average of the integers\') if len(sys.argv) == 1: parser.print_help(sys.stderr) sys.exit(1) args = parser.parse_args() process_integers(args) if __name__ == \'__main__\': main() ``` # Submission Guidelines 1. Implement the script based on the requirements. 2. Ensure proper handling of arguments and errors. 3. Include comments in your code to explain the functionalities. Good luck!","solution":"import argparse import sys def process_integers(args): if args.file: with open(args.file, \'r\') as file: integers = [int(line.strip()) for line in file] else: integers = args.integers if args.sum: result = sum(integers) elif args.product: result = 1 for i in integers: result *= i elif args.average: result = sum(integers) / len(integers) else: result = None print(result) def main(): parser = argparse.ArgumentParser(description=\'Process a list of integers.\') parser.add_argument(\'integers\', metavar=\'N\', type=int, nargs=\'*\', help=\'an integer for processing\') parser.add_argument(\'--file\', type=str, help=\'file containing integers, one per line\') group = parser.add_mutually_exclusive_group(required=True) group.add_argument(\'--sum\', action=\'store_true\', help=\'calculate the sum of the integers\') group.add_argument(\'--product\', action=\'store_true\', help=\'calculate the product of the integers\') group.add_argument(\'--average\', action=\'store_true\', help=\'calculate the average of the integers\') if len(sys.argv) == 1: parser.print_help(sys.stderr) sys.exit(1) args = parser.parse_args() process_integers(args) if __name__ == \'__main__\': main()"},{"question":"**Problem Statement:** You are provided with a dataset of penguin measurements. Your task is to create a function that visualizes various aspects of this dataset using the seaborn library. Specifically, you will generate four different plots showcasing different utilizations of the `seaborn.ecdfplot` function. **Function Signature:** ```python def generate_penguin_plots(data): pass ``` **Input:** - `data`: A pandas DataFrame containing at least the following columns: - `bill_length_mm` - `flipper_length_mm` - `species` **Output:** - The function should output four subplots (in a single figure) showing the following: 1. ECDF of `flipper_length_mm` on the x-axis. 2. ECDF of `flipper_length_mm` on the y-axis. 3. ECDF of `bill_length_mm` with a hue mapping based on `species`, with counts on the y-axis. 4. Empirical complementary CDF of `bill_length_mm` with a hue mapping based on `species`. **Constraints:** - You must use the seaborn library for all plots. - The function should be well-structured and handle edge cases (e.g., missing columns). **Example Usage:** ```python import seaborn as sns penguins = sns.load_dataset(\\"penguins\\") generate_penguin_plots(penguins) ``` **Expected Output:** - A single figure containing four correctly labeled subplots as described. You are expected to use seaborn\'s `ecdfplot` function to complete this task.","solution":"import seaborn as sns import matplotlib.pyplot as plt def generate_penguin_plots(data): Generates four seaborn ECDF plots of penguin measurements. Parameters: data (DataFrame): A pandas DataFrame containing penguin measurements with columns \'bill_length_mm\', \'flipper_length_mm\', and \'species\'. Returns: A plot with four subplots visualizing the described Penguin data using seaborn\'s ecdfplot. # Check if necessary columns are present in the data required_columns = {\'bill_length_mm\', \'flipper_length_mm\', \'species\'} if not required_columns.issubset(data.columns): raise ValueError(f\\"Data must contain the following columns: {required_columns}\\") fig, axes = plt.subplots(2, 2, figsize=(15, 10)) # Plot 1: ECDF of flipper_length_mm on the x-axis sns.ecdfplot(data=data, x=\'flipper_length_mm\', ax=axes[0, 0]) axes[0, 0].set_title(\'ECDF of Flipper Length (X-axis)\') # Plot 2: ECDF of flipper_length_mm on the y-axis sns.ecdfplot(data=data, y=\'flipper_length_mm\', ax=axes[0, 1]) axes[0, 1].set_title(\'ECDF of Flipper Length (Y-axis)\') # Plot 3: ECDF of bill_length_mm with hue based on species sns.ecdfplot(data=data, x=\'bill_length_mm\', hue=\'species\', ax=axes[1, 0]) axes[1, 0].set_title(\'ECDF of Bill Length with Species Hue\') # Plot 4: Empirical complementary CDF of bill_length_mm with hue based on species sns.ecdfplot(data=data, x=\'bill_length_mm\', hue=\'species\', complementary=True, ax=axes[1, 1]) axes[1, 1].set_title(\'Empirical Complementary CDF of Bill Length with Species Hue\') fig.tight_layout() plt.show()"},{"question":"**Json Utilities Using Python** You are required to implement a utility class `JsonUtils` that provides helper methods for working with JSON data. Your implementation should include the following methods: 1. **`read_json_from_file(file_path: str) -> dict`**: - Reads JSON data from a file specified by `file_path` and returns it as a dictionary. - Raise an appropriate error if the file does not contain valid JSON. 2. **`write_json_to_file(data: dict, file_path: str) -> None`**: - Writes the given dictionary `data` to a file specified by `file_path` in JSON format. - Ensure that the data written is compliant with JSON standards. 3. **`validate_json_structure(data: dict, structure: dict) -> bool`**: - Validates that the JSON data `data` matches the expected structure `structure`. - The `structure` is a dictionary where the keys are the expected keys in `data` and the values are the expected types of the corresponding values in `data`. - Return `True` if the structure matches, `False` otherwise. 4. **`filter_json_by_keys(data: dict, keys: list) -> dict`**: - Returns a new dictionary containing only the key-value pairs from `data` that are specified in the `keys` list. **Input and Output Formats:** 1. **`read_json_from_file`** - **Input**: A string file path (e.g., `\\"/path/to/file.json\\"`) - **Output**: A dictionary containing the JSON data 2. **`write_json_to_file`** - **Input**: A dictionary of data and a string file path (e.g., `{\\"name\\": \\"John\\"}`, `\\"/path/to/file.json\\"`) - **Output**: None (the function writes to the file but returns nothing) 3. **`validate_json_structure`** - **Input**: Two dictionaries: `data` and `structure` (e.g., `{\\"name\\": \\"John\\"}`, `{\\"name\\": str}`) - **Output**: A boolean indicating whether the structure matches 4. **`filter_json_by_keys`** - **Input**: A dictionary and a list of keys (e.g., `{\\"name\\": \\"John\\", \\"age\\": 30}`, `[\\"name\\"]`) - **Output**: A dictionary filtered by the specified keys **Constraints:** - All file paths are assumed to be valid and accessible for reading/writing. - The `structure` dictionary in `validate_json_structure` method only specifies expected types that are basic Python types (e.g., `int`, `float`, `str`, `list`, `dict`). **Example Usage:** ```python json_utils = JsonUtils() # Read JSON from file data = json_utils.read_json_from_file(\\"data.json\\") print(data) # Write JSON to file json_utils.write_json_to_file({\\"name\\": \\"Jane\\"}, \\"output.json\\") # Validate JSON structure is_valid = json_utils.validate_json_structure({\\"name\\": \\"Jane\\"}, {\\"name\\": str}) print(is_valid) # Output: True # Filter JSON by keys filtered_data = json_utils.filter_json_by_keys({\\"name\\": \\"Jane\\", \\"age\\": 25}, [\\"name\\"]) print(filtered_data) # Output: {\\"name\\": \\"Jane\\"} ``` You must implement the `JsonUtils` class in Python to complete the assessment.","solution":"import json class JsonUtils: @staticmethod def read_json_from_file(file_path: str) -> dict: Reads JSON data from a file and returns it as a dictionary. try: with open(file_path, \'r\') as file: data = json.load(file) return data except json.JSONDecodeError: raise ValueError(\\"Invalid JSON in file\\") except FileNotFoundError: raise FileNotFoundError(f\\"File not found: {file_path}\\") @staticmethod def write_json_to_file(data: dict, file_path: str) -> None: Writes the given dictionary `data` to a file in JSON format. with open(file_path, \'w\') as file: json.dump(data, file, indent=4) @staticmethod def validate_json_structure(data: dict, structure: dict) -> bool: Validates that the JSON data `data` matches the expected structure `structure`. for key, expected_type in structure.items(): if key not in data or not isinstance(data[key], expected_type): return False return True @staticmethod def filter_json_by_keys(data: dict, keys: list) -> dict: Returns a new dictionary containing only the key-value pairs from `data` specified in the `keys` list. return {key: data[key] for key in keys if key in data}"},{"question":"# Outlier and Novelty Detection with scikit-learn Your task is to implement a function that uses multiple scikit-learn outlier detection methods to identify outliers in a given dataset. The function should fit each model to the training data, make predictions on test data, and return the results. Requirements: - You will use the following scikit-learn estimators: - `OneClassSVM` - `IsolationForest` - `EllipticEnvelope` - `LocalOutlierFactor` - **Training and Testing Data**: - The training data **X_train** is a 2D numpy array of shape `(n_samples_train, n_features)`. - The testing data **X_test** is a 2D numpy array of shape `(n_samples_test, n_features)`. - **Function Signature**: ```python def detect_outliers(X_train, X_test): Parameters: - X_train (numpy.ndarray): Training data of shape (n_samples_train, n_features) - X_test (numpy.ndarray): Testing data of shape (n_samples_test, n_features) Returns: - dict: A dictionary containing the predictions from each model with the following structure: { \\"OneClassSVM\\": array, \\"IsolationForest\\": array, \\"EllipticEnvelope\\": array, \\"LocalOutlierFactor\\": array } Each value is a numpy array of shape (n_samples_test, ) containing 1 for inliers and -1 for outliers. ``` - **Constraints**: - Do not use `predict` on the training data when `LocalOutlierFactor` is set for novelty detection. - Handle any exceptions that may arise, particularly with fitting and predicting. Example Usage: ```python import numpy as np # Example data X_train = np.array([[0, 0], [1, 1], [0, 1], [9, 8], [8, 9], [10, 10]]) X_test = np.array([[0, 0], [10, 10], [1, 0], [9, 9]]) results = detect_outliers(X_train, X_test) # Expected output: Dictionary with predictions from each model print(results) ``` Note: - For `LocalOutlierFactor`, ensure to use it appropriately for novelty detection. - Make sure to handle scaling for models that may require it (e.g., `EllipticEnvelope` assumes Gaussian distribution). - The `contamination` parameter can be set to `0.1` for methods that utilize it. Your implementation will be assessed based on correctness, clarity, and efficiency.","solution":"from sklearn.svm import OneClassSVM from sklearn.ensemble import IsolationForest from sklearn.covariance import EllipticEnvelope from sklearn.neighbors import LocalOutlierFactor import numpy as np def detect_outliers(X_train, X_test): Detect outliers using multiple scikit-learn estimators. Parameters: - X_train (numpy.ndarray): Training data of shape (n_samples_train, n_features) - X_test (numpy.ndarray): Testing data of shape (n_samples_test, n_features) Returns: - dict: A dictionary containing the predictions from each model. results = {} # OneClassSVM try: svm_model = OneClassSVM(gamma=\'auto\').fit(X_train) results[\'OneClassSVM\'] = svm_model.predict(X_test) except Exception as e: results[\'OneClassSVM\'] = str(e) # IsolationForest try: iso_forest_model = IsolationForest(contamination=0.1).fit(X_train) results[\'IsolationForest\'] = iso_forest_model.predict(X_test) except Exception as e: results[\'IsolationForest\'] = str(e) # EllipticEnvelope try: elliptic_env_model = EllipticEnvelope(contamination=0.1).fit(X_train) results[\'EllipticEnvelope\'] = elliptic_env_model.predict(X_test) except Exception as e: results[\'EllipticEnvelope\'] = str(e) # LocalOutlierFactor try: lof_model = LocalOutlierFactor(novelty=True, contamination=0.1).fit(X_train) results[\'LocalOutlierFactor\'] = lof_model.predict(X_test) except Exception as e: results[\'LocalOutlierFactor\'] = str(e) return results"},{"question":"You are provided with a binary classification dataset and a base classifier. Your task is to implement a custom scoring function that maximizes the recall rate for the positive class and use `TunedThresholdClassifierCV` to find the optimal decision threshold for this classifier. Requirements 1. **Input:** - A binary classification dataset `X` and labels `y`. - A base classifier instantiated but not fitted, e.g., `DecisionTreeClassifier()`. 2. **Output:** - The optimized decision threshold. - The recall score of the classifier on the provided dataset using the optimized threshold. 3. **Implementation should include:** - A custom scoring function that maximizes recall for the positive class. - Using `TunedThresholdClassifierCV` to tune the decision threshold. - Calculating and returning the recall score and the optimized threshold. Constraints: - Use a 5-fold stratified cross-validation for tuning the threshold. - Assume binary classification with labels `{0, 1}`, where `1` is the positive class. - Performance Requirement: The entire process (including model training and threshold tuning) should execute in under 1 minute on a dataset with 1000 samples and 20 features. Example ```python from sklearn.datasets import make_classification from sklearn.tree import DecisionTreeClassifier # Given binary classification dataset X, y = make_classification(n_samples=1000, n_features=20, random_state=0) # Given base classifier base_classifier = DecisionTreeClassifier(max_depth=2, random_state=0) # Your solution here optimized_threshold, recall_score = tune_decision_threshold(X, y, base_classifier) print(f\\"Optimized Threshold: {optimized_threshold}\\") print(f\\"Recall Score: {recall_score}\\") ``` # Complete the function `tune_decision_threshold` below: ```python from sklearn.model_selection import TunedThresholdClassifierCV from sklearn.metrics import make_scorer, recall_score from sklearn.base import BaseEstimator import numpy as np def tune_decision_threshold(X: np.ndarray, y: np.ndarray, classifier: BaseEstimator): Tunes the decision threshold for the given classifier to maximize recall. Parameters: X (np.ndarray): Feature matrix. y (np.ndarray): Target vector. classifier (BaseEstimator): An instantiated sklearn classifier. Returns: tuple: Optimized threshold and recall score. pos_label = 1 scorer = make_scorer(recall_score, pos_label=pos_label) model = TunedThresholdClassifierCV(classifier, scoring=scorer) # Fit the model to find the best threshold model.fit(X, y) # Retrieve the best threshold and recall score optimized_threshold = model.best_threshold_ recall_score_value = recall_score(y, model.predict(X)) return optimized_threshold, recall_score_value ``` # Notes: - Utilize the provided documentation to understand how to utilize `TunedThresholdClassifierCV` and `make_scorer`. - Ensure to import the necessary modules and handle edge cases where necessary.","solution":"from sklearn.tree import DecisionTreeClassifier from sklearn.metrics import make_scorer, recall_score from sklearn.base import BaseEstimator from sklearn.model_selection import StratifiedKFold from sklearn.preprocessing import label_binarize import numpy as np class TunedThresholdClassifierCV: def __init__(self, estimator, scoring, cv=5): self.estimator = estimator self.scoring = scoring self.cv = cv self.best_threshold_ = None def fit(self, X, y): skf = StratifiedKFold(n_splits=self.cv) thresholds = np.linspace(0, 1, 101) best_score = -1 for train_idx, val_idx in skf.split(X, y): X_train, X_val = X[train_idx], X[val_idx] y_train, y_val = y[train_idx], y[val_idx] self.estimator.fit(X_train, y_train) probas = self.estimator.predict_proba(X_val)[:, 1] for threshold in thresholds: preds = (probas >= threshold).astype(int) score = self.scoring(self.estimator, X_val, y_val, preds) if score > best_score: best_score = score self.best_threshold_ = threshold # Fit the estimator on the whole dataset self.estimator.fit(X, y) def predict(self, X): probas = self.estimator.predict_proba(X)[:, 1] return (probas >= self.best_threshold_).astype(int) def custom_recall_scorer(estimator, X, y, predictions): Custom recall scorer to work with threshold tuning return recall_score(y, predictions, pos_label=1) def tune_decision_threshold(X: np.ndarray, y: np.ndarray, classifier: BaseEstimator): Tunes the decision threshold for the given classifier to maximize recall. Parameters: X (np.ndarray): Feature matrix. y (np.ndarray): Target vector. classifier (BaseEstimator): An instantiated sklearn classifier. Returns: tuple: Optimized threshold and recall score. scorer = custom_recall_scorer model = TunedThresholdClassifierCV(classifier, scoring=scorer) # Fit the model to find the best threshold model.fit(X, y) # Retrieve the best threshold and recall score optimized_threshold = model.best_threshold_ recall_score_value = recall_score(y, model.predict(X), pos_label=1) return optimized_threshold, recall_score_value # Example usage if __name__ == \\"__main__\\": from sklearn.datasets import make_classification # Given binary classification dataset X, y = make_classification(n_samples=1000, n_features=20, random_state=0) # Given base classifier base_classifier = DecisionTreeClassifier(max_depth=2, random_state=0) # Your solution here optimized_threshold, recall_score_value = tune_decision_threshold(X, y, base_classifier) print(f\\"Optimized Threshold: {optimized_threshold}\\") print(f\\"Recall Score: {recall_score_value}\\")"},{"question":"**Objective:** Implement a function that performs Unicode string preparation according to a specific subset of tables from the `stringprep` module. This function will normalize the string to ensure it is suitable for internet protocols, focusing on handling space characters, control characters, and case-folding. # Function Specification `prepare_internet_string(input_string: str) -> str` 1. **Input:** - `input_string` (str): A Unicode string that needs to be prepared. 2. **Output:** - (str): A normalized Unicode string that has been processed according to certain preparation rules. 3. **Process:** - Remove characters that are in table C.1 (space characters) and C.2 (control characters). - Apply case-folding mapping from table B.2. - Return the final prepared string. 4. **Constraints:** - The input string may contain any Unicode characters. - The function should handle strings up to a length of 1000 characters efficiently. Example: ```python print(prepare_internet_string(\\"Hellou200BWorld!\\")) # Outputs: \\"helloworld!\\" print(prepare_internet_string(\\" Invalid Stringt\\")) # Outputs: \\"invalidstring\\" ``` # Implementation Guidelines 1. **Removing Space Characters:** Use `stringprep.in_table_c11_c12` to check and remove all characters that are classified as space characters (both ASCII and non-ASCII). 2. **Removing Control Characters:** Use `stringprep.in_table_c21_c22` to check and remove all characters classified as control characters (both ASCII and non-ASCII). 3. **Case-folding:** Use `stringprep.map_table_b2` to apply case-folding to each character in the string. 4. **Combining Steps:** Ensure the function iterates over the string and applies the above rules in an efficient manner. **Note:** This question requires students to demonstrate their understanding of Unicode string handling and leveraging the `stringprep` module functions effectively.","solution":"import stringprep def prepare_internet_string(input_string: str) -> str: Normalize the string to ensure it is suitable for internet protocols by handling space characters, control characters, and case-folding. prepared_string = [] for char in input_string: # Remove space characters (Table C.1) if stringprep.in_table_c11_c12(char): continue # Remove control characters (Table C.2) if stringprep.in_table_c21_c22(char): continue # Apply case-folding (Table B.2) prepared_string.append(stringprep.map_table_b2(char)) return \'\'.join(prepared_string)"},{"question":"Coding Assessment Question # Objective: Implement a simplified version of a file encoder and decoder using a custom hexadecimal-based encoding scheme inspired by the `binhex` module. # Task: You are to create a Python class `HexFileConverter` that provides methods to encode and decode files using a custom hexadecimal-based scheme. # Specifications: 1. **Class**: `HexFileConverter` 2. **Methods**: ```python def encode_file(self, input_path: str, output_path: str) -> None: Reads a binary file from `input_path`, encodes its content using a custom hexadecimal-based scheme, and writes the encoded data to `output_path`. :param input_path: Path to the input binary file. :param output_path: Path to the output encoded file. :raises RuntimeError: If encoding fails. ``` ```python def decode_file(self, input_path: str, output_path: str) -> None: Reads an encoded file from `input_path`, decodes its content from the custom hexadecimal format, and writes the binary data to `output_path`. :param input_path: Path to the input encoded file. :param output_path: Path to the output binary file. :raises RuntimeError: If decoding fails. ``` 3. **Custom Hexadecimal Encoding Scheme**: - Each byte of the binary file should be converted to its hexadecimal representation (2 characters). - The encoded file should store the hexadecimal string directly. 4. **Constraints**: - Input and output files can be assumed to fit into memory. - Special characters or non-ASCII data in input files must be properly handled. - Error handling should be implemented to raise a `RuntimeError` when encoding/decoding fails due to file read/write issues or invalid content. 5. **Performance**: - The solution should be efficient enough to handle moderate file sizes (e.g., up to a few megabytes). # Example: Given a binary file `example.bin` containing the byte sequence `b\'x01x02xABxCD\'`: - After encoding using `encode_file(\'example.bin\', \'encoded.hex\')`, `encoded.hex` should contain the string: `0102abcd`. - After decoding using `decode_file(\'encoded.hex\', \'decoded.bin\')`, `decoded.bin` should contain the original byte sequence `b\'x01x02xABxCD\'`. Write the complete class `HexFileConverter` with the specified methods. **Note:** You are not required to handle the full binhex4 format or Macintosh file specifics; the focus is on the hexadecimal encoding/decoding functionality.","solution":"class HexFileConverter: def encode_file(self, input_path: str, output_path: str) -> None: Reads a binary file from `input_path`, encodes its content using a custom hexadecimal-based scheme, and writes the encoded data to `output_path`. :param input_path: Path to the input binary file. :param output_path: Path to the output encoded file. :raises RuntimeError: If encoding fails. try: with open(input_path, \'rb\') as infile: binary_data = infile.read() hex_data = binary_data.hex() with open(output_path, \'w\') as outfile: outfile.write(hex_data) except Exception as e: raise RuntimeError(f\\"Encoding failed: {e}\\") def decode_file(self, input_path: str, output_path: str) -> None: Reads an encoded file from `input_path`, decodes its content from the custom hexadecimal format, and writes the binary data to `output_path`. :param input_path: Path to the input encoded file. :param output_path: Path to the output binary file. :raises RuntimeError: If decoding fails. try: with open(input_path, \'r\') as infile: hex_data = infile.read() binary_data = bytes.fromhex(hex_data) with open(output_path, \'wb\') as outfile: outfile.write(binary_data) except Exception as e: raise RuntimeError(f\\"Decoding failed: {e}\\")"},{"question":"# Objective This coding assessment question is designed to test the student\'s ability to use the `ast` module for analyzing and transforming Python code. # Question: You are provided with a Python function represented as a string. Your task is to write a function that uses the `ast` module to parse this string and transform its Abstract Syntax Tree (AST) by performing the following modifications: 1. Double all integer literals present in the function. 2. Convert all variable names to uppercase. The input will be a single function definition as a string, and the output should be the modified function as a string. # Function Signature ```python def transform_function_code(function_code: str) -> str: pass ``` # Input * `function_code` (str): A valid Python function code as a string. # Output * str: The transformed function as a string with modified integer literals and variable names. # Constraints - The input function code will always be syntactically correct. - The input function will not contain any nested functions or classes. - The modifications must preserve the original structure and behavior of the function, apart from the specified transformations. # Example ```python # Input: function_code = def example_function(x, y): result = x + y + 2 return result * 3 # Output: def EXAMPLE_FUNCTION(X, Y): RESULT = X + Y + 4 return RESULT * 6 ``` # Evaluation Criteria - Correctness: Ensure all integer literals are doubled and variable names are converted to uppercase. - Code readability and structure. - Use of the `ast` module to achieve the desired transformations. Good luck!","solution":"import ast import astor class Transformer(ast.NodeTransformer): def visit_Num(self, node): if isinstance(node.n, int): return ast.copy_location(ast.Num(n=node.n * 2), node) return node def visit_Name(self, node): if isinstance(node.ctx, ast.Store) or isinstance(node.ctx, ast.Load): return ast.copy_location(ast.Name(id=node.id.upper(), ctx=node.ctx), node) return node def visit_FunctionDef(self, node): node.name = node.name.upper() node.args.args = [ast.arg(arg=arg.arg.upper(), annotation=arg.annotation) for arg in node.args.args] self.generic_visit(node) return node def transform_function_code(function_code: str) -> str: tree = ast.parse(function_code) transformer = Transformer() transformed_tree = transformer.visit(tree) return astor.to_source(transformed_tree) # Example usage function_code = def example_function(x, y): result = x + y + 2 return result * 3 print(transform_function_code(function_code))"},{"question":"Overview Dimensionality reduction is a critical step in preprocessing high-dimensional data. In this assessment, you will implement a pipeline that reduces the dimensionality of a dataset using Principal Component Analysis (PCA) and then applies supervised learning to classify the data. Task Write a function `pca_classifier_pipeline(data, labels, n_components)` that fulfills the following requirements: 1. Reduces the dimensionality of the data to `n_components` using PCA. 2. Uses a standard supervised learning algorithm (e.g., Support Vector Machine) to classify the data based on the reduced components. 3. Returns the accuracy of the classification on the provided data. Input - `data`: A 2D numpy array of shape (n_samples, n_features), representing the dataset. - `labels`: A 1D numpy array of shape (n_samples,), representing the class labels for the data. - `n_components`: An integer specifying the number of principal components to retain. Output - A float representing the accuracy of the classifier on the dataset after dimensionality reduction. Constraints and Assumptions - You may use any relevant classes and functions from the scikit-learn library, including but not limited to `PCA`, `StandardScaler`, `SVM`, and `Pipeline`. - Assume that `data` and `labels` are preprocessed and do not contain missing values. - You must use PCA for dimensionality reduction and SVM for classification. - Ensure that the pipeline integrates both the PCA and the supervised learning steps seamlessly. Example ```python import numpy as np data = np.array([[0.5, 1.2, 3.1], [4.3, 1.9, 2.4], [0.9, 0.4, 1.5], [3.5, 2.1, 4.0]]) labels = np.array([0, 1, 0, 1]) n_components = 2 accuracy = pca_classifier_pipeline(data, labels, n_components) print(\\"Accuracy:\\", accuracy) ``` The function should implement the PCA and SVM steps, returning the accuracy of the classifier. Reference Documentation Refer to the scikit-learn documentation and examples for detailed guidance on implementing PCA, SVM, and pipelining.","solution":"import numpy as np from sklearn.decomposition import PCA from sklearn.preprocessing import StandardScaler from sklearn.svm import SVC from sklearn.pipeline import Pipeline from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score def pca_classifier_pipeline(data, labels, n_components): Reduces dimensionality of the data using PCA and classifies using SVM. Parameters: data (numpy.ndarray): A 2D array of shape (n_samples, n_features) representing the dataset. labels (numpy.ndarray): A 1D array of shape (n_samples,) representing the class labels. n_components (int): Number of principal components to retain. Returns: float: The accuracy of the classification. # Split the data into train and test sets X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42) # Define the PCA and SVM pipeline pipeline = Pipeline([ (\'scaler\', StandardScaler()), (\'pca\', PCA(n_components=n_components)), (\'svm\', SVC()) ]) # Fit the pipeline on the training data pipeline.fit(X_train, y_train) # Predict the labels for the test data y_pred = pipeline.predict(X_test) # Calculate and return the accuracy accuracy = accuracy_score(y_test, y_pred) return accuracy"},{"question":"**Question:** You are given a dataset of penguin measurements comprising variables like species, island, bill length, bill depth, flipper length, and body mass. Your task is to create a multi-plot grid visualizing the relationships between these variables conditioned on the species of penguins. # Requirements: 1. Use the `PairGrid` class to create a grid of scatter plots showing the relationships between the numerical variables. 2. Color-code the points by the species of penguins. 3. Customize the diagonal of the grid to display histograms. 4. Add a legend to the grid. 5. The numerical variables to be plotted are: `bill_length_mm`, `bill_depth_mm`, `flipper_length_mm`, and `body_mass_g`. 6. Set appropriate aspect ratios and figure sizes for clarity. # Input: The dataset can be loaded directly from seaborn\'s `load_dataset` method: ```python import seaborn as sns penguins = sns.load_dataset(\\"penguins\\") ``` # Output: The output should be a `PairGrid` plot where: - Each scatter plot represents the relationship between different numerical variables. - Histograms are displayed on the diagonal. - Different species are represented by different colors and indicated in the legend. - The entire plot is aesthetically pleasing and clear with appropriate aspect ratios and figure size. # Constraints: - Ensure all plotting functions adhere to the required signature and customization specifications mentioned. - Handle any potential data-related issues (e.g., missing values) gracefully. # Hint: Refer to the seaborn documentation on `PairGrid` and the example usage to implement this visualization effectively. ```python import seaborn as sns import matplotlib.pyplot as plt # Load dataset penguins = sns.load_dataset(\\"penguins\\") # Drop rows with missing values penguins = penguins.dropna() # Create PairGrid g = sns.PairGrid(penguins, hue=\\"species\\") # Map the plots g.map_diag(sns.histplot) g.map_offdiag(sns.scatterplot) # Add a legend g.add_legend() # Adjust the plot g.set_titles(\\"{col_name}\\") g.set(xticks=[], yticks=[]) # Show the plot plt.show() ``` Complete the code above to create the required visualization.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_penguins_pairgrid(): # Load dataset penguins = sns.load_dataset(\\"penguins\\") # Drop rows with missing values penguins = penguins.dropna() # Create PairGrid g = sns.PairGrid(penguins, hue=\\"species\\", aspect=1.4, height=2.5) # Map the plots g.map_diag(sns.histplot, kde=False) g.map_offdiag(sns.scatterplot) # Add a legend g.add_legend() # Show the plot plt.show()"},{"question":"# File System Explorer with the `stat` Module **Objective:** Write a Python function that recursively traverses a given directory, collects detailed information about each file, and returns a summary containing the count and types of files. Your function should use the `stat` module to interpret file properties. **Function Signature:** ```python def explore_directory(directory_path: str) -> dict: pass ``` **Input:** - `directory_path` (str): The path of the directory to be traversed. **Output:** - Returns a dictionary containing: - `\\"total_files\\"`: Total number of files encountered. - `\\"total_directories\\"`: Total number of directories encountered. - `\\"total_symlinks\\"`: Total number of symbolic links encountered. - `\\"total_others\\"`: Total number of other types of files encountered. - `\\"detailed_info\\"`: A list of dictionaries with detailed information about each file. Each dictionary should have: - `\\"path\\"`: The full path to the file. - `\\"type\\"`: The type of the file (one of \\"directory\\", \\"regular file\\", \\"symbolic link\\", \\"other\\"). - `\\"mode\\"`: The file mode as a string in the form \'-rwxrwxrwx\'. **Constraints:** - Use `os.lstat()` to avoid resolving symbolic links. - Use the `stat` module functions for file type checking. - Use `stat.filemode()` to convert file mode to a human-readable string. - Assume that you have read and write permissions for all files and directories in `directory_path`. **Example Usage:** ```python directory_summary = explore_directory(\\"/path/to/directory\\") print(directory_summary) # Example output: # { # \\"total_files\\": 10, # \\"total_directories\\": 5, # \\"total_symlinks\\": 2, # \\"total_others\\": 1, # \\"detailed_info\\": [ # {\\"path\\": \\"/path/to/directory/file1.txt\\", \\"type\\": \\"regular file\\", \\"mode\\": \\"-rw-r--r--\\"}, # {\\"path\\": \\"/path/to/directory/symlink\\", \\"type\\": \\"symbolic link\\", \\"mode\\": \\"lrwxrwxrwx\\"}, # ... # ] # } ``` **Hint:** Use recursion to traverse directories, the `os.lstat()` function to retrieve file status, and the various `stat` module functions to determine file types and properties.","solution":"import os import stat def explore_directory(directory_path: str) -> dict: summary = { \\"total_files\\": 0, \\"total_directories\\": 0, \\"total_symlinks\\": 0, \\"total_others\\": 0, \\"detailed_info\\": [] } def traverse(path): try: entries = os.listdir(path) except PermissionError: # Skip directories for which we don\'t have permission return for entry in entries: full_path = os.path.join(path, entry) try: info = os.lstat(full_path) except FileNotFoundError: # Skip files that may have been deleted continue file_info = { \\"path\\": full_path, \\"mode\\": stat.filemode(info.st_mode) } if stat.S_ISDIR(info.st_mode): file_info[\\"type\\"] = \\"directory\\" summary[\\"total_directories\\"] += 1 summary[\\"detailed_info\\"].append(file_info) traverse(full_path) elif stat.S_ISREG(info.st_mode): file_info[\\"type\\"] = \\"regular file\\" summary[\\"total_files\\"] += 1 summary[\\"detailed_info\\"].append(file_info) elif stat.S_ISLNK(info.st_mode): file_info[\\"type\\"] = \\"symbolic link\\" summary[\\"total_symlinks\\"] += 1 summary[\\"detailed_info\\"].append(file_info) else: file_info[\\"type\\"] = \\"other\\" summary[\\"total_others\\"] += 1 summary[\\"detailed_info\\"].append(file_info) traverse(directory_path) return summary"},{"question":"You are provided with information on coroutine objects and some of the C API functions related to them. Despite the provided format focusing on low-level details, you need to demonstrate your understanding of coroutines at the Python level. # Problem Statement: Write a Python code that demonstrates the use of coroutine functions using `async` and `await` keywords. Create a coroutine function that performs the following tasks: 1. **Task 1**: Fetch a list of URLs asynchronously and return their contents. Use dummy URLs for this demonstration since actual network access is not required. 2. **Task 2**: Count the number of words in each fetched content and return the counts. 3. Main function should orchestrate the tasks and print out the total word count from all URLs. # Specific Instructions: 1. Define a coroutine function `fetch_url_content(url)` that takes a URL as an argument and returns a dummy string representing the content. 2. Define a coroutine function `count_words_in_content(content)` that takes a string content, counts the words, and returns the word count. 3. Define an `async` main function `main(urls)`: * It should call `fetch_url_content(url)` for each URL in the given list. * It should then call `count_words_in_content(content)` for each fetched content. * Accumulate the word counts and print the total word count. # Constraints: * Use dummy URLs and dummy content. * Use the concepts of asynchronous programming in Python (i.e., `async` and `await`). * Ensure that your code handles multiple URLs efficiently using asynchronous calls. # Input: * A list of strings, where each string is a URL (e.g., `[\\"http://example.com/1\\", \\"http://example.com/2\\"]`). # Output: * Print the total count of words from all URLs. # Example: ```python import asyncio async def fetch_url_content(url): return \\"This is dummy content for URL: \\" + url async def count_words_in_content(content): return len(content.split()) async def main(urls): total_word_count = 0 for url in urls: content = await fetch_url_content(url) word_count = await count_words_in_content(content) total_word_count += word_count print(f\\"Total word count: {total_word_count}\\") # Example usage urls = [\\"http://example.com/1\\", \\"http://example.com/2\\"] asyncio.run(main(urls)) ``` # Explanation: * The `fetch_url_content` coroutine simulates fetching content from a URL asynchronously. * The `count_words_in_content` coroutine counts the number of words in the content string. * The `main` coroutine orchestrates the fetching and counting tasks, summing up the word counts and printing the total. Write your implementation based on the example provided.","solution":"import asyncio async def fetch_url_content(url): Simulate fetching content from a URL. return f\\"This is dummy content for URL: {url}\\" async def count_words_in_content(content): Count the number of words in the given content. return len(content.split()) async def main(urls): Orchestrate fetching of URLs and counting words in their content. total_word_count = 0 tasks = [fetch_url_content(url) for url in urls] contents = await asyncio.gather(*tasks) word_count_tasks = [count_words_in_content(content) for content in contents] word_counts = await asyncio.gather(*word_count_tasks) total_word_count = sum(word_counts) print(f\\"Total word count: {total_word_count}\\") # Example usage urls = [\\"http://example.com/1\\", \\"http://example.com/2\\"] asyncio.run(main(urls))"},{"question":"**Question: Ensuring Reproducibility in PyTorch** You have been provided with the task of developing a PyTorch script that ensures reproducible results across multiple runs of the script. Your script should utilize various methods to control sources of randomness and enforce deterministic behavior as described in the provided documentation. Write a script that performs the following tasks: 1. Set seeds to ensure reproducibility across PyTorch, Python, and NumPy. 2. Configure cuda convolution settings to disable benchmarking for determinism. 3. Enable deterministic algorithms in PyTorch. 4. Create a simple neural network and train it on random data for one epoch. 5. Verify that running the script multiple times with the same initial seeds gives identical results. **Script Requirements:** 1. **Setting the seeds:** - Use `torch.manual_seed()` to seed the PyTorch RNG. - Use `random.seed()` to seed Python\'s RNG. - Use `np.random.seed()` to seed NumPy\'s RNG. 2. **Configure cuda settings:** - Disable CUDA convolution benchmarking using `torch.backends.cudnn.benchmark = False`. 3. **Enable deterministic algorithms:** - Use `torch.use_deterministic_algorithms(True)` to enforce deterministic algorithms. 4. **Neural Network and Data:** - Define a simple neural network class. - Create a dataset of random tensors. - Use a DataLoader to load the dataset. - Train the neural network for one epoch. 5. **Reproducibility Check:** - Ensure that running the script multiple times produces the same model parameters and outputs. **Example Input/Output:** - The script should not take any input. - Printing the model parameters and the loss after training should suffice for verifying reproducibility. **Constraints:** - Use PyTorch version compatible with CUDA 10.2 or greater. - Ensure that the script runs on both CPU and GPU (if available). **Performance Requirements:** - Although deterministic operations may slow down training, ensure that the script completes running in a reasonable amount of time (e.g., within a few minutes). ```python # Your reproducible PyTorch script here ``` **Notes:** - Include comments in your code explaining each step to ensure clarity. - Assume basic familiarity with PyTorch, neural networks, and training loops.","solution":"import torch import torch.nn as nn import torch.optim as optim import numpy as np import random # 1. Set seeds to ensure reproducibility across PyTorch, Python, and NumPy. def set_seeds(seed=42): torch.manual_seed(seed) random.seed(seed) np.random.seed(seed) if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed) # 2. Configure CUDA settings def configure_cuda(): torch.backends.cudnn.benchmark = False torch.use_deterministic_algorithms(True) # 4. Define a simple neural network class SimpleNN(nn.Module): def __init__(self): super(SimpleNN, self).__init__() self.linear = nn.Linear(10, 1) def forward(self, x): return self.linear(x) # 4. Train the neural network for one epoch on random data def train(): set_seeds() configure_cuda() # Create random data x_data = torch.randn(100, 10) y_data = torch.randn(100, 1) # Create data loader dataset = torch.utils.data.TensorDataset(x_data, y_data) dataloader = torch.utils.data.DataLoader(dataset, batch_size=10, shuffle=True) # Initialize the neural network model = SimpleNN() criterion = nn.MSELoss() optimizer = optim.SGD(model.parameters(), lr=0.01) # Training loop for one epoch model.train() for inputs, targets in dataloader: optimizer.zero_grad() outputs = model(inputs) loss = criterion(outputs, targets) loss.backward() optimizer.step() return model.state_dict(), loss.item() if __name__ == \\"__main__\\": # To ensure reproducibility, we can check if two runs give the same results state_dict1, loss1 = train() state_dict2, loss2 = train() print(f\\"Loss from first run: {loss1}\\") print(f\\"Loss from second run: {loss2}\\") # Verify if the state dictionaries and losses are identical identical_parameters = all(torch.equal(state_dict1[key], state_dict2[key]) for key in state_dict1) identical_loss = loss1 == loss2 print(f\\"Identical parameters: {identical_parameters}\\") print(f\\"Identical loss: {identical_loss}\\")"},{"question":"You are required to implement a custom encoder and decoder for handling a specific type of Python object that does not natively serialize to JSON. Specifically, design a system to handle objects of a `Point` class and serialize them into a JSON format, and then deserialize them back into Python objects. The `Point` class represents a point in 3D space with `x`, `y`, and `z` coordinates. Your task is to: 1. Implement the `Point` class. 2. Implement a custom JSON encoder `PointEncoder` subclassing `json.JSONEncoder` to encode `Point` objects. 3. Implement a custom function `point_decoder` to use with `json.loads` to decode JSON back into `Point` objects. Specifications: 1. **Point Class:** - The class should be named `Point`. - It should have an `__init__` method to initialize `x`, `y`, and `z` coordinates. - It should have a `__repr__` method for string representation in the format `Point(x, y, z)`. 2. **PointEncoder:** - This class should inherit from `json.JSONEncoder`. - It should override the `default` method to serialize `Point` objects into JSON format `{\\"__Point__\\": true, \\"x\\": val_x, \\"y\\": val_y, \\"z\\": val_z}`. 3. **point_decoder:** - This function should take a dictionary as input and, if it indicates a `Point` object, return an instance of `Point`. Otherwise, return the dictionary unchanged. 4. **Functions:** - `encode_point(point: Point) -> str` that takes a `Point` object and returns its JSON string representation using `json.dumps` with `PointEncoder`. - `decode_point(json_str: str) -> Point` that takes a JSON string and returns a `Point` object using `json.loads` with `point_decoder`. Examples: ```python # Example Point class instantiation: p = Point(1, 2, 3) print(p) # Output: Point(1, 2, 3) # Encoding a Point object: json_str = encode_point(p) print(json_str) # Output: \'{\\"__Point__\\": true, \\"x\\": 1, \\"y\\": 2, \\"z\\": 3}\' # Decoding a JSON string: decoded_point = decode_point(json_str) print(decoded_point) # Output: Point(1, 2, 3) ``` Requirements: - You need to implement the following functions and classes: - `class Point` - `class PointEncoder` - `def point_decoder(dct: dict) -> Point` - `def encode_point(point: Point) -> str` - `def decode_point(json_str: str) -> Point` Constraints: - Ensure the implementation handles nested structures and maintain the expected performance. - The JSON string representation should be readable and maintain the data integrity. Good luck!","solution":"import json class Point: def __init__(self, x, y, z): self.x = x self.y = y self.z = z def __repr__(self): return f\\"Point({self.x}, {self.y}, {self.z})\\" class PointEncoder(json.JSONEncoder): def default(self, obj): if isinstance(obj, Point): return {\\"__Point__\\": True, \\"x\\": obj.x, \\"y\\": obj.y, \\"z\\": obj.z} return super().default(obj) def point_decoder(dct): if \\"__Point__\\" in dct: return Point(dct[\\"x\\"], dct[\\"y\\"], dct[\\"z\\"]) return dct def encode_point(point): return json.dumps(point, cls=PointEncoder) def decode_point(json_str): return json.loads(json_str, object_hook=point_decoder)"},{"question":"**Seaborn Plot Customization Challenge** # Objective: You have been provided with the documentation snippets of Seaborn\'s objects API. Using this knowledge, your task is to create a complex, multi-layered plot. The plot should provide insights into the `tips` dataset by displaying various relationships and different styling aspects covered in the documentation. # Instructions: 1. Load the `tips` dataset from Seaborn. 2. Create a base plot visualizing the relationship between `total_bill` and `tip`. 3. Add the following layers: - A scatter plot (`Dot`) to show the individual data points. - A regression line (`Line` with `PolyFit` transform) to show the trend line. - A histogram (`Bar` with `Hist` transform) showing the distribution of `total_bill` over days, separated by time (Lunch/Dinner). - Annotations in the legend explaining each layer. 4. Adjust the aesthetics of the plot to make it visually appealing: - Use different colors and transparency settings where appropriate. - Map the size of the scatter plot points to the `size` variable in the dataset. - Use facet plots to separate the data by the `day` of the week. # Requirements: 1. **Data Source**: Use the `tips` dataset provided by Seaborn. 2. **Input**: None required; the dataset loading and plot generation code should be self-contained. 3. **Output**: A visual plot showing the required layers and customizations. # Constraints: - Ensure all plot customizations and data mappings showcase different aspects of the Seaborn objects API features outlined in the documentation. - The plot should be meaningful and provide clear insights into the data relationships. # Hint: Use the `so.Plot` class to start building your plot and `add`, `facet`, `scale`, and `label` methods to superimpose and customize the necessary layers. # Performance: The plot should render efficiently without noticeable lag; consider optimizing the plot if rendering issues are observed. **Sample Code Structure**: ```python import seaborn.objects as so from seaborn import load_dataset # Load the dataset tips = load_dataset(\\"tips\\") # Create the base plot plot = so.Plot(tips, x=\\"total_bill\\", y=\\"tip\\") # Add Scatter Plot Layer plot.add(so.Dot(pointsize=\\"size\\"), color=\\"sex\\") # Add Regression Line Layer plot.add(so.Line(color=\\"red\\"), so.PolyFit(), label=\\"Trend Line\\") # Add Histogram (Bar) Layer and facets plot.facet(col=\\"day\\").add(so.Bar(), so.Hist(), color=\\"time\\") # Customize the aesthetics plot.label(y=\\"Tips Amount\\", x=\\"Total Bill\\") plot.scale(pointsize=(2, 10)) # Display the plot plot.show() ``` **Note**: Ensure you understand every aspect of the code and modify the sample structure to meet the requirements fully.","solution":"import seaborn.objects as so from seaborn import load_dataset def create_customized_plot(): # Load the dataset tips = load_dataset(\\"tips\\") # Create the base plot plot = so.Plot(tips, x=\\"total_bill\\", y=\\"tip\\") # Add Scatter Plot Layer plot.add(so.Dot(pointsize=\\"size\\"), color=\\"sex\\") # Add Regression Line Layer plot.add(so.Line(color=\\"red\\"), so.PolyFit(), label=\\"Trend Line\\") # Add Histogram (Bar) Layer and facet by day plot.facet(col=\\"day\\").add(so.Bar(), so.Hist(), color=\\"time\\") # Customize the aesthetics plot.label(y=\\"Tips Amount\\", x=\\"Total Bill\\") plot.scale(pointsize=(2, 10)) # Show the plot plot.show() # Example usage: # create_customized_plot()"},{"question":"Objective Demonstrate your understanding of seaborn by creating a customized plot with specific aesthetic mappings and scales. Problem Statement You are given a dataset of `cars` with the following columns: - `weight`: Weight of the car (in lbs) - `acceleration`: Time to accelerate from 0 to 60 mph (in seconds) - `origin`: Country of origin of the car (`usa`, `japan`, `europe`) - `mpg`: Miles per gallon (Fuel efficiency) Your task is to: 1. Create a scatter plot using seaborn objects (`so.Plot`) with `weight` on the x-axis and `acceleration` on the y-axis. 2. Map the `origin` column to the color of the points and the `mpg` column to their size. 3. Customize the color scale to use a nominal (categorical) palette with the following colors for the `origin`: - `usa`: Green - `japan`: Blue - `europe`: Red 4. Set the point sizes to range between 5 and 50. 5. Implement a square root transformation on the y-axis. Requirements - Write a function `create_custom_plot(dataframe: pd.DataFrame) -> se.Plot` where: - `dataframe`: A pandas DataFrame containing the columns `weight`, `acceleration`, `origin`, and `mpg`. - Returns: A seaborn plot object with the specified customizations. Constraints - Assume the `dataframe` contains no missing values. - You must use seaborn\'s `objects` API. Example ```python import seaborn.objects as so import pandas as pd def create_custom_plot(dataframe: pd.DataFrame) -> so.Plot: # Your implementation here pass # Example Usage # Loading an example dataset cars = pd.DataFrame({ \'weight\': [3500, 3200, 3700, 3000, 3100], \'acceleration\': [12, 13, 15, 14, 16], \'origin\': [\'usa\', \'japan\', \'europe\', \'usa\', \'japan\'], \'mpg\': [20, 25, 30, 22, 27] }) # Create the plot custom_plot = create_custom_plot(cars) custom_plot.show() ```","solution":"import pandas as pd import seaborn.objects as so import matplotlib.pyplot as plt def create_custom_plot(dataframe: pd.DataFrame) -> so.Plot: custom_palette = {\'usa\': \'green\', \'japan\': \'blue\', \'europe\': \'red\'} plot = (so.Plot(dataframe, x=\'weight\', y=\'acceleration\') .add(so.Dot(), color=\'origin\', size=\'mpg\') .scale(color=so.Nominal(custom_palette), size=(5,50)) .scale(y=\\"sqrt\\")) return plot"},{"question":"# Question: Synthetic Data Generation and Visualization with Scikit-Learn **Objective**: This assignment assesses your ability to use various synthetic data generation functions from the `sklearn.datasets` module to create and visualize datasets. You will also demonstrate your understanding by applying a clustering algorithm to one of the datasets. **Task**: 1. **Dataset Generation**: - Generate the following datasets using the functions from `sklearn.datasets`: 1. A dataset with three normally-distributed blobs using `make_blobs`. 2. A dataset for binary classification with two features and two clusters per class using `make_classification`. 3. A dataset with concentric circles using `make_circles`. 2. **Visualization**: - Create scatter plots for each of the generated datasets with appropriate titles. 3. **Clustering Application**: - Apply the KMeans clustering algorithm from `sklearn.cluster` to the dataset generated using `make_blobs`. - Visualize the clustering result using a scatter plot, highlighting the different clusters with distinct colors. **Input and Constraints**: - Use the following settings for the dataset generation: 1. `make_blobs`: centers=3, cluster_std=0.5, random_state=0 2. `make_classification`: n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=2, random_state=1 3. `make_circles`: noise=0.1, factor=0.3, random_state=0 **Output**: - Scatter plots for each dataset with appropriate labels and titles. - A scatter plot showing the clustering results after applying KMeans. **Your Code Solution Should Include**: - Importing necessary libraries. - Functions to generate and visualize each dataset. - Application of the KMeans algorithm and visualization of the results. ```python # Example structure of your solution import matplotlib.pyplot as plt from sklearn.datasets import make_blobs, make_classification, make_circles from sklearn.cluster import KMeans # Generate datasets def generate_datasets(): X_blobs, y_blobs = make_blobs(centers=3, cluster_std=0.5, random_state=0) X_classification, y_classification = make_classification(n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=2, random_state=1) X_circles, y_circles = make_circles(noise=0.1, factor=0.3, random_state=0) return (X_blobs, y_blobs), (X_classification, y_classification), (X_circles, y_circles) # Visualization function def plot_datasets(datasets): fig, axes = plt.subplots(1, 3, figsize=(15, 5)) titles = [\\"Three normally-distributed clusters\\", \\"Classification dataset\\", \\"Concentric circles\\"] for ax, data, title in zip(axes, datasets, titles): X, y = data ax.scatter(X[:, 0], X[:, 1], c=y) ax.set_title(title) plt.show() # KMeans clustering def apply_kmeans_and_plot(X, y): kmeans = KMeans(n_clusters=3, random_state=0) y_kmeans = kmeans.fit_predict(X) plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, cmap=\'viridis\') plt.title(\\"KMeans Clustering Result\\") plt.show() # Main function def main(): datasets = generate_datasets() plot_datasets(datasets) apply_kmeans_and_plot(datasets[0][0], datasets[0][1]) if __name__ == \\"__main__\\": main() ``` **Note**: Make sure your plots are properly labeled with titles and legends if necessary.","solution":"import matplotlib.pyplot as plt from sklearn.datasets import make_blobs, make_classification, make_circles from sklearn.cluster import KMeans def generate_datasets(): X_blobs, y_blobs = make_blobs(centers=3, cluster_std=0.5, random_state=0) X_classification, y_classification = make_classification(n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=2, random_state=1) X_circles, y_circles = make_circles(noise=0.1, factor=0.3, random_state=0) return (X_blobs, y_blobs), (X_classification, y_classification), (X_circles, y_circles) def plot_datasets(datasets): fig, axes = plt.subplots(1, 3, figsize=(15, 5)) titles = [\\"Three normally-distributed clusters\\", \\"Classification dataset\\", \\"Concentric circles\\"] for ax, data, title in zip(axes, datasets, titles): X, y = data ax.scatter(X[:, 0], X[:, 1], c=y) ax.set_title(title) plt.show() def apply_kmeans_and_plot(X, y): kmeans = KMeans(n_clusters=3, random_state=0) y_kmeans = kmeans.fit_predict(X) plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, cmap=\'viridis\') plt.title(\\"KMeans Clustering Result\\") plt.show() def main(): datasets = generate_datasets() plot_datasets(datasets) apply_kmeans_and_plot(datasets[0][0], datasets[0][1]) if __name__ == \\"__main__\\": main()"},{"question":"Conditional Neural Network with `torch.cond` Problem Statement You are required to implement a PyTorch neural network that uses the `torch.cond` function to adapt its behavior based on the properties of the input data. Specifically, the network should take a tensor as input and apply different sets of transformations depending on the sum of its elements. Write a class `ConditionalNet` that extends `torch.nn.Module` with the following requirements: 1. **Constructor (`__init__` method)**: Initialize two fully connected linear layers (`self.fc1` and `self.fc2`) and one ReLU activation function (`self.relu`). 2. **Forward method (`forward` method)**: - The method takes an input tensor `x` of shape `(batch_size, input_features)`. - If the sum of the elements of the input tensor `x` is greater than a specified threshold (e.g., 10), apply the following transformation sequence: - `self.fc1` -> `self.relu`. - Otherwise, apply the following transformation sequence: - `self.fc2` -> `self.relu`. - Use `torch.cond` to implement the conditional logic based on the sum of the elements of `x`. Input - A tensor `x` of shape `(batch_size, input_features)`. Output - A tensor that is the result of applying the appropriate transformation sequence to `x`. Constraints - You may assume that `input_features` is a positive integer, and `batch_size` can vary. - The threshold for switching between `true_fn` and `false_fn` is set to 10. Example Usage ```python import torch class ConditionalNet(torch.nn.Module): def __init__(self, input_features, output_features): super(ConditionalNet, self).__init__() self.fc1 = torch.nn.Linear(input_features, output_features) self.fc2 = torch.nn.Linear(input_features, output_features) self.relu = torch.nn.ReLU() def forward(self, x: torch.Tensor) -> torch.Tensor: def true_fn(x: torch.Tensor): return self.relu(self.fc1(x)) def false_fn(x: torch.Tensor): return self.relu(self.fc2(x)) return torch.cond(x.sum() > 10, true_fn, false_fn, (x,)) # Example input_features = 5 output_features = 3 net = ConditionalNet(input_features, output_features) x = torch.randn(4, input_features) output = net.forward(x) print(output) ``` Notes - Ensure the implementation correctly handles tensors of varying batch sizes. - You are not required to write training code, but focus on implementing the conditional operation.","solution":"import torch import torch.nn as nn class ConditionalNet(nn.Module): def __init__(self, input_features, output_features): super(ConditionalNet, self).__init__() self.fc1 = nn.Linear(input_features, output_features) self.fc2 = nn.Linear(input_features, output_features) self.relu = nn.ReLU() def forward(self, x): if x.sum() > 10: x = self.fc1(x) else: x = self.fc2(x) return self.relu(x)"},{"question":"You are tasked with implementing a Python function that mimics certain behaviors of the C APIs described in the provided documentation for formatted string conversion and comparisons. Your task will be divided into several parts to assess fundamental and advanced Python concepts. # Part A: Implement Formatted String Conversion Create a function `format_string_conversion(x: float, size: int) -> str` that mimics the behavior of `PyOS_snprintf`. Given a floating-point number `x` and a buffer `size`, return a formatted string of `x` truncated to fit within the given buffer size including a null-terminated character. The format should follow `%.2f` (i.e., 2 decimal places). If the buffer size is too small, raise a `ValueError` with an appropriate error message. **Constraints:** - The buffer `size` will be a positive integer. - The output should be null-terminated if it fits within the buffer. **Example:** ```python format_string_conversion(123.456, 10) # should return \'123.46\' format_string_conversion(123.456, 6) # should raise a ValueError indicating buffer too small ``` # Part B: Create a Function for Locale-Independent String-to-Float Conversion Write a function `locale_independent_str_to_float(s: str) -> float` that mimics the behavior of `PyOS_string_to_double`. Given a string `s` representing a floating-point number, convert it to a float. If the string is invalid, raise a `ValueError`. Ensure that leading and trailing spaces do not invalidate the string. **Constraints:** - The input string `s` should be a valid string representation of a floating-point number. - Leading and trailing spaces should not affect conversion. **Example:** ```python locale_independent_str_to_float(\\" 123.456 \\") # should return 123.456 locale_independent_str_to_float(\\"abc\\") # should raise a ValueError ``` # Part C: Case-Insensitive String Comparison Write two functions `case_insensitive_strcmp(s1: str, s2: str) -> int` and `case_insensitive_strncmp(s1: str, s2: str, n: int) -> int` that mimic `PyOS_stricmp` and `PyOS_strnicmp` respectively. The functions should return `0` if the strings are equivalent ignoring case, a negative number if the first string should come before the second in lexicographical order, and a positive number if the first string should come after the second. **Constraints:** - The input strings `s1` and `s2` will be non-null strings. - The input `n` for `strncicmp` will be a positive integer. **Example:** ```python case_insensitive_strcmp(\\"Hello\\", \\"hello\\") # should return 0 case_insensitive_strncmp(\\"Example\\", \\"examine\\", 4) # should return 0 ``` # Additional Notes - Do not use external libraries for string formatting or comparison, only built-in Python functionalities. - Ensure that your functions properly handle edge cases.","solution":"def format_string_conversion(x: float, size: int) -> str: Converts a floating-point number to a formatted string with two decimal places. Ensures that the result fits within the given buffer size (including null-termination). formatted_str = f\\"{x:.2f}\\" if len(formatted_str) + 1 <= size: return formatted_str else: raise ValueError(f\\"Buffer size {size} is too small for the formatted string\\") def locale_independent_str_to_float(s: str) -> float: Converts a string to a float in a locale-independent way. Leading and trailing spaces are ignored. s = s.strip() try: return float(s) except ValueError: raise ValueError(f\\"Invalid string for conversion to float: \'{s}\'\\") def case_insensitive_strcmp(s1: str, s2: str) -> int: Case-insensitive string comparison. Returns 0 if strings are equal, a negative number if s1 should appear before s2, and a positive number if s1 should appear after s2 in lexicographical order. s1_lower = s1.lower() s2_lower = s2.lower() if s1_lower < s2_lower: return -1 elif s1_lower > s2_lower: return 1 else: return 0 def case_insensitive_strncmp(s1: str, s2: str, n: int) -> int: Case-insensitive comparison of the first n characters of two strings. Returns 0 if the first n characters are equal, a negative number if s1 should appear before s2, and a positive number if s1 should appear after s2 in lexicographical order. s1_lower = s1[:n].lower() s2_lower = s2[:n].lower() if s1_lower < s2_lower: return -1 elif s1_lower > s2_lower: return 1 else: return 0"},{"question":"# Coding Assessment: Terminal-based Quiz Application **Objective**: Implement a terminal-based quiz application using the `curses` module that provides a user-friendly interface for navigating through questions, selecting answers, and receiving feedback. Requirements: 1. **Initial Setup**: - Initialize the curses application. - Set up the color pairs. - Create a main window for displaying the quiz interface. 2. **Quiz Interface**: - Display the question and a list of possible answers. - Navigate through the options using the UP and DOWN arrow keys. - Select an answer using the ENTER key. - Provide feedback (correct or incorrect) after each selection. - Track and display the score. 3. **Quiz Flow**: - Display one question at a time. - After the user submits an answer, show whether it was correct or incorrect. - Automatically proceed to the next question or end the quiz if there are no more questions. - Exit the application gracefully on user command (typically Ctrl-C or a special key). Input and Output Format: - **Input**: A list of questions, each containing the question text, a list of options, and the index of the correct option. ```python quiz_data = [ { \\"question\\": \\"What is the capital of France?\\", \\"options\\": [\\"Berlin\\", \\"Madrid\\", \\"Paris\\", \\"Rome\\"], \\"correct_option\\": 2 }, # More questions... ] ``` - **Output**: The terminal should display questions and options, allow selection of answers, provide feedback, and display the final score. Constraints: - The application should handle unexpected inputs gracefully. - Ensure the terminal settings are restored to their original state upon exiting the application. Performance Requirements: - The application should be responsive to user interactions. - Minimize screen flicker by efficiently using curses functions (`refresh()`, `noutrefresh()`, `doupdate()`, etc.). # Example Usage: 1. **Main Window and Question Display**: ``` -------------------------------------- | Question 1: What is the capital of France? | | | | -> 1. Berlin | | 2. Madrid | | 3. Paris | | 4. Rome | -------------------------------------- ``` 2. **Feedback and Score**: ``` -------------------------------------- | Your answer is Correct! | | Current score: 1/1 | | Press any key to continue... | -------------------------------------- ``` # Detailed Steps: 1. **Initialize Curses and Setup Colors**: - Initialize the curses library and set up the main window. - Define color pairs for highlighting selected options and providing feedback. 2. **Display Quiz Question**: - Render the current question and options. - Handle user navigation through the options using UP/DOWN keys. - Capture user selection using the ENTER key. 3. **Provide Feedback and Track Score**: - Check if the selected answer is correct. - Update the score based on the user\'s answer. - Display feedback and the current score. 4. **Proceed to the Next Question**: - After displaying feedback, proceed to the next question. - If all questions have been answered, display the final score and terminate the application gracefully. 5. **Handle Cleanup on Exit**: - Restore terminal settings even if the application exits unexpectedly (e.g., user presses Ctrl-C). # Implementation: Implement a Python function `terminal_quiz(quiz_data)` that takes the quiz data as input and runs the quiz in the terminal. ```python import curses from curses import textpad def terminal_quiz(quiz_data): def main(stdscr): curses.curs_set(0) curses.start_color() curses.init_pair(1, curses.COLOR_BLACK, curses.COLOR_WHITE) # Prepare the main window h, w = stdscr.getmaxyx() win = curses.newwin(h, w, 0, 0) current_question = 0 score = 0 while current_question < len(quiz_data): win.clear() q = quiz_data[current_question] win.addstr(2, 2, f\\"Question {current_question + 1}: {q[\'question\']}\\") selected_option = 0 options = q[\'options\'] num_options = len(options) while True: for i, option in enumerate(options): if i == selected_option: win.attron(curses.color_pair(1)) win.addstr(4 + i, 4, f\\"{i + 1}. {option}\\") win.attroff(curses.color_pair(1)) else: win.addstr(4 + i, 4, f\\"{i + 1}. {option}\\") key = win.getch() if key == curses.KEY_UP: selected_option = (selected_option - 1) % num_options elif key == curses.KEY_DOWN: selected_option = (selected_option + 1) % num_options elif key == curses.KEY_ENTER or key in [10, 13]: break # Check the answer if selected_option == q[\'correct_option\']: feedback = \\"Correct!\\" score += 1 else: feedback = f\\"Incorrect. The correct answer was: {options[q[\'correct_option\']]}\\" # Display feedback win.addstr(10, 2, feedback) win.addstr(12, 2, f\\"Current score: {score}/{current_question + 1}\\") win.addstr(14, 2, \\"Press any key to continue...\\") win.getch() current_question += 1 # End of quiz win.clear() win.addstr(h // 2, w // 2 - len(\\"Quiz finished!\\") // 2, \\"Quiz finished!\\") win.addstr(h // 2 + 2, w // 2 - len(f\\"Your final score is {score}/{len(quiz_data)}\\") // 2, f\\"Your final score is {score}/{len(quiz_data)}\\") win.addstr(h // 2 + 4, w // 2 - len(\\"Press any key to exit...\\") // 2, \\"Press any key to exit...\\") win.getch() curses.wrapper(main) # Example quiz data quiz_data = [ { \\"question\\": \\"What is the capital of France?\\", \\"options\\": [\\"Berlin\\", \\"Madrid\\", \\"Paris\\", \\"Rome\\"], \\"correct_option\\": 2 }, { \\"question\\": \\"What is the capital of Spain?\\", \\"options\\": [\\"Lisbon\\", \\"Madrid\\", \\"Barcelona\\", \\"Seville\\"], \\"correct_option\\": 1 } ] # Run the quiz terminal_quiz(quiz_data) ```","solution":"import curses from curses import textpad def terminal_quiz(quiz_data): def main(stdscr): curses.curs_set(0) curses.start_color() curses.init_pair(1, curses.COLOR_BLACK, curses.COLOR_WHITE) # Prepare the main window h, w = stdscr.getmaxyx() win = curses.newwin(h, w, 0, 0) current_question = 0 score = 0 while current_question < len(quiz_data): win.clear() q = quiz_data[current_question] win.addstr(2, 2, f\\"Question {current_question + 1}: {q[\'question\']}\\") selected_option = 0 options = q[\'options\'] num_options = len(options) while True: for i, option in enumerate(options): if i == selected_option: win.attron(curses.color_pair(1)) win.addstr(4 + i, 4, f\\"{i + 1}. {option}\\") win.attroff(curses.color_pair(1)) else: win.addstr(4 + i, 4, f\\"{i + 1}. {option}\\") key = win.getch() if key == curses.KEY_UP: selected_option = (selected_option - 1) % num_options elif key == curses.KEY_DOWN: selected_option = (selected_option + 1) % num_options elif key == curses.KEY_ENTER or key in [10, 13]: break # Check the answer if selected_option == q[\'correct_option\']: feedback = \\"Correct!\\" score += 1 else: feedback = f\\"Incorrect. The correct answer was: {options[q[\'correct_option\']]}\\" # Display feedback win.addstr(10, 2, feedback) win.addstr(12, 2, f\\"Current score: {score}/{current_question + 1}\\") win.addstr(14, 2, \\"Press any key to continue...\\") win.getch() current_question += 1 # End of quiz win.clear() win.addstr(h // 2, w // 2 - len(\\"Quiz finished!\\") // 2, \\"Quiz finished!\\") win.addstr(h // 2 + 2, w // 2 - len(f\\"Your final score is {score}/{len(quiz_data)}\\") // 2, f\\"Your final score is {score}/{len(quiz_data)}\\") win.addstr(h // 2 + 4, w // 2 - len(\\"Press any key to exit...\\") // 2, \\"Press any key to exit...\\") win.getch() curses.wrapper(main)"},{"question":"Objective: Implement a parallel matrix multiplication using NumPy and the `multiprocessing.shared_memory` module to demonstrate understanding of Python\'s shared memory capabilities. Problem Description: Write a function `parallel_matrix_multiplication(A: np.ndarray, B: np.ndarray) -> np.ndarray` which performs matrix multiplication on two given 2D NumPy arrays `A` and `B` using shared memory to achieve parallel computation. Constraints: 1. Both matrices `A` and `B` will be compatible for matrix multiplication. 2. Elements of `A` and `B` will be of type `float64`. 3. You should utilize a number of worker processes to perform the computation in parallel—decide the number based on the number of available CPU cores. Requirements: 1. Your implementation must use Python\'s `multiprocessing.shared_memory` for sharing the data across processes. 2. You should carefully manage the creation and cleanup of shared memory blocks to avoid leaks. 3. The function should return the resultant matrix as a NumPy array. Input: - Two 2D NumPy arrays `A` and `B` Output: - A 2D NumPy array representing the result of `A @ B` Performance: - Ensure the solution is optimized for parallel execution. Example: ```python import numpy as np A = np.array([[1.0, 2.0], [3.0, 4.0]]) B = np.array([[5.0, 6.0], [7.0, 8.0]]) result = parallel_matrix_multiplication(A, B) # Expected Output: # array([[19.0, 22.0], # [43.0, 50.0]]) ``` # Notes: 1. You may refer to the documentation provided for `multiprocessing.shared_memory` for details on how to use the shared memory features. 2. Make sure to handle synchronization where necessary to ensure correct matrix computations.","solution":"import numpy as np import multiprocessing as mp from multiprocessing import shared_memory import os def worker(a_shape, b_shape, shm_name_A, shm_name_B, shm_name_C, row_indices): existing_shm_A = shared_memory.SharedMemory(name=shm_name_A) existing_shm_B = shared_memory.SharedMemory(name=shm_name_B) existing_shm_C = shared_memory.SharedMemory(name=shm_name_C) A = np.ndarray(a_shape, dtype=np.float64, buffer=existing_shm_A.buf) B = np.ndarray(b_shape, dtype=np.float64, buffer=existing_shm_B.buf) C = np.ndarray((a_shape[0], b_shape[1]), dtype=np.float64, buffer=existing_shm_C.buf) for i in row_indices: C[i] = np.dot(A[i], B) existing_shm_A.close() existing_shm_B.close() existing_shm_C.close() def parallel_matrix_multiplication(A: np.ndarray, B: np.ndarray) -> np.ndarray: if A.shape[1] != B.shape[0]: raise ValueError(\\"Incompatible shapes for matrix multiplication\\") num_workers = os.cpu_count() chunk_size = int(np.ceil(A.shape[0] / num_workers)) shm_A = shared_memory.SharedMemory(create=True, size=A.nbytes) shm_B = shared_memory.SharedMemory(create=True, size=B.nbytes) shm_C = shared_memory.SharedMemory(create=True, size=A.shape[0] * B.shape[1] * 8) # float64 has 8 bytes shm_A_arr = np.ndarray(A.shape, dtype=np.float64, buffer=shm_A.buf) shm_B_arr = np.ndarray(B.shape, dtype=np.float64, buffer=shm_B.buf) shm_C_arr = np.ndarray((A.shape[0], B.shape[1]), dtype=np.float64, buffer=shm_C.buf) np.copyto(shm_A_arr, A) np.copyto(shm_B_arr, B) indices = [(i * chunk_size, min((i + 1) * chunk_size, A.shape[0])) for i in range(num_workers)] processes = [] for start, end in indices: p = mp.Process(target=worker, args=(A.shape, B.shape, shm_A.name, shm_B.name, shm_C.name, range(start, end))) processes.append(p) p.start() for p in processes: p.join() result = np.copy(shm_C_arr) shm_A.close() shm_A.unlink() shm_B.close() shm_B.unlink() shm_C.close() shm_C.unlink() return result"},{"question":"You are tasked with designing a function that imports a module in Python using the concepts mentioned in the documentation. Your function should be able to handle various scenarios and edge cases as outlined below. # Problem Statement Write a Python function `import_module_custom(name: str, globals_=None, locals_=None, fromlist=(), level=0) -> ModuleType` that imports a module by its name. This function should closely mimic the behavior of Python\'s built-in `__import__` function with a few additional customizations: 1. If the module is already imported, it should simply return the module object. 2. Handle re-importing a module if a specific flag `reload_module` is set to `True`. 3. Allow for importing submodules and ensuring that an incomplete import doesn\'t leave a module in an inconsistent state in the `sys.modules`. # Input - `name` (str): The name of the module to import. This can be a top-level module or a submodule specified as a string. - `globals_` (dict, optional): A dictionary representing the global namespace in which the module is imported. Defaults to `None`. - `locals_` (dict, optional): A dictionary representing the local namespace in which the module is imported. Defaults to `None`. - `fromlist` (sequence, optional): A list of submodule names to import. Defaults to an empty tuple. - `level` (int, optional): The level of import. 0 is absolute import, positive values represent relative import corresponding to the number of parent directories to search. Defaults to 0. - `reload_module` (bool): A flag specifying whether to reload the module if it is already imported. Defaults to `False`. # Output - Returns the module object that corresponds to the imported module. If an import fails, raise an appropriate Python exception. # Requirements 1. Use the `__import__` function from Python’s built-in functionality to import the module. 2. If `reload_module` is `True` and the module is already imported, use `importlib.reload` to reload the module. 3. Handle errors gracefully using try-except blocks and ensure that partially imported modules are not left in `sys.modules`. # Constraints - `name` should be a valid Python module name. - Preserve thread safety and avoid race conditions during module import. # Example Usage ```python import sys from types import ModuleType import importlib def import_module_custom(name: str, globals_=None, locals_=None, fromlist=(), level=0, reload_module=False) -> ModuleType: try: module = __import__(name, globals_, locals_, fromlist, level) if reload_module and name in sys.modules: module = importlib.reload(module) return module except ModuleNotFoundError: raise except Exception as e: if name in sys.modules: del sys.modules[name] raise ImportError(f\\"Failed to import module {name}: {str(e)}\\") # Example usage if __name__ == \\"__main__\\": print(\\"Starting Import\\") try: mod = import_module_custom(\\"math\\", reload_module=True) print(f\\"Module {mod.__name__} imported successfully\\") except ImportError as e: print(e) ``` In this example, the `import_module_custom` function tries to import the `math` module and reloads it if it\'s already imported, then returns the module object. Handle any errors that might occur during the import and ensure the module is imported cleanly.","solution":"import sys from types import ModuleType import importlib def import_module_custom(name: str, globals_=None, locals_=None, fromlist=(), level=0, reload_module=False) -> ModuleType: Custom module importer that allows for reloading and handles partial imports. Parameters: name (str): The name of the module to import. globals_ (dict, optional): Global namespace into which the module is imported. Defaults to None. locals_ (dict, optional): Local namespace into which the module is imported. Defaults to None. fromlist (sequence, optional): Submodules to load from the module. Defaults to an empty tuple. level (int, optional): Import level. 0 represents absolute import. Defaults to 0. reload_module (bool, optional): Flag indicating whether to reload the module if it is already imported. Defaults to False. Returns: ModuleType: The imported module object. Raises: ImportError: If the module cannot be imported. try: module = __import__(name, globals_, locals_, fromlist, level) if reload_module and name in sys.modules: module = importlib.reload(module) return module except ModuleNotFoundError: raise except Exception as e: if name in sys.modules: del sys.modules[name] raise ImportError(f\\"Failed to import module {name}: {str(e)}\\")"},{"question":"Objective: To demonstrate your understanding of the `pwd` module and its functionalities in Python by implementing a function that processes and returns specific information from the Unix user account and password database. Question: Write a Python function that utilizes the `pwd` module to find all users whose home directory path starts with a given prefix. Your function should accept a string input, which represents the prefix of the home directory paths to look for, and return a list of dictionaries. Each dictionary should contain the following information for each matching user: - `login_name` - `user_id` - `group_id` - `full_name` - `home_directory` Function Signature: ```python def find_users_by_home_prefix(prefix: str) -> list: pass ``` Input: - `prefix` (str): The prefix of the home directory paths to search for. Output: - `List[Dict[str, any]]`: A list of dictionaries. Each dictionary contains: - `login_name` (str): Login name (pw_name) - `user_id` (int): Numerical user ID (pw_uid) - `group_id` (int): Numerical group ID (pw_gid) - `full_name` (str): Full name or comment field (pw_gecos) - `home_directory` (str): User home directory (pw_dir) Constraints: - The length of `prefix` will be between 1 and 100 characters. - Assume the `pwd` module is available and operational in the environment. Example: ```python # Suppose there are users with the following home directories in the database: # /home/user1 # /home/user2 # /usr/local/admin # /usr/local/root find_users_by_home_prefix(\\"/home\\") # Output: # [ # { # \\"login_name\\": \\"user1\\", # \\"user_id\\": 1001, # \\"group_id\\": 1001, # \\"full_name\\": \\"User One\\", # \\"home_directory\\": \\"/home/user1\\" # }, # { # \\"login_name\\": \\"user2\\", # \\"user_id\\": 1002, # \\"group_id\\": 1002, # \\"full_name\\": \\"User Two\\", # \\"home_directory\\": \\"/home/user2\\" # } # ] ``` Guidelines: 1. Use the `pwd.getpwall()` function to retrieve the list of all available password database entries. 2. Filter the entries based on their home directory prefix. 3. Construct the list of dictionaries containing the required user information for the matching entries. 4. Ensure your function handles cases where no matching users are found gracefully by returning an empty list.","solution":"import pwd def find_users_by_home_prefix(prefix: str) -> list: Finds all users whose home directory path starts with the given prefix. Args: prefix (str): The prefix of the home directory paths to search for. Returns: List[Dict[str, any]]: A list of dictionaries for each matching user. # Retrieve the list of all password database entries all_users = pwd.getpwall() # Filter users whose home directory starts with the given prefix matching_users = [ { \\"login_name\\": user.pw_name, \\"user_id\\": user.pw_uid, \\"group_id\\": user.pw_gid, \\"full_name\\": user.pw_gecos, \\"home_directory\\": user.pw_dir } for user in all_users if user.pw_dir.startswith(prefix) ] return matching_users"},{"question":"# Task # Objective Your task is to implement a function `get_user_groups(username: str) -> List[str]` that retrieves the list of groups a specified user belongs to on a Unix system. # Requirements 1. The implementation should use the `pwd` and `grp` modules: - `pwd` to get information about a specific user. - `grp` to retrieve group details. 2. Your function should take a single input: - `username` (a string): The username for which to fetch group memberships. 3. The output should be a list of strings, each representing a group name that the user is a member of. # Input Format - `username` (str): A valid Unix username. # Output Format - List of strings: The list of group names the user belongs to. # Example ```python def get_user_groups(username: str) -> List[str]: pass # Example usage: print(get_user_groups(\'someuser\')) # Output might look something like [\'staff\', \'sudo\', \'users\'] ``` # Constraints 1. The implementation should handle Unix-specific behavior and make proper use of relevant modules. 2. Consider edge cases where the username might not exist. 3. The solution should be efficient in terms of accessing and processing the group and user information. # Notes - Assume the code will be executed in a Unix-based environment. - Do not use external libraries apart from the standard `pwd` and `grp` modules. # Performance - Aim to minimize the number of calls to system libraries by efficiently accessing and filtering the necessary data. # Additional Information Refer to the Python documentation for the `pwd` and `grp` modules for any additional methods you might need.","solution":"import pwd import grp def get_user_groups(username: str): Retrieves the list of groups a specified user belongs to on a Unix system. :param username: A valid Unix username. :return: List of group names the user belongs to. try: user_info = pwd.getpwnam(username) except KeyError: return [] user_groups = [] for group in grp.getgrall(): if user_info.pw_gid == group.gr_gid or username in group.gr_mem: user_groups.append(group.gr_name) return user_groups"},{"question":"**Title: File System Operations with `pathlib`** **Objective:** Write a Python function using the `pathlib` module that performs various file system operations. This task will assess your understanding of object-oriented filesystem paths and your ability to manipulate and query them efficiently. **Description:** You are required to implement a function `organize_directory` in Python which takes a single argument, `directory_path` (a string representing the path to a directory). This function should perform the following operations: 1. **List all files and directories**: Print the names of all files and subdirectories in the given directory. 2. **Create a subdirectory**: Create a new subdirectory called \\"Processed\\" within the given directory. 3. **Move `.txt` files**: Move all `.txt` files from the given directory to the \\"Processed\\" subdirectory. Print the names of the files being moved. 4. **Delete empty files**: Identify and delete any empty files in the given directory and in the \\"Processed\\" subdirectory. Print the names of the files being deleted. **Function Signature:** ```python def organize_directory(directory_path: str) -> None: pass ``` **Input:** - `directory_path` (str): A string representing the path to a directory. **Output:** - The function should print the names of the files and directories as specified in the operations. **Constraints:** - Assume the input directory path is valid. - Assume you have necessary permissions for file operations in the given directory. - The function should handle large numbers of files efficiently. **Example:** ```python # Assuming the directory structure is as follows: # test_dir/ # ├── file1.txt # ├── file2.txt # ├── script.py # ├── empty_file.txt # └── sub_dir/ # └── file3.txt organize_directory(\'test_dir\') # Expected output: # Listing all files and directories in test_dir: # file1.txt # file2.txt # script.py # empty_file.txt # sub_dir/ # # Creating \'Processed\' subdirectory in test_dir # Moving file1.txt to Processed # Moving file2.txt to Processed # Deleting empty_file.txt # The resulting directory structure should be: # test_dir/ # ├── Processed/ # │ ├── file1.txt # │ ├── file2.txt # ├── script.py # └── sub_dir/ # └── file3.txt ``` **Note:** - Use the `pathlib` module to handle filesystem operations. - Ensure the function is robust and handles any unexpected cases gracefully.","solution":"from pathlib import Path def organize_directory(directory_path: str) -> None: Organize the directory as per the specified operations. Args: - directory_path: A string representing the path to a directory. dir_path = Path(directory_path) # List all files and directories print(\\"Listing all files and directories in\\", directory_path) for item in dir_path.iterdir(): print(item.name) # Create \\"Processed\\" subdirectory processed_dir = dir_path / \\"Processed\\" if not processed_dir.exists(): print(\\"Creating \'Processed\' subdirectory in\\", directory_path) processed_dir.mkdir() # Move .txt files to \\"Processed\\" subdirectory for item in dir_path.iterdir(): if item.is_file() and item.suffix == \'.txt\': print(\\"Moving\\", item.name, \\"to Processed\\") item.rename(processed_dir / item.name) # Delete empty files in the given directory and \\"Processed\\" subdirectory for path in [dir_path, processed_dir]: for item in path.iterdir(): if item.is_file() and item.stat().st_size == 0: print(\\"Deleting\\", item.name) item.unlink()"},{"question":"# Python Object Interaction: Iterator and Sequence Implementation Problem Statement You are required to implement a custom sequence object that behaves like a Python list but with additional iterator protocol support. Your implementation should demonstrate a deep understanding of the sequence and iterator protocols. Requirements 1. **CustomSequence Class**: - Implement a class `CustomSequence` that mimics the behavior of a Python list. - The class should support common list operations (e.g., indexing, appending elements). 2. **Iterator Protocol**: - Implement the iterator protocol in your `CustomSequence` class such that you can iterate over its elements. Implementation Details 1. **Constructor**: - The constructor should initialize the sequence with an optional iterable. If no iterable is provided, it initializes an empty sequence. ```python def __init__(self, iterable=None): # Your code here ``` 2. **Indexing**: - Implement indexing to get and set items in the sequence. - Raise `IndexError` for invalid indices. ```python def __getitem__(self, index): # Your code here def __setitem__(self, index, value): # Your code here ``` 3. **Append Method**: - Implement an `append` method to add new elements to the sequence. ```python def append(self, value): # Your code here ``` 4. **Iterator Protocol**: - Implement the `__iter__` and `__next__` methods to enable iteration over the sequence. ```python def __iter__(self): # Your code here def __next__(self): # Your code here ``` Example Usage ```python seq = CustomSequence([1, 2, 3]) print(seq[1]) # Output: 2 seq[1] = 10 print(seq[1]) # Output: 10 seq.append(4) print(seq[3]) # Output: 4 for elem in seq: print(elem) # Output: 1 10 3 4 ``` Constraints - Do not use Python\'s built-in list class for storage. You must manage the sequence using lower-level constructs. - Ensure that your implementation efficiently handles typical operations (access, append). - You may assume the elements are integers for simplicity. Performance Requirements - Indexing should be O(1). - Appending should be amortized O(1). - Iteration should be O(n), where n is the number of elements in the sequence.","solution":"class CustomSequence: def __init__(self, iterable=None): self._data = iterable if iterable is not None else [] self._index = 0 def __getitem__(self, index): if isinstance(index, int) and 0 <= index < len(self._data): return self._data[index] else: raise IndexError(\'Index out of range\') def __setitem__(self, index, value): if isinstance(index, int) and 0 <= index < len(self._data): self._data[index] = value else: raise IndexError(\'Index out of range\') def append(self, value): self._data.append(value) def __iter__(self): self._index = 0 return self def __next__(self): if self._index < len(self._data): result = self._data[self._index] self._index += 1 return result else: raise StopIteration"},{"question":"# Custom Neural Network Parameter Initialization In this task, you are required to implement a custom neural network in PyTorch and initialize its parameters using specific initialization methods from the `torch.nn.init` module. Requirements 1. **Network Architecture**: - Implement a neural network with one input layer, one hidden layer, and one output layer. - The input layer should have 10 neurons. - The hidden layer should have 20 neurons. - The output layer should have 1 neuron. 2. **Parameter Initialization**: - Initialize the weights of the input layer using `xavier_uniform_`. - Initialize the weights of the hidden layer using `kaiming_normal_`. - Initialize the bias terms of both layers with zeros. Input and Output Formats - **Input**: There are no direct inputs to the function you need to implement, but you should define a class `CustomNet` that constructs and initializes the network as described. - **Output**: An instance of the `CustomNet` class when created will have its parameters initialized appropriately. To verify correctness, you should include a method `print_initial_weights` that prints the initial weights and biases. Constraints and Limitations - You should use only the functions provided in the `torch.nn.init` module for parameter initialization. - Do not use any additional external libraries or packages. Performance Requirements - Ensure that the initialization is efficient and makes use of the proper PyTorch functions. # Your Task Implement the `CustomNet` class with the described architecture and initialization methods. Include a method `print_initial_weights` to print the initialized weights and biases for verification. Example: ```python class CustomNet(nn.Module): def __init__(self): super(CustomNet, self).__init__() # Define layers self.input_layer = nn.Linear(10, 20) self.hidden_layer = nn.Linear(20, 1) # Initialize weights nn.init.xavier_uniform_(self.input_layer.weight) nn.init.kaiming_normal_(self.hidden_layer.weight) # Initialize biases nn.init.zeros_(self.input_layer.bias) nn.init.zeros_(self.hidden_layer.bias) def forward(self, x): x = self.input_layer(x) x = torch.relu(x) x = self.hidden_layer(x) return x def print_initial_weights(self): print(f\\"Input Layer Weights: {self.input_layer.weight}\\") print(f\\"Hidden Layer Weights: {self.hidden_layer.weight}\\") print(f\\"Input Layer Biases: {self.input_layer.bias}\\") print(f\\"Hidden Layer Biases: {self.hidden_layer.bias}\\") # Example usage: model = CustomNet() model.print_initial_weights() ``` Ensure that the weights and biases are initialized correctly before proceeding to train the network.","solution":"import torch import torch.nn as nn class CustomNet(nn.Module): def __init__(self): super(CustomNet, self).__init__() # Define layers self.input_layer = nn.Linear(10, 20) self.hidden_layer = nn.Linear(20, 1) # Initialize weights nn.init.xavier_uniform_(self.input_layer.weight) nn.init.kaiming_normal_(self.hidden_layer.weight) # Initialize biases nn.init.zeros_(self.input_layer.bias) nn.init.zeros_(self.hidden_layer.bias) def forward(self, x): x = self.input_layer(x) x = torch.relu(x) x = self.hidden_layer(x) return x def print_initial_weights(self): print(f\\"Input Layer Weights: {self.input_layer.weight}\\") print(f\\"Hidden Layer Weights: {self.hidden_layer.weight}\\") print(f\\"Input Layer Biases: {self.input_layer.bias}\\") print(f\\"Hidden Layer Biases: {self.hidden_layer.bias}\\") # Example usage: model = CustomNet() model.print_initial_weights()"},{"question":"**Custom Command-Line Task Management System** # Problem Statement You are to implement a custom command-line-based task management system using the `cmd` module in Python. This system should allow users to manage a list of tasks by adding, viewing, deleting, and marking tasks as complete. Additionally, implement a record and playback functionality to record commands and replay them. # Requirements 1. **Commands to Implement:** - `add <task>`: Add a new task to the list. - `delete <task_number>`: Delete a task by its number. - `view`: View all tasks with their current status (completed or not). - `complete <task_number>`: Mark a task as completed. - `record <filename>`: Start recording the commands issued by the user. - `playback <filename>`: Replay the commands from the specified file. - `exit`: Terminate the command-loop and exit the program. 2. **Command Syntax and Behavior:** - Implement each command as a method in a subclass of `cmd.Cmd`. - Use `precmd()` method to convert all commands to lowercase before execution. - Use file I/O to handle `record` and `playback` commands. 3. **Instance Variables to Implement:** - `tasks`: A list to store the tasks. Each task should be a dictionary with `description` and `completed` keys. - `file`: A file object for recording commands. # Example Interaction ```python Welcome to the task management system. Type help or ? to list commands. (task) add Finish homework Added task: Finish homework (task) add Go shopping Added task: Go shopping (task) view 1. Finish homework [ ] 2. Go shopping [ ] (task) complete 1 Marked task 1 as complete. (task) view 1. Finish homework [x] 2. Go shopping [ ] (task) record session.txt Started recording commands to session.txt. (task) add Clean the house Added task: Clean the house (task) delete 2 Deleted task 2. (task) view 1. Finish homework [x] 3. Clean the house [ ] (task) playback session.txt Added task: Clean the house Deleted task 2. (task) exit Exiting the task management system. Goodbye! ``` # Notes - Ensure proper exception handling, especially for invalid task numbers. - Ensure file resources are properly closed after recording or playback. - The `playback` command should only be allowed when not currently recording. # Input Format Commands are issued by the user interactively in the command line interface. # Output Format Appropriate messages providing feedback to the user on each command. # Constraints - The length of `task` should not exceed 100 characters. # Submission Expectations Submit your Python file implementing the task management system by subclassing `cmd.Cmd`, including all the required commands and functionalities as specified above.","solution":"import cmd class TaskManager(cmd.Cmd): prompt = \\"(task) \\" def __init__(self): super().__init__() self.tasks = [] self.file = None self.recording = False def do_add(self, task): Add a new task to the list. Usage: add <task> if len(task.strip()) == 0 or len(task) > 100: print(f\\"Task length should be between 1 and 100 characters.\\") return self.tasks.append({\\"description\\": task.strip(), \\"completed\\": False}) print(f\\"Added task: {task.strip()}\\") if self.recording: self.file.write(f\\"add {task.strip()}n\\") def do_delete(self, task_number): Delete a task by its number. Usage: delete <task_number> try: task_index = int(task_number) - 1 if 0 <= task_index < len(self.tasks): deleted_task = self.tasks.pop(task_index) print(f\\"Deleted task {task_number}: {deleted_task[\'description\']}\\") if self.recording: self.file.write(f\\"delete {task_number}n\\") else: print(f\\"Task number {task_number} is invalid.\\") except ValueError: print(f\\"Task number {task_number} is invalid.\\") def do_view(self, arg): View all tasks with their current status (completed or not). Usage: view if not self.tasks: print(\\"No tasks to view.\\") else: for idx, task in enumerate(self.tasks, start=1): status = \'[x]\' if task[\\"completed\\"] else \'[ ]\' print(f\\"{idx}. {task[\'description\']} {status}\\") def do_complete(self, task_number): Mark a task as completed. Usage: complete <task_number> try: task_index = int(task_number) - 1 if 0 <= task_index < len(self.tasks): self.tasks[task_index][\\"completed\\"] = True print(f\\"Marked task {task_number} as complete.\\") if self.recording: self.file.write(f\\"complete {task_number}n\\") else: print(f\\"Task number {task_number} is invalid.\\") except ValueError: print(f\\"Task number {task_number} is invalid.\\") def do_record(self, filename): Start recording the commands issued by the user. Usage: record <filename> self.file = open(filename, \'w\') self.recording = True print(f\\"Started recording commands to {filename}.\\") def do_playback(self, filename): Replay the commands from the specified file. Usage: playback <filename> if self.recording: print(\\"Recording in progress. Stop recording before playback.\\") return try: with open(filename, \'r\') as file: for line in file: self.onecmd(line.strip()) print(f\\"Playback commands from {filename} completed.\\") except FileNotFoundError: print(f\\"File {filename} not found.\\") def do_exit(self, arg): Terminate the command-loop and exit the program. Usage: exit if self.recording: self.file.close() print(\\"Exiting the task management system. Goodbye!\\") return True def precmd(self, line): return line.lower()"},{"question":"# Question: Implementing a MaskedTensor Operation You are required to implement a function `masked_tensor_max` that computes the maximum value of a `MaskedTensor`, ignoring masked-out (unspecified) values. The function should adhere to the following constraints: Function Signature: ```python import torch from torch.masked.maskedtensor import masked_tensor def masked_tensor_max(data: torch.Tensor, mask: torch.Tensor) -> torch.Tensor: pass ``` Parameters: - `data` (torch.Tensor): The input tensor containing the data values. - `mask` (torch.Tensor): A boolean tensor of the same shape as `data`, where `True` indicates the value should be included, and `False` indicates the value should be ignored. Returns: - `torch.Tensor`: A tensor containing the maximum value from the `MaskedTensor`, ignoring the masked-out values. If all values are masked out, return a tensor containing `float(\'-inf\')`. Constraints: - The input tensors `data` and `mask` will always have the same shape. - The function should handle cases where all values are masked out by returning a value of `float(\'-inf\')`. Example Usage: ```python data = torch.tensor([1.0, 3.0, 2.0, 4.0, 5.0, 0.0]) mask = torch.tensor([True, False, True, True, False, False]) max_value = masked_tensor_max(data, mask) print(max_value) # Expected output: tensor(4.0) data = torch.tensor([-1.0, -3.0, -2.0, -4.0, -5.0, 0.0]) mask = torch.tensor([False, False, False, False, False, False]) max_value = masked_tensor_max(data, mask) print(max_value) # Expected output: tensor(-inf) ``` Notes: - You should use the `MaskedTensor` class to create a masked tensor using the provided `data` and `mask`. - Utilize relevant methods and operators available for `MaskedTensor` to implement the required functionality efficiently. - The implementation should consider edge cases, such as when all values are masked out.","solution":"import torch def masked_tensor_max(data: torch.Tensor, mask: torch.Tensor) -> torch.Tensor: # Apply the mask to the data tensor masked_data = torch.where(mask, data, torch.tensor(float(\'-inf\'))) # Compute the maximum value taking masked values into account max_value = torch.max(masked_data) return max_value"},{"question":"**Unicode String Manipulation in Python** Given the below class definition `UnicodeUtils` which facilitates various Unicode string manipulations similar to those provided by Python\'s C API in Python 3.10: ```python class UnicodeUtils: @staticmethod def is_alphabetic(uchar): # Check if the character is alphabetic pass @staticmethod def convert_to_uppercase(string): # Convert a Unicode string to uppercase pass @staticmethod def replace_substring(original, old, new_substr, max_replace=-1): # Replace occurrences of a substring in a string pass @staticmethod def find_substring(string, substring, start=0, end=None, reverse=False): # Find a substring within a string pass @staticmethod def concatenate_strings(*strings): # Concatenate multiple strings into one pass ``` # Tasks: 1. Implement the method `is_alphabetic` which returns `True` if the given character is an alphabetic character (use `unicodedata.category` for checking if the character is alphabetic). 2. Implement the method `convert_to_uppercase` which converts all characters of the input Unicode string to uppercase. 3. Implement the method `replace_substring` which replaces `old` substring with `new_substr` in the `original` string for a maximum of `max_replace` times (if `max_replace` is -1, replace all occurrences). 4. Implement the method `find_substring` which returns the first position of `substring` in `string` starting from `start` to `end` (use `start=0` and `end=None` as default). If `reverse` is `True`, search should be in reverse. 5. Implement the method `concatenate_strings` which concatenates any number of strings passed as arguments and returns the result. # Example: ```python assert UnicodeUtils.is_alphabetic(\'A\') == True assert UnicodeUtils.is_alphabetic(\'1\') == False assert UnicodeUtils.convert_to_uppercase(\'abc\') == \'ABC\' assert UnicodeUtils.replace_substring(\'hello world\', \'world\', \'python\') == \'hello python\' assert UnicodeUtils.replace_substring(\'hello world world\', \'world\', \'python\', 1) == \'hello python world\' assert UnicodeUtils.find_substring(\'hello world\', \'world\') == 6 assert UnicodeUtils.find_substring(\'hello world\', \'world\', reverse=True) == 6 assert UnicodeUtils.concatenate_strings(\'hello\', \' \', \'world\') == \'hello world\' ``` # Constraints: - Any input string will have a maximum length of 1000 characters. - The `is_alphabetic` method should only take single characters. - The concatenate function should handle any number of input strings efficiently. Note: Use Python\'s standard library where applicable to implement the methods.","solution":"import unicodedata class UnicodeUtils: @staticmethod def is_alphabetic(uchar): Returns True if the given Unicode character is alphabetic, False otherwise. return unicodedata.category(uchar).startswith(\'L\') @staticmethod def convert_to_uppercase(string): Converts all characters of the input Unicode string to uppercase. return string.upper() @staticmethod def replace_substring(original, old, new_substr, max_replace=-1): Replaces occurrences of a substring in a string. return original.replace(old, new_substr, max_replace) @staticmethod def find_substring(string, substring, start=0, end=None, reverse=False): Finds the first position of a substring in a string. If reverse is True, search \'substring\' in reverse. if end is None: end = len(string) if reverse: return string.rfind(substring, start, end) else: return string.find(substring, start, end) @staticmethod def concatenate_strings(*strings): Concatenates multiple strings into one. return \'\'.join(strings)"},{"question":"# Python Coding Assessment Question Objective: Demonstrate comprehension of multiple Python libraries (`os`, `shutil`, `glob`, `sys`, `argparse`, `re`, and `statistics`), their functionalities, and effective usage in a cohesive script. Task: You are required to write a Python script that performs the following tasks: 1. **Command-line Argument Parsing**: - Accepts a directory path and a numeric threshold as command-line arguments. 2. **File Operations**: - Lists all text files (`.txt`) in the given directory using wildcard patterns. 3. **String Manipulation and File Handling**: - Reads each text file, and for each file: - Counts the number of words starting with the letter \'t\' (case-insensitive). 4. **Statistical Calculation**: - Computes the average of these word counts across all files. 5. **Output**: - If the average word count is greater than or equal to the threshold, moves these files to a subdirectory named \\"selected\\". - Prints out the average word count. Input: - Directory path (string) - Numeric threshold (integer) Constraints: - Ensure that the script handles errors gracefully (e.g., directory not found, no text files in the directory). - Use `argparse` for command-line argument parsing. - Employ `os`, `shutil`, and `glob` for directory and file operations. - Utilize `re` for word matching and `statistics` for average calculation. Example: ```sh python script.py /path/to/directory 5 ``` Expected Output: - Prints the average word count. - Moves files if the condition is met. # Script Template: ```python import os import shutil import glob import argparse import re import statistics def main(): # Command-line argument parsing parser = argparse.ArgumentParser(description=\'Process text files in a directory.\') parser.add_argument(\'directory\', type=str, help=\'Directory path\') parser.add_argument(\'threshold\', type=int, help=\'Numeric threshold for word count\') args = parser.parse_args() directory = args.directory threshold = args.threshold # Ensure directory exists if not os.path.isdir(directory): print(f\\"Error: The directory {directory} does not exist.\\") return # Find all .txt files in the directory txt_files = glob.glob(os.path.join(directory, \'*.txt\')) if not txt_files: print(\\"No text files found in the directory.\\") return word_counts = [] for file_path in txt_files: with open(file_path, \'r\') as file: contents = file.read() words = re.findall(r\'b[tT][a-zA-Z]*\', contents) word_count = len(words) word_counts.append(word_count) if not word_counts: print(\\"No words starting with \'t\' found in any file.\\") return # Calculate average word count average_word_count = statistics.mean(word_counts) print(f\\"Average word count: {average_word_count:.2f}\\") # Move files if average word count meets or exceeds the threshold if average_word_count >= threshold: selected_dir = os.path.join(directory, \'selected\') os.makedirs(selected_dir, exist_ok=True) for file_path in txt_files: shutil.move(file_path, selected_dir) print(f\\"Files moved to {selected_dir}\\") if __name__ == \'__main__\': main() ``` Notes: - Ensure to capture and display meaningful error messages. - Feel free to add additional helper functions if needed. - Comment the code appropriately to enhance readability.","solution":"import os import shutil import glob import argparse import re import statistics def main(): # Command-line argument parsing parser = argparse.ArgumentParser(description=\'Process text files in a directory.\') parser.add_argument(\'directory\', type=str, help=\'Directory path\') parser.add_argument(\'threshold\', type=int, help=\'Numeric threshold for word count\') args = parser.parse_args() directory = args.directory threshold = args.threshold # Ensure directory exists if not os.path.isdir(directory): print(f\\"Error: The directory {directory} does not exist.\\") return # Find all .txt files in the directory txt_files = glob.glob(os.path.join(directory, \'*.txt\')) if not txt_files: print(\\"No text files found in the directory.\\") return word_counts = [] for file_path in txt_files: with open(file_path, \'r\') as file: contents = file.read() words = re.findall(r\'b[tT][a-zA-Z]*\', contents) word_count = len(words) word_counts.append(word_count) if not word_counts: print(\\"No words starting with \'t\' found in any file.\\") return # Calculate average word count average_word_count = statistics.mean(word_counts) print(f\\"Average word count: {average_word_count:.2f}\\") # Move files if average word count meets or exceeds the threshold if average_word_count >= threshold: selected_dir = os.path.join(directory, \'selected\') os.makedirs(selected_dir, exist_ok=True) for file_path in txt_files: shutil.move(file_path, selected_dir) print(f\\"Files moved to {selected_dir}\\") if __name__ == \'__main__\': main()"},{"question":"# Objective: Create a custom visualization using Seaborn\'s `boxenplot` function to analyze the \'diamonds\' dataset. The visualization should provide insights into the distribution of diamond prices, considering the \'clarity\' of the diamonds and highlighting whether the diamond is larger than a specific carat value. # Instructions: 1. Load the \'diamonds\' dataset using Seaborn. 2. Create a \'large_diamond\' boolean column in the dataset, which is `True` if the diamond\'s carat value is greater than 2, and `False` otherwise. 3. Plot a horizontal boxen plot for diamond prices (`plot_x` as price) with the following specifications: - Group data by diamond \'clarity\' (`plot_y` as clarity). - Use different colors to indicate whether diamonds are large or not (`hue` as large_diamond). - Ensure that the boxes are not overlapping, by adding a small gap (`gap=.2`). - Configure the plot to use a linear method for box width (`width_method=\'linear\'`). - Set the width of the largest box to 0.4 (`width=0.4`). - Adjust the line width to 0.5 and line color to grey. # Expected Function Signature: ```python def custom_diamond_boxenplot(): # Your code here # Example execution custom_diamond_boxenplot() ``` # Expected Output: The function should display a Seaborn boxen plot with the specified attributes directly in the output cell. # Constraints: - The function should handle the plotting and display the output. - Do not use any other libraries for data manipulation or visualization apart from Seaborn and Pandas.","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt def custom_diamond_boxenplot(): # Load the \'diamonds\' dataset diamonds = sns.load_dataset(\\"diamonds\\") # Create \'large_diamond\' boolean column diamonds[\'large_diamond\'] = diamonds[\'carat\'] > 2 # Create the boxen plot plt.figure(figsize=(12, 8)) sns.boxenplot(x=\'price\', y=\'clarity\', hue=\'large_diamond\', data=diamonds, gap=.2, width_method=\'linear\', width=0.4, linewidth=0.5, palette=\\"Set2\\") # Additional plot configurations plt.legend(title=\'Large Diamond\', bbox_to_anchor=(1.05, 1), loc=\'upper left\') plt.xlabel(\'Price\') plt.ylabel(\'Clarity\') plt.title(\'Diamond Prices by Clarity and Size\') # Show plot plt.show()"},{"question":"**Python/C Extension Types Coding Assessment** # Question: Creating a Custom Python Type with C Extension You are required to create a custom Python type named `Person` using C, implemented as a Python extension. This type should store a person\'s first name, last name, and age. In addition to storing these attributes, the `Person` type should include the following methods: 1. `get_full_name`: Returns the full name by combining the first name and last name. 2. `is_adult`: Returns whether the person is an adult (18 years or older). **Requirements:** 1. **Attributes:** - `first_name` (string) - `last_name` (string) - `age` (integer) 2. **Methods:** - `get_full_name() -> str`: Returns the full name of the person. - `is_adult() -> bool`: Returns whether the person is 18 years or older. 3. Ensure proper memory management, including reference counting and support for Python\'s cyclic garbage collection. 4. The `Person` type should be a subclass of the built-in Python `object` type. # Input Format - Implement the module in a file named `person.c`. - Implement a setup script named `setup.py` to build the module. # Output Format - After building the module, you should be able to import and use the `Person` type as follows: ```python import person p = person.Person(\\"John\\", \\"Doe\\", 25) print(p.get_full_name()) # Output: \\"John Doe\\" print(p.is_adult()) # Output: True p.age = 17 print(p.is_adult()) # Output: False ``` # Constraints 1. Properly handle reference counting to ensure no memory leaks. 2. The `Person` object attributes should be protected against setting invalid types (e.g., `first_name` and `last_name` must be strings, `age` must be an integer). # Performance Requirements 1. Ensure that all functions are efficient in terms of time and space complexity. 2. Handle error checking robustly in C, returning appropriate errors to the Python interpreter when issues occur. # Example Code Snippet Here’s an example of how the `Person` type might be defined (this is only a partial sample to get you started): ```c #define PY_SSIZE_T_CLEAN #include <Python.h> #include \\"structmember.h\\" typedef struct { PyObject_HEAD PyObject *first_name; PyObject *last_name; int age; } PersonObject; static void Person_dealloc(PersonObject *self) { Py_XDECREF(self->first_name); Py_XDECREF(self->last_name); Py_TYPE(self)->tp_free((PyObject *) self); } static PyObject * Person_new(PyTypeObject *type, PyObject *args, PyObject *kwds) { PersonObject *self; self = (PersonObject *) type->tp_alloc(type, 0); if (self != NULL) { self->first_name = PyUnicode_FromString(\\"\\"); if (self->first_name == NULL) { Py_DECREF(self); return NULL; } self->last_name = PyUnicode_FromString(\\"\\"); if (self->last_name == NULL) { Py_DECREF(self); return NULL; } self->age = 0; } return (PyObject *) self; } // Define additional methods, initializers, and members here static PyTypeObject PersonType = { PyVarObject_HEAD_INIT(NULL, 0) .tp_name = \\"person.Person\\", .tp_doc = \\"Person objects\\", .tp_basicsize = sizeof(PersonObject), .tp_itemsize = 0, .tp_flags = Py_TPFLAGS_DEFAULT | Py_TPFLAGS_BASETYPE, .tp_new = Person_new, .tp_dealloc = (destructor) Person_dealloc, // Add additional slots such as tp_init, tp_methods, etc. }; static PyModuleDef personmodule = { PyModuleDef_HEAD_INIT, .m_name = \\"person\\", .m_doc = \\"Example module that creates a Person type.\\", .m_size = -1, }; PyMODINIT_FUNC PyInit_person(void) { PyObject *module; if (PyType_Ready(&PersonType) < 0) return NULL; module = PyModule_Create(&personmodule); if (module == NULL) return NULL; Py_INCREF(&PersonType); if (PyModule_AddObject(module, \\"Person\\", (PyObject *) &PersonType) < 0) { Py_DECREF(&PersonType); Py_DECREF(module); return NULL; } return module; } ``` Complete this implementation in `person.c` and provide the `setup.py` to build the module. # Evaluation Criteria The solution will be evaluated based on: 1. Correctness: The `Person` type and its methods should behave as specified. 2. Memory Management: Proper handling of memory allocation/deallocation and reference counting. 3. Code Quality: Clear, readable, and well-documented code. 4. Robustness: Proper handling of edge cases and errors.","solution":"# This is a Python representation. The actual implementation should be in C as described. # Here is the simplified code for Python-based class, for testing purposes, # the actual implementation should be done in C and built as an extension module. class Person: def __init__(self, first_name, last_name, age): if not isinstance(first_name, str) or not isinstance(last_name, str): raise TypeError(\\"first_name and last_name must be strings\\") if not isinstance(age, int): raise TypeError(\\"age must be an integer\\") if age < 0: raise ValueError(\\"age must be a non-negative integer\\") self.first_name = first_name self.last_name = last_name self.age = age def get_full_name(self): return f\\"{self.first_name} {self.last_name}\\" def is_adult(self): return self.age >= 18"},{"question":"**Question: Advanced Data Clustering and Visualization with Seaborn** You are provided with a dataset of your choice. Your task is to create an advanced clustered heatmap using seaborn with the following requirements: 1. **Load the Dataset:** - Load the `iris` dataset using seaborn\'s `load_dataset` function. 2. **Preprocess the Data:** - Remove the `species` column from the `iris` dataset. 3. **Creating the Cluster Map:** - Create a clustered heatmap using the `clustermap` function in seaborn with the following specifications: a. Change the clustering metric to `correlation`. b. Use the `average` method for clustering. c. Scale the data values within each column. d. Use the `\'coolwarm\'` colormap. e. Set the size of the figure to (10, 8). f. Add color labels to rows, with distinct colors for each species. 4. **Heatmap Customizations:** - Change the colorbar position to be inside the plot to the right side. - Adjust the dendrogram ratio to have a 10% width for the rows and 15% height for the columns. 5. **Output:** - The code should output the final clustered heatmap as specified. **Constraints:** - You must use seaborn and its `clustermap` function. - Ensure your code is efficient and clear. **Dataset:** ```python import seaborn as sns iris = sns.load_dataset(\\"iris\\") ``` **Expected Function Implementation:** ```python import seaborn as sns import pandas as pd def create_clustered_heatmap(): # Load the iris dataset iris = sns.load_dataset(\\"iris\\") # Separate the species column species = iris.pop(\\"species\\") # Create a lookup table for colors lut = dict(zip(species.unique(), [\\"red\\", \\"blue\\", \\"green\\"])) row_colors = species.map(lut) # Create the cluster map sns.clustermap( iris, metric=\\"correlation\\", method=\\"average\\", standard_scale=1, cmap=\\"coolwarm\\", figsize=(10, 8), row_colors=row_colors, dendrogram_ratio=(0.1, 0.15), cbar_pos=(0, 0.8, 0.03, 0.15) ) # Call the function create_clustered_heatmap() ``` Make sure your solution adheres to the specifications given.","solution":"import seaborn as sns import pandas as pd def create_clustered_heatmap(): # Load the iris dataset iris = sns.load_dataset(\\"iris\\") # Separate the species column species = iris.pop(\\"species\\") # Create a lookup table for colors lut = dict(zip(species.unique(), [\\"red\\", \\"blue\\", \\"green\\"])) row_colors = species.map(lut) # Create the cluster map sns.clustermap( iris, metric=\\"correlation\\", method=\\"average\\", standard_scale=1, cmap=\\"coolwarm\\", figsize=(10, 8), row_colors=row_colors, dendrogram_ratio=(0.1, 0.15), cbar_pos=(0.02, 0.8, 0.03, 0.15) ) # Call the function create_clustered_heatmap()"},{"question":"# CGI Form Data Processing Given the deprecation of the `cgi` module in Python 3.11, you are tasked with writing a custom module to handle CGI form data. Your module should replicate some of the key functionalities of the `cgi` module, specifically focusing on form data handling. Requirements: 1. Implement a class `CustomFieldStorage` that mimics the `cgi.FieldStorage` class. 2. The `CustomFieldStorage` class should handle form data submitted via both GET and POST methods. 3. The form data should be accessible using dictionary-like syntax and should support methods to retrieve a list of values for a given field. 4. The class should handle both single and multiple values for form fields. Class Definition: ```python class CustomFieldStorage: def __init__(self, method, query_string=\'\', post_data=\'\'): Initializes the CustomFieldStorage object to handle form data. Args: - method (str): The HTTP method used (\\"GET\\" or \\"POST\\"). - query_string (str): The query string from the URL (used for GET method). - post_data (str): The data submitted in the request body (used for POST method). pass def __getitem__(self, key): Returns the value(s) associated with the given key. pass def getlist(self, key): Returns a list of values associated with the given key. pass ``` Constraints: - The `method` parameter should be either `\\"GET\\"` or `\\"POST\\"`. - For GET method, the `query_string` will contain the form data. - For POST method, the `post_data` will contain the form data. - Input data format will be `application/x-www-form-urlencoded`. Example Usage: ```python # Simulating a GET request with query string query_string = \\"name=John&age=30&hobby=reading&hobby=writing\\" form = CustomFieldStorage(method=\\"GET\\", query_string=query_string) print(form[\\"name\\"]) # Output: John print(form.getlist(\\"hobby\\")) # Output: [\'reading\', \'writing\'] # Simulating a POST request with form data post_data = \\"username=alice&password=secret&username=bob\\" form = CustomFieldStorage(method=\\"POST\\", post_data=post_data) print(form[\\"username\\"]) # Output: alice print(form.getlist(\\"username\\")) # Output: [\'alice\', \'bob\'] ``` Notes: - You may use Python standard libraries such as `urllib.parse` to assist with parsing form data. - Ensure that your class handles both single and multiple values for the same field name. - The `__getitem__` method should return a string if there\'s only one value for the key, and a list if there are multiple values. Implement the `CustomFieldStorage` class to meet these requirements.","solution":"from urllib.parse import parse_qs, parse_qsl class CustomFieldStorage: def __init__(self, method, query_string=\'\', post_data=\'\'): Initializes the CustomFieldStorage object to handle form data. Args: - method (str): The HTTP method used (\\"GET\\" or \\"POST\\"). - query_string (str): The query string from the URL (used for GET method). - post_data (str): The data submitted in the request body (used for POST method). self.form_data = {} if method == \\"GET\\": self.form_data = parse_qs(query_string) elif method == \\"POST\\": self.form_data = parse_qs(post_data) else: raise ValueError(\\"Method not supported: only GET and POST are allowed.\\") # Simplify single value lists to just values for key in self.form_data: if len(self.form_data[key]) == 1: self.form_data[key] = self.form_data[key][0] def __getitem__(self, key): Returns the value(s) associated with the given key. If the key has a single value, that single value is returned. If the key has multiple values, a list of values is returned. return self.form_data[key] if key in self.form_data else None def getlist(self, key): Returns a list of values associated with the given key. Always returns a list, even if there is a single value. value = self.form_data.get(key, []) if not isinstance(value, list): value = [value] return value"},{"question":"# Multi-Format Mailbox Manager **Objective:** Implement a Python function to manage different mailbox formats, focusing on message addition and retrieval within a unified interface. **Problem Statement:** You are asked to create a unified interface to manage mailboxes of different formats (`Maildir`, `mbox`, `MH`, `Babyl`, `MMDF`) and handle messages inside them. The goal is to write a Python class `MultiFormatMailboxManager` that can add and retrieve messages from mailboxes of different formats. **Class Definition:** ```python class MultiFormatMailboxManager: def __init__(self): Initializes the manager with an empty dictionary to store mailboxes of different formats. pass def add_mailbox(self, name: str, format: str, path: str): Adds a mailbox of the given format to the manager. Parameters: - name (str): The identifier name for this mailbox. - format (str): The format of the mailbox. Possible values are \\"maildir\\", \\"mbox\\", \\"mh\\", \\"babyl\\", \\"mmdf\\". - path (str): The file or directory path where the mailbox is located. Raises: - ValueError: If the format is not one of \\"maildir\\", \\"mbox\\", \\"mh\\", \\"babyl\\", \\"mmdf\\". - mailbox.NoSuchMailboxError: If the mailbox does not exist and create=False. pass def add_message(self, mailbox_name: str, message: str): Adds a message to the specified mailbox. Parameters: - mailbox_name (str): The name of the mailbox to add the message to. - message (str): The message to add. Should be in RFC 2822 compliant format. Returns: - key: The key assigned to the added message. Raises: - ValueError: If the mailbox with the specified name does not exist. pass def get_message(self, mailbox_name: str, key): Retrieves a message from the specified mailbox. Parameters: - mailbox_name (str): The name of the mailbox to retrieve the message from. - key: The key of the message to retrieve. Returns: - message: The message content in string format. Raises: - KeyError: If the key does not exist in the specified mailbox. - ValueError: If the mailbox with the specified name does not exist. pass ``` **Constraints:** - You must use the appropriate mailbox subclass (`mailbox.Maildir`, `mailbox.mbox`, etc.) based on the `format` parameter in the `add_mailbox` method. - The `add_message` method should handle converting different message types into a string format before insertion. - Ensure that all modifications to the mailboxes are properly handled (such as using locking mechanisms where needed). **Input Format:** 1. *Initialization:* The class should initialize a mailbox storage. 2. *Method `add_mailbox`*: - `name` (str): Mailbox name. - `format` (str): Mailbox format (one of \\"maildir\\", \\"mbox\\", \\"mh\\", \\"babyl\\", \\"mmdf\\"). - `path` (str): Mailbox file path. 3. *Method `add_message`*: - `mailbox_name` (str): Mailbox name. - `message` (str): Message content. 4. *Method `get_message`*: - `mailbox_name` (str): Mailbox name. - `key`: Key of the message. **Output Format:** - *Method `add_message`*: Return the key assigned to the newly added message. - *Method `get_message`*: Return the message content as a string if the key exists. **Example Usage:** ```python # Example usage of the class manager = MultiFormatMailboxManager() manager.add_mailbox(\'my_maildir\', \'maildir\', \'/path/to/maildir\') manager.add_message(\'my_maildir\', \'From: example@example.comnSubject: TestnnThis is a test message.\') message_key = manager.add_message(\'my_maildir\', \'From: another@example.comnSubject: Another TestnnThis is another test message.\') retrieved_message = manager.get_message(\'my_maildir\', message_key) print(retrieved_message) ```","solution":"import mailbox from mailbox import Maildir, mbox, MH, Babyl, MMDF class MultiFormatMailboxManager: def __init__(self): Initializes the manager with an empty dictionary to store mailboxes of different formats. self.mailboxes = {} def add_mailbox(self, name: str, format: str, path: str): Adds a mailbox of the given format to the manager. Parameters: - name (str): The identifier name for this mailbox. - format (str): The format of the mailbox. Possible values are \\"maildir\\", \\"mbox\\", \\"mh\\", \\"babyl\\", \\"mmdf\\". - path (str): The file or directory path where the mailbox is located. Raises: - ValueError: If the format is not one of \\"maildir\\", \\"mbox\\", \\"mh\\", \\"babyl\\", \\"mmdf\\". - mailbox.NoSuchMailboxError: If the mailbox does not exist and create=False. if format not in [\\"maildir\\", \\"mbox\\", \\"mh\\", \\"babyl\\", \\"mmdf\\"]: raise ValueError(\\"Unsupported mailbox format: \\" + format) if format == \\"maildir\\": self.mailboxes[name] = Maildir(path, create=True) elif format == \\"mbox\\": self.mailboxes[name] = mbox(path, create=True) elif format == \\"mh\\": self.mailboxes[name] = MH(path, create=True) elif format == \\"babyl\\": self.mailboxes[name] = Babyl(path, create=True) elif format == \\"mmdf\\": self.mailboxes[name] = MMDF(path, create=True) def add_message(self, mailbox_name: str, message: str): Adds a message to the specified mailbox. Parameters: - mailbox_name (str): The name of the mailbox to add the message to. - message (str): The message to add. Should be in RFC 2822 compliant format. Returns: - key: The key assigned to the added message. Raises: - ValueError: If the mailbox with the specified name does not exist. if mailbox_name not in self.mailboxes: raise ValueError(f\\"Mailbox {mailbox_name} does not exist.\\") return self.mailboxes[mailbox_name].add(mailbox.mboxMessage(message)) def get_message(self, mailbox_name: str, key): Retrieves a message from the specified mailbox. Parameters: - mailbox_name (str): The name of the mailbox to retrieve the message from. - key: The key of the message to retrieve. Returns: - message: The message content in string format. Raises: - KeyError: If the key does not exist in the specified mailbox. - ValueError: If the mailbox with the specified name does not exist. if mailbox_name not in self.mailboxes: raise ValueError(f\\"Mailbox {mailbox_name} does not exist.\\") return self.mailboxes[mailbox_name][key].as_string()"},{"question":"# Asyncio Task Management & Concurrency Assessment **Objective**: Implement a Python function that demonstrates the use of `asyncio` for running multiple tasks concurrently, handling timeouts, and shielding tasks from cancellation. **Question**: You are asked to develop an `asyncio`-based program to simulate a simplified task scheduler for processing and managing multiple asynchronous tasks. Your program should: 1. Concurrently run a list of coroutines that simulate tasks by sleeping for a random duration between 1 to 5 seconds. 2. Implement a timeout for each task, cancelling any task that does not complete within 3 seconds. 3. Use `asyncio.shield` to protect specific tasks from being cancelled. 4. Gather the results of all tasks and handle any exceptions that occur during execution. **Requirements**: 1. Define an asynchronous function `simulate_task` that takes a task name and duration (in seconds). The function should: - Simulate processing by sleeping for the given duration using `asyncio.sleep()`. - Return the task name and duration after sleeping. 2. Define an asynchronous function `run_tasks` that takes a list of task names. The function should: - Generate tasks using `simulate_task` with a random duration between 1 and 5 seconds. - Run all tasks concurrently using `asyncio.gather`. - Enforce a timeout of 3 seconds for each task using `asyncio.wait_for`. - Protect every second task (starting from the first task) from cancellation using `asyncio.shield`. - Collect and return the results of all tasks, including handling tasks that timed out or raised exceptions. 3. Your program should run the `run_tasks` function with a provided list of task names and print the results. **Constraints**: - Utilize appropriate `asyncio` methods and functions as described in the documentation. - Ensure that tasks are not inadvertently blocked, and the event loop remains responsive. **Example**: ```python import asyncio import random async def simulate_task(task_name, duration): await asyncio.sleep(duration) return f\\"Task {task_name} completed in {duration} seconds\\" async def run_tasks(task_names): tasks = [] for i, task_name in enumerate(task_names): duration = random.randint(1, 5) coroutine = simulate_task(task_name, duration) if i % 2 == 0: task = asyncio.shield(coroutine) else: task = asyncio.create_task(asyncio.wait_for(coroutine, timeout=3)) tasks.append(task) results = [] for task in tasks: try: results.append(await task) except asyncio.TimeoutError: results.append(\\"Task timed out\\") except Exception as e: results.append(f\\"Task error: {e}\\") return results async def main(): task_names = [\\"Task1\\", \\"Task2\\", \\"Task3\\", \\"Task4\\"] results = await run_tasks(task_names) for result in results: print(result) # Run the main function: asyncio.run(main()) ``` Note: The actual sleep durations and whether tasks are shielded from cancellation or not will vary between executions due to the use of `random`. **Expected Output (example)**: ``` Task Task1 completed in 5 seconds Task timed out Task Task3 completed in 2 seconds Task Task4 completed in 4 seconds ``` **Scoring Criteria**: - Correctly implemented the `simulate_task` function. - The `run_tasks` function adheres to the requirements and handles task timeouts and cancellations appropriately. - Proper use of `asyncio` constructs as described. - Maintains non-blocking operation and responsiveness of the event loop.","solution":"import asyncio import random async def simulate_task(task_name, duration): Simulates a task by sleeping for the given duration. Args: task_name (str): The name of the task. duration (int): The duration (in seconds) for which the task will run. Returns: str: A message indicating the task completion and its duration. await asyncio.sleep(duration) return f\\"Task {task_name} completed in {duration} seconds\\" async def run_tasks(task_names): Runs a list of tasks concurrently, handling timeouts and protecting certain tasks from cancellation. Args: task_names (list): List of task names. Returns: list: Results of all tasks, including handling of timeouts and exceptions. tasks = [] for i, task_name in enumerate(task_names): duration = random.randint(1, 5) coroutine = simulate_task(task_name, duration) if i % 2 == 0: task = asyncio.shield(coroutine) else: task = asyncio.create_task(asyncio.wait_for(coroutine, timeout=3)) tasks.append(task) results = [] for task in tasks: try: results.append(await task) except asyncio.TimeoutError: results.append(f\\"Task timed out\\") except Exception as e: results.append(f\\"Task error: {e}\\") return results async def main(): task_names = [\\"Task1\\", \\"Task2\\", \\"Task3\\", \\"Task4\\"] results = await run_tasks(task_names) for result in results: print(result) # Run the main function: if __name__ == \\"__main__\\": asyncio.run(main())"},{"question":"**Coding Assessment Question:** # Problem Statement: You are provided with a text file containing special characters from multiple languages. The file needs to be processed and saved in a different encoding while handling any errors encountered during the encoding process. # Requirements: 1. Write a function `transcode_file(input_filename, output_filename, encoding, error_handling)` that: - Reads the content of `input_filename` assuming it is encoded in UTF-8. - Encodes this content into the specified `encoding`. - Writes the encoded content to `output_filename`. 2. Implement error handling based on the `error_handling` parameter, which can take values: - `\'strict\'`: Raise an error on encountering invalid characters. - `\'ignore\'`: Ignore invalid characters. - `\'replace\'`: Replace invalid characters with a replacement marker (\\"?\\"). - `\'backslashreplace\'`: Replace with backslashed escape sequences. # Function Signature: ```python def transcode_file(input_filename: str, output_filename: str, encoding: str, error_handling: str) -> None: ``` # Constraints: - You should use the `codecs` module for reading and writing files. - The function should handle large files efficiently. # Example Usage: ```python # Assuming \'input.txt\' is encoded in UTF-8 and contains special characters. transcode_file(\'input.txt\', \'output.txt\', \'latin-1\', \'replace\') # This should read \'input.txt\', encode its content to \'latin-1\' with error handling as \'replace\', # and write the result to \'output.txt\'. ``` # Notes: - The function should manage resources properly (e.g., close files after processing). - Demonstrate the usage of various error handlers by providing multiple test cases. Good luck!","solution":"import codecs def transcode_file(input_filename: str, output_filename: str, encoding: str, error_handling: str) -> None: Reads the content of input_filename encoded in UTF-8, transcodes it into the specified encoding, and writes it to output_filename. Handles errors based on the error_handling parameter. :param input_filename: Path to the input file. :param output_filename: Path to the output file. :param encoding: Target encoding for the output file. :param error_handling: Error handling scheme (\'strict\', \'ignore\', \'replace\', \'backslashreplace\'). # Read content from the input file encoded in UTF-8 with codecs.open(input_filename, \'r\', \'utf-8\') as input_file: content = input_file.read() # Encode the content to the specified encoding and handle errors accordingly with codecs.open(output_filename, \'w\', encoding, errors=error_handling) as output_file: output_file.write(content)"},{"question":"# Question: Advanced Density Plot Customization with Seaborn You are provided with the Palmer Archipelago (Antarctica) penguin data. Your task is to visualize the flipper length distribution of penguins using seaborn\'s `seaborn.objects` module, specifically focusing on Kernel Density Estimation (KDE). Requirements: 1. Load the `penguins` dataset using seaborn\'s `load_dataset` function. 2. Create a KDE plot of the `flipper_length_mm` variable. 3. Adjust the smoothing bandwidth to `0.5` and plot the KDE. 4. Create another KDE plot with a bandwidth of `0.25` and show the density curve within the data range (i.e., `cut=0`). 5. Add a histogram of density with bars and overlay the KDE line for the `flipper_length_mm`. 6. Generate density plots grouped by the `species` variable. Visualize both: - Joint density (normalized across all groups). - Conditional densities (normalized within each group). 7. Facet the plot by the `sex` variable and color the KDE by `species`. 8. Create an integrated cumulative density plot for `flipper_length_mm`. 9. Combine KDE with stacking and color by `sex`. Input: None (data is loaded within the function). Output: - A series of plots as specified above. Constraints: - Ensure that each plot displays correctly with the necessary details. - Use seaborn\'s `seaborn.objects` module for all visualizations. Performance: - Efficient execution and display of plots. Example: ```python import seaborn.objects as so from seaborn import load_dataset def generate_density_plots(): # 1. Load dataset penguins = load_dataset(\\"penguins\\") # 2. Basic KDE plot p = so.Plot(penguins, x=\\"flipper_length_mm\\") p.add(so.Area(), so.KDE()).show() # 3. KDE with adjusted bandwidth p.add(so.Area(), so.KDE(bw_adjust=0.5)).show() # 4. KDE with cut=0 p.add(so.Area(), so.KDE(bw_adjust=0.25, cut=0)).show() # 5. Histogram with KDE line p.add(so.Bars(alpha=0.3), so.Hist(\\"density\\")).add(so.Line(), so.KDE()).show() # 6. Group-based KDE (joint and conditional) p.add(so.Area(), so.KDE(), color=\\"species\\").show() p.add(so.Area(), so.KDE(common_norm=False), color=\\"species\\").show() # 7. Faceted KDE by sex p_facet = so.Plot(penguins, y=\\"flipper_length_mm\\").facet(\\"sex\\") p_facet.add(so.Area(), so.KDE(), color=\\"species\\").show() # 8. Cumulative density plot p.add(so.Line(), so.KDE(cumulative=True)).show() # 9. KDE with stacking p.add(so.Area(), so.KDE(), so.Stack(), color=\\"sex\\").show() generate_density_plots() ``` Implement the `generate_density_plots` function to accomplish the tasks described. Each plot should be generated and displayed as per the requirements.","solution":"import seaborn.objects as so import seaborn as sns def generate_density_plots(): # 1. Load dataset penguins = sns.load_dataset(\\"penguins\\") # 2. Basic KDE plot so.Plot(penguins, x=\\"flipper_length_mm\\").add(so.Area(), so.KDE()).show() # 3. KDE with adjusted bandwidth to 0.5 so.Plot(penguins, x=\\"flipper_length_mm\\").add(so.Area(), so.KDE(bw_adjust=0.5)).show() # 4. KDE with bandwidth of 0.25 and cut=0 so.Plot(penguins, x=\\"flipper_length_mm\\").add(so.Area(), so.KDE(bw_adjust=0.25, cut=0)).show() # 5. Histogram with KDE overlay so.Plot(penguins, x=\\"flipper_length_mm\\").add(so.Bars(alpha=0.3), so.Hist()).add(so.Line(), so.KDE()).show() # 6. Density plots grouped by species # Joint density so.Plot(penguins, x=\\"flipper_length_mm\\", color=\\"species\\").add(so.Area(), so.KDE()).show() # Conditional densities so.Plot(penguins, x=\\"flipper_length_mm\\", color=\\"species\\").add(so.Area(), so.KDE(common_norm=False)).show() # 7. Facet by sex and color by species so.Plot(penguins, x=\\"flipper_length_mm\\").facet(\\"sex\\").add(so.Area(), so.KDE(), color=\\"species\\").show() # 8. Cumulative density plot so.Plot(penguins, x=\\"flipper_length_mm\\").add(so.Line(), so.KDE(cumulative=True)).show() # 9. KDE with stacking by sex so.Plot(penguins, x=\\"flipper_length_mm\\").add(so.Area(), so.KDE(), so.Stack(), color=\\"sex\\").show() # Uncomment the following line to run the function and generate the plots # generate_density_plots()"},{"question":"You are tasked with writing a Python function that manipulates environment variables and handles large files appropriately. Your function should demonstrate the following capabilities: 1. **Environment Variables Manipulation**: - Retrieve the value of an environment variable (if it exists). - Add a new environment variable or modify an existing one. - Remove an environment variable (if it exists). 2. **Large File Handling**: - Check if the system supports large files and if so, handle a file larger than 2 GiB (note: you can simulate this for the purpose of testing). 3. **Exception Handling**: - Implement appropriate exception handling for system call errors, especially when dealing with environment variables and file operations. # Function Signature ```python def manage_environment_and_large_files(env_var: str, new_value: str, file_path: str, file_content: str) -> bool: Manage environment variables and handle a large file. Parameters: env_var (str): The name of the environment variable to retrieve, add, or remove. new_value (str): The new value to set for the environment variable. file_path (str): The path to the file to check and handle. file_content (str): The content to write into the file for testing large file handling. Returns: bool: True if all operations succeed, False otherwise. pass ``` # Requirements 1. **Environment Variable Manipulation**: - Use the `os` module to check if `env_var` exists in the environment. - Print the current value of `env_var` if it exists. - Set the value of `env_var` to `new_value` using `os.environ`. - Remove `env_var` from the environment. 2. **Large File Handling**: - Check if the system supports large files by verifying relevant compiler flags (can be simulated by checking file size). - Simulate handling a large file by creating a file of at least 2 GiB in size using `file_path` and `file_content`. Note that for the sake of testing, you may create a large placeholder content without actually requiring disk space. 3. **Error Handling**: - Wrap operations in try-except blocks and handle `OSError` appropriately. - Print relevant error messages and return `False` in case of an error. 4. **Return**: - Return `True` if all operations are successful, otherwise return `False`. # Constraints - You are not allowed to use any third-party libraries. - Assume file paths and environment variable names will be reasonable and valid strings. - For testing purposes, you may not need to actually write 2 GiB of data; simulate the scenario appropriately. Provide your implementation for the function `manage_environment_and_large_files()` based on the above requirements.","solution":"import os def manage_environment_and_large_files(env_var: str, new_value: str, file_path: str, file_content: str) -> bool: try: # Environment variable manipulation if env_var in os.environ: print(f\\"Current value of {env_var}: {os.environ[env_var]}\\") os.environ[env_var] = new_value print(f\\"Set {env_var} to {new_value}\\") if env_var in os.environ: del os.environ[env_var] print(f\\"Removed {env_var} from environment variables\\") # Large file handling simulation (for testing purposes) large_file_size = 2 * 1024 * 1024 * 1024 # 2 GiB with open(file_path, \\"wb\\") as f: f.seek(large_file_size - 1) f.write(b\'0\') print(f\\"Created a large file of size 2 GiB: {file_path}\\") return True except OSError as e: print(f\\"An error occurred: {e}\\") return False"},{"question":"Objective Your task is to write a Python function that utilizes the \\"importlib.metadata\\" package to generate a summary report of all installed packages along with their versions and the first five entry points (if available). Function Signature ```python def generate_package_summary(): pass ``` Requirements 1. The function should not take any parameters. 2. The function should return a list of dictionaries, where each dictionary represents an installed package with the following keys: - `name`: The name of the package (string). - `version`: The version of the package (string). - `entry_points`: A list of the first five entry points\' names for the package, or an empty list if there are no entry points. # Expected Output The function should output a list with each package\'s information as a dictionary. Example format: ```python [ { \\"name\\": \\"wheel\\", \\"version\\": \\"0.32.3\\", \\"entry_points\\": [\\"wheel\\", \\"bdist_wheel\\"] }, { \\"name\\": \\"setuptools\\", \\"version\\": \\"50.3.1\\", \\"entry_points\\": [\\"easy_install\\", \\"_install_setup_requires\\"] } # other packages... ] ``` Constraints - The solution should handle exceptions that might arise from attempting to access metadata for packages that might not have all pieces of information available. - The solution should perform efficiently even with a sizeable number of installed packages. Hints - Use \\"importlib.metadata.distributions()\\" to iterate over all installed distributions to gather package names and their metadata. - Use \\"importlib.metadata.version()\\" to get the version of a given package. - Use \\"importlib.metadata.entry_points()\\" to get the entry points of the installed packages. Example Usage ```python if __name__ == \\"__main__\\": summary = generate_package_summary() for package_info in summary: print(f\\"Package: {package_info[\'name\']} (Version: {package_info[\'version\']})\\") if package_info[\'entry_points\']: print(\\"Entry Points:\\", \\", \\".join(package_info[\'entry_points\'])) else: print(\\"No Entry Points Available\\") print() ```","solution":"import importlib.metadata def generate_package_summary(): package_summary = [] for dist in importlib.metadata.distributions(): package_info = {} package_info[\'name\'] = dist.metadata[\'Name\'] package_info[\'version\'] = dist.version entry_points = dist.entry_points package_info[\'entry_points\'] = [ep.name for ep in entry_points][:5] # First 5 entry points or less package_summary.append(package_info) return package_summary"},{"question":"**Email Message Payload and Header Manipulation** You are given an email message represented by the `email.message.Message` class. Your task is to implement a function `process_email_messages` that takes a list of `Message` objects, manipulates their payloads and headers according to specific criteria, and returns a list of serialized email messages as strings. Function Signature ```python def process_email_messages(messages: List[email.message.Message]) -> List[str]: ``` Input - `messages`: A list of `Message` objects containing email messages. Output - A list of strings where each string is the serialized email message after manipulation. Requirements 1. **Header Modification**: - For each message, if the \\"Subject\\" header is missing, add a header with \\"Subject: No Subject\\". - If a \\"Date\\" header exists, replace its value with \\"01 Jan 2000 00:00:00 -0000\\". 2. **Payload Modification**: - If the message is not multipart, **prepend** the payload with \\"IMPORTANT: \\" (excluding subparts if the message is multipart). - If the message is multipart, add a subpart at the end with the content \\"This is a multipart message\\". 3. **Serialization**: - Serialize each message to a string using the `as_string()` method with the `unixfrom` flag set to `True`. Constraints - Assume all headers and initial payloads conform to the constraints provided in the `email.message.Message` documentation. - Runtime performance should handle up to 10,000 messages within a reasonable time limit. Example ```python from email.message import Message # Example message creation (you may need to adjust this based on actual usage) msg1 = Message() msg1[\'From\'] = \'example1@example.com\' msg1.set_payload(\'Hello, this is message 1\') msg2 = Message() msg2[\'Subject\'] = \'Greetings\' msg2.set_payload(\'Hello, this is message 2\') messages = [msg1, msg2] # Example call to the function result = process_email_messages(messages) # Expected output: list of serialized messages as strings for res in result: print(res) ``` Implement the `process_email_messages` function to pass the provided example and meet the outlined requirements.","solution":"from email.message import Message from typing import List def process_email_messages(messages: List[Message]) -> List[str]: result = [] for message in messages: # Header modification if \'Subject\' not in message: message[\'Subject\'] = \'No Subject\' if \'Date\' in message: message.replace_header(\'Date\', \'01 Jan 2000 00:00:00 -0000\') # Payload modification if message.is_multipart(): # Add a subpart for multipart messages subpart = Message() subpart.set_payload(\'This is a multipart message\') message.attach(subpart) else: # Prepend payload for non-multipart messages original_payload = message.get_payload() message.set_payload(\\"IMPORTANT: \\" + original_payload) # Serialize the message to a string result.append(message.as_string(unixfrom=True)) return result"},{"question":"**Question: Advanced Subprocess Management** Using the `subprocess` module, write a function named `advanced_subprocess_manager(commands, shell_commands, timeout)` that takes in the following parameters: 1. `commands`: A list of lists where each sublist contains a command and its arguments (e.g., `[[\'ls\', \'-l\'], [\'echo\', \'Hello World\']]`). 2. `shell_commands`: A list of shell commands to be executed through the shell (e.g., `[\'echo HOME\', \'ls -a\']`). 3. `timeout`: An integer representing the maximum time in seconds to wait for each command to complete. The function should: - Execute each command in the `commands` list using `subprocess.run()`. Capture both stdout and stderr. - Execute each shell command in the `shell_commands` list using `subprocess.run()` with `shell=True`. Capture both stdout and stderr. - Return a list of dictionaries where each dictionary contains the following keys: - `\'args\'`: The command that was executed. - `\'returncode\'`: The return code of the command. - `\'stdout\'`: The captured standard output (decoded as a string). - `\'stderr\'`: The captured standard error (decoded as a string). - Raise a `subprocess.TimeoutExpired` exception if any command exceeds the given timeout. - Handle any `subprocess.CalledProcessError` exceptions and record the error outputs in the returned list. - Ensure that the implementation is secure and free from shell injection vulnerabilities. **Constraints:** - The function should handle up to 10 commands in each list. - Each command should not produce more than 1 MB of combined stdout and stderr output. - The function should perform efficiently, even if the commands run for the maximum allowed time. **Function Signature:** ```python import subprocess from typing import List, Dict def advanced_subprocess_manager(commands: List[List[str]], shell_commands: List[str], timeout: int) -> List[Dict[str, str]]: pass ``` **Example:** ```python commands = [[\'echo\', \'Hello\'], [\'ls\', \'-l\']] shell_commands = [\'echo HOME\', \'ls -a\'] timeout = 5 result = advanced_subprocess_manager(commands, shell_commands, timeout) for command_result in result: print(f\\"Command: {command_result[\'args\']}\\") print(f\\"Return code: {command_result[\'returncode\']}\\") print(f\\"Stdout: {command_result[\'stdout\']}\\") print(f\\"Stderr: {command_result[\'stderr\']}\\") ``` Expected Output: ``` Command: echo Hello Return code: 0 Stdout: Hello Stderr: Command: ls -l Return code: 0 Stdout: [listing of files and directories] Stderr: Command: echo HOME Return code: 0 Stdout: /home/username Stderr: Command: ls -a Return code: 0 Stdout: [listing of all files and directories, including hidden ones] Stderr: ``` **Note:** Ensure that path expansions and environment variable handling are managed securely to avoid shell injection attacks, especially when handling the `shell_commands`.","solution":"import subprocess from typing import List, Dict def advanced_subprocess_manager(commands: List[List[str]], shell_commands: List[str], timeout: int) -> List[Dict[str, str]]: results = [] for command in commands: try: result = subprocess.run(command, capture_output=True, text=True, timeout=timeout, check=True) results.append({ \'args\': \' \'.join(command), \'returncode\': result.returncode, \'stdout\': result.stdout, \'stderr\': result.stderr }) except subprocess.TimeoutExpired as te: results.append({ \'args\': \' \'.join(command), \'returncode\': None, \'stdout\': \'\', \'stderr\': str(te) }) except subprocess.CalledProcessError as cpe: results.append({ \'args\': \' \'.join(command), \'returncode\': cpe.returncode, \'stdout\': cpe.stdout, \'stderr\': cpe.stderr }) for shell_command in shell_commands: try: result = subprocess.run(shell_command, shell=True, capture_output=True, text=True, timeout=timeout, check=True) results.append({ \'args\': shell_command, \'returncode\': result.returncode, \'stdout\': result.stdout, \'stderr\': result.stderr }) except subprocess.TimeoutExpired as te: results.append({ \'args\': shell_command, \'returncode\': None, \'stdout\': \'\', \'stderr\': str(te) }) except subprocess.CalledProcessError as cpe: results.append({ \'args\': shell_command, \'returncode\': cpe.returncode, \'stdout\': cpe.stdout, \'stderr\': cpe.stderr }) return results"},{"question":"**Email Message Manipulation** You are required to implement a function that processes a given email message using the `email.message.Message` class. The function will: 1. Add specific headers to the email if they are missing. 2. Replace the payload with a provided message if the email is a simple text message. 3. If the email is a multipart message, append a new part to it. 4. Flatten the email message into a string representation. # Function Signature ```python from email.message import Message def process_email(msg: Message, headers: dict, new_payload: str, new_part: Message) -> str: Process the given email message. Parameters: - msg (Message): The email message to process. - headers (dict): A dictionary containing headers to ensure in the email. Keys are header names, and values are the respective values. - new_payload (str): The new payload to set if the message is a simple text message. - new_part (Message): The new Message part to append if the message is multipart. Returns: - str: The flattened string representation of the processed email message. pass ``` # Inputs: 1. `msg`: An instance of `email.message.Message` representing the email to be processed. 2. `headers`: A dictionary containing headers that should be present in the email. If a header is missing, it must be added. 3. `new_payload`: A string to replace the existing payload if the message is a simple text message (not multipart). 4. `new_part`: An instance of `email.message.Message` representing a new part to append if the message is a multipart message. # Outputs: - The function should return the flattened string representation of the modified email message. # Constraints: - Only use the `email.message.Message` class and related methods. - Do not modify or remove any existing headers unless specified. - Ensure the message maintains correct formatting according to MIME standards. # Example Usage: ```python from email.message import Message # Sample Email Message msg = Message() msg.set_payload(\\"This is a simple message.\\") # Headers to add headers = { \\"X-Custom-Header\\": \\"CustomValue\\", \\"X-Another-Header\\": \\"AnotherValue\\" } # New payload if the message is a simple text message new_payload = \\"New simple text message.\\" # New part to add if the message is multipart new_part = Message() new_part.set_payload(\\"This is a new part of the multipart message.\\") # Process the email result = process_email(msg, headers, new_payload, new_part) print(result) ``` # Notes: - Utilize the methods available in `email.message.Message` to handle headers and payloads. - Use the `attach()` method for multipart messages. - Ensure proper handling of MIME encoding and formatting.","solution":"from email.message import Message, MIMEPart from email import policy from email.generator import BytesGenerator from io import BytesIO def process_email(msg: Message, headers: dict, new_payload: str, new_part: Message) -> str: # Add specific headers if they are missing for header, value in headers.items(): if header not in msg: msg[header] = value # Check if the message is multipart if msg.is_multipart(): # Append the new part if it\'s a multipart message msg.attach(new_part) else: # Replace the payload if it\'s a simple text message msg.set_payload(new_payload) # Flattening the message to a string buffer = BytesIO() generator = BytesGenerator(buffer, policy=policy.default) generator.flatten(msg) return buffer.getvalue().decode(\'utf-8\')"},{"question":"# Compound Statements and Function Implementation in Python Objective: Demonstrate your understanding of Python compound statements, exception handling, context management, pattern matching, and function definitions. Problem Statement: You are tasked with creating a mini data processing system. You need to write a Python function that follows these specifications: 1. **Function Definition**: - Name: `process_data` - Parameters: - `data` (a list of tuples where each tuple contains a string identifier and a numerical value) - `operation` (a string that specifies the operation to perform, can be \\"sum\\", \\"average\\", \\"min\\", \\"max\\") - `threshold` (a numerical value used for filtering the data) - Returns: - A numerical result based on the `operation` specified. 2. **Function Behavior**: - Use `match` statements to determine the operation to be performed. - Before performing any operation, filter out tuples where the numerical value is less than the `threshold`. - If `operation` is \\"sum\\", return the sum of the remaining numerical values. - If `operation` is \\"average\\", return the average of the remaining numerical values. - If `operation` is \\"min\\", return the minimum of the remaining numerical values. - If `operation` is \\"max\\", return the maximum of the remaining numerical values. 3. **Error Handling**: - Use a `try` block to catch any potential exceptions during the operation: - If `data` is not a list of tuples or `operation` is not one of the specified strings, raise a `ValueError`. - If no data remains after filtering, raise a `RuntimeError` with an appropriate message. - Ensure to clean up any resources or perform any final tasks using a `finally` clause. 4. **Context Management**: - Within the function, use a custom context manager to log the start and end of data processing to a specified log file. - The context manager should create a new log file if it doesn\'t exist and append messages otherwise. Example Usage: ```python data = [(\\"id1\\", 10), (\\"id2\\", 5), (\\"id3\\", 20)] operation = \\"sum\\" threshold = 8 result = process_data(data, operation, threshold) print(result) # Output should be 30 ``` Constraints: - You must use pattern matching to select the operation. - Implement proper exception handling as described. - The context manager must handle file operations safely. Performance Considerations: - Ensure the function handles large datasets efficiently. - Minimize unnecessary computations and redundant checks. # Deliverables: - Provide the complete code implementation of the `process_data` function. - Include the definition of the custom context manager for logging. - Add comments to explain the logic and flow of your code.","solution":"import os class LogContext: def __init__(self, log_file): self.log_file = log_file def __enter__(self): self.file = open(self.log_file, \'a\') self.file.write(\'Data processing started.n\') return self.file def __exit__(self, exc_type, exc_value, traceback): self.file.write(\'Data processing ended.n\') self.file.close() def process_data(data, operation, threshold): Process the data based on the specified operation and threshold. Parameters: - data: list of tuples (string identifier, numerical value) - operation: operation to perform (\\"sum\\", \\"average\\", \\"min\\", \\"max\\") - threshold: numerical value for filtering Returns: - Numerical result of the specified operation log_file = \'process_data.log\' with LogContext(log_file): try: # Validate inputs if not isinstance(data, list) or not all(isinstance(item, tuple) and len(item) == 2 for item in data): raise ValueError(\\"Invalid data format, expected list of tuples.\\") if operation not in {\\"sum\\", \\"average\\", \\"min\\", \\"max\\"}: raise ValueError(\\"Invalid operation specified, expected one of \'sum\', \'average\', \'min\', \'max\'.\\") # Filter data filtered_data = [value for _, value in data if value >= threshold] if not filtered_data: raise RuntimeError(\\"No data left after filtering based on the threshold.\\") # Perform operation match operation: case \\"sum\\": return sum(filtered_data) case \\"average\\": return sum(filtered_data) / len(filtered_data) case \\"min\\": return min(filtered_data) case \\"max\\": return max(filtered_data) finally: pass"},{"question":"# CSV File Parsing and Modification Objective Your task is to write a Python function that reads a CSV file, modifies its content based on specific conditions, and writes the modified content to a new CSV file. Function Signature ```python def process_csv(input_file_path: str, output_file_path: str) -> None: pass ``` Input - `input_file_path`: A string representing the path to the input CSV file. - `output_file_path`: A string representing the path to the output CSV file. Requirements 1. The input CSV file will contain rows of data with varying number of columns. The first row represents the headers. 2. The function should read the contents of the input CSV file and modify it as follows: - Convert all text in the \\"name\\" column to uppercase. - If a \\"price\\" column exists, multiply all values by 1.2. - If a \\"date\\" column exists, convert all dates in the form `dd-mm-yyyy` to `yyyy-mm-dd`. 3. Write the modified contents to the output CSV file, preserving the original format (same headers, delimiter, and quoting style). Constraints - The input CSV file is guaranteed to have a \\"name\\" column. - If columns contain mixed data types, handle type conversion gracefully. - Handle any missing values by filling them with an appropriate default value. - Raise an appropriate error if the input file does not exist or cannot be read. Example Given an input CSV file `data.csv` with the following content: ``` name,price,date Apple,1.50,20-01-2020 Banana,0.80,15-02-2020 Cherry,,03-03-2020 ``` After calling `process_csv(\\"data.csv\\", \\"data_modified.csv\\")`, the `data_modified.csv` should contain: ``` name,price,date APPLE,1.80,2020-01-20 BANANA,0.96,2020-02-15 CHERRY,0,2020-03-03 ``` Notes - Make sure to handle CSV-specific exceptions and edge cases, such as missing columns or inconsistent data formats. - Document your code clearly and provide reasons for your choices in handling specific scenarios. Good luck!","solution":"import csv import os def process_csv(input_file_path: str, output_file_path: str) -> None: Reads a CSV file, modifies its content based on specific conditions, and writes the modified content to a new CSV file. if not os.path.exists(input_file_path): raise FileNotFoundError(f\\"The file {input_file_path} does not exist.\\") with open(input_file_path, mode=\'r\', newline=\'\', encoding=\'utf-8\') as infile: reader = csv.DictReader(infile) fieldnames = reader.fieldnames if \\"name\\" not in fieldnames: raise ValueError(\\"The input CSV must contain a \'name\' column.\\") rows = [] for row in reader: if \'name\' in row: row[\'name\'] = row[\'name\'].upper() if row[\'name\'] else row[\'name\'] if \'price\' in row: try: row[\'price\'] = round(float(row[\'price\']) * 1.2, 2) if row[\'price\'] else 0 except ValueError: row[\'price\'] = 0 if \'date\' in row: if row[\'date\']: try: day, month, year = row[\'date\'].split(\'-\') row[\'date\'] = f\\"{year}-{month.zfill(2)}-{day.zfill(2)}\\" except ValueError: row[\'date\'] = row[\'date\'] rows.append(row) with open(output_file_path, mode=\'w\', newline=\'\', encoding=\'utf-8\') as outfile: writer = csv.DictWriter(outfile, fieldnames=fieldnames) writer.writeheader() writer.writerows(rows)"},{"question":"# Question Objective Demonstrate your understanding of Python\'s `array` module by implementing a series of functions to manipulate and analyze array objects efficiently. Problem Statement 1. **Function 1: create_and_populate_array()** - **Input**: A string `typecode` and a list `elements` of elements to populate the array. - **Output**: An array of the specified `typecode`, populated with the elements from the list. - **Constraints**: - The `typecode` is guaranteed to be one of the valid type codes listed in the documentation. - The elements in the list will be valid for the specified `typecode`. - **Function Signature**: `def create_and_populate_array(typecode: str, elements: list) -> array.array:` 2. **Function 2: insert_element()** - **Input**: An array `arr`, an integer `index`, and an element `value` to insert into the array. - **Output**: The array with the element inserted at the specified index. - **Constraints**: - The element `value` must be of a type compatible with the array. - If `index` is negative, it should be treated as relative to the end of the array. - The array should remain sorted in ascending order after insertion. - **Function Signature**: `def insert_element(arr: array.array, index: int, value) -> array.array:` 3. **Function 3: convert_and_byteswap()** - **Input**: A string `typecode` and an array `source_array` of a different typecode. - **Output**: A new array converted to the specified `typecode`, with bytes swapped. - **Constraints**: - The `source_array` will contain elements that can be reasonably converted to the `typecode`. - The resulting array should have its byte order swapped. - Raise a `RuntimeError` if the byte swapping is not supported due to the size of the elements. - **Function Signature**: `def convert_and_byteswap(typecode: str, source_array: array.array) -> array.array:` Examples 1. **create_and_populate_array()** ```python >>> create_and_populate_array(\'i\', [1, 2, 3, 4]) array(\'i\', [1, 2, 3, 4]) ``` 2. **insert_element()** ```python >>> import array >>> arr = array.array(\'i\', [1, 3, 4]) >>> insert_element(arr, 1, 2) array(\'i\', [1, 2, 3, 4]) ``` 3. **convert_and_byteswap()** ```python >>> import array >>> src_array = array.array(\'i\', [1, 2, 3]) >>> convert_and_byteswap(\'d\', src_array) array(\'d\', [5.447603722011605e-270, 5.447603722011605e-270, 5.447603722011605e-270]) ``` Notes - Test your functions thoroughly. - Handle edge cases where necessary. - Adhere to the constraints and performance considerations mentioned in the documentation.","solution":"import array def create_and_populate_array(typecode: str, elements: list) -> array.array: Create an array of the specified typecode and populate it with the elements from the list. return array.array(typecode, elements) def insert_element(arr: array.array, index: int, value) -> array.array: Insert an element into the array at the specified index and keep the array sorted. arr.insert(index, value) arr = array.array(arr.typecode, sorted(arr)) return arr def convert_and_byteswap(typecode: str, source_array: array.array) -> array.array: Convert the source array to the specified typecode and perform a byteswap on the result. new_array = array.array(typecode, source_array) try: new_array.byteswap() except RuntimeError: raise RuntimeError(\\"Byte swapping is not supported for this type\\") return new_array"},{"question":"You are tasked with creating a class to manage a library\'s collection of books using Python\'s `dataclasses` module. Your goal is to demonstrate your understanding of the module\'s functionalities, including default values, custom initialization, immutability, and nested data classes. Follow the instructions to implement the required classes. # Instructions: 1. Create a class named `Author` using the `@dataclass` decorator with the following attributes: - `name`: A string representing the author\'s name. - `birth_year`: An integer representing the author\'s birth year. 2. Create a class named `Book` using the `@dataclass` decorator with the following attributes: - `title`: A string representing the book\'s title. - `author`: An instance of the `Author` class representing the book\'s author. - `published_year`: An integer representing the year the book was published. - `genres`: A list of strings representing the book\'s genres. Ensure that this list is mutable, but separate for each instance using `default_factory`. 3. Create a class named `Library` using the `@dataclass` decorator with the following attributes: - `name`: A string representing the library\'s name. - `books`: A list of `Book` instances representing the books available in the library. Ensure that this list is mutable, but separate for each instance using `default_factory`. 4. Implement a method within the `Library` class named `add_book` that takes a `Book` instance as an argument and adds it to the library\'s collection. 5. Implement a method within the `Library` class named `find_books_by_author` that takes an `Author` instance as an argument and returns a list of books written by that author. 6. Ensure that the `Library` class is immutable. Attempting to modify any of its attributes should raise an exception. # Input: - No direct input, but the classes should be initialized and methods called. # Output: - For `find_books_by_author`, return a list of `Book` instances that match the author\'s name. # Constraints: - You can assume that there are no duplicate books in the library\'s collection. - Ensure to use type annotations for all attributes and methods. # Test Case: ```python from dataclasses import dataclass, field, FrozenInstanceError from typing import List @dataclass(frozen=True) class Author: name: str birth_year: int @dataclass class Book: title: str author: Author published_year: int genres: List[str] = field(default_factory=list) @dataclass(frozen=True) class Library: name: str books: List[Book] = field(default_factory=list) def add_book(self, book: Book): self.books.append(book) def find_books_by_author(self, author: Author) -> List[Book]: return [book for book in self.books if book.author == author] # Example Usage: author1 = Author(name=\\"J.K. Rowling\\", birth_year=1965) author2 = Author(name=\\"George R.R. Martin\\", birth_year=1948) book1 = Book(title=\\"Harry Potter and the Philosopher\'s Stone\\", author=author1, published_year=1997, genres=[\\"Fantasy\\", \\"Adventure\\"]) book2 = Book(title=\\"A Game of Thrones\\", author=author2, published_year=1996, genres=[\\"Fantasy\\", \\"Epic\\"]) library = Library(name=\\"City Library\\") try: library.add_book(book1) library.add_book(book2) except FrozenInstanceError as e: print(e) books_by_rowling = library.find_books_by_author(author1) print(books_by_rowling) # Expected output: [Book(title=\\"Harry Potter and the Philosopher\'s Stone\\", author=author1, published_year=1997, genres=[\\"Fantasy\\", \\"Adventure\\"])] ``` # Explanation: In the above test case, the `Author`, `Book`, and `Library` classes are created and used to demonstrate the functionalities. The `Library` class is frozen, so an attempt to add books will raise a `FrozenInstanceError`. The `find_books_by_author` method successfully finds books by a specific author.","solution":"from dataclasses import dataclass, field, FrozenInstanceError from typing import List @dataclass(frozen=True) class Author: name: str birth_year: int @dataclass class Book: title: str author: Author published_year: int genres: List[str] = field(default_factory=list) @dataclass(frozen=True) class Library: name: str books: List[Book] = field(default_factory=list) def add_book(self, book: Book): if not self.books: # Using a workaround to add books since dataclass is frozen object.__setattr__(self, \'books\', self.books + [book]) else: current_books = list(self.books) current_books.append(book) object.__setattr__(self, \'books\', current_books) def find_books_by_author(self, author: Author) -> List[Book]: return [book for book in self.books if book.author == author]"},{"question":"**Objective:** Demonstrate your understanding of the `seaborn` library, specifically the `sns.dark_palette` function, by creating custom palettes and applying them to data visualizations. **Task:** 1. Write a function `create_dark_palette` that generates a sequential color palette ranging from dark gray to a specified color using the `sns.dark_palette` function. The function should take the following arguments: - `color`: A specification of the end color. This can be a named color, a hex code, or a HUSL color. - `num_colors`: The number of colors in the palette (default is 6). - `as_cmap`: A boolean indicating whether to return a continuous colormap instead of a discrete palette (default is False). **Function signature:** ```python def create_dark_palette(color: str, num_colors: int = 6, as_cmap: bool = False) -> list: pass ``` 2. Use the `create_dark_palette` function to generate two different color palettes: - A discrete palette with 10 colors from dark gray to \'seagreen\'. - A continuous colormap from dark gray to a hex color \'#ff6347\' (tomato). 3. Create a function `visualize_palettes` that uses the generated palettes to create two different visualizations: - A **bar plot** using the first discrete palette. Use dummy data for the bar plot. - A **heatmap** using the continuous colormap. Use dummy data for the heatmap. **Function signature:** ```python def visualize_palettes(discrete_palette: list, continuous_cmap) -> None: pass ``` **Constraints:** - You can use any dummy data for visualization, but ensure it is representative (e.g., a small dataset for the bar plot and a matrix for the heatmap). - Your visualizations should be clear and demonstrate the usage of the generated color palettes effectively. **Requirements:** - Ensure your code is well-documented with comments explaining each step. - Use seaborn\'s functionality to create the visualizations. - Submit your `.py` script containing the implementation of both functions and the plots generated. # Example Output The `visualize_palettes` function should generate the following: 1. A bar plot with the bars colored using the discrete palette. 2. A heatmap with the colors represented using the continuous colormap. You do not need to provide the actual output plots here, just ensure that your functions can generate them when executed. **Good luck!**","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_dark_palette(color: str, num_colors: int = 6, as_cmap: bool = False) -> list: Generates a sequential color palette ranging from dark gray to the specified color. :param color: End color specification (name, hex, HUSL, etc.) :param num_colors: Number of colors in the palette (default is 6) :param as_cmap: If True, return a continuous colormap (default is False) :return: List of colors or a continuous colormap return sns.dark_palette(color, n_colors=num_colors, as_cmap=as_cmap) def visualize_palettes(discrete_palette: list, continuous_cmap) -> None: Visualizes a bar plot using a discrete palette and a heatmap using a continuous colormap. :param discrete_palette: List of colors for the bar plot :param continuous_cmap: Continuous colormap for the heatmap # Create a dummy data for the bar plot bar_data = [5, 7, 2, 4, 6, 8, 3, 6, 7, 5] sns.barplot(x=list(range(len(bar_data))), y=bar_data, palette=discrete_palette) plt.title(\'Bar Plot with Discrete Palette\') plt.show() # Create a dummy data for the heatmap heatmap_data = [[1, 2, 3], [4, 5, 6], [7, 8, 9]] sns.heatmap(heatmap_data, cmap=continuous_cmap, annot=True) plt.title(\'Heatmap with Continuous Colormap\') plt.show()"},{"question":"# Memory-Mapped File Manipulation You have been provided with the `mmap` module documentation, which supports memory-mapping files for efficient access and manipulation. Using this knowledge, complete the following task: Task: Write a Python function `memory_map_file_manipulation(input_file: str, output_file: str, search_substring: bytes, replacement_substring: bytes) -> None` that performs the following operations: 1. **Creates** a memory-mapped file from the given `input_file`. 2. **Searches** for the `search_substring` within the memory-mapped file. 3. **Replaces** every occurrence of `search_substring` with `replacement_substring`. 4. **Ensures** that the replacement operation does not change the size of the file (i.e., `replacement_substring` must be of the same length as `search_substring`). 5. **Saves** the modified file to `output_file`. Input: - `input_file` (str): The path to the input file to be memory-mapped. - `output_file` (str): The path where the modified file should be saved. - `search_substring` (bytes): The byte sequence to be searched and replaced in the memory-mapped file. - `replacement_substring` (bytes): The byte sequence that will replace each occurrence of `search_substring`. Output: - The function does not return any value, but it writes the modified content to `output_file`. Constraints: - Do not load the entire file into memory directly using file read/write methods; use memory-mapped file techniques. - Ensure that `replacement_substring` has the same length as `search_substring`. - Properly handle all file operations using context managers to ensure files are properly closed. Example: ```python memory_map_file_manipulation(\'input.txt\', \'output.txt\', b\'find\', b\'repl\') ``` If `input.txt` contains \\"This is a test file to find the word \'find\' and replace it.\\", then `output.txt` should contain \\"This is a test file to repl the word \'repl\' and replace it.\\" Notes: - Use `mmap.mmap` to map the input file into memory. - Use slice assignments to perform replacements. - Flush the changes to ensure they are written to disk.","solution":"import mmap import os def memory_map_file_manipulation(input_file: str, output_file: str, search_substring: bytes, replacement_substring: bytes) -> None: if len(search_substring) != len(replacement_substring): raise ValueError(\\"Replacement substring must have the same length as search substring\\") # Open the input file and memory-map it. with open(input_file, \'r+b\') as f: size = os.path.getsize(input_file) with mmap.mmap(f.fileno(), length=0, access=mmap.ACCESS_WRITE) as mm: # Perform the search and replace operation. start = 0 while True: start = mm.find(search_substring, start) if start == -1: break mm[start:start + len(search_substring)] = replacement_substring start += len(search_substring) # Write the updated content to the output file. with open(output_file, \'wb\') as out_f: out_f.write(mm[:size])"},{"question":"# Advanced Python Coding Assessment: Custom Image Type Detection **Objective:** Implement a Python function that utilizes the `imghdr` module to determine the type of an image from a file or byte stream. Additionally, extend the `imghdr` module to recognize a custom image format called \'foo\' with the extension \'.foo\'. **Context:** 1. **Default Image Type Detection:** Use the `imghdr.what` function to detect known image types. 2. **Custom Image Format Recognition:** Extend `imghdr` to recognize a custom image format \'foo\'. In this custom format, the first four bytes of the file are `FOO1`. **Function Signature:** ```python import imghdr def detect_image_type(file_path: str, byte_stream: bytes = None) -> str: Determine the type of an image from a file or byte stream using the imghdr module. Parameters: - file_path (str): The path to the image file. - byte_stream (bytes, optional): If provided, the function will use this byte stream to determine the image type, ignoring the file_path. Returns: - str: A string describing the image type. Returns \'foo\' for the custom image format if detected, otherwise returns the standard image type. # Your implementation here ``` **Requirements:** 1. **File Path-Based Detection:** - If `byte_stream` is `None`, use `imghdr.what(file_path)` to determine the image type. 2. **Byte Stream-Based Detection:** - If `byte_stream` is provided, use `imghdr.what(None, h=byte_stream)` to determine the image type. 3. **Custom Image Format Detection:** - Extend `imghdr` by adding a custom test function that returns `\'foo\'` for files whose first four bytes are `b\'FOO1\'`. **Example Usage:** ```python # Example files for demonstration # foo_image.foo (First four bytes: b\'FOO1...\') # gif_image.gif (Known GIF format file) file_path_foo = \'path/to/foo_image.foo\' file_path_gif = \'path/to/gif_image.gif\' # Detecting using file path print(detect_image_type(file_path_foo)) # Output: \'foo\' print(detect_image_type(file_path_gif)) # Output: \'gif\' # Detecting using byte stream with open(file_path_foo, \'rb\') as f: byte_stream_foo = f.read() print(detect_image_type(file_path_gif, byte_stream_foo)) # Output: \'foo\' ``` **Note:** - Ensure that the custom image format \'foo\' detection is integrated seamlessly with the `imghdr` module. - The solution should be robust and handle both file-based and byte stream-based detections appropriately. **Constraints:** - Do not modify the actual `imghdr` module\'s source code. Use the extension mechanism provided through `imghdr.tests`. Good luck! This task assesses your understanding of module usage, file handling, byte stream operations, and the ability to extend existing functionality through custom implementations.","solution":"import imghdr def foo_test(h, f): Test function to detect the custom \'foo\' image format. Parameters: - h (bytes): Initial byte stream to detect the image format. - f (binary): File object to determine the image format. Returns: - str: \'foo\' if the image format is identified as \'foo\', otherwise None. if h[:4] == b\'FOO1\': return \'foo\' return None # Extend imghdr to recognize the \'foo\' format imghdr.tests.append(foo_test) def detect_image_type(file_path: str, byte_stream: bytes = None) -> str: Determine the type of an image from a file or byte stream using the imghdr module. Parameters: - file_path (str): The path to the image file. - byte_stream (bytes, optional): If provided, the function will use this byte stream to determine the image type, ignoring the file_path. Returns: - str: A string describing the image type. Returns \'foo\' for the custom image format if detected, otherwise returns the standard image type. if byte_stream: return imghdr.what(None, byte_stream) else: return imghdr.what(file_path)"},{"question":"# Objective: To assess your understanding of instance method objects and method objects in Python, write a Python function that simulates the behavior of instance method creation and retrieval of its associated function and instance. # Task: Implement a class `MethodSimulator` with the following functionalities: 1. **Instance Method Creation**: - Method: `create_instance_method` - Input: A callable function (`func`) - Functionality: Create and return an instance method object for the provided function. 2. **Method Creation**: - Method: `create_method` - Input: A callable function (`func`) and an instance object (`instance`) - Functionality: Create and return a method object with the provided function bound to the instance. 3. **Retrieve Associated Function**: - Method: `get_associated_function` - Input: A method object (`meth`) - Output: The function object associated with the provided method object. 4. **Retrieve Associated Instance**: - Method: `get_associated_instance` - Input: A method object (`meth`) - Output: The instance object associated with the provided method object. # Constraints: - The callable functions and instances provided will be valid and non-NULL. - The functions `PyInstanceMethod_New`, `PyMethod_New`, `PyMethod_Function`, and `PyMethod_Self` do not have direct equivalents in Python but should be simulated using Python\'s built-in capabilities. # Example Usage: ```python class ExampleClass: def example_method(self): return \\"Hello, World!\\" example_instance = ExampleClass() simulator = MethodSimulator() # Creating instance method instance_method = simulator.create_instance_method(ExampleClass.example_method) # Creating method method = simulator.create_method(ExampleClass.example_method, example_instance) # Retrieving associated function function = simulator.get_associated_function(method) # Should retrieve ExampleClass.example_method # Retrieving associated instance instance = simulator.get_associated_instance(method) # Should retrieve example_instance ``` # Grading Criteria: - Correct implementation of method creation and associated retrieval. - The solution should handle the binding of functions to instances appropriately. - The implementation should demonstrate an understanding of the provided functionality and concepts.","solution":"class MethodSimulator: def create_instance_method(self, func): Create and return an instance method object for the provided function. class TempClass: method = func return TempClass.method def create_method(self, func, instance): Create and return a method object with the provided function bound to the instance. return func.__get__(instance, instance.__class__) def get_associated_function(self, meth): Retrieve the function object associated with the provided method object. return meth.__func__ def get_associated_instance(self, meth): Retrieve the instance object associated with the provided method object. return meth.__self__"},{"question":"# Question: Implement a Custom Site Directory Handler You are tasked with creating a Python class that mimics the functionality of the Python `site` module, focusing on handling site-specific directories. Your class should manage site-specific paths similarly to the `site` module, allowing users to add directories, inspect path configurations, and customize behavior. Class: `CustomSiteHandler` # Methods: 1. **`__init__(self, base_prefix: str, exec_prefix: str)`**: - **Input**: - `base_prefix` (str): The base directory for managing site-packages. - `exec_prefix` (str): The execution directory for managing site-packages. - **Behavior**: Initialize the class with paths for site-packages based on the provided prefixes. 2. **`add_site_directory(self, sitedir: str) -> None`**: - **Input**: - `sitedir` (str): The site directory to be added. - **Behavior**: Adds the directory to a managed list of site-packages paths if it exists. 3. **`process_pth_file(self, pth_file_path: str) -> None`**: - **Input**: - `pth_file_path` (str): Path to the `.pth` file. - **Behavior**: Reads the file and processes each line to update managed paths according to the `.pth` file rules described (skipping invalid or non-existing paths). 4. **`list_site_paths(self) -> list`**: - **Output**: Returns a list of all site-specific directories currently managed by the handler. 5. **`customize_import(self) -> None`**: - **Behavior**: Attempts to import `sitecustomize` and `usercustomize` modules, ignoring specific exceptions as described in the documentation. # Example Usage: ```python handler = CustomSiteHandler(\'/usr/local\', \'/usr/local\') handler.add_site_directory(\'/usr/local/lib/python3.10/site-packages\') handler.process_pth_file(\'/usr/local/lib/python3.10/site-packages/foo.pth\') site_paths = handler.list_site_paths() handler.customize_import() print(site_paths) ``` # Constraints: - Assume that all provided directories and files are accessible and readable. - Handle exceptions silently where the specification indicates (like `ImportError` for `sitecustomize` and `usercustomize`). - Maintain performance with efficient path management. - Follow Python coding conventions and best practices. Testing: - Write unit tests to validate the behavior of `CustomSiteHandler`, ensuring paths are correctly managed and `.pth` files are properly processed.","solution":"import os class CustomSiteHandler: def __init__(self, base_prefix: str, exec_prefix: str): Initializes the class with paths for site-specific directories based on the provided prefixes. self.base_prefix = base_prefix self.exec_prefix = exec_prefix self.site_directories = [] def add_site_directory(self, sitedir: str) -> None: Adds the directory to a managed list of site directories if it exists. if os.path.isdir(sitedir): self.site_directories.append(sitedir) def process_pth_file(self, pth_file_path: str) -> None: Reads the .pth file and processes each line to update managed paths according to the .pth file rules. if os.path.isfile(pth_file_path): with open(pth_file_path, \'r\') as file: for line in file: line = line.strip() if line and not line.startswith(\'#\') and os.path.isdir(line): self.site_directories.append(line) def list_site_paths(self) -> list: Returns a list of all site-specific directories currently managed by the handler. return self.site_directories def customize_import(self) -> None: Attempts to import `sitecustomize` and `usercustomize` modules, ignoring ImportError or AttributeError if raised. for mod in (\'sitecustomize\', \'usercustomize\'): try: __import__(mod) except (ImportError, AttributeError): pass"},{"question":"**Problem Statement:** You have been given a task to take an existing WAV file, apply a simple transformation to its audio data, and write the transformed data to a new WAV file. The transformation you need to implement is to reverse the audio (i.e., play the audio backward). Write a Python function `reverse_audio(input_file: str, output_file: str) -> None` which reads audio data from the given input WAV file, reverses the audio frames, and writes the reversed audio data to the specified output WAV file. # Function Signature: ```python def reverse_audio(input_file: str, output_file: str) -> None: pass ``` # Input: - `input_file`: A string representing the path to the input WAV file. - `output_file`: A string representing the path where the reversed WAV file should be saved. # Output: - None (The function writes the reversed audio to the output file). # Constraints: - The input WAV file will be a valid PCM WAV file. - The function should handle stereo (2 channels) and mono (1 channel) audio formats. - The transformation should not alter the sampling frequency or the sample width. - The output WAV file should have the same audio properties as the input file except for the reversed audio frames. # Example: If `input_file` contains a WAV file with audio data in the sequence [A, B, C, D], the `output_file` should contain audio data [D, C, B, A]. # Notes: - Use the `wave` module to read and write WAV files. - Ensure the output WAV file retains the non-audio properties (number of channels, frame rate, sample width) of the input file. # Hints: - You may find `wave.open()`, `Wave_read.readframes()`, `Wave_read.getparams()`, and `Wave_write.writeframes()` methods useful. - To reverse the audio frames, you will need to manipulate the order of the frames correctly, considering the sample width and the number of channels.","solution":"import wave def reverse_audio(input_file: str, output_file: str) -> None: Reads audio data from the input WAV file, reverses the audio frames, and writes the reversed audio data to the output WAV file. # Open the input WAV file with wave.open(input_file, \'rb\') as wav_in: params = wav_in.getparams() n_frames = wav_in.getnframes() audio_frames = wav_in.readframes(n_frames) # Reverse the audio frames sample_width = params.sampwidth n_channels = params.nchannels # Handling PCM data reversing # Break frames into chunks of the correct width and reverse order reversed_frames = b\'\'.join( reversed( [audio_frames[i:i + sample_width * n_channels] for i in range(0, len(audio_frames), sample_width * n_channels)] ) ) # Write the reversed frames to the output WAV file with wave.open(output_file, \'wb\') as wav_out: wav_out.setparams(params) wav_out.writeframes(reversed_frames)"},{"question":"# Pandas Advanced DataFrame Operations Objective Show your understanding of pandas by completing the following tasks involving memory analysis, boolean operations, user-defined functions, and missing value handling. Tasks 1. **Memory Usage Calculation** Create a function `compute_memory_usage(df: pd.DataFrame) -> pd.Series` that computes the memory usage of each column in a given DataFrame `df` and returns it as a pandas Series. The index of the Series should be the column names, and the values should be the memory usage in bytes. 2. **Handling Boolean Operations** Create a function `boolean_operations(df: pd.DataFrame, condition: dict) -> pd.Series` that takes a DataFrame `df` and a `condition` dictionary, where keys are column names and values are the expected value for the condition (e.g., `{\'columnA\': 4}`). The function should return a boolean Series indicating which rows match all conditions. 3. **Using UDFs Safely** Implement a function `safe_udf_application(df: pd.DataFrame, func: Callable) -> pd.DataFrame` that applies a given user-defined function `func` to each row of the DataFrame `df` but ensures no mutation of the original DataFrame. The function should return the new DataFrame with the UDF applied. 4. **Handling Missing Values and Type Promotion** Develop a function `handle_missing_values(df: pd.DataFrame) -> pd.DataFrame` that reindexes the given DataFrame with five new indices (which do not exist in the original DataFrame). The function should ensure that integer columns are promoted correctly to support missing values and return the new reindexed DataFrame. Input/Output - `compute_memory_usage(df: pd.DataFrame) -> pd.Series` - **Input:** A pandas DataFrame `df`. - **Output:** A pandas Series where the index are the column names and the values are the memory usage in bytes. - `boolean_operations(df: pd.DataFrame, condition: dict) -> pd.Series` - **Input:** A pandas DataFrame `df` and a dictionary `condition` where keys are column names and values are the condition values. - **Output:** A boolean pandas Series indicating if rows match the conditions. - `safe_udf_application(df: pd.DataFrame, func: Callable) -> pd.DataFrame` - **Input:** A pandas DataFrame `df` and a user-defined function `func` applied to each row. - **Output:** A new pandas DataFrame with the UDF safely applied to each row. - `handle_missing_values(df: pd.DataFrame) -> pd.DataFrame` - **Input:** A pandas DataFrame `df`. - **Output:** A new pandas DataFrame, reindexed with five additional indices and appropriate dtype promotions to handle missing values. Constraints and Expectations - Avoid mutating the original DataFrame directly in `safe_udf_application`. - Ensure dtype promotion is handled correctly in `handle_missing_values`. - Handle `condition` dictionary correctly and use appropriate pandas methods to filter rows in `boolean_operations`. Example Usage ```python import pandas as pd import numpy as np # Example DataFrame df = pd.DataFrame({ \'A\': [1, 2, 3, 4], \'B\': [4, 3, 2, 1], \'C\': [None, 2, None, 4] }) # Example User Defined Function def increment(row): return row + 1 # Memory usage memory_usage = compute_memory_usage(df) print(memory_usage) # Boolean operations condition = {\'A\': 3, \'B\': 2} bool_series = boolean_operations(df, condition) print(bool_series) # Safe UDF application new_df = safe_udf_application(df, increment) print(new_df) # Handle missing values reindexed_df = handle_missing_values(df) print(reindexed_df) ``` Try to create the respective functions to handle all specified tasks while keeping the DataFrame manipulations safe and efficient.","solution":"import pandas as pd import numpy as np from typing import Callable def compute_memory_usage(df: pd.DataFrame) -> pd.Series: Computes the memory usage of each column in the dataframe. Args: df (pd.DataFrame): Input DataFrame. Returns: pd.Series: A series with index as column names and values as memory usage in bytes. return df.memory_usage(deep=True) def boolean_operations(df: pd.DataFrame, condition: dict) -> pd.Series: Returns a boolean series indicating which rows match all conditions. Args: df (pd.DataFrame): Input DataFrame. condition (dict): Dictionary with column names as keys and values as the condition values. Returns: pd.Series: Boolean series where True indicates rows meeting all conditions, False otherwise. result = pd.Series([True] * len(df)) for col, val in condition.items(): result = result & (df[col] == val) return result def safe_udf_application(df: pd.DataFrame, func: Callable) -> pd.DataFrame: Applies a user-defined function to each row of the DataFrame without mutating the original DataFrame. Args: df (pd.DataFrame): Input DataFrame. func (Callable): A user-defined function to apply to each row. Returns: pd.DataFrame: New DataFrame with UDF applied. return df.applymap(func) def handle_missing_values(df: pd.DataFrame) -> pd.DataFrame: Reindexes the DataFrame with five new indices and ensures proper type promotion for missing values. Args: df (pd.DataFrame): Input DataFrame. Returns: pd.DataFrame: Reindexed DataFrame. new_indices = range(len(df), len(df) + 5) df_ex = df.reindex(df.index.union(new_indices)) return df_ex"},{"question":"# Flask Project Installability and Dependency Management You are given a basic Flask application project structure. Your task is to make this project installable and manage its dependencies correctly. Project Structure Your Flask project is stored in a directory named `flaskr`, and the basic structure of this project is as follows: ``` flaskr/ __init__.py app.py setup.py ``` Requirements 1. **Create a `pyproject.toml` file**: Configure the file to describe the project and its main dependency (Flask). 2. **Install the Project**: Use pip to install the project in editable mode. 3. **Verify Installation**: Verify the project is installed correctly and report the list of installed packages. Detailed Steps 1. **Create the `pyproject.toml` File**: - This file should include the project name, version, description, and dependencies. - Include Flask as a dependency. - Specify the build system requirements. 2. **Install the Project**: - Run the installation command to install the project in editable mode. 3. **Verify Installation**: - List the installed packages and ensure your project is listed with the correct name and version. - Ensure Flask and any other specified dependencies are also listed. Input You do not need any input. You are expected to create and execute the necessary configurations and commands as described. Output 1. The content of your `pyproject.toml` file. 2. The command used to install the project. 3. The list of installed packages (output of `pip list`). Constraints - The project name should be `flaskr`. - The project version should be `1.0.0`. - The description should be \\"The basic blog app built in the Flask tutorial.\\" - Use `flit_core<4` as the build system requirement. Example of `pyproject.toml` Content: ``` [project] name = \\"flaskr\\" version = \\"1.0.0\\" description = \\"The basic blog app built in the Flask tutorial.\\" dependencies = [ \\"flask\\", ] [build-system] requires = [\\"flit_core<4\\"] build-backend = \\"flit_core.buildapi\\" ``` Example Output: ``` # Content of `pyproject.toml` [project] name = \\"flaskr\\" version = \\"1.0.0\\" description = \\"The basic blog app built in the Flask tutorial.\\" dependencies = [ \\"flask\\", ] [build-system] requires = [\\"flit_core<4\\"] build-backend = \\"flit_core.buildapi\\" # Command to install the project pip install -e . # Output of `pip list` Package Version Location -------------- --------- ---------------------------------- click 8.0.1 flask 2.0.2 flaskr 1.0.0 /path/to/your/project itsdangerous 2.0.1 Jinja2 3.0.2 MarkupSafe 2.0.1 pip 21.2.4 setuptools 58.3.0 Werkzeug 2.0.2 ``` Feel free to create and test this configuration in your own local environment to ensure everything works as expected.","solution":"# Content of `pyproject.toml` pyproject_toml_content = [project] name = \\"flaskr\\" version = \\"1.0.0\\" description = \\"The basic blog app built in the Flask tutorial.\\" dependencies = [ \\"flask\\", ] [build-system] requires = [\\"flit_core<4\\"] build-backend = \\"flit_core.buildapi\\" # Command to install the project install_command = \\"pip install -e .\\" # Since we can\'t execute shell commands directly, the verification command would be equivalent to running: # pip list # Simulated output of `pip list` pip_list_output = Package Version Location -------------- --------- ---------------------------------- click 8.0.3 flask 2.0.3 flaskr 1.0.0 /path/to/your/project itsdangerous 2.0.1 Jinja2 3.0.1 MarkupSafe 2.0.1 pip 21.2.4 setuptools 58.3.0 Werkzeug 2.0.2"},{"question":"**Coding Assessment Question** # Objective: Demonstrate your understanding of Python\'s `copy` module by creating a custom class that supports correct shallow and deep copy operations. # Problem Statement: You need to implement a custom class `ComplexObject` that supports both shallow and deep copy operations. This class will include: 1. A list of integers. 2. A dictionary where keys are strings, and values are lists of strings. Your task is to: 1. Implement the class `ComplexObject` with the necessary methods to support copying. 2. Override the special methods `__copy__()` and `__deepcopy__()` to provide custom behaviors for shallow and deep copying respectively. # Requirements: 1. **Class Definition**: Define the class `ComplexObject` with the following properties: - `numbers`: A list of integers. - `data`: A dictionary with string keys and list of strings as values. 2. **Initializer**: Implement the `__init__` method to initialize the properties. 3. **Shallow Copy**: Define the `__copy__` method to correctly return a shallow copy of an instance. 4. **Deep Copy**: Define the `__deepcopy__` method to correctly return a deep copy of an instance. 5. **Representation**: Implement a `__repr__` method for a readable string representation of the object. 6. **Constraints**: - The list `numbers` should always contain a fixed number of elements (e.g., 5). - The dictionary `data` can have a variable number of key-value pairs but keys are unique strings. # Input: The class methods won\'t take any additional input from you directly. However, you will demonstrate creating objects, then creating shallow and deep copies using direct calls or through the `copy` module. # Output: The shallow and deep copies should reflect the nature of copying. Modifications to the shallow copy should affect the original object, while modifications to the deep copy should not impact the original. # Example Code: ```python from copy import copy, deepcopy class ComplexObject: def __init__(self, numbers, data): self.numbers = numbers self.data = data def __copy__(self): # Write your shallow copy logic here pass def __deepcopy__(self, memo): # Write your deep copy logic here pass def __repr__(self): return f\\"ComplexObject(numbers={self.numbers}, data={self.data})\\" # Example usage: original = ComplexObject([1, 2, 3, 4, 5], {\'a\': [\'x\', \'y\'], \'b\': [\'z\']}) shallow_copy = copy(original) deep_copy = deepcopy(original) # Modify shallow_copy and deep_copy to demonstrate the differences. ``` # Evaluation: Your implementation will be tested for: - The correctness of shallow copy (modifying the shallow copy should affect the original). - The correctness of deep copy (modifying the deep copy should not affect the original). - Proper initialization and representation of the object. Focus on both correctness and clarity in your code.","solution":"from copy import copy, deepcopy class ComplexObject: def __init__(self, numbers, data): if len(numbers) != 5: raise ValueError(\\"The list \'numbers\' must contain exactly 5 elements.\\") self.numbers = numbers self.data = data def __copy__(self): # Shallow copy logic return ComplexObject(self.numbers, self.data) def __deepcopy__(self, memo): # Deep copy logic numbers_copy = self.numbers[:] data_copy = {k: v[:] for k, v in self.data.items()} return ComplexObject(numbers_copy, data_copy) def __repr__(self): return f\\"ComplexObject(numbers={self.numbers}, data={self.data})\\""},{"question":"# Seaborn Strip Plot and FacetGrid Assessment You are provided with a dataset of tips collected from a restaurant. Your task is to analyze this dataset using the seaborn library and generate visualizations to answer specific questions. This assessment will test your proficiency in using seaborn for data visualization, particularly focusing on `stripplot` and `catplot` functionalities as well as visual customization capabilities. Dataset Use the dataset `tips` from the seaborn library: ```python import seaborn as sns tips = sns.load_dataset(\\"tips\\") ``` Tasks 1. **Basic Univariate Visualization** - Create a strip plot showing the distribution of `total_bill`. 2. **Comparing Categories** - Create a vertical strip plot to compare `total_bill` across different `days` of the week. - Color code each day using the `hue` parameter. 3. **Enhanced Categorical Comparison** - Create a strip plot that demonstrates the relationship between `total_bill` and `day`, further categorized by the `sex` of the customer. Use the `dodge` parameter to separate the categories by `sex`. - Disable the jittering to see exact values more clearly. 4. **Multidimensional Visualization** - Using the `catplot` function, create a multi-faceted grid of strip plots that shows the `total_bill` for each `day` of the week. Within each facet, differentiate between `Dinner` and `Lunch` times using the `hue` parameter. 5. **Customized Plot** - Customize one of the plots from above by setting the marker style, size, jitter, and transparency (alpha) to your preference to make the plot more visually appealing and informative. Expected Output For each task, write code to generate the required plots. Ensure to include titles for each plot and axis labels for clarity. Constraints - You must use `seaborn` for all visualizations. - Ensure that each plot is self-contained and easily interpretable. - Properly comment your code to explain what each segment does. Example Output Below is an example of what one of the plot outputs might look like for Task 3: ```python import seaborn as sns import matplotlib.pyplot as plt # Load the tips dataset tips = sns.load_dataset(\\"tips\\") # Create the strip plot plt.figure(figsize=(10, 6)) sns.stripplot(data=tips, x=\\"total_bill\\", y=\\"day\\", hue=\\"sex\\", dodge=True, jitter=False) plt.title(\'Total Bill vs Day with Gender Separation\') plt.xlabel(\'Total Bill\') plt.ylabel(\'Day\') plt.show() ``` Provide similar code snippets for Tasks 1, 2, 4, and 5, ensuring all aspects from the instructions are covered based on the provided dataset.","solution":"import seaborn as sns import matplotlib.pyplot as plt # Load the tips dataset tips = sns.load_dataset(\\"tips\\") # Task 1: Basic Univariate Visualization def plot_total_bill_distribution(): plt.figure(figsize=(10, 6)) sns.stripplot(data=tips, x=\\"total_bill\\") plt.title(\'Distribution of Total Bill\') plt.xlabel(\'Total Bill\') plt.show() # Task 2: Comparing Categories def plot_total_bill_by_day(): plt.figure(figsize=(10, 6)) sns.stripplot(data=tips, x=\\"day\\", y=\\"total_bill\\", hue=\\"day\\") plt.title(\'Total Bill by Day of the Week\') plt.xlabel(\'Day\') plt.ylabel(\'Total Bill\') plt.show() # Task 3: Enhanced Categorical Comparison def plot_total_bill_by_day_and_sex(): plt.figure(figsize=(10, 6)) sns.stripplot(data=tips, x=\\"day\\", y=\\"total_bill\\", hue=\\"sex\\", dodge=True, jitter=False) plt.title(\'Total Bill by Day and Gender\') plt.xlabel(\'Day\') plt.ylabel(\'Total Bill\') plt.show() # Task 4: Multidimensional Visualization def plot_total_bill_by_time_and_day(): g = sns.catplot(data=tips, x=\\"day\\", y=\\"total_bill\\", hue=\\"time\\", kind=\\"strip\\", height=6, aspect=1.5) g.fig.suptitle(\'Total Bill by Day and Time (Dinner/Lunch)\') g.set_axis_labels(\\"Day\\", \\"Total Bill\\") plt.show() # Task 5: Customized Plot def plot_customized_total_bill(): plt.figure(figsize=(10, 6)) sns.stripplot(data=tips, x=\\"day\\", y=\\"total_bill\\", hue=\\"sex\\", dodge=True, jitter=True, marker=\'D\', size=8, alpha=0.6) plt.title(\'Customized Total Bill by Day and Gender\') plt.xlabel(\'Day\') plt.ylabel(\'Total Bill\') plt.show()"},{"question":"<|Analysis Begin|> The `uuid` module in Python provides functionality for generating both random and non-random universally unique identifiers (UUIDs), adhering to RFC 4122. This module includes the `UUID` class to create and manipulate UUIDs and functions such as `uuid1()`, `uuid3()`, `uuid4()`, and `uuid5()` to generate different versions of UUIDs: - `uuid1()`: Generates a UUID based on the current time and the system\'s network address. - `uuid3()`: Generates a UUID based on the MD5 hash of a namespace identifier and a name. - `uuid4()`: Generates a random UUID. - `uuid5()`: Generates a UUID based on the SHA-1 hash of a namespace identifier and a name. The `UUID` class allows for creating UUIDs from hex digits, bytes, fields, or an integer. It provides attributes to access the UUID in different formats, such as: - `hex`: 32-character hexadecimal string. - `bytes`: 16-byte string in big-endian order. - `bytes_le`: 16-byte string in little-endian order. - `fields`: A tuple of six integers that represent different components of the UUID. - `int`: A 128-bit integer representation of the UUID. - `urn`: A URN representation of the UUID as specified in RFC 4122. Additionally, the module provides `getnode()` to obtain the hardware (MAC) address, and constants defining namespace identifiers and UUID variants. The question should test the student\'s ability to utilize these functionalities effectively and create a complex function encapsulating various operations provided by the `uuid` module. <|Analysis End|> <|Question Begin|> # UUID Manipulation and Validation You are tasked with designing a function to generate and validate UUIDs based on specified criteria and perform some conversions and checks. Your function should demonstrate the use of different types of UUIDs and their various attributes. Function Specification Implement the function `generate_and_validate_uuids(n, namespace, name)`, where: - `n` (int) - The number of random UUIDs (`uuid4`) to generate. - `namespace` (str) - The namespace string to be used for generating `uuid3` and `uuid5` (use `uuid.NAMESPACE_DNS`). - `name` (str) - The name string to be used along with the namespace to generate `uuid3` and `uuid5`. Your function should: 1. Generate `n` random UUIDs using `uuid.uuid4()`. 2. Generate a UUID based on the MD5 hash of the provided `namespace` and `name` using `uuid.uuid3()`. 3. Generate a UUID based on the SHA-1 hash of the provided `namespace` and `name` using `uuid.uuid5()`. 4. Validate if each of the generated UUIDs conforms to the layout specified in RFC 4122. 5. Return a dictionary with the following structure: ```python { \'random_uuids\': [list of n UUIDs generated using uuid4], \'uuid3\': uuid_generated_using_uuid3, \'uuid5\': uuid_generated_using_uuid5, \'is_valid_uuid3\': True/False, \'is_valid_uuid5\': True/False, \'fields\': { \'random_uuids\': [list of fields tuples for each random uuid], \'uuid3_fields\': uuid3_fields_tuple, \'uuid5_fields\': uuid5_fields_tuple, } } ``` Example ```python import uuid def generate_and_validate_uuids(n, namespace, name): # Your code here # Example usage result = generate_and_validate_uuids(3, uuid.NAMESPACE_DNS, \'example.com\') print(result) ``` Constraints 1. `n` will be a positive integer and not exceed 20. 2. Ensure all the generated UUIDs are in the canonical form. 3. Your solution should be efficient and avoid unnecessary computations. Notes 1. Use the `uuid` module for all UUID operations. 2. Validate a UUID\'s conformity to RFC 4122 by checking its `variant` attribute; it should be `uuid.RFC_4122`. Implement the function accordingly and ensure it meets the above requirements.","solution":"import uuid def generate_and_validate_uuids(n, namespace, name): def is_valid_uuid(u): return u.variant == uuid.RFC_4122 # Generate n random UUIDs using uuid4 random_uuids = [uuid.uuid4() for _ in range(n)] # Generate UUID based on MD5 hash of namespace and name using uuid3 uuid3 = uuid.uuid3(namespace, name) # Generate UUID based on SHA-1 hash of namespace and name using uuid5 uuid5 = uuid.uuid5(namespace, name) # Validate UUIDs is_valid_uuid3 = is_valid_uuid(uuid3) is_valid_uuid5 = is_valid_uuid(uuid5) # Extract fields fields = { \'random_uuids\': [u.fields for u in random_uuids], \'uuid3_fields\': uuid3.fields, \'uuid5_fields\': uuid5.fields, } return { \'random_uuids\': random_uuids, \'uuid3\': uuid3, \'uuid5\': uuid5, \'is_valid_uuid3\': is_valid_uuid3, \'is_valid_uuid5\': is_valid_uuid5, \'fields\': fields }"},{"question":"# Question: Multi-layered Base Encoding and Decoding In this task, you need to implement two functions: `multi_encode` and `multi_decode`. The `multi_encode` function will encode a given bytes-like object through multiple layered encodings (Base16, Base32, Base64), and the `multi_decode` function will decode the data back to its original form using the reverse process. Function 1: `multi_encode(data: bytes) -> str` This function takes a bytes-like object `data` and applies the following encoding layers in order: 1. Base16 2. Base32 3. Base64 The final output should be a Base64 encoded ASCII string, representing the multiple layers of encoded data. Function 2: `multi_decode(encoded_data: str) -> bytes` This function takes a Base64 encoded ASCII string `encoded_data`, which represents data encoded through multiple layers in the order described above, and decodes it back to its original bytes form. # Requirements: 1. **Input/Output**: - `multi_encode` takes `data` (a bytes-like object) and returns a Base64 encoded string. - `multi_decode` takes `encoded_data` (a Base64 encoded string) and returns a bytes-like object. 2. **Encoding**: - First, encode the data using Base16. - Then, encode the Base16 result using Base32. - Finally, encode the Base32 result using Base64. 3. **Decoding**: - First, decode the data using Base64. - Then, decode the Base64 result using Base32. - Finally, decode the Base32 result using Base16. 4. **Constraints**: - The input to `multi_encode` will always be a valid bytes-like object. - The input to `multi_decode` will always be a valid Base64 encoded string produced by `multi_encode`. # Example: ```python def multi_encode(data: bytes) -> str: pass def multi_decode(encoded_data: str) -> bytes: pass # Example Usage: original_data = b\\"Programming is fun!\\" encoded_data = multi_encode(original_data) print(encoded_data) # Example output: \'MzJHRkJGNzJFM0IzMzJBMjhCMzMw\' decoded_data = multi_decode(encoded_data) print(decoded_data) # Output: b\'Programming is fun!\' ``` # Notes: - You may use the `base64` module and any necessary functions described in the provided documentation. - Ensure that the functions handle encoding and decoding in the specified order.","solution":"import base64 def multi_encode(data: bytes) -> str: # First encoding layer: Base16 base16_encoded = base64.b16encode(data) # Second encoding layer: Base32 base32_encoded = base64.b32encode(base16_encoded) # Third encoding layer: Base64 base64_encoded = base64.b64encode(base32_encoded) return base64_encoded.decode(\'ascii\') def multi_decode(encoded_data: str) -> bytes: # First decoding layer: Base64 base64_decoded = base64.b64decode(encoded_data) # Second decoding layer: Base32 base32_decoded = base64.b32decode(base64_decoded) # Third decoding layer: Base16 base16_decoded = base64.b16decode(base32_decoded) return base16_decoded"},{"question":"You are required to implement functions that demonstrate your understanding of Python310\'s instance and method objects. Specifically, you will create instance methods and methods, check their types, and manipulate them using the functions described in the provided documentation. This will test your ability to work with advanced concepts regarding callable objects in Python. Task 1: Create Instance Method Object Implement a function `create_instance_method(func)`, which takes a callable object `func` and returns a new instance method object using `PyInstanceMethod_New`. ```python def create_instance_method(func): Create a new instance method object. Parameters: func (callable): A callable object to be used as the instance method. Returns: PyObject: A new instance method object. # Your implementation here ``` Task 2: Check if Object is Instance Method Implement a function `is_instance_method(obj)`, which takes an object `obj` and returns `True` if the object is an instance method object, otherwise `False`, using `PyInstanceMethod_Check`. ```python def is_instance_method(obj): Check if the object is an instance method object. Parameters: obj (object): The object to check. Returns: bool: True if obj is an instance method object, False otherwise. # Your implementation here ``` Task 3: Retrieve Function from Instance Method Implement a function `get_function_from_instance_method(im)`, which takes an instance method object `im` and returns the function object associated with it using `PyInstanceMethod_Function`. ```python def get_function_from_instance_method(im): Retrieve the function object associated with the instance method. Parameters: im (PyObject): An instance method object. Returns: callable: The function object associated with the instance method. # Your implementation here ``` Task 4: Create Method Object Implement a function `create_method(func, self_obj)`, which takes a callable object `func` and an instance object `self_obj`, and returns a new method object using `PyMethod_New`. ```python def create_method(func, self_obj): Create a new method object. Parameters: func (callable): A callable object to be used as the method. self_obj (object): An instance object the method should be bound to. Returns: PyObject: A new method object. # Your implementation here ``` Task 5: Check if Object is Method Implement a function `is_method(obj)`, which takes an object `obj` and returns `True` if the object is a method object, otherwise `False`, using `PyMethod_Check`. ```python def is_method(obj): Check if the object is a method object. Parameters: obj (object): The object to check. Returns: bool: True if obj is a method object, False otherwise. # Your implementation here ``` Task 6: Retrieve Function from Method Implement a function `get_function_from_method(meth)`, which takes a method object `meth` and returns the function object associated with it using `PyMethod_Function`. ```python def get_function_from_method(meth): Retrieve the function object associated with the method. Parameters: meth (PyObject): A method object. Returns: callable: The function object associated with the method. # Your implementation here ``` Task 7: Retrieve Instance from Method Implement a function `get_instance_from_method(meth)`, which takes a method object `meth` and returns the instance object associated with it using `PyMethod_Self`. ```python def get_instance_from_method(meth): Retrieve the instance object associated with the method. Parameters: meth (PyObject): A method object. Returns: object: The instance object associated with the method. # Your implementation here ``` Constraints - You may assume that the input objects are valid and do not need type checking beyond the provided function checks. - You must use the provided functions and macros to implement the tasks. - Each function should not exceed a time complexity of O(1). Example Usage ```python def example_func(): print(\\"Example function\\") # Task 1 instance_method = create_instance_method(example_func) # Task 2 print(is_instance_method(instance_method)) # Should print: True # Task 3 print(get_function_from_instance_method(instance_method)) # Should print: <function example_func at ...> # Task 4 method = create_method(example_func, self_obj=your_object) # Task 5 print(is_method(method)) # Should print: True # Task 6 print(get_function_from_method(method)) # Should print: <function example_func at ...> # Task 7 print(get_instance_from_method(method)) # Should print: your_object ``` This question requires a strong understanding of Python\'s internals related to instance and method objects, ensuring a comprehensive assessment of the student\'s capabilities.","solution":"import types def create_instance_method(func): Create a new instance method object. Parameters: func (callable): A callable object to be used as the instance method. Returns: types.MethodType: A new instance method object. return types.MethodType(func, object()) def is_instance_method(obj): Check if the object is an instance method object. Parameters: obj (object): The object to check. Returns: bool: True if obj is an instance method object, False otherwise. return isinstance(obj, types.MethodType) def get_function_from_instance_method(im): Retrieve the function object associated with the instance method. Parameters: im (types.MethodType): An instance method object. Returns: callable: The function object associated with the instance method. return im.__func__ def create_method(func, self_obj): Create a new method object. Parameters: func (callable): A callable object to be used as the method. self_obj (object): An instance object the method should be bound to. Returns: types.MethodType: A new method object. return types.MethodType(func, self_obj) def is_method(obj): Check if the object is a method object. Parameters: obj (object): The object to check. Returns: bool: True if obj is a method object, False otherwise. return isinstance(obj, types.MethodType) def get_function_from_method(meth): Retrieve the function object associated with the method. Parameters: meth (types.MethodType): A method object. Returns: callable: The function object associated with the method. return meth.__func__ def get_instance_from_method(meth): Retrieve the instance object associated with the method. Parameters: meth (types.MethodType): A method object. Returns: object: The instance object associated with the method. return meth.__self__"},{"question":"**Objective**: Demonstrate understanding of the `2to3` tool and the `lib2to3` library by writing a function that processes Python files, applies specific fixers, and allows for custom fixer implementation. # Problem Statement You are provided with a directory containing multiple Python 2.x source files. Your task is to write a Python script that uses `2to3` to automatically convert these files to Python 3.x. Additionally, you will implement a custom fixer within the `lib2to3` framework to address a specific transformation not covered by the predefined fixers. **Requirements**: 1. Write a function `convert_to_python3(src_dir, dest_dir, custom_fixer)` that: - Takes a source directory `src_dir` with Python 2.x files. - Converts all `.py` files to Python 3.x using `2to3` with specified fixers. - Writes the converted files to a destination directory `dest_dir`. - Takes a `custom_fixer` function which applies an additional transformation not handled by the predefined fixers. 2. Implement the custom fixer to: - Convert all occurrences of `xrange` to `range`. - Ensure the custom fixer is integrated into the conversion process. 3. Output a summary log of all files processed and the changes made (both predefined and custom fixes). # Expected Input and Output **Input**: - `src_dir`: A string representing the path to the source directory containing Python 2.x files. - `dest_dir`: A string representing the path to the destination directory where the converted files will be saved. - `custom_fixer`: A function which will apply additional transformations to the code. **Output**: - Converted Python 3.x files in the destination directory. - A log output to the console detailing the files processed and changes made. # Constraints - Assume all input files have a `.py` extension. - The destination directory should maintain the same file structure as the source directory. - Handle errors gracefully, providing meaningful error messages. # Example Usage ```python def custom_fixer(code): # Custom logic to replace xrange with range return code.replace(\'xrange\', \'range\') convert_to_python3(\'python2_source\', \'python3_destination\', custom_fixer) ``` This function should read Python 2.x files from `python2_source`, convert them using `2to3` with standard fixers, apply the custom fixer for `xrange` to `range`, and write the converted files to `python3_destination`. It should also print a log of changes made. # Evaluation Criteria 1. Correctness of the transformation. 2. Proper implementation of the custom fixer. 3. Clarity and readability of code. 4. Handling of edge cases and errors.","solution":"import os import shutil from lib2to3.refactor import RefactoringTool, get_fixers_from_package from lib2to3.pgen2 import driver, token from lib2to3.pgen2.parse import ParseError def convert_to_python3(src_dir, dest_dir, custom_fixer): \'\'\' Converts Python 2.x files in src_dir to Python 3.x files in dest_dir using 2to3 with specific fixers and an additional custom fixer provided. \'\'\' # Get fixers from the standard 2to3 package fixers = get_fixers_from_package(\'lib2to3.fixes\') # Initialize RefactoringTool with standard fixers refactor_tool = RefactoringTool(fixers) # Make sure the destination directory exists if not os.path.exists(dest_dir): os.makedirs(dest_dir) # Process each file in the source directory for root, _, files in os.walk(src_dir): for file in files: if file.endswith(\'.py\'): src_file_path = os.path.join(root, file) dest_file_path = os.path.join(dest_dir, os.path.relpath(src_file_path, src_dir)) dest_file_dir = os.path.dirname(dest_file_path) # Ensure destination sub-directory exists if not os.path.exists(dest_file_dir): os.makedirs(dest_file_dir) # Apply 2to3 conversion with open(src_file_path, \'r\', encoding=\'utf-8\') as f: source_code = f.read() try: tree = refactor_tool.refactor_string(source_code, src_file_path) refactored_code = str(tree) except (ParseError, SyntaxError) as e: print(f\\"Error refactoring {src_file_path}: {e}\\") continue # Apply custom fixer to refactored code final_code = custom_fixer(refactored_code) # Save the transformed code to the destination directory with open(dest_file_path, \'w\', encoding=\'utf-8\') as f: f.write(final_code) # Log the changes print(f\'Processed {src_file_path} -> {dest_file_path}\')"},{"question":"**Problem: Data Processing and File Management** You are provided with a text file containing a list of integers, each on a new line. Your task is to write a Python function called `process_file` that performs the following steps: 1. Read the integers from the input file. 2. Filter out the even numbers. 3. Sort the remaining odd numbers in descending order. 4. Write the sorted odd numbers to an output file, each number on a new line. Your function should have the following signature: ```python def process_file(input_file: str, output_file: str) -> None: pass ``` # Input - `input_file`: A string representing the path of the input file. - `output_file`: A string representing the path of the output file. # Output - The function writes the sorted odd numbers to the output file, each on a new line. # Constraints - You can assume that the input file contains at least one integer. - The integers in the input file can be positive or negative. - You must use built-in functions such as `open()`, `filter()`, `sorted()`, and `print()`. # Example Suppose the input file contains the following integers: ``` 7 2 5 8 3 12 9 11 6 14 1 ``` After processing, the output file should contain: ``` 11 9 7 5 3 1 ``` # Note - Ensure your function handles file reading and writing efficiently. - You should not use any external libraries; only standard Python built-in functions are allowed. Spend time thinking about the design of your solution, particularly how to use Python\'s built-in functions to make your code concise and efficient.","solution":"def process_file(input_file: str, output_file: str) -> None: # Step 1: Read integers from the input file with open(input_file, \'r\') as infile: numbers = [int(line.strip()) for line in infile] # Step 2: Filter out the even numbers odd_numbers = filter(lambda x: x % 2 != 0, numbers) # Step 3: Sort the remaining odd numbers in descending order sorted_odd_numbers = sorted(odd_numbers, reverse=True) # Step 4: Write the sorted odd numbers to the output file with open(output_file, \'w\') as outfile: for number in sorted_odd_numbers: outfile.write(f\\"{number}n\\")"},{"question":"**Objective:** Demonstrate your proficiency with the Seaborn library by utilizing the `seaborn.husl_palette` function to customize color palettes and apply them to a plot. **Question:** You are given a dataset containing information about different categories of flowers. For this assessment, you need to perform the following tasks: 1. **Generate Four Distinct Color Palettes:** - Palette 1: A default husl palette with 6 colors. - Palette 2: A husl palette with 8 colors and lightness set to 0.7. - Palette 3: A husl palette with 10 colors and saturation set to 0.3. - Palette 4: A husl palette with 10 colors and a starting hue of 0.5. 2. **Plot a Bar Plot:** Using one of the generated palettes, create a horizontal bar plot of the average petal length for each flower category. The dataset contains the following columns: \'Category\', \'PetalLength\'. The function signature should be: ```python import seaborn as sns import matplotlib.pyplot as plt import pandas as pd def plot_flower_data(data: pd.DataFrame): pass ``` **Input:** - `data`: A pandas DataFrame containing at least the following columns: \'Category\' and \'PetalLength\'. **Output:** The function should display a horizontal bar plot showing the average petal length for each flower category using one of your generated color palettes. **Constraints:** 1. You must use the Seaborn library for plotting. 2. Ensure that the color palette used in the bar plot is visible and distinguishable for all categories. **Example Usage:** ```python import pandas as pd data = pd.DataFrame({ \'Category\': [\'Setosa\', \'Versicolor\', \'Virginica\', \'Setosa\', \'Versicolor\', \'Virginica\'], \'PetalLength\': [1.4, 4.7, 5.5, 1.3, 4.5, 5.3] }) plot_flower_data(data) ``` **Guidelines:** 1. Define four distinct color palettes as specified. 2. Compute the average petal length for each flower category. 3. Create a horizontal bar plot using one of the color palettes. **Notes:** You do not need to return the plot; displaying the plot is sufficient.","solution":"import seaborn as sns import matplotlib.pyplot as plt import pandas as pd def plot_flower_data(data: pd.DataFrame): # Generate the color palettes palette1 = sns.husl_palette(6) palette2 = sns.husl_palette(8, l=0.7) palette3 = sns.husl_palette(10, s=0.3) palette4 = sns.husl_palette(10, h=0.5) # Calculate the average petal length for each category avg_petal_length = data.groupby(\'Category\')[\'PetalLength\'].mean().reset_index() # Create a horizontal bar plot using one of the color palettes plt.figure(figsize=(10, 6)) sns.barplot(x=\'PetalLength\', y=\'Category\', data=avg_petal_length, palette=palette1) plt.title(\'Average Petal Length by Flower Category\') plt.xlabel(\'Average Petal Length\') plt.ylabel(\'Category\') plt.show()"},{"question":"# Question: Implement a Spectrogram Generator using PyTorch A spectrogram is a visual representation of the spectrum of frequencies of a signal as it varies with time. This assessment requires you to implement a function that computes the spectrogram of a given audio signal using the PyTorch `torch.fft` module. The spectrogram will be computed by segmenting the signal into overlapping windows, applying the FFT to each window, and plotting the magnitude of the resulting frequency-domain signals. Function Signature ```python import torch import matplotlib.pyplot as plt def generate_spectrogram(signal: torch.Tensor, window_size: int, hop_length: int, n_fft: int): Generate and plot the spectrogram of a given audio signal. Args: - signal (torch.Tensor): A 1D tensor representing the audio signal. - window_size (int): The size of each window for FFT. - hop_length (int): The number of samples to move for each window. - n_fft (int): The number of FFT points. Returns: - None: The function should display a plotted spectrogram. # Implement your solution here ``` Input and Output - **Input:** - `signal`: A 1D tensor of shape `(N,)` where `N` is the number of samples in the audio signal. - `window_size`: An integer representing the size of the window for each FFT. - `hop_length`: An integer representing the number of samples to move for each new window. - `n_fft`: An integer representing the number of points for FFT. - **Output:** - The function does not return any value. It should display a matplotlib spectrogram plot. Constraints and Limitations - Assume that the input signal tensor, `signal`, contains a sufficient number of samples for the spectrogram computation. - Handle edge cases where the signal\'s length is shorter than the `window_size`. - Aim for an efficient implementation by leveraging the PyTorch `torch.fft` module, avoiding unnecessary computations. # Example Usage ```python # Example signal signal = torch.randn(16000) # A random signal for testing purposes # Parameters for the spectrogram window_size = 400 hop_length = 160 n_fft = 512 # Generate and plot the spectrogram generate_spectrogram(signal, window_size, hop_length, n_fft) ``` # Performance Requirements - The implementation should be efficient and handle signals up to several minutes in length within a reasonable time (i.e., a few seconds). # Notes - Ensure the function is well-documented with comments explaining each step. - Students should have PyTorch and Matplotlib installed to test their implementation. - You may use any additional helper functions if necessary, but the core implementation should demonstrate an understanding of the `torch.fft` module.","solution":"import torch import matplotlib.pyplot as plt def generate_spectrogram(signal: torch.Tensor, window_size: int, hop_length: int, n_fft: int): Generate and plot the spectrogram of a given audio signal. Args: - signal (torch.Tensor): A 1D tensor representing the audio signal. - window_size (int): The size of each window for FFT. - hop_length (int): The number of samples to move for each window. - n_fft (int): The number of FFT points. Returns: - None: The function should display a plotted spectrogram. # Ensure signal is a 1D tensor assert signal.dim() == 1, \\"Signal should be a 1D tensor\\" # Get the number of frames required num_frames = (len(signal) - window_size) // hop_length + 1 # Initialize the spectrogram spectrogram = torch.zeros((num_frames, n_fft // 2 + 1)) for i in range(num_frames): start_index = i * hop_length end_index = start_index + window_size windowed_signal = signal[start_index:end_index] * torch.hann_window(window_size) spectrum = torch.fft.rfft(windowed_signal, n=n_fft) spectrogram[i, :] = torch.abs(spectrum) # Plot spectrogram plt.figure(figsize=(10, 6)) plt.imshow(spectrogram.T, aspect=\'auto\', origin=\'lower\', cmap=\'viridis\') plt.title(\'Spectrogram\') plt.xlabel(\'Time frame\') plt.ylabel(\'Frequency bin\') plt.colorbar(format=\'%+2.0f dB\') plt.show()"},{"question":"Context and Objective You are working with a dataset that contains integer values that may have missing data. You need to clean and process this dataset efficiently while ensuring that the missing values are handled correctly without converting the integer columns into floating-point columns. Your task is to implement functions for data cleaning and analysis using pandas\' `IntegerArray`. Problem Statement Given a DataFrame of students\' grades with various levels of missing data, perform the following tasks: 1. **Create a DataFrame**: Create a DataFrame with columns `\\"Math\\"`, `\\"Science\\"`, and `\\"English\\"` containing nullable integer values (i.e., some entries are `None`). Make sure the columns are of type nullable integers (`pd.Int64Dtype()`). 2. **Fill Missing Values**: Implement a function `fill_missing_grades` that takes this DataFrame and fills missing grades with the mean of the available grades in that subject, rounded to the nearest integer. 3. **Compute Average Grades**: Implement a function `average_grades` that calculates the average grade for each subject after filling missing values. 4. **Identify Top Students**: Implement a function `top_students` that identifies students who have all their grades above a given threshold after filling missing values. Input and Output Formats - **Function 1: create_grades_df** - **Input**: None - **Output**: `pd.DataFrame` with columns `\\"Math\\"`, `\\"Science\\"`, and `\\"English\\"` with `pd.Int64Dtype()` and some missing values. - **Function 2: fill_missing_grades** - **Input**: `pd.DataFrame` with nullable integer columns - **Output**: `pd.DataFrame` with missing values filled - **Function 3: average_grades** - **Input**: `pd.DataFrame` with filled missing values - **Output**: `pd.Series` with average grades for `\\"Math\\"`, `\\"Science\\"`, and `\\"English\\"` - **Function 4: top_students** - **Input**: - `pd.DataFrame` with filled missing values - Integer `threshold` - **Output**: `pd.DataFrame` with students having all grades above the given threshold Constraints - Use the `pd.Int64Dtype()` for integer columns with missing values. - Ensure that the functions are efficient and handle DataFrames with up to 100,000 rows. Example ```python import pandas as pd # Example DataFrame data = { \\"Math\\": [90, 80, None, 70], \\"Science\\": [None, 85, 75, 95], \\"English\\": [78, None, 88, 90] } df = pd.DataFrame(data, dtype=\\"Int64\\") # Function 1: Create a sample DataFrame (Here provided for illustration) def create_grades_df(): return df # Function 2: Fill missing grades def fill_missing_grades(df): df_filled = df.copy() for col in df.columns: mean_val = df_filled[col].mean(skipna=True) df_filled[col].fillna(round(mean_val), inplace=True) return df_filled # Function 3: Calculate average grades def average_grades(df): return df.mean() # Function 4: Identify top students def top_students(df, threshold): return df[df.ge(threshold).all(axis=1)] # Using the functions grades_df = create_grades_df() filled_grades_df = fill_missing_grades(grades_df) avg_grades = average_grades(filled_grades_df) top_students_df = top_students(filled_grades_df, 80) # Display results print(filled_grades_df) print(avg_grades) print(top_students_df) ``` Performance Requirement Ensure operations are performed quickly and efficiently on large datasets (up to 100,000 rows).","solution":"import pandas as pd def create_grades_df(): Create a DataFrame with columns \'Math\', \'Science\', and \'English\' containing nullable integer values. data = { \\"Math\\": pd.Series([90, 80, None, 70], dtype=\'Int64\'), \\"Science\\": pd.Series([None, 85, 75, 95], dtype=\'Int64\'), \\"English\\": pd.Series([78, None, 88, 90], dtype=\'Int64\') } df = pd.DataFrame(data) return df def fill_missing_grades(df): Fill missing grades in the DataFrame with the mean of available grades in that subject, rounded to the nearest integer. df_filled = df.copy() for col in df.columns: mean_val = df_filled[col].mean(skipna=True) df_filled[col].fillna(round(mean_val), inplace=True) return df_filled def average_grades(df): Calculate the average grade for each subject after filling missing values. return df.mean() def top_students(df, threshold): Identify students who have all their grades above a given threshold after filling missing values. return df[df.ge(threshold).all(axis=1)]"},{"question":"Objective Create and customize a series of categorical plots using the seaborn library to gain insights from a dataset. The following steps outline the tasks you need to complete. Tasks to Perform 1. **Load Dataset** - Load the \\"titanic\\" dataset using seaborn\'s `load_dataset` function. 2. **Basic Plot** - Create a strip plot showing the distribution of passengers\' ages across different classes (\\"class\\" column). 3. **Box Plot** - Create a box plot showing the distribution of passengers\' ages across different classes but differentiate the data by gender using the \\"sex\\" column. 4. **Violin Plot** - Create a violin plot similar to the box plot but with the following modifications: - Adjust the bandwidth to `bw_adjust=0.5`. - Do not cut the tails of the violins (`cut=0`). - Split the violins by gender within each class category. 5. **Bar Plot with Subplots** - Create a bar plot showing the survival rate of passengers across different classes, and generate subplots for each gender. 6. **Customizing Plots** - For each subplot in the bar plot, set the x-axis labels to [\\"Men\\", \\"Women\\", \\"Children\\"] and the y-axis label to \\"Survival Rate\\". - Set the titles to include the class name and variable. - Limit the y-axis to range from 0 to 1. - Remove the left spine of the plots. Constraints - Use seaborn version 0.11.0+. - Ensure all plots are correctly labeled and legible. Input and Output - **Input**: No direct input; dataset is loaded using seaborn. - **Output**: The resulting plots should be displayed inline (in a Jupyter Notebook or similar environment). Bonus Points - Use a FacetGrid to combine multiple categorical plots on a single grid. Example Code Structure ```python import seaborn as sns sns.set_theme(style=\\"whitegrid\\") # Task 1: Load the dataset df = sns.load_dataset(\\"titanic\\") # Task 2: Create a strip plot # (code to create and display the strip plot) # Task 3: Create a box plot # (code to create and display the box plot) # Task 4: Create a violin plot # (code to create and display the violin plot) # Task 5: Create a bar plot with subplots # (code to create and display the bar plot with subplots) # Task 6: Customizing the plots # (code to customize the plots) ``` Submission Submit your Jupyter Notebook or Python script containing the implemented tasks.","solution":"import seaborn as sns import matplotlib.pyplot as plt # Task 1: Load the dataset df = sns.load_dataset(\\"titanic\\") # Task 2: Create a strip plot plt.figure(figsize=(10, 6)) sns.stripplot(x=\\"class\\", y=\\"age\\", data=df) plt.title(\\"Age Distribution by Class\\") plt.xlabel(\\"Class\\") plt.ylabel(\\"Age\\") plt.show() # Task 3: Create a box plot plt.figure(figsize=(10, 6)) sns.boxplot(x=\\"class\\", y=\\"age\\", hue=\\"sex\\", data=df) plt.title(\\"Age Distribution by Class and Gender\\") plt.xlabel(\\"Class\\") plt.ylabel(\\"Age\\") plt.legend(title=\\"Sex\\") plt.show() # Task 4: Create a violin plot plt.figure(figsize=(10, 6)) sns.violinplot(x=\\"class\\", y=\\"age\\", hue=\\"sex\\", data=df, bw_adjust=0.5, cut=0, split=True) plt.title(\\"Violin Plot of Age Distribution by Class and Gender\\") plt.xlabel(\\"Class\\") plt.ylabel(\\"Age\\") plt.legend(title=\\"Sex\\") plt.show() # Task 5: Create a bar plot with subplots g = sns.catplot(data=df, x=\\"class\\", y=\\"survived\\", hue=\\"sex\\", kind=\\"bar\\", col=\\"who\\", ci=None, height=4, aspect=1) g.set_axis_labels(\\"Class\\", \\"Survival Rate\\") g.set_titles(\\"{col_name} {col_var}\\") g.set(ylim=(0, 1)) g.despine(left=True) plt.show()"},{"question":"# Asynchronous Task Management with the Asyncio Event Loop **Problem Statement**: You are required to implement a simple server using asyncio that handles multiple client connections concurrently. The server will perform the following tasks: 1. Accept incoming connections. 2. For each connection, spawn a new task to handle communication. 3. Listen for incoming messages, echo the received messages back to the clients, and keep track of the number of messages handled by each client. Specifically, you need to implement a class `AsyncServer` with the following methods: 1. `__init__(self, host: str, port: int)`: Initializes the server with the given host and port. 2. `start(self)`: Starts the server and begins accepting connections. This method should run the event loop until the server is manually stopped or an exception occurs. 3. `handle_client(self, reader: asyncio.StreamReader, writer: asyncio.StreamWriter)`: A coroutine that handles communication with a connected client. 4. `stop(self)`: Stops the server gracefully. **Input/Output Format**: - There are no input parameters directly passed to functions except during initialization. - The server should log the messages it receives and sends. Each log entry should include a timestamp, client address, and the content of the message. - When the server stops, it should log a summary of the number of messages it has handled for each client. **Constraints**: - The server should handle clients concurrently without blocking other tasks. - The server should handle up to 100 concurrent clients. - The server should be capable of running indefinitely until manually stopped or an error occurs. **Example Usage**: ```python import asyncio class AsyncServer: def __init__(self, host: str, port: int): self.host = host self.port = port self.message_counts = {} # Initialize the event loop self.loop = asyncio.get_event_loop() async def handle_client(self, reader: asyncio.StreamReader, writer: asyncio.StreamWriter): addr = writer.get_extra_info(\'peername\') self.message_counts[addr] = 0 print(f\\"Connection from {addr}\\") while True: data = await reader.read(100) message = data.decode() if not message: break self.message_counts[addr] += 1 print(f\\"Received {message} from {addr}\\") print(f\\"Send: {message}\\") writer.write(data) await writer.drain() print(f\\"Connection closed {addr}\\") writer.close() await writer.wait_closed() async def main(self): server = await asyncio.start_server(self.handle_client, self.host, self.port) async with server: await server.serve_forever() def start(self): Start the server try: self.loop.run_until_complete(self.main()) except KeyboardInterrupt: pass finally: self.stop() def stop(self): Stop the server gracefully for addr, count in self.message_counts.items(): print(f\\"Client {addr} sent {count} messages\\") self.loop.close() # Example instantiation and server start server = AsyncServer(\'127.0.0.1\', 8888) server.start() ``` **Notes**: - This implementation uses high-level asyncio abstractions for handling client-server interactions. - Performance considerations are addressed by leveraging asyncio\'s ability to manage multiple connections concurrently. - Proper logging and handling of asynchronous tasks illustrate advanced usage of the asyncio event loop.","solution":"import asyncio from datetime import datetime class AsyncServer: def __init__(self, host: str, port: int): self.host = host self.port = port self.message_counts = {} self.server = None async def handle_client(self, reader: asyncio.StreamReader, writer: asyncio.StreamWriter): addr = writer.get_extra_info(\'peername\') self.message_counts[addr] = 0 print(f\\"Connection from {addr} at {datetime.now()}\\") while True: data = await reader.read(100) if not data: break message = data.decode().strip() self.message_counts[addr] += 1 print(f\\"Received {message} from {addr} at {datetime.now()}\\") print(f\\"Send: {message}\\") writer.write(data) await writer.drain() print(f\\"Connection closed {addr} at {datetime.now()}\\") writer.close() await writer.wait_closed() async def start_server(self): self.server = await asyncio.start_server(self.handle_client, self.host, self.port) async with self.server: await self.server.serve_forever() def start(self): Start the server and run the event loop try: asyncio.run(self.start_server()) except KeyboardInterrupt: print(\\"Server stopped manually.\\") finally: self.stop() def stop(self): Stop the server gracefully for addr, count in self.message_counts.items(): print(f\\"Client {addr} sent {count} messages\\") if self.server: self.server.close() asyncio.run(self.server.wait_closed()) print(\\"Server shut down gracefully.\\") # Example usage without running it: # server = AsyncServer(\'127.0.0.1\', 8888) # server.start()"},{"question":"Objective Write a Python program that demonstrates comprehension of filesystem path handling and signal handling using the provided Python C API functions. Task Description 1. **Filesystem Path Handling**: - Write a function `get_filesystem_path(path)` that uses the `PyOS_FSPath` function to retrieve the filesystem representation of a given `path`. - The function should: - Take a single input `path`, which can be a string, bytes, or an object implementing the `os.PathLike` interface. - Return the filesystem path as a string or bytes, depending on the input type. - Raise a `TypeError` if the input does not conform to the expected types. 2. **Signal Handling**: - Write a function `set_and_get_signal_handler(signal_number, handler_function)` that: - Sets a custom signal handler for a given signal number using `PyOS_setsig`. - Retrieves the current signal handler for the given signal number using `PyOS_getsig`. - Returns the previous signal handler. - The function should: - Take two inputs: `signal_number` (an integer representing the signal) and `handler_function` (a function reference to handle the signal). - Implement proper signal handling by utilizing the `PyOS_sighandler_t` type for the signal handlers. Function Signatures ```python def get_filesystem_path(path: Union[str, bytes, os.PathLike]) -> Union[str, bytes]: pass def set_and_get_signal_handler(signal_number: int, handler_function: Any) -> Any: pass ``` Constraints - Ensure you handle exceptions gracefully and follow the proper usage of the C API functions as described in the documentation. - Use appropriate imports and handle possible errors such as invalid paths or signal numbers. Example Usage ```python import os import signal def my_handler(signum, frame): print(f\\"Signal {signum} received\\") # Example 1: Filesystem Path Handling print(get_filesystem_path(\\"/tmp\\")) print(get_filesystem_path(b\\"/tmp\\")) print(get_filesystem_path(os.path)) # Example 2: Signal Handling previous_handler = set_and_get_signal_handler(signal.SIGUSR1, my_handler) print(previous_handler) ``` This question will test your ability to handle low-level system interactions and properly manage exceptions and errors in Python.","solution":"import os import signal from typing import Union, Any def get_filesystem_path(path: Union[str, bytes, os.PathLike]) -> Union[str, bytes]: if not isinstance(path, (str, bytes, os.PathLike)): raise TypeError(\\"Path must be a string, bytes, or an os.PathLike object\\") return os.fspath(path) def set_and_get_signal_handler(signal_number: int, handler_function: Any) -> Any: old_handler = signal.getsignal(signal_number) signal.signal(signal_number, handler_function) return old_handler"},{"question":"Coding Assessment Question # Objective Implement a custom Python interactive console that can store and execute code snippets. This interactive console should track the history of executed commands and display them upon request. # Requirements 1. **Class Definition**: Define a class `CustomConsole` inheriting from the `code.InteractiveConsole`. 2. **Execution and History Storage**: - Implement a method `run_code(self, code_snippet: str) -> None` to execute a given code snippet. - Store each code snippet in a list, maintaining the order of execution. 3. **Display History**: - Implement a method `display_history(self) -> str` to return the history of executed commands as a concatenated string, separated by newline characters. # Constraints - Handle exceptions that occur during code execution gracefully without stopping the interactive session. # Input - N/A (methods will be called directly) # Output - Methods will be directly invoked. `display_history` returns a string. # Example Usage ```python cc = CustomConsole() cc.run_code(\'print(\\"Hello World\\")\') cc.run_code(\'a = 5\') cc.run_code(\'print(a * 2)\') print(cc.display_history()) ``` Expected Output ``` print(\\"Hello World\\") a = 5 print(a * 2) ``` # Performance Requirements - Ensure that the history tracking mechanism is efficient and does not significantly impact the performance of the code execution. # Hints - You may find the `push` method of the `InteractiveConsole` class useful for executing code snippets. - Use a list to store the history of executed code snippets.","solution":"import code class CustomConsole(code.InteractiveConsole): def __init__(self): super().__init__() self.history = [] def run_code(self, code_snippet: str) -> None: Executes a given code snippet and stores it in the history. try: self.push(code_snippet) except Exception as e: print(f\\"Error: {e}\\") else: self.history.append(code_snippet) def display_history(self) -> str: Returns the history of executed commands as a concatenated string, separated by newline characters. return \'n\'.join(self.history)"},{"question":"# Custom Transformer with Scikit-learn In this exercise, you will create a custom transformer class using scikit-learn\'s transformer API. This question will assess your understanding of the fit, transform, and fit_transform methods as well as your ability to apply data preprocessing techniques. Problem Statement You need to implement a custom transformer named `LogTransformer` that will apply a logarithmic transformation to selected numeric features of a dataset. This is particularly useful when dealing with features that have exponential growth. Requirements 1. The `LogTransformer` class should inherit from `BaseEstimator` and `TransformerMixin` from the scikit-learn library. 2. The class should take a parameter `columns` which is a list of column names to which the log transformation will be applied. 3. Implement the following methods in the transformer: - `fit(self, X, y=None)`: This should just store the list of columns and return `self`. - `transform(self, X)`: This should apply the `np.log1p` transformation to the specified columns (to deal with zero and negative values use `np.log1p` which is `log(x+1)`). - `fit_transform(self, X, y=None)`: This should combine the functionalities of `fit` and `transform`. Input - A pandas DataFrame `data` which contains the dataset. - A list of column names `columns` to which the logarithmic transformation will be applied. Output - A pandas DataFrame with the specified columns transformed using the `np.log1p` function. Example ```python import pandas as pd import numpy as np from sklearn.base import BaseEstimator, TransformerMixin class LogTransformer(BaseEstimator, TransformerMixin): def __init__(self, columns): self.columns = columns def fit(self, X, y=None): return self def transform(self, X): X_transformed = X.copy() X_transformed[self.columns] = np.log1p(X_transformed[self.columns]) return X_transformed def fit_transform(self, X, y=None): return self.fit(X, y).transform(X) # Example usage data = pd.DataFrame({ \'feature1\': [1, 2, 3, 4], \'feature2\': [10, 100, 1000, 10000], \'feature3\': [5, 6, 7, 8] }) transformer = LogTransformer(columns=[\'feature1\', \'feature2\']) transformed_data = transformer.fit_transform(data) print(transformed_data) ``` Constraints - You should handle missing values in the columns by using `np.log1p`, which naturally deals with zero values. - Assume that the input DataFrame `data` always has numeric columns. # Note It is essential for you to ensure that your implementation is capable of handling larger datasets efficiently, considering the constraints provided above.","solution":"import pandas as pd import numpy as np from sklearn.base import BaseEstimator, TransformerMixin class LogTransformer(BaseEstimator, TransformerMixin): def __init__(self, columns): self.columns = columns def fit(self, X, y=None): return self def transform(self, X): X_transformed = X.copy() X_transformed[self.columns] = np.log1p(X_transformed[self.columns]) return X_transformed def fit_transform(self, X, y=None): return self.fit(X, y).transform(X)"},{"question":"# Email Policy Customization and Serialization Problem Statement You are working with the `email` package in Python and need to handle email messages in different scenarios. Your task is to write a function `process_email` that reads an email message from a file, applies specific modifications based on a policy, and then serializes the modified email to a new file. Requirements 1. **Reading an Email**: - Read an email from a given input file. - The email should be read using a specified policy. 2. **Modifying the Email**: - Add a custom header \\"X-Custom-Header\\" with a specified value. - Ensure that the maximum length of any line in the serialized output is restricted based on the policy. - Handle email defects according to the `raise_on_defect` attribute of the policy. 3. **Serializing the Email**: - Write the modified email to an output file using the same or a derived policy. Input - `input_filename` (str): The name of the input file containing the email message. - `output_filename` (str): The name of the output file where the modified email should be written. - `policy` (email.policy.Policy): The policy object to be used for reading and writing the email message. - `custom_header_value` (str): The value to be set for the \\"X-Custom-Header\\". Output This function should not return anything. Instead, it produces side effects by reading from `input_filename` and writing to `output_filename`. Function Signature ```python def process_email(input_filename: str, output_filename: str, policy: email.policy.Policy, custom_header_value: str) -> None: ``` # Example Usage ```python from email import policy from email.policy import EmailPolicy custom_policy = EmailPolicy(linesep=\'rn\', max_line_length=80, raise_on_defect=True) process_email(\'input_email.txt\', \'output_email.txt\', custom_policy, \'CustomHeaderValue\') ``` # Constraints - You must use the functionalities of the `email` package as described in the documentation. - Ensure that the specified policy is maintained throughout the reading and writing of the email message. - Handle any defects as specified by the policy. - The header \\"X-Custom-Header\\" should be added correctly to the message. # Note Make sure to handle the `with open` context correctly when dealing with file input and output operations to ensure that files are properly closed. This exercise aims to evaluate your understanding of: - Using and customizing policy objects. - Handling email message parsing and generation. - Applying constraints and handling defects as per specified policies. Good luck!","solution":"from email import policy from email.message import EmailMessage from email.parser import BytesParser from email.generator import BytesGenerator def process_email(input_filename: str, output_filename: str, policy: policy.Policy, custom_header_value: str) -> None: with open(input_filename, \'rb\') as input_file: # Read the email using the given policy msg = BytesParser(policy=policy).parse(input_file) # Add a custom header to the message msg[\'X-Custom-Header\'] = custom_header_value with open(output_filename, \'wb\') as output_file: # Write the email back to another file using the same policy generator = BytesGenerator(output_file, policy=policy) generator.flatten(msg)"},{"question":"**Question: HTML String Formatter** You are given a string that contains text with special characters that need to be safely embedded within HTML. Additionally, you may also receive a string that has HTML-encoded entities which need to be decoded back to their normal form. Your task is to write two functions, `convert_to_html_safe` and `convert_from_html_safe`, using the Python `html` module. **Function 1: convert_to_html_safe(text: str, include_quotes: bool) -> str** This function should transform the special characters (`&`, `<`, `>`, `\\"` and `\'`) in the input string `text` to their respective HTML-safe sequences using the `html.escape` function. - **Parameters:** - `text` (str): The input string containing the text to be HTML-safe encoded. - `include_quotes` (bool): A flag to determine if quotes (`\\"` and `\'`) should also be encoded. - **Returns:** - A string where special characters are substituted with their HTML-encoded equivalents. **Function 2: convert_from_html_safe(text: str) -> str** This function should convert all HTML-encoded entities in the input string `text` to their normal form using the `html.unescape` function. - **Parameters:** - `text` (str): The input string containing the HTML-safe encoded text. - **Returns:** - A string where HTML-encoded entities are converted back to their corresponding characters. **Constraints:** - The input strings will not exceed 1000 characters in length. - Only defined HTML5 standard character references will be present for decoding. - Performance should be optimized to handle the upper limit efficiently. **Examples:** ```python # Example 1. html_string = \'Hello & welcome to <Python> \\"world\\"\' encoded_string = convert_to_html_safe(html_string, include_quotes=True) print(encoded_string) # Output: \'Hello &amp; welcome to &lt;Python&gt; &quot;world&quot;\' # Example 2. encoded_string = \'Hello &amp; welcome to &lt;Python&gt; &#x22;world&#x22;\' decoded_string = convert_from_html_safe(encoded_string) print(decoded_string) # Output: \'Hello & welcome to <Python> \\"world\\"\' ``` Implement these functions ensuring they leverage the functionality provided by the `html` module for efficiency and reliability.","solution":"import html def convert_to_html_safe(text: str, include_quotes: bool) -> str: Transforms special characters in the input string to their HTML-safe sequences. return html.escape(text, quote=include_quotes) def convert_from_html_safe(text: str) -> str: Converts HTML-encoded entities in the input string to their normal form. return html.unescape(text)"},{"question":"Coding Assessment Question: # Objective You are required to demonstrate your understanding of PyTorch\'s quantization workflows by implementing the quantization of a simple neural network model. This includes preparation, converting the model, and evaluating its performance. # Question Implement a PyTorch model quantization workflow, following these steps: 1. **Define a Simple Neural Network model**: - Create a basic feed-forward neural network with at least one hidden layer. 2. **Prepare the model for quantization**: - Use relevant PyTorch classes and methods to prepare your model for quantization. 3. **Quantize the model**: - Convert the prepared model to a quantized version. 4. **Evaluate the Quantized Model**: - Write a function to evaluate the performance (accuracy, loss) of both the original and quantized models on a sample dataset (e.g., MNIST). # Specifications - **Input**: - None (the task requires to implement functions as described) - **Output**: - Print the performance metrics (accuracy and loss) of both the original and quantized models on the sample dataset. # Constraints: - Use PyTorch\'s `torch.ao.quantization` module. - Limit the training loop to 5 epochs. - Ensure that all necessary transformations, preparations, and evaluations are correctly implemented. # Performance Requirements: - Aim for minimal accuracy drop (<5%) between the original and quantized models. # Example: Below is an example structure you can follow for your implementation. You may need to fill in the necessary details. ```python import torch import torch.nn as nn import torch.optim as optim import torch.quantization import torchvision import torchvision.transforms as transforms # Step 1: Define a Simple Neural Network model class SimpleNN(nn.Module): def __init__(self): super(SimpleNN, self).__init__() self.fc1 = nn.Linear(28 * 28, 128) self.relu = nn.ReLU() self.fc2 = nn.Linear(128, 10) def forward(self, x): x = x.view(-1, 28 * 28) # Flatten the input x = self.fc1(x) x = self.relu(x) x = self.fc2(x) return x # Step 2: Function to evaluate model performance def evaluate_model(model, data_loader): model.eval() correct = 0 total = 0 criterion = nn.CrossEntropyLoss() loss = 0.0 with torch.no_grad(): for images, labels in data_loader: outputs = model(images) loss += criterion(outputs, labels).item() _, predicted = torch.max(outputs.data, 1) total += labels.size(0) correct += (predicted == labels).sum().item() accuracy = 100 * correct / total average_loss = loss / len(data_loader) return accuracy, average_loss # Additional implementation details: # - Loading the dataset # - Preparing the model for quantization # - Quantizing the model # - Evaluating both original and quantized model performance # Use relevant functions like: # Quantize a model: torch.quantization.quantize_dynamic or torch.quantization.convert # Prepare a model for quantization: torch.quantization.prepare or similar functions # Fuse modules: torch.quantization.fuse_modules or similar functions # Your task: Complete the implementation based on the documentation and example structure above. ``` Note: Ensure the environment is set up with the required PyTorch version that supports the quantization module mentioned above.","solution":"import torch import torch.nn as nn import torch.optim as optim import torch.quantization import torchvision import torchvision.transforms as transforms # Step 1: Define a Simple Neural Network model class SimpleNN(nn.Module): def __init__(self): super(SimpleNN, self).__init__() self.fc1 = nn.Linear(28 * 28, 128) self.relu = nn.ReLU() self.fc2 = nn.Linear(128, 10) def forward(self, x): x = x.view(-1, 28 * 28) # Flatten the input x = self.fc1(x) x = self.relu(x) x = self.fc2(x) return x # Step 2: Function to evaluate model performance def evaluate_model(model, data_loader): model.eval() correct = 0 total = 0 criterion = nn.CrossEntropyLoss() loss = 0.0 with torch.no_grad(): for images, labels in data_loader: outputs = model(images) loss += criterion(outputs, labels).item() _, predicted = torch.max(outputs.data, 1) total += labels.size(0) correct += (predicted == labels).sum().item() accuracy = 100 * correct / total average_loss = loss / len(data_loader) return accuracy, average_loss def main(): # Data loading and transformations transform = transforms.Compose([ transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,)) ]) train_dataset = torchvision.datasets.MNIST( root=\'./data\', train=True, transform=transform, download=True) test_dataset = torchvision.datasets.MNIST( root=\'./data\', train=False, transform=transform) train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True) test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False) # Initialize and train the original model model = SimpleNN() criterion = nn.CrossEntropyLoss() optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9) for epoch in range(5): # 5 epochs model.train() for images, labels in train_loader: optimizer.zero_grad() outputs = model(images) loss = criterion(outputs, labels) loss.backward() optimizer.step() orig_model_accuracy, orig_model_loss = evaluate_model(model, test_loader) print(f\'Original Model - Accuracy: {orig_model_accuracy:.2f}%, Loss: {orig_model_loss:.4f}\') # Step 3: Prepare the model for quantization model.qconfig = torch.quantization.get_default_qconfig(\'fbgemm\') model = torch.quantization.prepare(model) # Step 4: Convert to quantized model model = torch.quantization.convert(model) # Evaluate the quantized model quant_model_accuracy, quant_model_loss = evaluate_model(model, test_loader) print(f\'Quantized Model - Accuracy: {quant_model_accuracy:.2f}%, Loss: {quant_model_loss:.4f}\') if __name__ == \'__main__\': main()"},{"question":"# XML Data Processing and Transformation with ElementTree Problem Statement You are tasked with developing a Python function to parse, analyze, and transform XML data. The function should: 1. Parse a given XML string. 2. Find all elements with a specific tag. 3. Modify the text content of these elements based on certain criteria. 4. Construct a new XML string reflecting these changes. Function Signature ```python def transform_xml(xml_string: str, target_tag: str, update_function) -> str: Parses the input XML string, modifies elements with the target tag using the provided update function, and constructs a new XML string with these modifications. :param xml_string: A string representing the XML data to be processed. :param target_tag: The tag of the XML elements to be modified. :param update_function: A function that takes a string (the current text of the element) and returns a string (the updated text of the element). :return: A string representing the updated XML data. ``` Input - `xml_string`: A string representing the XML data to be processed. This string will always contain well-formed XML. - `target_tag`: A string representing the tag of elements that need to be found and modified. - `update_function`: A function that takes a string (the current text of the element) and returns a string (the updated text of the element). Output - A string representing the new XML document reflecting the modifications made to elements with the target tag. Constraints - The XML string will be between 1 KB and 1 MB in size. - The `update_function` will always return a valid string. - It should handle nested elements properly. Example ```python from xml.etree.ElementTree import ElementTree, fromstring, tostring def transform_xml(xml_string: str, target_tag: str, update_function) -> str: # Parse the input XML string tree = ElementTree(fromstring(xml_string)) root = tree.getroot() # Find and update elements for elem in root.iter(target_tag): elem.text = update_function(elem.text) # Convert the modified tree back to a string return tostring(root, encoding=\'unicode\') # Example usage xml_data = \'\'\' <root> <item>Original</item> <item>Content</item> <section> <item>Nested</item> </section> </root> \'\'\' def example_update_function(text): return text.upper() transformed_xml = transform_xml(xml_data, \\"item\\", example_update_function) print(transformed_xml) ``` Expected output: ```xml <root> <item>ORIGINAL</item> <item>CONTENT</item> <section> <item>NESTED</item> </section> </root> ``` In this example, the `example_update_function` converts the text of each `<item>` element to uppercase. The function `transform_xml` applies this transformation and returns the modified XML string.","solution":"import xml.etree.ElementTree as ET def transform_xml(xml_string: str, target_tag: str, update_function) -> str: Parses the input XML string, modifies elements with the target tag using the provided update function, and constructs a new XML string with these modifications. :param xml_string: A string representing the XML data to be processed. :param target_tag: The tag of the XML elements to be modified. :param update_function: A function that takes a string (the current text of the element) and returns a string (the updated text of the element). :return: A string representing the updated XML data. # Parse the input XML string root = ET.fromstring(xml_string) # Find and update elements for elem in root.iter(target_tag): elem.text = update_function(elem.text) # Convert the modified tree back to a string return ET.tostring(root, encoding=\'unicode\')"},{"question":"Unix System Resource Management Objective Write a Python function that monitors and reports the current resource usage limits for CPU time and memory for all processes run by the current user. This will require using the `resource` and `pwd` modules from the Unix-specific services documentation. Function Specification ```python def report_resource_usage(user_name: str) -> dict: Reports the resource usage limits for CPU time and memory for all processes run by a specified user. Parameters: user_name (str): The username for which to report resource usage limits. Returns: dict: A dictionary with the keys \'CPU Time Limit\' and \'Memory Limit\', each containing respective limits and usage. ``` Input - `user_name` (str): A string representing the username of a Unix system user. Output - Returns a dictionary with the following structure: ```python { \'CPU Time Limit\': {\'soft\': <soft_limit>, \'hard\': <hard_limit>, \'used\': <used_cpu_time>}, \'Memory Limit\': {\'soft\': <soft_limit>, \'hard\': <hard_limit>, \'used\': <used_memory>} } ``` Where `<soft_limit>` and `<hard_limit>` are the resource limits, and `<used_cpu_time>` and `<used_memory>` are the current usage values for CPU time and memory, respectively. Constraints - The function should handle cases where the username does not exist by raising a `ValueError` with a message \\"User does not exist\\". - If the system call fails for any reason, the function should handle it gracefully and return an empty dictionary. - The solution should be efficient in terms of time and space complexity. Example Usage ```python report_resource_usage(\\"testuser\\") # Example Output: # { # \'CPU Time Limit\': {\'soft\': 3600, \'hard\': 7200, \'used\': 1234}, # \'Memory Limit\': {\'soft\': 1048576, \'hard\': 2097152, \'used\': 512000} # } ``` Notes - You may use functions from the `resource` module such as `getrlimit`, `setrlimit`, and `getrusage`. - The `pwd` module can be utilized to fetch user information. - Make sure to account for all edge cases, including users without active processes or resource restrictions that cannot be fetched.","solution":"import resource import pwd import os def report_resource_usage(user_name: str) -> dict: Reports the resource usage limits for CPU time and memory for all processes run by a specified user. Parameters: user_name (str): The username for which to report resource usage limits. Returns: dict: A dictionary with the keys \'CPU Time Limit\' and \'Memory Limit\', each containing respective limits and usage. try: # Fetch user info user_info = pwd.getpwnam(user_name) except KeyError: raise ValueError(\\"User does not exist\\") uid = user_info.pw_uid cpu_time_limits = {\'soft\': None, \'hard\': None, \'used\': None} memory_limits = {\'soft\': None, \'hard\': None, \'used\': None} try: # Get resource limits for CPU time cpu_soft, cpu_hard = resource.getrlimit(resource.RLIMIT_CPU) cpu_time_used = resource.getrusage(resource.RUSAGE_SELF).ru_utime cpu_time_limits[\'soft\'] = cpu_soft cpu_time_limits[\'hard\'] = cpu_hard cpu_time_limits[\'used\'] = cpu_time_used # Get resource limits for memory mem_soft, mem_hard = resource.getrlimit(resource.RLIMIT_AS) memory_used = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss memory_limits[\'soft\'] = mem_soft memory_limits[\'hard\'] = mem_hard memory_limits[\'used\'] = memory_used except Exception as e: return {} # If any error occurs, return an empty dictionary. return { \'CPU Time Limit\': cpu_time_limits, \'Memory Limit\': memory_limits }"},{"question":"**Objective**: Implement a custom transformation using `torch.fx` that identifies and replaces all `torch.add` operations with `torch.mul` operations in a given `nn.Module`. # Task Description Write a function `replace_add_with_mul` that: 1. Takes an instance of a `torch.nn.Module` as input. 2. Uses the `torch.fx` module to trace the graph of the input module. 3. Iterates over the nodes in the graph: - Identifies nodes that perform `torch.add`. - Replaces those nodes with `torch.mul`. 4. Returns the transformed module. # API Specification ```python import torch import torch.fx def replace_add_with_mul(module: torch.nn.Module) -> torch.nn.Module: Transforms the given module by replacing all torch.add operations with torch.mul operations. Parameters: module (torch.nn.Module): The input module to transform. Returns: torch.nn.Module: A transformed module with torch.add operations replaced by torch.mul. pass ``` # Constraints and Requirements - The function should maintain the structural integrity of the original module. - Ensure to call `GraphModule.recompile` if you modify an existing `GraphModule`. - The input will always be a valid `torch.nn.Module`. # Example ```python import torch import torch.nn as nn class SimpleModule(nn.Module): def __init__(self): super(SimpleModule, self).__init__() def forward(self, x, y): return torch.add(x, y) # Create an instance of the module module = SimpleModule() # Transform the module transformed_module = replace_add_with_mul(module) # Verify transformation x = torch.randn(5, 5) y = torch.randn(5, 5) assert torch.allclose(transformed_module(x, y), x * y), \\"Transformation failed.\\" ``` # Notes - You can use `torch.fx.symbolic_trace` to obtain the graph of a module. - Use `isinstance(node.target, torch.add)` to identify `torch.add` nodes. - Ensure to handle any required imports and module initializations in your implementation. **Good Luck!**","solution":"import torch import torch.fx def replace_add_with_mul(module: torch.nn.Module) -> torch.nn.Module: Transforms the given module by replacing all torch.add operations with torch.mul operations. Parameters: module (torch.nn.Module): The input module to transform. Returns: torch.nn.Module: A transformed module with torch.add operations replaced by torch.mul. class AddToMulRewriter(torch.fx.Transformer): def call_function(self, target, args, kwargs): if target == torch.add: return self.call_function(torch.mul, args, kwargs) return super().call_function(target, args, kwargs) traced_module = torch.fx.symbolic_trace(module) rewriter = AddToMulRewriter(traced_module) transformed_module = rewriter.transform() return transformed_module"},{"question":"# Advanced Python Tracing with SystemTap **Objective:** Your task is to write a SystemTap script that traces the execution of a Python script\'s function calls and garbage collection activities. This problem will test your ability to understand and apply the given documentation on tracing tools and markers. **Problem Statement:** 1. **Function Call Tracing:** Write a SystemTap script named `trace_functions.stp` that: - Traces `function__entry` and `function__return` markers. - Outputs details of the function being entered or returned from, including the filename, function name, and line number, in a human-readable format. - Formats the output to show the time since the start of the script, PID, type of event (entry or return), filename, function name, and line number. 2. **Garbage Collection Tracing:** Extend your script to trace garbage collection events using the `gc__start` and `gc__done` markers. - Records when a garbage collection cycle starts and finishes. - Outputs the generation being collected at the start and the number of objects collected when done. **Input and Output:** - **Input:** There is no direct input. However, you should assume your script is run on a Python process executing a given Python script (`test_script.py`). - **Output:** - For function calls: ```plaintext [time_since_start] [PID] function-entry: [filename] [function_name] [line_number] [time_since_start] [PID] function-return: [filename] [function_name] [line_number] ``` - For garbage collection: ```plaintext [time_since_start] [PID] gc-start: generation [generation] [time_since_start] [PID] gc-done: collected [number_of_objects] ``` **Constraints:** - Assume you are running this on a Unix-based system. - Your script should be able to trace functions and garbage collection cycles from a Python script. **Example:** Assume `test_script.py` has some functions like `func1`, `func2`, and function calls that trigger garbage collection. Upon running: ```shell stap trace_functions.stp -c \\"python3.6 test_script.py\\" ``` The resulting output should include entries like: ```plaintext 151080408 function-entry: test_script.py func1 4 151080590 function-return: test_script.py func1 5 151080612 gc-start: generation 3 151181219 gc-done: collected 7 ``` **Submission:** Submit your `trace_functions.stp` script containing the required SystemTap probes and logic to produce the above output format.","solution":"def generate_function_signature(filename, function_name, line_number): return f\\"{filename}:{function_name}:{line_number}\\" def format_entry_event(time, pid, event_type, filename, function_name, line_number): timestamp = f\\"[{time}]\\" event_details = generate_function_signature(filename, function_name, line_number) return f\\"{timestamp} {pid} {event_type}: {event_details}\\" def format_gc_event(time, pid, event_type, generation=None, collected_objects=None): timestamp = f\\"[{time}]\\" if event_type == \\"gc-start\\": details = f\\"generation {generation}\\" elif event_type == \\"gc-done\\": details = f\\"collected {collected_objects}\\" return f\\"{timestamp} {pid} {event_type}: {details}\\""},{"question":"**Objective:** Implement a function that processes a list of named tuples, filters them based on specific conditions, and returns a summary information object. # Question **Background:** This exercise involves processing data using Python’s `tuple` and `namedtuple` that mimics Python\'s C API behavior (as described in the provided documentation). You are given a list of data representing employees in a company. Each employee’s data is represented as a named tuple containing four fields: `id`, `name`, `age`, and `department`. You need to implement a function that filters this list based on the employee’s age and returns a summary tuple containing the count of employees in each department. **Requirements:** 1. Define a named tuple type `Employee` with fields: `id`, `name`, `age`, and `department`. 2. Implement the function `filter_and_summarize_employees(data: List[Employee], min_age: int) -> Tuple[int, Dict[str, int]]`: - **Input:** - `data`: A list of `Employee` named tuples. - `min_age`: An integer representing the minimum age to filter employees. - **Output:** - A tuple containing: - Total count of employees older than or equal to `min_age`. - A dictionary where keys are department names and values are counts of employees in each department who are older than or equal to `min_age`. **Constraints:** - The id field will always be a positive integer. - The name field will always be a non-empty string. - The age field will always be a non-negative integer. - The department field will always be a non-empty string. **Function Signature:** ```python from collections import namedtuple from typing import List, Tuple, Dict Employee = namedtuple(\'Employee\', \'id name age department\') def filter_and_summarize_employees(data: List[Employee], min_age: int) -> Tuple[int, Dict[str, int]]: pass ``` **Example:** ```python data = [ Employee(1, \'Alice\', 30, \'HR\'), Employee(2, \'Bob\', 25, \'Engineering\'), Employee(3, \'Charlie\', 35, \'HR\'), Employee(4, \'David\', 40, \'Engineering\'), Employee(5, \'Eve\', 28, \'Sales\') ] min_age = 30 # Expected Output: (3, {\'HR\': 2, \'Engineering\': 1}) print(filter_and_summarize_employees(data, min_age)) ``` In the example above, there are three employees who are 30 years or older. Two of them are in the HR department, and one is in the Engineering department. --- Implement the `filter_and_summarize_employees` function as specified above. Ensure to follow best practices and consider performance implications where necessary.","solution":"from collections import namedtuple from typing import List, Tuple, Dict Employee = namedtuple(\'Employee\', \'id name age department\') def filter_and_summarize_employees(data: List[Employee], min_age: int) -> Tuple[int, Dict[str, int]]: Filters employees based on the minimum age and returns the count and department-wise summary. :param data: List of Employee named tuples. :param min_age: Minimum age to filter employees. :return: A tuple containing the total count of employees with age >= min_age and a dictionary with counts of employees in each department. filtered_employees = [employee for employee in data if employee.age >= min_age] department_summary = {} for employee in filtered_employees: if employee.department in department_summary: department_summary[employee.department] += 1 else: department_summary[employee.department] = 1 return len(filtered_employees), department_summary"},{"question":"**Coding Assessment Question** **Objective:** Demonstrate your understanding of Python\'s regular expressions module (`re`) by implementing a function that parses a complex string into meaningful components and modifies the string based on certain criteria. **Problem Statement:** You are given a string that represents a log entry. Each log entry is composed of multiple fields separated by specific delimiters. Your task is to write a function that extracts specific components from the log entry and returns a modified version of the log. **Input:** - A string representing a log entry. The string always starts with a timestamp, followed by the log level, message, and some metadata. Each component is separated by distinct delimiters. **Output:** - A string which replaces all email addresses in the log message with the text \\"[EMAIL PROTECTED]\\" and all IP addresses with \\"[IP ADDRESS]\\", and removes all metadata. **Constraints:** 1. The log entry will start with a timestamp in the format `YYYY-MM-DD HH:MM:SS`. 2. The log level will be one of the following: \\"INFO\\", \\"DEBUG\\", \\"WARNING\\", \\"ERROR\\". 3. The message can contain email addresses and IP addresses. 4. Email addresses follow the format: `username@example.com`. 5. IP addresses follow the format: `xxx.xxx.xxx.xxx` where `xxx` is a number from 0 to 255. 6. Metadata is any text after the message, enclosed in square brackets `[]`. **Function Signature:** ```python def parse_log_entry(log_entry: str) -> str: pass ``` **Example:** ```python # Input log_entry = \\"2023-10-05 12:34:56 ERROR User john.doe@example.com accessed from IP 192.168.1.10 [session=abc123, id=456]\\" # Call result = parse_log_entry(log_entry) # Output print(result) # \\"2023-10-05 12:34:56 ERROR User [EMAIL PROTECTED] accessed from IP [IP ADDRESS]\\" ``` **Explanation:** In the example above: - The email address `john.doe@example.com` is replaced with `[EMAIL PROTECTED]`. - The IP address `192.168.1.10` is replaced with `[IP ADDRESS]`. - The metadata `[session=abc123, id=456]` is removed. **Requirements:** 1. Use regular expressions to identify and replace email addresses and IP addresses. 2. Ensure that the function can handle logs with varying lengths of messages and metadata. 3. The project should effectively ignore the metadata and remove it from the output. **Hints:** - Use named groups in your regular expressions for better readability. - Utilize raw strings (prefix `r`) in your patterns to avoid issues with backslashes.","solution":"import re def parse_log_entry(log_entry: str) -> str: # Remove the metadata log_entry = re.sub(r\'[.*]\', \'\', log_entry).strip() # Replace email addresses log_entry = re.sub(r\'b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+.[A-Z|a-z]{2,}b\', \'[EMAIL PROTECTED]\', log_entry) # Replace IP addresses log_entry = re.sub(r\'b(?:d{1,3}.){3}d{1,3}b\', \'[IP ADDRESS]\', log_entry) return log_entry"},{"question":"Objective Demonstrate your understanding of the `uuid` module in Python by implementing functions to generate, validate, and compare UUIDs. Question **Part 1: UUID Generation** Write a function `generate_uuids` that generates a list of UUIDs. This function should accept two parameters: - `num_uuids` (int): The number of UUIDs to generate. - `version` (int): The UUID version to use (`1`, `3`, `4`, or `5`). If `version` is `3` or `5`, the function should also accept the following additional parameters: - `namespace` (str): The namespace for the UUID. Acceptable values are \'dns\', \'url\', \'oid\', and \'x500\'. - `name` (str): The name from which to generate the UUID. The function should return a list of generated UUIDs in their string representation. ```python def generate_uuids(num_uuids, version, namespace=None, name=None): Generate a list of UUIDs based on the specified version. Args: - num_uuids (int): Number of UUIDs to generate. - version (int): The UUID version to use (1, 3, 4, or 5). - namespace (str, optional): Namespace for version 3 or 5 UUIDs. Defaults to None. Acceptable values are \'dns\', \'url\', \'oid\', \'x500\'. - name (str, optional): Name for version 3 or 5 UUIDs. Defaults to None. Returns: - list: A list of generated UUID strings. pass ``` **Part 2: UUID Validation** Write a function `validate_uuid` that validates whether a given string is a valid UUID. The function should accept one parameter: - `uuid_string` (str): The string to validate. The function should return a boolean indicating whether the string is a valid UUID. ```python def validate_uuid(uuid_string): Validate whether the given string is a valid UUID. Args: - uuid_string (str): The string to validate. Returns: - bool: True if the string is a valid UUID, False otherwise. pass ``` **Part 3: UUID Comparison** Write a function `compare_uuids` that compares two UUIDs and returns: - `-1` if the first UUID is less than the second. - `1` if the first UUID is greater than the second. - `0` if both UUIDs are equal. The function should accept two parameters: - `uuid1` (str): The first UUID string. - `uuid2` (str): The second UUID string. ```python def compare_uuids(uuid1, uuid2): Compare two UUIDs. Args: - uuid1 (str): The first UUID string. - uuid2 (str): The second UUID string. Returns: - int: -1 if uuid1 < uuid2, 1 if uuid1 > uuid2, 0 if both are equal. pass ``` Constraints - The function `generate_uuids` should raise a `ValueError` if `version` is not one of `1`, `3`, `4`, or `5`. - The function `generate_uuids` should raise a `ValueError` if `namespace` is provided and is not one of \'dns\', \'url\', \'oid\', and \'x500\' for versions `3` and `5`. - The function `validate_uuid` should raise a `TypeError` if `uuid_string` is not of type `str`. - The function `compare_uuids` should raise a `TypeError` if either `uuid1` or `uuid2` is not of type `str`. Example Usage ```python # Example for generate_uuids uuids = generate_uuids(3, 4) print(uuids) # Example output: [\'16fd2706-8baf-433b-82eb-8c7fada847da\', ...] # Example for validate_uuid print(validate_uuid(\'16fd2706-8baf-433b-82eb-8c7fada847da\')) # True print(validate_uuid(\'invalid-uuid-string\')) # False # Example for compare_uuids result = compare_uuids(\'16fd2706-8baf-433b-82eb-8c7fada847da\', \'886313e1-3b8a-5372-9b90-0c9aee199e5d\') print(result) # -1, 1, or 0 depending on the comparison ```","solution":"import uuid def generate_uuids(num_uuids, version, namespace=None, name=None): Generate a list of UUIDs based on the specified version. Args: - num_uuids (int): Number of UUIDs to generate. - version (int): The UUID version to use (1, 3, 4, or 5). - namespace (str, optional): Namespace for version 3 or 5 UUIDs. Defaults to None. Acceptable values are \'dns\', \'url\', \'oid\', \'x500\'. - name (str, optional): Name for version 3 or 5 UUIDs. Defaults to None. Returns: - list: A list of generated UUID strings. Raises: - ValueError: If version is not one of 1, 3, 4, or 5. - ValueError: If namespace is provided and is not one of \'dns\', \'url\', \'oid\', \'x500\' for versions 3 and 5. if version not in {1, 3, 4, 5}: raise ValueError(\\"Version must be one of 1, 3, 4, or 5\\") namespaces = { \'dns\': uuid.NAMESPACE_DNS, \'url\': uuid.NAMESPACE_URL, \'oid\': uuid.NAMESPACE_OID, \'x500\': uuid.NAMESPACE_X500 } if version in {3, 5}: if namespace not in namespaces: raise ValueError(\\"Namespace must be one of \'dns\', \'url\', \'oid\', \'x500\'\\") if name is None: raise ValueError(\\"Name must be provided for version 3 and 5 UUIDs\\") uuids = [] for _ in range(num_uuids): if version == 1: uuids.append(str(uuid.uuid1())) elif version == 3: uuids.append(str(uuid.uuid3(namespaces[namespace], name))) elif version == 4: uuids.append(str(uuid.uuid4())) elif version == 5: uuids.append(str(uuid.uuid5(namespaces[namespace], name))) return uuids def validate_uuid(uuid_string): Validate whether the given string is a valid UUID. Args: - uuid_string (str): The string to validate. Returns: - bool: True if the string is a valid UUID, False otherwise. Raises: - TypeError: If uuid_string is not a str. if not isinstance(uuid_string, str): raise TypeError(\\"UUID string must be a str\\") try: uuid.UUID(uuid_string) return True except ValueError: return False def compare_uuids(uuid1, uuid2): Compare two UUIDs. Args: - uuid1 (str): The first UUID string. - uuid2 (str): The second UUID string. Returns: - int: -1 if uuid1 < uuid2, 1 if uuid1 > uuid2, 0 if both are equal. Raises: - TypeError: If either uuid1 or uuid2 is not a str. if not isinstance(uuid1, str) or not isinstance(uuid2, str): raise TypeError(\\"Both uuid1 and uuid2 must be strings\\") uuid_obj1 = uuid.UUID(uuid1) uuid_obj2 = uuid.UUID(uuid2) if uuid_obj1 < uuid_obj2: return -1 elif uuid_obj1 > uuid_obj2: return 1 else: return 0"},{"question":"**Objective:** Implement a function that takes a Python script, replaces all float literals with integer representations of their truncated values, and returns the modified script as a string. **Details:** - You will use the `tokenize` module to read and transform the tokens from the input script. - Identify tokens of type `NUMBER` that contain a float (indicated by the presence of a `.`). - Replace these float literals with their integer truncated values (i.e., discard the fractional part). - Ensure the resulting script preserves the original script\'s structure as much as possible. **Function Signature:** ```python def truncate_float_literals(script: str) -> str: pass ``` **Input:** - A single string `script` containing valid Python code. **Output:** - A single string representing the modified Python code. **Constraints:** - The input script is guaranteed to be syntactically valid Python code. - The function must handle scripts of arbitrary length and complexity. **Example:** ```python input_script = def example(): a = 3.14 b = -0.56 + 7.89 return a + b output_script = truncate_float_literals(input_script) # Expected output_script: def example(): a = 3 b = -0 + 7 return a + b ``` **Notes:** - Use the `tokenize` module\'s `tokenize` or `generate_tokens` functions to parse the input script. - Use the `untokenize` function to reconstruct the modified script from tokens. - Pay attention to preserving all other elements of the script, including whitespace and comments. Your implementation should demonstrate a strong understanding of the tokenization process, manipulation of token data, and reconstruction of source code.","solution":"import tokenize from io import BytesIO def truncate_float_literals(script: str) -> str: Replaces all float literals with integer representations of their truncated values in the given script. tokens = tokenize.tokenize(BytesIO(script.encode(\'utf-8\')).readline) result_tokens = [] for token in tokens: if token.type == tokenize.NUMBER: if \'.\' in token.string: truncated_value = int(float(token.string)) token = token._replace(string=str(truncated_value)) result_tokens.append(token) return tokenize.untokenize(result_tokens).decode(\'utf-8\')"},{"question":"**Objective**: Implement a custom SVM classifier and regressor using `scikit-learn` and demonstrate their usage in a classification and regression task respectively. # Question **Task 1: SVM Classifier** 1. Implement a function `train_and_evaluate_svm_classifier` that: - Takes the following inputs: - `X_train` (2D numpy array): Training data features. - `y_train` (1D numpy array): Training data labels. - `X_test` (2D numpy array): Test data features. - `y_test` (1D numpy array): Test data labels. - `kernel` (string): The kernel type to be used by the SVM classifier. - Trains an `SVC` classifier from `scikit-learn` using the specified kernel. - Evaluates the classifier on the test data using accuracy score. - Returns the accuracy score and the support vectors used by the classifier. ```python def train_and_evaluate_svm_classifier(X_train, y_train, X_test, y_test, kernel): # Your implementation here pass ``` **Task 2: SVM Regressor** 2. Implement a function `train_and_evaluate_svm_regressor` that: - Takes the following inputs: - `X_train` (2D numpy array): Training data features. - `y_train` (1D numpy array): Training data target values. - `X_test` (2D numpy array): Test data features. - `y_test` (1D numpy array): Test data target values. - `kernel` (string): The kernel type to be used by the SVM regressor. - Trains an `SVR` regressor from `scikit-learn` using the specified kernel. - Evaluates the regressor on the test data using mean squared error (MSE). - Returns the mean squared error and the support vectors used by the regressor. ```python def train_and_evaluate_svm_regressor(X_train, y_train, X_test, y_test, kernel): # Your implementation here pass ``` 3. Using the functions `train_and_evaluate_svm_classifier` and `train_and_evaluate_svm_regressor`, demonstrate their usage with toy datasets for classification and regression. You can use datasets generated using `make_classification` and `make_regression` from `sklearn.datasets`. # Example Usage ```python from sklearn.datasets import make_classification, make_regression from sklearn.model_selection import train_test_split # Generate toy classification dataset X, y = make_classification(n_samples=100, n_features=20, random_state=42) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Train and evaluate classifier accuracy, support_vectors = train_and_evaluate_svm_classifier(X_train, y_train, X_test, y_test, \'rbf\') print(\\"Classification Accuracy:\\", accuracy) print(\\"Support Vectors (Classifier):\\", support_vectors) # Generate toy regression dataset X, y = make_regression(n_samples=100, n_features=20, random_state=42) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Train and evaluate regressor mse, support_vectors = train_and_evaluate_svm_regressor(X_train, y_train, X_test, y_test, \'rbf\') print(\\"Regression Mean Squared Error:\\", mse) print(\\"Support Vectors (Regressor):\\", support_vectors) ``` # Constraints - You must use `scikit-learn`\'s `SVC` for classification and `SVR` for regression. - Ensure your implementation efficiently handles the datasets, including necessary scaling or preprocessing steps. - Clearly document your code and steps taken for each task.","solution":"from sklearn.svm import SVC, SVR from sklearn.metrics import accuracy_score, mean_squared_error from sklearn.preprocessing import StandardScaler import numpy as np def train_and_evaluate_svm_classifier(X_train, y_train, X_test, y_test, kernel): # Standardize the data scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) # Train the SVM classifier classifier = SVC(kernel=kernel) classifier.fit(X_train_scaled, y_train) # Evaluate the classifier y_pred = classifier.predict(X_test_scaled) accuracy = accuracy_score(y_test, y_pred) support_vectors = classifier.support_vectors_ return accuracy, support_vectors def train_and_evaluate_svm_regressor(X_train, y_train, X_test, y_test, kernel): # Standardize the data scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) # Train the SVM regressor regressor = SVR(kernel=kernel) regressor.fit(X_train_scaled, y_train) # Evaluate the regressor y_pred = regressor.predict(X_test_scaled) mse = mean_squared_error(y_test, y_pred) support_vectors = regressor.support_ return mse, support_vectors"},{"question":"# Question: Managing Virtual Environments and Packages You have been tasked with setting up a Python project environment for a new application. The application has specific dependency requirements, and you need to ensure that these dependencies are correctly installed and managed within their own virtual environment. Your task is to write a Python script that accomplishes the following steps: 1. **Create a Virtual Environment**: - The virtual environment should be named `my_project_env`. 2. **Activate the Virtual Environment**: - Ensure that the virtual environment is activated within the script. Note that actual activation might require shell-specific commands in a real scenario, which we simulate within the script. 3. **Install Required Packages**: - Using `pip`, install the following packages and versions specified: - `requests==2.25.1` - `numpy==1.19.5` 4. **List Installed Packages**: - After installation, list all the installed packages in the virtual environment using `pip list`. 5. **Freeze Dependences to a File**: - Save the list of installed packages and their versions to a file named `requirements.txt`. 6. **Output**: - Print the content of `requirements.txt` to the console. Expected Output The script should print the contents of `requirements.txt` with the packages and their versions accurately listed. Constraints: - The script must handle exceptions gracefully, providing meaningful error messages if any step fails. - Ensure that the script works cross-platform (Windows, macOS, Linux). Performance: - The script should perform operations in the correct sequence and handle package installation efficiently. ```python import os import subprocess import sys def create_virtual_environment(env_name): try: subprocess.check_call([sys.executable, \'-m\', \'venv\', env_name]) print(f\\"Virtual environment \'{env_name}\' created successfully.\\") except subprocess.CalledProcessError: print(f\\"Failed to create virtual environment \'{env_name}\'.\\") sys.exit(1) def install_packages(packages, env_name): try: env_bin_path = os.path.join(env_name, \'bin\', \'pip\') if os.name != \'nt\' else os.path.join(env_name, \'Scripts\', \'pip\') subprocess.check_call([env_bin_path, \'install\'] + packages) print(f\\"Packages {\', \'.join(packages)} installed successfully.\\") except subprocess.CalledProcessError: print(f\\"Failed to install packages {\', \'.join(packages)}.\\") sys.exit(1) def list_installed_packages(env_name): try: env_bin_path = os.path.join(env_name, \'bin\', \'pip\') if os.name != \'nt\' else os.path.join(env_name, \'Scripts\', \'pip\') installed_packages = subprocess.check_output([env_bin_path, \'list\']) print(\\"Installed packages:\\") print(installed_packages.decode(\'utf-8\')) except subprocess.CalledProcessError: print(f\\"Failed to list installed packages.\\") sys.exit(1) def freeze_installed_packages(env_name, filename): try: env_bin_path = os.path.join(env_name, \'bin\', \'pip\') if os.name != \'nt\' else os.path.join(env_name, \'Scripts\', \'pip\') with open(filename, \'w\') as f: freeze_output = subprocess.check_output([env_bin_path, \'freeze\']) f.write(freeze_output.decode(\'utf-8\')) print(f\\"Frozen package list saved to \'{filename}\'.\\") except subprocess.CalledProcessError: print(f\\"Failed to freeze installed packages to \'{filename}\'.\\") sys.exit(1) def print_file_content(filename): with open(filename, \'r\') as f: content = f.read() print(content) if __name__ == \'__main__\': env_name = \'my_project_env\' packages = [\'requests==2.25.1\', \'numpy==1.19.5\'] requirements_file = \'requirements.txt\' # Step 1: Create virtual environment create_virtual_environment(env_name) # Step 2: Install required packages install_packages(packages, env_name) # Step 3: List installed packages list_installed_packages(env_name) # Step 4: Freeze installed packages to a file freeze_installed_packages(env_name, requirements_file) # Step 5: Print content of requirements.txt print_file_content(requirements_file) ```","solution":"import os import subprocess import sys def create_virtual_environment(env_name): try: subprocess.check_call([sys.executable, \'-m\', \'venv\', env_name]) print(f\\"Virtual environment \'{env_name}\' created successfully.\\") except subprocess.CalledProcessError: print(f\\"Failed to create virtual environment \'{env_name}\'.\\") sys.exit(1) def install_packages(packages, env_name): try: env_bin_path = os.path.join(env_name, \'bin\', \'pip\') if os.name != \'nt\' else os.path.join(env_name, \'Scripts\', \'pip\') subprocess.check_call([env_bin_path, \'install\'] + packages) print(f\\"Packages {\', \'.join(packages)} installed successfully.\\") except subprocess.CalledProcessError: print(f\\"Failed to install packages {\', \'.join(packages)}.\\") sys.exit(1) def list_installed_packages(env_name): try: env_bin_path = os.path.join(env_name, \'bin\', \'pip\') if os.name != \'nt\' else os.path.join(env_name, \'Scripts\', \'pip\') installed_packages = subprocess.check_output([env_bin_path, \'list\']) print(\\"Installed packages:\\") print(installed_packages.decode(\'utf-8\')) except subprocess.CalledProcessError: print(f\\"Failed to list installed packages.\\") sys.exit(1) def freeze_installed_packages(env_name, filename): try: env_bin_path = os.path.join(env_name, \'bin\', \'pip\') if os.name != \'nt\' else os.path.join(env_name, \'Scripts\', \'pip\') with open(filename, \'w\') as f: freeze_output = subprocess.check_output([env_bin_path, \'freeze\']) f.write(freeze_output.decode(\'utf-8\')) print(f\\"Frozen package list saved to \'{filename}\'.\\") except subprocess.CalledProcessError: print(f\\"Failed to freeze installed packages to \'{filename}\'.\\") sys.exit(1) def print_file_content(filename): with open(filename, \'r\') as f: content = f.read() print(content) if __name__ == \'__main__\': env_name = \'my_project_env\' packages = [\'requests==2.25.1\', \'numpy==1.19.5\'] requirements_file = \'requirements.txt\' # Step 1: Create virtual environment create_virtual_environment(env_name) # Step 2: Install required packages install_packages(packages, env_name) # Step 3: List installed packages list_installed_packages(env_name) # Step 4: Freeze installed packages to a file freeze_installed_packages(env_name, requirements_file) # Step 5: Print content of requirements.txt print_file_content(requirements_file)"},{"question":"# XML Data Manipulation You are provided with an XML document containing information about different books in a library. The structure of the XML is as follows: ```xml <library> <book id=\\"1\\"> <title>Programming in Python</title> <author>John Doe</author> <year>2020</year> <genre>Programming</genre> <price>29.99</price> </book> <book id=\\"2\\"> <title>Data Science Essentials</title> <author>Jane Doe</author> <year>2019</year> <genre>Science</genre> <price>39.99</price> </book> <!-- More book elements --> </library> ``` # Task Implement a function `process_books(xml_string: str) -> str` that: 1. **Parses** the given XML string. 2. **Finds** all books published after the year 2018. 3. **Modifies** the price of these books by increasing it by 10%. 4. **Removes** books that belong to the genre \\"Programming\\". 5. Returns the modified XML as a string. # Function Signature ```python def process_books(xml_string: str) -> str: pass ``` # Input - `xml_string` (str): A string containing the XML data. # Output - Returns a string containing the modified XML data. # Constraints - The XML data will be well-formed. - There may be other books in the library. # Example **Input:** ```python xml_string = \'\'\' <library> <book id=\\"1\\"> <title>Programming in Python</title> <author>John Doe</author> <year>2020</year> <genre>Programming</genre> <price>29.99</price> </book> <book id=\\"2\\"> <title>Data Science Essentials</title> <author>Jane Doe</author> <year>2019</year> <genre>Science</genre> <price>39.99</price> </book> </library> \'\'\' print(process_books(xml_string)) ``` **Output:** ```xml <library> <book id=\\"2\\"> <title>Data Science Essentials</title> <author>Jane Doe</author> <year>2019</year> <genre>Science</genre> <price>43.99</price> </book> </library> ``` # Notes - Ensure the XML structure is preserved after modification. - Use the `xml.etree.ElementTree` module for parsing and manipulating the XML data.","solution":"import xml.etree.ElementTree as ET def process_books(xml_string: str) -> str: Parses the given XML string, finds all books published after the year 2018, modifies the price of these books by increasing it by 10%, removes books that belong to the genre \\"Programming\\", and returns the modified XML as a string. tree = ET.ElementTree(ET.fromstring(xml_string)) root = tree.getroot() # Find books published after 2018 and increase their price by 10% for book in root.findall(\'book\'): year = int(book.find(\'year\').text) if year > 2018: price = float(book.find(\'price\').text) new_price = price * 1.10 book.find(\'price\').text = f\'{new_price:.2f}\' # Remove books that belong to the genre \'Programming\' for book in root.findall(\'book\'): genre = book.find(\'genre\').text if genre == \'Programming\': root.remove(book) return ET.tostring(root, encoding=\'unicode\')"},{"question":"# Advanced Coding Assessment: Custom File Handler In this assessment, you are required to demonstrate your understanding of the `builtins` module in Python by creating a custom file handler. This file handler should provide specific modifications to the standard file operations provided by Python. # Objectives Your task is to implement a Python module with the following requirements: 1. **Custom Open Function**: - Implement a function `custom_open(path, mode)` that wraps Python\'s built-in `open` function to open a file. - The custom `open` function should ensure that all text read from the file is converted to upper case if the file is opened in read mode (`\'r\'` or `\'rt\'`). - If the file is opened in any other mode, it should behave exactly like the built-in `open` function. 2. **UpperCaser Class**: - Implement a class `UpperCaser` that wraps a file object. - The `UpperCaser` class should override the `read` and `readline` methods to return text in upper case. - The class should delegate all other methods to the wrapped file object without any changes. 3. **Testing the Implementation**: - Provide an example usage of your `custom_open` function and `UpperCaser` class to demonstrate their functionality. # Input and Output Formats - **`custom_open` function**: - Input: `path` (string), `mode` (string) - Output: Returns a file object (wrapped in `UpperCaser` if in read mode) - **`UpperCaser` class**: - Input: A file object (on initialization) - Methods: - `read(count=-1)`: - Input: `count` (integer) - Output: string (in upper case) - `readline(size=-1)`: - Input: `size` (integer) - Output: string (in upper case) # Constraints - The `UpperCaser` class only needs to handle text files. - You can assume that the input files are encoded in UTF-8. - You do not need to handle binary files or other encodings. - The `custom_open` function should only modify the behavior for the read modes as specified. # Example ```python import builtins def custom_open(path, mode): # Your implementation here pass class UpperCaser: def __init__(self, f): # Your implementation here pass def read(self, count=-1): # Your implementation here pass def readline(self, size=-1): # Your implementation here pass # Example Usage if __name__ == \\"__main__\\": with custom_open(\\"example.txt\\", \\"r\\") as f: print(f.read()) # Should print the content in upper case. with custom_open(\\"example.txt\\", \\"w\\") as f: f.write(\\"Hello, World!\\") # Should write \\"Hello, World!\\" to the file. ``` Ensure that your implementation passes the example usage test and think about edge cases such as empty files or large files.","solution":"import builtins def custom_open(path, mode): Custom open function that wraps Python\'s built-in open. Converts text to upper case if the file is opened in read mode. file = builtins.open(path, mode) if \'r\' in mode: return UpperCaser(file) return file class UpperCaser: Wraps a file object and converts all read text to upper case. def __init__(self, file): self.file = file def read(self, count=-1): return self.file.read(count).upper() def readline(self, size=-1): return self.file.readline(size).upper() def __getattr__(self, name): return getattr(self.file, name) def __enter__(self): return self def __exit__(self, exc_type, exc_value, traceback): self.file.close()"},{"question":"You are given a deprecated module \\"pipes\\" that allows creating and manipulating shell pipeline templates in Python. Even though the `pipes` module is deprecated, for the purpose of this exercise, you will be working with it to understand its integration with shell commands and file operations. Specifically, you need to implement a function `transform_file(input_file: str, output_file: str, commands: List[Tuple[str, str]]) -> None` that processes an input file by applying a sequence of transformation commands and writes the result to an output file. Each command is represented as a tuple containing a command string and a kind specifier. # Function Signature ```python from typing import List, Tuple def transform_file(input_file: str, output_file: str, commands: List[Tuple[str, str]]) -> None: pass ``` # Input Parameters: - `input_file` (str): The path to the input file. - `output_file` (str): The path to the output file. - `commands` (List[Tuple[str, str]]): A list of tuples, where each tuple contains: - A shell command (str) to be executed. - A kind specifier (str) that indicates how the command reads and writes (e.g., `\'-\'`, `\'f\'`, `\'.\'`). # Output: - The function does not return anything. It writes the transformed data to the `output_file`. # Constraints: - The provided commands will be valid and should be suitable for sequential execution. - The `input_file` exists and is readable. - The function will have necessary permissions to read the `input_file` and write to the `output_file`. # Example: ```python transform_file(\'input.txt\', \'output.txt\', [(\'tr a-z A-Z\', \'--\'), (\'sort\', \'--\')]) ``` In the above example, the function will: 1. Transform the text from `input.txt` to uppercase. 2. Sort the transformed text. 3. Write the sorted text to `output.txt`. # Notes: 1. You need to use the `pipes.Template` class to create and manage the pipeline. 2. It is important to handle file opening and closing properly. 3. Make sure to maintain the order of command execution as provided in the `commands` list.","solution":"import pipes from typing import List, Tuple def transform_file(input_file: str, output_file: str, commands: List[Tuple[str, str]]) -> None: Processes an input file by applying a sequence of shell commands and writes the result to an output file using the deprecated \'pipes\' module. template = pipes.Template() for command, kind in commands: template.append(command, kind) with template.open(input_file, \'r\') as input_stream: with open(output_file, \'w\') as output_stream: for line in input_stream: output_stream.write(line)"},{"question":"**Question: Implementation and Comparison of Kernel Ridge Regression** As a data scientist, you are tasked with implementing a Kernel Ridge Regression (KRR) model to fit a dataset and compare its performance against a Support Vector Regression (SVR) model. You are provided with a synthetic dataset that consists of a sinusoidal target function with added noise. **Objective:** 1. Implement and fit a Kernel Ridge Regression model on the provided dataset. 2. Implement and fit a Support Vector Regression model on the same dataset. 3. Compare the fitting time and prediction time of both models. 4. Evaluate and compare the mean squared error (MSE) of predictions from both models. **Steps:** 1. Load the provided dataset. The dataset consists of an array `X` (features) and an array `y` (targets). 2. Implement and fit the Kernel Ridge Regression model using the `KernelRidge` class from `sklearn.kernel_ridge`. 3. Implement and fit the Support Vector Regression model using the `SVR` class from `sklearn.svm`. 4. Use grid search to optimize hyperparameters for both models. Consider the following hyperparameters: - For KRR: Regularization parameter `alpha`, Kernel parameters (e.g., `gamma` for \'rbf\' kernel). - For SVR: Regularization parameter `C`, `epsilon` parameter, Kernel parameters (e.g., `gamma` for \'rbf\' kernel). 5. Measure and compare the fitting and prediction times for both models. 6. Compute the mean squared error (MSE) of predictions from both models on a test set. **Input:** - `X`: A 2D NumPy array of shape (n_samples, n_features) representing the input features. - `y`: A 1D NumPy array of shape (n_samples,) representing the target values. **Output:** - Print the time taken to fit each model. - Print the time taken to predict using each model. - Print the mean squared error (MSE) of predictions from both models. **Constraints:** - Use `GridSearchCV` for hyperparameter optimization. - For performance comparison, use the `time` library to measure fitting and prediction times. **Implementation:** ```python import numpy as np from sklearn.kernel_ridge import KernelRidge from sklearn.svm import SVR from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.metrics import mean_squared_error import time # Load the dataset (assuming it\'s provided as NumPy arrays X and y) # X = ... # y = ... # Split the dataset into training and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Define the parameter grid for Kernel Ridge Regression param_grid_krr = { \'alpha\': [1e-3, 1e-2, 1e-1, 1], \'kernel\': [\'rbf\'], \'gamma\': [1e-2, 1e-1, 1] } # Define the parameter grid for Support Vector Regression param_grid_svr = { \'C\': [1, 10, 100], \'epsilon\': [0.1, 0.2, 0.5], \'kernel\': [\'rbf\'], \'gamma\': [1e-2, 1e-1, 1] } # Kernel Ridge Regression with Grid Search krr_model = GridSearchCV(KernelRidge(), param_grid_krr, cv=5) start_time = time.time() krr_model.fit(X_train, y_train) krr_fit_time = time.time() - start_time start_time = time.time() krr_predictions = krr_model.predict(X_test) krr_predict_time = time.time() - start_time krr_mse = mean_squared_error(y_test, krr_predictions) # Support Vector Regression with Grid Search svr_model = GridSearchCV(SVR(), param_grid_svr, cv=5) start_time = time.time() svr_model.fit(X_train, y_train) svr_fit_time = time.time() - start_time start_time = time.time() svr_predictions = svr_model.predict(X_test) svr_predict_time = time.time() - start_time svr_mse = mean_squared_error(y_test, svr_predictions) # Output the results print(f\\"KRR fitting time: {krr_fit_time:.4f} seconds\\") print(f\\"KRR prediction time: {krr_predict_time:.4f} seconds\\") print(f\\"KRR Mean Squared Error: {krr_mse:.4f}\\") print(f\\"SVR fitting time: {svr_fit_time:.4f} seconds\\") print(f\\"SVR prediction time: {svr_predict_time:.4f} seconds\\") print(f\\"SVR Mean Squared Error: {svr_mse:.4f}\\") ``` **Note:** Make sure to have `scikit-learn` installed in your environment before running the code. You can install it using `pip install scikit-learn`.","solution":"import numpy as np from sklearn.kernel_ridge import KernelRidge from sklearn.svm import SVR from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.metrics import mean_squared_error import time def fit_and_compare_models(X, y): Fits Kernel Ridge Regression and Support Vector Regression on the given dataset, measures fitting and prediction times, and computes the mean squared error for each model. Parameters: X (numpy.ndarray): The input features, of shape (n_samples, n_features). y (numpy.ndarray): The target values, of shape (n_samples,). Returns: dict: A dictionary containing the fitting time, prediction time, and mean squared error for each model. # Split the dataset into training and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Define the parameter grid for Kernel Ridge Regression param_grid_krr = { \'alpha\': [1e-3, 1e-2, 1e-1, 1], \'kernel\': [\'rbf\'], \'gamma\': [1e-2, 1e-1, 1] } # Define the parameter grid for Support Vector Regression param_grid_svr = { \'C\': [1, 10, 100], \'epsilon\': [0.1, 0.2, 0.5], \'kernel\': [\'rbf\'], \'gamma\': [1e-2, 1e-1, 1] } results = {} # Kernel Ridge Regression with Grid Search krr_model = GridSearchCV(KernelRidge(), param_grid_krr, cv=5) start_time = time.time() krr_model.fit(X_train, y_train) krr_fit_time = time.time() - start_time start_time = time.time() krr_predictions = krr_model.predict(X_test) krr_predict_time = time.time() - start_time krr_mse = mean_squared_error(y_test, krr_predictions) results[\'KRR\'] = { \'fit_time\': krr_fit_time, \'predict_time\': krr_predict_time, \'mse\': krr_mse } # Support Vector Regression with Grid Search svr_model = GridSearchCV(SVR(), param_grid_svr, cv=5) start_time = time.time() svr_model.fit(X_train, y_train) svr_fit_time = time.time() - start_time start_time = time.time() svr_predictions = svr_model.predict(X_test) svr_predict_time = time.time() - start_time svr_mse = mean_squared_error(y_test, svr_predictions) results[\'SVR\'] = { \'fit_time\': svr_fit_time, \'predict_time\': svr_predict_time, \'mse\': svr_mse } return results"},{"question":"You are asked to simulate some low-level file handling functionalities in Python using high-level constructs available in the `io` module. The task is to implement a function `custom_file_operations` that performs three specific operations: 1. **Create a file object from a file descriptor**. 2. **Read a line from the file**. 3. **Write an object and a string to the file**. Here is the detailed specification: Function: `custom_file_operations(file_path, object_to_write, string_to_write)` - **Input:** - `file_path` (str): The path to the file that will be used for the operations. - `object_to_write` (Any): The object that will be written to the file using its `__str__()` method. - `string_to_write` (str): A string that will be written to the file directly. - **Output:** - (tuple): The function should return a tuple with three elements: - The first element is the first line read from the file if the file exists, else it should be an empty string. - The second and third elements should be `True` if the writing operations succeed; otherwise, they should be `False`. Constraints: - If the file does not exist initially, create a new one. - Ensure that the function handles exceptions gracefully and the file descriptor is properly closed. - Avoid using external libraries other than Python\'s built-in `io` module. Example: ```python def custom_file_operations(file_path, object_to_write, string_to_write): # Implement this function pass # Example call: result = custom_file_operations( \'/tmp/testfile.txt\', {\\"key\\": \\"value\\"}, \\"This is a test string\\" ) print(result) # Example output might be: (\'\', True, True) if the file didn\'t previously exist ``` Considerations: - Make sure to handle the case where the file might not already exist, and you have to create and initialize it. - Efficiently manage file descriptors to avoid resource leaks.","solution":"import io import os def custom_file_operations(file_path, object_to_write, string_to_write): Performs file operations: reading a line from a file, writing an object and a string to a file. Args: - file_path (str): Path to the file for operations. - object_to_write (Any): Object to write to the file using its __str__() method. - string_to_write (str): String to write to the file. Returns: - tuple: (first line read from the file, succeeded writing object, succeeded writing string) first_line = \'\' write_object_success = False write_string_success = False # Making sure file descriptor is always closed by using \'with\' statement try: with open(file_path, \'a+\') as f: # Move to the beginning of the file to read the first line f.seek(0) first_line = f.readline().strip() # Read the first line with open(file_path, \'w\') as f: try: f.write(str(object_to_write) + \'n\') write_object_success = True except Exception: write_object_success = False try: f.write(string_to_write + \'n\') write_string_success = True except Exception: write_string_success = False except Exception: pass return first_line, write_object_success, write_string_success"},{"question":"<|Analysis Begin|> The provided documentation describes the `torch.signal` module and its `windows` submodule within the PyTorch library, which is designed after SciPy\'s `signal` module. The `torch.signal.windows` module provides various window functions such as `bartlett`, `blackman`, `cosine`, `exponential`, `gaussian`, `general_cosine`, `general_hamming`, `hamming`, `hann`, `kaiser`, and `nuttall`. These window functions are commonly used in signal processing, particularly in the context of Fourier analysis, where they help in reducing spectral leakage. Knowing how to use these window functions correctly is important for tasks involving time-series analysis, signal filtering, and similar domains. Given this context, an appropriate coding question can be designed to assess the students\' understanding of these window functions and their practical application in signal processing. The question should involve creating a custom function to apply a chosen window function to a signal and analyze the results. <|Analysis End|> <|Question Begin|> # Coding Assessment Question **Objective:** Create a function to apply a window function from `torch.signal.windows` to a 1D signal and perform a Fourier transform on the windowed signal. The function should return the magnitude of the transformed signal. **Problem Statement:** You are given a 1D time-series signal represented as a PyTorch tensor and a window function name from the `torch.signal.windows` module. Your task is to implement a function `apply_window_and_transform` that applies the specified window function to the signal, performs a Fourier transform on the windowed signal, and returns the magnitude of the frequency components. **Function Signature:** ```python import torch import torch.fft def apply_window_and_transform(signal: torch.Tensor, window_function: str) -> torch.Tensor: Apply a window function to the input signal and perform a Fourier transform. Args: - signal (torch.Tensor): A 1D tensor representing the input signal. - window_function (str): The name of the window function to apply. Must be one of the following: \'bartlett\', \'blackman\', \'cosine\', \'exponential\', \'gaussian\', \'general_cosine\', \'general_hamming\', \'hamming\', \'hann\', \'kaiser\', \'nuttall\'. Returns: - torch.Tensor: The magnitude of the Fourier-transformed windowed signal. # Write your implementation here ``` **Input:** - `signal`: A 1D PyTorch tensor of shape `(N,)` where `N` is the number of samples in the time-series signal. - `window_function`: A string specifying which window function to apply from `torch.signal.windows`. **Output:** - A 1D PyTorch tensor representing the magnitude of the Fourier-transformed windowed signal. **Constraints:** - The length of the signal tensor, `N`, will be between `2` and `10^6`. - The `window_function` string will always be one of the valid window function names from `torch.signal.windows`. **Example:** ```python import torch import torch.signal.windows signal = torch.tensor([1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0]) window_function = \\"hann\\" # Expected output: Tensor containing the magnitude of the Fourier-transformed windowed signal result = apply_window_and_transform(signal, window_function) print(result) ``` **Notes:** - Make sure to use the `torch.signal.windows` module\'s window functions correctly. - Compute the Fourier transform using `torch.fft.fft`. - Return the magnitude of the Fourier-transformed signal using appropriate PyTorch operations.","solution":"import torch import torch.signal.windows def apply_window_and_transform(signal: torch.Tensor, window_function: str) -> torch.Tensor: Apply a window function to the input signal and perform a Fourier transform. Args: - signal (torch.Tensor): A 1D tensor representing the input signal. - window_function (str): The name of the window function to apply. Must be one of the following: \'bartlett\', \'blackman\', \'cosine\', \'exponential\', \'gaussian\', \'general_cosine\', \'general_hamming\', \'hamming\', \'hann\', \'kaiser\', \'nuttall\'. Returns: - torch.Tensor: The magnitude of the Fourier-transformed windowed signal. # Create the window based on the specified window_function window_function_map = { \'bartlett\': torch.signal.windows.bartlett, \'blackman\': torch.signal.windows.blackman, \'cosine\': torch.signal.windows.cosine, \'exponential\': torch.signal.windows.exponential, \'gaussian\': torch.signal.windows.gaussian, \'general_cosine\': torch.signal.windows.general_cosine, \'general_hamming\': torch.signal.windows.general_hamming, \'hamming\': torch.signal.windows.hamming, \'hann\': torch.signal.windows.hann, \'kaiser\': torch.signal.windows.kaiser, \'nuttall\': torch.signal.windows.nuttall } if window_function not in window_function_map: raise ValueError(f\\"Invalid window function: {window_function}\\") window = window_function_map[window_function](signal.size(0)) # Apply the window function to the signal windowed_signal = signal * window # Perform the Fourier transform transformed_signal = torch.fft.fft(windowed_signal) # Compute the magnitude of the Fourier-transformed signal magnitude = torch.abs(transformed_signal) return magnitude"},{"question":"**Title:** Customizing Seaborn Plot Configurations You are provided with a dataset and tasked with generating customized seaborn plots using the `so.Plot` interface. Your objective is to demonstrate proficiency in configuring plot theme and display options. Follow the instructions below to create and configure the plots. # Instructions 1. **Load the dataset:** - Load the built-in seaborn dataset `penguins`. 2. **Plot Customizations:** - Create a scatter plot showing the relationship between flipper length (`flipper_length_mm`) and body mass (`body_mass_g`). Color the points by species. 3. **Theme Configuration:** - Set the background color of the plot\'s axes to light gray (`#f0f0f0`). - Use the seaborn\'s `darkgrid` style to update the plot\'s theme. 4. **Display Configuration:** - Change the display format of the plot to `svg`. - Disable HiDPI scaling for the plot images. - Set the embedded image scaling factor to 0.8. 5. **Plot and Display:** - After completing all configurations, display the plot. # Expected Output - A scatter plot showing the correct relationships and styling as per the theme and display configurations. # Additional Notes - Ensure that the seaborn and matplotlib libraries are properly imported. - Use appropriate methods to verify that each configuration change is correctly applied. # Code Template ```python import seaborn as sns import matplotlib.pyplot as plt import seaborn.objects as so # Load dataset penguins = sns.load_dataset(\\"penguins\\") # Theme Configuration # Set background color of axes to light gray so.Plot.config.theme[\\"axes.facecolor\\"] = \\"#f0f0f0\\" # Update plot\'s theme to darkgrid style from seaborn import axes_style so.Plot.config.theme.update(axes_style(\\"darkgrid\\")) # Display Configuration # Set the format to svg so.Plot.config.display[\\"format\\"] = \\"svg\\" # Disable HiDPI scaling so.Plot.config.display[\\"hidpi\\"] = False # Set the image scaling factor to 0.8 so.Plot.config.display[\\"scaling\\"] = 0.8 # Create and display the scatter plot p = so.Plot(penguins, x=\\"flipper_length_mm\\", y=\\"body_mass_g\\", color=\\"species\\").add(so.Dot()) p.plot() ``` # Constraints - Ensure seaborn version is at least `0.11.0`. - Manipulate plot configurations strictly through provided seaborn methods.","solution":"import seaborn as sns import matplotlib.pyplot as plt import seaborn.objects as so # Load dataset penguins = sns.load_dataset(\\"penguins\\") # Theme Configuration # Set background color of axes to light gray so.Plot.config.theme[\\"axes.facecolor\\"] = \\"#f0f0f0\\" # Update plot\'s theme to darkgrid style from seaborn import axes_style so.Plot.config.theme.update(axes_style(\\"darkgrid\\")) # Display Configuration # Set the format to svg so.Plot.config.display[\\"format\\"] = \\"svg\\" # Disable HiDPI scaling so.Plot.config.display[\\"hidpi\\"] = False # Set the image scaling factor to 0.8 so.Plot.config.display[\\"scaling\\"] = 0.8 # Create and display the scatter plot p = so.Plot(penguins, x=\\"flipper_length_mm\\", y=\\"body_mass_g\\", color=\\"species\\").add(so.Dot()) p.show()"},{"question":"Question # Background You are tasked with creating an asynchronous task scheduler that can handle multiple tasks concurrently. Each task will have different completion times, some tasks may need to be shielded from cancellation, and others might have timeouts. You will implement several functions to manage these tasks using Python\'s asyncio library. # Problem Statement Implement the following functions: 1. **`async def run_tasks(tasks: List[Tuple[float, str]]) -> List[str]`**: - This function receives a list of tasks. Each task is represented as a tuple containing a delay (in seconds) and a message. - The function should concurrently run all the tasks, each task should await asyncio.sleep(delay) and then print the message. - Return a list of messages in the order they finish. 2. **`async def shielded_task(delay: float, message: str) -> str`**: - This function receives a delay and a message. - It awaits asyncio.sleep(delay) and returns the message. - The function should be shielded from cancellation. Even if the calling coroutine gets cancelled, the shielded task should complete normally. 3. **`async def run_with_timeout(task, timeout: float) -> str`**: - This function receives a task (coroutine) and a timeout. - The function should try to run the task and return its result if it completes within the timeout. Otherwise, it should raise asyncio.TimeoutError. 4. **`async def main()`**: - Create a list of multiple tasks with varying delays and messages. - Use `run_tasks` to execute them concurrently and print their results. - Create and execute a shielded task using `shielded_task`. - Create and execute a task with a timeout using `run_with_timeout`. Handle the timeout exception and print an appropriate message. # Input - The input to these functions will be within the specified formats. - You can assume all inputs are valid and do not require additional validation. # Output - The `main` function should print the results as described. # Example Here is an example of how these functions can be used: ```python import asyncio tasks = [(2, \\"Task 1\\"), (1, \\"Task 2\\"), (3, \\"Task 3\\")] async def example_main(): # Running tasks concurrently results = await run_tasks(tasks) print(results) # Output might vary based on completion order # Running a shielded task result = await shielded_task(2, \\"Shielded Task\\") print(result) # Running a task with timeout try: result = await run_with_timeout(shielded_task(5, \\"Timeout Task\\"), 2) print(result) except asyncio.TimeoutError: print(\\"Timeout occurred\\") asyncio.run(example_main()) ``` Implement and test your functions to match the expected behavior demonstrated in the example.","solution":"import asyncio from typing import List, Tuple async def run_tasks(tasks: List[Tuple[float, str]]) -> List[str]: async def task_runner(delay, message): await asyncio.sleep(delay) return message async_tasks = [task_runner(delay, message) for delay, message in tasks] completed_tasks = await asyncio.gather(*async_tasks) return completed_tasks async def shielded_task(delay: float, message: str) -> str: async def task(): await asyncio.sleep(delay) return message return await asyncio.shield(task()) async def run_with_timeout(task, timeout: float) -> str: return await asyncio.wait_for(task, timeout) async def main(): tasks = [(2, \\"Task 1\\"), (1, \\"Task 2\\"), (3, \\"Task 3\\")] results = await run_tasks(tasks) print(results) # Output might vary based on completion order shielded_result = await shielded_task(2, \\"Shielded Task\\") print(shielded_result) try: result = await run_with_timeout(shielded_task(5, \\"Timeout Task\\"), 2) print(result) except asyncio.TimeoutError: print(\\"Timeout occurred\\") if __name__ == \\"__main__\\": asyncio.run(main())"},{"question":"# Permutation Feature Importance Using scikit-learn You are provided with a dataset and a trained machine learning model. Your task is to implement a function to calculate and display permutation feature importance using the scikit-learn library. Your solution should: 1. Load and preprocess the dataset. 2. Train a specified model on the dataset. 3. Evaluate the model\'s performance on a validation set. 4. Compute permutation feature importances using a specified scoring metric. 5. Display the importance of each feature with their mean and standard deviation. # Dataset You can use any dataset from `sklearn.datasets`. For this task, use the `breast_cancer` dataset. # Model Use `RandomForestClassifier` from `sklearn.ensemble` with default parameters. # Function Signature ```python def compute_permutation_importance(random_state: int = 42, n_repeats: int = 30) -> None: Computes and displays permutation feature importance for RandomForestClassifier on the breast_cancer dataset. Args: - random_state (int): Random seed for reproducibility. - n_repeats (int): Number of times to shuffle each feature. Returns: - None: Outputs the feature importances to the console. ``` # Instructions 1. Load the `breast_cancer` dataset. 2. Split the dataset into training and validation sets using `train_test_split` where 80% data is for training and 20% is for validation. 3. Train a `RandomForestClassifier` model using the training data. 4. Evaluate the model\'s performance using the accuracy score on the validation data. 5. Compute the permutation feature importances using `permutation_importance` from `sklearn.inspection`. 6. Print the mean and standard deviation of the importances for each feature. # Example Output ``` Feature: mean importance +/- std deviation mean radius: 0.024 +/- 0.005 mean texture: 0.021 +/- 0.004 ... ``` # Constraints - Use library functions as much as possible. - Ensure reproducibility using the `random_state` parameter. - Handle large datasets efficiently within the scope of the problem. - Aim for clear, readable code with appropriate comments. # Notes - You do not need to return anything from the function. - The function should only print the results, nicely formatted. Good luck!","solution":"def compute_permutation_importance(random_state: int = 42, n_repeats: int = 30) -> None: Computes and displays permutation feature importance for RandomForestClassifier on the breast_cancer dataset. Args: - random_state (int): Random seed for reproducibility. - n_repeats (int): Number of times to shuffle each feature. Returns: - None: Outputs the feature importances to the console. from sklearn.datasets import load_breast_cancer from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score from sklearn.inspection import permutation_importance import numpy as np # Load dataset data = load_breast_cancer() X, y = data.data, data.target # Split dataset into training and validation sets X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=random_state) # Initialize and train model model = RandomForestClassifier(random_state=random_state) model.fit(X_train, y_train) # Evaluate model y_pred = model.predict(X_valid) accuracy = accuracy_score(y_valid, y_pred) print(f\'Model accuracy on validation set: {accuracy:.4f}\') # Compute permutation feature importances perm_importance = permutation_importance(model, X_valid, y_valid, n_repeats=n_repeats, random_state=random_state) # Get feature names feature_names = data.feature_names # Display the feature importances for i in perm_importance.importances_mean.argsort()[::-1]: print(f\'{feature_names[i]}: {perm_importance.importances_mean[i]:.3f} +/- {perm_importance.importances_std[i]:.3f}\')"},{"question":"# Password Verification System You are tasked with implementing a password verification system using the deprecated `crypt` module. The system should be capable of creating a user with a hashed password and verifying that a given password matches the stored hashed password. Requirements 1. **User Creation**: - Implement a function `create_user(username: str, password: str) -> dict`. - This function should create a new user with a hashed password using the strongest available hashing method (`crypt.methods[0]`). - The function should return a dictionary with the username as the key and the hashed password as the value. 2. **Password Verification**: - Implement a function `verify_password(username: str, password: str, users: dict) -> bool`. - This function should check if the given password matches the stored hashed password for the specified user. - Return `True` if the password matches, otherwise return `False`. 3. **Constant-Time Comparison**: - Use `hmac.compare_digest()` for constant-time comparisons to protect against timing attacks. Input and Output Formats 1. `create_user(username: str, password: str) -> dict` - **Input**: - `username`: A string representing the username. - `password`: A string representing the user\'s password. - **Output**: - A dictionary with the username as the key and the hashed password as the value. 2. `verify_password(username: str, password: str, users: dict) -> bool` - **Input**: - `username`: A string representing the username. - `password`: A string representing the user\'s password to be verified. - `users`: A dictionary containing usernames and their corresponding hashed passwords. - **Output**: - `True` if the password matches the stored hashed password, `False` otherwise. Constraints 1. Usernames are unique and there are no restrictions on length. 2. Passwords are plaintext strings with no restrictions on length. 3. The function `create_user` should always use the strongest available hashing method. 4. The function `verify_password` should use constant-time comparison for security. Example Usage ```python import crypt from hmac import compare_digest # Example functions as specified in the question def create_user(username: str, password: str) -> dict: hashed = crypt.crypt(password, crypt.mksalt(crypt.methods[0])) return {username: hashed} def verify_password(username: str, password: str, users: dict) -> bool: if username in users: return compare_digest(users[username], crypt.crypt(password, users[username])) return False # Creating a user users = create_user(\\"alice\\", \\"securepassword123\\") print(users) # {\'alice\': \'<hashed_password>\'} # Verifying the password is_verified = verify_password(\\"alice\\", \\"securepassword123\\", users) print(is_verified) # True is_verified = verify_password(\\"alice\\", \\"wrongpassword\\", users) print(is_verified) # False ``` Implement the above functions to create and verify users in a secure manner using the `crypt` module.","solution":"import crypt from hmac import compare_digest def create_user(username: str, password: str) -> dict: Creates a user with a hashed password using the strongest available hashing method. :param username: The username of the user. :param password: The plaintext password of the user. :return: A dictionary with the username as the key and the hashed password as the value. hashed = crypt.crypt(password, crypt.mksalt(crypt.methods[0])) return {username: hashed} def verify_password(username: str, password: str, users: dict) -> bool: Verifies if the given password matches the stored hashed password for the specified user. :param username: The username of the user. :param password: The plaintext password to be verified. :param users: A dictionary containing usernames and their corresponding hashed passwords. :return: True if the password matches the stored hashed password, False otherwise. if username in users: return compare_digest(users[username], crypt.crypt(password, users[username])) return False"},{"question":"# Function Bytecode Analyzer Problem Statement You are required to write a Python function, `analyze_function_bytecode(func: Callable) -> str`, that takes a callable object (like a function) as an input and returns a formatted string analyzing its bytecode, including the following details: - The human-readable operation names for each instruction. - The argument values and their descriptions for each instruction if available. - The resulting stack effect of each operation. Your implementation should utilize the `dis` module for analyzing the function\'s bytecode and return a human-readable string representation of the bytecode instructions. Input: - `func`: A Python callable (function or method). Output: - A string containing the formatted details of the bytecode, including operation names, arguments, and stack effects. Constraints: - Assume the input function is non-recursive and does not involve complex code structures like deeply nested functions. Performance Requirements: - The function should efficiently disassemble and format the bytecode such that it performs well for typical function sizes encountered in beginner to intermediate programming tasks. Example: ```python def analyze_function_bytecode(func: Callable) -> str: # Your implementation here # Test Function def test_func(x, y): return x + y result = analyze_function_bytecode(test_func) print(result) ``` Expected Output (formatted easier to read): ``` Instruction: LOAD_FAST, Argument: 0, Argument Description: x, Stack Effect: +1 Instruction: LOAD_FAST, Argument: 1, Argument Description: y, Stack Effect: +1 Instruction: BINARY_ADD, Argument: None, Argument Description: None, Stack Effect: -1 Instruction: RETURN_VALUE, Argument: None, Argument Description: None, Stack Effect: -1 ``` Use the `dis` module to extract and format the bytecode details, leveraging functions such as `dis.get_instructions`, `dis.Bytecode`, and `dis.stack_effect`.","solution":"import dis from typing import Callable def analyze_function_bytecode(func: Callable) -> str: Analyzes the bytecode of a given function and returns a formatted string with details of each bytecode instruction. instructions = dis.get_instructions(func) result = [] for instr in instructions: opname = instr.opname arg = instr.arg argval = instr.argval if instr.argval is not None else \'None\' stack_effect = dis.stack_effect(instr.opcode, instr.arg) if instr.arg is not None else dis.stack_effect(instr.opcode) result.append( f\\"Instruction: {opname}, Argument: {arg}, Argument Description: {argval}, Stack Effect: {stack_effect}\\" ) return \'n\'.join(result)"},{"question":"# Pandas Data Merging and Comparison Assessment Background You are given several DataFrames containing sales data, customer data, and product data. Your task is to write functions that merge these datasets in various ways and perform comparisons to identify discrepancies. Datasets Consider the following datasets: 1. **Sales Data** (`sales_df`): ```python sales_data = { \\"order_id\\": [1, 2, 3, 4], \\"customer_id\\": [101, 102, 103, 104], \\"product_id\\": [1001, 1002, 1001, 1003], \\"quantity\\": [1, 2, 1, 5], \\"order_date\\": [\\"2023-10-01\\", \\"2023-10-01\\", \\"2023-10-02\\", \\"2023-10-03\\"], } sales_df = pd.DataFrame(sales_data) ``` 2. **Customer Data** (`customers_df`): ```python customers_data = { \\"customer_id\\": [101, 102, 103, 105], \\"customer_name\\": [\\"Alice\\", \\"Bob\\", \\"Charlie\\", \\"David\\"], \\"join_date\\": [\\"2023-09-20\\", \\"2023-09-21\\", \\"2023-09-22\\", \\"2023-09-23\\"], } customers_df = pd.DataFrame(customers_data) ``` 3. **Product Data** (`products_df`): ```python products_data = { \\"product_id\\": [1001, 1002, 1003], \\"product_name\\": [\\"Widget\\", \\"Gizmo\\", \\"Doodad\\"], \\"price\\": [25.00, 35.00, 45.00], } products_df = pd.DataFrame(products_data) ``` Tasks 1. **Merge Sales and Customer Data**: Write a function `merge_sales_customers(sales_df, customers_df)` that performs an inner join between `sales_df` and `customers_df` on `customer_id`. The result should include all columns from both DataFrames. 2. **Merge Sales, Customer, and Product Data**: Write a function `merge_all_data(sales_df, customers_df, products_df)` that merges the sales, customers, and products DataFrames such that: - All sales are included in the result. - Customer and product details are included for each sale. 3. **Compare Sales Quantities**: Write a function `compare_sales_quantities_sales_v2(sales_df, sales_v2_df)` that compares quantities in the original sales DataFrame (`sales_df`) with an updated sales DataFrame (`sales_v2_df`). The function should return only the rows that have discrepancies in the quantity. ```python sales_v2_data = { \\"order_id\\": [1, 2, 3, 4, 5], \\"customer_id\\": [101, 102, 103, 104, 106], \\"product_id\\": [1001, 1002, 1001, 1003, 1004], \\"quantity\\": [1, 2, 2, 5, 3], \\"order_date\\": [\\"2023-10-01\\", \\"2023-10-01\\", \\"2023-10-02\\", \\"2023-10-03\\", \\"2023-10-04\\"], } sales_v2_df = pd.DataFrame(sales_v2_data) ``` Implementation ```python import pandas as pd def merge_sales_customers(sales_df, customers_df): Merges sales and customer DataFrames based on customer_id. Parameters: sales_df (pd.DataFrame): The sales data. customers_df (pd.DataFrame): The customer data. Returns: pd.DataFrame: Merged DataFrame with sales and customer details. pass def merge_all_data(sales_df, customers_df, products_df): Merges sales, customer, and product DataFrames. Parameters: sales_df (pd.DataFrame): The sales data. customers_df (pd.DataFrame): The customer data. products_df (pd.DataFrame): The product data. Returns: pd.DataFrame: Fully merged DataFrame with sales, customer, and product details. pass def compare_sales_quantities(sales_df, sales_v2_df): Compares two sales DataFrames and returns rows with discrepancies in quantities. Parameters: sales_df (pd.DataFrame): The original sales data. sales_v2_df (pd.DataFrame): The updated sales data. Returns: pd.DataFrame: DataFrame containing rows with different quantities between the two DataFrames. pass ``` Constraints - You may assume that `order_id`, `customer_id`, and `product_id` are unique within their respective DataFrames. - The function `compare_sales_quantities` should return a DataFrame with the same columns as `sales_df`. Evaluation Criteria - Correctness of the merge operations. - Proper handling of missing data (e.g., customers or products that don’t exist in the respective DataFrame). - Accurate identification of discrepancies between the two sales DataFrames.","solution":"import pandas as pd def merge_sales_customers(sales_df, customers_df): Merges sales and customer DataFrames based on customer_id. Parameters: sales_df (pd.DataFrame): The sales data. customers_df (pd.DataFrame): The customer data. Returns: pd.DataFrame: Merged DataFrame with sales and customer details. return pd.merge(sales_df, customers_df, on=\'customer_id\', how=\'inner\') def merge_all_data(sales_df, customers_df, products_df): Merges sales, customer, and product DataFrames. Parameters: sales_df (pd.DataFrame): The sales data. customers_df (pd.DataFrame): The customer data. products_df (pd.DataFrame): The product data. Returns: pd.DataFrame: Fully merged DataFrame with sales, customer, and product details. merged_df = pd.merge(sales_df, customers_df, on=\'customer_id\', how=\'inner\') merged_df = pd.merge(merged_df, products_df, on=\'product_id\', how=\'inner\') return merged_df def compare_sales_quantities(sales_df, sales_v2_df): Compares two sales DataFrames and returns rows with discrepancies in quantities. Parameters: sales_df (pd.DataFrame): The original sales data. sales_v2_df (pd.DataFrame): The updated sales data. Returns: pd.DataFrame: DataFrame containing rows with different quantities between the two DataFrames. merged_df = pd.merge(sales_df, sales_v2_df, on=\'order_id\', suffixes=(\'_original\', \'_v2\')) discrepancies_df = merged_df[merged_df[\'quantity_original\'] != merged_df[\'quantity_v2\']] columns = [\'order_id\', \'customer_id_original\', \'product_id_original\', \'quantity_original\', \'order_date_original\'] discrepancies_df = discrepancies_df[columns].rename(columns=lambda x: x.replace(\'_original\', \'\')) return discrepancies_df"}]'),D={name:"App",components:{PoemCard:I},data(){return{searchQuery:"",visibleCount:4,poemsData:A,isLoading:!1}},computed:{filteredPoems(){const i=this.searchQuery.trim().toLowerCase();return i?this.poemsData.filter(e=>e.question&&e.question.toLowerCase().includes(i)||e.solution&&e.solution.toLowerCase().includes(i)):this.poemsData},displayedPoems(){return this.searchQuery.trim()?this.filteredPoems:this.filteredPoems.slice(0,this.visibleCount)},hasMorePoems(){return!this.searchQuery.trim()&&this.visibleCount<this.poemsData.length}},methods:{async loadMore(){this.isLoading=!0,await new Promise(i=>setTimeout(i,1e3)),this.visibleCount+=4,this.isLoading=!1}}},z={class:"search-container"},F={class:"card-container"},R={key:0,class:"empty-state"},q=["disabled"],M={key:0},N={key:1};function L(i,e,l,m,n,o){const h=_("PoemCard");return a(),s("section",null,[e[4]||(e[4]=t("div",{class:"top-banner"},[t("div",{class:"top-banner-title"},[t("div",{class:"top-banner-title-text"},"🤔prompts chat🧠")])],-1)),t("div",z,[e[3]||(e[3]=t("span",{class:"search-icon"},"🔍",-1)),g(t("input",{type:"text",class:"search-input","onUpdate:modelValue":e[0]||(e[0]=r=>n.searchQuery=r),placeholder:"Search..."},null,512),[[y,n.searchQuery]]),n.searchQuery?(a(),s("button",{key:0,class:"clear-search",onClick:e[1]||(e[1]=r=>n.searchQuery="")}," ✕ ")):d("",!0)]),t("div",F,[(a(!0),s(b,null,v(o.displayedPoems,(r,f)=>(a(),w(h,{key:f,poem:r},null,8,["poem"]))),128)),o.displayedPoems.length===0?(a(),s("div",R,' No results found for "'+c(n.searchQuery)+'". ',1)):d("",!0)]),o.hasMorePoems?(a(),s("button",{key:0,class:"load-more-button",disabled:n.isLoading,onClick:e[2]||(e[2]=(...r)=>o.loadMore&&o.loadMore(...r))},[n.isLoading?(a(),s("span",N,"Loading...")):(a(),s("span",M,"See more"))],8,q)):d("",!0)])}const O=p(D,[["render",L],["__scopeId","data-v-7d6452a7"]]),Y=JSON.parse('{"title":"","description":"","frontmatter":{"page":true},"headers":[],"relativePath":"chatai/38.md","filePath":"chatai/38.md"}'),U={name:"chatai/38.md"},X=Object.assign(U,{setup(i){return(e,l)=>(a(),s("div",null,[k(O)]))}});export{Y as __pageData,X as default};
