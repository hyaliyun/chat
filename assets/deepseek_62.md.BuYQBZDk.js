import{_ as p,o as a,c as n,a as t,m as c,t as u,C as g,M as _,U as y,f as d,F as b,p as v,e as w,q as x}from"./chunks/framework.B1z0IdBH.js";const k={name:"PoemCard",props:{poem:{type:Object,required:!0}}},T={class:"poem-container"},C={class:"review"},E={class:"review-title"},P={class:"review-content"};function S(i,e,l,m,s,o){return a(),n("div",T,[t("div",C,[t("div",E,[e[0]||(e[0]=t("span",{class:"icon"},"question:",-1)),c(u(l.poem.question),1)]),e[2]||(e[2]=t("div",{class:"divider"},null,-1)),t("p",P,[e[1]||(e[1]=t("span",{class:"icon"},"answer:",-1)),c(u(l.poem.solution),1)])])])}const A=p(k,[["render",S],["__scopeId","data-v-0c2ce5a7"]]),I=JSON.parse('[{"question":"<|Analysis Begin|> The provided document outlines how to create built distributions in Python using the `bdist` command family in the distutils module. It includes detailed information about the various formats available for packaging and distributing Python modules, such as `gztar`, `zip`, and `rpm`, among others. It also explains how to cross-compile on Windows and the role of post-installation scripts. Key aspects covered in the document: 1. Understanding built distributions and their types (e.g., `gztar`, `bztar`, `zip`, `rpm`). 2. Using the `bdist` command with various formats and options. 3. Creating RPM packages and customizing the `.spec` file. 4. Cross-compiling on Windows platforms. 5. Writing and executing post-installation scripts in Windows installers. <|Analysis End|> <|Question Begin|> **Question: Creating Custom Built Distributions** You are tasked with developing a Python module distribution for a complex scientific library that needs to be easily installable on both Unix and Windows systems. Apart from the standard distributions (source code), the users require built distributions for easier installation without needing to compile the extensions manually. To develop a deeper understanding of Python\'s `bdist` commands in the `distutils` module, implement the following functions: 1. **create_built_distribution**: This function should create a built distribution for the current platform in the format specified by the user. 2. **create_rpm_distribution**: This function should create an RPM package with custom metadata specified by the user. # Function 1: create_built_distribution **Input:** - `format` (str): The format of the built distribution. Valid values are `\\"gztar\\"`, `\\"bztar\\"`, `\\"xztar\\"`, `\\"ztar\\"`, `\\"tar\\"`, `\\"zip\\"`, `\\"rpm\\"`, `\\"pkgtool\\"`, `\\"sdux\\"`, `\\"msi\\"`. **Output:** - A built distribution file in the specified format created in the `dist/` directory. **Constraints:** - The function should handle unsupported formats gracefully by raising a `ValueError` with an appropriate message. # Function 2: create_rpm_distribution **Input:** - `packager` (str): The name and email of the packager, e.g., `\\"John Doe <jdoe@example.org>\\"`. - `vendor` (str): The name of the vendor. - `distribution_name` (str): Name of the distribution. - `version` (str): Version of the distribution. **Output:** - A binary RPM package created in the `dist/` directory. **Constraints:** - Ensure that the `bdist_rpm` command is available. If not, handle the exception by printing an appropriate error message. # Example Usage: ```python def create_built_distribution(format): # Your implementation here pass def create_rpm_distribution(packager, vendor, distribution_name, version): # Your implementation here pass # Example calls create_built_distribution(\\"gztar\\") create_rpm_distribution(\\"John Doe <jdoe@example.org>\\", \\"Example Vendor\\", \\"example_distribution\\", \\"1.0.0\\") ``` # Notes: - Your implementation should utilize the `subprocess` module to execute the necessary `setup.py` commands. - The functions should print detailed messages about the steps being executed and any errors encountered. - Testing the functions requires a proper `setup.py` script in the current directory. Implementing the above functions will demonstrate your understanding of creating built distributions using Python\'s `distutils` module.","solution":"import subprocess import os def create_built_distribution(format): Create a built distribution for the current platform in the specified format. Parameters: format (str): The format of the built distribution. Valid values are \\"gztar\\", \\"bztar\\", \\"xztar\\", \\"ztar\\", \\"tar\\", \\"zip\\", \\"rpm\\", \\"pkgtool\\", \\"sdux\\", \\"msi\\". Raises: ValueError: If an unsupported format is provided. valid_formats = [\\"gztar\\", \\"bztar\\", \\"xztar\\", \\"ztar\\", \\"tar\\", \\"zip\\", \\"rpm\\", \\"pkgtool\\", \\"sdux\\", \\"msi\\"] if format not in valid_formats: raise ValueError(f\\"Unsupported format: {format}. Valid formats are {valid_formats}\\") try: result = subprocess.run( [\\"python\\", \\"setup.py\\", \\"bdist\\", f\\"--formats={format}\\"], capture_output=True, text=True, check=True ) print(result.stdout) except subprocess.CalledProcessError as e: print(\\"An error occurred during the build process.\\") print(e.stderr) def create_rpm_distribution(packager, vendor, distribution_name, version): Create an RPM package with custom metadata specified by the user. Parameters: packager (str): The name and email of the packager, e.g., \\"John Doe <jdoe@example.org>\\". vendor (str): The name of the vendor. distribution_name (str): Name of the distribution. version (str): Version of the distribution. try: result = subprocess.run( [\\"python\\", \\"setup.py\\", \\"bdist_rpm\\", f\\"--packager={packager}\\", f\\"--vendor={vendor}\\", f\\"--name={distribution_name}\\", f\\"--version={version}\\"], capture_output=True, text=True, check=True ) print(result.stdout) except subprocess.CalledProcessError as e: print(\\"An error occurred during the RPM package build process.\\") print(e.stderr) except FileNotFoundError: print(\\"The `bdist_rpm` command is not available. Please ensure you have rpm tools installed.\\")"},{"question":"# Seaborn Color Palette Generation and Visualization **Objective:** You are tasked with creating a function that generates color palettes from specified colormaps and visualizes them using seaborn. **Problem Statement:** Write a function `generate_and_visualize_palette` that takes the following inputs: 1. `palette_name` (str): Name of the colormap to generate the palette from (e.g., \\"viridis\\", \\"Set2\\"). 2. `num_colors` (int): Number of colors to generate from the palette. 3. `as_cmap` (bool): If True, return the continuous colormap instead of discrete samples. The function should: 1. Generate the color palette using `sns.mpl_palette`. 2. Visualize the generated palette using seaborn\'s built-in themes and display the colors on a bar plot. **Input:** - `palette_name` (str): The name of the colormap (e.g., \\"viridis\\", \\"Set2\\"). - `num_colors` (int): The number of colors to generate from the colormap. - `as_cmap` (bool): If True, return a continuous colormap; if False, return discrete samples (default is False). **Output:** - A visualization (bar plot) of the generated color palette. **Constraints:** - `palette_name` should be a valid colormap name recognized by seaborn. - `num_colors` should be a positive integer. - If `num_colors` is greater than the number of distinct colors in a qualitative colormap, only the available distinct colors are returned. **Example:** ```python def generate_and_visualize_palette(palette_name, num_colors, as_cmap=False): # Import necessary libraries import seaborn as sns import matplotlib.pyplot as plt # Generate the color palette palette = sns.mpl_palette(palette_name, num_colors, as_cmap=as_cmap) # Create a bar plot to visualize the colors plt.figure(figsize=(8, 2)) for i, color in enumerate(palette): plt.bar(i, 1, color=color) plt.xticks([]) plt.yticks([]) plt.title(f\\"{palette_name} Palette ({num_colors} colors)\\") plt.show() # Example usage: generate_and_visualize_palette(\\"viridis\\", 10) generate_and_visualize_palette(\\"Set2\\", 8) generate_and_visualize_palette(\\"viridis\\", 256, as_cmap=True) ``` # Note: - Your function should handle edge cases, like invalid palette names or non-positive values for `num_colors`, gracefully by raising appropriate exceptions or warnings. - Use seaborn\'s current theme settings for visualization.","solution":"import seaborn as sns import matplotlib.pyplot as plt def generate_and_visualize_palette(palette_name, num_colors, as_cmap=False): Generates a color palette from the specified colormap and visualizes it. Parameters: palette_name (str): Name of the colormap to generate the palette from. num_colors (int): Number of colors to generate from the palette. as_cmap (bool): If True, return the continuous colormap instead of discrete samples. Raises: ValueError: If `num_colors` is not a positive integer. ValueError: If `palette_name` is not a recognized colormap name. if not isinstance(num_colors, int) or num_colors <= 0: raise ValueError(\\"num_colors must be a positive integer\\") try: # Generate the color palette palette = sns.color_palette(palette_name, num_colors, as_cmap=as_cmap) except ValueError as e: raise ValueError(f\\"Invalid palette name: {palette_name}. Error: {str(e)}\\") # Create a bar plot to visualize the colors plt.figure(figsize=(8, 2)) if as_cmap: sns.heatmap([list(range(num_colors))]*10, cmap=palette, cbar=False) else: for i, color in enumerate(palette): plt.bar(i, 1, color=color) plt.xticks([]) plt.yticks([]) plt.title(f\\"{palette_name} Palette ({num_colors} colors)\\") plt.show()"},{"question":"# Email Handling and MIME Document Generation **Objective:** Design a Python function that parses a given raw email string to extract the necessary details, then re-constructs and sends the email by converting it to a MIME formatted document. This will demonstrate your understanding of email parsing, MIME document generation, and handling various components of emails such as headers and payloads. **Function Signature:** ```python def process_and_send_email(raw_email: str, new_recipient: str) -> str: Parses a given raw email string, modifies its recipient, and returns the MIME formatted email string. :param raw_email: A string representing the raw email to be parsed. :param new_recipient: A string representing the new recipient\'s email address. :return: A string representing the MIME formatted email. pass ``` **Input:** 1. `raw_email` (String): A raw email string containing headers and body, formatted as per RFC 5322. 2. `new_recipient` (String): The new recipient email address in the format `username@example.com`. **Output:** - Returns the re-constructed email as a MIME formatted string. **Constraints:** - The function should handle and preserve common email headers like `From`, `To`, `Subject`, `Date`, etc. - The function should correctly change the recipient to the `new_recipient`. - The function should ensure that the MIME structure of the email is valid. - The function should handle any charset encodings and Unicode characters correctly. **Example:** ```python raw_email = From: sender@example.com To: oldrecipient@example.com Subject: Test Email Date: Fri, 7 Jun 2021 10:35:01 -0000 Content-Type: text/plain; charset=\\"utf-8\\" This is a test email body. new_recipient = \\"newrecipient@example.com\\" formatted_email = process_and_send_email(raw_email, new_recipient) print(formatted_email) Expected Output: A MIME formatted email string where the \'To\' header is changed to \'newrecipient@example.com\' and the rest of the email remains unchanged. ``` **Notes:** - You may assume that the raw email input is always well-formed. - Ensure to handle MIME types and encoding accurately to avoid any data corruption. - Use the `email` and related sub-modules from the Python `email` package for handling email parsing and generation. Hints: - Use `email.parser` and `email.policy` for parsing the raw email. - Use `email.message.EmailMessage` for constructing the MIME formatted message. - Handle headers and payloads carefully while reconstructing the email.","solution":"from email import policy from email.parser import BytesParser from email.message import EmailMessage from io import BytesIO def process_and_send_email(raw_email: str, new_recipient: str) -> str: Parses a given raw email string, modifies its recipient, and returns the MIME formatted email string. :param raw_email: A string representing the raw email to be parsed. :param new_recipient: A string representing the new recipient\'s email address. :return: A string representing the MIME formatted email. # Parse the raw email raw_bytes = BytesIO(raw_email.encode(\'utf-8\')) msg = BytesParser(policy=policy.default).parse(raw_bytes) # Modify the recipient msg.replace_header(\'To\', new_recipient) # Create a new EmailMessage to construct the MIME formatted document email_message = EmailMessage() for header, value in msg.items(): email_message[header] = value email_message.set_content(msg.get_body().get_content()) # Return the MIME formatted email string return email_message.as_string()"},{"question":"# Quoted-Printable Encoding and Decoding You are required to implement two functions, `custom_encode(input_string: bytes, quotetabs: bool, header: bool)` and `custom_decode(encoded_string: bytes, header: bool)`, that make use of the `quopri` module to encode and decode MIME quoted-printable data respectively. These functions should handle binary data correctly and respect the specified parameters. Function 1: `custom_encode(input_string: bytes, quotetabs: bool, header: bool) -> bytes` **Input:** - `input_string`: A bytes object containing the data to be encoded. - `quotetabs`: A boolean flag to control whether to encode embedded spaces and tabs. - `header`: A boolean flag to control if spaces are encoded as underscores. **Output:** - Returns a bytes object containing the encoded data. **Constraints:** - The input data will be a valid bytes object. - Ensure that spaces and tabs are encoded at the end of lines as per **RFC 1521**. Function 2: `custom_decode(encoded_string: bytes, header: bool) -> bytes` **Input:** - `encoded_string`: A bytes object containing the data to be decoded. - `header`: A boolean flag to control if underscores should be decoded as spaces. **Output:** - Returns a bytes object containing the decoded data. **Constraints:** - The input encoded data will be a valid bytes object in quoted-printable format. Requirements 1. Implement both `custom_encode` and `custom_decode` functions. 2. Use the `quopri` module\'s functions to perform the encoding and decoding. 3. Do not use any external libraries other than the standard Python libraries. # Example ```python import quopri def custom_encode(input_string: bytes, quotetabs: bool, header: bool) -> bytes: encoded_bytes = quopri.encodestring(input_string, quotetabs=quotetabs, header=header) return encoded_bytes def custom_decode(encoded_string: bytes, header: bool) -> bytes: decoded_bytes = quopri.decodestring(encoded_string, header=header) return decoded_bytes # Test cases input_str = b\\"Hello World! This is an example with spaces, tabst, and newlinesn.\\" encoded_str = custom_encode(input_str, quotetabs=True, header=False) print(encoded_str) # Expected to print the quoted-printable encoded string decoded_str = custom_decode(encoded_str, header=False) print(decoded_str) # Expected to print the original input_string ```","solution":"import quopri def custom_encode(input_string: bytes, quotetabs: bool, header: bool) -> bytes: Encodes the input_string using quoted-printable encoding. :param input_string: A bytes object containing the data to be encoded. :param quotetabs: A boolean flag to control whether to encode embedded spaces and tabs. :param header: A boolean flag to control if spaces are encoded as underscores. :return: A bytes object containing the encoded data. encoded_bytes = quopri.encodestring(input_string, quotetabs=quotetabs, header=header) return encoded_bytes def custom_decode(encoded_string: bytes, header: bool) -> bytes: Decodes the encoded_string from quoted-printable encoding. :param encoded_string: A bytes object containing the data to be decoded. :param header: A boolean flag to control if underscores should be decoded as spaces. :return: A bytes object containing the decoded data. decoded_bytes = quopri.decodestring(encoded_string, header=header) return decoded_bytes"},{"question":"# **Coding Assessment Question** Title: Concurrent URL Fetching with Timeout Objective: Implement a function using the `asyncio` library to concurrently fetch the content from multiple URLs within a specified timeout period. Problem Statement: You are required to implement an asynchronous function `fetch_urls(urls: List[str], timeout: int) -> List[Optional[str]]` that takes: - `urls`: a list of URL strings - `timeout`: the maximum allowed time (in seconds) to fetch all URLs The function should fetch the content of all the given URLs concurrently and return their contents as a list. If fetching any URL exceeds the given timeout, return `None` for that URL\'s content. Input: - `urls` (List[str]): A list of URL strings (1 ≤ len(urls) ≤ 100). - `timeout` (int): An integer denoting the maximum allowed time in seconds to fetch all URLs (1 ≤ timeout ≤ 10). Output: - List[Optional[str]]: A list of strings or `None` values, where each entry corresponds to the content fetched from the respective URL in the input list. If fetching a URL exceeds the timeout, `None` should be placed at its position. Constraints: - The function should make use of `asyncio` for concurrent fetching. - Network operations should be timed out using `asyncio` timeout utilities. - Handle exceptions gracefully and ensure the program does not crash due to common networking issues. Example: ```python from typing import List, Optional import asyncio from aiohttp import ClientSession, ClientTimeout async def fetch_urls(urls: List[str], timeout: int) -> List[Optional[str]]: async def fetch(session, url): try: async with session.get(url) as response: return await response.text() except asyncio.TimeoutError: return None async def fetch_all(urls, timeout): timeout = ClientTimeout(total=timeout) async with ClientSession(timeout=timeout) as session: tasks = [fetch(session, url) for url in urls] return await asyncio.gather(*tasks) return await fetch_all(urls, timeout) # Example usage: urls = [ \\"https://www.example.com\\", \\"https://www.python.org\\" ] timeout = 5 results = asyncio.run(fetch_urls(urls, timeout)) print(results) # Prints list of contents or None values ``` Notes: - You may use any well-known library for HTTP requests that supports asyncio, like `aiohttp`. - Ensure that the function handles network timeouts and exceptions properly. Explain how `asyncio` is utilized in your implementation and why it is beneficial in handling network-bound tasks.","solution":"from typing import List, Optional import asyncio from aiohttp import ClientSession, ClientTimeout async def fetch_urls(urls: List[str], timeout: int) -> List[Optional[str]]: async def fetch(session, url): try: async with session.get(url) as response: return await response.text() except asyncio.TimeoutError: return None except Exception: return None async def fetch_all(urls, timeout): timeout = ClientTimeout(total=timeout) async with ClientSession(timeout=timeout) as session: tasks = [fetch(session, url) for url in urls] return await asyncio.gather(*tasks) return await fetch_all(urls, timeout) # Example usage: # urls = [ # \\"https://www.example.com\\", # \\"https://www.python.org\\" # ] # timeout = 5 # results = asyncio.run(fetch_urls(urls, timeout)) # print(results)"},{"question":"# Question: Custom Sorting with Multiple Criteria You are given a list of dictionaries, where each dictionary represents a product with the following keys: `name`, `price`, and `rating`. Implement a function `custom_sort_products(products: List[Dict[str, Any]], sort_by: List[Tuple[str, bool]]) -> List[Dict[str, Any]]` that sorts this list of products based on multiple criteria defined in `sort_by`. **Function Signature:** ```python from typing import List, Dict, Tuple, Any def custom_sort_products(products: List[Dict[str, Any]], sort_by: List[Tuple[str, bool]]) -> List[Dict[str, Any]]: ``` **Input:** - `products`: A list of dictionaries, where each dictionary contains: - `name` (string): The name of the product. - `price` (float): The price of the product. - `rating` (float): The rating of the product. - `sort_by`: A list of tuples. Each tuple contains: - A string representing the key by which to sort (`\\"name\\"`, `\\"price\\"`, `\\"rating\\"`). - A boolean where `True` means sorting in descending order, and `False` means sorting in ascending order. **Output:** - Return the sorted list of product dictionaries. Sorting should be stable and follow the order of criteria as specified in `sort_by`. **Example:** ```python products = [ {\\"name\\": \\"Product A\\", \\"price\\": 30.0, \\"rating\\": 4.5}, {\\"name\\": \\"Product B\\", \\"price\\": 20.0, \\"rating\\": 4.7}, {\\"name\\": \\"Product C\\", \\"price\\": 20.0, \\"rating\\": 4.4}, {\\"name\\": \\"Product D\\", \\"price\\": 25.0, \\"rating\\": 4.8} ] sort_by = [(\\"price\\", False), (\\"rating\\", True)] assert custom_sort_products(products, sort_by) == [ {\\"name\\": \\"Product C\\", \\"price\\": 20.0, \\"rating\\": 4.4}, {\\"name\\": \\"Product B\\", \\"price\\": 20.0, \\"rating\\": 4.7}, {\\"name\\": \\"Product D\\", \\"price\\": 25.0, \\"rating\\": 4.8}, {\\"name\\": \\"Product A\\", \\"price\\": 30.0, \\"rating\\": 4.5} ] ``` **Constraints:** - All product dictionaries will have the keys `name`, `price`, and `rating` with appropriate value types. - The `sort_by` list is guaranteed to have valid keys and booleans. - The number of products will not exceed 10^4 for performance considerations. **Note:** - Implement your solution using the `sorted()` function and key functions. - The sorting function should leverage Python\'s stability in sorts.","solution":"from typing import List, Dict, Tuple, Any def custom_sort_products(products: List[Dict[str, Any]], sort_by: List[Tuple[str, bool]]) -> List[Dict[str, Any]]: Sorts a list of products based on the given sorting criteria. Parameters: - products (List[Dict[str, Any]]): A list of product dictionaries. - sort_by (List[Tuple[str, bool]]): A list of tuples specifying the key and order for sorting. Returns: - List[Dict[str, Any]]: The sorted list of product dictionaries. for key, descending in reversed(sort_by): products = sorted(products, key=lambda x: x[key], reverse=descending) return products"},{"question":"<|Analysis Begin|> After reviewing the provided documentation, \\"logging.config\\" is a module in Python that contains functions to configure the logging module using dictionaries or configuration files. The key functions in this module are `dictConfig()` which sets up logging configuration via a dictionary and `fileConfig()` which does the same via configuration files. Additionally, there are functions like `listen()` and `stopListening()` for configuring logging over a socket. Key points from the documentation: 1. **dictConfig(config)**: Configures logging using a dictionary. The dictionary schema is described in detail within the documentation. 2. **fileConfig(fname, defaults=None, disable_existing_loggers=True, encoding=None)**: Reads and sets up logging configuration from a configuration file. 3. **listen(port=DEFAULT_LOGGING_CONFIG_PORT, verify=None)**: Starts a socket server to listen for logging configurations. 4. **stopListening()**: Stops the listening server that was created with `listen()`. Given the information, we can create a challenging question that focuses on configuring the logging system using `dictConfig()` and `fileConfig()` in a comprehensive way, ensuring that students demonstrate a solid understanding of logging configuration in Python. <|Analysis End|> <|Question Begin|> # Logging Configuration in Python using `logging.config` **Objective:** Your task is to configure Python’s logging system using the `logging.config` module. You will create logging configurations using both dictionary-based and file-based approaches and ensure that they work as expected by logging messages to different handlers. # Task 1: Dictionary-Based Configuration 1. Create a logging configuration dictionary `config_dict` that includes: - Two formatters: - `simple`: which logs messages in the format `%(levelname)s - %(message)s`. - `detailed`: which logs messages in the format `%(asctime)s - %(name)s - %(levelname)s - %(message)s`. - One filter named `info_filter` that filters out all messages below the level INFO. - Two handlers: - `console`: A `StreamHandler` that logs to the console using the `simple` formatter and applies `info_filter`. - `file`: A `RotatingFileHandler` that logs to a file named `app.log`. This handler uses the `detailed` formatter and rotates the file every 1KB up to 3 backup files. - One logger `app_logger` that: - Uses the `console` and `file` handlers. - Has a logging level set to `DEBUG`. - Is set to propagate messages. 2. Apply the configuration using `dictConfig()`. # Task 2: File-Based Configuration 1. Create a logging configuration file `logging.conf` with the following settings: - Two formatters configured as in Task 1. - One filter configured as in Task 1. - Two handlers configured as in Task 1. - One logger `app_logger` configured as in Task 1. 2. Apply the configuration using `fileConfig()` by reading from `logging.conf`. # Task 3: Verification 1. Create a script that: - Configures logging using `dictConfig` from the configuration dictionary in Task 1. - Logs messages at different severity levels (DEBUG, INFO, WARNING, ERROR, CRITICAL). - Checks that the logs appear in both the console and `app.log`, formatted according to the configured formatters and filtered appropriately. 2. Modify the script to configure logging using `fileConfig` from the configuration file `logging.conf`. - Log messages with the same severity levels to verify that both configurations produce identical results. **Constraints:** - Your handlers should ensure that only one instance of `app.log` exists, with backups managed as specified. - Ensure the configurations are resilient to errors and handle any potential issues (such as invalid configurations or missing files) gracefully. - Test the solution in a multi-threaded environment to ensure thread safety in the logging setup. # Example: ```python import logging import logging.config from logging.handlers import RotatingFileHandler # Provided dictionary-based configuration config_dict = { \'version\': 1, \'formatters\': { \'simple\': { \'format\': \'%(levelname)s - %(message)s\' }, \'detailed\': { \'format\': \'%(asctime)s - %(name)s - %(levelname)s - %(message)s\' }, }, \'filters\': { \'info_filter\': { \'()\': logging.Filter, \'name\': \'\' }, }, \'handlers\': { \'console\': { \'class\': \'logging.StreamHandler\', \'formatter\': \'simple\', \'level\': \'DEBUG\', \'filters\': [\'info_filter\'], \'stream\': \'ext://sys.stdout\', }, \'file\': { \'class\': \'logging.handlers.RotatingFileHandler\', \'formatter\': \'detailed\', \'level\': \'DEBUG\', \'filename\': \'app.log\', \'maxBytes\': 1024, \'backupCount\': 3, \'encoding\': \'utf8\' }, }, \'loggers\': { \'app_logger\': { \'handlers\': [\'console\', \'file\'], \'level\': \'DEBUG\', \'propagate\': True } } } # Function to configure logging from dictionary def configure_logging_from_dict(): logging.config.dictConfig(config_dict) # Function to configure logging from file def configure_logging_from_file(file_path=\'logging.conf\'): logging.config.fileConfig(file_path) # Function to log sample messages def log_messages(): logger = logging.getLogger(\'app_logger\') logger.debug(\'This is a DEBUG message\') logger.info(\'This is an INFO message\') logger.warning(\'This is a WARNING message\') logger.error(\'This is an ERROR message\') logger.critical(\'This is a CRITICAL message\') # Main script logic if __name__ == \'__main__\': configure_logging_from_dict() log_messages() configure_logging_from_file() log_messages() ``` Ensure that all configurations and script elements follow the guidelines and constraints provided.","solution":"import logging import logging.config from logging.handlers import RotatingFileHandler # Provided dictionary-based configuration config_dict = { \'version\': 1, \'formatters\': { \'simple\': { \'format\': \'%(levelname)s - %(message)s\' }, \'detailed\': { \'format\': \'%(asctime)s - %(name)s - %(levelname)s - %(message)s\' }, }, \'filters\': { \'info_filter\': { \'()\': logging.Filter, \'name\': \'\' }, }, \'handlers\': { \'console\': { \'class\': \'logging.StreamHandler\', \'formatter\': \'simple\', \'level\': \'DEBUG\', \'filters\': [\'info_filter\'], \'stream\': \'ext://sys.stdout\', }, \'file\': { \'class\': \'logging.handlers.RotatingFileHandler\', \'formatter\': \'detailed\', \'level\': \'DEBUG\', \'filename\': \'app.log\', \'maxBytes\': 1024, \'backupCount\': 3, \'encoding\': \'utf8\' }, }, \'loggers\': { \'app_logger\': { \'handlers\': [\'console\', \'file\'], \'level\': \'DEBUG\', \'propagate\': True } } } # Function to configure logging from dictionary def configure_logging_from_dict(): logging.config.dictConfig(config_dict) # Function to configure logging from file def configure_logging_from_file(file_path=\'logging.conf\'): logging.config.fileConfig(file_path) # Function to log sample messages def log_messages(): logger = logging.getLogger(\'app_logger\') logger.debug(\'This is a DEBUG message\') logger.info(\'This is an INFO message\') logger.warning(\'This is a WARNING message\') logger.error(\'This is an ERROR message\') logger.critical(\'This is a CRITICAL message\') # Main script logic if __name__ == \'__main__\': configure_logging_from_dict() log_messages() configure_logging_from_file() log_messages()"},{"question":"**Problem Statement:** You are provided with a dataset consisting of points generated from a mixture of Gaussian distributions. Your task is to implement Gaussian Mixture Models (GMM) and Bayesian Gaussian Mixture Models (BGMM) using scikit-learn, fit these models to the dataset, and evaluate their performance based on Bayesian Information Criterion (BIC). You will also compare the results based on different model initialization methods. # Requirements: 1. **Data Input**: The input dataset should be generated using the `make_blobs` function from `sklearn.datasets` with a specified number of centers (clusters). You should standardize the features using `StandardScaler` from `sklearn.preprocessing`. 2. **Implement GMM**: - Fit a GMM to the standardized dataset with a specified number of components. - Evaluate and print the BIC for different numbers of components (clusters). - Predict the cluster assignments for the data points. 3. **Implement BGMM**: - Fit a BGMM to the standardized dataset, using Dirichlet process priors. - Adjust the `weight_concentration_prior` parameter and fit the model to observe the impact on the number of active components. - Predict the cluster assignments for the data points. 4. **Initialization Methods**: - Compare the performance (e.g., convergence, BIC) of GMM and BGMM with different initialization methods: `k-means`, `k-means++`, `random_from_data`, and `random`. # Input Format: - An integer `n_samples` representing the number of data points to generate. - An integer `n_features` representing the number of features for each data point. - A list of integers `n_components_range` specifying the range of components to consider. # Output Format: - Print the BIC for each number of components considered in `n_components_range` for both GMM and BGMM. - Print the active number of components for BGMM after adjusting the `weight_concentration_prior`. - Plot the clustered data points with different colors for each predicted cluster. # Constraints: - Use sklearn\'s `GaussianMixture` and `BayesianGaussianMixture` classes. - Ensure the code is optimized for performance, handling datasets of up to `n_samples = 10,000`. # Example: ```python import numpy as np from sklearn.datasets import make_blobs from sklearn.preprocessing import StandardScaler from sklearn.mixture import GaussianMixture, BayesianGaussianMixture import matplotlib.pyplot as plt # Generate and standardize dataset n_samples = 3000 n_features = 2 centers = 3 X, _ = make_blobs(n_samples=n_samples, centers=centers, n_features=n_features, random_state=42) scaler = StandardScaler() X = scaler.fit_transform(X) # GMM with different number of components n_components_range = [1, 2, 3, 4, 5] bic_gmm = [] for n_components in n_components_range: gmm = GaussianMixture(n_components=n_components, random_state=42) gmm.fit(X) bic_gmm.append(gmm.bic(X)) print(\\"BIC for GMM:\\", bic_gmm) # BGMM with different weight_concentration_prior weight_concentration_prior_range = [0.01, 1, 10] for weight_concentration_prior in weight_concentration_prior_range: bgmm = BayesianGaussianMixture(n_components=10, weight_concentration_prior=weight_concentration_prior, random_state=42) bgmm.fit(X) print(f\\"Active components for BGMM with weight_concentration_prior={weight_concentration_prior}: {np.sum(bgmm.weights_ > 1e-3)}\\") # Plotting results for GMM plt.figure(figsize=(10, 6)) gmm = GaussianMixture(n_components=centers, random_state=42) gmm.fit(X) pred = gmm.predict(X) plt.scatter(X[:, 0], X[:, 1], c=pred, s=10, cmap=\'viridis\') plt.title(\'Gaussian Mixture Model Clustering\') plt.xlabel(\'Feature 1\') plt.ylabel(\'Feature 2\') plt.show() ``` # Notes: - Use appropriate axis labels and legends for the plots. - Repeat the clustering and BIC calculation for different initialization methods and report the findings.","solution":"import numpy as np from sklearn.datasets import make_blobs from sklearn.preprocessing import StandardScaler from sklearn.mixture import GaussianMixture, BayesianGaussianMixture import matplotlib.pyplot as plt def generate_dataset(n_samples, n_features, centers): Generate and standardize dataset. Args: - n_samples: int, number of data points to generate. - n_features: int, number of features for each data point. - centers: int, number of centers (clusters) for the dataset. Returns: - X: numpy array of shape (n_samples, n_features), standardized dataset. X, _ = make_blobs(n_samples=n_samples, centers=centers, n_features=n_features, random_state=42) scaler = StandardScaler() X = scaler.fit_transform(X) return X def fit_gmm(X, n_components_range): Fit GMM to the standardized dataset with specified range of components and print the BIC for each. Args: - X: numpy array, standardized dataset. - n_components_range: list, range of components to consider. Returns: - bic_gmm: list, BIC for each number of components. bic_gmm = [] for n_components in n_components_range: gmm = GaussianMixture(n_components=n_components, random_state=42) gmm.fit(X) bic_gmm.append(gmm.bic(X)) return bic_gmm def fit_bgmm(X, weight_concentration_prior_range): Fit BGMM to the standardized dataset with different weight concentration priors. Args: - X: numpy array, standardized dataset. - weight_concentration_prior_range: list, range of weight concentration priors to consider. Returns: - active_components_bgmm: list, active components for each weight concentration prior. active_components_bgmm = [] for weight_concentration_prior in weight_concentration_prior_range: bgmm = BayesianGaussianMixture(n_components=10, weight_concentration_prior=weight_concentration_prior, random_state=42) bgmm.fit(X) active_components_bgmm.append(np.sum(bgmm.weights_ > 1e-3)) return active_components_bgmm def plot_clusters(X, labels, title): Plot the clustered data points with different colors for each predicted cluster. Args: - X: numpy array, standardized dataset. - labels: numpy array, predicted cluster labels. - title: string, title for the plot. plt.figure(figsize=(10, 6)) plt.scatter(X[:, 0], X[:, 1], c=labels, s=10, cmap=\'viridis\') plt.title(title) plt.xlabel(\'Feature 1\') plt.ylabel(\'Feature 2\') plt.show()"},{"question":"Objective Implement a Python function that utilizes scikit-learn\'s Linear Discriminant Analysis (LDA) and Quadratic Discriminant Analysis (QDA) to classify a provided dataset. Your implementation should also include a performance comparison between the two classifiers. Requirements 1. Write a function `classify_and_compare(data: np.ndarray, labels: np.ndarray) -> dict`: - `data` (numpy.ndarray): A matrix (features x samples) representing the dataset. - `labels` (numpy.ndarray): A vector representing the class labels corresponding to the samples in the dataset. 2. Your function should: - Use LDA and QDA to classify the data. - Split the data into training and testing sets (80% training, 20% testing). - Train both LDA and QDA on the training set and evaluate on the testing set. - Return a dictionary with the accuracy of each classifier on the testing set. Constraints - You should use scikit-learn\'s `LinearDiscriminantAnalysis` and `QuadraticDiscriminantAnalysis` classes. - Use a random state of `42` for reproducibility when splitting the data. - Ensure that the accuracy scores are computed correctly using appropriate metrics from scikit-learn. Example Usage ```python import numpy as np # Example dataset data = np.array([[1.2, 3.3, 2.1], [3.4, 1.5, 2.0], [2.3, 2.2, 3.1], [0.9, 1.3, 0.7], [1.6, 1.9, 2.0], [3.5, 3.1, 5.2]]) labels = np.array([0, 1, 0, 1, 0, 1]) # Function call result = classify_and_compare(data, labels) # Expected output format: # { # \'LDA_accuracy\': 0.75, # \'QDA_accuracy\': 0.67 # } print(result) ``` Evaluation Criteria - Correct implementation and use of LDA and QDA. - Correct splitting of data and evaluation of classifier performance. - Appropriate use of scikit-learn functions and methods. - The accuracy of both classifiers should be returned in the specified format.","solution":"from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score import numpy as np def classify_and_compare(data: np.ndarray, labels: np.ndarray) -> dict: # Split the data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42) # Initialize the classifiers lda = LinearDiscriminantAnalysis() qda = QuadraticDiscriminantAnalysis() # Train the LDA classifier and make predictions lda.fit(X_train, y_train) lda_predictions = lda.predict(X_test) lda_accuracy = accuracy_score(y_test, lda_predictions) # Train the QDA classifier and make predictions qda.fit(X_train, y_train) qda_predictions = qda.predict(X_test) qda_accuracy = accuracy_score(y_test, qda_predictions) # Return the accuracy of each classifier return { \'LDA_accuracy\': lda_accuracy, \'QDA_accuracy\': qda_accuracy }"},{"question":"Your task is to implement a function that runs a cross-validated grid search on a given dataset using different levels of parallelism. You will test different types of classifiers and compare their computation times under different parallel settings. # Requirements: 1. You must experiment with at least two different classifiers from scikit-learn. 2. The function should allow tuning the `n_jobs` parameter for joblib (higher-level parallelism) and the `OMP_NUM_THREADS` (lower-level parallelism) environment variable. 3. Capture and compare the computation times under different settings. 4. Output should include the best parameters found for each classifier and a comparison of computation times. # Function Signature: ```python def parallel_grid_search(X, y, classifiers, param_grids, n_jobs_list, omp_num_threads_list): Perform cross-validated grid search with different levels of parallelism. Parameters: - X (array-like or pandas DataFrame): Feature matrix. - y (array-like or pandas Series): Target variable. - classifiers (list): List of scikit-learn classifier instances. - param_grids (list): List of dictionaries with parameters to be searched for each classifier. - n_jobs_list (list): List of integers for joblib\'s n_jobs parameter to test. - omp_num_threads_list (list): List of integers for OpenMP\'s OMP_NUM_THREADS to test. Returns: - results (dict): Dictionary containing the best parameters found and the computation times. ``` # Example Usage: ```python from sklearn.datasets import load_iris from sklearn.ensemble import RandomForestClassifier from sklearn.svm import SVC X, y = load_iris(return_X_y=True) classifiers = [RandomForestClassifier(), SVC()] param_grids = [ {\'n_estimators\': [10, 50], \'max_depth\': [3, None]}, {\'kernel\': [\'linear\', \'rbf\'], \'C\': [0.1, 1]} ] n_jobs_list = [1, 2, 4] omp_num_threads_list = [1, 2, 4] results = parallel_grid_search(X, y, classifiers, param_grids, n_jobs_list, omp_num_threads_list) ``` # Constraints: - Ensure that the implementation is efficient and does not cause oversubscription. - Use appropriate context managers for setting `OMP_NUM_THREADS`. # Notes: - You can use `GridSearchCV` from scikit-learn for the grid search. - Use `timeit` or similar utilities to measure computation time. - Consider using `joblib.parallel_backend` and environment variable settings within the function.","solution":"import time import os import joblib import numpy as np from sklearn.model_selection import GridSearchCV def parallel_grid_search(X, y, classifiers, param_grids, n_jobs_list, omp_num_threads_list): from contextlib import contextmanager from joblib import parallel_backend @contextmanager def set_env(variable, value): original_value = os.environ.get(variable, None) os.environ[variable] = str(value) try: yield finally: if original_value is None: del os.environ[variable] else: os.environ[variable] = original_value results = {} for classifier, param_grid in zip(classifiers, param_grids): classifier_name = type(classifier).__name__ results[classifier_name] = [] for n_jobs in n_jobs_list: for omp_num_threads in omp_num_threads_list: with set_env(\'OMP_NUM_THREADS\', omp_num_threads): start_time = time.time() with parallel_backend(\'threading\', n_jobs=n_jobs): grid_search = GridSearchCV(classifier, param_grid, cv=3, n_jobs=n_jobs) grid_search.fit(X, y) end_time = time.time() elapsed_time = end_time - start_time results[classifier_name].append({ \'n_jobs\': n_jobs, \'OMP_NUM_THREADS\': omp_num_threads, \'best_params\': grid_search.best_params_, \'computation_time\': elapsed_time }) return results"},{"question":"# Classification Using Nearest Neighbors You are tasked with implementing a classification model using the K-Nearest Neighbors (KNN) algorithm from the `scikit-learn` library. The goal is to classify a given dataset into its respective classes. # Dataset The dataset consists of the following features: - `features`: A NumPy array of shape `(n_samples, n_features)` representing the feature matrix. - `labels`: A NumPy array of shape `(n_samples,)` representing the class labels. # Requirements 1. **Function Implementation:** - Implement a function `knn_classification` that takes the following inputs: - `features`: a NumPy array of shape `(n_samples, n_features)`. - `labels`: a NumPy array of shape `(n_samples,)`. - `test_samples`: a NumPy array of shape `(m_samples, n_features)` representing the test feature matrix. - `k`: an integer representing the number of neighbors to use. - The function should return a NumPy array of shape `(m_samples,)` containing the predicted class labels for the test samples. 2. **Performance Considerations:** - Use `KNeighborsClassifier` from `sklearn.neighbors`. - Ensure to use an appropriate algorithm (`\'auto\'`, `\'ball_tree\'`, `\'kd_tree\'`, `\'brute\'`) based on the dataset. 3. **Implementation Constraints:** - Your solution should handle both dense and sparse matrices. - Consider different distance metrics available in the `sklearn.neighbors.KDTree` and `sklearn.neighbors.BallTree`. # Example ```python import numpy as np from sklearn.model_selection import train_test_split # Sample data features = np.array([ [1, 2], [2, 3], [3, 1], [6, 5], [7, 8], [8, 6] ]) labels = np.array([0, 0, 0, 1, 1, 1]) # Splitting the data X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.5, random_state=42) # Implement the function to perform KNN classification def knn_classification(features, labels, test_samples, k): from sklearn.neighbors import KNeighborsClassifier # Create KNN classifier knn = KNeighborsClassifier(n_neighbors=k, algorithm=\'auto\') # Fit the model knn.fit(features, labels) # Predict the class labels for test samples predictions = knn.predict(test_samples) return predictions # Testing the function predictions = knn_classification(X_train, y_train, X_test, k=3) print(\\"Predicted class labels:\\", predictions) ``` # Evaluation Criteria - Correct implementation of the `knn_classification` function. - Proper use of the `KNeighborsClassifier`. - Efficient handling of different data formats and performance considerations. - Accuracy of predictions on provided test cases. # Note You are encouraged to explore the official `scikit-learn` documentation on K-Nearest Neighbors to understand the different parameters and their effects on the model\'s performance.","solution":"import numpy as np from sklearn.neighbors import KNeighborsClassifier def knn_classification(features, labels, test_samples, k): Performs K-Nearest Neighbors classification. Parameters: - features (numpy array): The feature matrix of shape (n_samples, n_features) - labels (numpy array): The class labels of shape (n_samples,) - test_samples (numpy array): The test feature matrix of shape (m_samples, n_features) - k (int): The number of neighbors to use Returns: - numpy array: Predicted class labels for the test samples of shape (m_samples,) # Create KNN classifier knn = KNeighborsClassifier(n_neighbors=k, algorithm=\'auto\') # Fit the model knn.fit(features, labels) # Predict the class labels for test samples predictions = knn.predict(test_samples) return predictions"},{"question":"Coding Assessment Question # Context In Python, working with buffer protocols allows direct manipulation of memory views on objects. The old buffer protocol functions, while deprecated, provide compatibility for older Python 2.x code but lack proper control over resource lifetimes. The new buffer protocol addresses this by providing better-managed access through functions such as `PyObject_GetBuffer` and `PyBuffer_Release`. # Objective Write a Python function that demonstrates the transition from using old buffer protocol functions to the new buffer protocol by implementing a function that retrieves and releases a buffer view over an object. The function should also handle errors appropriately and provide information about the buffer. # Task You are required to implement a function `manage_buffer(obj)` that: 1. Accepts a Python object `obj` which supports the buffer interface. 2. Retrieves a buffer view on `obj` using the new buffer protocol. 3. Checks if the object supports only readable or writable buffers and print respective messages. 4. Prints the contents (or a portion) of the buffer. 5. Ensures that the buffer is released properly after use. 6. Handles any errors that may occur during the process. # Function Signature ```python def manage_buffer(obj: object) -> None: pass ``` # Constraints - You should use the `PyObject_GetBuffer` and `PyBuffer_Release` functions for managing the buffer. - You must handle possible exceptions and errors gracefully, providing meaningful messages to the user. - The buffer contents printed should be limited to the first 100 bytes for brevity. # Example Usage ```python # Example objects supporting buffer example_bytes = b\\"Example buffer content for demonstration purposes.\\" * 3 manage_buffer(example_bytes) # Output should include readable checks and content, appropriately handling buffer release. ``` # Notes - You can use the `memoryview` object in Python to interact with the buffer protocol. - Ensure the function is self-contained and performs all necessary operations within itself, without relying on external variables or states. # Evaluation Criteria - Correct and efficient use of new buffer protocol functions. - Proper error handling and resource management. - Clear and informative output reflecting the buffer\'s state and contents.","solution":"def manage_buffer(obj: object) -> None: Manages a buffer view over an object using the new buffer protocol. :param obj: A Python object supporting the buffer interface. try: # Using memoryview to interact with the buffer protocol with memoryview(obj) as mv: # Checking if the buffer is writable or only readable if mv.readonly: print(\\"The buffer is read-only.\\") else: print(\\"The buffer is writable.\\") # Printing the contents (or a portion) of the buffer, limited to the first 100 bytes buffer_contents = mv.tobytes() print(f\\"Buffer contents (first 100 bytes): {buffer_contents[:100]}\\") except TypeError: print(\\"The provided object does not support the buffer interface\\") except Exception as e: print(f\\"An error occurred while managing the buffer: {e}\\")"},{"question":"Neural Network Initialization in PyTorch You are tasked with implementing and initializing a neural network in PyTorch. This exercise will assess your understanding of neural network weight initialization techniques and their appropriate usage. Task 1. **Define a Simple Neural Network**: - Create a neural network class `SimpleNN` that inherits from `torch.nn.Module`. - The network should contain: - An input layer taking inputs of size 784 (e.g., 28x28 images flattened). - Two hidden layers with ReLU activations. - An output layer producing 10 outputs (e.g., for classification of digits 0-9). 2. **Initialize the Network Parameters**: - Initialize the weights of the input and hidden layers using `xavier_uniform_`. - Initialize the biases of all layers to zero using `zeros_`. 3. **Implement the Initialization**: - Write a method `initialize_weights` within your `SimpleNN` class that performs the above initializations. 4. **Testing the Initialization**: - Write a function `test_initialization` that creates an instance of `SimpleNN`, calls `initialize_weights`, and prints out the first 5 weights of each layer to validate the initialization. Expected Input and Output Formats - **Input**: - No specific inputs are provided during initialization. - **Output**: - The output of `test_initialization` should contain the first 5 weights of each layer printed to the console. Constraints - Ensure you use `torch.no_grad` mode while initializing the parameters. - The network architecture should strictly follow the specifications. Example ```python class SimpleNN(torch.nn.Module): def __init__(self): super(SimpleNN, self).__init__() self.fc1 = torch.nn.Linear(784, 128) self.fc2 = torch.nn.Linear(128, 64) self.fc3 = torch.nn.Linear(64, 10) def forward(self, x): x = torch.nn.functional.relu(self.fc1(x)) x = torch.nn.functional.relu(self.fc2(x)) x = self.fc3(x) return x def initialize_weights(self): for m in self.modules(): if isinstance(m, torch.nn.Linear): torch.nn.init.xavier_uniform_(m.weight) torch.nn.init.zeros_(m.bias) def test_initialization(): model = SimpleNN() model.initialize_weights() # Print the first 5 weights of each layer print(\\"First layer weights:\\", model.fc1.weight.data[:5]) print(\\"Second layer weights:\\", model.fc2.weight.data[:5]) print(\\"Output layer weights:\\", model.fc3.weight.data[:5]) # Call the test function test_initialization() ``` Ensure your implementation matches the structure provided and validates the proper initialization of weights.","solution":"import torch class SimpleNN(torch.nn.Module): def __init__(self): super(SimpleNN, self).__init__() self.fc1 = torch.nn.Linear(784, 128) self.fc2 = torch.nn.Linear(128, 64) self.fc3 = torch.nn.Linear(64, 10) def forward(self, x): x = torch.nn.functional.relu(self.fc1(x)) x = torch.nn.functional.relu(self.fc2(x)) x = self.fc3(x) return x def initialize_weights(self): Initialize the weights of the input and hidden layers using xavier_uniform_ and the biases of all layers to zero using zeros_. with torch.no_grad(): for m in self.modules(): if isinstance(m, torch.nn.Linear): torch.nn.init.xavier_uniform_(m.weight) torch.nn.init.zeros_(m.bias) def test_initialization(): model = SimpleNN() model.initialize_weights() # Print the first 5 weights of each layer to validate initialization print(\\"First layer weights:\\", model.fc1.weight.data.view(-1)[:5]) print(\\"Second layer weights:\\", model.fc2.weight.data.view(-1)[:5]) print(\\"Output layer weights:\\", model.fc3.weight.data.view(-1)[:5])"},{"question":"Coding Assessment Question # Task Using the knowledge of Python exceptions, including their hierarchy and attributes, your task is to implement a custom exception class and a function that performs division operations with robust error handling. The function should handle specific built-in exceptions and demonstrate exception chaining. # Requirements 1. Implement a custom exception called `CustomDivisionError` that inherits from `Exception`. It should accept two arguments: an error message and a code. The string representation of this exception should display both the message and the code. 2. Implement a function `safe_divide` that takes two arguments: `numerator` and `denominator`. 3. The `safe_divide` function should: - Perform the division and return the result. - Handle and raise a `CustomDivisionError` with appropriate messages and codes when: - The denominator is zero (`ZeroDivisionError`). - Any other `ValueError` or `TypeError` occurs during the division operation. - Use exception chaining (`raise new_exc from original_exc`) to retain the context of the original exception. 4. Demonstrate the function and its exception handling with a set of test cases that trigger the following errors: - Zero division. - Invalid types (e.g., passing a string as the numerator or denominator). - Any remaining edge cases you deem necessary. # Input - Two arguments `numerator` and `denominator`, which can be of type `int`, `float`, or `str`. # Output - Returns the result of the division if successful. - Raises `CustomDivisionError` with a detailed message and error code if an error occurs. # Example ```python class CustomDivisionError(Exception): def __init__(self, message, code): self.message = message self.code = code super().__init__(message) def __str__(self): return f\'{self.message} (Error Code: {self.code})\' def safe_divide(numerator, denominator): try: result = float(numerator) / float(denominator) return result except ZeroDivisionError as e: raise CustomDivisionError(\\"Denominator cannot be zero\\", 1001) from e except (ValueError, TypeError) as e: raise CustomDivisionError(\\"Invalid input types for division\\", 1002) from e # Test Cases try: print(safe_divide(10, 2)) # Expected output: 5.0 except CustomDivisionError as e: print(e) try: print(safe_divide(10, 0)) # Expected CustomDivisionError: Denominator cannot be zero (Error Code: 1001) except CustomDivisionError as e: print(e) try: print(safe_divide(\\"ten\\", 5)) # Expected CustomDivisionError: Invalid input types for division (Error Code: 1002) except CustomDivisionError as e: print(e) ``` # Constraints - Ensure you handle the exceptions with meaningful messages and chain the exceptions properly. - Demonstrate good practices in exception handling with a clear and concise implementation.","solution":"class CustomDivisionError(Exception): def __init__(self, message, code): self.message = message self.code = code super().__init__(message) def __str__(self): return f\'{self.message} (Error Code: {self.code})\' def safe_divide(numerator, denominator): try: result = float(numerator) / float(denominator) return result except ZeroDivisionError as e: raise CustomDivisionError(\\"Denominator cannot be zero\\", 1001) from e except (ValueError, TypeError) as e: raise CustomDivisionError(\\"Invalid input types for division\\", 1002) from e"},{"question":"You are tasked with creating a Python class that simulates a simplified version of a container type using the `GenericAlias` for type safety and consistency in storing elements. Here\'s the detailed task breakdown: **Objective:** Create a `GenericContainer` class which enforces type consistency of its elements using `GenericAlias`. **Requirements:** 1. **Define the GenericContainer Class**: - The class should enforce that all elements added to it are of a specific type. - Use the `GenericAlias` to create a generic type-safe container. 2. **Implementation Details**: - A `__init__` method that initializes the container and enforces the element type. - An `add_element` method that adds elements to the container, ensuring they match the specified type. - A method to retrieve all elements of the container. - Proper error handling to raise exceptions when type constraints are violated. 3. **Type Hinting**: - Utilize Python\'s type hinting to ensure type safety in method signatures. **Expected Input and Output:** - Creation of `GenericContainer` instance with a particular type. - Adding elements of the specified type should succeed. - Adding elements of the wrong type should raise a `TypeError`. - Retrieval of elements should return a list of the contained elements. **Constraints**: - You should not use any external libraries apart from Python\'s standard library. - Performance is not a primary concern, but the methods should work efficiently for typical usage. # Example Usage: ```python from typing import Generic, TypeVar T = TypeVar(\'T\') class GenericContainer(Generic[T]): def __init__(self, element_type: type): self._element_type = element_type self._elements = [] def add_element(self, element: T): if not isinstance(element, self._element_type): raise TypeError(f\\"Element must be of type {self._element_type.__name__}\\") self._elements.append(element) def get_elements(self) -> list[T]: return self._elements # Example usage: string_container = GenericContainer(str) string_container.add_element(\\"hello\\") string_container.add_element(\\"world\\") print(string_container.get_elements()) # Output: [\'hello\', \'world\'] int_container = GenericContainer(int) int_container.add_element(1) int_container.add_element(2) print(int_container.get_elements()) # Output: [1, 2] - string_container.add_element(100) # Should raise TypeError - int_container.add_element(\\"not_an_int\\") # Should raise TypeError ``` Implement the `GenericContainer` class as described above.","solution":"from typing import Generic, TypeVar, GenericAlias T = TypeVar(\'T\') class GenericContainer(Generic[T]): def __init__(self, element_type: type): self._element_type = element_type self._elements = [] def add_element(self, element: T): if not isinstance(element, self._element_type): raise TypeError(f\\"Element must be of type {self._element_type.__name__}\\") self._elements.append(element) def get_elements(self) -> list[T]: return self._elements # Example usage: # string_container = GenericContainer(str) # string_container.add_element(\\"hello\\") # string_container.add_element(\\"world\\") # print(string_container.get_elements()) # Output: [\'hello\', \'world\'] # int_container = GenericContainer(int) # int_container.add_element(1) # int_container.add_element(2) # print(int_container.get_elements()) # Output: [1, 2] # string_container.add_element(100) # Should raise TypeError # int_container.add_element(\\"not_an_int\\") # Should raise TypeError"},{"question":"**Problem Statement: Analyzing Sales Data** You are provided with a dataset containing sales data for various products across different regions. Your task is to implement a series of functions to clean, transform, analyze, and summarize this data using `pandas`. # Input Format The input will be a CSV file with the following columns: - `Date`: The date of the sale. - `Region`: The region where the sale was made. - `Product`: The name of the product sold. - `Quantity`: The amount of product sold. - `Price`: The price at which the product was sold. # Functions to Implement 1. **filter_sales_data**: - **Input**: DataFrame, string (`region`) - **Output**: DataFrame - **Description**: Filter the sales data to include only the sales from the specified region. 2. **calculate_total_sales**: - **Input**: DataFrame - **Output**: Series (with Date as index and total sales amount per day) - **Description**: Calculate the total sales amount per day. 3. **average_sales_per_product**: - **Input**: DataFrame - **Output**: DataFrame (with columns `Product` and `AverageSales`) - **Description**: Calculate the average sales for each product across all regions. 4. **sales_summary**: - **Input**: DataFrame - **Output**: DataFrame - **Description**: Return a summary DataFrame with the following columns: - `Product`: Name of the product. - `TotalQuantity`: Total quantity sold. - `TotalRevenue`: Total revenue generated (`Quantity * Price`). - `AveragePrice`: Average price at which the product was sold. # Function Signatures ```python import pandas as pd def filter_sales_data(df: pd.DataFrame, region: str) -> pd.DataFrame: pass def calculate_total_sales(df: pd.DataFrame) -> pd.Series: pass def average_sales_per_product(df: pd.DataFrame) -> pd.DataFrame: pass def sales_summary(df: pd.DataFrame) -> pd.DataFrame: pass ``` # Constraints - The data may contain missing values, which should be handled appropriately. - The `Date` column must be parsed as `datetime`. - Performance should be considered for large datasets. # Example Usage ```python import pandas as pd # Sample Data data = { \'Date\': [\'2021-01-01\', \'2021-01-02\', \'2021-01-03\', \'2021-01-01\', \'2021-01-02\'], \'Region\': [\'North\', \'South\', \'North\', \'West\', \'East\'], \'Product\': [\'A\', \'B\', \'A\', \'C\', \'A\'], \'Quantity\': [10, 15, 20, 25, 10], \'Price\': [100, 150, 100, 200, 100], } df = pd.DataFrame(data) # Convert Date to datetime df[\'Date\'] = pd.to_datetime(df[\'Date\']) # 1. Filter Sales Data north_sales = filter_sales_data(df, \'North\') print(north_sales) # 2. Calculate Total Sales total_sales = calculate_total_sales(df) print(total_sales) # 3. Average Sales Per Product avg_sales_product = average_sales_per_product(df) print(avg_sales_product) # 4. Sales Summary summary = sales_summary(df) print(summary) ``` **Note**: Ensure your functions are efficient and handle all edge cases appropriately.","solution":"import pandas as pd def filter_sales_data(df: pd.DataFrame, region: str) -> pd.DataFrame: Filter the sales data to include only the sales from the specified region. return df[df[\'Region\'] == region].reset_index(drop=True) def calculate_total_sales(df: pd.DataFrame) -> pd.Series: Calculate the total sales amount per day. df = df.copy() df[\'TotalSales\'] = df[\'Quantity\'] * df[\'Price\'] return df.groupby(\'Date\')[\'TotalSales\'].sum() def average_sales_per_product(df: pd.DataFrame) -> pd.DataFrame: Calculate the average sales for each product across all regions. avg_sales = df.groupby(\'Product\').apply(lambda x: (x[\'Quantity\'] * x[\'Price\']).sum() / x[\'Quantity\'].sum()) return avg_sales.reset_index(name=\'AverageSales\') def sales_summary(df: pd.DataFrame) -> pd.DataFrame: Return a summary DataFrame with Total Quantity, Total Revenue, and Average Price per product. df = df.copy() df[\'TotalRevenue\'] = df[\'Quantity\'] * df[\'Price\'] summary = df.groupby(\'Product\').agg( TotalQuantity = pd.NamedAgg(column=\'Quantity\', aggfunc=\'sum\'), TotalRevenue = pd.NamedAgg(column=\'TotalRevenue\', aggfunc=\'sum\'), AveragePrice = pd.NamedAgg(column=\'Price\', aggfunc=\'mean\') ).reset_index() return summary"},{"question":"Objective: Demonstrate your understanding of the `seaborn` library, specifically focusing on the `hls_palette` function to generate and utilize custom color palettes for data visualization. Problem Statement: You are provided with a dataset of tips from a restaurant\'s customers (tip.csv). Your task is to visualize various aspects of this dataset using seaborn and the `hls_palette` function. You must create custom palettes to highlight different features of the dataset. Dataset Information: The dataset contains the following columns: - `total_bill`: Total bill amount. - `tip`: Tip given. - `sex`: Gender of the person who paid the bill. - `smoker`: Whether the person is a smoker. - `day`: Day of the week. - `time`: Time of the day (Lunch or Dinner). - `size`: Number of people at the table. Requirements: 1. **Load the Dataset:** Load the `tip.csv` dataset into a pandas dataframe. 2. **Basic Scatter Plot:** - Use seaborn to create a basic scatter plot showing `total_bill` vs. `tip`. - Use a default color palette. 3. **Customized Color Palette:** - Create a custom HLS color palette with the following specifications: - 10 colors - Lightness set to 0.5 - Saturation set to 0.8 - Use this palette to recolor the scatter plot based on the `day` of the week. 4. **Adjusted Saturation Plot:** - Create another custom HLS palette with: - Lightness set to 0.6 - Saturation reduced to 0.3 - Use this palette to create a bar plot showing the average `total_bill` per `day`. 5. **Hue Shifted Palette:** - Create an HLS palette starting from hue 0.25, keeping lightness and saturation as default. - Use this palette to create a KDE plot for the distribution of `total_bill`. 6. **Continuous Colormap:** - Generate a continuous colormap with the HLS palette. - Use this colormap in a heatmap showing the correlation between all numerical columns in the dataset. Constraints: - You must use the seaborn library and the `hls_palette` function for all visualizations. - Your solutions should be efficient and make good use of seaborn\'s capabilities for customizing palettes. Expected Input and Output Formats: - **Input:** The function to visualize data will take `tip.csv` file as input. - **Output:** The function must display the required seaborn plots with custom palettes. Performance Requirements: - The visualizations must be generated efficiently without significant delay when working with a dataset of this size. Example: Given the dataset, your result should include the following: 1. A default scatter plot for `total_bill` vs. `tip`. 2. A scatter plot recolored using a custom palette for the `day`. 3. A bar plot colored by a custom palette showing average `total_bill` per `day`. 4. A KDE plot for the distribution of `total_bill` using a hue shifted palette. 5. A heatmap showing correlations using a continuous HLS colormap. ```python # Example function signature def visualize_tips_data(file_path: str): import seaborn as sns import pandas as pd import matplotlib.pyplot as plt # Load the dataset tips = pd.read_csv(file_path) # Basic Scatter Plot # Your implementation goes here # Customized Color Palette Scatter Plot # Your implementation goes here # Adjusted Saturation Bar Plot # Your implementation goes here # Hue Shifted Palette KDE Plot # Your implementation goes here # Continuous Colormap Heatmap # Your implementation goes here # Show plots plt.show() ```","solution":"def visualize_tips_data(file_path: str): import seaborn as sns import pandas as pd import matplotlib.pyplot as plt # Load the dataset tips = pd.read_csv(file_path) # 1. Basic Scatter Plot plt.figure(figsize=(8, 6)) sns.scatterplot(data=tips, x=\'total_bill\', y=\'tip\') plt.title(\'Scatter Plot of Total Bill vs Tip\') plt.show() # 2. Customized Color Palette Scatter Plot plt.figure(figsize=(8, 6)) custom_palette_1 = sns.hls_palette(10, l=0.5, s=0.8) sns.scatterplot(data=tips, x=\'total_bill\', y=\'tip\', hue=\'day\', palette=custom_palette_1) plt.title(\'Scatter Plot of Total Bill vs Tip with Custom HLS Palette by Day\') plt.show() # 3. Adjusted Saturation Bar Plot plt.figure(figsize=(8, 6)) custom_palette_2 = sns.hls_palette(10, l=0.6, s=0.3) sns.barplot(data=tips, x=\'day\', y=\'total_bill\', palette=custom_palette_2, ci=None) plt.title(\'Average Total Bill per Day with Custom HLS Palette\') plt.show() # 4. Hue Shifted Palette KDE Plot plt.figure(figsize=(8, 6)) custom_palette_3 = sns.hls_palette(10, h=0.25) sns.kdeplot(data=tips, x=\'total_bill\', hue=\'sex\', palette=custom_palette_3, fill=True) plt.title(\'KDE Plot of Total Bill Distribution with Hue Shifted Palette\') plt.show() # 5. Continuous Colormap Heatmap plt.figure(figsize=(10, 8)) heatmap_palette = sns.hls_palette(256) corr = tips[[\'total_bill\', \'tip\', \'size\']].corr() sns.heatmap(corr, annot=True, cmap=heatmap_palette) plt.title(\'Heatmap of Correlation Matrix\') plt.show()"},{"question":"# Pandas Advanced Exercise: DataFrame Manipulation and Arithmetic Operations Objective: Demonstrate your understanding of pandas DataFrame manipulations, data alignment, and arithmetic operations. Problem Statement: You are given two dictionaries representing data collected from different sources. Your task is to: 1. Create `Series` objects from these dictionaries. 2. Create a `DataFrame` from these `Series`. 3. Perform certain operations on the `DataFrame`. 4. Handle missing data appropriately. 5. Add new columns derived from existing columns. Input: 1. Two dictionaries: - `data1 = {\\"A\\": [1, 2, np.nan, 4], \\"B\\": [5, np.nan, 7, 8], \\"C\\": [9, 10, 11, 12]}` - `data2 = {\\"B\\": [1, 2, 3], \\"D\\": [4, 5, 6], \\"E\\": [7, 8, 9]}` 2. A list of labels for rows: `index_labels = [\\"r1\\", \\"r2\\", \\"r3\\", \\"r4\\"]` Expected Steps and Outputs: 1. Create `Series` objects from `data1` and `data2`, and ensure the indices are labeled as given in `index_labels`. 2. Create a `DataFrame` named `df1` by combining the `Series` from `data1`. 3. Create another `DataFrame` named `df2` by combining the `Series` from `data2`. Since `df2` has fewer rows than `index_labels`, use `index_labels[:3]` for its indices. 4. Align `df2` with `df1` such that both DataFrames have the same indices and union of columns. 5. Handle missing data by filling NaN values in `df1` with the mean value of each column. 6. Add a new column `Sum` to `df1` which is the row-wise sum of columns `A`, `B`, and `C`. 7. Perform an element-wise addition of `df1` and `df2`. If an element in `df2` is missing, treat it as zero (do not fill df2 with zeros explicitly). Constraints: 1. You must use pandas methods and attributes as much as possible. 2. Ensure no data is discarded unless explicitly specified (e.g., handling missing values). Sample Output: ```python import pandas as pd import numpy as np # Input data data1 = {\\"A\\": [1, 2, np.nan, 4], \\"B\\": [5, np.nan, 7, 8], \\"C\\": [9, 10, 11, 12]} data2 = {\\"B\\": [1, 2, 3], \\"D\\": [4, 5, 6], \\"E\\": [7, 8, 9]} index_labels = [\\"r1\\", \\"r2\\", \\"r3\\", \\"r4\\"] # Steps to produce the expected output: s1 = pd.Series(data1[\\"A\\"], index=index_labels) s2 = pd.Series(data1[\\"B\\"], index=index_labels) s3 = pd.Series(data1[\\"C\\"], index=index_labels) df1 = pd.DataFrame({\'A\': s1, \'B\': s2, \'C\': s3}) print(\\"DataFrame df1:n\\", df1) s4 = pd.Series(data2[\\"B\\"], index=index_labels[:3]) s5 = pd.Series(data2[\\"D\\"], index=index_labels[:3]) s6 = pd.Series(data2[\\"E\\"], index=index_labels[:3]) df2 = pd.DataFrame({\'B\': s4, \'D\': s5, \'E\': s6}) print(\\"DataFrame df2:n\\", df2) df2 = df2.reindex(index_labels) print(\\"Aligned df2:n\\", df2) df1_filled = df1.apply(lambda x: x.fillna(x.mean()), axis=0) print(\\"DataFrame df1 with NaNs filled:n\\", df1_filled) df1_filled[\'Sum\'] = df1_filled[[\'A\', \'B\', \'C\']].sum(axis=1) print(\\"DataFrame df1 with Sum column:n\\", df1_filled) result_df = df1_filled.add(df2, fill_value=0) print(\\"Element-wise addition of df1 and df2:n\\", result_df) ``` Output of the above code: ``` DataFrame df1: A B C r1 1.0 5.0 9.0 r2 2.0 NaN 10.0 r3 NaN 7.0 11.0 r4 4.0 8.0 12.0 DataFrame df2: B D E r1 1.0 4.0 7.0 r2 2.0 5.0 8.0 r3 3.0 6.0 9.0 Aligned df2: B D E r1 1.0 4.0 7.0 r2 2.0 5.0 8.0 r3 3.0 6.0 9.0 r4 NaN NaN NaN DataFrame df1 with NaNs filled: A B C r1 1.0 5.0 9.0 r2 2.0 6.0 10.0 r3 2.333333 7.0 11.0 r4 4.0 8.0 12.0 DataFrame df1 with Sum column: A B C Sum r1 1.0 5.0 9.0 15.0 r2 2.0 6.0 10.0 18.0 r3 2.333333 7.0 11.0 20.333333 r4 4.0 8.0 12.0 24.0 Element-wise addition of df1 and df2: A B C D E Sum r1 1.0 6.0 9.0 4.0 7.0 15.0 r2 2.0 8.0 10.0 5.0 8.0 18.0 r3 2.333333 10.0 11.0 6.0 9.0 20.333333 r4 4.0 8.0 12.0 NaN NaN 24.0 ``` **Note:** Make sure to implement the code without any hardcoded values other than the provided data.","solution":"import pandas as pd import numpy as np def dataframe_manipulation(data1, data2, index_labels): # Create Series from data1 and data2 s1 = pd.Series(data1[\\"A\\"], index=index_labels) s2 = pd.Series(data1[\\"B\\"], index=index_labels) s3 = pd.Series(data1[\\"C\\"], index=index_labels) s4 = pd.Series(data2[\\"B\\"], index=index_labels[:3]) s5 = pd.Series(data2[\\"D\\"], index=index_labels[:3]) s6 = pd.Series(data2[\\"E\\"], index=index_labels[:3]) # Create DataFrames from the Series df1 = pd.DataFrame({\'A\': s1, \'B\': s2, \'C\': s3}) df2 = pd.DataFrame({\'B\': s4, \'D\': s5, \'E\': s6}) # Align df2 with df1 df2 = df2.reindex(index_labels) # Fill NaN values in df1 with the mean of each column df1_filled = df1.apply(lambda x: x.fillna(x.mean()), axis=0) # Add new column \'Sum\' to df1 which is the row-wise sum of columns \'A\', \'B\', \'C\' df1_filled[\'Sum\'] = df1_filled[[\'A\', \'B\', \'C\']].sum(axis=1) # Perform an element-wise addition of df1 and df2 result_df = df1_filled.add(df2, fill_value=0) return df1, df2, df1_filled, result_df"},{"question":"# Python API for Tuple Manipulation Objective: Create a Python module that mimics a subset of the C-API for manipulating tuples, but in pure Python. Task: Implement a Python class `CustomTuple` with the following methods: 1. `__init__(self, *args)`: - Initializes the tuple with the given elements. 2. `size(self) -> int`: - Returns the size of the tuple. 3. `get_item(self, pos: int)`: - Returns the item at the specified position. - Raises an `IndexError` if the position is out of bounds. 4. `set_item(self, pos: int, value)`: - Sets the item at the specified position to the new value. - Raises an `IndexError` if the position is out of bounds. 5. `get_slice(self, low: int, high: int)`: - Returns a new `CustomTuple` that is a slice of the original tuple from `low` (inclusive) to `high` (exclusive). - Raises an `IndexError` if the slicing indices are out of bounds. 6. `resize(self, new_size: int)`: - Resizes the tuple to the new size by truncating or padding with `None`. - If the new size is greater, the new elements should be `None`. Constraints: - The `CustomTuple` should internally use a list to store the tuple elements. - You are not allowed to use the built-in `tuple` type for storing the elements. - The resize operation should only alter the tuple if it contains exactly one reference to the object, mimicking the immutability restriction (hint: use a simple reference count emulation). Example Usage: ```python t = CustomTuple(1, 2, 3) print(t.size()) # Output: 3 print(t.get_item(0)) # Output: 1 t.set_item(2, 4) print(t.get_item(2)) # Output: 4 s = t.get_slice(0, 2) print(s.size()) # Output: 2 print(s.get_item(1)) # Output: 2 t.resize(5) print(t.size()) # Output: 5 print(t.get_item(3)) # Output: None ``` Submission: Submit the complete implementation of the `CustomTuple` class.","solution":"class CustomTuple: def __init__(self, *args): self._elements = list(args) self._ref_count = 1 # Simulating a simplistic reference counter def size(self) -> int: return len(self._elements) def get_item(self, pos: int): if pos < 0 or pos >= len(self._elements): raise IndexError(\\"Index out of bounds\\") return self._elements[pos] def set_item(self, pos: int, value): if pos < 0 or pos >= len(self._elements): raise IndexError(\\"Index out of bounds\\") self._elements[pos] = value def get_slice(self, low: int, high: int): if low < 0 or high > len(self._elements) or low > high: raise IndexError(\\"Index out of bounds\\") return CustomTuple(*self._elements[low:high]) def resize(self, new_size: int): if new_size < 0: raise ValueError(\\"New size must be non-negative\\") if self._ref_count == 1: # Only allow resizing if we have exactly one reference current_size = len(self._elements) if new_size < current_size: self._elements = self._elements[:new_size] else: self._elements.extend([None] * (new_size - current_size))"},{"question":"Objective Design and implement an asynchronous program that utilizes asyncio’s synchronization primitives to simulate an online ticket booking system. Problem Statement You need to implement a ticket booking system for a concert. This system must support the following functionalities: 1. Booking a ticket. 2. Checking the number of remaining tickets. You are given a fixed number of tickets initially. The booking should be handled in a thread-safe manner without overselling the tickets. Multiple users can attempt to book tickets simultaneously, and each booking operation should ensure that no tickets are oversold. Requirements: 1. **Function Definitions**: - `async def book_ticket(ticket_system)`: - This coroutine should attempt to book one ticket. If successful, print \\"Ticket booked!\\" and decrement the remaining tickets count. If no tickets are left, print \\"Sold Out!\\". - `async def remaining_tickets(ticket_system)`: - This coroutine should return the number of remaining tickets. 2. **Synchronization Mechanism**: - Use an appropriate asyncio synchronization primitive to ensure that ticket booking is handled in a thread-safe manner. 3. **Input**: - `ticket_system`: An instance of the `TicketBookingSystem` class. 4. **Class Definition**: - You need to define a class `TicketBookingSystem` which maintains the count of tickets and handles the booking and querying in a synchronized manner. Initialize the system with a given number of tickets. 5. **Example Usage**: ```python import asyncio from your_module import TicketBookingSystem, book_ticket, remaining_tickets async def main(): system = TicketBookingSystem(5) # Initialize TicketBookingSystem with 5 tickets # Create multiple tasks to book tickets concurrently tasks = [book_ticket(system) for _ in range(7)] await asyncio.gather(*tasks) # Check remaining tickets print(f\\"Remaining Tickets: {await remaining_tickets(system)}\\") asyncio.run(main()) ``` Constraints: - Use Python 3.10 and the `asyncio` module. - You are not allowed to use any other concurrency libraries or modules besides `asyncio`. - Ensure that the implementation does not oversell tickets. Expected Output: The output should indicate whether each booking attempt was successful or if tickets were sold out, and finally, it should print the number of remaining tickets. ```python Ticket booked! Ticket booked! Ticket booked! Ticket booked! Ticket booked! Sold Out! Sold Out! Remaining Tickets: 0 ``` Performance Requirements: - The implementation should handle multiple booking attempts concurrently without overselling tickets. - Ensure the solution performs efficiently with minimal lock contention.","solution":"import asyncio class TicketBookingSystem: def __init__(self, tickets): self.tickets = tickets self.lock = asyncio.Lock() async def book_ticket(self): async with self.lock: if self.tickets > 0: self.tickets -= 1 print(\\"Ticket booked!\\") return True else: print(\\"Sold Out!\\") return False async def remaining_tickets(self): async with self.lock: return self.tickets async def book_ticket(ticket_system): await ticket_system.book_ticket() async def remaining_tickets(ticket_system): return await ticket_system.remaining_tickets()"},{"question":"You are tasked with creating a script that takes a list of file paths, encodes them to uuencode format, and decodes them back to their original form to ensure data integrity. Function Signature ```python def encode_decode_files(file_paths: List[str], encoded_dir: str, decoded_dir: str) -> None: ``` Implementation Details 1. **Parameters**: - `file_paths`: A list of strings where each string is a path to a file to be encoded. - `encoded_dir`: A string representing the directory where encoded files should be saved. - `decoded_dir`: A string representing the directory where decoded files should be saved. 2. **Steps**: - For each file in the `file_paths` list, perform the following steps: - Read the file and encode it using `uu.encode()`. Save the encoded file in the `encoded_dir` directory with an extension `.uue`. - Read the encoded file and decode it using `uu.decode()`. Save the decoded file in the `decoded_dir` directory, ensuring it retains its original name. 3. **Constraints**: - Ensure the directories `encoded_dir` and `decoded_dir` exist before saving the files. - Handle exceptions gracefully, specifically catching `uu.Error` and printing an appropriate error message. 4. **Input/Output**: - Input: A list `[ \\"file1.txt\\", \\"file2.bin\\", ...]`, path to `encoded_dir`, path to `decoded_dir`. - Output: Encoded files saved with `.uue` extension and decoded files saved with their original names in specified directories. 5. **Performance**: - Efficiency in reading/writing files. - Proper exception handling to avoid program crashes. Example ```python file_paths = [\\"sample1.txt\\", \\"sample2.jpg\\"] encoded_dir = \\"./encoded\\" decoded_dir = \\"./decoded\\" encode_decode_files(file_paths, encoded_dir, decoded_dir) ``` After running this function: - Encoded files (`sample1.txt.uue`, `sample2.jpg.uue`) should be present in `./encoded` directory. - Decoded files (`sample1.txt`, `sample2.jpg`) should be present in `./decoded` directory, matching the original files in content.","solution":"import os import uu from typing import List def encode_decode_files(file_paths: List[str], encoded_dir: str, decoded_dir: str) -> None: Encodes each file in file_paths to uuencode format and saves it to encoded_dir, then decodes it back to the original format and saves it to decoded_dir. # Ensure the directories exist os.makedirs(encoded_dir, exist_ok=True) os.makedirs(decoded_dir, exist_ok=True) for file_path in file_paths: file_name = os.path.basename(file_path) # Encode the file try: encoded_file_path = os.path.join(encoded_dir, file_name + \'.uue\') with open(file_path, \'rb\') as input_file: with open(encoded_file_path, \'wb\') as encoded_file: uu.encode(input_file, encoded_file) except uu.Error as e: print(f\\"Failed to encode {file_path}: {e}\\") continue # Decode the file try: decoded_file_path = os.path.join(decoded_dir, file_name) with open(encoded_file_path, \'rb\') as input_encoded_file: with open(decoded_file_path, \'wb\') as decoded_file: uu.decode(input_encoded_file, decoded_file) except uu.Error as e: print(f\\"Failed to decode {encoded_file_path}: {e}\\")"},{"question":"Objective You are required to implement a function that processes a list of strings to find and format specific text patterns. Description Implement a function `format_contact_info(text_list: List[str]) -> List[str]` that takes a list of strings as input. Each string contains a person\'s name and their contact information (email and phone number). Your task is to find and format all phone numbers and emails in the strings and return the formatted list. 1. **Email Format**: Convert all email addresses to lowercase. 2. **Phone Number Format**: Format phone numbers to follow the pattern `(XXX) XXX-XXXX`. Input - A list of strings (1 ≤ len(text_list) ≤ 1000) - Each string may contain a person\'s name, email, and phone number in arbitrary order. Output - A list of strings with the emails converted to lowercase and phone numbers formatted to `(XXX) XXX-XXXX`. Constraints - Assume emails follow the pattern `user@domain.tld`. - Assume phone numbers are US-based and can be in one of these formats: `123-456-7890`, `123.456.7890`, or `(123) 456 7890`. Example ```python text_list = [ \\"John Doe Email: JOHN.DOE@EXAMPLE.COM Phone: 123-456-7890\\", \\"Jane Smith Phone: (987) 654 3210 Email: JANE_SMITH@EXAMPLE.ORG\\", \\"Alice Johnson Email: Alice.J@Example.Com Phone: 321.654.9870\\", ] formatted_list = format_contact_info(text_list) # Expected Output: # [ # \\"John Doe email: john.doe@example.com phone: (123) 456-7890\\", # \\"Jane Smith phone: (987) 654-3210 email: jane_smith@example.org\\", # \\"Alice Johnson email: alice.j@example.com phone: (321) 654-9870\\", #] ``` Function Signature ```python from typing import List def format_contact_info(text_list: List[str]) -> List[str]: pass ``` Hint You may find regular expressions useful for identifying and reformatting the phone numbers and emails.","solution":"import re from typing import List def format_contact_info(text_list: List[str]) -> List[str]: def format_phone_number(phone: str) -> str: digits = re.sub(r\'D\', \'\', phone) return f\\"({digits[:3]}) {digits[3:6]}-{digits[6:]}\\" formatted_list = [] for text in text_list: # Find and format email to lowercase emails = re.findall(r\'b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+.[A-Z|a-z]+b\', text) for email in emails: text = text.replace(email, email.lower()) # Find and format phone numbers phones = re.findall(r\'bd{3}[-.]d{3}[-.]d{4}b|(d{3})s*d{3}[-s]d{4}\', text) for phone in phones: formatted_phone = format_phone_number(phone) text = text.replace(phone, formatted_phone) formatted_list.append(text) return formatted_list"},{"question":"<|Analysis Begin|> The provided documentation is a comprehensive guide on using Python’s logging module, with examples and detailed explanations covering many aspects such as logging configuration, using logging in multiple modules, threading, multiple handlers and formatters, custom handling of levels, writing filters, logging to various destinations, and handling logging in network or multi-process scenarios. Key aspects covered: 1. Basic logging setup and usage. 2. Configuring logging in various modules. 3. Using multiple handlers and formatters. 4. Customizing log levels and filters. 5. Sending and receiving logs across networks. 6. Logging in a multi-threaded or multi-processed environment. 7. Adding contextual information to logs. 8. Customizing `LogRecord`, `Formatter`, and other logging components. 9. Handling large logs with rotation and compression. Given the extensive details provided, creating a coding question that tests a deep understanding of the logging module and its advanced features is feasible. The question can focus on designing a logging system that incorporates several advanced features such as custom log filters, multiple handlers, and handling logs in a multi-threaded environment. <|Analysis End|> <|Question Begin|> # Coding Assessment Question **Objective:** Design and implement a comprehensive logging system using the Python logging module that effectively handles logging in a multi-threaded environment, uses multiple handlers with custom formats, and includes a custom log filter. # Problem Statement: You are tasked with creating a logging system for a financial application that must log messages to both a console and a file. The logging system must handle multiple threads generating log messages and must filter out certain messages based on custom logic. # Requirements: 1. **Logging to Multiple Handlers:** - Log messages should be sent to both the console and a file. - The file should use a rotating file handler to avoid large log files. 2. **Custom Log Filter:** - Implement a custom log filter that filters out log messages containing the word \\"private\\". 3. **Multi-threading:** - Simulate multiple threads generating log messages. - Ensure that the logging system handles messages from all threads gracefully and without performance issues or message loss. # Implementation Details: 1. **Logger Configuration:** - Create a logger named `financial_app`. - Set the logging level to `DEBUG` to capture all log messages. - Configure the logger to use two handlers: - A console handler that logs `INFO` level and above messages. - A rotating file handler that logs all messages and rotates the log file after it reaches 1MB, keeping the last 5 logs. 2. **Custom Filter:** - Implement a custom log filter class (`PrivateDataFilter`) that filters out all messages containing the word \\"private\\". 3. **Multi-threaded Logging:** - Implement a function that simulates log messages from different threads. - Use Python’s `threading` module to spawn multiple threads, each generating log messages at different levels and including some messages with the word \\"private\\" for filtering. # Input Parameters: Your function should accept the following parameters: 1. `num_threads` (int): The number of threads to spawn. 2. `num_messages` (int): The number of log messages each thread should generate. # Expected Output: The output should include: - Log messages written to the console, excluding messages filtered by `PrivateDataFilter`. - Log files created and rotated by the rotating file handler, containing all log messages except those filtered out. # Constraints: - The system should handle up to 10 threads each generating up to 1000 log messages efficiently. - Ensure the log messages are not lost and are thread-safe. # Example: Here is an example of how you might use your logging system: ```python configure_logging_system() simulate_log_messages(num_threads=5, num_messages=50) ``` # Function Signatures: ```python def configure_logging_system() -> None: Configure the logging system with required handlers, filters, and formats. pass def simulate_log_messages(num_threads: int, num_messages: int) -> None: Simulate multiple threads generating log messages. Parameters: num_threads (int): The number of threads to spawn. num_messages (int): The number of log messages each thread should generate. pass ``` **Deliverables:** 1. The implementation of the `configure_logging_system` function. 2. The implementation of the `simulate_log_messages` function. 3. Any additional helper classes or functions required to complete the task. Ensure your code is well-documented and includes comments explaining key parts of the implementation.","solution":"import logging import threading from logging.handlers import RotatingFileHandler class PrivateDataFilter(logging.Filter): def filter(self, record): return \'private\' not in record.msg def configure_logging_system(): logger = logging.getLogger(\'financial_app\') logger.setLevel(logging.DEBUG) # Console handler for INFO level and above console_handler = logging.StreamHandler() console_handler.setLevel(logging.INFO) console_formatter = logging.Formatter(\'%(levelname)s - %(message)s\') console_handler.setFormatter(console_formatter) # Rotating file handler for all levels file_handler = RotatingFileHandler(\'financial_app.log\', maxBytes=1_000_000, backupCount=5) file_handler.setLevel(logging.DEBUG) file_formatter = logging.Formatter(\'%(asctime)s - %(name)s - %(levelname)s - %(message)s\') file_handler.setFormatter(file_formatter) # Adding custom filter private_data_filter = PrivateDataFilter() console_handler.addFilter(private_data_filter) file_handler.addFilter(private_data_filter) # Add handlers to the logger logger.addHandler(console_handler) logger.addHandler(file_handler) def worker_thread(thread_id, num_messages): logger = logging.getLogger(\'financial_app\') for i in range(num_messages): if i % 5 == 0: logger.debug(f\'Thread {thread_id} debug message {i}\') elif i % 5 == 1: logger.info(f\'Thread {thread_id} info message {i}\') elif i % 5 == 2: logger.warning(f\'Thread {thread_id} warning message {i}\') elif i % 5 == 3: logger.error(f\'Thread {thread_id} error message {i}\') else: logger.critical(f\'Thread {thread_id} critical private message {i}\') def simulate_log_messages(num_threads, num_messages): threads = [] for i in range(num_threads): thread = threading.Thread(target=worker_thread, args=(i, num_messages)) threads.append(thread) thread.start() for thread in threads: thread.join()"},{"question":"# Question: Advanced Seaborn Customization Challenge You are tasked with creating a function that generates a complex, aesthetically pleasing plot using Seaborn. Your function should: 1. Utilize Seaborn\'s figure styles to create a subplot grid with different styles. 2. Apply context scaling for different plot elements. 3. Implement custom aesthetic modifications using the `rc` parameters. 4. Use appropriate themes that fit the context of each subplot. Function Signature ```python def advanced_seaborn_plot(data: np.ndarray): Generates a complex plot using various Seaborn customizations. Args: data (np.ndarray): A 2D NumPy array of numerical data to be visualized. Returns: None Displays a multi-faceted plot using Seaborn and Matplotlib. ``` Input - `data`: A 2D NumPy array (shape: nxm) where `n` is the number of observations and `m` is the number of variables. Task Requirements 1. **Create a subplot grid (2x2)**: - Top-left plot: Style as `darkgrid`, context as `notebook`. - Top-right plot: Style as `whitegrid`, context as `talk`. - Bottom-left plot: Style as `ticks`, context as `poster`. - Bottom-right plot: Style as `white`, context as `paper`. 2. **Apply a custom face color** for the `ticks` style plot: Set `axes.facecolor` to a light grey (`\\".9\\"`). 3. **Remove the top and right spines** for the `white` style plot. 4. **Create a simple line plot** in each subplot using the provided `data`. Each subplot should visualize the same data but with the different styles applied. 5. **Ensure consistent aspect ratio** for all subplots. 6. **Set the overall theme** to `set_theme` with default parameters before creating individual plots. Example Here\'s an example of how your function might be called, although you will have to use visualization libraries, so the actual output will be the plotted figures: ```python import numpy as np data = np.random.normal(size=(20, 6)) + np.arange(6) / 2 advanced_seaborn_plot(data) ``` Constraints - Use Seaborn and Matplotlib libraries. - The function should display the plot directly; no need to return any value. This challenge tests your ability to apply multiple customization techniques within Seaborn and create a comprehensive, visually appealing figure that demonstrates advanced plotting skills.","solution":"import numpy as np import matplotlib.pyplot as plt import seaborn as sns def advanced_seaborn_plot(data: np.ndarray): Generates a complex plot using various Seaborn customizations. Args: data (np.ndarray): A 2D NumPy array of numerical data to be visualized. Returns: None Displays a multi-faceted plot using Seaborn and Matplotlib. # Set the overall theme sns.set_theme() # Create a 2x2 subplot grid fig, axes = plt.subplots(2, 2, figsize=(12, 10)) # Top-left plot: Style as `darkgrid`, context as `notebook` with sns.plotting_context(\\"notebook\\"): sns.set_style(\\"darkgrid\\") sns.lineplot(data=data, ax=axes[0, 0]) axes[0, 0].set_title(\'Darkgrid Style - Notebook Context\') # Top-right plot: Style as `whitegrid`, context as `talk` with sns.plotting_context(\\"talk\\"): sns.set_style(\\"whitegrid\\") sns.lineplot(data=data, ax=axes[0, 1]) axes[0, 1].set_title(\'Whitegrid Style - Talk Context\') # Bottom-left plot: Style as `ticks`, context as `poster` with sns.plotting_context(\\"poster\\"): sns.set_style(\\"ticks\\", {\'axes.facecolor\': \'.9\'}) sns.lineplot(data=data, ax=axes[1, 0]) axes[1, 0].set_title(\'Ticks Style - Poster Context\') sns.despine(ax=axes[1, 0]) # Bottom-right plot: Style as `white`, context as `paper` with sns.plotting_context(\\"paper\\"): sns.set_style(\\"white\\") sns.lineplot(data=data, ax=axes[1, 1]) axes[1, 1].set_title(\'White Style - Paper Context\') sns.despine(ax=axes[1, 1]) # Adjust layout and show plot plt.tight_layout() plt.show()"},{"question":"Objective: Create visualizations using `seaborn.objects` to compare the flipper lengths of penguins across different islands, and further break down the analysis by the sex of the penguins. Task: 1. Load the `penguins` dataset from seaborn. 2. Create a bar plot that compares the number of penguins on different islands. 3. Create a histogram to estimate the distribution of `flipper_length_mm` of penguins across different islands. 4. Modify the histogram to normalize the data to proportions within each island. 5. Further divide the histogram to show distributions for each sex within each island, and represent this part-whole relationship using a stack transformation. Requirements: - **Input:** - Load the `penguins` dataset using `seaborn.load_dataset(\\"penguins\\")`. - **Output:** A series of plots visualizing the data as described above. - **Constraints:** - Use `seaborn.objects.Plot`, `so.Bar`, `so.Hist`, `so.Stack` to create the plots. - Normalize the histogram data using the `stat=\\"proportion\\"` parameter appropriately. - Facet the plots to compare distributions across islands and add additional layers of grouping by sex. - Represent part-whole relationships using the stack transformation. Example Code Outline: ```python import seaborn.objects as so from seaborn import load_dataset # Load the penguins dataset penguins = load_dataset(\'penguins\') # Task 2: Create a bar plot for the number of penguins on different islands bar_plot = so.Plot(penguins, \'island\').add(so.Bar(), so.Hist()) bar_plot.show() # Task 3: Create a histogram to estimate flipper length distribution across islands histogram_plot = so.Plot(penguins, \'flipper_length_mm\').facet(\'island\').add(so.Bars(), so.Hist()) histogram_plot.show() # Task 4: Normalize the histogram data to proportions within each island normalized_histogram_plot = so.Plot(penguins, \'flipper_length_mm\').facet(\'island\').add(so.Bars(), so.Hist(stat=\'proportion\')) normalized_histogram_plot.show() # Task 5: Further divide the histogram by sex and represent using stack transformation stacked_histogram_plot = so.Plot(penguins, \'flipper_length_mm\').facet(\'island\').add(so.Bars(), so.Hist(stat=\'proportion\'), so.Stack(), color=\'sex\') stacked_histogram_plot.show() ``` Notes: - Ensure you appropriately use the `so.Bars()`, `so.Hist()`, `so.Stack()`, and `color` for sex differentiation within plots. - Use `facet` to create subplots for each island. Complete these tasks to demonstrate your comprehension of using `seaborn.objects` to visualize and compare grouped data distributions.","solution":"import seaborn.objects as so from seaborn import load_dataset def plot_penguin_distributions(): # Load the penguins dataset penguins = load_dataset(\'penguins\') # Task 2: Create a bar plot for the number of penguins on different islands bar_plot = so.Plot(penguins, \'island\').add(so.Bar(), so.Hist()) bar_plot.show() # Task 3: Create a histogram to estimate flipper length distribution across islands histogram_plot = so.Plot(penguins, \'flipper_length_mm\').facet(\'island\').add(so.Bars(), so.Hist()) histogram_plot.show() # Task 4: Normalize the histogram data to proportions within each island normalized_histogram_plot = so.Plot(penguins, \'flipper_length_mm\').facet(\'island\').add(so.Bars(), so.Hist(stat=\'proportion\')) normalized_histogram_plot.show() # Task 5: Further divide the histogram by sex and represent using stack transformation stacked_histogram_plot = so.Plot(penguins, \'flipper_length_mm\').facet(\'island\').add(so.Bars(), so.Hist(stat=\'proportion\'), so.Stack(), color=\'sex\') stacked_histogram_plot.show()"},{"question":"Objective: Demonstrate your understanding of the Python `site` module by implementing a function that dynamically manages module search paths and validates the presence of certain packages in the system. Problem Statement: You are tasked with writing a Python function named `validate_module_paths` that performs the following actions: 1. **Identify Paths**: Identify and add all site-specific directories to the Python module search path. 2. **Process Configuration Files**: Ensure that any `.pth` files in the identified directories are correctly processed, adding any valid paths to `sys.path`. 3. **Check for Modules**: Verify the presence of certain necessary modules in the updated `sys.path`. 4. **Return the Result**: Return a dictionary containing: - \\"site_paths\\": A list of all site-specific directories added to `sys.path`. - \\"modules_present\\": A list of module names that are present in the updated `sys.path`. - \\"modules_missing\\": A list of module names that are missing. Function Signature: ```python def validate_module_paths(module_names: list) -> dict: pass ``` Input: 1. `module_names`: A list of strings, where each string is the name of a module to check for (e.g., [\\"numpy\\", \\"pandas\\", \\"nonexistentmodule\\"]). Output: 1. A dictionary with the keys \\"site_paths\\", \\"modules_present\\", and \\"modules_missing\\". Example Usage: ```python result = validate_module_paths([\\"numpy\\", \\"pandas\\", \\"requests\\", \\"fake_module\\"]) print(result) ``` Expected Output: ```python { \\"site_paths\\": [\\"/usr/local/lib/python3.X/site-packages\\", ...], # actual paths may vary \\"modules_present\\": [\\"numpy\\", \\"pandas\\", \\"requests\\"], \\"modules_missing\\": [\\"fake_module\\"] } ``` Constraints: - Do not use any global variables. The function must be self-contained. - Assume that there may be any number of site-specific and user-specific directories. - The function should handle any valid Python environment, including virtual environments. Notes: - Use `site.getsitepackages()` and `site.getusersitepackages()` functions to get the paths. - Utilize the `importlib.util.find_spec` function to check for module existence.","solution":"import sys import site import importlib.util def validate_module_paths(module_names): Adds all site-specific directories to the Python module search path, processes .pth files, and verifies the presence of specified modules. Args: - module_names (list): A list of module names to check for their presence. Returns: - dict: A dictionary containing site paths, present modules, and missing modules. site_paths = site.getsitepackages() + [site.getusersitepackages()] # Add all site paths to sys.path if not already present for path in site_paths: if path not in sys.path: sys.path.append(path) # Prepare the output modules_present = [] modules_missing = [] # Check for requested modules for module in module_names: if importlib.util.find_spec(module) is not None: modules_present.append(module) else: modules_missing.append(module) return { \\"site_paths\\": site_paths, \\"modules_present\\": modules_present, \\"modules_missing\\": modules_missing, }"},{"question":"**Problem Statement: Robust File Reader** You are tasked with writing a function called `robust_file_reader` which reads a specified file and processes its contents line by line. The file reading operations should be robust, handling exceptions gracefully, and ensuring the file gets closed properly using compound statements. # Function Signature ```python def robust_file_reader(file_path: str) -> None: pass ``` # Input - `file_path`: a string representing the path to the file that needs to be read. # Requirements 1. **Reading the File**: Use a **with statement** to open the file, ensuring it gets properly closed after reading, even if an error occurs. 2. **Exception Handling**: Use a **try-except block** to handle potential errors: - If the file does not exist, catch the appropriate exception and print `\\"File not found.\\"`. - If there is an error in reading the file, catch the appropriate exception and print `\\"Error reading file.\\"`. - Catch any other general exceptions and print `\\"An unexpected error occurred.\\"`. 3. **Processing Each Line**: For each line in the file, strip leading and trailing whitespace and then print the line. # Constraints - Assume the file contains text data. - The function should handle large files efficiently. # Example Usage ```python # Assume the file at \'example.txt\' has the following contents: # \\"Hello, World!nPython is fun.nLearn Python.n\\" robust_file_reader(\'example.txt\') ``` # Expected Output ``` Hello, World! Python is fun. Learn Python. ``` # Additional Information - Ensure that the function does not raise any exceptions, and all are handled within the function. - The function should only print the lines if the file is read successfully.","solution":"def robust_file_reader(file_path: str) -> None: Reads a file and prints its contents line by line. Handles exceptions gracefully and ensures the file gets closed properly. try: with open(file_path, \'r\') as file: for line in file: print(line.strip()) except FileNotFoundError: print(\\"File not found.\\") except IOError: print(\\"Error reading file.\\") except Exception as e: print(f\\"An unexpected error occurred: {e}\\")"},{"question":"Objective: Write a function to analyze a given time series data using pandas\' rolling, expanding, and exponentially weighted window functions. The function should return a DataFrame with calculated statistics for each window type. Function Signature: ```python def analyze_time_series(df: pd.DataFrame, column: str, window_size: int) -> pd.DataFrame: pass ``` Inputs: - `df` (pd.DataFrame): A DataFrame containing the time series data with at least one numeric column and a DateTime index. - `column` (str): The name of the column on which to perform the window functions. - `window_size` (int): The size of the rolling and exponentially weighted windows. Outputs: - `result_df` (pd.DataFrame): A DataFrame with the following columns: - \'rolling_mean\': The mean of the rolling window. - \'rolling_sum\': The sum of the rolling window. - \'expanding_mean\': The mean of the expanding window. - \'expanding_sum\': The sum of the expanding window. - \'ewm_mean\': The mean of the exponentially weighted window. - \'ewm_sum\': The sum of the exponentially weighted window. Constraints: - You must handle edge cases where the window size exceeds the length of the time series. - Ensure the input DataFrame has a DateTime index. Example: ```python import pandas as pd data = { \'value\': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] } index = pd.date_range(start=\'2022-01-01\', periods=10, freq=\'D\') df = pd.DataFrame(data, index=index) result_df = analyze_time_series(df, \'value\', 3) print(result_df) ``` Expected Output: The resulting DataFrame should display the calculated statistics according to the specified window size and should have the DateTime index inherited from the input DataFrame.","solution":"import pandas as pd def analyze_time_series(df: pd.DataFrame, column: str, window_size: int) -> pd.DataFrame: Analyze a given time series data using rolling, expanding, and exponentially weighted window functions. Args: - df (pd.DataFrame): The input time series DataFrame with DateTime index. - column (str): The column on which to perform the window functions. - window_size (int): The size of the rolling and exponentially weighted windows. Returns: - result_df (pd.DataFrame): A DataFrame with calculated statistics for each window type. result_df = pd.DataFrame(index=df.index) # Rolling statistics result_df[\'rolling_mean\'] = df[column].rolling(window=window_size).mean() result_df[\'rolling_sum\'] = df[column].rolling(window=window_size).sum() # Expanding statistics result_df[\'expanding_mean\'] = df[column].expanding().mean() result_df[\'expanding_sum\'] = df[column].expanding().sum() # Exponentially weighted moving statistics result_df[\'ewm_mean\'] = df[column].ewm(span=window_size).mean() result_df[\'ewm_sum\'] = df[column].ewm(span=window_size).sum() return result_df"},{"question":"**Objective:** Implement a function that generates a dataset using one of scikit-learn\'s dataset generators and performs specific manipulations on the generated data. # Problem Statement Write a function `generate_and_plot_datasets` that uses scikit-learn\'s `make_classification` to generate a dataset and applies a series of transformations to the dataset. Specifically, you will: 1. Use `make_classification` to generate a dataset. 2. Normalize the dataset using `StandardScaler` from scikit-learn. 3. Apply a PCA transformation to reduce the dataset to 2 dimensions. 4. Plot the transformed dataset. Function Signature ```python def generate_and_plot_datasets(n_samples: int, n_features: int, n_informative: int, n_clusters_per_class: int, n_classes: int, random_state: int): Generates a classification dataset, normalizes it, applies PCA, and plots the result. Parameters: - n_samples: int - The number of samples to generate. - n_features: int - The total number of features. - n_informative: int - The number of informative features. - n_clusters_per_class: int - The number of clusters per class. - n_classes: int - The number of classes. - random_state: int - The seed used by the random number generator. Output: - This function should display a scatter plot of the transformed dataset. pass ``` Constraints - You must use `make_classification` from `sklearn.datasets` to generate the dataset. - The dataset should be normalized using `StandardScaler` from `sklearn.preprocessing`. - Apply PCA using `PCA` from `sklearn.decomposition` to reduce the data to 2 dimensions. - Use `matplotlib.pyplot` to plot the 2-dimensional data. Example The function call: ```python generate_and_plot_datasets(n_samples=100, n_features=20, n_informative=5, n_clusters_per_class=1, n_classes=3, random_state=42) ``` should generate a dataset with the specified parameters, normalize the dataset, reduce it to 2 dimensions using PCA, and display a scatter plot color-coded by class labels. # Notes - Ensure that the plot has meaningful axis titles and a legend to differentiate the classes. - Handle any edge cases, such as invalid parameter values (e.g., negative values for `n_samples`). **Resources:** - [make_classification documentation](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html) - [StandardScaler documentation](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) - [PCA documentation](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) - [matplotlib.pyplot documentation](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.html)","solution":"import matplotlib.pyplot as plt from sklearn.datasets import make_classification from sklearn.preprocessing import StandardScaler from sklearn.decomposition import PCA def generate_and_plot_datasets(n_samples: int, n_features: int, n_informative: int, n_clusters_per_class: int, n_classes: int, random_state: int): Generates a classification dataset, normalizes it, applies PCA, and plots the result. Parameters: - n_samples: int - The number of samples to generate. - n_features: int - The total number of features. - n_informative: int - The number of informative features. - n_clusters_per_class: int - The number of clusters per class. - n_classes: int - The number of classes. - random_state: int - The seed used by the random number generator. Output: - This function should display a scatter plot of the transformed dataset. # Generate the dataset X, y = make_classification(n_samples=n_samples, n_features=n_features, n_informative=n_informative, n_clusters_per_class=n_clusters_per_class, n_classes=n_classes, random_state=random_state) # Normalize the dataset scaler = StandardScaler() X_scaled = scaler.fit_transform(X) # Apply PCA to reduce to 2 dimensions pca = PCA(n_components=2) X_pca = pca.fit_transform(X_scaled) # Plot the dataset plt.figure(figsize=(10, 7)) scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap=\'viridis\', edgecolor=\'k\', s=60) plt.xlabel(\'PCA Component 1\') plt.ylabel(\'PCA Component 2\') plt.title(\'PCA of Generated Dataset\') plt.legend(*scatter.legend_elements(), title=\\"Class\\") plt.grid(True) plt.show()"},{"question":"Implement a function in PyTorch that computes the normalized L2 error between two tensors. Function Signature ```python def compute_normalized_l2_error(x: torch.Tensor, y: torch.Tensor) -> float: Computes the normalized L2 error between two tensors x and y. Args: x (torch.Tensor): The first input tensor. y (torch.Tensor): The second input tensor. Returns: float: The normalized L2 error between x and y. ``` Input - **x**: A tensor of any shape. - **y**: Another tensor of the same shape as x. Output - A single float value representing the normalized L2 error between the two tensors. Performance Constraints - The function should handle tensors with millions of elements efficiently. - Utilizing built-in PyTorch operations for tensor computation is encouraged for optimal performance. Example ```python import torch x = torch.tensor([1.0, 2.0, 3.0, 4.0]) y = torch.tensor([1.1, 2.1, 3.0, 4.0]) error = compute_normalized_l2_error(x, y) print(error) # Expected output should be a small value reflecting the normalized L2 difference. ``` Notes 1. The normalized L2 error is calculated as: [ text{normalized_l2_error} = frac{|x - y|_2}{|x|_2} ] Where (|x - y|_2) is the L2 norm (Euclidean distance) of the difference between tensors x and y, and (|x|_2) is the L2 norm of tensor x. 2. Ensure your implementation is numerically stable and handles edge cases gracefully. Hints - Utilize functions like `torch.norm` for computing L2 norm. - Handle possible zero divisions by including necessary checks.","solution":"import torch def compute_normalized_l2_error(x: torch.Tensor, y: torch.Tensor) -> float: Computes the normalized L2 error between two tensors x and y. Args: x (torch.Tensor): The first input tensor. y (torch.Tensor): The second input tensor. Returns: float: The normalized L2 error between x and y. if x.shape != y.shape: raise ValueError(\\"Input tensors must have the same shape\\") l2_norm_diff = torch.norm(x - y, p=2) l2_norm_x = torch.norm(x, p=2) if l2_norm_x == 0: raise ValueError(\\"L2 norm of tensor x must not be zero to avoid division by zero.\\") normalized_l2_error = l2_norm_diff / l2_norm_x return normalized_l2_error.item()"},{"question":"**Objective:** Demonstrate understanding of basic operations with numbers, strings, and lists in Python. Task: You are given a string containing a sequence of positive integers separated by commas. Your task is to write a function `process_sequence(data: str) -> dict` that performs the following operations: 1. **Parse the String:** Convert the string into a list of integers. 2. **Calculate Statistics:** Calculate the following statistics: - The sum of all numbers. - The average value of the numbers. - The smallest and largest number in the list. 3. **Generate Adjusted List:** Create a new list where each number is increased by the value of the smallest number in the original list. 4. **String Manipulation:** Convert the adjusted list back into a string with values separated by hyphens (`-`). 5. **Result Dictionary:** Return a dictionary containing all the calculated statistics and the adjusted string. Function Signature: ```python def process_sequence(data: str) -> dict: pass ``` Input: - `data` (str): A string of integers separated by commas. Example: `\\"3,5,7,2,8\\"` Output: - A dictionary with the following keys: - `\\"sum\\"`: Integer representing the sum of all numbers. - `\\"average\\"`: Float representing the average of the numbers. - `\\"smallest\\"`: Integer representing the smallest number. - `\\"largest\\"`: Integer representing the largest number. - `\\"adjusted_sequence\\"`: String representing the adjusted list as hyphen-separated values. You can assume: - The input string is always non-empty and correctly formatted. Example: ```python assert process_sequence(\\"3,5,7,2,8\\") == { \\"sum\\": 25, \\"average\\": 5.0, \\"smallest\\": 2, \\"largest\\": 8, \\"adjusted_sequence\\": \\"5-7-9-4-10\\" } ``` Requirements: - Implement the function considering proper coding practices. - Ensure the function handles edge cases like having a single number or all numbers being the same. - Remember to validate the input format and handle any potential errors gracefully.","solution":"def process_sequence(data: str) -> dict: Processes a sequence of integers separated by commas and computes various statistics. Parameters: - data (str): A string of integers separated by commas. Returns: - dict: A dictionary containing the calculated statistics and adjusted sequence. # Convert the input string into a list of integers numbers = list(map(int, data.split(\',\'))) # Calculate statistics total_sum = sum(numbers) average = total_sum / len(numbers) smallest = min(numbers) largest = max(numbers) # Generate the adjusted list adjusted_list = [num + smallest for num in numbers] adjusted_sequence = \'-\'.join(map(str, adjusted_list)) # Return the result as a dictionary result = { \\"sum\\": total_sum, \\"average\\": average, \\"smallest\\": smallest, \\"largest\\": largest, \\"adjusted_sequence\\": adjusted_sequence } return result"},{"question":"# Kernel Approximation - Coding Assessment Objective: The goal of this assessment is to evaluate your understanding of kernel approximation methods using scikit-learn, specifically the Nystroem method and RBFSampler. Problem Statement: You are provided with a dataset containing two features and a target variable, which is binary. Implement a machine learning pipeline using the Nystroem method and RBFSampler to approximate the feature mappings and train a linear classifier. Requirements: 1. **Load and Split Data**: Load the dataset and split it into training and testing sets. 2. **Kernel Approximations**: - Use the Nystroem method to approximate the kernel mapping. - Use RBFSampler to approximate the RBF kernel mapping. 3. **Train Classifiers**: - Train a linear SVM using Stochastic Gradient Descent (SGDClassifier) on the feature mappings obtained from each method. 4. **Evaluate Performance**: - Evaluate and compare the performance of both classifiers on the test set using accuracy. Input Format: - A CSV file named `data.csv` containing three columns: `feature1`, `feature2`, and `target`. - `feature1` and `feature2` are the input features. - `target` is the binary target variable. Output Format: - Print the accuracy score of the classifier using Nystroem method. - Print the accuracy score of the classifier using RBFSampler. Constraints: - Use `n_components=100` in both Nystroem and RBFSampler. - Use `gamma=1` for the RBFSampler. Example: Assuming the CSV data format: ``` feature1,feature2,target 0.1,0.2,0 1.1,1.2,1 ... ``` Code Template: ```python import pandas as pd from sklearn.model_selection import train_test_split from sklearn.kernel_approximation import Nystroem, RBFSampler from sklearn.linear_model import SGDClassifier from sklearn.metrics import accuracy_score # Load Data data = pd.read_csv(\'data.csv\') # Split Data X = data[[\'feature1\', \'feature2\']] y = data[\'target\'] X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Nystroem Method nystroem = Nystroem(n_components=100, kernel=\'rbf\') X_train_nystroem = nystroem.fit_transform(X_train) X_test_nystroem = nystroem.transform(X_test) # Train Linear Classifier on Nystroem features clf_nystroem = SGDClassifier(max_iter=1000, tol=1e-3) clf_nystroem.fit(X_train_nystroem, y_train) y_pred_nystroem = clf_nystroem.predict(X_test_nystroem) # Evaluate Performance accuracy_nystroem = accuracy_score(y_test, y_pred_nystroem) print(f\\"Nystroem Method Accuracy: {accuracy_nystroem}\\") # RBFSampler Method rbf_sampler = RBFSampler(gamma=1, n_components=100) X_train_rbf = rbf_sampler.fit_transform(X_train) X_test_rbf = rbf_sampler.transform(X_test) # Train Linear Classifier on RBF features clf_rbf = SGDClassifier(max_iter=1000, tol=1e-3) clf_rbf.fit(X_train_rbf, y_train) y_pred_rbf = clf_rbf.predict(X_test_rbf) # Evaluate Performance accuracy_rbf = accuracy_score(y_test, y_pred_rbf) print(f\\"RBFSampler Method Accuracy: {accuracy_rbf}\\") ```","solution":"import pandas as pd from sklearn.model_selection import train_test_split from sklearn.kernel_approximation import Nystroem, RBFSampler from sklearn.linear_model import SGDClassifier from sklearn.metrics import accuracy_score def run_kernel_approximation_pipeline(csv_file): # Load Data data = pd.read_csv(csv_file) # Split Data X = data[[\'feature1\', \'feature2\']] y = data[\'target\'] X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Nystroem Method nystroem = Nystroem(n_components=100, kernel=\'rbf\') X_train_nystroem = nystroem.fit_transform(X_train) X_test_nystroem = nystroem.transform(X_test) # Train Linear Classifier on Nystroem features clf_nystroem = SGDClassifier(max_iter=1000, tol=1e-3) clf_nystroem.fit(X_train_nystroem, y_train) y_pred_nystroem = clf_nystroem.predict(X_test_nystroem) # Evaluate Performance accuracy_nystroem = accuracy_score(y_test, y_pred_nystroem) print(f\\"Nystroem Method Accuracy: {accuracy_nystroem}\\") # RBFSampler Method rbf_sampler = RBFSampler(gamma=1, n_components=100) X_train_rbf = rbf_sampler.fit_transform(X_train) X_test_rbf = rbf_sampler.transform(X_test) # Train Linear Classifier on RBF features clf_rbf = SGDClassifier(max_iter=1000, tol=1e-3) clf_rbf.fit(X_train_rbf, y_train) y_pred_rbf = clf_rbf.predict(X_test_rbf) # Evaluate Performance accuracy_rbf = accuracy_score(y_test, y_pred_rbf) print(f\\"RBFSampler Method Accuracy: {accuracy_rbf}\\") return accuracy_nystroem, accuracy_rbf"},{"question":"**Pandas Data Manipulation Challenge** You are given two datasets in CSV format: `student_records.csv` and `course_info.csv`. Your task is to clean, merge, and transform the data to answer specific queries. # Input: 1. **student_records.csv**: ``` student_id,name,course_id,grade 1,John Doe,CS101,85 2,Jane Smith,CS102, 3,Bob Johnson,CS101,78 4,Alice White,CS103,90 5,Charlie Brown,CS102,93 6,Eve Adams,CS101,88 ``` 2. **course_info.csv**: ``` course_id,course_name,instructor CS101,Intro to Computer Science,Dr. Smith CS102,Data Structures,Prof. Lee CS103,Databases,Dr. Johnson ``` # Tasks: 1. **Read both CSV files into DataFrames.** 2. **Handle missing data:** - Fill missing grades with the average grade of the corresponding course. 3. **Merge the two DataFrames on the `course_id` column.** 4. **Calculate the average grade for each course and include the course name and instructor in the result.** 5. **Identify the highest grade in each course, and the corresponding student and grade.** 6. **Output the following DataFrames:** - Cleaned and merged DataFrame with filled missing grade values. - DataFrame with the average grade for each course including the course name and instructor. - DataFrame with the highest grade for each course including student name, course name, and instructor. # Constraints: - Assume the CSV files fit into memory. - Your solution should efficiently handle filling missing data and merging DataFrames. # Expected Output: Output three DataFrames reflecting the tasks described above. ```python import pandas as pd import numpy as np # Step 1: Read the CSV files student_records = pd.read_csv(\'student_records.csv\') course_info = pd.read_csv(\'course_info.csv\') # Step 2: Handle missing data by filling missing grades with the average grade of the corresponding course. # Implementation steps go here... # Step 3: Merge the DataFrames on `course_id`. # Implementation steps go here... # Step 4: Calculate the average grade for each course and include the course name and instructor in the result. # Implementation steps go here... # Step 5: Identify the highest grade in each course. # Implementation steps go here... # Output the results # Please define your implementation and show the final outputs for the three DataFrames as described. ``` Please ensure that your code is properly commented and follows best practices for readability and efficiency. Your solution should handle any edge cases presented by the data.","solution":"import pandas as pd # Step 1: Read the CSV files def read_data(student_file, course_file): student_records = pd.read_csv(student_file) course_info = pd.read_csv(course_file) return student_records, course_info # Step 2: Handle missing data by filling missing grades with the average grade of the corresponding course def fill_missing_grades(df): average_grades = df.groupby(\'course_id\')[\'grade\'].transform(\'mean\') df[\'grade\'].fillna(average_grades, inplace=True) return df # Step 3: Merge the DataFrames on `course_id` def merge_data(student_records, course_info): merged_df = pd.merge(student_records, course_info, on=\'course_id\', how=\'left\') return merged_df # Step 4: Calculate the average grade for each course and include the course name and instructor in the result def calculate_average_grade(merged_df): average_df = merged_df.groupby([\'course_id\', \'course_name\', \'instructor\'], as_index=False)[\'grade\'].mean() average_df.rename(columns={\'grade\': \'average_grade\'}, inplace=True) return average_df # Step 5: Identify the highest grade in each course def highest_grade_per_course(merged_df): idx = merged_df.groupby(\'course_id\')[\'grade\'].idxmax() highest_df = merged_df.loc[idx, [\'student_id\', \'name\', \'course_id\', \'course_name\', \'instructor\', \'grade\']] highest_df.rename(columns={\'grade\': \'highest_grade\'}, inplace=True) return highest_df"},{"question":"# PyTorch Tensor Manipulation and Type Promotion **Objective**: Implement a function that takes two tensors as input, applies an arithmetic operation, and returns the result tensor in a specified dtype and device, while demonstrating a clear understanding of type promotion rules in PyTorch. # Problem Statement Implement a function `tensor_operation` that performs an element-wise addition of two input tensors (`tensor1` and `tensor2`) and returns the resulting tensor. The function should ensure the output tensor is in the specified dtype and device. Additionally, the function should handle type promotion according to PyTorch\'s rules and appropriately manage tensor devices. # Function Signature ```python import torch def tensor_operation( tensor1: torch.Tensor, tensor2: torch.Tensor, dtype: torch.dtype, device: torch.device) -> torch.Tensor: pass ``` # Input - `tensor1` (torch.Tensor): The first input tensor. - `tensor2` (torch.Tensor): The second input tensor. - `dtype` (torch.dtype): The desired data type of the output tensor. - `device` (torch.device): The desired device of the output tensor. # Output - Returns a torch.Tensor that is the result of adding `tensor1` and `tensor2`, casted to the specified `dtype` and moved to the specified `device`. # Constraints and Notes 1. The function should handle different input tensor dtypes and apply dtype promotion rules correctly. 2. The function should handle tensors allocated on different devices appropriately. 3. The function should cast the output tensor to the specified dtype and device. 4. You may assume that the input tensors are broadcastable (i.e., they can be added together). # Example ```python tensor1 = torch.randn((2, 2), dtype=torch.float32, device=torch.device(\'cpu\')) tensor2 = torch.ones((2, 2), dtype=torch.int16, device=torch.device(\'cuda:0\')) dtype = torch.float64 device = torch.device(\'cpu\') result = tensor_operation(tensor1, tensor2, dtype, device) print(result.dtype) # torch.float64 print(result.device) # cpu ``` # Requirements - Your implementation must correctly handle type promotion and device management. - You must not use any high-level libraries or frameworks other than PyTorch. # Testing Your Function You are encouraged to test your function with various input tensors of different dtypes and devices to ensure that it meets the requirements.","solution":"import torch def tensor_operation( tensor1: torch.Tensor, tensor2: torch.Tensor, dtype: torch.dtype, device: torch.device) -> torch.Tensor: Performs element-wise addition of tensor1 and tensor2 and returns the result tensor in the specified dtype and device. # Move tensors to a common device for the operation if tensor1.device != tensor2.device: tensor2 = tensor2.to(tensor1.device) # Perform the addition operation result = tensor1 + tensor2 # Cast the resulting tensor to the specified dtype and move it to the specified device result = result.to(dtype=dtype, device=device) return result"},{"question":"You are tasked with writing a comprehensive unit test suite for a Python class `DataProcessor`. This class is responsible for handling data processing operations that involve several asynchronous I/O operations and external service calls. The simplified version of the class is provided below: ```python import aiohttp import asyncio class DataProcessor: def __init__(self, service_url): self.service_url = service_url async def fetch_data(self): async with aiohttp.ClientSession() as session: async with session.get(self.service_url) as response: return await response.json() async def transform_data(self, data): # Simulate an asynchronous transformation await asyncio.sleep(1) return {k: v*2 for k, v in data.items()} async def save_data(self, transformed_data): async with aiohttp.ClientSession() as session: async with session.post(f\\"{self.service_url}/save\\", json=transformed_data) as response: return await response.status async def process(self): data = await self.fetch_data() transformed_data = await self.transform_data(data) save_status = await self.save_data(transformed_data) return save_status ``` # Requirements Create a unit test suite for the `DataProcessor` class using the `unittest.mock` module. The test suite should include the following test cases: 1. **Test Fetch Data**: - Mock the `aiohttp.ClientSession` to simulate fetching data. - Ensure that the `fetch_data` method returns the expected JSON data. 2. **Test Transform Data**: - Test that `transform_data` transforms the data correctly by doubling the values. 3. **Test Save Data**: - Mock the `aiohttp.ClientSession` to simulate saving data. - Ensure the `save_data` method returns the expected status code. 4. **Test Process End-to-End**: - Mock all external I/O operations (`fetch_data` and `save_data`). - Ensure that `process` method fetches, transforms, and saves data correctly, producing the expected final result status. # Input and Output - **Input**: No direct input; use mock objects to simulate method calls. - **Output**: Use assertions to validate behavior. No output is returned. # Constraints - Use `unittest` and `unittest.mock` modules only. - Simulate network calls and asynchronous operations effectively. - The test should not make any real network calls. # Sample Test Structure ```python import unittest from unittest.mock import AsyncMock, patch class TestDataProcessor(unittest.TestCase): @patch(\'aiohttp.ClientSession.get\') def test_fetch_data(self, mock_get): # Mocking response of `fetch_data` pass # implement the detailed test steps here async def test_transform_data(self): # Testing `transform_data` transformation pass # implement the detailed test steps here @patch(\'aiohttp.ClientSession.post\') def test_save_data(self, mock_post): # Mocking response of `save_data` pass # implement the detailed test steps here @patch.object(DataProcessor, \'fetch_data\', new_callable=AsyncMock) @patch.object(DataProcessor, \'save_data\', new_callable=AsyncMock) def test_process(self, mock_fetch, mock_save): # Mocking end-to-end process pass # implement the detailed test steps here if __name__ == \'__main__\': unittest.main() ``` Make sure you implement all necessary test cases and use appropriate mock configurations.","solution":"import aiohttp import asyncio class DataProcessor: def __init__(self, service_url): self.service_url = service_url async def fetch_data(self): async with aiohttp.ClientSession() as session: async with session.get(self.service_url) as response: return await response.json() async def transform_data(self, data): # Simulate an asynchronous transformation await asyncio.sleep(1) return {k: v*2 for k, v in data.items()} async def save_data(self, transformed_data): async with aiohttp.ClientSession() as session: async with session.post(f\\"{self.service_url}/save\\", json=transformed_data) as response: return response.status async def process(self): data = await self.fetch_data() transformed_data = await self.transform_data(data) save_status = await self.save_data(transformed_data) return save_status"},{"question":"**Objective:** Design and implement a Python script to perform multiple tasks involving file operations, string pattern matching, and statistical computations, encapsulating a comprehensive understanding of various Python standard library modules. **Problem Statement:** You are tasked with writing a Python program named `file_analyzer.py` that performs the following operations: 1. **File Operations:** - List all `.txt` files in the current directory using wildcard patterns. - Read the contents of each `.txt` file and create a dictionary where the key is the filename and the value is the list of lines in that file. 2. **String Pattern Matching:** - For each file, find all unique words that start with the letter \'a\' or \'e\' (case-insensitive) and print them. Consider words as sequences of letters only. 3. **Statistical Computations:** - Calculate the total number of words across all files. - Calculate the average word count per file. - Print the above statistics. **Constraints:** - The program should handle any number of `.txt` files in the directory. - Ensure that the operations are performed efficiently. - Handle potential exceptions (like file access issues) gracefully. **Performance Requirements:** - The program should perform the operations in a reasonable amount of time, even if there are a large number of files and lines in each file. **Expected Input and Output Formats:** *Input:* - The program should be executed in a directory containing multiple `.txt` files. *Output:* ``` Unique words starting with \'a\' or \'e\' in each file: <filename>: <word1, word2, ...> Total number of words: <total_words> Average number of words per file: <average_words_per_file> ``` **Example Execution:** Suppose the directory contains the following files with the following contents: *file1.txt*: ``` apple banana apricot elephant ``. *file2.txt*: ``` eggplant orange avocado ``` *Output*: ``` Unique words starting with \'a\' or \'e\' in each file: file1.txt: apple, apricot, elephant file2.txt: eggplant, avocado Total number of words: 6 Average number of words per file: 3.0 ``` **Additional Notes:** - Use the `glob` module to find `.txt` files. - Use the `re` module for string pattern matching. - Use modules from `os` and `shutil` where appropriate for file operations. - Use `statistics` module to calculate mean values efficiently. **Code Skeleton:** ```python import glob import os import re import statistics def list_txt_files(): return glob.glob(\'*.txt\') def read_file_contents(filename): with open(filename, \'r\') as file: return file.readlines() def find_unique_words(lines): words = set() for line in lines: line_words = re.findall(r\'b[a-zA-Z]*b\', line) for word in line_words: if word.lower().startswith((\'a\', \'e\')): words.add(word) return words def main(): files = list_txt_files() if not files: print(\\"No .txt files found in the directory.\\") return all_words_count = [] for file in files: lines = read_file_contents(file) unique_words = find_unique_words(lines) print(f\\"{file}: {\', \'.join(unique_words)}\\") all_words_count.append(sum(1 for line in lines for word in re.findall(r\'b[a-zA-Z]+b\', line))) total_words = sum(all_words_count) avg_words = statistics.mean(all_words_count) if all_words_count else 0 print(f\\"nTotal number of words: {total_words}\\") print(f\\"Average number of words per file: {avg_words}\\") if __name__ == \\"__main__\\": main() ``` **Submission Instructions:** Submit your solution as a `.py` file along with any additional comments or notes about your implementation.","solution":"import glob import re import statistics def list_txt_files(): Lists all .txt files in the current directory. return glob.glob(\'*.txt\') def read_file_contents(filename): Reads the contents of the given file. Parameters: filename (str): The name of the file to read. Returns: list: A list of lines from the file. try: with open(filename, \'r\') as file: return file.readlines() except Exception as e: print(f\\"Error reading file {filename}: {e}\\") return [] def find_unique_words(lines): Finds all unique words starting with \'a\' or \'e\' in the given lines. Parameters: lines (list): A list of lines from a file. Returns: set: A set of unique words starting with \'a\' or \'e\'. words = set() for line in lines: line_words = re.findall(r\'b[a-zA-Z]+b\', line) for word in line_words: if word.lower().startswith((\'a\', \'e\')): words.add(word.lower()) return words def calculate_statistics(word_counts): Calculates total number of words and average word count per file. Parameters: word_counts (list): A list of word counts for each file. Returns: tuple: Total number of words and average word count per file. total_words = sum(word_counts) avg_words = statistics.mean(word_counts) if word_counts else 0 return total_words, avg_words def main(): files = list_txt_files() if not files: print(\\"No .txt files found in the directory.\\") return word_counts = [] word_statistics = {} for file in files: lines = read_file_contents(file) unique_words = find_unique_words(lines) word_statistics[file] = unique_words word_count = sum(1 for line in lines for word in re.findall(r\'b[a-zA-Z]+b\', line)) word_counts.append(word_count) for file, words in word_statistics.items(): print(f\\"{file}: {\', \'.join(words)}\\") total_words, avg_words = calculate_statistics(word_counts) print(f\\"nTotal number of words: {total_words}\\") print(f\\"Average number of words per file: {avg_words}\\") if __name__ == \\"__main__\\": main()"},{"question":"**Assessment Question: Custom Sequence and Container Handling in Python** In this task, you are required to demonstrate your comprehension of sequence and container objects in Python by implementing a custom function that processes a mixed sequence of various Python objects, including certain custom checks and transformations. # Function Definition: ```python def process_objects(sequence): Processes a mixed sequence of Python objects and returns a dictionary composed of the following: 1. The count of each type of object (e.g., int, float, str, bool, NoneType, list, tuple, dict, set, etc.). 2. A list of all integers in the sequence. 3. A list of all strings converted to uppercase. 4. A boolean indicating if there are any floating-point numbers greater than 100.0. Args: sequence (iterable): A sequence containing various Python objects. Returns: dict: A dictionary with the following structure: { \\"type_count\\": { \\"int\\": int, \\"float\\": int, \\"str\\": int, \\"bool\\": int, \\"NoneType\\": int, \\"list\\": int, \\"tuple\\": int, \\"dict\\": int, \\"set\\": int, \\"...\\": int # Other types as needed. }, \\"integers\\": [int], \\"upper_strings\\": [str], \\"any_float_above_100\\": bool, } pass ``` # Input: - `sequence` (iterable): A sequence (e.g., list or tuple) containing various Python objects such as integers, floats, strings, booleans, `None`, lists, tuples, dictionaries, sets, etc. # Output: - A dictionary containing: - `\\"type_count\\"`: A dictionary showing the count of each type of object. - `\\"integers\\"`: A list of all integer values present in the sequence. - `\\"upper_strings\\"`: A list of all string values in uppercase. - `\\"any_float_above_100\\"`: A boolean indicating if there is any floating-point number greater than 100.0. # Constraints: - The function must handle sequences with up to 1000 elements. - The function should be efficient in terms of time complexity, with a target of `O(n)` where `n` is the length of the sequence. # Example: ```python sequence = [1, \\"hello\\", 3.14, True, None, [1, 2], (3, 4), {\\"a\\": 1}, {1, 2, 3}, 150.0, \\"WORLD\\"] result = process_objects(sequence) # Expected output: # { # \\"type_count\\": { # \\"int\\": 1, # \\"float\\": 2, # \\"str\\": 2, # \\"bool\\": 1, # \\"NoneType\\": 1, # \\"list\\": 1, # \\"tuple\\": 1, # \\"dict\\": 1, # \\"set\\": 1 # }, # \\"integers\\": [1], # \\"upper_strings\\": [\\"HELLO\\", \\"WORLD\\"], # \\"any_float_above_100\\": True # } ``` # Notes: - Ensure to check the types accurately and handle all common Python object types as listed. - Aim for clean and readable code with appropriate comments where necessary.","solution":"def process_objects(sequence): from collections import defaultdict # Initialize the results dictionary results = { \\"type_count\\": defaultdict(int), \\"integers\\": [], \\"upper_strings\\": [], \\"any_float_above_100\\": False } # Define a function to return the type name as string def get_type_name(obj): return type(obj).__name__ # Process each object in the sequence for obj in sequence: type_name = get_type_name(obj) results[\\"type_count\\"][type_name] += 1 if type_name == \\"int\\": results[\\"integers\\"].append(obj) elif type_name == \\"str\\": results[\\"upper_strings\\"].append(obj.upper()) elif type_name == \\"float\\" and obj > 100.0: results[\\"any_float_above_100\\"] = True return results"},{"question":"# Distributed Checkpointing with PyTorch In this task, you are required to implement a distributed model checkpointing and loading mechanism using `torch.distributed.checkpoint`. This will involve saving a model\'s state from multiple processes (ranks), and later loading the model\'s state accurately across different distributed setups. Requirements 1. **Setup**: - Initialize a simple neural network model using PyTorch. - Initialize a distributed process group with multiple ranks (e.g., using `torch.distributed.init_process_group`). 2. **Checkpoint Saving**: - Implement a function `save_distributed_checkpoint(model: torch.nn.Module, checkpoint_dir: str)` that: - Saves the model\'s state dictionary in a distributed manner to the specified checkpoint directory. - Ensures each rank creates its own checkpoint files. 3. **Checkpoint Loading**: - Implement a function `load_distributed_checkpoint(model: torch.nn.Module, checkpoint_dir: str)` that: - Loads the model\'s state dictionary from the checkpoint files saved by `save_distributed_checkpoint`. - Ensures the model\'s state is properly restored across different distributed setups. Input and Output Formats - Input: - `model`: An instance of `torch.nn.Module` representing the neural network. - `checkpoint_dir`: A string specifying the directory to save or load checkpoint files. - Output: - The functions do not return anything. They should raise appropriate exceptions if saving or loading fails. Constraints 1. Assume the model can fit within the memory limits of the distributed processes. 2. The solution must handle the synchronization among distributed processes. 3. The solution must ensure consistency of the model state across different ranks. 4. The solution should be resilient to different cluster topologies during loading. Example Usage ```python import torch import torch.distributed as dist from torch.nn.parallel import DistributedDataParallel as DDP # Initialize distributed process group dist.init_process_group(backend=\'nccl\', init_method=\'env://\') # Define a simple model model = torch.nn.Linear(10, 10).cuda() model = DDP(model) # Save the model state save_distributed_checkpoint(model, \\"/path/to/checkpoint_dir\\") # Load the model state load_distributed_checkpoint(model, \\"/path/to/checkpoint_dir\\") ``` Additional Notes - You are required to handle both synchronous and asynchronous checkpointing. - Consider using PyTorch tools and utilities like `torch.save`, `torch.load`, `torch.distributed.checkpoint.save`, and `torch.distributed.checkpoint.load`. Implement the functions `save_distributed_checkpoint` and `load_distributed_checkpoint` based on the described specifications.","solution":"import os import torch import torch.distributed as dist def save_distributed_checkpoint(model: torch.nn.Module, checkpoint_dir: str): Saves the model\'s state dictionary in a distributed manner to the specified checkpoint directory. Ensures each rank creates its own checkpoint file. # Ensure the directory exists os.makedirs(checkpoint_dir, exist_ok=True) # Get the rank of the current process rank = dist.get_rank() # Construct the filename using rank checkpoint_file = os.path.join(checkpoint_dir, f\'checkpoint_rank_{rank}.pt\') # Save the state dictionary torch.save(model.state_dict(), checkpoint_file) def load_distributed_checkpoint(model: torch.nn.Module, checkpoint_dir: str): Loads the model\'s state dictionary from the checkpoint files saved by `save_distributed_checkpoint`. Ensures the model\'s state is properly restored across different distributed setups. # Get the rank of the current process rank = dist.get_rank() # Construct the filename using rank checkpoint_file = os.path.join(checkpoint_dir, f\'checkpoint_rank_{rank}.pt\') # Check if the file exists if not os.path.exists(checkpoint_file): raise FileNotFoundError(f\'Checkpoint file not found: {checkpoint_file}\') # Load the state dictionary state_dict = torch.load(checkpoint_file) # Load the state dictionary into the model model.load_state_dict(state_dict)"},{"question":"# Advanced XML Parsing, Manipulation, and Validation with `xml.etree.ElementTree` **Objective**: Write a Python function to parse, validate, modify, and save an XML document using the `xml.etree.ElementTree` module. **Background**: You are provided with an XML document containing information about various books in a library. The document includes book titles, authors, publication years, and genres. Some books also have optional attributes and nested elements for additional data such as reviews and ratings. **Key Requirements**: 1. **Parse the XML Document**: Read the XML data from a file or string input using `xml.etree.ElementTree`. 2. **Validate the Structure**: - Ensure that each book has a title, author, and publication year. - Optional: Include a validation step for a correct XML structure (well-formed XML). 3. **Modification**: - Add a new review sub-element with text content to each book element. - Increment the rating attribute for each book, if present. - Delete any book entries older than a given year. 4. **Save the Modified XML**: Write the modified XML document to a new file. # Function Definition ```python def process_books_xml(xml_input: str, year_threshold: int, new_review: str) -> str: Processes an XML string containing information about books: 1. Parses the XML document from the input string. 2. Validates that each book has a title, author, and publication year. 3. Adds a new review to each book. 4. Increments the rating attribute for each book by 1, if present. 5. Removes books published before the given threshold year. 6. Saves the modified XML document to an output file. Parameters: xml_input (str): A string containing XML data of books. year_threshold (int): The year threshold to remove old books. new_review (str): The review text to be added to each book. Returns: str: The path to the saved XML file. pass ``` # Input and Output - **Input**: - `xml_input`: A string containing the XML data with book details. - `year_threshold`: An integer representing the year threshold for removing books. - `new_review`: A string containing the new review text to be added to each book. - **Output**: - Return the file path to the modified XML document. # Constraints - Assume the XML input is a well-formed XML string. - `xml_input` will always contain at least one book entry. # Example XML Input ```xml <library> <book> <title>Sample Book 1</title> <author>Author A</author> <year>2010</year> <genre>Fiction</genre> <rating>4</rating> </book> <book> <title>Sample Book 2</title> <author>Author B</author> <year>2005</year> <genre>Science</genre> </book> </library> ``` # Expected Output (File Content) ```xml <library> <book> <title>Sample Book 1</title> <author>Author A</author> <year>2010</year> <genre>Fiction</genre> <rating>5</rating> <review>Excellent read!</review> </book> </library> ``` **Note**: - In the provided example, `Sample Book 2` is removed as its publication year is 2005, and we assume the threshold year is 2010. - A `review` is added with the text `Excellent read!` and the rating is incremented for `Sample Book 1`. # Implementation Notes - Use `xml.etree.ElementTree` for parsing and creating XML data. - Use `find`, `findall`, and `SubElement` methods to navigate the XML tree and perform required modifications. - Use `ElementTree.write()` to save the modified XML document to a file.","solution":"import xml.etree.ElementTree as ET def process_books_xml(xml_input: str, year_threshold: int, new_review: str) -> str: Processes an XML string containing information about books: 1. Parses the XML document from the input string. 2. Validates that each book has a title, author, and publication year. 3. Adds a new review to each book. 4. Increments the rating attribute for each book by 1, if present. 5. Removes books published before the given threshold year. 6. Saves the modified XML document to an output file. Parameters: xml_input (str): A string containing XML data of books. year_threshold (int): The year threshold to remove old books. new_review (str): The review text to be added to each book. Returns: str: The path to the saved XML file. try: # Parse the XML string root = ET.fromstring(xml_input) # Validate the XML structure for book in root.findall(\'book\'): title = book.find(\'title\') author = book.find(\'author\') year = book.find(\'year\') if title is None or author is None or year is None: raise ValueError(\\"Each book must have a title, author, and publication year\\") # Iterate over the books and apply modifications books_to_keep = [] for book in root.findall(\'book\'): pub_year = int(book.find(\'year\').text) if pub_year >= year_threshold: # Add a new review sub-element review = ET.SubElement(book, \'review\') review.text = new_review # Increment rating if it exists rating_element = book.find(\'rating\') if rating_element is not None: rating_element.text = str(int(rating_element.text) + 1) books_to_keep.append(book) # Remove all books first for book in root.findall(\'book\'): root.remove(book) # Add back only the valid books for book in books_to_keep: root.append(book) # Write the modified XML to a new file tree = ET.ElementTree(root) output_file = \\"modified_books.xml\\" tree.write(output_file) return output_file except ET.ParseError: raise ValueError(\\"Invalid XML structure\\")"},{"question":"In Python, a lexically valid source file is divided into tokens such as keywords, operators, string literals, and numeric literals. Each of these tokens adheres to strict syntactical rules. You are required to implement a function that takes a string representing a Python source line and returns a list of tokens found in that line. Your implementation should be capable of recognizing identifiers, keywords, numeric literals, string literals, operators, and delimiters. The function should disregard comments and whitespace beyond what is needed to tokenize correctly. # Expected Function Signature ```python def tokenize_python_line(code_line: str) -> List[str]: pass ``` # Input * `code_line` (str): A string representing a single line of Python code. The line will contain valid Python syntax. # Output * List of strings: A list where each element is a valid token found in the `code_line`. The order of tokens should reflect their order in the input string. # Constraints 1. The input string will be a valid line of Python code. 2. The input will not contain multiline statements or comments spanning multiple lines. 3. String literals can include single, double, and triple quotes. 4. The tokenize function does not need to handle the newline character. # Examples 1. **Example 1** ```python code_line = \\"def my_function():\\" print(tokenize_python_line(code_line)) # [\\"def\\", \\"my_function\\", \\"(\\", \\")\\", \\":\\"] ``` 2. **Example 2** ```python code_line = \\"result = 3 + 4.5 * my_var\\" print(tokenize_python_line(code_line)) # [\\"result\\", \\"=\\", \\"3\\", \\"+\\", \\"4.5\\", \\"*\\", \\"my_var\\"] ``` 3. **Example 3** ```python code_line = \\"# This is a comment\\" print(tokenize_python_line(code_line)) # [] ``` 4. **Example 4** ```python code_line = \\"if \'hello\' in my_string:\\" print(tokenize_python_line(code_line)) # [\\"if\\", \\"\'hello\'\\", \\"in\\", \\"my_string\\", \\":\\"] ``` # Notes 1. Keywords should be identified correctly and differentiated from identifiers. 2. Ensure that string literals are parsed correctly, including those with escape sequences. 3. Operators and delimiters should be identified as separate tokens. 4. Comments should be ignored entirely. 5. Numeric literals, including those with underscores, should be detected accurately. 6. You may assume that there are no malformed input lines. Good luck!","solution":"import tokenize from io import BytesIO def tokenize_python_line(code_line: str) -> list: tokens = [] # Convert the code line to bytes-like object line_bytes = BytesIO(code_line.encode(\'utf-8\')) # Tokenize using the tokenize module for token in tokenize.tokenize(line_bytes.readline): # Ignore encoding token and newlines, and comments if token.type in (tokenize.ENCODING, tokenize.ENDMARKER, tokenize.NEWLINE, tokenize.NL): continue if token.type == tokenize.COMMENT: break tokens.append(token.string) return tokens"},{"question":"**Coding Challenge: Binary File Transfer with UUencoding** **Background:** The `uu` module in Python provides functionalities to encode and decode files using the uuencode format. Even though it is deprecated in favor of modern alternatives like `base64`, it serves as an excellent means to practice file handling and encoding/decoding operations. In this challenge, you will implement functions to encode and decode binary data files. **Task:** You are required to write two functions: `custom_uuencode` and `custom_uudecode`. **Function 1: `custom_uuencode`** This function should take an input file path, an output file path, and an optional filename and permission mode. It should use the `uu.encode` function to uuencode the input file and save the result in the output file. **Input:** - `input_file_path` (str): Path to the input binary file. - `output_file_path` (str): Path to the output encoded file. - `name` (str, optional): Specifying the default name in the uuencoded header. Default is None. - `mode` (int, optional): File permission bits to include in the header of the encoded file. Default is None. - `use_backtick` (bool, optional): If True, zeros will be represented by the backtick character instead of spaces. Default is False. **Output:** - None ```python def custom_uuencode(input_file_path: str, output_file_path: str, name: str = None, mode: int = None, use_backtick: bool = False) -> None: pass ``` **Function 2: `custom_uudecode`** This function should take an input encoded file path and an output file path. It should use the `uu.decode` function to decode the input file and save the result in the output file. Ensure that if the output file already exists and is non-empty, it raises an exception using `uu.Error`. **Input:** - `input_file_path` (str): Path to the input encoded file. - `output_file_path` (str): Path to the output decoded file. - `permission_mode` (int, optional): If provided, sets the permission bits when creating a new file. Default is None. - `quiet` (bool, optional): If True, suppresses warnings if decoding errors are encountered. Default is False. **Output:** - None ```python def custom_uudecode(input_file_path: str, output_file_path: str, permission_mode: int = None, quiet: bool = False) -> None: pass ``` **Constraints:** 1. You must handle file-related exceptions gracefully and raise `uu.Error` if a critical error occurs. 2. You should not use any libraries outside of the standard Python library. 3. For consistent behavior, ensure the file-opening modes are correct (binary reading `\'rb\'` and binary writing `\'wb\'`). **Example Usage:** ```python # Example file paths input_bin_file = \\"path/to/input.bin\\" output_enc_file = \\"path/to/output.enc\\" decoded_bin_file = \\"path/to/decoded_output.bin\\" # Encode the binary file custom_uuencode(input_bin_file, output_enc_file, name=\\"testfile\\", mode=0o644) # Decode the previously encoded file custom_uudecode(output_enc_file, decoded_bin_file) ``` By completing this task, you will demonstrate your understanding of file operations, encoding/decoding mechanisms, and exception handling in Python.","solution":"import uu import os def custom_uuencode(input_file_path: str, output_file_path: str, name: str = None, mode: int = None, use_backtick: bool = False) -> None: Encodes the input file using uuencode and writes the result to the output file. Parameters: input_file_path (str): Path to the input binary file. output_file_path (str): Path to the output encoded file. name (str, optional): Name to use in the uuencoded header. Default is None. mode (int, optional): File permission bits for the header. Default is None. use_backtick (bool, optional): If True, use backtick for zeros. Default is False. try: with open(input_file_path, \'rb\') as input_file, open(output_file_path, \'wb\') as output_file: uu.encode(input_file, output_file, name=name, mode=mode, backtick=use_backtick) except Exception as e: raise uu.Error(f\\"Error encoding the file: {e}\\") def custom_uudecode(input_file_path: str, output_file_path: str, permission_mode: int = None, quiet: bool = False) -> None: Decodes the uuencoded input file and writes the result to the output file. Parameters: input_file_path (str): Path to the input encoded file. output_file_path (str): Path to the output decoded file. permission_mode (int, optional): Permission mode for the output file. Default is None. quiet (bool, optional): If True, suppresses warnings on decoding errors. Default is False. try: if os.path.exists(output_file_path) and os.path.getsize(output_file_path) > 0: raise uu.Error(\\"Output file already exists and is non-empty.\\") with open(input_file_path, \'rb\') as input_file, open(output_file_path, \'wb\') as output_file: uu.decode(input_file, output_file, mode=permission_mode, quiet=quiet) except Exception as e: raise uu.Error(f\\"Error decoding the file: {e}\\")"},{"question":"# Bytecode Analysis with the `dis` Module The `dis` module in Python is a powerful tool for analyzing CPython bytecode. Understanding bytecode is important for deep insights into Python functions and code behavior. In this task, you will use the `dis` module to analyze a given Python function and extract specific details from its bytecode. Task Write a Python function `analyze_bytecode` that takes a function object as input and performs the following tasks: 1. **Disassemble the function** and return the disassembled bytecode as a formatted multi-line string. 2. **Identify and return all unique global variables** used in the function by analyzing the bytecode instructions. 3. **Find and return all the line numbers** where instructions are executed in the given function. The function `analyze_bytecode` should return a dictionary with the following keys: - `disassembled_code`: A formatted string containing the disassembled bytecode. - `global_vars`: A list of unique global variable names used in the function. - `line_numbers`: A list of line numbers where instructions are executed. Input - `func`: A Python function object. Output - A dictionary with the keys `disassembled_code`, `global_vars`, and `line_numbers`, as specified above. Example ```python import dis def sample_func(a, b): c = a + b print(c) result = analyze_bytecode(sample_func) print(result[\'disassembled_code\']) # Should print the disassembled bytecode of sample_func print(result[\'global_vars\']) # Should print [\'print\'] print(result[\'line_numbers\']) # Should print a list of line numbers where instructions are executed ``` Constraints - The function should handle valid Python functions only. - Assume the input function will not contain any syntax errors. Notes - You may use the `dis.Bytecode` class and other functions from the `dis` module as needed. - You should ensure that your function adheres to the specified input-output format. # Implementation ```python def analyze_bytecode(func): import dis # Your code here return { \'disassembled_code\': disassembled_code, \'global_vars\': global_vars, \'line_numbers\': line_numbers } ``` Your implementation should follow the guidelines provided and ensure correct results for the given tasks. Good luck!","solution":"def analyze_bytecode(func): import dis from io import StringIO # Capture the disassembled bytecode as a string disassembled_code_io = StringIO() dis.dis(func, file=disassembled_code_io) disassembled_code = disassembled_code_io.getvalue() # Use a set to collect unique global variable names global_vars = set() line_numbers = set() # Analyze the bytecode bytecode = dis.Bytecode(func) for instruction in bytecode: if instruction.opname == \'LOAD_GLOBAL\': global_vars.add(instruction.argval) line_numbers.add(instruction.starts_line) return { \'disassembled_code\': disassembled_code, \'global_vars\': list(global_vars), \'line_numbers\': list(filter(None, line_numbers)) # Remove None values }"},{"question":"# Asynchronous Echo Server with Connection Count You are required to implement an asynchronous echo server using the `asyncore` module that can handle multiple client connections simultaneously. Your server should: 1. Accept incoming connections from multiple clients. 2. Echo any received data back to the sender. 3. Keep track of the current number of active connections and print this number each time a new client connects or disconnects. Requirements: - Implement a class `EchoHandler` inheriting from `asyncore.dispatcher_with_send` to handle individual client connections. - Implement a class `ConnectionCountingServer` inheriting from `asyncore.dispatcher` to accept new connections and dispatch them to `EchoHandler`. - Track active connections globally and ensure that the count is updated and printed correctly. Input: - The server should be initiated with a host (as a string) and port (as an integer). Output: - Each time a new client connects or disconnects, print the current number of active connections. Example: ```python import asyncore class EchoHandler(asyncore.dispatcher_with_send): # Your implementation here class ConnectionCountingServer(asyncore.dispatcher): # Your implementation here # Example usage server = ConnectionCountingServer(\'localhost\', 8080) asyncore.loop() ``` **Constraints:** - Use only the `asyncore` module for handling asynchronous sockets. - The server should run indefinitely until manually stopped. Notes: - You can refer to the documentation and examples provided to understand how the `asyncore` module works. - Ensure proper management of the active connection count.","solution":"import asyncore import socket active_connections = 0 class EchoHandler(asyncore.dispatcher_with_send): def __init__(self, sock): super().__init__(sock) global active_connections active_connections += 1 print(f\\"New connection. Active connections: {active_connections}\\") def handle_read(self): data = self.recv(1024) if data: self.send(data) def handle_close(self): global active_connections active_connections -= 1 print(f\\"Connection closed. Active connections: {active_connections}\\") self.close() class ConnectionCountingServer(asyncore.dispatcher): def __init__(self, host, port): super().__init__() self.create_socket(socket.AF_INET, socket.SOCK_STREAM) self.set_reuse_addr() self.bind((host, port)) self.listen(5) def handle_accept(self): socket, address = self.accept() print(f\\"Accepted connection from {address}\\") EchoHandler(socket) # For running the server: # server = ConnectionCountingServer(\'localhost\', 8080) # asyncore.loop()"},{"question":"# Compression and Decompression with LZMA **Problem Statement:** You are tasked with implementing a Python module that provides methods for compressing and decompressing files using the LZMA algorithm. Your task includes the ability to handle different levels of compression, verify the integrity of the compressed data, and manage the file operations efficiently. **Your task:** 1. **Compress a File:** - Implement a function `compress_file(input_file: str, output_file: str, preset: int = 6, check: int = lzma.CHECK_CRC64) -> None` that compresses a given file using the LZMA algorithm. - Parameters: - `input_file` (str): Path to the input file that needs to be compressed. - `output_file` (str): Path to save the compressed output file. - `preset` (int, optional): An integer between 0 and 9 that sets the compression level. Default is 6. - `check` (int, optional): The type of integrity check to include. Default is `lzma.CHECK_CRC64`. 2. **Decompress a File:** - Implement a function `decompress_file(input_file: str, output_file: str) -> None` that decompresses a given LZMA-compressed file. - Parameters: - `input_file` (str): Path to the compressed input file. - `output_file` (str): Path to save the decompressed output file. Both functions must handle any exceptions that may arise during the file operations and compression/decompression processes. Your functions should also check the validity of the input parameters (e.g., ensure that `preset` is within the valid range). **Constraints:** - Your functions must not use any external libraries except for `lzma`. - The `input_file` must exist and be readable. - The `output_file` should be writable. - If the specified `output_file` already exists, it should be overwritten. **Example Usage:** ```python # Compress a file compress_file(\'example.txt\', \'example.xz\', preset=9, check=lzma.CHECK_SHA256) # Decompress a file decompress_file(\'example.xz\', \'example_decompressed.txt\') ``` **Additional Information:** - Detailed documentation for the `lzma` module is provided in the problem description. Refer to it for understanding the parameters and methods used for compression and decompression. - Test your functions with different files, sizes, and compression levels to ensure they work correctly under various scenarios. **Evaluation Criteria:** - Correctness of the compression and decompression processes. - Proper error handling and validation of input parameters. - Efficient file I/O operations. - Code readability and adherence to Pythonic practices.","solution":"import lzma import os def compress_file(input_file: str, output_file: str, preset: int = 6, check: int = lzma.CHECK_CRC64) -> None: Compresses a given file using the LZMA algorithm. Args: input_file (str): Path to the input file. output_file (str): Path to save the compressed file. preset (int, optional): Compression level, from 0 to 9. Default is 6. check (int, optional): Integrity check type. Default is lzma.CHECK_CRC64. if not (0 <= preset <= 9): raise ValueError(\\"Preset must be an integer between 0 and 9\\") if not os.path.isfile(input_file): raise FileNotFoundError(f\\"Input file \'{input_file}\' does not exist\\") try: with open(input_file, \'rb\') as infile, lzma.open(output_file, \'wb\', preset=preset, check=check) as outfile: outfile.write(infile.read()) except Exception as e: raise RuntimeError(f\\"Error occurred while compressing file: {e}\\") def decompress_file(input_file: str, output_file: str) -> None: Decompresses a given LZMA-compressed file. Args: input_file (str): Path to the compressed input file. output_file (str): Path to save the decompressed file. if not os.path.isfile(input_file): raise FileNotFoundError(f\\"Input file \'{input_file}\' does not exist\\") try: with lzma.open(input_file, \'rb\') as infile, open(output_file, \'wb\') as outfile: outfile.write(infile.read()) except Exception as e: raise RuntimeError(f\\"Error occurred while decompressing file: {e}\\")"},{"question":"# Question: Implementing and Using Custom Autograd Functions in PyTorch Background: PyTorch\'s autograd system allows for automatic differentiation of tensor operations. Sometimes, however, it is necessary to define custom operations where both the forward and backward (gradient) computations are explicitly defined. `torch.autograd.Function` allows users to do this by implementing static methods `forward` and `backward`. Task: 1. Implement a custom autograd function `MyExp` that mimics the exponential function but includes some additional logging. The function should output (`exp(x)`) for the forward pass and for the backward pass it should compute the gradients and simply log that the backward pass has been called. 2. Write a training loop that uses your custom autograd function within a simple feed-forward neural network. The network should have a single linear layer followed by the custom exponential function. 3. Ensure that the computation graph is managed correctly, such that gradients are appropriately tracked, and the forward and backward passes work as expected. Requirements: 1. Define the custom autograd function `MyExp`. 2. Implement the forward and backward static methods for `MyExp`. 3. Use the custom function in a simple neural network. 4. Train the network on a simple dataset (e.g., random inputs) and print out the logging information from the backward pass to confirm it is working correctly. Constraints: - Ensure that intermediary results needed for backward pass calculations are correctly saved. - Avoid modifying the original input tensor in-place to avoid unexpected side-effects. Example Usage of `MyExp` in a Neural Network: ```python import torch import torch.nn as nn # Define the custom autograd function class MyExp(torch.autograd.Function): @staticmethod def forward(ctx, input): result = input.exp() ctx.save_for_backward(result) return result @staticmethod def backward(ctx, grad_output): result, = ctx.saved_tensors print(\\"Backward pass called\\") return grad_output * result # Use the custom autograd function in a simple neural network class SimpleNet(nn.Module): def __init__(self): super(SimpleNet, self).__init__() self.linear = nn.Linear(1, 1) def forward(self, x): x = self.linear(x) # Apply the custom exponential function x = MyExp.apply(x) return x # Training loop model = SimpleNet() criterion = nn.MSELoss() optimizer = torch.optim.SGD(model.parameters(), lr=0.01) # Generate some random data inputs = torch.randn(10, 1) targets = torch.randn(10, 1) # Training for epoch in range(10): optimizer.zero_grad() outputs = model(inputs) loss = criterion(outputs, targets) loss.backward() optimizer.step() print(f\'Epoch {epoch+1}, Loss: {loss.item()}\') ``` Expected Output: The logging from the backward pass should be printed in each training iteration, confirming that the custom backpropagation is being executed correctly. Example log lines would include \\"Backward pass called\\" for each gradient computation step.","solution":"import torch import torch.nn as nn # Define the custom autograd function class MyExp(torch.autograd.Function): @staticmethod def forward(ctx, input): result = input.exp() ctx.save_for_backward(result) return result @staticmethod def backward(ctx, grad_output): result, = ctx.saved_tensors print(\\"Backward pass called\\") return grad_output * result # Use the custom autograd function in a simple neural network class SimpleNet(nn.Module): def __init__(self): super(SimpleNet, self).__init__() self.linear = nn.Linear(1, 1) def forward(self, x): x = self.linear(x) # Apply the custom exponential function x = MyExp.apply(x) return x # Training loop function def train_network(): model = SimpleNet() criterion = nn.MSELoss() optimizer = torch.optim.SGD(model.parameters(), lr=0.01) # Generate some random data inputs = torch.randn(10, 1) targets = torch.randn(10, 1) # Training for epoch in range(10): optimizer.zero_grad() outputs = model(inputs) loss = criterion(outputs, targets) loss.backward() optimizer.step() print(f\'Epoch {epoch+1}, Loss: {loss.item()}\') if __name__ == \\"__main__\\": train_network()"},{"question":"Implementing Precise Comparisons of Floating-Point Numbers **Background**: Floating-point arithmetic in Python can produce results that are very close to but not exactly equal to the expected results, due to the binary representation of decimal numbers. This can lead to unexpected behavior when comparing floating-point numbers. # Task: Write a Python function `precise_float_comparison` that takes two floating-point numbers and a tolerance value as input and returns whether the two numbers are equal within the given tolerance. Additionally, provide a function to accurately add a list of floating-point numbers using the `decimal` module. # Function Signatures: ```python def precise_float_comparison(a: float, b: float, tolerance: float) -> bool: Compares two floating-point numbers within a given tolerance. Parameters: a (float): The first floating-point number. b (float): The second floating-point number. tolerance (float): The maximum allowed difference for the numbers to be considered equal. Returns: bool: True if the numbers are equal within the specified tolerance, else False. pass def accurate_sum(numbers: list) -> float: Accurately sums a list of floating-point numbers using the decimal module. Parameters: numbers (list of float): The list of floating-point numbers to be summed. Returns: float: The accurate sum of the numbers. pass ``` # Input Format: - For `precise_float_comparison`: - `a` and `b`: Two floating-point numbers (e.g., 0.1, 1.00000001). - `tolerance`: A floating-point number representing the tolerance level (e.g., 1e-9). - For `accurate_sum`: - `numbers`: A list of floating-point numbers (e.g., [0.1, 0.1, 0.1]). # Output Format: - The output of `precise_float_comparison` should be a boolean: `True` if the two numbers are equal within the specified tolerance, otherwise `False`. - The output of `accurate_sum` should be a floating-point number representing the sum of the input list. # Constraints: - The tolerance will always be a positive number. - The list of numbers for summation will contain at least one element. # Example: ```python # Example 1: Using precise_float_comparison a = 0.1 b = 0.10000000000000001 tolerance = 1e-9 print(precise_float_comparison(a, b, tolerance)) # Output: True # Example 2: Using accurate_sum numbers = [0.1, 0.1, 0.1] print(accurate_sum(numbers)) # Output: 0.3 ``` # Explanation: 1. In the first example, `precise_float_comparison` checks if `a` and `b` are equal within the given tolerance, accounting for possible representation errors. 2. In the second example, `accurate_sum` computes the sum of the list of floating-point numbers accurately using the `decimal` module to mitigate precision loss.","solution":"from decimal import Decimal, getcontext def precise_float_comparison(a: float, b: float, tolerance: float) -> bool: Compares two floating-point numbers within a given tolerance. Parameters: a (float): The first floating-point number. b (float): The second floating-point number. tolerance (float): The maximum allowed difference for the numbers to be considered equal. Returns: bool: True if the numbers are equal within the specified tolerance, else False. return abs(a - b) <= tolerance def accurate_sum(numbers: list) -> float: Accurately sums a list of floating-point numbers using the decimal module. Parameters: numbers (list of float): The list of floating-point numbers to be summed. Returns: float: The accurate sum of the numbers. getcontext().prec = 50 # Set a high precision for accurate sum total = Decimal(0) for number in numbers: total += Decimal(str(number)) return float(total)"},{"question":"Objective: Demonstrate your understanding of Python\'s `array` module by implementing a function that performs various operations on arrays as specified. Problem Statement: Write a function `process_array_operations` that performs the following operations on arrays based on the given instructions. Function Signature: ```python def process_array_operations(operations: list) -> array.array: pass ``` Input: - `operations` (list): A list of tuples, where each tuple represents an operation to be performed. Each tuple contains: - A string representing the operation type, which can be one of the following: - `\'create\'`: Create a new array. - `\'append\'`: Append a value to the array. - `\'count\'`: Count the occurrences of a value in the array. - `\'extend\'`: Extend the array with elements from another iterable. - `\'pop\'`: Remove and return the element at the given index. - `\'remove\'`: Remove the first occurrence of a value. - `\'reverse\'`: Reverse the array. - `\'tobytes\'`: Convert the array to bytes. - Additional parameters based on the operation type: - For `\'create\'`: A typecode (string) and an initializer (list of values). - For `\'append\'`, `\'count\'`, `\'remove\'`: A value to append, count, or remove. - For `\'extend\'`: An iterable of values to extend the array with. - For `\'pop\'`: An index (integer). - For `\'tobytes\'`, `\'reverse\'`: No additional parameters. Output: - Returns the final array after performing all the operations. Constraints: - The typecode for the `\'create\'` operation must be one of the valid type codes defined below. - The initializer list for the `\'create\'` operation must contain elements of a type that matches the specified typecode. Type Codes: - `\'b\'`, `\'B\'`, `\'u\'`, `\'h\'`, `\'H\'`, `\'i\'`, `\'I\'`, `\'l\'`, `\'L\'`, `\'q\'`, `\'Q\'`, `\'f\'`, `\'d\'` Example: ```python ops = [ (\'create\', \'i\', [1, 2, 3]), (\'append\', 4), (\'count\', 2), (\'extend\', [5, 6]), (\'pop\', 2), (\'remove\', 5), (\'reverse\',), (\'tobytes\',) ] result = process_array_operations(ops) print(result) ``` Expected Output: ```plaintext array(\'i\', [6, 4, 3, 2, 1]) ``` # Notes: - You must use the `array` module to create and manipulate arrays. - Ensure you handle invalid operations gracefully and raise appropriate exceptions if the operations are not valid or parameters are incorrect.","solution":"import array def process_array_operations(operations): arr = None for operation in operations: op_type = operation[0] if op_type == \'create\': _, typecode, initializer = operation arr = array.array(typecode, initializer) elif op_type == \'append\': _, value = operation arr.append(value) elif op_type == \'count\': _, value = operation arr.count(value) elif op_type == \'extend\': _, iterable = operation arr.extend(iterable) elif op_type == \'pop\': _, index = operation arr.pop(index) elif op_type == \'remove\': _, value = operation arr.remove(value) elif op_type == \'reverse\': arr.reverse() elif op_type == \'tobytes\': arr = array.array(arr.typecode, arr.tobytes()) else: raise ValueError(f\\"Invalid operation: {op_type}\\") return arr"},{"question":"You are tasked with designing functions to help a library handle floating point operations more effectively using Python\'s C-API. The aim is to create Python bindings for these functionalities to leverage their efficiency in optimized tasks. # Requirements: 1. **Function Name:** `create_float_from_string` - **Input:** A string representing a floating-point number. - **Output:** A new floating-point number if the string is valid, or `None` if the string is invalid. - **Constraints:** The input string must represent a valid floating-point number. 2. **Function Name:** `get_float_value` - **Input:** A Python object. - **Output:** The floating-point value of the object if it is a float or has `__float__()` method, or raise a `ValueError` otherwise. - **Constraints:** The object must be convertible to a float. 3. **Function Name:** `get_float_limits` - **Output:** A dictionary containing the maximum and minimum normalized positive float values. - **Example Output:** `{\\"max\\": <maximum float value>, \\"min\\": <minimum float value>}` **Function Implementation Use-Cases:** ```python def create_float_from_string(s: str) -> float: Create a floating-point number from a string. If the string does not represent a valid floating point number, return None. Args: s (str): The input string. Returns: float: The created floating-point number or None if invalid. pass def get_float_value(obj) -> float: Return the floating-point value of the object if it is a float or has __float__ method. Raises ValueError if the object cannot be converted to float. Args: obj: The input object. Returns: float: The floating-point value. Raises: ValueError: If the object cannot be converted to float. pass def get_float_limits() -> dict: Return a dictionary containing the maximum and minimum normalized positive float values. Returns: dict: Dictionary with keys \\"max\\" and \\"min\\" and their respective float limits. pass ``` # Explanation 1. **`create_float_from_string(s: str) -> float`:** This function uses `PyFloat_FromString` to create a new floating-point object from a string. If the string does not represent a valid floating-point number, the function returns `None`. 2. **`get_float_value(obj) -> float`:** This function uses `PyFloat_AsDouble` to retrieve a C double representation of the contents of `obj` if `obj` is a float or has a `__float__()` method. If the conversion is not possible, it raises a `ValueError`. 3. **`get_float_limits() -> dict`:** This function uses `PyFloat_GetMax` and `PyFloat_GetMin` to retrieve the maximum and minimum representable float values, respectively, and returns them in a dictionary. Implement these functions to facilitate efficient floating-point operations for a library.","solution":"def create_float_from_string(s: str) -> float: Create a floating-point number from a string. If the string does not represent a valid floating point number, return None. Args: s (str): The input string. Returns: float: The created floating-point number or None if invalid. try: return float(s) except ValueError: return None def get_float_value(obj) -> float: Return the floating-point value of the object if it is a float or has __float__ method. Raises ValueError if the object cannot be converted to float. Args: obj: The input object. Returns: float: The floating-point value. Raises: ValueError: If the object cannot be converted to float. try: return float(obj) except (TypeError, ValueError): raise ValueError(\\"The object cannot be converted to float.\\") def get_float_limits() -> dict: Return a dictionary containing the maximum and minimum normalized positive float values. Returns: dict: Dictionary with keys \\"max\\" and \\"min\\" and their respective float limits. import sys return {\\"max\\": sys.float_info.max, \\"min\\": sys.float_info.min}"},{"question":"<|Analysis Begin|> The provided documentation centers around using the `Dodge` transform in the `seaborn.objects` module, specifically demonstrating its application in modifying mark positions and widths in plots where data points overlap. The examples cover various scenarios: 1. Using `Dodge` to separate bars/marks on a plot. 2. Filling empty spaces when categories are not fully crossed. 3. Adding gaps between dodged marks. 4. Dodging when multiple semantic variables are combined. 5. Dodging only a subset of variables. 6. Combining `Dodge` with other transforms like `Jitter`. Based on this information, a coding assessment question can be designed to evaluate a student\'s understanding of the `Dodge` transform as well as their ability to integrate various seaborn functionalities to create insightful visualizations. <|Analysis End|> <|Question Begin|> # Question: You have been given the `tips` dataset, which is a sample dataset in the seaborn library. This dataset contains information about the tips received by waiters in a restaurant, including the following columns: - `total_bill`: Total bill amount (numeric) - `tip`: Tip amount (numeric) - `sex`: Gender of the person paying the bill (categorical: `Male`, `Female`) - `smoker`: Whether the payer is a smoker (categorical: `Yes`, `No`) - `day`: Day of the week (categorical: `Thur`, `Fri`, `Sat`, `Sun`) - `time`: Time of day (categorical: `Lunch`, `Dinner`) - `size`: Number of people in the party (numeric) Your task is to create a bar plot that visualizes the sum of total bills for each day of the week, separated by gender. Further: - Use the `Dodge` transform to handle overlapping bars. - Ensure there is a small gap between the dodged bars. - Add labels to the bars indicating the sum of total bills. **Input:** You do not need any input from the user. You will directly use the `tips` dataset from seaborn. **Output:** A bar plot visualizing the sum of total bills for each day of the week, dodged by gender, with gaps between bars and labels on each bar. **Constraints:** - Use `seaborn.objects` for creating the plot. - Clearly label each bar with the sum of total bills. **Performance Requirements:** The generated plot should be clear and easy to interpret. # Code Template ```python import seaborn.objects as so from seaborn import load_dataset import matplotlib.pyplot as plt # Load the dataset tips = load_dataset(\\"tips\\") # Your code to create the plot p = so.Plot(tips, x=\\"day\\", y=\\"total_bill\\", color=\\"sex\\") p.add(so.Bar(), so.Agg(\\"sum\\"), so.Dodge(gap=0.1)) # Add labels p.add(so.Bar(), so.Agg(\\"sum\\"), so.Dodge(gap=0.1)) p.label() # Display the plot plt.show() ``` **Instructions:** 1. Load the `tips` dataset. 2. Create a `seaborn.objects.Plot` object with `day` as the x variable, `total_bill` as the y variable, and `sex` as the color variable. 3. Add a `Bar` layer to the plot using the `Dodge` transform with a gap to separate the bars for different genders. 4. Label each bar with the sum of total bills. 5. Display the plot using `plt.show()`.","solution":"import seaborn.objects as so from seaborn import load_dataset import matplotlib.pyplot as plt def create_dodged_barplot(): # Load the dataset tips = load_dataset(\\"tips\\") # Create the plot with dodged bars p = ( so.Plot(tips, x=\\"day\\", y=\\"total_bill\\", color=\\"sex\\") .add(so.Bar(), so.Agg(\\"sum\\"), so.Dodge(gap=0.1)) .label(bars=True) # Add labels onto each bar ) # Display the plot p.show() # Example usage create_dodged_barplot()"},{"question":"**Task**: You are tasked with implementing a Python function that converts various numeric data types to Python integers and vice versa. This function should handle the following conversions: 1. A given C `long` to a Python integer. 2. A given C `unsigned long` to a Python integer. 3. A given C `double` to a Python integer. 4. A string representing an integer in a given base (between 2 and 36) to a Python integer. 5. A Python integer back to a C `long`. 6. Handle overflow or invalid conversions appropriately as specified by the provided documentation. Your task is to implement the following function: ```python def convert_and_handle_errors(value, conversion_type, base=10): Converts various C numeric types to Python integers and vice versa. Parameters: - value: The input value to be converted. - conversion_type: A string indicating the type of conversion to perform. It can be one of the following: \'long_to_py\', \'ulong_to_py\', \'double_to_py\', \'str_to_py\', \'py_to_long\'. - base: The base of the integer when conversion_type is \'str_to_py\'. Default is 10. Returns: - The converted value or raises an appropriate error if conversion is not possible. Raises: - ValueError: If the base is invalid or if the string conversion fails. - OverflowError: If the conversion results in an overflow situation. - TypeError: If the provided value is of an incorrect type for the specified conversion. pass ``` **Examples:** 1. `convert_and_handle_errors(123, \'long_to_py\')` should return the Python integer `123`. 2. `convert_and_handle_errors(12345, \'ulong_to_py\')` should return the Python integer `12345`. 3. `convert_and_handle_errors(123.456, \'double_to_py\')` should return the Python integer `123`. 4. `convert_and_handle_errors(\'7b\', \'str_to_py\', 16)` should return the Python integer `123`. 5. `convert_and_handle_errors(123, \'py_to_long\')` should return the C long `123`. **Constraints:** 1. Ensure that invalid conversions raise appropriate exceptions, such as `ValueError`, `OverflowError`, or `TypeError`. 2. Handle edge cases such as very large or very small numbers that might cause overflows. 3. The function should be efficient and handle conversions in constant time O(1), except for string parsing which should be O(n). **Note:** You may use pseudo-code or plain Python to describe any intricate details of the solution which are not directly related to Python\'s built-in operations. **Hint:** Use the functions mentioned in the provided documentation to perform the relevant conversions and handle any errors.","solution":"def convert_and_handle_errors(value, conversion_type, base=10): Converts various C numeric types to Python integers and vice versa. Parameters: - value: The input value to be converted. - conversion_type: A string indicating the type of conversion to perform. It can be one of the following: \'long_to_py\', \'ulong_to_py\', \'double_to_py\', \'str_to_py\', \'py_to_long\'. - base: The base of the integer when conversion_type is \'str_to_py\'. Default is 10. Returns: - The converted value or raises an appropriate error if conversion is not possible. Raises: - ValueError: If the base is invalid or if the string conversion fails. - OverflowError: If the conversion results in an overflow situation. - TypeError: If the provided value is of an incorrect type for the specified conversion. if conversion_type == \'long_to_py\': if not isinstance(value, int): raise TypeError(\\"Expected an integer for long_to_py conversion\\") return int(value) elif conversion_type == \'ulong_to_py\': if not isinstance(value, int) or value < 0: raise TypeError(\\"Expected a non-negative integer for ulong_to_py conversion\\") return int(value) elif conversion_type == \'double_to_py\': if not isinstance(value, (int, float)): raise TypeError(\\"Expected a float for double_to_py conversion\\") return int(value) elif conversion_type == \'str_to_py\': if not isinstance(value, str): raise TypeError(\\"Expected a string for str_to_py conversion\\") try: return int(value, base) except ValueError as e: raise ValueError(\\"String to integer conversion failed, check the base and the value\\") from e elif conversion_type == \'py_to_long\': if not isinstance(value, int): raise TypeError(\\"Expected an integer for py_to_long conversion\\") return value else: raise ValueError(\\"Unknown conversion type specified\\")"},{"question":"Write a Python program that processes a list of file paths and performs some operations on each file. Specifically, your program should do the following: 1. For each file in the list, attempt to open it and read its contents. 2. If the file does not exist, catch the appropriate exception and log an error message indicating the missing file. 3. If the file is not accessible (e.g., permission denied), catch the appropriate exception and log an error message indicating the access issue. 4. Convert the contents of each successfully read file to integers. If the file content is not valid integers, catch the appropriate exception and log an error message indicating the invalid data. 5. Sum the integers obtained from each file and maintain a running total. 6. Ensure that every file you open is properly closed, even if an error occurs. Finally, print the total sum of all valid integers read from the files. **Function Signature:** ```python def process_files(file_paths: list[str]) -> int: # Your implementation here ``` **Input:** - `file_paths`: A list of strings where each string is a file path. **Output:** - An integer that is the sum of valid integers read from all files. **Constraints and Requirements:** - Your program should handle at least several types of exceptions including `FileNotFoundError`, `PermissionError`, and `ValueError`. - You must use the `with` statement to open files to ensure they are always closed properly. - Log appropriate error messages for each type of exception caught. - Ensure the program is robust and performs clean-up actions where necessary. **Example Usage:** ```python file_paths = [\\"file1.txt\\", \\"file2.txt\\", \\"file3.txt\\"] total_sum = process_files(file_paths) print(f\\"Total sum of integers from valid files: {total_sum}\\") ``` In this example, if `file1.txt` contains valid integers and is accessible, those integers should be added to the total sum. If either `file2.txt` or `file3.txt` do not exist or contain invalid data, appropriate error messages should be logged, and these files should not disrupt the processing of other files. **Note:** - You need not create the files or provide their content. Focus on the implementation logic. - Ensure that your program operates efficiently and handles different possible exceptions gracefully.","solution":"def process_files(file_paths): total_sum = 0 for file_path in file_paths: try: with open(file_path, \'r\') as file: content = file.read() numbers = [int(x) for x in content.split()] total_sum += sum(numbers) except FileNotFoundError: print(f\\"Error: The file {file_path} does not exist.\\") except PermissionError: print(f\\"Error: Permission denied to access the file {file_path}.\\") except ValueError: print(f\\"Error: The file {file_path} contains non-integer values.\\") except Exception as e: print(f\\"An unexpected error occurred while processing the file {file_path}: {e}\\") return total_sum"},{"question":"# Asynchronous Task Management and Synchronization You are tasked with implementing a small server that handles client requests asynchronously using Python\'s `asyncio` module. The server must perform the following tasks: 1. Accept multiple client connections concurrently. 2. Use an asynchronous queue to distribute work to several worker tasks. 3. Ensure proper synchronization between tasks to manage shared resources safely. Requirements 1. Implement an asynchronous server that can handle multiple client connections: - The server should listen on a specified port and handle incoming connections. - For each client connection, the server should read a message, process it, and send back a response. 2. Use an `asyncio.Queue` to distribute incoming client requests to worker tasks: - The server should place each client request into the queue. - Implement worker tasks that retrieve requests from the queue, process them, and return the results. 3. Implement synchronization to ensure that shared resources are accessed safely: - Use an `asyncio.Lock` to protect shared resources. Function Signatures ```python async def handle_client(reader: asyncio.StreamReader, writer: asyncio.StreamWriter) -> None: Function to handle client connection. Reads a message from the client, processes it, and sends back a response. :param reader: StreamReader object :param writer: StreamWriter object ... async def client_request_processor(queue: asyncio.Queue, lock: asyncio.Lock) -> None: Worker task to process client requests. Retrieves requests from the queue, processes them, and returns the results. :param queue: Asynchronous queue containing client requests :param lock: Asynchronous lock for synchronization ... async def main() -> None: Main function to start the server and worker tasks. Sets up the server to listen on a specified port and start worker tasks. ... ``` Example Usage 1. Start the server: ```python asyncio.run(main()) ``` 2. Client sends a request: ```python reader, writer = await asyncio.open_connection(\'127.0.0.1\', PORT) writer.write(b\'Hello, server!\') await writer.drain() response = await reader.read(100) print(\'Received:\', response.decode()) ``` Constraints - The server should handle at least 10 concurrent connections. - Implement at least three worker tasks to process the queue. - Ensure that the program correctly handles task cancellation and shutdown gracefully. # Additional Notes - The message processing can be a simple transformation (e.g., reversing the string, converting to uppercase, etc.). - Handle exceptions properly to ensure the server does not crash unexpectedly.","solution":"import asyncio async def handle_client(reader: asyncio.StreamReader, writer: asyncio.StreamWriter) -> None: Function to handle client connection. Reads a message from the client, processes it, and sends back a response. data = await reader.read(100) message = data.decode() addr = writer.get_extra_info(\'peername\') print(f\\"Received {message!r} from {addr!r}\\") process_message = message[::-1] # Example processing: reversing the string print(f\\"Send: {process_message!r}\\") writer.write(process_message.encode()) await writer.drain() print(\\"Close the connection\\") writer.close() async def client_request_processor(queue: asyncio.Queue, lock: asyncio.Lock) -> None: Worker task to process client requests. Retrieves requests from the queue, processes them, and returns the results. while True: reader, writer = await queue.get() async with lock: await handle_client(reader, writer) queue.task_done() async def main() -> None: Main function to start the server and worker tasks. Sets up the server to listen on a specified port and start worker tasks. queue = asyncio.Queue() lock = asyncio.Lock() port = 8888 server = await asyncio.start_server(lambda r, w: queue.put_nowait((r, w)), \'127.0.0.1\', port) addresses = \', \'.join(str(sock.getsockname()) for sock in server.sockets) print(f\'Serving on {addresses}\') worker_tasks = [asyncio.create_task(client_request_processor(queue, lock)) for _ in range(3)] async with server: await server.serve_forever() if __name__ == \'__main__\': try: asyncio.run(main()) except KeyboardInterrupt: print(\\"Server stopped manually.\\")"},{"question":"**Coding Assessment Question** # Objective Your task is to implement a Python function using the `syslog` module to configure the system logger, send log messages of varying priorities, and handle different logging facilities and options. # Function Signature ```python def configure_and_log( ident: str, facility: str, include_pid: bool, messages: dict ) -> None: ``` # Input - `ident` (str): An identifier string that is prepended to every message. - `facility` (str): A string representing the logging facility to be used (one of \'LOG_USER\', \'LOG_MAIL\', \'LOG_DAEMON\', etc.). - `include_pid` (bool): A boolean that indicates whether the process ID should be included in the log messages. - `messages` (dict): A dictionary where keys are priority levels (one of \'LOG_EMERG\', \'LOG_ALERT\', \'LOG_CRIT\', etc.) and values are the messages to be logged at those levels. # Constraints - The input strings for `ident`, `facility`, and the dictionary keys for `messages` will be valid and within the predefined constants in the `syslog` module. - You should use the `syslog` constants for priorities and facilities. - Your solution should call `syslog.openlog()`, `syslog.syslog()`, and `syslog.closelog()` appropriately. - Ensure that messages are logged following the provided configuration (including or excluding the PID). # Example ```python def configure_and_log( ident: str, facility: str, include_pid: bool, messages: dict ) -> None: import syslog log_option = syslog.LOG_PID if include_pid else 0 facility_const = getattr(syslog, facility) syslog.openlog(ident=ident, logoption=log_option, facility=facility_const) for priority, message in messages.items(): priority_const = getattr(syslog, priority) syslog.syslog(priority_const, message) syslog.closelog() # Example usage configure_and_log( ident=\'my_application\', facility=\'LOG_USER\', include_pid=True, messages={ \'LOG_INFO\': \'Information message\', \'LOG_ERR\': \'Error message\' } ) ``` This example configures the logger to include the process ID and use the \'LOG_USER\' facility. It logs an informational message and an error message with the specified priorities. # Explanation - `syslog.openlog()` is called with `ident`, `logoption`, and `facility`. - For each log message, `syslog.syslog()` is used with the appropriate priority and message. - `syslog.closelog()` is called at the end to reset the syslog module values. # Performance Requirements - The function should execute within reasonable time limits given typical system logging constraints.","solution":"import syslog def configure_and_log( ident: str, facility: str, include_pid: bool, messages: dict ) -> None: Configures the system logger and sends log messages of varying priorities. Parameters: ident (str): An identifier string that is prepended to every message. facility (str): The logging facility to be used (one of \'LOG_USER\', \'LOG_MAIL\', \'LOG_DAEMON\', etc.). include_pid (bool): Whether the process ID should be included in the log messages. messages (dict): A dictionary where keys are priority levels (one of \'LOG_EMERG\', \'LOG_ALERT\', \'LOG_CRIT\', etc.) and values are the messages to be logged at those levels. log_option = syslog.LOG_PID if include_pid else 0 facility_const = getattr(syslog, facility) syslog.openlog(ident=ident, logoption=log_option, facility=facility_const) for priority, message in messages.items(): priority_const = getattr(syslog, priority) syslog.syslog(priority_const, message) syslog.closelog()"},{"question":"**Coding Assessment Question:** # Instructions: You are given a CSV file named `sales_data.csv` that contains information about monthly sales. The dataset has the following columns: - `Month` (string): The month of the sales data. - `Sales` (integer): The number of sales made. Some values may be missing. Your task is to write a Python function using pandas that reads this CSV file, processes the data to handle missing values using the nullable integer type, and then returns specific statistical information and a modified DataFrame as described below. # Requirements: 1. Read the data from the `sales_data.csv` file. 2. Convert the `Sales` column to a nullable integer type using `Int64Dtype`. 3. Calculate the total sales, ignoring any missing values. 4. Fill the missing values in the `Sales` column with the mean of the non-missing `Sales` values. 5. Create a new column `Sales_Difference` that shows the difference between each month\'s sales and the previous month\'s sales. The first month should have `NA` as its value for this column. 6. Return the following: - The total sales as an integer. - The modified DataFrame with the `Sales` column as a nullable integer type and the new `Sales_Difference` column. # Constraints: - You must use pandas for all data manipulations. - You should handle missing values using `pandas.NA` and the `Int64Dtype`. # Example Usage: ```python # Assume the sales_data.csv file contains the following data: # Month,Sales # Jan,100 # Feb,200 # Mar, # Apr,150 total_sales, modified_df = process_sales_data(\'sales_data.csv\') print(total_sales) # Should print the total sales ignoring missing values. print(modified_df) ``` The output of `modified_df` should be: ``` Month Sales Sales_Difference 0 Jan 100 <NA> 1 Feb 200 100 2 Mar 150 -50 # assuming the missing value is filled with 150 3 Apr 150 0 ``` **Note**: The actual values in the DataFrame columns may vary based on the data in your CSV file. # Function Signature: ```python import pandas as pd def process_sales_data(file_path: str): # Your code here ```","solution":"import pandas as pd def process_sales_data(file_path: str): # Read the data from the CSV file df = pd.read_csv(file_path) # Convert the Sales column to a nullable integer type df[\'Sales\'] = df[\'Sales\'].astype(\'Int64\') # Calculate the total sales ignoring missing values total_sales = df[\'Sales\'].sum(skipna=True) # Fill the missing values in the Sales column with the mean of the non-missing Sales values sales_mean = df[\'Sales\'].mean(skipna=True) df[\'Sales\'] = df[\'Sales\'].fillna(sales_mean).astype(\'Int64\') # Create a new column Sales_Difference df[\'Sales_Difference\'] = df[\'Sales\'].diff().astype(\'Int64\') return total_sales, df"},{"question":"**Question:** You have been given a dataset containing information about daily weather observations. Using the Seaborn `objects` module, create a comprehensive plot that showcases the following aspects of the data: 1. The dataset contains information about `temperature` (in Celsius), `humidity` (percentage), `wind_speed` (km/h), and the `day` of the observation. 2. Plot a scatter plot of `temperature` vs `humidity`, and fit a linear regression line to the scatter plot. 3. Represent the `wind_speed` using the size of the points in the scatter plot. 4. Use different colors for the points to differentiate between weekends and weekdays. 5. Additionally, create a facet for each unique `day` to visualize the daily variations. The resulting plot should be well-labeled and include a legend indicating the significance of colors and point sizes. **Input:** * A dataset `weather` with the following structure: ``` day temperature humidity wind_speed Monday 21.5 45 15 Tuesday 22.5 50 10 ... ``` You can assume that the data is loaded as follows: ```python weather = load_dataset(\\"weather\\") ``` **Expected Output:** The plot should include: * A primary scatter plot of `temperature` vs `humidity`. * A linear regression line fitted to the scatter plot. * Variable point sizes based on `wind_speed`. * Points colored differently to distinguish between weekends and weekdays. * Separate facets for each `day`. **Constraints:** * The solution should illustrate the proper use of Seaborn `objects` and associated transformations. * Ensure that the plot is informative, well-labeled, and visually distinguishable. **Example solution:** ```python import seaborn.objects as so from seaborn import load_dataset # Load the dataset weather = load_dataset(\\"weather\\") # Pre-process data to add a \'weekend\' flag weather[\'weekend\'] = weather[\'day\'].apply(lambda x: \'Weekend\' if x in [\'Saturday\', \'Sunday\'] else \'Weekday\') # Create the plot plot = ( so.Plot(weather, x=\\"temperature\\", y=\\"humidity\\", color=\\"weekend\\", pointsize=\\"wind_speed\\") .add(so.Dot(), so.Jitter()) .add(so.Line(), so.PolyFit()) .facet(col=\\"day\\") .label(x=\\"Temperature (C)\\", y=\\"Humidity (%)\\", color=\\"Day Type\\", pointsize=\\"Wind Speed (km/h)\\") ) plot ```","solution":"import seaborn.objects as so import seaborn as sns def plot_weather_data(weather): Plots weather data showcasing temperature vs humidity with a linear regression line, wind speed represented by point size, and different colors for weekends and weekdays. Facets are used to visualize daily variations. Parameters: weather (DataFrame): The weather dataset containing \'day\', \'temperature\', \'humidity\', and \'wind_speed\' columns. # Add a \'weekend\' flag to the dataset weather[\'weekend\'] = weather[\'day\'].apply(lambda x: \'Weekend\' if x in [\'Saturday\', \'Sunday\'] else \'Weekday\') # Create the plot plot = ( so.Plot(weather, x=\\"temperature\\", y=\\"humidity\\", color=\\"weekend\\", pointsize=\\"wind_speed\\") .add(so.Dot(), so.Jitter()) .add(so.Line(), so.PolyFit()) .facet(col=\\"day\\") .label(x=\\"Temperature (C)\\", y=\\"Humidity (%)\\", color=\\"Day Type\\", pointsize=\\"Wind Speed (km/h)\\") ) plot.show() # Create the plot_weather_data function to plot the weather data passed to it # No need to return anything as plot.show() renders the plot"},{"question":"# Question: AU File Manipulator You are tasked with manipulating Sun AU audio files using the `sunau` module. This exercise will assess your understanding of file I/O operations, handling object methods, and managing audio file properties. # Objectives 1. Write a function `extract_info(filename)` that extracts important details from a given Sun AU file. 2. Develop a function `create_au_copy(src_filename, dst_filename)` that reads audio data from the source file, and writes it to a new destination file, copying all properties. # Function 1: `extract_info(filename)` This function reads the metadata from a Sun AU file and returns a dictionary containing the following key-value pairs: - `\\"nchannels\\"`: Number of audio channels. - `\\"sampwidth\\"`: Sample width in bytes. - `\\"framerate\\"`: Sampling frequency. - `\\"nframes\\"`: Number of audio frames. - `\\"comptype\\"`: Compression type. - `\\"compname\\"`: Human-readable version of the compression type. Input - `filename` (str): The path to the Sun AU file to be read. Output - `info` (dict): A dictionary containing the metadata fields. Example ```python filename = \'example.au\' info = extract_info(filename) # Expected output might be like the following # info = { # \\"nchannels\\": 2, # \\"sampwidth\\": 2, # \\"framerate\\": 44100, # \\"nframes\\": 100000, # \\"comptype\\": \'NONE\', # \\"compname\\": \'not compressed\' # } ``` # Function 2: `create_au_copy(src_filename, dst_filename)` This function opens an existing Sun AU file, reads its contents, and writes an identical copy to a new file. Input - `src_filename` (str): The path to the source Sun AU file. - `dst_filename` (str): The path to the destination Sun AU file where the copy will be stored. Output - None. The function should create a new AU file at `dst_filename` containing the copied audio data. Example ```python src_filename = \'example.au\' dst_filename = \'copy_example.au\' create_au_copy(src_filename, dst_filename) # Creates a copy of \'example.au\' named \'copy_example.au\' ``` # Constraints - Handle exceptions where appropriate, ensuring the original state is maintained if an error occurs. - Ensure that file handles are properly managed and closed after operations. # Performance Considerations - The functions should efficiently handle large audio files without running into memory issues by utilizing appropriate buffering techniques where necessary. - The script should execute within a reasonable time for files up to 50 MB in size.","solution":"import sunau def extract_info(filename): Extracts important details from a given Sun AU file. Parameters: filename (str): The path to the Sun AU file to be read. Returns: dict: A dictionary containing the metadata fields. info = {} try: with sunau.open(filename, \'rb\') as au_file: info[\\"nchannels\\"] = au_file.getnchannels() info[\\"sampwidth\\"] = au_file.getsampwidth() info[\\"framerate\\"] = au_file.getframerate() info[\\"nframes\\"] = au_file.getnframes() info[\\"comptype\\"] = au_file.getcomptype() info[\\"compname\\"] = au_file.getcompname() except Exception as e: print(f\\"Error reading AU file: {e}\\") return info def create_au_copy(src_filename, dst_filename): Creates an identical copy of a Sun AU file. Parameters: src_filename (str): The path to the source Sun AU file. dst_filename (str): The path to the destination Sun AU file where the copy will be stored. try: with sunau.open(src_filename, \'rb\') as src_file: with sunau.open(dst_filename, \'wb\') as dst_file: # Set all the properties of the destination file to match the source file dst_file.setnchannels(src_file.getnchannels()) dst_file.setsampwidth(src_file.getsampwidth()) dst_file.setframerate(src_file.getframerate()) dst_file.setcomptype(src_file.getcomptype(), src_file.getcompname()) # Read and write the frames data in chunks to handle large files efficiently while True: frames = src_file.readframes(1024) if not frames: break dst_file.writeframes(frames) except Exception as e: print(f\\"Error copying AU file: {e}\\")"},{"question":"**Objective:** The goal of this task is to test your understanding of creating and manipulating different fundamental and sequence objects in Python, as well as ensuring proper type checking. # Function Implementation You are required to implement a function `process_objects(data)` that performs the following: 1. **Type Checking:** Check the type of the input `data` to ensure it\'s a dictionary. If it\'s not, raise a `TypeError` with the message \\"Input must be a dictionary\\". 2. **Handling Different Object Types:** - The input dictionary `data` will have keys \'integer\', \'float\', \'boolean\', \'string\', \'list\', and \'tuple\'. - For each key, ensure the associated value is of the correct type: - \'integer\' should map to an integer object. - \'float\' should map to a floating-point object. - \'boolean\' should map to a boolean object. - \'string\' should map to a string (unicode) object. - \'list\' should map to a list of strings. - \'tuple\' should map to a tuple of integers. 3. **Operation on Objects:** - For the \'integer\' value, increment it by 1. - For the \'float\' value, multiply it by 2.0. - For the \'boolean\' value, negate it (True becomes False and vice versa). - For the \'string\' value, change it to uppercase. - For the \'list\' value, reverse the list. - For the \'tuple\' value, sum all the integers in the tuple. 4. **Return Result:** Return a dictionary with the same keys as the input, but with the processed values. # Input Format - A dictionary `data` with the keys \'integer\', \'float\', \'boolean\', \'string\', \'list\', and \'tuple\'. # Output Format - A dictionary with the same keys and the processed values as described. # Constraints - The \'integer\' value will be an integer. - The \'float\' value will be a floating point number. - The \'boolean\' value will be a boolean. - The \'string\' value will be a string. - The \'list\' value will be a list containing only strings. - The \'tuple\' value will be a tuple containing only integers. - You can assume the dictionary will always have these keys. # Example ```python input_data = { \'integer\': 5, \'float\': 3.5, \'boolean\': True, \'string\': \'hello\', \'list\': [\'apple\', \'banana\', \'cherry\'], \'tuple\': (1, 2, 3) } expected_output = { \'integer\': 6, \'float\': 7.0, \'boolean\': False, \'string\': \'HELLO\', \'list\': [\'cherry\', \'banana\', \'apple\'], \'tuple\': 6 } assert process_objects(input_data) == expected_output ``` # Function Signature ```python def process_objects(data: dict) -> dict: pass ``` **Note:** Ensure to implement type checking and handle any exceptions for incorrect input types as specified.","solution":"def process_objects(data): Processes a dictionary containing different fundamental and sequence objects. Args: data (dict): Input dictionary with keys \'integer\', \'float\', \'boolean\', \'string\', \'list\', and \'tuple\'. Returns: dict: Dictionary with processed values. if not isinstance(data, dict): raise TypeError(\\"Input must be a dictionary\\") processed_data = {} # Ensure correct type for each key if not isinstance(data.get(\'integer\'), int): raise TypeError(\\"The value for \'integer\' must be an int\\") if not isinstance(data.get(\'float\'), float): raise TypeError(\\"The value for \'float\' must be a float\\") if not isinstance(data.get(\'boolean\'), bool): raise TypeError(\\"The value for \'boolean\' must be a bool\\") if not isinstance(data.get(\'string\'), str): raise TypeError(\\"The value for \'string\' must be a str\\") if not isinstance(data.get(\'list\'), list) or not all(isinstance(item, str) for item in data.get(\'list\')): raise TypeError(\\"The value for \'list\' must be a list of strings\\") if not isinstance(data.get(\'tuple\'), tuple) or not all(isinstance(item, int) for item in data.get(\'tuple\')): raise TypeError(\\"The value for \'tuple\' must be a tuple of ints\\") # Perform processing processed_data[\'integer\'] = data[\'integer\'] + 1 processed_data[\'float\'] = data[\'float\'] * 2.0 processed_data[\'boolean\'] = not data[\'boolean\'] processed_data[\'string\'] = data[\'string\'].upper() processed_data[\'list\'] = list(reversed(data[\'list\'])) processed_data[\'tuple\'] = sum(data[\'tuple\']) return processed_data"},{"question":"**Objective:** Implement a Python program that reads, formats, and writes data to a file, demonstrating your understanding of string formatting, file I/O operations, and data serialization. **Problem Statement:** You are given a text file named `data.txt` that contains multiple lines. Each line has three values separated by commas: a name (string), an age (integer), and a favorite number (float). For example: ``` John Doe,25,3.14 Jane Smith,30,2.71 Alice Johnson,22,1.61 ``` Your task is to write a program that: 1. Reads the data from `data.txt`. 2. Formats the data into a table-like structure using formatted string literals (f-strings). 3. Writes the formatted data into a new file named `formatted_data.txt`. 4. Serializes the original data into a JSON file named `data.json`. **Instructions:** 1. Implement a function `read_data(filename: str) -> list` that reads the data from `filename` and returns a list of tuples, each containing a name, age, and favorite number. 2. Implement a function `format_data(data: list) -> str` that takes the list of tuples and returns a formatted string that aligns the columns neatly. 3. Implement a function `write_formatted_data(filename: str, formatted_data: str)` that writes the formatted string into `filename`. 4. Implement a function `serialize_data(filename: str, data: list)` that serializes the list of tuples into a JSON file `filename`. 5. Use `with` statements to handle file operations securely. **Constraints:** - Assume the file `data.txt` exists in the same directory as your script. - The age values are positive integers. - The favorite number values are positive floats. - Use UTF-8 encoding for reading and writing text files. **Example:** If the content of `data.txt` is as given in the problem statement, then: - `formatted_data.txt` should contain: ``` Name Age Favorite Number John Doe 25 3.14 Jane Smith 30 2.71 Alice Johnson 22 1.61 ``` - `data.json` should contain: ```json [ [\\"John Doe\\", 25, 3.14], [\\"Jane Smith\\", 30, 2.71], [\\"Alice Johnson\\", 22, 1.61] ] ``` **Deliverables:** 1. Your Python script `solution.py` that implements the required functions and reads from `data.txt`, processes the data, and writes to `formatted_data.txt` and `data.json`. 2. Ensure you handle any potential exceptions that might arise during file operations with appropriate error messages. Good luck!","solution":"import json def read_data(filename: str) -> list: data = [] try: with open(filename, \'r\', encoding=\'utf-8\') as file: for line in file: name, age, fav_number = line.strip().split(\',\') data.append((name, int(age), float(fav_number))) except FileNotFoundError: print(f\\"Error: {filename} not found.\\") except Exception as e: print(f\\"An error occurred: {e}\\") return data def format_data(data: list) -> str: formatted_lines = [\\"Name Age Favorite Number\\"] for name, age, fav_number in data: formatted_lines.append(f\\"{name:<13} {age:<5} {fav_number}\\") return \'n\'.join(formatted_lines) def write_formatted_data(filename: str, formatted_data: str): try: with open(filename, \'w\', encoding=\'utf-8\') as file: file.write(formatted_data) except Exception as e: print(f\\"An error occurred while writing to {filename}: {e}\\") def serialize_data(filename: str, data: list): try: with open(filename, \'w\', encoding=\'utf-8\') as file: json.dump(data, file) except Exception as e: print(f\\"An error occurred while writing to {filename}: {e}\\") if __name__ == \\"__main__\\": raw_data = read_data(\'data.txt\') formatted_data = format_data(raw_data) write_formatted_data(\'formatted_data.txt\', formatted_data) serialize_data(\'data.json\', raw_data)"},{"question":"# URL Manipulation and Validation You are tasked with creating a URL validator and manipulator function. This function should take in a list of URLs and perform the following tasks: 1. **Validation**: Ensure each URL has a valid scheme (`http` or `https`). If not, discard the URL. 2. **Normalization**: For each valid URL: - Parse the URL into its components. - Ensure that all percent-encoded characters in the path are decoded (e.g., `%20` becomes a space). - Convert all characters in the hostname to lowercase. - Remove any default ports (`80` for `http` and `443` for `https`). 3. **Reassembly**: Reassemble the components back into a normalized URL. 4. **Query Manipulation and Sorting**: If the URL contains a query string: - Parse the query string into key-value pairs. - Sort the parameters alphabetically by key. - Rebuild the query string with sorted parameters. Finally, return a list of the normalized URLs. Function Signature ```python from typing import List def normalize_urls(urls: List[str]) -> List[str]: pass ``` Expected Input and Output - **Input**: A list of strings, where each string is a URL. - **Output**: A list of normalized and validated URLs as strings. Example ```python urls = [ \\"HTTP://example.com:80/a%20b?z=1&y=2\\", \\"https://www.Example.com:443/foo/\\", \\"ftp://invalid.com/resource\\", \\"http://example.net:8080/data?x=5&b=2&a=1\\" ] normalized_urls = normalize_urls(urls) # Expected Output: # [ # \\"http://example.com/a b?y=2&z=1\\", # \\"https://www.example.com/foo/\\", # \\"http://example.net:8080/data?a=1&b=2&x=5\\" # ] ``` Constraints - Do not validate URLs in depth. Assume given URLs have syntactically valid format. - Focus on the tasks outlined without considering security implications beyond specified requirements.","solution":"from typing import List from urllib.parse import urlparse, urlunparse, parse_qsl, urlencode, quote, unquote def normalize_urls(urls: List[str]) -> List[str]: normalized_urls = [] for url in urls: parsed_url = urlparse(url) # Validation of the scheme if parsed_url.scheme not in [\'http\', \'https\']: continue # Components of the URL scheme = parsed_url.scheme netloc = parsed_url.netloc.lower() path = unquote(parsed_url.path) params = parsed_url.params query = parsed_url.query fragment = parsed_url.fragment # Remove default ports if (scheme == \'http\' and netloc.endswith(\':80\')): netloc = netloc[:-3] elif (scheme == \'https\' and netloc.endswith(\':443\')): netloc = netloc[:-4] # Reassemble the normalized URL parsed_query = parse_qsl(query) sorted_query = urlencode(sorted(parsed_query)) normalized_url = urlunparse((scheme, netloc, path, params, sorted_query, fragment)) normalized_urls.append(normalized_url) return normalized_urls"},{"question":"# Task: Design a custom module importer in Python using the `importlib` package functionalities. The custom importer should: 1. Dynamically import a specified module using `importlib`. 2. Facilitate reading a specific resource (e.g., a text file) from within the imported module. # Specifications: 1. Implement the function `dynamic_import_and_read_resource(module_name: str, resource_name: str) -> str` that: - Takes the `module_name` as a string representing the module to be imported. - Takes the `resource_name` as a string representing the name of the resource to be read from the module. - Returns the content of the resource as a string. 2. Ensure that the function: - Dynamically imports the specified module. - Reads the specified resource (text file) from within the dynamically imported module. - Returns the text content of the resource. # Input Format: - `module_name` (str): A string denoting the name of the module to import (e.g., \'collections\', \'json\'). - `resource_name` (str): A string denoting the name of the resource to read (e.g., \'file.txt\'). # Output Format: - A string representing the content of the resource file. # Constraints: - Assume the resource file exists within the specified module. - Assume the resource file is a text file with UTF-8 encoding. # Example: ```python content = dynamic_import_and_read_resource(\'my_package\', \'data.txt\') print(content) # Prints the content of \'data.txt\' located inside \'my_package\' ``` # Implementation Notes: - Use `importlib.import_module()` to dynamically import the module. - Use `importlib.resources.read_text()` to read the resource. # Solution Template: ```python import importlib import importlib.resources def dynamic_import_and_read_resource(module_name: str, resource_name: str) -> str: # Dynamically import the specified module module = importlib.import_module(module_name) # Read the specified resource from within the imported module with importlib.resources.open_text(module, resource_name) as resource_file: content = resource_file.read() return content ```","solution":"import importlib import importlib.resources def dynamic_import_and_read_resource(module_name: str, resource_name: str) -> str: Dynamically imports the specified module and reads the specified resource. Args: module_name (str): The name of the module to import. resource_name (str): The name of the resource to read from the module. Returns: str: The content of the resource file. # Dynamically import the specified module module = importlib.import_module(module_name) # Read and return the specified resource from within the imported module content = importlib.resources.read_text(module, resource_name) return content"},{"question":"**Objective:** In this task, you are required to demonstrate your understanding of the `sklearn.datasets` package by performing the following steps: 1. Fetch a specific real-world dataset using `sklearn.datasets`. 2. Perform basic data preprocessing. 3. Split the data into training and testing sets. 4. Return required details about the dataset and the split sets. **Task Description:** 1. **Loading the Dataset**: - Fetch the \\"Linnerud\\" dataset using the `sklearn.datasets` fetcher method. - Return the dataset as a tuple `(data, target)` using the parameter `return_X_y=True`. 2. **Data Preprocessing**: - Standardize the features by removing the mean and scaling to unit variance using `StandardScaler` from `sklearn.preprocessing`. 3. **Data Splitting**: - Split the dataset into training and testing sets, using an 80-20 split. Use `train_test_split` from `sklearn.model_selection`. - Set `random_state=42` to ensure reproducibility. 4. **Output Details**: - Return a dictionary with the following keys: - `data_shape`: A tuple containing the shape of the `data`. - `target_shape`: A tuple containing the shape of the `target`. - `train_data_shape`: A tuple containing the shape of the training data. - `test_data_shape`: A tuple containing the shape of the testing data. - `train_target_shape`: A tuple containing the shape of the training target. - `test_target_shape`: A tuple containing the shape of the testing target. - `description`: A brief description of the dataset (use the `DESCR` attribute). **Function Signature**: ```python def dataset_details() -> dict: # Your code here ``` **Constraints**: - Ensure that the data is standardized before splitting. - Use `random_state=42` for the `train_test_split`. **Example**: ```python result = dataset_details() print(result) ``` Expected output: ```python { \'data_shape\': (20, 3), \'target_shape\': (20, 3), \'train_data_shape\': (16, 3), \'test_data_shape\': (4, 3), \'train_target_shape\': (16, 3), \'test_target_shape\': (4, 3), \'description\': \'The Linnerud dataset is a multi-output regression dataset...n\' } ``` **Notes**: - The exact description may vary. - Ensure the function is self-contained and does not require any additional files or data to be provided by the user.","solution":"from sklearn.datasets import load_linnerud from sklearn.preprocessing import StandardScaler from sklearn.model_selection import train_test_split def dataset_details() -> dict: # Load the Linnerud dataset data, target = load_linnerud(return_X_y=True) description = load_linnerud().DESCR # Standardize the features scaler = StandardScaler() data = scaler.fit_transform(data) # Split the dataset into training and testing sets data_train, data_test, target_train, target_test = train_test_split(data, target, test_size=0.2, random_state=42) # Return the required details return { \'data_shape\': data.shape, \'target_shape\': target.shape, \'train_data_shape\': data_train.shape, \'test_data_shape\': data_test.shape, \'train_target_shape\': target_train.shape, \'test_target_shape\': target_test.shape, \'description\': description }"},{"question":"<|Analysis Begin|> The provided documentation contains several examples of using the Seaborn library to create boxplots from the Titanic dataset. The examples illustrate different features and customizations of boxplots that one can achieve using Seaborn, including: 1. Drawing a single horizontal boxplot. 2. Grouping by a categorical variable. 3. Creating nested grouping by two variables. 4. Modifying the appearance of the box plots (line art, gaps, colors, widths, etc.) 5. Grouping by a numeric variable and preserving its native scaling. 6. Customizing the plot using parameters of the underlying matplotlib function. The documentation offers a comprehensive overview of the basic and advanced functionalities related to creating boxplots with Seaborn, which suggests that we can design a question that evaluates the students’ ability to: * Load and prepare data. * Create various types of boxplots. * Customize their appearances according to specific requirements. <|Analysis End|> <|Question Begin|> **Question: Advanced Boxplot Customizations with Seaborn** You are provided with the Titanic dataset, which can be loaded via Seaborn. Your task is to demonstrate your understanding of Seaborn by performing the following steps: 1. Load the Titanic dataset. 2. Create a vertical boxplot displaying the \\"age\\" distribution for each class. 3. Customize the boxplot to include: * Separate boxes for cases where the passengers survived and where they did not (`alive` status). * Boxes being drawn with only line art and a small gap between them. 4. Add whiskers to cover the full range of the data, i.e., setting the whiskers to (0, 100). 5. Modify the appearance: * Narrow the boxes to half their default width. * Change the color and line width of the boxes to your preference. 6. Add a vertical line at age 25. # Input: None # Output: A detailed and well-customized boxplot as described above. # Constraints: - You must use the Seaborn library for all visualizations. - Ensure your code is well-commented to explain each customization step. # Example Code: ```python import seaborn as sns # Step 1: Load the Titanic dataset sns.set_theme(style=\\"whitegrid\\") titanic = sns.load_dataset(\\"titanic\\") # Step 2: Create a vertical boxplot displaying the \\"age\\" distribution for each class # Step 3: Separate boxes for cases where the passengers survived and where they did not (alive status) # Boxes being drawn with only line art and a small gap between them sns.catplot(data=titanic, x=\\"class\\", y=\\"age\\", hue=\\"alive\\", kind=\\"box\\", fill=False, gap=0.1) # Step 4: Add whiskers to cover the full range of the data sns.boxplot(data=titanic, x=\\"class\\", y=\\"age\\", hue=\\"alive\\", whis=(0, 100), fill=False, gap=0.1) # Step 5: Narrow the boxes to half their default width # Change the color and line width of the boxes to your preference sns.boxplot(data=titanic, x=\\"class\\", y=\\"age\\", hue=\\"alive\\", width=0.5, color=\\".8\\", linewidth=0.75, whis=(0, 100), fill=False, gap=0.1) # Step 6: Add a vertical line at age 25 ax = sns.boxplot(data=titanic, x=\\"class\\", y=\\"age\\", hue=\\"alive\\", width=0.5, color=\\".8\\", linewidth=0.75, whis=(0, 100), fill=False, gap=.1) ax.axhline(25, color=\\"r\\", linestyle=\\"--\\") plt.show() ``` Ensure that your resulting plot accurately reflects the described customizations.","solution":"import seaborn as sns import matplotlib.pyplot as plt def customized_titanic_boxplot(): Create a customized boxplot of the Titanic dataset showing the age distribution for each class, separated by survival status, with various customizations. # Step 1: Load the Titanic dataset sns.set_theme(style=\\"whitegrid\\") titanic = sns.load_dataset(\\"titanic\\") # Step 2: Create a vertical boxplot displaying the \\"age\\" distribution for each class # Step 3: Customize to include separate boxes for cases where the passengers survived and where they did not, # with boxes being drawn with only line art and a small gap between them. # Step 4: Add whiskers to cover the full range of the data (0, 100). # Step 5: Narrow the boxes to half their default width and change the color and line width. ax = sns.boxplot( data=titanic, x=\\"class\\", y=\\"age\\", hue=\\"alive\\", whis=(0, 100), width=0.5, linewidth=0.75, palette=\\"Set2\\" ) # Step 6: Add a vertical line at age 25 ax.axhline(25, color=\\"r\\", linestyle=\\"--\\") # Show the plot plt.show()"},{"question":"**Objective:** Your task is to implement a function leveraging the `linecache` module to read specified lines from a Python script file. The function should also handle edge cases gracefully, such as when a line number exceeds the total number of lines in the file or when the file does not exist. **Problem Statement:** Implement the following function: ```python def fetch_script_lines(filename: str, line_numbers: list) -> dict: Fetches specified lines from a given Python script file. Parameters: filename (str): The path to the Python script file. line_numbers (list): A list of integers representing the line numbers to fetch. Returns: dict: A dictionary where the keys are the provided line numbers and the values are the corresponding lines from the file. If the line number is invalid or does not exist, the value should be an empty string. pass ``` # Input: - `filename`: A string specifying the path to a Python script file. - `line_numbers`: A list of integers representing the line numbers that need to be fetched from the file. # Output: - Return a dictionary where the keys are the requested line numbers and the values are the lines from the file. If a line number is invalid (e.g., greater than the number of lines in the file) or if the file does not exist, the returned dictionary should have an empty string for those line numbers. **Example:** ```python file_content = def foo(): return \'foo\' def bar(): return \'bar\' filename = create_temp_file_with_content(file_content) # Assume this function creates and returns the filename of a temp file with the given content result = fetch_script_lines(filename, [1, 2, 3, 10]) print(result) # Expected output: {1: \'n\', 2: \'def foo():n\', 3: \' return \'foo\'n\', 10: \'\'} ``` # Constraints: - You may assume that the line numbers provided in the list are positive integers. - The function should handle exceptions gracefully, without raising errors to the user. # Performance Requirements: - The solution should be efficient in terms of time complexity regardless of the size of the file and number of requested lines. - Utilize caching mechanisms provided by the `linecache` module optimally. # Implementation Notes: - Use the `linecache.getline()` function to read lines. - Make sure to handle the edge cases such as non-existent files or line numbers beyond the file\'s length. Test your implementation thoroughly to cover various scenarios including valid inputs, invalid inputs, edge cases, and performance with large files.","solution":"import linecache def fetch_script_lines(filename: str, line_numbers: list) -> dict: Fetches specified lines from a given Python script file. Parameters: filename (str): The path to the Python script file. line_numbers (list): A list of integers representing the line numbers to fetch. Returns: dict: A dictionary where the keys are the provided line numbers and the values are the corresponding lines from the file. If the line number is invalid or does not exist, the value should be an empty string. output = {} for line_number in line_numbers: try: line = linecache.getline(filename, line_number) output[line_number] = line.rstrip(\'n\') # Remove newline character for consistency except Exception: output[line_number] = \'\' # Clear cache to avoid issues with reading updated files linecache.clearcache() return output"},{"question":"# Python Coding Assessment **Objective**: The purpose of this coding assessment is to evaluate your understanding of various built-in Python types and their methods, as well as truth value testing, comparisons, and common sequence operations. Problem Overview # Task: Integer Binary Transformations You are required to implement a function `binary_transformations` that takes an integer and returns a dictionary with the following information: 1. The binary representation of the integer. 2. The number of bits required to represent the integer in binary (excluding the sign and leading zeroes). 3. The number of `1` bits in the binary representation of the integer (also known as the population count or Hamming weight). # Function Signature ```python def binary_transformations(n: int) -> dict: pass ``` # Input - `n` (int): An integer (0 <= |n| <= 10**6). # Output - Returns a dictionary: - `\'binary\'`: The binary representation of `n` as a string prefixed with `\'0b\'`. - `\'bit_length\'`: An integer representing the number of bits required to represent `n` in binary format, excluding the sign and leading zeroes. - `\'bit_count\'`: An integer representing the number of `1` bits in the binary representation of `n`. # Example ```python binary_transformations(19) # Output: {\'binary\': \'0b10011\', \'bit_length\': 5, \'bit_count\': 3} binary_transformations(-37) # Output: {\'binary\': \'-0b100101\', \'bit_length\': 6, \'bit_count\': 3} ``` # Constraints and Considerations - Ensure that the returned binary representation is in the correct format with a `\'0b\'` prefix for positive integers and `\'-0b\'` for negative integers. - Utilize built-in methods where appropriate, such as `bin()`, `int.bit_length()`, and `int.bit_count()`. - The function should handle edge cases, such as zero, and large integers efficiently. # Implementation Notes - You can make use of methods like `bin`, `int.bit_length`, and `int.bit_count` to simplify your calculations.","solution":"def binary_transformations(n: int) -> dict: Returns a dictionary containing the binary representation, bit length, and bit count of the integer n. binary_representation = bin(n) bit_length = n.bit_length() bit_count = bin(abs(n)).count(\'1\') # Using abs(n) to ignore the sign for negative numbers return { \'binary\': binary_representation, \'bit_length\': bit_length, \'bit_count\': bit_count }"},{"question":"Task: You are required to create a Python application zip archive using the `zipapp` module. This archive should be executable and include a simple Python application. The steps include: 1. Creating a basic Python application with a main function. 2. Using the `zipapp.create_archive` Python API to archive this application. 3. Ensuring that the created zip archive is executable on Unix systems with the appropriate shebang line. 4. (Optional) Compress the archive if not already specified. Requirements: 1. **Python Application Structure**: - Create a directory named `myzipapp`. - Inside this directory, create a module file named `myapp.py` containing a function `main()` that prints \\"Hello, Zipapp World!\\". - Create an empty `__init__.py` file in the `myzipapp` directory to make it a package. 2. **Creating the Archive**: - Use `zipapp.create_archive()` function to create an archive from the `myzipapp` directory. - Name the output archive as `myzipapp.pyz`. - Add a shebang line to use `/usr/bin/env python` as the interpreter. - Set the `main` argument such that the created archive will run `myapp.main` when executed. 3. **Validation**: - Make sure the archive works by executing it and returning the output \\"Hello, Zipapp World!\\". - Compress the archive if it wasn\'t already specified during creation. Constraints: - Only use the `zipapp.create_archive()` function from the `zipapp` module to create the archive. - The application should be self-contained within the provided directory structure. Input: - The directory structure and files need to be created within the script. Output: - The created `myzipapp.pyz` archive should be executable and output \\"Hello, Zipapp World!\\" when run. Example Usage: ```python import zipapp # Define the source directory and the output archive name source_dir = \'myzipapp\' output_archive = \'myzipapp.pyz\' # Create the archive zipapp.create_archive(source=source_dir, target=output_archive, interpreter=\'/usr/bin/env python\', main=\'myapp:main\', compressed=True) # To run: # ./myzipapp.pyz # Output: Hello, Zipapp World! ``` Solution Template: ```python import os import zipapp # Step 1: Create the directory structure for our Python application os.makedirs(\'myzipapp\', exist_ok=True) # Step 2: Create the __init__.py file with open(\'myzipapp/__init__.py\', \'w\') as f: pass # Step 3: Create the myapp.py file with the main function myapp_content = def main(): print(\\"Hello, Zipapp World!\\") with open(\'myzipapp/myapp.py\', \'w\') as f: f.write(myapp_content) # Step 4: Use the zipapp module to create the archive output_archive = \'myzipapp.pyz\' zipapp.create_archive( source=\'myzipapp\', target=output_archive, interpreter=\'/usr/bin/env python\', main=\'myapp:main\', compressed=True ) print(f\\"Created archive: {output_archive}\\") # The script prepares the directory structure and creates the archive # To validate, run the resulting myzipapp.pyz file and check the output ``` Note: Make sure that you have write permissions in the directory where you are creating the archive and the shebang line works as intended for the Unix environment.","solution":"import os import zipapp # Step 1: Create the directory structure for our Python application os.makedirs(\'myzipapp\', exist_ok=True) # Step 2: Create the __init__.py file with open(\'myzipapp/__init__.py\', \'w\') as f: pass # Step 3: Create the myapp.py file with the main function myapp_content = def main(): print(\\"Hello, Zipapp World!\\") with open(\'myzipapp/myapp.py\', \'w\') as f: f.write(myapp_content) # Step 4: Use the zipapp module to create the archive output_archive = \'myzipapp.pyz\' zipapp.create_archive( source=\'myzipapp\', target=output_archive, interpreter=\'/usr/bin/env python\', main=\'myapp:main\', compressed=True ) print(f\\"Created archive: {output_archive}\\")"},{"question":"Objective You are tasked with creating a custom function to process text files containing MIME quoted-printable data using the `quopri` module. This function will utilize the `quopri.encode` and `quopri.decode` functionalities to convert data to and from quoted-printable format. The goal is to implement a utility that reads a text file, encodes its content to quoted-printable format, writes it to a new file, then reads this encoded file, decodes the content, and writes the decoded content back to another file. Instructions 1. Implement a function `process_files(input_path, encoded_path, decoded_path, quotetabs=True, header=False)` that performs the following: - Reads the plain text data from `input_path`. - Encodes this data using quoted-printable encoding and writes the encoded data to `encoded_path`. - Reads the quoted-printable encoded data from `encoded_path`. - Decodes this data back to the original format and writes it to `decoded_path`. Function Signature ```python import quopri def process_files(input_path: str, encoded_path: str, decoded_path: str, quotetabs: bool = True, header: bool = False) -> None: pass ``` Expected Input and Output - The input file specified by `input_path` will contain plain text. - The function should create an encoded file at `encoded_path` and a decoded file at `decoded_path`. Constraints - The function should handle files up to 10 MB. - Ensure that the decoded file has the same content as the original input file. Example Given an input file `input.txt`: ``` Hello, world! This is a test message. ``` When you encode and then decode using the `process_files` function: ```python process_files(\'input.txt\', \'encoded.txt\', \'decoded.txt\') ``` The `decoded.txt` should have the same content as `input.txt`. Notes - Pay attention to newline characters and whitespace when verifying that the decoded file matches the original input file. - Ensure the function handles binary file objects as described in the documentation.","solution":"import quopri def process_files(input_path: str, encoded_path: str, decoded_path: str, quotetabs: bool = True, header: bool = False) -> None: # Read the original data from the input file with open(input_path, \'rb\') as input_file: original_data = input_file.read() # Encode the data using quoted-printable encoding encoded_data = quopri.encodestring(original_data, quotetabs=quotetabs, header=header) # Write the encoded data to the encoded_path file with open(encoded_path, \'wb\') as encoded_file: encoded_file.write(encoded_data) # Read the encoded data back from the encoded_path file with open(encoded_path, \'rb\') as encoded_file: encoded_data_read = encoded_file.read() # Decode the data back to the original format decoded_data = quopri.decodestring(encoded_data_read, header=header) # Write the decoded data to the decoded_path file with open(decoded_path, \'wb\') as decoded_file: decoded_file.write(decoded_data)"},{"question":"You are tasked with creating a Python extension module using C that wraps several functionalities from the standard C library. The goal is to provide a fast alternative for certain operations. You will implement functions that will be callable from Python and handle memory management and error handling as described in the documentation. Objectives 1. **Function wrapping**: Implement a Python extension module with C that wraps the following functionalities: - A function to execute a system command (like `system` in the C standard library) and returns the exit status. - A function to calculate the factorial of an integer. - A function to concatenate two strings. 2. **Error Handling**: Ensure that your functions correctly handle and report errors to Python. 3. **Memory management**: Use reference counting correctly to manage the memory, preventing leaks or use-after-free errors. 4. **Module definition and initialization**: Define the methods for the module and ensure that it initializes properly. Instructions 1. **Creating the C Source File**: Create a C source file named `custommodule.c` and include the necessary headers: ```c #define PY_SSIZE_T_CLEAN #include <Python.h> #include <stdlib.h> #include <string.h> #include <errno.h> ``` 2. **Implement the Functions**: a. Implement a function `execute_system` to wrap the `system` call: ```c static PyObject* execute_system(PyObject* self, PyObject* args) { const char* command; int status; if (!PyArg_ParseTuple(args, \\"s\\", &command)) { return NULL; } status = system(command); if (status < 0) { PyErr_SetString(PyExc_RuntimeError, \\"System command failed\\"); return NULL; } return PyLong_FromLong(status); } ``` b. Implement a function `calculate_factorial` to calculate the factorial of an integer: ```c static PyObject* calculate_factorial(PyObject* self, PyObject* args) { long n, result = 1; if (!PyArg_ParseTuple(args, \\"l\\", &n)) { return NULL; } if (n < 0) { PyErr_SetString(PyExc_ValueError, \\"Factorial not defined for negative numbers\\"); return NULL; } for (long i = 1; i <= n; ++i) { result *= i; } return PyLong_FromLong(result); } ``` c. Implement a function `concat_strings` to concatenate two strings: ```c static PyObject* concat_strings(PyObject* self, PyObject* args) { const char *str1, *str2; char *result; PyObject* py_result; if (!PyArg_ParseTuple(args, \\"ss\\", &str1, &str2)) { return NULL; } result = (char*) malloc(strlen(str1) + strlen(str2) + 1); if (result == NULL) { PyErr_NoMemory(); return NULL; } strcpy(result, str1); strcat(result, str2); py_result = Py_BuildValue(\\"s\\", result); free(result); return py_result; } ``` 3. **Method Definitions and Initialization**: Define the methods and the module: ```c static PyMethodDef CustomMethods[] = { {\\"execute_system\\", execute_system, METH_VARARGS, \\"Execute a system command\\"}, {\\"calculate_factorial\\", calculate_factorial, METH_VARARGS, \\"Calculate the factorial of a number\\"}, {\\"concat_strings\\", concat_strings, METH_VARARGS, \\"Concatenate two strings\\"}, {NULL, NULL, 0, NULL} // Sentinel }; static struct PyModuleDef custommodule = { PyModuleDef_HEAD_INIT, \\"custommodule\\", \\"Example module that creates an extension using C\\", -1, CustomMethods }; PyMODINIT_FUNC PyInit_custommodule(void) { return PyModule_Create(&custommodule); } ``` 4. **Building the Module**: Write a `setup.py` script to compile and build the module: ```python from setuptools import setup, Extension module = Extension(\'custommodule\', sources=[\'custommodule.c\']) setup( name=\'custommodule\', version=\'1.0\', description=\'A custom extension module written in C\', ext_modules=[module] ) ``` Constraints 1. Use proper error handling and type checking as demonstrated in the examples. 2. Ensure that you handle memory allocation and deallocation correctly to avoid leaks. 3. Your functions should be robust and handle edge cases gracefully. Submission Submit the following: 1. The C source file (`custommodule.c`) with the implemented functions. 2. The `setup.py` script for building the module. 3. A short README file explaining how to build and run the module along with example usage of each function.","solution":"def factorial(n): Calculate the factorial of a non-negative integer n. if not isinstance(n, int): raise TypeError(\\"Input must be an integer\\") if n < 0: raise ValueError(\\"Factorial is not defined for negative numbers\\") result = 1 for i in range(2, n + 1): result *= i return result"},{"question":"# Abstract Base Class Implementation and Usage Your task is to create an abstract base class and its concrete subclasses to demonstrate understanding of the `abc` module, particularly focusing on vehicles. Requirements 1. Implement an abstract base class `Vehicle` that includes: - An abstract method `max_speed()` which returns the maximum speed of the vehicle. - An abstract method `number_of_wheels()` which returns the number of wheels on the vehicle. - An instance method `vehicle_details()` which returns a dictionary with the vehicle\'s maximum speed and number of wheels. 2. Implement two concrete subclasses `Car` and `Bike`: - The `Car` class should implement `max_speed` (returning 200) and `number_of_wheels` (returning 4). - The `Bike` class should implement `max_speed` (returning 80) and `number_of_wheels` (returning 2). 3. Implement a class `Truck` that uses a different method to determine its number of wheels: - This class should be registered as a virtual subclass of `Vehicle`. - Create a `Truck` class that has methods `max_speed` (returning 100) and `num_wheels` (returning 6). - Register `Truck` as a virtual subclass of `Vehicle` such that `issubclass(Truck, Vehicle)` returns `True` and `isinstance(Truck(), Vehicle)` returns `True`. 4. Write a function `test_vehicles()` that: - Creates instances of `Car`, `Bike`, and `Truck`. - Asserts the properties `max_speed` and `number_of_wheels` for each instance. - Verifies that all instances are recognized as `Vehicle`. Implementation ```python from abc import ABC, abstractmethod class Vehicle(ABC): @abstractmethod def max_speed(self): pass @abstractmethod def number_of_wheels(self): pass def vehicle_details(self): return { \'max_speed\': self.max_speed(), \'number_of_wheels\': self.number_of_wheels() } class Car(Vehicle): def max_speed(self): return 200 def number_of_wheels(self): return 4 class Bike(Vehicle): def max_speed(self): return 80 def number_of_wheels(self): return 2 class Truck: def max_speed(self): return 100 def num_wheels(self): return 6 Vehicle.register(Truck) def test_vehicles(): car = Car() bike = Bike() truck = Truck() assert car.max_speed() == 200 assert car.number_of_wheels() == 4 assert bike.max_speed() == 80 assert bike.number_of_wheels() == 2 assert truck.max_speed() == 100 assert truck.num_wheels() == 6 assert isinstance(car, Vehicle) assert isinstance(bike, Vehicle) assert isinstance(truck, Vehicle) # Execute test function if __name__ == \\"__main__\\": test_vehicles() print(\\"All tests passed!\\") ``` Constraints - You must use the `abc` module. - `Car` and `Bike` should directly inherit from `Vehicle`. - `Truck` should be a virtual subclass of `Vehicle` and should not inherit directly from `Vehicle`.","solution":"from abc import ABC, abstractmethod class Vehicle(ABC): @abstractmethod def max_speed(self): pass @abstractmethod def number_of_wheels(self): pass def vehicle_details(self): return { \'max_speed\': self.max_speed(), \'number_of_wheels\': self.number_of_wheels() } class Car(Vehicle): def max_speed(self): return 200 def number_of_wheels(self): return 4 class Bike(Vehicle): def max_speed(self): return 80 def number_of_wheels(self): return 2 class Truck: def max_speed(self): return 100 def number_of_wheels(self): return 6 Vehicle.register(Truck)"},{"question":"Coding Assessment Question # Objective Your task is to implement a function that simulates a mini transaction system. This will require handling various types of operations on accounts, including deposits, withdrawals, and balance checks, all while maintaining transaction history. # Requirements 1. Implement the `TransactionSystem` class with the following methods: - `deposit(account_id: int, amount: float) -> None`: Adds the `amount` to the `account_id`. - `withdraw(account_id: int, amount: float) -> bool`: Subtracts the `amount` if sufficient funds exist, otherwise return `False`. - `check_balance(account_id: int) -> float`: Returns the current balance of the `account_id`. - `get_transaction_history(account_id: int) -> list`: Returns a list of all transactions (as strings) performed on the `account_id` in the format \\"operation amount\\", where operation is \\"deposit\\" or \\"withdraw\\". 2. You should use dictionaries to store account balances and their respective transaction histories. 3. Ensure proper error handling for invalid transactions and correct balance updating. # Constraints - An account should start with a zero balance and no transaction history. - All amounts will be non-negative floats. # Example ```python ts = TransactionSystem() ts.deposit(101, 150.0) ts.deposit(101, 100.0) ts.withdraw(101, 60.0) balance = ts.check_balance(101) # Should return 190.0 history = ts.get_transaction_history(101) # Should return [\\"deposit 150.0\\", \\"deposit 100.0\\", \\"withdraw 60.0\\"] success = ts.withdraw(101, 200.0) # Should return False since insufficient funds ``` # Note - Make sure the class operates efficiently for multiple transactions across various accounts. - The order of transactions in the history list should correspond to the order in which they were performed. ```python class TransactionSystem: def __init__(self): self.accounts = {} def deposit(self, account_id: int, amount: float) -> None: if account_id not in self.accounts: self.accounts[account_id] = {\\"balance\\": 0.0, \\"history\\": []} self.accounts[account_id][\\"balance\\"] += amount self.accounts[account_id][\\"history\\"].append(f\\"deposit {amount}\\") def withdraw(self, account_id: int, amount: float) -> bool: if account_id in self.accounts and self.accounts[account_id][\\"balance\\"] >= amount: self.accounts[account_id][\\"balance\\"] -= amount self.accounts[account_id][\\"history\\"].append(f\\"withdraw {amount}\\") return True return False def check_balance(self, account_id: int) -> float: return self.accounts.get(account_id, {\\"balance\\": 0.0})[\\"balance\\"] def get_transaction_history(self, account_id: int) -> list: return self.accounts.get(account_id, {\\"history\\": []})[\\"history\\"] ``` Ensure your function passes the given examples and any additional test cases you think of.","solution":"class TransactionSystem: def __init__(self): self.accounts = {} def deposit(self, account_id: int, amount: float) -> None: if account_id not in self.accounts: self.accounts[account_id] = {\\"balance\\": 0.0, \\"history\\": []} self.accounts[account_id][\\"balance\\"] += amount self.accounts[account_id][\\"history\\"].append(f\\"deposit {amount}\\") def withdraw(self, account_id: int, amount: float) -> bool: if account_id in self.accounts and self.accounts[account_id][\\"balance\\"] >= amount: self.accounts[account_id][\\"balance\\"] -= amount self.accounts[account_id][\\"history\\"].append(f\\"withdraw {amount}\\") return True return False def check_balance(self, account_id: int) -> float: return self.accounts.get(account_id, {\\"balance\\": 0.0})[\\"balance\\"] def get_transaction_history(self, account_id: int) -> list: return self.accounts.get(account_id, {\\"history\\": []})[\\"history\\"]"},{"question":"**Objective:** Write a series of functions to demonstrate an understanding of the `linecache` module. **Tasks:** 1. Write a function `read_line_from_file(filename: str, lineno: int) -> str` that takes in a filename and a line number, and returns the specified line from the file using the `linecache.getline` function. If the line does not exist, it should return an empty string. 2. Write a function `cache_management_example(filename: str) -> Tuple[str, str, bool]` that: - Reads the 5th line from the specified file using `linecache.getline`. - Clears the cache using `linecache.clearcache`. - Reads the 5th line again to demonstrate the cache-clearing functionality. - Uses `linecache.checkcache` to verify if the cache is empty, since this function refreshes the cache status. **Input:** - `filename`: a string representing the path to a file. - `lineno`: an integer representing the line number to be read. **Output:** For `read_line_from_file`: - Returns the content of the specified line, including the terminating newline character if it exists. Returns an empty string if the line is not found. For `cache_management_example`: - Returns a tuple containing: - The content of the 5th line before clearing the cache. - The content of the 5th line after clearing the cache. - A boolean indicating whether the cache was correctly cleared (True if the cache was cleared, False otherwise). **Constraints:** - Assume the file exists and is readable. - Assume `lineno` is a positive integer. **Example:** ```python def read_line_from_file(filename: str, lineno: int) -> str: import linecache line = linecache.getline(filename, lineno) return line def cache_management_example(filename: str) -> Tuple[str, str, bool]: import linecache initial_line = linecache.getline(filename, 5) linecache.clearcache() cleared_line = linecache.getline(filename, 5) cache_valid = linecache.checkcache() is None return (initial_line, cleared_line, cache_valid) # Sample usage line = read_line_from_file(\'sample.txt\', 8) print(line) # Should print the 8th line of the file or \'\' if it doesn\'t exist print(cache_management_example(\'sample.txt\')) # Should print a tuple of (5th line content before clearing cache, 5th line content after clearing cache, True if cache is empty) ``` Make sure to test the functions with different files and line numbers to ensure comprehensive understanding and correct results.","solution":"def read_line_from_file(filename: str, lineno: int) -> str: Reads a specific line from a file using linecache. Parameters: filename (str): The path to the file. lineno (int): The line number to read. Returns: str: The content of the specified line, or an empty string if the line does not exist. import linecache return linecache.getline(filename, lineno) def cache_management_example(filename: str) -> tuple: Demonstrates the functionality of linecache\'s cache management. Parameters: filename (str): The path to the file. Returns: tuple: A tuple containing: - The content of the 5th line before clearing the cache. - The content of the 5th line after clearing the cache. - A boolean indicating whether the cache was correctly cleared. import linecache # Read the 5th line from the file initial_line = linecache.getline(filename, 5) # Clear the linecache linecache.clearcache() # Read the 5th line again to demonstrate that the cache has been cleared and refreshed cleared_line = linecache.getline(filename, 5) # Check the status of the cache linecache.checkcache() # Verify if the cache is empty, by checking if the line content before and after is the same cache_cleared = (initial_line != cleared_line) return (initial_line, cleared_line, cache_cleared)"},{"question":"**Question:** You are required to implement a function that introspects the current execution frame and gathers detailed information about the currently executing function, including its name, description, local and global variables, current line number, and the built-in functions accessible within the function. To achieve this, you will need to use the functions described in the provided documentation. **Function Signature:** ```python def gather_execution_info() -> dict: pass ``` **Expected Input and Output:** - The function does not take any input arguments. - The function returns a dictionary with the following structure: ```python { \\"function_name\\": str, \\"function_description\\": str, \\"line_number\\": int, \\"locals\\": dict, \\"globals\\": dict, \\"builtins\\": dict } ``` **Constraints:** - The function should be able to correctly introspect the frame regardless of the context in which `gather_execution_info()` is called. - Use the provided functions from the documentation to implement your solution. **Example:** Assume the following context: ```python def example_function(): x = 42 y = \\"example\\" info = gather_execution_info() print(info) example_function() ``` The expected output (the actual values may vary): ```python { \\"function_name\\": \\"example_function\\", \\"function_description\\": \\"()\\", \\"line_number\\": 3, \\"locals\\": {\\"x\\": 42, \\"y\\": \\"example\\"}, \\"globals\\": {...}, # Global variables in the module should be listed here \\"builtins\\": {...} # Built-in functions should be listed here } ``` Implement the function `gather_execution_info` to introspect the current execution frame as specified. **Note:** - The environment should support Python 3.10 or higher to ensure compatibility with the functions mentioned in the provided documentation.","solution":"import inspect def gather_execution_info() -> dict: Gathers detailed information about the currently executing function. frame = inspect.currentframe().f_back # Step back to the caller\'s frame # Retrieving the function name, description, current line number, locals, globals, and builtins function_name = frame.f_code.co_name function_description = inspect.getdoc(frame.f_globals[function_name]) line_number = frame.f_lineno locals_info = frame.f_locals globals_info = frame.f_globals builtins_info = frame.f_builtins return { \\"function_name\\": function_name, \\"function_description\\": function_description, \\"line_number\\": line_number, \\"locals\\": locals_info, \\"globals\\": globals_info, \\"builtins\\": builtins_info }"},{"question":"# Custom JSON Encoder and Decoder **Problem Statement:** You are tasked with creating a custom JSON encoder and decoder to handle special Python data structures that the default encoder and decoder can\'t handle directly. Specifically, you need to extend the JSON encoder and decoder to support complex numbers and datetime objects. # Requirements: 1. **Complex Numbers:** - A complex number should be encoded as a dictionary with the keys \\"real\\" and \\"imag\\". - For instance, `3 + 4j` should be encoded as `{\\"real\\": 3, \\"imag\\": 4}`. 2. **Datetime Objects:** - A datetime object should be encoded as a dictionary with the keys \\"year\\", \\"month\\", \\"day\\", \\"hour\\", \\"minute\\", \\"second\\". - For instance, `datetime.datetime(2023, 10, 25, 14, 30, 45)` should be encoded as `{\\"year\\": 2023, \\"month\\": 10, \\"day\\": 25, \\"hour\\": 14, \\"minute\\": 30, \\"second\\": 45}`. You need to write: 1. A custom encoder `CustomJSONEncoder` that extends `json.JSONEncoder`. 2. A custom decoder `custom_json_decoder` that can decode strings produced by your custom encoder. 3. Functions `custom_dumps` and `custom_loads` that use your custom encoder and decoder respectively to serialize and deserialize data. # Constraints: - You should only use the standard library for this task. - Your `custom_json_decoder` should be able to handle both complex numbers and datetime objects properly when they are presented in encoded JSON form. # Example Usage: ```python import datetime import json from custom_json import custom_dumps, custom_loads # Example data data = { \\"number\\": 42, \\"complex\\": 3 + 4j, \\"timestamp\\": datetime.datetime(2023, 10, 25, 14, 30, 45) } # Encode data to JSON encoded_json = custom_dumps(data) print(encoded_json) # Output should be a JSON string with complex and datetime objects properly encoded. # Decode JSON back to Python object decoded_data = custom_loads(encoded_json) print(decoded_data) # Output should be a dictionary with the complex number as a complex type # and datetime object as a datetime type. ``` # Implementation: ```python import json from datetime import datetime class CustomJSONEncoder(json.JSONEncoder): def default(self, obj): if isinstance(obj, complex): return {\\"real\\": obj.real, \\"imag\\": obj.imag} if isinstance(obj, datetime): return { \\"year\\": obj.year, \\"month\\": obj.month, \\"day\\": obj.day, \\"hour\\": obj.hour, \\"minute\\": obj.minute, \\"second\\": obj.second } return super().default(obj) def custom_json_decoder(dct): if \\"real\\" in dct and \\"imag\\" in dct: return complex(dct[\\"real\\"], dct[\\"imag\\"]) if all(k in dct for k in (\\"year\\", \\"month\\", \\"day\\", \\"hour\\", \\"minute\\", \\"second\\")): return datetime(dct[\\"year\\"], dct[\\"month\\"], dct[\\"day\\"], dct[\\"hour\\"], dct[\\"minute\\"], dct[\\"second\\"]) return dct def custom_dumps(obj, **kwargs): return json.dumps(obj, cls=CustomJSONEncoder, **kwargs) def custom_loads(s, **kwargs): return json.loads(s, object_hook=custom_json_decoder, **kwargs) ``` Ensure your implementation passes the example provided and handles edge cases such as encoding and decoding data that does not match the special types.","solution":"import json from datetime import datetime class CustomJSONEncoder(json.JSONEncoder): def default(self, obj): if isinstance(obj, complex): return {\\"real\\": obj.real, \\"imag\\": obj.imag} if isinstance(obj, datetime): return { \\"year\\": obj.year, \\"month\\": obj.month, \\"day\\": obj.day, \\"hour\\": obj.hour, \\"minute\\": obj.minute, \\"second\\": obj.second } return super().default(obj) def custom_json_decoder(dct): if \\"real\\" in dct and \\"imag\\" in dct: return complex(dct[\\"real\\"], dct[\\"imag\\"]) if all(k in dct for k in (\\"year\\", \\"month\\", \\"day\\", \\"hour\\", \\"minute\\", \\"second\\")): return datetime(dct[\\"year\\"], dct[\\"month\\"], dct[\\"day\\"], dct[\\"hour\\"], dct[\\"minute\\"], dct[\\"second\\"]) return dct def custom_dumps(obj, **kwargs): return json.dumps(obj, cls=CustomJSONEncoder, **kwargs) def custom_loads(s, **kwargs): return json.loads(s, object_hook=custom_json_decoder, **kwargs)"},{"question":"**Objective:** You are provided with a code snippet that imports a module using the deprecated `imp` module in Python, but we want to modernize this code by using `importlib` instead. Your task is to re-implement the given function to ensure it conforms with the latest Python standards. **Code Snippet:** ```python import imp import sys def import_module(module_name): Tries to import a module using the imp module. If the module is already cached in sys.modules, it returns the cached module; otherwise, it loads the module from the file system. Args: module_name (str): The name of the module to import. Returns: module: The imported module object. try: # Check if the module is already imported if module_name in sys.modules: return sys.modules[module_name] # Find and load the module fp, pathname, description = imp.find_module(module_name) try: return imp.load_module(module_name, fp, pathname, description) finally: # Close the file if it was opened if fp: fp.close() except ImportError as e: print(f\\"Error importing module {module_name}: {e}\\") return None ``` **Task:** Re-implement the `import_module` function using `importlib` instead of `imp`. The function should behave identically to the original, meaning it should return the module if it is already imported, find and load the module from the file system if not, and handle any import errors gracefully. **Constraints:** - You must use `importlib` and its utilities to implement this. - Do not use the deprecated `imp` module. - The function should maintain the same functionality and error handling. **Expected Function Signature:** ```python import importlib.util import sys def import_module(module_name: str): # Your code here ``` **Performance Requirements:** - The solution should be efficient and handle common edge cases such as module already imported, module not found, etc., gracefully. **Example Usage:** ```python # Assuming there exists a module named \'example_module.py\' in the current directory mod = import_module(\'example_module\') if mod: print(f\\"Module {mod.__name__} imported successfully.\\") else: print(\\"Module could not be imported.\\") ``` **Submit your implementation for review.**","solution":"import importlib.util import sys def import_module(module_name: str): Tries to import a module using importlib. If the module is already cached in sys.modules, it returns the cached module; otherwise, it loads the module from the file system. Args: module_name (str): The name of the module to import. Returns: module: The imported module object. try: # Check if the module is already imported if module_name in sys.modules: return sys.modules[module_name] # Try to find the spec for the module spec = importlib.util.find_spec(module_name) if spec is None: raise ImportError(f\\"Module {module_name} not found\\") # Create a new module based on the spec and execute it module = importlib.util.module_from_spec(spec) spec.loader.exec_module(module) # Cache the module in sys.modules sys.modules[module_name] = module return module except ImportError as e: print(f\\"Error importing module {module_name}: {e}\\") return None"},{"question":"# Python Coding Assessment Question: Creating and Updating Python Executable Archives You are tasked with managing Python executable archives using the `zipapp` module. Your job is to write functions that create an archive from a directory and update the interpreter of an existing archive. Function 1: `create_executable_archive` Write a function `create_executable_archive(source_dir: str, target_file: str, interpreter: str, main_func: str, compress: bool = False) -> None` that creates a Python executable archive from the source directory. **Input:** - `source_dir` (str): The directory containing the Python code to be archived. - `target_file` (str): The desired output file for the executable archive. - `interpreter` (str): The path to the Python interpreter that should be specified in the `#!` line. - `main_func` (str): The main function to be run when the archive is executed. - `compress` (bool): Whether to compress the archive contents or not (default is `False`). **Output:** - The function should not return anything, but it should create the executable archive at the specified `target_file`. Function 2: `update_executable_interpreter` Write a function `update_executable_interpreter(existing_archive: str, new_archive: str, new_interpreter: str) -> None` that updates the interpreter of an existing Python executable archive. **Input:** - `existing_archive` (str): The file name of the existing Python executable archive. - `new_archive` (str): The desired output file for the updated executable archive. - `new_interpreter` (str): The new Python interpreter path that should be specified in the `#!` line. **Output:** - The function should not return anything, but it should create the updated executable archive at the specified `new_archive`. **Constraints:** - The `existing_archive` should be a valid `.pyz` file. - The `new_archive` should not overwrite the `existing_archive`. **Examples:** ```python # Assuming the directory \'myapp\' contains the necessary Python files create_executable_archive(\'myapp\', \'myapp.pyz\', \'/usr/bin/env python3\', \'myapp:main\', compress=True) # Update the interpreter of the existing archive update_executable_interpreter(\'myapp.pyz\', \'myapp_updated.pyz\', \'/usr/bin/python3.9\') ``` **Notes:** - Use the `zipapp.create_archive()` for creating the executable archive. - Use the `zipapp.create_archive()` to update the interpreter by copying the existing archive and setting the new interpreter. Your implementation should ensure the integrity of the archives and handle any exceptions that might occur during the file operations. Good luck!","solution":"import zipapp def create_executable_archive(source_dir: str, target_file: str, interpreter: str, main_func: str, compress: bool = False) -> None: Creates a Python executable archive from the source directory. Args: - source_dir (str): The directory containing the Python code to be archived. - target_file (str): The desired output file for the executable archive. - interpreter (str): The path to the Python interpreter to be specified in the `#!` line. - main_func (str): The main function to be run when the archive is executed. - compress (bool): Whether to compress the archive contents or not. Returns: None, but creates the archive at specified target_file. zipapp.create_archive(source_dir, target=target_file, interpreter=interpreter, main=main_func, compressed=compress) def update_executable_interpreter(existing_archive: str, new_archive: str, new_interpreter: str) -> None: Updates the interpreter of an existing Python executable archive. Args: - existing_archive (str): The file name of the existing Python executable archive. - new_archive (str): The desired output file for the updated executable archive. - new_interpreter (str): The new Python interpreter path to be specified in the `#!` line. Returns: None, but creates the updated archive at specified new_archive. import shutil shutil.copy(existing_archive, new_archive) with open(new_archive, \'r+b\') as f: lines = f.readlines() if lines[0].startswith(b\'#!\'): lines[0] = f\'#!{new_interpreter}n\'.encode() f.seek(0) f.writelines(lines)"},{"question":"**Problem Statement:** A data processing pipeline requires handling various types of data, including custom Python objects. Your task is to implement a function that should: 1. Encode various types of Python objects including complex numbers, and custom class instances into a JSON formatted string. 2. Decode the JSON string back to its original Python objects, ensuring the integrity and type of the data. # Requirements: 1. Implement a class `CustomJSONHandler` with the following: - A method `to_json(obj: Any) -> str` to encode the provided Python object to a JSON string. - A method `from_json(json_str: str) -> Any` to decode the JSON string back to a Python object. # Specifications: - The `CustomJSONHandler` should support the following types: - Basic types: `int`, `float`, `str`, `list`, `dict`. - `complex` numbers. - Custom class instances with the class name `CustomClass` having attributes `name` (str) and `value` (int). - For custom classes, the JSON representation should include the class name as a key for proper decoding. # Input: - `to_json(obj: Any)`: A Python object of any supported type. - `from_json(json_str: str)`: A JSON string representing a Python object. # Output: - `to_json(obj: Any)`: JSON formatted string. - `from_json(json_str: str)`: Original Python object. # Constraints: - Ensure that the encoded JSON string is compact. - Ensure that the decoding process is efficient and correctly identifies the types. # Example: ```python # Example for a Custom class instance class CustomClass: def __init__(self, name: str, value: int): self.name = name self.value = value handler = CustomJSONHandler() # Encoding a complex number json_str = handler.to_json(3 + 4j) print(json_str) # Should output: \\"[3.0, 4.0]\\" # Decoding a complex number obj = handler.from_json(json_str) print(obj) # Should output: (3+4j) # Encoding a custom class instance custom_obj = CustomClass(\\"example\\", 42) json_str = handler.to_json(custom_obj) print(json_str) # Should output: \'{\\"__CustomClass__\\": true, \\"name\\": \\"example\\", \\"value\\": 42}\' # Decoding a custom class instance obj = handler.from_json(json_str) print(obj) # Should output an instance of CustomClass with name=\\"example\\" and value=42 ``` # Implementation Notes: - Utilize subclassing of `json.JSONEncoder` and `json.JSONDecoder` for handling the custom serializations. - Implement adequate checks and conversions to ensure all specified types are correctly encoded and decoded. ```python import json class CustomClass: def __init__(self, name: str, value: int): self.name = name self.value = value def __eq__(self, other): return isinstance(other, CustomClass) and self.name == other.name and self.value == other.value class CustomJSONEncoder(json.JSONEncoder): def default(self, obj): if isinstance(obj, complex): return [obj.real, obj.imag] elif isinstance(obj, CustomClass): return { \\"__CustomClass__\\": True, \\"name\\": obj.name, \\"value\\": obj.value } # Call the base class method for other types return super().default(obj) def custom_decoder(dct): if \\"__CustomClass__\\" in dct: return CustomClass(dct[\\"name\\"], dct[\\"value\\"]) return dct class CustomJSONHandler: def to_json(self, obj: Any) -> str: return json.dumps(obj, cls=CustomJSONEncoder, separators=(\',\', \':\')) def from_json(self, json_str: str) -> Any: return json.loads(json_str, object_hook=custom_decoder) ```","solution":"import json from typing import Any class CustomClass: def __init__(self, name: str, value: int): self.name = name self.value = value def __eq__(self, other): return isinstance(other, CustomClass) and self.name == other.name and self.value == other.value class CustomJSONEncoder(json.JSONEncoder): def default(self, obj): if isinstance(obj, complex): return {\\"__complex__\\": True, \\"real\\": obj.real, \\"imag\\": obj.imag} elif isinstance(obj, CustomClass): return { \\"__CustomClass__\\": True, \\"name\\": obj.name, \\"value\\": obj.value } return super().default(obj) def custom_decoder(dct): if \\"__complex__\\" in dct: return complex(dct[\\"real\\"], dct[\\"imag\\"]) if \\"__CustomClass__\\" in dct: return CustomClass(dct[\\"name\\"], dct[\\"value\\"]) return dct class CustomJSONHandler: def to_json(self, obj: Any) -> str: return json.dumps(obj, cls=CustomJSONEncoder, separators=(\',\', \':\')) def from_json(self, json_str: str) -> Any: return json.loads(json_str, object_hook=custom_decoder)"},{"question":"**Objective:** Implement a Python function that allows users to execute Python code contained in a string while specifying the execution context through global and local variables. This will demonstrate an understanding of managing execution environments using the Python C API. Task You are required to write a Python function called `execute_python_code` that takes the following parameters: - `code_str`: a string containing valid Python source code. - `globals_dict`: a dictionary representing the global variables to be used during the execution of the code. - `locals_dict`: a dictionary representing the local variables to be used during the execution of the code. The function should execute the provided Python code within the context of the provided global and local variables and return the result as a Python object. Input - `code_str`: str (e.g., `\\"print(\'Hello, World!\')\\"` or `\\"a + b\\"` where `a` and `b` are in the provided dictionaries) - `globals_dict`: dict - `locals_dict`: dict Output - The result of the code execution (can be of any type depending on the executed code). Constraints 1. If the code raises an exception during execution, the function should catch it and return a string message: \\"Exception caught: <error_message>\\". 2. Ensure that the global and local variable dictionaries are appropriately managed and isolated to avoid side effects. Example ```python # Example usage globals_dict = {\\"a\\": 10} locals_dict = {\\"b\\": 20} result = execute_python_code(\\"a + b\\", globals_dict, locals_dict) # Expected output: 30 print(result) ``` Additional Notes - Use `exec()`, `eval()`, or the `compile()` function in Python to perform the execution. - Maintain separation between global and local variables. - Ensure that the function handles any exceptions raised during code execution and returns the appropriate message. Performance Requirements - The function should perform efficiently for reasonably short strings of code and small dictionaries of variables. - Exception handling must not significantly degrade performance. **Hint:** You might find the `exec()` or `eval()` functions useful to dynamically execute Python code within the specified context.","solution":"def execute_python_code(code_str, globals_dict, locals_dict): Executes Python code contained in a string within provided global and local contexts. Parameters: - code_str: str, the Python source code to execute. - globals_dict: dict, the global variables to be used during execution. - locals_dict: dict, the local variables to be used during execution. Returns: - The result of the code execution or an error message in case of an exception. try: # Ensure globals_dict has access to built-ins exec_globals = globals_dict.copy() exec_globals.update({\'__builtins__\': __builtins__}) # Execute the code and fetch the locals dictionary updates exec(code_str, exec_globals, locals_dict) # Fetch the result from locals if available # Use a specific variable to store result if expecting explicit return return locals_dict except Exception as e: return f\\"Exception caught: {str(e)}\\" # Example usage globals_dict = {\\"a\\": 10} locals_dict = {\\"b\\": 20} result = execute_python_code(\\"c = a + b\\", globals_dict, locals_dict) print(result[\\"c\\"]) # Expected output: 30"},{"question":"Task You are provided with a dataset consisting of multiple numerical features and a categorical class label. Your task is to write a Python function that: 1. Reads the dataset from a CSV file. 2. Performs basic data preprocessing to handle any missing values. 3. Generates and saves different types of plots to visualize the data using functions from the `pandas.plotting` module. Function Signature ```python def analyze_and_plot_data(file_path: str, output_dir: str) -> None: pass ``` Input * `file_path` (str): The path to the input CSV file containing the dataset. * `output_dir` (str): The directory where the generated plots should be saved. Output * None. The function should save the plots directly into the specified `output_dir`. Detailed Requirements 1. Load the dataset using pandas. 2. Handle missing values by filling them with the mean of their respective columns. 3. Use the following plotting functions from `pandas.plotting` to create visualizations: * `scatter_matrix`: Create a scatter matrix plot to visualize the relationships between numerical features. Save it as `scatter_matrix.png`. * `andrews_curves`: Generate Andrews curves to visualize clusters of multivariate data according to the class labels. Save it as `andrews_curves.png`. * `parallel_coordinates`: Plot parallel coordinates to show each feature as a vertical axis, and each data instance as a line connecting these axes according to the class labels. Save it as `parallel_coordinates.png`. * `autocorrelation_plot`: Create and save an autocorrelation plot for a specified column, for example, the first numerical feature. Save it as `autocorrelation_plot.png`. 4. Ensure all plots are saved in the specified `output_dir`. Constraints * Assume the input CSV file has a header row. * The last column in the CSV file is the categorical class label. Below is a template to get you started: ```python import pandas as pd from pandas.plotting import scatter_matrix, andrews_curves, parallel_coordinates, autocorrelation_plot import os def analyze_and_plot_data(file_path: str, output_dir: str) -> None: # Step 1: Load dataset # Step 2: Handle missing values # Step 3: Generate plots # Ensure the output directory exists if not os.path.exists(output_dir): os.makedirs(output_dir) # Save scatter matrix plot scatter_matrix(df_numerical) plt.savefig(os.path.join(output_dir, \\"scatter_matrix.png\\")) # Save Andrews Curves plot andrews_curves(df, class_column) plt.savefig(os.path.join(output_dir, \\"andrews_curves.png\\")) # Save Parallel Coordinates plot parallel_coordinates(df, class_column) plt.savefig(os.path.join(output_dir, \\"parallel_coordinates.png\\")) # Save Autocorrelation plot autocorrelation_plot(df[first_numerical_column]) plt.savefig(os.path.join(output_dir, \\"autocorrelation_plot.png\\")) # Close all plots plt.close(\'all\') ``` Ensure to periodically test your function with different datasets to validate correctness and robustness.","solution":"import pandas as pd from pandas.plotting import scatter_matrix, andrews_curves, parallel_coordinates, autocorrelation_plot import os import matplotlib.pyplot as plt def analyze_and_plot_data(file_path: str, output_dir: str) -> None: # Step 1: Load dataset df = pd.read_csv(file_path) # Step 2: Handle missing values df.fillna(df.mean(numeric_only=True), inplace=True) # Extract the class column name (assumes it\'s the last column in the dataset) class_column = df.columns[-1] # Extracting numerical columns for scatter matrix df_numerical = df.select_dtypes(include=[float, int]) # Extract the first numerical column for autocorrelation plot first_numerical_column = df_numerical.columns[0] # Ensure the output directory exists if not os.path.exists(output_dir): os.makedirs(output_dir) # Step 3: Generate plots # Save scatter matrix plot scatter_matrix_fig = scatter_matrix(df_numerical, diagonal=\'kde\', figsize=(10, 10)) plt.savefig(os.path.join(output_dir, \\"scatter_matrix.png\\")) plt.close(\'all\') # Save Andrews Curves plot plt.figure() andrews_curves(df, class_column) plt.savefig(os.path.join(output_dir, \\"andrews_curves.png\\")) plt.close(\'all\') # Save Parallel Coordinates plot plt.figure() parallel_coordinates(df, class_column) plt.savefig(os.path.join(output_dir, \\"parallel_coordinates.png\\")) plt.close(\'all\') # Save Autocorrelation plot plt.figure() autocorrelation_plot(df[first_numerical_column]) plt.savefig(os.path.join(output_dir, \\"autocorrelation_plot.png\\")) plt.close(\'all\')"},{"question":"You are provided with three utility functions as part of PyTorch\'s Numeric Suite for FX: 1. `compute_sqnr(x, y)`: Computes the Signal to Quantization Noise Ratio (SQNR) between two tensors `x` and `y`. 2. `compute_normalized_l2_error(x, y)`: Computes the normalized L2 error between two tensors `x` and `y`. 3. `compute_cosine_similarity(x, y)`: Computes the cosine similarity between two tensors `x` and `y`. Your task is to implement a function that evaluates the performance of a quantized neural network model against its original version using these three metrics: SQNR, normalized L2 error, and cosine similarity. Function Signature ```python def evaluate_model_performance(original_model, quantized_model, input_tensor): Evaluates the performance of the quantized model against the original model using SQNR, normalized L2 error, and cosine similarity. Parameters: original_model (torch.nn.Module): The original neural network model. quantized_model (torch.nn.Module): The quantized version of the neural network model. input_tensor (torch.Tensor): A tensor that will be used as input to both models. Returns: dict: A dictionary containing the three metrics: { \\"sqnr\\": float, \\"normalized_l2_error\\": float, \\"cosine_similarity\\": float } pass ``` Input - `original_model`: A torch.nn.Module object representing the original neural network model. - `quantized_model`: A torch.nn.Module object representing the quantized version of the neural network model. - `input_tensor`: A torch.Tensor object representing the input data for the models. Output A dictionary containing the computed metrics: - `\\"sqnr\\"`: Float value representing the Signal to Quantization Noise Ratio between the outputs of the original and quantized models. - `\\"normalized_l2_error\\"`: Float value representing the normalized L2 error between the outputs of the original and quantized models. - `\\"cosine_similarity\\"`: Float value representing the cosine similarity between the outputs of the original and quantized models. Constraints - Ensure that the function utilizes the provided utility methods correctly. - The input tensor must be fed through both models in evaluation mode. - The models may have multiple outputs; ensure you handle such cases appropriately. - Assume that the input tensor is compatible with both models. Example ```python # Assume we already have defined an original model and its quantized version import torch # Example input tensor input_tensor = torch.randn(1, 3, 224, 224) # Evaluate performance metrics = evaluate_model_performance(original_model, quantized_model, input_tensor) print(metrics) # Output # { # \\"sqnr\\": <value>, # \\"normalized_l2_error\\": <value>, # \\"cosine_similarity\\": <value> # } ``` Note: You do not need to implement the `compute_sqnr`, `compute_normalized_l2_error`, and `compute_cosine_similarity` functions, as they are provided. Focus on correctly using them within your `evaluate_model_performance` function.","solution":"import torch def evaluate_model_performance(original_model, quantized_model, input_tensor): Evaluates the performance of the quantized model against the original model using SQNR, normalized L2 error, and cosine similarity. Parameters: original_model (torch.nn.Module): The original neural network model. quantized_model (torch.nn.Module): The quantized version of the neural network model. input_tensor (torch.Tensor): A tensor that will be used as input to both models. Returns: dict: A dictionary containing the three metrics: { \\"sqnr\\": float, \\"normalized_l2_error\\": float, \\"cosine_similarity\\": float } from torch.ao.ns.fx.utils import compute_sqnr, compute_normalized_l2_error, compute_cosine_similarity # Set both models to evaluation mode original_model.eval() quantized_model.eval() with torch.no_grad(): # Forward pass through both models original_output = original_model(input_tensor) quantized_output = quantized_model(input_tensor) # Assume outputs are tensors and not tuples/lists of tensors sqnr_value = compute_sqnr(original_output, quantized_output) normalized_l2_error_value = compute_normalized_l2_error(original_output, quantized_output) cosine_similarity_value = compute_cosine_similarity(original_output, quantized_output) return { \\"sqnr\\": sqnr_value, \\"normalized_l2_error\\": normalized_l2_error_value, \\"cosine_similarity\\": cosine_similarity_value }"},{"question":"# Email Sender with Attachments Objective Design a Python function that sends an email with multiple types of attachments (e.g., images, text files, and PDFs) to a specified list of recipients. Function Signature ```python def send_email_with_attachments(sender_email: str, sender_password: str, recipients: list, subject: str, body: str, attachments: list) -> None: pass ``` Description Implement the function `send_email_with_attachments` that takes the following parameters: - **sender_email** (str): The sender\'s email address. - **sender_password** (str): The sender\'s email password (for authentication). - **recipients** (list of str): A list of recipient email addresses. - **subject** (str): The subject of the email. - **body** (str): The plain text body of the email. - **attachments** (list of str): A list of file paths to the attachments to be included in the email. The function should authenticate with an SMTP server (e.g., Gmail\'s SMTP server) and send an email with the specified attachments. The function should correctly handle different MIME types based on the file extensions of the attachments. Constraints - Use SMTP server \'smtp.gmail.com\' and port 587 for sending the email. - Ensure that each attachment is correctly identified and attached with its appropriate MIME type. - Handle the login process securely and appropriately manage failed login attempts. - Assume all recipients have valid email addresses and that the sender\'s email address is using a Gmail account. Example Usage ```python send_email_with_attachments( sender_email=\\"example@gmail.com\\", sender_password=\\"password123\\", recipients=[\\"recipient1@example.com\\", \\"recipient2@example.com\\"], subject=\\"Monthly Report\\", body=\\"Please find the attached monthly report and images.\\", attachments=[\\"report.pdf\\", \\"image1.png\\", \\"note.txt\\"] ) ``` In this example, the function should send an email with the subject \\"Monthly Report\\" to the specified recipients, containing a plain text body and attachments: a PDF file, a PNG image, and a text file. Notes - You may need to enable \\"less secure app access\\" in your Gmail account settings to use the SMTP server for sending emails. - Consider edge cases such as providing no attachments or an invalid file path in the attachments list. - Ensure that the email headers are set correctly, and that the email content is properly formatted for both plain text and recipients\' email clients.","solution":"import smtplib from email.message import EmailMessage import mimetypes import os def send_email_with_attachments(sender_email: str, sender_password: str, recipients: list, subject: str, body: str, attachments: list) -> None: msg = EmailMessage() msg[\'From\'] = sender_email msg[\'To\'] = \', \'.join(recipients) msg[\'Subject\'] = subject msg.set_content(body) for file_path in attachments: if os.path.isfile(file_path): mime_type, _ = mimetypes.guess_type(file_path) if mime_type is None: mime_type = \'application/octet-stream\' main_type, sub_type = mime_type.split(\'/\', 1) with open(file_path, \'rb\') as file: msg.add_attachment(file.read(), maintype=main_type, subtype=sub_type, filename=os.path.basename(file_path)) else: raise ValueError(f\\"Attachment file {file_path} not found.\\") try: with smtplib.SMTP(\'smtp.gmail.com\', 587) as smtp: smtp.starttls() smtp.login(sender_email, sender_password) smtp.send_message(msg) except smtplib.SMTPException as e: print(f\\"Failed to send email: {e}\\")"},{"question":"# Coding Challenge: Advanced HTTP Client Implementation You are tasked with creating an advanced HTTP client utility using Python\'s `http.client` module. This utility will perform multiple operations, establish secure connections, handle custom headers, and manage different HTTP methods. # Objective Implement a class `AdvancedHTTPClient` that provides methods to: - Establish HTTP and HTTPS connections. - Send GET, POST, PUT, and DELETE requests. - Handle custom headers and URL parameters. - Retrieve and parse response headers and bodies. - Manage exceptions related to HTTP operations. # Requirements 1. **Class Definition:** ```python class AdvancedHTTPClient: def __init__(self, host, port=None, is_secure=False): # Initialize the connection (HTTP or HTTPS based on is_secure) pass def make_request(self, method, url, body=None, headers={}, params={}): # Send HTTP request (method: GET, POST, PUT, DELETE) # Handle URL parameters # Return response status, headers, and body pass def close(self): # Close the connection pass ``` 2. **Initialization:** - `__init__(self, host, port=None, is_secure=False)`: Initialize an `HTTPConnection` or `HTTPSConnection` based on the `is_secure` flag. Default port should be `80` for HTTP and `443` for HTTPS if not provided. 3. **Method - `make_request`:** - `make_request(self, method, url, body=None, headers={}, params={})`: Sends an HTTP request using the specified method (`GET`, `POST`, `PUT`, `DELETE`). It should handle additional URL parameters and headers. If the method expects a body (`POST`, `PUT`), the body should be included in the request. - The method should return a tuple `(status, response_headers, response_body)` corresponding to the HTTP status code, response headers, and response body. 4. **Closing the connection:** - `close(self)`: Closes the HTTP connection gracefully. 5. **Error Handling:** - The class should handle and raise appropriate exceptions for connection errors, invalid URLs, and HTTP protocol errors. # Constraints - You must use the `http.client` module for all HTTP operations. - The client should support both secure (HTTPS) and non-secure (HTTP) connections. - Ensure proper handling of character encoding for the request and response bodies. - Implement appropriate exception handling for network and protocol errors. # Example Usage ```python # Initialize a secure HTTPS client client = AdvancedHTTPClient(\\"www.python.org\\", is_secure=True) # Make a GET request status, headers, body = client.make_request(\\"GET\\", \\"/\\") print(status) print(headers) print(body[:200]) # Print first 200 characters of the body # Make a POST request with custom headers and body headers = {\\"Content-type\\": \\"application/x-www-form-urlencoded\\"} body = \\"param1=value1&param2=value2\\" status, headers, body = client.make_request(\\"POST\\", \\"/submit\\", body=body, headers=headers) print(status) print(headers) print(body) # Close the connection client.close() ``` # Note: - The utility should be robust and handle different edge cases related to HTTP connections (e.g., timeouts, redirects, etc.). - You are encouraged to refer to the provided documentation on the `http.client` module for detailed information on the methods and classes required for the implementation. Implement the `AdvancedHTTPClient` class to complete this challenge.","solution":"import http.client from urllib.parse import urlparse, urlencode class AdvancedHTTPClient: def __init__(self, host, port=None, is_secure=False): self.host = host self.port = port self.is_secure = is_secure if self.is_secure: self.conn = http.client.HTTPSConnection(host, port if port else 443) else: self.conn = http.client.HTTPConnection(host, port if port else 80) def make_request(self, method, url, body=None, headers={}, params={}): # Add URL parameters if any if params: url += \'?\' + urlencode(params) # Make an HTTP request self.conn.request(method, url, body, headers) # Get the response response = self.conn.getresponse() # Read the response body response_body = response.read().decode(\'utf-8\') # Get the response headers response_headers = dict(response.getheaders()) # Return status, headers, and body return response.status, response_headers, response_body def close(self): self.conn.close()"},{"question":"# Advanced Python Compilation with Future Statements **Objective** Your task is to implement a class that utilizes the functionalities provided by the `codeop` module to compile Python code, taking into account any future statements for subsequent compilations. **Requirements** - **Class Definition**: - `CustomCompiler`: - **Method 1**: `compile_code(source: str, filename: Optional[str] = \'<input>\', symbol: Optional[str] = \'single\') -> Any` - **Method 2**: `compile_future_code(source: str, filename: Optional[str] = \'<input>\', symbol: Optional[str] = \'single\') -> Any` - **Methods**: - `compile_code`: Compiles the given source code without considering future statements. It should use `codeop.compile_command`. - `compile_future_code`: Compiles the given source code while remembering any `__future__` statements for subsequent code compilations. It should use `codeop.CommandCompiler`. - **Return Values**: - Both methods should return a code object if the source is valid Python code or `None` if it is not (but possibly a prefix to valid code). They should also handle and propagate any relevant exceptions. **Input and Output** - Input: - `source`: A string containing Python code. - `filename`: An optional string representing the filename attribute of the code object (default is `\'<input>\'`). - `symbol`: An optional string indicating the code context — `\'single\'`, `\'exec\'`, or `\'eval\'` (default is `\'single\'`). - Output: - Returns a compiled code object if the source code is valid and an exception for invalid code. # Example ```python compiler = CustomCompiler() # Example 1: Valid code without future statements print(compiler.compile_code(\'a = 1\')) # Outputs: <code object <module> at 0x..., file \\"<input>\\", line 1> # Example 2: Syntax error try: compiler.compile_code(\'a = \') except SyntaxError as e: print(e) # Outputs: invalid syntax (<input>, line 1) # Example 3: Valid future code print(compiler.compile_future_code(\'from __future__ import division; a = 1/2\')) # Outputs: <code object <module> at 0x..., file \\"<input>\\", line 1> # Example 4: Subsequent code using future statement print(compiler.compile_future_code(\'b = 2/3\')) # <code object <module> at 0x..., file \\"<input>\\", line 1> ``` **Constraints** - Each input code should be a valid Python string. - Your implementation should properly handle exceptions and make use of the functionalities provided by the `codeop` module. # Notes - Ensure to handle different symbols (`\'single\'`, `\'exec\'`, `\'eval\'`) appropriately. - Provide appropriate error handling for invalid source code.","solution":"import codeop from typing import Optional, Any class CustomCompiler: def __init__(self): self.command_compiler = codeop.CommandCompiler() def compile_code(self, source: str, filename: Optional[str] = \'<input>\', symbol: Optional[str] = \'single\') -> Any: try: compiled_code = codeop.compile_command(source, filename, symbol) return compiled_code except Exception as e: raise e def compile_future_code(self, source: str, filename: Optional[str] = \'<input>\', symbol: Optional[str] = \'single\') -> Any: try: compiled_code = self.command_compiler(source, filename, symbol) return compiled_code except Exception as e: raise e"},{"question":"# Python Programming Assessment Question Objective: To assess your understanding of file handling with the `gzip` module, including reading from and writing to gzip files, as well as handling errors. Problem Statement: Write a Python function `compress_and_extract` that performs the following tasks: 1. Reads a plain text file and compresses its contents using the `gzip` format. 2. Stores the compressed data in a new file. 3. Reads the compressed file, decompresses its contents, and verifies that the decompressed content matches the original text. You are provided with the following inputs: - `input_file_path` (str): Path to the plain text file that needs to be compressed. - `gzip_file_path` (str): Path where the compressed gzip file should be stored. Your function should follow these steps: 1. Open the `input_file_path` and read its contents. 2. Compress the read contents and store them in the `gzip_file_path`. 3. Open the `gzip_file_path`, decompress the contents, and read the decompressed data. 4. Verify that the decompressed data matches the original content read from `input_file_path`. Function Signature: ```python def compress_and_extract(input_file_path: str, gzip_file_path: str) -> bool: pass ``` Constraints: 1. You are expected to handle any file exceptions, such as file not found, permission errors, etc. 2. The function should return `True` if the decompressed data matches the original text, otherwise `False`. 3. The function should be efficient in terms of memory usage and avoid loading large files entirely into memory if possible. Example: Suppose we have a plain text file `example.txt` with the following content: ``` Hello, World! Python gzip example. ``` - Input: ```python input_file_path = \\"example.txt\\" gzip_file_path = \\"example.txt.gz\\" ``` - Expected Output: ```python True # if the contents match after compression and decompression ``` Additional Information: Refer to the `gzip` module documentation for details on functions and methods you may need to use.","solution":"import gzip def compress_and_extract(input_file_path: str, gzip_file_path: str) -> bool: try: # Step 1: Read the contents of the input plain text file with open(input_file_path, \'rt\', encoding=\'utf-8\') as file: original_content = file.read() # Step 2: Compress the read contents and store them in the gzip file with gzip.open(gzip_file_path, \'wt\', encoding=\'utf-8\') as gz_file: gz_file.write(original_content) # Step 3: Decompress the contents of the gzip file and read the decompressed data with gzip.open(gzip_file_path, \'rt\', encoding=\'utf-8\') as gz_file: decompressed_content = gz_file.read() # Step 4: Verify that the decompressed data matches the original content return decompressed_content == original_content except (FileNotFoundError, IOError, OSError) as e: print(f\\"Error: {e}\\") return False"},{"question":"**Problem: Analyzing Installed Packages Information** You have been tasked to write a Python function that makes use of the `importlib.metadata` package to collect and summarize detailed information about installed packages in a Python environment. **Function Signature**: ```python def summarize_installed_packages(packages: list) -> dict: ``` **Input:** - `packages` (list): A list of package names (as strings) installed in the current Python environment. **Output:** - A dictionary where each key is a package name from the input list, and the corresponding value is another dictionary with the following keys and their respective values: * `\'version\'`: Version of the package (string). * `\'requires_python\'`: The required Python version for this package (string). * `\'dependencies\'`: A list of dependencies of the package (list of strings). * `\'entry_points\'`: A dictionary where keys are entry point groups and values are lists of entry point names within those groups. **Constraints:** - You may assume that all packages provided in the input list are installed in the current Python environment. - Handle cases where certain metadata fields (like `requires_python`, `dependencies`, or `entry_points`) might be missing. **Example:** ```python packages = [\'wheel\', \'setuptools\'] summary = summarize_installed_packages(packages) # Example output (actual output will vary based on the installed versions and their metadata): { \'wheel\': { \'version\': \'0.36.2\', \'requires_python\': \'>=3.6\', \'dependencies\': [\'filelock\', \'six\'], \'entry_points\': { \'console_scripts\': [\'wheel\'] } }, \'setuptools\': { \'version\': \'52.0.0\', \'requires_python\': \'>=3.5\', \'dependencies\': [\'certifi\'], \'entry_points\': { \'console_scripts\': [\'easy_install\', \'easy_install3\'] } } } ``` Note: The function should leverage the `importlib.metadata` module to gather the required information. **Performance Requirements:** The solution should be efficient but does not need to be optimized for extremely large numbers of packages. The primary goal is clarity and correctness in fetching and organizing the package metadata.","solution":"import importlib.metadata def summarize_installed_packages(packages: list) -> dict: Summarizes detailed information about installed packages. Args: packages (list): List of installed package names. Returns: dict: Dictionary with detailed information on each package. summary = {} for package in packages: try: metadata = importlib.metadata.metadata(package) version = metadata.get(\'Version\', \'unknown\') requires_python = metadata.get(\'Requires-Python\', \'N/A\') dependencies = importlib.metadata.requires(package) or [] entry_points = {} for entry_point in importlib.metadata.entry_points().get(package, []): entry_points.setdefault(entry_point.group, []).append(entry_point.name) summary[package] = { \'version\': version, \'requires_python\': requires_python, \'dependencies\': dependencies, \'entry_points\': entry_points, } except importlib.metadata.PackageNotFoundError: summary[package] = { \'version\': \'N/A\', \'requires_python\': \'N/A\', \'dependencies\': [], \'entry_points\': {} } return summary"},{"question":"# Logging System Integration and Customization **Objective:** Your task is to implement a custom logging solution using Python\'s built-in `logging` module. The solution will include creating custom handlers and formatters, configuring logging behavior, and demonstrating thread-safe logging. # Problem Statement: 1. **Implement a Custom Handler:** - Create a custom handler named `CustomStreamHandler` that inherits from `logging.StreamHandler`. - Override the `emit` method in `CustomStreamHandler` to prepend the string `\\"CustomHandler: \\"` to every log message. 2. **Implement a Custom Formatter:** - Create a custom formatter named `CustomFormatter`. - The `CustomFormatter` should format the log messages to include the log level, the name of the logger, and the actual log message in the following format: ``` [LOG LEVEL] logger_name: log_message ``` - Ensure that the format includes a timestamp in the format `YYYY-MM-DD HH:MM:SS.mmm`. 3. **Configure the Logging System:** - Configure the logging system such that: - The root logger uses the `CustomStreamHandler` and `CustomFormatter`. - The log level is set to `DEBUG`. 4. **Demonstrate Different Log Levels:** - Write a function named `log_demo` that demonstrates logging at various levels (`DEBUG`, `INFO`, `WARNING`, `ERROR`, `CRITICAL`) using the root logger. - Ensure that the logs show up with the custom format and through the custom handler. 5. **Thread-Safe Logging:** - Write a function named `log_in_threads` that spawns multiple threads. Each thread should log a unique message identified by the thread number. - Ensure that the logging system handles concurrent access correctly and logs messages in a thread-safe manner. - Use at least 3 threads to demonstrate this. # Constraints: - Your implementation should work without any external dependencies other than Python’s `logging` and `threading` modules. - The custom handler and formatter should be implemented as specified. - Ensure that the log messages are concise and clear. # Example Usage: ```python if __name__ == \\"__main__\\": # Configure the logging system. configure_logging() # Demonstrate logging at different levels. log_demo() # Demonstrate thread-safe logging. log_in_threads() ``` # Expected Output: The expected output should show the log messages formatted according to the custom formatter and should demonstrate concurrency without any issues in the logs.","solution":"import logging import threading from datetime import datetime class CustomStreamHandler(logging.StreamHandler): def emit(self, record): record.msg = \\"CustomHandler: \\" + record.msg super().emit(record) class CustomFormatter(logging.Formatter): def format(self, record): record_time = datetime.fromtimestamp(record.created).strftime(\'%Y-%m-%d %H:%M:%S.%f\')[:-3] formatted_message = f\\"[{record.levelname}] {record.name}: {record.msg} {record_time}\\" return formatted_message def configure_logging(): logger = logging.getLogger() logger.setLevel(logging.DEBUG) custom_handler = CustomStreamHandler() custom_formatter = CustomFormatter() custom_handler.setFormatter(custom_formatter) logger.addHandler(custom_handler) def log_demo(): logger = logging.getLogger() logger.debug(\\"This is a debug message\\") logger.info(\\"This is an info message\\") logger.warning(\\"This is a warning message\\") logger.error(\\"This is an error message\\") logger.critical(\\"This is a critical message\\") def log_in_threads(): def thread_logging(thread_num): logger = logging.getLogger() logger.info(f\\"Message from thread {thread_num}\\") threads = [] for i in range(3): thread = threading.Thread(target=thread_logging, args=(i,)) threads.append(thread) thread.start() for thread in threads: thread.join() if __name__ == \\"__main__\\": configure_logging() log_demo() log_in_threads()"},{"question":"# SAX Parser Coding Assessment **Objective**: Implement a SAX parser using the `xml.sax.xmlreader` module to read and process XML data. # Task You are provided with an XML document that contains information about books in a library. Your task is to write a function that parses this XML data incrementally and extracts information about books, storing this information in a list of dictionaries. # XML Data Example ```xml <library> <book> <title>Learn Python</title> <author>John Doe</author> <year>2021</year> <isbn>1234567890</isbn> </book> <book> <title>Advanced Python</title> <author>Jane Smith</author> <year>2020</year> <isbn>0987654321</isbn> </book> </library> ``` # Requirements 1. **Function Signature**: ```python def parse_books(xml_data: str) -> list: ``` 2. **Input**: - `xml_data`: a string containing the XML data. 3. **Output**: - A list of dictionaries, each dictionary containing book details with keys: `\'title\'`, `\'author\'`, `\'year\'`, and `\'isbn\'`. 4. **Constraints**: - You must use `xml.sax.xmlreader.IncrementalParser` for parsing the XML data. - You should handle XML tags and extract data incrementally using the `feed` method of `IncrementalParser`. # Example Usage ```python xml_data = <library> <book> <title>Learn Python</title> <author>John Doe</author> <year>2021</year> <isbn>1234567890</isbn> </book> <book> <title>Advanced Python</title> <author>Jane Smith</author> <year>2020</year> <isbn>0987654321</isbn> </book> </library> parsed_books = parse_books(xml_data) assert parsed_books == [ { \'title\': \'Learn Python\', \'author\': \'John Doe\', \'year\': \'2021\', \'isbn\': \'1234567890\' }, { \'title\': \'Advanced Python\', \'author\': \'Jane Smith\', \'year\': \'2020\', \'isbn\': \'0987654321\' } ] ``` # Hints 1. You will need to create custom content handlers to manage the SAX events and accumulate data. 2. Make use of `IncrementalParser.feed` to process the XML data in chunks. 3. Ensure proper error handling for well-formedness checks when closing the parser.","solution":"import xml.sax from xml.sax.handler import ContentHandler class BookHandler(ContentHandler): def __init__(self): self.current_data = \\"\\" self.book = {} self.books = [] def startElement(self, name, attrs): self.current_data = name if name == \\"book\\": self.book = {} def endElement(self, name): if name == \\"book\\": self.books.append(self.book) self.current_data = \\"\\" def characters(self, content): if self.current_data == \\"title\\": self.book[\\"title\\"] = content elif self.current_data == \\"author\\": self.book[\\"author\\"] = content elif self.current_data == \\"year\\": self.book[\\"year\\"] = content elif self.current_data == \\"isbn\\": self.book[\\"isbn\\"] = content def parse_books(xml_data: str) -> list: handler = BookHandler() parser = xml.sax.make_parser() parser.setContentHandler(handler) xml.sax.parseString(xml_data, handler) return handler.books"},{"question":"**Objective**: Demonstrate your understanding of Python\'s `inspect` module, particularly the signature and parameter introspection capabilities. **Task**: Implement a function `clone_function_with_defaults(func)` that takes a function `func` and returns a new function that has the same signature as `func` but with all parameters defaulting to `None` if they didn\'t have a default value. # Requirements: 1. **Input**: - A single argument `func`, which is a Python function. 2. **Output**: - A new function with the same signature as the input function but with all parameters defaulting to `None` if they were not previously assigned a default value. 3. **Constraints**: - The input function can have positional, keyword, and variadic arguments (both `*args` and `**kwargs`). - You must use the `inspect` module to retrieve and modify the function\'s signature. 4. **Example Execution**: ```python from inspect import signature def original_function(a, b, c=3, *args, d, e=5, **kwargs): pass new_function = clone_function_with_defaults(original_function) sig1 = signature(original_function) sig2 = signature(new_function) print(str(sig1)) # \\"(a, b, c=3, *args, d, e=5, **kwargs)\\" print(str(sig2)) # \\"(a=None, b=None, c=3, *args, d=None, e=5, **kwargs)\\" ``` # Implementation Details: 1. Use `inspect.signature` to get the signature of the input function. 2. Iterate over the parameters, using `Parameter.replace()` to assign a default value of `None` where necessary. 3. Use the modified parameters to create a new `Signature` object. 4. Define a new function that adheres to this signature and returns it. ```python import inspect def clone_function_with_defaults(func): sig = inspect.signature(func) new_params = [] for name, param in sig.parameters.items(): if param.default is inspect.Parameter.empty and param.kind in ( param.POSITIONAL_ONLY, param.POSITIONAL_OR_KEYWORD, param.KEYWORD_ONLY): new_param = param.replace(default=None) else: new_param = param new_params.append(new_param) new_sig = sig.replace(parameters=new_params) def new_func(*args, **kwargs): bound_args = new_sig.bind(*args, **kwargs) return func(*bound_args.args, **bound_args.kwargs) new_func.__signature__ = new_sig return new_func # Example usage def test_func(a, b, *args, d, e=5, **kwargs): return a, b, args, d, e, kwargs new_test_func = clone_function_with_defaults(test_func) print(inspect.signature(test_func)) print(inspect.signature(new_test_func)) # Should print: # (a, b, *args, d, e=5, **kwargs) # (a=None, b=None, *args, d=None, e=5, **kwargs) ``` # Performance Requirements: - Ensure that the implementation handles functions with a large number of parameters efficiently. - The solution should work for functions with complex signatures involving variadic and keyword arguments. # Notes: 1. The use of the `inspect` module is mandatory for this task. 2. The new function should maintain all properties of the original function, except for the updated default values.","solution":"import inspect def clone_function_with_defaults(func): Takes a function and returns a new function where all its parameters default to None if they didn\'t have a default value. sig = inspect.signature(func) new_params = [] for name, param in sig.parameters.items(): if (param.default is param.empty and (param.kind in ( param.POSITIONAL_ONLY, param.POSITIONAL_OR_KEYWORD, param.KEYWORD_ONLY))): new_params.append(param.replace(default=None)) else: new_params.append(param) new_sig = sig.replace(parameters=new_params) def new_func(*args, **kwargs): bound_args = new_sig.bind(*args, **kwargs) bound_args.apply_defaults() return func(*bound_args.args, **bound_args.kwargs) new_func.__signature__ = new_sig return new_func"},{"question":"# Question: Recursive Directory Inspection and Summary **Objective**: You will implement a Python function that recursively inspects a directory and returns a summary of its contents, categorized by file type. This will help in understanding the depth of directory structures and the variety of files stored within. **Function Specification**: - Implement a function `directory_summary(directory_path: str) -> dict` that will: - Traverse the directory recursively starting at `directory_path`. - Count the number of files for each file type (regular files, directories, symbolic links, sockets, character devices, block devices, FIFOs, doors, event ports, whiteouts). - Return a dictionary where keys are the file type names (as strings) and values are the counts of files of that type. **Expected Input**: - `directory_path`: A string representing the path to the directory to inspect. **Expected Output**: - A dictionary with the following keys: - `\\"regular_file\\"` - `\\"directory\\"` - `\\"symbolic_link\\"` - `\\"socket\\"` - `\\"character_device\\"` - `\\"block_device\\"` - `\\"fifo\\"` - `\\"door\\"` - `\\"event_port\\"` - `\\"whiteout\\"` - The values should be integers representing the count of each file type. **Constraints**: - The function should handle permissions errors gracefully and skip the files or directories that cannot be accessed. **Example**: ```python import os import stat def directory_summary(directory_path): summary = { \\"regular_file\\": 0, \\"directory\\": 0, \\"symbolic_link\\": 0, \\"socket\\": 0, \\"character_device\\": 0, \\"block_device\\": 0, \\"fifo\\": 0, \\"door\\": 0, \\"event_port\\": 0, \\"whiteout\\": 0 } def walktree(top): try: for f in os.listdir(top): pathname = os.path.join(top, f) try: mode = os.lstat(pathname).st_mode if stat.S_ISDIR(mode): summary[\\"directory\\"] += 1 walktree(pathname) elif stat.S_ISREG(mode): summary[\\"regular_file\\"] += 1 elif stat.S_ISLNK(mode): summary[\\"symbolic_link\\"] += 1 elif stat.S_ISSOCK(mode): summary[\\"socket\\"] += 1 elif stat.S_ISCHR(mode): summary[\\"character_device\\"] += 1 elif stat.S_ISBLK(mode): summary[\\"block_device\\"] += 1 elif stat.S_ISFIFO(mode): summary[\\"fifo\\"] += 1 elif hasattr(stat, \'S_ISDOOR\') and stat.S_ISDOOR(mode): summary[\\"door\\"] += 1 elif hasattr(stat, \'S_ISPORT\') and stat.S_ISPORT(mode): summary[\\"event_port\\"] += 1 elif hasattr(stat, \'S_ISWHT\') and stat.S_ISWHT(mode): summary[\\"whiteout\\"] += 1 except PermissionError: continue except PermissionError: pass walktree(directory_path) return summary # Example usage: # print(directory_summary(\\"/path/to/inspect\\")) ``` **Notes**: - Consider edge cases like empty directories, inaccessible files, and non-existent paths. - Avoid using external libraries; rely primarily on `os` and `stat`.","solution":"import os import stat def directory_summary(directory_path): summary = { \\"regular_file\\": 0, \\"directory\\": 0, \\"symbolic_link\\": 0, \\"socket\\": 0, \\"character_device\\": 0, \\"block_device\\": 0, \\"fifo\\": 0, \\"door\\": 0, \\"event_port\\": 0, \\"whiteout\\": 0 } def walktree(top): try: for f in os.listdir(top): pathname = os.path.join(top, f) try: mode = os.lstat(pathname).st_mode if stat.S_ISDIR(mode): summary[\\"directory\\"] += 1 walktree(pathname) elif stat.S_ISREG(mode): summary[\\"regular_file\\"] += 1 elif stat.S_ISLNK(mode): summary[\\"symbolic_link\\"] += 1 elif stat.S_ISSOCK(mode): summary[\\"socket\\"] += 1 elif stat.S_ISCHR(mode): summary[\\"character_device\\"] += 1 elif stat.S_ISBLK(mode): summary[\\"block_device\\"] += 1 elif stat.S_ISFIFO(mode): summary[\\"fifo\\"] += 1 elif hasattr(stat, \'S_ISDOOR\') and stat.S_ISDOOR(mode): summary[\\"door\\"] += 1 elif hasattr(stat, \'S_ISPORT\') and stat.S_ISPORT(mode): summary[\\"event_port\\"] += 1 elif hasattr(stat, \'S_ISWHT\') and stat.S_ISWHT(mode): summary[\\"whiteout\\"] += 1 except PermissionError: continue except PermissionError: pass walktree(directory_path) return summary"},{"question":"**Objective:** Implement a custom Python module execution feature using the `runpy` module. Your function should execute Python modules or scripts from a specified input and return the updated global variables after execution. The function should specifically handle changes to the `sys` module and demonstrate the ability to execute both individual module files and complete packages. **Task:** Write a function `execute_python_code(source, is_module=True, init_globals=None, alter_sys=False)`, which executes a Python script/module and returns its global variables. 1. **Parameters:** - `source` (str): The module name if `is_module` is True, or the file path if `is_module` is False. - `is_module` (bool): A flag indicating whether `source` is a module name or a file path. Default is True. - `init_globals` (dict): An optional dictionary of initial global variables to use during execution. Default is None. - `alter_sys` (bool): A flag indicating whether to modify the `sys` module during execution. Default is False. 2. **Returns:** - A dictionary representing the global variables after the execution of the script/module. 3. **Constraints:** - If `is_module` is True, the `source` must be an existing Python module. - If `is_module` is False, the `source` must be a valid file path to a Python script. 4. **Requirements:** - Use `runpy.run_module` and `runpy.run_path` appropriately based on the value of `is_module`. - Ensure that changes to the `sys` module are handled correctly and any alterations are reverted before the function returns. - Demonstrate error handling for non-existing modules or invalid file paths. **Example Usage:** ```python # Assuming a valid Python module named \'example_module\' exists glob_dict_module = execute_python_code(\'example_module\') print(glob_dict_module) # Assuming a valid Python script at path \'path/to/script.py\' exists glob_dict_script = execute_python_code(\'path/to/script.py\', is_module=False) print(glob_dict_script) ``` You should include appropriate error handling to ensure that invalid module names or file paths are appropriately managed without causing unhandled exceptions.","solution":"import runpy import sys import os def execute_python_code(source, is_module=True, init_globals=None, alter_sys=False): Executes a Python script/module and returns its global variables. Parameters: - source (str): The module name if is_module is True, or the file path if is_module is False. - is_module (bool): A flag indicating whether source is a module name or a file path. Default is True. - init_globals (dict): An optional dictionary of initial global variables to use during execution. Default is None. - alter_sys (bool): A flag indicating whether to modify the sys module during execution. Default is False. Returns: - A dictionary representing the global variables after the execution of the script/module. if init_globals is None: init_globals = {} original_sys_modules = sys.modules.copy() original_sys_path = sys.path[:] try: if is_module: result = runpy.run_module(source, init_globals=init_globals, alter_sys=alter_sys) else: if not os.path.isfile(source): raise FileNotFoundError(f\\"The file {source} does not exist.\\") result = runpy.run_path(source, init_globals=init_globals) except ImportError as e: raise ImportError(f\\"Could not import module \'{source}\'.\\") from e except FileNotFoundError as e: raise FileNotFoundError(f\\"File not found: {source}.\\") from e finally: if alter_sys: sys.modules = original_sys_modules sys.path = original_sys_path return result"},{"question":"# Time Series Data Resampling and Analysis You are provided with a time series dataset containing the daily average temperature of a city for a year. Your task is to analyze this data by using various resampling techniques provided by the pandas library. Specifically, you need to perform downsampling, upsampling, and various aggregate computations to extract meaningful insights from the data. Input - A CSV file `temperature_data.csv` with two columns: - `date`: Date in the format `YYYY-MM-DD`. - `avg_temp`: The average temperature recorded on that date. Tasks 1. **Load the Dataset**: - Load the dataset into a pandas DataFrame and ensure the `date` column is parsed as a `datetime` object and set as the index. 2. **Monthly Statistics**: - Resample the data to obtain the monthly average temperatures. 3. **Weekly Interpolation**: - Upsample the dataset to a daily frequency and use a linear interpolation method to fill in missing values for the new days. 4. **Aggregate Monthly Data**: - For each month, calculate and return the following statistics using resampling methods: - Mean temperature. - Maximum temperature. - Minimum temperature. - Total number of unique days recorded. 5. **Quarterly Analysis**: - Resample the data to a quarterly frequency and calculate the sum of temperatures for each quarter. 6. **Result Formatting**: - Return the results for each task in a well-formatted pandas DataFrame. Output The implemented function should return a dictionary containing the following keys and their corresponding pandas DataFrame outputs: - `\'monthly_avg_temp\'`: A DataFrame with the average temperature per month. - `\'weekly_interpolated\'`: A DataFrame with the daily temperatures after interpolation. - `\'monthly_stats\'`: A DataFrame with mean, max, min, and nunique temperatures per month. - `\'quarterly_sum_temp\'`: A DataFrame with the sum of temperatures per quarter. Constraints - You should use pandas functions and methods appropriately for each task. - Ensure your code is efficient and readable. Example ```python # Sample function signature import pandas as pd def analyze_temperature_data(file_path): # Load and preprocess the data df = pd.read_csv(file_path, parse_dates=[\'date\'], index_col=\'date\') # Task 1: Monthly Statistics monthly_avg_temp = df.resample(\'M\').mean() # Task 2: Weekly Interpolation upsampled_df = df.resample(\'D\').asfreq() weekly_interpolated = upsampled_df.interpolate(method=\'linear\') # Task 3: Aggregate Monthly Data monthly_stats = df.resample(\'M\').agg([\'mean\', \'max\', \'min\', \'nunique\']) # Task 4: Quarterly Analysis quarterly_sum_temp = df.resample(\'Q\').sum() return { \'monthly_avg_temp\': monthly_avg_temp, \'weekly_interpolated\': weekly_interpolated, \'monthly_stats\': monthly_stats, \'quarterly_sum_temp\': quarterly_sum_temp } # Example usage: result = analyze_temperature_data(\'temperature_data.csv\') # Access different results from the dictionary monthly_avg_temp = result[\'monthly_avg_temp\'] weekly_interpolated = result[\'weekly_interpolated\'] monthly_stats = result[\'monthly_stats\'] quarterly_sum_temp = result[\'quarterly_sum_temp\'] ```","solution":"import pandas as pd def analyze_temperature_data(file_path): # Load and preprocess the data df = pd.read_csv(file_path, parse_dates=[\'date\'], index_col=\'date\') # Task 1: Monthly Statistics monthly_avg_temp = df.resample(\'M\').mean() # Task 2: Weekly Interpolation upsampled_df = df.resample(\'D\').asfreq() weekly_interpolated = upsampled_df.interpolate(method=\'linear\') # Task 3: Aggregate Monthly Data monthly_stats = df.resample(\'M\').agg({ \'avg_temp\': [\'mean\', \'max\', \'min\', \'nunique\'] }) # Task 4: Quarterly Analysis quarterly_sum_temp = df.resample(\'Q\').sum() return { \'monthly_avg_temp\': monthly_avg_temp, \'weekly_interpolated\': weekly_interpolated, \'monthly_stats\': monthly_stats, \'quarterly_sum_temp\': quarterly_sum_temp }"},{"question":"**Advanced Sorting Challenge: Custom Multi-key Sorting** **Problem Statement:** You are given a list of tuples containing student information in the format `(name, grade, score)`, where: - `name` is a string representing the student\'s name. - `grade` is a string representing the student\'s grade (e.g., \'A\', \'B\', \'C\', etc.). - `score` is an integer representing the student\'s score. Your task is to implement a function `sort_students` that sorts this list of students based on the following criteria: 1. The primary sort key is the `grade`, in ascending order. 2. The secondary sort key is the `score`, in descending order. 3. If two students have the same `grade` and `score`, their original order should be preserved (stable sort). **Function Signature:** ```python def sort_students(students: list[tuple[str, str, int]]) -> list[tuple[str, str, int]]: ``` **Input:** - A list of tuples `students`, where each tuple contains: - `name`: A string (1 <= len(name) <= 50) - `grade`: A string (1 <= len(grade) <= 2) - `score`: An integer (0 <= score <= 100) **Output:** - A list of tuples sorted based on the criteria mentioned above. **Constraints:** - The list can have up to 1000 elements. - The grades are single characters from \'A\' to \'F\'. **Example:** ```python students = [ (\'john\', \'B\', 85), (\'jane\', \'A\', 91), (\'dave\', \'B\', 85), (\'sue\', \'C\', 75), (\'anne\', \'A\', 95) ] sorted_students = sort_students(students) print(sorted_students) ``` **Expected Output:** ```python [ (\'anne\', \'A\', 95), (\'jane\', \'A\', 91), (\'john\', \'B\', 85), (\'dave\', \'B\', 85), (\'sue\', \'C\', 75) ] ``` **Notes:** - Ensure you use the key functions to efficiently sort the students based on the given criteria. - Utilize the stability property of Python\'s sorting algorithms. Good luck!","solution":"def sort_students(students): Sorts the list of students based on the specified multi-key criteria: 1. Primary key: grade (ascending) 2. Secondary key: score (descending) # Single chained sorting using lambda to unpack and set key priorities return sorted(students, key=lambda student: (student[1], -student[2]))"},{"question":"# Unix Group Membership Analysis Write a function named `analyze_group_membership` that performs the following tasks: 1. Retrieves all group entries from the Unix group database using the `grp` module. 2. Returns a dictionary where: - Each key is a username that is a member of any group. - Each value is a set of group names that the user belongs to. # Requirements: - Make sure to handle possible entries that might start with \\"+\\" or \\"-\\" as mentioned in the documentation. These entries should be ignored. - Utilize the `grp.getgrall()` function to retrieve the group data. - Your solution should be efficient, aiming for a time complexity of O(n), where n is the total number of groups in the system. # Input: - No input parameters. # Output: - A dictionary with the structure described above. # Example: ```python # Considering the groups: # group1: {name: \'group1\', members: [\'user1\', \'user2\']} # group2: {name: \'group2\', members: [\'user2\', \'user3\']} # group3: {name: \'-group3\', members: [\'user4\']} # The resulting dictionary should be: # {\'user1\': {\'group1\'}, \'user2\': {\'group1\', \'group2\'}, \'user3\': {\'group2\'}} ``` ```python def analyze_group_membership(): import grp groups = grp.getgrall() user_groups = {} for group in groups: group_name, _, _, members = group if group_name.startswith((\'+\', \'-\')): continue for member in members: if member not in user_groups: user_groups[member] = set() user_groups[member].add(group_name) return user_groups ```","solution":"def analyze_group_membership(): import grp groups = grp.getgrall() user_groups = {} for group in groups: group_name = group.gr_name members = group.gr_mem if group_name.startswith((\'+\', \'-\')): continue for member in members: if member not in user_groups: user_groups[member] = set() user_groups[member].add(group_name) return user_groups"},{"question":"**Problem: Analyze and Group Words by Length** You are tasked to write a function that processes a list of words and groups them by their lengths. The function should return a dictionary where each key is a word length, and the corresponding value is a list of words with that length. Use the `defaultdict` class from the `collections` module to implement this functionality. # Function Signature ```python def group_words_by_length(words: List[str]) -> Dict[int, List[str]]: Groups words by their lengths. Parameters: - words (List[str]): A list of words (strings). Returns: - Dict[int, List[str]]: A dictionary where each key is a word length, and each value is a list of words with that length. ``` # Input - `words` (List[str]): A list of words (strings), with 0 ≤ len(words) ≤ 10^6 and 1 ≤ len(word) ≤ 1000 for each word. # Output - A dictionary where keys are integers representing word lengths, and values are lists of words with those lengths. # Constraints - Handle large input sizes efficiently. - The function should use the `defaultdict` class from the `collections` module. - Words should be grouped preserving their original order. # Example ```python words = [\\"apple\\", \\"bat\\", \\"banana\\", \\"art\\", \\"dog\\", \\"an\\", \\"batman\\"] output = group_words_by_length(words) print(output) ``` Expected Output ```python { 5: [\\"apple\\"], 3: [\\"bat\\", \\"art\\", \\"dog\\"], 6: [\\"banana\\", \\"batman\\"], 2: [\\"an\\"] } ``` # Notes - You can assume the input list `words` does not contain any empty strings. - The order of words in the lists should match the order in which they appear in the input list. Good luck, and make sure to test your code with edge cases and large inputs!","solution":"from typing import List, Dict from collections import defaultdict def group_words_by_length(words: List[str]) -> Dict[int, List[str]]: Groups words by their lengths. Parameters: - words (List[str]): A list of words (strings). Returns: - Dict[int, List[str]]: A dictionary where each key is a word length, and each value is a list of words with that length. length_dict = defaultdict(list) for word in words: length_dict[len(word)].append(word) return dict(length_dict)"},{"question":"**Objective:** You are required to implement a function that manages and processes event schedules using Python\'s `datetime` and `collections` modules. The function will handle both the creation and querying of events over a timeline. ```python from datetime import datetime, timedelta from collections import defaultdict def manage_events(events, start_date, end_date): This function processes a list of events and returns a schedule of events that fall between given dates. Parameters: events (list of dict): A list of event dictionaries. Each dictionary should have the following keys: - \'name\' (str): The name of the event. - \'start\' (str): The start date and time of the event in \'YYYY-MM-DD HH:MM\' format. - \'duration\' (int): Duration of the event in minutes. start_date (str): The start date for filtering events in \'YYYY-MM-DD HH:MM\' format. end_date (str): The end date for filtering events in \'YYYY-MM-DD HH:MM\' format. Returns: dict: A dictionary with dates as keys and lists of event names as values. Each event should be listed on each date it occurs. Dates in the result dict should be in \'YYYY-MM-DD\' format, and event lists should be sorted by start time. # Your code here pass ``` # Requirements: 1. **Input:** - `events`: A list of dictionaries where each dictionary describes an event with: - `\'name\'`: Name of the event (string). - `\'start\'`: Start date and time in \'YYYY-MM-DD HH:MM\' format (string). - `\'duration\'`: Duration of the event in minutes (integer). - `start_date` and `end_date`: Strings representing the start and end dates for filtering events in \'YYYY-MM-DD HH:MM\' format. 2. **Output:** - A dictionary where each key is a date (in \'YYYY-MM-DD\' format) and each value is a list of event names that occur on that date. Events should appear on each day they occur if they span multiple days. - Event lists should be sorted by their start time. 3. **Constraints:** - Ensure the function handles events that span multiple days correctly. - The function should be able to handle edge cases, such as events that start exactly at the `start_date` or end exactly at the `end_date`. - Performance should be considered for handling up to 10,000 events. # Example: ```python events = [ {\'name\': \'Event1\', \'start\': \'2023-10-01 09:00\', \'duration\': 180}, {\'name\': \'Event2\', \'start\': \'2023-10-01 16:00\', \'duration\': 60}, {\'name\': \'Event3\', \'start\': \'2023-10-02 08:00\', \'duration\': 1440}, ] start_date = \'2023-10-01 00:00\' end_date = \'2023-10-03 00:00\' print(manage_events(events, start_date, end_date)) # Expected Output: # { # \'2023-10-01\': [\'Event1\', \'Event2\'], # \'2023-10-02\': [\'Event3\'], # \'2023-10-03\': [\'Event3\'] # } ```","solution":"from datetime import datetime, timedelta from collections import defaultdict def manage_events(events, start_date, end_date): This function processes a list of events and returns a schedule of events that fall between given dates. Parameters: events (list of dict): A list of event dictionaries. Each dictionary should have the following keys: - \'name\' (str): The name of the event. - \'start\' (str): The start date and time of the event in \'YYYY-MM-DD HH:MM\' format. - \'duration\' (int): Duration of the event in minutes. start_date (str): The start date for filtering events in \'YYYY-MM-DD HH:MM\' format. end_date (str): The end date for filtering events in \'YYYY-MM-DD HH:MM\' format. Returns: dict: A dictionary with dates as keys and lists of event names as values. Each event should be listed on each date it occurs. Dates in the result dict should be in \'YYYY-MM-DD\' format, and event lists should be sorted by start time. def parse_datetime(date_str): return datetime.strptime(date_str, \'%Y-%m-%d %H:%M\') def format_date(date_obj): return date_obj.strftime(\'%Y-%m-%d\') start_date_dt = parse_datetime(start_date) end_date_dt = parse_datetime(end_date) schedule = defaultdict(list) for event in events: event_start = parse_datetime(event[\'start\']) event_end = event_start + timedelta(minutes=event[\'duration\']) if event_end > start_date_dt and event_start < end_date_dt: current_date = max(event_start, start_date_dt) event_end_date = min(event_end, end_date_dt) while current_date.date() <= event_end_date.date(): if current_date.date() == event_start.date(): schedule[format_date(current_date)].append((event_start, event[\'name\'])) else: schedule[format_date(current_date)].append((current_date, event[\'name\'])) current_date += timedelta(days=1) output_schedule = { date: [name for time, name in sorted(events, key=lambda x: x[0])] for date, events in schedule.items() } return output_schedule"},{"question":"# Concurrent Execution with Threads **Objective:** Write a Python script that demonstrates your understanding of concurrent execution using the `threading` module. You will implement a program that calculates the factorial of multiple numbers concurrently. **Description:** 1. Create a function `factorial(n)` that calculates the factorial of a given number `n`. 2. Create a function `calculate_factorials(numbers)` that: - Accepts a list of integers (`numbers`). - Creates a separate thread for each number in the list to compute its factorial using the `factorial(n)` function. - Uses a thread-safe data structure to store the results (use a dictionary with numbers as keys and their factorials as values). - Prints the thread name and result for each computation. 3. Ensure that the main thread waits for all threads to complete before printing a final message, \\"Factorial computations complete.\\" **Requirements:** - Use the `threading` module. - Use appropriate synchronization mechanisms to ensure thread-safe access to shared data. - The function `calculate_factorials(numbers)` must return the dictionary containing the original numbers and their corresponding factorials. **Input:** - A list of integers, e.g., `[5, 7, 9, 3]`. **Output:** - A dictionary with the input numbers as keys and their factorials as values, printed in any order. - Intermediate prints showing which thread is computing which factorial, e.g., \\"Thread-1 calculating factorial(5)=120\\". **Constraints:** - The input list can contain up to 10 integers, each integer ranging from 0 to 20. - Ensure efficient synchronization to avoid any race conditions. **Example:** ```python import threading def factorial(n): # Implement the factorial function here pass def calculate_factorials(numbers): # Implement the function to perform concurrent factorial calculations pass if __name__ == \\"__main__\\": numbers = [5, 7, 9, 3] result = calculate_factorials(numbers) print(result) # Expected output (order may vary): # Thread-1 calculating factorial(5)=120 # Thread-2 calculating factorial(7)=5040 # Thread-3 calculating factorial(9)=362880 # Thread-4 calculating factorial(3)=6 # {\\"5\\": 120, \\"7\\": 5040, \\"9\\": 362880, \\"3\\": 6} # Factorial computations complete. ```","solution":"import threading def factorial(n): Returns the factorial of a given number n. if n == 0: return 1 result = 1 for i in range(1, n + 1): result *= i return result def calculate_factorials(numbers): Calculate the factorial of each number in the list \'numbers\' concurrently using threads. result = {} threads = [] lock = threading.Lock() def worker(num): fac = factorial(num) thread_name = threading.current_thread().name with lock: result[num] = fac print(f\\"{thread_name} calculating factorial({num})={fac}\\") for number in numbers: thread = threading.Thread(target=worker, args=(number,)) threads.append(thread) thread.start() for thread in threads: thread.join() print(\\"Factorial computations complete.\\") return result"},{"question":"In this assignment, you will use PyTorch\'s `torch.ao.ns.fx.utils` module to evaluate the performance of a quantized model compared to its original version. You are required to implement a function that computes several metrics for this comparison. Function Signature ```python def evaluate_model_similarity(original_model, quantized_model, input_tensor): Compare the similarity between the original model and the quantized model using SQNR, normalized L2 error, and cosine similarity. Parameters: - original_model (torch.nn.Module): The original (non-quantized) model. - quantized_model (torch.nn.Module): The quantized version of the original model. - input_tensor (torch.Tensor): A sample input tensor to evaluate the model outputs. Returns: - dict: A dictionary containing the SQNR, normalized L2 error, and cosine similarity between the outputs of the two models. ``` Guidelines 1. **Functionality**: - The function should take three parameters: the original model, the quantized model, and an input tensor. - It should run both models on the input tensor, collect their output tensors, and compute the following metrics: - Signal to Quantization Noise Ratio (SQNR) - Normalized L2 error - Cosine similarity - The computed metrics should be returned as a dictionary with keys `sqnr`, `l2_error`, and `cos_similarity`. 2. **Constraints**: - You can assume that the `input_tensor` is suitable for both models. - The models are of type `torch.nn.Module`. 3. **Usage of Utility Functions**: - Make sure to use `compute_sqnr`, `compute_normalized_l2_error`, and `compute_cosine_similarity` from the `torch.ao.ns.fx.utils` module to perform the calculations. Example ```python import torch import torch.nn as nn import torch.ao.ns.fx.utils as ns_utils # Dummy models for illustration class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.fc = nn.Linear(10, 10) def forward(self, x): return self.fc(x) # Create original and quantized models original_model = SimpleModel() quantized_model = SimpleModel() # Assuming quantized version available # Create a sample input tensor input_tensor = torch.randn(1, 10) # Define the function using the utility functions def evaluate_model_similarity(original_model, quantized_model, input_tensor): original_output = original_model(input_tensor) quantized_output = quantized_model(input_tensor) sqnr = ns_utils.compute_sqnr(original_output, quantized_output) l2_error = ns_utils.compute_normalized_l2_error(original_output, quantized_output) cos_similarity = ns_utils.compute_cosine_similarity(original_output, quantized_output) return {\\"sqnr\\": sqnr, \\"l2_error\\": l2_error, \\"cos_similarity\\": cos_similarity} # Run the comparison metrics = evaluate_model_similarity(original_model, quantized_model, input_tensor) print(metrics) ``` In the implementation, ensure that you handle any potential issues such as matching tensor sizes and types, and make necessary imports where required.","solution":"import torch import torch.ao.ns.fx.utils as ns_utils def evaluate_model_similarity(original_model, quantized_model, input_tensor): Compare the similarity between the original model and the quantized model using SQNR, normalized L2 error, and cosine similarity. Parameters: - original_model (torch.nn.Module): The original (non-quantized) model. - quantized_model (torch.nn.Module): The quantized version of the original model. - input_tensor (torch.Tensor): A sample input tensor to evaluate the model outputs. Returns: - dict: A dictionary containing the SQNR, normalized L2 error, and cosine similarity between the outputs of the two models. # Obtain outputs from both models original_output = original_model(input_tensor) quantized_output = quantized_model(input_tensor) # Compute the metrics sqnr = ns_utils.compute_sqnr(original_output, quantized_output) l2_error = ns_utils.compute_normalized_l2_error(original_output, quantized_output) cos_similarity = ns_utils.compute_cosine_similarity(original_output, quantized_output) return { \\"sqnr\\": sqnr, \\"l2_error\\": l2_error, \\"cos_similarity\\": cos_similarity }"},{"question":"# Advanced Coding Assignment: Asynchronous Task Scheduler with Priorities Objective: Implement a Python function using `asyncio.PriorityQueue` to manage and execute tasks based on their priorities. Problem Statement: You are to create an asynchronous task scheduler that schedules and executes tasks based on their priorities. The tasks with the highest priority (lowest priority number) should be executed first. Each task will be a coroutine that needs to be awaited. # Function Signature ```python import asyncio async def task_scheduler(tasks): Schedule and execute tasks based on their priority. Args: tasks (List[Tuple[int, Callable[[], Awaitable]]]): List of tuples where the first element is the priority of the task (an integer, lower numbers indicate higher priority) and the second element is a no-argument coroutine function to be scheduled. Returns: List[Any]: Returns the results of each executed task in the order they were executed. pass ``` # Input: - `tasks`: A list of tuples. Each tuple consists of: - An integer representing the priority of the task (lower number indicates higher priority). - A no-argument coroutine function that represents the task to be executed. # Output: - Returns a list containing the results of each executed task in the order they were executed. # Constraints: - All tasks are coroutine functions with no arguments. - The priority of the tasks is an integer where a lower value means higher priority. - Tasks should be executed concurrently but following their priority order. # Example: ```python import asyncio async def task1(): await asyncio.sleep(1) return \\"Task 1 Completed\\" async def task2(): await asyncio.sleep(3) return \\"Task 2 Completed\\" async def task3(): await asyncio.sleep(2) return \\"Task 3 Completed\\" tasks = [ (1, task1), (3, task2), (2, task3) ] result = await task_scheduler(tasks) print(result) # Output: [\\"Task 1 Completed\\", \\"Task 3 Completed\\", \\"Task 2 Completed\\"] ``` # Hints: 1. Use `asyncio.PriorityQueue` to manage the task scheduling. 2. Use `async def` functions to define tasks. 3. The tasks should be retrieved from the priority queue in the order of their priority and then awaited for completion. 4. You may use `asyncio.create_task` to manage concurrent execution. 5. Make sure to handle the `await queue.get()` and `queue.task_done()` correctly in your implementation to ensure all tasks are processed before concluding.","solution":"import asyncio from typing import List, Tuple, Callable, Awaitable, Any async def task_scheduler(tasks: List[Tuple[int, Callable[[], Awaitable]]]) -> List[Any]: Schedule and execute tasks based on their priority. Args: tasks (List[Tuple[int, Callable[[], Awaitable]]]): List of tuples where the first element is the priority of the task (an integer, lower numbers indicate higher priority) and the second element is a no-argument coroutine function to be scheduled. Returns: List[Any]: Returns the results of each executed task in the order they were executed. queue = asyncio.PriorityQueue() # Put all tasks into the priority queue for priority, task_callable in tasks: await queue.put((priority, task_callable)) results = [] # Process tasks in order of priority while not queue.empty(): priority, task_callable = await queue.get() result = await task_callable() results.append(result) queue.task_done() return results"},{"question":"**Objective**: Demonstrate understanding of advanced concepts in the `subprocess` module. **Problem Statement**: You are tasked with creating a small utility in Python that performs the following actions: 1. Executes a given shell command (provided as a string) in a subprocess. 2. Captures and combines the standard output (`stdout`) and standard error (`stderr`) of the subprocess. 3. Allows setting a custom environment variable for the subprocess. 4. Implements a timeout mechanism, ensuring that the subprocess will be terminated if it runs too long. 5. Returns the combined output, the exit status of the process, and a flag indicating if the process was terminated due to a timeout. # Implementation Requirements: - Function Name: `run_command` - Inputs: - `command`: A string representing the shell command to execute. - `env_var`: A dictionary representing the environment variables to set for the subprocess. - `timeout`: An integer representing the timeout duration in seconds. - Outputs: - A tuple containing: - `combined_output`: A string containing the combined stdout and stderr of the subprocess. - `exit_status`: An integer indicating the exit status of the subprocess, where `0` indicates success, and any non-zero value indicates an error. - `timed_out`: A boolean that is `True` if the subprocess was terminated due to the timeout, otherwise `False`. - Constraints: - The function should handle any exceptions that may arise from running the subprocess and capture the output accordingly. - You are not allowed to use `shell=True` for security reasons. # Example Usage: ```python command = \\"echo \'Hello, World!\' && sleep 1 && echo \'Done\'\\" env_var = {\\"MY_VAR\\": \\"123\\"} timeout = 2 output, status, timed_out = run_command(command, env_var, timeout) print(f\\"Output: {output}\\") print(f\\"Exit Status: {status}\\") print(f\\"Timed Out: {timed_out}\\") ``` # Expected Output: ``` Output: Hello, World! Done Exit Status: 0 Timed Out: False ``` ```python command = \\"echo \'Start\' && sleep 2 && echo \'End\'\\" env_var = {\\"MY_VAR\\": \\"456\\"} timeout = 1 output, status, timed_out = run_command(command, env_var, timeout) print(f\\"Output: {output}\\") print(f\\"Exit Status: {status}\\") print(f\\"Timed Out: {timed_out}\\") ``` # Expected Output: ``` Output: Start Exit Status: 1 # Or an appropriate non-zero value depending on the system. Timed Out: True ``` # Notes: - Ensure the solution properly terminates the subprocess if the timeout is reached. - Correctly handle the subprocess output and errors. - Proper error handling for edge cases such as command not found or invalid environment variables.","solution":"import subprocess import threading def run_command(command, env_var, timeout): Executes a given shell command in a subprocess, captures combined stdout and stderr, allows setting custom environment variables, and implements timeout mechanism. Args: - command: str, shell command to execute. - env_var: dict, environment variables for the subprocess. - timeout: int, timeout duration in seconds. Returns: - tuple: (combined_output: str, exit_status: int, timed_out: bool) def target(): try: result = subprocess.run( command, shell=False, env=env_var, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True ) nonlocal combined_output, exit_status, had_timeout combined_output = result.stdout + result.stderr exit_status = result.returncode except Exception as exc: combined_output = str(exc) exit_status = 1 finally: had_timeout = False combined_output = \\"\\" exit_status = 1 had_timeout = True thread = threading.Thread(target=target) thread.start() thread.join(timeout) if thread.is_alive(): exit_status = 1 combined_output = \\"Process terminated due to timeout.\\" had_timeout = True return (combined_output, exit_status, had_timeout) return (combined_output, exit_status, had_timeout)"},{"question":"# Question: Implementing Float Operations Using Python\'s C API You are provided with an interface to Python\'s C API that handles floating point objects (`PyFloatObject`). Using the utilities described below, implement a Python function that constructs floating point numbers from strings and doubles, checks their types, and retrieves their maximum and minimum representable values. Provided Functions: 1. **PyFloat_Check(PyObject *p)**: Returns `True` if *p* is a `PyFloatObject` or a subtype of `PyFloatObject`; otherwise, returns `False`. 2. **PyFloat_CheckExact(PyObject *p)**: Returns `True` if *p* is exactly a `PyFloatObject` and not a subtype; otherwise, returns `False`. 3. **PyFloat_FromString(PyObject *str)**: Converts a string *str* to a `PyFloatObject`. Returns `None` on failure. 4. **PyFloat_FromDouble(double v)**: Converts a C double *v* to a `PyFloatObject`. Returns `None` on failure. 5. **PyFloat_AsDouble(PyObject *pyfloat)**: Returns the C `double` representation of the `PyFloatObject` *pyfloat*. If conversion fails, returns `-1.0`. 6. **PyFloat_GetMax()**: Returns the maximum representable float as a C `double`. 7. **PyFloat_GetMin()**: Returns the minimum representable normalized positive float as a C `double`. Function to Implement: Create a function `float_operations(string_value: str, double_value: float) -> dict` that: 1. Converts the `string_value` to a `PyFloatObject` using `PyFloat_FromString`. 2. Converts the `double_value` to a `PyFloatObject` using `PyFloat_FromDouble`. 3. Checks if the created objects are `PyFloatObject`s using `PyFloat_Check`. 4. Checks if the created objects are exactly `PyFloatObject`s (not subtypes) using `PyFloat_CheckExact`. 5. Retrieves the C `double` representations of the created `PyFloatObject`s using `PyFloat_AsDouble`. 6. Returns a dictionary containing the original values, type check results, and their C `double` representations. 7. Returns also the maximum and minimum representable values using `PyFloat_GetMax` and `PyFloat_GetMin`. Example: ```python def float_operations(string_value: str, double_value: float) -> dict: # Implementation here ... # Example call to the function: result = float_operations(\\"123.456\\", 789.012) # Expected Output: # { # \\"string_float\\": <PyFloatObject>, # \\"double_float\\": <PyFloatObject>, # \\"is_float_string\\": True, # \\"is_exact_float_string\\": True, # \\"is_float_double\\": True, # \\"is_exact_float_double\\": True, # \\"c_double_from_string\\": 123.456, # \\"c_double_from_double\\": 789.012, # \\"max_float\\": <DBL_MAX value>, # \\"min_float\\": <DBL_MIN value> # } ``` Notes: - If `PyFloat_FromString` or `PyFloat_FromDouble` fails, handle the error gracefully and indicate the failure in the output. - Ensure the function\'s output matches the expected output format, including proper handling for all operations.","solution":"def float_operations(string_value: str, double_value: float) -> dict: # Placeholder for actual implementation. The real implementation would use Python\'s C API. result = { \\"string_float\\": None, \\"double_float\\": None, \\"is_float_string\\": False, \\"is_exact_float_string\\": False, \\"is_float_double\\": False, \\"is_exact_float_double\\": False, \\"c_double_from_string\\": None, \\"c_double_from_double\\": None, \\"max_float\\": None, \\"min_float\\": None } # Simulate PyFloat_FromString try: string_float = float(string_value) result[\\"string_float\\"] = string_float result[\\"is_float_string\\"] = True result[\\"is_exact_float_string\\"] = True # Simulated, exact check result[\\"c_double_from_string\\"] = string_float except ValueError: result[\\"string_float\\"] = None # Simulate PyFloat_FromDouble double_float = float(double_value) result[\\"double_float\\"] = double_float result[\\"is_float_double\\"] = True result[\\"is_exact_float_double\\"] = True # Simulated, exact check result[\\"c_double_from_double\\"] = double_float # Simulate PyFloat_GetMax and PyFloat_GetMin import sys result[\\"max_float\\"] = sys.float_info.max result[\\"min_float\\"] = sys.float_info.min return result"},{"question":"# Question: Python `bytes` Object Manipulation You are tasked with implementing a series of functions that manipulate Python\'s bytes objects at a low level using the C API methods described in the documentation. This will require understanding of memory management, as well as accurate transformation of raw data to and from Python bytes objects. **Part 1**: Implement a function `create_bytes_from_string` that takes a string and returns a new Python bytes object created from this string using `PyBytes_FromString`. ```python def create_bytes_from_string(s: str) -> bytes: Create a new bytes object from the given string. :param s: Input string :return: Corresponding bytes object pass ``` **Part 2**: Implement a function `concat_bytes_objects` that takes two bytes objects and returns a new bytes object that is the concatenation of the two given bytes objects using `PyBytes_Concat`. ```python def concat_bytes_objects(bytes1: bytes, bytes2: bytes) -> bytes: Concatenate two bytes objects. :param bytes1: First bytes object :param bytes2: Second bytes object :return: New bytes object which is the concatenation of bytes1 and bytes2 pass ``` **Part 3**: Implement a function `resize_bytes_object` that takes a bytes object and a new size, and returns the resized bytes object using `_PyBytes_Resize`. ```python def resize_bytes_object(bytes_obj: bytes, new_size: int) -> bytes: Resize the given bytes object to a new size. :param bytes_obj: Original bytes object :param new_size: Desired size for the bytes object :return: Resized bytes object pass ``` **Part 4**: Implement a function `get_bytes_size` that takes a bytes object and returns its size using `PyBytes_Size`. ```python def get_bytes_size(bytes_obj: bytes) -> int: Get the size of the given bytes object. :param bytes_obj: A bytes object :return: Size of the bytes object pass ``` **Part 5**: Create a demonstration function `demo_bytes_operations` that: 1. Creates a bytes object from a string. 2. Concatenates two bytes objects. 3. Resizes a bytes object. 4. Retrieves the size of a bytes object. Implement the demo with appropriate print statements to verify correctness. ```python def demo_bytes_operations(): Demonstrate the bytes object operations. # Step 1: Create a bytes object from a string pass # Step 2: Concatenate two bytes objects pass # Step 3: Resize a bytes object pass # Step 4: Retrieve the size of a bytes object pass ``` **Constraints**: - You may assume all string inputs are valid C strings (null-terminated). - The sizes provided for resizing will be non-negative integers. - Handle any necessary error checking and exceptions. - You must use the described C API functions to manipulate bytes objects. Test your function implementations with a variety of inputs to ensure correctness.","solution":"def create_bytes_from_string(s: str) -> bytes: Create a new bytes object from the given string. :param s: Input string :return: Corresponding bytes object return s.encode() def concat_bytes_objects(bytes1: bytes, bytes2: bytes) -> bytes: Concatenate two bytes objects. :param bytes1: First bytes object :param bytes2: Second bytes object :return: New bytes object which is the concatenation of bytes1 and bytes2 return bytes1 + bytes2 def resize_bytes_object(bytes_obj: bytes, new_size: int) -> bytes: Resize the given bytes object to a new size. :param bytes_obj: Original bytes object :param new_size: Desired size for the bytes object :return: Resized bytes object current_size = len(bytes_obj) if new_size < current_size: return bytes_obj[:new_size] else: return bytes_obj + b\'x00\' * (new_size - current_size) def get_bytes_size(bytes_obj: bytes) -> int: Get the size of the given bytes object. :param bytes_obj: A bytes object :return: Size of the bytes object return len(bytes_obj) def demo_bytes_operations(): Demonstrate the bytes object operations. # Step 1: Create a bytes object from a string s = \\"Hello, World!\\" bytes_obj = create_bytes_from_string(s) print(f\\"Bytes from string \'{s}\': {bytes_obj}\\") # Step 2: Concatenate two bytes objects bytes1 = b\\"Hello, \\" bytes2 = b\\"World!\\" concatenated_bytes = concat_bytes_objects(bytes1, bytes2) print(f\\"Concatenated bytes: {concatenated_bytes}\\") # Step 3: Resize a bytes object resized_bytes = resize_bytes_object(bytes_obj, 8) print(f\\"Resized bytes (to 8): {resized_bytes}\\") resized_bytes = resize_bytes_object(bytes_obj, 20) print(f\\"Resized bytes (to 20): {resized_bytes}\\") # Step 4: Retrieve the size of a bytes object size = get_bytes_size(bytes_obj) print(f\\"Size of bytes object: {size}\\") # Run the demonstration demo_bytes_operations()"},{"question":"<|Analysis Begin|> The provided documentation indicates how to use seaborn to create bar plots and customize themes. It covers: - Basic usage of seaborn\'s `barplot` function. - Setting seaborn\'s theme using `sns.set_theme()` with or without arguments. - Customizing the theme by specifying style and palette. - Overriding seaborn parameters or defining additional matplotlib parameters using `rc`. This information provides a foundation for crafting an assessment question that tests students\' understanding of creating and customizing plots using seaborn. <|Analysis End|> <|Question Begin|> # Coding Assessment Question **Objective:** Assess the students\' ability to create and customize data visualizations using seaborn. **Question:** You are provided with a dataset containing information about sales performance across different regions. Write a function named `plot_sales_performance` that takes the following arguments: - `data`: A pandas DataFrame containing at least two columns: `\\"region\\"` and `\\"sales\\"`. - `theme_style`: A string indicating the desired seaborn style (e.g., `\\"whitegrid\\"`, `\\"dark\\"`, etc.). - `palette`: A string specifying the color palette to use for the bar plot. - `custom_params`: A dictionary containing any additional matplotlib rc parameters to override or set. The function should: 1. Set the seaborn theme using the provided `theme_style` and `palette`. 2. Apply any additional custom plotting parameters from `custom_params`. 3. Create a bar plot using seaborn that displays the total sales for each region. 4. Display the plot. **Input:** - `data`: A pandas DataFrame with at least the following columns: ``` region (str) | sales (int) -------------|----------- North | 1000 East | 1500 South | 1200 West | 900 ``` **Output:** - A bar plot displaying the total sales for each region with the specified theme, palette, and custom parameters. **Constraints:** 1. `data` is guaranteed to contain the columns `\\"region\\"` and `\\"sales\\"`. 2. `theme_style` will be a valid seaborn style string. 3. `palette` will be a valid seaborn palette string. 4. `custom_params` will be a valid dictionary for matplotlib rc parameters. **Example:** ```python import pandas as pd data = pd.DataFrame({ \\"region\\": [\\"North\\", \\"East\\", \\"South\\", \\"West\\"], \\"sales\\": [1000, 1500, 1200, 900] }) theme_style = \\"whitegrid\\" palette = \\"pastel\\" custom_params = {\\"axes.spines.right\\": False, \\"axes.spines.top\\": False} plot_sales_performance(data, theme_style, palette, custom_params) ``` This should display a bar plot with the specified configurations.","solution":"import seaborn as sns import matplotlib.pyplot as plt import pandas as pd def plot_sales_performance(data, theme_style, palette, custom_params): Plots the sales performance for different regions with custom seaborn theme, palette and additional matplotlib parameters. Parameters: - data (pd.DataFrame): A dataframe containing \'region\' and \'sales\' columns. - theme_style (str): Desired seaborn theme style. - palette (str): Seaborn color palette. - custom_params (dict): Additional matplotlib rc parameters. # Set the seaborn theme and palette sns.set_theme(style=theme_style, palette=palette) # Apply custom parameters if custom_params: plt.rcParams.update(custom_params) # Create the bar plot sns.barplot(x=\\"region\\", y=\\"sales\\", data=data) # Display the plot plt.show()"},{"question":"**Advanced Seaborn Visualization Task** In this task, you will use the Seaborn library to visualize a dataset and demonstrate your understanding of creating and customizing plots with seaborn objects. # Task Description You are provided with a dataset on health expenditure and life expectancy of various countries. You need to create a plot that visualizes the trajectory of health expenditure over time for each country, and modify the aesthetics of the plot such as markers and lines. Additionally, you will need to combine this plot with another type of plot that provides additional insights into the dataset. # Requirements: 1. **Load the Dataset**: - Use the `load_dataset` function from seaborn to load the `healthexp` dataset. 2. **Create a Plot with `Path`**: - Create a plot using `seaborn.objects.Plot` to plot \\"Spending_USD\\" against \\"Life_Expectancy\\", color the paths by \\"Country\\". 3. **Customize the Path**: - Add markers to the path. - Customize the markers to have a point size of 5 and linewidth of 1.5. - Fill the markers with white color. 4. **Add a Second Plot**: - Add another subplot (e.g., using a different plot type such as a bar plot) that shows the average health expenditure by country. - Use `FacetGrid` or any other relevant technique to combine the two plots. # Input and Output Formats - **Input**: No explicit input is taken from the user. The dataset must be loaded within the program. - **Output**: Display the combined plots (one with trajectories and the other with averages) using matplotlib. # Constraints - Use seaborn functionalities for plotting. - Ensure the plots are clear and visually distinguishable. - Write clean and well-documented code. # Example Solution Here is a skeleton of what your solution might involve: ```python import seaborn.objects as so from seaborn import load_dataset import seaborn as sns import matplotlib.pyplot as plt # Load dataset healthexp = load_dataset(\\"healthexp\\").sort_values([\\"Country\\", \\"Year\\"]) # Create a trajectory plot using Path p = so.Plot(healthexp, \\"Spending_USD\\", \\"Life_Expectancy\\", color=\\"Country\\") p.add(so.Path(marker=\\"o\\", pointsize=5, linewidth=1.5, fillcolor=\\"w\\")) # Additional plot for average health expenditure by country average_exp = healthexp.groupby(\'Country\')[\'Spending_USD\'].mean().reset_index() bar_plot = sns.barplot(data=average_exp, x=\'Country\', y=\'Spending_USD\') # Combine plots fig, ax = plt.subplots(2, 1, figsize=(12, 10)) p.on(ax[0]) # Add the first plot to the first subplot bar_plot.figure = fig # Add bar plot to the figure ax[1].add_artist(bar_plot) # Add the bar plot to the second subplot plt.show() ``` Write your complete solution by expanding and modifying the skeleton code provided above.","solution":"import seaborn.objects as so from seaborn import load_dataset import seaborn as sns import matplotlib.pyplot as plt def create_health_expenditure_plots(): # Load dataset healthexp = load_dataset(\\"healthexp\\").sort_values([\\"Country\\", \\"Year\\"]) # Create a trajectory plot using Path p = so.Plot(healthexp, \\"Spending_USD\\", \\"Life_Expectancy\\", color=\\"Country\\") p.add(so.Path(marker=\\"o\\", pointsize=5, linewidth=1.5, fillcolor=\\"w\\")).scale(color=sns.color_palette(\\"tab10\\")) # Create a bar plot for average health expenditure by country average_exp = healthexp.groupby(\'Country\')[\'Spending_USD\'].mean().reset_index() fig, axs = plt.subplots(2, 1, figsize=(15, 12), constrained_layout=True) # Add the first plot to the first subplot p.on(axs[0]) axs[0].set_title(\'Health Expenditure over Time by Country\', fontsize=16) # Bar plot for average expenditure sns.barplot(data=average_exp, x=\'Spending_USD\', y=\'Country\', ax=axs[1], palette=\\"tab10\\") axs[1].set_title(\'Average Health Expenditure by Country\', fontsize=16) axs[1].set_xlabel(\'Average Spending (USD)\') axs[1].set_ylabel(\'Country\') # Display the plots plt.show() # Call the function to create and display the plots create_health_expenditure_plots()"},{"question":"# PyTorch Coding Challenge: Compositional Function Transforms **Objective**: Implement a function utilizing `torch.func` to compute the per-sample gradients of a given neural network. Description You are provided with a simple feed-forward neural network and a loss function. Your task is to implement a function that computes the per-sample gradients for a batch of inputs and corresponding labels. Requirements 1. **Network Architecture**: A simple feed-forward neural network with one hidden layer. 2. **Loss Function**: Mean Squared Error (MSE) Loss. 3. **Transforms to Use**: Use `torch.func.grad` and `torch.func.vmap` to compute per-sample gradients. Function Signature ```python import torch from torch import nn import torch.func as F class SimpleNN(nn.Module): def __init__(self): super(SimpleNN, self).__init__() self.hidden = nn.Linear(10, 5) self.output = nn.Linear(5, 1) def forward(self, x): x = torch.relu(self.hidden(x)) x = self.output(x) return x def compute_per_sample_gradients(model, loss_fn, inputs, labels): Compute per-sample gradients for the given model, loss function, inputs, and labels. Args: model (nn.Module): The neural network model. loss_fn: The loss function (Mean Squared Error). inputs (torch.Tensor): A batch of input data of shape (batch_size, 10). labels (torch.Tensor): A batch of corresponding labels of shape (batch_size, 1). Returns: dict: Dictionary containing per-sample gradients for each parameter of the model. The keys are the parameter names, and the values are tensors of gradients with shape (batch_size, *parameter.shape). # Implement the function using `torch.func.grad` and `torch.func.vmap` here pass # Example usage: model = SimpleNN() loss_fn = nn.MSELoss(reduction=\'none\') inputs = torch.randn(32, 10) labels = torch.randn(32, 1) per_sample_grads = compute_per_sample_gradients(model, loss_fn, inputs, labels) for name, grad in per_sample_grads.items(): print(f\\"Parameter: {name}, Per-sample gradient shape: {grad.shape}\\") ``` Constraints 1. `inputs` and `labels` tensors are guaranteed to be of shapes `(batch_size, 10)` and `(batch_size, 1)` respectively. 2. Assume that the model, loss function, and all necessary imports and initializations are correctly set up as provided in the function signature. 3. Your function should return the per-sample gradients for each parameter in the model as a dictionary. Tips - Refer to the `torch.func.grad` and `torch.func.vmap` functionality documentation for details on how to use these transforms. - Ensure the computation is efficient and leverages the `torch.func` capabilities effectively. **Note**: Be sure not to modify the function signature. The provided test case should run successfully with your implemented function.","solution":"import torch from torch import nn import torch.func as F class SimpleNN(nn.Module): def __init__(self): super(SimpleNN, self).__init__() self.hidden = nn.Linear(10, 5) self.output = nn.Linear(5, 1) def forward(self, x): x = torch.relu(self.hidden(x)) x = self.output(x) return x def compute_per_sample_gradients(model, loss_fn, inputs, labels): def loss_fn_for_single_sample(model, inputs, labels): outputs = model(inputs.unsqueeze(0)) loss = loss_fn(outputs.squeeze(0), labels) return loss # Function to compute gradients for single sample def per_sample_grads(params, inputs_single, label_single): single_loss = loss_fn_for_single_sample(model, inputs_single, label_single) return torch.autograd.grad(single_loss, list(params), retain_graph=True) model_params = {name: param for name, param in model.named_parameters() if param.requires_grad} per_sample_gradients = {name: torch.zeros(inputs.size(0), *param.shape) for name, param in model_params.items()} for i in range(inputs.size(0)): single_input = inputs[i] single_label = labels[i] single_grads = per_sample_grads(model_params.values(), single_input, single_label) for (name, grad), single_grad in zip(model_params.items(), single_grads): per_sample_gradients[name][i] = single_grad return per_sample_gradients # Example usage: if __name__ == \\"__main__\\": model = SimpleNN() loss_fn = nn.MSELoss(reduction=\'none\') inputs = torch.randn(32, 10) labels = torch.randn(32, 1) per_sample_grads = compute_per_sample_gradients(model, loss_fn, inputs, labels) for name, grad in per_sample_grads.items(): print(f\\"Parameter: {name}, Per-sample gradient shape: {grad.shape}\\")"},{"question":"**Objective:** Evaluate the student\'s ability to use seaborn for creating residual plots and understanding their significance in regression analysis. **Problem Statement:** You are provided with the \\"mpg\\" dataset from seaborn. This dataset contains various attributes of different car models, including `weight`, `displacement`, `horsepower`, and `mpg` (miles per gallon). Your task is to create a function `create_residual_plots` that generates and saves the following residual plots: 1. **Simple Residual Plot:** - Plot the residuals of the regression between `weight` and `displacement`. 2. **Higher-Order Trend Test:** - Plot the residuals of the regression between `horsepower` and `mpg`, fitting a second-order polynomial. 3. **LOWESS Curve:** - Plot the residuals of the regression between `horsepower` and `mpg`, adding a LOWESS curve to emphasize any structure. Your function should save each plot as a separate image file named: - `residual_plot_1.png` - `residual_plot_2.png` - `residual_plot_3.png` **Function Signature:** ```python def create_residual_plots(): pass ``` **Expected Output:** The function should save the three residual plots as described above. **Constraints:** - Use seaborn for creating the plots. - Ensure the plots are named correctly and saved in the current working directory. **Example Usage:** ```python create_residual_plots() ``` This function does not return any value but will create and save the plots as image files in the current working directory. **Additional Notes:** - Utilize seaborn\'s `residplot` function and appropriate parameters to create the required plots. - You can use `sns.load_dataset(\'mpg\')` to load the dataset. - Ensure matplotlib is imported and used for saving image files.","solution":"import matplotlib.pyplot as plt import seaborn as sns import numpy as np def create_residual_plots(): Creates and saves the residual plots as specified in the problem statement. # Load the dataset mpg = sns.load_dataset(\'mpg\') # 1. Simple Residual Plot: weight vs displacement plt.figure() sns.residplot(x=\'weight\', y=\'displacement\', data=mpg) plt.title(\'Residuals of weight vs displacement\') plt.savefig(\'residual_plot_1.png\') plt.close() # 2. Higher-Order Trend Test: horsepower vs mpg fitting second-order polynomial plt.figure() sns.residplot(x=\'horsepower\', y=\'mpg\', data=mpg, order=2) plt.title(\'Residuals of horsepower vs mpg with 2nd order polynomial\') plt.savefig(\'residual_plot_2.png\') plt.close() # 3. LOWESS Curve: horsepower vs mpg plt.figure() sns.residplot(x=\'horsepower\', y=\'mpg\', data=mpg, lowess=True) plt.title(\'Residuals of horsepower vs mpg with LOWESS curve\') plt.savefig(\'residual_plot_3.png\') plt.close()"},{"question":"Naive Bayes Classification Objective: You are required to implement a naive Bayes classifier using the `scikit-learn` library and evaluate its performance on a given dataset. Problem Statement: 1. Load the Iris dataset from `scikit-learn`, which consists of 150 observations of iris flowers with 4 features: sepal length, sepal width, petal length, and petal width. The target variable is the species of the iris flower (setosa, versicolor, or virginica). 2. Split the dataset into training and testing sets using an 80-20 split. 3. Implement a Gaussian Naive Bayes classifier. 4. Train the classifier on the training data. 5. Evaluate the classifier\'s performance on the test data using accuracy, precision, recall, and F1-score metrics. Requirements: 1. You must use `scikit-learn`\'s Gaussian Naive Bayes implementation. 2. You are expected to split the data using `train_test_split` from `sklearn.model_selection`. 3. Use appropriate methods from `sklearn.metrics` to calculate the required evaluation metrics. Expected Method: Implement a function called `naive_bayes_iris_classifier()` which performs the following steps: ```python from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.naive_bayes import GaussianNB from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score def naive_bayes_iris_classifier(): # Load the iris dataset iris = load_iris() X = iris.data y = iris.target # Split the dataset into training and testing sets (80% train, 20% test) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Initialize the Gaussian Naive Bayes classifier gnb = GaussianNB() # Train the classifier on the training data gnb.fit(X_train, y_train) # Predict the target values for the test set y_pred = gnb.predict(X_test) # Calculate evaluation metrics accuracy = accuracy_score(y_test, y_pred) precision = precision_score(y_test, y_pred, average=\'macro\') recall = recall_score(y_test, y_pred, average=\'macro\') f1 = f1_score(y_test, y_pred, average=\'macro\') return accuracy, precision, recall, f1 # Example usage: accuracy, precision, recall, f1 = naive_bayes_iris_classifier() print(f\\"Accuracy: {accuracy}\\") print(f\\"Precision: {precision}\\") print(f\\"Recall: {recall}\\") print(f\\"F1 Score: {f1}\\") ``` Constraints: - The data split should be reproducible using a fixed random state. - Use the `average=\'macro\'` parameter for multi-class metrics to ensure each class is equally weighted. Performance Requirements: - Aim to keep your code efficient and concise. - The solution should complete within a reasonable time frame for large datasets.","solution":"from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.naive_bayes import GaussianNB from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score def naive_bayes_iris_classifier(): # Load the iris dataset iris = load_iris() X = iris.data y = iris.target # Split the dataset into training and testing sets (80% train, 20% test) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Initialize the Gaussian Naive Bayes classifier gnb = GaussianNB() # Train the classifier on the training data gnb.fit(X_train, y_train) # Predict the target values for the test set y_pred = gnb.predict(X_test) # Calculate evaluation metrics accuracy = accuracy_score(y_test, y_pred) precision = precision_score(y_test, y_pred, average=\'macro\') recall = recall_score(y_test, y_pred, average=\'macro\') f1 = f1_score(y_test, y_pred, average=\'macro\') return accuracy, precision, recall, f1"},{"question":"**Question: Implement a Secure Data Serialization Process** You are to implement a secure data serialization and deserialization process for a set of Python objects using the `marshal` module. Due to the known security issue of the `marshal` module, you must incorporate additional checking mechanisms to ensure that only a predefined set of safe data types is marshaled and unmarshalled. # Implementation Requirements: 1. Implement a function `secure_marshal_dump(value, file_path)` that: - Takes a Python object (`value`) and a string (`file_path`) representing a writable binary file. - Serializes the object using the `marshal` module and writes it to the file specified by `file_path`. - The function should only allow supported safe types: `int`, `float`, `str`, `bool`, `None`, `list`, `dict`, `tuple`, `set`. If an unsupported type is detected within `value`, the function should raise a `TypeError`. 2. Implement a function `secure_marshal_load(file_path)` that: - Takes a string (`file_path`) representing a readable binary file. - Deserializes the object from the file using the `marshal` module and returns it. - Before returning, the function should check that the deserialized object is composed of the supported safe types listed above. If the deserialized object contains unsupported types, the function should raise a `TypeError`. # Input and Output: - **Input**: - `secure_marshal_dump(value, file_path)`: - `value`: a Python object to be serialized. - `file_path`: a valid file path to a writable binary file. - `secure_marshal_load(file_path)`: - `file_path`: a valid file path to a readable binary file. - **Output**: - `secure_marshal_dump(value, file_path)` does not return anything. - `secure_marshal_load(file_path)` returns a Python object that was serialized in the specified file. # Constraints: - For simplicity, nested data structures in `value` can only contain other supported data types. - You should not use external libraries to check for types or for serialization, relying only on built-in functions and the `marshal` module. # Performance Requirements: - The solution should efficiently serialize and deserialize objects with minimal overhead beyond what is required to perform type checks. # Template: ```python import marshal def is_supported_type(obj): if isinstance(obj, (int, float, str, bool, type(None))): return True elif isinstance(obj, (list, tuple, set)): return all(is_supported_type(item) for item in obj) elif isinstance(obj, dict): return all(isinstance(key, (str, int)) and is_supported_type(value) for key, value in obj.items()) return False def secure_marshal_dump(value, file_path): if not is_supported_type(value): raise TypeError(\\"Unsupported type detected in the value.\\") with open(file_path, \'wb\') as f: marshal.dump(value, f) def secure_marshal_load(file_path): with open(file_path, \'rb\') as f: value = marshal.load(f) if not is_supported_type(value): raise TypeError(\\"Unsupported type detected in the marshaled data.\\") return value ``` **Note:** Remember that the `marshal` module is not intended to be secure against erroneous or maliciously constructed data per the documentation. This question ensures that code samples remain safe by vetting types before marshalling and unmarshalling.","solution":"import marshal def is_supported_type(obj): if isinstance(obj, (int, float, str, bool, type(None))): return True elif isinstance(obj, (list, tuple, set)): return all(is_supported_type(item) for item in obj) elif isinstance(obj, dict): return all(isinstance(key, (str, int)) and is_supported_type(value) for key, value in obj.items()) return False def secure_marshal_dump(value, file_path): if not is_supported_type(value): raise TypeError(\\"Unsupported type detected in the value.\\") with open(file_path, \'wb\') as f: marshal.dump(value, f) def secure_marshal_load(file_path): with open(file_path, \'rb\') as f: value = marshal.load(f) if not is_supported_type(value): raise TypeError(\\"Unsupported type detected in the marshaled data.\\") return value"},{"question":"# Python Coding Assessment Question # Objective: Write a Python function that fetches data from a given URL using the `urllib.request` module, handles HTTP errors gracefully, and processes the JSON response. # Task: Implement the function `fetch_data(url: str) -> dict` as follows: 1. The function should accept a URL as a string input. 2. Use the `urllib.request` module to perform a GET request to the provided URL. 3. Include a custom User-Agent header in the request with the value \\"Python-urllib/3.10\\". 4. Handle the following HTTP errors appropriately: - 404 (Not Found): Return a dictionary with the key `error` and value \\"Not Found\\". - 500 (Internal Server Error): Return a dictionary with the key `error` and value \\"Server Error\\". - Any other HTTP error: Return a dictionary with the key `error` and the value \\"HTTP Error: {code}\\", where `{code}` is the actual HTTP status code. 5. If the request is successful, assume the response is in JSON format and read the response into a dictionary. 6. Return the dictionary obtained from the JSON response. 7. Any unexpected errors should be caught and handled by returning a dictionary with the key `error` and the value \\"Unexpected Error\\". # Constraints: - The implementation should handle network-related errors and timeouts gracefully. - You may assume all URLs provided will be valid and properly formatted. # Example usage: ```python url = \\"https://api.example.com/data\\" response = fetch_data(url) print(response) ``` # Notes: - Ensure you use `urllib.request` for making HTTP requests. - Handle JSON decoding errors appropriately. # Sample Input and Output: - If the function is given a URL that returns a JSON response: Input: `https://api.example.com/data` Output: `{ \\"name\\": \\"example\\", \\"value\\": 42 }` - If the function encounters a 404 error: Input: `https://api.example.com/invalid` Output: `{ \\"error\\": \\"Not Found\\" }` - If the function encounters a 500 error: Input: `https://api.example.com/server-error` Output: `{ \\"error\\": \\"Server Error\\" }` - If the function encounters any other HTTP error, e.g., 403: Input: `https://api.example.com/forbidden` Output: `{ \\"error\\": \\"HTTP Error: 403\\" }` Implement the function according to the specifications provided.","solution":"import urllib.request import urllib.error import json def fetch_data(url: str) -> dict: headers = {\\"User-Agent\\": \\"Python-urllib/3.10\\"} try: req = urllib.request.Request(url, headers=headers) with urllib.request.urlopen(req) as response: if response.status == 200: return json.load(response) except urllib.error.HTTPError as e: if e.code == 404: return {\\"error\\": \\"Not Found\\"} elif e.code == 500: return {\\"error\\": \\"Server Error\\"} else: return {\\"error\\": f\\"HTTP Error: {e.code}\\"} except Exception as e: return {\\"error\\": \\"Unexpected Error\\"}"},{"question":"Objective You are required to implement a function that uses the `sched` module to schedule a series of events. Some events have dependencies, meaning they should be scheduled only after other events have completed. Problem Statement Implement a function `schedule_events(events, dependencies)` that takes in: 1. `events`: A list of tuples where each tuple represents an event in the format `(event_name, delay, priority)`. - `event_name` (str): The name of the event. - `delay` (int): Delay in seconds after which the event should be scheduled relative to the scheduler start time. - `priority` (int): Priority of the event. Lower numbers indicate higher priority. 2. `dependencies`: A dictionary where each key is an `event_name` and the value is a list of `event_name`(s) that must be completed before the key event can be scheduled. The function should: - Use the `sched.scheduler` class to schedule the events, ensuring that dependencies are respected. - Print the `event_name` and the current time when an event is executed. - Events without dependencies should be scheduled immediately according to their delay and priority. - Events with dependencies should be scheduled only after all their dependencies have run. Constraints - Assume that there is no circular dependency in the dependencies provided. - All event names are unique. - Events will always have valid delays and priorities (positive integers). Example ```python import sched import time def schedule_events(events, dependencies): s = sched.scheduler(time.time, time.sleep) def execute_event(event_name): print(f\\"Event: {event_name}, Time: {time.time()}\\") for dep_event in dependents.get(event_name, []): schedule_event(dep_event) def schedule_event(event_name): for event in events: if event[0] == event_name: s.enter(event[1], event[2], execute_event, argument=(event_name,)) break dependents = {key: [] for key in dependencies} for key, value in dependencies.items(): for dep in value: dependents[dep].append(key) for event in events: if event[0] not in dependencies: schedule_event(event[0]) s.run() # Example Usage: events = [ (\\"event1\\", 2, 1), (\\"event2\\", 3, 2), (\\"event3\\", 5, 1), (\\"event4\\", 1, 2) ] dependencies = { \\"event3\\": [\\"event1\\", \\"event2\\"] } schedule_events(events, dependencies) ``` Expected Output: ``` Event: event4, Time: <current_time> Event: event1, Time: <current_time + 2> Event: event2, Time: <current_time + 3> Event: event3, Time: <current_time + 5> ```","solution":"import sched import time def schedule_events(events, dependencies): s = sched.scheduler(time.time, time.sleep) def execute_event(event_name): print(f\\"Event: {event_name}, Time: {time.time()}\\") for dep_event in dependents.get(event_name, []): if dep_event not in completed_events: unscheduled_events[dep_event] -= 1 if unscheduled_events[dep_event] == 0: schedule_event(dep_event) completed_events.add(event_name) def schedule_event(event_name): for event in events: if event[0] == event_name: s.enter(event[1], event[2], execute_event, argument=(event_name,)) break dependents = {event[0]: [] for event in events} unscheduled_events = {} for key, value in dependencies.items(): unscheduled_events[key] = len(value) for dep in value: dependents[dep].append(key) completed_events = set() for event in events: if event[0] not in dependencies: schedule_event(event[0]) s.run()"},{"question":"**Challenging Coding Assessment Question** **Objective**: Assess students\' understanding and ability to manipulate LDA and QDA in scikit-learn, including dimensionality reduction. # Task Write a Python program using scikit-learn to perform the following tasks: 1. Load the Iris dataset from scikit-learn. 2. Apply both Linear Discriminant Analysis (LDA) and Quadratic Discriminant Analysis (QDA) to the dataset. 3. Perform a 80-20 train-test split for the Iris dataset. 4. Train both LDA and QDA classifiers using the training data. 5. For LDA: - Transform the dataset using the `transform` method to project it onto a subspace that maximizes class separation. - Set the number of components to 2. 6. Predict the class labels for the test set using both LDA and QDA. 7. Calculate and print the accuracy of both classifiers on the test set. 8. Visualize the decision boundaries of LDA and QDA along with the data points on a 2D plane. # Requirements - **Input**: No input required, use the provided Iris dataset. - **Output**: - Accuracy of LDA classifier on the test set. - Accuracy of QDA classifier on the test set. - 2D plots showing decision boundaries for LDA and QDA. # Constraints - Use scikit-learn\'s `LinearDiscriminantAnalysis` and `QuadraticDiscriminantAnalysis`. - Visualizations should be created using `matplotlib`. - Ensure that LDA and QDA are properly trained and evaluated using the split train-test data. # Performance Requirements - Ensure code efficiency; training and predictions should be performed in a reasonable time frame, given the small size of the Iris dataset. ```python import numpy as np import matplotlib.pyplot as plt from sklearn import datasets from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score # 1. Load the Iris dataset iris = datasets.load_iris() X = iris.data y = iris.target # 2. Train-test split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # 3. Initialize LDA and QDA lda = LinearDiscriminantAnalysis(n_components=2) qda = QuadraticDiscriminantAnalysis() # 4. Fit the classifiers on the training data lda.fit(X_train, y_train) qda.fit(X_train, y_train) # 5. Transform the dataset using LDA X_train_lda = lda.transform(X_train) X_test_lda = lda.transform(X_test) # 6. Predict test labels y_pred_lda = lda.predict(X_test) y_pred_qda = qda.predict(X_test) # 7. Calculate and print accuracy accuracy_lda = accuracy_score(y_test, y_pred_lda) accuracy_qda = accuracy_score(y_test, y_pred_qda) print(f\\"LDA Accuracy: {accuracy_lda}\\") print(f\\"QDA Accuracy: {accuracy_qda}\\") # 8. Visualize decision boundaries def plot_decision_boundaries(X, y, model_class, title, transform=None): X_min, X_max = X[:, 0].min() - 1, X[:, 0].max() + 1 y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1 xx, yy = np.meshgrid(np.linspace(X_min, X_max, 100), np.linspace(y_min, y_max, 100)) if transform: X_plot = np.c_[xx.ravel(), yy.ravel()] X_plot_transformed = transform.transform(X_plot) Z = model_class.predict(X_plot_transformed) else: Z = model_class.predict(np.c_[xx.ravel(), yy.ravel()]) Z = Z.reshape(xx.shape) plt.contourf(xx, yy, Z, alpha=0.4) plt.scatter(X[:, 0], X[:, 1], c=y, edgecolor=\'k\', s=20) plt.title(title) plt.show() # Plot LDA decision boundary (on transformed data) plot_decision_boundaries(X_test_lda, y_test, lda, \\"LDA Decision Boundary (Transformed Data)\\") # Plot QDA decision boundary (on original data) plot_decision_boundaries(X_test[:, :2], y_test, qda, \\"QDA Decision Boundary (Original Data)\\") ```","solution":"import numpy as np import matplotlib.pyplot as plt from sklearn import datasets from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score def lda_qda_on_iris(): # 1. Load the Iris dataset iris = datasets.load_iris() X = iris.data y = iris.target # 2. Train-test split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # 3. Initialize LDA and QDA lda = LinearDiscriminantAnalysis(n_components=2) qda = QuadraticDiscriminantAnalysis() # 4. Fit the classifiers on the training data lda.fit(X_train, y_train) qda.fit(X_train, y_train) # 5. Transform the dataset using LDA X_train_lda = lda.transform(X_train) X_test_lda = lda.transform(X_test) # 6. Predict test labels y_pred_lda = lda.predict(X_test) y_pred_qda = qda.predict(X_test) # 7. Calculate and print accuracy accuracy_lda = accuracy_score(y_test, y_pred_lda) accuracy_qda = accuracy_score(y_test, y_pred_qda) return accuracy_lda, accuracy_qda if __name__ == \'__main__\': accuracy_lda, accuracy_qda = lda_qda_on_iris() print(f\\"LDA Accuracy: {accuracy_lda}\\") print(f\\"QDA Accuracy: {accuracy_qda}\\")"},{"question":"<|Analysis Begin|> The provided documentation describes two general-purpose iterator objects in Python: the sequence iterator and the callable iterator. Here are the key points: 1. `PySeqIter_Type`: - Used with `PySeqIter_New()`, which returns an iterator for a sequence object. - Used by the one-argument form of the `iter()` built-in function. - Iteration ends when the sequence raises `IndexError`. 2. `PySeqIter_Check`: - Checks if an object is of type `PySeqIter_Type`. 3. `PyCallIter_Type`: - Used with `PyCallIter_New()`, which returns an iterator for a callable object with a sentinel value. - Used by the two-argument form of the `iter()` built-in function. 4. `PyCallIter_Check`: - Checks if an object is of type `PyCallIter_Type`. 5. `PySeqIter_New`: - Takes a sequence object and returns an iterator. - The iteration ends when the sequence raises `IndexError`. 6. `PyCallIter_New`: - Takes a callable object and a sentinel value, returns an iterator. - Iteration ends when the callable returns the sentinel value. Given the details on iterators, we can design a question that asks students to implement a custom iterator using these principles, ensuring they understand both sequence and callable-based iterations. <|Analysis End|> <|Question Begin|> # Custom Iterator Implementation You are required to implement a custom iterator in Python, demonstrating your understanding of sequence and callable-based iterators. Your task is two-fold: 1. **Implement `CustomSeqIter`**: - This iterator should be created using a sequence. - The iteration should terminate when the sequence has been exhausted (when `IndexError` is raised). 2. **Implement `CustomCallIter`**: - This iterator should be created using a callable and a sentinel value. - The iteration should terminate when the callable returns a value equal to the sentinel. # Input and Output Formats **CustomSeqIter**: - **Input**: A sequence (e.g., list, tuple) - **Output**: Yield each item from the sequence until it is exhausted. **CustomCallIter**: - **Input**: A callable object that takes no parameters, and a sentinel value. - **Output**: Yield values from the callable until it returns the sentinel value. # Implementation Details **CustomSeqIter**: ```python class CustomSeqIter: def __init__(self, sequence): self.sequence = sequence self.index = 0 def __iter__(self): return self def __next__(self): try: value = self.sequence[self.index] except IndexError: raise StopIteration self.index += 1 return value ``` **CustomCallIter**: ```python class CustomCallIter: def __init__(self, callable, sentinel): self.callable = callable self.sentinel = sentinel def __iter__(self): return self def __next__(self): value = self.callable() if value == self.sentinel: raise StopIteration return value ``` # Constraints - The sequence provided to `CustomSeqIter` can be any iterable that supports `__getitem__()`. - The callable provided to `CustomCallIter` should be callable without parameters and should return a value that can be compared with the sentinel. - You must handle exceptions where appropriate, particularly ending iteration with `StopIteration`. # Example Usage - For `CustomSeqIter`: ```python seq_iter = CustomSeqIter([1, 2, 3, 4]) for item in seq_iter: print(item) # Output: 1, 2, 3, 4 ``` - For `CustomCallIter`: ```python def counter(): counter.val += 1 return counter.val counter.val = 0 call_iter = CustomCallIter(counter, 5) for item in call_iter: print(item) # Output: 1, 2, 3, 4 ``` Implement these classes and ensure you test them with the provided examples. Feel free to add more tests to validate your implementation.","solution":"class CustomSeqIter: def __init__(self, sequence): self.sequence = sequence self.index = 0 def __iter__(self): return self def __next__(self): try: value = self.sequence[self.index] except IndexError: raise StopIteration self.index += 1 return value class CustomCallIter: def __init__(self, callable, sentinel): self.callable = callable self.sentinel = sentinel def __iter__(self): return self def __next__(self): value = self.callable() if value == self.sentinel: raise StopIteration return value"},{"question":"Objective Demonstrate your understanding of the Sequence Protocol and Number Protocol in Python by implementing a custom sequence class that can also handle basic arithmetic operations. Task You are required to implement a custom sequence class called `CustomList` that mimics the behavior of a Python list but includes additional functionality to support arithmetic operations on the elements of the list. Class Specification Your `CustomList` class should support the following operations: 1. **Initialization** ```python CustomList(iterable) ``` Initialize the class with an iterable (list, tuple, etc.). If no iterable is provided, initialize with an empty list. 2. **Sequence Protocol** - Implement methods to support `len()`, indexing (`__getitem__`, `__setitem__`), slicing, iteration, and membership testing. 3. **Number Protocol** - Implement methods to support addition (`__add__`, `__radd__`), subtraction (`__sub__`, `__rsub__`), multiplication (`__mul__`, `__rmul__`), and negation (`__neg__`). 4. **Other Methods** - `__repr__`: Provide a string representation of the `CustomList`. Examples ```python # Initialization cl = CustomList([1, 2, 3, 4]) # Sequence Protocol print(len(cl)) # Output: 4 print(cl[1]) # Output: 2 cl[1] = 5 print(cl[1]) # Output: 5 print(2 in cl) # Output: False print(list(cl)) # Output: [1, 5, 3, 4] # Number Protocol cl2 = cl + 2 # Adds 2 to each element: [3, 7, 5, 6] print(cl2) cl3 = cl - 2 # Subtracts 2 from each element: [-1, 3, 1, 2] print(cl3) cl4 = cl * 3 # Multiplies each element by 3: [3, 15, 9, 12] print(cl4) cl5 = -cl # Negates each element: [-1, -5, -3, -4] print(cl5) ``` Constraints - Your class methods should raise appropriate exceptions for invalid operations. - Ensure that your class handles edge cases, such as operations on an empty list or invalid index access.","solution":"class CustomList: def __init__(self, iterable=None): if iterable is None: self.data = [] else: self.data = list(iterable) def __len__(self): return len(self.data) def __getitem__(self, index): return self.data[index] def __setitem__(self, index, value): self.data[index] = value def __iter__(self): return iter(self.data) def __contains__(self, item): return item in self.data def __add__(self, other): if isinstance(other, CustomList): return CustomList([a + b for a, b in zip(self.data, other.data)]) else: return CustomList([a + other for a in self.data]) def __radd__(self, other): return self.__add__(other) def __sub__(self, other): if isinstance(other, CustomList): return CustomList([a - b for a, b in zip(self.data, other.data)]) else: return CustomList([a - other for a in self.data]) def __rsub__(self, other): return CustomList([other - a for a in self.data]) def __mul__(self, other): if isinstance(other, CustomList): return CustomList([a * b for a, b in zip(self.data, other.data)]) else: return CustomList([a * other for a in self.data]) def __rmul__(self, other): return self.__mul__(other) def __neg__(self): return CustomList([-a for a in self.data]) def __repr__(self): return f\\"CustomList({self.data})\\""},{"question":"You are provided with a buggy implementation of a function `calculate_statistics`. This function is supposed to calculate and return the minimum, maximum, and average values from a list of numbers. However, there are several issues within the function that cause it to behave incorrectly. Your task is to use the Python Debugger (`pdb`) to identify and fix these issues. ```python def calculate_statistics(numbers): # Sum all numbers total_sum = 0 for num in numbers: total_sum += num # Calculate the number of elements in the list count = len(numbers) # Calculate the minimum and maximum (this contains a bug) min_val = max(numbers) max_val = min(numbers) # Calculate average average = total_sum / count return min_val, max_val, average # Example usage: numbers = [1, 2, 3, 4, 5] print(calculate_statistics(numbers)) ``` # Requirements - Use `pdb` to diagnose and fix the issues in the `calculate_statistics` function. - Provide a summary of the bugs you found and describe how you used `pdb` to identify them. - After debugging, the function should correctly return the minimum, maximum, and average values from the list of numbers. # Constraints: - You are only allowed to modify the `calculate_statistics` function. The function signature and sample usage should remain unchanged. - Assume the list `numbers` will always contain at least one number and all elements will be integers or floats. # Performance Requirements - The function should have a time complexity of O(n), where n is the number of elements in the list. # Example Given the list `[1, 2, 3, 4, 5]`, the corrected function should return: ```python (1, 5, 3.0) ``` # Instructions 1. Insert `pdb` breakpoints in the `calculate_statistics` function as needed using `pdb.set_trace()` or `breakpoint()`. 2. Step through the code to identify logical errors. 3. Fix the issues and ensure all bugs are resolved. 4. Submit the corrected code along with a summary of the debugging process. # Submission - Corrected function implementation. - A detailed summary of the debugging process, including what commands were used and how each bug was identified and fixed.","solution":"def calculate_statistics(numbers): Calculates and returns the minimum, maximum, and average values from a list of numbers. # Sum all numbers total_sum = 0 for num in numbers: total_sum += num # Calculate the number of elements in the list count = len(numbers) # Calculate the minimum and maximum min_val = min(numbers) max_val = max(numbers) # Calculate average average = total_sum / count return min_val, max_val, average"},{"question":"You are tasked with building a small library that uses the `contextvars` module to manage configuration that can be altered in different contexts. This will help manage application-wide settings that may change in different parts of an application, especially in asynchronous tasks. Requirements 1. **Configuration Class**: Create a class `Configuration` that manages context-local settings using context variables. - The class should have methods to set and get configuration parameters. - Parameters should be initialized with a default value and then be capable of being overridden within a specific context. - Implement a method to execute a callback within a specific context where the configuration parameters might differ. 2. **Function Implementations**: - `__init__(self, configs: Dict[str, Any])`: Initialize the context variables for the configuration parameters provided. - `set_config(self, key: str, value: Any) -> contextvars.Token`: Set the value for a specific configuration parameter. - `get_config(self, key: str, default: Any = None) -> Any`: Get the value of a specific configuration parameter. - `reset_config(self, key: str, token: contextvars.Token) -> None`: Reset the configuration parameter to the value it had before it was changed. - `run_with_config(self, callback: Callable, **configs) -> Any`: Run the callback within a context where the configuration parameters are temporarily set to the provided values. Constraints - The configuration keys and values are strings. - If a configuration key does not exist, `get_config` should return `default` or raise `LookupError` if no default is provided. Example ```python import contextvars from typing import Dict, Any, Callable class Configuration: def __init__(self, configs: Dict[str, Any]): # Implementation here def set_config(self, key: str, value: Any) -> contextvars.Token: # Implementation here def get_config(self, key: str, default: Any = None) -> Any: # Implementation here def reset_config(self, key: str, token: contextvars.Token) -> None: # Implementation here def run_with_config(self, callback: Callable, **configs) -> Any: # Implementation here # Example usage def main(): def print_config(): print(config.get_config(\\"feature_x\\")) config = Configuration({\\"feature_x\\": \\"disabled\\"}) config.set_config(\\"feature_x\\", \\"enabled\\") print_config() # Should print \\"enabled\\" token = config.set_config(\\"feature_y\\", \\"value_y\\") config.reset_config(\\"feature_y\\", token) config.run_with_config(print_config, feature_x=\\"overridden\\") if __name__ == \\"__main__\\": main() ``` Implement the `Configuration` class based on the above specification, ensuring that the contexts are properly managed.","solution":"import contextvars from typing import Dict, Any, Callable class Configuration: def __init__(self, configs: Dict[str, Any]): self._configs = {key: contextvars.ContextVar(key, default=value) for key, value in configs.items()} def set_config(self, key: str, value: Any) -> contextvars.Token: if key not in self._configs: self._configs[key] = contextvars.ContextVar(key, default=value) return self._configs[key].set(value) def get_config(self, key: str, default: Any = None) -> Any: if key not in self._configs: if default is not None: return default raise LookupError(f\'Configuration key \\"{key}\\" not found.\') return self._configs[key].get() def reset_config(self, key: str, token: contextvars.Token) -> None: if key not in self._configs: raise LookupError(f\'Configuration key \\"{key}\\" not found.\') self._configs[key].reset(token) def run_with_config(self, callback: Callable, **configs) -> Any: tokens = {} try: for key, value in configs.items(): tokens[key] = self.set_config(key, value) return callback() finally: for key, token in tokens.items(): self.reset_config(key, token) # Example usage def main(): def print_config(): print(config.get_config(\\"feature_x\\")) config = Configuration({\\"feature_x\\": \\"disabled\\"}) config.set_config(\\"feature_x\\", \\"enabled\\") print_config() # Should print \\"enabled\\" token = config.set_config(\\"feature_y\\", \\"value_y\\") config.reset_config(\\"feature_y\\", token) config.run_with_config(print_config, feature_x=\\"overridden\\") if __name__ == \\"__main__\\": main()"},{"question":"# Coding Assessment: Merging and Joining DataFrames in pandas **Objective:** Implement a function using pandas that demonstrates your understanding of merging and joining `DataFrame` objects. The function should merge multiple DataFrames from different datasets to produce a single cohesive dataset containing meaningful insights. **Problem Statement:** You are provided with three datasets: 1. Customer details (`customers`): Contains information about customers. 2. Orders (`orders`): Contains information about customer orders. 3. Products (`products`): Contains information about products. Each dataset is provided as a pandas `DataFrame`. Your task is to implement a function `merge_datasets(customers, orders, products)` that: 1. Merges these datasets to create a master dataset that includes all customer details along with their corresponding orders and products. 2. Returns the master dataset. **Function signature:** ```python import pandas as pd def merge_datasets(customers: pd.DataFrame, orders: pd.DataFrame, products: pd.DataFrame) -> pd.DataFrame: pass ``` **Input:** 1. `customers` (pd.DataFrame): A DataFrame containing customer details with the following columns: - `customer_id` (string): Unique identifier for each customer. - `name` (string): Name of the customer. - `email` (string): Email address of the customer. 2. `orders` (pd.DataFrame): A DataFrame containing order details with the following columns: - `order_id` (string): Unique identifier for each order. - `customer_id` (string): Unique identifier for each customer. - `product_id` (string): Unique identifier for each product. - `quantity` (integer): Number of products ordered. - `order_date` (string): Date when the order was placed. 3. `products` (pd.DataFrame): A DataFrame containing product details with the following columns: - `product_id` (string): Unique identifier for each product. - `product_name` (string): Name of the product. - `price` (float): Price of the product. **Output:** - A pandas DataFrame containing the merged data with the following columns: - `customer_id` - `name` - `email` - `order_id` - `product_id` - `product_name` - `quantity` - `order_date` - `price` **Constraints:** 1. Ensure that all customer orders are included in the final master dataset, even if they do not match with a product record. 2. Sort the final dataset by `order_date` in ascending order. **Example:** ```python import pandas as pd # Example DataFrames customers = pd.DataFrame({ \\"customer_id\\": [\\"C1\\", \\"C2\\"], \\"name\\": [\\"Alice\\", \\"Bob\\"], \\"email\\": [\\"alice@example.com\\", \\"bob@example.com\\"] }) orders = pd.DataFrame({ \\"order_id\\": [\\"O1\\", \\"O2\\"], \\"customer_id\\": [\\"C1\\", \\"C2\\"], \\"product_id\\": [\\"P1\\", \\"P3\\"], \\"quantity\\": [2, 1], \\"order_date\\": [\\"2023-01-01\\", \\"2023-01-03\\"] }) products = pd.DataFrame({ \\"product_id\\": [\\"P1\\", \\"P2\\"], \\"product_name\\": [\\"Product 1\\", \\"Product 2\\"], \\"price\\": [100.0, 200.0] }) # Expected function usage result = merge_datasets(customers, orders, products) print(result) ``` **Expected Output:** ``` customer_id name email order_id product_id product_name quantity order_date price 0 C1 Alice alice@example.com O1 P1 Product 1 2 2023-01-01 100.0 1 C2 Bob bob@example.com O2 P3 NaN 1 2023-01-03 NaN ``` Implement the `merge_datasets` function to achieve the above functionality. **Assessment Criteria:** - Correctness of the merging logic. - Proper handling of missing values. - Adherence to input and output format. - Code readability and comments explaining the logic.","solution":"import pandas as pd def merge_datasets(customers: pd.DataFrame, orders: pd.DataFrame, products: pd.DataFrame) -> pd.DataFrame: Merges customers, orders, and products dataframes into a master dataframe. Parameters: - customers (pd.DataFrame): Customer details dataframe. - orders (pd.DataFrame): Orders details dataframe. - products (pd.DataFrame): Products details dataframe. Returns: - pd.DataFrame: Merged dataframe containing all customer details along with their orders and product information. # Merge orders with products to get product details in orders orders_products_merged = pd.merge(orders, products, how=\'left\', on=\'product_id\') # Merge the resulting dataframe with customers to get complete details master_df = pd.merge(orders_products_merged, customers, how=\'left\', on=\'customer_id\') # Reorder the columns as specified master_df = master_df[[\'customer_id\', \'name\', \'email\', \'order_id\', \'product_id\', \'product_name\', \'quantity\', \'order_date\', \'price\']] # Sort the DataFrame by \'order_date\' in ascending order master_df = master_df.sort_values(by=\'order_date\').reset_index(drop=True) return master_df"},{"question":"# Task Description You are tasked with creating and visualizing a series of custom color palettes in seaborn. Your function should generate dark color palettes using various color specifications and display them. # Problem Statement Implement a function `create_and_display_palettes` in Python that performs the following tasks: 1. **Create Dark Palettes**: - Generate a dark palette, taking input as a named color. - Generate a dark palette using a specified color in hex format. - Generate a dark palette using the husl system with specified parameters. - Generate an extended dark palette with a user-defined number of colors. - Generate a continuous colormap using a specified color in hex format. 2. **Display Palettes**: - Using seaborn\'s visualization capabilities, display each of the created palettes side by side for comparison. # Function Signature ```python def create_and_display_palettes(): pass ``` # Expected Input and Output Format - **Input**: The function does not take any arguments. - **Output**: The function should not return anything but should display the generated palettes. # Constraints - Use the seaborn library version 0.11.2 or later. - Ensure the function covers all color specification formats provided in the documentation. - The outputs should be visual representations displayed within a Python environment (e.g., Jupyter notebook or interactive Python script). # Example While examples are not typically necessary for such visualization tasks, ensure that the output displays multiple dark palettes showcasing different coloring methods mentioned. # Notes - Make sure to use appropriate seaborn functions (`dark_palette` and others as necessary) to create the palettes. - Use a plotting function to display the palettes. You may use matplotlib for custom visual arrangements if needed. # Hints - Use the `sns.dark_palette` function for generating palettes. - Refer to Seaborn and Matplotlib documentation for additional customization and visualization methods. Good luck!","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_and_display_palettes(): Generate dark color palettes using various specifications and display them side by side. # Generating dark palettes using different specifications named_color_palette = sns.dark_palette(\\"navy\\", as_cmap=True) hex_color_palette = sns.dark_palette(\\"#4A90E2\\", as_cmap=True) husl_palette = sns.dark_palette((260, 75, 60), input=\\"husl\\", as_cmap=True) extended_palette = sns.dark_palette(\\"orange\\", n_colors=8) continuous_palette = sns.dark_palette(\\"#FF6347\\", n_colors=100, input=\\"hex\\", as_cmap=True) # Setting up the plot fig, axs = plt.subplots(1, 5, figsize=(15, 2)) fig.suptitle(\\"Dark Palettes\\") # Displaying each palette sns.heatmap([range(10)], cmap=named_color_palette, ax=axs[0], cbar=False) axs[0].set_title(\\"Named color\\") sns.heatmap([range(10)], cmap=hex_color_palette, ax=axs[1], cbar=False) axs[1].set_title(\\"Hex color\\") sns.heatmap([range(10)], cmap=husl_palette, ax=axs[2], cbar=False) axs[2].set_title(\\"Husl system\\") sns.heatmap([range(10)], cmap=sns.color_palette(extended_palette), ax=axs[3], cbar=False) axs[3].set_title(\\"Extended palette\\") sns.heatmap([range(100)], cmap=continuous_palette, ax=axs[4], cbar=False) axs[4].set_title(\\"Continuous colormap\\") for ax in axs: ax.set_xticks([]) ax.set_yticks([]) plt.show()"},{"question":"Advanced Asynchronous Handling in Python # Objective: Demonstrate your understanding of Python\'s asyncio module and your ability to handle various exceptions during asynchronous operations. # Problem Statement: You are tasked with implementing an asynchronous file reader that reads data from a file over a network using asyncio streams. Your implementation must handle various scenarios where exceptions may be raised. Specifically, you need to handle `asyncio.IncompleteReadError`, `asyncio.TimeoutError`, and `asyncio.LimitOverrunError`. # Function Signature: ```python async def read_data_from_network(reader: asyncio.StreamReader, timeout: float) -> bytes: Reads data asynchronously from the network with error handling. Parameters: - reader: an instance of asyncio.StreamReader connected to the network stream. - timeout: a float representing the maximum time to wait for a read operation. Returns: - The complete data read as bytes. Raises: - asyncio.TimeoutError if the read operation takes longer than the specified timeout. - asyncio.IncompleteReadError if the read operation does not complete entirely. - asyncio.LimitOverrunError if the buffer limit is exceeded while looking for a separator. ``` # Instructions: 1. Use the `reader` to read data from the network stream. 2. The read operation should be completed within the specified `timeout`. 3. Handle the `asyncio.IncompleteReadError` exception to retry the read operation from where it left off. 4. Handle the `asyncio.TimeoutError` exception to stop the operation and raise the error. 5. Handle the `asyncio.LimitOverrunError` exception by consuming the specified bytes and continuing the read operation. 6. Ensure your function returns the entire data read as bytes when successful. # Constraints: - You may assume that the reader is correctly connected and ready to read data. - You should focus on handling exceptions properly based on the given descriptions. - The buffer size for the reader is 4096 bytes by default. - The file may contain large amounts of binary data. # Example: ```python async def example_usage(): reader, writer = await asyncio.open_connection(\'localhost\', 8888) try: data = await read_data_from_network(reader, 10.0) print(\\"Data read successfully:\\", data) except asyncio.TimeoutError: print(\\"Operation timed out.\\") except asyncio.IncompleteReadError as e: print(\\"Incomplete read error:\\", e) # Handle incomplete read except asyncio.LimitOverrunError as e: print(\\"Limit overrun error:\\", e) # Handle limit overrun finally: writer.close() await writer.wait_closed() ``` # Notes: - Ensure that your implementation is clean, efficient, and well-documented. - Consider edge cases and error handling comprehensively.","solution":"import asyncio async def read_data_from_network(reader: asyncio.StreamReader, timeout: float) -> bytes: Reads data asynchronously from the network with error handling. Parameters: - reader: an instance of asyncio.StreamReader connected to the network stream. - timeout: a float representing the maximum time to wait for a read operation. Returns: - The complete data read as bytes. Raises: - asyncio.TimeoutError if the read operation takes longer than the specified timeout. - asyncio.IncompleteReadError if the read operation does not complete entirely. - asyncio.LimitOverrunError if the buffer limit is exceeded while looking for a separator. total_data = bytearray() while True: try: data = await asyncio.wait_for(reader.read(4096), timeout) if data == b\'\': # EOF break total_data.extend(data) except asyncio.IncompleteReadError as e: total_data.extend(e.partial) break except asyncio.TimeoutError as e: raise e except asyncio.LimitOverrunError as e: await reader.readexactly(e.consumed) return bytes(total_data)"},{"question":"# Email Retrieval and Analysis You are required to develop a Python function that connects to an IMAP server, logs in using a provided username and password, selects the inbox, retrieves all email messages, and prints basic information for each email. Function Signature ```python def retrieve_emails(username: str, password: str, server: str) -> None: pass ``` Input - `username` (str): The username for the email account. - `password` (str): The password for the email account. - `server` (str): The IMAP server address (e.g., \'imap.gmail.com\'). Output - The function should print the following details for each email in the inbox: - Email number - Subject - Sender - Date Constraints - You may assume the server uses the default IMAP port (143) and supports the IMAP4 protocol. - If at any point an error occurs (such as authentication failure, connection issues, or no messages in the inbox), the function should print an appropriate error message and gracefully exit. Example Usage Assuming you have an account with the username `example@example.com` and password `password` on the IMAP server `imap.example.com`: ```python retrieve_emails(\'example@example.com\', \'password\', \'imap.example.com\') ``` This should connect to the IMAP server, log in, select the inbox, and print the subject, sender, and date of each email in the inbox. Notes - You can refer to the `imaplib` documentation provided. - Make sure to consider security practices for handling login credentials. - Ensure that the connection is closed properly after fetching the emails.","solution":"import imaplib import email def retrieve_emails(username: str, password: str, server: str) -> None: try: # Connect to the server mail = imaplib.IMAP4_SSL(server) # Log in to the account mail.login(username, password) # Select the inbox mail.select(\'inbox\') # Search for all emails status, message_numbers = mail.search(None, \'ALL\') if status != \'OK\': print(\\"No messages found!\\") return # Process each email for num in message_numbers[0].split(): status, data = mail.fetch(num, \'(RFC822)\') if status != \'OK\': print(f\\"Failed to fetch email {num}\\") continue # Parse the email content msg = email.message_from_bytes(data[0][1]) # Get email details email_number = num.decode() subject = email.header.decode_header(msg[\\"subject\\"])[0][0] from_ = msg.get(\\"From\\") date = msg.get(\\"Date\\") # Decode subject if necessary if isinstance(subject, bytes): subject = subject.decode() # Print email details print(f\\"Email Number: {email_number}\\") print(f\\"Subject: {subject}\\") print(f\\"From: {from_}\\") print(f\\"Date: {date}\\") print(\\"=\\"*50) # Addition for better separation between emails # Logout and close the connection mail.logout() except imaplib.IMAP4.error as e: print(f\\"IMAP error: {e}\\") except Exception as e: print(f\\"An error occurred: {e}\\")"},{"question":"**Objective**: Demonstrate your understanding of the Python `random` module by implementing a custom random number generator class and utilizing it to simulate specific real-world scenarios. # Problem Statement You are required to implement a custom random number generator class that leverages the `random` module to provide additional functionality. Specifically, you need to design a class `CustomRandom` that extends the `random.Random` class and adds the following methods: 1. **random_list(self, n, distribution_type=\'uniform\', params=None)**: - Generates a list of `n` random numbers based on the specified `distribution_type`. - Supports the following `distribution_type` values: - `\'uniform\'` (default): Use `uniform(a, b)` function. `params` should be a tuple `(a, b)`. - `\'normal\'`: Use `normalvariate(mu, sigma)` function. `params` should be a tuple `(mu, sigma)`. - `\'exponential\'`: Use `expovariate(lambd)` function. `params` should be a tuple `(lambd,)`. - `\'gamma\'`: Use `gammavariate(alpha, beta)` function. `params` should be a tuple `(alpha, beta)`. - Returns a list of `n` random numbers generated according to the specified distribution. 2. **weighted_choice(self, population, weights)**: - Selects a random element from the `population` based on the provided `weights`. - `population` is a list of elements, and `weights` is a list of probabilities (summing to 1). - Returns a randomly selected element from `population`. 3. **bootstrap_mean(self, data, num_samples=1000)**: - Performs bootstrapping on the provided `data` list to estimate the mean and its confidence interval. - Generates `num_samples` bootstrapped samples. - Returns a tuple `(mean, lower_bound, upper_bound)` where `mean` is the bootstrapped sample mean, and `lower_bound` and `upper_bound` represent the 5th and 95th percentiles of the bootstrapped means, respectively. # Implementation Details - Implement a class `CustomRandom` that extends `random.Random`. - The `random_list` method should properly handle `params` to configure the distribution. - The `weighted_choice` method should utilize the appropriate functions from the `random` module to select an element based on weights. - The `bootstrap_mean` method should calculate the mean and confidence intervals as specified. # Constraints and Requirements - You may assume that the inputs to the `random_list` and `weighted_choice` methods are valid and well-formed. - Use the functions provided by the `random` module for generating random numbers and distributions. - Ensure your implementation is efficient and follows best practices for coding and clarity. # Example Usage ```python import random class CustomRandom(random.Random): def random_list(self, n, distribution_type=\'uniform\', params=None): # Implementation here def weighted_choice(self, population, weights): # Implementation here def bootstrap_mean(self, data, num_samples=1000): # Implementation here # Example usage: cr = CustomRandom() # Generate a list of 5 random numbers using the normal distribution with mu=0 and sigma=1 normal_samples = cr.random_list(5, \'normal\', (0, 1)) print(\\"Normal samples:\\", normal_samples) # Select a random element from a weighted population population = [\'a\', \'b\', \'c\'] weights = [0.5, 0.3, 0.2] selected = cr.weighted_choice(population, weights) print(\\"Weighted choice:\\", selected) # Perform bootstrapping to estimate the mean and confidence interval data = [2, 4, 6, 8, 10, 12, 14, 16, 18, 20] mean, lower, upper = cr.bootstrap_mean(data, num_samples=1000) print(f\\"Bootstrap mean: {mean}, 90% CI: [{lower}, {upper}]\\") ```","solution":"import random class CustomRandom(random.Random): def random_list(self, n, distribution_type=\'uniform\', params=None): if distribution_type == \'uniform\': a, b = params if params else (0, 1) return [self.uniform(a, b) for _ in range(n)] elif distribution_type == \'normal\': mu, sigma = params if params else (0, 1) return [self.normalvariate(mu, sigma) for _ in range(n)] elif distribution_type == \'exponential\': lambd = params[0] if params else 1.0 return [self.expovariate(lambd) for _ in range(n)] elif distribution_type == \'gamma\': alpha, beta = params if params else (1, 1) return [self.gammavariate(alpha, beta) for _ in range(n)] else: raise ValueError(\\"Unsupported distribution type\\") def weighted_choice(self, population, weights): return random.choices(population, weights)[0] def bootstrap_mean(self, data, num_samples=1000): means = [] for _ in range(num_samples): sample = [random.choice(data) for _ in range(len(data))] means.append(sum(sample) / len(sample)) mean = sum(means) / len(means) lower_bound = sorted(means)[int(0.05 * num_samples)] upper_bound = sorted(means)[int(0.95 * num_samples)] return mean, lower_bound, upper_bound"},{"question":"**Objective:** Your task is to implement a simple neural network using PyTorch and utilize the `torch.utils.module_tracker.ModuleTracker` to track and display the hierarchy of modules as they process an input tensor. # Problem Statement: 1. Implement a simple neural network in PyTorch with at least three different types of layers (e.g., `nn.Conv2d`, `nn.ReLU`, `nn.Linear`). 2. Use `torch.utils.module_tracker.ModuleTracker` to track the position inside the `nn.Module` hierarchy as the input tensor passes through the network. 3. Display the names of each module as they process the input tensor. # Requirements and Constraints: - Define the network as a subclass of `torch.nn.Module`. - Initialize the `ModuleTracker` and use it within the network to track module executions. - The implementation should be generalizable and not hard-coded for a specific input shape or size. - Efficient handling of the tracker and ensuring it correctly reflects the module hierarchy. # Input: - The neural network\'s forward method will accept a tensor, `input_tensor` of shape `(batch_size, channels, height, width)`. # Output: - Print the names of the modules (in execution order) to the console as they process the `input_tensor`. # Example: ```python import torch import torch.nn as nn import torch.utils.module_tracker as mt class SimpleNet(nn.Module): def __init__(self): super(SimpleNet, self).__init__() self.conv1 = nn.Conv2d(in_channels=3, out_channels=8, kernel_size=3) self.relu = nn.ReLU() self.fc1 = nn.Linear(in_features=8*6*6, out_features=10) # Initialize module tracker self.tracker = mt.ModuleTracker() def forward(self, x): # Start tracking with self.tracker.track(self, \'conv1\'): x = self.conv1(x) with self.tracker.track(self, \'relu\'): x = self.relu(x) x = x.view(x.size(0), -1) with self.tracker.track(self, \'fc1\'): x = self.fc1(x) return x def print_module_hierarchy(self): # Print tracked modules for module_name in self.tracker.modules: print(module_name) # Test the network with random input net = SimpleNet() input_tensor = torch.randn(1, 3, 8, 8) output = net(input_tensor) net.print_module_hierarchy() ``` In this example, the expected output should show the hierarchical names of the modules that processed the input tensor. # Notes: - Ensure that module tracking and printing are correctly implemented. - Test your implementation with different input shapes to validate its generalizability.","solution":"import torch import torch.nn as nn import torch.utils.hooks as hooks class SimpleNet(nn.Module): def __init__(self): super(SimpleNet, self).__init__() self.conv1 = nn.Conv2d(in_channels=3, out_channels=8, kernel_size=3) self.relu = nn.ReLU() self.fc1 = nn.Linear(in_features=8*6*6, out_features=10) self.module_names = [] # Add hooks to capture module names during forward pass self.conv1.register_forward_hook(self.hook) self.relu.register_forward_hook(self.hook) self.fc1.register_forward_hook(self.hook) def hook(self, module, input, output): # Append module class name to the list self.module_names.append(module.__class__.__name__) def forward(self, x): x = self.conv1(x) x = self.relu(x) x = x.view(x.size(0), -1) # Flatten the tensor x = self.fc1(x) return x def print_module_hierarchy(self): # Print tracked module names for module_name in self.module_names: print(module_name) # Example usage if __name__ == \\"__main__\\": net = SimpleNet() input_tensor = torch.randn(1, 3, 8, 8) output = net(input_tensor) net.print_module_hierarchy()"},{"question":"Objective: To assess the learners\' ability to effectively utilize various built-in functions in Python 3.10 to implement a moderately complex solution that involves data processing and transformation. Problem Statement: You are given a list of strings, where each string contains pairs of items separated by semi-colons. Each pair consists of a key and a value separated by a colon. Your task is to implement a function `process_data(data)` that transforms this input into a dictionary of lists. The keys of the dictionary will be the unique keys from all pairs and each key maps to a list containing all its corresponding values from all strings in the input list. The values in the list should be unique and sorted. Example: For the input: ```python data = [ \\"a:1;b:2;c:3\\", \\"a:4;c:5;a:1\\", \\"b:7;c:6;b:2\\" ] ``` The output should be: ```python { \'a\': [\'1\', \'4\'], \'b\': [\'2\', \'7\'], \'c\': [\'3\', \'5\', \'6\'] } ``` Function Signature: ```python def process_data(data: list) -> dict: pass ``` Constraints: - Each key-value pair within a string will be unique. - The keys and values are non-empty strings. - The input list contains at least one string. Performance Requirements: - Efficient handling of the input list and processing each string. - Proper usage of built-in functions such as `map()`, `filter()`, `set()`, `sorted()`, etc., to ensure the operation is both performant and readable. Example: ```python data = [ \\"k1:v1;k2:v2;k3:v3\\", \\"k1:v4;k2:v5\\", \\"k3:v6;k1:v1\\" ] assert process_data(data) == { \'k1\': [\'v1\', \'v4\'], \'k2\': [\'v2\', \'v5\'], \'k3\': [\'v3\', \'v6\'] } ``` Note: Remember to demonstrate your understanding of built-in functions and ensure that your solution is clean, efficient, and properly documented.","solution":"def process_data(data): Transforms a list of strings containing pairs of key-value items into a dictionary where each key maps to a unique sorted list of values. Parameters: data (list of str): A list of strings, where each string contains pairs of items separated by semicolons. Each pair consists of a key and a value separated by a colon. Returns: dict: A dictionary where each key maps to a list of unique, sorted values. result = {} for record in data: pairs = record.split(\\";\\") for pair in pairs: key, value = pair.split(\\":\\") if key not in result: result[key] = set() result[key].add(value) # Convert sets to sorted lists for key in result: result[key] = sorted(result[key]) return result"},{"question":"**Objective:** Demonstrate your understanding of the `wsgiref` module by creating a WSGI application with specific functionalities and validating it for WSGI compliance. # Problem Statement You are required to implement and serve a simple WSGI application that performs the following tasks: 1. Parses the path and query string from the request URL. 2. Returns a different response based on the parsed path. 3. Adds custom headers to the response using `wsgiref.headers`. 4. Includes a validation step to ensure the WSGI compliance of your application. # Task Breakdown 1. **WSGI Application:** - Create a function `my_wsgi_app(environ, start_response)` that serves as your WSGI application. - Parse the request path and query string using utilities from `wsgiref.util`. - If the path is `/hello`, the application should return a plain text response \\"Hello, World!\\". - If the path is `/goodbye`, the application should return a plain text response \\"Goodbye, World!\\". - For any other path, the application should return \\"404 Not Found\\". 2. **Response Headers:** - Use `wsgiref.headers.Headers` to manipulate and include custom headers in your response. For example, include a header `X-Custom-Header` with the value `MyCustomValue`. 3. **Validation:** - Wrap your WSGI application using `wsgiref.validate.validator` to ensure compliance with the WSGI specification. 4. **Serving the Application:** - Use `wsgiref.simple_server.make_server` to serve your WSGI application on `localhost` at port `8080`. - Ensure that the server runs indefinitely and handles incoming requests appropriately. # Specifications - **Input:** The WSGI application will receive HTTP requests with various paths. - **Output:** The WSGI application should return appropriate HTTP responses based on the request path. # Example ```python from wsgiref.util import setup_testing_defaults, request_uri from wsgiref.headers import Headers from wsgiref.validate import validator from wsgiref.simple_server import make_server def my_wsgi_app(environ, start_response): # Your implementation here # Return a response based on the request path pass # Wrap the application with validator validated_app = validator(my_wsgi_app) # Serve the application with make_server(\'\', 8080, validated_app) as httpd: print(\\"Serving on port 8080...\\") httpd.serve_forever() ``` # Constraints - You may assume that the request paths will only be `/hello`, `/goodbye`, or some other undefined path. - The `environ` dictionary in the WSGI application will contain all necessary CGI-style environment variables as per PEP 3333. # Performance Requirements - The server should handle multiple sequential requests efficiently. - Ensure that adding headers and parsing the request path does not introduce significant overhead. # Submission Submit the complete `my_wsgi_app` function along with the server setup code. Ensure your code is well-documented with comments explaining each step.","solution":"from wsgiref.util import setup_testing_defaults, request_uri from wsgiref.headers import Headers from wsgiref.validate import validator from wsgiref.simple_server import make_server def my_wsgi_app(environ, start_response): # Setup basic default environment variables needed for the request setup_testing_defaults(environ) # Get the request path path = environ[\'PATH_INFO\'] # Prepare the response body and status code based on the path if path == \'/hello\': status = \'200 OK\' response_body = \'Hello, World!\' elif path == \'/goodbye\': status = \'200 OK\' response_body = \'Goodbye, World!\' else: status = \'404 Not Found\' response_body = \'404 Not Found\' # Setup the response headers headers = Headers() headers.add_header(\'Content-Type\', \'text/plain\') headers.add_header(\'X-Custom-Header\', \'MyCustomValue\') # Start the response start_response(status, headers.items()) # Return the response body return [response_body.encode(\'utf-8\')] # Wrap the application with validator validated_app = validator(my_wsgi_app) # Serve the application if __name__ == \'__main__\': with make_server(\'\', 8080, validated_app) as httpd: print(\\"Serving on port 8080...\\") httpd.serve_forever()"},{"question":"# Custom Multi-Head Self-Attention Layer Implementation Objective You are required to implement a custom multi-head self-attention module using PyTorch. The aim of this exercise is to ensure you understand and can manually implement the core components of attention mechanisms using PyTorch, particularly leveraging the experimental features from `torch.nn.attention.experimental`. Task Implement a class `CustomMultiHeadSelfAttention` that performs multi-head self-attention on input data. Your class should inherit from `torch.nn.Module` and utilize the features available in `torch.nn.attention.experimental`. Specifications: 1. **Class Name**: `CustomMultiHeadSelfAttention` 2. **Inheritance**: `torch.nn.Module` 3. **Initialization Parameters**: - `embed_dim` (int): Dimension of the input embeddings. - `num_heads` (int): Number of attention heads. 4. **Methods**: - `forward(self, x: torch.Tensor) -> torch.Tensor`: The forward method that will perform the multi-head self-attention operation. Constraints - You must manage the experimental nature of the API and ensure compatibility with standard PyTorch tensor operations. - Your implementation should handle batched input data (i.e., support batching). Input and Output Formats - **Input**: - `x`: A 3D tensor of shape `(batch_size, seq_length, embed_dim)`, where `batch_size` is the number of sequences in a batch, `seq_length` is the length of each sequence, and `embed_dim` is the dimension of the embeddings. - **Output**: - A 3D tensor of shape `(batch_size, seq_length, embed_dim)` which represents the output from the multi-head self-attention mechanism applied to the input data. Example ```python import torch from torch.nn.attention.experimental import CustomMultiHeadSelfAttention # Define input tensor with shape (2, 5, 8) x = torch.rand(2, 5, 8) # Initialize the module with embedding dimension 8 and 2 attention heads attention_layer = CustomMultiHeadSelfAttention(embed_dim=8, num_heads=2) # Perform forward pass output = attention_layer(x) print(output.shape) # Expected output shape: (2, 5, 8) ``` Hint Carefully manage the split and concatenation of heads when computing multi-head attention to ensure the shapes and tensor operations are consistent. --- # Implementation Notes This question tests your ability to understand and implement a complex neural network component that is often used in state-of-the-art models. Ensure your implementation is efficient and leverages PyTorch\'s tensor operations effectively.","solution":"import torch import torch.nn as nn class CustomMultiHeadSelfAttention(nn.Module): def __init__(self, embed_dim, num_heads): super(CustomMultiHeadSelfAttention, self).__init__() self.embed_dim = embed_dim self.num_heads = num_heads self.head_dim = embed_dim // num_heads assert self.head_dim * num_heads == embed_dim, \\"embed_dim must be divisible by num_heads\\" self.q_proj = nn.Linear(embed_dim, embed_dim) self.k_proj = nn.Linear(embed_dim, embed_dim) self.v_proj = nn.Linear(embed_dim, embed_dim) self.out_proj = nn.Linear(embed_dim, embed_dim) self.scale = self.head_dim ** -0.5 def split_heads(self, x, batch_size): return x.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2) def forward(self, x): batch_size, seq_length, embed_dim = x.size() q = self.q_proj(x) k = self.k_proj(x) v = self.v_proj(x) q = self.split_heads(q, batch_size) k = self.split_heads(k, batch_size) v = self.split_heads(v, batch_size) # Scaled Dot-Product Attention attn_weights = torch.matmul(q, k.transpose(-1, -2)) * self.scale attn_weights = nn.Softmax(dim=-1)(attn_weights) attn_output = torch.matmul(attn_weights, v) # Concatenate heads attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_length, embed_dim) output = self.out_proj(attn_output) return output"},{"question":"**Question: WAV File Analysis and Conversion** You are provided with a set of WAV files, and your task is to analyze and convert these files based on certain criteria. Write a Python function that reads a WAV file, extracts specific information, and creates a modified version of the file under certain constraints. # Function Signature ```python def analyze_and_convert_wav(input_file_path: str, output_file_path: str) -> None: ``` # Input - `input_file_path`: A string representing the file path of the input WAV file (e.g., \\"input.wav\\"). - `output_file_path`: A string representing the file path where the modified WAV file should be saved (e.g., \\"output.wav\\"). # Output - The function should not return anything. It should create a new WAV file at `output_file_path` based on the specifications described below. # Requirements 1. **File Analysis**: The function should read the input WAV file and print the following properties: - Number of channels (e.g., mono or stereo). - Sample width in bytes. - Frame rate (sampling frequency). - Number of audio frames. - Compression type. 2. **File Conversion**: - If the input file is stereo (2 channels), convert it to mono by averaging the two channels. - Reduce the frame rate by half. - Save the resulting audio in a new WAV file at `output_file_path`. # Constraints - Ensure that the WAV file is in \\"WAVE_FORMAT_PCM\\" format. - Handle exceptions gracefully: If the input file is not a valid WAV file or does not meet the above requirements, print an appropriate error message. # Example ```python # Example usage: analyze_and_convert_wav(\\"input.wav\\", \\"output.wav\\") ``` - This function should read \\"input.wav\\", print its properties, and then create a mono version with half the original frame rate saved as \\"output.wav\\". # Notes - Use the `wave` module to read and write WAV files. - Pay attention to reading and writing frames correctly and in appropriate formats. **Hint**: Use `Wave_read` methods to extract information and `Wave_write` methods to set up the new WAV file with the desired properties.","solution":"import wave import numpy as np def analyze_and_convert_wav(input_file_path: str, output_file_path: str) -> None: try: with wave.open(input_file_path, \'rb\') as in_wav: # Extract and print the properties n_channels = in_wav.getnchannels() sample_width = in_wav.getsampwidth() frame_rate = in_wav.getframerate() n_frames = in_wav.getnframes() comp_type = in_wav.getcomptype() print(f\\"Number of channels: {n_channels}\\") print(f\\"Sample width (bytes): {sample_width}\\") print(f\\"Frame rate: {frame_rate}\\") print(f\\"Number of audio frames: {n_frames}\\") print(f\\"Compression type: {comp_type}\\") if comp_type != \'NONE\': print(f\\"Compression type {comp_type} is not supported.\\") return # Read frames frames = in_wav.readframes(n_frames) audio_data = np.frombuffer(frames, dtype=np.int16) if n_channels == 2: # If stereo, convert to mono audio_data = audio_data.reshape(-1, 2).mean(axis=1).astype(np.int16) # Reduce frame rate by half new_frame_rate = frame_rate // 2 # Write new wav file with modified data with wave.open(output_file_path, \'wb\') as out_wav: out_wav.setnchannels(1) out_wav.setsampwidth(sample_width) out_wav.setframerate(new_frame_rate) out_wav.writeframes(audio_data.tobytes()) except wave.Error as e: print(f\\"Error processing WAV file: {e}\\") except Exception as e: print(f\\"An error occurred: {e}\\")"},{"question":"Objective Write a Python function `list_python_files(root_dir)` that takes a directory path as input and returns a list of all Python files (`*.py`) recursively from that directory and its subdirectories. The function should use the `pathlib` module. Function Signature ```python from pathlib import Path from typing import List def list_python_files(root_dir: str) -> List[str]: pass ``` Input - `root_dir`: A string representing the path to the root directory that needs to be searched. Output - A list of strings where each string is the relative path to a Python file from the root directory. Constraints - The function should handle errors gracefully, such as the root directory not existing. - Use the `pathlib.Path` class provided by the `pathlib` module to manipulate paths and perform the required operations. - The function should only return paths of files that end with a `.py` extension. Example ```python # Assuming the following file structure: # root_dir/ # ├── file1.py # ├── file2.txt # └── dir1/ # ├── file3.py # └── file4.ipynb # For the root directory path \'root_dir\', the function should return: # [\'file1.py\', \'dir1/file3.py\'] files_list = list_python_files(\'root_dir\') print(files_list) # Output: [\'file1.py\', \'dir1/file3.py\'] ``` Additional Information - You may use the `Path.glob()` or `Path.rglob()` methods to list files recursively. - Ensure that the order of the returned list is according to the natural order of the directories and files. - Do not include hidden files (files starting with a dot).","solution":"from pathlib import Path from typing import List def list_python_files(root_dir: str) -> List[str]: try: root_path = Path(root_dir) if not root_path.is_dir(): return [] python_files = [] for file_path in root_path.rglob(\\"*.py\\"): if not any(part.startswith(\'.\') for part in file_path.parts): python_files.append(str(file_path.relative_to(root_dir))) return python_files except Exception as e: return []"},{"question":"Advanced Subprocess Management Objective: Demonstrate your understanding of the `subprocess` module by developing a Python script that meets specific criteria for managing subprocesses and handling their input/output. Problem Statement: You are required to write a function `execute_commands` that accepts a list of shell commands, executes each command using `subprocess.Popen`, captures their output (stdout and stderr), and returns a dictionary with the command as the key and its corresponding result as the value. Function Signature: ```python def execute_commands(commands: list) -> dict: pass ``` Input: - `commands` (list): A list of strings where each string is a shell command to be executed. Example: `[\\"ls -l\\", \\"echo Hello World\\", \\"cat non_existent_file\\"]` Output: - Returns a dictionary where each key is a command string and the value is a dictionary with the following keys: - `stdout`: Captured standard output of the command (as a string). - `stderr`: Captured standard error of the command (as a string, may be empty if no errors occurred). - `returncode`: The return code of the command execution (0 for success, non-zero for failure). Constraints: - Each command should be executed in a new subprocess. - The function should properly capture and handle both stdout and stderr. - Ensure appropriate exception handling for scenarios such as timeouts, command failures, and non-zero return codes. Performance Requirements: - The function should handle multiple commands efficiently and should ensure that subprocesses do not cause deadlocks. Example: ```python commands = [\\"ls -l\\", \\"echo Hello World\\", \\"cat non_existent_file\\"] result = execute_commands(commands) print(result) ``` Expected Output: ```python { \\"ls -l\\": { \\"stdout\\": \\"<stdout from \'ls -l\'>\\", \\"stderr\\": \\"\\", \\"returncode\\": 0 }, \\"echo Hello World\\": { \\"stdout\\": \\"Hello Worldn\\", \\"stderr\\": \\"\\", \\"returncode\\": 0 }, \\"cat non_existent_file\\": { \\"stdout\\": \\"\\", \\"stderr\\": \\"cat: non_existent_file: No such file or directoryn\\", \\"returncode\\": 1 } } ``` Notes: - Ensure you use the appropriate methods and constructors from the `subprocess` module to achieve the desired functionality. - Prioritize security and handle subprocess creation safely, avoiding shell injections whenever possible. - Make sure to capture and report both stdout and stderr accurately in the returned dictionary.","solution":"import subprocess def execute_commands(commands: list) -> dict: results = {} for command in commands: try: # Execute the command process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True, text=True) stdout, stderr = process.communicate() results[command] = { \\"stdout\\": stdout, \\"stderr\\": stderr, \\"returncode\\": process.returncode } except Exception as e: # Handle exceptions and add to results results[command] = { \\"stdout\\": \\"\\", \\"stderr\\": str(e), \\"returncode\\": -1 } return results"},{"question":"You are given a dataset with highly imbalanced classes where false negatives are critical. Imagine this is a medical dataset where you predict the presence of a disease, and missing a positive case is far more severe than a false positive. # Task: 1. Load the dataset and split it into a training set and a testing set. 2. Train a classifier using `LogisticRegression`. 3. Implement the `TunedThresholdClassifierCV` to tune the decision threshold, optimizing for the `recall` metric. 4. Evaluate the classifier with the tuned threshold using the testing set. 5. Compare the performance with the default threshold (0.5). # Input: - You will use the `make_classification` function to generate the dataset with the following parameters: `n_samples=1000`, `weights=[0.05, 0.95]`, `random_state=42`. - Use an 80-20 split for train-test split of the data. # Output: - Print the recall score and threshold for both the default and tuned models. - Print the confusion matrix for both models to show the comparison in classification outcomes. # Constraints: - Ensure cross-validation does not overfit. - Use appropriate scikit-learn tools as discussed in the documentation. # Example: ```python from sklearn.datasets import make_classification from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression from sklearn.metrics import recall_score, confusion_matrix from sklearn.model_selection import TunedThresholdClassifierCV # Step 1: Generate dataset X, y = make_classification(n_samples=1000, weights=[0.05, 0.95], random_state=42) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Step 2: Train the classifier model = LogisticRegression(solver=\'liblinear\') model.fit(X_train, y_train) # Step 3: Implement TunedThresholdClassifierCV from sklearn.metrics import make_scorer scorer = make_scorer(recall_score, pos_label=1) tuned_model = TunedThresholdClassifierCV(model, scoring=scorer) tuned_model.fit(X_train, y_train) # Step 4: Evaluate the classifier y_pred_default = model.predict(X_test) y_pred_tuned = tuned_model.predict(X_test) # Default threshold metrics print(\\"Default Model Recall:\\", recall_score(y_test, y_pred_default)) print(\\"Default Model Confusion Matrix:n\\", confusion_matrix(y_test, y_pred_default)) # Tuned threshold metrics print(\\"Tuned Model Recall:\\", recall_score(y_test, y_pred_tuned)) print(\\"Tuned Model Confusion Matrix:n\\", confusion_matrix(y_test, y_pred_tuned)) print(\\"Tuned Model Best Threshold:\\", tuned_model.best_threshold_) ``` # Notes: - Thoroughly explain each step and why it is necessary. - Identify any assumptions or interpretations made.","solution":"from sklearn.datasets import make_classification from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression from sklearn.metrics import recall_score, confusion_matrix from sklearn.model_selection import cross_val_predict from sklearn.metrics import precision_recall_curve import numpy as np def tuned_threshold_classifier_cv(model, X_train, y_train, scoring): Tune the decision threshold using cross-validated predictions to optimize for the recall metric. y_scores = cross_val_predict(model, X_train, y_train, cv=5, method=\'decision_function\') precision, recall, thresholds = precision_recall_curve(y_train, y_scores) f1_scores = 2 * (precision * recall) / (precision + recall) best_index = np.argmax(f1_scores) best_threshold = thresholds[best_index] return best_threshold # Step 1: Generate dataset X, y = make_classification(n_samples=1000, weights=[0.05, 0.95], random_state=42) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Step 2: Train the classifier model = LogisticRegression(solver=\'liblinear\') model.fit(X_train, y_train) # Step 3: Implement tuning of decision threshold best_threshold = tuned_threshold_classifier_cv(model, X_train, y_train, scoring=recall_score) # Step 4: Evaluate the classifier with the tuned threshold y_scores = model.decision_function(X_test) y_pred_tuned = (y_scores >= best_threshold).astype(int) y_pred_default = model.predict(X_test) # Default threshold metrics default_recall = recall_score(y_test, y_pred_default) default_confusion_matrix = confusion_matrix(y_test, y_pred_default) # Tuned threshold metrics tuned_recall = recall_score(y_test, y_pred_tuned) tuned_confusion_matrix = confusion_matrix(y_test, y_pred_tuned) print(\\"Default Model Recall:\\", default_recall) print(\\"Default Model Confusion Matrix:n\\", default_confusion_matrix) print(\\"Tuned Model Recall:\\", tuned_recall) print(\\"Tuned Model Confusion Matrix:n\\", tuned_confusion_matrix) print(\\"Tuned Model Best Threshold:\\", best_threshold)"},{"question":"# Python310 Coding Assessment Question Problem Statement You are tasked with creating a Python function that mimics the behavior of some of the manifest template commands provided with the `sdist` command. Your function should take a list of filenames and a list of commands, then determine which files should be included in the final distribution based on the given commands. Your task is to implement a function `filter_files(filenames: List[str], commands: List[str]) -> List[str]` with the following requirements: Input - `filenames`: A list of strings representing filenames. - `commands`: A list of strings representing the commands mentioned in the documentation (`include`, `exclude`, `recursive-include`, `recursive-exclude`, `global-include`, `global-exclude`, `prune`, and `graft`). Output - A list of strings representing the filenames that should be included in the final distribution. Constraints - All filenames will be relative paths using Unix-style forward slashes (e.g., \\"dir/subdir/file.txt\\"). - Commands should be processed in the order they appear in the list. - The function should correctly handle nested directories based on the provided commands. Example ```python def filter_files(filenames: List[str], commands: List[str]) -> List[str]: # Your implementation here pass # Example usage filenames = [ \\"dir1/file1.txt\\", \\"dir1/file2.txt\\", \\"dir2/subdir/file3.txt\\", \\"dir2/subdir/file4.txt\\", \\"file5.txt\\" ] commands = [ \\"include *.txt\\", \\"exclude dir2/subdir/file3.txt\\", \\"recursive-include dir1 *.txt\\", \\"graft dir2\\" ] print(filter_files(filenames, commands)) ``` Expected Output: ```python [ \\"dir1/file1.txt\\", \\"dir1/file2.txt\\", \\"dir2/subdir/file4.txt\\", \\"file5.txt\\" ] ``` Notes - An `include` command adds all matching files to the distribution. - An `exclude` command removes matching files that were previously included. - `recursive-include` adds all matching files within a specified directory and its subdirectories. - `recursive-exclude` removes all matching files within a specified directory and its subdirectories if they were previously included. - `global-include` and `global-exclude` operate over the entire file list. - `prune` removes all files under the specified directory. - `graft` adds all files under the specified directory. **Hint:** You may find the `fnmatch` module useful for matching Unix-style glob patterns.","solution":"from typing import List import fnmatch def filter_files(filenames: List[str], commands: List[str]) -> List[str]: included_files = set() for command in commands: parts = command.split() action = parts[0] pattern = parts[1] if len(parts) > 1 else \\"\\" if action == \\"include\\": included_files.update(fnmatch.filter(filenames, pattern)) elif action == \\"exclude\\": for file in fnmatch.filter(filenames, pattern): included_files.discard(file) elif action == \\"recursive-include\\": directory = parts[1] pattern = parts[2] for file in filenames: if file.startswith(directory) and fnmatch.fnmatch(file[len(directory)+1:], pattern): included_files.add(file) elif action == \\"recursive-exclude\\": directory = parts[1] pattern = parts[2] for file in filenames: if file.startswith(directory) and fnmatch.fnmatch(file[len(directory)+1:], pattern): included_files.discard(file) elif action == \\"global-include\\": included_files.update(fnmatch.filter(filenames, pattern)) elif action == \\"global-exclude\\": for file in fnmatch.filter(filenames, pattern): included_files.discard(file) elif action == \\"prune\\": directory = pattern for file in filenames: if file.startswith(directory): included_files.discard(file) elif action == \\"graft\\": directory = pattern for file in filenames: if file.startswith(directory): included_files.add(file) return sorted(included_files)"},{"question":"Objective: Implement a function that takes a list of URLs and opens each URL in a new tab of the specified browser. If the specified browser is not available or not supported, it should open the URLs in the default browser. Function Signature: ```python def open_urls_in_browser(urls: list, browser_name: str) -> bool: pass ``` Input: - `urls` (list): A list of URLs (strings) to be opened. - `browser_name` (str): The name of the browser in which the URLs should be opened. Possible values include `\'mozilla\'`, `\'firefox\'`, `\'netscape\'`, `\'galeon\'`, `\'epiphany\'`, `\'skipstone\'`, `\'kfmclient\'`, `\'konqueror\'`, `\'kfm\'`, `\'mosaic\'`, `\'opera\'`, `\'grail\'`, `\'links\'`, `\'elinks\'`, `\'lynx\'`, `\'w3m\'`, `\'windows-default\'`, `\'macosx\'`, `\'safari\'`, `\'google-chrome\'`, `\'chrome\'`, `\'chromium\'`, `\'chromium-browser\'`. Output: - Returns `True` if all URLs were successfully opened in the specified browser or the default browser if the specified one wasn\'t available. - Returns `False` if none of the URLs could be opened. Constraints: - You should properly handle any errors that might occur during the process, such as invalid URLs or unavailable browsers. - Ensure that the code works cross-platform (Windows, macOS, Unix/Linux). Example: ```python urls = [\'https://www.python.org\', \'https://www.github.com\'] browser_name = \'firefox\' assert open_urls_in_browser(urls, browser_name) == True ``` Notes: 1. Make use of the `webbrowser` module functions provided in the documentation. 2. The function should return a boolean indicating whether the operation was fully successful or not. 3. You may need to import additional modules for error handling and validation.","solution":"import webbrowser def open_urls_in_browser(urls: list, browser_name: str) -> bool: Opens a list of URLs in the specified browser. If the browser is not available, the function will attempt to open the URLs in the default browser. Parameters: urls (list): A list of URLs to be opened. browser_name (str): The name of the browser to use. Returns: bool: True if all URLs were successfully opened, False otherwise. if not urls: return False try: browser = webbrowser.get(browser_name) except webbrowser.Error: browser = webbrowser.get() # Default browser all_opened = True for url in urls: try: browser.open_new_tab(url) except Exception: all_opened = False return all_opened"},{"question":"**Question: Implement a Quoted-Printable Encoder/Decoder Using the `quopri` Module** You are tasked with creating a Python class `QuotedPrintableHandler` that provides functionality to encode and decode quoted-printable data using the methods available in the `quopri` module. Your task is to implement this class with the following methods: # Class Definition ```python class QuotedPrintableHandler: def __init__(self): pass def encode_file(self, input_file_path: str, output_file_path: str, quotetabs: bool = False, header: bool = False) -> None: pass def decode_file(self, input_file_path: str, output_file_path: str, header: bool = False) -> None: pass def encode_string(self, data: bytes, quotetabs: bool = False, header: bool = False) -> bytes: pass def decode_string(self, data: bytes, header: bool = False) -> bytes: pass ``` # Requirements 1. **`encode_file(input_file_path: str, output_file_path: str, quotetabs: bool = False, header: bool = False) -> None`**: - Reads binary data from `input_file_path`. - Encodes the data using quoted-printable encoding. - Writes the encoded data to `output_file_path`. - Ensure spaces and tabs are handled based on `quotetabs` parameter. Handle headers if the `header` parameter is set to `True`. 2. **`decode_file(input_file_path: str, output_file_path: str, header: bool = False) -> None`**: - Reads binary data from `input_file_path`. - Decodes the data from quoted-printable encoding. - Writes the decoded data to `output_file_path`. - Handle headers if the `header` parameter is set to `True`. 3. **`encode_string(data: bytes, quotetabs: bool = False, header: bool = False) -> bytes`**: - Encodes the given `data` using quoted-printable encoding. - Returns the encoded data as bytes. - Ensure spaces and tabs are handled based on `quotetabs` parameter. Handle headers if the `header` parameter is set to `True`. 4. **`decode_string(data: bytes, header: bool = False) -> bytes`**: - Decodes the given `data` from quoted-printable encoding. - Returns the decoded data as bytes. - Handle headers if the `header` parameter is set to `True`. # Constraints - All file handling should be done in binary mode. - Proper error handling should be implemented for file operations. - Ensure the methods are efficient in terms of time and space complexity. # Example Usage ```python handler = QuotedPrintableHandler() # Encode and decode files handler.encode_file(\'input.bin\', \'encoded.bin\', quotetabs=True, header=True) handler.decode_file(\'encoded.bin\', \'decoded.bin\', header=True) # Encode and decode strings encoded_data = handler.encode_string(b\'Sample data with special characters: !@#%^&*()\', quotetabs=True, header=True) decoded_data = handler.decode_string(encoded_data, header=True) print(encoded_data) print(decoded_data) ``` Your implementation should demonstrate your understanding of file handling, bytes operations, and the use of the `quopri` module.","solution":"import quopri class QuotedPrintableHandler: def __init__(self): pass def encode_file(self, input_file_path: str, output_file_path: str, quotetabs: bool = False, header: bool = False) -> None: try: with open(input_file_path, \'rb\') as input_file: data = input_file.read() encoded_data = quopri.encodestring(data, quotetabs=quotetabs, header=header) with open(output_file_path, \'wb\') as output_file: output_file.write(encoded_data) except IOError as e: raise e def decode_file(self, input_file_path: str, output_file_path: str, header: bool = False) -> None: try: with open(input_file_path, \'rb\') as input_file: data = input_file.read() decoded_data = quopri.decodestring(data, header=header) with open(output_file_path, \'wb\') as output_file: output_file.write(decoded_data) except IOError as e: raise e def encode_string(self, data: bytes, quotetabs: bool = False, header: bool = False) -> bytes: return quopri.encodestring(data, quotetabs=quotetabs, header=header) def decode_string(self, data: bytes, header: bool = False) -> bytes: return quopri.decodestring(data, header=header)"},{"question":"**Objective:** Implement a Python function that demonstrates the understanding and functionality of instance method objects and method objects as described in the provided documentation. Implement the following function: ```python def method_objects_demo(py_func, instance): Demonstrates the creation and manipulation of instance and regular method objects. Parameters: py_func (callable): A simple Python function to be wrapped as a method. instance (object): An instance of a user-defined class. Returns: dict: A dictionary containing: - \'instance_method\': A new instance method object created with py_func. - \'instance_method_func\': The function object retrieved from the instance method. - \'method\': A new method object created with py_func and instance. - \'method_func\': The function object retrieved from the method object. - \'method_self\': The instance object associated with the method object. # Your code here return { \'instance_method\': instance_method, \'instance_method_func\': instance_method_func, \'method\': method, \'method_func\': method_func, \'method_self\': method_self } ``` # Requirements: 1. **Create an instance method object** (`instance_method`) using `py_func`. 2. **Retrieve the function** object (`instance_method_func`) from the `instance_method`. 3. **Create a method object** (`method`) using `py_func` and `instance`. 4. **Retrieve the function** object (`method_func`) from the `method` object. 5. **Retrieve the instance** object (`method_self`) from the `method` object. # Example Usage: ```python def sample_function(): return \\"Hello, world!\\" class MyClass: pass instance = MyClass() result = method_objects_demo(sample_function, instance) print(result[\'instance_method\']) # <method object> print(result[\'instance_method_func\']) # <function sample_function> print(result[\'method\']) # <method object> print(result[\'method_func\']) # <function sample_function> print(result[\'method_self\']) # <MyClass instance> ``` # Notes: - Ensure to handle and check types correctly. - Use the provided document\'s functions for creating and manipulating method objects.","solution":"import types def method_objects_demo(py_func, instance): Demonstrates the creation and manipulation of instance and regular method objects. Parameters: py_func (callable): A simple Python function to be wrapped as a method. instance (object): An instance of a user-defined class. Returns: dict: A dictionary containing: - \'instance_method\': A new instance method object created with py_func. - \'instance_method_func\': The function object retrieved from the instance method. - \'method\': A new method object created with py_func and instance. - \'method_func\': The function object retrieved from the method object. - \'method_self\': The instance object associated with the method object. # Create an instance method object using py_func instance_method = types.MethodType(py_func, instance) # Retrieve the function object from the instance method instance_method_func = instance_method.__func__ # Create a method object using py_func and instance method = types.MethodType(py_func, instance) # Retrieve the function object from the method object method_func = method.__func__ # Retrieve the instance object from the method object method_self = method.__self__ return { \'instance_method\': instance_method, \'instance_method_func\': instance_method_func, \'method\': method, \'method_func\': method_func, \'method_self\': method_self }"},{"question":"Objective: Create a multi-plot grid using the `seaborn` library that demonstrates your understanding of advanced facets customization and plotting techniques. Task: Write a Python function that takes a `pandas` DataFrame and generates a complex multi-plot grid using `seaborn.FacetGrid`. Your function should: 1. Create a `FacetGrid` with different subplots for each combination of day and time from the \'tips\' dataset. 2. Plot a histogram of `total_bill` and include a reference line for the median value of `total_bill` on each subplot. 3. Use different colors for different `sex` categories in the dataset. 4. Annotate each subplot with the total count of entries (`n`) from the respective subset of data. 5. Ensure all subplots have the same axis limits and aspect ratio. 6. Save the generated plot to a file named `\\"complex_facet_plot.png\\"`. Input: - `dataframe`: A `pandas` DataFrame containing at least the following columns: `total_bill`, `tip`, `time`, `sex`, and `day`. Output: - Save the generated multi-plot grid to a file named `\\"complex_facet_plot.png\\"`. Constraints: - You must use the specified seaborn methods and demonstrate your understanding of seaborn\'s `FacetGrid` functionalities. - Ensure clear, readable, and well-commented code. Example function signature: ```python def create_complex_facet_plot(dataframe): pass ``` Notes: - Make sure you handle the required imports. - Assume the input DataFrame follows the same structure as the `tips` dataset from seaborn.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt import numpy as np def create_complex_facet_plot(dataframe): # Creating the FacetGrid object for different combinations of day and time g = sns.FacetGrid(dataframe, col=\\"day\\", row=\\"time\\", hue=\\"sex\\", margin_titles=True) # Function to plot histograms and reference lines def plot_histogram_with_median(data, color, **kwargs): sns.histplot(data[\\"total_bill\\"], color=color, **kwargs) plt.axvline(data[\\"total_bill\\"].median(), color=\'black\', linestyle=\'--\') plt.text(data[\\"total_bill\\"].median(), plt.ylim()[1]*0.9, f\'Median: {data[\\"total_bill\\"].median():.2f}\', color=\'black\', fontsize=10) # Annotate with the total count of entries count = len(data) plt.text(0.5, 0.5, f\'n={count}\', horizontalalignment=\'center\', verticalalignment=\'center\', transform=plt.gca().transAxes, fontsize=12) # Plotting the data g.map_dataframe(plot_histogram_with_median, bins=10) # Adding legend g.add_legend() # Adjust axis limits and aspect ratio g.set(xlim=(0, dataframe[\\"total_bill\\"].max())) g.fig.tight_layout() # Save the plot to a file plt.savefig(\\"complex_facet_plot.png\\")"},{"question":"Objective: To assess your understanding of memoryview objects in Python, you will implement a function that handles memoryview creation, modification, and inspection. Problem Statement: You need to implement a function `memoryview_operations` that performs the following actions: 1. **Create a memoryview** from a given bytearray. 2. **Modify the memoryview** by adding a specific value to each element of the bytearray. 3. **Inspect the memoryview** to check whether it is read-only or read/write. 4. **Return the updated bytearray** and the read/write status as a tuple. Function Signature: ```python def memoryview_operations(data: bytearray, value_to_add: int) -> tuple: pass ``` Input: - `data` (bytearray): A bytearray object containing the initial data. - `value_to_add` (int): An integer value to be added to each element of the bytearray. Output: - A tuple `(updated_bytearray, is_read_write)` where: - `updated_bytearray` is the modified bytearray after adding `value_to_add` to each element. - `is_read_write` is a boolean indicating if the memoryview is read/write (`True`) or read-only (`False`). Constraints: - You may assume that `data` contains only non-negative integers less than 256. - `value_to_add` will be between 0 and 255. - The memoryview should allow modification of the underlying data. Example: ```python # Example 1: data = bytearray([1, 2, 3]) value_to_add = 4 result = memoryview_operations(data, value_to_add) # Expected Output: (bytearray([5, 6, 7]), True) # Example 2: data = bytearray([255, 0, 1]) value_to_add = 1 result = memoryview_operations(data, value_to_add) # Expected Output: (bytearray([0, 1, 2]), True) ``` Notes: - Your implementation should handle the case where adding `value_to_add` exceeds 255 by wrapping around using modulo operation. - Ensure that the memoryview you create is writable. - You are expected to use the `memoryview` function provided by the Python standard library.","solution":"def memoryview_operations(data: bytearray, value_to_add: int) -> tuple: Performs operations on a memoryview of the given bytearray. Args: data (bytearray): A bytearray object containing the initial data. value_to_add (int): An integer value to be added to each element of the bytearray. Returns: tuple: (updated_bytearray, is_read_write) where updated_bytearray is the modified bytearray and is_read_write is a boolean indicating if the memoryview is read/write (True) or read-only (False). # Create a memoryview from the bytearray mv = memoryview(data) # Modify the memoryview for i in range(len(mv)): mv[i] = (mv[i] + value_to_add) % 256 # Inspect the memoryview is_read_write = not mv.readonly # Return the updated bytearray and the read/write status return (data, is_read_write)"},{"question":"# Custom Causal Attention Mechanism In this assessment, you are required to implement a custom attention mechanism that applies various causal biases to the attention scores. Part 1: Implement the `apply_causal_bias` Function Create a function `apply_causal_bias` which applies a specified causal bias to an attention score tensor. # Function Signature: ```python def apply_causal_bias(attention_scores: torch.Tensor, bias_type: str) -> torch.Tensor: Apply a specified causal bias on the attention score tensor. Parameters: - attention_scores (torch.Tensor): A tensor of shape (batch_size, seq_len, seq_len) representing attention scores. - bias_type (str): Type of causal bias to apply. Can be \'upper_left\' or \'lower_right\'. Returns: - torch.Tensor: The attention score tensor after applying the causal bias. ``` # Constraints: - The input `attention_scores` tensor will always be 3-dimensional. - The `bias_type` will only be \'upper_left\' or \'lower_right\'. # Details: - Use `torch.nn.attention.bias.causal_upper_left` and `torch.nn.attention.bias.causal_lower_right` to apply the respective biases based on the `bias_type`. Part 2: Create a Custom Attention Layer Using the `apply_causal_bias` function, implement a custom Attention layer that can be used in a neural network. # Class Signature: ```python import torch.nn as nn class CustomCausalAttention(nn.Module): def __init__(self, embed_dim: int): Initialize the CustomCausalAttention layer. Parameters: - embed_dim (int): The dimension of the embedding space. super(CustomCausalAttention, self).__init__() self.q_linear = nn.Linear(embed_dim, embed_dim) self.k_linear = nn.Linear(embed_dim, embed_dim) self.v_linear = nn.Linear(embed_dim, embed_dim) self.out_linear = nn.Linear(embed_dim, embed_dim) def forward(self, x: torch.Tensor, bias_type: str): Forward pass for the custom causal attention layer. Parameters: - x (torch.Tensor): Input tensor of shape (batch_size, seq_len, embed_dim) - bias_type (str): Type of causal bias to apply. Can be \'upper_left\' or \'lower_right\'. Returns: - torch.Tensor: Output tensor after applying self-attention. batch_size, seq_len, embed_dim = x.size() Q = self.q_linear(x) K = self.k_linear(x) V = self.v_linear(x) attention_scores = torch.bmm(Q, K.transpose(1, 2)) / (embed_dim**0.5) attention_scores = apply_causal_bias(attention_scores, bias_type) attention_weights = torch.softmax(attention_scores, dim=-1) output = torch.bmm(attention_weights, V) return self.out_linear(output) ``` # Input and Output: - The input tensor `x` will have the shape `(batch_size, seq_len, embed_dim)`. - The output tensor should have the same shape as the input tensor after applying the causal attention mechanism. # Task: 1. Implement the `apply_causal_bias` function. 2. Implement the `CustomCausalAttention` class and ensure that causal bias is correctly applied within the forward pass. Example Usage: ```python batch_size = 2 seq_len = 3 embed_dim = 4 x = torch.rand(batch_size, seq_len, embed_dim) attention_layer = CustomCausalAttention(embed_dim) output = attention_layer(x, \'upper_left\') print(output.shape) # Expected: torch.Size([2, 3, 4]) ``` Make sure to test your implementation with various input sizes and bias types to validate the correctness and robustness of your solution.","solution":"import torch import torch.nn as nn import torch.nn.functional as F def apply_causal_bias(attention_scores: torch.Tensor, bias_type: str) -> torch.Tensor: Apply a specified causal bias on the attention score tensor. Parameters: - attention_scores (torch.Tensor): A tensor of shape (batch_size, seq_len, seq_len) representing attention scores. - bias_type (str): Type of causal bias to apply. Can be \'upper_left\' or \'lower_right\'. Returns: - torch.Tensor: The attention score tensor after applying the causal bias. assert bias_type in [\'upper_left\', \'lower_right\'], \\"bias_type must be \'upper_left\' or \'lower_right\'\\" batch_size, seq_len, _ = attention_scores.size() if bias_type == \'upper_left\': mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool() elif bias_type == \'lower_right\': mask = torch.tril(torch.ones(seq_len, seq_len), diagonal=-1).bool() mask = mask.to(attention_scores.device) attention_scores = attention_scores.masked_fill(mask, float(\'-inf\')) return attention_scores class CustomCausalAttention(nn.Module): def __init__(self, embed_dim: int): Initialize the CustomCausalAttention layer. Parameters: - embed_dim (int): The dimension of the embedding space. super(CustomCausalAttention, self).__init__() self.q_linear = nn.Linear(embed_dim, embed_dim) self.k_linear = nn.Linear(embed_dim, embed_dim) self.v_linear = nn.Linear(embed_dim, embed_dim) self.out_linear = nn.Linear(embed_dim, embed_dim) def forward(self, x: torch.Tensor, bias_type: str): Forward pass for the custom causal attention layer. Parameters: - x (torch.Tensor): Input tensor of shape (batch_size, seq_len, embed_dim) - bias_type (str): Type of causal bias to apply. Can be \'upper_left\' or \'lower_right\'. Returns: - torch.Tensor: Output tensor after applying self-attention. batch_size, seq_len, embed_dim = x.size() Q = self.q_linear(x) K = self.k_linear(x) V = self.v_linear(x) attention_scores = torch.bmm(Q, K.transpose(1, 2)) / (embed_dim ** 0.5) attention_scores = apply_causal_bias(attention_scores, bias_type) attention_weights = F.softmax(attention_scores, dim=-1) output = torch.bmm(attention_weights, V) return self.out_linear(output)"},{"question":"Objective: Write a Python script to manage and maintain compressed log files using the `bz2` module. Your task is to create functions that cover file compression/decompression, handling incremental data streams, and ensuring data integrity. Requirements: 1. **Function 1: `compress_logs(input_file: str, output_file: str, compresslevel: int = 9) -> None`** - **Description**: Reads a text file containing log data, compresses the content using the specified compression level, and writes the compressed data to the output file. - **Input**: - `input_file`: Path to the input text file containing log data. - `output_file`: Path to the output compressed file. - `compresslevel`: (Optional) Compression level (1-9), default is 9. - **Output**: None. - **Constraints**: The input file must exist and be a valid text file. 2. **Function 2: `decompress_logs(input_file: str, output_file: str) -> None`** - **Description**: Reads a compressed file, decompresses the data, and writes the decompressed content to the output file. - **Input**: - `input_file`: Path to the input compressed file. - `output_file`: Path to the output text file. - **Output**: None. - **Constraints**: The input file must be a valid bzip2-compressed file. 3. **Function 3: `check_file_integrity(original_file: str, decompressed_file: str) -> bool`** - **Description**: Compares the content of the original text file with the decompressed file to verify data integrity. - **Input**: - `original_file`: Path to the original text file. - `decompressed_file`: Path to the decompressed text file. - **Output**: Returns `True` if the files\' contents match, otherwise `False`. - **Constraints**: None. Example Usage: ```python # Assuming \'logs.txt\' is the original log file # Compress the log file compress_logs(\'logs.txt\', \'logs_compressed.bz2\') # Decompress the log file decompress_logs(\'logs_compressed.bz2\', \'logs_decompressed.txt\') # Check file integrity is_valid = check_file_integrity(\'logs.txt\', \'logs_decompressed.txt\') print(f\\"Integrity check passed: {is_valid}\\") # Output: Integrity check passed: True ``` Performance Requirements: - Ensure efficient file reading and writing operations. - Maintain low memory usage, especially for large files. Constraints and Limitations: - Proper error handling should be implemented to manage file read/write exceptions. - Ensure the functions handle different compression levels appropriately. **Notes**: - Use the `bz2` module\'s capabilities effectively. - Write clean, maintainable, and well-documented code. - You are encouraged to include additional helper functions if necessary.","solution":"import bz2 import os def compress_logs(input_file: str, output_file: str, compresslevel: int = 9) -> None: Reads a text file containing log data, compresses the content using the specified compression level, and writes the compressed data to the output file. :param input_file: Path to the input text file containing log data. :param output_file: Path to the output compressed file. :param compresslevel: (Optional) Compression level (1-9), default is 9. if not os.path.exists(input_file): raise FileNotFoundError(f\\"The input file {input_file} does not exist.\\") with open(input_file, \'rb\') as f_in, bz2.BZ2File(output_file, \'wb\', compresslevel=compresslevel) as f_out: for data in iter(lambda: f_in.read(100 * 1024), b\'\'): f_out.write(data) def decompress_logs(input_file: str, output_file: str) -> None: Reads a compressed file, decompresses the data, and writes the decompressed content to the output file. :param input_file: Path to the input compressed file. :param output_file: Path to the output text file. if not os.path.exists(input_file): raise FileNotFoundError(f\\"The input file {input_file} does not exist.\\") with bz2.BZ2File(input_file, \'rb\') as f_in, open(output_file, \'wb\') as f_out: for data in iter(lambda: f_in.read(100 * 1024), b\'\'): f_out.write(data) def check_file_integrity(original_file: str, decompressed_file: str) -> bool: Compares the content of the original text file with the decompressed file to verify data integrity. :param original_file: Path to the original text file. :param decompressed_file: Path to the decompressed text file. :return: Returns True if the files\' contents match, otherwise False. with open(original_file, \'rb\') as f1, open(decompressed_file, \'rb\') as f2: while True: chunk1 = f1.read(100 * 1024) chunk2 = f2.read(100 * 1024) if chunk1 != chunk2: return False if not chunk1: # End of both files return True"},{"question":"# Pandas Series Manipulation and Analysis You are given a dataset of students\' grades in different subjects stored in a pandas Series. Each element in the Series represents the grade of a student in a specific subject, and the index represents the student\'s ID and subject in the format `StudentID_Subject`. Your task is to perform the following operations using pandas Series methods. Tasks: 1. **Load Data**: Write a function `load_data` that takes a dictionary as input and converts it into a pandas Series. ```python def load_data(data: dict) -> pd.Series: pass ``` 2. **Handle Missing Data**: Write a function `handle_missing_data` that takes a Series as input and fills missing values with the mean grade for that subject. ```python def handle_missing_data(grades: pd.Series) -> pd.Series: pass ``` 3. **Compute Statistics**: Write a function `compute_statistics` that takes a Series and computes the mean, median, and standard deviation of the grades for each subject. Return these statistics as a DataFrame with subjects as rows and statistics as columns. ```python def compute_statistics(grades: pd.Series) -> pd.DataFrame: pass ``` 4. **Restructure Data**: Write a function `reshape_data` that converts the Series into a DataFrame with `StudentID` as rows and `Subject` as columns, where each cell contains the corresponding grade. ```python def reshape_data(grades: pd.Series) -> pd.DataFrame: pass ``` 5. **Identify Top Performers**: Write a function `identify_top_performers` that takes the reshaped DataFrame and returns the top 3 students with the highest average grades. ```python def identify_top_performers(grades_df: pd.DataFrame) -> pd.Series: pass ``` Input: - A dictionary where keys are strings in the format `StudentID_Subject` and values are either integers (grades) or `NaN`. Output: - For each function, return the respective processed Series or DataFrame as specified. Example: ```python data = { \\"1_Math\\": 88, \\"1_Science\\": 92, \\"2_Math\\": 79, \\"2_Science\\": np.nan, \\"3_Math\\": np.nan, \\"3_Science\\": 85 } # Let\'s assume we have implemented our functions. grades_series = load_data(data) filled_grades_series = handle_missing_data(grades_series) statistics_df = compute_statistics(filled_grades_series) reshaped_data = reshape_data(filled_grades_series) top_performers = identify_top_performers(reshaped_data) ``` In this task, you will utilize various functionalities of pandas Series and DataFrame to manipulate and analyze data. Make sure you use efficient pandas methods to perform these operations.","solution":"import pandas as pd import numpy as np def load_data(data: dict) -> pd.Series: Load data from dictionary into a pandas Series. return pd.Series(data) def handle_missing_data(grades: pd.Series) -> pd.Series: Fill missing values with the mean grade for the respective subject. filled_grades = grades.copy() subjects = filled_grades.index.str.split(\'_\').str[1] means = filled_grades.groupby(subjects).transform(lambda x: x.fillna(x.mean())) filled_grades.fillna(means, inplace=True) return filled_grades def compute_statistics(grades: pd.Series) -> pd.DataFrame: Compute mean, median, and standard deviation for each subject. subjects = grades.index.str.split(\'_\').str[1] stats = grades.groupby(subjects).agg([\'mean\', \'median\', \'std\']) return stats def reshape_data(grades: pd.Series) -> pd.DataFrame: Reshape the Series into a DataFrame with StudentID as rows and Subjects as columns. reshaped = grades.rename_axis(\'Student_Subject\').reset_index() reshaped[[\'StudentID\', \'Subject\']] = reshaped[\'Student_Subject\'].str.split(\'_\', expand=True) reshaped_data = reshaped.pivot(index=\'StudentID\', columns=\'Subject\', values=0) return reshaped_data def identify_top_performers(grades_df: pd.DataFrame) -> pd.Series: Identify the top 3 students with the highest average grades. average_grades = grades_df.mean(axis=1) top_performers = average_grades.nlargest(3) return top_performers"},{"question":"Problem Statement You are working with a dataset that contains missing values. Your task is to design a custom imputer that uses a combination of `SimpleImputer` and `KNNImputer` to handle missing values and integrates into a machine learning pipeline with a classifier to make predictions. Specifically, you should: 1. Use `SimpleImputer` to replace missing values in all columns with their respective column means. 2. Use `KNNImputer` to further refine the imputed values using the 3 nearest neighbors. 3. Mark any values that were initially missing using `MissingIndicator`. 4. Train a classifier (e.g., `DecisionTreeClassifier`) on the processed dataset and evaluate its performance. Guidelines 1. **Input and Output Formats**: - **Input**: - A dataset where features have missing values (represented as `np.nan`). - A target variable for classification. - **Output**: - Accuracy of the model on the test set. 2. **Constraints**: - Assume that the dataset fits into memory. - Do not use other imputation techniques beyond those mentioned. - The pipeline should be fully automated from preprocessing to prediction. 3. **Performance Requirements**: - Aim for a clear, correct implementation rather than optimizations. Example Here is the skeleton code structure you need to complete: ```python import numpy as np import pandas as pd from sklearn.impute import SimpleImputer, KNNImputer, MissingIndicator from sklearn.pipeline import Pipeline, FeatureUnion, make_pipeline from sklearn.compose import ColumnTransformer from sklearn.tree import DecisionTreeClassifier from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score # Load your dataset # Example dataset: X, y = load_some_data() # Note: Replace `load_some_data` with actual data loading code. X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Constructing the pipeline simple_imputer = SimpleImputer(strategy=\'mean\') knn_imputer = KNNImputer(n_neighbors=3) missing_indicator = MissingIndicator() # Combine imputers and indicator preprocessor = FeatureUnion(transformer_list=[ (\'simple_imputer\', simple_imputer), (\'knn_imputer\', knn_imputer), (\'missing_indicator\', missing_indicator) ]) # Create the full pipeline with a classifier pipeline = make_pipeline(preprocessor, DecisionTreeClassifier()) # Train the model pipeline.fit(X_train, y_train) # Make predictions y_pred = pipeline.predict(X_test) # Evaluate the model accuracy = accuracy_score(y_test, y_pred) print(f\\"Model accuracy: {accuracy}\\") # Return the accuracy ``` Your task is to complete the code, ensuring all components are correctly used and integrated.","solution":"import numpy as np import pandas as pd from sklearn.impute import SimpleImputer, KNNImputer, MissingIndicator from sklearn.pipeline import Pipeline, FeatureUnion, make_pipeline from sklearn.compose import ColumnTransformer from sklearn.tree import DecisionTreeClassifier from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score def custom_imputer_classifier(X, y): This function handles missing values using SimpleImputer and KNNImputer, marks initially missing values using MissingIndicator, incorporates them into a pipeline, trains a DecisionTreeClassifier, and evaluates its performance. Args: - X (pd.DataFrame): Feature dataset with missing values. - y (pd.Series): Target variable. Returns: - float: Accuracy of the model on the test set. # Split the data into training and testing sets. X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Define the individual preprocessors. simple_imputer = SimpleImputer(strategy=\'mean\') knn_imputer = KNNImputer(n_neighbors=3) missing_indicator = MissingIndicator() # Combine imputers and indicator into a single pipeline. preprocessor = ColumnTransformer( transformers=[ (\'simple_imputer\', simple_imputer, X.columns), # apply to all columns (\'knn_imputer\', knn_imputer, X.columns) # apply to all columns ], remainder=\'passthrough\' # retain remaining columns ) # Add the missing indicator preprocessor_with_indicator = FeatureUnion( transformer_list=[ (\'imputation_pipeline\', make_pipeline(preprocessor)), (\'missing_indicator\', missing_indicator) ]) # Create the full pipeline with a classifier. pipeline = Pipeline(steps=[ (\'preprocessor_with_indicator\', preprocessor_with_indicator), (\'classifier\', DecisionTreeClassifier(random_state=42)) ]) # Train the model. pipeline.fit(X_train, y_train) # Make predictions. y_pred = pipeline.predict(X_test) # Evaluate the model. accuracy = accuracy_score(y_test, y_pred) return accuracy"},{"question":"**Objective:** You are required to write a Python script that processes command-line arguments using the deprecated `optparse` module. This task is designed to test your ability to work with command-line tools, handle user inputs, and perform basic file I/O operations. **Task:** Write a Python script called `file_processor.py` that performs the following functions: 1. Accepts a file path as a mandatory positional argument. 2. Accepts two optional command-line arguments: `--uppercase` and `--wordcount`. 3. Reads the content of the specified file. 4. If `--uppercase` is provided, converts the content of the file to uppercase. 5. If `--wordcount` is provided, prints the number of words in the file. 6. If both options are provided, apply both transformations in the order specified above. 7. Outputs the modified content to the console. # Input Format: - A mandatory positional argument for the file path. - Optional arguments `--uppercase` and `--wordcount`. # Output Format: - The modified content of the file is printed on the console. If `--wordcount` is provided, the word count is printed after the content. # Constraints: - Assume the input file is a plain text file. - Ensure proper error handling for file operations. - Use the deprecated `optparse` module for parsing command-line arguments. # Example Usage: ```bash python file_processor.py sample.txt --uppercase --wordcount ``` # Example Output: If `sample.txt` contains \\"Hello world from Python.\\" ``` HELLO WORLD FROM PYTHON. Word Count: 4 ``` # Implementation Notes: 1. Familiarize yourself with the `optparse` module and its usage for defining and handling options. 2. Implement the script such that it can handle exceptions like file not found or read errors gracefully. 3. Ensure that your script yields the correct results irrespective of the order of the optional arguments.","solution":"import os from optparse import OptionParser def process_file(filepath, uppercase, wordcount): try: with open(filepath, \'r\') as file: content = file.read() if uppercase: content = content.upper() print(content) if wordcount: words = content.split() print(f\\"Word Count: {len(words)}\\") except FileNotFoundError: print(f\\"Error: The file \'{filepath}\' does not exist.\\") except Exception as e: print(f\\"Error: {str(e)}\\") if __name__ == \\"__main__\\": parser = OptionParser(usage=\\"usage: %prog [options] filepath\\") parser.add_option(\\"--uppercase\\", action=\\"store_true\\", dest=\\"uppercase\\", default=False, help=\\"Convert file content to uppercase\\") parser.add_option(\\"--wordcount\\", action=\\"store_true\\", dest=\\"wordcount\\", default=False, help=\\"Display the word count of the file content\\") (options, args) = parser.parse_args() if len(args) != 1: parser.error(\\"A file path must be provided as a positional argument.\\") else: filepath = args[0] process_file(filepath, options.uppercase, options.wordcount)"},{"question":"# PyTorch Custom Profiler Implementation **Objective**: You are required to design and implement a custom profiler in PyTorch that tracks the time taken by specific operations during model execution. The profiler should log operator names, their inputs, and the duration for which each operator runs. Finally, the profiler should save this profiling information to a file. **Task**: 1. Implement a function `setup_custom_profiler` that initializes the custom profiler. 2. Implement the profiler to track the performance of various PyTorch operators. 3. Save the profiling data to a file in JSON format. Function Signature: ```python def setup_custom_profiler(sampling_rate: float, output_file: str) -> None: Sets up a custom profiler that logs information about PyTorch operators. Args: - sampling_rate (float): The probability with which an operator\'s execution should be profiled. - output_file (str): The file to which profiling data should be saved. Returns: - None pass ``` Requirements: 1. **Profiling Setup**: The `setup_custom_profiler` function should initialize the profiler with a given sampling rate. 2. **Logging Details**: Log the following details: - Operator name. - Number of inputs. - Operator execution time. 3. **Sampling Rate**: The profiler should only sample a percentage of the operators, as specified by the `sampling_rate` parameter. 4. **Output File**: Save the profiling logs to a JSON file passed as `output_file`. Constraints: - You may use PyTorch version 1.7.0 or newer. - The profiler should not significantly impact the performance of the model execution. Example Usage: ```python import torch from torch import nn class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.fc = nn.Linear(10, 5) def forward(self, x): return self.fc(x) def main(): model = SimpleModel() input_tensor = torch.randn(1, 10) # Set up the custom profiler with a sampling rate of 0.5 and output to \'profiler_output.json\' setup_custom_profiler(0.5, \'profiler_output.json\') # Perform the forward pass output = model(input_tensor) if __name__ == \\"__main__\\": main() ``` **Grading Criteria**: - Correctness: The solution correctly implements the custom profiler. - Efficiency: Profiling should be light and not significantly impact the model performance. - Completeness: All required details are logged and saved to the specified output file in JSON format. - Code Quality: The code should be well-organized, documented, and follow Python best practices.","solution":"import torch import json import time class CustomProfiler: def __init__(self, sampling_rate, output_file): self.sampling_rate = sampling_rate self.output_file = output_file self.logs = [] def profile_op(self, op_name, inputs, op_run): if torch.rand(1).item() < self.sampling_rate: start_time = time.time() output = op_run(*inputs) end_time = time.time() self.logs.append({ \\"op_name\\": op_name, \\"num_inputs\\": len(inputs), \\"duration\\": end_time - start_time }) return output else: return op_run(*inputs) def save_logs(self): with open(self.output_file, \'w\') as f: json.dump(self.logs, f) _profiler = None def setup_custom_profiler(sampling_rate: float, output_file: str) -> None: global _profiler _profiler = CustomProfiler(sampling_rate, output_file) def profile_function(op_name): def decorator(op_run): def wrapper(*inputs): global _profiler if _profiler: return _profiler.profile_op(op_name, inputs, op_run) else: return op_run(*inputs) return wrapper return decorator def save_profiling_results(): global _profiler if _profiler: _profiler.save_logs()"},{"question":"# Dynamic File Search and Process Execution Utility As a Python developer, you are tasked with creating a utility that searches for files within a given directory that match a certain pattern and then processes these files using a specific system command. Problem Description Write a Python function `process_matching_files` that takes three inputs: 1. `directory`: A string representing the path to the directory to search within. 2. `pattern`: A string that represents the pattern to match file names. 3. `command`: A string command to be executed using the matched files. The command should replace each occurrence of `{file}` within the command string with the actual file path. The function should: 1. Recursively search the specified directory for all files matching the given pattern. 2. For each matched file, execute the specified command where `{file}` is replaced with the file path. 3. Capture and return the output of each command execution in a dictionary where the keys are the file paths, and the values are the corresponding command outputs. Example ```python import os def process_matching_files(directory: str, pattern: str, command: str) -> dict: Search for files within `directory` matching the `pattern`, and execute `command` for each matched file. Args: directory (str): Path to the directory to search within. pattern (str): The pattern to match file names. command (str): Command to execute with each matched file. Returns: dict: A dictionary where keys are file paths and values are command outputs. # Implement the function here pass # Example usage: output = process_matching_files(\'test_dir\', \'*.txt\', \'cat {file}\') print(output) ``` Constraints 1. Assume that the file matching pattern follows Unix shell-style wildcards. 2. The command execution must ensure proper handling of special characters in file names. 3. Errors in command execution should be captured and logged appropriately but must not prevent the function from processing other files. 4. You may not use any additional libraries beyond the Python Standard Library. Expected Output The function should return a dictionary with the file paths as keys and the command outputs as values. If no files match the pattern, return an empty dictionary. # Performance Requirements - The function should efficiently search through directories and handle cases with a large number of files. - Command execution should be properly managed with a focus on security and resource handling.","solution":"import os import fnmatch import subprocess def process_matching_files(directory: str, pattern: str, command: str) -> dict: Search for files within `directory` matching the `pattern`, and execute `command` for each matched file. Args: directory (str): Path to the directory to search within. pattern (str): The pattern to match file names. command (str): Command to execute with each matched file. Returns: dict: A dictionary where keys are file paths and values are command outputs. matched_files = [] for root, _, files in os.walk(directory): for filename in fnmatch.filter(files, pattern): full_path = os.path.join(root, filename) matched_files.append(full_path) outputs = {} for file in matched_files: cmd = command.replace(\'{file}\', file) try: result = subprocess.run(cmd, shell=True, capture_output=True, text=True) outputs[file] = result.stdout except subprocess.CalledProcessError as e: outputs[file] = f\\"Error occurred: {e.output}\\" return outputs"},{"question":"Objective The goal of this exercise is to validate your understanding and ability to work with Python\'s `compileall` module programmatically, specifically demonstrating your familiarity with its functionality and parameters. Problem Statement You are required to implement a function `custom_compile(directory: str, regex: str = None, max_recursion: int = 1, show_output: bool = True) -> bool`. This function should: 1. Recursively compile all `.py` files in the given `directory`. 2. Exclude files that match the provided `regex` (if any). 3. Limit recursion into subdirectories to a maximum of `max_recursion` levels. 4. Only show output if the `show_output` parameter is set to `True`. The function should return `True` if all files compiled successfully, and `False` otherwise. Function Signature ```python def custom_compile(directory: str, regex: str = None, max_recursion: int = 1, show_output: bool = True) -> bool: ``` Parameters - `directory`: A string representing the path to the directory to be compiled. - `regex`: A string representing the regular expression to match files that should be excluded from compilation. This is optional and defaults to `None`. - `max_recursion`: An integer representing the maximum recursion level into subdirectories. This is optional and defaults to `1`. - `show_output`: A boolean which, if set to `True`, prints compilation messages. This is optional and defaults to `True`. Constraints - Only `.py` files should be compiled. - If `regex` is provided, use it to filter out files that match this regular expression from being compiled. - The function should handle any errors gracefully and return `False` if any compilation errors occur. Example ```python result = custom_compile(\'my_project\', regex=r\'.*test.*\', max_recursion=2, show_output=False) print(result) # Expected output: True or False based on success of compilation. ``` Notes You may use the `compileall` module and its functions to achieve the required behavior. Make sure to handle file paths appropriately and ensure your function adheres to the specified parameters.","solution":"import compileall import re from pathlib import Path def custom_compile(directory: str, regex: str = None, max_recursion: int = 1, show_output: bool = True) -> bool: Recursively compiles all .py files in the given directory, excluding files that match the provided regex, limiting recursion to max_recursion levels, and optionally showing output. Args: - directory (str): The path to the directory to compile. - regex (str, optional): Regular expression to exclude files. Default is None. - max_recursion (int, optional): Maximum recursion level into subdirectories. Default is 1. - show_output (bool, optional): Print compilation messages if True. Default is True. Returns: - bool: True if all files compiled successfully, False otherwise. directories = [] # Collect directories up to max_recursion levels def collect_directories(dir_path: Path, current_level: int): if current_level > max_recursion: return directories.append(dir_path) for sub_dir in dir_path.iterdir(): if sub_dir.is_dir(): collect_directories(sub_dir, current_level + 1) # Initial collection of directories collect_directories(Path(directory), 0) success = True for dir_path in directories: if regex: excludes = re.compile(regex) else: excludes = None result = compileall.compile_dir( dir_path, maxlevels=0, ddir=None, force=False, rx=excludes, quiet=not show_output ) if not result: success = False return success"},{"question":"**Coding Assessment Question: Trace Execution and Generate Coverage Report** Using the `trace` module, write a Python program that traces the execution of a provided Python script, generates annotated statement coverage listings, and outputs the coverage results in a specified directory. Your program should support both command-line and programmatic interfaces. # Inputs: 1. **Script filepath (string)**: The filepath of the Python script to be executed and traced. 2. **Output directory (string)**: The directory where the coverage report files will be written. # Outputs: - Annotated coverage report files in the specified output directory, showing how many times each statement was executed. # Constraints: - You should handle cases where the script file does not exist or the output directory is not writeable. - The program should be able to run from the command line using appropriate options from the `trace` module. # Requirements: 1. **Command-Line Interface**: Your program should accept the script filepath and output directory as command-line arguments. - Example usage: `python trace_solution.py -f <script_filepath> -o <output_directory>` 2. **Programmatic Interface**: Use the `trace.Trace` class to perform the trace and generate the coverage report. # Example: Given a script `example.py` with the following content: ```python def foo(): for i in range(5): print(i) foo() ``` The output should be an annotated file `example.cover` with content similar to: ```plaintext 1: def foo(): 5: for i in range(5): 5: print(i) ``` # Implementation Details: 1. Create a `Trace` object configured to count the number of times each statement is executed. 2. Use the `run` method of the `Trace` object to execute the given script. 3. Write the coverage results to the specified directory using the `write_results` method of the `CoverageResults` object. **Starter Code:** ```python import trace def trace_script(script_filepath, output_directory): # Implement the programmatic interface here. pass if __name__ == \\"__main__\\": import sys import argparse parser = argparse.ArgumentParser(description=\'Trace script execution and generate coverage report.\') parser.add_argument(\'-f\', \'--file\', type=str, required=True, help=\'Script file to trace\') parser.add_argument(\'-o\', \'--output\', type=str, required=True, help=\'Output directory for coverage report\') args = parser.parse_args() trace_script(args.file, args.output) print(f\\"Coverage report generated in directory: {args.output}\\") ``` # Notes: - Make sure to handle any exceptions or errors that may arise. - You can refer to the `trace` module\'s documentation for additional options and detailed usage.","solution":"import os import trace def trace_script(script_filepath, output_directory): Traces the execution of the provided script and generates a coverage report. :param script_filepath: str, path to the Python script to trace :param output_directory: str, directory where the coverage report will be written if not os.path.exists(script_filepath): raise FileNotFoundError(f\\"The script file \'{script_filepath}\' does not exist.\\") if not os.path.isdir(output_directory): raise NotADirectoryError(f\\"The output directory \'{output_directory}\' does not exist or is not a directory.\\") tracer = trace.Trace(count=True, trace=False) try: # Execute the script file tracer.run(f\'exec(open(\\"{script_filepath}\\").read())\') # Generate the coverage report in the specified directory results = tracer.results() results.write_results(show_missing=True, summary=True, coverdir=output_directory) except Exception as e: print(f\\"An error occurred while tracing the script: {e}\\")"},{"question":"Objective: Design and implement a Python C extension that exposes a function to Python, leveraging the `PyArg_ParseTuple()` and `Py_BuildValue()` functions. Problem Statement: Create a Python C extension that provides a function `calculate_statistics` which takes a list of integers as input and returns a tuple containing the minimum, maximum, and average of the list. Requirements: 1. **Function Name**: `calculate_statistics` 2. **Input**: A list of integers. 3. **Output**: A tuple `(min_val, max_val, avg_val)` where: - `min_val` is the minimum value in the list. - `max_val` is the maximum value in the list. - `avg_val` is the average of all the values in the list, rounded to 2 decimal places. 4. **Constraints**: - The list should contain at least one element, otherwise an appropriate Python exception should be raised. Implementation Details: 1. **Header File (CalculateStatistics.h)**: - Declarations for the function and any necessary setup for the Python module. 2. **Source File (CalculateStatistics.c)**: - Implementation of the `calculate_statistics` function. - Usage of `PyArg_ParseTuple()` to parse the input. - Usage of `Py_BuildValue()` to construct the return value. - Proper error handling following the patterns mentioned in the provided documentation. Example: ```python import calculate_statistics result = calculate_statistics([1, 2, 3, 4, 5]) print(result) # Output might be (1, 5, 3.0) ``` Feel free to refer to the official Python documentation for creating C extensions for additional guidance on setting up your build environment and compiling the extension. Submission: Submit the following files: 1. `CalculateStatistics.h` 2. `CalculateStatistics.c` 3. `setup.py` for building the extension module. 4. A Python script `test_calculate_statistics.py` demonstrating how to use the module and verifying its correctness with various test cases. # Note: Ensure that you follow best practices in memory management and error handling, as well as provide comments in your code to explain the key sections. Good luck!","solution":"def calculate_statistics(numbers): Calculates and returns the minimum, maximum, and average of a list of integers. :param numbers: List of integers :return: Tuple containing (min_val, max_val, avg_val) if not numbers: raise ValueError(\\"The list should contain at least one element\\") min_val = min(numbers) max_val = max(numbers) avg_val = round(sum(numbers) / len(numbers), 2) return (min_val, max_val, avg_val)"},{"question":"# PyTorch Special Functions Assessment Objective: You are tasked with demonstrating your understanding of various special functions in the `torch.special` module by implementing a function that performs a complex mathematical computation using these functions. Problem Statement: Create a function `special_math_operations` that takes a 1D tensor `x` as input and returns a 1D tensor where each element is computed using the formula: [ y_i = text{bessel_j0}(x_i) + text{logit}(text{sigmoid}(x_i)) + text{erf}(text{sqrt}(|x_i|)) ] Where: - (text{bessel_j0}) is the Bessel function of the first kind of order 0. - (text{logit}) is the logit function, the inverse of the sigmoid function. - (text{sigmoid}) is the sigmoid function: (sigma(x) = frac{1}{1 + e^{-x}}). - (text{erf}) is the error function. - (text{sqrt}) is the square root function. Requirements: 1. The input tensor `x` can contain any real numbers. 2. The function should handle tensors of any size, performing the operation element-wise. 3. You are allowed to use functions only from `torch` and `torch.special` modules. 4. Ensure that your implementation is efficient and leverages tensor operations. Function Signature: ```python import torch def special_math_operations(x: torch.Tensor) -> torch.Tensor: pass ``` Example: ```python import torch result = special_math_operations(torch.tensor([1.0, -2.0, 0.5])) print(result) # Expected output: A tensor containing the applied operations on [1.0, -2.0, 0.5] ``` Constraints and Limitations: - You must not use loops; all operations should be vectorized. - The input tensor `x` will have a length `n` such that (1 leq n leq 10^6). Performance Requirements: - The function should execute efficiently for large tensors up to the maximum length constraint. Good luck!","solution":"import torch def special_math_operations(x: torch.Tensor) -> torch.Tensor: Performs a series of special mathematical operations on the input tensor x. Args: x (torch.Tensor): Input 1D tensor containing real numbers. Returns: torch.Tensor: Output tensor after applying the specified operations element-wise. bessel_j0_values = torch.special.bessel_j0(x) sigmoid_values = torch.sigmoid(x) logit_values = torch.special.logit(sigmoid_values) sqrt_abs_values = torch.sqrt(torch.abs(x)) erf_values = torch.special.erf(sqrt_abs_values) y = bessel_j0_values + logit_values + erf_values return y"},{"question":"As a data scientist, it is crucial to ensure the correctness and performance of your data processing code. In this task, you will use pandas\' testing and error handling functionalities to write robust tests for a given function. You are tasked with implementing a function called `calculate_statistics` that computes the mean, median, standard deviation, and variance of columns in a given `DataFrame`. Additionally, you must write tests for this function using pandas\' `testing.assert_frame_equal` to ensure the function works correctly. Function Signature ```python import pandas as pd def calculate_statistics(df: pd.DataFrame, columns: list) -> pd.DataFrame: Calculates the mean, median, standard deviation, and variance for specified columns in a DataFrame. Parameters: df (pd.DataFrame): Input DataFrame containing the data. columns (list): List of column names for which to compute the statistics. Returns: pd.DataFrame: DataFrame containing the calculated statistics for the specified columns. pass ``` Requirements 1. The function should compute the following statistics for each specified column: - Mean (`mean`) - Median (`median`) - Standard Deviation (`std`) - Variance (`var`) 2. The result should be a DataFrame where each row corresponds to one of these statistics and each column corresponds to the input columns. 3. Handle any exceptions that might arise, such as invalid column names or operations on non-numeric data. Use appropriate pandas exception classes. 4. Write a set of tests using pandas\' `testing.assert_frame_equal` to verify your `calculate_statistics` function. The tests should cover: - Valid input with numeric columns - Presence of non-numeric columns (should raise an appropriate error) - Invalid column names (should raise an appropriate error) Example ```python # Example DataFrame data = { \\"A\\": [1, 2, 3, 4, 5], \\"B\\": [5, 4, 3, 2, 1], \\"C\\": [\\"a\\", \\"b\\", \\"c\\", \\"d\\", \\"e\\"] } df = pd.DataFrame(data) # Calculate statistics for columns A and B result_df = calculate_statistics(df, [\\"A\\", \\"B\\"]) # Expected output DataFrame expected_data = { \\"A\\": [3.0, 3.0, 1.581139, 2.5], \\"B\\": [3.0, 3.0, 1.581139, 2.5] } expected_df = pd.DataFrame(expected_data, index=[\\"mean\\", \\"median\\", \\"std\\", \\"var\\"]) # Test the result pd.testing.assert_frame_equal(result_df, expected_df) ```","solution":"import pandas as pd def calculate_statistics(df: pd.DataFrame, columns: list) -> pd.DataFrame: Calculates the mean, median, standard deviation, and variance for specified columns in a DataFrame. Parameters: df (pd.DataFrame): Input DataFrame containing the data. columns (list): List of column names for which to compute the statistics. Returns: pd.DataFrame: DataFrame containing the calculated statistics for the specified columns. if not all(col in df.columns for col in columns): raise KeyError(\\"One or more columns are missing from the DataFrame.\\") df_numeric = df[columns] if not df_numeric.apply(lambda s: pd.to_numeric(s, errors=\'coerce\').notna().all()).all(): raise TypeError(\\"One or more columns contain non-numeric data.\\") stats_data = {} for column in columns: stats_data[column] = [ df_numeric[column].mean(), df_numeric[column].median(), df_numeric[column].std(), df_numeric[column].var() ] result_df = pd.DataFrame(stats_data, index=[\\"mean\\", \\"median\\", \\"std\\", \\"var\\"]) return result_df"},{"question":"Objective: To assess your understanding of color palettes in seaborn and your ability to create and customize different types of palettes for visualizing categorical, sequential, and diverging data. Problem Statement: You are tasked with creating a series of plots using different types of color palettes in seaborn. The objectives are to: 1. Make a scatter plot using a qualitative palette to represent different categories. 2. Create a heatmap using a sequential palette to represent numeric data. 3. Generate a scatter plot using a diverging palette to highlight deviations from a midpoint value. Implement the function `palette_plots` that will generate and save the plots to specified file paths. Function Signature: ```python import seaborn as sns import matplotlib.pyplot as plt import pandas as pd def palette_plots(dataset_url: str, scatter_plot_path: str, heatmap_path: str, diverging_plot_path: str) -> None: # Your code here pass ``` Input: - `dataset_url` (str): URL to a dataset in CSV format that contains at least two numeric columns and one categorical column. - `scatter_plot_path` (str): File path to save the scatter plot image. - `heatmap_path` (str): File path to save the heatmap image. - `diverging_plot_path` (str): File path to save the diverging palette image. Output: This function should not return anything. It should save the generated plots to the specified file paths. Constraints: - You must use seaborn for the visualizations. - You should use appropriate color palettes for each plot based on the data you visualize. - Ensure that the plots are clearly readable and well-labeled. Example: ```python palette_plots( dataset_url=\'https://raw.githubusercontent.com/mwaskom/seaborn-data/master/penguins.csv\', scatter_plot_path=\'scatter_plot.png\', heatmap_path=\'heatmap.png\', diverging_plot_path=\'diverging_plot.png\' ) ``` Requirements: 1. **Scatter Plot**: Use a qualitative palette to distinguish categories (island) in the dataset. Plot two variables that represent numeric data (bill_length_mm vs bill_depth_mm). 2. **Heatmap**: Use a sequential palette to create a heatmap of the correlation matrix of the numeric columns in the dataset. 3. **Diverging Plot**: Use a diverging palette to create a scatter plot with one categorical variable (species) and two numeric variables (flipper_length_mm vs body_mass_g). Highlight deviations with the palette. Additional Notes: - Make sure to handle cases where the required columns might not exist. - Add appropriate titles, labels, and legends to your plots. - Save the plots with the provided file paths.","solution":"import seaborn as sns import matplotlib.pyplot as plt import pandas as pd def palette_plots(dataset_url: str, scatter_plot_path: str, heatmap_path: str, diverging_plot_path: str) -> None: # Load dataset from the provided URL df = pd.read_csv(dataset_url) # Check if the dataset contains the required columns required_columns_scatter = [\'island\', \'bill_length_mm\', \'bill_depth_mm\'] required_columns_heatmap = [\'bill_length_mm\', \'bill_depth_mm\', \'flipper_length_mm\', \'body_mass_g\'] required_columns_diverging = [\'species\', \'flipper_length_mm\', \'body_mass_g\'] for col in required_columns_scatter + required_columns_heatmap + required_columns_diverging: if col not in df.columns: raise ValueError(f\\"Required column \'{col}\' not in dataset\\") # Scatter plot with qualitative palette plt.figure(figsize=(10, 6)) scatter_plot = sns.scatterplot(data=df, x=\'bill_length_mm\', y=\'bill_depth_mm\', hue=\'island\', palette=\'Set2\') scatter_plot.set(title=\'Scatter Plot with Qualitative Palette\', xlabel=\'Bill Length (mm)\', ylabel=\'Bill Depth (mm)\') plt.legend(title=\'Island\') plt.savefig(scatter_plot_path) plt.close() # Heatmap with sequential palette plt.figure(figsize=(10, 6)) correlation_matrix = df[required_columns_heatmap].corr() heatmap = sns.heatmap(correlation_matrix, annot=True, cmap=\'YlGnBu\') heatmap.set(title=\'Heatmap with Sequential Palette\') plt.savefig(heatmap_path) plt.close() # Scatter plot with diverging palette plt.figure(figsize=(10, 6)) scatter_diverging = sns.scatterplot(data=df, x=\'flipper_length_mm\', y=\'body_mass_g\', hue=\'species\', palette=\'RdYlBu\', size=\'body_mass_g\', sizes=(20, 200)) scatter_diverging.set(title=\'Scatter Plot with Diverging Palette\', xlabel=\'Flipper Length (mm)\', ylabel=\'Body Mass (g)\') plt.legend(title=\'Species\') plt.savefig(diverging_plot_path) plt.close()"},{"question":"# Advanced PyTorch Performance Optimization Task **Objective:** Demonstrate your understanding of PyTorch by implementing a function that ensures optimization by verifying and converting the input conditions for using the persistent algorithm. **Problem Statement:** You are given a tensor and your task is to implement a function `optimize_for_persistent_algorithm` which receives this tensor and ensures that all conditions for using a persistent algorithm are satisfied. If these conditions are not already satisfied, your function should convert the tensor to meet all conditions. **Function Signature:** ```python def optimize_for_persistent_algorithm(tensor: torch.Tensor) -> torch.Tensor: pass ``` **Input:** - `tensor` (torch.Tensor): A tensor that needs to be optimized. **Output:** - A new tensor that satisfies the following conditions: 1. Data is on the GPU, 2. Data type is `torch.float16`, 3. Not in `PackedSequence` format. **Constraints:** - Ensure CUDNN is enabled. - The function should not alter sequences if the provided tensor is in `PackedSequence` format. In this case, just return the original tensor. - Assume V100 GPU environment. **Examples:** ```python import torch # Example 1: A tensor on CPU and float32 input_tensor = torch.randn(3, 3, dtype=torch.float32) output_tensor = optimize_for_persistent_algorithm(input_tensor) print(output_tensor.dtype) # torch.float16 print(output_tensor.device) # cuda:0 # Example 2: A tensor already on GPU but float32 input_tensor = torch.randn(3, 3, dtype=torch.float32, device=\'cuda:0\') output_tensor = optimize_for_persistent_algorithm(input_tensor) print(output_tensor.dtype) # torch.float16 print(output_tensor.device) # cuda:0 # Example 3: A tensor already in required format input_tensor = torch.randn(3, 3, dtype=torch.float16, device=\'cuda:0\') output_tensor = optimize_for_persistent_algorithm(input_tensor) print(output_tensor.dtype) # torch.float16 print(output_tensor.device) # cuda:0 ``` Make sure to handle the scenarios appropriately to ensure the tensor is optimized as per the conditions outlined.","solution":"import torch from torch.nn.utils.rnn import PackedSequence def optimize_for_persistent_algorithm(tensor: torch.Tensor) -> torch.Tensor: Ensures that the tensor is optimized for using the persistent algorithm with the following conditions: 1. Data is on the GPU, 2. Data type is torch.float16, 3. Not in PackedSequence format. Args: - tensor (torch.Tensor): The input tensor to optimize. Returns: - torch.Tensor: The optimized tensor. if isinstance(tensor, PackedSequence): return tensor # Ensure tensor is on the GPU if not tensor.is_cuda: tensor = tensor.cuda() # Ensure tensor is of type float16 if tensor.dtype != torch.float16: tensor = tensor.half() return tensor"},{"question":"# Advanced Coding Assessment: Implementing and Testing a Calculator You are tasked with implementing a simple calculator class `Calculator` that supports basic arithmetic operations (addition, subtraction, multiplication, and division) and writing a comprehensive suite of tests using the `doctest` module. Your goal is to ensure the correctness of your implementation with rigorous doctests. Requirements 1. **Class Implementation:** - Implement the `Calculator` class with the following methods: ```python class Calculator: def add(self, a: float, b: float) -> float: Return the sum of a and b. pass def subtract(self, a: float, b: float) -> float: Return the subtraction of b from a. pass def multiply(self, a: float, b: float) -> float: Return the multiplication of a and b. pass def divide(self, a: float, b: float) -> float: Return the division of a by b. Raise ValueError if b is zero. pass ``` - Ensure your methods are correctly implemented according to their descriptions. 2. **Doctests:** - Add doctests to each method’s docstring to verify its functionality. - Ensure to cover normal cases, edge cases (e.g., zero division), and corner cases (e.g., very large numbers). Example for method docstring with doctests: ```python def add(self, a: float, b: float) -> float: Return the sum of a and b. >>> calc = Calculator() >>> calc.add(2, 3) 5 >>> calc.add(-2, 3) 1 >>> calc.add(-2, -3) -5 >>> calc.add(1.5, 2.5) 4.0 pass ``` 3. **Advanced Test Scenarios:** - In addition to simple positive tests, include tests that: - Validate exception handling. - Utilize `doctest` option flags for any special formatting required. - Demonstrate the use of doctest directives (e.g., skipping tests). 4. **Integration with the `DoctestRunner`:** - Implement a script that will run all doctests within the `Calculator` class using `doctest.testmod()`. - Ensure that the script provides clear output indicating the success or failure of each test. Input and Output - You are providing the entire implementation within a single script file. - The input consists of method calls documented within their respective docstrings. - The output will be the result of running `doctest.testmod()` with either a pass message or details of any test failures. Script Execution Here\'s an example of how your script should be structured to run the tests: ```python if __name__ == \\"__main__\\": import doctest doctest.testmod(verbose=True) ``` Constraints - The `divide` function should handle division by zero as a special case and raise a `ValueError`. - Ensure the tests are automated, with no need for manual intervention. Performance - No specific performance requirements for the calculator operations as they are simple arithmetic computations. Good luck, and make sure to test thoroughly!","solution":"class Calculator: def add(self, a: float, b: float) -> float: Return the sum of a and b. >>> calc = Calculator() >>> calc.add(2, 3) 5 >>> calc.add(-2, 3) 1 >>> calc.add(-2, -3) -5 >>> calc.add(1.5, 2.5) 4.0 return a + b def subtract(self, a: float, b: float) -> float: Return the subtraction of b from a. >>> calc = Calculator() >>> calc.subtract(10, 5) 5 >>> calc.subtract(-3, 2) -5 >>> calc.subtract(2, -3) 5 >>> calc.subtract(-2, -3) 1 >>> calc.subtract(1.5, 0.5) 1.0 return a - b def multiply(self, a: float, b: float) -> float: Return the multiplication of a and b. >>> calc = Calculator() >>> calc.multiply(2, 3) 6 >>> calc.multiply(-2, 3) -6 >>> calc.multiply(2, -3) -6 >>> calc.multiply(0, 5) 0 >>> calc.multiply(1.5, 2) 3.0 return a * b def divide(self, a: float, b: float) -> float: Return the division of a by b. Raise ValueError if b is zero. >>> calc = Calculator() >>> calc.divide(10, 2) 5.0 >>> calc.divide(-6, 2) -3.0 >>> calc.divide(6, -2) -3.0 >>> calc.divide(7, 2) 3.5 >>> calc.divide(1, 0) Traceback (most recent call last): ... ValueError: Cannot divide by zero if b == 0: raise ValueError(\\"Cannot divide by zero\\") return a / b"},{"question":"# Advanced PyTorch Assessment: Tensor Manipulation and Matrix Operations Objective: To assess your understanding of PyTorch\'s tensor operations and optimization techniques through a series of tasks involving tensor creation, manipulation, and advanced matrix operations. Problem Statement: 1. **Tensor Creation and Initialization:** Write a function `create_special_tensor` that takes two integers `n` and `m`, and creates an `n x m` tensor such that: - The elements on the diagonal are set to 1. - Elements above the diagonal are set to -1. - Elements below the diagonal are set to 0. **Function Signature:** ```python def create_special_tensor(n: int, m: int) -> torch.Tensor: ``` 2. **Random Sampling and Normalization:** Write a function `normalize_random_tensor` that: - Creates a tensor of shape `(n, m)` filled with random values sampled from a standard normal distribution (`mean=0`, `std=1`). - Normalizes this tensor so that the values in each row sum to 1. **Function Signature:** ```python def normalize_random_tensor(n: int, m: int) -> torch.Tensor: ``` 3. **Matrix Multiplication with Gradients:** Write a function `matrix_operations` that: - Creates two tensors `A` and `B` of shape `(n, m)` with random values sampled from a uniform distribution between 0 and 1. - Computes the matrix product `C = A @ B.T`. - Disables gradient computation and performs element-wise multiplication of `C` by a scalar value `k`. - Returns the resulting matrix. **Function Signature:** ```python def matrix_operations(n: int, m: int, k: float) -> torch.Tensor: ``` 4. **Advanced Reduction Operation:** Write a function `row_max_sum` that: - Takes a tensor `X` of shape `(n, m)`. - Finds the maximum value in each row. - Returns the sum of these maximum values. **Function Signature:** ```python def row_max_sum(X: torch.Tensor) -> float: ``` Constraints: - Use PyTorch to perform all operations. - Ensure your code is efficient and adheres to best practices in PyTorch. Input/Output Examples: 1. **create_special_tensor(4, 4)** ```plaintext Output: tensor([[ 1., -1., -1., -1.], [ 0., 1., -1., -1.], [ 0., 0., 1., -1.], [ 0., 0., 0., 1.]]) ``` 2. **normalize_random_tensor(3, 3)** ```plaintext Output: tensor([[0.5294, -0.5291, 1.0000], [0.3266, 0.4265, 0.2469], [0.3726, -0.1119, 0.7393]]) # Each row sums to 1. ``` 3. **matrix_operations(2, 3, 2)** ```plaintext Output: tensor([[0.9478, 0.2257, 0.4564], [0.2456, 0.6754, 0.1234]]) # Example result of element-wise multiplication by k=2. ``` 4. **row_max_sum(torch.tensor([[1, 2, 3], [4, 5, 6]]))** ```plaintext Output: 9.0 # Maximum values are 3 and 6, and their sum is 9. ``` The functions should be implemented to meet the stated functionalities and constraints.","solution":"import torch def create_special_tensor(n: int, m: int) -> torch.Tensor: Creates an n x m tensor with: - Elements on the diagonal set to 1. - Elements above the diagonal set to -1. - Elements below the diagonal set to 0. tensor = torch.zeros((n, m)) for i in range(n): for j in range(m): if i == j: tensor[i, j] = 1 elif i < j: tensor[i, j] = -1 return tensor def normalize_random_tensor(n: int, m: int) -> torch.Tensor: Creates an n x m tensor filled with random values from a standard normal distribution and normalizes each row to sum to 1. tensor = torch.randn((n, m)) row_sums = tensor.sum(dim=1, keepdim=True) normalized_tensor = tensor / row_sums return normalized_tensor def matrix_operations(n: int, m: int, k: float) -> torch.Tensor: Creates two tensors A and B of shape (n, m) with random values between 0 and 1, computes their matrix product C = A @ B.T, disables gradient computation, and performs element-wise multiplication of C by scalar value k. A = torch.rand((n, m)) B = torch.rand((n, m)) C = torch.matmul(A, B.T) with torch.no_grad(): result = C * k return result def row_max_sum(X: torch.Tensor) -> float: Takes a tensor X of shape (n, m), finds the maximum value in each row, and returns the sum of these maximum values. max_values = torch.max(X, dim=1)[0] return torch.sum(max_values).item()"},{"question":"Objective: To assess advanced understanding of file parsing, exception handling, and dictionary manipulation using the `netrc` module. Problem Statement: You are required to implement a class called `NetrcAuthenticator` which makes use of the `netrc` module to manage and retrieve FTP authentication details. Tasks: 1. **Initialization:** - Implement the `__init__` method to initialize an instance of the `NetrcAuthenticator` class. It should take an optional argument `file_path`. If `file_path` is provided, load the specified `.netrc` file; otherwise, load the default `.netrc` file from the user\'s home directory. - Handle and raise appropriate exceptions (`FileNotFoundError`, `netrc.NetrcParseError`) with custom messages to provide clear diagnostic information. 2. **Authentication Retrieval:** - Implement a method `get_credentials(self, host)` in the `NetrcAuthenticator` class that takes a host name as input and returns a dictionary with keys `login`, `account`, and `password`. If the host is not found, and there is no default entry, raise a `ValueError` with an appropriate message. 3. **String Representation:** - Override the `__repr__` method to return a string representation of the `.netrc` file content contained in the `NetrcAuthenticator` instance. Constraints: - Raise `ValueError` if the host is not found and there is no default entry. - Raise `FileNotFoundError` with a custom message if the specified `.netrc` file is not found. - Raise `netrc.NetrcParseError` with a custom message on parse errors. - Passwords must only contain ASCII printable characters except whitespace. Example: ```python # Sample usage try: auth = NetrcAuthenticator(\\"/custom/path/to/.netrc\\") creds = auth.get_credentials(\\"example.com\\") print(creds) except Exception as e: print(e) # Output for valid .netrc file and existing host: # {\'login\': \'user123\', \'account\': \'account_name\', \'password\': \'password123\'} # Output for valid .netrc file but non-existing host: # ValueError: No entry found for host \\"example.com\\" and no default entry available. # String representation of the .netrc file print(repr(auth)) ``` Implement the `NetrcAuthenticator` class with the specified methods and exception handling.","solution":"import os import netrc class NetrcAuthenticator: def __init__(self, file_path=None): Initialize the NetrcAuthenticator with a specified file path or the default .netrc file. self.file_path = file_path or os.path.expanduser(\'~/.netrc\') try: self.netrc_content = netrc.netrc(self.file_path) except FileNotFoundError: raise FileNotFoundError(f\\"The specified .netrc file was not found at: {self.file_path}\\") except netrc.NetrcParseError: raise netrc.NetrcParseError(f\\"Failed to parse the .netrc file at: {self.file_path}\\") def get_credentials(self, host): Return a dictionary with keys `login`, `account`, and `password` for the given host. Raise ValueError if the host is not found and no default entry is available. auth = self.netrc_content.authenticators(host) if auth is None: default_host = self.netrc_content.authenticators(\'default\') if default_host is None: raise ValueError(f\'No entry found for host \\"{host}\\" and no default entry available.\') auth = default_host login, account, password = auth return { \'login\': login, \'account\': account, \'password\': password } def __repr__(self): Return a string representation of the .netrc file content. return str(self.netrc_content)"},{"question":"**Title:** Advanced List Management with Bisect **Objective:** Implement a custom data structure `SortedList` that maintains a list of elements in sorted order using the `bisect` module. This structure should have efficient methods for inserting elements and performing binary search operations. **Problem Statement:** You need to create a class `SortedList` that supports the following operations: 1. **Insert an element** into the list while maintaining sorted order. 2. **Find an element**: Return the index of the element if it exists, otherwise return -1. 3. **Find the leftmost element greater than or equal to** a given value. 4. **Find the rightmost element less than or equal to** a given value. Provide the implementation for the `SortedList` class with the following methods: - `def __init__(self)`: Initializes an empty `SortedList`. - `def insert(self, x: int) -> None`: Inserts an element `x` into the list, maintaining sorted order. - `def find(self, x: int) -> int`: Returns the index of element `x` in the list if it exists; otherwise, returns -1. - `def find_ge(self, x: int) -> int`: Returns the leftmost element greater than or equal to `x`. Raises a `ValueError` if no such element exists. - `def find_le(self, x: int) -> int`: Returns the rightmost element less than or equal to `x`. Raises a `ValueError` if no such element exists. **Constraints:** - Each method should run in O(log n) for searches and O(n) for insertions. - The input list contains integers and may include negative numbers. **Example:** ```python s_list = SortedList() s_list.insert(5) s_list.insert(1) s_list.insert(8) s_list.insert(3) assert s_list.find(5) == 2 assert s_list.find(7) == -1 assert s_list.find_ge(4) == 5 assert s_list.find_ge(8) == 8 assert s_list.find_le(7) == 5 assert s_list.find_le(1) == 1 ``` **Performance Requirements:** - Ensure efficient insertion and search operations. You are required to use the `bisect` module in your implementation.","solution":"import bisect class SortedList: def __init__(self): self._list = [] def insert(self, x: int) -> None: bisect.insort(self._list, x) def find(self, x: int) -> int: index = bisect.bisect_left(self._list, x) if index != len(self._list) and self._list[index] == x: return index return -1 def find_ge(self, x: int) -> int: index = bisect.bisect_left(self._list, x) if index != len(self._list): return self._list[index] raise ValueError(f\\"No element found greater than or equal to {x}\\") def find_le(self, x: int) -> int: index = bisect.bisect_right(self._list, x) if index: return self._list[index-1] raise ValueError(f\\"No element found less than or equal to {x}\\")"},{"question":"You are tasked with implementing a custom logging system using the Unix `syslog` library routines provided by the `syslog` module in Python. Specifically, you are to write a function that logs messages with various levels of priority and customizes the logging behavior by setting appropriate options. # Function Signature ```python def custom_syslog(message: str, priority: int = syslog.LOG_INFO, ident: str = None, logoption: int = 0, facility: int = syslog.LOG_USER) -> None: pass ``` # Input - `message` (str): The message to be logged. - `priority` (int, optional): The priority level of the log message. Default is `syslog.LOG_INFO`. - `ident` (str, optional): A string that is prepended to every message. Defaults to None, which will use `sys.argv[0]`. - `logoption` (int, optional): A bit field to set logging options. Default is 0. - `facility` (int, optional): The facility code for the log message. Default is `syslog.LOG_USER`. # Output The function should not return any value. It should log the given message with the specified priority and custom options using the Unix `syslog` system. # Constraints 1. Use the `syslog` module functions such as `syslog.syslog()`, `syslog.openlog()`, and `syslog.closelog()` to implement the desired functionality. 2. Ensure that messages with the given priority level are logged. 3. If the `ident` parameter is provided, log messages should be prepended with this identifier. 4. Utilize the appropriate log options and facilities. # Example ```python import syslog # Function implementation def custom_syslog(message: str, priority: int = syslog.LOG_INFO, ident: str = None, logoption: int = 0, facility: int = syslog.LOG_USER) -> None: if ident is not None: syslog.openlog(ident=ident, logoption=logoption, facility=facility) else: syslog.openlog(logoption=logoption, facility=facility) syslog.syslog(priority, message) syslog.closelog() # Test case custom_syslog(\'System initiated\', priority=syslog.LOG_INFO, ident=\'TestScript\', logoption=syslog.LOG_PID, facility=syslog.LOG_SYSLOG) ``` This example logs the message \\"System initiated\\" with the priority level `LOG_INFO`, identifier \\"TestScript\\", includes the process ID in the log, and sets the log facility to `LOG_SYSLOG`. # Additional Notes Make sure to test your function with various combinations of input parameters to ensure that it works correctly with different priority levels, identifiers, log options, and facilities.","solution":"import syslog def custom_syslog(message: str, priority: int = syslog.LOG_INFO, ident: str = None, logoption: int = 0, facility: int = syslog.LOG_USER) -> None: Logs a message to the system logger. Parameters: message (str): The message to be logged. priority (int, optional): The priority level of the log message. Default is syslog.LOG_INFO. ident (str, optional): A string that is prepended to every message. Defaults to None. logoption (int, optional): A bit field to set logging options. Default is 0. facility (int, optional): The facility code for the log message. Default is syslog.LOG_USER. if ident is not None: syslog.openlog(ident=ident, logoption=logoption, facility=facility) else: syslog.openlog(logoption=logoption, facility=facility) syslog.syslog(priority, message) syslog.closelog()"},{"question":"# Complex Math Solver You are tasked with creating a function that performs multiple mathematical operations on a given set of inputs and returns a specific result. You need to leverage various functions from the `math` module to complete this task. Function Signature ```python def complex_math_solver(a: float, b: float, n: int, k: int) -> float: pass ``` Parameters - `a` (float): A floating-point number representing the base for exponentiation. - `b` (float): A floating-point number representing the base for logarithm and the second number for comparison. - `n` (int): An integer for number-theoretic operations. - `k` (int): An integer for combinatorial operations. Returns - (float): A floating-point number representing the final result after performing a series of mathematical operations. Operations to Perform 1. Compute `a` raised to the power of `b` using `math.pow()`. 2. Compute the natural logarithm of `b` using `math.log()`. 3. Compute the binomial coefficient `C(n, k)` using `math.comb()`. 4. Compute the Euclidean distance between the points `(a, b)` and `(n, k)` using `math.dist()`. 5. Check if `a` and `b` are close to each other using `math.isclose()` with default tolerances. **Steps:** 1. Use `math.pow(a, b)` and store the result in `result_pow`. 2. Use `math.log(b)` and store the result in `result_log`. 3. Use `math.comb(n, k)` and store the result in `result_comb`. 4. Use `math.dist([a, b], [n, k])` and store the result in `result_dist`. 5. Use `math.isclose(a, b)` and store the result in `result_isclose`. 6. Combine the results as follows: - If `result_isclose` is `True`, return the sum of `result_pow` and `result_dist`. - Otherwise, return the product of `result_pow`, `result_log`, and `result_comb`. Example ```python result = complex_math_solver(2.0, 3.0, 5, 2) print(result) # Output depends on the combination of operations (detailed expected output is not provided to encourage independent reasoning) ``` Constraints 1. `a` and `b` are positive floating-point numbers. 2. `n` and `k` are non-negative integers with `k <= n`. Make sure to handle edge cases and validate inputs as needed.","solution":"import math def complex_math_solver(a: float, b: float, n: int, k: int) -> float: Performs multiple mathematical operations and returns a specific result. Parameters: - a (float): A floating-point number representing the base for exponentiation. - b (float): A floating-point number representing the base for logarithm and the second number for comparison. - n (int): An integer for number-theoretic operations. - k (int): An integer for combinatorial operations. Returns: - float: The result after performing the specified operations. # Compute a raised to the power of b result_pow = math.pow(a, b) # Compute the natural logarithm of b result_log = math.log(b) # Compute the binomial coefficient C(n, k) result_comb = math.comb(n, k) # Compute the Euclidean distance between points (a, b) and (n, k) result_dist = math.dist([a, b], [n, k]) # Check if a and b are close to each other result_isclose = math.isclose(a, b) # Combine the results based on checking if a and b are close if result_isclose: return result_pow + result_dist else: return result_pow * result_log * result_comb"},{"question":"**Coding Assessment Question** **Context**: You are tasked with writing a Python script to manage and compare temporary files within a specified directory. The script should be able to create temporary files, write data to them, compare their contents, and eventually clean up the temporary files. This will test your understanding of file and directory operations using the `pathlib`, `tempfile`, and `filecmp` modules. **Requirements**: 1. **Create Temporary Files**: - Write a function `create_temp_files(directory: str, n: int) -> list` where `directory` is the path where temporary files should be created, and `n` is the number of temporary files to create. - Each temporary file should be created within the provided directory using the `tempfile` module. - Return a list of `pathlib.Path` objects representing the temporary files created. 2. **Write Data to Files**: - Write a function `write_data_to_files(files: list, data: list)` where `files` is a list of `pathlib.Path` objects representing the temporary files created, and `data` is a list of strings to be written to the files. - Each string in the `data` list should be written to the corresponding file in the `files` list. 3. **Compare File Contents**: - Write a function `compare_files(file1: pathlib.Path, file2: pathlib.Path) -> bool` that compares the contents of two files using the `filecmp` module. - Return `True` if the contents are identical, `False` otherwise. 4. **File Cleanup**: - Write a function `cleanup_files(files: list)` to remove all files listed in the `files` list. **Constraints**: - Assume that the `directory` provided to the `create_temp_files` function exists and is writable. - The number of temporary files (`n`) and the number of data strings in the `data` list will be the same. - You must use the `pathlib`, `tempfile`, and `filecmp` modules as specified. **Example Usage**: ```python from pathlib import Path import tempfile import filecmp def create_temp_files(directory: str, n: int) -> list: # Implementation goes here def write_data_to_files(files: list, data: list): # Implementation goes here def compare_files(file1: pathlib.Path, file2: pathlib.Path) -> bool: # Implementation goes here def cleanup_files(files: list): # Implementation goes here # Example temp_dir = \\"/path/to/temp/dir\\" num_files = 3 file_data = [\\"Data for file 1\\", \\"Data for file 2\\", \\"Data for file 3\\"] # Step 1: Create temporary files temp_files = create_temp_files(temp_dir, num_files) # Step 2: Write data to files write_data_to_files(temp_files, file_data) # Step 3: Compare files are_files_equal = compare_files(temp_files[0], temp_files[1]) # Should be False if data is different # Step 4: Clean up files cleanup_files(temp_files) ``` Write the implementation for all the above mentioned functions.","solution":"from pathlib import Path import tempfile import filecmp def create_temp_files(directory: str, n: int) -> list: Create n temporary files in the specified directory. :param directory: Directory path to create temporary files in. :param n: Number of temporary files to create. :return: List of Path objects representing the created temporary files. temp_files = [] for _ in range(n): temp_file = tempfile.NamedTemporaryFile(dir=directory, delete=False) temp_files.append(Path(temp_file.name)) temp_file.close() # Close the file so it can be written to later return temp_files def write_data_to_files(files: list, data: list): Write data to the corresponding temporary files. :param files: List of Path objects representing the temporary files. :param data: List of strings to write to the corresponding files. for file, content in zip(files, data): with file.open(\'w\') as f: f.write(content) def compare_files(file1: Path, file2: Path) -> bool: Compare the contents of two files. :param file1: Path object for the first file. :param file2: Path object for the second file. :return: True if the contents of both files are identical, False otherwise. return filecmp.cmp(file1, file2, shallow=False) def cleanup_files(files: list): Remove all files listed in the files list. :param files: List of Path objects representing the files to remove. for file in files: file.unlink()"},{"question":"# Question You are required to implement a function `analyze_tensor` that takes a PyTorch tensor as input and returns a dictionary with the following elements: 1. `shape`: The shape of the tensor as a `torch.Size` object. 2. `num_dimensions`: The number of dimensions of the tensor. 3. `first_dimension_size`: The size of the tensor\'s first dimension. 4. `last_dimension_size`: The size of the tensor\'s last dimension. 5. `is_square_matrix`: A boolean indicating whether the tensor is a square 2D matrix (both dimensions are equal). Your implementation should handle tensors of any dimensionality (1D, 2D, 3D, etc.). Input Format - A PyTorch tensor of any dimensionality. Output Format - A dictionary with the keys specified above and their corresponding values. Constraints - Ensure that your solution can handle edge cases such as tensors with zero dimensions and high-dimensional tensors. - Do not use any library outside of PyTorch and standard Python libraries. Example ```python import torch # Example Tensor tensor = torch.ones(10, 20, 30) # Expected Output { \'shape\': torch.Size([10, 20, 30]), \'num_dimensions\': 3, \'first_dimension_size\': 10, \'last_dimension_size\': 30, \'is_square_matrix\': False } ``` Performance requirements - The function should efficiently handle tensors with large dimensions and sizes. Implement the function `analyze_tensor` below: ```python def analyze_tensor(tensor): # Your code here ```","solution":"import torch def analyze_tensor(tensor): Analyzes the provided PyTorch tensor and returns a dictionary containing various properties of the tensor. Parameters: tensor (torch.Tensor): The tensor to be analyzed. Returns: dict: A dictionary with the analyzed properties of the tensor. shape = tensor.shape num_dimensions = tensor.dim() first_dimension_size = tensor.size(0) if num_dimensions > 0 else None last_dimension_size = tensor.size(-1) if num_dimensions > 0 else None is_square_matrix = num_dimensions == 2 and shape[0] == shape[1] return { \'shape\': shape, \'num_dimensions\': num_dimensions, \'first_dimension_size\': first_dimension_size, \'last_dimension_size\': last_dimension_size, \'is_square_matrix\': is_square_matrix }"},{"question":"Suppose you are working on a project management tool that tracks tasks and their dependencies. You need to implement the core functionality using the `TopologicalSorter` class from the `graphlib` module to handle task scheduling. # Task Your task is to implement a function `schedule_tasks` that takes a list of tasks with their dependencies and returns a valid order of task execution if possible. If a cycle is detected in the tasks\' dependencies, the function should return a list representing the detected cycle. The function signature is: ```python def schedule_tasks(tasks: List[Tuple[str, List[str]]]) -> List[str] ``` # Input - `tasks`: A list of tuples, where each tuple contains a task (as a string) and a list of dependencies (each dependency is a string representing a task that must be completed before the given task). All tasks are uniquely identified by strings. # Output - A list of strings representing the tasks in a valid topological order. If no valid order exists due to a cycle, return a list representing the detected cycle. # Constraints - The maximum number of tasks is (10^4). - Each task and its dependencies are represented as strings. # Example ```python tasks = [ (\\"D\\", [\\"B\\", \\"C\\"]), (\\"C\\", [\\"A\\"]), (\\"B\\", [\\"A\\"]), (\\"A\\", []) ] print(schedule_tasks(tasks)) # Output: [\'A\', \'C\', \'B\', \'D\'] or any valid topological order tasks = [ (\\"D\\", [\\"B\\", \\"C\\"]), (\\"C\\", [\\"A\\"]), (\\"B\\", [\\"A\\"]), (\\"A\\", [\\"D\\"]) # Introducing a cycle ] print(schedule_tasks(tasks)) # Output: [\'D\', \'A\', \'C\'] or any representation of the detected cycle ``` # Note - You should use the `graphlib.TopologicalSorter` class to perform the topological sort. - Ensure proper handling of cycles using the `CycleError` exception. Good luck!","solution":"from typing import List, Tuple from graphlib import TopologicalSorter, CycleError def schedule_tasks(tasks: List[Tuple[str, List[str]]]) -> List[str]: ts = TopologicalSorter() for task, dependencies in tasks: ts.add(task, *dependencies) try: return list(ts.static_order()) except CycleError as e: return e.args[1] # Return the cycle that was detected"},{"question":"**Question: Implement a PyTorch XPU Computation and Memory Management Function** You are required to implement a function `xpu_computation_and_memory_mgmt` that performs the following tasks: 1. **Initialization**: - Ensure the XPU device is initialized. - Check if an XPU device is available; if not, raise a `RuntimeError`. 2. **Device Handling**: - Retrieve the current XPU device and its properties (name and capability). - Print the device\'s name and capability. 3. **Random Number Generation**: - Set a manual seed for RNG to `42`. - Generate a random tensor of shape `(1000, 1000)` on the XPU device. 4. **Memory Management**: - Before any operations, capture and print the initial memory allocated and reserved on the XPU device. - Perform an element-wise multiplication of the tensor with itself. - Print the peak memory allocated and reserved after the operation. - Clear the cache memory after the computation. - Print the final memory allocated and reserved after clearing the cache. 5. **Synchronization**: - Ensure the computation stream is synchronized before querying memory statistics. # Function Signature ```python def xpu_computation_and_memory_mgmt(): pass ``` # Expected Output Your function should print: 1. XPU device name 2. XPU device capability 3. Initial memory allocated 4. Initial memory reserved 5. Peak memory allocated 6. Peak memory reserved 7. Final memory allocated 8. Final memory reserved # Constraints - Assume that the XPU device, if available, supports the necessary operations. - Ensure efficient memory usage and proper synchronization of operations. ```python # Example Usage xpu_computation_and_memory_mgmt() ``` # Notes - You may need to use various functions from the `torch.xpu` module such as `is_available`, `set_device`, `current_device`, `memory_allocated`, `memory_reserved`, `manual_seed`, `empty_cache`, etc. - Consider edge cases where the XPU might not be available and handle them appropriately.","solution":"import torch def xpu_computation_and_memory_mgmt(): if not torch.xpu.is_available(): raise RuntimeError(\\"XPU device is not available\\") device = torch.xpu.current_device() device_name = torch.xpu.get_device_name(device) device_capability = torch.xpu.get_device_capability(device) print(f\\"Device Name: {device_name}\\") print(f\\"Device Capability: {device_capability}\\") torch.manual_seed(42) tensor = torch.rand((1000, 1000), device=\'xpu\') initial_mem_allocated = torch.xpu.memory_allocated(device) initial_mem_reserved = torch.xpu.memory_reserved(device) print(f\\"Initial memory allocated: {initial_mem_allocated}\\") print(f\\"Initial memory reserved: {initial_mem_reserved}\\") result_tensor = tensor * tensor torch.xpu.synchronize() peak_mem_allocated = torch.xpu.max_memory_allocated(device) peak_mem_reserved = torch.xpu.max_memory_reserved(device) print(f\\"Peak memory allocated: {peak_mem_allocated}\\") print(f\\"Peak memory reserved: {peak_mem_reserved}\\") torch.xpu.empty_cache() final_mem_allocated = torch.xpu.memory_allocated(device) final_mem_reserved = torch.xpu.memory_reserved(device) print(f\\"Final memory allocated: {final_mem_allocated}\\") print(f\\"Final memory reserved: {final_mem_reserved}\\")"},{"question":"# Goal: Implement and compare Outlier Detection and Novelty Detection using scikit-learn\'s `IsolationForest` and `OneClassSVM` models given datasets for training and testing. # Dataset: - `X_train`: A numpy array of shape (n_samples_train, n_features) representing the training dataset. - `X_test`: A numpy array of shape (n_samples_test, n_features) representing the new observations to be tested. - Assume the datasets are preloaded. # Implementation: 1. Fit the `IsolationForest` and `OneClassSVM` models for outlier detection using the `X_train` data. 2. Fit the `OneClassSVM` and `LocalOutlierFactor` models for novelty detection using the `X_train` data (ensure `LocalOutlierFactor` is instantiated with `novelty=True`). 3. Use the fitted models to predict anomalies in the `X_test` data. # Constraints: - Use `contamination=0.1` for the `IsolationForest`. - Use the RBF kernel for `OneClassSVM` and set `nu=0.1`. - Use default parameters for `neighbors.LocalOutlierFactor`. # Steps: 1. **Outlier Detection:** - Fit `IsolationForest` and `OneClassSVM` on `X_train` data. - Use `predict` method to label observations in `X_test` (Inliers = 1, Outliers = -1). 2. **Novelty Detection:** - Fit `OneClassSVM` and `neighbors.LocalOutlierFactor` (with `novelty=True`) on `X_train` data. - Use `predict` method to label observations in `X_test` (Inliers = 1, Outliers = -1). 3. **Evaluation:** - Compare and report the number of outliers detected by each method. # Example function signature: ```python from sklearn.ensemble import IsolationForest from sklearn.svm import OneClassSVM from sklearn.neighbors import LocalOutlierFactor def detect_anomalies(X_train, X_test): # Outlier Detection using IsolationForest iso_forest = IsolationForest(contamination=0.1) iso_forest.fit(X_train) outliers_isoforest = iso_forest.predict(X_test) # Outlier Detection using OneClassSVM one_class_svm_outlier = OneClassSVM(kernel=\\"rbf\\", nu=0.1) one_class_svm_outlier.fit(X_train) outliers_svm = one_class_svm_outlier.predict(X_test) # Novelty Detection using OneClassSVM one_class_svm_novelty = OneClassSVM(kernel=\\"rbf\\", nu=0.1) one_class_svm_novelty.fit(X_train) novelties_svm = one_class_svm_novelty.predict(X_test) # Novelty Detection using LocalOutlierFactor lof = LocalOutlierFactor(novelty=True) lof.fit(X_train) novelties_lof = lof.predict(X_test) # Return the labels from each detection method return { \\"outliers_isoforest\\": outliers_isoforest, \\"outliers_svm\\": outliers_svm, \\"novelties_svm\\": novelties_svm, \\"novelties_lof\\": novelties_lof } ``` # Expected Output: - A dictionary with keys `\\"outliers_isoforest\\"`, `\\"outliers_svm\\"`, `\\"novelties_svm\\"`, `\\"novelties_lof\\"` representing the predictions for the `X_test` set. - Lists/arrays of labels, with inliers as `1` and outliers as `-1`. # Note: - Ensure that your implementation adheres to the constraints provided. - Explain any deviations taken from the instructions.","solution":"from sklearn.ensemble import IsolationForest from sklearn.svm import OneClassSVM from sklearn.neighbors import LocalOutlierFactor def detect_anomalies(X_train, X_test): Perform Outlier and Novelty detection using IsolationForest, OneClassSVM, and LocalOutlierFactor. Parameters: X_train (numpy array): Training dataset of shape (n_samples_train, n_features) X_test (numpy array): Testing dataset of shape (n_samples_test, n_features) Returns: dict: Dictionary containing predictions from each model for X_test # Outlier Detection using IsolationForest iso_forest = IsolationForest(contamination=0.1) iso_forest.fit(X_train) outliers_isoforest = iso_forest.predict(X_test) # Outlier Detection using OneClassSVM one_class_svm_outlier = OneClassSVM(kernel=\\"rbf\\", nu=0.1) one_class_svm_outlier.fit(X_train) outliers_svm = one_class_svm_outlier.predict(X_test) # Novelty Detection using OneClassSVM one_class_svm_novelty = OneClassSVM(kernel=\\"rbf\\", nu=0.1) one_class_svm_novelty.fit(X_train) novelties_svm = one_class_svm_novelty.predict(X_test) # Novelty Detection using LocalOutlierFactor lof = LocalOutlierFactor(novelty=True) lof.fit(X_train) novelties_lof = lof.predict(X_test) # Return the labels from each detection method return { \\"outliers_isoforest\\": outliers_isoforest, \\"outliers_svm\\": outliers_svm, \\"novelties_svm\\": novelties_svm, \\"novelties_lof\\": novelties_lof }"},{"question":"You are required to implement a function that takes an existing Python object supporting the buffer interface and performs specific manipulations using memoryview objects. Task Write a function `manipulate_memoryview(obj, offset, length)` that: 1. Creates a memoryview object from the given `obj`. 2. Extracts a contiguous chunk of the memory starting from the given `offset` and of the specified `length`. 3. If the memory is not contiguous, the function should handle making a copy to ensure contiguity. 4. Returns a new bytes object with the manipulated contents of the memory chunk. Input - `obj`: An object that supports the buffer interface (e.g., `bytearray`, `bytes`, etc.). - `offset`: An integer indicating the starting position of the chunk within the memory. - `length`: An integer indicating the length of the memory chunk to be extracted. Output - A bytes object containing the manipulated contents of the specified memory chunk. Constraints - The offset and length should not exceed the bounds of the object\'s memory space. - Handle cases where the memory might be non-contiguous, ensuring the result is from a contiguous chunk of memory. Example ```python def manipulate_memoryview(obj, offset, length): # Implement the function here. # Example usage: original = bytearray([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) result = manipulate_memoryview(original, 2, 5) print(result) # Output: b\'x02x03x04x05x06\' ``` In this example, the function should correctly handle the memory view, possibly making a copy if the memory is not contiguous, and return the expected bytes object.","solution":"def manipulate_memoryview(obj, offset, length): Creates a memoryview from the given object, extracts a chunk starting from an offset and of the specified length, and ensures the chunk is contiguous. Returns a bytes object. # Create the memoryview mv = memoryview(obj) # Extract the required chunk chunk = mv[offset:offset + length] # Ensure the chunk is contiguous and convert to bytes if not chunk.contiguous: chunk = chunk.tobytes() else: chunk = bytes(chunk) return chunk"},{"question":"**Objective**: To assess the understanding and application of asynchronous programming concepts in Python using the `asyncio` package. **Problem Statement:** You are tasked with developing a simple asynchronous simulation of a library system where users can borrow and return books. The library system must handle multiple users concurrently and ensure that books are available before allowing a borrow action. Implement the following functions: 1. **`async def borrow_book(user_id: int, book_title: str) -> str`**: - Simulates a user borrowing a book. - Waits for a random time between 1 to 3 seconds to simulate the time taken for the operation. - If the book is available in the `library_books` list, \\"borrows\\" the book (removes it from the list) and returns a success message. - If the book is not available, returns a failure message. 2. **`async def return_book(user_id: int, book_title: str) -> str`**: - Simulates a user returning a book. - Waits for a random time between 1 to 2 seconds to simulate the time taken for the operation. - Adds the book back to the `library_books` list and returns a success message. 3. **`async def manage_library(users: List[Tuple[int, str, str]]) -> None`**: - Manages multiple users\' book borrow and return actions concurrently. - Each element in `users` is a tuple `(user_id, action, book_title)` where `action` is either `\\"borrow\\"` or `\\"return\\"`. - Use `asyncio.gather()` to run all user actions concurrently. - Print the result of each action. **Constraints**: - A book title can only be borrowed if it is available in `library_books`. - Books are identified by their titles and are unique in the library\'s collection. - The `library_books` list is initially shared and can be modified by the functions. **Example**: ```python import asyncio import random from typing import List, Tuple library_books = [\\"book1\\", \\"book2\\", \\"book3\\", \\"book4\\", \\"book5\\"] # Your implementation here async def main(): users = [ (1, \\"borrow\\", \\"book1\\"), (2, \\"borrow\\", \\"book2\\"), (3, \\"return\\", \\"book1\\"), (4, \\"borrow\\", \\"book1\\"), (5, \\"return\\", \\"book2\\"), (6, \\"borrow\\", \\"book3\\") ] await manage_library(users) asyncio.run(main()) ``` **Expected Output**: ``` User 1 borrowed book1 User 2 borrowed book2 User 3 returned book1 User 4 borrowed book1 User 5 returned book2 User 6 borrowed book3 ``` Implement these functions ensuring proper use of asynchronous programming techniques as discussed. **Input**: - A list of tuples representing user actions: `List[Tuple[int, str, str]]` **Output**: - Printed statements of each user action result. **Note**: - Use `asyncio.sleep()` to simulate time delays. - Handle concurrency using `asyncio.create_task()` and `asyncio.gather()`.","solution":"import asyncio import random from typing import List, Tuple library_books = [\\"book1\\", \\"book2\\", \\"book3\\", \\"book4\\", \\"book5\\"] async def borrow_book(user_id: int, book_title: str) -> str: await asyncio.sleep(random.randint(1, 3)) if book_title in library_books: library_books.remove(book_title) return f\\"User {user_id} borrowed {book_title}\\" else: return f\\"User {user_id} failed to borrow {book_title}, not available\\" async def return_book(user_id: int, book_title: str) -> str: await asyncio.sleep(random.randint(1, 2)) library_books.append(book_title) return f\\"User {user_id} returned {book_title}\\" async def manage_library(users: List[Tuple[int, str, str]]) -> None: tasks = [] for user_id, action, book_title in users: if action == \\"borrow\\": tasks.append(asyncio.create_task(borrow_book(user_id, book_title))) elif action == \\"return\\": tasks.append(asyncio.create_task(return_book(user_id, book_title))) results = await asyncio.gather(*tasks) for result in results: print(result) # Example main usage async def main(): users = [ (1, \\"borrow\\", \\"book1\\"), (2, \\"borrow\\", \\"book2\\"), (3, \\"return\\", \\"book1\\"), (4, \\"borrow\\", \\"book1\\"), (5, \\"return\\", \\"book2\\"), (6, \\"borrow\\", \\"book3\\") ] await manage_library(users) # Run the example if __name__ == \\"__main__\\": asyncio.run(main())"},{"question":"**Objective**: Demonstrate your understanding of using the `concurrent.futures` module to asynchronously execute tasks with both thread and process pools. Problem: You are required to write a function `factorial_sum(n: int) -> int` that calculates the sum of the factorials of all numbers from 1 to `n` (inclusive) using parallel task execution. You should utilize both `ThreadPoolExecutor` and `ProcessPoolExecutor` to demonstrate their effectiveness. # Function Signature: ```python def factorial_sum(n: int) -> int: ``` # Input: - `n`: An integer (1 ≤ n ≤ 1000), representing the upper limit of the range of numbers for which factorials are calculated. # Output: - Returns an integer representing the sum of the factorials of all numbers from 1 to `n` (inclusive). # Requirements: 1. Implement a helper function `compute_factorial(num: int) -> int` that computes the factorial of a given number. 2. Create a function `factorial_sum` that: - Utilizes `ThreadPoolExecutor` to compute factorials in parallel. - Utilizes `ProcessPoolExecutor` to compute factorials in parallel. - Compares the performance (execution time) of both approaches and prints out the execution time for each. - Returns the sum of the factorial results. # Constraints: - Ensure that the function handles potential exceptions and edge cases gracefully. - Use appropriate synchronization techniques if necessary to handle shared resources. # Example: ```python >>> factorial_sum(5) # Should return 153 since 1! + 2! + 3! + 4! + 5! = 153 # Additionally prints execution times of both ThreadPoolExecutor and ProcessPoolExecutor approaches. ``` # Notes: - Optimize the implementation to handle the upper limits efficiently. - Ensure that the function is well-documented and follows good coding practices.","solution":"import concurrent.futures import math import time def compute_factorial(num: int) -> int: Compute the factorial of a specified number. Args: num (int): The number to compute the factorial for. Returns: int: The factorial of the number. return math.factorial(num) def factorial_sum(n: int) -> int: Calculate the sum of the factorials of all numbers from 1 to n (inclusive) using both ThreadPoolExecutor and ProcessPoolExecutor to demonstrate their effectiveness. Args: n (int): The upper limit of the range of numbers for which factorials are calculated. Returns: int: The sum of the factorial results. numbers = list(range(1, n + 1)) # Using ThreadPoolExecutor start_time = time.time() with concurrent.futures.ThreadPoolExecutor() as executor: thread_results = list(executor.map(compute_factorial, numbers)) thread_time = time.time() - start_time # Using ProcessPoolExecutor start_time = time.time() with concurrent.futures.ProcessPoolExecutor() as executor: process_results = list(executor.map(compute_factorial, numbers)) process_time = time.time() - start_time print(f\\"ThreadPoolExecutor took {thread_time:.4f} seconds\\") print(f\\"ProcessPoolExecutor took {process_time:.4f} seconds\\") # Ensure that results from both executors are the same assert thread_results == process_results, \\"Results from ThreadPoolExecutor and ProcessPoolExecutor do not match.\\" return sum(thread_results) # Example usage (this should be removed in the final solution to meet requirements): # print(factorial_sum(5)) # Expected output: 153"},{"question":"You are tasked with creating a task scheduler that regularly processes job tasks using an `asyncio.Queue`. The tasks can have different priorities, and the scheduler must ensure that higher priority tasks are handled before lower priority ones. Implement the following functions to manage and process this task scheduler: 1. **enqueue_tasks(queue: asyncio.PriorityQueue, tasks: List[Tuple[int, Any]]) -> None**: - This function takes an `asyncio.PriorityQueue` and a list of tasks. Each task is represented as a tuple with a priority (an integer) and a task (any type of data). - The function should add all tasks to the queue using their priority. 2. **process_tasks(queue: asyncio.PriorityQueue) -> None**: - This function takes an `asyncio.PriorityQueue`. - It should continuously retrieve and process tasks from the queue based on their priority until the queue is empty. - After retrieving each task, it should simulate processing by waiting for a short duration using `asyncio.sleep`. - Use the `task_done` method to indicate task completion. 3. **main() -> None**: - This is the main function that initializes the task scheduler. - Create an `asyncio.PriorityQueue`. - Enqueue a set of sample tasks with varying priorities. - Ensure that there are at least 10 sample tasks with at least 5 different priority levels. - Run the scheduler to process all the tasks in priority order. # Constraints - Use `asyncio.PriorityQueue` for the queue. - Simulate task processing with `asyncio.sleep` (use a sleep duration of 0.1 seconds for simplicity). - Ensure proper usage of `task_done` and `join` to handle task completion and blocking correctly. # Input and Output - No explicit input; you are required to demonstrate the functionality through the implementation of the provided functions. - Print the task being processed and its priority. # Example ```python import asyncio async def enqueue_tasks(queue, tasks): for priority, task in tasks: await queue.put((priority, task)) async def process_tasks(queue): while not queue.empty(): priority, task = await queue.get() await asyncio.sleep(0.1) # Simulate task processing print(f\'Processed task: {task} with priority: {priority}\') queue.task_done() async def main(): queue = asyncio.PriorityQueue() tasks = [(3, \'task1\'), (1, \'task2\'), (4, \'task3\'), (2, \'task4\'), (5, \'task5\')] await enqueue_tasks(queue, tasks) await queue.join() # Block until all tasks are processed asyncio.run(main()) ``` # Notes - The functions `enqueue_tasks` and `process_tasks` should be `async` functions that can be used with asyncio. - You can assume that the tasks are provided as a list of tuples where each tuple contains a priority and the actual task.","solution":"import asyncio from typing import List, Tuple, Any async def enqueue_tasks(queue: asyncio.PriorityQueue, tasks: List[Tuple[int, Any]]) -> None: Adds all tasks to the queue using their priority. :param queue: asyncio.PriorityQueue to add tasks to. :param tasks: List of tasks, where each task is a tuple of (priority, task). for priority, task in tasks: await queue.put((priority, task)) async def process_tasks(queue: asyncio.PriorityQueue) -> None: Continuously retrieves and processes tasks from the queue based on priority. :param queue: asyncio.PriorityQueue to retrieve tasks from. while not queue.empty(): priority, task = await queue.get() await asyncio.sleep(0.1) # Simulate task processing print(f\'Processed task: {task} with priority: {priority}\') queue.task_done() await queue.join() # Block until all tasks have been processed async def main() -> None: Main function to initialize the task scheduler, enqueue tasks, and run the scheduler. queue = asyncio.PriorityQueue() tasks = [ (3, \'task1\'), (1, \'task2\'), (4, \'task3\'), (2, \'task4\'), (5, \'task5\'), (1, \'task6\'), (2, \'task7\'), (3, \'task8\'), (4, \'task9\'), (5, \'task10\') ] await enqueue_tasks(queue, tasks) await process_tasks(queue) if __name__ == \\"__main__\\": asyncio.run(main())"},{"question":"Implement a custom subclass of `reprlib.Repr` to format the `dict` and `set` data types with specific rules: 1. **For dictionaries**: - Limit the number of key-value pairs displayed to 3. - If truncated, the representation should end with an ellipsis `\\" ... \\"`. 2. **For sets**: - Limit the number of elements displayed to 4. - If truncated, the representation should end with an ellipsis `\\" ... \\"`. You should also implement the custom formatting and use the subclass to print representations of example objects. Implementation Details: 1. **Subclass `reprlib.Repr`**: - Override the `repr_dict` method for dictionary formatting. - Override the `repr_set` method for set formatting. 2. **Create an instance of your subclass**: - Use the instance to print representations of the following example objects: a. A dictionary with more than 3 key-value pairs. b. A set with more than 4 elements. Function Signature: ```python import reprlib class CustomRepr(reprlib.Repr): def repr_dict(self, obj, level): pass def repr_set(self, obj, level): pass # Create an instance of CustomRepr custom_repr = CustomRepr() # Test objects example_dict = {\'a\': 1, \'b\': 2, \'c\': 3, \'d\': 4, \'e\': 5} example_set = {1, 2, 3, 4, 5, 6} # Print the custom representations print(custom_repr.repr(example_dict)) print(custom_repr.repr(example_set)) ``` # Constraints: - Your custom dictionary representation should not exceed 3 key-value pairs. - Your custom set representation should not exceed 4 elements. - Aim for efficient and readable implementations. # Example Output: ```python >>> print(custom_repr.repr(example_dict)) {\'a\': 1, \'b\': 2, \'c\': 3, ...} >>> print(custom_repr.repr(example_set)) {1, 2, 3, 4, ...} ``` You are expected to provide complete and functional code demonstrating the requested custom behavior.","solution":"import reprlib class CustomRepr(reprlib.Repr): def repr_dict(self, obj, level): if len(obj) > 3: items = list(obj.items())[:3] return \'{\' + \', \'.join(f\'{repr(k)}: {repr(v)}\' for k, v in items) + \', ... }\' else: return repr(obj) def repr_set(self, obj, level): if len(obj) > 4: items = list(obj)[:4] return \'{\' + \', \'.join(repr(e) for e in items) + \', ... }\' else: return repr(obj) # Create an instance of CustomRepr custom_repr = CustomRepr() # Test objects example_dict = {\'a\': 1, \'b\': 2, \'c\': 3, \'d\': 4, \'e\': 5} example_set = {1, 2, 3, 4, 5, 6} # Print the custom representations print(custom_repr.repr(example_dict)) print(custom_repr.repr(example_set))"},{"question":"# Question: Debugging CGI Scripts with `cgitb` You are tasked with managing a CGI-based web application and need to ensure that any uncaught exceptions are properly reported for debugging purposes. To handle these exceptions, you will use the `cgitb` module. Instructions: 1. **Initialization Step**: - Write a Python script that starts by enabling the `cgitb` module to handle exceptions. - Ensure that exceptions are displayed in HTML format and, additionally, logged to a folder named \\"logs\\". 2. **Implement a Function**: - Implement a function named `calculate_sum(a, b)` that: - Raises a `TypeError` if either `a` or `b` is not an integer. - Returns the sum of `a` and `b`. - Add a try-except block to catch exceptions, and use `cgitb` to handle and display the exception details. 3. **Simulate Errors**: - In the main section of your script, call the `calculate_sum` function with various incorrect inputs (such as strings or lists) to generate exceptions. - Ensure that these exceptions are both printed in HTML format (to standard output) and logged to the \\"logs\\" directory. Provide the complete Python script with clear and functional implementation. Expected Script Structure: ```python import cgitb import os # Enable cgitb to handle exceptions, outputting to HTML and logging to \\"logs\\" directory cgitb.enable(logdir=\\"logs\\", format=\\"html\\") def calculate_sum(a, b): try: if not isinstance(a, int) or not isinstance(b, int): raise TypeError(\\"Both arguments must be integers\\") return a + b except Exception: cgitb.handler() # Simulate errors with incorrect inputs to generate exceptions if __name__ == \\"__main__\\": print(calculate_sum(\\"hello\\", 5)) print(calculate_sum(10, [1, 2])) print(calculate_sum(10, None)) ``` Constraints: 1. The `cgitb.enable` function should be called only once at the beginning to set up the exception handling. 2. The `calculate_sum` function must be robust and handle exceptions using `cgitb.handler()` for detailed traceback reports. Evaluation Criteria: 1. Correct use of the `cgitb` module for exception handling. 2. Proper implementation of the `calculate_sum` function, including type checking and exception raising. 3. Ability to handle and log uncaught exceptions. 4. Code readability and structure.","solution":"import cgitb import os # Enable cgitb to handle exceptions, outputting to HTML and logging to the \\"logs\\" directory cgitb.enable(logdir=\\"logs\\", format=\\"html\\") def calculate_sum(a, b): try: if not isinstance(a, int) or not isinstance(b, int): raise TypeError(\\"Both arguments must be integers\\") return a + b except Exception: cgitb.handler() return None # Simulate errors with incorrect inputs to generate exceptions if __name__ == \\"__main__\\": print(calculate_sum(\\"hello\\", 5)) print(calculate_sum(10, [1, 2])) print(calculate_sum(10, None))"},{"question":"Implementing and Logging Custom Metrics in PyTorch **Objective:** Understand and implement custom event handling and stat logging in PyTorch using the `torch.monitor` module. Problem Statement As a data scientist, you are working on a deep learning project using PyTorch and need to monitor certain metrics during training. Specifically, you want to log custom events for loss and accuracy at every epoch. You must implement a solution to log these metrics and write custom event handlers that handle these logged events. Your task is to: 1. **Define a Training Loop:** Simulate a simple training loop for a neural network. 2. **Implement Logging:** Use `torch.monitor` to log the loss and accuracy at each epoch. 3. **Event Handling:** Create and register custom event handlers to process and print the logged metrics. 4. **Performance:** Ensure minimal performance impact during logging. Requirements - Implement a training loop that runs for 5 epochs. - At each epoch, simulate the calculation of `loss` and `accuracy`. For simplicity, you can use random values for these metrics. - Use `torch.monitor.log_event` to log the `loss` and `accuracy` for each epoch. - Define custom event handlers using `torch.monitor.register_event_handler` to print out the `loss` and `accuracy` after each epoch. - Ensure all aspects of the event and stat lifecycle are captured, including registration and unregistration of event handlers. Input and Output - **Input:** None required. - **Output:** Printed statements of `loss` and `accuracy` for each epoch. Constraints - You must use the `torch.monitor` module for event and stat handling. Implementation Details You need to: 1. Simulate a training loop with 5 epochs. 2. Log random loss and accuracy using `torch.monitor.log_event`. 3. Create event handlers for loss and accuracy, register them, and print the metrics. **Note:** You can assume access to the necessary PyTorch libraries. Example Output ```plaintext Epoch 1 - Loss: 0.4567, Accuracy: 0.8765 Epoch 2 - Loss: 0.4235, Accuracy: 0.8891 Epoch 3 - Loss: 0.3998, Accuracy: 0.9002 Epoch 4 - Loss: 0.3769, Accuracy: 0.9124 Epoch 5 - Loss: 0.3542, Accuracy: 0.9265 ``` **Here is a starting template to help you structure your solution:** ```python import torch import random # Function to simulate a random float metric def simulate_metric(): return round(random.uniform(0.3, 0.5), 4) # Define event handlers def loss_event_handler(event_name, value, time): print(f\\"Epoch {time} - Loss: {value:.4f}\\") def accuracy_event_handler(event_name, value, time): print(f\\"Epoch {time} - Accuracy: {value:.4f}\\") def main(): # Register event handlers torch.monitor.register_event_handler(\'loss_event\', loss_event_handler) torch.monitor.register_event_handler(\'accuracy_event\', accuracy_event_handler) # Simulate training loop for epoch in range(1, 6): loss = simulate_metric() accuracy = simulate_metric() # Log events torch.monitor.log_event(\'loss_event\', loss, epoch) torch.monitor.log_event(\'accuracy_event\', accuracy, epoch) # Unregister event handlers after training torch.monitor.unregister_event_handler(\'loss_event\') torch.monitor.unregister_event_handler(\'accuracy_event\') if __name__ == \'__main__\': main() ```","solution":"import torch import random from datetime import datetime # Function to simulate a random float metric def simulate_metric(): return round(random.uniform(0.3, 0.5), 4) # Define event handlers def loss_event_handler(event_name, value, time): print(f\\"Epoch {time} - Loss: {value:.4f}\\") def accuracy_event_handler(event_name, value, time): print(f\\"Epoch {time} - Accuracy: {value:.4f}\\") def main(): # Register event handlers torch.monitor.register_event_handler(\'loss_event\', loss_event_handler) torch.monitor.register_event_handler(\'accuracy_event\', accuracy_event_handler) # Simulate training loop for epoch in range(1, 6): loss = simulate_metric() accuracy = simulate_metric() # Log events torch.monitor.log_event(\'loss_event\', loss, epoch) torch.monitor.log_event(\'accuracy_event\', accuracy, epoch) # Unregister event handlers after training torch.monitor.unregister_event_handler(\'loss_event\') torch.monitor.unregister_event_handler(\'accuracy_event\') if __name__ == \'__main__\': main()"},{"question":"Objective You are tasked with implementing a function that uses the `xdrlib` module to encode and decode a custom data structure. The goal is to assess your ability to utilize the `Packer` and `Unpacker` classes to handle multiple data types and ensure correct data serialization and deserialization. Requirements 1. **Function Implementation**: - Implement a function `encode_data(data: dict) -> bytes` that takes a dictionary as input, packs its content using `xdrlib.Packer`, and returns the packed byte stream. - Implement a function `decode_data(encoded_data: bytes) -> dict` that takes the packed byte stream from the previous function, unpacks it using `xdrlib.Unpacker`, and returns the original dictionary. 2. **Data Structure**: - The input dictionary will contain the following keys and associated values: - `\'name\'` (string): A name of maximum length 100. - `\'age\'` (integer): An age value. - `\'weight\'` (float): A weight value. - `\'friends\'` (list of strings): A list of friends\' names of variable length (each name of maximum length 50). 3. **Constraints**: - Ensure all strings are padded to maintain 4-byte alignment as required by `xdrlib`. - Handle potential exceptions and provide meaningful error messages if packing or unpacking fails. 4. **Performance**: - Aim for efficiency in both time and space complexity. The solution should handle up to 1000 friends in the `\'friends\'` list efficiently. Function Signatures ```python import xdrlib def encode_data(data: dict) -> bytes: pass def decode_data(encoded_data: bytes) -> dict: pass ``` Example ```python data = { \'name\': \'Alice\', \'age\': 30, \'weight\': 70.5, \'friends\': [\'Bob\', \'Charlie\', \'David\'] } encoded = encode_data(data) print(f\\"Encoded data: {encoded}\\") decoded = decode_data(encoded) print(f\\"Decoded data: {decoded}\\") # Expected Output: # Encoded data: <some byte stream> # Decoded data: { # \'name\': \'Alice\', # \'age\': 30, # \'weight\': 70.5, # \'friends\': [\'Bob\', \'Charlie\', \'David\'] # } ``` Notes - Carefully manage the order in which data is packed and unpacked to maintain the integrity and correctness of the data structure. - Test the functions with various input cases to ensure robustness. Good luck!","solution":"import xdrlib def encode_data(data: dict) -> bytes: packer = xdrlib.Packer() try: # Pack \'name\' packer.pack_string(data[\'name\'].encode(\'utf-8\')) # Pack \'age\' packer.pack_int(data[\'age\']) # Pack \'weight\' packer.pack_float(data[\'weight\']) # Pack \'friends\' list friends = data[\'friends\'] packer.pack_int(len(friends)) # Number of friends for friend in friends: packer.pack_string(friend.encode(\'utf-8\')) except Exception as e: raise ValueError(f\\"Error while packing data: {e}\\") return packer.get_buffer() def decode_data(encoded_data: bytes) -> dict: unpacker = xdrlib.Unpacker(encoded_data) try: # Unpack \'name\' name = unpacker.unpack_string().decode(\'utf-8\') # Unpack \'age\' age = unpacker.unpack_int() # Unpack \'weight\' weight = unpacker.unpack_float() # Unpack \'friends\' list num_friends = unpacker.unpack_int() friends = [] for _ in range(num_friends): friends.append(unpacker.unpack_string().decode(\'utf-8\')) except Exception as e: raise ValueError(f\\"Error while unpacking data: {e}\\") return {\'name\': name, \'age\': age, \'weight\': weight, \'friends\': friends}"},{"question":"Objective: Implement a caching system for expensive computations using Python\'s `weakref` module. The cache should hold the results of computations weakly so that they do not prevent garbage collection of the inputs. Details: 1. Create a class `ExpensiveComputationCache` that utilizes weak references to store the results of computations. 2. The class should have the following methods: - `compute(input_data)`: This method takes an input, performs a computationally expensive task on it, and returns the result. If the result for the given input already exists in the cache, return the cached result instead of recomputing it. - `get_cache_size()`: This method returns the current number of items in the cache. Requirements: - Use `weakref.WeakValueDictionary` to store the computational results. - Ensure that the cache does not hold strong references to the results or the input objects. - Design the cache so that the computation gets called only when the result is not already cached or when the cached result has been garbage collected. Example: ```python import weakref class ExpensiveComputationCache: def __init__(self): self._cache = weakref.WeakValueDictionary() def compute(self, input_data): if input_data in self._cache: print(\\"Using cached result\\") return self._cache[input_data] else: # Simulate an expensive computation result = input_data * input_data self._cache[input_data] = result print(\\"Computing result\\") return result def get_cache_size(self): return len(self._cache) # Usage cache = ExpensiveComputationCache() print(cache.compute(3)) # Output: Computing result # 9 print(cache.compute(3)) # Output: Using cached result # 9 print(f\\"Cache size: {cache.get_cache_size()}\\") # Output: Cache size: 1 ``` Notes: - Input objects should be suitable for weak reference dictionaries. You might need to ensure that the input is hashable and can be used as a weak key. - Remember that when the only references to an object are weak references, it can be garbage collected. Your solution should handle this gracefully. - Do not modify the inputs or results during the process. This question requires understanding and application of `weakref` concepts and proper use of weak references to manage a cache efficiently.","solution":"import weakref class ExpensiveComputationCache: def __init__(self): self._cache = weakref.WeakKeyDictionary() def compute(self, input_data): # Using a local reference dictionary to avoid holding strong references if input_data in self._cache: print(\\"Using cached result\\") return self._cache[input_data] else: # Simulate an expensive computation result = input_data.value * input_data.value self._cache[input_data] = result print(\\"Computing result\\") return result def get_cache_size(self): return len(self._cache) class Computable: A wrapper class to make the object suitable for WeakKeyDictionary. def __init__(self, value): self.value = value def __hash__(self): return hash(self.value) def __eq__(self, other): return isinstance(other, Computable) and self.value == other.value"},{"question":"You are required to implement a function `transform_html_entities(text: str, reverse: bool) -> str` that takes a text string containing HTML entity names and decodes them into their corresponding Unicode characters. Additionally, if the `reverse` parameter is `True`, it should encode the Unicode characters back into their HTML entity names. Function Signature ```python def transform_html_entities(text: str, reverse: bool) -> str: pass ``` Parameters - `text` (str): A string that may contain HTML entity names or Unicode characters. - `reverse` (bool): If `True`, encode the Unicode characters into HTML entity names. If `False`, decode the HTML entity names into Unicode characters. Returns - `str`: The transformed text. # Examples ```python # Example 1 input_text = \\"The price is &gt; 100 &amp; less than &lt; 200.\\" reverse = False assert transform_html_entities(input_text, reverse) == \\"The price is > 100 & less than < 200.\\" # Example 2 input_text = \\"The price is > 100 & less than < 200.\\" reverse = True assert transform_html_entities(input_text, reverse) == \\"The price is &gt; 100 &amp; less than &lt; 200.\\" ``` # Constraints - You must use the dictionaries provided in the `html.entities` module to perform the transformations. - The input string may contain multiple HTML entity names or Unicode characters. - The transformation should handle both lower-case and upper-case HTML entity names. # Notes - HTML entity names are generally case-insensitive, but they are defined in lower-case in conventional usage. - The `name2codepoint` dictionary maps HTML entity names to Unicode code points. - The `codepoint2name` dictionary maps Unicode code points to HTML entity names. - Consider edge cases like multiple consecutive entities or characters that do not have a corresponding HTML entity name. # Hints - Utilize regular expressions to find and replace entities efficiently. - Convert the string to and from character codes considering the `reverse` flag. Make sure your solution is efficient and correctly handles a diverse set of input cases.","solution":"import html from html.entities import name2codepoint, codepoint2name def transform_html_entities(text: str, reverse: bool) -> str: if reverse: # Convert Unicode characters to HTML entities return \'\'.join(f\\"&{codepoint2name[ord(char)]};\\" if ord(char) in codepoint2name else char for char in text) else: # Convert HTML entities to Unicode characters return html.unescape(text)"},{"question":"You are given multiple text files, each containing several lines of text. Your task is to write a Python function that processes these files and produces a summary report. The summary report should include: 1. The total number of lines read across all files. 2. A dictionary where the keys are filenames and the values are the number of lines in each file. 3. A dictionary where the keys are filenames and the values are lists containing the first and last lines of each file (including the newline character if present). Function Signature ```python def process_files(file_list: list) -> tuple: Processes the provided list of files and returns a summary report. Args: - file_list (list): A list of filenames (str) to be processed. Returns: - tuple: A tuple containing: - total_lines (int): Total number of lines read across all files. - lines_per_file (dict): A dictionary with filenames as keys and the number of lines as values. - boundary_lines (dict): A dictionary with filenames as keys and lists containing the first and last line of each file. pass ``` Input - `file_list` (list): A list of filenames (strings) to be processed. The files may be compressed (`.gz` or `.bz2`). Output - `total_lines` (int): Total number of lines read across all files. - `lines_per_file` (dict): A dictionary with filenames as keys and the number of lines as values. - `boundary_lines` (dict): A dictionary with filenames as keys and lists containing the first and last lines of each file. Constraints - Ensure your solution handles empty files correctly. - You may assume that filenames are valid and accessible. - Your function should efficiently handle large files without using excessive memory. Example ```python # Assume \'file1.txt\', \'file2.txt\', and \'compressed_file.gz\' exist in the current directory. file_list = [\'file1.txt\', \'file2.txt\', \'compressed_file.gz\'] # Expected output could be something like: # (total_lines, lines_per_file, boundary_lines) = # (50, {\'file1.txt\': 20, \'file2.txt\': 15, \'compressed_file.gz\': 15}, # {\'file1.txt\': [\'First line of file1n\', \'Last line of file1n\'], # \'file2.txt\': [\'First line of file2n\', \'Last line of file2n\'], # \'compressed_file.gz\': [\'First line of compressed_filen\', \'Last line of compressed_filen\']}) print(process_files(file_list)) ``` Note - You should use the `fileinput` module to process the files. - Utilize the `fileinput.hook_compressed` opening hook to handle compressed files.","solution":"import fileinput def process_files(file_list: list) -> tuple: Processes the provided list of files and returns a summary report. Args: - file_list (list): A list of filenames (str) to be processed. Returns: - tuple: A tuple containing: - total_lines (int): Total number of lines read across all files. - lines_per_file (dict): A dictionary with filenames as keys and the number of lines as values. - boundary_lines (dict): A dictionary with filenames as keys and lists containing the first and last line of each file. total_lines = 0 lines_per_file = {} boundary_lines = {} for filename in file_list: with fileinput.input(files=[filename], openhook=fileinput.hook_compressed) as f: lines = list(f) lines_count = len(lines) lines_per_file[filename] = lines_count total_lines += lines_count if lines_count > 0: boundary_lines[filename] = [lines[0], lines[-1]] else: boundary_lines[filename] = [\'\', \'\'] return total_lines, lines_per_file, boundary_lines"},{"question":"# Data Visualization with Seaborn **Objective:** Create a comprehensive data visualization using Seaborn that illustrates various relationships and distributions within a dataset. **Dataset:** You will use the built-in `tips` dataset from Seaborn. This dataset contains information about the tips left by customers in a restaurant, and includes the following columns: - `total_bill`: Total bill (cost of the meal) - `tip`: Tip given by the customer - `sex`: Gender of the person paying for the meal - `smoker`: Whether the person was a smoker or not - `day`: Day of the week - `time`: Time of the day (Lunch or Dinner) - `size`: Size of the party (number of people) **Task:** 1. **Load the Dataset:** Begin by loading the `tips` dataset using Seaborn\'s `load_dataset` function. 2. **Scatter Plot with Regression:** Create a scatter plot showing the relationship between `total_bill` and `tip`. Add a regression line to the plot to indicate the trend. 3. **Categorical Data Representation:** Create a violin plot to visualize the distribution of `total_bill` for different days of the week, while categorizing by `smoker` status. The violin plot should include split violins for smokers and non-smokers. 4. **Distribution Plot:** Create a distribution plot for the `tip` column. Include both a histogram and a Kernel Density Estimate (KDE). 5. **Pairwise Relationships:** Create a pair plot to visualize pairwise relationships and distributions for the `total_bill`, `tip`, and `size` columns. Use different colors for different days of the week. 6. **Custom Styling:** Apply a custom Seaborn theme to your plots. Set the style to \\"whitegrid\\" and scale up the font size for better readability. **Input and Output Formats:** - **Input:** - No external input required; the dataset will be loaded within the script. - **Output:** - Four plots as described in the task. **Constraints:** - Ensure that the plots are clear and well-labeled. - Use appropriate Seaborn functions to achieve the desired output. **Requirements:** - Your solution should demonstrate an understanding of the Seaborn library\'s capabilities and customization options. - Plots must be generated as visual outputs and shown in the notebook/script. **Example:** ```python # Sample Solution Example import seaborn as sns import matplotlib.pyplot as plt # Load the dataset tips = sns.load_dataset(\\"tips\\") # Set the theme sns.set_theme(style=\\"whitegrid\\", font_scale=1.5) # Scatter plot with regression line sns.lmplot(data=tips, x=\\"total_bill\\", y=\\"tip\\") plt.show() # Violin plot for total_bill by day and smoker status sns.violinplot(data=tips, x=\\"day\\", y=\\"total_bill\\", hue=\\"smoker\\", split=True) plt.show() # Distribution plot for tip sns.histplot(tips[\'tip\'], kde=True) plt.show() # Pair plot for total_bill, tip, and size by day sns.pairplot(tips, vars=[\\"total_bill\\", \\"tip\\", \\"size\\"], hue=\\"day\\") plt.show() ```","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_plots(): # Load the dataset tips = sns.load_dataset(\\"tips\\") # Set the theme sns.set_theme(style=\\"whitegrid\\", font_scale=1.5) # Scatter plot with regression line plt.figure() sns.lmplot(data=tips, x=\\"total_bill\\", y=\\"tip\\") plt.title(\'Scatter Plot with Regression Line\') plt.show() # Violin plot for total_bill by day and smoker status plt.figure() sns.violinplot(data=tips, x=\\"day\\", y=\\"total_bill\\", hue=\\"smoker\\", split=True) plt.title(\'Violin Plot for Total Bill by Day and Smoker Status\') plt.show() # Distribution plot for tip plt.figure() sns.histplot(tips[\'tip\'], kde=True) plt.title(\'Distribution Plot for Tip\') plt.show() # Pair plot for total_bill, tip, and size by day plt.figure() sns.pairplot(tips, vars=[\\"total_bill\\", \\"tip\\", \\"size\\"], hue=\\"day\\") plt.title(\'Pair Plot for Total Bill, Tip, and Size by Day\') plt.show()"},{"question":"# Email Retrieval Script Using `poplib` Objective: You are required to write a Python script that connects to a POP3 mail server, authenticates with the server using both plain text password and APOP methods, retrieves and displays emails, handles errors appropriately, and logs each action taken. Tasks: 1. **Establish Connection**: - Create a connection to a POP3 server using the `poplib.POP3` class. Include support for SSL/TLS using `POP3_SSL` if needed. - Ensure the connection has a reasonable timeout (e.g., 10 seconds). 2. **Authentication**: - First, try to authenticate using the plain text `user` and `pass_` methods. - If the server supports APOP, use the `apop` method for authentication. 3. **Retrieve Emails**: - Retrieve a list of emails from the server. - For each email, display the message header plus the first 10 lines of the body. 4. **Error Handling**: - Raise and handle the `poplib.error_proto` exception for any connection or command errors. - Log each interaction with the server (e.g., connections, commands sent, responses received) for auditing purposes. 5. **Disconnection**: - Ensure the connection to the POP3 server is correctly closed using the `quit` method. Constraints: - Use only the `poplib` module for POP3 interactions. - The script should be implemented in a class-based manner. - Do not use any third-party libraries except for logging. - Ensure the script logs messages for connections, commands sent, and responses received to a file named `mail.log`. Input: - Email server hostname. - Port number (default 110 for POP3, 995 for POP3 SSL). - Username. - Password. Output: - Print email headers and the first 10 lines of each email to the console. - Log all actions to `mail.log`. Example: ```python class EmailClient: def __init__(self, host, port, use_ssl=False): # Initialize connection parameters pass def connect(self): # Establish connection and handle SSL if use_ssl is True pass def authenticate(self, username, password): # Try plain text and APOP authentication pass def fetch_emails(self): # Retrieve and display emails pass def close_connection(self): # Properly close the connection pass if __name__ == \\"__main__\\": host = \'pop.example.com\' port = 110 # Use 995 for SSL username = \'your_username\' password = \'your_password\' client = EmailClient(host, port, use_ssl=False) try: client.connect() client.authenticate(username, password) client.fetch_emails() finally: client.close_connection() ``` Notes: - Test your script with a real POP3 server. - Ensure proper logging and error handling throughout the script.","solution":"import poplib import logging class EmailClient: def __init__(self, host, port, use_ssl=False): self.host = host self.port = port self.use_ssl = use_ssl self.server = None logging.basicConfig(filename=\'mail.log\', level=logging.INFO) def connect(self): try: if self.use_ssl: self.server = poplib.POP3_SSL(self.host, self.port, timeout=10) else: self.server = poplib.POP3(self.host, self.port, timeout=10) logging.info(f\\"Connected to server {self.host}:{self.port} successfully.\\") except Exception as e: logging.error(f\\"Failed to connect to server {self.host}:{self.port}: {e}\\") raise def authenticate(self, username, password): try: apop_supported = self.server.capabilities().get(\'RESP-CODES\', None) is not None if apop_supported: self.server.apop(username, password) logging.info(\\"Authenticated using APOP successfully.\\") else: self.server.user(username) self.server.pass_(password) logging.info(\\"Authenticated using plain text successfully.\\") except poplib.error_proto as e: logging.error(f\\"Authentication failed: {e}\\") raise def fetch_emails(self): try: num_messages = len(self.server.list()[1]) logging.info(f\\"Number of messages: {num_messages}\\") for i in range(num_messages): resp, lines, octets = self.server.retr(i+1) message = \\"n\\".join(line.decode(\'utf-8\') for line in lines) header, body = message.split(\\"nn\\", 1) print(f\\"Message {i+1} Header:n{header}\\") print(\\"Body (first 10 lines):\\") print(\\"n\\".join(body.split(\\"n\\")[:10])) logging.info(f\\"Retrieved message {i+1} successfully.\\") except poplib.error_proto as e: logging.error(f\\"Failed to retrieve messages: {e}\\") raise def close_connection(self): if self.server: try: self.server.quit() logging.info(\\"Disconnected from server successfully.\\") except poplib.error_proto as e: logging.error(f\\"Error in closing connection: {e}\\") # Example usage: if __name__ == \\"__main__\\": host = \'pop.example.com\' port = 110 # Use 995 for SSL username = \'your_username\' password = \'your_password\' client = EmailClient(host, port, use_ssl=False) try: client.connect() client.authenticate(username, password) client.fetch_emails() finally: client.close_connection()"},{"question":"# Multithreaded Inventory Management System You are tasked with implementing a multithreaded inventory management system for a warehouse where multiple threads represent worker robots performing various tasks such as adding items to inventory, fulfilling orders, and generating reports. This system needs to make use of threading for concurrent access and updating of shared resources. Your inventory management system should adhere to these requirements: 1. **Worker Robots (Threads)**: - **Adder Robot**: Adds `N` items to the inventory. - **Fulfiller Robot**: Fulfills an order by removing `N` items from the inventory. - **Reporter Robot**: Generates a report of the current inventory state. 2. **Inventory Management**: - Use a `Semaphore` to restrict the total number of items in the inventory to a maximum capacity. - Use a `Lock` to ensure that inventory updates are thread-safe. 3. **Functions to Implement**: - `InventoryManagement`: - Constructor initializes the inventory, semaphore, and lock. - `add_items(self, N: int)`: - Adder robot adds `N` items to the inventory. - The adding process should secure the semaphore and lock. - `fulfill_order(self, N: int)`: - Fulfiller robot fulfills an order by removing `N` items from the inventory. - The removing process should secure the semaphore and lock. - `generate_report(self)`: - Reporter robot generates and prints the current inventory state. - The report generation should secure the lock to ensure consistency. 4. **Constraints**: - Inventory capacity should be set to `100` items. - The operations should be thread-safe and efficient. 5. **Example Usage**: ```python import threading class InventoryManagement: def __init__(self, capacity=100): # Initialize inventory, semaphore for capacity, and lock pass def add_items(self, N): # Code to add items to inventory pass def fulfill_order(self, N): # Code to fulfill order from inventory pass def generate_report(self): # Code to generate inventory report pass def runner(): inventory = InventoryManagement() adder_thread = threading.Thread(target=inventory.add_items, args=(10,)) fulfiller_thread = threading.Thread(target=inventory.fulfill_order, args=(5,)) reporter_thread = threading.Thread(target=inventory.generate_report) adder_thread.start() fulfiller_thread.start() reporter_thread.start() adder_thread.join() fulfiller_thread.join() reporter_thread.join() if __name__ == \\"__main__\\": runner() ``` Implement the `InventoryManagement` class and its methods to ensure thread-safety and synchronization. Use appropriate threading mechanisms to achieve the required concurrency and resource management.","solution":"import threading class InventoryManagement: def __init__(self, capacity=100): self.capacity = capacity self.inventory = 0 self.semaphore = threading.Semaphore(capacity) self.lock = threading.Lock() def add_items(self, N): for _ in range(N): self.semaphore.acquire() with self.lock: self.inventory += 1 def fulfill_order(self, N): for _ in range(N): with self.lock: if self.inventory > 0: self.inventory -= 1 self.semaphore.release() else: break def generate_report(self): with self.lock: print(f\\"Current inventory: {self.inventory}\\") def runner(): inventory = InventoryManagement() adder_thread = threading.Thread(target=inventory.add_items, args=(10,)) fulfiller_thread = threading.Thread(target=inventory.fulfill_order, args=(5,)) reporter_thread = threading.Thread(target=inventory.generate_report) adder_thread.start() fulfiller_thread.start() reporter_thread.start() adder_thread.join() fulfiller_thread.join() reporter_thread.join() if __name__ == \\"__main__\\": runner()"},{"question":"**Objective:** Implement a function that processes a list of strings by escaping certain special characters for safe HTML display and then reverses this process to convert the escaped sequences back to their original form. **Task:** Write a Python function that takes a list of strings as input and returns a list of tuples, where each tuple contains the original string, the escaped string, and the unescaped string. Your function should use the `html.escape` and `html.unescape` functions from the `html` module to achieve this. **Function Signature:** ```python def process_html(strings: list[str]) -> list[tuple[str, str, str]]: # Your code goes here ``` **Input:** - `strings`: A list of strings where each string may contain various special characters like `&`, `<`, and `>`, among others. **Output:** - A list of tuples, where each tuple consists of: - The original string - The HTML-escaped string - The unescaped string converted back from the HTML-escaped string **Constraints:** - The input list may contain up to 1000 strings. - Each string in the input list will have a maximum length of 1000 characters. - The system should handle both named and numeric character references. **Example:** ```python strings = [\\"Hello & Welcome <User>\\", \\"5 > 3\\", \\"Usage of quotes: \\"Test\\" & \'Demo\'\\"] result = process_html(strings) ``` After processing, `result` should be: ```python [ (\\"Hello & Welcome <User>\\", \\"Hello &amp; Welcome &lt;User&gt;\\", \\"Hello & Welcome <User>\\"), (\\"5 > 3\\", \\"5 &gt; 3\\", \\"5 > 3\\"), (\\"Usage of quotes: \\"Test\\" & \'Demo\'\\", \\"Usage of quotes: &quot;Test&quot; &amp; &#x27;Demo&#x27;\\", \\"Usage of quotes: \\"Test\\" & \'Demo\'\\") ] ``` **Notes:** - Be sure to consider cases with both quotes and apostrophes (`\\"`, `\'`) when escaping, depending on the `quote` parameter in `html.escape`. Good luck and happy coding!","solution":"import html def process_html(strings: list[str]) -> list[tuple[str, str, str]]: Takes a list of strings and returns a list of tuples. Each tuple contains: - The original string - The HTML-escaped string - The HTML-unescaped string (back to its original form) result = [] for s in strings: escaped_str = html.escape(s) unescaped_str = html.unescape(escaped_str) result.append((s, escaped_str, unescaped_str)) return result"},{"question":"# Python File Handling and Debugging with Development Mode Problem Statement You are given a text file containing multiple lines of text. Your task is to write a Python program that reads this file and performs the following operations: 1. Counts the number of lines in the file. 2. Checks for the presence of any specific target word in each line. 3. Writes the lines containing the target word to another output file. 4. Handles resource management and error handling appropriately to avoid warnings and errors when the Python Development Mode is enabled. Requirements 1. Implement your solution in a function `process_file(input_file: str, output_file: str, target_word: str) -> int`. This function takes three arguments: - `input_file`: The path to the input text file. - `output_file`: The path to the output text file where lines containing the target word should be written. - `target_word`: The word to search for in each line of the input file. 2. The function should return the total number of lines in the input file. 3. Ensure that all file resources are properly closed to avoid `ResourceWarning` and other issues. 4. If the target word is found in a line, write the line to the output file and handle any potential I/O errors. 5. Use a context manager to manage file operations to guarantee that files are closed properly. 6. Make sure to test your code with the Python Development Mode to verify no warnings or errors are emitted. Constraints - The input file can be assumed to contain a maximum of 100,000 lines. - The length of each line does not exceed 1000 characters. - The function should handle and log errors gracefully. Example ```python # Assume \'input.txt\' contains: # Hello world # This is a Python script # Find the word Python in the text # Calling the function: number_of_lines = process_file(\\"input.txt\\", \\"output.txt\\", \\"Python\\") # The caller expects `number_of_lines` to be 3. # And \'output.txt\' should contain: # This is a Python script # Find the word Python in the text print(number_of_lines) # Output: 3 ``` Additional Notes - To run your script with Python Development Mode and check for potential issues, use: ```bash python3 -X dev your_script.py ``` - Additionally, you can enable `tracemalloc` to provide detailed allocation traceback: ```bash python3 -X dev -X tracemalloc=5 your_script.py ```","solution":"import logging def process_file(input_file: str, output_file: str, target_word: str) -> int: Processes the input file, counts the number of lines, and writes those containing the target word to the output file. line_count = 0 try: with open(input_file, \'r\') as in_file: with open(output_file, \'w\') as out_file: for line in in_file: line_count += 1 if target_word in line: out_file.write(line) except IOError as e: logging.error(f\\"I/O error occurred: {e}\\") except Exception as e: logging.error(f\\"An unexpected error occurred: {e}\\") return line_count"},{"question":"Problem Statement **Resource Management and Control** You are required to write a Python function that monitors and controls system resources for a given Python process. The function should achieve the following: 1. **Retrieve Current Limits**: Fetch and print the current soft and hard limits for the maximum number of open file descriptors (`RLIMIT_NOFILE`). 2. **Modify Limits**: Attempt to increase the soft limit of `RLIMIT_NOFILE` by a specified amount. If the new soft limit exceeds the hard limit or cannot be set for any reason, handle the exception gracefully and print an appropriate error message. 3. **Retrieve Usage**: Fetch and print the resource usage of the current process (`RUSAGE_SELF`), specifically focusing on the user CPU time and system CPU time. 4. **Report**: Finally, print the updated limits (if changed) or state why the change was not made. # Inputs and Outputs - **Input**: A single integer `increment` specifying the amount by which you wish to increase the soft limit of `RLIMIT_NOFILE`. - **Output**: The function should print the current and updated (or attempted) resource limits, resource usage information, and any error messages resulting from unsuccessful attempts to change limits. # Function Signature ```python def monitor_and_control_resources(increment: int) -> None: pass ``` # Example ```python from resource import * def monitor_and_control_resources(increment: int) -> None: try: # Retrieve current limits soft, hard = getrlimit(RLIMIT_NOFILE) print(f\\"Current limits: Soft={soft}, Hard={hard}\\") # Attempt to modify the soft limit new_soft = soft + increment try: setrlimit(RLIMIT_NOFILE, (new_soft, hard)) print(f\\"Updated limits: Soft={new_soft}, Hard={hard}\\") except ValueError as e: print(f\\"Failed to set new limits: {e}\\") # Retrieve resource usage usage = getrusage(RUSAGE_SELF) print(f\\"Resource usage: User time={usage.ru_utime}, System time={usage.ru_stime}\\") except Exception as e: print(f\\"An error occurred: {e}\\") # Example usage monitor_and_control_resources(1024) ``` # Constraints - Assume reasonable limits for the number of open files. - Make sure the function handles all potential exceptions gracefully. - The function should work on Unix-like systems where these resource controls are available. **Note**: Make sure to test your code on a Unix system, as the `resource` module behavior might differ across different operating systems.","solution":"import resource def monitor_and_control_resources(increment: int) -> None: try: # Retrieve current limits soft, hard = resource.getrlimit(resource.RLIMIT_NOFILE) print(f\\"Current limits: Soft={soft}, Hard={hard}\\") # Attempt to modify the soft limit new_soft = soft + increment try: resource.setrlimit(resource.RLIMIT_NOFILE, (new_soft, hard)) print(f\\"Updated limits: Soft={new_soft}, Hard={hard}\\") except ValueError as e: print(f\\"Failed to set new limits: {e}\\") # Retrieve resource usage usage = resource.getrusage(resource.RUSAGE_SELF) print(f\\"Resource usage: User time={usage.ru_utime}, System time={usage.ru_stime}\\") except Exception as e: print(f\\"An error occurred: {e}\\") # Example usage monitor_and_control_resources(1024)"},{"question":"Objective Your task is to implement a custom PyTorch autograd function that computes the forward and backward passes for the function ( f(x) = sqrt{x} ) while correctly handling the gradient computation in the presence of PyTorch\'s autograd mechanics and in-place operations. Problem Statement Implement a custom autograd function, `SqrtFunction`, which performs the following operations: 1. **Forward Pass**: Computes (y = sqrt{x}). 2. **Backward Pass**: Computes the gradient of the loss with respect to the input (x). You need to ensure that the implementation properly: - Saves intermediary tensors necessary for the backward pass using `save_for_backward`. - Handles the case where an in-place operation modifies the input tensor. Function Specification - **Class Name**: `SqrtFunction` - **Method Definitions**: - `@staticmethod def forward(ctx, input: torch.Tensor) -> torch.Tensor` - `@staticmethod def backward(ctx, grad_output: torch.Tensor) -> torch.Tensor` Input - `input`: A PyTorch tensor with `requires_grad=True`, representing the input tensor (x) to the forward function. Output - The forward method should return a tensor representing ( sqrt{x} ). - The backward method should return the gradient tensor with respect to the input (x). Constraints - The input tensor will always contain non-negative values to ensure the function is defined. - The implementation should handle in-place operations correctly. - Use the `save_for_backward` method to store tensors during the forward pass. Performance Requirements - Ensure your implementation is efficient and does not introduce unnecessary computational overhead. Example Usage ```python import torch from torch.autograd import Function class SqrtFunction(Function): @staticmethod def forward(ctx, input): # Implement forward pass here pass @staticmethod def backward(ctx, grad_output): # Implement backward pass here pass # Example usage x = torch.tensor([4.0, 9.0, 16.0], requires_grad=True) sqrt_x = SqrtFunction.apply(x) sqrt_x.sum().backward() print(x.grad) # Expected output: tensor([0.2500, 0.1667, 0.1250]) ``` Additional Requirements - Include error handling to ensure the function works as expected for the given constraints. - Write proper documentation for your code. - Add test cases to validate your implementation. **Notes**: - The `save_for_backward` method should be used within the forward method to store the necessary tensors, which can be retrieved in the backward method. - You can leverage the context variable (`ctx`) to store any information you need for backward computation.","solution":"import torch from torch.autograd import Function class SqrtFunction(Function): @staticmethod def forward(ctx, input): Performs the forward pass for the custom sqrt function. Args: ctx: Context to store information for the backward pass. input: Input tensor. Returns: Tensor representing the sqrt of input. # Save input for backward pass ctx.save_for_backward(input) # Compute the forward pass return input.sqrt() @staticmethod def backward(ctx, grad_output): Performs the backward pass for the custom sqrt function. Args: ctx: Context containing information saved during the forward pass. grad_output: Gradient of the loss with respect to the output. Returns: Tensor representing the gradient with respect to the input. # Retrieve saved input input, = ctx.saved_tensors # Compute the gradient of the loss with respect to the input grad_input = grad_output * 0.5 / input.sqrt() return grad_input"},{"question":"<|Analysis Begin|> The provided documentation is for the \\"pipes\\" module in Python, which is deprecated as of Python 3.11. The \\"pipes\\" module allows interaction with shell pipelines through templates. Key features include: 1. **Class Template** which allows creating and managing pipelines. 2. **Methods**: - `reset()`: Resets the pipeline template. - `clone()`: Clones the pipeline template. - `debug(flag)`: Enables or disables debugging. - `append(cmd, kind)`: Adds a new command to the end of the pipeline. - `prepend(cmd, kind)`: Adds a new command to the beginning of the pipeline. - `open(file, mode)`: Opens a file with the specified mode. - `copy(infile, outfile)`: Copies the content of infile to outfile through the pipeline. The module requires a POSIX compatible shell and is only available on Unix systems. Given the deprecation of this module and the shift towards using the \\"subprocess\\" module, an engaging question could involve handling pipelines using `subprocess` instead of `pipes`, while allowing students to understand the transition between both modules. <|Analysis End|> <|Question Begin|> **Coding Assessment Question:** Python has deprecated the \\"pipes\\" module and suggests using the \\"subprocess\\" module instead for handling pipelines. In this task, you are required to write a function that mimics some functionalities of the `pipes.Template` class using the `subprocess` module. # Objective: Write a class `Pipeline` that allows creating and managing sequences of shell commands executed via pipelines, similarly to the deprecated `pipes.Template` class. # Class: `Pipeline` # Methods: 1. `__init__(self)`: Initializes an empty pipeline. 2. `append(self, cmd)`: Appends a command to the end of the pipeline. 3. `prepend(self, cmd)`: Prepends a command to the beginning of the pipeline. 4. `execute(self, input=None)`: Executes the pipeline. If `input` is provided (string), it acts as standard input to the first command. # Input and Output: - `append(cmd)`: `cmd` is a string representing a shell command. - `prepend(cmd)`: `cmd` is a string representing a shell command. - `execute(input)`: `input` is an optional string for the standard input of the pipeline. This method returns the combined standard output of the pipeline. # Example Usage: ```python pipe = Pipeline() pipe.append(\\"tr a-z A-Z\\") output = pipe.execute(\\"hello world\\") print(output) # Prints: HELLO WORLD ``` # Constraints: - Commands will only contain basic POSIX shell commands. - Assume that the shell commands, when chained together, will not produce errors. - The pipeline should handle at least 10 commands without significant performance degradation. # Notes: - You are required to use the `subprocess` module to manage the pipeline. - Ensure that the pipeline functionality handles the sequencing of commands as described. # Your Task: Implement the `Pipeline` class as per the specification above.","solution":"import subprocess class Pipeline: def __init__(self): self.commands = [] def append(self, cmd): self.commands.append(cmd) def prepend(self, cmd): self.commands.insert(0, cmd) def execute(self, input=None): if not self.commands: raise ValueError(\\"No commands in the pipeline.\\") input_data = input.encode() if input else None # Setup initial process proc = subprocess.Popen(self.commands[0], shell=True, stdin=subprocess.PIPE if input_data else None, stdout=subprocess.PIPE, stderr=subprocess.PIPE) output, error = proc.communicate(input=input_data) for cmd in self.commands[1:]: proc = subprocess.Popen(cmd, shell=True, stdin=subprocess.PIPE if output else None, stdout=subprocess.PIPE, stderr=subprocess.PIPE) output, error = proc.communicate(input=output) if proc.returncode != 0: raise subprocess.CalledProcessError(proc.returncode, self.commands[-1], error) return output.decode()"},{"question":"You are tasked with creating a watchdog system for a multi-threaded Python application using the `faulthandler` module. The goal is to monitor the application for unresponsive behavior and produce a detailed traceback dump if a thread becomes unresponsive. Create the following two functions: 1. `watchdog_enable(timeout: int, logfile: str) -> None` - Enables the `faulthandler` with a mechanism to dump the tracebacks of all threads to a specified log file after a given timeout period. - If any thread becomes unresponsive (i.e., does not return from a function call) for more than `timeout` seconds, the `faulthandler` should dump the current state of all threads to the log file. - Parameters: - `timeout`: An integer representing the timeout in seconds. - `logfile`: A string representing the path to the log file where tracebacks will be dumped. 2. `watchdog_disable() -> None` - Disables the `faulthandler` watchdog. - This should cancel any previously set timeout for dumping tracebacks and unregister any signal handlers. Constraints: - Assume a multi-threaded environment. - The `logfile` must be kept open as per the faulthandler constraints. Expected Input and Output: - `watchdog_enable(5, \\"error.log\\")` will start monitoring and will dump the traceback in \\"error.log\\" if a thread is unresponsive for more than 5 seconds. - `watchdog_disable()` will stop the monitoring and unregister the handlers. You are also provided with a simple test case setup to evaluate your function: ```python import threading import time def long_running_task(): time.sleep(10) def test_watchdog(): watchdog_enable(5, \\"traceback.log\\") t = threading.Thread(target=long_running_task) t.start() t.join() watchdog_disable() test_watchdog() ``` In the test above, your `watchdog_enable` function should trigger and write a traceback to \\"traceback.log\\" after 5 seconds since `long_running_task` runs longer than the given timeout. Note: - Ensure that your log file handling complies with the issue described in the documentation regarding file descriptors.","solution":"import faulthandler import threading import time def watchdog_enable(timeout: int, logfile: str) -> None: Enables the faulthandler watchdog to dump the tracebacks of all threads to a specified log file after the given timeout period. Parameters: timeout (int): Timeout period in seconds. logfile (str): Path to the log file where tracebacks will be dumped. global watchdog_file watchdog_file = open(logfile, \'w\') faulthandler.dump_traceback_later(timeout, file=watchdog_file) def watchdog_disable() -> None: Disables the faulthandler watchdog. faulthandler.cancel_dump_traceback_later() global watchdog_file watchdog_file.close()"},{"question":"You are tasked with writing a Python script that ensures `pip` is installed and up-to-date in a given environment. You should utilize the `ensurepip` module\'s API to accomplish this. The script must also allow for optional user-specific installation and the ability to specify an alternative root directory. Requirements 1. Create a function `ensure_pip_installed(upgrade: bool = False, user: bool = False, root: str = None) -> str`. - **Parameters**: - `upgrade` (bool): If `True`, upgrade an existing installation of `pip` to the latest version available in `ensurepip`. Defaults to `False`. - `user` (bool): If `True`, install `pip` in the user site-packages directory. Defaults to `False`. - `root` (str): If provided, install `pip` relative to this root directory. Defaults to `None`. - **Returns**: - A string indicating the status of the operation (either \\"pip installed\\" or \\"pip upgraded\\"). 2. Implement the function using the `ensurepip.bootstrap` method. 3. Properly handle exceptions, especially when conflicting options such as `user` and `root` are provided together. 4. Ensure the function respects the environment it is executed in (i.e., handle virtual environments correctly). 5. Write a main script that demonstrates how to invoke this function with different scenarios and print the results. Example ```python def ensure_pip_installed(upgrade: bool = False, user: bool = False, root: str = None) -> str: import ensurepip try: ensurepip.bootstrap(upgrade=upgrade, user=user, root=root) if upgrade: return \\"pip upgraded\\" else: return \\"pip installed\\" except ValueError as e: raise ValueError(\\"Invalid combination of parameters: {}\\".format(e)) except Exception as e: raise RuntimeError(\\"An error occurred while installing/upgrading pip: {}\\".format(e)) if __name__ == \\"__main__\\": print(ensure_pip_installed(upgrade=True)) print(ensure_pip_installed(user=True)) print(ensure_pip_installed(root=\\"/some/custom/root/directory\\")) ``` Constraints - The function should explicitly avoid using both `user=True` and a non-`None` `root` value together, as these options are conflicting. - Ensure the function behaves correctly whether executed in a global environment or a virtual environment. - Properly document any exceptions that can be raised by the function. Testing Test the function in various scenarios: - Default installation. - Upgrading an existing `pip` installation. - User-specific installation. - Installation to a specified root directory. # Notes - Remember to import the `ensurepip` module in your function definition. - Due to the potential side effects on `sys.path` and `os.environ`, be cautious about the function\'s impact within different execution contexts.","solution":"import ensurepip def ensure_pip_installed(upgrade: bool = False, user: bool = False, root: str = None) -> str: Ensures that pip is installed and up-to-date, optionally installing it for the user or specifying a root directory. :param upgrade: If True, upgrades pip if it is already installed. :param user: If True, installs pip in the user site-packages directory. :param root: If provided, installs/updates pip relative to this root directory. :return: A string indicating whether pip was installed or upgraded. :raises ValueError: if user and root are both specified. :raises RuntimeError: for other installation issues. if user and root is not None: raise ValueError(\\"Cannot specify both \'user\' and \'root\' parameters\\") try: ensurepip.bootstrap(upgrade=upgrade, user=user, root=root) return \\"pip upgraded\\" if upgrade else \\"pip installed\\" except Exception as e: raise RuntimeError(f\\"An error occurred while installing/upgrading pip: {e}\\") if __name__ == \\"__main__\\": import sys args = sys.argv[1:] if \'--upgrade\' in args: upgrade = True else: upgrade = False if \'--user\' in args: user = True else: user = False if \'--root\' in args: root_index = args.index(\'--root\') + 1 root = args[root_index] if root_index < len(args) else None else: root = None try: print(ensure_pip_installed(upgrade=upgrade, user=user, root=root)) except Exception as e: print(str(e))"},{"question":"You are creating a logging system for an application that requires different logging configurations for development and production environments. Your task is to implement a function `configure_logging(env: str) -> None` that configures the logging based on the environment passed as an argument. # Requirements 1. **Development Configuration**: - Console logging with `DEBUG` level. - Formatter should include the timestamp, log level, and message. - Log messages should propagate to higher-level loggers. 2. **Production Configuration**: - File logging with `ERROR` level. - Formatter should include the timestamp, log level, logger name, and message. - Log messages should not propagate to higher-level loggers. - Log file should be named `app.log` and should be in the current directory. 3. Use `dictConfig` to configure logging. # Function Signature ```python def configure_logging(env: str) -> None: pass ``` # Constraints - The `env` parameter will always be either `development` or `production`. - Assume the necessary imports (`import logging`, `from logging.config import dictConfig`) are made. # Example Usage ```python configure_logging(\'development\') ``` This should set up logging as per the development configuration described above. ```python configure_logging(\'production\') ``` This should set up logging as per the production configuration described above. # Notes - Ensure that the dictionary used in `dictConfig` is correctly structured and adheres to the described schema. - Handle potential configuration errors gracefully by raising exceptions with appropriate error messages if the configuration is invalid.","solution":"import logging from logging.config import dictConfig def configure_logging(env: str) -> None: if env == \'development\': config = { \'version\': 1, \'disable_existing_loggers\': False, \'formatters\': { \'default\': { \'format\': \'%(asctime)s - %(levelname)s - %(message)s\', }, }, \'handlers\': { \'console\': { \'class\': \'logging.StreamHandler\', \'formatter\': \'default\', \'level\': \'DEBUG\', }, }, \'root\': { \'level\': \'DEBUG\', \'handlers\': [\'console\'], }, \'propagate\': True, } elif env == \'production\': config = { \'version\': 1, \'disable_existing_loggers\': False, \'formatters\': { \'default\': { \'format\': \'%(asctime)s - %(levelname)s - %(name)s - %(message)s\', }, }, \'handlers\': { \'file\': { \'class\': \'logging.FileHandler\', \'formatter\': \'default\', \'level\': \'ERROR\', \'filename\': \'app.log\', }, }, \'root\': { \'level\': \'ERROR\', \'handlers\': [\'file\'], }, \'propagate\': False, } else: raise ValueError(f\\"Invalid environment: {env}\\") dictConfig(config)"},{"question":"**Problem Statement: Custom Built-in Wrapper** In Python, the `builtins` module provides direct access to all built-in functions and constants. In this exercise, you will create a wrapper around Python’s built-in `sorted` function to add new functionality. **Task**: Implement a custom `sorted()` function that, in addition to sorting the given iterable, also counts how many elements are greater than a given threshold. **Requirements**: 1. Your custom `sorted()` function should use Python\'s built-in `sorted()` function for sorting. 2. Your function should take an iterable and a threshold as input arguments. 3. Your function should return a tuple containing: - The sorted iterable - The count of elements greater than the given threshold **Function Signature**: ```python def sorted_with_threshold(iterable, threshold): # your code here ``` **Input**: - `iterable`: A list of integers `[-10000, 10000]` which may contain duplicate values. - `threshold`: An integer `[-10000, 10000]`. **Output**: - A tuple consisting of: 1. A list of integers representing the sorted iterable. 2. An integer representing the count of elements greater than the threshold. **Example**: ```python result = sorted_with_threshold([4, 1, 5, 2, 3], 3) print(result) # Output: ([1, 2, 3, 4, 5], 2) ``` Here, the sorted list is `[1, 2, 3, 4, 5]`, and there are two elements (`4` and `5`) greater than the threshold `3`. **Constraints**: - Do not use any other libraries or modules other than `builtins`. **Notes**: - Assume that the iterable contains only integers. - Handle the case where the iterable could be empty. **Evaluation Criteria**: - Correctness: The solution must return accurate results for various test cases. - Efficiency: The solution should handle large inputs efficiently. - Code Quality: The solution should be well-documented and follow good coding practices.","solution":"def sorted_with_threshold(iterable, threshold): Returns a tuple containing the sorted list and count of elements greater than the given threshold. :param iterable: A list of integers to be sorted :param threshold: An integer to compare against elements in the list :return: Tuple (sorted list, count of elements greater than threshold) sorted_list = sorted(iterable) count = sum(1 for x in sorted_list if x > threshold) return (sorted_list, count)"},{"question":"# Question: Multi-Dict Aggregator Using ChainMap and Counter You are tasked with creating a function that aggregates data from multiple sources using ChainMap and Counter from the `collections` module. The goal is to combine several dictionaries and compute counts of individual items, prioritizing the order of earlier dictionaries for updating values. Function Signature ```python def aggregate_data(dicts: list[dict[str, int]]) -> dict[str, int]: ``` Input - `dicts`: A list of dictionaries, each having string keys and integer values. Output - Returns a single dictionary combining all input dictionaries. If a key appears in multiple dictionaries, its value should be updated based on the sum of all occurrences from the earlier dictionaries in the list. Constraints - Dictionaries can have common keys. - Values in dictionaries are non-negative integers. - The input list contains at least one dictionary. Example ```python dict_a = {\\"apple\\": 3, \\"banana\\": 2} dict_b = {\\"banana\\": 1, \\"apple\\": 5, \\"orange\\": 2} dict_c = {\\"banana\\": 4, \\"grape\\": 5} result = aggregate_data([dict_a, dict_b, dict_c]) print(result) ``` Expected Output ```python {\'apple\': 8, \'banana\': 7, \'orange\': 2, \'grape\': 5} ``` Detailed Explanation 1. **Combining dictionaries**: Use ChainMap to combine the dictionaries while ensuring the search order is preserved. The first dictionary in the chain has the highest priority for values. 2. **Counting and Aggregation**: Use Counter to sum the values of any common keys across the dictionaries. 3. Implement the function `aggregate_data` such that it creates a ChainMap from the list of dictionaries and then iterates through the keys, using Counter to tally up the values for a comprehensive aggregated result. Testing your Function ```python dict_a = {\\"apple\\": 3, \\"banana\\": 2, \\"cherry\\": 1} dict_b = {\\"banana\\": 1, \\"apple\\": 5, \\"cherry\\": 4} dict_c = {\\"cherry\\": 2, \\"apple\\": 2, \\"grape\\": 5} print(aggregate_data([dict_a, dict_b, dict_c])) # Expected Output: {\'apple\': 10, \'banana\': 3, \'cherry\': 7, \'grape\': 5} ``` Implement the function `aggregate_data` to fulfill the specified input and output requirements effectively using the ChainMap and Counter from the collections module.","solution":"from collections import ChainMap, Counter def aggregate_data(dicts: list[dict[str, int]]) -> dict[str, int]: Aggregates data from multiple dictionaries by combining them and summing values for common keys. Parameters: - dicts: List of dictionaries with string keys and integer values Returns: - A single dictionary with combined keys and summed values # Initialize a ChainMap with the dictionaries list combined_map = ChainMap(*dicts) # Use Counter to tally all occurrences from the ChainMap aggregated_counter = Counter() for d in reversed(dicts): aggregated_counter.update(d) return dict(aggregated_counter)"},{"question":"# PLS and CCA Implementation and Comparison Problem Statement You are provided with a dataset that contains two matrices, `X` and `Y`, which have a high number of variables and possibly high multicollinearity. Your task is to implement functions using the Scikit-learn\'s cross-decomposition module to perform Partial Least Squares (PLS) and Canonical Correlation Analysis (CCA) on this dataset. Specifically, you will: 1. Perform dimensionality reduction on `X` and `Y` using PLSCanonical. 2. Compare the performance of `PLSSVD`, `PLSRegression`, and `CCA` on the same dataset. 3. Implement a function to transform new `X` data using the transformation matrix from `PLSCanonical`. Input and Output Formats - **Input**: - `X` (numpy.ndarray): ( n times d ) matrix of predictors. - `Y` (numpy.ndarray): ( n times t ) matrix of targets. - `n_components` (int): Number of components to keep for the PLSCanonical model. - `new_X` (numpy.ndarray): New data matrix ( m times d ) to be transformed using the PLSCanonical model. - **Output**: - `results` (dict): Dictionary containing the results of PLSCanonical, PLSSVD, PLSRegression, and CCA. Specifically: - `\'PLSCanonical\'`: A tuple with transformed X and Y matrices (( bar{X}, bar{Y} )). - `\'PLSSVD\'`: A tuple with transformed X and Y matrices (( bar{X}_SVD, bar{Y}_SVD )). - `\'PLSRegression\'`: A tuple with transformed X and Y matrices (( bar{X}_Reg, bar{Y}_Reg )). - `\'CCA\'`: A tuple with transformed X and Y matrices (( bar{X}_CCA, bar{Y}_CCA )). - `new_X_transformed` (numpy.ndarray): Transformed new data matrix using the PLSCanonical model. Constraints - Assume that `X` and `Y` contain no missing values. - The number of components (`n_components`) should be a positive integer less than the rank of the smaller matrix between ( X text{ and } Y ). Requirements 1. Implement a function `pls_and_cca(X, Y, n_components, new_X)` that meets the input and output specifications. 2. Use Scikit-learn\'s built-in classes for `PLSCanonical`, `PLSSVD`, `PLSRegression`, and `CCA`. 3. Ensure that your solution is efficient and handles large datasets appropriately. Example ```python import numpy as np # Example data X = np.random.rand(100, 50) Y = np.random.rand(100, 10) new_X = np.random.rand(5, 50) n_components = 5 results, new_X_transformed = pls_and_cca(X, Y, n_components, new_X) ``` In this problem, students will need to understand: - How to use the different cross-decomposition algorithms in scikit-learn. - The process of dimensionality reduction using PLS. - Comparing performance and transformations using various algorithms.","solution":"from sklearn.cross_decomposition import PLSCanonical, PLSSVD, PLSRegression, CCA import numpy as np def pls_and_cca(X, Y, n_components, new_X): Perform dimensionality reduction on X and Y using different cross-decomposition methods, and transform new X data using PLSCanonical model. Parameters: - X: np.ndarray, shape (n_samples, n_features) - Y: np.ndarray, shape (n_samples, n_targets) - n_components: int, number of components to keep - new_X: np.ndarray, shape (m_samples, n_features) Returns: - results: dict, containing transformed X and Y matrices for different methods - new_X_transformed: np.ndarray, transformed new_X using the PLSCanonical model # Initialize the models pls_canonical = PLSCanonical(n_components=n_components) pls_svd = PLSSVD(n_components=n_components) pls_regression = PLSRegression(n_components=n_components) cca_model = CCA(n_components=n_components) # Fit the models and transform X and Y X_canonical, Y_canonical = pls_canonical.fit_transform(X, Y) X_svd, Y_svd = pls_svd.fit_transform(X, Y) X_reg, Y_reg = pls_regression.fit_transform(X, Y) X_cca, Y_cca = cca_model.fit_transform(X, Y) # Transform new X data new_X_transformed = pls_canonical.transform(new_X) # Store the results results = { \'PLSCanonical\': (X_canonical, Y_canonical), \'PLSSVD\': (X_svd, Y_svd), \'PLSRegression\': (X_reg, Y_reg), \'CCA\': (X_cca, Y_cca) } return results, new_X_transformed"},{"question":"**Objective:** Evaluate your understanding of Seaborn\'s capabilities for generating various types of error bars in data visualization. You are given a dataset of test scores from two different classes. Your task is to visualize the distribution of scores using different types of error bars to help understand the spread and uncertainty of the data. **Instructions:** 1. Load the provided dataset into a Pandas DataFrame. 2. Create the following visualizations using Seaborn: - A point plot with error bars representing the standard deviation of the scores. - A point plot with error bars representing a 95% percentile interval. - A point plot with error bars representing the standard error of the scores. - A point plot with error bars representing a 95% confidence interval. **Dataset format:** The dataset is in CSV format with two columns: `class` (categorical, indicating class names) and `score` (numerical, indicating test scores). **Example input:** ```python import pandas as pd data = pd.DataFrame({ \\"class\\": [\\"A\\", \\"A\\", \\"A\\", \\"B\\", \\"B\\", \\"B\\"], \\"score\\": [78, 85, 88, 92, 76, 83] }) ``` **Expected output:** Four separate plots each showing the distribution of scores for each class with different types of error bars as described in the instructions. Use Seaborn\'s `pointplot` function for plotting. **Constraints:** - Make sure to use the correct parameters for each type of error bar. - Ensure that plots are well-labeled and easy to interpret. # Example Code ```python import pandas as pd import seaborn as sns import matplotlib.pyplot as plt # Load the dataset data = pd.read_csv(\'path_to_dataset.csv\') # Set the theme sns.set_theme(style=\\"darkgrid\\") # Create point plots with error bars plt.figure(figsize=(14, 10)) # Standard deviation error bars plt.subplot(2, 2, 1) sns.pointplot(x=\\"class\\", y=\\"score\\", data=data, errorbar=\\"sd\\", capsize=.3) plt.title(\\"Standard Deviation Error Bars\\") # Percentile interval error bars plt.subplot(2, 2, 2) sns.pointplot(x=\\"class\\", y=\\"score\\", data=data, errorbar=(\\"pi\\", 95), capsize=.3) plt.title(\\"95% Percentile Interval Error Bars\\") # Standard error bars plt.subplot(2, 2, 3) sns.pointplot(x=\\"class\\", y=\\"score\\", data=data, errorbar=\\"se\\", capsize=.3) plt.title(\\"Standard Error Bars\\") # Confidence interval error bars plt.subplot(2, 2, 4) sns.pointplot(x=\\"class\\", y=\\"score\\", data=data, errorbar=\\"ci\\", n_boot=1000, capsize=.3) plt.title(\\"95% Confidence Interval Error Bars\\") plt.tight_layout() plt.show() ``` **Note:** Replace `\'path_to_dataset.csv\'` with the actual path to your dataset file.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def visualize_scores_with_error_bars(csv_path): Visualizes the distribution of scores using different types of error bars. Parameters: - csv_path (str): Path to the CSV file containing the dataset. # Load the dataset data = pd.read_csv(csv_path) # Set the theme sns.set_theme(style=\\"darkgrid\\") # Create point plots with error bars plt.figure(figsize=(14, 10)) # Standard deviation error bars plt.subplot(2, 2, 1) sns.pointplot(x=\\"class\\", y=\\"score\\", data=data, errorbar=\\"sd\\", capsize=.3) plt.title(\\"Standard Deviation Error Bars\\") # Percentile interval error bars plt.subplot(2, 2, 2) sns.pointplot(x=\\"class\\", y=\\"score\\", data=data, errorbar=(\\"pi\\", 95), capsize=.3) plt.title(\\"95% Percentile Interval Error Bars\\") # Standard error bars plt.subplot(2, 2, 3) sns.pointplot(x=\\"class\\", y=\\"score\\", data=data, errorbar=\\"se\\", capsize=.3) plt.title(\\"Standard Error Bars\\") # Confidence interval error bars plt.subplot(2, 2, 4) sns.pointplot(x=\\"class\\", y=\\"score\\", data=data, errorbar=\\"ci\\", n_boot=1000, capsize=.3) plt.title(\\"95% Confidence Interval Error Bars\\") plt.tight_layout() plt.show()"},{"question":"# Scikit-learn Utilities Coding Challenge Objective The goal of this assessment is to evaluate your understanding and application of scikit-learn\'s utility functions by performing data validation, manipulation, and a simple machine learning task using provided utilities. Problem Statement You are given a dataset with features and labels for a classification problem. The dataset may contain missing values, infinite values, or both. Your task is to: 1. Validate the features and labels using appropriate scikit-learn utility functions. 2. Handle any missing or infinite values in the features by replacing them with the mean of the respective columns. 3. Implement a custom function to shuffle the dataset while maintaining the corresponding order between features and labels. 4. Fit a simple linear model on the validated and shuffled dataset and predict on a given test dataset. Specifications 1. **Function Signature:** ```python def validate_and_train(X: np.ndarray, y: np.ndarray, test_X: np.ndarray, random_state: int) -> np.ndarray: ``` 2. **Input:** - `X`: A 2D numpy array with shape (n_samples, n_features) containing the feature data, potentially with NaNs or Infs. - `y`: A 1D numpy array with shape (n_samples,) containing the label data. - `test_X`: A 2D numpy array with shape (n_test_samples, n_features) containing the test feature data. - `random_state`: An integer used to seed the random number generator for shuffling the data. 3. **Output:** - A 1D numpy array with shape (n_test_samples,) containing the predicted labels for `test_X`. 4. **Operational Constraints:** - Use `sklearn.utils.check_X_y` to validate `X` and `y`. - Use `sklearn.utils.shuffle` to shuffle `X` and `y` consistently. - Replace missing (NaN) or infinite values in `X` with the mean of the respective columns. - Use `sklearn.linear_model.LogisticRegression` for fitting the model and making predictions. Example Usage: ```python import numpy as np # Example Data X = np.array([[1, np.nan, 3], [4, 5, 6], [7, 8, np.inf], [np.nan, 11, 12]]) y = np.array([0, 1, 0, 1]) test_X = np.array([[2, 3, 4], [5, 6, 7]]) # Provided random state for reproducibility random_state = 42 # Function Call predictions = validate_and_train(X, y, test_X, random_state) print(predictions) ``` # Instructions: 1. Implement the `validate_and_train` function following the specifications provided. 2. Ensure robust handling of NaNs and Infs in the dataset. 3. Shuffle the dataset before training to ensure model robustness. 4. Train a logistic regression model on the processed data. 5. Output predictions for `test_X`. # Evaluation Criteria: - Correctness and efficiency of data validation and handling. - Proper implementation of the shuffling function. - Accurate training and prediction using logistic regression. - Code clarity and adherence to the problem constraints.","solution":"import numpy as np from sklearn.utils import check_X_y, shuffle from sklearn.linear_model import LogisticRegression def validate_and_train(X: np.ndarray, y: np.ndarray, test_X: np.ndarray, random_state: int) -> np.ndarray: # Validate X and y X, y = check_X_y(X, y, force_all_finite=False) # Replace NaN and inf values in X with column means col_mean = np.nanmean(np.where(np.isfinite(X), X, np.nan), axis=0) inds = np.where(~np.isfinite(X)) X[inds] = np.take(col_mean, inds[1]) # Shuffle X and y X, y = shuffle(X, y, random_state=random_state) # Train logistic regression model model = LogisticRegression() model.fit(X, y) # Predict on test_X predictions = model.predict(test_X) return predictions"},{"question":"**Objective:** Demonstrate understanding of dynamic class creation and modification using the `types` module. Task Description You are tasked with creating a dynamic class factory function that uses Python\'s `types` module. This factory function will create classes with specified properties and methods dynamically. Additionally, you will need to modify one of these dynamically created classes to demonstrate deeper understanding of the module. Requirements 1. **Factory Function: `create_dynamic_class`** Implement a function `create_dynamic_class` that takes the following parameters: - `name`: The name of the class to be created. - `bases`: An optional tuple of base classes (default is an empty tuple). - `attributes`: A dictionary containing class attributes and methods. The function should use `types.new_class` to create a new class with the given name, bases, and attributes. 2. **Enhancement Function: `modify_dynamic_class`** Implement a function `modify_dynamic_class` that takes the following parameters: - `cls`: The class to be modified. - `modifications`: A dictionary containing attributes and/or methods to add to the class. The function should update the class\'s attribute dictionary with the new attributes and methods. 3. **Demonstration** Use the `create_dynamic_class` function to create a class `Animal` with the following attributes/methods: - `species` (class attribute, default value \\"Unknown\\") - `__init__` (method that initializes instance attribute `name`) - `describe` (method that prints a string description of the animal) Use the `modify_dynamic_class` function to add a new method to the `Animal` class: - `make_sound` (method that prints \\"Some sound\\") The goal is to verify that an instance of the dynamically created and modified `Animal` class works as expected. Input: - For `create_dynamic_class`: - `name`: `str` - `bases`: `tuple` - `attributes`: `dict` - For `modify_dynamic_class`: - `cls`: `type` - `modifications`: `dict` Output: - For `create_dynamic_class`: - `type`: The dynamically created class - For `modify_dynamic_class`: - `None` Constraints: - You should use the `types.new_class` and `types.prepare_class` functions where appropriate. - Attributes and methods passed in the `attributes` and `modifications` dictionaries can be any valid class attribute or method. Example Usage: ```python def create_dynamic_class(name, bases=(), attributes={}): # Your implementation here def modify_dynamic_class(cls, modifications): # Your implementation here # Create \'Animal\' class Animal = create_dynamic_class( name=\'Animal\', bases=(), attributes={ \'species\': \'Unknown\', \'__init__\': lambda self, name: setattr(self, \'name\', name), \'describe\': lambda self: print(f\\"{self.name} is a(n) {self.species}\\") } ) # Modify \'Animal\' class modify_dynamic_class(Animal, { \'make_sound\': lambda self: print(\\"Some sound\\") }) # Demonstrate functionality a = Animal(\'Dog\') a.describe() # Output: \\"Dog is a(n) Unknown\\" a.make_sound() # Output: \\"Some sound\\" ```","solution":"import types def create_dynamic_class(name, bases=(), attributes={}): Creates a new class dynamically with the given name, base classes, and attributes. cls = types.new_class(name, bases) for key, value in attributes.items(): setattr(cls, key, value) return cls def modify_dynamic_class(cls, modifications): Modifies an existing class by adding the given attributes and methods. for key, value in modifications.items(): setattr(cls, key, value) # Example usage: # Create \'Animal\' class Animal = create_dynamic_class( name=\'Animal\', bases=(), attributes={ \'species\': \'Unknown\', \'__init__\': lambda self, name: setattr(self, \'name\', name), \'describe\': lambda self: print(f\\"{self.name} is a(n) {self.species}\\") } ) # Modify \'Animal\' class modify_dynamic_class(Animal, { \'make_sound\': lambda self: print(\\"Some sound\\") })"},{"question":"# Persistent Storage using Python\'s `shelve` Module Problem Description: You are required to implement a function that manages a persisted phonebook using Python\'s `shelve` module. This phonebook should allow for adding, updating, retrieving, and deleting phonebook entries persistently. Implement a class `PersistentPhoneBook` with the following methods: - `__init__(self, filename: str, writeback: bool = False)`: Initializes the phonebook. If the file already exists, it should use the existing data; otherwise, it should create a new file. - `add_entry(self, name: str, number: str) -> None`: Adds a new entry to the phonebook. If the name already exists, it should update the number. - `get_number(self, name: str) -> str`: Retrieves the phone number for the given name. If the name does not exist, it should raise a `KeyError`. - `delete_entry(self, name: str) -> None`: Deletes the entry for the given name. If the name does not exist, it should raise a `KeyError`. - `list_entries(self) -> list`: Returns a list of all entries in the phonebook in the form of `(name, number)` tuples. - `__del__(self)`: Ensures that the shelf is properly closed. Constraints: - The `name` and `number` should be non-empty strings. - You must handle exceptions and ensure that the shelf is always closed to avoid data corruption. - Do not assume concurrent access; only a single instance of the phonebook will access the file at any time. Example Usage: ```python phonebook = PersistentPhoneBook(\'phonebook.db\') # Add entries phonebook.add_entry(\'Alice\', \'123-456-7890\') phonebook.add_entry(\'Bob\', \'234-567-8901\') # Retrieve number assert phonebook.get_number(\'Alice\') == \'123-456-7890\' # Update entry phonebook.add_entry(\'Alice\', \'321-654-0987\') assert phonebook.get_number(\'Alice\') == \'321-654-0987\' # List all entries print(phonebook.list_entries()) # Outputs: [(\'Alice\', \'321-654-0987\'), (\'Bob\', \'234-567-8901\')] # Delete entry phonebook.delete_entry(\'Bob\') print(phonebook.list_entries()) # Outputs: [(\'Alice\', \'321-654-0987\')] # Clean up del phonebook ``` Notes: - Ensure efficient handling of data with the correct use of the `writeback` parameter. - Document and handle any exceptions properly to avoid runtime errors.","solution":"import shelve class PersistentPhoneBook: def __init__(self, filename: str, writeback: bool = False): self.filename = filename self.writeback = writeback self.shelf = shelve.open(self.filename, writeback=self.writeback) def add_entry(self, name: str, number: str) -> None: if not name or not number: raise ValueError(\\"Name and number must be non-empty strings.\\") self.shelf[name] = number def get_number(self, name: str) -> str: if name in self.shelf: return self.shelf[name] else: raise KeyError(f\\"No entry found for name: {name}\\") def delete_entry(self, name: str) -> None: if name in self.shelf: del self.shelf[name] else: raise KeyError(f\\"No entry found for name: {name}\\") def list_entries(self) -> list: return list(self.shelf.items()) def __del__(self): self.shelf.close()"},{"question":"# Question: Working with Annotations in Python You are required to create a function that processes annotations in Python objects. The function should handle different types of objects, including functions, classes, modules, and callables created using `functools.partial()`. The goal is to retrieve the annotations dictionary in a consistent manner, following the best practices described in the documentation. Your task is to implement the following function: ```python def get_annotations(obj): This function retrieves the annotations dictionary from a given object. Args: obj (Any): The object from which to retrieve annotations. This can be a function, class, module, or callable. Returns: dict: The annotations dictionary of the object. If the object has no annotations, return an empty dictionary. If annotations cannot be determined, return None. # You can implement any helper functions as needed ``` **Requirements:** 1. For Python 3.10 and newer: - Use `inspect.get_annotations()` if available. - Use `getattr(obj, \'__annotations__\', None)` if inspecting specific known objects like functions, classes, or modules. 2. For Python 3.9 and older: - Use the strategies for accessing annotations described in the documentation. 3. Ensure that the returned annotations dictionary is always a dictionary or `None` if annotations are not available. 4. If the annotations contain stringized values, handle them appropriately (optional bonus). **Example:** ```python from functools import partial def sample_function(a: int, b: str) -> bool: pass class SampleClass: x: int y: str = \\"hello\\" def main(): # Function annotations print(get_annotations(sample_function)) # {\'a\': int, \'b\': str, \'return\': bool} # Class annotations print(get_annotations(SampleClass)) # {\'x\': int, \'y\': str} # Module annotations (if any) import sys print(get_annotations(sys)) # Output will depend on sys module (likely None or empty dictionary) # Callable created using partial partial_function = partial(sample_function, 1) print(get_annotations(partial_function)) # {\'a\': int, \'b\': str, \'return\': bool} if __name__ == \\"__main__\\": main() ``` **Constraints:** - Ensure compatibility with both Python 3.9 and Python 3.10 or newer. - Handle edge cases such as objects without annotations. **Notes:** - You can assume that no module will have `__annotations__` set to `None`. - You do not need to handle C extensions or compiled modules.","solution":"import inspect from functools import partial def get_annotations(obj): This function retrieves the annotations dictionary from a given object. Args: obj (Any): The object from which to retrieve annotations. This can be a function, class, module, or callable. Returns: dict: The annotations dictionary of the object. If the object has no annotations, return an empty dictionary. If annotations cannot be determined, return None. if hasattr(obj, \'__annotations__\'): return getattr(obj, \'__annotations__\', {}) # For callables created using functools.partial if isinstance(obj, partial): return get_annotations(obj.func) if inspect.ismodule(obj) or inspect.isclass(obj) or inspect.isfunction(obj) or inspect.isbuiltin(obj): return getattr(obj, \'__annotations__\', {}) return None"},{"question":"# Seaborn Coding Assessment Question Objective Assess the ability to apply seaborn\'s styling capabilities in the context of creating and customizing complex plots. Problem Statement You are given a dataset represented as a DataFrame, and you need to create several plots using Seaborn. Each plot should be styled differently using the `sns.axes_style()` function. Additionally, you will create a multi-plot grid where each subplot uses a different style. Input 1. A pandas DataFrame `df` containing three columns: `Category`, `Value1`, and `Value2`. 2. A list of style names `styles`, containing four distinct style names like `[\\"darkgrid\\", \\"whitegrid\\", \\"dark\\", \\"white\\"]`. Example: ```python import pandas as pd data = { \\"Category\\": [\\"A\\", \\"B\\", \\"C\\", \\"D\\"], \\"Value1\\": [10, 20, 10, 25], \\"Value2\\": [15, 25, 10, 30] } df = pd.DataFrame(data) styles = [\\"darkgrid\\", \\"whitegrid\\", \\"dark\\", \\"white\\"] ``` Output 1. Four individual seaborn plots, each using a different style specified in the `styles` list. 2. A multi-plot grid (2x2) where each subplot uses a different style from the `styles` list. Constraints - Use the `sns.axes_style()` function appropriately. - Utilize the context manager feature of `sns.axes_style()` for temporary styling within plots. - Ensure all the plots are displayed and properly labeled. Performance Requirements - Ensure the code executes efficiently within a reasonable time frame. Example Using the provided DataFrame and styles list, you need to write a function `create_styled_plots` that takes the DataFrame and styles list as input and generates the described plots. ```python import seaborn as sns import matplotlib.pyplot as plt def create_styled_plots(df, styles): fig, axes = plt.subplots(2, 2, figsize=(12, 10)) for i, style in enumerate(styles): with sns.axes_style(style): if i < 4: plt.subplot(2, 2, i + 1) sns.barplot(x=\\"Category\\", y=\\"Value1\\", data=df) plt.title(f\\"Style: {style}\\") # Show the multi-plot grid plt.tight_layout() plt.show() # Generate individual plots with different styles for style in styles: with sns.axes_style(style): plt.figure(figsize=(6, 4)) sns.lineplot(x=\\"Category\\", y=\\"Value1\\", data=df, marker=\\"o\\") plt.title(f\\"Line Plot with Style: {style}\\") plt.show() ``` The function will generate four individual styled line plots and a 2x2 grid of bar plots using the specified styles.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_styled_plots(df, styles): Create several plots using Seaborn with different styles. Parameters: df (pd.DataFrame): The input DataFrame containing columns \'Category\', \'Value1\', and \'Value2\'. styles (list of str): A list of style names for Seaborn plots. # Generate individual plots with different styles for style in styles: with sns.axes_style(style): plt.figure(figsize=(6, 4)) sns.lineplot(x=\\"Category\\", y=\\"Value1\\", data=df, marker=\\"o\\") plt.title(f\\"Line Plot with Style: {style}\\") plt.show() # Create a multi-plot grid with different styles fig, axes = plt.subplots(2, 2, figsize=(12, 10)) for i, style in enumerate(styles): with sns.axes_style(style): ax = axes[i//2, i%2] sns.barplot(x=\\"Category\\", y=\\"Value1\\", data=df, ax=ax) ax.set_title(f\\"Style: {style}\\") # Show the multi-plot grid plt.tight_layout() plt.show()"},{"question":"Enhanced Calculator with State You are required to implement a class structure that acts as an enhanced calculator. The calculator should support basic arithmetic operations (addition, subtraction, multiplication, and division) and should also keep track of a computational state that can be queried and updated. Implement the class `EnhancedCalculator` with the following methods: 1. **Initialization**: - `__init__(self)`: Initialize the calculator with a predefined state `state` set to zero. 2. **Basic Operations**: - `add(self, value)`: Add `value` to the current state. - `subtract(self, value)`: Subtract `value` from the current state. - `multiply(self, value)`: Multiply the current state by `value`. - `divide(self, value)`: Divide the current state by `value`. If `value` is zero, raise a `ZeroDivisionError` with the message \\"Cannot divide by zero\\". 3. **State Management**: - `reset(self)`: Reset the state to zero. - `get_state(self)`: Return the current state. 4. **History**: - `history(self)`: Return a list of all operations performed as strings. # Constraints: - The operations should be performed in the order they are called. - The calculator should handle negative and positive values correctly. - Resetting the calculator should clear its history. - The `history()` method should return a list of strings where each string describes an operation. For example, if the state was updated by adding 5, then multiplying by 2, the history list should be `[\\"add 5\\", \\"multiply 2\\"]`. # Example Usage: ```python calc = EnhancedCalculator() calc.add(5) calc.multiply(2) state = calc.get_state() # state should be 10 history = calc.history() # history should be [\\"add 5\\", \\"multiply 2\\"] calc.reset() state = calc.get_state() # state should be 0 history = calc.history() # history should be [] try: calc.divide(0) except ZeroDivisionError as e: print(e) # Output: Cannot divide by zero ``` # Your Task: Implement the `EnhancedCalculator` class ensuring the above functionality. Your implementation should be efficient and make correct use of Python\'s simple statements as detailed in the documentation.","solution":"class EnhancedCalculator: def __init__(self): self.state = 0 self._history = [] def add(self, value): self.state += value self._history.append(f\\"add {value}\\") def subtract(self, value): self.state -= value self._history.append(f\\"subtract {value}\\") def multiply(self, value): self.state *= value self._history.append(f\\"multiply {value}\\") def divide(self, value): if value == 0: raise ZeroDivisionError(\\"Cannot divide by zero\\") self.state /= value self._history.append(f\\"divide {value}\\") def reset(self): self.state = 0 self._history = [] def get_state(self): return self.state def history(self): return self._history"},{"question":"**Title**: Implementing a URL Fetcher with Multiple Features **Objective**: Demonstrate your understanding of urllib.request module by implementing a URL fetcher that can handle basic and advanced operations such as GET and POST requests, custom headers, and error handling. **Question**: Implement a class `URLFetcher` that provides methods to: 1. Fetch the content of a URL using a GET request. 2. Send data to a URL using a POST request. 3. Add custom headers to a request. 4. Handle exceptions and return appropriate error messages. # Class Definition ```python class URLFetcher: def __init__(self): self.headers = {} def add_header(self, key: str, value: str): Add a custom header to be included in requests. Args: key (str): Header key (e.g., \'User-Agent\'). value (str): Header value (e.g., \'Mozilla/5.0\'). pass def fetch_get(self, url: str) -> str: Fetch the content of the given URL using a GET request. Args: url (str): The URL to fetch. Returns: str: The content of the URL. Raises: URLError: If there is a problem with the URL. HTTPError: If the server returns an error response. pass def fetch_post(self, url: str, data: dict) -> str: Send data to a URL using a POST request. Args: url (str): The URL to send data to. data (dict): The data to send in the POST request. Returns: str: The response from the server. Raises: URLError: If there is a problem with the URL. HTTPError: If the server returns an error response. pass ``` # Input and Output Formats - **Input**: The URL to fetch, and in case of POST, a dictionary of data. - **Output**: The content of the URL or response from the server as a string. - **Constraints**: - The URL must be a well-formed HTTP or HTTPS URL. - In case of network errors, the method should raise appropriate exceptions with informative messages. - You must handle HTTP redirections and portray the final fetched URL. # Example Usage ```python fetcher = URLFetcher() fetcher.add_header(\'User-Agent\', \'Mozilla/5.0\') # Fetching content with GET try: content = fetcher.fetch_get(\'http://example.com\') print(content) except URLError as e: print(f\'Failed to reach the server: {e.reason}\') except HTTPError as e: print(f\'Server couldn\'t fulfill the request. Error code: {e.code}\') # Sending data with POST data = {\'key1\': \'value1\', \'key2\': \'value2\'} try: response = fetcher.fetch_post(\'http://example.com/api\', data) print(response) except URLError as e: print(f\'Failed to reach the server: {e.reason}\') except HTTPError as e: print(f\'Server couldn\'t fulfill the request. Error code: {e.code}\') ``` **Note**: Ensure you handle various edge cases, such as redirect handling, by following HTTP response codes properly. Your class methods should be robust and reliable for practical use cases in web scraping or API interaction.","solution":"import urllib.request import urllib.parse from urllib.error import URLError, HTTPError class URLFetcher: def __init__(self): self.headers = {} def add_header(self, key: str, value: str): Add a custom header to be included in requests. Args: key (str): Header key (e.g., \'User-Agent\'). value (str): Header value (e.g., \'Mozilla/5.0\'). self.headers[key] = value def fetch_get(self, url: str) -> str: Fetch the content of the given URL using a GET request. Args: url (str): The URL to fetch. Returns: str: The content of the URL. Raises: URLError: If there is a problem with the URL. HTTPError: If the server returns an error response. request = urllib.request.Request(url, headers=self.headers) try: with urllib.request.urlopen(request) as response: return response.read().decode(\'utf-8\') except HTTPError as e: raise e except URLError as e: raise e def fetch_post(self, url: str, data: dict) -> str: Send data to a URL using a POST request. Args: url (str): The URL to send data to. data (dict): The data to send in the POST request. Returns: str: The response from the server. Raises: URLError: If there is a problem with the URL. HTTPError: If the server returns an error response. data_encoded = urllib.parse.urlencode(data).encode(\'utf-8\') request = urllib.request.Request(url, data=data_encoded, headers=self.headers) try: with urllib.request.urlopen(request) as response: return response.read().decode(\'utf-8\') except HTTPError as e: raise e except URLError as e: raise e"},{"question":"# Seaborn Regression Plot Challenge Using the seaborn library and the `mpg` dataset, your task is to create a comprehensive visualization to evaluate different regression models on `mpg` data. This question will assess your ability to apply various regression techniques using seaborn and customize the visual appearance of regression plots. Instructions: 1. **Data Loading:** - Load the `mpg` dataset from seaborn. 2. **Subplots Setup:** - Create a figure with 2 rows and 2 columns of subplots. 3. **Plotting:** - For each subplot, create a different `regplot` as described below: - **Subplot 1 (1st row, 1st column):** Plot a linear regression of `weight` vs `acceleration`. - **Subplot 2 (1st row, 2nd column):** Plot a second-order polynomial regression of `weight` vs `mpg`. - **Subplot 3 (2nd row, 1st column):** Plot a log-linear regression of `displacement` vs `mpg` with logarithmic scaling on the x-axis. - **Subplot 4 (2nd row, 2nd column):** Plot a locally-weighted scatterplot smoothing (LOWESS) regression of `horsepower` vs `mpg`. 4. **Customization:** - Disable the confidence interval for the subplot displaying linear regression (Subplot 1). - Customize the appearance of the polynomial regression plot (Subplot 2): Use `ci=95`, `marker=\\"o\\"`, `color=\\"blue\\"`, and set `line_kws=dict(color=\\"green\\")`. 5. **Output:** - Provide a well-labeled plot (including titles for each subplot and axis labels) and ensure it is displayed correctly. Constraints: - Ensure your code runs efficiently and plots all subplots within a single figure. - Use seaborn\'s styling options to make the plots visually appealing. Example Function Signature: ```python import seaborn as sns import matplotlib.pyplot as plt def plot_regression_variants(): # Write your implementation here pass # Call the function to display the plot plot_regression_variants() ``` This question tests your ability to use seaborn for advanced plotting and customization, leveraging the `regplot` function\'s capabilities in different scenarios and creating a cohesive, well-labeled figure.","solution":"import seaborn as sns import matplotlib.pyplot as plt import numpy as np def plot_regression_variants(): # Load the dataset mpg = sns.load_dataset(\'mpg\') # Create a figure with 2 rows and 2 columns of subplots fig, axes = plt.subplots(2, 2, figsize=(15, 12)) # Subplot 1: Linear regression of weight vs acceleration sns.regplot(x=\'weight\', y=\'acceleration\', data=mpg, ax=axes[0, 0], ci=None) axes[0, 0].set_title(\'Linear Regression of Weight vs Acceleration\') axes[0, 0].set_xlabel(\'Weight\') axes[0, 0].set_ylabel(\'Acceleration\') # Subplot 2: Second-order polynomial regression of weight vs mpg sns.regplot(x=\'weight\', y=\'mpg\', data=mpg, order=2, ax=axes[0, 1], ci=95, marker=\'o\', color=\'blue\', line_kws=dict(color=\'green\')) axes[0, 1].set_title(\'Polynomial Regression (2nd order) of Weight vs MPG\') axes[0, 1].set_xlabel(\'Weight\') axes[0, 1].set_ylabel(\'MPG\') # Subplot 3: Log-linear regression of displacement vs mpg with logarithmic scaling on x-axis sns.regplot(x=np.log(mpg[\'displacement\']), y=\'mpg\', data=mpg, ax=axes[1, 0]) axes[1, 0].set_xscale(\'log\') axes[1, 0].set_title(\'Log-Linear Regression of Displacement vs MPG\') axes[1, 0].set_xlabel(\'Log of Displacement\') axes[1, 0].set_ylabel(\'MPG\') # Subplot 4: LOWESS regression of horsepower vs mpg sns.regplot(x=\'horsepower\', y=\'mpg\', data=mpg, ax=axes[1, 1], lowess=True) axes[1, 1].set_title(\'LOWESS Regression of Horsepower vs MPG\') axes[1, 1].set_xlabel(\'Horsepower\') axes[1, 1].set_ylabel(\'MPG\') # Adjust layout plt.tight_layout() # Show plot plt.show() # Call the function to display the plot plot_regression_variants()"},{"question":"**Objective:** Create a custom email generator using the `BytesGenerator` class from the `email.generator` module. The task is to write a function that serializes an `EmailMessage` object, including specific handling for both text and binary parts, and ensuring compliance with email RFC standards. **Problem Statement:** You need to implement a function `custom_email_generator(outfp, msg, unixfrom=False, linesep=None, mangle_from_=None, maxheaderlen=None, policy=None)` that: 1. Initializes a `BytesGenerator` with the provided output file-like object `outfp`. 2. Uses the `flatten` method to serialize the `msg` into the provided output stream with the parameters explained below. 3. Handles both textual and binary parts correctly, ensuring headers are wrapped as per `maxheaderlen` and binary data is converted using an appropriate transfer encoding. 4. Supports Unix mailbox format header when `unixfrom` is `True`. **Function Signature:** ```python def custom_email_generator(outfp, msg, unixfrom=False, linesep=None, mangle_from_=None, maxheaderlen=None, policy=None): # Implementation here ``` **Input:** - `outfp`: A file-like object to which the serialized email will be written. It must support a `write` method that accepts binary data. - `msg`: An `EmailMessage` object that is to be serialized. - `unixfrom` (Optional): A boolean indicating whether to include the Unix \\"From\\" envelope header. Defaults to `False`. - `linesep` (Optional): A string to use as the line separator between lines of the serialized message. Defaults to `None`. - `mangle_from_` (Optional): A boolean to indicate whether to modify lines starting with \\"From \\". Defaults to `None`. - `maxheaderlen` (Optional): An integer specifying the maximum length of the header lines. Defaults to `None`. - `policy` (Optional): A policy object to control the message generation. Defaults to `None`. **Output:** - None. The serialized email is written directly to the provided `outfp`. **Constraints:** - The function must handle both text and binary parts of the email message, ensuring compliance with RFC standards. - It should use the `BytesGenerator` for serialization and must respect the provided parameters to control the final output format. **Example Usage:** Assuming `email` and `EmailMessage` have been properly imported and `msg` is a valid `EmailMessage` object: ```python import io from email.message import EmailMessage from custom_email_module import custom_email_generator # Create an EmailMessage object (msg) and populate it as needed msg = EmailMessage() msg[\'Subject\'] = \'Test Email\' msg.set_content(\'This is a test email with text content.\') # Binary payload example with open(\'example_attachment.png\', \'rb\') as f: file_data = f.read() msg.add_attachment(file_data, maintype=\'image\', subtype=\'png\', filename=\'example_attachment.png\') # Output stream output_stream = io.BytesIO() # Generate the email custom_email_generator(output_stream, msg, unixfrom=True, linesep=\'rn\') # Output the serialized email content print(output_stream.getvalue().decode(\'utf-8\')) ``` In the above example, a custom email is generated and printed, showcasing the proper serialization of both text and binary parts in adherence to specified format constraints.","solution":"from email.generator import BytesGenerator def custom_email_generator(outfp, msg, unixfrom=False, linesep=None, mangle_from_=None, maxheaderlen=None, policy=None): Serializes an EmailMessage object into a provided output stream. Parameters: outfp : file-like object The output stream where the email is written. msg : EmailMessage The EmailMessage object to be serialized. unixfrom : bool, optional Whether to include the Unix \\"From\\" header. Defaults to False. linesep : str, optional The line separator to use. Defaults to None. mangle_from_ : bool, optional Whether to modify lines starting with \\"From \\". Defaults to None. maxheaderlen : int, optional The maximum length of header lines. Defaults to None. policy : object, optional The policy to use for generating the email. Defaults to None. generator = BytesGenerator(outfp, policy=policy, mangle_from_=mangle_from_, maxheaderlen=maxheaderlen) generator.flatten(msg, unixfrom=unixfrom, linesep=linesep)"},{"question":"# Python Code Porting and Compatibility Assessment Problem Statement: You have been provided with a snippet of Python 2 code that performs several operations involving division, string manipulation, and file handling. The goal of this exercise is to port this code to be compatible with both Python 2.7 and Python 3.x while maintaining proper functionality and performance. Below is the original Python 2 code: ```python # Original Python 2 code def perform_operations(file_path): # Read the content of the file with open(file_path, \'r\') as f: content = f.read() # Convert content to uppercase upper_content = content.upper() # Calculate the result of division numbers = [int(num) for num in upper_content.split() if num.isdigit()] division_results = [num / 2 for num in numbers] # Return the modified content and division results return upper_content, division_results # Example usage modified_content, div_results = perform_operations(\'example.txt\') print(\\"Modified Content:\\") print(modified_content) print(\\"Division Results:\\") print(div_results) ``` Task: 1. **Port the provided code to be compatible with both Python 2.7 and Python 3.x.** 2. **Handle division operations carefully to ensure correct results in both versions.** 3. **Properly distinguish between text (string) and binary data where needed.** 4. **Write tests to ensure the code works correctly and maintains functionality across Python versions.** Constraints: - The input file will always contain text data. - The numbers in the file are space-separated. - Use the `io.open()` function for file operations. - Use the appropriate future imports to ensure compatibility. Expected Input and Output: - **Input:** Path to a text file containing space-separated text and numbers (e.g., `\'example.txt\'`). - **Output:** A tuple containing the modified file content (in uppercase) and a list of division results (numbers divided by 2). Performance Requirements: - The function should handle files up to 1MB efficiently. Solution Template: ```python from __future__ import absolute_import from __future__ import division from __future__ import print_function import io def perform_operations(file_path): # Write your code here following the guidelines above pass # Tests def test_perform_operations(): # Create a test file with example content test_file_path = \'test_file.txt\' with io.open(test_file_path, \'w\', encoding=\'utf-8\') as test_file: test_file.write(\\"123 abc 456 def 789\\") # Call the function and check the results modified_content, div_results = perform_operations(test_file_path) assert modified_content == \\"123 ABC 456 DEF 789\\".upper(), \\"Content did not match expected result\\" assert div_results == [61.5, 228.0, 394.5], \\"Division results did not match expected result\\" print(\\"All tests passed.\\") # Uncomment below line to run tests # test_perform_operations() ``` Good luck! Make sure to follow the constraints and performance requirements carefully.","solution":"from __future__ import absolute_import from __future__ import division from __future__ import print_function from __future__ import unicode_literals import io def perform_operations(file_path): # Read the content of the file with io.open(file_path, \'r\', encoding=\'utf-8\') as f: content = f.read() # Convert content to uppercase upper_content = content.upper() # Calculate the result of division numbers = [int(num) for num in upper_content.split() if num.isdigit()] division_results = [num / 2.0 for num in numbers] # Return the modified content and division results return upper_content, division_results"},{"question":"**Function Vectorization and Differentiability with Limited In-Place Operations using PyTorch** # Objective Demonstrate your understanding of `torch.func` by implementing and transforming a function that is compatible with `vmap` and `grad`, and handles in-place operations cautiously. # Problem You are provided with a function `transform_and_multiply` that takes two tensors and some transformation function, applies the transformation in place on the first tensor, and then multiplies element-wise with the second tensor. Re-implement the `transform_and_multiply` function to ensure it is compatible with `vmap` and also implement a gradient function using `grad` which computes the gradient of the result with respect to the first tensor. # Steps 1. **Re-implement function**: Transform the `transform_and_multiply` function to be compatible with `vmap`. 2. **Compute Gradient**: Implement a gradient function using `grad`. # Requirements - Do not use any global variable mutations. - Ensure no in-place operations violate `vmap` constraints. - Handle the random transformation appropriately using `vmap`\'s randomness flag. # Specification 1. **Function Definitions**: ```python import torch from torch.func import vmap, grad def transform_and_multiply(x: torch.Tensor, y: torch.Tensor, transform) -> torch.Tensor: Apply transformation function to the first tensor in-place (compatible with vmap), and then return element-wise multiplication with the second tensor. Args: x (torch.Tensor): The input tensor to transform. y (torch.Tensor): The second tensor to multiply with. transform (callable): The transformation function applied on x. Returns: torch.Tensor: The result of element-wise multiplication # To be implemented raise NotImplementedError(\\"This function needs to be implemented.\\") def compute_gradient(x: torch.Tensor, y: torch.Tensor, transform) -> torch.Tensor: Compute gradient of transform_and_multiply with respect to x using torch.func.grad. Args: x (torch.Tensor): The input tensor to transform. y (torch.Tensor): The second tensor to multiply with. transform (callable): The transformation function applied on x. Returns: torch.Tensor: Gradient of the result with respect to x. # To be implemented raise NotImplementedError(\\"This function needs to be implemented.\\") ``` 2. **Usage Example**: ```python def example_transform(tensor): return tensor.sin().cos() x = torch.randn(3, 2) y = torch.ones(3, 2) # Transform and multiply result = transform_and_multiply(x, y, example_transform) # Compute gradient grad_result = compute_gradient(x, y, example_transform) print(\\"Result:\\", result) print(\\"Gradient:\\", grad_result) ``` 3. **Constraints**: - You should not use any global variables. - Ensure your transformations and operations are compatible with `vmap` and avoid illegal in-place operations. - The function `compute_gradient` should make use of `torch.func.grad` for computing the gradient. # Scoring Criteria - Correct implementation of `transform_and_multiply` function. - Correct computation of gradients using `compute_gradient`. - Handling tensor transformations without violating `vmap` constraints. - Adhering to the guidelines for in-place operations and randomness control. # Note Refer to documentation for specific limitations and required methods.","solution":"import torch from torch.func import vmap, grad def transform_and_multiply(x: torch.Tensor, y: torch.Tensor, transform) -> torch.Tensor: Apply transformation function to the first tensor (without in-place modification), and then return element-wise multiplication with the second tensor. Args: x (torch.Tensor): The input tensor to transform. y (torch.Tensor): The second tensor to multiply with. transform (callable): The transformation function applied on x. Returns: torch.Tensor: The result of element-wise multiplication transformed_x = transform(x) return transformed_x * y def compute_gradient(x: torch.Tensor, y: torch.Tensor, transform) -> torch.Tensor: Compute gradient of transform_and_multiply with respect to x using torch.func.grad. Args: x (torch.Tensor): The input tensor to transform. y (torch.Tensor): The second tensor to multiply with. transform (callable): The transformation function applied on x. Returns: torch.Tensor: Gradient of the result with respect to x. def wrapped_transform_and_multiply(input_x): return transform_and_multiply(input_x, y, transform).sum() return grad(wrapped_transform_and_multiply)(x)"},{"question":"# Advanced Python Coding Assessment: Custom Importer Python\'s `importlib` module provides tools not only to perform imports but also to implement custom importers. In this task, you will need to implement a custom importer using `importlib` functionalities. Task Description: You are required to create a custom importer that can import modules from a provided directory path. Implement the following functionalities using the `importlib` module: 1. **CustomPathFinder**: This should be a subclass of `importlib.abc.MetaPathFinder`. It should locate and return a module spec for a requested module if it exists in the specified directory. 2. **CustomLoader**: This should be a subclass of `importlib.abc.Loader`. It should be able to: - Load a module given a `ModuleSpec`. - Execute the module’s code. 3. **import_from_directory**: A function that integrates the CustomPathFinder into `sys.meta_path` (the list of meta path finders). This function should: - Accept a directory path and a module name. - Use CustomPathFinder and CustomLoader to import the specified module from the given directory. - Return the imported module. Implementation Details: 1. **CustomPathFinder** should implement `find_spec` method. - **Arguments**: - `fullname` (str): The fully qualified name of the module. - `path` (Optional[str]): A directory path given. - `target` (Optional[ModuleType]): The module object that the finder may use to guess about the correct spec. 2. **CustomLoader** should implement: - `create_module(spec)` - `exec_module(module)` 3. The `import_from_directory` function integrates everything and carries out the module import: - **Arguments**: - `directory` (str): The directory to search for the module. - `module_name` (str): The name of the module to import. **Constraints:** - The directory will only contain `.py` files corresponding to modules. - Handle all necessary cache invalidations. - Ensure compatibility with Python 3.6 and above. **Example Usage:** ```python # Suppose we have a directory \'custom_modules\' with \'mymodule.py\'. import_from_directory(\'/path/to/custom_modules\', \'mymodule\') import mymodule print(mymodule.some_function()) ``` This should enable importing \'mymodule\' from the specified directory. # Submission: - CustomPathFinder class implementation. - CustomLoader class implementation. - import_from_directory function implementation. - The solution should be contained within a single Python file. Use the provided documentation snippets and follow the examples to create a robust, working solution. You will be assessed on correctness, adherence to best practices, and the effective use of `importlib` functionalities. Good luck!","solution":"import sys import importlib.util import importlib.abc import os class CustomPathFinder(importlib.abc.MetaPathFinder): def __init__(self, directory): self.directory = directory def find_spec(self, fullname, path, target=None): # Create a module name from the fullname module_name = fullname.split(\'.\')[-1] # Generate the path to the module file module_path = os.path.join(self.directory, f\'{module_name}.py\') if not os.path.isfile(module_path): return None spec = importlib.util.spec_from_file_location(fullname, module_path, loader=CustomLoader(module_path)) return spec class CustomLoader(importlib.abc.Loader): def __init__(self, module_path): self.module_path = module_path def create_module(self, spec): # Default module creation semantics return None def exec_module(self, module): with open(self.module_path, \'r\') as file: code = file.read() exec(code, module.__dict__) def import_from_directory(directory, module_name): if directory not in sys.path: sys.path.append(directory) finder = CustomPathFinder(directory) sys.meta_path.insert(0, finder) try: module = importlib.import_module(module_name) finally: # Clean up by removing the finder from meta_path sys.meta_path.remove(finder) return module"},{"question":"# Objective: Create Partial Dependence Plots (PDP) and Individual Conditional Expectation (ICE) plots for a given dataset using Scikit-learn\'s `PartialDependenceDisplay` and interpret the results. # Problem Statement: You are given a dataset that contains various features and a target. Your task is to: 1. Train a machine learning model using the dataset. 2. Generate Partial Dependence Plots (PDP) for specified features. 3. Generate Individual Conditional Expectation (ICE) plots for specified features. 4. Interpret the results of the plots. # Input: - A dataset with features and a target. - A list of features for which PDP and ICE plots need to be generated. # Output: - PDP plots for the specified features. - ICE plots for the specified features. - A brief interpretation of the generated plots. # Detailed Steps: 1. Load the dataset and split it into features and target variables. 2. Train a chosen machine learning model (e.g., Gradient Boosting Classifier) on the dataset. 3. Generate PDP plots for the specified features. 4. Generate ICE plots for the specified features. 5. Provide a brief interpretation of the generated plots. # Constraints: - Use Scikit-learn\'s `PartialDependenceDisplay`. - Visualize the results using Matplotlib or any other suitable visualization library. # Example: Consider the following dataset: ```python from sklearn.datasets import make_friedman1 X, y = make_friedman1(n_samples=1000, n_features=10, random_state=0) ``` - Train a Gradient Boosting Regressor on this dataset. - Generate PDP and ICE plots for features `[0, 1]`. Code Template: ```python import numpy as np import matplotlib.pyplot as plt from sklearn.datasets import make_friedman1 from sklearn.ensemble import GradientBoostingRegressor from sklearn.inspection import PartialDependenceDisplay # Step 1: Load the dataset X, y = make_friedman1(n_samples=1000, n_features=10, random_state=0) # Step 2: Train the model model = GradientBoostingRegressor(n_estimators=100, max_depth=3, learning_rate=0.1, random_state=0) model.fit(X, y) # Step 3: Generate PDP plots features = [0, 1, (0, 1)] PartialDependenceDisplay.from_estimator(model, X, features) plt.show() # Step 4: Generate ICE plots features = [0, 1] PartialDependenceDisplay.from_estimator(model, X, features, kind=\'individual\') plt.show() # Step 5: Interpretation # Provide a brief interpretation of the plots based on the results obtained. ``` Interpretation: - PDP Plot Interpretation: Discuss the average effect of the specified features on the target. - ICE Plot Interpretation: Highlight any heterogeneity in the relationship between the features and the target for individual samples.","solution":"import numpy as np import matplotlib.pyplot as plt from sklearn.datasets import make_friedman1 from sklearn.ensemble import GradientBoostingRegressor from sklearn.inspection import PartialDependenceDisplay def generate_pdp_and_ice(): # Step 1: Load the dataset X, y = make_friedman1(n_samples=1000, n_features=10, random_state=0) # Step 2: Train the model model = GradientBoostingRegressor(n_estimators=100, max_depth=3, learning_rate=0.1, random_state=0) model.fit(X, y) # Step 3: Generate PDP plots features = [0, 1, (0, 1)] pdp_disp = PartialDependenceDisplay.from_estimator(model, X, features) plt.show() # Step 4: Generate ICE plots ice_features = [0, 1] ice_disp = PartialDependenceDisplay.from_estimator(model, X, ice_features, kind=\'individual\') plt.show() return pdp_disp, ice_disp"},{"question":"# Question You have been given the task of optimizing a PyTorch model\'s performance on a CUDA-enabled GPU. To achieve this, you need to interact with various PyTorch backends, particularly CUDA and cuDNN. **Objective:** Write a function `optimize_pytorch_performance` that does the following: 1. Checks if CUDA and cuDNN are available. 2. Configures the following settings based on the detected hardware capabilities: - Enable TensorFloat-32 tensor cores for GEMM and convolutions if the GPU supports it. - Enable FP16 and BF16 reduced precision reductions if support is detected. - Set cuDNN to benchmark mode and configure it to use deterministic algorithms for reproducibility. - Manage the `cufft_plan_cache` by setting its max size to a specified value and clearing it afterwards. # Function Signature ```python def optimize_pytorch_performance(cufft_max_size: int) -> dict: pass ``` # Input: - `cufft_max_size` (int): The maximum size to set for the cuFFT plan cache. # Output: - A dictionary with the following keys and values: - `\\"cuda_available\\"`: (bool) True if CUDA is available, False otherwise. - `\\"cudnn_available\\"`: (bool) True if cuDNN is available, False otherwise. - `\\"tf32_enabled\\"`: (bool) True if TensorFloat-32 was enabled successfully, False otherwise. - `\\"fp16_reduction_enabled\\"`: (bool) True if FP16 reductions were enabled successfully, False otherwise. - `\\"bf16_reduction_enabled\\"`: (bool) True if BF16 reductions were enabled successfully, False otherwise. - `\\"cu_fft_plan_cache_size\\"`: (int) The size of the cuFFT plan cache after it was cleared. # Constraints: - Assume the function is executed on a machine with a CUDA-enabled GPU. - Use appropriate PyTorch backend functions and attributes to check hardware capabilities and configure settings. # Example: ```python result = optimize_pytorch_performance(1000) print(result) # Output might be: # { # \\"cuda_available\\": True, # \\"cudnn_available\\": True, # \\"tf32_enabled\\": True, # \\"fp16_reduction_enabled\\": True, # \\"bf16_reduction_enabled\\": True, # \\"cu_fft_plan_cache_size\\": 0 # } ``` # Notes: - Make sure to handle exceptions appropriately, especially in cases where certain backends might not be available. Implement the function `optimize_pytorch_performance` to fulfill the requirements described above.","solution":"import torch def optimize_pytorch_performance(cufft_max_size: int) -> dict: Optimize PyTorch performance settings based on CUDA and cuDNN capabilities. Args: cufft_max_size (int): The maximum size to set for the cuFFT plan cache. Returns: dict: A dictionary with the optimization settings status. results = { \\"cuda_available\\": torch.cuda.is_available(), \\"cudnn_available\\": False, \\"tf32_enabled\\": False, \\"fp16_reduction_enabled\\": False, \\"bf16_reduction_enabled\\": False, \\"cu_fft_plan_cache_size\\": 0 } if not results[\\"cuda_available\\"]: return results if torch.backends.cudnn.is_available(): results[\\"cudnn_available\\"] = True torch.backends.cudnn.benchmark = True torch.backends.cudnn.deterministic = True if torch.cuda.get_device_properties(0).major >= 8: torch.backends.cuda.matmul.allow_tf32 = True torch.backends.cudnn.allow_tf32 = True results[\\"tf32_enabled\\"] = True if torch.cuda.get_device_capability()[0] >= 7: torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction = True results[\\"fp16_reduction_enabled\\"] = True if torch.cuda.get_device_capability()[0] >= 8: torch.backends.cuda.matmul.allow_bf16_reduced_precision_reduction = True results[\\"bf16_reduction_enabled\\"] = True torch.backends.cuda.cufft_plan_cache.max_size = cufft_max_size torch.backends.cuda.cufft_plan_cache.clear() results[\\"cu_fft_plan_cache_size\\"] = torch.backends.cuda.cufft_plan_cache.size return results"},{"question":"Objective Your task is to implement a simplified version of the **Sequence Protocol**. Specifically, you are required to create a class that mimics the behavior of a Python list, supporting a few essential sequence operations. Task Description Implement a class named `CustomList` that mimics the behavior of a Python list. Your `CustomList` class should support the following sequence operations: 1. **Getting the length** of the sequence using `len()`. 2. **Indexing** to access elements (e.g., `obj[index]`). 3. **Slicing** to get a subsequence (e.g., `obj[start:stop]`). 4. **Concatenation** to combine two sequences (e.g., `obj1 + obj2`). 5. **Repetition** to repeat the sequence (e.g., `obj * n`). 6. **Iteration** over the sequence using a loop. Class Definition ```python class CustomList: def __init__(self, initial_data): # Initialize your sequence with the given initial_data pass def __len__(self): # Return the number of elements in the sequence pass def __getitem__(self, index): # Return the element or slice at the given index pass def __add__(self, other): # Concatenate with another sequence and return the result pass def __mul__(self, n): # Repeat the sequence n times and return the result pass def __iter__(self): # Return an iterator for the sequence pass ``` Constraints 1. The sequence will always contain only integers. 2. The `initial_data` will be a list of integers. 3. For concatenation, both sequences will contain only integers. 4. Repetition count `n` will be a non-negative integer. Example Usage ```python # Initialization custom_list = CustomList([1, 2, 3]) # Getting length print(len(custom_list)) # Output: 3 # Indexing print(custom_list[1]) # Output: 2 # Slicing print(custom_list[1:3]) # Output: [2, 3] # Concatenation another_list = CustomList([4, 5]) print(custom_list + another_list) # Output: [1, 2, 3, 4, 5] # Repetition print(custom_list * 2) # Output: [1, 2, 3, 1, 2, 3] # Iteration for item in custom_list: print(item) # Output: # 1 # 2 # 3 ``` Performance Requirements 1. All operations should have a time complexity of O(n) or better, where n is the length of the sequence. 2. Memory usage should be kept to a minimum while maintaining clear and understandable code.","solution":"class CustomList: def __init__(self, initial_data): self.data = initial_data def __len__(self): return len(self.data) def __getitem__(self, index): return self.data[index] def __add__(self, other): return CustomList(self.data + other.data) def __mul__(self, n): return CustomList(self.data * n) def __iter__(self): return iter(self.data)"},{"question":"# Complex Number Operations in Python using `cmath` Module You are provided with a set of operations to be performed on complex numbers. Your task is to implement a function `perform_complex_operations` that takes a list of tuples as input. Each tuple will represent an operation and the associated arguments. The function should return a list of results corresponding to each operation. Function Signature ```python def perform_complex_operations(operations: list) -> list: ``` Input - `operations`: A list of tuples where each tuple `(operation, *args)` represents: - `operation` (string): The operation name, which can be one of the following: - \\"polar\\": convert a complex number to its polar coordinates. - \\"rect\\": convert polar coordinates (r, phi) back to a complex number. - \\"exp\\": calculate e raised to the power of the complex number. - \\"log\\": compute the natural logarithm of a complex number. - \\"log10\\": compute the base-10 logarithm. - \\"sqrt\\": calculate the square root. - \\"acos\\": compute the arc cosine. - \\"asin\\": compute the arc sine. - \\"atan\\": compute the arc tangent. - \\"cos\\": compute the cosine. - \\"sin\\": compute the sine. - \\"tan\\": compute the tangent. - \\"abs\\": compute the modulus (absolute value) of the complex number. - `args`: Arguments needed for the operation. These will vary based on the operation. # Example Input ```python [ (\\"polar\\", complex(1, 1)), (\\"rect\\", 1.4142135623730951, 0.7853981633974483), (\\"exp\\", complex(0, math.pi)), (\\"log\\", complex(1, 0)), (\\"sqrt\\", complex(-1, 0)), (\\"cos\\", complex(0, math.pi)) ] ``` Output - A list of results corresponding to the operations in the input list. # Example Output ```python [ (1.4142135623730951, 0.7853981633974483), (1.0000000000000002+1j), (-1+1.2246467991473532e-16j), 0j, 1j, (11.591953275521519-0j) ] ``` Constraints - Assume that all inputs are valid as per the specified operations. - Use the `cmath` module for performing operations. Example ```python import cmath import math def perform_complex_operations(operations): results = [] for operation in operations: op = operation[0] args = operation[1:] if op == \\"polar\\": result = cmath.polar(*args) elif op == \\"rect\\": result = cmath.rect(*args) elif op == \\"exp\\": result = cmath.exp(*args) elif op == \\"log\\": result = cmath.log(*args) elif op == \\"log10\\": result = cmath.log10(*args) elif op == \\"sqrt\\": result = cmath.sqrt(*args) elif op == \\"acos\\": result = cmath.acos(*args) elif op == \\"asin\\": result = cmath.asin(*args) elif op == \\"atan\\": result = cmath.atan(*args) elif op == \\"cos\\": result = cmath.cos(*args) elif op == \\"sin\\": result = cmath.sin(*args) elif op == \\"tan\\": result = cmath.tan(*args) elif op == \\"abs\\": result = abs(*args) # using built-in abs function results.append(result) return results ```","solution":"import cmath def perform_complex_operations(operations): results = [] for operation in operations: op = operation[0] args = operation[1:] if op == \\"polar\\": result = cmath.polar(*args) elif op == \\"rect\\": result = cmath.rect(*args) elif op == \\"exp\\": result = cmath.exp(*args) elif op == \\"log\\": result = cmath.log(*args) elif op == \\"log10\\": result = cmath.log10(*args) elif op == \\"sqrt\\": result = cmath.sqrt(*args) elif op == \\"acos\\": result = cmath.acos(*args) elif op == \\"asin\\": result = cmath.asin(*args) elif op == \\"atan\\": result = cmath.atan(*args) elif op == \\"cos\\": result = cmath.cos(*args) elif op == \\"sin\\": result = cmath.sin(*args) elif op == \\"tan\\": result = cmath.tan(*args) elif op == \\"abs\\": result = abs(*args) # using built-in abs function results.append(result) return results"},{"question":"Objective: Write a function that compares two lists of strings and returns a detailed comparison report. The function should utilize the `difflib.SequenceMatcher` class to identify matching and differing subsequences and generate a human-readable delta using the `difflib.Differ` or `difflib.ndiff()` function. Function Signature: ```python def compare_texts(text1: List[str], text2: List[str]) -> List[str]: Compare two lists of strings and return a detailed comparison report. Parameters: text1 (List[str]): The first list of strings to compare. text2 (List[str]): The second list of strings to compare. Returns: List[str]: A list of strings representing the detailed comparison report. ``` Requirements: 1. Use `difflib.SequenceMatcher` to analyze the similarity between `text1` and `text2`. 2. Generate a human-readable delta showing the differences and similarities between the two texts using either `difflib.Differ` or `difflib.ndiff()`. 3. The delta should indicate lines that are unique to `text1`, unique to `text2`, and common to both. 4. Ensure that the output format is easy to read and understand. Example: ```python text1 = [ \\"1. Beautiful is better than ugly.\\", \\"2. Explicit is better than implicit.\\", \\"3. Simple is better than complex.\\", \\"4. Complex is better than complicated.\\" ] text2 = [ \\"1. Beautiful is better than ugly.\\", \\"3. Simple is better than complex.\\", \\"4. Complicated is better than complex.\\", \\"5. Flat is better than nested.\\" ] output = compare_texts(text1, text2) for line in output: print(line) ``` Expected Output: ``` 1. Beautiful is better than ugly. - 2. Explicit is better than implicit. - 3. Simple is better than complex. + 3. Simple is better than complex. ?- ++ - 4. Complex is better than complicated. ?- ^ ---- ^ + 4. Complicated is better than complex. ?- ++++ ^ ^ + 5. Flat is better than nested. ``` Constraints: 1. Each line in the input lists (text1 and text2) will be non-empty strings. 2. Both input lists can have different lengths. 3. Ensure the performance is optimized for lists containing up to 1000 strings. Implement the `compare_texts` function to meet the requirements.","solution":"from typing import List import difflib def compare_texts(text1: List[str], text2: List[str]) -> List[str]: Compare two lists of strings and return a detailed comparison report. Parameters: text1 (List[str]): The first list of strings to compare. text2 (List[str]): The second list of strings to compare. Returns: List[str]: A list of strings representing the detailed comparison report. diff = difflib.ndiff(text1, text2) return list(diff)"},{"question":"Objective: Implement a function that processes a mixed list of various types (integers, floats, strings, lists, tuples, and dictionaries) and returns a filtered result based on specific criteria. Problem Statement: Write a function `process_items(items: list) -> list` that takes a list of mixed items and returns a new list filtered and formatted according to the following rules: - Integers and floats should be squared if they are even or round down to the nearest integer if they are odd. - Strings should be converted to their ASCII representation (an ASCII representation for each character). - Lists should be reversed. - Tuples should have their elements sorted. - Dictionaries should have their keys sorted. Input: - A list `items` containing a mix of integers, floats, strings, lists, tuples, and dictionaries. Output: - A new list with processed items according to the rules defined above. Constraints: - Each item in the input list will be of type `int`, `float`, `str`, `list`, `tuple`, or `dict`. - The input list will have at least one item and no more than 100 items. - The integers and floats will be in the range of `-10^6` to `10^6`. - The strings are guaranteed to be non-empty and contain only printable ASCII characters. Examples: ```python def process_items(items: list) -> list: # Implement the processing as per the rules given pass # Example print(process_items([2, 3.5, \\"hello\\", [1, 2, 3], (3, 1, 2), {\'b\': 2, \'a\': 1}])) # Output: [4, 3, [104, 101, 108, 108, 111], [3, 2, 1], (1, 2, 3), {\'a\': 1, \'b\': 2}] ``` Notes: - Even integers should be squared: e.g., 4 should become 16. - Odd integers should be rounded down to the nearest integer: e.g., 3.5 should become 3. - Each character in strings should be replaced with its ASCII value: e.g., \\"hi\\" should become `[104, 105]`. - Lists should be reversed: e.g., [1, 2, 3] should become [3, 2, 1]. - Tuples should be sorted in ascending order: e.g., (3, 1, 2) should become (1, 2, 3). - Dictionaries should have their keys sorted: e.g., {\'b\': 2, \'a\': 1} should become {\'a\': 1, \'b\': 2}.","solution":"def process_items(items: list) -> list: Processes items in a list according to specified rules: - Integers and floats should be squared if even or rounded down if odd. - Strings should be converted to their ASCII representation. - Lists should be reversed. - Tuples should be sorted. - Dictionaries should have their keys sorted. def process_item(item): if isinstance(item, int): return item**2 if item % 2 == 0 else item elif isinstance(item, float): return int(item) elif isinstance(item, str): return [ord(char) for char in item] elif isinstance(item, list): return item[::-1] elif isinstance(item, tuple): return tuple(sorted(item)) elif isinstance(item, dict): return {k: item[k] for k in sorted(item)} else: raise ValueError(\\"Unsupported item type\\") return [process_item(item) for item in items]"},{"question":"# Question: Advanced Customization and Visualization with Seaborn Color Palettes You are provided with sales data for three different product categories over the past 12 months. Your task is to create a visually appealing line plot using `seaborn` that highlights each product category with a unique color scheme. You will define a custom color palette and apply it to the plot. Additionally, you should format the graph to be publication-ready. Dataset The dataset is provided as a dictionary below: ```python months = [\\"Jan\\", \\"Feb\\", \\"Mar\\", \\"Apr\\", \\"May\\", \\"Jun\\", \\"Jul\\", \\"Aug\\", \\"Sep\\", \\"Oct\\", \\"Nov\\", \\"Dec\\"] data = { \\"Electronics\\": [200, 220, 230, 210, 250, 240, 260, 270, 300, 320, 330, 340], \\"Clothing\\": [150, 160, 170, 180, 190, 200, 210, 220, 210, 190, 180, 170], \\"Home Goods\\": [100, 110, 120, 130, 140, 150, 170, 190, 210, 230, 250, 270] } ``` Requirements 1. **Custom Color Palette**: - Create and use a custom color palette with three distinct colors using the `seaborn` library. - The colors should be perceptually uniform to ensure that the differences in product categories are clear. 2. **Line Plot**: - Create a line plot using `seaborn` displaying the sales data for each product category across the 12 months. - Ensure each line represents a product category with the respective color from the custom palette. 3. **Plot Customizations**: - Add titles and labels to the x-axis, y-axis, and the plot itself. - Include a legend indicating the color assigned to each product category. - Format the plot to be publication-ready (e.g. gridlines, font sizes, line styles). Input and Output Format - **Input**: The dataset dictionary and months list provided above. - **Output**: A line plot with the stipulated customizations. Constraints - You must use `seaborn` for creating the plot. - The line plot should be clear, with distinct colors for each product category. Example Result The expected result is a matplotlib figure created using seaborn that visually represents the sales of each category across 12 months, with a custom color palette applied. ```python import seaborn as sns import matplotlib.pyplot as plt # Your implementation here ``` Note - Make sure to include comments explaining each part of your code. - Document any assumptions or steps you take to achieve the result.","solution":"import seaborn as sns import matplotlib.pyplot as plt import pandas as pd def create_sales_plot(months, data): Creates a sales line plot with custom color palette for given sales data. Args: months (list): List of month names. data (dict): Dictionary containing sales data for different product categories. Returns: None # Create a DataFrame from the provided dictionary df = pd.DataFrame(data, index=months).reset_index().melt(id_vars=\'index\', var_name=\'Category\', value_name=\'Sales\') df.rename(columns={\'index\': \'Month\'}, inplace=True) # Define custom color palette custom_palette = sns.color_palette(\\"husl\\", 3) # Perceptually uniform chosen # Create the line plot using seaborn plt.figure(figsize=(12, 6)) sns.lineplot(data=df, x=\'Month\', y=\'Sales\', hue=\'Category\', palette=custom_palette) # Customizing the plot plt.title(\'Monthly Sales Data for Different Product Categories\') plt.xlabel(\'Month\') plt.ylabel(\'Sales\') plt.legend(title=\'Product Category\', loc=\'upper left\') plt.grid(True) plt.show()"},{"question":"Adjust Normalization Layers in a PyTorch Model Objective Create a PyTorch neural network model that dynamically replaces BatchNorm layers with GroupNorm layers, based on given constraints and settings. This task will assess your understanding of PyTorch models, normalization techniques, and functional programming in Python. Task 1. Write a class `CustomModel` that extends `torch.nn.Module` and initializes a simple convolutional neural network with BatchNorm layers. 2. Implement a method within the class or a standalone function `adjust_normalization` which will: - Replace all BatchNorm layers with GroupNorm layers, given the number of groups `G`. - Ensure any replacement respects the constraint `C % G == 0`. - Alternatively, change BatchNorm layers to not track running stats if `track_running_stats` is set to `False`. 3. Demonstrate the functionality with an example network. Input and Output Formats - **Input**: - A list of configuration dictionaries. Each dictionary will contain the following keys: - `type` (str): Type of normalization. Can be `\\"Group\\"` for GroupNorm or `\\"Batch\\"` for BatchNorm. - `track_running_stats` (bool): Indicates whether to track running stats (only applicable for BatchNorm). - `G` (int, optional): Number of groups for GroupNorm (only applicable when `type` is `\\"Group\\"`). - **Output**: - The modified PyTorch model with appropriate normalization layers. Constraints and Limitations: - Ensure the replacement respects `C % G == 0` when `type` is `\\"Group\\"`. If not possible, set `G = C`. - If `type` is `\\"Batch\\"` with `track_running_stats` set to `False`, modify existing BatchNorm layers accordingly. - The network architecture and initial parameters can be simple but should at least include a few convolutional and normalization layers. Example Usage: ```python import torch import torch.nn as nn import torch.nn.functional as F class CustomModel(nn.Module): def __init__(self): super(CustomModel, self).__init__() self.conv1 = nn.Conv2d(3, 16, 3, 1) self.bn1 = nn.BatchNorm2d(16) self.conv2 = nn.Conv2d(16, 32, 3, 1) self.bn2 = nn.BatchNorm2d(32) self.fc1 = nn.Linear(32 * 6 * 6, 10) def forward(self, x): x = self.conv1(x) x = F.relu(self.bn1(x)) x = self.conv2(x) x = F.relu(self.bn2(x)) x = F.adaptive_avg_pool2d(x, (6, 6)) x = x.view(-1, 32 * 6 * 6) x = self.fc1(x) return x def adjust_normalization(model, config): # Implementation of normalization adjustment pass # Testing the functionality model = CustomModel() config = [ {\\"type\\": \\"Group\\", \\"G\\": 4}, {\\"type\\": \\"Batch\\", \\"track_running_stats\\": False} ] adjust_normalization(model, config) print(model) ``` Implement the body of the function `adjust_normalization` to make the necessary adjustments to the model\'s normalization layers as per the provided configuration.","solution":"import torch import torch.nn as nn import torch.nn.functional as F class CustomModel(nn.Module): def __init__(self): super(CustomModel, self).__init__() self.conv1 = nn.Conv2d(3, 16, 3, 1) self.bn1 = nn.BatchNorm2d(16) self.conv2 = nn.Conv2d(16, 32, 3, 1) self.bn2 = nn.BatchNorm2d(32) self.fc1 = nn.Linear(32 * 6 * 6, 10) def forward(self, x): x = self.conv1(x) x = F.relu(self.bn1(x)) x = self.conv2(x) x = F.relu(self.bn2(x)) x = F.adaptive_avg_pool2d(x, (6, 6)) x = x.view(-1, 32 * 6 * 6) x = self.fc1(x) return x def adjust_normalization(model, config): def replace_layer(module, config): for name, child in module.named_children(): if isinstance(child, nn.BatchNorm2d): if config[\\"type\\"] == \\"Group\\": C = child.num_features G = config[\\"G\\"] if C % config[\\"G\\"] == 0 else C setattr(module, name, nn.GroupNorm(G, C)) elif config[\\"type\\"] == \\"Batch\\": running_stats = config.get(\\"track_running_stats\\", True) new_bn = nn.BatchNorm2d(child.num_features, track_running_stats=running_stats) setattr(module, name, new_bn) else: replace_layer(child, config) for conf in config: replace_layer(model, conf) # Example usage model = CustomModel() config = [ {\\"type\\": \\"Group\\", \\"G\\": 4}, {\\"type\\": \\"Batch\\", \\"track_running_stats\\": False} ] adjust_normalization(model, config)"},{"question":"**Advanced Coding Assessment: Implement and Compare Linear Models in scikit-learn** **Objective:** To assess your understanding of various linear regression models in scikit-learn by implementing, fitting, and comparing the performance of several models on a dataset. **Task:** You are provided with a dataset of housing prices. Your task is to implement and compare the following regression models: `LinearRegression`, `Ridge`, `Lasso`, and `ElasticNet` using scikit-learn. You will: 1. Load the dataset and preprocess it as necessary. 2. Split the dataset into training and testing sets. 3. Fit each model to the training data. 4. Predict the housing prices on the test data using each model. 5. Evaluate and compare the performance of each model using appropriate metrics such as Mean Squared Error (MSE) and R² score. **Dataset Description:** The dataset `housing.csv` contains the following columns: - `Number of Rooms` - `Number of Bathrooms` - `Area (sq ft)` - `Age of House (years)` - `Price` **Instructions:** 1. **Data Loading and Preprocessing:** - Load the dataset from the `housing.csv` file. - Separate the features (X) from the target variable (y) where `y` is the `Price`. - Handle any missing values if present. 2. **Data Splitting:** - Split the dataset into a training set (80%) and a testing set (20%). 3. **Model Implementation and Fitting:** - Implement the following models: - `LinearRegression` - `Ridge` with `alpha=1.0` - `Lasso` with `alpha=1.0` - `ElasticNet` with `alpha=1.0` and `l1_ratio=0.5` - Fit each model to the training data. 4. **Predictions:** - Use each fitted model to predict the prices on the test data. 5. **Evaluation:** - Calculate the following for each model: - Mean Squared Error (MSE) - R² score - Output these metrics in a comparative table or plot. 6. **Conclusion:** - Provide a brief discussion on how each model performed and the potential reasons for any differences in performance. **Constraints:** - Do not use any algorithms or techniques outside of those available in scikit-learn for the models specified. - Your code should be clear, well-commented, and should handle any edge cases or exceptions gracefully. **Performance Requirements:** - The solution should be efficient with a complexity that can handle typical sizes of datasets used in practical scenarios. **Example**: Here\'s a snippet to illustrate loading and splitting the dataset: ```python import pandas as pd from sklearn.model_selection import train_test_split from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet from sklearn.metrics import mean_squared_error, r2_score # Load the data data = pd.read_csv(\'housing.csv\') # Preprocess the data X = data.drop(\'Price\', axis=1) y = data[\'Price\'] # Split the data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Implement and fit the models models = { \'LinearRegression\': LinearRegression(), \'Ridge\': Ridge(alpha=1.0), \'Lasso\': Lasso(alpha=1.0), \'ElasticNet\': ElasticNet(alpha=1.0, l1_ratio=0.5) } results = {} for name, model in models.items(): model.fit(X_train, y_train) y_pred = model.predict(X_test) mse = mean_squared_error(y_test, y_pred) r2 = r2_score(y_test, y_pred) results[name] = {\'MSE\': mse, \'R2\': r2} # Print the results for name, metrics in results.items(): print(f\\"{name}:ntMSE: {metrics[\'MSE\']}ntR²: {metrics[\'R2\']}\\") ``` This example should help you structure your solution. Work through each step methodically and ensure your code is robust and well-documented. Good luck!","solution":"import pandas as pd from sklearn.model_selection import train_test_split from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet from sklearn.metrics import mean_squared_error, r2_score def load_and_preprocess_data(filepath): # Load the dataset data = pd.read_csv(filepath) # Separate features and target variable X = data.drop(\'Price\', axis=1) y = data[\'Price\'] return X, y def split_data(X, y, test_size=0.2, random_state=42): return train_test_split(X, y, test_size=test_size, random_state=random_state) def fit_and_evaluate_models(X_train, X_test, y_train, y_test): # Define the models models = { \'LinearRegression\': LinearRegression(), \'Ridge\': Ridge(alpha=1.0), \'Lasso\': Lasso(alpha=1.0), \'ElasticNet\': ElasticNet(alpha=1.0, l1_ratio=0.5) } results = {} # Train and evaluate each model for name, model in models.items(): model.fit(X_train, y_train) y_pred = model.predict(X_test) mse = mean_squared_error(y_test, y_pred) r2 = r2_score(y_test, y_pred) results[name] = {\'MSE\': mse, \'R2\': r2} return results def main(filepath): # Load and preprocess the data X, y = load_and_preprocess_data(filepath) # Split the data into training and testing sets X_train, X_test, y_train, y_test = split_data(X, y) # Fit and evaluate models results = fit_and_evaluate_models(X_train, X_test, y_train, y_test) # Print the results for name, metrics in results.items(): print(f\\"{name}:ntMSE: {metrics[\'MSE\']}ntR²: {metrics[\'R2\']}\\") return results"},{"question":"Question # Objective: Create utility functions for escaping and unescaping XML character entities and preparing attribute values for XML elements, similar to the functionality provided in the `xml.sax.saxutils` module. # Task: You are tasked with implementing three functions: 1. `custom_escape(data: str, entities: dict = None) -> str` 2. `custom_unescape(data: str, entities: dict = None) -> str` 3. `custom_quoteattr(data: str, entities: dict = None) -> str` # Function Specifications: 1. **custom_escape(data, entities={})** - **Input**: * `data`: A string containing the data to be escaped. * `entities`: (Optional) A dictionary where each key-value pair represents a character and its escaped representation. - **Output**: A string with `\\"&\\"`, `\\"<\\"`, `\\">\\"` always escaped, and other characters escaped based on the `entities` dictionary. - **Constraints**: 1. If `entities` is not provided, it should default to an empty dictionary. 2. The keys and values in the `entities` dictionary must be strings. - **Example**: ```python custom_escape(\\"5 < 6 & 7 > 4\\") # Output: \\"5 &lt; 6 &amp; 7 &gt; 4\\" custom_escape(\\"5 < 6 & 7 > 4\\", {\\"5\\": \\"five\\", \\"4\\": \\"four\\"}) # Output: \\"five &lt; 6 &amp; 7 &gt; four\\" ``` 2. **custom_unescape(data, entities={})** - **Input**: * `data`: A string containing the data to be unescaped. * `entities`: (Optional) A dictionary where each key-value pair represents an escaped character and its unescaped representation. - **Output**: A string with `\\"&amp;\\"`, `\\"&lt;\\"`, `\\"&gt;\\"` always unescaped, and other characters unescaped based on the `entities` dictionary. - **Constraints**: 1. If `entities` is not provided, it should default to an empty dictionary. 2. The keys and values in the `entities` dictionary must be strings. - **Example**: ```python custom_unescape(\\"5 &lt; 6 &amp; 7 &gt; 4\\") # Output: \\"5 < 6 & 7 > 4\\" custom_unescape(\\"five &lt; 6 &amp; 7 &gt; four\\", {\\"five\\": \\"5\\", \\"four\\": \\"4\\"}) # Output: \\"5 < 6 & 7 > 4\\" ``` 3. **custom_quoteattr(data, entities={})** - **Input**: * `data`: A string containing the data to be prepared as an XML attribute value. * `entities`: (Optional) A dictionary where each key-value pair represents a character and its escaped representation. - **Output**: A quoted version of `data` ready to be used as an attribute value, with necessary characters escaped. - **Constraints**: 1. If `entities` is not provided, it should default to an empty dictionary. 2. The keys and values in the `entities` dictionary must be strings. 3. The function should choose the quote character (single or double) that avoids encoding quote characters in the `data`. If both types of quotes are present in `data`, it should wrap `data` in double quotes and escape double quotes within. - **Example**: ```python custom_quoteattr(\\"Hello \'world\'\\") # Output: \'\\"Hello \'world\'\\"\' custom_quoteattr(\'Hello \\"world\\"\') # Output: \\"\'Hello \\"world\\"\'\\" ``` # Constraints: 1. Do not use any external libraries apart from the built-in libraries. 2. Handle edge cases such as empty strings and strings with no escapable characters. Implement all three functions ensuring they follow the specifications mentioned above.","solution":"def custom_escape(data: str, entities: dict = None) -> str: Escape XML characters in the data string. if entities is None: entities = {} # These are the basic XML escape entities base_entities = {\\"&\\": \\"&amp;\\", \\"<\\": \\"&lt;\\", \\">\\": \\"&gt;\\"} # Update with additional entities if provided base_entities.update(entities) # Escape data using the combined entities for key, value in base_entities.items(): data = data.replace(key, value) return data def custom_unescape(data: str, entities: dict = None) -> str: Unescape XML characters in the data string. if entities is None: entities = {} # These are the basic XML escape entities base_entities = {\\"&amp;\\": \\"&\\", \\"&lt;\\": \\"<\\", \\"&gt;\\": \\">\\"} # Update with additional entities if provided base_entities.update(entities) # Unescape data using the combined entities for key, value in base_entities.items(): data = data.replace(key, value) return data def custom_quoteattr(data: str, entities: dict = None) -> str: Quote and escape an XML attribute value. if entities is None: entities = {} # Escape data first data = custom_escape(data, entities) # Determine which quote to use if \'\\"\' in data and \\"\'\\" in data: # Both quotes present, use double quotes and escape the internal double quotes quoted_data = \'\\"\' + data.replace(\'\\"\', \\"&quot;\\") + \'\\"\' elif \'\\"\' in data: # Double quotes are in data, use single quotes quoted_data = \\"\'\\" + data + \\"\'\\" else: # Single quotes are in data or neither quotes, use double quotes quoted_data = \'\\"\' + data + \'\\"\' return quoted_data"},{"question":"**Python Coding Assessment Question** **Objective:** You are tasked with creating a function that retrieves information from the Unix shadow password database and filters it based on provided criteria. **Problem Statement:** You need to implement a function `filter_shadow_entries(criteria: dict) -> list` that accepts a dictionary `criteria` and returns a list of shadow password database entries that match the provided criteria. The criteria will specify the attribute and the value to match. The attributes correspond to the members of the shadow password structure (e.g., sp_namp for login name, sp_max for maximum number of days between changes). **Function Signature:** ```python def filter_shadow_entries(criteria: dict) -> list: pass ``` **Input:** - `criteria` (dict): A dictionary where the keys are attribute names (str) and the values are the corresponding values to match. **Output:** - A list of shadow password database entries (tuple-like objects) that match all of the provided criteria. **Constraints:** - You can assume that the `criteria` dictionary will contain valid attribute names. - The function should handle cases where the user does not have the privileges required to access the shadow password database by raising a `PermissionError`. - If the attribute values do not match any entry, an empty list should be returned. - Python version 3.10 or above should be used. **Example:** ```python # Example of usage: criteria = { \'sp_namp\': \'root\', \'sp_max\': 99999 } result = filter_shadow_entries(criteria) # Expected output (assuming the `root` entry exists and has a `sp_max` value of 99999): # [(shadow password entry corresponding to the root user)] ``` **Notes:** - Ensure that you handle any exceptions appropriately and raise a `PermissionError` if access is denied. - Use the `spwd.getspall()` function to retrieve all shadow password entries and filter them based on the criteria. **Evaluation Criteria:** - Correct implementation of the function as per the specified requirements. - Proper handling of permission issues. - Accurate filtering of shadow password entries based on the provided criteria. - Code quality and adherence to Python best practices.","solution":"import spwd def filter_shadow_entries(criteria: dict) -> list: Filters entries in the Unix shadow password database based on provided criteria. Parameters: criteria (dict): A dictionary where the keys are attribute names and the values are the corresponding values to match. Returns: list: A list of shadow password database entries that match all of the provided criteria. Raises: PermissionError: If access to the shadow password database is denied. try: all_entries = spwd.getspall() except PermissionError: raise PermissionError(\\"Access to the shadow password database is denied.\\") # Filter entries based on criteria matching_entries = [] for entry in all_entries: match = all(getattr(entry, key) == value for key, value in criteria.items()) if match: matching_entries.append(entry) return matching_entries"},{"question":"# Shared Memory Coordination in Python Problem Statement You are tasked with implementing a function that coordinates multiple processes to compute the sum of an array by dividing the work among them. You will use the `multiprocessing.shared_memory` module to allow the processes to share the input array and to store their results. In particular, you need to: 1. Divide the input array among a specified number of worker processes. 2. Each process will compute the sum of its assigned portion of the array and store the result in a shared memory segment designated for the results. 3. After all processes complete their computation, the main process will sum the partial results produced by each worker to get the final total sum. Input - A list of integers `arr`. - An integer `num_workers` indicating the number of worker processes to use. - Each worker should be assigned as equal a portion of the array as possible (some workers might have one less element). Output - An integer representing the total sum of the elements in the array. Implementation 1. Define the function `parallel_sum`. 2. Use `multiprocessing.shared_memory.SharedMemory` to create shared memory blocks for the input array and for storing the partial results. 3. Divide the work among the `num_workers` processes, each computing the sum of its assigned portion. 4. Aggregate the partial results into the final sum. 5. Ensure proper cleanup of shared memory resources. Constraints - The length of `arr` will be at least 1 and at most 10,000. - The number of worker processes `num_workers` will be at least 1 and at most the length of `arr`. Example ```python from multiprocessing import Process, shared_memory import numpy as np def worker(arr, start, end, shm_name, index): # Attach to the existing shared memory block shm = shared_memory.SharedMemory(name=shm_name) array = np.ndarray((end - start,), dtype=np.int64, buffer=shm.buf) partial_sum = sum(array[start:end]) # Attach to the shared memory for results shm_results = shared_memory.SharedMemory(name=f\'results_{index}\') result_array = np.ndarray((1,), dtype=np.int64, buffer=shm_results.buf) result_array[0] = partial_sum # Cleanup shm.close() shm_results.close() def parallel_sum(arr, num_workers): # Your implementation here pass # Example usage: arr = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] num_workers = 3 result = parallel_sum(arr, num_workers) print(result) # Should output 55 ``` This function should demonstrate the practical use of shared memory for splitting tasks among multiple worker processes and the coordination of these processes to produce a combined result.","solution":"from multiprocessing import Process, shared_memory import numpy as np def worker(arr_shm_name, start, end, results_shm_name, worker_index): shm = shared_memory.SharedMemory(name=arr_shm_name) array = np.ndarray((end - start,), dtype=np.int64, buffer=shm.buf[start * np.dtype(np.int64).itemsize:]) partial_sum = np.sum(array) results_shm = shared_memory.SharedMemory(name=results_shm_name) results_array = np.ndarray((1,), dtype=np.int64, buffer=results_shm.buf[worker_index * np.dtype(np.int64).itemsize:]) results_array[0] = partial_sum shm.close() results_shm.close() def parallel_sum(arr, num_workers): arr_len = len(arr) step = arr_len // num_workers processes = [] shm_arr = shared_memory.SharedMemory(create=True, size=arr_len * np.dtype(np.int64).itemsize) shm_arr_array = np.ndarray(arr_len, dtype=np.int64, buffer=shm_arr.buf) np.copyto(shm_arr_array, arr) results_shm = shared_memory.SharedMemory(create=True, size=num_workers * np.dtype(np.int64).itemsize) results_array = np.ndarray(num_workers, dtype=np.int64, buffer=results_shm.buf) for i in range(num_workers): start = i * step end = (i + 1) * step if i != num_workers - 1 else arr_len p = Process(target=worker, args=(shm_arr.name, start, end, results_shm.name, i)) p.start() processes.append(p) for p in processes: p.join() total_sum = np.sum(results_array) shm_arr.close() shm_arr.unlink() results_shm.close() results_shm.unlink() return total_sum"},{"question":"**Tensor Attributes Handling and Operations:** Given the following specifications about PyTorch tensor attributes, your task is to implement a function that: 1. Creates a tensor with a specified `dtype`, `device`, and `layout`. 2. Performs a set of arithmetic operations and returns the resultant tensor along with its `dtype` and `device`. 3. Ensures that the output tensor adheres to type promotion rules and device compatibility. Implement the function `create_and_operate_tensors` in PyTorch with the following details: # Function Signature ```python def create_and_operate_tensors(data, dtype_str, device_str, operation): Parameters: data (list of list of numbers): Initial data to create the tensor. dtype_str (str): Data type of the tensor elements. (e.g., \'float32\', \'int64\') device_str (str): Device where the tensor will be allocated. (e.g., \'cpu\', \'cuda:0\') operation (str): Arithmetic operation to perform. (e.g., \'add\', \'mul\') Returns: tuple: Resultant tensor, its dtype, and its device. ``` # Input - `data`: A list of list of numerical values to initialize the tensor. - `dtype_str`: A string denoting the desired tensor\'s data type. - `device_str`: A string denoting the device where the tensor will be allocated. - `operation`: A string specifying the arithmetic operation (e.g., \'add\', \'mul\') to perform, involving another tensor with the same data but on a different `dtype` and `device`. # Output - A tuple containing: - The resultant tensor after applying the specified operation. - The `dtype` of the resultant tensor. - The `device` of the resultant tensor. # Constraints - Ensure that the arithmetic operation respects PyTorch\'s type promotion rules. - Handle the possible movement of tensors across devices properly. # Example ```python data = [[1, 2], [3, 4]] dtype_str = \'float32\' device_str = \'cuda:0\' operation = \'add\' result_tensor, result_dtype, result_device = create_and_operate_tensors(data, dtype_str, device_str, operation) print(result_tensor) print(result_dtype) # torch.float32 print(result_device) # cuda:0 ``` # Notes - The additional tensor for the operation will be created with the same `data` but with `dtype` as `int64` and `device` as `cpu`. - Ensure that appropriate type promotion and device movements are handled, keeping efficiency in mind. # Evaluation - Correctness of tensor creation based on specified attributes. - Proper application of arithmetic operations and resulting `dtype` and `device`.","solution":"import torch def create_and_operate_tensors(data, dtype_str, device_str, operation): Parameters: data (list of list of numbers): Initial data to create the tensor. dtype_str (str): Data type of the tensor elements. (e.g., \'float32\', \'int64\') device_str (str): Device where the tensor will be allocated. (e.g., \'cpu\', \'cuda:0\') operation (str): Arithmetic operation to perform. (e.g., \'add\', \'mul\') Returns: tuple: Resultant tensor, its dtype, and its device. dtype = getattr(torch, dtype_str) device = torch.device(device_str) tensor_a = torch.tensor(data, dtype=dtype, device=device) tensor_b = torch.tensor(data, dtype=torch.int64, device=torch.device(\'cpu\')) if device_str != \'cpu\': tensor_b = tensor_b.to(device) if operation == \'add\': result_tensor = tensor_a + tensor_b elif operation == \'mul\': result_tensor = tensor_a * tensor_b else: raise ValueError(\\"Unsupported operation. Supported operations are \'add\' and \'mul\'\\") return (result_tensor, result_tensor.dtype, result_tensor.device)"},{"question":"**Objective:** Implement a custom `Function` in PyTorch that computes the element-wise exponential of a tensor but uses a temporary file to save the result during the forward pass, and retrieves it during the backward pass. This is to simulate a scenario where intermediary results are saved to disk due to memory constraints. **Requirements:** 1. Define a custom PyTorch `Function` named `ExpWithTempFile` that: - Saves the result of the forward pass (exponential of the input tensor) to a temporary file. - Retrieves the saved result during the backward pass to compute gradients. 2. Implement a test case to ensure that the gradients are correctly computed. **Steps:** 1. **Forward Pass**: - Compute the element-wise exponential of the input tensor. - Save the result to a temporary file. - Return the result tensor. 2. **Backward Pass**: - Load the result from the temporary file. - Use the loaded result to compute the gradient of the loss with respect to the input tensor using the chain rule. 3. **Temporary File Handling**: - Use a class to manage the lifecycle of the temporary file (create, save tensor, and delete the file after usage). **Input Format:** A tensor `x` of shape `(n,)` with `requires_grad=True`. **Output Format:** The tensor containing the exponential of each element in `x` and the gradients with respect to `x` after performing backward pass. **Performance Constraints**: The operations should correctly simulate the saving and loading of intermediate results to disk to mimic a scenario of handling large tensors that do not fit into memory. **Code Template:** ```python import torch import os import pickle import tempfile from torch.autograd import Function class SelfDeletingTempFile: def __init__(self): self.name = None def save_tensor(self, tensor): self.name = tempfile.mktemp() with open(self.name, \'wb\') as f: pickle.dump(tensor, f) def load_tensor(self): with open(self.name, \'rb\') as f: tensor = pickle.load(f) return tensor def __del__(self): if self.name and os.path.exists(self.name): os.remove(self.name) class ExpWithTempFile(Function): @staticmethod def forward(ctx, input): result = torch.exp(input) temp_file = SelfDeletingTempFile() temp_file.save_tensor(result) ctx.temp_file = temp_file return result @staticmethod def backward(ctx, grad_output): result = ctx.temp_file.load_tensor() return grad_output * result # You may test your implementation using this block x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True) y = ExpWithTempFile.apply(x) y.backward(torch.ones_like(x)) print(f\'Input: {x}\') print(f\'Exponential: {y}\') print(f\'Gradients: {x.grad}\') ``` 1. Implement the `SelfDeletingTempFile` class to handle the temporary file\'s lifecycle. 2. Modify the `ExpWithTempFile` class to correctly save the result during the forward pass and retrieve it during the backward pass. 3. Ensure your implementation correctly prints the input tensor, the computed exponential in the forward pass, and the gradients after the backward pass.","solution":"import torch import os import pickle import tempfile from torch.autograd import Function class SelfDeletingTempFile: def __init__(self): self.name = None def save_tensor(self, tensor): self.name = tempfile.mktemp() with open(self.name, \'wb\') as f: pickle.dump(tensor, f) def load_tensor(self): with open(self.name, \'rb\') as f: tensor = pickle.load(f) return tensor def __del__(self): if self.name and os.path.exists(self.name): os.remove(self.name) class ExpWithTempFile(Function): @staticmethod def forward(ctx, input): result = torch.exp(input) temp_file = SelfDeletingTempFile() temp_file.save_tensor(result) ctx.temp_file = temp_file return result @staticmethod def backward(ctx, grad_output): result = ctx.temp_file.load_tensor() return grad_output * result"},{"question":"# Seaborn Categorical Data Plotting Assessment **Objective:** Demonstrate your comprehension of `seaborn` for visualizing categorical data by implementing and customizing various plots. **Task:** 1. **Load and Prepare the Dataset** - Load the `tips` dataset from seaborn. - Add a new column `weekend` designating whether the day is a weekend (Saturday or Sunday). 2. **Create and Customize Plots** - **Scatter Plot:** 1. Create a `catplot` scatter plot (use `kind=\'swarm\'`) showing `total_bill` on the y-axis and `day` on the x-axis. 2. Use the `hue` parameter to differentiate the points based on the `time` of the day (Lunch/Dinner). - **Box Plot:** 1. Create a `catplot` box plot showing `total_bill` on the y-axis and `day` on the x-axis. 2. Use the `hue` parameter to differentiate the boxes by `weekend`. 3. Disable dodging so that the boxes for weekdays and weekends are not shifted. - **Violin Plot:** 1. Create a `catplot` violin plot showing `total_bill` on the y-axis and `day` on the x-axis. 2. Split the violins based on the `sex` of the customer. 3. Display individual observations inside the violins using `inner=\\"stick\\"`. 4. Adjust the bandwidth (`bw_adjust`) to `0.5` to smooth the density estimate. - **Bar Plot:** 1. Create a `catplot` bar plot showing the average `total_bill` on the y-axis and `day` on the x-axis. 2. Use the `hue` parameter to differentiate based on `time`. 3. Display 95% confidence intervals around the estimates. **Input:** None. The dataset will be loaded within the function. **Output:** Plots displayed inline using `matplotlib`. **Constraints:** - Use the `seaborn` library for all visualizations. - The plots should be properly labeled and have appropriate titles. **Performance Requirements:** The plots should render within a reasonable time frame for interactive use, typically under a few seconds with the `tips` dataset. # Implementation In a Jupyter notebook, implement the function `visualize_categorical_data()` as detailed below: ```python import seaborn as sns import matplotlib.pyplot as plt def visualize_categorical_data(): # Load the dataset tips = sns.load_dataset(\\"tips\\") # Create a \'weekend\' column tips[\'weekend\'] = tips[\'day\'].isin([\'Sat\', \'Sun\']) # Scatter Plot sns.catplot(data=tips, x=\\"day\\", y=\\"total_bill\\", hue=\\"time\\", kind=\\"swarm\\") plt.title(\'Swarm Plot of Total Bill by Day and Time\') plt.show() # Box Plot sns.catplot(data=tips, x=\\"day\\", y=\\"total_bill\\", hue=\\"weekend\\", kind=\\"box\\", dodge=False) plt.title(\'Box Plot of Total Bill by Day and Weekend\') plt.show() # Violin Plot sns.catplot(data=tips, x=\\"day\\", y=\\"total_bill\\", hue=\\"sex\\", kind=\\"violin\\", bw_adjust=0.5, cut=0, split=True, inner=\\"stick\\") plt.title(\'Violin Plot of Total Bill by Day and Sex\') plt.show() # Bar Plot sns.catplot(data=tips, x=\\"day\\", y=\\"total_bill\\", hue=\\"time\\", kind=\\"bar\\", ci=95) plt.title(\'Bar Plot of Average Total Bill by Day and Time\') plt.show() # Test the function visualize_categorical_data() ``` This exercise will test your ability to work with seaborn for visualizing categorical data and customizing plots to convey meaningful insights.","solution":"import seaborn as sns import matplotlib.pyplot as plt def visualize_categorical_data(): # Load the dataset tips = sns.load_dataset(\\"tips\\") # Create a \'weekend\' column tips[\'weekend\'] = tips[\'day\'].isin([\'Sat\', \'Sun\']) # Scatter Plot sns.catplot(data=tips, x=\\"day\\", y=\\"total_bill\\", hue=\\"time\\", kind=\\"swarm\\") plt.title(\'Swarm Plot of Total Bill by Day and Time\') plt.show() # Box Plot sns.catplot(data=tips, x=\\"day\\", y=\\"total_bill\\", hue=\\"weekend\\", kind=\\"box\\", dodge=False) plt.title(\'Box Plot of Total Bill by Day and Weekend\') plt.show() # Violin Plot sns.catplot(data=tips, x=\\"day\\", y=\\"total_bill\\", hue=\\"sex\\", kind=\\"violin\\", bw_adjust=0.5, cut=0, split=True, inner=\\"stick\\") plt.title(\'Violin Plot of Total Bill by Day and Sex\') plt.show() # Bar Plot sns.catplot(data=tips, x=\\"day\\", y=\\"total_bill\\", hue=\\"time\\", kind=\\"bar\\", ci=95) plt.title(\'Bar Plot of Average Total Bill by Day and Time\') plt.show() # Test the function visualize_categorical_data()"},{"question":"**Objective**: Implement a PyTorch function that performs specified tensor operations while ensuring all memory is initialized, even when the `torch.utils.deterministic.fill_uninitialized_memory` is set to `False`. **Task**: You are required to write a function `initialize_tensor_and_ensure_determinism` that takes the following parameters: - `shape` (tuple of integers): The shape of the tensor to be initialized. - `value_type` (string): The type of the values to fill in the tensor. It should be one of `\'float\'`, `\'complex\'`, or `\'int\'`. - `fill_value` (optional, depends on `value_type`): The value to fill in the tensor. If not provided, use NaN for `\'float\'` and `\'complex\'`, and the maximum integer value for `\'int\'`. The function should: 1. Create an uninitialized tensor using the appropriate PyTorch function (`torch.empty`, `torch.empty_like`, etc.). 2. Fill the tensor with the specified fill value or default values as described. 3. Ensure that the tensor initialization and filling process is deterministic regardless of the `torch.utils.deterministic.fill_uninitialized_memory` setting. **Input**: - `shape: tuple(int, ...)`: The desired shape of the tensor. - `value_type: str`: Specifies the type of values to fill the tensor with, possible values are `\'float\'`, `\'complex\'`, `\'int\'`. - `fill_value: (optional, depends on value_type)`: The value to fill in the tensor. **Output**: - A PyTorch tensor of the specified shape and value type, deterministically initialized with the specified or default fill value. **Constraints**: - You can assume that the `value_type` is always valid (`\'float\'`, `\'complex\'`, or `\'int\'`). - You should adhere to PyTorch practices for tensor operations. - The function should be efficient but correctness and deterministic behavior are the priority. **Example**: ```python import torch import torch.utils.deterministic def initialize_tensor_and_ensure_determinism(shape, value_type, fill_value=None): # Ensure fill_uninitialized_memory is off torch.utils.deterministic.fill_uninitialized_memory = False if value_type == \'float\': tensor = torch.empty(shape, dtype=torch.float) fill_value = fill_value if fill_value is not None else float(\'nan\') elif value_type == \'complex\': tensor = torch.empty(shape, dtype=torch.complex64) fill_value = fill_value if fill_value is not None else complex(\'nan\') elif value_type == \'int\': tensor = torch.empty(shape, dtype=torch.int) fill_value = fill_value if fill_value is not None else torch.iinfo(torch.int).max tensor.fill_(fill_value) return tensor # Example Usage tensor = initialize_tensor_and_ensure_determinism((2, 2), \'float\') print(tensor) # Should print a tensor of shape (2, 2) filled with NaN or the specified value deterministically. ```","solution":"import torch import torch.utils.deterministic def initialize_tensor_and_ensure_determinism(shape, value_type, fill_value=None): # Ensure fill_uninitialized_memory is off torch.utils.deterministic.fill_uninitialized_memory = False if value_type == \'float\': tensor = torch.empty(shape, dtype=torch.float) fill_value = fill_value if fill_value is not None else float(\'nan\') elif value_type == \'complex\': tensor = torch.empty(shape, dtype=torch.complex64) fill_value = fill_value if fill_value is not None else complex(float(\'nan\')) elif value_type == \'int\': tensor = torch.empty(shape, dtype=torch.int) fill_value = fill_value if fill_value is not None else torch.iinfo(torch.int).max tensor.fill_(fill_value) return tensor"},{"question":"Understanding Copy-on-Write (CoW) in Pandas Objective: The objective of this assessment is to evaluate your understanding of pandas\' Copy-on-Write (CoW) mechanism. Specifically, you will modify DataFrame and Series objects, ensuring compliance with CoW rules and avoiding unintended side effects. Problem Statement: Given a DataFrame `df` and several operations to perform, refactor the code to comply with CoW rules and achieve the desired modifications without triggering unintended updates to multiple objects. Additionally, ensure that the underlying NumPy arrays are handled according to CoW principles. 1. **Ensure modifications to views do not affect the parent DataFrame:** Modify the following code snippet to prevent unintended alterations to `df` when `subset` is updated: ```python import pandas as pd df = pd.DataFrame({\\"foo\\": [1, 2, 3], \\"bar\\": [4, 5, 6]}) subset = df[\\"foo\\"] subset.iloc[0] = 100 # Refactor this line ``` 2. **Update with proper CoW compliance without side effects:** Refactor the following chained assignment and inplace modification to be CoW-compliant: ```python df = pd.DataFrame({\\"foo\\": [1, 2, 3], \\"bar\\": [4, 5, 6]}) df[\\"foo\\"][df[\\"bar\\"] > 5] = 100 # Refactor this line df[\\"foo\\"].replace(1, 5, inplace=True) # Refactor this line ``` 3. **Handle read-only NumPy arrays:** Correctly modify the underlying NumPy array returned from a DataFrame, ensuring compliance with CoW, and modify the array as required: ```python df = pd.DataFrame({\\"a\\": [1, 2], \\"b\\": [3, 4]}) arr = df.to_numpy() arr[0, 0] = 100 # Refactor this line ``` Input: - A pandas DataFrame `df` as shown in the code snippets. - Specific instructions to modify values within the DataFrame and the underlying NumPy array. Output: - Python code snippets with the necessary modifications to comply with CoW rules and ensure no unintended side effects: - Code snippet ensuring `df` is not modified when `subset` is updated. - Refactored chained assignment and inplace modification. - Updated code to handle read-only NumPy arrays and make modifications as specified. Constraints: - You must use modern CoW-compliant pandas techniques. - Avoid using deprecated or old-style pandas operations that can cause inadvertent side effects. Evaluation: Your solution will be evaluated based on: - Correctness: Modifications must comply with CoW rules. - Efficiency: Changes should not introduce unnecessary copies or significantly degrade performance. - Clarity: Refactored code should be clean, readable, and maintainable. **Note:** This assessment tests your ability to adapt to new mechanisms introduced in pandas 3.0+. Understanding the behavior of operations before and after CoW implementation is crucial.","solution":"import pandas as pd # Ensure modifications to views do not affect the parent DataFrame df = pd.DataFrame({\\"foo\\": [1, 2, 3], \\"bar\\": [4, 5, 6]}) subset = df[\\"foo\\"].copy() # Create a copy to avoid modifying the parent DataFrame subset.iloc[0] = 100 # Modifies the copy, not the original DataFrame # Update with proper CoW compliance without side effects df = pd.DataFrame({\\"foo\\": [1, 2, 3], \\"bar\\": [4, 5, 6]}) df.loc[df[\\"bar\\"] > 5, \\"foo\\"] = 100 # Use .loc to avoid chained assignment df[\\"foo\\"] = df[\\"foo\\"].replace(1, 5) # Avoid inplace operation by reassigning # Handle read-only NumPy arrays df = pd.DataFrame({\\"a\\": [1, 2], \\"b\\": [3, 4]}) arr = df.to_numpy().copy() # Copy the array to avoid modifying the original DataFrame arr[0, 0] = 100 # Modify the copy"},{"question":"# Question: Advanced Seaborn Heatmap Customization You are given a dataset containing information on various species and their measurements for different variables in a CSV file named `species_measurements.csv`. Your task is to perform the following steps to create a customized heatmap using Seaborn. Instructions: 1. **Load the Dataset**: Load the dataset from the CSV file named `species_measurements.csv` using pandas. 2. **Pivot the Data**: Pivot the DataFrame such that the rows represent different species, the columns represent different variables, and the values are the measurements. You can assume the dataset has columns named `Species`, `Variable`, and `Measurement`. 3. **Create a Heatmap**: Using Seaborn, create a heatmap with the following customizations: - Show the values of each cell as text annotations. - Format the annotations to one decimal place. - Use a colormap of your choice or a custom colormap. - Add lines between cells with a linewidth of 0.5. - Set the colormap norm such that the minimum value of the colormap is 0 and the maximum is 100. - Customize the x-axis and y-axis labels to be empty and position the x-axis ticks on the top. Input: You can assume the `species_measurements.csv` file has the following structure: ``` Species,Variable,Measurement Species1,Variable1,Value1 Species1,Variable2,Value2 ... SpeciesN,VariableM,ValueNM ``` Output: The output should be the Seaborn heatmap plot customized as per the instructions. Constraints: - Ensure that the heatmap is clearly readable with proper annotations and formatting. - Use the seaborn library for plotting and the pandas library for data manipulation. Example Code: The following is an outline of the steps you need to implement: ```python import pandas as pd import seaborn as sns import matplotlib.pyplot as plt # 1. Load the dataset data = pd.read_csv(\'species_measurements.csv\') # 2. Pivot the data pivot_data = data.pivot(index=\'Species\', columns=\'Variable\', values=\'Measurement\') # 3. Create the heatmap with customizations ax = sns.heatmap(pivot_data, annot=True, fmt=\\".1f\\", cmap=\\"coolwarm\\", linewidth=0.5, vmin=0, vmax=100) ax.set(xlabel=\'\', ylabel=\'\') ax.xaxis.tick_top() # Display the plot plt.show() ``` Ensure your final plot meets all the requirements and runs correctly with the provided dataset.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def create_custom_heatmap(file_path): Function loads data from a CSV file, pivots the data, and creates a customized heatmap. Args: - file_path (str): Path to the CSV file containing the dataset Returns: - matplotlib.axes._subplots.AxesSubplot: Axes object with the heatmap # 1. Load the dataset data = pd.read_csv(file_path) # 2. Pivot the data pivot_data = data.pivot(index=\'Species\', columns=\'Variable\', values=\'Measurement\') # 3. Create the heatmap with customizations ax = sns.heatmap(pivot_data, annot=True, fmt=\\".1f\\", cmap=\\"coolwarm\\", linewidth=0.5, vmin=0, vmax=100) ax.set(xlabel=\'\', ylabel=\'\') ax.xaxis.tick_top() # Display the plot plt.show() return ax"},{"question":"**Problem Statement: Efficient I/O Multiplexing using the `select` module** You are asked to implement a function that monitors multiple file descriptors and handles I/O events efficiently. The function should operate on both Unix-like and Windows operating systems. It should prefer platform-optimized methods (`epoll` on Linux, `/dev/poll` on Solaris, `kqueue` on BSD), but fall back to the generic `select()` function if necessary. # Function Specification Function Name `io_multiplexing` Parameters - `read_fds`: A list of file descriptors or objects with a `fileno()` method to be monitored for reading readiness. - `write_fds`: A list of file descriptors or objects with a `fileno()` method to be monitored for writing readiness. - `except_fds`: A list of file descriptors or objects with a `fileno()` method to be monitored for exceptional conditions. - `timeout`: An optional floating-point number specifying the maximum wait time in seconds. If omitted or None, the function should block indefinitely. Return A tuple `(read_list, write_list, except_list)`, where each list contains the file descriptors ready for the corresponding I/O operation. # Constraints - The function must handle edge cases where all input lists are empty. - The implementation should be efficient and preferably use platform-specific methods where applicable. - The `timeout` value should be handled accurately, including being recomputed if the call is interrupted by a signal. # Example Usage ```python def sample_usage(): import socket # Create sample file descriptors server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM) server_socket.bind((\'localhost\', 0)) server_socket.listen(1) client_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM) client_socket.connect(server_socket.getsockname()) read_fds = [server_socket, client_socket] write_fds = [] except_fds = [] timeout = 5.0 read_ready, write_ready, except_ready = io_multiplexing(read_fds, write_fds, except_fds, timeout) print(f\\"Ready for reading: {read_ready}\\") print(f\\"Ready for writing: {write_ready}\\") print(f\\"Exceptional conditions: {except_ready}\\") # Ensure your function handles the above example properly. # Additional Notes - You may utilize various platform-specific techniques (`epoll`, `poll`, `kqueue`) as needed. - Ensure your solution is well-documented and handles exceptions gracefully. ``` # Implementation Requirements - You may use the `select` module and any other standard library modules required. - Ensure compatibility with both Unix-like systems and Windows. Good luck, and happy coding!","solution":"import select import sys def io_multiplexing(read_fds, write_fds, except_fds, timeout=None): Monitors multiple file descriptors for specific IO events using the most efficient method available. Parameters: - read_fds: list of file descriptors or objects with fileno() method for read readiness. - write_fds: list of file descriptors or objects with fileno() method for write readiness. - except_fds: list of file descriptors or objects with fileno() method for exceptional conditions. - timeout: maximum wait time in seconds (float). Blocks indefinitely if None. Returns: A tuple (read_list, write_list, except_list) of ready file descriptors. # Convert objects with fileno() method to file descriptors read_fds = [fd.fileno() if hasattr(fd, \'fileno\') else fd for fd in read_fds] write_fds = [fd.fileno() if hasattr(fd, \'fileno\') else fd for fd in write_fds] except_fds = [fd.fileno() if hasattr(fd, \'fileno\') else fd for fd in except_fds] if sys.platform.startswith(\'linux\'): # Use epoll on Linux if available try: epoll = select.epoll() for fd in read_fds: epoll.register(fd, select.EPOLLIN) for fd in write_fds: epoll.register(fd, select.EPOLLOUT) for fd in except_fds: epoll.register(fd, select.EPOLLERR) events = epoll.poll(timeout) read_ready = [fd for fd, event in events if event & select.EPOLLIN] write_ready = [fd for fd, event in events if event & select.EPOLLOUT] except_ready = [fd for fd, event in events if event & select.EPOLLERR] return read_ready, write_ready, except_ready except Exception: pass if sys.platform.startswith(\'sunos\'): # Use /dev/poll on Solaris if available try: import devpoll dp = devpoll.devpoll() for fd in read_fds: dp.register(fd, select.POLLIN) for fd in write_fds: dp.register(fd, select.POLLOUT) for fd in except_fds: dp.register(fd, select.POLLERR) events = dp.poll(timeout) read_ready = [fd for fd, event in events if event & select.POLLIN] write_ready = [fd for fd, event in events if event & select.POLLOUT] except_ready = [fd for fd, event in events if event & select.POLLERR] return read_ready, write_ready, except_ready except Exception: pass if sys.platform.startswith(\'darwin\') or \'bsd\' in sys.platform: # Use kqueue on BSD if available try: kq = select.kqueue() kevents = [] for fd in read_fds: kevents.append(select.kevent(fd, filter=select.KQ_FILTER_READ, flags=select.KQ_EV_ADD)) for fd in write_fds: kevents.append(select.kevent(fd, filter=select.KQ_FILTER_WRITE, flags=select.KQ_EV_ADD)) for fd in except_fds: kevents.append(select.kevent(fd, filter=select.KQ_FILTER_READ, flags=select.KQ_EV_ADD | select.KQ_EV_ERROR)) events = kq.control(kevents, len(kevents), timeout) read_ready = [kev.ident for kev in events if kev.filter == select.KQ_FILTER_READ] write_ready = [kev.ident for kev in events if kev.filter == select.KQ_FILTER_WRITE] except_ready = [kev.ident for kev in events if kev.flags & select.KQ_EV_ERROR] return read_ready, write_ready, except_ready except Exception: pass # Default fallback to select method read_ready, write_ready, except_ready = select.select(read_fds, write_fds, except_fds, timeout) return read_ready, write_ready, except_ready"},{"question":"You are required to create a Python script that handles HTTP cookies using the `http.cookiejar` module. Your script should include the following functionalities: 1. **Load Cookies**: Load cookies from a file in the Mozilla cookies.txt format. 2. **Access Website**: Use the loaded cookies to access a specified URL. 3. **Extract New Cookies**: Extract any new cookies set by the server during the visit to the URL. 4. **Filter Cookies**: Define a custom cookie policy that blocks cookies from specified domains and allowlist only specified domains. 5. **Save Cookies**: Save the updated cookies back to the file while ignoring session cookies. # Details 1. Load Cookies - Use a file path for cookies in the Mozilla cookies.txt format (can be path passed as an argument). 2. Access Website - Use the loaded cookies to access a URL (passed as an argument). 3. Extract New Cookies - Extract any new cookies that the server sends in the response and store them in your CookieJar. 4. Filter Cookies - Define a `DefaultCookiePolicy` where: - RFC 2965 cookies should be handled. - Custom domains to block should be provided as a list. - Only specific allowed domains should be allowed to set or get cookies. 5. Save Cookies - Save the updated cookies back to the file in Mozilla cookies.txt format, ignoring any session cookies. # Function Signature ```python def handle_cookies( cookie_file: str, url: str, blocked_domains: list, allowed_domains: list ) -> None: pass ``` # Input - `cookie_file` (string): The file path to the Mozilla cookies.txt file. - `url` (string): The URL to access. - `blocked_domains` (list): List of domain names to block cookies. - `allowed_domains` (list): List of domain names that are allowed to set and retrieve cookies. # Constraints - Ensure that your script can handle unexpected errors gracefully. - The extracted and saved cookies must comply with the policy set. # Example ```python handle_cookies( cookie_file=\\"./cookies.txt\\", url=\\"http://example.com/\\", blocked_domains=[\\"ads.net\\", \\".ads.net\\"], allowed_domains=[\\"example.com\\"] ) ``` This will load cookies from `./cookies.txt`, access `http://example.com/`, extract any new cookies, filter them according to the blocked and allowed domains, and save the updated cookies back to `./cookies.txt` ignoring session cookies. # Notes - Make use of `MozillaCookieJar` for loading and saving cookies. - Utilize `urllib.request` to open the URL with cookie handling. - Implement a custom `DefaultCookiePolicy` for filtering cookies. - Ensure compliance with both Netscape and RFC 2965 cookie protocols as needed.","solution":"import http.cookiejar import urllib.request def handle_cookies(cookie_file: str, url: str, blocked_domains: list, allowed_domains: list) -> None: # Load the cookies from file cookie_jar = http.cookiejar.MozillaCookieJar() cookie_jar.load(cookie_file, ignore_discard=True, ignore_expires=True) # Custom cookie policy class CustomCookiePolicy(http.cookiejar.DefaultCookiePolicy): def set_ok(self, cookie, request): if any(cookie.domain.endswith(blocked) for blocked in blocked_domains): return False if any(cookie.domain.endswith(allowed) for allowed in allowed_domains): return True return False def return_ok(self, cookie, request): return self.set_ok(cookie, request) # Assign the custom cookie policy to the cookie jar cookie_jar.set_policy(CustomCookiePolicy(rfc2965=True)) # Build the opener with HTTP cookie processor opener = urllib.request.build_opener(urllib.request.HTTPCookieProcessor(cookie_jar)) # Access the URL try: opener.open(url) except urllib.error.URLError as e: print(e) # Save the updated cookies back to the file, ignoring session cookies cookie_jar.save(cookie_file, ignore_discard=True, ignore_expires=True)"},{"question":"# Question: Advanced Data Visualization using Seaborn Point Plot Objective: Write a Python function using `seaborn` that: 1. Loads a specific dataset from seaborn. 2. Produces a detailed point plot utilizing multiple facets of the seaborn library as outlined below. 3. Saves the plot to a file. Function Signature: ```python def advanced_pointplot(save_file_path: str) -> None: pass ``` Requirements: 1. Load the \\"penguins\\" dataset using `sns.load_dataset`. 2. Create a point plot with the following specifications: - The x-axis should be \\"island\\". - The y-axis should be \\"body_mass_g\\". - Differentiate the plot by using the \\"sex\\" column for `hue`. - Use different markers and linestyles for each value in the \\"sex\\" column. - Represent the error bars using the standard deviation. - Customize the plot to have a gray color (`color=\\".5\\"`), and use a diamond marker (`marker=\\"D\\"`). - Set a cap size for error bars to be 0.4. 3. The plot should be clear and presentable, with appropriate titles and axis labels. 4. Save the generated plot to the file path given by `save_file_path`. Input: - `save_file_path` (str): A string representing the full path (including the filename) where the plot should be saved. Output: - The function should save the plot to the specified file. No return value is necessary. Example: ```python advanced_pointplot(\\"/path/to/save/advanced_plot.png\\") ``` Constraints: - Make sure all necessary libraries (pandas, seaborn, matplotlib) are imported and used appropriately. - Handle exceptions that may arise from loading the dataset or saving the file.","solution":"import seaborn as sns import matplotlib.pyplot as plt def advanced_pointplot(save_file_path: str) -> None: # Load the \\"penguins\\" dataset try: penguins = sns.load_dataset(\\"penguins\\") except Exception as e: print(f\\"Error loading dataset: {e}\\") return # Plot the data with the required specifications try: plt.figure(figsize=(10, 6)) sns.pointplot( data=penguins, x=\\"island\\", y=\\"body_mass_g\\", hue=\\"sex\\", markers=[\\"o\\", \\"D\\"], linestyles=[\\"-\\", \\"--\\"], errorbar=\\"sd\\", capsize=0.4, color=\'.5\' ) plt.title(\\"Penguin Body Mass by Island and Sex\\") plt.xlabel(\\"Island\\") plt.ylabel(\\"Body Mass (g)\\") plt.legend(title=\\"Sex\\") # Save the plot to the specified file path plt.savefig(save_file_path) plt.close() except Exception as e: print(f\\"Error creating or saving the plot: {e}\\")"},{"question":"**Objective**: Use the `shlex` module to write a function that processes a series of shell-like commands and extracts specific information from them. This will test students\' ability to handle lexical analysis and understand the functionality provided by the `shlex` library. **Problem**: Write a Python function `extract_shell_commands(script: str) -> dict` that takes a single string input representing a script composed of multiple shell-like commands. Your function should return a dictionary where keys are command names and values are lists of arguments for each command. **Function Signature**: ```python def extract_shell_commands(script: str) -> dict: ``` # Input - `script` (str): A string containing one or more shell commands. Commands are separated by a semicolon (`;`). Each command can have multiple arguments separated by spaces. # Output - A dictionary where each key is a command name and the corresponding value is a list of arguments passed to that command. # Constraints 1. Commands do not contain nested commands. 2. Commands can have quoted strings which should be treated as single arguments. 3. Handle both POSIX and non-POSIX modes for splitting the commands. 4. You should handle common shell punctuation and escaping characters. # Example ```python script = \\"echo \'Hello, World!\'; mkdir new_folder; rm -rf old_folder\\" result = extract_shell_commands(script) print(result) ``` Expected Output: ```python { \'echo\': [\'Hello, World!\'], \'mkdir\': [\'new_folder\'], \'rm\': [\'-rf\', \'old_folder\'] } ``` # Notes - You will need to use `shlex.split` for splitting the script into individual commands. - Use `shlex.shlex` to handle each command and extract the arguments. - Ensure that the function works correctly in both POSIX and non-POSIX modes by experimenting with the `posix` parameter. **Performance Requirements**: - The function should run efficiently even for larger scripts containing multiple commands with numerous arguments. - Handle edge cases such as empty commands or commands with excessive spacing gracefully.","solution":"import shlex def extract_shell_commands(script: str) -> dict: Extracts shell commands and their arguments from a script string. Parameters: script (str): A string containing shell commands separated by semicolons. Returns: dict: A dictionary where keys are command names and the corresponding values are lists of arguments. # Split the script into individual commands based on semicolon commands = script.split(\';\') result = {} for command in commands: command = command.strip() if command: # Use shlex to split the command into parts safely parts = shlex.split(command) cmd_name = parts[0] args = parts[1:] result[cmd_name] = args return result"},{"question":"# Seaborn Coding Assessment Question **Objective:** You are given a dataset of `brain_networks`, and you need to perform the following tasks: 1. Load and preprocess the dataset. 2. Create a custom pair plot using seaborn\'s `objects` module. **Dataset Description:** - Name: `brain_networks` - It contains columns that represent different brain networks, timepoints, and hemispheres. **Tasks:** 1. **Load the Dataset**: - Use `seaborn.load_dataset` to load the `brain_networks` dataset. - Perform the following modifications: - Rename the axis to \\"timepoint\\". - Stack the dataset on the first three levels of columns. - Group the dataset by `timepoint`, `network`, and `hemi`. - Calculate the mean values for the grouped data. - Unstack the dataset by `network`. - Reset the index. - Filter the rows to keep only the data where `timepoint < 100`. 2. **Create and Customize the Plot:** - Utilize `so.Plot` to set up the plot. - Use the `pair` method to create pair plots for Time Points [5, 8, 12, 15] vs. Networks [6, 13, 16]. - Use the `layout` method to set the subplot size to (10, 6). - Ensure the `x` and `y` axes are shared among subplots. - Add `Paths` to the plot with a `linewidth` of 2, an `alpha` of 0.7, and colored by `hemi`. **Constraints:** - You are not allowed to use plotting functions outside the seaborn `objects` module. - The dataset should be preprocessed as described before plotting. **Expected Input:** ```python # No input parameters needed as the dataset can be loaded without arguments. ``` **Expected Output:** - A matplotlib figure displaying the specified pair plot. **Code:** ```python import seaborn.objects as so from seaborn import load_dataset # Step 1: Load and preprocess the dataset networks = ( load_dataset(\\"brain_networks\\", header=[0, 1, 2], index_col=0) .rename_axis(\\"timepoint\\") .stack([0, 1, 2]) .groupby([\\"timepoint\\", \\"network\\", \\"hemi\\"]) .mean() .unstack(\\"network\\") .reset_index() .query(\\"timepoint < 100\\") ) # Step 2: Create and customize the plot p = ( so.Plot(networks) .pair(x=[\\"5\\", \\"8\\", \\"12\\", \\"15\\"], y=[\\"6\\", \\"13\\", \\"16\\"]) .layout(size=(10, 6)) .share(x=True, y=True) .add(so.Paths(linewidth=2, alpha=0.7), color=\\"hemi\\") ) # Render the plot p.show() ```","solution":"import seaborn.objects as so from seaborn import load_dataset def preprocess_and_plot_brain_networks(): # Step 1: Load and preprocess the dataset networks = ( load_dataset(\\"brain_networks\\", header=[0, 1, 2], index_col=0) .rename_axis(\\"timepoint\\") .stack([0, 1, 2]) .groupby([\\"timepoint\\", \\"network\\", \\"hemi\\"]) .mean() .unstack(\\"network\\") .reset_index() .query(\\"timepoint < 100\\") ) # Step 2: Create and customize the plot p = ( so.Plot(networks) .pair(x=[\\"5\\", \\"8\\", \\"12\\", \\"15\\"], y=[\\"6\\", \\"13\\", \\"16\\"]) .layout(size=(10, 6)) .share(x=True, y=True) .add(so.Paths(linewidth=2, alpha=0.7), color=\\"hemi\\") ) return p # Calling the function to render the plot plot = preprocess_and_plot_brain_networks() plot.show()"},{"question":"Custom Warning Handling and Filtering Objective: Implement a function that generates various types of warnings, applies custom filters to them, and handles the warnings in specified ways. Task: 1. **Create a function `custom_warning_emitter(inputs)`** that: - Takes a list of tuples as input where each tuple contains: 1. Warning message (str) 2. Warning category (str, one of: \'UserWarning\', \'DeprecationWarning\', \'SyntaxWarning\', \'RuntimeWarning\', \'FutureWarning\') 3. Line number (int) - Emits a warning for each tuple using the `warn()` function based on the provided message, category, and stack level corresponding to the line number. 2. **Create a function `apply_warning_filter(action, category)`** that: - Takes two parameters: 1. Action (str, one of: \'default\', \'error\', \'ignore\', \'always\', \'module\', \'once\') 2. Category (str, one of the categories defined above) - Applies a filter with the given action to warnings of the given category using `filterwarnings()`. 3. **Create a function `handle_warnings(inputs, action, category)`** that: - Takes the same list of inputs as `custom_warning_emitter` and the same `action` and `category` as `apply_warning_filter`. - Calls `custom_warning_emitter(inputs)` to emit the warnings. - Calls `apply_warning_filter(action, category)` to apply the filter. - Uses the `catch_warnings` context manager to capture and report the number of warnings emitted, the types of warnings, and the specific messages. Constraints: - `inputs` is guaranteed to be a list of tuples, each containing: - A string message between 1 and 100 characters. - A valid warning category as specified. - An integer line number greater than 0. - `action` will always be one of the valid actions. - `category` will always be one of the valid categories. Output: - `handle_warnings(inputs, action, category)` should return a list of dictionaries, where each dictionary contains: - `message`: The warning message. - `category`: The category of the warning. - `line number`: The line number where the warning was generated. Example: ```python inputs = [ (\\"Outdated function call\\", \\"DeprecationWarning\\", 10), (\\"Possible syntax issue\\", \\"SyntaxWarning\\", 20), (\\"Potential runtime problem\\", \\"RuntimeWarning\\", 30) ] action = \\"always\\" category = \\"SyntaxWarning\\" output = handle_warnings(inputs, action, category) # Example Output: # [ # {\\"message\\": \\"Outdated function call\\", \\"category\\": \\"DeprecationWarning\\", \\"line number\\": 10}, # {\\"message\\": \\"Possible syntax issue\\", \\"category\\": \\"SyntaxWarning\\", \\"line number\\": 20}, # {\\"message\\": \\"Potential runtime problem\\", \\"category\\": \\"RuntimeWarning\\", \\"line number\\": 30} # ] ``` Note: The example output may vary based on the `action` and `category` filter applied, and the context in which warnings are emitted and captured. Solution Template: ```python import warnings def custom_warning_emitter(inputs): # Implement this function pass def apply_warning_filter(action, category): # Implement this function pass def handle_warnings(inputs, action, category): # Implement this function pass ```","solution":"import warnings def custom_warning_emitter(inputs): warning_category_map = { \'UserWarning\': UserWarning, \'DeprecationWarning\': DeprecationWarning, \'SyntaxWarning\': SyntaxWarning, \'RuntimeWarning\': RuntimeWarning, \'FutureWarning\': FutureWarning } for message, category, lineno in inputs: warning_category = warning_category_map.get(category) if warning_category: warnings.warn_explicit(message, warning_category, __file__, lineno) def apply_warning_filter(action, category): warning_category_map = { \'UserWarning\': UserWarning, \'DeprecationWarning\': DeprecationWarning, \'SyntaxWarning\': SyntaxWarning, \'RuntimeWarning\': RuntimeWarning, \'FutureWarning\': FutureWarning } warning_category = warning_category_map.get(category) if warning_category: warnings.filterwarnings(action, category=warning_category) def handle_warnings(inputs, action, category): apply_warning_filter(action, category) with warnings.catch_warnings(record=True) as caught_warnings: warnings.simplefilter(action) custom_warning_emitter(inputs) results = [] for warning in caught_warnings: result = { \'message\': str(warning.message), \'category\': warning.category.__name__, \'line number\': warning.lineno } results.append(result) return results"},{"question":"# Linear and Quadratic Discriminant Analysis Implementation and Comparison **Task**: Implement functions to use Linear Discriminant Analysis (LDA) for both classification and dimensionality reduction, and Quadratic Discriminant Analysis (QDA) for classification. Compare their performance on a synthetic dataset. **Part 1**: Implementing LDA for Classification 1. Write a function `fit_lda(X_train, y_train)` that fits an LDA model to the training data. 2. Write a function `predict_lda(X_test, lda_model)` that uses the fitted LDA model to predict labels for the test data. **Part 2**: Implementing LDA for Dimensionality Reduction 1. Write a function `transform_lda(X, lda_model, n_components)` that reduces the dimensionality of the input data to `n_components` using the fitted LDA model. **Part 3**: Implementing QDA for Classification 1. Write a function `fit_qda(X_train, y_train)` that fits a QDA model to the training data. 2. Write a function `predict_qda(X_test, qda_model)` that uses the fitted QDA model to predict labels for the test data. **Part 4**: Comparing Performance 1. Generate a synthetic dataset with three classes and 10 features using `make_classification` from `sklearn.datasets`. 2. Split the dataset into training and testing sets. 3. Fit both LDA and QDA models to the training data. 4. Predict the labels for the test data using both models. 5. Evaluate and compare the accuracy of LDA and QDA on the test data. 6. Using LDA, reduce the dimensionality of the dataset to 2 components and visualize the decision boundaries by plotting the reduced data. **Expected Input and Output Formats**: 1. **fit_lda(X_train, y_train)** * Input: `X_train` (numpy.ndarray of shape `(n_samples, n_features)`), `y_train` (numpy.ndarray of shape `(n_samples,)`) * Output: Fitted LDA model 2. **predict_lda(X_test, lda_model)** * Input: `X_test` (numpy.ndarray of shape `(n_samples, n_features)`), `lda_model` (fitted LDA model) * Output: Predictions (numpy.ndarray of shape `(n_samples,)`) 3. **transform_lda(X, lda_model, n_components)** * Input: `X` (numpy.ndarray of shape `(n_samples, n_features)`), `lda_model` (fitted LDA model), `n_components` (int) * Output: Transformed data (numpy.ndarray of shape `(n_samples, n_components)`) 4. **fit_qda(X_train, y_train)** * Input: `X_train` (numpy.ndarray of shape `(n_samples, n_features)`), `y_train` (numpy.ndarray of shape `(n_samples,)`) * Output: Fitted QDA model 5. **predict_qda(X_test, qda_model)** * Input: `X_test` (numpy.ndarray of shape `(n_samples, n_features)`), `qda_model` (fitted QDA model) * Output: Predictions (numpy.ndarray of shape `(n_samples,)`) 6. **Comparison and Visualization** * Use print statements to display the accuracy of each model. * Use matplotlib to visualize the decision boundaries of LDA on the reduced 2-dimensional data. **Constraints**: - Ensure the synthetic dataset has three classes and 10 features. - Use a random state for reproducibility. - Limit the visualization to 2D for simplicity. **Performance Requirements**: - Evaluate the accuracy of the models using appropriate metrics. - The visualization should clearly distinguish the decision boundaries of the reduced dataset.","solution":"import numpy as np from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis from sklearn.datasets import make_classification from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score import matplotlib.pyplot as plt def fit_lda(X_train, y_train): Fits an LDA model to the training data. Parameters: - X_train: numpy.ndarray of shape (n_samples, n_features) - y_train: numpy.ndarray of shape (n_samples,) Returns: - lda_model: Fitted LDA model lda_model = LinearDiscriminantAnalysis() lda_model.fit(X_train, y_train) return lda_model def predict_lda(X_test, lda_model): Predicts labels for the test data using the fitted LDA model. Parameters: - X_test: numpy.ndarray of shape (n_samples, n_features) - lda_model: Fitted LDA model Returns: - Predicted labels: numpy.ndarray of shape (n_samples,) return lda_model.predict(X_test) def transform_lda(X, lda_model, n_components): Reduces the dimensionality of the input data using LDA. Parameters: - X: numpy.ndarray of shape (n_samples, n_features) - lda_model: Fitted LDA model - n_components: int, number of components to keep Returns: - Transformed data: numpy.ndarray of shape (n_samples, n_components) return lda_model.transform(X)[:, :n_components] def fit_qda(X_train, y_train): Fits a QDA model to the training data. Parameters: - X_train: numpy.ndarray of shape (n_samples, n_features) - y_train: numpy.ndarray of shape (n_samples,) Returns: - qda_model: Fitted QDA model qda_model = QuadraticDiscriminantAnalysis() qda_model.fit(X_train, y_train) return qda_model def predict_qda(X_test, qda_model): Predicts labels for the test data using the fitted QDA model. Parameters: - X_test: numpy.ndarray of shape (n_samples, n_features) - qda_model: Fitted QDA model Returns: - Predicted labels: numpy.ndarray of shape (n_samples,) return qda_model.predict(X_test) # Generate synthetic dataset X, y = make_classification(n_samples=300, n_features=10, n_informative=5, n_redundant=2, n_classes=3, random_state=1) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1) # Fit LDA model lda_model = fit_lda(X_train, y_train) y_pred_lda = predict_lda(X_test, lda_model) accuracy_lda = accuracy_score(y_test, y_pred_lda) # Fit QDA model qda_model = fit_qda(X_train, y_train) y_pred_qda = predict_qda(X_test, qda_model) accuracy_qda = accuracy_score(y_test, y_pred_qda) print(f\\"LDA Accuracy: {accuracy_lda}\\") print(f\\"QDA Accuracy: {accuracy_qda}\\") # Transform and visualize LDA X_reduced = transform_lda(X, lda_model, 2) plt.figure(figsize=(10, 7)) colors = [\'r\', \'g\', \'b\'] for idx, class_label in enumerate(np.unique(y)): plt.scatter(X_reduced[y == class_label, 0], X_reduced[y == class_label, 1], color=colors[idx], label=f\\"Class {class_label}\\") plt.legend() plt.xlabel(\\"Component 1\\") plt.ylabel(\\"Component 2\\") plt.title(\\"LDA Reduced Data to 2 Components\\") plt.show()"},{"question":"**Kernel Approximation Using Scikit-learn** You are given a dataset consisting of feature vectors and corresponding binary class labels. Your task is to implement two different kernel approximation techniques using scikit-learn and compare their performance on the given dataset. Specifically, you should use the Nystroem method and the RBFSampler for kernel approximation, fit a linear Support Vector Machine (SVM) classifier on the transformed data, and calculate the accuracy for both methods. # Given: - `X_train`: A list of lists, where each sub-list represents the feature vector of a training sample. - `y_train`: A list of binary labels corresponding to the training samples. - `X_test`: A list of lists, where each sub-list represents the feature vector of a test sample. - `y_test`: A list of binary labels corresponding to the test samples. # Output: - Return a tuple containing two accuracy scores: (accuracy_nystroem, accuracy_rbf), where: - `accuracy_nystroem` is the accuracy of the linear SVM classifier using the Nystroem approximation. - `accuracy_rbf` is the accuracy of the linear SVM classifier using the RBFSampler approximation. # Function Signature: ```python from typing import List, Tuple import numpy as np def kernel_approximation_performance( X_train: List[List[float]], y_train: List[int], X_test: List[List[float]], y_test: List[int] ) -> Tuple[float, float]: # Your code here ``` # Example: ```python # Sample data X_train = [[0, 0], [1, 1], [1, 0], [0, 1]] y_train = [0, 0, 1, 1] X_test = [[0.5, 0.5], [1, 0], [0, 1]] y_test = [0, 1, 1] accuracy_nystroem, accuracy_rbf = kernel_approximation_performance(X_train, y_train, X_test, y_test) print(f\\"Nystroem Accuracy: {accuracy_nystroem}\\") print(f\\"RBF Accuracy: {accuracy_rbf}\\") ``` # Instructions: 1. Implement the Nystroem method for kernel approximation and fit a linear SVM classifier on the transformed training data. Evaluate the accuracy on the transformed test data. 2. Implement the RBFSampler method for kernel approximation and fit a linear SVM classifier on the transformed training data. Evaluate the accuracy on the transformed test data. 3. Compare and return the accuracy scores for both methods. # Constraints: - Use `n_components=100` for both `Nystroem` and `RBFSampler`. - Set `gamma=1` for `RBFSampler`. - You may initialize the random state to ensure reproducibility. # Libraries to Use: - `sklearn.kernel_approximation.Nystroem` - `sklearn.kernel_approximation.RBFSampler` - `sklearn.linear_model.SGDClassifier` - `sklearn.metrics.accuracy_score` Use the provided documentation and knowledge of scikit-learn functions to complete the task.","solution":"from typing import List, Tuple from sklearn.kernel_approximation import Nystroem, RBFSampler from sklearn.linear_model import SGDClassifier from sklearn.metrics import accuracy_score import numpy as np def kernel_approximation_performance( X_train: List[List[float]], y_train: List[int], X_test: List[List[float]], y_test: List[int] ) -> Tuple[float, float]: # Convert lists to numpy arrays for sklearn compatibility X_train = np.array(X_train) y_train = np.array(y_train) X_test = np.array(X_test) y_test = np.array(y_test) # Nystroem method nystroem = Nystroem(n_components=100, random_state=42) X_train_nystroem = nystroem.fit_transform(X_train) X_test_nystroem = nystroem.transform(X_test) clf_nystroem = SGDClassifier(random_state=42) clf_nystroem.fit(X_train_nystroem, y_train) y_pred_nystroem = clf_nystroem.predict(X_test_nystroem) accuracy_nystroem = accuracy_score(y_test, y_pred_nystroem) # RBFSampler method rbf_sampler = RBFSampler(gamma=1, n_components=100, random_state=42) X_train_rbf = rbf_sampler.fit_transform(X_train) X_test_rbf = rbf_sampler.transform(X_test) clf_rbf = SGDClassifier(random_state=42) clf_rbf.fit(X_train_rbf, y_train) y_pred_rbf = clf_rbf.predict(X_test_rbf) accuracy_rbf = accuracy_score(y_test, y_pred_rbf) return accuracy_nystroem, accuracy_rbf"},{"question":"You are tasked with creating a secure login system for a Unix-based application. Users\' passwords should be stored in a hashed format to ensure security. Implement the following functions: 1. **hash_password(plaintext: str, method: Optional[str] = None, rounds: Optional[int] = None) -> str:** - Takes a plaintext password and optionally a hashing method and number of rounds. - Hashes the password using the strongest available method by default or a specified method. - Returns the hashed password. 2. **verify_password(plaintext: str, hashed_password: str) -> bool:** - Takes a plaintext password and a hashed password. - Checks if the plaintext password, when hashed, matches the hashed password. - Returns `True` if the passwords match, `False` otherwise. Input: - `hash_password`: - `plaintext`: A string representing the user\'s password (1 ≤ len(plaintext) ≤ 100). - `method`: A string specifying the hashing method to use (e.g., `\\"SHA512\\"`, `\\"SHA256\\"`, `\\"BLOWFISH\\"`, `\\"MD5\\"`, `\\"CRYPT\\"`) or `None` to use the strongest method available. - `rounds`: An integer specifying the number of rounds for SHA256, SHA512, and Blowfish, or `None` for default rounds. - `verify_password`: - `plaintext`: A string representing the user\'s password (1 ≤ len(plaintext) ≤ 100). - `hashed_password`: A string representing the hashed password. Output: - `hash_password`: Returns a string representing the hashed password. - `verify_password`: Returns `True` if the hashed password matches the plaintext password, `False` otherwise. Example: ```python hashed_pw = hash_password(\\"mysecretpassword\\", method=\\"SHA512\\", rounds=10000) print(verify_password(\\"mysecretpassword\\", hashed_pw)) # Output: True print(verify_password(\\"wrongpassword\\", hashed_pw)) # Output: False ``` Constraints: - Ensure you use the `crypt` module. - Handle any possible exceptions that may occur while hashing or verifying passwords. Performance: - The hashing and verification functions should be efficient, considering that password hashing involves computational overhead for security. # Implementation: ```python import crypt from typing import Optional from hmac import compare_digest as compare_hash def hash_password(plaintext: str, method: Optional[str] = None, rounds: Optional[int] = None) -> str: # Determine the method to use for hashing if method: method_attr = getattr(crypt, f\'METHOD_{method.upper()}\', None) if method_attr is None: raise ValueError(f\\"Invalid method: {method}\\") else: method_attr = None # Create a salt using the specified method and rounds salt = crypt.mksalt(method=method_attr, rounds=rounds) # Hash the plaintext password hashed_password = crypt.crypt(plaintext, salt) return hashed_password def verify_password(plaintext: str, hashed_password: str) -> bool: # Hash the plaintext password using the hashed_password as salt rehashed_password = crypt.crypt(plaintext, hashed_password) # Compare the rehashed password with the given hashed password return compare_hash(rehashed_password, hashed_password) # Example Usage hashed_pw = hash_password(\\"mysecretpassword\\", method=\\"SHA512\\", rounds=10000) print(verify_password(\\"mysecretpassword\\", hashed_pw)) # Output: True print(verify_password(\\"wrongpassword\\", hashed_pw)) # Output: False ``` # Notes: - You may assume that all necessary methods (`METHOD_SHA512`, `METHOD_SHA256`, etc.) are available on the platform where the code runs.","solution":"import crypt import hmac from typing import Optional def hash_password(plaintext: str, method: Optional[str] = None, rounds: Optional[int] = None) -> str: Hashes a plaintext password using the specified method and number of rounds. Returns the hashed password. if method: method_attr = getattr(crypt, f\'METHOD_{method.upper()}\', None) if method_attr is None: raise ValueError(f\\"Invalid method: {method}\\") else: method_attr = crypt.METHOD_SHA512 # Default to the strongest method available # Create a salt using the specified method and rounds salt = crypt.mksalt(method=method_attr, rounds=rounds) # Hash the plaintext password hashed_password = crypt.crypt(plaintext, salt) return hashed_password def verify_password(plaintext: str, hashed_password: str) -> bool: Verifies if a plaintext password matches the hashed password. Returns `True` if they match, `False` otherwise. # Hash the plaintext password using the hashed_password as salt rehashed_password = crypt.crypt(plaintext, hashed_password) # Compare the rehashed password with the given hashed password return hmac.compare_digest(rehashed_password, hashed_password)"},{"question":"# Function Tracing and Monitoring in Python Problem Statement You are required to implement a Python function decorator that can trace the sequence of function calls in a script. The decorator should log the entry and exit of each decorated function along with their arguments and the time taken for execution. Additionally, you should implement a simple log summarizer that outputs the sequence of function calls along with how many times each function was called. Requirements 1. **Function Decorator Implementation:** - The decorator should be able to decorate any Python function. - It should log the function entry, arguments received (both positional and keyword arguments), and time taken for execution upon exit. - Output should be printed to the console in a human-readable format. 2. **Log Summarizer Implementation:** - This part of the task involves summarizing the logs by counting the number of times each function was called. - Summarized logs should be outputted to the console as a dictionary where the key is the function name and the value is the number of times it was called. Constraints: - Function names in the log summation should be sorted in alphabetical order. - The maximum length of each argument (when stringified) should not exceed 100 characters. # Input & Output Example **Example Script to be Decorated:** ```python @trace_decorator def foo(a, b): return a + b @trace_decorator def bar(*args, **kwargs): return sum(args) @trace_decorator def baz(x): return x*x result1 = foo(3, 4) result2 = bar(1, 2, 3, x=4) result3 = baz(5) result4 = foo(10, 20) summarize_logs() ``` **Expected Console Output:** ```plaintext Entering foo(a=3, b=4) Exiting foo with result=7 in 0.00012 sec Entering bar(args=(1, 2, 3), kwargs={\'x\': 4}) Exiting bar with result=6 in 0.00015 sec Entering baz(x=5) Exiting baz with result=25 in 0.00010 sec Entering foo(a=10, b=20) Exiting foo with result=30 in 0.00011 sec Summary: {\'bar\': 1, \'baz\': 1, \'foo\': 2} ``` Implementation Details - **trace_decorator function:** - Implement the tracing decorator in such a way that it captures the function name, arguments, and execution time. - The decorator should log entries and exits of the functions to the console during their invocation. - **summarize_logs function:** - Implement a function to summarize the logs and display how many times each decorated function was called. Technical Constraints: - You may use the `functools` module to implement the decorator. - The `time` module can be utilized to measure the execution time. - Consider using global data structures to store logs and counts. Good luck!","solution":"import functools import time # Global dictionary to keep track of function call counts log_counts = {} def trace_decorator(func): @functools.wraps(func) def wrapper(*args, **kwargs): start_time = time.time() func_name = func.__name__ # Logging function entry formatted_args = \\", \\".join([f\\"{a}\\"[:100] for a in args]) formatted_kwargs = \\", \\".join([f\\"{k}={v}\\"[:100] for k, v in kwargs.items()]) print(f\\"Entering {func_name} ({formatted_args}, {formatted_kwargs})\\") # Call the actual function result = func(*args, **kwargs) # Logging function exit end_time = time.time() execution_time = end_time - start_time print(f\\"Exiting {func_name} with result={result} in {execution_time:.5f} sec\\") # Update the log count for the function if func_name in log_counts: log_counts[func_name] += 1 else: log_counts[func_name] = 1 return result return wrapper def summarize_logs(): # Sort logs by function names and print summary sorted_log_counts = dict(sorted(log_counts.items())) print(f\\"Summary: {sorted_log_counts}\\")"},{"question":"Objective Create a function to encode and decode files in a simplified binhex-like format. You will mimic the core functionalities of the deprecated `binhex` module in Python. Problem Statement You need to implement a `FileEncoderDecoder` class with the following methods: 1. `encode(self, input_file: str, output_file: str) -> None`: This method takes an input binary file and converts it to a simplified encoded text file. The output file should contain ASCII text representation of the binary content. 2. `decode(self, input_file: str, output_file: str) -> None`: This method takes an encoded text file and decodes it back to its original binary format. Encoding Format For simplification, every byte in the input binary file will be represented by two hexadecimal characters in the encoded file. Method Details - `encode(input_file, output_file)`: Reads bytes from `input_file`, converts each byte to its hexadecimal representation, and writes the hex string into `output_file`. - `decode(input_file, output_file)`: Reads the hex string from `input_file`, converts each pair of hex characters back to their binary representation, and writes the resulting bytes into `output_file`. Exception Handling - Raise an exception if the input file does not exist or cannot be read. - Raise an exception if the output file cannot be written. Constraints - The length of file names should not exceed 255 characters. - Ensure that the output file is closed properly even if an error occurs. Example Usage ```python # Create an instance of the class file_handler = FileEncoderDecoder() # Encode a binary file to a hex file file_handler.encode(\\"input_binaryfile.bin\\", \\"output_encodedfile.txt\\") # Decode the hex file back to a binary file file_handler.decode(\\"output_encodedfile.txt\\", \\"output_binaryfile.bin\\") ``` Input Format - The `input_file` and `output_file` parameters are strings representing the file paths. Output Format - Both functions return `None`. Performance Requirements - Ensure that your implementation can handle files up to 100 MB efficiently. Class Definition Template ```python class FileEncoderDecoder: def encode(self, input_file: str, output_file: str) -> None: # Implement the encoding logic here pass def decode(self, input_file: str, output_file: str) -> None: # Implement the decoding logic here pass ``` You are required to implement the `FileEncoderDecoder` class based on the provided specifications.","solution":"class FileEncoderDecoder: def encode(self, input_file: str, output_file: str) -> None: try: with open(input_file, \'rb\') as infile, open(output_file, \'w\') as outfile: while byte := infile.read(1): outfile.write(f\\"{byte.hex()}\\") except Exception as e: raise Exception(f\\"An error occurred during encoding: {str(e)}\\") def decode(self, input_file: str, output_file: str) -> None: try: with open(input_file, \'r\') as infile, open(output_file, \'wb\') as outfile: hexdata = infile.read() outfile.write(bytes.fromhex(hexdata)) except Exception as e: raise Exception(f\\"An error occurred during decoding: {str(e)}\\")"},{"question":"Objective The following coding task is designed to assess your understanding of the `sqlite3` module in Python, including creating and working with databases, handling transactions, and using advanced features such as custom aggregates and conversions. Task You are required to implement a Python function that interacts with an SQLite database to perform multiple operations. The task involves the following steps: 1. Create an SQLite database in memory and set up a table for storing sensor data. 2. Insert a given list of sensor readings (with timestamps) into the database. 3. Implement a custom SQL aggregate function to calculate the average sensor reading. 4. Retrieve the average sensor reading using this custom aggregate function. 5. Ensure that the entire operation is performed within a transaction and handle any exceptions that occur. Function Signature ```python def process_sensor_data(sensor_data: List[Tuple[str, float]]) -> float: pass ``` Input - `sensor_data`: A list of tuples containing sensor data. Each tuple consists of: - A timestamp string in the format `\'YYYY-MM-DD HH:MM:SS\'`. - A float value representing the sensor reading. Output - Returns the average sensor reading as a float. Constraints - Use parameterized queries to prevent SQL injection. - Handle any exceptions that might occur during database operations and ensure the database connection is properly closed. - The custom aggregate function should be implemented in Python. Example ```python sensor_data = [ (\\"2023-10-01 12:00:00\\", 23.5), (\\"2023-10-01 12:10:00\\", 25.1), (\\"2023-10-01 12:20:00\\", 22.8), (\\"2023-10-01 12:30:00\\", 24.0) ] average_reading = process_sensor_data(sensor_data) print(average_reading) # Output: 23.85 ``` Hints - Refer to the `create_aggregate` method for defining custom SQL aggregate functions. - Use the `with` statement for the `Connection` object to handle transactions gracefully. - Utilize placeholders (`?`) for binding Python values to SQL statements to avoid SQL injection. Notes - Ensure that you commit your transactions to save changes to the database. - Make sure to close the cursor and connection objects properly to avoid resource leaks.","solution":"import sqlite3 from typing import List, Tuple class Average: def __init__(self): self.sum = 0 self.count = 0 def step(self, value): if value is not None: self.sum += value self.count += 1 def finalize(self): if self.count == 0: return None return self.sum / self.count def process_sensor_data(sensor_data: List[Tuple[str, float]]) -> float: connection = sqlite3.connect(\\":memory:\\") try: cursor = connection.cursor() connection.create_aggregate(\\"my_avg\\", 1, Average) cursor.execute( CREATE TABLE sensor_data ( timestamp TEXT NOT NULL, reading REAL NOT NULL ) ) cursor.executemany(\\"INSERT INTO sensor_data (timestamp, reading) VALUES (?, ?)\\", sensor_data) cursor.execute(\\"SELECT my_avg(reading) FROM sensor_data\\") result = cursor.fetchone()[0] connection.commit() except sqlite3.Error as e: connection.rollback() raise e finally: cursor.close() connection.close() return result"},{"question":"**Question: Filesystem Analyzer** You are tasked with creating a program that interprets the statuses and types of file system entries within a given directory. Your program should recursively descend through all subdirectories and output detailed information regarding each file and directory encountered. You must make efficient use of the functions and constants provided by the `stat` module to achieve this. # Function Definition ```python def analyze_filesystem(root: str) -> None: Recursively descends through the filesystem starting at the specified root directory. Prints the type and permissions of each file or directory encountered. :param root: The root directory to start analyzing from. ``` # Requirements 1. **Input**: The function takes a single argument: - `root` (str): The root directory path as a string. 2. **Output**: The function should print the type and permissions of each file or directory in the following format: `\\"<type_string>: <permissions_string> <path>\\"`. - `<type_string>`: A string representing the type of file, based on `stat.S_IS*` functions (e.g., \\"Directory\\", \\"Regular File\\"). - `<permissions_string>`: The permissions of the file in the format \'-rwxrwxrwx\' obtained from `stat.filemode(mode)`. - `<path>`: The full path to the file or directory. 3. **Constraints and Considerations**: - The function should handle cases where file access permissions might not allow reading directories or files. - The analysis should efficiently avoid multiple `stat` system calls for the same file. - Your program should support symbolic links and be capable of identifying them separately. - Use exception handling to manage any errors that arise during file operations. 4. **Performance**: The function should be designed considering the potential impact of traversing large directory structures. # Example ```python import os import stat def analyze_filesystem(root: str) -> None: def walktree(top: str): try: for f in os.listdir(top): pathname = os.path.join(top, f) try: mode = os.lstat(pathname).st_mode except (OSError, PermissionError): print(f\\"Cannot access {pathname}\\") continue if stat.S_ISDIR(mode): print(f\\"Directory: {stat.filemode(mode)} {pathname}\\") walktree(pathname) elif stat.S_ISREG(mode): print(f\\"Regular File: {stat.filemode(mode)} {pathname}\\") elif stat.S_ISLNK(mode): print(f\\"Symbolic Link: {stat.filemode(mode)} {pathname}\\") else: print(f\\"Other: {stat.filemode(mode)} {pathname}\\") except (OSError, PermissionError): print(f\\"Cannot access {top}\\") walktree(root) # Sample usage # analyze_filesystem(\\"/path/to/directory\\") ``` - **Sample Output**: ``` Directory: drwxr-xr-x /path/to/directory Regular File: -rw-r--r-- /path/to/directory/file1.txt Symbolic Link: lrwxrwxrwx /path/to/directory/link Cannot access /path/to/directory/protected_dir ``` Use the `os.lstat` and corresponding `mode` functions from the `stat` module to verify file types and permissions correctly. Implement the code to ensure robustness and handle potential accessibility issues appropriately.","solution":"import os import stat def analyze_filesystem(root: str) -> None: def walktree(top: str): try: for f in os.listdir(top): pathname = os.path.join(top, f) try: mode = os.lstat(pathname).st_mode except (OSError, PermissionError): print(f\\"Cannot access {pathname}\\") continue if stat.S_ISDIR(mode): print(f\\"Directory: {stat.filemode(mode)} {pathname}\\") walktree(pathname) elif stat.S_ISREG(mode): print(f\\"Regular File: {stat.filemode(mode)} {pathname}\\") elif stat.S_ISLNK(mode): print(f\\"Symbolic Link: {stat.filemode(mode)} {pathname}\\") else: print(f\\"Other: {stat.filemode(mode)} {pathname}\\") except (OSError, PermissionError): print(f\\"Cannot access {top}\\") walktree(root)"},{"question":"# Advanced Coding Assessment: Serialization in PyTorch Objective: You are required to demonstrate your understanding of PyTorch\'s serialization and deserialization mechanisms by implementing a function to save and load a custom neural network model while addressing tensor view relationships and managing storage efficiency. Problem Statement: Implement a class `SerializableModel` which encapsulates a neural network and provides methods to serialize and deserialize the model along with its state dictionary and any associated metadata. This class should handle the following functionalities: 1. **Network Initialization**: - Initialize a simple neural network with one hidden layer. 2. **Serialization**: - Save the model\'s state along with metadata to a specified file such that the view relationships and storage efficiency are optimized. 3. **Deserialization**: - Load the model and its state from a specified file, ensuring that view relationships are preserved. Requirements: - **Inputs**: - For initialization: accepts input size, hidden layer size, and output size. - For serialization: accepts file path to save the model. - For deserialization: accepts file path to load the model. - **Output**: - For serialization: Writes the model and its state to the specified file. - For deserialization: Loads and returns an instance of the model with its state restored. - **Constraints**: - Ensure optimized storage usage by preventing unnecessary saving of large tensors when possible. - Preserve view relationships, ensuring any changes in loaded views reflect in the original tensor. Class API: ```python import torch import torch.nn as nn class SerializableModel: def __init__(self, input_size, hidden_size, output_size): Initialize the neural network model. Args: input_size (int): The size of the input layer. hidden_size (int): The size of the hidden layer. output_size (int): The size of the output layer. # Your implementation goes here pass def save_model(self, file_path): Save the model and its state to the specified file. Ensure optimized storage usage. Args: file_path (str): The path of the file where the model should be saved. # Your implementation goes here pass @staticmethod def load_model(file_path): Load the model and its state from the specified file. Args: file_path (str): The path of the file from where to load the model. Returns: SerializableModel: An instance of the model with the state restored. # Your implementation goes here pass ``` Example Usage: ```python if __name__ == \\"__main__\\": # Initialize the model model = SerializableModel(input_size=10, hidden_size=20, output_size=1) # Save the model model.save_model(\'model.pth\') # Load the model loaded_model = SerializableModel.load_model(\'model.pth\') # Verify if the loading was successful print(loaded_model) ``` Focus on implementing the `SerializableModel` class with the given methods, ensuring that tensor serialization is handled effectively, thereby preserving view relationships and optimizing storage usage.","solution":"import torch import torch.nn as nn class SerializableModel(nn.Module): def __init__(self, input_size, hidden_size, output_size): Initialize the neural network model. Args: input_size (int): The size of the input layer. hidden_size (int): The size of the hidden layer. output_size (int): The size of the output layer. super(SerializableModel, self).__init__() self.hidden = nn.Linear(input_size, hidden_size) self.output = nn.Linear(hidden_size, output_size) def forward(self, x): Define the forward pass. Args: x (Tensor): The input tensor. Returns: Tensor: The output tensor. x = torch.relu(self.hidden(x)) x = self.output(x) return x def save_model(self, file_path): Save the model and its state to the specified file. Ensure optimized storage usage. Args: file_path (str): The path of the file where the model should be saved. torch.save({ \'model_state_dict\': self.state_dict(), \'input_size\': self.hidden.in_features, \'hidden_size\': self.hidden.out_features, \'output_size\': self.output.out_features, }, file_path) @staticmethod def load_model(file_path): Load the model and its state from the specified file. Args: file_path (str): The path of the file from where to load the model. Returns: SerializableModel: An instance of the model with the state restored. checkpoint = torch.load(file_path) model = SerializableModel(checkpoint[\'input_size\'], checkpoint[\'hidden_size\'], checkpoint[\'output_size\']) model.load_state_dict(checkpoint[\'model_state_dict\']) return model"},{"question":"Custom SAX ContentHandler Implementation Objective The goal of this assessment is to implement a custom SAX `ContentHandler` that processes an XML document and extracts specific information from it. Problem Statement You are provided with an XML document containing data about a collection of books. Each book has several attributes, including title, author, genre, price, and publish date. Your task is to implement a SAX `ContentHandler` to parse the XML document and extract the title and author of each book published after the year 2000. XML Structure ```xml <catalog> <book id=\\"bk101\\"> <author>Gambardella, Matthew</author> <title>XML Developer\'s Guide</title> <genre>Computer</genre> <price>44.95</price> <publish_date>2000-10-01</publish_date> </book> <book id=\\"bk102\\"> <author>Ralls, Kim</author> <title>Midnight Rain</title> <genre>Fantasy</genre> <price>5.95</price> <publish_date>2001-12-16</publish_date> </book> <!-- More book entries --> </catalog> ``` Task 1. Import necessary modules (`xml.sax` and `xml.sax.handler`). 2. Create a class `BookContentHandler` that inherits from `xml.sax.handler.ContentHandler`. 3. Implement methods to handle start and end of elements to extract and store book titles and authors for books published after the year 2000. 4. The data should be stored in a list of tuples in the format `[(title, author), ...]`. 5. Write a function `parse_books(xml_file)` that takes the file path of the XML document and uses your custom handler to parse the file. 6. The function should return the list of tuples containing the titles and authors of the relevant books. Constraints - The publish date will be in the format `YYYY-MM-DD`. - Assume that all relevant elements and attributes are correctly formatted and present in the XML. - Handle multiple book entries efficiently. - You can assume the XML file is reasonably small, so memory and performance constraints are minimal. Input - `xml_file`: A string representing the file path to the XML document. Output - A list of tuples containing titles and authors of the books published after the year 2000. Example ```python xml_file = \'books.xml\' result = parse_books(xml_file) print(result) # Output: [(\'Midnight Rain\', \'Ralls, Kim\'), ...] ``` Implementation Details 1. **ContentHandler Class:** - Override `startElement`, `endElement`, and `characters` methods to process the XML structure. - Use instance variables to keep track of the current element being processed and store relevant data. 2. **parse_books Function:** - Initialize the SAX parser. - Set the custom content handler. - Parse the provided XML file. - Return the extracted list of book titles and authors. Notes - Ensure to handle character data correctly in the `characters` method, as it can be delivered in chunks. - Make sure to reset state variables appropriately to avoid data contamination between elements.","solution":"import xml.sax class BookContentHandler(xml.sax.handler.ContentHandler): def __init__(self): self.current_element = \'\' self.current_title = \'\' self.current_author = \'\' self.current_publish_date = \'\' self.books = [] self.is_title = False self.is_author = False self.is_publish_date = False def startElement(self, name, attrs): self.current_element = name if name == \'title\': self.is_title = True elif name == \'author\': self.is_author = True elif name == \'publish_date\': self.is_publish_date = True def endElement(self, name): if name == \'book\': if self.current_publish_date[:4].isdigit() and int(self.current_publish_date[:4]) > 2000: self.books.append((self.current_title, self.current_author)) self.current_title = \'\' self.current_author = \'\' self.current_publish_date = \'\' elif name == \'title\': self.is_title = False elif name == \'author\': self.is_author = False elif name == \'publish_date\': self.is_publish_date = False self.current_element = \'\' def characters(self, content): if self.is_title: self.current_title += content.strip() elif self.is_author: self.current_author += content.strip() elif self.is_publish_date: self.current_publish_date += content.strip() def parse_books(xml_file): parser = xml.sax.make_parser() handler = BookContentHandler() parser.setContentHandler(handler) parser.parse(xml_file) return handler.books"},{"question":"# Task: Command-Line Argument Parser for File Processing Objective Implement a command-line utility in Python using the `argparse` module. This utility should process files based on various command-line arguments that define the operations to be performed on the files. Requirements 1. **Positional Arguments**: - **`filepath`**: Path to the file to be processed. 2. **Optional Arguments**: - **`-r`, `--read`**: Read and display the contents of the file. - **`-w`, `--write`**: Write data to the file. Requires the `--content` argument to specify the data to write. - **`-d`, `--delete`**: Delete the file. - **`-c`, `--content`**: The content to write to the file. Only relevant if the `--write` option is specified. - **`-v`, `--verbose`**: Increase output verbosity. Can be used multiple times to increase verbosity level. - **`-q`, `--quiet`**: Run the utility in quiet mode with minimal output, mutually exclusive with the `--verbose` option. 3. **Constraints**: - If both `--read` and `--write` are specified, the utility should first read the file, display the content, and then write the new content provided via `--content`. - The `--delete` option should be mutually exclusive with `--read` and `--write`. - Ensure that the file exists before performing operations other than `--write`. Input and Output Formats - Input: Command-line arguments as specified. - Output: Various output messages based on the provided arguments, file contents, error messages, etc. Performance Requirements - The utility should handle files of typical sizes used in daily tasks efficiently. Example Usage ``` python file_util.py sample.txt --read -vv Displaying file content with verbosity level 2: This is a sample content in the file. python file_util.py sample.txt --write --content \\"New content\\" -v Writing new content to sample.txt with verbosity level 1. python file_util.py sample.txt --delete -q ``` Implementation Notes 1. Ensure that appropriate error messages are displayed if the file is not found or if invalid arguments are provided. 2. Use the `argparse` module to handle the command-line argument parsing. 3. Implement helper functions where necessary to keep the code organized and modular. # Implementation ```python import argparse import os def main(): parser = argparse.ArgumentParser(description=\\"Process files with various operations such as read, write, and delete.\\") # Positional argument parser.add_argument(\\"filepath\\", type=str, help=\\"Path to the file to be processed.\\") # Optional arguments parser.add_argument(\\"-r\\", \\"--read\\", action=\\"store_true\\", help=\\"Read and display the contents of the file.\\") parser.add_argument(\\"-w\\", \\"--write\\", action=\\"store_true\\", help=\\"Write data to the file. Requires --content.\\") parser.add_argument(\\"-d\\", \\"--delete\\", action=\\"store_true\\", help=\\"Delete the file.\\") parser.add_argument(\\"-c\\", \\"--content\\", type=str, help=\\"Content to write to the file.\\") parser.add_argument(\\"-v\\", \\"--verbose\\", action=\\"count\\", default=0, help=\\"Increase output verbosity.\\") parser.add_argument(\\"-q\\", \\"--quiet\\", action=\\"store_true\\", help=\\"Run the utility in quiet mode with minimal output.\\") # Parse arguments args = parser.parse_args() # Check for mutually exclusive options if args.quiet and args.verbose > 0: parser.error(\\"--quiet and --verbose are mutually exclusive.\\") if args.delete and (args.read or args.write): parser.error(\\"--delete cannot be used with --read or --write.\\") # Verbosity level if args.verbose >= 2: print(f\\"Running \'{__file__}\' with verbosity level {args.verbose}\\") def verbose_print(msg, level=1): if args.verbose >= level and not args.quiet: print(msg) # Handling file operations filepath = args.filepath if not os.path.exists(filepath) and not args.write: parser.error(f\\"The file {filepath} does not exist.\\") if args.read: with open(filepath, \'r\') as file: content = file.read() verbose_print(f\\"Reading file {filepath}:\\") print(content) if args.write: content = args.content if not content: parser.error(\\"--content is required with --write.\\") with open(filepath, \'w\') as file: file.write(content) verbose_print(f\\"Writing content to {filepath}:\\") print(f\\"Successfully wrote content to {filepath}.\\") if args.delete: os.remove(filepath) verbose_print(f\\"Deleted file {filepath}.\\") more_msg = f\\"{filepath} successfully deleted.\\" if not args.quiet else \\"\\" print(more_msg) if __name__ == \\"__main__\\": main() ```","solution":"import argparse import os def main(): parser = argparse.ArgumentParser(description=\\"Process files with various operations such as read, write, and delete.\\") # Positional argument parser.add_argument(\\"filepath\\", type=str, help=\\"Path to the file to be processed.\\") # Optional arguments parser.add_argument(\\"-r\\", \\"--read\\", action=\\"store_true\\", help=\\"Read and display the contents of the file.\\") parser.add_argument(\\"-w\\", \\"--write\\", action=\\"store_true\\", help=\\"Write data to the file. Requires --content.\\") parser.add_argument(\\"-d\\", \\"--delete\\", action=\\"store_true\\", help=\\"Delete the file.\\") parser.add_argument(\\"-c\\", \\"--content\\", type=str, help=\\"Content to write to the file.\\") parser.add_argument(\\"-v\\", \\"--verbose\\", action=\\"count\\", default=0, help=\\"Increase output verbosity.\\") parser.add_argument(\\"-q\\", \\"--quiet\\", action=\\"store_true\\", help=\\"Run the utility in quiet mode with minimal output.\\") # Parse arguments args = parser.parse_args() # Check for mutually exclusive options if args.quiet and args.verbose > 0: parser.error(\\"--quiet and --verbose are mutually exclusive.\\") if args.delete and (args.read or args.write): parser.error(\\"--delete cannot be used with --read or --write.\\") # Verbosity level if args.verbose >= 2: print(f\\"Running \'{__file__}\' with verbosity level {args.verbose}\\") def verbose_print(msg, level=1): if args.verbose >= level and not args.quiet: print(msg) # Handling file operations filepath = args.filepath if not os.path.exists(filepath) and not args.write: parser.error(f\\"The file {filepath} does not exist.\\") if args.read: with open(filepath, \'r\') as file: content = file.read() verbose_print(f\\"Reading file {filepath}:\\") print(content) if args.write: content = args.content if not content: parser.error(\\"--content is required with --write.\\") with open(filepath, \'w\') as file: file.write(content) verbose_print(f\\"Writing content to {filepath}:\\") print(f\\"Successfully wrote content to {filepath}.\\") if args.delete: os.remove(filepath) verbose_print(f\\"Deleted file {filepath}.\\") more_msg = f\\"{filepath} successfully deleted.\\" if not args.quiet else \\"\\" print(more_msg) if __name__ == \\"__main__\\": main()"},{"question":"**Asynchronous Tasks and Future Objects** **Objective:** Implement and manage asynchronous operations using `asyncio.Future` objects to ensure that your function can handle low-level callback-based code and integrate it with high-level async/await code. Demonstrate the ability to set, retrieve results, and manage exceptions with `Future` objects. **Task:** You are required to implement an asynchronous function with the following requirements: 1. **Function Signature:** ```python async def manage_task(futures: list) -> dict: ``` 2. **Inputs:** - `futures`: A list of tuples where each tuple contains two elements: * An integer `delay` representing the number of seconds to wait before setting a result. * A value `val` that will be the result to set after the delay. Example: [(1, \'foo\'), (2, \'bar\'), (3, \'baz\')] 3. **Outputs:** - A dictionary where each key is the index of the corresponding future from the input list, and the value is the result set for that future after the delay. 4. **Implementation Details:** - Create a Future object for each tuple in the `futures` list. - Schedule a task for each Future object that: * Waits for the specified delay. * Sets the provided value as the result of the Future object. - Ensure all Future objects are awaited, and their results are collected in the output dictionary. 5. **Constraints:** - Make sure the function handles setting results on the Future objects asynchronously. - Handle any exceptions that may occur if a Future is encountered with an invalid state. 6. **Example:** ```python import asyncio async def manage_task(futures): # Your implementation here # Testing the function test_futures = [(1, \'foo\'), (2, \'bar\'), (3, \'baz\')] result = asyncio.run(manage_task(test_futures)) print(result) # Expected output: {0: \'foo\', 1: \'bar\', 2: \'baz\'} ``` **Note:** Demonstrate your understanding of creating Future objects, managing their states, handling asyncio events, and dealing with asynchronous delays in your solution.","solution":"import asyncio async def _set_future_result(future, delay, value): Sets the result of the provided Future object after a specified delay. await asyncio.sleep(delay) future.set_result(value) async def manage_task(futures): Manages multiple Future objects, setting their results asynchronously after specified delays. Args: futures: A list of tuples. Each tuple contains: - An integer `delay` representing delay in seconds. - A value `val` that will be set as the result after the delay. Returns: A dictionary with the index of the Future as the key and the result set as the value. future_objects = [] tasks = [] for index, (delay, value) in enumerate(futures): future = asyncio.Future() future_objects.append(future) task = asyncio.create_task(_set_future_result(future, delay, value)) tasks.append(task) await asyncio.gather(*tasks) result_dict = {index: future.result() for index, future in enumerate(future_objects)} return result_dict"},{"question":"# Question: Advanced Plot Customization and Annotation with Seaborn Your task is to analyze a dataset using Seaborn and create a custom plot that provides valuable insights into the data. You will work with the \\"iris\\" dataset, a famous dataset containing measurements of different Iris flowers. Follow the steps to load, manipulate, and visualize the data: 1. **Load and Prepare the Data:** - Load the `iris` dataset from Seaborn. - Calculate the mean petal width for each species and add it as a new column, `mean_petal_width`. 2. **Create the Plot:** - Create a scatter plot showing the relationship between `petal_length` (X-axis) and `sepal_length` (Y-axis). - Color the points based on the `species`. 3. **Annotate the Plot:** - Add text annotations for each species, displaying the `species` name near the points. Adjust the vertical alignment of the text to be above the points. - Highlight the mean petal width of each species by adding a vertical line at the position of the mean petal width on the X-axis. - Add horizontal lines for the minimum and maximum sepal length across all species on the Y-axis. 4. **Enhancements:** - Customize the text annotation color based on a third variable, `species`, with each species having a distinct color. - Use additional matplotlib parameters to set the font weight of the text annotations to bold. **Input Format:** None. The function should not take any arguments. **Output Format:** A matplotlib figure object with the customized plot. **Constraints:** - You must use the `seaborn.objects` interface to create the plot and add annotations. - Ensure the plot legends and labels are clear and informative. **Example Implementation:** Here\'s a brief outline to get you started. You can fill in the details based on the requirements above: ```python import seaborn.objects as so import seaborn as sns from matplotlib import pyplot as plt def create_annotated_iris_plot(): # Load the dataset iris = sns.load_dataset(\\"iris\\") # Calculate mean petal width mean_petal_width = iris.groupby(\\"species\\")[\\"petal_width\\"].mean().reset_index() # Merge with original dataset iris = iris.merge(mean_petal_width, on=\\"species\\", suffixes=(\\"\\", \\"_mean\\")) # Creating the plot p = ( so.Plot(iris, x=\\"petal_length\\", y=\\"sepal_length\\", color=\\"species\\") .add(so.Dots()) .add(so.Text(text=\\"species\\", valign=\\"bottom\\"), halign=\\"species\\", offset=10) .scale(halign={\\"setosa\\": \\"left\\", \\"versicolor\\": \\"right\\", \\"virginica\\": \\"center\\"}) ) # Adding mean petal width lines for _, row in mean_petal_width.iterrows(): plt.axvline(row[\\"petal_width_mean\\"], color=\\"gray\\", linestyle=\\"--\\") # Adding min/max sepal length lines plt.axhline(iris[\\"sepal_length\\"].min(), color=\\"gray\\", linestyle=\\"--\\") plt.axhline(iris[\\"sepal_length\\"].max(), color=\\"gray\\", linestyle=\\"--\\") # Additional matplotlib parameters plt.gca().set_prop_cycle(None) plt.xlabel(\\"Petal Length\\") plt.ylabel(\\"Sepal Length\\") plt.legend(title=\\"Species\\") plt.title(\\"Iris Species Petal and Sepal Lengths\\") return p # To execute the function plot = create_annotated_iris_plot() plot.show() ``` Provide a complete and executable function based on this outline.","solution":"import seaborn.objects as so import seaborn as sns import matplotlib.pyplot as plt def create_annotated_iris_plot(): # Load the iris dataset iris = sns.load_dataset(\\"iris\\") # Calculate mean petal width for each species mean_petal_width = iris.groupby(\\"species\\")[\\"petal_width\\"].mean().reset_index().rename(columns={\\"petal_width\\": \\"mean_petal_width\\"}) # Merge the mean petal width back with the original dataset iris = iris.merge(mean_petal_width, on=\\"species\\") # Create the plot p = ( so.Plot(iris, x=\\"petal_length\\", y=\\"sepal_length\\", color=\\"species\\") .add(so.Dots()) ) # Add annotations and lines ax = plt.gca() colors = {\\"setosa\\": \\"blue\\", \\"versicolor\\": \\"green\\", \\"virginica\\": \\"red\\"} for species, color in colors.items(): subset = iris[iris[\\"species\\"] == species] mean_width = subset[\\"mean_petal_width\\"].iloc[0] # Vertical line for mean petal width plt.axvline(mean_width, color=color, linestyle=\\"--\\", label=f\'Mean Petal Width: {species}\') # Text annotations for _, row in subset.iterrows(): plt.text(row[\\"petal_length\\"], row[\\"sepal_length\\"], row[\\"species\\"], fontsize=10, color=color, fontweight=\'bold\', verticalalignment=\'bottom\') # Horizontal lines for min and max sepal length min_sepal_length = iris[\\"sepal_length\\"].min() max_sepal_length = iris[\\"sepal_length\\"].max() plt.axhline(min_sepal_length, color=\\"gray\\", linestyle=\\"--\\") plt.axhline(max_sepal_length, color=\\"gray\\", linestyle=\\"--\\") # Set labels and title plt.xlabel(\\"Petal Length\\") plt.ylabel(\\"Sepal Length\\") plt.title(\\"Iris Species: Petal Length vs Sepal Length\\") plt.legend() # Show the plot plt.show() return ax # Create and show the plot create_annotated_iris_plot()"},{"question":"# Advanced Python Functionality and Decorators **Objective:** Create a Python decorator that automates the logging of function arguments and results. The decorator should provide a clear, formatted log detailing the function call. This task will test your understanding of decorators, argument parsing, and introspection. **Problem Statement:** You are tasked with creating a decorator named `log_function_details` that logs the details of function calls. This decorator should: 1. Log the name of the function being called. 2. Log the arguments the function was called with, both positional (`*args`) and keyword (`**kwargs`). 3. Log the return value of the function. **Requirements:** 1. The log should be formatted for readability, clearly distinguishing between positional and keyword arguments. 2. Ensure that the logging does not interfere with the function\'s return value or side effects. **Input:** - A function definition which can take any number of arguments. **Output:** - A decorated function that logs before calling the function and logs the return value after the function execution. **Constraints:** - Do not use any external logging libraries; you may use Python\'s `print` function. - The decorator should handle any kind of function signature. # Example: ```python @log_function_details def add(a, b): Adds two numbers. return a + b result = add(3, 5) ``` **Expected Output:** ```plaintext Calling function \'add\' Positional args: (3, 5) Keyword args: {} Return from function \'add\': 8 ``` # Task: Implement the decorator `log_function_details`. ```python def log_function_details(func): def wrapper(*args, **kwargs): # Log the function detail (positional args, keyword args) print(f\\"Calling function \'{func.__name__}\'\\") print(f\\" Positional args: {args}\\") print(f\\" Keyword args: {kwargs}\\") # Call the function result = func(*args, **kwargs) # Log the returned value print(f\\"Return from function \'{func.__name__}\': {result}\\") return result return wrapper # Example usage @log_function_details def add(a, b): Adds two numbers. return a + b # Test the function result = add(3, 5) ``` # Additional Test Cases: ```python @log_function_details def greet(name, greeting=\\"Hello\\"): return f\\"{greeting}, {name}!\\" print(greet(\\"World\\")) print(greet(\\"Alice\\", greeting=\\"Hi\\")) @log_function_details def multiply(x, y, show=False): result = x * y if show: print(f\\"Multiplication result is: {result}\\") return result print(multiply(2, 3)) print(multiply(4, 5, show=True)) ``` **Expected Test Outputs:** - For `greet(\\"World\\")`: ```plaintext Calling function \'greet\' Positional args: (\'World\',) Keyword args: {} Return from function \'greet\': Hello, World! Hello, World! ``` - For `greet(\\"Alice\\", greeting=\\"Hi\\")`: ```plaintext Calling function \'greet\' Positional args: (\'Alice\',) Keyword args: {\'greeting\': \'Hi\'} Return from function \'greet\': Hi, Alice! Hi, Alice! ``` - For `multiply(2, 3)`: ```plaintext Calling function \'multiply\' Positional args: (2, 3) Keyword args: {} Return from function \'multiply\': 6 6 ``` - For `multiply(4, 5, show=True)`: ```plaintext Calling function \'multiply\' Positional args: (4, 5) Keyword args: {\'show\': True} Multiplication result is: 20 Return from function \'multiply\': 20 20 ```","solution":"def log_function_details(func): A decorator that logs the details of function calls and their return values. def wrapper(*args, **kwargs): # Log the function details (positional args, keyword args) print(f\\"Calling function \'{func.__name__}\'\\") print(f\\" Positional args: {args}\\") print(f\\" Keyword args: {kwargs}\\") # Call the function result = func(*args, **kwargs) # Log the returned value print(f\\"Return from function \'{func.__name__}\': {result}\\") return result return wrapper # Example usage @log_function_details def add(a, b): Adds two numbers. return a + b @log_function_details def greet(name, greeting=\\"Hello\\"): Greets a person. return f\\"{greeting}, {name}!\\" @log_function_details def multiply(x, y, show=False): Multiplies two numbers. result = x * y if show: print(f\\"Multiplication result is: {result}\\") return result"},{"question":"# **Python Import System Assessment** **Objective**: Evaluate the understanding of Python\'s import system, particularly focusing on the `importlib` module, custom finders and loaders, and module caching mechanisms. **Problem Statement** You are tasked with creating a custom import mechanism for your Python project. This mechanism will dynamically import specific modules only if they are not already present in the module cache (`sys.modules`). Additionally, you need to create a custom finder that can locate modules directly from a specific directory. Write a Python program that: 1. **Custom Finder**: - Create a class `CustomFinder` that extends Python\'s import mechanism. This finder should be initialized with a directory path where it will look for modules. - Implement the `find_spec()` method that checks if the module is in the directory and returns an appropriate module specification. 2. **Custom Import Function**: - Implement a function `custom_import(module_name: str, directory: str) -> None`: - This function should first check if the module is already present in `sys.modules`. - If not, it should use the custom finder to search for and load the module from the specified directory. - If the module cannot be found, raise a `ModuleNotFoundError`. - Print appropriate messages to indicate each step (checking cache, using custom finder, loading module). 3. **Testing**: - Demonstrate the functionality with a directory containing some test modules. - Ensure that the same module is not reloaded if it is already in `sys.modules`. **Constraints**: - Assume the provided directory names and module names are valid and do not contain any special characters. - Only use standard Python libraries (no external dependencies). - Ensure proper error handling and informative print statements for debugging. **Input/Output**: - The directory path and module names will be provided as strings. - The function `custom_import` should not return anything but print the steps. **Example Usage**: ```python # Assuming the directory \'my_modules\' contains \'module_a.py\', \'module_b.py\' custom_import(\'module_a\', \'my_modules\') # Output: # Checking cache for module: module_a # Module not in cache. Using Custom Finder. # Module \'module_a\' found and loaded successfully from \'my_modules/module_a.py\'. custom_import(\'module_a\', \'my_modules\') # Output: # Checking cache for module: module_a # Module \'module_a\' is already present in sys.modules. ``` Implement the `CustomFinder` class and the `custom_import` function.","solution":"import sys import importlib.util import os class CustomFinder: def __init__(self, directory): self.directory = directory def find_spec(self, fullname, path, target=None): module_name = fullname.split(\'.\')[-1] module_path = os.path.join(self.directory, f\\"{module_name}.py\\") if os.path.isfile(module_path): spec = importlib.util.spec_from_file_location(fullname, module_path) return spec return None def custom_import(module_name: str, directory: str) -> None: print(f\\"Checking cache for module: {module_name}\\") if module_name in sys.modules: print(f\\"Module \'{module_name}\' is already present in sys.modules.\\") return print(f\\"Module not in cache. Using Custom Finder.\\") finder = CustomFinder(directory) spec = finder.find_spec(module_name, [directory]) if spec is None: raise ModuleNotFoundError(f\\"Module \'{module_name}\' not found in directory \'{directory}\'\\") module = importlib.util.module_from_spec(spec) sys.modules[module_name] = module spec.loader.exec_module(module) print(f\\"Module \'{module_name}\' found and loaded successfully from \'{spec.origin}\'\\") # Example usage of the custom_import function. # You can create a directory \'my_modules\' and put \'module_a.py\' inside it to test the functionality. # custom_import(\'module_a\', \'my_modules\')"},{"question":"**Custom Mapping Implementation** # Objective Implement a custom mapping-like class in Python that mimics the core functionalities of Python\'s dictionary. This class should support adding, retrieving, deleting items, and checking for the existence of keys, similar to the functionality described in the provided documentation. # Problem Statement Create a class called `CustomMapping` that: 1. **Initialization**: Initializes an empty mapping. 2. **Set Item**: Allows setting a value for a given key. 3. **Get Item**: Allows retrieving the value for a given key. 4. **Delete Item**: Allows deleting a key-value pair based on the key. 5. **Check Key**: Allows checking if a key exists in the mapping. 6. **Get Keys**: Returns a list of all keys in the mapping. 7. **Get Values**: Returns a list of all values in the mapping. 8. **Get Items**: Returns a list of all items (key-value pairs) in the mapping. # Constraints - Keys will be strings. - There can be at most 10,000 items in the mapping. - Your implementation should be efficient in terms of time complexity for each of these operations. # Class Definition ```python class CustomMapping: def __init__(self): # Initializes an empty mapping pass def set_item(self, key: str, value: any): # Sets the key to the specified value pass def get_item(self, key: str) -> any: # Returns the value for the specified key pass def delete_item(self, key: str): # Deletes the key-value pair for the specified key pass def has_key(self, key: str) -> bool: # Checks if the specified key exists in the mapping pass def get_keys(self) -> list: # Returns a list of all keys pass def get_values(self) -> list: # Returns a list of all values pass def get_items(self) -> list: # Returns a list of all items (key-value pairs) pass ``` # Example Usage ```python cm = CustomMapping() cm.set_item(\'a\', 1) cm.set_item(\'b\', 2) assert cm.get_item(\'a\') == 1 assert cm.has_key(\'b\') == True assert cm.has_key(\'c\') == False assert set(cm.get_keys()) == {\'a\', \'b\'} assert set(cm.get_values()) == {1, 2} assert set(cm.get_items()) == {(\'a\', 1), (\'b\', 2)} cm.delete_item(\'a\') assert cm.get_item(\'a\') is None assert cm.has_key(\'a\') == False ``` # Notes - You should handle the case when attempting to get or delete a non-existent key gracefully. - You can use built-in Python data structures to implement this class.","solution":"class CustomMapping: def __init__(self): # Initializes an empty mapping self.mapping = {} def set_item(self, key: str, value: any): # Sets the key to the specified value self.mapping[key] = value def get_item(self, key: str) -> any: # Returns the value for the specified key, or None if the key does not exist return self.mapping.get(key) def delete_item(self, key: str): # Deletes the key-value pair for the specified key if key in self.mapping: del self.mapping[key] def has_key(self, key: str) -> bool: # Checks if the specified key exists in the mapping return key in self.mapping def get_keys(self) -> list: # Returns a list of all keys return list(self.mapping.keys()) def get_values(self) -> list: # Returns a list of all values return list(self.mapping.values()) def get_items(self) -> list: # Returns a list of all items (key-value pairs) return list(self.mapping.items())"},{"question":"# Custom Matrix Class Implementation Problem Statement You are required to implement a custom container class named `Matrix`, representing a 2D matrix. This class should: 1. Inherit from `collections.abc.Sequence` and `collections.abc.Sized`. 2. Implement all required abstract methods (`__getitem__`, `__len__`). 3. Include additional methods (`transpose`, `to_flat_list`) to demonstrate an understanding of custom methods in a class with multiple inheritance. Requirements 1. **Inheritance and Abstract Methods**: - `Matrix` should be a subclass of `Sequence` and `Sized`. - You must provide concrete implementations for the following methods: - `__len__(self) -> int`: Returns the number of rows in the matrix. - `__getitem__(self, index) -> list`: Returns the row at the specified index. 2. **Additional Methods**: - `transpose(self) -> Matrix`: Returns a new `Matrix` object that is the transpose of the current matrix. - `to_flat_list(self) -> list`: Returns a flat list containing all elements of the matrix. 3. **Constructor**: - The constructor should accept a list of lists representing the matrix. 4. **Example Usage**: ```python m = Matrix([[1, 2, 3], [4, 5, 6]]) print(len(m)) # Output: 2 print(m[0]) # Output: [1, 2, 3] print(m.transpose()) # Output: Matrix([[1, 4], [2, 5], [3, 6]]) print(m.to_flat_list()) # Output: [1, 2, 3, 4, 5, 6] ``` Implementation Constraints - Assume all input matrices are non-empty and rectangular (all rows have the same number of columns). - Focus on correctness and clear, readable code. - Performance considerations are secondary but try to avoid unnecessary complexity. Interface Definition ```python from collections.abc import Sequence, Sized class Matrix(Sequence, Sized): def __init__(self, matrix: list[list]): # Initialize the matrix data def __len__(self) -> int: # Return the number of rows def __getitem__(self, index: int) -> list: # Return the specified row def transpose(self) -> \'Matrix\': # Return a transposed Matrix def to_flat_list(self) -> list: # Return a flat list of elements ``` Implement the `Matrix` class following the specifications above.","solution":"from collections.abc import Sequence, Sized class Matrix(Sequence, Sized): def __init__(self, matrix: list[list]): self.matrix = matrix def __len__(self) -> int: Returns the number of rows in the matrix. return len(self.matrix) def __getitem__(self, index: int) -> list: Returns the row at the specified index. return self.matrix[index] def transpose(self) -> \'Matrix\': Returns a new Matrix object that is the transpose of the current matrix. transposed_matrix = [list(row) for row in zip(*self.matrix)] return Matrix(transposed_matrix) def to_flat_list(self) -> list: Returns a flat list containing all elements of the matrix. return [item for row in self.matrix for item in row]"},{"question":"# Question: Implement Simplified Partial Least Squares Regression In this task, you are required to implement a simplified version of the `PLSRegression` algorithm in scikit-learn. Your implementation will focus on understanding and applying the core steps of the PLSRegression as described in the given documentation. Function Signature ```python def simplified_pls_regression(X: np.ndarray, Y: np.ndarray, n_components: int) -> Tuple[np.ndarray, np.ndarray]: Perform a simplified version of Partial Least Squares Regression on the given data. Parameters: X (np.ndarray): Input data matrix of shape (n_samples, n_features). Y (np.ndarray): Target data matrix of shape (n_samples, n_targets). n_components (int): Number of components to keep. Returns: Tuple[np.ndarray, np.ndarray]: A tuple containing: - Transformed input data matrix (np.ndarray of shape (n_samples, n_components)). - Coefficient matrix for predicting Y from X (np.ndarray of shape (n_features, n_targets)). ``` Requirements 1. **Center the Data**: Center the matrices X and Y by subtracting their means. 2. **Compute Weights**: Compute the weight vectors `u` and `v`. Use the first left and right singular vectors of the cross-covariance matrix of X and Y for each component. 3. **Projection and Scores**: Project X and Y using the computed weights to obtain the scores. 4. **Loadings**: Compute the loadings and deflate X and Y. 5. **Transformation Matrix**: Compute the transformation matrix for X. 6. **Prediction Coefficients**: Compute the coefficient matrix for predicting Y from X. Constraints - Assume `n_samples > n_features` and `n_samples > n_targets`. - You must not use pre-built PLS functions from scikit-learn. - Performance should be suitable for reasonably sized datasets (e.g., n_samples, n_features, n_targets < 1000). Here is a sample input and output for verification: # Sample Input ```python import numpy as np X = np.array([[0.1, 0.2], [0.4, 0.5], [0.7, 0.8]]) Y = np.array([[0.1], [0.3], [0.5]]) n_components = 1 ``` # Expected Output ```python (transformed_X, coef_matrix) = simplified_pls_regression(X, Y, n_components) print(transformed_X.shape) # Output: (3, 1) print(coef_matrix.shape) # Output: (2, 1) ``` Note Make sure your implementation strictly follows the steps under \\"PLSRegression\\" in the documentation. Include error handling for cases where the input dimensions are invalid.","solution":"import numpy as np def simplified_pls_regression(X: np.ndarray, Y: np.ndarray, n_components: int) -> tuple: Perform a simplified version of Partial Least Squares Regression on the given data. Parameters: X (np.ndarray): Input data matrix of shape (n_samples, n_features). Y (np.ndarray): Target data matrix of shape (n_samples, n_targets). n_components (int): Number of components to keep. Returns: tuple: A tuple containing: - Transformed input data matrix (np.ndarray of shape (n_samples, n_components)). - Coefficient matrix for predicting Y from X (np.ndarray of shape (n_features, n_targets)). n_samples, n_features = X.shape _, n_targets = Y.shape # Center the data X_mean = np.mean(X, axis=0) Y_mean = np.mean(Y, axis=0) X_centered = X - X_mean Y_centered = Y - Y_mean W = np.zeros((n_features, n_components)) Q = np.zeros((n_targets, n_components)) T = np.zeros((n_samples, n_components)) for i in range(n_components): # Calculate the weight vectors using SVD u, s, vh = np.linalg.svd(np.dot(X_centered.T, Y_centered), full_matrices=False) w = u[:, 0] q = vh.T[:, 0] # Normalize the weights w /= np.linalg.norm(w) # Calculate the scores t = np.dot(X_centered, w) t = t[:, np.newaxis] # Calculate the loadings p = np.dot(X_centered.T, t) / np.dot(t.T, t) q = np.dot(Y_centered.T, t) / np.dot(t.T, t) # Save weights and scores W[:, i] = w Q[:, i] = q.squeeze() T[:, i] = t.squeeze() # Deflate the matrices X_centered -= np.dot(t, p.T) Y_centered -= np.dot(t, q.T) # Compute the coefficients matrix for predicting Y from X B = np.dot(W, np.linalg.inv(np.dot(W.T, W))) C = np.dot(B, Q.T) # Transform X transformed_X = np.dot(X - X_mean, W) return transformed_X, C"},{"question":"**Objective:** Implement an async function that performs a series of network I/O operations using asyncio. You must demonstrate comprehensive exception handling for all specified exceptions in the asyncio documentation: `TimeoutError`, `CancelledError`, `InvalidStateError`, `SendfileNotAvailableError`, `IncompleteReadError`, and `LimitOverrunError`. **Problem Statement:** You are to develop a network client simulator using asyncio. Your task is to perform read operations from a set of predefined network endpoints. **Requirements:** - Implement an `async` function `network_client_simulator(endpoints: List[str], timeout: int) -> List[str]` that accepts a list of network endpoints (URLs) and a timeout value (in seconds). - For each endpoint: - Attempt to perform an asynchronous network read operation. Simulate this by sleeping for a random period and then raising one of the documented exceptions. - Handle each exception appropriately: - `TimeoutError`: Log the timeout error and skip to the next endpoint. - `CancelledError`: Log the cancellation, perform any cleanup if needed, and raise the exception again. - `InvalidStateError`: Log the invalid state error and continue. - `SendfileNotAvailableError`: Log the error related to the sendfile syscall. - `IncompleteReadError`: Log the incomplete read and report the number of partial bytes read. - `LimitOverrunError`: Log the buffer limit error and report the number of consumed bytes. - Return a list of strings reporting the status of each endpoint read attempt. **Input:** - `endpoints` - A list of string URLs (e.g., `[\\"http://example.com\\", \\"http://test.com\\"]`) - `timeout` - An integer representing the timeout value in seconds. **Output:** - Return a list of status messages for each endpoint in the format: - `\\"SUCCESS: <endpoint>\\"` - `\\"TIMEOUT: <endpoint>\\"` - `\\"CANCELLED: <endpoint>\\"` - `\\"INVALID_STATE: <endpoint>\\"` - `\\"SENDFILE_NOT_AVAILABLE: <endpoint>\\"` - `\\"INCOMPLETE_READ: <endpoint>, partial bytes: <bytes_read>\\"` - `\\"LIMIT_OVERRUN: <endpoint>, consumed bytes: <bytes_consumed>\\"` **Constraints:** - You must use the `asyncio` library. - Simulate the exceptions to demonstrate your understanding of exception handling. - Ensure each exception is simulated at least once during your testing. **Example:** ```python import asyncio import random from typing import List async def network_client_simulator(endpoints: List[str], timeout: int) -> List[str]: results = [] for endpoint in endpoints: try: await asyncio.sleep(random.uniform(0, 2)) # Simulates network I/O # Randomly raise an exception to simulate different scenarios exception_choice = random.choice([ asyncio.TimeoutError, asyncio.CancelledError, asyncio.InvalidStateError, asyncio.SendfileNotAvailableError, asyncio.IncompleteReadError(expected=100, partial=b\\"partial_data\\"), asyncio.LimitOverrunError(consumed=50)] ) raise exception_choice() except asyncio.TimeoutError: results.append(f\\"TIMEOUT: {endpoint}\\") except asyncio.CancelledError: results.append(f\\"CANCELLED: {endpoint}\\") raise except asyncio.InvalidStateError: results.append(f\\"INVALID_STATE: {endpoint}\\") except asyncio.SendfileNotAvailableError: results.append(f\\"SENDFILE_NOT_AVAILABLE: {endpoint}\\") except asyncio.IncompleteReadError as e: results.append(f\\"INCOMPLETE_READ: {endpoint}, partial bytes: {len(e.partial)}\\") except asyncio.LimitOverrunError as e: results.append(f\\"LIMIT_OVERRUN: {endpoint}, consumed bytes: {e.consumed}\\") return results # Example usage: endpoints = [\\"http://example.com\\", \\"http://test.com\\"] timeout = 5 results = asyncio.run(network_client_simulator(endpoints, timeout)) print(results) ``` **Note:** Your implementation should incorporate proper logging and appropriate use of asyncio constructs to handle network operations and exceptions effectively.","solution":"import asyncio import logging import random from typing import List # Setting up logging logging.basicConfig(level=logging.INFO) async def network_client_simulator(endpoints: List[str], timeout: int) -> List[str]: results = [] for endpoint in endpoints: try: await asyncio.sleep(random.uniform(0, 2)) # Simulates network I/O with random delay # Randomly raise an exception to simulate different scenarios exception_choice = random.choice([ asyncio.TimeoutError, asyncio.CancelledError, asyncio.InvalidStateError, asyncio.SendfileNotAvailableError, asyncio.IncompleteReadError(expected=100, partial=b\\"partial_data\\"), asyncio.LimitOverrunError(consumed=50)] ) raise exception_choice() except asyncio.TimeoutError: logging.error(f\\"Timeout occurred for endpoint {endpoint}\\") results.append(f\\"TIMEOUT: {endpoint}\\") except asyncio.CancelledError: logging.error(f\\"Operation was cancelled for endpoint {endpoint}\\") results.append(f\\"CANCELLED: {endpoint}\\") raise except asyncio.InvalidStateError: logging.error(f\\"Invalid state error for endpoint {endpoint}\\") results.append(f\\"INVALID_STATE: {endpoint}\\") except asyncio.SendfileNotAvailableError: logging.error(f\\"Sendfile not available error for endpoint {endpoint}\\") results.append(f\\"SENDFILE_NOT_AVAILABLE: {endpoint}\\") except asyncio.IncompleteReadError as e: logging.error(f\\"Incomplete read error for endpoint {endpoint}, partial bytes: {len(e.partial)}\\") results.append(f\\"INCOMPLETE_READ: {endpoint}, partial bytes: {len(e.partial)}\\") except asyncio.LimitOverrunError as e: logging.error(f\\"Limit overrun error for endpoint {endpoint}, consumed bytes: {e.consumed}\\") results.append(f\\"LIMIT_OVERRUN: {endpoint}, consumed bytes: {e.consumed}\\") return results # Example usage: # endpoints = [\\"http://example.com\\", \\"http://test.com\\"] # timeout = 5 # results = asyncio.run(network_client_simulator(endpoints, timeout)) # print(results)"},{"question":"# Floating Point Handling in Python You are tasked with implementing a function that verifies and processes a collection of strings and numbers, converts them to Python floating point objects, and performs certain calculations. Your implementation should utilize the provided Python C API for float objects, mimicking its behavior. **Task:** Write a Python function `process_float_objects(data)` that takes a list of strings and numbers as input and performs the following operations: 1. **Validation**: - Check if each element in the list is a valid float or can be converted to a float. - Raise a `ValueError` with a message \\"Invalid input detected\\" if any element cannot be converted to a float. 2. **Conversion and Collection**: - Convert each valid string or number to a Python floating point object. - Store these floating point objects in a list. 3. **Calculations**: - Calculate and return the following: - The sum of all the floating point objects. - The maximum floating point object. - The minimum floating point object. - The average of all the floating point objects. **Function Signature**: ```python def process_float_objects(data: list) -> tuple: # Your implementation here pass ``` **Input**: - `data`: A list containing strings and numbers. **Output**: - A tuple containing: - The sum of all floats. - The maximum float. - The minimum float. - The average of all floats. **Example**: ```python input_data = [\\"1.23\\", 4.56, \\"7.89\\", 10.12] result = process_float_objects(input_data) print(result) # Output should be (23.8, 10.12, 1.23, 5.95) ``` **Constraints**: - You may assume that the list contains at least one element. - Strings will contain valid representations of floats or integers, but your function should handle and report invalid entries. Focus on utilizing the functionality akin to what `PyFloat_FromString`, `PyFloat_FromDouble`, `PyFloat_AsDouble`, and similar functions would provide to ensure accuracy in your calculations. **Note**: You need to handle the conversion logic using Python\'s capabilities (not actual C functions), but the functionality should respect the behavior described in the documentation.","solution":"def process_float_objects(data): Process a list of strings and numbers, convert them to floats, and perform calculations. Args: data (list): A list containing strings and numbers. Returns: tuple: A tuple consisting of (sum, max, min, average). Raises: ValueError: If any element cannot be converted to a float. float_objects = [] for item in data: try: float_obj = float(item) float_objects.append(float_obj) except ValueError: raise ValueError(\\"Invalid input detected\\") total = sum(float_objects) max_value = max(float_objects) min_value = min(float_objects) avg_value = total / len(float_objects) return (total, max_value, min_value, avg_value)"},{"question":"**Asynchronous HTTP Server with Task Reporting** You are tasked with creating an asynchronous HTTP server that listens for incoming requests and performs various tasks. You must use Python\'s `asyncio` package to handle the concurrency. The server should have the following features: 1. **Endpoint `/task`**: - When accessed via a POST request, it starts a new task that simulates a long-running operation. - Respond with a unique `task_id` for tracking. 2. **Endpoint `/status/{task_id}`**: - When accessed via a GET request, it returns the current status of the referenced task (`pending`, `running`, or `completed`). - If the task does not exist, it should return a `404 Not Found` error. 3. **Task Management**: - Use `asyncio.create_task()` to manage tasks. - Ensure that tasks are properly awaited. - Ensure any exceptions during the task execution are logged and retrieved. 4. **Thread Safety**: - Implement mechanisms to safely interact with the task dictionary from different OS threads if necessary. 5. **Debug Mode**: - Enable debug mode to detect common issues such as unawaited coroutines. - Log slow operations and tasks taking longer than a specified threshold. 6. **Handling Blocking Code**: - Implement a function that simulates blocking I/O using `asyncio.sleep` and runs it using `loop.run_in_executor()`. # Requirements: - **Input**: - POST request to `/task`: Starts a new task. - GET request to `/status/{task_id}`: Check the status of a task. - **Output**: - JSON response for task creation and status checking. - Example: `{\\"task_id\\": 1}` for task creation. - Example: `{\\"task_id\\": 1, \\"status\\": \\"running\\"}` for status checking. # Constraints: - Tasks should simulate long-running operations (e.g., `asyncio.sleep(30)`). - Proper error handling must be implemented for non-existent tasks. - Implement comprehensive logging of task start, completion, and errors. # Performance Requirements: - Properly handle multiple concurrent connections. - Efficiently manage memory and CPU usage for tracking active tasks. # Implementation Tips: 1. Use `aiohttp` for creating the HTTP server. 2. Store the tasks in a dictionary with the task id as the key. 3. Make use of `asyncio.create_task()` to handle initiating tasks. 4. Ensure thread safety with the appropriate use of `asyncio` synchronization primitives or thread-safe queues. 5. Enable and configure asyncio\'s debug mode to aid in development and troubleshooting. **Example Usage:** ```python # Sample POST request to create a task: POST /task { } # Sample response for task creation: { \\"task_id\\": 1 } # Sample GET request to check task status: GET /status/1 # Sample response for task status: { \\"task_id\\": 1, \\"status\\": \\"running\\" } ``` Please implement the server as specified above.","solution":"import asyncio import logging import uuid from aiohttp import web # Configure logging logging.basicConfig(level=logging.INFO) logger = logging.getLogger(__name__) asyncio.get_event_loop().set_debug(True) # Dictionary to store tasks tasks = {} def generate_task_id(): return str(uuid.uuid4()) async def long_running_task(task_id): logger.info(f\\"Task {task_id} started\\") try: await asyncio.sleep(30) # Simulate long-running operation logger.info(f\\"Task {task_id} completed\\") tasks[task_id][\\"status\\"] = \\"completed\\" except Exception as e: logger.error(f\\"Task {task_id} failed with exception: {e}\\") tasks[task_id][\\"status\\"] = \\"error\\" tasks[task_id][\\"exception\\"] = str(e) async def handle_task(request): task_id = generate_task_id() task = asyncio.create_task(long_running_task(task_id)) tasks[task_id] = { \\"task\\": task, \\"status\\": \\"running\\", \\"exception\\": None } return web.json_response({\\"task_id\\": task_id}) async def handle_status(request): task_id = request.match_info[\'task_id\'] task_info = tasks.get(task_id) if not task_info: raise web.HTTPNotFound() return web.json_response({\\"task_id\\": task_id, \\"status\\": task_info[\\"status\\"]}) app = web.Application() app.add_routes([ web.post(\'/task\', handle_task), web.get(\'/status/{task_id}\', handle_status) ]) if __name__ == \'__main__\': web.run_app(app, port=8080)"},{"question":"Objective: Design a multi-threaded system in Python to simulate a production line where multiple producers generate tasks, and multiple consumers process those tasks. You will need to use the `queue` module to manage task exchanges between producers and consumers safely. Problem Statement: Implement a class `ProductionLine` that simulates a production line using a FIFO queue. The class should handle the following functionalities: 1. **Initialization**: The queue should have a maximum size to control the number of tasks it holds. 2. **Task Production**: Multiple producer threads should be able to add tasks to the queue. 3. **Task Consumption**: Multiple consumer threads should process (remove) tasks from the queue. 4. **Task Completion Tracking**: Ensure that all produced tasks are successfully processed before the program ends. Implement the following methods in the `ProductionLine` class: 1. `__init__(self, maxsize: int)`: Constructor to initialize the queue with a given maximum size. 2. `start_producer(self, producer_id: int, num_tasks: int)`: Method to simulate a producer generating a specific number of tasks. 3. `start_consumer(self, consumer_id: int)`: Method to simulate a consumer processing tasks. 4. `wait_for_completion(self)`: Method to block until all tasks have been processed. Constraints: - The queue should block producers when it reaches the maximum size and blocks consumers when it is empty. - Use appropriate thread synchronization mechanisms to ensure thread safety. - Implement logging to print the status of task production and consumption. Example Usage: ```python import threading from production_line import ProductionLine # Instantiate the production line with a max queue size of 10 production_line = ProductionLine(maxsize=10) # Start producer threads for i in range(5): threading.Thread(target=production_line.start_producer, args=(i, 20)).start() # Start consumer threads for i in range(3): threading.Thread(target=production_line.start_consumer, args=(i,)).start() # Wait for all tasks to complete production_line.wait_for_completion() print(\\"All tasks have been processed.\\") ``` Expected Output: ``` Producer 0: Added task 0 Producer 0: Added task 1 ... Consumer 0: Processed task 0 Consumer 0: Processed task 1 ... All tasks have been processed. ``` Implement the `ProductionLine` class and its methods to solve this problem. Ensure your implementation properly handles thread synchronization and task tracking.","solution":"import threading import queue import logging logging.basicConfig(level=logging.INFO, format=\'%(asctime)s - %(message)s\') class ProductionLine: def __init__(self, maxsize: int): self.queue = queue.Queue(maxsize=maxsize) self.lock = threading.Lock() self.total_tasks_produced = 0 self.total_tasks_consumed = 0 self.all_tasks = 0 self.producer_threads = [] self.consumer_threads = [] def start_producer(self, producer_id: int, num_tasks: int): for i in range(num_tasks): item = (producer_id, i) self.queue.put(item) # Blocks if the queue is full with self.lock: self.total_tasks_produced += 1 logging.info(f\'Producer {producer_id}: Added task {i}\') with self.lock: self.all_tasks += num_tasks def start_consumer(self, consumer_id: int): while True: try: item = self.queue.get(timeout=5) # Blocks if the queue is empty with self.lock: self.total_tasks_consumed += 1 logging.info(f\'Consumer {consumer_id}: Processed task {item}\') self.queue.task_done() except queue.Empty: return def wait_for_completion(self): self.queue.join() # Blocks until all tasks are marked as done for thread in self.consumer_threads: thread.join() def run_producers_and_consumers(self, producers_tasks, num_consumers): # Start producer threads for producer_details in producers_tasks: producer_thread = threading.Thread(target=self.start_producer, args=producer_details) self.producer_threads.append(producer_thread) producer_thread.start() # Start consumer threads for i in range(num_consumers): consumer_thread = threading.Thread(target=self.start_consumer, args=(i,)) self.consumer_threads.append(consumer_thread) consumer_thread.start() # Wait for producers to finish for thread in self.producer_threads: thread.join() # Wait for all tasks to complete self.wait_for_completion()"},{"question":"# Coding Challenge: Custom Module Importer and Metadata Retrieval **Objective:** Demonstrate your understanding of Python\'s import system by implementing a custom module importer and retrieving metadata from a specified package. **Task:** 1. **Custom Importer**: - Implement a class `CustomImporter` that: - Inherits from `importlib.abc.Loader` and `importlib.abc.MetaPathFinder`. - Overrides the necessary methods to import modules from a specified directory. 2. **Metadata Retrieval**: - Implement a function `get_metadata(package_name: str) -> dict` that: - Uses `importlib.metadata` to retrieve the metadata for a given package. - Returns a dictionary containing the package version, entry points, and distribution requirements. **Constraints:** - Your code should not use the standard `import` statement for importing the target modules, but instead use your `CustomImporter`. - Ensure your implementation can handle non-existent modules gracefully by raising appropriate exceptions. - Assume the target directory for module import is \\"./custom_modules\\". **Input:** - A string `package_name` indicating the name of the package for which the metadata is to be retrieved. **Output:** - For `CustomImporter`: Successful importation of modules from the specified directory. - For `get_metadata`: - A dictionary with keys `\\"version\\"`, `\\"entry_points\\"`, and `\\"requirements\\"`, and their respective values from the package metadata. **Performance Requirements:** - The custom importer and metadata retrieval functions should be efficient enough to handle a reasonably large number of modules and metadata entries without significant delay. **Example Usage:** You are expected to demonstrate your solution using the `CustomImporter` to import a module named `example_module` from the \\"./custom_modules\\" directory and retrieve the metadata for an arbitrary package, e.g., `requests`. ```python import sys class CustomImporter(importlib.abc.Loader, importlib.abc.MetaPathFinder): # Implement necessary methods for custom importing from \'./custom_modules\' pass def get_metadata(package_name: str) -> dict: # Implement metadata retrieval using importlib.metadata pass if __name__ == \\"__main__\\": # Register the custom importer sys.meta_path.insert(0, CustomImporter()) # Example of importing a module using the custom importer try: import example_module print(\\"Module \'example_module\' imported successfully.\\") except ModuleNotFoundError as e: print(str(e)) # Example of retrieving metadata for the \'requests\' package metadata = get_metadata(\\"requests\\") print(metadata) ``` **Notes:** - Include sufficient error handling and edge cases to demonstrate robust and reliable code. - Make sure to provide library imports for `importlib.abc`, `importlib.metadata`, and other necessary components.","solution":"import importlib.abc import importlib.util import importlib.metadata import sys import os class CustomImporter(importlib.abc.Loader, importlib.abc.MetaPathFinder): def __init__(self): self.module_directory = \\"./custom_modules\\" def find_spec(self, fullname, path, target=None): if path is None or path == \\"\\": path = [os.getcwd()] for entry in path: potential_module_path = os.path.join(entry, self.module_directory, fullname + \\".py\\") if os.path.isfile(potential_module_path): spec = importlib.util.spec_from_file_location(fullname, potential_module_path, loader=self) return spec return None def create_module(self, spec): # Default behavior is to return None, which triggers default module creation return None def exec_module(self, module): module_path = module.__spec__.origin with open(module_path, \\"r\\") as file: code = file.read() exec(code, module.__dict__) def get_metadata(package_name: str) -> dict: try: # Retrieve package distribution distribution = importlib.metadata.distribution(package_name) # Extract metadata information version = distribution.version entry_points = list(distribution.entry_points) requirements = list(distribution.requires or []) # Return the metadata dictionary return { \\"version\\": version, \\"entry_points\\": entry_points, \\"requirements\\": requirements } except importlib.metadata.PackageNotFoundError: raise ValueError(f\\"Package \'{package_name}\' not found\\") # Example Usage if __name__ == \\"__main__\\": # Register the custom importer sys.meta_path.insert(0, CustomImporter()) # Example of importing a module using the custom importer try: import example_module print(\\"Module \'example_module\' imported successfully.\\") except ModuleNotFoundError as e: print(str(e)) # Example of retrieving metadata for the \'requests\' package try: metadata = get_metadata(\\"requests\\") print(metadata) except ValueError as e: print(str(e))"},{"question":"# Support Vector Machine (SVM) Implementation and Performance Tuning **Objective:** Implement and evaluate an SVM using scikit-learn. The objective of this exercise is to assess your understanding of SVMs, including: - Model fitting and prediction - Handling multi-class classification - Kernel functions - Model performance tuning (hyperparameter optimization) **Problem Statement:** You are given a dataset that contains several features and a categorical target variable. Your task is to build an SVM classifier to predict the target variable based on the features provided. Additionally, you need to fine-tune the hyperparameters to optimize classification performance. **Dataset:** You can use any multi-class dataset available in scikit-learn (for example, the Iris dataset). **Requirements:** 1. **Data Preparation:** - Load the dataset and perform necessary preprocessing (e.g., splitting data, scaling features). 2. **Model Implementation:** - Implement an SVM classifier using `SVC` from scikit-learn. - Configure the SVM to handle multi-class classification using an appropriate strategy (`One-vs-One` or `One-vs-Rest`). - Use the RBF (Radial Basis Function) kernel for the SVM. 3. **Hyperparameter Tuning:** - Perform hyperparameter tuning using grid search (`GridSearchCV`) to find the optimal values of `C` and `gamma`. - Evaluate the model performance using cross-validation. 4. **Model Evaluation:** - Train the SVM classifier with the optimal hyperparameters on the training dataset. - Evaluate the model on the test dataset and report the accuracy, precision, recall, and F1-score for each class. 5. **Code Implementation:** - Implement your code within a single function, `train_and_evaluate_svm`, which accepts the dataset and returns the evaluation metrics. **Function Signature:** ```python def train_and_evaluate_svm(X_train, X_test, y_train, y_test): Train and evaluate an SVM classifier. Parameters: - X_train: numpy.ndarray : Training data features - X_test: numpy.ndarray : Test data features - y_train: numpy.ndarray : Training data labels - y_test: numpy.ndarray : Test data labels Returns: - metrics_dict: dict : Dictionary containing accuracy, precision, recall, and F1-score for each class pass ``` **Example Usage:** ```python # Example usage with the Iris dataset from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler # Load the dataset iris = load_iris() X, y = iris.data, iris.target # Split the dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Standardize the features scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) # Call the function with the prepared data metrics = train_and_evaluate_svm(X_train, X_test, y_train, y_test) # Print the metrics print(metrics) ``` **Notes:** - Make sure to handle class imbalance if present. - Use relevant scikit-learn functions for model training, tuning, and evaluation. - Comment your code and explain your logic for better understanding. **Constraints:** - Use the scikit-learn library for SVM implementation and evaluation. - Ensure the solution is efficient and follows best practices in machine learning.","solution":"import numpy as np from sklearn.svm import SVC from sklearn.model_selection import GridSearchCV from sklearn.metrics import classification_report def train_and_evaluate_svm(X_train, X_test, y_train, y_test): Train and evaluate an SVM classifier. Parameters: - X_train: numpy.ndarray : Training data features - X_test: numpy.ndarray : Test data features - y_train: numpy.ndarray : Training data labels - y_test: numpy.ndarray : Test data labels Returns: - metrics_dict: dict : Dictionary containing accuracy, precision, recall, and F1-score for each class # Define the SVM classifier with RBF kernel svm = SVC(kernel=\'rbf\') # Set the parameters for grid search param_grid = { \'C\': [0.1, 1, 10, 100], \'gamma\': [1, 0.1, 0.01, 0.001] } # Perform grid search with cross-validation grid_search = GridSearchCV(svm, param_grid, refit=True, cv=5) grid_search.fit(X_train, y_train) # Get the best parameters best_params = grid_search.best_params_ print(f\\"Best Parameters: {best_params}\\") # Train the final model with the best parameters best_svm = grid_search.best_estimator_ best_svm.fit(X_train, y_train) # Predict on the test set y_pred = best_svm.predict(X_test) # Generate classification report report = classification_report(y_test, y_pred, output_dict=True) return report"},{"question":"Objective To assess the understanding of the email package\'s policy objects in Python 3.10, students will implement a custom email policy and demonstrate its usage in reading, modifying, and serializing an email message. Problem Statement You are required to implement a custom email policy that enforces the following constraints and behavior: 1. **Max Line Length**: The maximum length of a line should be restricted to 72 characters. 2. **Line Separator**: Use `rn` as the line separator. 3. **Content Transfer Encoding Type**: Allow both `7bit` and `8bit` encodings, with a default to `8bit`. 4. **From Line Mangle**: Enable mangling of lines starting with \\"From\\" in the message body. 5. **Raise on Defect**: Any defects encountered should raise an error. 6. **Verify Generated Headers**: Ensure headers are correctly formatted before writing. After implementing the policy, you will use it to: 1. Read an email message from a given file. 2. Add a custom header to the email message. 3. Serialize the modified email message to a new file using the custom policy. Implementation Details 1. Implement a class `CustomEmailPolicy` that inherits from `email.policy.EmailPolicy`. 2. Within `CustomEmailPolicy`, override the necessary properties to enforce the constraints specified. 3. Write a function `modify_and_serialize_email(input_file: str, output_file: str, custom_policy: email.policy.Policy)` that: - Reads the email message from `input_file` using the `custom_policy`. - Adds a header `\\"X-Custom-Header: CustomValue\\"` to the email message. - Serializes the modified email message to `output_file` using the `custom_policy`. Here is the template to get you started: ```python import os from email import message_from_binary_file from email.generator import BytesGenerator from email.policy import EmailPolicy class CustomEmailPolicy(EmailPolicy): def __init__(self, **kw): super().__init__( max_line_length=72, linesep=\'rn\', cte_type=\'8bit\', raise_on_defect=True, mangle_from_=True, verify_generated_headers=True, **kw ) def modify_and_serialize_email(input_file: str, output_file: str, custom_policy: EmailPolicy): with open(input_file, \'rb\') as f: msg = message_from_binary_file(f, policy=custom_policy) # Add custom header msg[\'X-Custom-Header\'] = \'CustomValue\' with open(output_file, \'wb\') as f: generator = BytesGenerator(f, policy=custom_policy) generator.flatten(msg) # Example usage input_file = \'path/to/input_email.eml\' output_file = \'path/to/output_email.eml\' custom_policy = CustomEmailPolicy() modify_and_serialize_email(input_file, output_file, custom_policy) ``` Constraints - The input email file (`input_file`) will be a valid MIME email message. - The output email file (`output_file`) must contain the modified email message. - Ensure that your implementation adheres to the specified constraints and behavior. Evaluation Criteria - Correct implementation of the custom email policy with specified constraints. - Proper reading, modifying, and serializing of the email message using the custom policy. - Handling any errors or defects as per the policy.","solution":"import os from email import message_from_binary_file, policy from email.generator import BytesGenerator from email.policy import EmailPolicy class CustomEmailPolicy(EmailPolicy): def __init__(self, **kw): super().__init__( max_line_length=72, linesep=\'rn\', cte_type=\'8bit\', raise_on_defect=True, mangle_from_=True, **kw ) def modify_and_serialize_email(input_file: str, output_file: str, custom_policy: EmailPolicy): with open(input_file, \'rb\') as f: msg = message_from_binary_file(f, policy=custom_policy) # Add custom header msg[\'X-Custom-Header\'] = \'CustomValue\' with open(output_file, \'wb\') as f: generator = BytesGenerator(f, policy=custom_policy) generator.flatten(msg) # Example usage # input_file = \'path/to/input_email.eml\' # output_file = \'path/to/output_email.eml\' # custom_policy = CustomEmailPolicy() # modify_and_serialize_email(input_file, output_file, custom_policy)"},{"question":"Objective To assess your understanding of using the `sklearn.datasets` module to load, preprocess, and handle datasets, particularly from OpenML, and ensuring correct data types and management. Question You are provided with the task to download the \'miceprotein\' dataset from OpenML, perform basic preprocessing, and prepare the data for training a machine learning model. Instructions 1. Using the `fetch_openml` function from `sklearn.datasets`, download the \'miceprotein\' dataset of version 4 from OpenML. 2. Ensure that the dataset contains no missing values. If you find any missing values, fill them using the median value of the respective feature. 3. Convert categorical features in the dataset to numerical using appropriate sklearn encoders. 4. Split the dataset into training and testing sets with an 80-20 ratio. Use the stratified split based on the target classes. 5. Output the shapes of the training and testing sets. Input and Output Formats - Input: No direct input required (The function should handle downloading the dataset and processing it). - Output: Print the shapes of the resulting training and testing datasets. Constraints - You must use scikit-learn (sklearn) for downloading and preprocessing the data. - Ensure correct handling of categorical data and missing values. Example ```python from sklearn.datasets import fetch_openml from sklearn.model_selection import train_test_split from sklearn.preprocessing import OrdinalEncoder import numpy as np import pandas as pd def preprocess_miceprotein_data(): # Step 1: Fetching the dataset mice = fetch_openml(name=\'miceprotein\', version=4, as_frame=True) # Step 2: Handling missing values data = mice.data data.fillna(data.median(), inplace=True) # Step 3: Converting categorical features to numerical categorical_columns = data.select_dtypes(include=[\'object\']).columns encoder = OrdinalEncoder() data[categorical_columns] = encoder.fit_transform(data[categorical_columns]) # Step 4: Splitting the dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split( data, mice.target, test_size=0.2, stratify=mice.target, random_state=42) # Step 5: Output the shapes print(\'Training data shape:\', X_train.shape, y_train.shape) print(\'Testing data shape:\', X_test.shape, y_test.shape) # Execute the function preprocess_miceprotein_data() ```","solution":"from sklearn.datasets import fetch_openml from sklearn.model_selection import train_test_split from sklearn.preprocessing import OrdinalEncoder import numpy as np import pandas as pd def preprocess_miceprotein_data(): # Step 1: Fetching the dataset mice = fetch_openml(name=\'miceprotein\', version=4, as_frame=True) # Step 2: Handling missing values data = mice.data data.fillna(data.median(), inplace=True) # Step 3: Converting categorical features to numerical categorical_columns = data.select_dtypes(include=[\'object\']).columns encoder = OrdinalEncoder() data[categorical_columns] = encoder.fit_transform(data[categorical_columns]) # Step 4: Splitting the dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split( data, mice.target, test_size=0.2, stratify=mice.target, random_state=42) # Step 5: Output the shapes return X_train.shape, X_test.shape, y_train.shape, y_test.shape # Execute the function and print the results train_shape, test_shape, y_train_shape, y_test_shape = preprocess_miceprotein_data() print(f\'Training data shape: {train_shape}\') print(f\'Testing data shape: {test_shape}\')"},{"question":"**Problem Statement:** You are tasked with writing a Python function that reads data from a binary file, processes it by converting it to a textual representation, and writes the result to a text file. The function should handle the appropriate encoding specified by the user and use buffered I/O for optimal performance. **Function Signature:** ```python def process_binary_to_text(input_binary_file: str, output_text_file: str, encoding: str) -> None: pass ``` **Parameters:** - `input_binary_file` (str): The path to the input binary file. - `output_text_file` (str): The path to the output text file. - `encoding` (str): The encoding to use for converting the binary data to text. **Instructions:** 1. Open the input binary file using the `io` module and read its contents. 2. Convert the binary data to text using the specified encoding. 3. Write the text data to the output text file using buffered I/O for efficient performance. 4. Ensure any errors in encoding or file operations are appropriately handled. 5. Validate that the output text file contains the expected textual representation of the binary data. **Constraints:** - Do not use any external libraries except Python\'s standard library. - The function should handle files of arbitrary sizes efficiently. **Example:** ```python # Assume `input_binary_file` contains binary data representing the UTF-8 encoded string \\"Hello, World!\\" input_binary_file = \'example.bin\' output_text_file = \'output.txt\' encoding = \'utf-8\' process_binary_to_text(input_binary_file, output_text_file, encoding) # After execution, `output.txt` should contain \\"Hello, World!\\" ``` **Notes:** - You can use `io.BytesIO` for creating in-memory binary streams and `io.StringIO` for text streams if needed for testing. - Make sure to handle any potential exceptions, such as `FileNotFoundError` for missing input files or `UnicodeDecodeError` for encoding issues. **Performance Considerations:** - Use buffered I/O classes (`BufferedReader`, `BufferedWriter`) to optimize the performance when dealing with files. - Ensure that the function works efficiently for large files by reading and writing data in chunks instead of loading the entire file into memory. Good luck, and happy coding!","solution":"import io def process_binary_to_text(input_binary_file: str, output_text_file: str, encoding: str) -> None: Reads data from a binary file, converts it to a textual representation using the specified encoding, and writes the result to a text file. Args: input_binary_file (str): The path to the input binary file. output_text_file (str): The path to the output text file. encoding (str): The encoding to use for converting the binary data to text. try: with open(input_binary_file, \'rb\') as f_in: with open(output_text_file, \'w\', encoding=encoding) as f_out: buffer_size = 1024 while chunk := f_in.read(buffer_size): text_data = chunk.decode(encoding) f_out.write(text_data) except FileNotFoundError as e: print(f\\"File not found: {e.filename}\\") except UnicodeDecodeError as e: print(f\\"Encoding error: {e}\\") except Exception as e: print(f\\"An unexpected error occurred: {e}\\")"},{"question":"# Python Coding Assessment: Py_Compile Module **Objective**: Implement a custom Python function that utilizes the `py_compile` module to compile a list of Python source files and handle errors efficiently. Additionally, create helper functions to enhance error reporting and logging. # Instructions You are required to implement two functions: 1. `compile_python_files(files: List[str], output_dir: str = None, optimize: int = 0, quiet: int = 0) -> List[str]` 2. `log_compile_errors(errors: List[Exception], log_file: str) -> None` Function 1: `compile_python_files` **Description**: This function takes a list of Python source files and compiles them using the `py_compile` module. If an error occurs during the compilation of any file, it should be handled gracefully, and all errors should be collected for further logging. - **Input**: - `files` (List[str]): A list of paths to Python source files to be compiled. - `output_dir` (str | None): The directory where the compiled byte-code files should be saved. If `None`, the default location is used. - `optimize` (int): Optimization level to be used for the compilation. - `quiet` (int): Specifies the error handling behavior (0, 1, or 2). - **Output**: - Returns a list of paths to the successfully compiled byte-code files. - **Constraints**: - Raise appropriate exceptions for invalid file paths or permissions. - Ensure the function is efficient concerning time and space complexities. - **Behavior**: - If a file fails to compile, collect the exception and proceed with the remaining files. - Use a helper function to log the collected errors. Function 2: `log_compile_errors` **Description**: This function takes a list of exceptions and logs them to a specified file. - **Input**: - `errors` (List[Exception]): A list of exceptions raised during the compilation process. - `log_file` (str): The file path where the error log will be saved. - **Output**: - None - **Constraints**: - Log the errors in an organized format with timestamps and detailed information. # Example ```python files_to_compile = [\'script1.py\', \'script2.py\', \'script3.py\'] output_directory = \'/compiled_files\' compiled_files = compile_python_files(files_to_compile, output_directory, optimize=2, quiet=1) print(\\"Compiled Files:\\", compiled_files) # Assuming some errors occurred during the compilation errors = [PyCompileError(\\"error compiling script2.py\\"), PyCompileError(\\"error compiling script3.py\\")] log_compile_errors(errors, \'/path/to/logfile.log\') ``` # Notes - Ensure your solution properly handles files that may not exist or are inaccessible. - Your solution must efficiently compile the files and capture errors without halting the script unexpectedly. # Acceptance Criteria - The `compile_python_files` function should compile all given source files, handle errors gracefully, and return paths to successfully compiled files. - The `log_compile_errors` function should log all errors in a readable and structured format. - The combined functionality should demonstrate an understanding of error handling, file operations, and module usage in Python.","solution":"import py_compile import os from datetime import datetime from typing import List def compile_python_files(files: List[str], output_dir: str = None, optimize: int = 0, quiet: int = 0) -> List[str]: compiled_files = [] errors = [] for file in files: try: compiled_file = py_compile.compile(file, cfile=None, dfile=None, doraise=True, optimize=optimize, quiet=quiet) if output_dir and compiled_file: # Move the compiled file to output_dir if necessary base_name = os.path.basename(compiled_file) target_path = os.path.join(output_dir, base_name) os.replace(compiled_file, target_path) compiled_files.append(target_path) else: compiled_files.append(compiled_file) except Exception as e: errors.append(e) if errors: log_compile_errors(errors, \\"compile_errors.log\\") # Log errors to a default log file return compiled_files def log_compile_errors(errors: List[Exception], log_file: str) -> None: with open(log_file, \'a\') as log: for error in errors: log.write(f\\"{datetime.now()} - {str(error)}n\\")"},{"question":"**Title:** Implementing a Simple Task Scheduler with `datetime` and `collections` **Objective:** To assess the student\'s understanding of date and time manipulation using the `datetime` module, and their ability to manage and operate on a collection of tasks using the `collections` module. **Question:** You are required to implement a simplified task scheduler in Python. The scheduler should allow adding tasks with specified execution times, listing all scheduled tasks, and deleting tasks based on their execution times. # Requirements: 1. **Task Class:** - Create a class `Task` with the following properties: - `name` (str): Name of the task. - `execution_time` (datetime): The time at which the task is scheduled to execute. - Implement the `__str__` method to return a string representation of the task in the format: \\"Task(name: {name}, execution_time: {execution_time})\\". 2. **Scheduler Class:** - Create a class `Scheduler` with the following methods: - `add_task(self, task: Task) -> None`: Adds a new task to the scheduler. - `remove_task(self, task: Task) -> bool`: Removes a task from the scheduler. Returns `True` if the task was successfully removed, `False` if the task was not found. - `list_tasks(self) -> list`: Returns a list of all scheduled tasks, sorted by execution time. - `next_task(self) -> Task`: Returns the next task to be executed (the task with the nearest execution time). If no tasks are available, return `None`. # Input/Output: - **add_task**: Accepts a `Task` object as input and saves it to the scheduler. - **remove_task**: Accepts a `Task` object as input, removes it from the scheduler, and returns a boolean result indicating success or failure. - **list_tasks**: Returns a list of `Task` objects, sorted by their `execution_time`. - **next_task**: Returns the next `Task` to be executed based on the current time. # Constraints: - Execution times for tasks are unique. - The system should handle at least 1000 tasks efficiently. # Examples: ```python from datetime import datetime from collections import namedtuple # Define the Task and Scheduler classes above # Example usage: task1 = Task(name=\\"Task1\\", execution_time=datetime(2023, 10, 1, 14, 30)) task2 = Task(name=\\"Task2\\", execution_time=datetime(2023, 10, 1, 12, 45)) task3 = Task(name=\\"Task3\\", execution_time=datetime(2023, 10, 1, 16, 00)) scheduler = Scheduler() scheduler.add_task(task1) scheduler.add_task(task2) scheduler.add_task(task3) print([str(task) for task in scheduler.list_tasks()]) # Output: [ # \\"Task(name: Task2, execution_time: 2023-10-01 12:45:00)\\", # \\"Task(name: Task1, execution_time: 2023-10-01 14:30:00)\\", # \\"Task(name: Task3, execution_time: 2023-10-01 16:00:00)\\" # ] print(scheduler.next_task()) # Output: Task(name: Task2, execution_time: 2023-10-01 12:45:00) scheduler.remove_task(task2) print([str(task) for task in scheduler.list_tasks()]) # Output: [ # \\"Task(name: Task1, execution_time: 2023-10-01 14:30:00)\\", # \\"Task(name: Task3, execution_time: 2023-10-01 16:00:00)\\" # ] ``` Note: Assume that the current time is within the scheduled task times for the `next_task` method to return meaningful results. **Evaluation Criteria:** - Correct implementation of the `Task` and `Scheduler` classes. - Proper usage of the `datetime` module for date and time operations. - Efficient management of tasks using an appropriate data structure from the `collections` module. - Code readability and adherence to Python coding standards.","solution":"from datetime import datetime from collections import defaultdict class Task: def __init__(self, name: str, execution_time: datetime): self.name = name self.execution_time = execution_time def __str__(self): return f\\"Task(name: {self.name}, execution_time: {self.execution_time})\\" class Scheduler: def __init__(self): self.tasks = [] def add_task(self, task: Task) -> None: self.tasks.append(task) self.tasks.sort(key=lambda x: x.execution_time) def remove_task(self, task: Task) -> bool: for t in self.tasks: if t.execution_time == task.execution_time and t.name == task.name: self.tasks.remove(t) return True return False def list_tasks(self) -> list: return sorted(self.tasks, key=lambda x: x.execution_time) def next_task(self) -> Task: if self.tasks: return self.tasks[0] return None"},{"question":"**Context:** Your team is developing a Python-based web application using CGI scripts. Proper error handling and logging are crucial for debugging and maintaining the application. You are tasked with implementing a flexible error-handling mechanism using the `cgitb` module to manage exceptions, provide detailed traceback information, and log errors for later analysis. # Task: Write a Python function `setup_error_handling(display: bool, logdir: str, context: int, format: str) -> None` that configures the `cgitb` module to handle tracebacks according to the specified parameters: - `display` (bool): If `True`, display the traceback in the browser; otherwise, suppress it. - `logdir` (str): Directory where log files will be saved. If empty, do not write log files. - `context` (int): Number of lines of context to display around the current line of source code in the traceback. - `format` (str): Output format for the traceback, either \'html\' or \'text\'. Additionally, implement a function `handle_exception()` that will: 1. Generate a sample exception to see the error handling in action. 2. Use `cgitb.handler()` to manage the exception. **Constraints:** - The `logdir` (if specified) must exist, and the program should verify its presence before enabling logging. - Context lines must be a positive integer. - Format should be either \'html\' or \'text\'. If an invalid format is given, default to \'html\'. **Example:** ```python import os def setup_error_handling(display, logdir, context, format): import cgitb # Ensure \'logdir\' is either an existing directory or None if logdir and not os.path.isdir(logdir): raise ValueError(\\"Log directory does not exist.\\") # Validate context is a positive integer if context <= 0: raise ValueError(\\"Context must be a positive integer.\\") # Validate format if format not in [\'html\', \'text\']: format = \'html\' # Enable cgitb with the provided parameters cgitb.enable(display=display, logdir=logdir, context=context, format=format) def handle_exception(): import cgitb import sys try: # Intentional error for demonstration 1 / 0 except Exception: cgitb.handler() ``` **Output:** - Depending on the configuration passed to `setup_error_handling`, the traceback from the `handle_exception` will either be displayed in the browser, logged to a file, or both. **Testing your implementation:** To test, call `setup_error_handling` with various parameter combinations and follow it by `handle_exception` to observe the results. You may create temporary directories for logging purposes during testing. This task will evaluate your understanding of exception handling, file operations, and the `cgitb` module in Python.","solution":"import os import cgitb def setup_error_handling(display: bool, logdir: str, context: int, format: str) -> None: Configures the cgitb module to handle tracebacks according to the specified parameters. Parameters: display (bool): If True, display the traceback in the browser; otherwise, suppress it. logdir (str): Directory where log files will be saved. If empty, do not write log files. context (int): Number of lines of context to display around the current line of source code in the traceback. format (str): Output format for the traceback, either \'html\' or \'text\'. # Ensure \'logdir\' is either an existing directory or an empty string for no logging if logdir and not os.path.isdir(logdir): raise ValueError(\\"Log directory does not exist.\\") # Validate context is a positive integer if context <= 0: raise ValueError(\\"Context must be a positive integer.\\") # Validate format if format not in [\'html\', \'text\']: format = \'html\' # Enable cgitb with the provided parameters cgitb.enable(display=display, logdir=logdir, context=context, format=format) def handle_exception(): Generates a sample exception to see the error handling in action. try: # Intentional error for demonstration raise ValueError(\\"This is a test error.\\") except Exception: cgitb.handler()"},{"question":"You are a data analyst working on analyzing the famous \\"Diamonds\\" dataset using the seaborn library. Your task involves creating and customizing various plots to present your findings effectively. Please complete the tasks below using the seaborn.objects module. Task 1: Loading Data and Basic Plotting 1. Load the \\"Diamonds\\" dataset from seaborn. 2. Create a plot object that visualizes the distribution of diamond prices using a histogram. Use a logarithmic scale for the x-axis. **Function Signature:** ```python def plot_diamond_prices(): import seaborn.objects as so from seaborn import load_dataset diamonds = load_dataset(\\"diamonds\\") p = so.Plot(diamonds, \\"price\\").scale(x=\\"log\\") p.add(so.Bars(), so.Hist()) return p ``` Task 2: Customizing and Stacking 1. Customize the histogram to show different colors for different cuts of the diamonds. 2. Resolve the overlapping by stacking the bars. **Function Signature:** ```python def plot_customized_stacked_histogram(): import seaborn.objects as so from seaborn import load_dataset diamonds = load_dataset(\\"diamonds\\") p = so.Plot(diamonds, \\"price\\").scale(x=\\"log\\") p.add(so.Bars(), so.Hist(), so.Stack(), color=\\"cut\\") return p ``` Task 3: Advanced Customization 1. Use the transformed plot from Task 2. 2. Further customize the plot by setting the edge width of the bars to `0` and varying the transparency based on the `clarity` of the diamonds. **Function Signature:** ```python def plot_advanced_customized_histogram(): import seaborn.objects as so from seaborn import load_dataset diamonds = load_dataset(\\"diamonds\\") p = so.Pplot(diamonds, \\"price\\").scale(x=\\"log\\") p.add(so.Bars(edgewidth=0), so.Hist(), so.Stack(), alpha=\\"clarity\\") return p ``` Task 4: Unfilled and Narrow Bars 1. Use the plot from Task 2. 2. Draw unfilled bars with a custom edge color and narrow the bars for further clarity. **Function Signature:** ```python def plot_unfilled_narrow_histogram(): import seaborn.objects as so from seaborn import load_dataset diamonds = load_dataset(\\"diamonds\\") p = so.Plot(diamonds, \\"price\\").scale(x=\\"log\\") p.add(so.Bars(fill=False, edgecolor=\\"C0\\", edgewidth=1.5), so.Hist()) hist = so.Hist(binwidth=.075, binrange=(2, 5)) p = p.add(so.Bars(), hist).add(so.Bars(color=\\".9\\", width=.5), hist, data=diamonds.query(\\"cut == \'Ideal\'\\")) return p ``` # Submission Write a Python script or Jupyter notebook that: - Defines the four functions as specified. - Calls and displays the plots from each function. **Constraints:** - Ensure your script does not contain any syntax errors. - Your plots should be clear, and the customization should be visible in the generated plots. **Good luck!**","solution":"import seaborn.objects as so from seaborn import load_dataset def plot_diamond_prices(): Load the Diamonds dataset and create a histogram of diamond prices with a logarithmic scale on the x-axis. diamonds = load_dataset(\\"diamonds\\") p = so.Plot(diamonds, x=\\"price\\").scale(x=\\"log\\") p.add(so.Bars(), so.Hist()) return p def plot_customized_stacked_histogram(): Create a customized histogram to show different colors for different cuts of the diamonds. Resolve overlapping by stacking the bars. diamonds = load_dataset(\\"diamonds\\") p = so.Plot(diamonds, x=\\"price\\", color=\\"cut\\").scale(x=\\"log\\") p.add(so.Bars(), so.Hist(), so.Stack()) return p def plot_advanced_customized_histogram(): Further customize the stacked histogram by setting the edge width of the bars to 0 and varying the transparency based on the clarity of the diamonds. diamonds = load_dataset(\\"diamonds\\") p = so.Plot(diamonds, x=\\"price\\", alpha=\\"clarity\\").scale(x=\\"log\\") p.add(so.Bars(edgewidth=0), so.Hist(), so.Stack()) return p def plot_unfilled_narrow_histogram(): Draw unfilled bars with a custom edge color and narrow the bars for further clarity. diamonds = load_dataset(\\"diamonds\\") p = so.Plot(diamonds, x=\\"price\\").scale(x=\\"log\\") hist = so.Hist(binwidth=.075, binrange=(2, 5)) p.add(so.Bars(fill=False, edgecolor=\\"C0\\", edgewidth=1.5), so.Hist()).add(so.Bars(), hist).add(so.Bars(color=\\".9\\", width=.5), hist, data=diamonds.query(\\"cut == \'Ideal\'\\")) return p"},{"question":"# Custom PyTorch Function for Matrix Trace For this assessment, you are required to implement a custom PyTorch function to perform the trace of a matrix, which is the sum of all the elements along the main diagonal of a square matrix. This custom operation should be differentiable and compatible with PyTorch\'s autograd. # Task 1. Implement a custom `Function` subclass, `MatrixTraceFunction`, which computes the trace of a given square matrix. 2. Include the forward and backward methods in your implementation. 3. Create a wrapper function `matrix_trace` that applies this custom `Function`. 4. Validate your implementation using `torch.autograd.gradcheck`. # Detailed Instructions 1. **Subclass Function and Implement Forward Method** - In the forward pass, compute the trace of the input matrix. - Ensure that the input is a square matrix. 2. **Implement Backward Method** - In the backward pass, ensure correct gradient computation. - Return the gradient with respect to the input matrix. 3. **Create Wrapper Function** - Define a function `matrix_trace(matrix)` to utilize `MatrixTraceFunction`. 4. **Validate Implementation** - Use `torch.autograd.gradcheck` to verify that your backward method correctly computes gradients. # Constraints - You must use PyTorch\'s custom `Function` API. - Ensure that the input matrix is a PyTorch tensor with `requires_grad=True`. - Raise appropriate errors for invalid inputs. # Input Format - A single input tensor of shape `(n, n)` where `n` is a positive integer, representing a square matrix. # Output Format - A scalar tensor containing the trace of the input matrix. # Example ```python import torch from torch.autograd import Function class MatrixTraceFunction(Function): @staticmethod def forward(ctx, matrix): if matrix.shape[0] != matrix.shape[1]: raise ValueError(\\"Input must be a square matrix\\") ctx.save_for_backward(matrix) trace_value = torch.trace(matrix) return trace_value @staticmethod def backward(ctx, grad_output): (matrix,) = ctx.saved_tensors grad_input = grad_output * torch.eye(matrix.size(0), device=matrix.device) return grad_input def matrix_trace(matrix): return MatrixTraceFunction.apply(matrix) # Example Usage matrix = torch.randn(3, 3, dtype=torch.double, requires_grad=True) trace_result = matrix_trace(matrix) trace_result.backward() # Validating Gradient matrix = torch.randn(3, 3, dtype=torch.double, requires_grad=True) assert torch.autograd.gradcheck(matrix_trace, (matrix,)) ``` Create the custom function and ensure proper functionality by validating gradients. This task will test your understanding of creating custom operations, manipulating tensors, and implementing both forward and backward passes in PyTorch.","solution":"import torch from torch.autograd import Function class MatrixTraceFunction(Function): @staticmethod def forward(ctx, matrix): if matrix.shape[0] != matrix.shape[1]: raise ValueError(\\"Input must be a square matrix\\") ctx.save_for_backward(matrix) trace_value = torch.trace(matrix) return trace_value @staticmethod def backward(ctx, grad_output): (matrix,) = ctx.saved_tensors grad_input = grad_output * torch.eye(matrix.size(0), device=matrix.device) return grad_input def matrix_trace(matrix): return MatrixTraceFunction.apply(matrix) # Example usage matrix = torch.randn(3, 3, dtype=torch.double, requires_grad=True) trace_result = matrix_trace(matrix) trace_result.backward()"},{"question":"**Advanced Coding Challenge: Custom Recursive File Search** **Objective**: Implement a custom file search function that mimics and extends the functionality of the `glob.glob()` function with additional features. **Problem Statement**: You are required to implement a function `custom_glob(pattern: str, base_dir: str = \'.\', include_hidden: bool = False, sorted_results: bool = False) -> list:` that searches for files matching the specified `pattern` starting from `base_dir`. Your function should behave similarly to `glob.glob()` but with additional features as described below. **Function Specifications**: 1. **Input Format**: - `pattern` (str): The pattern to match file paths against, supporting Unix shell-style wildcards (`*`, `?`, `[]`). - `base_dir` (str): The base directory to start the search from. Default is the current directory (`.`). - `include_hidden` (bool): Whether to include hidden files (those starting with a dot) in the results. Default is `False`. - `sorted_results` (bool): Whether to return the results sorted alphabetically. Default is `False`. 2. **Output Format**: - A list of strings representing the paths of matched files relative to `base_dir`. **Constraints**: - Do not use `glob.glob()` or `glob.iglob()`. - Handle large directory trees efficiently. - Ensure correct matching of paths, including handling hidden files based on the `include_hidden` parameter. - Provide an option to sort the results with the `sorted_results` parameter. **Performance Requirements**: - Your function should be able to handle large numbers of files and directories without excessive memory usage. - It should perform efficiently for both small and large directory structures. **Example**: ```python # Given a directory structure: # test_dir/ # ├── 1.gif # ├── 2.txt # ├── card.gif # ├── .hidden.gif # └── sub/ # └── 3.txt # Invoking the function: matches = custom_glob(\'*.gif\', base_dir=\'test_dir\', include_hidden=True, sorted_results=True) print(matches) # Output: [\'1.gif\', \'.hidden.gif\', \'card.gif\'] matches = custom_glob(\'*.txt\', base_dir=\'test_dir\', recursive=True) print(matches) # Output: [\'2.txt\', \'sub/3.txt\'] ``` **Notes**: - The implementation should use the `os` and `fnmatch` modules to replicate the functionality. - Think carefully about edge cases, such as empty directories or non-existent paths. **Hint**: - Utilize `os.walk()` for recursive directory traversal. - Use `fnmatch.fnmatch()` to handle pattern matching against filenames. Happy coding!","solution":"import os import fnmatch def custom_glob(pattern: str, base_dir: str = \'.\', include_hidden: bool = False, sorted_results: bool = False) -> list: Searches for files matching the specified pattern starting from base_dir with options to include hidden files and sort results. :param pattern: Pattern to match file paths against. :param base_dir: Base directory to start the search from. Default is current directory. :param include_hidden: Whether to include hidden files in the results. Default is False. :param sorted_results: Whether to return the results sorted alphabetically. Default is False. :return: List of matched file paths relative to base_dir. matches = [] for root, dirs, files in os.walk(base_dir): # If not including hidden files, remove hidden directories from the search if not include_hidden: dirs[:] = [d for d in dirs if not d.startswith(\'.\')] for filename in files: if not include_hidden and filename.startswith(\'.\'): continue if fnmatch.fnmatch(filename, pattern): matches.append(os.path.relpath(os.path.join(root, filename), base_dir)) if sorted_results: matches.sort() return matches"},{"question":"You are tasked with writing a Python script that reads a text file, processes its contents, and writes the results to a new file. The script should work with both Python 2.7 and Python 3. Ensure that your solution adheres to the following requirements: 1. **Reading and Writing Files**: - Use `io.open()` to open the files for reading and writing. - Ensure that the script can handle and correctly differentiate between text and binary data. 2. **String Literals**: - Ensure that all string literals are correctly prefixed with `b` or `u` to denote binary and text data respectively. 3. **Division**: - Handle division operations carefully to avoid differences between integer and float results across Python versions. 4. **Handling Feature Detection**: - Use feature detection to handle differences in importing modules. # Expected Input and Output - **Input**: - A text file containing multiple lines of text. - **Output**: - A new text file where each line from the input file is processed to remove leading and trailing whitespace and then reversed. # Example: - **Input File (input.txt)**: ``` Hello World Python 2 and Python 3 ``` - **Output File (output.txt)**: ``` dlroW olleH 3 nohtyP dna 2 nohtyP ``` # Constraints: - The script should be compatible with both Python 2.7 and Python 3.x. # Guidelines: 1. Use `io.open()` for file operations. 2. Mark all binary literals with a `b` prefix and textual literals with a `u` prefix. 3. Use `from __future__ import division` to handle division cases. 4. Implement feature-detection for imports where necessary. # Solution Template ```python from __future__ import absolute_import, division, print_function import io import sys def process_file(input_path, output_path): try: with io.open(input_path, \'r\', encoding=\'utf-8\') as infile: with io.open(output_path, \'w\', encoding=\'utf-8\') as outfile: for line in infile: processed_line = line.strip()[::-1] outfile.write(processed_line + u\'n\') except Exception as e: print(\\"An error occurred:\\", e) if __name__ == \\"__main__\\": if len(sys.argv) != 3: print(\\"Usage: python script.py <input_file> <output_file>\\") sys.exit(1) input_path = sys.argv[1] output_path = sys.argv[2] process_file(input_path, output_path) ```","solution":"from __future__ import absolute_import, division, print_function import io import sys def process_file(input_path, output_path): try: with io.open(input_path, \'r\', encoding=\'utf-8\') as infile: with io.open(output_path, \'w\', encoding=\'utf-8\') as outfile: for line in infile: processed_line = line.strip()[::-1] # Remove leading/trailing whitespace and reverse the line outfile.write(processed_line + u\'n\') except Exception as e: print(\\"An error occurred:\\", e) if __name__ == \\"__main__\\": if len(sys.argv) != 3: print(\\"Usage: python script.py <input_file> <output_file>\\") sys.exit(1) input_path = sys.argv[1] output_path = sys.argv[2] process_file(input_path, output_path)"},{"question":"Data Preprocessing Using Scikit-learn Objective: The goal of this exercise is to assess your ability to perform data preprocessing using Scikit-learn\'s `sklearn.preprocessing` module. You will need to apply a combination of preprocessing techniques to a provided dataset and implement a pipeline to streamline these operations. Problem Statement: You are given a dataset with both numerical and categorical features. Your task is to perform the following preprocessing steps on the dataset and implement a `Pipeline` to combine these steps. Dataset: ```python import numpy as np import pandas as pd data = { \'Age\': [25, 32, NaN, 45, 23, 36, 29], \'Income\': [50000, 60000, 55000, 45000, 48000, NaN, 52000], \'Gender\': [\'Male\', \'Female\', \'Female\', \'Male\', \'Male\', \'Female\', \'Female\'], \'Occupation\': [\'Engineer\', \'Doctor\', \'Artist\', \'Engineer\', \'Artist\', \'Doctor\', \'Engineer\'] } df = pd.DataFrame(data) ``` Steps to follow: 1. **Standardize the Numerical Features**: Normalize the numerical features `Age` and `Income` using `StandardScaler` so they have a zero mean and unit variance. 2. **Handle Missing Values**: Use an appropriate strategy to handle missing values in the numerical columns. 3. **Scale the Numerical Features to a Range**: Scale the numerical features to the range [0, 1] using `MinMaxScaler`. 4. **Encode Categorical Features**: Use `OneHotEncoder` to encode the categorical features `Gender` and `Occupation`. 5. **Create a Pipeline**: Implement a pipeline to combine all the preprocessing steps. Input: - A DataFrame with columns: `Age`, `Income`, `Gender`, and `Occupation`. Output: - A transformed numpy array after applying all the preprocessing steps. Constraints: - You should not modify the original DataFrame. - Handle the missing values in a way that is appropriate for the given context. - Use Scikit-learn transformers and pipeline for the implementation. Implementation Requirements: - Function Name: `preprocess_data` - Input: `df` (Pandas DataFrame) - Output: Transformed numpy array # Function Signature: ```python def preprocess_data(df: pd.DataFrame) -> np.ndarray: pass ``` # Example: ```python import numpy as np import pandas as pd from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder from sklearn.pipeline import Pipeline from sklearn.compose import ColumnTransformer from sklearn.impute import SimpleImputer data = { \'Age\': [25, 32, np.nan, 45, 23, 36, 29], \'Income\': [50000, 60000, 55000, 45000, 48000, np.nan, 52000], \'Gender\': [\'Male\', \'Female\', \'Female\', \'Male\', \'Male\', \'Female\', \'Female\'], \'Occupation\': [\'Engineer\', \'Doctor\', \'Artist\', \'Engineer\', \'Artist\', \'Doctor\', \'Engineer\'] } df = pd.DataFrame(data) # Your code here transformed_data = preprocess_data(df) print(transformed_data) ``` # Expected Output: The expected output will be a numpy array where the numerical columns have been standardized and scaled, categorical columns have been one-hot encoded, and missing values have been appropriately handled.","solution":"import numpy as np import pandas as pd from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder from sklearn.pipeline import Pipeline from sklearn.compose import ColumnTransformer from sklearn.impute import SimpleImputer def preprocess_data(df: pd.DataFrame) -> np.ndarray: # Define numerical and categorical columns numerical_cols = [\'Age\', \'Income\'] categorical_cols = [\'Gender\', \'Occupation\'] # Preprocessing for numerical data numerical_transformer = Pipeline(steps=[ (\'imputer\', SimpleImputer(strategy=\'mean\')), # Handle missing values (\'scaler\', StandardScaler()), # Standardize to zero mean and unit variance (\'minmax\', MinMaxScaler()) # Scale to range [0, 1] ]) # Preprocessing for categorical data categorical_transformer = Pipeline(steps=[ (\'imputer\', SimpleImputer(strategy=\'most_frequent\')), # Handle missing values (\'onehot\', OneHotEncoder(handle_unknown=\'ignore\')) # One Hot Encode ]) # Combine preprocessors preprocessor = ColumnTransformer( transformers=[ (\'num\', numerical_transformer, numerical_cols), (\'cat\', categorical_transformer, categorical_cols) ] ) # Preprocess the data transformed_data = preprocessor.fit_transform(df) return transformed_data"},{"question":"**Objective:** This task will assess your understanding of PyTorch\'s backend configurations and your ability to use them to optimize the performance of computations. You will need to implement a function that checks the availability of multiple backends and configures optimal settings based on their capabilities. **Task:** Write a function `configure_pytorch_backends()` that performs the following tasks: 1. Checks and prints whether CUDA, cuDNN, and MKL backends are available. 2. If CUDA is available, configure the following settings: - Enable TensorFloat-32 (TF32) on matrix multiplications if available. - Enable reduced precision reductions (both for FP16 and BF16). - Clear the cuFFT plan cache and set its maximum size to 1024. 3. If cuDNN is available, set `benchmark` to `True` and `deterministic` to `False`. 4. If MKL is available, print the version of MKL being used. **Function Signature:** ```python def configure_pytorch_backends(): pass ``` **Expected Outputs:** - Prints indicating the availability of CUDA, cuDNN, and MKL. - Configurations should be set as per the requirements when available. **Example:** ```python configure_pytorch_backends() ``` *Example Output:* ``` CUDA Available: True cuDNN Available: True MKL Available: True TF32 Enabled: True FP16 Reduced Precision Reduction Enabled: True BF16 Reduced Precision Reduction Enabled: True CuFFT plan cache cleared and max size set to 1024 cuDNN benchmark: True cuDNN deterministic: False MKL Version: (version printed here) ``` **Notes:** - Ensure that the function handles cases where one or more of the specified backends might not be available. - Use the provided PyTorch backend functions and attributes to achieve the necessary configurations and checks. - Handle edge cases and potential exceptions gracefully, providing useful output messages where appropriate.","solution":"import torch import torch.backends.cudnn as cudnn def configure_pytorch_backends(): Configures PyTorch backend settings based on the availability of CUDA, cuDNN, and MKL. # Check CUDA availability cuda_available = torch.cuda.is_available() print(f\\"CUDA Available: {cuda_available}\\") if cuda_available: # Enable TensorFloat-32 (TF32) on matrix multiplications if available try: torch.backends.cuda.matmul.allow_tf32 = True print(\\"TF32 Enabled: True\\") except Exception as e: print(f\\"Failed to enable TF32: {e}\\") # Enable reduced precision reductions (both for FP16 and BF16) try: torch.backends.cudnn.allow_fp16_reduced_precision_reduction = True print(\\"FP16 Reduced Precision Reduction Enabled: True\\") except Exception as e: print(f\\"Failed to enable FP16 Reduced Precision Reduction: {e}\\") try: torch.backends.cudnn.allow_bf16_reduced_precision_reduction = True print(\\"BF16 Reduced Precision Reduction Enabled: True\\") except Exception as e: print(f\\"Failed to enable BF16 Reduced Precision Reduction: {e}\\") # Clear the cuFFT plan cache and set its maximum size to 1024 try: torch.cuda.cufft_plan_cache.clear() torch.cuda.cufft_plan_cache.max_size = 1024 print(\\"CuFFT plan cache cleared and max size set to 1024\\") except Exception as e: print(f\\"Failed to configure cuFFT plan cache: {e}\\") # Check cuDNN availability cudnn_available = cudnn.is_available() print(f\\"cuDNN Available: {cudnn_available}\\") if cudnn_available: # Set benchmark to True and deterministic to False try: cudnn.benchmark = True cudnn.deterministic = False print(\\"cuDNN benchmark: True\\") print(\\"cuDNN deterministic: False\\") except Exception as e: print(f\\"Failed to configure cuDNN: {e}\\") # Check MKL availability and print MKL version try: mkl_version = torch._C._get_mkl_version() print(f\\"MKL Available: {bool(mkl_version)}\\") if mkl_version: print(f\\"MKL Version: {mkl_version}\\") except AttributeError: print(\\"MKL Available: False\\")"},{"question":"You are required to create a dynamic class using the utilities provided by the `types` module. This class should have certain attributes and methods, and allow for dynamic updates at runtime. This problem will test your understanding of dynamic type creation as well as the ability to manipulate class attributes and methods dynamically. # Requirements: 1. **Class Creation**: * Define a function `create_dynamic_class` that creates and returns a new class with the following specifications: - The class should be named `DynamicPerson`. - It should have a base class `object`. - It should have an attribute `species` set to `\\"Human\\"`. 2. **Adding Methods Dynamically**: * Add a method `greet` to the class which returns the string `\\"Hello, my name is \\"` followed by the person\'s name. The `name` should be provided dynamically upon instantiation. 3. **Instance Creation**: * Instantiate an object of the class with the name `\\"Alice\\"` and ensure that the `greet` method works correctly. 4. **Dynamic Update**: * Add a new method `update_name` to the class after it has been created. This method should allow updating the name of the person. # Constraints: - The solution must use the `types.new_class` utility for creating the class. - Use the `exec_body` argument to add methods dynamically. - Ensure that updating the class with new methods does not affect previously created instances unless explicitly invoked. # Input Format: - There is no input for the function. The function will be called without any arguments. # Output Format: - Return an instance of the created class with the applied methods. The output instance should show the correct behavior for `greet` and `update_name` methods. # Example: ```python def create_dynamic_class(): # Implementation here dynamic_person_instance = create_dynamic_class() print(dynamic_person_instance.greet()) # Output: \\"Hello, my name is Alice\\" dynamic_person_instance.update_name(\\"Bob\\") print(dynamic_person_instance.greet()) # Output: \\"Hello, my name is Bob\\" ``` # Test your implementation thoroughly to ensure all dynamic behaviors are captured correctly.","solution":"import types def create_dynamic_class(): Create a DynamicPerson class with a method greet and add a method to update the name dynamically. # Define the exec_body function def exec_body(ns): # Set the initial species attribute ns[\'species\'] = \\"Human\\" # Define the __init__ function def __init__(self, name): self.name = name # Add the __init__ function to the namespace ns[\'__init__\'] = __init__ # Define the greet function def greet(self): return f\\"Hello, my name is {self.name}\\" # Add the greet function to the namespace ns[\'greet\'] = greet # Create the DynamicPerson class DynamicPerson = types.new_class(\'DynamicPerson\', (object,), {}, exec_body) # Create an instance of DynamicPerson person = DynamicPerson(\\"Alice\\") # Define and dynamically add the update_name method def update_name(self, new_name): self.name = new_name setattr(DynamicPerson, \'update_name\', update_name) # Return the instance return person"},{"question":"**Question: Fault Handler Implementation and Testing** **Objective:** The objective of this assignment is to test your understanding and ability to implement and utilize Python\'s `faulthandler` module. You are required to write a Python script that demonstrates the usage of the `faulthandler` module to handle and log tracebacks in the event of a segmentation fault. **Task:** 1. **Enable the Fault Handler:** - Write a function `enable_fault_handler(log_file: str) -> None` that enables the fault handler to log all thread tracebacks to a specified log file. The function should take a single parameter: - `log_file` (str): The file path where the tracebacks should be logged. 2. **Trigger a Segmentation Fault:** - Write a function `trigger_segmentation_fault() -> None` that intentionally causes a segmentation fault. (Hint: Use `ctypes.string_at(0)` to access an invalid memory location and cause a segmentation fault.) 3. **Log Traceback on Segmentation Fault:** - In the main block of your script, call `enable_fault_handler` with a specified log file path, and then call `trigger_segmentation_fault`. Ensure that when the script runs and causes a segmentation fault, the traceback is logged to your specified file. **Input and Output:** - The script does not take any input from the user. - The output should be written to the log file specified in the `enable_fault_handler` function. **Constraints:** 1. Use only the functions provided by the `faulthandler` module to perform the task. 2. Ensure that the log file remains open until the script finishes executing or the fault handler is explicitly disabled. **Example Usage:** ```python import faulthandler import ctypes def enable_fault_handler(log_file: str) -> None: with open(log_file, \'w\') as f: faulthandler.enable(file=f, all_threads=True) def trigger_segmentation_fault() -> None: ctypes.string_at(0) if __name__ == \\"__main__\\": enable_fault_handler(\\"traceback.log\\") trigger_segmentation_fault() ``` In the example above: - The `enable_fault_handler` function is called with \\"traceback.log\\" as the log file path. - A segmentation fault is triggered using `trigger_segmentation_fault`. You should verify that \\"traceback.log\\" contains the traceback information for the segmentation fault that occurred. **Note: Testing this script will halt the Python interpreter due to the segmentation fault. Use a safe environment to test and review the created log file for the expected traceback output.**","solution":"import faulthandler import ctypes def enable_fault_handler(log_file: str) -> None: Enables the faulthandler to log all thread tracebacks to the specified log file. Args: log_file (str): The file path where the tracebacks should be logged. with open(log_file, \'w\') as f: faulthandler.enable(file=f, all_threads=True) def trigger_segmentation_fault() -> None: Triggers a segmentation fault intentionally. ctypes.string_at(0) if __name__ == \\"__main__\\": enable_fault_handler(\\"traceback.log\\") trigger_segmentation_fault()"},{"question":"# Unix User and Group Management Problem Statement In Unix-like operating systems, managing users and their associated groups is a fundamental aspect. You are required to write a Python function that uses the `pwd` and `grp` modules to display user and group information. Implement a function `get_user_group_info(user: str) -> dict` that accepts a username and returns a dictionary containing details about the user and their associated groups. Requirements 1. The function should return a dictionary with the following keys: - `\'uid\'`: User ID - `\'gid\'`: Primary Group ID - `\'home\'`: Home Directory - `\'groups\'`: A list of group names the user belongs to, including the primary group. 2. If the user does not exist, raise a `ValueError` with an appropriate message. Constraints - The function should only use the `pwd` and `grp` modules for this implementation. - The solution should handle cases where the user is a member of multiple groups efficiently. Function Signature ```python import pwd import grp def get_user_group_info(user: str) -> dict: pass ``` Example ```python # Assuming there exists a user with the username \\"john\\" who is: # - A member of primary group \\"users\\" # - Also a member of groups \\"developers\\" and \\"admins\\" result = get_user_group_info(\\"john\\") print(result) ``` Expected Output: ```plaintext { \'uid\': 1001, \'gid\': 100, \'home\': \'/home/john\', \'groups\': [\'users\', \'developers\', \'admins\'] } ``` Note - Use the `pwd` module to fetch user details like User ID, Group ID, and Home Directory. - Use the `grp` module to fetch the list of groups the user belongs to.","solution":"import pwd import grp def get_user_group_info(user: str) -> dict: try: user_info = pwd.getpwnam(user) except KeyError: raise ValueError(f\\"User \'{user}\' does not exist\\") uid = user_info.pw_uid gid = user_info.pw_gid home = user_info.pw_dir user_groups = [] for g in grp.getgrall(): if user in g.gr_mem or g.gr_gid == gid: user_groups.append(g.gr_name) return { \'uid\': uid, \'gid\': gid, \'home\': home, \'groups\': user_groups }"},{"question":"You are required to develop a Python program that mimics a simple server-client model using the asyncio event loop methods. The task focuses on your understanding of scheduling callbacks, creating futures/tasks, managing network connections, and handling asynchronous IO. # Problem Statement: 1. **Server Implementation** - Create a server that listens for incoming client connections on a specified port. - Upon receiving a connection, the server should send a welcome message to the client. - The server should handle multiple clients concurrently. - For each connected client, the server should repeatedly read any incoming message and echo it back to the client, prefixed with \\"Echo: \\". 2. **Client Implementation** - Create a client that connects to the server. - After connecting, the client should send a message: \\"Hello, server!\\" to the server. - The client should then read the echoed message from the server and display it. # Requirements: 1. Use `asyncio`\'s low-level API (focus on `asyncio.get_event_loop()`, `loop.run_until_complete()`, `loop.create_server()`, `loop.create_connection()`, `add_reader()`, etc.). 2. Implement proper stopping and closing of the event loop. 3. Ensure that the server handles multiple clients concurrently. 4. Consider edge cases such as handling client disconnections. # Inputs: - **Server** - A port number on which the server listens. - **Client** - The server\'s address and port number to which it connects. # Outputs: - Server should log connection events and echoed messages. - Client should print the message received from the server. # Constraints: - The port number should be a valid integer between 1024 and 65535. - The server should handle a maximum of 5 concurrent clients. # Performance: - Ensure non-blocking IO is effectively used to handle multiple clients concurrently. # Example: 1. **Server** - Start the server on port `8888`. 2. **Clients** - Connect to the server at port `8888`. - Send a message to the server: \\"Hello, server!\\". - Receive and print the response: \\"Echo: Hello, server!\\". ```python import asyncio async def handle_client(reader, writer): client_addr = writer.get_extra_info(\'peername\') print(f\\"Connected to {client_addr}\\") writer.write(\\"Welcome to the echo server!n\\".encode()) await writer.drain() while True: data = await reader.read(100) if not data: break message = data.decode() print(f\\"Received {message} from {client_addr}\\") writer.write(f\\"Echo: {message}\\".encode()) await writer.drain() print(f\\"Connection from {client_addr} closed\\") writer.close() await writer.wait_closed() async def start_server(): server = await asyncio.start_server(handle_client, \'127.0.0.1\', 8888) addr = server.sockets[0].getsockname() print(f\'Serving on {addr}\') async with server: await server.serve_forever() async def client_message(): reader, writer = await asyncio.open_connection(\'127.0.0.1\', 8888) writer.write(\\"Hello, server!n\\".encode()) await writer.drain() data = await reader.read(100) print(f\\"Received: {data.decode()}\\") print(\\"Close the connection\\") writer.close() await writer.wait_closed() if __name__ == \'__main__\': loop = asyncio.get_event_loop() server_task = loop.create_task(start_server()) client_task = loop.create_task(client_message()) try: loop.run_until_complete(asyncio.gather(server_task, client_task)) except KeyboardInterrupt: pass finally: loop.run_until_complete(loop.shutdown_asyncgens()) loop.close() ``` # Notes: - Ensure to include appropriate exception handling mechanisms. - Make sure that the event loop and all transports are properly closed at the end of the program.","solution":"import asyncio async def handle_client(reader, writer): client_addr = writer.get_extra_info(\'peername\') print(f\\"Connected to {client_addr}\\") writer.write(\\"Welcome to the echo server!n\\".encode()) await writer.drain() while True: data = await reader.read(100) if not data: break message = data.decode() print(f\\"Received {message.strip()} from {client_addr}\\") writer.write(f\\"Echo: {message}\\".encode()) await writer.drain() print(f\\"Connection from {client_addr} closed\\") writer.close() await writer.wait_closed() async def start_server(): server = await asyncio.start_server(handle_client, \'127.0.0.1\', 8888) addr = server.sockets[0].getsockname() print(f\'Serving on {addr}\') async with server: await server.serve_forever() async def client_message(): reader, writer = await asyncio.open_connection(\'127.0.0.1\', 8888) writer.write(\\"Hello, server!n\\".encode()) await writer.drain() data = await reader.read(100) print(f\\"Received: {data.decode().strip()}\\") print(\\"Close the connection\\") writer.close() await writer.wait_closed() def run_server_and_client(): loop = asyncio.get_event_loop() server_task = loop.create_task(start_server()) client_task = loop.create_task(client_message()) try: loop.run_until_complete(asyncio.gather(server_task, client_task)) except KeyboardInterrupt: pass finally: loop.run_until_complete(loop.shutdown_asyncgens()) loop.close()"},{"question":"# Question: Implement Custom Logging with RotatingFileHandler **Objective:** Utilize Python\'s `logging.handlers.RotatingFileHandler` to implement a logging mechanism that saves logs to a file, rotating the file when it reaches a specified size. **Description:** Write a Python function `setup_custom_logger(name: str, file_path: str, max_bytes: int, backup_count: int) -> logging.Logger` that sets up a logger which logs messages to a file. This file should rotate (i.e., create a new file) when it reaches a certain size (`max_bytes`). The function should also handle retaining a specified number of backup files (`backup_count`). **Details:** - The logger should use `logging.handlers.RotatingFileHandler`. - The logger should log DEBUG and higher level messages. - The log file should follow the naming pattern: `file_path`, `file_path.1`, `file_path.2`, ..., `file_path.backup_count`. **Function Signature:** ```python import logging from logging.handlers import RotatingFileHandler def setup_custom_logger(name: str, file_path: str, max_bytes: int, backup_count: int) -> logging.Logger: pass ``` **Input:** - `name` (str): The name of the logger. - `file_path` (str): The path to the log file. - `max_bytes` (int): Maximum file size (in bytes) before rotating. - `backup_count` (int): Number of backup files to keep. **Output:** - A configured `logging.Logger` instance. **Constraints:** - `max_bytes` should be a positive integer. - `backup_count` should be a positive integer. **Example Usage:** ```python # Example usage logger = setup_custom_logger(\\"my_logger\\", \\"app.log\\", 1024, 5) logger.debug(\\"This is a debug message\\") logger.info(\\"This is an info message\\") logger.error(\\"This is an error message\\") ``` **Additional Notes:** - Ensure your function handles appropriate configuration and file rotation. - Include comments within your code to explain your logic. - You may need to refer to the Python logging documentation for advanced setups and troubleshooting.","solution":"import logging from logging.handlers import RotatingFileHandler def setup_custom_logger(name: str, file_path: str, max_bytes: int, backup_count: int) -> logging.Logger: Sets up a custom logger with RotatingFileHandler that rotates logs when file reaches a certain size. :param name: str: Name of the logger. :param file_path: str: Path to the log file. :param max_bytes: int: Maximum file size in bytes before rotating. :param backup_count: int: Number of backup files to keep. :return: logging.Logger: Configured logger instance. if max_bytes <= 0 or backup_count < 0: raise ValueError(\\"max_bytes must be positive and backup_count cannot be negative\\") # Create a logger with the specified name logger = logging.getLogger(name) logger.setLevel(logging.DEBUG) # Create a rotating file handler handler = RotatingFileHandler(file_path, maxBytes=max_bytes, backupCount=backup_count) handler.setLevel(logging.DEBUG) # Create a formatter and set it for the handler formatter = logging.Formatter(\'%(asctime)s - %(name)s - %(levelname)s - %(message)s\') handler.setFormatter(formatter) # Add the handler to the logger logger.addHandler(handler) return logger"},{"question":"# Out-of-Core Learning System Implementation Objective: Implement an out-of-core learning system using Scikit-learn to classify text documents. Your task is to design a system that processes data in mini-batches, extracts features using vectorization, and trains an incremental classifier. Instructions: 1. **Streaming Instances**: Implement a data generator named `data_stream` that takes a list of documents and yields them in mini-batches of a specified size. ```python def data_stream(documents, batch_size): Generator that yields mini-batches of documents. Parameters: - documents (list): List of text documents. - batch_size (int): Size of mini-batches. Yields: - List of documents in the current mini-batch. pass ``` 2. **Feature Extraction**: Use `HashingVectorizer` from `sklearn.feature_extraction.text` to transform the documents into feature vectors. 3. **Incremental Learning**: Use `SGDClassifier` from `sklearn.linear_model` to perform incremental learning. Initialize the classifier before processing the mini-batches. 4. **Putting It All Together**: - Stream the data using the `data_stream` generator. - Extract features using `HashingVectorizer`. - Perform incremental learning using `SGDClassifier`. - Assume that the labels for the documents are available in a corresponding list `labels`. 5. **Function Implementation**: Implement a function `train_out_of_core_model(documents, labels, classes, batch_size=100)` that trains an out-of-core learning model and returns the trained classifier. ```python from sklearn.feature_extraction.text import HashingVectorizer from sklearn.linear_model import SGDClassifier import numpy as np import itertools def data_stream(documents, batch_size): Generator that yields mini-batches of documents. Parameters: - documents (list): List of text documents. - batch_size (int): Size of mini-batches. Yields: - List of documents in the current mini-batch. for i in range(0, len(documents), batch_size): yield documents[i:i + batch_size] def train_out_of_core_model(documents, labels, classes, batch_size=100): Train an out-of-core learning model. Parameters: - documents (list): List of text documents for training. - labels (list): List of corresponding labels. - classes (list): List of all possible classes. - batch_size (int): Size of mini-batches. Default is 100. Returns: - Trained SGDClassifier instance. vectorizer = HashingVectorizer() classifier = SGDClassifier() # Stream data and perform incremental learning for batch_docs, batch_labels in zip(data_stream(documents, batch_size), data_stream(labels, batch_size)): X_batch = vectorizer.transform(batch_docs) y_batch = np.array(batch_labels) classifier.partial_fit(X_batch, y_batch, classes=classes) return classifier ``` Constraints: - The `documents`, `labels`, and `classes` parameters will be lists containing text data, corresponding labels, and all possible classes respectively. - The `batch_size` will be an integer specifying the number of documents in each mini-batch. Example Usage: ```python documents = [\\"text document one\\", \\"text document two\\", \\"text document three\\", ... ] labels = [0, 1, 0, ...] classes = [0, 1] model = train_out_of_core_model(documents, labels, classes, batch_size=2) ``` This problem will test your ability to handle large-scale data by implementing out-of-core learning using Scikit-learn\'s incremental learning capabilities. Make sure your implementation is efficient and can process data in chunks without loading everything into memory at once.","solution":"from sklearn.feature_extraction.text import HashingVectorizer from sklearn.linear_model import SGDClassifier import numpy as np def data_stream(documents, batch_size): Generator that yields mini-batches of documents. Parameters: - documents (list): List of text documents. - batch_size (int): Size of mini-batches. Yields: - List of documents in the current mini-batch. for i in range(0, len(documents), batch_size): yield documents[i:i + batch_size] def train_out_of_core_model(documents, labels, classes, batch_size=100): Train an out-of-core learning model. Parameters: - documents (list): List of text documents for training. - labels (list): List of corresponding labels. - classes (list): List of all possible classes. - batch_size (int): Size of mini-batches. Default is 100. Returns: - Trained SGDClassifier instance. vectorizer = HashingVectorizer() classifier = SGDClassifier() # Stream data and perform incremental learning for batch_docs, batch_labels in zip(data_stream(documents, batch_size), data_stream(labels, batch_size)): X_batch = vectorizer.transform(batch_docs) y_batch = np.array(batch_labels) classifier.partial_fit(X_batch, y_batch, classes=classes) return classifier"},{"question":"Objective: You are required to implement an asynchronous class `AsyncResourcePool` that manages a pool of resources. The pool should handle multiple asynchronous coroutines requesting access to a limited number of resources. Requirements: 1. **Initialization**: - The `AsyncResourcePool` should be initialized with a specific number of resources. - Use `asyncio.Semaphore` to manage the pool of resources. 2. **Methods**: - `async def acquire(self)`: This coroutine method should wait until a resource is available and then provide exclusive access to it. - `def release(self)`: This method should release a previously acquired resource back to the pool. 3. **Usage**: - The `AsyncResourcePool` should be used within an `async with` statement for acquiring resources. Input: - The class takes an integer `resource_count` during initialization that specifies the number of resources in the pool. Output: - The `acquire` method should return `True` when a resource is successfully acquired. - The `release` method does not need to return a value. Constraints: - The pool should not allow over-release of resources (i.e., releasing more resources than acquired). - Proper error handling should be implemented if release is called without acquiring a resource first. Example: ```python import asyncio class AsyncResourcePool: def __init__(self, resource_count): self._semaphore = asyncio.BoundedSemaphore(value=resource_count) async def acquire(self): await self._semaphore.acquire() return True def release(self): self._semaphore.release() # Example usage async def worker(name, pool): async with pool: print(f\'{name} acquired resource\') await asyncio.sleep(1) print(f\'{name} releasing resource\') async def main(): pool = AsyncResourcePool(2) await asyncio.gather(worker(\'worker-1\', pool), worker(\'worker-2\', pool), worker(\'worker-3\', pool)) asyncio.run(main()) ``` In the example, `pool` is an instance of `AsyncResourcePool` with 2 resources. Three workers try to acquire resources, but since there are only two resources, one worker will wait until a resource is released. Complete the implementation of the `AsyncResourcePool` class.","solution":"import asyncio class AsyncResourcePool: def __init__(self, resource_count): self._resource_count = resource_count self._semaphore = asyncio.BoundedSemaphore(value=resource_count) async def acquire(self): await self._semaphore.acquire() return True def release(self): if self._semaphore._value < self._resource_count: self._semaphore.release() else: raise ValueError(\\"Releasing more resources than acquired.\\")"},{"question":"# Task You are given a 1D signal represented as a PyTorch tensor. Your task is to apply a specified window function from the `torch.signal.windows` submodule to this signal and compute the resultant windowed signal. Specifically, you need to: 1. Normalize the input signal to have zero mean and unit variance. 2. Apply a specified window function to the normalized signal. 3. Return the windowed signal as a PyTorch tensor. # Requirements - Your function should be named `apply_window_function`. - The function should accept the following inputs: - `signal` (torch.Tensor): A 1D PyTorch tensor representing the input signal. - `window_type` (str): A string specifying the type of window function to apply. It can be one of the following: `\\"bartlett\\"`, `\\"blackman\\"`, `\\"cosine\\"`, `\\"exponential\\"`, `\\"gaussian\\"`, `\\"hamming\\"`, `\\"hann\\"`, `\\"kaiser\\"`, or `\\"nuttall\\"`. - `window_args` (dict): A dictionary containing additional arguments required for the specified window function (e.g., `beta` for `kaiser` or `std` for `gaussian`). # Function Signature ```python import torch def apply_window_function(signal: torch.Tensor, window_type: str, window_args: dict) -> torch.Tensor: pass ``` # Constraints - The length of `signal` will be at least 10 and at most 10,000. - The `window_type` will always be a valid window function from the specified window functions. - `window_args` will contain appropriate additional arguments required for the window function. # Example ```python import torch from torch.signal.windows import hann signal = torch.tensor([1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0]) window_type = \\"hann\\" window_args = {} # Normalize the signal mean = signal.mean() std = signal.std() normalized_signal = (signal - mean) / std # Apply the Hann window window = hann(len(signal)) windowed_signal = normalized_signal * window # Expected Output print(apply_window_function(signal, window_type, window_args)) # Output: tensor([-1.5667, -1.2185, -0.8703, -0.5221, -0.1740, 0.1740, 0.5221, 0.8703, 1.2185, 1.5667]) ``` # Notes 1. You should implement the normalization and window application steps inside the `apply_window_function`. 2. You may assume that the required functions and modules from `torch.signal` are available and imported for use in your implementation. 3. Edge cases like very small signals or signals with no variance should be handled gracefully.","solution":"import torch import torch.signal.windows as windows def apply_window_function(signal: torch.Tensor, window_type: str, window_args: dict) -> torch.Tensor: Applies a specified window function to a normalized 1D signal. Parameters: signal (torch.Tensor): A 1D PyTorch tensor representing the input signal. window_type (str): The type of window function to apply. window_args (dict): Additional arguments required for the window function. Returns: torch.Tensor: The windowed signal. # Normalize the input signal to have zero mean and unit variance mean = signal.mean() std = signal.std() normalized_signal = (signal - mean) / std # Apply the specified window function to the normalized signal window_func = getattr(windows, window_type) window = window_func(len(signal), **window_args) windowed_signal = normalized_signal * window return windowed_signal"},{"question":"Objective: Create a Python script that demonstrates your ability to dynamically import and utilize Python modules from a ZIP archive. Problem Statement: You are provided with a ZIP file named `modules.zip` which contains two Python modules `module_a.py` and `module_b.py`. Each module contains a single function: - `module_a.py` contains a function `def greet(name):` that takes a string `name` and returns a string `Hello, {name}!` - `module_b.py` contains a function `def add(x, y):` that takes two integers `x` and `y` and returns their sum. Write a Python function `import_and_use_modules(zip_file: str) -> dict` that: 1. Imports the modules from the provided ZIP archive. 2. Invokes the `greet` function from `module_a` with your name. 3. Invokes the `add` function from `module_b` with the integers 3 and 7. 4. Returns a dictionary with the results of the function calls: - The key `greet_result` should contain the result of the `greet` function. - The key `add_result` should contain the result of the `add` function. Input: - `zip_file` (str): The path to the ZIP file containing the Python modules (`modules.zip`). Output: - A dictionary with the following structure: ```python { \\"greet_result\\": str, \\"add_result\\": int } ``` Constraints: - You are not allowed to modify the ZIP file or the modules within it. - You must use the `zipimport` module to load the modules. Example: Given the ZIP file `modules.zip` with the structure: ``` modules.zip ├── module_a.py └── module_b.py ``` `module_a.py`: ```python def greet(name): return f\\"Hello, {name}!\\" ``` `module_b.py`: ```python def add(x, y): return x + y ``` When running the function `import_and_use_modules(\'modules.zip\')`, the expected output would be: ```python { \\"greet_result\\": \\"Hello, YourName!\\", \\"add_result\\": 10 } ``` Note: Replace `YourName` with your actual name. Instructions: - Write the implementation of the function `import_and_use_modules`. - Upload the `modules.zip` file to the working directory of your script before executing it. This question tests your understanding of Python\'s `zipimport` module and dynamic module loading capabilities.","solution":"import zipimport def import_and_use_modules(zip_file: str) -> dict: # Create an importer object for the provided ZIP file importer = zipimport.zipimporter(zip_file) # Import the modules dynamically from the ZIP file module_a = importer.load_module(\'module_a\') module_b = importer.load_module(\'module_b\') # Use the imported modules to call the functions greet_result = module_a.greet(\\"YourName\\") # Replace with your actual name add_result = module_b.add(3, 7) # Return the results in a dictionary return { \\"greet_result\\": greet_result, \\"add_result\\": add_result }"},{"question":"**Coding Assessment Question:** # Objective: Demonstrate your understanding of the Python `_thread` module by creating a multi-threaded application that processes a list of integers to compute their factorials. The application should synchronize access to a shared resource using locks and handle thread-specific errors appropriately. # Problem Statement: You are tasked with implementing a function `compute_factorials(nums, num_threads)` where: - `nums` (List[int]): A list of integers for which factorials need to be computed. - `num_threads` (int): The number of threads to create for processing the list of integers. Each thread should: 1. Pick a number from the list `nums` and compute its factorial. 2. Store the computed factorials in a thread-safe shared dictionary `factorials`, with the key being the number and the value being its factorial. 3. Synchronize access to the shared dictionary using locks to avoid race conditions. # Constraints: - Ensure that `nums` only contains non-negative integers. - Avoid using the higher-level `threading` module; use only the `_thread` module and its functions. - The solution should create exactly `num_threads` threads, distributing the workload among them. # Requirements: 1. Implement the function `compute_factorials(nums, num_threads)`: - Allocate a lock for synchronizing access to the shared dictionary `factorials`. - Start `num_threads` threads that will compute factorials concurrently. - Use appropriate exception handling for thread-specific errors. - The function should return the dictionary `factorials` containing computed factorials. 2. Optional: You may implement a helper function `_compute_factorial(n)` to compute the factorial of a number `n`. # Input and Output: - **Input:** - `nums = [5, 8, 12]` - `num_threads = 3` - **Output:** - `{5: 120, 8: 40320, 12: 479001600}` # Example: ```python import _thread def compute_factorials(nums, num_threads): factorials = {} lock = _thread.allocate_lock() def _compute_factorial(n): if n == 0: return 1 result = 1 for i in range(1, n + 1): result *= i return result def worker(): while nums: lock.acquire() if not nums: lock.release() break num = nums.pop() lock.release() try: result = _compute_factorial(num) except Exception as e: print(f\\"Error computing factorial of {num}: {e}\\") _thread.exit() lock.acquire() factorials[num] = result lock.release() for _ in range(num_threads): _thread.start_new_thread(worker, ()) # Wait for all threads to finish (not an elegant way, but required as _thread does not provide join functionality) import time time.sleep(2) return factorials ``` **Note:** The threads are joined using sleep which is not an ideal way but is required since `_thread` does not provide a built-in way to join threads. Implement and test your function with given inputs to ensure correctness.","solution":"import _thread import time def compute_factorials(nums, num_threads): Compute the factorials of given numbers using multiple threads. Parameters: nums (List[int]): List of non-negative integers. num_threads (int): Number of threads to use. Returns: Dict[int, int]: A dictionary of numbers and their factorials. factorials = {} lock = _thread.allocate_lock() def _compute_factorial(n): if n == 0: return 1 result = 1 for i in range(1, n + 1): result *= i return result def worker(): while True: lock.acquire() if not nums: lock.release() break num = nums.pop() lock.release() try: result = _compute_factorial(num) except Exception as e: print(f\\"Error computing factorial of {num}: {e}\\") _thread.exit() lock.acquire() factorials[num] = result lock.release() for _ in range(num_threads): _thread.start_new_thread(worker, ()) # Wait for all threads to finish (not an elegant way, but required as _thread does not provide join functionality) while len(factorials) < len(nums): time.sleep(0.1) return factorials"},{"question":"# Advanced Coding Assessment Question Problem Statement You are implementing a persistent storage system for a simple task management application. This system should utilize Python\'s `pickle` module for serialization and deserialization of task objects. Tasks should be serializable, and you will need to implement custom methods to handle complex state and external resources. A task is represented by the following class definition: ```python class Task: def __init__(self, task_id, description, completed=False): self.task_id = task_id self.description = description self.completed = completed ``` The Task Management System stores tasks in an SQLite database and references them with their IDs when pickling. You need to ensure that tasks are reloaded correctly from the database when unpickling. The database schema is: ```sql CREATE TABLE tasks ( id INTEGER PRIMARY KEY, description TEXT, completed INTEGER ); ``` Your task is to: 1. Implement the persistent storage system with custom `Pickler` and `Unpickler` classes that handle Task objects using their IDs. 2. Provide methods to save a Task object to the database and load it back to ensure the Task\'s state is correctly managed across sessions. Task Implementation 1. **Class Definition**: - Implement the `Task` class. 2. **Custom Pickler and Unpickler**: - Define `DBPickler` subclass of `pickle.Pickler` and implement the `persistent_id` method to handle Task references. - Define `DBUnpickler` subclass of `pickle.Unpickler` and implement the `persistent_load` method to load Task objects from the database. 3. **Database Operations**: - Implement methods to save and load `Task` objects to and from the database. Input * You should create a SQLite database named `task_manager.db` and use it for storing tasks. * Example tasks: - `Task(1, \\"Task 1 description\\", False)` - `Task(2, \\"Task 2 description\\", True)` Output * Ensure that saving and loading tasks maintain their original state. Example Code Structure ```python import sqlite3 import pickle class Task: def __init__(self, task_id, description, completed=False): self.task_id = task_id self.description = description self.completed = completed class DBPickler(pickle.Pickler): def persistent_id(self, obj): if isinstance(obj, Task): return (\\"Task\\", obj.task_id) return None class DBUnpickler(pickle.Unpickler): def __init__(self, file, connection): super().__init__(file) self.connection = connection def persistent_load(self, pid): cursor = self.connection.cursor() if pid[0] == \\"Task\\": task_id = pid[1] cursor.execute(\\"SELECT id, description, completed FROM tasks WHERE id=?\\", (task_id,)) row = cursor.fetchone() if row: return Task(row[0], row[1], bool(row[2])) raise pickle.UnpicklingError(\\"Unrecognized persistent id\\") def save_task_to_db(task, connection): cursor = connection.cursor() cursor.execute(\\"INSERT INTO tasks (id, description, completed) VALUES (?, ?, ?)\\", (task.task_id, task.description, int(task.completed))) connection.commit() def load_task_from_db(task_id, connection): cursor = connection.cursor() cursor.execute(\\"SELECT id, description, completed FROM tasks WHERE id=?\\", (task_id,)) row = cursor.fetchone() if row: return Task(row[0], row[1], bool(row[2])) return None def save_task(task, filename): with open(filename, \'wb\') as file: DBPickler(file).dump(task) def load_task(filename, connection): with open(filename, \'rb\') as file: return DBUnpickler(file, connection).load() # Example usage: if __name__ == \\"__main__\\": conn = sqlite3.connect(\'task_manager.db\') # Initialize database and table conn.execute(\'\'\' CREATE TABLE IF NOT EXISTS tasks ( id INTEGER PRIMARY KEY, description TEXT, completed INTEGER )\'\'\') # Create and save tasks task1 = Task(1, \\"Task 1 description\\", False) task2 = Task(2, \\"Task 2 description\\", True) save_task_to_db(task1, conn) save_task_to_db(task2, conn) save_task(task1, \'task1.pkl\') save_task(task2, \'task2.pkl\') # Load saved tasks (simulating different sessions) loaded_task1 = load_task(\'task1.pkl\', conn) loaded_task2 = load_task(\'task2.pkl\', conn) print(loaded_task1.description) # \\"Task 1 description\\" print(loaded_task2.description) # \\"Task 2 description\\" conn.close() ``` Constraints - Focus on ensuring that the process of pickling and unpickling correctly restores Task objects from the database. - Handle error cases appropriately, such as when a Task is not found in the database. Good luck!","solution":"import sqlite3 import pickle class Task: def __init__(self, task_id, description, completed=False): self.task_id = task_id self.description = description self.completed = completed class DBPickler(pickle.Pickler): def persistent_id(self, obj): if isinstance(obj, Task): return (\\"Task\\", obj.task_id) return None class DBUnpickler(pickle.Unpickler): def __init__(self, file, connection): super().__init__(file) self.connection = connection def persistent_load(self, pid): cursor = self.connection.cursor() if pid[0] == \\"Task\\": task_id = pid[1] cursor.execute(\\"SELECT id, description, completed FROM tasks WHERE id=?\\", (task_id,)) row = cursor.fetchone() if row: return Task(row[0], row[1], bool(row[2])) raise pickle.UnpicklingError(\\"Unrecognized persistent id\\") def save_task_to_db(task, connection): cursor = connection.cursor() cursor.execute(\\"INSERT INTO tasks (id, description, completed) VALUES (?, ?, ?)\\" \\"ON CONFLICT(id) DO UPDATE SET description=excluded.description, completed=excluded.completed\\", (task.task_id, task.description, int(task.completed))) connection.commit() def load_task_from_db(task_id, connection): cursor = connection.cursor() cursor.execute(\\"SELECT id, description, completed FROM tasks WHERE id=?\\", (task_id,)) row = cursor.fetchone() if row: return Task(row[0], row[1], bool(row[2])) return None def save_task(task, filename): with open(filename, \'wb\') as file: DBPickler(file).dump(task) def load_task(filename, connection): with open(filename, \'rb\') as file: return DBUnpickler(file, connection).load()"},{"question":"# PyTorch Gradient Check Implementation **Objective:** Implement a custom PyTorch function and use `torch.autograd.gradcheck` and `torch.autograd.gradgradcheck` to verify the correctness of its gradient computations. **Instructions:** 1. **Function Implementation:** - Implement a custom PyTorch function `my_func` that performs operations on its input tensor. This function should be differentiable. - Implement the corresponding PyTorch custom `Function` class that defines `forward` and `backward` methods for `my_func`. 2. **Gradient Check:** - Use `torch.autograd.gradcheck` to verify the gradients computed by `my_func`. - Use `torch.autograd.gradgradcheck` to verify the second-order gradients of `my_func`. **Requirements:** - Your function should take a real-valued tensor as input and return a real-valued tensor as output. - Ensure that all necessary constraints and typical edge cases (like zero, negative, and large values) are handled. - Pass both `gradcheck` and `gradgradcheck` for various test cases. **Steps:** 1. **Custom Function Implementation:** Implement the following function and corresponding backward computation: ```python import torch from torch.autograd import Function class MyFunc(Function): @staticmethod def forward(ctx, input): # Compute any operation, for example, y = x^3 + 4 * x^2 - 7 * x ctx.save_for_backward(input) return input ** 3 + 4 * input ** 2 - 7 * input @staticmethod def backward(ctx, grad_output): input, = ctx.saved_tensors grad_input = grad_output * (3 * input ** 2 + 8 * input - 7) return grad_input def my_func(input): return MyFunc.apply(input) ``` 2. **Gradient Check:** - Write a function to run `gradcheck` and `gradgradcheck` on `my_func`. ```python def run_gradcheck(): input = torch.randn(5, 5, dtype=torch.double, requires_grad=True) assert torch.autograd.gradcheck(my_func, input), \\"Gradcheck failed!\\" assert torch.autograd.gradgradcheck(my_func, input), \\"Gradgradcheck failed!\\" ``` 3. **Validation:** - Validate your implementation by calling `run_gradcheck()`. **Input Format:** - No input needs to be provided directly. Implement the function with given test cases. **Output Format:** - Output should confirm the passing of `gradcheck` and `gradgradcheck`. **Constraints:** - Use the default tolerance values for `gradcheck` and `gradgradcheck`. - Your implementation should not use any additional libraries outside of PyTorch. **Example usage:** ```python # Run gradient check run_gradcheck() ``` The script should run without any assertion errors, demonstrating that the gradients and higher-order gradients of your function are correctly implemented.","solution":"import torch from torch.autograd import Function class MyFunc(Function): @staticmethod def forward(ctx, input): # Compute any operation, for example, y = x^3 + 4 * x^2 - 7 * x ctx.save_for_backward(input) return input ** 3 + 4 * input ** 2 - 7 * input @staticmethod def backward(ctx, grad_output): input, = ctx.saved_tensors grad_input = grad_output * (3 * input ** 2 + 8 * input - 7) return grad_input def my_func(input): return MyFunc.apply(input) def run_gradcheck(): input = torch.randn(5, 5, dtype=torch.double, requires_grad=True) gradcheck_result = torch.autograd.gradcheck(my_func, input) gradgradcheck_result = torch.autograd.gradgradcheck(my_func, input) return gradcheck_result, gradgradcheck_result"},{"question":"**Question: Create a Terminal-based Chat Application using the `pty` module** In this task, you need to implement a terminal-based chat application that mimics a simple chat server-client interaction. The server and client will be separate processes, with the server process able to handle multiple clients simultaneously. The `pty` module will be used to manage pseudo-terminal I/O for communication between the server and client processes. # Requirements: 1. **Server Process**: - The server should listen for client connections on a specified pseudo-terminal. - It should be able to handle input messages from clients and broadcast these messages to all connected clients. - The server should log all messages to a file named `chat_log.txt`. 2. **Client Process**: - The client should connect to the server’s pseudo-terminal. - It should be able to send messages to the server and receive broadcast messages from the server. - The client process should handle user inputs via standard input (terminal) and display messages received from the server. # Implementation Details: - Define a function `start_server()` that sets up and runs the server process. - Define a function `start_client()` that sets up and runs a client process. - Use `pty.openpty()` to create pseudo-terminal pairs for server-client communication. - Use `pty.spawn()` to manage the spawning of the client process and handle I/O redirection between the server and clients. - Ensure proper synchronization between the server and clients to manage multiple client connections. # Example Usage: ```python # Starting the server start_server() # Starting a client (this can be run in a new terminal instance) start_client() ``` # Constraints: - Only standard Python libraries and the `pty` module are allowed. - Ensure robust error handling for terminal I/O operations. - The application should correctly handle the termination of client processes and any cleanup tasks needed in such cases. # Hints: - Use the `os` and `fcntl` modules for file descriptor operations. - Utilize the `select` module for multiplexing I/O operations between multiple clients. Implement the `start_server` and `start_client` functions and relevant supporting code to complete the chat application.","solution":"import os import pty import fcntl import select import logging LOG_FILE = \'chat_log.txt\' def setup_logger(): logger = logging.getLogger(\'chat_logger\') handler = logging.FileHandler(LOG_FILE) formatter = logging.Formatter(\'%(asctime)s - %(message)s\') handler.setFormatter(formatter) logger.addHandler(handler) logger.setLevel(logging.INFO) return logger logger = setup_logger() def read_from_client(client_fd, clients): try: message = os.read(client_fd, 1024).decode() if message: logger.info(message) broadcast_message(message, client_fd, clients) else: remove_client(client_fd, clients) except OSError: remove_client(client_fd, clients) def remove_client(client_fd, clients): if client_fd in clients: clients.remove(client_fd) os.close(client_fd) def broadcast_message(message, sender_fd, clients): for client_fd in clients: if client_fd != sender_fd: os.write(client_fd, message.encode()) def start_server(): master_fd, slave_fd = pty.openpty() print(f\\"Server running on pty: {os.ttyname(slave_fd)}\\") clients = [] try: while True: readable_fds, _, _ = select.select([master_fd] + clients, [], []) for fd in readable_fds: if fd == master_fd: client_fd = os.open(os.ttyname(slave_fd), os.O_RDWR) clients.append(client_fd) else: read_from_client(fd, clients) except KeyboardInterrupt: print(\\"Server shutting down...\\") finally: for client_fd in clients: os.close(client_fd) os.close(master_fd) os.close(slave_fd) def start_client(): master_fd, slave_fd = pty.openpty() client_fd = os.open(os.ttyname(slave_fd), os.O_RDWR) def client_input_handler(): while True: input_message = os.read(0, 1024).decode() os.write(client_fd, input_message.encode()) def client_output_handler(): while True: try: message = os.read(client_fd, 1024).decode() if message: print(message) except OSError: break import threading input_thread = threading.Thread(target=client_input_handler) output_thread = threading.Thread(target=client_output_handler) input_thread.start() output_thread.start() input_thread.join() output_thread.join() os.close(client_fd) os.close(master_fd) os.close(slave_fd)"},{"question":"Context: You are working on a project that involves converting between different tensor libraries using DLPack for efficient memory sharing. As a part of this project, you need to implement functionality in PyTorch that can convert a tensor from one format to another using DLPack. Task: Write a Python function that takes an input tensor, converts it to a DLPack tensor, and then converts it back to a PyTorch tensor. Additionally, verify that the final tensor is identical to the original tensor in terms of content. Function Signature: ```python import torch from torch.utils.dlpack import to_dlpack, from_dlpack def roundtrip_tensor_conversion(tensor: torch.Tensor) -> bool: Convert a PyTorch tensor to a DLPack tensor and then back to a PyTorch tensor. Verify that the final tensor is identical to the original tensor. Args: - tensor (torch.Tensor): The input PyTorch tensor to be converted. Returns: - bool: True if the final tensor is identical to the original tensor in terms of content, False otherwise. # Your implementation here ``` Input: - `tensor`: A PyTorch tensor of any shape, dtype, and device. Output: - `bool`: A boolean value indicating whether the final tensor matches the original tensor in terms of content. Constraints: - The function should handle tensors of any dtype and should work on both CPU and GPU. Example: ```python input_tensor = torch.tensor([[1, 2], [3, 4]], dtype=torch.float32) assert roundtrip_tensor_conversion(input_tensor) == True input_tensor = torch.tensor([1, 2, 3, 4], dtype=torch.int64).to(\'cuda\') assert roundtrip_tensor_conversion(input_tensor) == True ``` Hints: 1. Utilize the `torch.utils.dlpack.to_dlpack` function to convert a PyTorch tensor to a DLPack tensor. 2. Use the `torch.utils.dlpack.from_dlpack` function to convert the DLPack tensor back to a PyTorch tensor. 3. Ensure that the content and dtype of the tensor are preserved during the conversion process.","solution":"import torch from torch.utils.dlpack import to_dlpack, from_dlpack def roundtrip_tensor_conversion(tensor: torch.Tensor) -> bool: Convert a PyTorch tensor to a DLPack tensor and then back to a PyTorch tensor. Verify that the final tensor is identical to the original tensor. Args: - tensor (torch.Tensor): The input PyTorch tensor to be converted. Returns: - bool: True if the final tensor is identical to the original tensor in terms of content, False otherwise. # Convert to DLPack tensor dlpack_tensor = to_dlpack(tensor) # Convert back to PyTorch tensor converted_tensor = from_dlpack(dlpack_tensor) # Verify if the tensors are identical return torch.equal(tensor, converted_tensor)"},{"question":"# Pandas: Copy-on-Write (CoW) Operations Objective Implement functions to demonstrate the principles of the Copy-on-Write (CoW) feature in pandas. Your task is to handle DataFrame modifications in a way that adheres to CoW rules, preventing unintended side-effects and ensuring efficient memory usage. Functions to Implement 1. **update_dataframe_safe(df: pd.DataFrame, index: int, col: str, value: any) -> pd.DataFrame** * Update the value at a specific index in a column of the DataFrame without causing side-effects on the original DataFrame. * **Input**: * `df` (pd.DataFrame): The original DataFrame. * `index` (int): The index position to update. * `col` (str): The column name to update. * `value` (any): The new value to set. * **Output**: * (pd.DataFrame): A new DataFrame with the updated value. * **Constraints**: * Ensure no side-effects on the original DataFrame. * Avoid chained assignments. 2. **update_multiple_columns_safe(df: pd.DataFrame, conditions: dict, col_updates: dict) -> pd.DataFrame** * Update multiple columns based on given conditions safely. * **Input**: * `df` (pd.DataFrame): The original DataFrame. * `conditions` (dict): A dictionary where keys are column names and values are conditions to filter rows. Example: `{\\"col1\\": lambda x: x > 5}`. * `col_updates` (dict): A dictionary where keys are column names to update and values are the new value or a function to generate new value. Example: `{\\"col2\\": 100, \\"col3\\": lambda x: x*2}`. * **Output**: * (pd.DataFrame): A new DataFrame with the updated values. * **Constraints**: * Use `loc` or `iloc` to update values to adhere to CoW rules. * Ensure no side-effects on the original DataFrame. * **Performance Requirement**: * Handle DataFrames with up to 10^6 rows efficiently. Example Usage ```python import pandas as pd # Function 1 Example df = pd.DataFrame({\'A\': [1, 2, 3], \'B\': [4, 5, 6]}) new_df = update_dataframe_safe(df, index=1, col=\'A\', value=20) print(df) # Original DataFrame should remain unchanged print(new_df) # New DataFrame with updated value # Function 2 Example df = pd.DataFrame({\'A\': [1, 2, 6], \'B\': [4, 5, 6], \'C\': [7, 8, 9]}) conditions = {\'A\': lambda x: x > 5} col_updates = {\'B\': 100, \'C\': lambda x: x * 2} new_df = update_multiple_columns_safe(df, conditions, col_updates) print(df) # Original DataFrame should remain unchanged print(new_df) # New DataFrame with values updated based on conditions ``` Ensure your solution follows CoW principles for predictable behavior and performance improvements.","solution":"import pandas as pd import copy def update_dataframe_safe(df: pd.DataFrame, index: int, col: str, value: any) -> pd.DataFrame: Updates the value at a specific index in a column of the DataFrame without causing side-effects on the original DataFrame. new_df = df.copy(deep=True) # ensure there are no side-effects on the original DataFrame new_df.at[index, col] = value return new_df def update_multiple_columns_safe(df: pd.DataFrame, conditions: dict, col_updates: dict) -> pd.DataFrame: Updates multiple columns based on given conditions safely. new_df = df.copy(deep=True) # ensure there are no side-effects on the original DataFrame for col, condition in conditions.items(): mask = new_df[col].apply(condition) for update_col, update_value in col_updates.items(): if callable(update_value): new_df.loc[mask, update_col] = new_df.loc[mask, update_col].apply(update_value) else: new_df.loc[mask, update_col] = update_value return new_df"},{"question":"# Asynchronous Task Management and Synchronization Problem Statement You are required to design a simple task management system using Python\'s asyncio library. The system should perform the following tasks: 1. Define three asynchronous coroutines `task1`, `task2`, and `task3` that each print a start message, wait for a random amount of time between 1 and 3 seconds, and then print a completion message. 2. Create a main coroutine that initializes and runs these tasks concurrently. 3. Ensure that the main coroutine waits for all tasks to complete before it finishes execution. 4. Synchronize the tasks such that `task2` only starts execution once `task1` has completed, and `task3` only starts execution once `task2` has completed. Function Signature ```python import asyncio import random async def task1(): # Implement task1 coroutine here pass async def task2(): # Implement task2 coroutine here pass async def task3(): # Implement task3 coroutine here pass async def main(): # Implement the main coroutine here to manage and synchronize tasks pass # Entry point to run the main coroutine if __name__ == \'__main__\': asyncio.run(main()) ``` Input - There is no input for this task. Output - The output should be the printed messages from the coroutines indicating the progress and completion of each task. Constraints - Use the asyncio library for managing the concurrency. - Make sure the tasks run in the specified order with the appropriate synchronization. - Utilize the appropriate synchronization primitives from asyncio to ensure the task order (e.g., asyncio.Event). Example Output ``` Task 1 started Task 1 completed Task 2 started Task 2 completed Task 3 started Task 3 completed ``` Performance Requirements - The program should efficiently handle the asynchronous execution without blocking the event loop unnecessarily. Additional Notes - Remember to import any necessary modules such as `asyncio` and `random`. - Use awaitable methods like `asyncio.sleep` to simulate the tasks\' delays. Good luck!","solution":"import asyncio import random async def task1(event1): print(\\"Task 1 started\\") await asyncio.sleep(random.randint(1, 3)) print(\\"Task 1 completed\\") event1.set() async def task2(event1, event2): await event1.wait() print(\\"Task 2 started\\") await asyncio.sleep(random.randint(1, 3)) print(\\"Task 2 completed\\") event2.set() async def task3(event2): await event2.wait() print(\\"Task 3 started\\") await asyncio.sleep(random.randint(1, 3)) print(\\"Task 3 completed\\") async def main(): event1 = asyncio.Event() event2 = asyncio.Event() t1 = asyncio.create_task(task1(event1)) t2 = asyncio.create_task(task2(event1, event2)) t3 = asyncio.create_task(task3(event2)) await asyncio.gather(t1, t2, t3) # Entry point to run the main coroutine if __name__ == \'__main__\': asyncio.run(main())"},{"question":"**Problem Statement: Copy-on-Write in pandas** You are given two DataFrames representing sales and inventory data for a fictional store. With pandas 3.0\'s Copy-on-Write (CoW) features, your task is to write functions to handle various data transformation and extraction operations while ensuring no unintentional side effects on shared data. **Dataset Example:** ```python sales = pd.DataFrame({ \\"item_id\\": [1, 2, 3], \\"quantity\\": [10, 5, 8], \\"price\\": [100, 200, 150] }) inventory = pd.DataFrame({ \\"item_id\\": [1, 2, 3], \\"stock\\": [50, 20, 30] }) ``` **Tasks:** 1. **Update Inventory based on Sales:** Write a function `update_inventory(sales, inventory)` that takes current sales and inventory DataFrames, subtracts the quantity of each item sold from the inventory and returns the updated inventory DataFrame. Ensure no side effects impact the original inventory DataFrame. 2. **Calculate Total Sales:** Write a function `calculate_total_sales(sales)` that calculates the total sales for each item (quantity * price) and returns a Series with `item_id` as index and total sales as values. 3. **Enforce Price Floor:** Write a function `enforce_price_floor(sales, floor_price)` that takes the sales DataFrame and a floor price. Any item priced below the floor price should be updated to the floor price. Return the updated DataFrame without affecting the original DataFrame. 4. **Stock Valuation:** Write a function `stock_valuation(inventory, sales)` that calculates the stock valuation for each item, considering the sales price and current stock levels. Return a Series with `item_id` as index and stock valuation (stock * price) as values. **Function Signatures:** ```python import pandas as pd def update_inventory(sales: pd.DataFrame, inventory: pd.DataFrame) -> pd.DataFrame: Updates inventory based on sales. Parameters: sales (pd.DataFrame): DataFrame containing sales data. inventory (pd.DataFrame): DataFrame containing inventory data. Returns: pd.DataFrame: Updated inventory DataFrame. pass def calculate_total_sales(sales: pd.DataFrame) -> pd.Series: Calculate total sales. Parameters: sales (pd.DataFrame): DataFrame containing sales data. Returns: pd.Series: Series containing total sales per item. pass def enforce_price_floor(sales: pd.DataFrame, floor_price: float) -> pd.DataFrame: Enforce a price floor for items. Parameters: sales (pd.DataFrame): DataFrame containing sales data. floor_price (float): Minimum price for items. Returns: pd.DataFrame: DataFrame with updated prices. pass def stock_valuation(inventory: pd.DataFrame, sales: pd.DataFrame) -> pd.Series: Calculates stock valuation. Parameters: inventory (pd.DataFrame): DataFrame containing inventory data. sales (pd.DataFrame): DataFrame containing sales data. Returns: pd.Series: Series containing stock valuation per item. pass ``` **Constraints:** - Ensure your implementation handles edge cases like negative stock levels or empty DataFrames. - Do not alter the original DataFrames; any transformation should return a new DataFrame or Series. **Example Usage:** ```python sales = pd.DataFrame({ \\"item_id\\": [1, 2, 3], \\"quantity\\": [10, 5, 8], \\"price\\": [100, 200, 150] }) inventory = pd.DataFrame({ \\"item_id\\": [1, 2, 3], \\"stock\\": [50, 20, 30] }) updated_inventory = update_inventory(sales, inventory) print(updated_inventory) total_sales = calculate_total_sales(sales) print(total_sales) updated_sales = enforce_price_floor(sales, 120) print(updated_sales) valuation = stock_valuation(inventory, sales) print(valuation) ``` Each function should demonstrate proper handling of Copy-on-Write principles to ensure predictable and efficient operations.","solution":"import pandas as pd def update_inventory(sales: pd.DataFrame, inventory: pd.DataFrame) -> pd.DataFrame: Updates inventory based on sales. Parameters: sales (pd.DataFrame): DataFrame containing sales data. inventory (pd.DataFrame): DataFrame containing inventory data. Returns: pd.DataFrame: Updated inventory DataFrame. updated_inventory = inventory.copy() for _, row in sales.iterrows(): updated_inventory.loc[updated_inventory[\'item_id\'] == row[\'item_id\'], \'stock\'] -= row[\'quantity\'] return updated_inventory def calculate_total_sales(sales: pd.DataFrame) -> pd.Series: Calculate total sales. Parameters: sales (pd.DataFrame): DataFrame containing sales data. Returns: pd.Series: Series containing total sales per item. return sales.set_index(\'item_id\')[\'quantity\'] * sales.set_index(\'item_id\')[\'price\'] def enforce_price_floor(sales: pd.DataFrame, floor_price: float) -> pd.DataFrame: Enforce a price floor for items. Parameters: sales (pd.DataFrame): DataFrame containing sales data. floor_price (float): Minimum price for items. Returns: pd.DataFrame: DataFrame with updated prices. updated_sales = sales.copy() updated_sales.loc[updated_sales[\'price\'] < floor_price, \'price\'] = floor_price return updated_sales def stock_valuation(inventory: pd.DataFrame, sales: pd.DataFrame) -> pd.Series: Calculates stock valuation. Parameters: inventory (pd.DataFrame): DataFrame containing inventory data. sales (pd.DataFrame): DataFrame containing sales data. Returns: pd.Series: Series containing stock valuation per item. price_map = sales.set_index(\'item_id\')[\'price\'] return inventory.set_index(\'item_id\')[\'stock\'] * price_map"},{"question":"Objective Implement and compose different function transforms using the `torch.func` module to compute gradients and per-sample-gradients for a given function. This will test your comprehension of `torch.func`\'s functionalities and your ability to apply them. Question You are given a function `f(x)` defined as follows: ```python import torch def f(x): return torch.sum(torch.sin(x) * torch.exp(x)) ``` # Tasks: 1. **Compute the gradient of `f`**: Implement a function `compute_gradient` that takes a tensor input `x` and returns the gradient of `f` at `x`. Use `torch.func.grad`. 2. **Compute the per-sample-gradients of `f` for a batch of inputs**: Implement a function `compute_per_sample_gradients` that takes a batch of inputs `X` (a 2D tensor where each row represents a different input) and returns the gradients of `f` for each input in the batch. Utilize `torch.func.vmap` and `torch.func.grad` for this task. # Input and Output Specifications: 1. **Function `compute_gradient`**: - **Input**: A 1D tensor `x` of shape `(n,)`. - **Output**: A 1D tensor representing the gradient of `f` at `x`. 2. **Function `compute_per_sample_gradients`**: - **Input**: A 2D tensor `X` of shape `(m, n)` where `m` is the number of inputs and `n` is the dimension of each input. - **Output**: A 2D tensor of shape `(m, n)` where each row is the gradient of `f` at the corresponding input in `X`. # Constraints: - The input tensors `x` and `X` will contain floating-point numbers. - The gradient computation should be efficient and make use of `torch.func` capabilities as described. # Example: ```python import torch import torch.func as func def f(x): return torch.sum(torch.sin(x) * torch.exp(x)) def compute_gradient(x): # Your implementation here pass def compute_per_sample_gradients(X): # Your implementation here pass # Example usage: x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True) print(compute_gradient(x)) # Expected output: Gradient of f at x X = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]], requires_grad=True) print(compute_per_sample_gradients(X)) # Expected output: Per-sample gradients of f for each input in X ``` Submit your code with correct implementations for both `compute_gradient` and `compute_per_sample_gradients`.","solution":"import torch import torch.func as func def f(x): return torch.sum(torch.sin(x) * torch.exp(x)) def compute_gradient(x): grad_f = func.grad(f) return grad_f(x) def compute_per_sample_gradients(X): grad_f = func.grad(f) vmap_grad_f = func.vmap(grad_f) return vmap_grad_f(X)"},{"question":"Objective You are to implement a custom sorted list class that supports various insertion and searching operations by utilizing the `bisect` module for maintaining the list in sorted order. This class should demonstrate your understanding of the `bisect` module functions as well as your ability to manage and manipulate lists efficiently. Class Specification Implement a class `SortedList` with the following methods: 1. **`__init__(self, key=None)`**: - Initializes an empty `SortedList`. - `key` (optional): a function that specifies the sorting key for the elements. 2. **`add(self, element)`**: - Adds an `element` to the `SortedList` while maintaining the sorted order. 3. **`remove(self, element)`**: - Removes the first occurrence of `element` from the `SortedList`. - Should raise a `ValueError` if `element` is not found in the list. 4. **`find_lt(self, element)`**: - Returns the rightmost value less than `element`. - Should raise a `ValueError` if no such element exists. 5. **`find_le(self, element)`**: - Returns the rightmost value less than or equal to `element`. - Should raise a `ValueError` if no such element exists. 6. **`find_gt(self, element)`**: - Returns the leftmost value greater than `element`. - Should raise a `ValueError` if no such element exists. 7. **`find_ge(self, element)`**: - Returns the leftmost value greater than or equal to `element`. - Should raise a `ValueError` if no such element exists. 8. **`__contains__(self, element)`**: - Checks if `element` is in the `SortedList`. - Returns `True` if found, `False` otherwise. 9. **`__len__(self)`**: - Returns the number of elements in the `SortedList`. 10. **`__getitem__(self, index)`**: - Provides indexed access to elements. Constraints - The elements added to the list should be comparable or should provide a custom key function. - Use the `bisect` module functions to handle sorting aspects. - The class should raise appropriate errors for invalid operations. Example Usage ```python from bisect_example import SortedList # Initialize an empty sorted list sl = SortedList() # Add elements sl.add(5) sl.add(1) sl.add(3) # List should maintain sorted order print(list(sl)) # Output: [1, 3, 5] # Remove an element sl.remove(3) print(list(sl)) # Output: [1, 5] # Attempt to find elements print(sl.find_lt(5)) # Output: 1 print(sl.find_le(5)) # Output: 5 print(sl.find_gt(1)) # Output: 5 print(sl.find_ge(5)) # Output: 5 # Check length print(len(sl)) # Output: 2 # Check membership print(1 in sl) # Output: True print(3 in sl) # Output: False # Index access print(sl[0]) # Output: 1 ``` Implement this class in a file named `bisect_example.py`.","solution":"from bisect import bisect_left, bisect_right, insort class SortedList: def __init__(self, key=None): self._list = [] self._key = key def add(self, element): insort(self._list, element if self._key is None else self._key(element)) def remove(self, element): pos = bisect_left(self._list, element if self._key is None else self._key(element)) if pos != len(self._list) and self._list[pos] == (element if self._key is None else self._key(element)): self._list.pop(pos) else: raise ValueError(f\\"{element} not found in list\\") def find_lt(self, element): pos = bisect_left(self._list, element if self._key is None else self._key(element)) if pos: return self._list[pos-1] raise ValueError(f\\"No elements less than {element}\\") def find_le(self, element): pos = bisect_right(self._list, element if self._key is None else self._key(element)) if pos: return self._list[pos-1] raise ValueError(f\\"No elements less than or equal to {element}\\") def find_gt(self, element): pos = bisect_right(self._list, element if self._key is None else self._key(element)) if pos != len(self._list): return self._list[pos] raise ValueError(f\\"No elements greater than {element}\\") def find_ge(self, element): pos = bisect_left(self._list, element if self._key is None else self._key(element)) if pos != len(self._list): return self._list[pos] raise ValueError(f\\"No elements greater than or equal to {element}\\") def __contains__(self, element): pos = bisect_left(self._list, element if self._key is None else self._key(element)) return pos != len(self._list) and self._list[pos] == (element if self._key is None else self._key(element)) def __len__(self): return len(self._list) def __getitem__(self, index): return self._list[index]"},{"question":"**Objective:** Write a Python function that interprets and executes Python code provided as a string. Your function should be able to handle three different types of input - a complete program, interactive input, and an expression. The function should determine the type of input automatically and handle it appropriately. **Function Signature:** ```python def interpret_python_code(code: str) -> None: pass ``` **Input:** - `code` (str): A string containing Python code. It may represent a complete program, interactive input, or an expression. **Output:** - The function doesn\'t return anything but should print the output or errors based on the code execution. **Constraints:** - The input string will always be a valid Python code. - The `code` string can be empty. - Assume standard input and output as the mode of communication. **Examples:** Example 1: ```python code = \\"print(\'Hello, World!\')\\" interpret_python_code(code) ``` Output: ``` Hello, World! ``` Example 2: ```python code = \\"x = 10ny = 20nprint(x + y)\\" interpret_python_code(code) ``` Output: ``` 30 ``` Example 3: ```python code = \\"3 + 4\\" interpret_python_code(code) ``` Output: ``` 7 ``` **Notes:** - For complete programs or interactive input, use the `exec()` function. - For evaluating expressions, use the `eval()` function. - The function should be able to differentiate between statements and expressions automatically. - Handle any exceptions that may arise during code execution gracefully and print an appropriate error message.","solution":"def interpret_python_code(code: str) -> None: Interprets and executes Python code provided as a string. if code.strip() == \'\': return # if the input is empty, don\'t do anything try: # First, attempt to evaluate it as an expression result = eval(code) if result is not None: print(result) except SyntaxError: # If eval fails due to a SyntaxError, treat it as a block of statements try: exec(code) except Exception as e: print(f\\"Error while executing code: {e}\\") except Exception as e: print(f\\"Error while evaluating expression: {e}\\")"},{"question":"**Coding Assessment Question** # Objective Demonstrate your understanding of creating and using custom color palettes in seaborn by performing the following tasks. # Task 1. **Create Custom Palettes**: - Using seaborn, create a dark palette using a specific color in HEX format `#FFA07A` (Light Salmon). - Create another dark palette using the [hue, saturation, lightness] (husl) system with values (240, 100, 50). 2. **Visualization**: - Use these palettes to create visualizations. Create a `seaborn` `barplot` using the `tips` dataset provided by seaborn. Use the Light Salmon dark palette for one barplot and the husl system palette for another barplot. The barplots should show the average total bill for each day of the week. 3. **Comparison**: - Display the generated barplots side by side using Matplotlib subplots to compare the different color palettes. # Input and Output Formats - **Input**: No input from the user. - **Output**: Two barplots displayed side by side comparing the different color palettes. # Constraints - You must use the `seaborn.dark_palette` function to create the palettes. - You must use the tips dataset from seaborn. # Performance Requirements The solution should be efficient and should not take more than a couple of seconds to render the plots. # Implementation Template ```python import seaborn as sns import matplotlib.pyplot as plt # Load the tips dataset tips = sns.load_dataset(\\"tips\\") # Step 1: Create the custom palettes # Palette 1: Using HEX format palette_hex = sns.dark_palette(\\"#FFA07A\\") # Palette 2: Using husl system palette_husl = sns.dark_palette((240, 100, 50), input=\\"husl\\") # Step 2: Create the visualizations # Barplot with HEX palette plt.figure(figsize=(12, 6)) plt.subplot(1, 2, 1) sns.barplot(x=\\"day\\", y=\\"total_bill\\", data=tips, palette=palette_hex) plt.title(\\"Barplot with HEX Palette\\") # Barplot with husl system palette plt.subplot(1, 2, 2) sns.barplot(x=\\"day\\", y=\\"total_bill\\", data=tips, palette=palette_husl) plt.title(\\"Barplot with HUSL Palette\\") # Step 3: Display the plots plt.tight_layout() plt.show() ``` Note - Make sure to handle any necessary imports. - Remember to set a visual theme using `sns.set_theme()`.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_custom_palettes_and_visualize(): # Load the tips dataset tips = sns.load_dataset(\\"tips\\") # Step 1: Create the custom palettes # Palette 1: Using HEX format palette_hex = sns.dark_palette(\\"#FFA07A\\", reverse=True) # Reversed for better contrast in dark palette # Palette 2: Using husl system palette_husl = sns.dark_palette((240, 100, 50), input=\\"husl\\") # Set the seaborn theme sns.set_theme(style=\\"darkgrid\\") # Step 2: Create the visualizations fig, axes = plt.subplots(1, 2, figsize=(14, 6)) # Barplot with HEX palette sns.barplot(ax=axes[0], x=\\"day\\", y=\\"total_bill\\", data=tips, palette=palette_hex) axes[0].set_title(\\"Barplot with HEX Palette\\") # Barplot with husl system palette sns.barplot(ax=axes[1], x=\\"day\\", y=\\"total_bill\\", data=tips, palette=palette_husl) axes[1].set_title(\\"Barplot with HUSL Palette\\") # Step 3: Display the plots plt.tight_layout() plt.show()"},{"question":"# PyTorch Sparse Tensor Assessment Question Objective: To assess the student\'s understanding of creating, manipulating, and converting sparse tensors in PyTorch. Question: **Task: Sparse Tensor Manipulation and Conversion** You are required to implement the following functions using PyTorch sparse tensors: 1. `create_sparse_matrix(dense_matrix)` This function takes a 2D dense tensor and converts it to a sparse CSR tensor. - **Input:** - `dense_matrix`: A 2D PyTorch tensor of size `(m, n)` with dtype `torch.float32`. - **Output:** - A sparse CSR tensor converted from the `dense_matrix`. 2. `sparse_matrix_addition(sparse_matrix_1, sparse_matrix_2)` This function takes two sparse CSR tensors and returns their sum as a sparse CSR tensor. - **Input:** - `sparse_matrix_1`: A sparse CSR tensor. - `sparse_matrix_2`: A sparse CSR tensor of the same size as `sparse_matrix_1`. - **Output:** - A sparse CSR tensor representing the sum of `sparse_matrix_1` and `sparse_matrix_2`. 3. `sparse_to_dense_tensor(sparse_matrix)` This function takes a sparse CSR tensor and converts it back to a dense tensor. - **Input:** - `sparse_matrix`: A sparse CSR tensor. - **Output:** - A dense tensor converted from the `sparse_matrix`. 4. `create_hybrid_sparse_tensor(dense_matrix)` This function takes a dense 3D tensor with two sparse dimensions and one dense dimension, and converts it to a hybrid sparse tensor in COO format. - **Input:** - `dense_matrix`: A 3D PyTorch tensor of size `(m, n, p)` with dtype `torch.float32`. The first two dimensions represent sparse dimensions, and the last dimension represents a dense dimension. - **Output:** - A hybrid sparse tensor in COO format converted from the `dense_matrix`. Constraints: - You must use PyTorch\'s built-in methods for sparse tensor manipulation. - Do not use any external libraries or custom implementations for sparse tensor conversion or operations. - Ensure that your functions are efficient and handle edge cases appropriately. Example Usage: ```python import torch # Example dense matrix dense_matrix = torch.tensor([ [0, 2, 0], [3, 0, 4], [0, 0, 5] ], dtype=torch.float32) # Convert dense matrix to sparse CSR tensor sparse_matrix = create_sparse_matrix(dense_matrix) print(sparse_matrix) # Example sparse matrix addition sparse_matrix_1 = create_sparse_matrix(torch.tensor([ [0, 1, 0], [0, 0, 2], [3, 0, 0]], dtype=torch.float32)) sparse_matrix_2 = create_sparse_matrix(torch.tensor([ [0, 0, 4], [0, 0, 1], [2, 1, 0]], dtype=torch.float32)) sum_sparse_matrix = sparse_matrix_addition(sparse_matrix_1, sparse_matrix_2) print(sum_sparse_matrix) # Convert sparse CSR tensor back to dense tensor dense_from_sparse = sparse_to_dense_tensor(sparse_matrix) print(dense_from_sparse) # Example dense to hybrid sparse tensor conversion hybrid_dense_matrix = torch.tensor([ [[0, 0], [1, 2]], [[3, 0], [0, 4]] ], dtype=torch.float32) hybrid_sparse_tensor = create_hybrid_sparse_tensor(hybrid_dense_matrix) print(hybrid_sparse_tensor) ``` Write your implementations below: ```python import torch def create_sparse_matrix(dense_matrix): # Your implementation here pass def sparse_matrix_addition(sparse_matrix_1, sparse_matrix_2): # Your implementation here pass def sparse_to_dense_tensor(sparse_matrix): # Your implementation here pass def create_hybrid_sparse_tensor(dense_matrix): # Your implementation here pass ```","solution":"import torch def create_sparse_matrix(dense_matrix): Converts a dense tensor to a sparse CSR tensor. :param dense_matrix: A 2D PyTorch tensor of size (m, n) with dtype torch.float32 :return: A sparse CSR tensor converted from the dense_matrix return dense_matrix.to_sparse_csr() def sparse_matrix_addition(sparse_matrix_1, sparse_matrix_2): Adds two sparse CSR tensors and returns the resulting sparse CSR tensor. :param sparse_matrix_1: A sparse CSR tensor :param sparse_matrix_2: A sparse CSR tensor of the same size as sparse_matrix_1 :return: A sparse CSR tensor representing the sum of sparse_matrix_1 and sparse_matrix_2 return sparse_matrix_1 + sparse_matrix_2 def sparse_to_dense_tensor(sparse_matrix): Converts a sparse CSR tensor back to a dense tensor. :param sparse_matrix: A sparse CSR tensor :return: A dense tensor converted from the sparse_matrix return sparse_matrix.to_dense() def create_hybrid_sparse_tensor(dense_matrix): Converts a dense 3D tensor with two sparse dimensions and one dense dimension to a hybrid sparse tensor in COO format. :param dense_matrix: A 3D PyTorch tensor of size (m, n, p) with dtype torch.float32 The first two dimensions are sparse dimensions, and the last dimension is a dense dimension. :return: A hybrid sparse tensor in COO format converted from the dense_matrix return dense_matrix.to_sparse()"},{"question":"# Custom Context Manager for File Operations You are required to implement a custom context manager to manage file operations using the `contextmanager` decorator from the `contextlib` module. This context manager should handle the opening and closing of a file, as well as performing actions such as reading, writing, and handling exceptions during the file operations. Specifications: 1. **Function Signature:** ```python from contextlib import contextmanager @contextmanager def file_manager(file_name: str, mode: str): # Implementation here ``` 2. **Function Behavior:** - The function `file_manager` should: - Open the file with the specified `file_name` and `mode`. - Yield the file object to the caller to perform any required operations within the `with` block. - Ensure that the file is properly closed after exiting the `with` block. - Handle any exceptions that occur during the file operations, ensuring that the file is closed even in the event of an error. 3. **Usage Example:** ```python try: with file_manager(\'example.txt\', \'w\') as f: f.write(\\"Hello, world!\\") except Exception as e: print(f\\"An error occurred: {e}\\") try: with file_manager(\'example.txt\', \'r\') as f: content = f.read() print(content) except Exception as e: print(f\\"An error occurred: {e}\\") ``` 4. **Constraints:** - The input `file_name` will be a valid string representing the file name. - The input `mode` will be a valid file mode string, such as `\'r\'`, `\'w\'`, `\'a\'`, etc. 5. **Performance Requirements:** - Ensure efficient and proper handling of file resources, with minimal overhead introduced by the context manager. Additional Information: - Utilize the details provided in the `contextlib` documentation to implement and handle the context manager functionality. - Test the implemented context manager thoroughly with different file modes and operations to ensure correct behavior under various scenarios. Implement the `file_manager` function as described above.","solution":"from contextlib import contextmanager @contextmanager def file_manager(file_name: str, mode: str): Custom context manager for file operations. file_name: str : The name of the file to be managed. mode: str : The mode in which the file should be opened. f = None try: f = open(file_name, mode) yield f except Exception as e: raise e finally: if f is not None: f.close()"},{"question":"**Question:** You are required to implement a function `compute_special_math_values`, which takes a list of floating-point numbers and a list of integer commands. This function should perform specific mathematical operations based on the provided commands. # Function Signature ```python def compute_special_math_values(numbers: list, commands: list) -> list: ``` # Input - `numbers` (list of floats): A list of floating-point numbers. - `commands` (list of integers): A list of integer commands where each integer represents a specific mathematical operation to be performed on the corresponding floating-point number. # Commands and their corresponding operations: 1. **math.ceil(x)** - Get the smallest integer greater than or equal to x. 2. **math.factorial(x)** - Get the factorial of the integer part of x. Raise ValueError if x is negative. 3. **math.fsum** - Compute the accurate floating-point sum of the list. 4. **math.isqrt(x)** - Get the integer square root of the non-negative integer part of x. Raise ValueError if x is negative. 5. **math.pow(x, y)** - Compute x raised to the power y where y is another number in the `numbers` list at the index mentioned by the next integer in `commands`. 6. **math.log(x)** - Compute the natural logarithm of x. Raises ValueError if x is non-positive. # Output - Returns a list of results where each result corresponds to the operation applied to each `number` based on the provided `commands`. # Example ```python numbers = [4.7, 3.0, 2.0, -1.0, 6.0] commands = [1, 2, 4, 6, 5, 2] # operations to be performed will be: # math.ceil(4.7) -> 5 # math.factorial(3.0) -> 6 # math.isqrt(2.0) -> 1 # Raises ValueError because -1.0 is negative and will stop executing further # However, math.pow(6.0, next number in `numbers list`) -> math.pow(6.0, 2.0) -> 36.0 (this won’t be computed because of the error in the previous step) # The final result is [5, 6, 1] compute_special_math_values(numbers, commands) # should return [5, 6, 1] ``` # Constraints: 1. You can assume the length of `numbers` and `commands` will be same. 2. Raise ValueError in the cases mentioned and stop further execution. You are required to use the appropriate functions from the provided \'math\' module documentation to meet the requirements. Be sure to handle exceptions as specified for negative numbers or other invalid inputs.","solution":"import math def compute_special_math_values(numbers, commands): results = [] i = 0 while i < len(numbers): number = numbers[i] command = commands[i] if command == 1: result = math.ceil(number) elif command == 2: int_part = int(number) if int_part < 0: raise ValueError(\\"Cannot compute factorial of a negative number\\") result = math.factorial(int_part) elif command == 3: result = math.fsum(numbers) elif command == 4: int_part = int(number) if int_part < 0: raise ValueError(\\"Cannot compute integer square root of a negative number\\") result = math.isqrt(int_part) elif command == 5: if i + 1 >= len(numbers): raise ValueError(\\"Next number required for power computation is missing\\") y = numbers[i + 1] result = math.pow(number, y) i += 1 # increment i an extra time to skip the next number used in the power computation elif command == 6: if number <= 0: raise ValueError(\\"Cannot compute natural logarithm of a non-positive number\\") result = math.log(number) results.append(result) i += 1 return results"},{"question":"Objective Write a Python function that emulates a simplified version of the `compileall.compile_dir` function to recursively compile all `.py` files in a given directory. Your function should accept similar parameters and demonstrate the ability to handle those appropriately. Task Implement a function `custom_compile_dir` with the following signature: ```python def custom_compile_dir(dir: str, maxlevels: int = None, force: bool = False, rx: re.Pattern = None, quiet: int = 0) -> bool: pass ``` Function Parameters 1. **dir** (*str*): The directory to start the recursive compilation from. 2. **maxlevels** (*int*, optional): Maximum depth of recursion. If `None`, there is no limit (default: None). 3. **force** (*bool*, optional): If `True`, forcefully recompile all files even if they are up to date (default: False). 4. **rx** (*re.Pattern*, optional): A compiled regex pattern. Files matching this pattern should be excluded from compilation (default: None). 5. **quiet** (*int*, optional): Controls the verbosity of the output: - `0`: Print filenames and other information to stdout. - `1`: Only print errors. - `2`: Suppress all output (default: 0). Constraints - Use only standard library modules. - You are not expected to byte-compile files but simulate by printing a message to stdout indicating the compilation (e.g., \\"Compiling file: <filename>\\"). - Handling of `.py` files only (ignore other file types). - Directory traversal should be recursive, respecting the `maxlevels` constraint. - If `force` is `False`, simulate the checking for up-to-dateness by a placeholder condition (e.g., always treating files as outdated). - Implement file exclusion based on the regex pattern if provided. - The function should return `True` if all files are compiled successfully and `False` otherwise. - You may use the `os`, `os.path`, and `re` modules to facilitate directory traversal and pattern matching. Example ```python import re # Example usage of the function result = custom_compile_dir(\'/path/to/dir\', maxlevels=2, force=True, rx=re.compile(r\'test_.*.py\'), quiet=1) print(result) ``` Notes - Test the function thoroughly with various configurations of the parameters to ensure correct behavior. - Ensure the function handles different edge cases like non-existent directories, invalid inputs, etc.","solution":"import os import re def custom_compile_dir(dir: str, maxlevels: int = None, force: bool = False, rx: re.Pattern = None, quiet: int = 0) -> bool: Recursively compiles all `.py` files in a given directory. Parameters: - dir (str): Directory to start recursive compilation from. - maxlevels (int, optional): Maximum depth of recursion. If None, no limit. Defaults to None. - force (bool, optional): Forcefully recompile all files even if up to date. Defaults to False. - rx (re.Pattern, optional): A compiled regex pattern. Files matching this pattern should be excluded. Defaults to None. - quiet (int, optional): Verbosity of the output. Defaults to 0. - 0: Print filenames and other information to stdout. - 1: Only print errors. - 2: Suppress all output. Returns: - bool: True if all files are compiled successfully, False otherwise. def is_excluded(filename): return rx and rx.search(filename) def compile_file(filepath): if quiet == 0: print(f\\"Compiling file: {filepath}\\") # Placeholder for the actual compilation process return True # Simulating successful compilation def recursive_compile(current_dir, level): if maxlevels is not None and level > maxlevels: return True try: for entry in os.listdir(current_dir): entry_path = os.path.join(current_dir, entry) if os.path.isdir(entry_path): if not recursive_compile(entry_path, level + 1): return False elif entry.endswith(\'.py\') and not is_excluded(entry): if force or not os.path.exists(entry_path + \'c\'): # Simulate out-of-date check if not compile_file(entry_path): return False except Exception as e: if quiet < 2: print(f\\"Error processing directory {current_dir}: {str(e)}\\") return False return True return recursive_compile(dir, 0)"},{"question":"**Objective**: Implement a function and demonstrate understanding of Python\'s name resolution, scope rules, and exception handling. --- **Problem Statement**: You are given a simple Python program that has nested functions and uses global variables. Your task is to implement a function `process_data(data_list)` that processes a list of data. The function should: 1. Calculate and return the sum of all numbers in `data_list`. 2. Raise a `ValueError` if `data_list` is empty with the message \\"Data list cannot be empty\\". 3. Use a global variable `multiplier` to multiply the sum before returning it. 4. Demonstrate proper use of local, global, and nonlocal variables inside nested functions. ```python # Global variable multiplier = 2 def process_data(data_list): Processes a list of data and returns the result after multiplying with the global multiplier. Parameters: data_list (list): List of numerical values to be processed. Returns: int: Processed result. Raises: ValueError: If the data_list is empty. def validate_data(): # Check if data_list is empty and raise an exception if so. if not data_list: raise ValueError(\\"Data list cannot be empty\\") def calculate_sum(): # Calculate the sum of data_list local_sum = sum(data_list) return local_sum def apply_multiplier(sum_value): # Use global variable multiplier and nonlocal variable sum_value global multiplier return sum_value * multiplier # Validate the data validate_data() # Calculate the sum data_sum = calculate_sum() # Apply the multiplier and return the result result = apply_multiplier(data_sum) return result # Example usage: # data = [1, 2, 3, 4, 5] # print(process_data(data)) # Output should be 30 (15 * 2) ``` --- **Input**: - `data_list`: A list of integers (e.g., `[1, 2, 3, 4, 5]`). **Output**: - The product of the sum of the numbers in the list and the global variable `multiplier`. **Constraints**: - `data_list` may be empty. **Exceptions**: - Raise `ValueError` if `data_list` is empty with the message \\"Data list cannot be empty\\". --- **Notes**: - Ensure that you properly handle the different scopes (local, global, and nonlocal) in the nested functions. - Validate the input list before performing any calculations. --- **Your Task**: Implement the `process_data` function as described above and ensure it adheres to the constraints and specified behaviors.","solution":"# Global variable multiplier = 2 def process_data(data_list): Processes a list of data and returns the result after multiplying with the global multiplier. Parameters: data_list (list): List of numerical values to be processed. Returns: int: Processed result. Raises: ValueError: If the data_list is empty. def validate_data(): # Check if data_list is empty and raise an exception if so. if not data_list: raise ValueError(\\"Data list cannot be empty\\") def calculate_sum(): # Calculate the sum of data_list local_sum = sum(data_list) return local_sum def apply_multiplier(sum_value): # Use global variable multiplier and nonlocal variable sum_value global multiplier return sum_value * multiplier # Validate the data validate_data() # Calculate the sum data_sum = calculate_sum() # Apply the multiplier and return the result result = apply_multiplier(data_sum) return result"},{"question":"Objective: To assess the student\'s ability to write, document, and test Python code using the `unittest` and `unittest.mock` frameworks. Problem Statement: You are required to implement a simple class `BankAccount` and write unit tests for it using the `unittest` framework. Additionally, you need to mock a method to test how your class behaves when interacting with an external system. # Class: BankAccount Implement a class `BankAccount` with the following specifications: Attributes: - `account_number`: A unique string representing the account number. - `balance`: A float representing the current balance of the account. Methods: 1. **`__init__(self, account_number: str, balance: float = 0.0) -> None`**: - Constructor to initialize the account number and balance. 2. **`deposit(self, amount: float) -> None`**: - Adds the specified amount to the account balance. - Raises a `ValueError` if the amount is negative. 3. **`withdraw(self, amount: float) -> None`**: - Subtracts the specified amount from the account balance. - Raises a `ValueError` if the amount is negative or if there are insufficient funds. 4. **`get_balance(self) -> float`**: - Returns the current balance of the account. 5. **`external_transfer(self, external_account: str, amount: float) -> bool`**: - Simulates transferring `amount` to an external account. It relies on an external system to process the transfer. - For this problem, assume this method returns `True` if the transfer is successful and `False` otherwise. # Task: 1. Implement the `BankAccount` class. 2. Write a `unittest` test case class to test the functionalities of `BankAccount`: - Test the initialization, deposit, withdraw, and get_balance methods. - Ensure that invalid operations, such as withdrawing more than the balance, raise an appropriate `ValueError`. 3. Mock the `external_transfer` method to test its interaction within your class without actually performing any external system operations. # Input and Output Requirements: Example: ```python # Example usage account = BankAccount(\\"12345\\") account.deposit(100.0) account.withdraw(50.0) assert account.get_balance() == 50.0 # Mocking external_transfer with mock.patch.object(BankAccount, \'external_transfer\', return_value=True): result = account.external_transfer(\\"54321\\", 20.0) assert result is True ``` # Constraints: - `account_number` is a non-empty string. - `amount` for deposit and withdraw operations must be a positive float. - Assume no concurrency issues (i.e., no need to handle multiple threads accessing the account simultaneously). # Additional Information: - Make sure to include appropriate docstrings and comments in your code. - Test coverage should be complete and cover all edge cases.","solution":"class BankAccount: A simple Bank Account class to handle basic banking operations. def __init__(self, account_number: str, balance: float = 0.0) -> None: Initialize the bank account with an account number and an initial balance. self.account_number = account_number self.balance = balance def deposit(self, amount: float) -> None: Deposit a specified amount into the bank account. Raises a ValueError if the deposit amount is negative. if amount < 0: raise ValueError(\\"Deposit amount cannot be negative.\\") self.balance += amount def withdraw(self, amount: float) -> None: Withdraw a specified amount from the bank account. Raises a ValueError if the withdrawal amount is negative or if there are insufficient funds. if amount < 0: raise ValueError(\\"Withdrawal amount cannot be negative.\\") if amount > self.balance: raise ValueError(\\"Insufficient funds.\\") self.balance -= amount def get_balance(self) -> float: Return the current balance of the bank account. return self.balance def external_transfer(self, external_account: str, amount: float) -> bool: Simulates transferring a specified amount to an external account. For this problem, we assume the transfer is always successful and return True. In a real system, this method would interact with an external system. # This method would normally call an external system. # For this stub, assume it always returns True. return True"},{"question":"**Question**: You are provided with a dataset of continuous numerical features and a target variable. You need to preprocess this dataset using several transformations, then fit a linear regression model on the processed data. Tasks: 1. **Standardize** the dataset so that each feature has zero mean and unit variance. 2. **Generate Polynomial Features** of degree 2 for the standardized dataset. 3. **Normalize** the dataset to have unit norm using the `l2` norm. 4. Fit a linear regression model on this processed data. Implement a function `preprocess_and_fit` that takes in a 2D NumPy array `X` and a 1D NumPy array `y`, and returns the fitted linear regression model. ```python import numpy as np from sklearn.preprocessing import StandardScaler, PolynomialFeatures, normalize from sklearn.linear_model import LinearRegression def preprocess_and_fit(X: np.ndarray, y: np.ndarray) -> LinearRegression: Preprocess the dataset and fit a linear regression model. Parameters: X (np.ndarray): 2D array of shape (n_samples, n_features) containing the input features. y (np.ndarray): 1D array of shape (n_samples,) containing the target variable. Returns: LinearRegression: The fitted linear regression model. # 1. Standardize the dataset scaler = StandardScaler() X_scaled = scaler.fit_transform(X) # 2. Generate polynomial features of degree 2 poly = PolynomialFeatures(degree=2) X_poly = poly.fit_transform(X_scaled) # 3. Normalize the dataset to have unit norm X_normalized = normalize(X_poly, norm=\'l2\') # 4. Fit a linear regression model model = LinearRegression() model.fit(X_normalized, y) return model ``` **Constraints**: - The input array `X` can have any number of samples and features. - Assume that the input data does not contain any missing values. **Expected Output**: The function should return a fitted linear regression model that you can then use to predict or analyze the results on new data. **Validation**: You can validate the correctness of your function by: 1. Generating a synthetic dataset. 2. Applying the function to preprocess the data and fit the model. 3. Checking if the model coefficients are reasonable and if it can make predictions without errors. ```python from sklearn.datasets import make_regression # Generate synthetic data X, y = make_regression(n_samples=100, n_features=3, noise=0.1, random_state=42) # Fit the model model = preprocess_and_fit(X, y) # Print model coefficients print(model.coef_) ```","solution":"import numpy as np from sklearn.preprocessing import StandardScaler, PolynomialFeatures, normalize from sklearn.linear_model import LinearRegression def preprocess_and_fit(X: np.ndarray, y: np.ndarray) -> LinearRegression: Preprocess the dataset and fit a linear regression model. Parameters: X (np.ndarray): 2D array of shape (n_samples, n_features) containing the input features. y (np.ndarray): 1D array of shape (n_samples,) containing the target variable. Returns: LinearRegression: The fitted linear regression model. # 1. Standardize the dataset scaler = StandardScaler() X_scaled = scaler.fit_transform(X) # 2. Generate polynomial features of degree 2 poly = PolynomialFeatures(degree=2) X_poly = poly.fit_transform(X_scaled) # 3. Normalize the dataset to have unit norm X_normalized = normalize(X_poly, norm=\'l2\') # 4. Fit a linear regression model model = LinearRegression() model.fit(X_normalized, y) return model"},{"question":"# Quoted-Printable Encoding and Decoding You are tasked with implementing a custom utility using the `quopri` module to handle quoted-printable encoding and decoding for both files and byte strings. The utility should provide functionality to: 1. Encode the contents of a binary file to quoted-printable format and save the result to a new file. 2. Decode a quoted-printable encoded binary file and save the result to a new file. 3. Encode a given byte string to quoted-printable format. 4. Decode a given quoted-printable encoded byte string. Implement the following class `QuotedPrintableUtility` with the specified methods: ```python import quopri class QuotedPrintableUtility: @staticmethod def encode_file(input_file_path, output_file_path, quotetabs, header=False): Encodes the contents of the input file using quoted-printable encoding and writes to output file. Args: input_file_path (str): Path to the input binary file. output_file_path (str): Path to the output binary file where encoded data will be saved. quotetabs (bool): Flag to determine whether to encode embedded tabs and spaces. header (bool): If true, encodes spaces as underscores as per RFC 1522. pass @staticmethod def decode_file(input_file_path, output_file_path, header=False): Decodes a quoted-printable encoded file and writes the decoded data to an output file. Args: input_file_path (str): Path to the input binary file with quoted-printable encoded data. output_file_path (str): Path to the output binary file where decoded data will be saved. header (bool): If true, decodes underscores as spaces. pass @staticmethod def encode_string(input_string, quotetabs=False, header=False): Encodes a given byte string using quoted-printable encoding. Args: input_string (bytes): The byte string to be encoded. quotetabs (bool): Flag to determine whether to encode embedded tabs and spaces. header (bool): If true, encodes spaces as underscores as per RFC 1522. Returns: bytes: The quoted-printable encoded byte string. pass @staticmethod def decode_string(encoded_string, header=False): Decodes a quoted-printable encoded byte string. Args: encoded_string (bytes): The quoted-printable encoded byte string to be decoded. header (bool): If true, decodes underscores as spaces. Returns: bytes: The decoded byte string. pass ``` # Constraints - The input file contents and byte strings will contain valid ASCII or binary data as per quoted-printable encoding standards. - Ensure the handling of binary file objects during file reading and writing. - Target this utility to be both effective and efficient concerning encoding and decoding operations. # Example Usage ```python # Example file encoding and decoding QuotedPrintableUtility.encode_file(\'input.bin\', \'encoded_output.bin\', quotetabs=True) QuotedPrintableUtility.decode_file(\'encoded_output.bin\', \'decoded_output.bin\') # Example string encoding and decoding encoded_bytes = QuotedPrintableUtility.encode_string(b\\"Hello World!\\") decoded_bytes = QuotedPrintableUtility.decode_string(encoded_bytes) print(decoded_bytes) # Output should be b\'Hello World!\' ``` Implement the above class methods to meet the requirements.","solution":"import quopri class QuotedPrintableUtility: @staticmethod def encode_file(input_file_path, output_file_path, quotetabs=False, header=False): Encodes the contents of the input file using quoted-printable encoding and writes to output file. Args: input_file_path (str): Path to the input binary file. output_file_path (str): Path to the output binary file where encoded data will be saved. quotetabs (bool): Flag to determine whether to encode embedded tabs and spaces. header (bool): If true, encodes spaces as underscores as per RFC 1522. with open(input_file_path, \'rb\') as input_file: with open(output_file_path, \'wb\') as output_file: quopri.encode(input_file, output_file, quotetabs=quotetabs, header=header) @staticmethod def decode_file(input_file_path, output_file_path, header=False): Decodes a quoted-printable encoded file and writes the decoded data to an output file. Args: input_file_path (str): Path to the input binary file with quoted-printable encoded data. output_file_path (str): Path to the output binary file where decoded data will be saved. header (bool): If true, decodes underscores as spaces. with open(input_file_path, \'rb\') as input_file: with open(output_file_path, \'wb\') as output_file: quopri.decode(input_file, output_file, header=header) @staticmethod def encode_string(input_string, quotetabs=False, header=False): Encodes a given byte string using quoted-printable encoding. Args: input_string (bytes): The byte string to be encoded. quotetabs (bool): Flag to determine whether to encode embedded tabs and spaces. header (bool): If true, encodes spaces as underscores as per RFC 1522. Returns: bytes: The quoted-printable encoded byte string. return quopri.encodestring(input_string, quotetabs=quotetabs, header=header) @staticmethod def decode_string(encoded_string, header=False): Decodes a quoted-printable encoded byte string. Args: encoded_string (bytes): The quoted-printable encoded byte string to be decoded. header (bool): If true, decodes underscores as spaces. Returns: bytes: The decoded byte string. return quopri.decodestring(encoded_string, header=header)"},{"question":"**Question: `Keyword and Soft Keyword Categorization`** In this exercise, you are required to create a function `categorize_identifiers` that will take a list of strings and return a dictionary categorizing these strings into three categories: \\"keywords\\", \\"soft keywords\\", and \\"others\\". Use the `keyword` module to determine which strings belong to each category. # Function Signature ```python def categorize_identifiers(identifiers: list) -> dict: # Your code here ``` # Input - `identifiers`: A list of strings (`identifiers`) where each string may be a Python keyword, a Python soft keyword, or neither. # Output - A dictionary with the following structure: ```python { \\"keywords\\": [list of keywords], \\"soft_keywords\\": [list of soft keywords], \\"others\\": [list of strings that are neither] } ``` # Constraints - All elements of `identifiers` are strings with a length of at most 100 characters. - The length of the input list `identifiers` does not exceed 1000. # Example ```python identifiers = [\\"def\\", \\"if\\", \\"match\\", \\"normal\\", \\"class\\", \\"soft_kw\\"] result = categorize_identifiers(identifiers) print(result) # Output should be: # { # \\"keywords\\": [\\"def\\", \\"if\\", \\"class\\"], # \\"soft_keywords\\": [\\"match\\"], # \\"others\\": [\\"normal\\", \\"soft_kw\\"] # } ``` # Performance Requirements - Your implementation should efficiently categorize the identifiers within the constraints. An efficient solution would involve checking each identifier once. # Implementation Notes - You must use the `keyword.iskeyword` and `keyword.issoftkeyword` functions. - You may also leverage `keyword.kwlist` and `keyword.softkwlist` as needed, but direct use of the check functions is recommended.","solution":"import keyword def categorize_identifiers(identifiers: list) -> dict: Categorize the given identifiers into keywords, soft keywords, and others. Parameters: identifiers (list): List of strings to be categorized. Returns: dict: Dictionary categorizing the identifiers. categorized = { \\"keywords\\": [], \\"soft_keywords\\": [], \\"others\\": [] } for identifier in identifiers: if keyword.iskeyword(identifier): categorized[\\"keywords\\"].append(identifier) elif keyword.issoftkeyword(identifier): categorized[\\"soft_keywords\\"].append(identifier) else: categorized[\\"others\\"].append(identifier) return categorized"},{"question":"Coding Assessment Question # Pandas Visualization Challenge Objective: Demonstrate your ability to manipulate data using pandas and create various types of plots with customization options. Dataset You will use a dataset containing information about monthly sales and expenses of a company over two years (2021-2022). The dataset includes the following columns: - `Month`: The month (in format YYYY-MM). - `Sales`: Sales amount in dollars. - `Expenses`: Expenses amount in dollars. - `Category`: The category of expense (e.g., \'Marketing\', \'R&D\', \'Operations\'). An example of the dataset: ``` Month Sales Expenses Category 0 2021-01 1000 400 Marketing 1 2021-02 1500 500 Marketing 2 2021-03 1700 450 R&D ... ``` Tasks: 1. **Data Preparation:** - Read the dataset from a CSV file. - Convert the `Month` column to a datetime format. 2. **Line Plot:** - Create a line plot showing monthly sales and expenses over time. - Customize the plot to include a title, labels for the x and y axes, and a legend. 3. **Bar Plot:** - Create a bar plot showing the total expenses for each category across all months. - Customize the plot to include different colors for each category and a title. 4. **Scatter Plot:** - Create a scatter plot with sales on the x-axis and expenses on the y-axis. - Color the points based on the expense category. - Add a title, labels, and a legend. 5. **Box Plot:** - Create a box plot showing the distribution of sales and expenses, grouped by category. - Customize the plot to include colors and a title. Input: - The path to the CSV file containing the dataset. Output: - Four plots as described above. Constraints: - Use the plot methods from pandas. - Ensure all plots are well-labeled and include a legend for clarity. Performance: - Code should be efficient and make use of pandas’ capabilities for data manipulation. ```python import pandas as pd import matplotlib.pyplot as plt # Read the dataset from a CSV file data = pd.read_csv(\\"path_to_your_file.csv\\") # 1. Data Preparation # Convert the Month column to datetime format data[\'Month\'] = pd.to_datetime(data[\'Month\']) # 2. Line Plot for Sales and Expenses plt.figure(figsize=(10, 6)) plt.plot(data[\'Month\'], data[\'Sales\'], label=\'Sales\', marker=\'o\') plt.plot(data[\'Month\'], data[\'Expenses\'], label=\'Expenses\', marker=\'x\') plt.title(\'Monthly Sales and Expenses\') plt.xlabel(\'Month\') plt.ylabel(\'Amount in Dollars\') plt.legend() plt.grid(True) plt.show() # 3. Bar Plot for Total Expenses per Category total_expenses_by_category = data.groupby(\'Category\')[\'Expenses\'].sum() total_expenses_by_category.plot(kind=\'bar\', color=[\'blue\', \'orange\', \'green\'], figsize=(10, 6)) plt.title(\'Total Expenses per Category\') plt.xlabel(\'Category\') plt.ylabel(\'Total Expenses in Dollars\') plt.show() # 4. Scatter Plot of Sales vs Expenses, colored by Category categories = data[\'Category\'].unique() colors = [\'red\', \'blue\', \'green\'] plt.figure(figsize=(10, 6)) for category, color in zip(categories, colors): subset = data[data[\'Category\'] == category] plt.scatter(subset[\'Sales\'], subset[\'Expenses\'], label=category, color=color) plt.title(\'Sales vs Expenses\') plt.xlabel(\'Sales in Dollars\') plt.ylabel(\'Expenses in Dollars\') plt.legend() plt.grid(True) plt.show() # 5. Box Plot of Sales and Expenses grouped by Category data.boxplot(column=[\'Sales\', \'Expenses\'], by=\'Category\', figsize=(10, 6)) plt.title(\'Distribution of Sales and Expenses by Category\') plt.suptitle(\'\') plt.xlabel(\'Category\') plt.ylabel(\'Amount in Dollars\') plt.show() ``` **Note:** Provide your dataset path in the `pd.read_csv(\\"path_to_your_file.csv\\")`.","solution":"import pandas as pd import matplotlib.pyplot as plt # Function to read the dataset from a CSV file and prepare it def read_and_prepare_data(filepath): data = pd.read_csv(filepath) # Convert the Month column to datetime format data[\'Month\'] = pd.to_datetime(data[\'Month\']) return data # Function to create a line plot for sales and expenses def plot_sales_and_expenses(data): plt.figure(figsize=(10, 6)) plt.plot(data[\'Month\'], data[\'Sales\'], label=\'Sales\', marker=\'o\') plt.plot(data[\'Month\'], data[\'Expenses\'], label=\'Expenses\', marker=\'x\') plt.title(\'Monthly Sales and Expenses\') plt.xlabel(\'Month\') plt.ylabel(\'Amount in Dollars\') plt.legend() plt.grid(True) plt.show() # Function to create a bar plot for total expenses per category def plot_total_expenses_by_category(data): total_expenses_by_category = data.groupby(\'Category\')[\'Expenses\'].sum() total_expenses_by_category.plot(kind=\'bar\', color=[\'blue\', \'orange\', \'green\'], figsize=(10, 6)) plt.title(\'Total Expenses per Category\') plt.xlabel(\'Category\') plt.ylabel(\'Total Expenses in Dollars\') plt.show() # Function to create a scatter plot of sales vs expenses def plot_scatter_sales_vs_expenses(data): categories = data[\'Category\'].unique() colors = [\'red\', \'blue\', \'green\'] plt.figure(figsize=(10, 6)) for category, color in zip(categories, colors): subset = data[data[\'Category\'] == category] plt.scatter(subset[\'Sales\'], subset[\'Expenses\'], label=category, color=color) plt.title(\'Sales vs Expenses\') plt.xlabel(\'Sales in Dollars\') plt.ylabel(\'Expenses in Dollars\') plt.legend() plt.grid(True) plt.show() # Function to create a box plot of sales and expenses grouped by category def plot_box_sales_and_expenses(data): data.boxplot(column=[\'Sales\', \'Expenses\'], by=\'Category\', figsize=(10, 6)) plt.title(\'Distribution of Sales and Expenses by Category\') plt.suptitle(\'\') plt.xlabel(\'Category\') plt.ylabel(\'Amount in Dollars\') plt.show()"},{"question":"Background You are required to implement a Python script that can be used to log system usage statistics into a file. The script should accept command-line arguments for configuration (such as specifying the log file name and logging level). The script will utilize logging to capture and store current system time and available memory in a file. Requirements 1. **Command-Line Arguments:** - `-f` or `--file`: The name of the log file where the log entries will be saved (default: `system.log`). - `-l` or `--level`: Logging level which can be `DEBUG`, `INFO`, `WARNING`, `ERROR`, or `CRITICAL` (default: `INFO`). 2. **Logging Functionality:** - Configure logging based on the specified log file and logging level. - Log entries should include: - Timestamp of the log entry. - A message containing the current system time and available memory (in MB). 3. **File I/O:** - Use the `os` module to retrieve system memory information. - Ensure that the log file and paths are correctly handled. Specifications - **Input:** - Command-line arguments for specifying the log file name and logging level. - **Output:** - A log file with the name specified in the command-line argument containing the log entries. - **Constraints:** - If no command-line arguments are provided, defaults should be used. - Use appropriate logging levels as specified. - **Performance:** - Ensure that the log file writing is efficient. - Handle file paths and potential file I/O errors gracefully. Example Usage ```bash python log_system.py --file output.log --level DEBUG ``` Implementation Skeleton ```python import argparse import logging import os import time def parse_arguments(): parser = argparse.ArgumentParser(description=\\"System Usage Logger\\") parser.add_argument(\'-f\', \'--file\', type=str, default=\'system.log\', help=\'Log file name\') parser.add_argument(\'-l\', \'--level\', type=str, default=\'INFO\', choices=[\'DEBUG\', \'INFO\', \'WARNING\', \'ERROR\', \'CRITICAL\'], help=\'Logging level\') return parser.parse_args() def configure_logging(file_name, log_level): logging.basicConfig(filename=file_name, level=getattr(logging, log_level.upper()), format=\'%(asctime)s - %(message)s\') def log_system_stats(): current_time = time.strftime(\'%Y-%m-%d %H:%M:%S\') mem_info = os.sysconf(\'SC_PAGE_SIZE\') * os.sysconf(\'SC_PHYS_PAGES\') / (1024 ** 2) logging.info(f\'Current Time: {current_time}, Available Memory: {mem_info} MB\') if __name__ == \\"__main__\\": args = parse_arguments() configure_logging(args.file, args.level) log_system_stats() ``` # Tasks 1. Complete the `log_system_stats` function to correctly log the system time and available memory. 2. Ensure that the script handles command-line arguments correctly and uses defaults when needed. 3. Verify that the logging levels and configurations are correctly applied.","solution":"import argparse import logging import os import time def parse_arguments(): parser = argparse.ArgumentParser(description=\\"System Usage Logger\\") parser.add_argument(\'-f\', \'--file\', type=str, default=\'system.log\', help=\'Log file name\') parser.add_argument(\'-l\', \'--level\', type=str, default=\'INFO\', choices=[\'DEBUG\', \'INFO\', \'WARNING\', \'ERROR\', \'CRITICAL\'], help=\'Logging level\') return parser.parse_args() def configure_logging(file_name, log_level): logging.basicConfig(filename=file_name, level=getattr(logging, log_level.upper()), format=\'%(asctime)s - %(message)s\') def log_system_stats(): current_time = time.strftime(\'%Y-%m-%d %H:%M:%S\') mem_info = os.sysconf(\'SC_PAGE_SIZE\') * os.sysconf(\'SC_PHYS_PAGES\') / (1024 ** 2) logging.info(f\'Current Time: {current_time}, Available Memory: {mem_info:.2f} MB\') if __name__ == \\"__main__\\": args = parse_arguments() configure_logging(args.file, args.level) log_system_stats()"},{"question":"# Task: Implement a Custom Data Structure Implement a Python class called `DataProcessor` that processes data with the following requirements: 1. **Initialization**: - The `DataProcessor` should be initialized with a list of numbers. If no list is provided during initialization, an empty list should be used. 2. **Instance Methods**: - Implement the method `add_number(num)` which adds a number to the list. - Implement the method `remove_number(num)` which removes the first occurrence of the number from the list. If the number is not found in the list, it raises a `ValueError` with a custom message. - Create a method `average()` which returns the arithmetic average of the numbers in the list. If the list is empty, it returns `None`. 3. **Class Method**: - Implement a class method `from_string(data_string)` that takes a string of comma-separated numbers and returns an instance of `DataProcessor` initialized with the provided numbers. If the string is empty, it returns an instance with an empty list. 4. **String Representation**: - Override the `__str__` and `__repr__` methods to return the string representation of the current list formatted as `\\"DataProcessor([num1, num2, ...])\\"`. 5. **Comprehensions**: - Implement a method `squares()` that returns a list of the squares of all numbers in the list using list comprehension. # Inputs and Outputs - `DataProcessor([1, 2, 3])` creates an instance with the list `[1, 2, 3]`. - `add_number(num)` adds `num` to the data list. - `remove_number(num)` removes `num` from the data list. If `num` is not found, raises `ValueError`. - `average()` returns the average of the numbers or `None` if the list is empty. - `from_string(data_string)` returns an instance of `DataProcessor` initialized with numbers from the `data_string`. - `__str__` and `__repr__` return the string `\\"DataProcessor([num1, num2, ...])\\"`. - `squares()` returns a list of squares of numbers in the data list. # Constraints - You must use list comprehension for the `squares` method. - You may assume that all inputs to the `from_string` method are valid numerical strings separated by commas. # Example Usage ```python # Initialize with a list of numbers dp = DataProcessor([1, 2, 3]) # Add a number to the list dp.add_number(4) # Now the list is [1, 2, 3, 4] # Remove a number from the list dp.remove_number(2) # Now the list is [1, 3, 4] # Get the average of numbers print(dp.average()) # Output: 2.6666666666666665 # Create an instance from a comma-separated string of numbers dp2 = DataProcessor.from_string(\\"5,6,7\\") # Now dp2 list is [5, 6, 7] # String representation print(dp2) # Output: DataProcessor([5, 6, 7]) # Get squares of numbers in the list print(dp2.squares()) # Output: [25, 36, 49] ``` # Additional Notes - Ensure to handle edge cases such as removing a number that doesn’t exist in the list, averaging an empty list, and handling an empty initialization. - Utilize appropriate exception handling and message passing when errors or invalid operations occur.","solution":"class DataProcessor: def __init__(self, numbers=None): if numbers is None: self.numbers = [] else: self.numbers = numbers def add_number(self, num): self.numbers.append(num) def remove_number(self, num): if num in self.numbers: self.numbers.remove(num) else: raise ValueError(f\\"Number {num} not found in the list.\\") def average(self): if not self.numbers: return None return sum(self.numbers) / len(self.numbers) @classmethod def from_string(cls, data_string): if data_string.strip(): numbers = list(map(int, data_string.split(\',\'))) else: numbers = [] return cls(numbers) def squares(self): return [num ** 2 for num in self.numbers] def __str__(self): return f\\"DataProcessor({self.numbers})\\" def __repr__(self): return self.__str__()"},{"question":"Objective Write a Python function that reads a `setup.cfg` file, parses the configurations related to a specified Distutils command, and returns these configurations as a dictionary. Problem Statement You need to implement a function called `parse_setup_config` which takes two parameters: 1. `file_path` (str): The path to the `setup.cfg` file. 2. `command` (str): The Distutils command section whose configurations you want to parse. The function should return a dictionary where the keys are option names and the values are the corresponding option values from the `setup.cfg` file for the specified command. Input - `file_path` (str): File path to the `setup.cfg`. - `command` (str): The command section to parse (e.g., \\"build_ext\\"). Output - `dict`: A dictionary containing the configurations for the specified command. If the command section does not exist in the file, return an empty dictionary. If the file does not exist, the function should raise a `FileNotFoundError`. Constraints - The `setup.cfg` file follows the syntax described in the provided documentation. - Assume that the sections and options in the `setup.cfg` file are correctly formatted. - The command sections and options are case-sensitive. Example Given the following `setup.cfg` file: ``` [build_ext] inplace=1 include_dirs=include [install] optimize=1 [bdist_rpm] release=1 packager=John Doe <jdoe@example.com> doc_files=CHANGES.txt README.txt USAGE.txt doc/ examples/ ``` When `parse_setup_config(\'setup.cfg\', \'build_ext\')` is called: **Output:** ```python { \'inplace\': \'1\', \'include_dirs\': \'include\' } ``` Function Signature ```python def parse_setup_config(file_path: str, command: str) -> dict: pass ``` Notes - The function should handle multi-line values appropriately by concatenating them into a single string with spaces. - Blank lines and comments in the configuration file should be ignored. - Use appropriate exception handling to manage file operations.","solution":"import configparser def parse_setup_config(file_path: str, command: str) -> dict: Parses the setup.cfg file to get the configurations for a specified command. Args: file_path (str): The path to the setup.cfg file. command (str): The command section to parse (e.g., \\"build_ext\\"). Returns: dict: A dictionary containing the configurations for the specified command. parser = configparser.ConfigParser() # Check if the file exists and read it try: with open(file_path, \'r\') as f: parser.read_file(f) except FileNotFoundError: raise FileNotFoundError(f\\"The file {file_path} does not exist.\\") # Check if the command section exists in the file if command in parser: config_dict = dict(parser[command]) else: config_dict = {} return config_dict"},{"question":"# Question: Implement a Module Reloader using the `imp` module Using the deprecated `imp` module in Python, implement a function `reload_module(path: str) -> None` that mimics the module reloading process. The function should: 1. Use `imp.find_module` to locate the module given its path. 2. Use `imp.load_module` to load the module and ensure that it can be reloaded. 3. Close the file handle correctly if it’s opened during the process. Additionally, handle exceptions gracefully: - If the module is not found, print an appropriate message. - If there are issues loading the module, print the error details. # Implementation Details: - **Function Signature**: ```python def reload_module(path: str) -> None: pass ``` - **Input**: - `path` (str): The filesystem path to the Python module (e.g., \\"/foo/bar/baz.py\\"). - **Output**: - None. The function should print appropriate messages based on the outcomes as described. - **Constraints**: - Use of `imp` module functions as described (with consideration of their deprecated status). - Properly manage resource allocation, specifically ensure file handles are closed. # Example Usage: ```python # Assuming /foo/bar/baz.py exists and is a valid module reload_module(\\"/foo/bar/baz.py\\") # Output might include: # Module baz reloaded successfully. ``` **Note**: Although the `imp` module is deprecated, this exercise aims to test your ability to work with module internals and manage resources properly in Python.","solution":"import imp import os def reload_module(path: str) -> None: try: module_name = os.path.splitext(os.path.basename(path))[0] file, pathname, description = imp.find_module(module_name, [os.path.dirname(path)]) try: imp.load_module(module_name, file, pathname, description) print(f\\"Module {module_name} reloaded successfully.\\") except Exception as e: print(f\\"Error loading module {module_name}: {e}\\") finally: if file: file.close() except ImportError as e: print(f\\"Module not found: {e}\\")"},{"question":"Complex Number Arithmetic Manager **Objective**: Implement a Python class `ComplexNumberManager` that demonstrates the usage of complex number functionalities as described in the provided documentation. Class: `ComplexNumberManager` Your task is to define a class `ComplexNumberManager` that can perform the following operations on complex numbers using the Python C-API functions described. # Methods: 1. **`__init__(self)`**: Initializes the complex number manager, no parameters. 2. **`create_complex_number(self, real: float, imag: float) -> complex`**: - Input: Two float values representing the real and imaginary parts of the complex number. - Output: A new Python complex number. 3. **`add(self, num1: complex, num2: complex) -> complex`**: - Input: Two Python complex numbers. - Output: Sum of `num1` and `num2` as a Python complex number using the `Py_complex` structure. 4. **`subtract(self, num1: complex, num2: complex) -> complex`**: - Input: Two Python complex numbers. - Output: Difference between `num1` and `num2` as a Python complex number using the `Py_complex` structure. 5. **`multiply(self, num1: complex, num2: complex) -> complex`**: - Input: Two Python complex numbers. - Output: Product of `num1` and `num2` as a Python complex number using the `Py_complex` structure. 6. **`divide(self, num1: complex, num2: complex) -> complex`**: - Input: Two Python complex numbers. - Output: Quotient of `num1` divided by `num2` as a Python complex number using the `Py_complex` structure. - Constraint: If the divisor (`num2`) is zero, return `None` and set an appropriate error message. 7. **`exponentiate(self, num: complex, exp: complex) -> complex`**: - Input: One base complex number and one exponent complex number. - Output: The result of raising `num` to the power of `exp` as a Python complex number using the `Py_complex` structure. Constraints and Limitations: - The methods must use the conversion functions between Python complex objects and C `Py_complex` structures as described in the documentation. - Operations must handle edge cases such as zero division and return appropriate error responses. - Ensure performance efficiency when converting between data types. Example Usage: ```python # Initialize Complex Number Manager cnm = ComplexNumberManager() # Create complex numbers comp1 = cnm.create_complex_number(1.5, -2.5) comp2 = cnm.create_complex_number(-3.0, 4.0) # Perform operations sum_result = cnm.add(comp1, comp2) diff_result = cnm.subtract(comp1, comp2) prod_result = cnm.multiply(comp1, comp2) quot_result = cnm.divide(comp1, comp2) pow_result = cnm.exponentiate(comp1, comp2) # Output results print(f\\"Sum: {sum_result}, Difference: {diff_result}, Product: {prod_result}, Quotient: {quot_result}, Power: {pow_result}\\") ``` Submission: Submit the `ComplexNumberManager` class definition and ensure it correctly implements all required methods as described.","solution":"class ComplexNumberManager: def __init__(self): pass def create_complex_number(self, real: float, imag: float) -> complex: return complex(real, imag) def add(self, num1: complex, num2: complex) -> complex: return num1 + num2 def subtract(self, num1: complex, num2: complex) -> complex: return num1 - num2 def multiply(self, num1: complex, num2: complex) -> complex: return num1 * num2 def divide(self, num1: complex, num2: complex) -> complex: if num2 == 0: return None # Division by zero error return num1 / num2 def exponentiate(self, num: complex, exp: complex) -> complex: return num ** exp"},{"question":"**Objective**: Demonstrate your understanding of the `marshal` module for object serialization and deserialization. Using the `marshal` module, implement a function capable of saving an arbitrary Python object to a file and later reading it back. Your function should handle cases where the saved object\'s types are unsupported by `marshal`. **Function Specifications**: 1. **Function Name**: `save_and_load_object` 2. **Arguments**: - `obj`: The Python object to be saved. This can include any supported type or a combination of them. - `version`: An optional integer specifying the version of the `marshal` format. Default is `marshal.version`. 3. **Output**: - Return the object after it has been saved to a file and then loaded back. 4. **Constraints**: - If the object contains any unsupported type, it should save the file, return the exact same object while ignoring unsupported cases. - Manage exceptions appropriately and ensure the function is robust to errors. **Function Signature**: ```python import marshal import os def save_and_load_object(obj, version=marshal.version): filename = \\"temp_marshal_file.bin\\" try: # Save the object to the file with open(filename, \'wb\') as file: marshal.dump(obj, file, version=version) # Load the object back from the file with open(filename, \'rb\') as file: loaded_obj = marshal.load(file) return loaded_obj except ValueError as ve: print(f\\"ValueError: {ve}\\") return obj # Return the original object if there was an error except Exception as e: print(f\\"An error occurred: {e}\\") return None finally: # Clean up the file if os.path.exists(filename): os.remove(filename) ``` # Example Usage: ```python # Example 1: Supported object types my_dict = {\\"key1\\": \\"value1\\", \\"key2\\": 42} result = save_and_load_object(my_dict) assert result == my_dict # Example 2: Unsupported object types class UnsupportedClass: pass unsupported_obj = UnsupportedClass() result = save_and_load_object(unsupported_obj) assert result == unsupported_obj ``` **Additional Notes**: - Pay attention to handling exceptions correctly. - Ensure temporary files are cleaned up after use to avoid clutter. - Performance is not the primary focus, but ensure the function works efficiently within reasonable constraints.","solution":"import marshal import os def save_and_load_object(obj, version=marshal.version): filename = \\"temp_marshal_file.bin\\" try: # Save the object to the file with open(filename, \'wb\') as file: marshal.dump(obj, file, version=version) # Load the object back from the file with open(filename, \'rb\') as file: loaded_obj = marshal.load(file) return loaded_obj except ValueError as ve: print(f\\"ValueError: {ve}\\") return obj # Return the original object if there was an error except Exception as e: print(f\\"An error occurred: {e}\\") return obj # Return the original object if there was an error finally: # Clean up the file if os.path.exists(filename): os.remove(filename)"},{"question":"You\'re required to implement a simple TCP echo server using Python\'s `select` module. The server should handle multiple clients simultaneously by leveraging the `select.select()` function. Objective: Create a TCP echo server that can: 1. Accept multiple client connections. 2. Read data from the clients and echo it back to them. 3. Handle socket closure and errors gracefully. Requirements: - **Function to Implement:** `run_echo_server(host: str, port: int)` - **Input:** - `host` (str): The host address on which the server will listen. - `port` (int): The port number on which the server will listen. - **Operation:** - The server should use non-blocking sockets. - Use the `select.select()` function to monitor multiple client sockets. - For every connected client, read the data when it\'s available and write it back (echo) to the client. - Properly handle clients that disconnect. - **Output:** None. The function runs the server indefinitely until it is manually stopped (e.g., using a keyboard interrupt). Constraints: - The server should handle up to 100 simultaneous client connections. - It should handle partial reads and writes correctly. - Typical message size will be limited to 1 KB but should handle messages up to 4 KB. Tips: - Use a dictionary to manage client sockets and their buffers. - Remember to clean up the sockets and remove them from the monitoring list when a client disconnects. - Handle exceptions like `socket.error` or `ConnectionResetError` to ensure the server remains running even if individual clients encounter issues. Example Usage: ```python if __name__ == \\"__main__\\": run_echo_server(\\"127.0.0.1\\", 12345) ``` This will start the echo server on `localhost` and port `12345`.","solution":"import socket import select def run_echo_server(host: str, port: int): Runs a TCP echo server that listens to the specified host and port. Accepts multiple client connections, reads data from the clients, and echoes it back to them. Parameters: host (str): The host address on which to listen. port (int): The port number on which to listen. server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM) server_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1) server_socket.setblocking(False) server_socket.bind((host, port)) server_socket.listen(100) inputs = [server_socket] outputs = [] message_queues = {} while True: readable, writable, exceptional = select.select(inputs, outputs, inputs) for s in readable: if s is server_socket: client_socket, client_address = server_socket.accept() client_socket.setblocking(False) inputs.append(client_socket) message_queues[client_socket] = b\\"\\" else: data = s.recv(4096) if data: message_queues[s] += data if s not in outputs: outputs.append(s) else: if s in outputs: outputs.remove(s) inputs.remove(s) s.close() del message_queues[s] for s in writable: if s in message_queues and message_queues[s]: next_msg = message_queues[s] s.send(next_msg) message_queues[s] = b\\"\\" if s in outputs: outputs.remove(s) for s in exceptional: inputs.remove(s) if s in outputs: outputs.remove(s) s.close() del message_queues[s] if __name__ == \\"__main__\\": run_echo_server(\\"127.0.0.1\\", 12345)"},{"question":"**Question: Implementing a Thread-Safe Counter** You are required to implement a thread-safe counter using Python\'s low-level threading capabilities provided by the `_thread` module. The counter should support the following operations in a thread-safe manner: 1. **Increment:** Increases the counter by a specified amount. 2. **Decrement:** Decreases the counter by a specified amount. 3. **Get Value:** Returns the current value of the counter. You must ensure that multiple threads can increment and decrement the counter concurrently without causing race conditions. # Specifications 1. **Class Definition:** - Define a class `ThreadSafeCounter`. 2. **Methods:** - `__init__(self)`: Initializes the counter to 0 and creates the necessary lock for synchronization. - `increment(self, value)`: Increments the counter by `value`. - `decrement(self, value)`: Decrements the counter by `value`. - `get_value(self) -> int`: Returns the current value of the counter. 3. **Concurrency Management:** - Use `_thread.allocate_lock` to create a lock. - Use `lock.acquire()` and `lock.release()` to ensure that increments and decrements are thread-safe. 4. **Exception Handling:** - If an error occurs in any thread while modifying the counter, the exception should be handled gracefully without crashing the program. 5. **Performance Requirements:** - Ensure that acquiring and releasing the lock does not cause significant performance degradation. # Example Usage ```python import _thread import time class ThreadSafeCounter: def __init__(self): self.counter = 0 self.lock = _thread.allocate_lock() def increment(self, value): with self.lock: self.counter += value def decrement(self, value): with self.lock: self.counter -= value def get_value(self) -> int: with self.lock: return self.counter # Testing the ThreadSafeCounter in a multi-threaded environment def thread_increment(counter, n): for _ in range(n): counter.increment(1) def thread_decrement(counter, n): for _ in range(n): counter.decrement(1) counter = ThreadSafeCounter() # Start threads to increment the counter for _ in range(5): _thread.start_new_thread(thread_increment, (counter, 100)) # Start threads to decrement the counter for _ in range(3): _thread.start_new_thread(thread_decrement, (counter, 100)) # Allow threads to complete time.sleep(1) # Print the counter value print(counter.get_value()) ``` # Constraints - Use only the `_thread` module for threading and synchronization. - Ensure the solution handles high contention scenarios efficiently. - You can assume the increment and decrement values are always positive integers.","solution":"import _thread class ThreadSafeCounter: def __init__(self): self.counter = 0 self.lock = _thread.allocate_lock() def increment(self, value): with self.lock: self.counter += value def decrement(self, value): with self.lock: self.counter -= value def get_value(self) -> int: with self.lock: return self.counter"},{"question":"# Function Object Manipulation in Python **Objective:** Write a Python function utilizing the `ctypes` library to interact with Python function objects at the C level. You are to create a new Python function object with a specified code object and a global variables dictionary. Additionally, you should be able to retrieve and manipulate the function object\'s attributes, such as argument defaults and annotations. **Description:** Implement a function `create_and_modify_function` that does the following: 1. **Create a New Function Object** - Accept a code object and a dictionary for global variables as parameters. - Utilize `PyFunction_New` to create a new function object. - Return this function object. 2. **Set Function Defaults** - Accept a tuple of default values and set these as the defaults for the function object. 3. **Set Function Annotations** - Accept a dictionary of annotations and set these as the annotations for the function object. 4. **Retrieve Function Information** - Retrieve and return the function object\'s code, globals, defaults, and annotations. **Function Signature:** ```python import ctypes def create_and_modify_function(code, globals_dict, defaults, annotations): Create a new function object and modify its attributes. Parameters: - code: A Python code object. - globals_dict: A dictionary containing global variables. - defaults: A tuple of default argument values. - annotations: A dictionary containing function annotations. Returns: - A dictionary with keys \'code\', \'globals\', \'defaults\', \'annotations\' containing the respective attributes of the function object. pass ``` **Constraints:** - Assume the code object and globals dictionary are correctly provided and do not need additional validation. - The function should handle setting and getting the specified attributes robustly, raising appropriate errors if any manipulation fails. **Example usage:** ```python def example_function(a, b=5): Example function. return a + b code_obj = example_function.__code__ globals_dict = globals() defaults = (10,) annotations = {\'a\': int, \'b\': int, \'return\': int} result = create_and_modify_function(code_obj, globals_dict, defaults, annotations) print(result) ``` **Expected Output:** ```python { \'code\': <code object example_function>, \'globals\': {\'__name__\': \'__main__\', ...}, \'defaults\': (10,), \'annotations\': {\'a\': int, \'b\': int, \'return\': int} } ``` Ensure that your implementation correctly interacts with and modifies the function object using the provided API functions from the `ctypes` library.","solution":"import ctypes # Define necessary constants and types from the Python C API. PyFunction_New = ctypes.pythonapi.PyFunction_New PyFunction_New.argtypes = [ctypes.py_object, ctypes.py_object] PyFunction_New.restype = ctypes.py_object def create_and_modify_function(code, globals_dict, defaults, annotations): Create a new function object and modify its attributes. Parameters: - code: A Python code object. - globals_dict: A dictionary containing global variables. - defaults: A tuple of default argument values. - annotations: A dictionary containing function annotations. Returns: - A dictionary with keys \'code\', \'globals\', \'defaults\', \'annotations\' containing the respective attributes of the function object. # Create the new function object func = PyFunction_New(code, globals_dict) # Set function defaults func.__defaults__ = defaults # Set function annotations func.__annotations__ = annotations # Retrieve and return the function attributes result = { \'code\': func.__code__, \'globals\': func.__globals__, \'defaults\': func.__defaults__, \'annotations\': func.__annotations__ } return result"},{"question":"# Coding Challenge **Objective:** To assess your knowledge of the `errno` module and your ability to handle system errors in Python. **Task:** Write a Python function named `handle_error(error_code)` that takes an integer `error_code` as its input and performs the following: 1. **Translate Error Code:** - The function should translate the provided `error_code` into a human-readable error message using the `os.strerror` function. 2. **Raise Specific Exception:** - The function should raise the appropriate Python exception corresponding to the given `error_code` if one is mapped. If there is no specific Python exception mapped, raise a generic `OSError`. 3. **Return Error Message:** - Return the human-readable error message. **Input:** - An integer `error_code` representing a system error code. **Output:** - Return a string which is the human-readable error message corresponding to the input error code. - Raise an appropriate exception based on the error code. **Example:** ```python import errno def handle_error(error_code): import os error_message = os.strerror(error_code) # Check for specific exceptions if error_code in errno.errorcode: exception_name = errno.errorcode[error_code] if exception_name == \\"EPERM\\": raise PermissionError(error_message) elif exception_name == \\"ENOENT\\": raise FileNotFoundError(error_message) elif exception_name == \\"ESRCH\\": raise ProcessLookupError(error_message) elif exception_name == \\"EINTR\\": raise InterruptedError(error_message) elif exception_name == \\"EAGAIN\\": raise BlockingIOError(error_message) elif exception_name == \\"EACCES\\": raise PermissionError(error_message) elif exception_name == \\"ECHILD\\": raise ChildProcessError(error_message) elif exception_name == \\"EEXIST\\": raise FileExistsError(error_message) elif exception_name == \\"ENOTDIR\\": raise NotADirectoryError(error_message) elif exception_name == \\"EISDIR\\": raise IsADirectoryError(error_message) elif exception_name == \\"EBADF\\": raise BrokenPipeError(error_message) elif exception_name == \\"ETIMEDOUT\\": raise TimeoutError(error_message) elif exception_name == \\"ECONNABORTED\\": raise ConnectionAbortedError(error_message) elif exception_name == \\"ECONNRESET\\": raise ConnectionResetError(error_message) elif exception_name == \\"ECONNREFUSED\\": raise ConnectionRefusedError(error_message) elif exception_name == \\"EHOSTUNREACH\\": raise BrokenPipeError(error_message) elif exception_name == \\"EALREADY\\": raise BlockingIOError(error_message) elif exception_name == \\"EADDRINUSE\\": raise OSError(error_message) # Default for non-specific else: raise OSError(error_message) # Default for non-specific return error_message # Example Usage: try: handle_error(errno.ENOENT) # This should raise FileNotFoundError except Exception as e: print(e) # Output: No such file or directory ``` **Constraints:** - Assume that the input `error_code` is always a valid integer. - You can assume that the module `errno` is fully imported. **Notes:** - You should leverage the `errno` module to map error codes to exceptions and error messages properly. - Consider all the provided error codes and their corresponding exceptions as per the given documentation. **Performance Requirements:** - The function should handle the translation and exception raising efficiently.","solution":"import errno import os def handle_error(error_code): Translates a system error code to a human-readable message and raises the appropriate exception. Args: error_code (int): The system error code. Raises: PermissionError: For error code EPERM or EACCES. FileNotFoundError: For error code ENOENT. ProcessLookupError: For error code ESRCH. InterruptedError: For error code EINTR. BlockingIOError: For error code EAGAIN or EALREADY. ChildProcessError: For error code ECHILD. FileExistsError: For error code EEXIST. NotADirectoryError: For error code ENOTDIR. IsADirectoryError: For error code EISDIR. BrokenPipeError: For error code EBADF or EHOSTUNREACH. TimeoutError: For error code ETIMEDOUT. ConnectionAbortedError: For error code ECONNABORTED. ConnectionResetError: For error code ECONNRESET. ConnectionRefusedError: For error code ECONNREFUSED. OSError: For other error codes. Returns: str: The human-readable error message. error_message = os.strerror(error_code) exception_map = { errno.EPERM: PermissionError, errno.ENOENT: FileNotFoundError, errno.ESRCH: ProcessLookupError, errno.EINTR: InterruptedError, errno.EAGAIN: BlockingIOError, errno.EACCES: PermissionError, errno.ECHILD: ChildProcessError, errno.EEXIST: FileExistsError, errno.ENOTDIR: NotADirectoryError, errno.EISDIR: IsADirectoryError, errno.EBADF: BrokenPipeError, errno.ETIMEDOUT: TimeoutError, errno.ECONNABORTED: ConnectionAbortedError, errno.ECONNRESET: ConnectionResetError, errno.ECONNREFUSED: ConnectionRefusedError, errno.EHOSTUNREACH: BrokenPipeError, errno.EALREADY: BlockingIOError, errno.EADDRINUSE: OSError, } if error_code in exception_map: raise exception_map[error_code](error_message) else: raise OSError(error_message) return error_message"},{"question":"# Permutation Feature Importance Analysis Objective You are tasked with evaluating the importance of features in a dataset for a regression problem using permutation feature importance. You will train a regression model, compute feature importances using permutations, and handle correlated features. Your implementation should follow the guidelines and constraints provided. Instructions 1. Load the **Diabetes dataset** from `sklearn.datasets`. 2. Split the dataset into training and validation sets. 3. Train a Ridge regression model using the training set. 4. Compute the permutation feature importances using the validation set with the `permutation_importance` function. 5. Identify and handle any correlated features before computing the final feature importances. 6. Return the final feature importances and rank them in descending order along with their importances mean and standard deviations. Function Signature ```python def permutation_feature_importance_analysis(n_repeats: int = 30, random_state: int = 0) -> None: pass ``` Example ```python result = permutation_feature_importance_analysis(n_repeats=30, random_state=0) for feature, importance_mean, importance_std in result: print(f\\"{feature:<10} {importance_mean:.3f} +/- {importance_std:.3f}\\") ``` Expected Output - A list of tuples containing feature names, their mean importance, and their standard deviation in descending order based on their importance. Guidelines - Use `load_diabetes` from `sklearn.datasets`. - Use `train_test_split` from `sklearn.model_selection` to split the data. - Train a `Ridge` regression model with an `alpha` parameter of `1e-2`. - Use `permutation_importance` from `sklearn.inspection` to compute feature importances. - Print each feature importance in the format specified. - Implement a function to handle correlated features, for instance, by using hierarchical clustering and removing one feature from each correlated cluster. Additional Notes - Ensure your code handles correlated features appropriately, as permuting one of the correlated features might not reflect its true importance. - The solution should be optimized for readability and efficiency.","solution":"from sklearn.datasets import load_diabetes from sklearn.model_selection import train_test_split from sklearn.linear_model import Ridge from sklearn.inspection import permutation_importance from scipy.cluster.hierarchy import linkage, fcluster import numpy as np def handle_correlated_features(X, threshold=0.9): Identify and handle correlated features by clustering and returning one feature per cluster. corr_matrix = np.corrcoef(X, rowvar=False) dis_matrix = 1 - np.abs(corr_matrix) linked = linkage(dis_matrix, \'single\') cluster_ids = fcluster(linked, threshold, criterion=\'distance\') selected_features = [] for cluster_id in np.unique(cluster_ids): selected_features.append(np.where(cluster_ids == cluster_id)[0][0]) return X[:, selected_features], selected_features def permutation_feature_importance_analysis(n_repeats: int = 30, random_state: int = 0) -> None: # Load dataset diabetes = load_diabetes() X, y = diabetes.data, diabetes.target # Handle correlated features X, selected_features = handle_correlated_features(X, threshold=0.9) feature_names = [diabetes.feature_names[i] for i in selected_features] # Train-test split X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=random_state) # Train Ridge regression model model = Ridge(alpha=1e-2, random_state=random_state) model.fit(X_train, y_train) # Compute permutation feature importances result = permutation_importance(model, X_val, y_val, n_repeats=n_repeats, random_state=random_state) # Extract and sort importances feature_importances = [] for i in range(len(feature_names)): feature_importances.append((feature_names[i], result.importances_mean[i], result.importances_std[i])) feature_importances.sort(key=lambda x: x[1], reverse=True) return feature_importances"},{"question":"Objective: The goal of this question is to assess your understanding of asynchronous execution using the `torch.futures` package. You will write code to handle multiple asynchronous operations and combine their results. Problem Statement: You are given an asynchronous function `fetch_data` that fetches data from various sources. This function returns a `torch.futures.Future` object. Your task is to implement a function `process_multiple_sources` that takes a list of source identifiers, uses the `fetch_data` function to asynchronously fetch data from all sources, waits for all fetch operations to complete, and then aggregates the results. Function Signature: ```python def process_multiple_sources(sources: List[int]) -> List: pass ``` Input: - `sources`: A list of integers where each integer represents a unique data source identifier. Output: - Returns a list of results from all data sources, ordered by the appearance of sources in the input list. Example: Suppose `fetch_data` works as follows: ```python def fetch_data(source: int) -> torch.futures.Future: # Simulating an async fetch operation future = torch.futures.Future() def async_fetch(): import time import random time.sleep(random.uniform(0.1, 0.5)) future.set_result(f\\"data_from_source_{source}\\") import threading threading.Thread(target=async_fetch).start() return future ``` For the input: ```python sources = [1, 2, 3] ``` The output should be: ```python [\'data_from_source_1\', \'data_from_source_2\', \'data_from_source_3\'] ``` Note: The result order must match the input order of `sources`. Constraints: 1. You must use the `torch.futures` package and its utilities. 2. Handle potential delays in asynchronous tasks appropriately. 3. Ensure the main thread/process waits for all asynchronous operations to complete before returning results. Good Luck!","solution":"from typing import List import torch import threading import time def fetch_data(source: int) -> torch.futures.Future: # Simulating an async fetch operation future = torch.futures.Future() def async_fetch(): time.sleep(0.1) # Simulate some delay future.set_result(f\\"data_from_source_{source}\\") threading.Thread(target=async_fetch).start() return future def process_multiple_sources(sources: List[int]) -> List: futures = [fetch_data(source) for source in sources] results = [future.wait() for future in futures] return results"},{"question":"**Python Coding Assessment** # Overview You are required to implement a Python function that can handle the conversion between Python\'s arbitrary precision integers and C\'s fixed-size integer types. This function should handle errors and overflow properly, leveraging the principles of Python 3.10 integer object management. # Objective Implement a function `convert_integer` that takes a Python integer and returns its representation in a specified C type. The function should handle the following C integer types: - `long` - `unsigned long` - `long long` - `unsigned long long` The function should raise appropriate exceptions for overflow and invalid conversions. # Function Signature ```python def convert_integer(value: int, target_type: str) -> Union[int, None]: Convert a Python integer to a specified C integer type. Args: - value: The Python integer to be converted. - target_type: A string specifying the target C integer type. Must be one of [\\"long\\", \\"unsigned long\\", \\"long long\\", \\"unsigned long long\\"] Returns: - C integer representation of the value if conversion is successful. Raises: - ValueError: If the target_type is invalid or the value cannot be converted due to overflow. ``` # Requirements 1. **Inputs**: - `value`: An integer representing the value to be converted. - `target_type`: A string that can be `\\"long\\"`, `\\"unsigned long\\"`, `\\"long long\\"`, or `\\"unsigned long long\\"`. 2. **Output**: - The function should return the C integer representation of the value if successful. 3. **Constraints**: - The function should raise a `ValueError` if the `target_type` is invalid. - The function should handle overflow cases appropriately using Python exceptions. # Examples: ```python # Example 1: Convert to long print(convert_integer(1234567890, \\"long\\")) # Expected output: 1234567890 # Example 2: Handle overflow when converting to unsigned long print(convert_integer(-1, \\"unsigned long\\")) # Expected output: ValueError # Example 3: Convert to unsigned long long print(convert_integer(12345678901234567890, \\"unsigned long long\\")) # Expected output: 12345678901234567890 # Example 4: Invalid target type print(convert_integer(1234567890, \\"float\\")) # Expected output: ValueError ``` # Notes - Utilize the functions `PyLong_As*` and `PyLong_From*` as mentioned in the documentation to achieve the conversions. - Proper error handling is crucial. Ensure that your function raises `ValueError` with a descriptive message for any error encountered during conversion.","solution":"def convert_integer(value: int, target_type: str) -> int: Convert a Python integer to a specified C integer type. Args: - value: The Python integer to be converted. - target_type: A string specifying the target C integer type. Must be one of [\\"long\\", \\"unsigned long\\", \\"long long\\", \\"unsigned long long\\"] Returns: - C integer representation of the value if conversion is successful. Raises: - ValueError: If the target_type is invalid or the value cannot be converted due to overflow. # Define the bounds for each C integer type bounds = { \\"long\\": (-2**31, 2**31 - 1), \\"unsigned long\\": (0, 2**32 - 1), \\"long long\\": (-2**63, 2**63 - 1), \\"unsigned long long\\": (0, 2**64 - 1), } # Check if target_type is valid if target_type not in bounds: raise ValueError(f\\"Invalid target type: {target_type}. Must be one of {list(bounds.keys())}\\") # Get the min and max range for the type min_val, max_val = bounds[target_type] # Check if the value is within the range if not (min_val <= value <= max_val): raise ValueError(f\\"Value {value} out of range for {target_type}\\") return value"},{"question":"Objective Implement a custom autograd Function in PyTorch for a given mathematical function, then use `torch.autograd.gradcheck` to verify the gradients computed by your custom autograd Function. Problem Statement Given a mathematical function defined as: [ f(x) = frac{1}{2} |x|_2^2 ] where ( |x|_2 ) is the L2 norm of the vector ( x ). 1. Implement a custom autograd Function in PyTorch named `SquaredL2Norm`. 2. Verify the gradient computations for this custom function using `torch.autograd.gradcheck`. Implementation Details 1. Your implementation should define the forward and backward methods for the `SquaredL2Norm` function. 2. The forward method should compute the L2 norm squared. 3. The backward method should return the gradient of the output with respect to the input. Requirements - Use PyTorch\'s `torch.nn.Module` to define the custom autograd function. - Use `torch.autograd.gradcheck` to test the correctness of your backward method. Input Format - The input to your function will be a PyTorch tensor ( x ) of shape `(N,)` where ( N ) is the number of elements in the tensor. Output Format - Your function should return a PyTorch tensor containing the result of the squared L2 norm computation. Example ```python import torch from torch.autograd import Function class SquaredL2Norm(Function): @staticmethod def forward(ctx, input): ctx.save_for_backward(input) return 0.5 * torch.sum(input ** 2) @staticmethod def backward(ctx, grad_output): input, = ctx.saved_tensors return grad_output * input # Example usage and gradcheck input = torch.randn(5, dtype=torch.double, requires_grad=True) output = SquaredL2Norm.apply(input) print(output) # Perform gradcheck test = torch.autograd.gradcheck(SquaredL2Norm.apply, input) print(\\"Gradcheck passed:\\", test) ``` Constraints - Ensure that your backward implementation correctly computes the gradient. Performance Requirements - The gradient check should pass, indicating that the numerical gradients and analytical gradients are consistent up to a certain tolerance. Notes - Make sure to use `dtype=torch.double` for the inputs when performing the `gradcheck` as it requires double precision for numerical accuracy.","solution":"import torch from torch.autograd import Function class SquaredL2Norm(Function): @staticmethod def forward(ctx, input): ctx.save_for_backward(input) return 0.5 * torch.sum(input ** 2) @staticmethod def backward(ctx, grad_output): input, = ctx.saved_tensors return grad_output * input # Example usage and gradcheck input = torch.randn(5, dtype=torch.double, requires_grad=True) output = SquaredL2Norm.apply(input) print(output) # Perform gradcheck test = torch.autograd.gradcheck(SquaredL2Norm.apply, input) print(\\"Gradcheck passed:\\", test)"},{"question":"**Question: Configuration File Manager** You are tasked with developing a Python program that reads, modifies, and saves configuration settings using the `configparser` module. This is useful when managing settings in an application. # Requirements: 1. **Reading Configuration:** - Write a function `read_config(file_path: str) -> dict` that reads a configuration file and returns a dictionary of sections and their key-value pairs. ```python def read_config(file_path: str) -> dict: pass ``` 2. **Modifying Configuration:** - Write a function `modify_config(file_path: str, section: str, key: str, value: str) -> None` that modifies an entry in the configuration file. If the section or the key doesn\'t exist, your function should raise a `KeyError`. ```python def modify_config(file_path: str, section: str, key: str, value: str) -> None: pass ``` 3. **Saving New Configuration:** - Write a function `save_new_config(file_path: str, new_file_path: str, config_data: dict) -> None` that saves the given configuration data to a new file. The `config_data` parameter is a dictionary with sections as keys and a dictionary of key-value pairs as values. ```python def save_new_config(file_path: str, new_file_path: str, config_data: dict) -> None: pass ``` # Constraints: - The configuration file uses a standard INI format. - All functions should handle exceptions gracefully and provide meaningful error messages for common issues, such as file not found, permission denied, etc. - Ensure the performance is optimal for files up to 10MB in size. # Example: Assume `config.ini` has the following content: ``` [General] name = sample_application version = 1.0 [Settings] theme = light enable_logging = True ``` - Example usage of `read_config`: ```python config = read_config(\'config.ini\') # Output: # { # \'General\': {\'name\': \'sample_application\', \'version\': \'1.0\'}, # \'Settings\': {\'theme\': \'light\', \'enable_logging\': \'True\'} # } ``` - Example usage of `modify_config`: ```python modify_config(\'config.ini\', \'Settings\', \'theme\', \'dark\') # This will update the theme in the Settings section to \'dark\'. ``` - Example usage of `save_new_config`: ```python new_config_data = { \'General\': {\'name\': \'sample_application\', \'version\': \'1.1\'}, \'Settings\': {\'theme\': \'dark\', \'enable_logging\': \'False\'} } save_new_config(\'config.ini\', \'new_config.ini\', new_config_data) # This will create a new file called \'new_config.ini\' with the updated settings. ``` Your implementation should adhere to Python 3.10 standards and effectively use the `configparser` module.","solution":"import configparser import os def read_config(file_path: str) -> dict: Reads an INI configuration file and returns its contents as a dictionary. config = configparser.ConfigParser() if not os.path.exists(file_path): raise FileNotFoundError(f\\"File {file_path} not found.\\") config.read(file_path) config_dict = {section: dict(config.items(section)) for section in config.sections()} return config_dict def modify_config(file_path: str, section: str, key: str, value: str) -> None: Modifies a specific key in a given section of an INI configuration file. Raises a KeyError if the section or key does not exist. config = configparser.ConfigParser() if not os.path.exists(file_path): raise FileNotFoundError(f\\"File {file_path} not found.\\") config.read(file_path) if not config.has_section(section): raise KeyError(f\\"Section {section} not found in the config file.\\") if not config.has_option(section, key): raise KeyError(f\\"Key {key} not found in section {section}.\\") config.set(section, key, value) with open(file_path, \'w\') as configfile: config.write(configfile) def save_new_config(file_path: str, new_file_path: str, config_data: dict) -> None: Saves the provided configuration data to a new file. config = configparser.ConfigParser() for section, options in config_data.items(): config.add_section(section) for key, value in options.items(): config.set(section, key, value) with open(new_file_path, \'w\') as configfile: config.write(configfile)"},{"question":"**Objective:** You are required to create a Python function that performs an HTTP POST request to a provided URL with a set of given parameters and then reads and processes the HTTP response. This question will test your understanding of setting up HTTP connections, handling requests and responses, and dealing with potential errors. **Task:** 1. **Function Definition:** ```python def perform_post_request(url: str, params: dict) -> dict: Makes an HTTP POST request to the specified URL with given parameters. Args: url (str): The endpoint URL to which the POST request is sent. params (dict): A dictionary of parameters to include in the POST request body. Returns: dict: A dictionary containing the status code, reason, and response data (if successful). Raises: http.client.InvalidURL: If the URL is not valid. http.client.CannotSendRequest: If the request cannot be sent (e.g., improper connection state). http.client.ResponseNotReady: If the response is not ready to be read. http.client.BadStatusLine: If an invalid HTTP status line is received. ``` 2. **Requirements:** - The function should use the HTTPConnection class to make the request. - The parameters (`params`) should be URL-encoded and included in the body of the POST request. - Custom headers `{\\"Content-type\\": \\"application/x-www-form-urlencoded\\", \\"Accept\\": \\"text/plain\\"}` should be included. - The function should return a dictionary with the following keys: - `status_code`: HTTP status code of the response - `reason`: Reason phrase of the HTTP response - `data`: The content of the response body if the request was successful - Handle exceptions appropriately, raising the corresponding custom exceptions. 3. **Example Usage:** ```python url = \\"http://httpbin.org/post\\" params = {\\"name\\": \\"John Doe\\", \\"email\\": \\"john.doe@example.com\\"} response = perform_post_request(url, params) print(response) ``` The `response` dictionary should contain the HTTP status code, reason, and response data obtained from the server. **Constraints:** - Assume the URL provided is always HTTP. - Ensure proper error handling for common exceptions as documented. **Performance Requirements:** - The function should be efficient and handle timeouts or connection errors gracefully. You are expected to write a complete, testable function based on the description above.","solution":"import http.client import urllib.parse from http.client import InvalidURL, CannotSendRequest, ResponseNotReady, BadStatusLine def perform_post_request(url: str, params: dict) -> dict: Makes an HTTP POST request to the specified URL with given parameters. Args: url (str): The endpoint URL to which the POST request is sent. params (dict): A dictionary of parameters to include in the POST request body. Returns: dict: A dictionary containing the status code, reason, and response data (if successful). Raises: http.client.InvalidURL: If the URL is not valid. http.client.CannotSendRequest: If the request cannot be sent (e.g., improper connection state). http.client.ResponseNotReady: If the response is not ready to be read. http.client.BadStatusLine: If an invalid HTTP status line is received. # Parse the URL parsed_url = urllib.parse.urlparse(url) if not parsed_url.hostname: raise InvalidURL(\\"Invalid URL provided.\\") # Prepare the connection and request details connection = http.client.HTTPConnection(parsed_url.hostname, parsed_url.port or 80) headers = { \\"Content-type\\": \\"application/x-www-form-urlencoded\\", \\"Accept\\": \\"text/plain\\" } body = urllib.parse.urlencode(params) try: # Send the POST request connection.request(\\"POST\\", parsed_url.path, body, headers) # Get the response response = connection.getresponse() # Read the response data response_data = response.read().decode() return { \\"status_code\\": response.status, \\"reason\\": response.reason, \\"data\\": response_data } except (CannotSendRequest, ResponseNotReady, BadStatusLine) as e: raise e finally: connection.close()"},{"question":"# Question You have been given a Python script that performs several operations, and your task is to set up a robust logging mechanism for it. Implement the following steps: 1. Define a function `setup_logging(loglevel: str, logfile: str)` that sets up the logging configuration programmatically. This function should: - Configure logging to log messages to both a console and a specified log file. - Use different log formats for the console and the file. The console logs should include the log level and the message, while the file logs should include the date, time, module name, log level, and the message. - Ensure the log level can be set dynamically from a string input parameter (`loglevel`), which can be \\"DEBUG\\", \\"INFO\\", \\"WARNING\\", \\"ERROR\\", or \\"CRITICAL\\". 2. Define a function `log_messages()` that logs messages at all five standard logging levels (DEBUG, INFO, WARNING, ERROR, CRITICAL). 3. Create a logging configuration file (`logging.conf`) and then write a function `setup_logging_from_file(config_file: str)` that sets up logging using this configuration file. Below is the initial Python script that you can use for integrating the logging functionality: ```python import logging import logging.config def setup_logging(loglevel: str, logfile: str): # Implement logging setup programmatically pass def log_messages(): # Implement logging messages at various levels pass def setup_logging_from_file(config_file: str): # Implement logging setup from a configuration file pass if __name__ == \\"__main__\\": # Example usage of the above functions setup_logging(\\"DEBUG\\", \\"example.log\\") log_messages() # Switching to file-based logging configuration setup_logging_from_file(\\"logging.conf\\") log_messages() ``` Details for `logging.conf` Your configuration file should be able to replicate the logging settings from `setup_logging`. It must contain: - A logger named \\"example\\" that logs to both console and file. - A console handler with a different format compared to the file handler. - A file handler logging to the same file specified in `setup_logging`. Example Output 1. Console output format for `DEBUG` level: ``` DEBUG: This is a debug message. ``` 2. File output format for `DEBUG` level: ``` 2023-10-05 12:00:00,000 - __main__ - DEBUG - This is a debug message. ``` Constraints - Ensure you handle possible errors such as invalid logging levels gracefully, possibly through exceptions. - Ensure thread-safety for logging if used in a multi-threaded environment. Make sure your implementation is clean, efficient, and well-documented.","solution":"import logging import logging.config import sys def setup_logging(loglevel: str, logfile: str): Sets up logging configuration programmatically. Parameters: loglevel (str): The log level to set (\\"DEBUG\\", \\"INFO\\", \\"WARNING\\", \\"ERROR\\", or \\"CRITICAL\\"). logfile (str): The path to the log file. numeric_level = getattr(logging, loglevel.upper(), None) if not isinstance(numeric_level, int): raise ValueError(f\\"Invalid log level: {loglevel}\\") # Configure logging logging.basicConfig(level=numeric_level, format=\'%(asctime)s - %(name)s - %(levelname)s - %(message)s\', datefmt=\'%Y-%m-%d %H:%M:%S\', handlers=[ logging.FileHandler(logfile), logging.StreamHandler(sys.stdout) ]) console = logging.StreamHandler(sys.stdout) console.setLevel(numeric_level) console.setFormatter(logging.Formatter(\'%(levelname)s: %(message)s\')) file_handler = logging.FileHandler(logfile) file_handler.setLevel(numeric_level) file_handler.setFormatter(logging.Formatter(\'%(asctime)s - %(name)s - %(levelname)s - %(message)s\')) logging.getLogger().handlers = [] logging.getLogger().addHandler(console) logging.getLogger().addHandler(file_handler) logging.getLogger().setLevel(numeric_level) def log_messages(): Logs messages at all five standard logging levels. logging.debug(\\"This is a debug message.\\") logging.info(\\"This is an info message.\\") logging.warning(\\"This is a warning message.\\") logging.error(\\"This is an error message.\\") logging.critical(\\"This is a critical message.\\") def setup_logging_from_file(config_file: str): Sets up logging configuration from a configuration file. Parameters: config_file (str): The path to the logging configuration file. logging.config.fileConfig(config_file)"},{"question":"# Custom Class and Special Methods Implementation Context In this task, you are required to demonstrate your understanding of Python\'s object model, special method implementation, and context management. You will design a flexible class representing a mathematical vector in an n-dimensional space. This class should support usual arithmetic operations, enable customized attribute access, and follow proper resource management practices. Requirements 1. **Class Definition**: Define a class `Vector` that represents a vector in n-dimensional space. 2. **Initialization**: - The class should be initialized with a list of coordinates. - Example: `v = Vector([1, 2, 3])`. 3. **String Representation**: - Implement `__repr__` and `__str__` methods to give official and informal string representations of the vector. 4. **Arithmetic Operations**: - Implement special methods to support addition (`+`), subtraction (`-`), and dot product (`*`) of vectors. - Example: `v1 + v2`, `v1 - v2`, `v1 * v2` where `v1` and `v2` are instances of the `Vector` class. 5. **Attribute Access**: - Customize attribute access to provide an easy way to get vector length using a property `length`. - Handle attribute access for inexistent attributes by raising an `AttributeError`. 6. **Context Management**: - Implement context management to manage resources if any, using `__enter__` and `__exit__`. Additional Constraints - If the vectors involved in operations have differing dimensions, raise a `ValueError` with a descriptive message. - Ensure that operations are optimized for performance and handle potential edge cases, such as empty vectors. Example Usage ```python v1 = Vector([1, 2, 3]) v2 = Vector([4, 5, 6]) print(v1) # Informal string representation print(repr(v2)) # Official string representation v3 = v1 + v2 # Vector addition v4 = v1 - v2 # Vector subtraction dot_product = v1 * v2 # Dot product print(f\\"v3: {v3}\\") print(f\\"v4: {v4}\\") print(f\\"Dot Product: {dot_product}\\") print(f\\"Length of v1: {v1.length}\\") with Vector([1, 2]) as v: print(\\"Inside context manager\\") ``` Guidelines - Use the `__init__`, `__repr__`/`__str__`, `__add__`, `__sub__`, `__mul__`, `__getattr__`, `__enter__`, and `__exit__` methods to implement the required functionalities. - Thoroughly test your implementation to ensure it behaves correctly under various scenarios.","solution":"class Vector: A class representing an n-dimensional mathematical vector. def __init__(self, coordinates): if not coordinates: raise ValueError(\\"Vector must have at least one coordinate\\") self.coordinates = coordinates def __repr__(self): return f\\"Vector({self.coordinates})\\" def __str__(self): return f\\"Vector{self.coordinates}\\" def __add__(self, other): if len(self.coordinates) != len(other.coordinates): raise ValueError(\\"Vectors must be the same dimension for addition\\") result = [a + b for a, b in zip(self.coordinates, other.coordinates)] return Vector(result) def __sub__(self, other): if len(self.coordinates) != len(other.coordinates): raise ValueError(\\"Vectors must be the same dimension for subtraction\\") result = [a - b for a, b in zip(self.coordinates, other.coordinates)] return Vector(result) def __mul__(self, other): if len(self.coordinates) != len(other.coordinates): raise ValueError(\\"Vectors must be the same dimension for dot product\\") result = sum(a * b for a, b in zip(self.coordinates, other.coordinates)) return result @property def length(self): return sum(x**2 for x in self.coordinates) ** 0.5 def __getattr__(self, item): raise AttributeError(f\\"\'Vector\' object has no attribute \'{item}\'\\") def __enter__(self): # If there were resources to manage, we would initialize them here. return self def __exit__(self, exc_type, exc_value, traceback): # If there were resources to clean up, we would finalize them here. pass"},{"question":"# Question: Implement a Multi-threaded Counter with Synchronization You are tasked with implementing a multi-threaded counter using the low-level `_thread` module. This counter will be shared among multiple threads, and proper synchronization must be used to ensure accurate results. **Instructions:** 1. Implement a class `ThreadSafeCounter` with the following methods: - `__init__(self)`: Initializes the counter to zero and creates a lock. - `increment(self)`: Safely increments the counter by one. - `get_value(self)`: Returns the current value of the counter. 2. Write a function `increment_counter(counter, num_increments)` that takes an instance of `ThreadSafeCounter` and increments it `num_increments` times. 3. Write a function `run_threads(num_threads, num_increments)` that: - Creates an instance of `ThreadSafeCounter`. - Creates and starts `num_threads` threads, each running `increment_counter`. - Waits for all threads to complete. - Returns the final value of the counter. **Constraints:** - You must use the `_thread` module for threading and synchronization. - Ensure that the counter operations are thread-safe. - Assume that the number of threads and increments are positive integers. **Example Usage:** ```python import _thread from threading import Event class ThreadSafeCounter: def __init__(self): self.counter = 0 self.lock = _thread.allocate_lock() def increment(self): with self.lock: self.counter += 1 def get_value(self): with self.lock: return self.counter def increment_counter(counter, num_increments): for _ in range(num_increments): counter.increment() def run_threads(num_threads, num_increments): counter = ThreadSafeCounter() threads = [] for _ in range(num_threads): _thread.start_new_thread(increment_counter, (counter, num_increments)) # Use Event for synchronization since _thread does not provide thread joining # Using Event to make the main thread wait for a bit for demonstration purposes. # In real applications, use the higher-level `threading` module which provides easier synchronization done_event = Event() done_event.wait(1) return counter.get_value() # Example final_count = run_threads(5, 1000) print(f\\"Final counter value: {final_count}\\") # Should be 5000 ``` **Note:** The provided example uses an `Event` for synchronization for the demonstration purpose. In actual implementations, waiting for threads to complete should be done in a more robust manner using the higher-level `threading` module for easier thread management and synchronization.","solution":"import _thread from time import sleep class ThreadSafeCounter: def __init__(self): self.counter = 0 self.lock = _thread.allocate_lock() def increment(self): with self.lock: self.counter += 1 def get_value(self): with self.lock: return self.counter def increment_counter(counter, num_increments): for _ in range(num_increments): counter.increment() def run_threads(num_threads, num_increments): counter = ThreadSafeCounter() for _ in range(num_threads): _thread.start_new_thread(increment_counter, (counter, num_increments)) # Waiting for threads to finish (not the best way, but since _thread has no join, we sleep a bit) sleep(1) return counter.get_value()"},{"question":"# Question: Problem Statement: You are required to create a class `DataAnalyzer` to analyze a dataset of numbers. This class should provide several methods which leverage the functionalities of the `functools` module to demonstrate caching, partial application, and calculating cumulative sums. Requirements: 1. Implement a class `DataAnalyzer` that takes an iterable of numbers as input during initialization. 2. The class should include: - A method `variance` to calculate the **variance** of the numbers. Make use of `@functools.cache` to cache results for this method. - A method `update_numbers` to update the dataset. Utilize `@functools.partialmethod` to allow toggling specific numbers to a new state (e.g., turning them on/off). - A method `cumulative_sum` to calculate the **cumulative sum** of the numbers in the dataset. Use `functools.reduce` to achieve this functionality. Implementation Guidelines: - **Input**: - The `DataAnalyzer` class constructor will take an iterable of numbers. - The `update_numbers` method will take a number and its new state (boolean). - The `cumulative_sum` method will take no arguments but use the dataset held by the instance. - **Output**: - The `variance` method should return the variance of the dataset. - The `update_numbers` method should update the specified number to the new state within the dataset. - The `cumulative_sum` method should return the cumulative sum of the dataset. - **Performance**: - The caching mechanism in the `variance` method should ensure results are efficiently retrieved on repeated calls with the same data. - The `cumulative_sum` method should leverage `functools.reduce` for optimal reduction of the dataset. Example Usage: ```python from functools import cache, partialmethod, reduce class DataAnalyzer: def __init__(self, numbers): self.numbers = list(numbers) @cache def variance(self): mean = sum(self.numbers) / len(self.numbers) return sum((x - mean) ** 2 for x in self.numbers) / len(self.numbers) def update_state(self, index, state): self.numbers[index] = state # Partial methods to update numbers turn_on = partialmethod(update_state, state=True) turn_off = partialmethod(update_state, state=False) def cumulative_sum(self): return reduce(lambda x, y: x + y, self.numbers) # Example: analyzer = DataAnalyzer([1, 2, 3, 4, 5]) print(analyzer.variance()) # Should print the variance of the numbers analyzer.turn_off(2) print(analyzer.numbers) # Updated list with the third number turned off (set to False) print(analyzer.cumulative_sum()) # Cumulative sum of the list ``` Constraints: - The dataset for the `DataAnalyzer` can be assumed to contain non-negative integers. - Ensure the implementation details use concepts covered in the `functools` module documentation.","solution":"from functools import cache, partialmethod, reduce class DataAnalyzer: def __init__(self, numbers): self.numbers = list(numbers) @cache def variance(self): mean = sum(self.numbers) / len(self.numbers) return sum((x - mean) ** 2 for x in self.numbers) / len(self.numbers) def update_state(self, index, new_value): self.numbers[index] = new_value turn_on = partialmethod(update_state, new_value=1) turn_off = partialmethod(update_state, new_value=0) def cumulative_sum(self): return reduce(lambda x, y: x + y, self.numbers)"},{"question":"# Codec Registration, Encoding, and Error Handling in Python Background: Python provides a comprehensive suite of functions for managing codec registrations, encoding, and decoding of objects, as well as handling errors that occur during these processes. Understanding how to use these functions effectively is crucial for handling various text encoding tasks in Python. Task: You are required to implement a small module that registers a custom codec, encodes a string, handles any errors encountered during encoding, and then decodes the string back to its original form. Requirements: 1. **Custom Codec Registration**: - Implement a search function that registers a custom codec to encode and decode strings using a specific encoding (e.g., \'rot13\'). - Use the `PyCodec_Register` function to register this codec. 2. **Encoding and Decoding Functions**: - Implement functions `encode_string` and `decode_string` that use `PyCodec_Encode` and `PyCodec_Decode` respectively to encode and decode a given string using your custom codec. - Include error handling using custom error handling functions where necessary. Use `PyCodec_ReplaceErrors` to handle any encoding errors by replacing problematic characters with a \\"?\\". 3. **Main Function**: - Write a `main` function that demonstrates registering the codec, encoding a string, handling errors during encoding, and decoding the string back to its original form. 4. **Input and Output**: - **Input**: A string to be encoded. - **Output**: The encoded string, and the decoded string. Constraints: - Your implementation should handle potential encoding errors gracefully using the error handling mechanism. - Ensure that the custom codec is properly registered before attempting to encode or decode. Example: ```python if __name__ == \\"__main__\\": input_str = \\"hello, world!\\" main(input_str) # Output: # Encoded String: uryyb, jbeyq! # Decoded String: hello, world! ``` Performance: - The solution should efficiently handle strings up to 10^6 characters. - Ensure proper exception handling to avoid abrupt program termination. # Implementation Details: ```python def register_rot13_codec(): # Define and register your custom codec here pass def encode_string(input_str): # Use PyCodec_Encode to encode the input string pass def decode_string(encoded_str): # Use PyCodec_Decode to decode the input string pass def main(input_str): # Register custom codec, encode and decode the input string pass if __name__ == \\"__main__\\": input_str = \\"hello, world!\\" main(input_str) ``` Students are expected to fill in the implementations of the functions outlined above, ensuring that they properly work with the PyCodec API functions specified in the documentation.","solution":"import codecs def rot13_encoder(input, errors=\'strict\'): return codecs.encode(input, \'rot_13\'), len(input) def rot13_decoder(input, errors=\'strict\'): return codecs.decode(input, \'rot_13\'), len(input) def register_rot13_codec(): codec_info = codecs.CodecInfo( name=\'rot13\', encode=rot13_encoder, decode=rot13_decoder, incrementalencoder=None, incrementaldecoder=None, streamwriter=None, streamreader=None ) try: codecs.register(lambda name: codec_info if name == \'rot13\' else None) except Exception as e: print(f\\"Error registering codec: {e}\\") def encode_string(input_str): try: return codecs.encode(input_str, \'rot13\') except Exception as e: print(f\\"Error encoding string: {e}\\") def decode_string(encoded_str): try: return codecs.decode(encoded_str, \'rot13\') except Exception as e: print(f\\"Error decoding string: {e}\\") def main(input_str): register_rot13_codec() encoded_str = encode_string(input_str) print(f\\"Encoded String: {encoded_str}\\") decoded_str = decode_string(encoded_str) print(f\\"Decoded String: {decoded_str}\\") return encoded_str, decoded_str if __name__ == \\"__main__\\": input_str = \\"hello, world!\\" main(input_str)"},{"question":"**Question: Advanced Enum Usage in Python** **Objective:** In this coding task, you are required to implement a class that extends `Enum` functionality to represent a set of planets with some specific features. The task involves several key concepts from the `enum` module, such as creating enums with custom methods, using `auto` for automatic value assignment, and ensuring unique enum members. **Instructions:** 1. Define an enumeration class `Planet` using the `Enum` base class. Each planet should have the following attributes: - `MERCURY` (value: 3.303e+23 kg, 2.4397e6 meters) - `VENUS` (value: 4.869e+24 kg, 6.0518e6 meters) - `EARTH` (value: 5.976e+24 kg, 6.37814e6 meters) - `MARS` (value: 6.421e+23 kg, 3.3972e6 meters) - `JUPITER` (value: 1.9e+27 kg, 7.1492e7 meters) - `SATURN` (value: 5.688e+26 kg, 6.0268e7 meters) - `URANUS` (value: 8.686e+25 kg, 2.5559e7 meters) - `NEPTUNE` (value: 1.024e+26 kg, 2.4746e7 meters) Each planet\'s value should be a tuple containing its mass and radius. 2. Each planet should have the following methods: - `surface_gravity(self)`: calculates the surface gravity of the planet using the formula: [ text{{surface gravity}} = frac{G times text{{mass}}}{(text{{radius}}^2)} ] Where G is the universal gravitational constant (6.67300E-11 m^3 kg^-1 s^-2). - `__str__(self)`: returns a custom string representation in the format \\"<Planet Name>: Mass = X kg, Radius = Y meters\\" 3. Ensure that all enum members have unique values using the `unique` decorator. 4. Iterate through your `Planet` enum and print each planet\'s name using the custom string representation defined in the `__str__()` method along with its surface gravity. **Function Signature:** ```python from enum import Enum, unique @unique class Planet(Enum): def __new__(cls, mass, radius): obj = object.__new__(cls) obj._value_ = (mass, radius) obj.mass = mass obj.radius = radius return obj def surface_gravity(self): G = 6.67300E-11 return G * self.mass / (self.radius ** 2) def __str__(self): return f\\"<{self.name}>: Mass = {self.mass} kg, Radius = {self.radius} meters\\" # Define all planets here # Testing the enum for planet in Planet: print(f\\"{planet}: Surface gravity = {planet.surface_gravity()} m/s^2\\") ``` **Expected Output:** ``` <MERCURY>: Mass = 3.303e+23 kg, Radius = 2439700.0 meters: Surface gravity = 3.7030267229659395e-11 m/s^2 <VENUS>: Mass = 4.869e+24 kg, Radius = 6051800.0 meters: Surface gravity = 9.380701188939663e-11 m/s^2 <EARTH>: Mass = 5.976e+24 kg, Radius = 6378140.0 meters: Surface gravity = 9.802652743337129 m/s^2 <MARS>: Mass = 6.421e+23 kg, Radius = 3397200.0 meters: Surface gravity = 3.7207604741831775e-11 m/s^2 <JUPITER>: Mass = 1.9e+27 kg, Radius = 71492000.0 meters: Surface gravity = 2.5281274771966307e-10 m/s^2 <SATURN>: Mass = 5.688e+26 kg, Radius = 60268000.0 meters: Surface gravity = 1.4976568468472242e-10 m/s^2 <URANUS>: Mass = 8.686e+25 kg, Radius = 25559000.0 meters: Surface gravity = 1.2617417752121858e-10 m/s^2 <NEPTUNE>: Mass = 1.024e+26 kg, Radius = 24746000.0 meters: Surface gravity = 1.3149279771997755e-10 m/s^2 ``` **Constraints:** - Use the `@unique` decorator to ensure each enumeration member has a unique value. - Use the `Enum` base class for creating the `Planet` enumeration. - Handle the defined methods to ensure accurate surface gravity calculations. **Additional Note:** This task assesses the student\'s ability to create and manipulate enumerated constants with unique member values and custom methods within Python\'s `enum` module. Proper implementation should demonstrate understanding of the module\'s capabilities and constraints.","solution":"from enum import Enum, unique @unique class Planet(Enum): MERCURY = (3.303e+23, 2.4397e6) VENUS = (4.869e+24, 6.0518e6) EARTH = (5.976e+24, 6.37814e6) MARS = (6.421e+23, 3.3972e6) JUPITER = (1.9e+27, 7.1492e7) SATURN = (5.688e+26, 6.0268e7) URANUS = (8.686e+25, 2.5559e7) NEPTUNE = (1.024e+26, 2.4746e7) def __init__(self, mass, radius): self.mass = mass self.radius = radius def surface_gravity(self): G = 6.67300E-11 return G * self.mass / (self.radius ** 2) def __str__(self): return f\\"<{self.name}>: Mass = {self.mass} kg, Radius = {self.radius} meters\\" # Testing the enum for planet in Planet: print(f\\"{planet}: Surface gravity = {planet.surface_gravity()} m/s^2\\")"},{"question":"# Python Coding Assessment: Advanced IP Management Objective To assess your understanding of the Python `ipaddress` module, you are required to implement functions that manipulate and validate IP addresses, networks, and interfaces, ensuring both correct and efficient handling of IPv4 and IPv6 addresses. Problem Statement You are tasked with implementing three functions: 1. `validate_ip_address(ip_str: str) -> bool` 2. `calculate_usable_hosts(network_str: str) -> int` 3. `find_subnets(network_str: str, new_prefix: int) -> List[str]` 4. `summarize_addresses(addresses: List[str]) -> str` Function Specifications 1. **Function `validate_ip_address(ip_str: str) -> bool`**: - **Input**: A string `ip_str`, representing an IP address. - **Output**: Returns `True` if the `ip_str` is a valid IPv4 or IPv6 address, otherwise returns `False`. 2. **Function `calculate_usable_hosts(network_str: str) -> int`**: - **Input**: A string `network_str`, representing an IP network in CIDR notation (e.g., \'192.0.2.0/24\'). - **Output**: Returns the number of usable host addresses in the network. For example, \'192.0.2.0/24\' should return 254 (excluding network and broadcast addresses for IPv4). - **Constraints**: Only for IPv4 networks for this problem. 3. **Function `find_subnets(network_str: str, new_prefix: int) -> List[str]`**: - **Input**: - `network_str`: A string representing the base network in CIDR notation (e.g., \'192.0.2.0/24\'). - `new_prefix`: An integer representing the new prefix length for the subnets. - **Output**: A list of strings representing the network addresses of the subnets calculated from the original network with the new prefix. 4. **Function `summarize_addresses(addresses: List[str]) -> str`**: - **Input**: A list of strings, each representing an IP address (IPv4 or IPv6). - **Output**: A string representing the summarized address range. # Implementation ```python from typing import List import ipaddress def validate_ip_address(ip_str: str) -> bool: try: ipaddress.ip_address(ip_str) return True except ValueError: return False def calculate_usable_hosts(network_str: str) -> int: network = ipaddress.ip_network(network_str, strict=False) if network.version == 4: return network.num_addresses - 2 # Subtract network and broadcast addresses else: raise ValueError(\\"Only IPv4 networks are supported\\") def find_subnets(network_str: str, new_prefix: int) -> List[str]: network = ipaddress.ip_network(network_str, strict=False) subnets = list(network.subnets(new_prefix=new_prefix)) return [str(subnet) for subnet in subnets] def summarize_addresses(addresses: List[str]) -> str: ips = [ipaddress.ip_address(addr) for addr in addresses] summarized_networks = ipaddress.collapse_addresses(ips) return \', \'.join(str(network) for network in summarized_networks) ``` # Constraints and Notes - Use the `ipaddress` module. - Handle exceptions where necessary. - Ensure efficient use of resources, especially for large networks. - Assume the input strings are well-formed unless specified otherwise. # Example Usage ```python print(validate_ip_address(\'192.0.2.1\')) # True print(validate_ip_address(\'256.0.0.0\')) # False print(calculate_usable_hosts(\'192.0.2.0/24\')) # 254 print(find_subnets(\'192.0.2.0/24\', 26)) # [\'192.0.2.0/26\', \'192.0.2.64/26\', ...] print(summarize_addresses([\'192.0.2.1\', \'192.0.2.2\', \'192.0.2.3\', \'192.0.2.4\'])) # 192.0.2.1/30 ``` Good luck with your implementation!","solution":"from typing import List import ipaddress def validate_ip_address(ip_str: str) -> bool: Validates if the given string is a valid IPv4 or IPv6 address. try: ipaddress.ip_address(ip_str) return True except ValueError: return False def calculate_usable_hosts(network_str: str) -> int: Calculates the number of usable host addresses in the given IPv4 network. network = ipaddress.ip_network(network_str, strict=False) if network.version == 4: return max(0, network.num_addresses - 2) # Subtract network and broadcast addresses else: raise ValueError(\\"Only IPv4 networks are supported\\") def find_subnets(network_str: str, new_prefix: int) -> List[str]: Calculates subnetworks with the new prefix length from the given network. network = ipaddress.ip_network(network_str, strict=False) subnets = list(network.subnets(new_prefix=new_prefix)) return [str(subnet) for subnet in subnets] def summarize_addresses(addresses: List[str]) -> List[str]: Summarizes a list of IP addresses into the smallest possible list of CIDR notations. ips = [ipaddress.ip_address(addr) for addr in addresses] summarized_networks = ipaddress.collapse_addresses(ips) return [str(network) for network in summarized_networks]"},{"question":"# Path Management and File Metadata Challenge You are tasked with creating a utility function that takes a base directory and a list of file paths (potentially relative or absolute) and organizes the information. Your mission is to write a function `analyze_paths` which takes two arguments: - `base_dir`: A string representing the root directory. - `paths`: A list of strings representing various file paths. The function should return a dictionary of analyses where each key is the normalized absolute version of a path from `paths`, and the values are another dictionary containing: - Whether the path exists or not. - Whether it\'s a file, directory, or symbolic link. - The directory name and basename of the path. - The size of the file (in bytes) if it exists and is a file. - The time of last modification if it exists. - The relative path to the base directory. # Constraints: 1. The function must handle paths in both Unix and Windows style. 2. If the path does not exist, appropriate indicators should be set in the result dictionary. 3. You must not access the filesystem more than necessary - use the appropriate `os.path` methods efficiently. 4. Ensure the solution is performant as `paths` could be a large list. # Input: ```python base_dir = \\"/home/user/project\\" paths = [\\"src/file.py\\", \\"/etc/hosts\\", \\"README.md\\", \\"~/docs/notes.txt\\"] ``` # Output: ```python { \\"/home/user/project/src/file.py\\": { \\"exists\\": True, # or False \\"isfile\\": True, # or False \\"isdir\\": False, # or True \\"islink\\": False, # or True \\"dirname\\": \\"/home/user/project/src\\", \\"basename\\": \\"file.py\\", \\"size\\": 1234, # or None if doesn\'t exist \\"mtime\\": 1630587265.123456, # or None if doesn\'t exist \\"relpath\\": \\"src/file.py\\" }, # Similar information for other paths... } ``` # Function Signature: ```python def analyze_paths(base_dir: str, paths: list) -> dict: pass ``` Your task is to implement this function by using the `os.path` module functionalities described.","solution":"import os def analyze_paths(base_dir: str, paths: list) -> dict: results = {} for path in paths: # Expand user home directory symbol \'~\' path = os.path.expanduser(path) # Make path absolute if not os.path.isabs(path): path = os.path.join(base_dir, path) # Normalize the path norm_path = os.path.normpath(path) # Check path attributes exists = os.path.exists(norm_path) isfile = os.path.isfile(norm_path) isdir = os.path.isdir(norm_path) islink = os.path.islink(norm_path) dirname = os.path.dirname(norm_path) basename = os.path.basename(norm_path) # Initialize details size = None mtime = None if exists and isfile: size = os.path.getsize(norm_path) mtime = os.path.getmtime(norm_path) # Calculate relative path relpath = os.path.relpath(norm_path, base_dir) results[norm_path] = { \\"exists\\": exists, \\"isfile\\": isfile, \\"isdir\\": isdir, \\"islink\\": islink, \\"dirname\\": dirname, \\"basename\\": basename, \\"size\\": size, \\"mtime\\": mtime, \\"relpath\\": relpath } return results"},{"question":"**Coding Assessment Question** **Objective:** Demonstrate your understanding of the `pwd` module in Python by implementing functions to manage and retrieve user information from the Unix user account and password database. **Question:** You are required to implement a function `get_user_info(user_identifier)` that retrieves information about a Unix user based on either their username or user ID. Additionally, implement a function `list_all_users()` that lists all available user information. # Requirements 1. **Function 1: `get_user_info(user_identifier)`** - **Input**: - `user_identifier` (str or int): If the input is a string, it represents the username. If it is an integer, it represents the user ID. - **Output**: A dictionary with the following keys and corresponding values from the password database entry: - `name`: Login name (str) - `passwd`: Encrypted password or placeholder (str) - `uid`: Numerical user ID (int) - `gid`: Numerical group ID (int) - `gecos`: User name or comment field (str) - `dir`: User home directory (str) - `shell`: User command interpreter (str) - **Constraints**: - If the entry cannot be found, raise a `KeyError` with an appropriate message. 2. **Function 2: `list_all_users()`** - **Output**: A list of dictionaries, each representing a user with the keys as described in Function 1. # Example Usage ```python # Example function calls user_info = get_user_info(\\"username\\") print(user_info) # Output: {\'name\': \'username\', \'passwd\': \'*\', \'uid\': 1000, \'gid\': 1000, \'gecos\': \'User Name\', \'dir\': \'/home/username\', \'shell\': \'/bin/bash\'} user_info = get_user_info(1000) print(user_info) # Output: {\'name\': \'username\', \'passwd\': \'*\', \'uid\': 1000, \'gid\': 1000, \'gecos\': \'User Name\', \'dir\': \'/home/username\', \'shell\': \'/bin/bash\'} all_users = list_all_users() print(all_users) # Output: [{\'name\': \'root\', \'passwd\': \'x\', \'uid\': 0, \'gid\': 0, \'gecos\': \'root\', \'dir\': \'/root\', \'shell\': \'/bin/bash\'}, {...}, ...] ``` # Notes 1. Import the `pwd` module to access the user account and password database. 2. Handle errors gracefully and provide meaningful error messages. 3. Ensure the code adheres to Python\'s best practices and is well-documented. Good luck and happy coding!","solution":"import pwd def get_user_info(user_identifier): Retrieves user information based on username or user ID. :param user_identifier: str for username, int for user ID :return: dict with user information :raises KeyError: if the user is not found try: if isinstance(user_identifier, str): pwd_entry = pwd.getpwnam(user_identifier) elif isinstance(user_identifier, int): pwd_entry = pwd.getpwuid(user_identifier) else: raise TypeError(\\"user_identifier must be a string (username) or an integer (user ID)\\") return { \'name\': pwd_entry.pw_name, \'passwd\': pwd_entry.pw_passwd, \'uid\': pwd_entry.pw_uid, \'gid\': pwd_entry.pw_gid, \'gecos\': pwd_entry.pw_gecos, \'dir\': pwd_entry.pw_dir, \'shell\': pwd_entry.pw_shell } except KeyError: raise KeyError(f\\"No such user with identifier: {user_identifier}\\") def list_all_users(): Lists all users in the Unix user account and password database. :return: list of dictionaries, each containing user information all_users = [] for pwd_entry in pwd.getpwall(): user_info = { \'name\': pwd_entry.pw_name, \'passwd\': pwd_entry.pw_passwd, \'uid\': pwd_entry.pw_uid, \'gid\': pwd_entry.pw_gid, \'gecos\': pwd_entry.pw_gecos, \'dir\': pwd_entry.pw_dir, \'shell\': pwd_entry.pw_shell } all_users.append(user_info) return all_users"},{"question":"# Context You are tasked with optimizing a machine learning model\'s prediction performance using scikit-learn. This involves minimizing the latency and maximizing the throughput of predictions. Your optimization will involve selecting appropriate input data representations, configuring the model appropriately, and exploiting advanced techniques described in the documentation. # Objective Implement a function `optimize_predictions` that optimizes the latency and throughput of a scikit-learn model\'s predictions. # Function Signature ```python def optimize_predictions(X_train, y_train, X_test, model, sparse_input=False, assume_finite=True): Optimize the prediction performance of a given scikit-learn model. Parameters: X_train (numpy.ndarray or scipy.sparse matrix): Training features. y_train (numpy.ndarray): Training labels. X_test (numpy.ndarray or scipy.sparse matrix): Testing features. model (sklearn.base.BaseEstimator): The scikit-learn model to be used. sparse_input (bool, optional): If True, use sparse matrix representation for input data. Defaults to False. assume_finite (bool, optional): If True, skip finiteness checks for input data during prediction. Defaults to True. Returns: numpy.ndarray: Predicted values for X_test. float: Prediction latency (in seconds) for a fixed number of test samples. float: Prediction throughput (number of predictions per second). pass ``` # Instructions 1. **Input Parameters**: - `X_train`: Training features as a dense numpy array or a sparse matrix. - `y_train`: Training labels as a numpy array. - `X_test`: Testing features as a dense numpy array or a sparse matrix. - `model`: A scikit-learn estimator to be used for training and prediction. - `sparse_input`: A boolean indicating whether to use sparse matrix representation for input data. - `assume_finite`: A boolean indicating whether to skip finiteness checks for input data during prediction for reduced validation overhead. 2. **Process**: - If `sparse_input` is True, convert `X_train` and `X_test` to sparse matrix format. - Configure the scikit-learn environment according to the `assume_finite` parameter to reduce validation overhead. - Fit the model using `X_train` and `y_train`. - Predict the values for `X_test` and measure the prediction latency. - Calculate the prediction throughput. 3. **Output**: - Return the predicted values for `X_test`. - Return the prediction latency for a specific number of test samples. - Return the prediction throughput, which is the number of predictions per second. 4. **Constraints**: - Ensure all features and labels are finite if `assume_finite` is False. - Optimize for both latency and throughput as per the guidelines in the provided documentation. ----------------- # Example Usage ```python from sklearn.datasets import make_classification from sklearn.linear_model import SGDClassifier import numpy as np # Generate a sample dataset X_train, y_train = make_classification(n_samples=1000, n_features=20, random_state=42) X_test, _ = make_classification(n_samples=200, n_features=20, random_state=42) # Initialize model model = SGDClassifier() # Optimize and predict predictions, latency, throughput = optimize_predictions(X_train, y_train, X_test, model, sparse_input=True, assume_finite=True) print(\\"Predictions:\\", predictions) print(\\"Latency (seconds):\\", latency) print(\\"Throughput (predictions per second):\\", throughput) ``` # Performance Requirements - The purpose of this function is not only to make predictions but also to optimize them for latency and throughput based on different input configurations and constraints. # Notes - You can use helper functions if needed to modularize your code. - Perform necessary imports within the function to keep it self-contained. - You may use documentation referenced techniques such as chunking, sparsification, or model reshaping where needed.","solution":"import numpy as np from sklearn.base import BaseEstimator from scipy.sparse import csr_matrix from time import time from sklearn.utils import check_array from sklearn import config_context def optimize_predictions(X_train, y_train, X_test, model, sparse_input=False, assume_finite=True): Optimize the prediction performance of a given scikit-learn model. Parameters: X_train (numpy.ndarray or scipy.sparse matrix): Training features. y_train (numpy.ndarray): Training labels. X_test (numpy.ndarray or scipy.sparse matrix): Testing features. model (sklearn.base.BaseEstimator): The scikit-learn model to be used. sparse_input (bool, optional): If True, use sparse matrix representation for input data. Defaults to False. assume_finite (bool, optional): If True, skip finiteness checks for input data during prediction. Defaults to True. Returns: numpy.ndarray: Predicted values for X_test. float: Prediction latency (in seconds) for a fixed number of test samples. float: Prediction throughput (number of predictions per second). if sparse_input: X_train = csr_matrix(X_train) X_test = csr_matrix(X_test) else: X_train = np.asarray(X_train) X_test = np.asarray(X_test) with config_context(assume_finite=assume_finite): model.fit(X_train, y_train) start_time = time() predictions = model.predict(X_test) end_time = time() latency = end_time - start_time throughput = X_test.shape[0] / latency return predictions, latency, throughput"},{"question":"# Python Lexical Analyzer Write a Python function that takes a list of strings as input, each string representing a line of Python code, and returns a list of tokens extracted from these lines. This function should specifically handle: 1. Identifying and categorizing: Reserved Keywords, Identifiers, Literals (including Strings and Numbers), Operators, and Delimiters. 2. Managing line continuation using explicit line joining (i.e., handling backslashes). 3. Ignoring comments. 4. Handling strings and bytes literals (including raw and formatted strings). Your function should be named `parse_python_code` and should have the following signature: ```python def parse_python_code(lines: List[str]) -> List[str]: ... ``` **Input:** - `lines`: A list of strings where each string is a line of Python code. **Output:** - Returns a list of tokens (strings) extracted from the input lines, in the order they appear in the code. **Examples:** ```python lines = [ \\"def func(arg1, arg2):\\", \\" # This is a comment\\", \\" result = arg1 + arg2\\", \\" return result\\" ] print(parse_python_code(lines)) # Output: [\'def\', \'func\', \'(\', \'arg1\', \',\', \'arg2\', \')\', \':\', \'result\', \'=\', \'arg1\', \'+\', \'arg2\', \'return\', \'result\'] ``` **Constraints:** 1. Whitespace is only needed to separate tokens where ambiguity might otherwise occur. 2. Assume valid Python code as input. 3. Your solution should not use Python’s built-in `tokenize` library or any other similar library. The goal is to implement the lexical analysis manually. # Hints: 1. Utilize regular expressions for identifying various tokens. 2. Maintain a state machine to manage different contexts such as within a string or handling line continuation. # Notes: - You do not need to account for `INDENT` and `DEDENT` tokens for this task. - Handle both single and triple-quoted strings. - Ensure correct handling of escape sequences within strings and byte literals. - Pay attention to identifiers and keywords, especially noting the soft keywords introduced in Python 3.10.","solution":"import re from typing import List def parse_python_code(lines: List[str]) -> List[str]: keywords = {\'False\', \'None\', \'True\', \'and\', \'as\', \'assert\', \'async\', \'await\', \'break\', \'class\', \'continue\', \'def\', \'del\', \'elif\', \'else\', \'except\', \'finally\', \'for\', \'from\', \'global\', \'if\', \'import\', \'in\', \'is\', \'lambda\', \'nonlocal\', \'not\', \'or\', \'pass\', \'raise\', \'return\', \'try\', \'while\', \'with\', \'yield\'} delimiters = \'(){}[],:;.\' operators = \'+-*/%&|^~<>=!\' token_specification = [ (\'COMMENT\', r\'#.*\'), # Comments (\'STRING\', r\'((?:[^\\"]|.)*|\'\'\'(?:[^\']|.)*\'\'\'|\\"(?:[^\\"]|.)*\\"|\'(?:[^\']|.)*\')\'), # Double and single quoted strings (\'NUMBER\', r\'bd+(.d*)?b\'), # Integer or decimal number (\'ID\', r\'[A-Za-z_][A-Za-z0-9_]*\'), # Identifiers (\'OP\', r\'[+-*/%&|^~<>!=]+\'), # Operators (\'DELIM\', r\'[(){}[],:;.]\'), # Delimiters (\'NEWLINE\', r\'n\'), # Line endings (\'SKIP\', r\'[ tr]+\'), # Skip over spaces and tabs (\'MISMATCH\', r\'.\'), # Any other character ] tok_regex = \'|\'.join(f\'(?P<{pair[0]}>{pair[1]})\' for pair in token_specification) def tokenize(code: str): for mo in re.finditer(tok_regex, code): kind = mo.lastgroup value = mo.group() if kind == \'NEWLINE\': continue elif kind == \'SKIP\': continue elif kind == \'COMMENT\': continue elif kind == \'ID\' and value in keywords: yield value else: yield value code = \'n\'.join(lines) tokens = list(tokenize(code)) return tokens"},{"question":"Objective: This exercise evaluates your ability to use the `urllib.request` module to fetch web resources, handle HTTP requests and responses, manage exceptions, and add custom headers to requests. Problem Description: You are required to implement a function that fetches the HTML content of a given URL. The function should handle various HTTP errors and add custom headers to simulate a specific user agent while making the request. Function Signature: ```python def fetch_html_content(url: str, user_agent: str) -> str: pass ``` Input: - `url` (str): The URL of the web resource to fetch. - `user_agent` (str): A string representing the user agent to be added in the request headers. Output: - Returns a string containing the HTML content of the requested URL. Constraints: - The function should handle the following HTTP error codes gracefully by returning specific messages: - `404` (Not Found): \\"Error 404: The resource could not be found.\\" - `403` (Forbidden): \\"Error 403: Access to the resource is forbidden.\\" - `401` (Unauthorized): \\"Error 401: Authorization required.\\" - Other `HTTPError`: \\"HTTP Error: {code}\\" (where `{code}` is the error code) - If a `URLError` occurs (e.g., unreachable server), return: \\"URL Error: {reason}\\" (where `{reason}` is the error reason). - Use the provided `user_agent` string as the User-Agent header in the HTTP request. Example: ```python # Example user agent user_agent = \\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\\" # URL that might trigger 404 Error url_404 = \\"http://www.example.com/nonexistent.html\\" # Successful request url_success = \\"http://www.example.com\\" print(fetch_html_content(url_404, user_agent)) # Output: \\"Error 404: The resource could not be found.\\" print(fetch_html_content(url_success, user_agent)) # Output: The HTML content of http://www.example.com ``` Requirements: 1. Parse the URL to validate its format before making the request. 2. Send an HTTP GET request to fetch the content. 3. Add the provided `user_agent` to the request headers. 4. Handle `HTTPError` and `URLError` appropriately and return specified error messages. 5. If no errors occur, return the HTML content of the page. Implement the `fetch_html_content` function in Python, using the `urllib.request` module.","solution":"import urllib.request import urllib.error def fetch_html_content(url: str, user_agent: str) -> str: Fetches the HTML content of a given URL with a custom User-Agent header. Args: url (str): The URL of the web resource to fetch. user_agent (str): The user agent string to add in the request headers. Returns: str: The HTML content of the URL or an error message. req = urllib.request.Request(url, headers={\'User-Agent\': user_agent}) try: with urllib.request.urlopen(req) as response: return response.read().decode(\'utf-8\') except urllib.error.HTTPError as e: if e.code == 404: return \\"Error 404: The resource could not be found.\\" elif e.code == 403: return \\"Error 403: Access to the resource is forbidden.\\" elif e.code == 401: return \\"Error 401: Authorization required.\\" else: return f\\"HTTP Error: {e.code}\\" except urllib.error.URLError as e: return f\\"URL Error: {e.reason}\\""},{"question":"# Question: Custom JSON Serialization and Deserialization You are tasked with creating a module that both serializes and deserializes a specific set of objects to and from JSON strings. Specifically, the module should handle encoding and decoding of a custom `Person` class which contains the individual\'s name, age, and a list of skills. Requirements: 1. Implement a `Person` class with the following specifications: - `name`: A string representing the name of the person. - `age`: An integer representing the age of the person. - `skills`: A list of strings representing the skills the person has. 2. Implement custom JSON encoding and decoding logic for instances of the `Person` class using Python’s `json` module: - The custom encoder should encode `Person` instances to a JSON object with attributes \\"name\\", \\"age\\", and \\"skills\\". - The custom decoder should decode JSON objects back into `Person` instances. 3. Implement two functions, `person_to_json(person: Person, indent: int = None) -> str` and `json_to_person(json_str: str) -> Person`: - `person_to_json(person: Person, indent: int = None)`: This function takes a `Person` instance and returns a JSON string representation of the person. The `indent` parameter is optional and controls the indentation of the output JSON. - `json_to_person(json_str: str)`: This function takes a JSON string representation of a person and returns a `Person` instance. 4. Ensure that your implementation handles the following edge cases: - The JSON string representation should maintain the order of the keys in the `Person` class attributes: \\"name\\", \\"age\\", \\"skills\\". - The decoder should raise a `ValueError` if any of the required keys (\\"name\\", \\"age\\", \\"skills\\") are missing in the JSON string. Example Usage: ```python # Example Person object person = Person(name=\\"John Doe\\", age=30, skills=[\\"Python\\", \\"Data Analysis\\"]) # Serialize Person object to JSON string json_str = person_to_json(person, indent=4) print(json_str) # Output (formatted for easier reading): # { # \\"name\\": \\"John Doe\\", # \\"age\\": 30, # \\"skills\\": [ # \\"Python\\", # \\"Data Analysis\\" # ] # } # Deserialize JSON string to Person object new_person = json_to_person(json_str) print(new_person.name) # Output: John Doe print(new_person.age) # Output: 30 print(new_person.skills) # Output: [\'Python\', \'Data Analysis\'] ``` Constraints: - The name attribute is a non-empty string. - The age attribute is a positive integer. - The skills attribute is a list of non-empty strings. Provide your implementation below: ```python import json from typing import List class Person: def __init__(self, name: str, age: int, skills: List[str]): self.name = name self.age = age self.skills = skills def __eq__(self, other): if isinstance(other, Person): return self.name == other.name and self.age == other.age and self.skills == other.skills return False class PersonEncoder(json.JSONEncoder): def default(self, obj): if isinstance(obj, Person): return {\\"name\\": obj.name, \\"age\\": obj.age, \\"skills\\": obj.skills} return super().default(obj) def person_to_json(person: Person, indent: int = None) -> str: pass def json_to_person(json_str: str) -> Person: pass # Your implementation for person_to_json and json_to_person should go here ```","solution":"import json from typing import List, Dict, Any class Person: def __init__(self, name: str, age: int, skills: List[str]): if not name or not isinstance(name, str): raise ValueError(\\"Name must be a non-empty string.\\") if not isinstance(age, int) or age <= 0: raise ValueError(\\"Age must be a positive integer.\\") if not isinstance(skills, list) or not all(isinstance(skill, str) and skill for skill in skills): raise ValueError(\\"Skills must be a list of non-empty strings.\\") self.name = name self.age = age self.skills = skills def __eq__(self, other): if isinstance(other, Person): return self.name == other.name and self.age == other.age and self.skills == other.skills return False class PersonEncoder(json.JSONEncoder): def default(self, obj: Any) -> Any: if isinstance(obj, Person): return {\\"name\\": obj.name, \\"age\\": obj.age, \\"skills\\": obj.skills} return super().default(obj) def person_to_json(person: Person, indent: int = None) -> str: return json.dumps(person, cls=PersonEncoder, indent=indent) def json_to_person(json_str: str) -> Person: data = json.loads(json_str) if not all(key in data for key in [\\"name\\", \\"age\\", \\"skills\\"]): raise ValueError(\\"Missing required Person attributes in JSON string.\\") return Person(name=data[\\"name\\"], age=data[\\"age\\"], skills=data[\\"skills\\"])"},{"question":"# HTML Content Sanitizer You are given a web application that takes user-generated content and displays it on a webpage. To prevent HTML injection attacks, you need to sanitize and validate the HTML content provided by the users. Your task is to write two functions using the `html` module: 1. **sanitize_html**: This function should convert characters `&`, `<`, `>`, `\\"` and `\'` in a given string to their HTML-safe sequences. 2. **validate_html**: This function should convert any HTML-safe sequences back to their corresponding characters. Function 1: `sanitize_html` - **Input**: A string `s` containing HTML content. - **Output**: A string with the special characters converted to their HTML-safe sequences. **Example**: ```python assert sanitize_html(\'<script>alert(\\"Hi!\\")<\/script>\') == \'&lt;script&gt;alert(&quot;Hi!&quot;)&lt;/script&gt;\' ``` Function 2: `validate_html` - **Input**: A string `s` containing HTML-safe sequences. - **Output**: A string with the HTML-safe sequences converted back to their corresponding characters. **Example**: ```python assert validate_html(\'&lt;script&gt;alert(&quot;Hi!&quot;)&lt;/script&gt;\') == \'<script>alert(\\"Hi!\\")<\/script>\' ``` # Constraints - The length of string `s` will not exceed 10^4 characters. - Performance should be considered, especially for longer strings. # Implementation ```python import html def sanitize_html(s: str) -> str: Convert special characters in the string `s` to HTML-safe sequences. return html.escape(s, quote=True) def validate_html(s: str) -> str: Convert HTML-safe sequences in the string `s` back to their corresponding characters. return html.unescape(s) ``` Test your functions with different inputs to ensure they work correctly.","solution":"import html def sanitize_html(s: str) -> str: Convert special characters in the string `s` to HTML-safe sequences. return html.escape(s, quote=True) def validate_html(s: str) -> str: Convert HTML-safe sequences in the string `s` back to their corresponding characters. return html.unescape(s)"},{"question":"Given a large text data, you are required to write two functions using the `bz2` module: 1. `compress_text_to_file(text: str, filename: str, compresslevel: int = 9) -> None`: This function should compress the provided text data and write it to a `.bz2` file. The function should: - Take as input the text to be compressed, the output filename, and the compression level (default is 9). - Compress the text and save the compressed data to the specified filename. 2. `decompress_file_to_text(filename: str) -> str`: This function should decompress the provided `.bz2` file and return the decompressed text. The function should: - Take as input the filename of the `.bz2` file to be decompressed. - Read the compressed file and return the decompressed text. # Input and Output - The `compress_text_to_file` function should not return anything (i.e., `None`). - The `decompress_file_to_text` function should return the decompressed text as a string. # Constraints - The text data may be large, so the solution should handle large inputs efficiently. - The compression level is an integer between 1 (least compression) and 9 (most compression). - The filename must end with a `.bz2` extension. # Example Usage ```python text_data = Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus lacinia odio vitae vestibulum vestibulum. Cras venenatis euismod malesuada. # Compress the text data to a file compress_text_to_file(text_data, \\"example.bz2\\", compresslevel=5) # Decompress the text data from the file decompressed_text = decompress_file_to_text(\\"example.bz2\\") # Check if the decompressed text matches the original text assert decompressed_text == text_data ``` # Performance Consideration Your solution should handle large text inputs efficiently, especially considering memory usage during compression and decompression. # Notes - Ensure you handle file operations carefully, including opening and closing files. - Use exceptions to handle potential errors such as file not found or read/write errors.","solution":"import bz2 def compress_text_to_file(text: str, filename: str, compresslevel: int = 9) -> None: Compresses the given text and writes it to a .bz2 file. Parameters: text (str): The text to compress. filename (str): The name of the output file, must end with .bz2. compresslevel (int): The compression level from 1 to 9. if not filename.endswith(\'.bz2\'): raise ValueError(\\"Filename must end with \'.bz2\'\\") compressed_data = bz2.compress(text.encode(\'utf-8\'), compresslevel=compresslevel) with open(filename, \'wb\') as f: f.write(compressed_data) def decompress_file_to_text(filename: str) -> str: Decompresses the .bz2 file and returns the decompressed text. Parameters: filename (str): The name of the .bz2 file to decompress. Returns: str: The decompressed text. if not filename.endswith(\'.bz2\'): raise ValueError(\\"Filename must end with \'.bz2\'\\") with open(filename, \'rb\') as f: compressed_data = f.read() decompressed_data = bz2.decompress(compressed_data) return decompressed_data.decode(\'utf-8\')"},{"question":"Processing XML with PullDOM **Objective:** Your task is to use the `xml.dom.pulldom` module to parse and process an XML document. You need to implement a function that takes an XML document as input and extracts specific information based on the structure and content of the XML. **Problem Statement:** Write a function `extract_high_price_items(xml_string: str, threshold: int) -> List[Dict[str, Any]]` that takes: - `xml_string` (a string): a well-formed XML document containing items for sale. - `threshold` (an integer): a price threshold. The function should return a list of dictionaries. Each dictionary should contain details of items where the price attribute exceeds the given threshold. Each dictionary should contain: - `name`: the name of the item (extracted from the `name` attribute of the `item` elements). - `price`: the price of the item (extracted from the `price` attribute of the `item` elements). **Input:** - `xml_string` (str): A string representation of the XML document. - `threshold` (int): The price threshold value. **Output:** - A list of dictionaries containing details of items where the item price is greater than the threshold. **Constraints:** - Assume the XML is always well-formed. - The `price` attribute is always a numeric string. **Example:** ```python xml_data = \'\'\' <items> <item name=\\"Laptop\\" price=\\"1200\\"></item> <item name=\\"Mouse\\" price=\\"25\\"></item> <item name=\\"Keyboard\\" price=\\"75\\"></item> <item name=\\"Monitor\\" price=\\"300\\"></item> </items> \'\'\' threshold = 100 # Expected Output # [{\'name\': \'Laptop\', \'price\': 1200}, {\'name\': \'Monitor\', \'price\': 300}, {\'name\': \'Keyboard\', \'price\': 75}] extract_high_price_items(xml_data, threshold) ``` **Detailed Requirements:** 1. You should use the `xml.dom.pulldom` module to parse the XML string. 2. Iterate through the events using the pull parser mechanism. 3. For each `item` element, check if the price attribute exceeds the threshold. 4. Expand the node only when required to fetch more data (if applicable). 5. Return the list of dictionaries in no particular order. Use the provided example to guide your implementation. **Performance Requirements:** The function should handle large XML documents efficiently by leveraging the pull parsing model to avoid loading the entire DOM into memory.","solution":"import xml.dom.pulldom from typing import List, Dict, Any def extract_high_price_items(xml_string: str, threshold: int) -> List[Dict[str, Any]]: Returns a list of items where the price exceeds the given threshold. doc = xml.dom.pulldom.parseString(xml_string) items_above_threshold = [] for event, node in doc: if event == \\"START_ELEMENT\\" and node.tagName == \\"item\\": price = int(node.getAttribute(\\"price\\")) if price > threshold: item_info = { \\"name\\": node.getAttribute(\\"name\\"), \\"price\\": price } items_above_threshold.append(item_info) return items_above_threshold"},{"question":"# Mapping Operations Implementation In this assessment, you will be required to implement a class in Python that mimics the functionalities described by the mapping protocol as detailed in the provided documentation. Specifically, you will focus on the following methods: 1. `check(mapping)`: Determine if an object provides the mapping protocol. 2. `size(mapping)`: Returns the number of items in a mapping. 3. `get_item(mapping, key)`: Retrieve the value associated with a key. 4. `set_item(mapping, key, value)`: Set the value for a specific key. 5. `del_item(mapping, key)`: Remove an item by key. 6. `has_key(mapping, key)`: Checks if a key is present in the mapping. 7. `keys(mapping)`: Returns a list of all keys in the mapping. 8. `values(mapping)`: Returns a list of all values in the mapping. 9. `items(mapping)`: Returns a list of key-value pairs in the mapping. You should define a class `MappingOperations` and implement the aforementioned methods. Below are the method signatures and additional constraints: ```python class MappingOperations: @staticmethod def check(mapping) -> bool: Returns True if the object provides the mapping protocol, otherwise False. pass @staticmethod def size(mapping) -> int: Returns the number of items in the mapping. pass @staticmethod def get_item(mapping, key): Retrieves the value associated with the key in the mapping. pass @staticmethod def set_item(mapping, key, value) -> None: Sets the value for the specified key in the mapping. pass @staticmethod def del_item(mapping, key) -> None: Removes the item with the specified key from the mapping. pass @staticmethod def has_key(mapping, key) -> bool: Returns True if the key is present in the mapping, otherwise False. pass @staticmethod def keys(mapping) -> list: Returns a list of all keys in the mapping. pass @staticmethod def values(mapping) -> list: Returns a list of all values in the mapping. pass @staticmethod def items(mapping) -> list: Returns a list of key-value pairs in the mapping. pass ``` # Constraints - You may not use any third-party libraries. - The mapping input will always be a dictionary. - The key type will vary but will always be hashable. - The methods should handle exceptions gracefully and return `None` or `False` as appropriate if an operation fails. # Sample Usage ```python mapping = {\\"a\\": 1, \\"b\\": 2, \\"c\\": 3} op = MappingOperations() print(op.check(mapping)) # True print(op.size(mapping)) # 3 print(op.get_item(mapping, \\"a\\")) # 1 op.set_item(mapping, \\"d\\", 4) print(op.get_item(mapping, \\"d\\")) # 4 op.del_item(mapping, \\"b\\") print(op.size(mapping)) # 2 print(op.has_key(mapping, \\"c\\")) # True print(op.keys(mapping)) # [\'a\', \'c\', \'d\'] print(op.values(mapping)) # [1, 3, 4] print(op.items(mapping)) # [(\'a\', 1), (\'c\', 3), (\'d\', 4)] ``` You are expected to implement all the methods correctly to pass the assessment. Good luck!","solution":"class MappingOperations: @staticmethod def check(mapping) -> bool: Returns True if the object provides the mapping protocol, otherwise False. return isinstance(mapping, dict) @staticmethod def size(mapping) -> int: Returns the number of items in the mapping. if isinstance(mapping, dict): return len(mapping) return 0 @staticmethod def get_item(mapping, key): Retrieves the value associated with the key in the mapping. if isinstance(mapping, dict): return mapping.get(key) return None @staticmethod def set_item(mapping, key, value) -> None: Sets the value for the specified key in the mapping. if isinstance(mapping, dict): mapping[key] = value @staticmethod def del_item(mapping, key) -> None: Removes the item with the specified key from the mapping. if isinstance(mapping, dict): mapping.pop(key, None) @staticmethod def has_key(mapping, key) -> bool: Returns True if the key is present in the mapping, otherwise False. if isinstance(mapping, dict): return key in mapping return False @staticmethod def keys(mapping) -> list: Returns a list of all keys in the mapping. if isinstance(mapping, dict): return list(mapping.keys()) return [] @staticmethod def values(mapping) -> list: Returns a list of all values in the mapping. if isinstance(mapping, dict): return list(mapping.values()) return [] @staticmethod def items(mapping) -> list: Returns a list of key-value pairs in the mapping. if isinstance(mapping, dict): return list(mapping.items()) return []"},{"question":"# PyTorch Numerical Properties Objective Write a Python function using PyTorch to compare the numerical properties of different data types provided by the `torch.finfo` and `torch.iinfo` classes. The function should print a report of the properties in a human-readable format. Function Signature ```python def compare_numerical_properties(): pass ``` Requirements 1. The function should print the numerical properties of the following floating-point data types using `torch.finfo`: `torch.float32`, `torch.float64`, `torch.float16`, `torch.bfloat16`. 2. The function should print the numerical properties of the following integer data types using `torch.iinfo`: `torch.uint8`, `torch.int8`, `torch.int16`, `torch.int32`, `torch.int64`. 3. Each report should include the following properties: - For floating-point types (using `torch.finfo`): bits, eps, max, min, tiny, smallest_normal, and resolution. - For integer types (using `torch.iinfo`): bits, max, and min. 4. The report should be formatted clearly to ensure readability. Constraints - Do not use any external libraries other than PyTorch. - The function should not take any inputs. Example Output The function should print output similar to the following (truncated for brevity): ```plaintext Floating Point Data Types: - torch.float32: bits: 32 eps: 1.1920928955078125e-07 max: 3.4028234663852886e+38 min: -3.4028234663852886e+38 tiny: 1.1754943508222875e-38 smallest_normal: 1.1754943508222875e-38 resolution: 1e-07 ... Integer Data Types: - torch.int32: bits: 32 max: 2147483647 min: -2147483648 ... ``` This task will assess the students\' ability to: - Utilize PyTorch documentation. - Implement functionality to fetch and display numerical properties of different data types. - Format the output in a readable and organized manner.","solution":"import torch def compare_numerical_properties(): float_dtypes = [torch.float32, torch.float64, torch.float16, torch.bfloat16] int_dtypes = [torch.uint8, torch.int8, torch.int16, torch.int32, torch.int64] print(\\"Floating Point Data Types:\\") for dtype in float_dtypes: finfo = torch.finfo(dtype) print(f\\"- {dtype}:\\") print(f\\" bits: {finfo.bits}\\") print(f\\" eps: {finfo.eps}\\") print(f\\" max: {finfo.max}\\") print(f\\" min: {finfo.min}\\") print(f\\" tiny: {finfo.tiny}\\") print(f\\" smallest_normal: {finfo.tiny}\\") print(f\\" resolution: {finfo.eps}\\") print(\\"nInteger Data Types:\\") for dtype in int_dtypes: iinfo = torch.iinfo(dtype) print(f\\"- {dtype}:\\") print(f\\" bits: {iinfo.bits}\\") print(f\\" max: {iinfo.max}\\") print(f\\" min: {iinfo.min}\\") # Execute function to display numerical properties compare_numerical_properties()"},{"question":"**Objective:** Implement a function `wav_to_image` that reads a WAV file and generates an image where the colors are determined based on the audio data. The image\'s pixel intensities should be mapped using color conversions. **Function Signature:** ```python def wav_to_image(wav_file: str, width: int, height: int) -> Image: pass ``` **Input:** - `wav_file` (str): The path to a WAV file. - `width` (int): The width of the output image. - `height` (int): The height of the output image. **Output:** - An `Image` object created from the audio data with dimensions `width` x `height`. **Requirements:** 1. **Reading WAV Data:** Use the `wave` module to read the WAV file and obtain audio frames. 2. **Audio Processing:** Extract the amplitude data from the audio frames and normalize it. 3. **Color Conversion:** Use the `colorsys` module to convert the normalized audio data into color values in the RGB color space. 4. **Image Creation:** Use an appropriate image library (e.g., PIL) to create an image from the RGB values. **Constraints:** - The WAV file used as input is guaranteed to be a valid 16-bit PCM file. - The function should handle stereo (2-channel) and mono (1-channel) audio data. - The output image should have the provided width and height dimensions. **Performance Requirement:** - The function should be able to process a 10-second stereo WAV file with a sampling rate of 44.1 kHz and produce a corresponding image with dimensions `800x600` within 1 minute. **Example:** ```python from PIL import Image def wav_to_image(wav_file: str, width: int, height: int) -> Image: import wave import colorsys from PIL import Image # Open the WAV file with wave.open(wav_file, \'r\') as wav: n_channels = wav.getnchannels() sampwidth = wav.getsampwidth() framerate = wav.getframerate() n_frames = wav.getnframes() audio_frames = wav.readframes(n_frames) # Extract amplitude data # Assuming 16-bit PCM audio format import struct if sampwidth == 2: fmt = f\\"{n_frames * n_channels}h\\" amplitudes = struct.unpack(fmt, audio_frames) # Normalize amplitude data to the range [0, 1] max_amplitude = 2**15 normalized_amplitudes = [((amplitude + max_amplitude) / (2 * max_amplitude)) for amplitude in amplitudes] # Convert the normalized audio data to RGB using colorsys image_data = [] for amplitude in normalized_amplitudes[:width * height]: # Limit to image size hue = amplitude # Use amplitude as hue value r, g, b = colorsys.hsv_to_rgb(hue, 1.0, 1.0) image_data.append((int(r * 255), int(g * 255), int(b * 255))) # Create an image image = Image.new(\\"RGB\\", (width, height)) image.putdata(image_data) return image # Example usage image = wav_to_image(\\"example.wav\\", 800, 600) image.show() ``` **Notes:** - This is just an example of possible usage; your implementation might differ. - Be sure to handle edge cases and exceptions where necessary.","solution":"from PIL import Image import wave import colorsys import struct def wav_to_image(wav_file: str, width: int, height: int) -> Image: Converts a WAV file to an image with dimensions width x height. # Open the WAV file with wave.open(wav_file, \'r\') as wav: # Get audio parameters n_channels = wav.getnchannels() sampwidth = wav.getsampwidth() framerate = wav.getframerate() n_frames = wav.getnframes() audio_frames = wav.readframes(n_frames) # Ensure we are working with 16-bit PCM data assert sampwidth == 2, \\"Only 16-bit PCM WAV files are supported.\\" # Unpack audio frames fmt = f\\"{n_frames * n_channels}h\\" amplitudes = struct.unpack(fmt, audio_frames) # Normalize amplitude data to the range [0, 1] max_amplitude = 2**15 normalized_amplitudes = [((amplitude + max_amplitude) / (2 * max_amplitude)) for amplitude in amplitudes] # Prepare image data image_data = [] idx = 0 for _ in range(height): row = [] for _ in range(width): if idx < len(normalized_amplitudes): amplitude = normalized_amplitudes[idx] idx += 1 else: amplitude = 0.0 hue = amplitude # Use amplitude as hue value r, g, b = colorsys.hsv_to_rgb(hue, 1.0, 1.0) row.append((int(r * 255), int(g * 255), int(b * 255))) image_data.extend(row) # Create and return image image = Image.new(\\"RGB\\", (width, height)) image.putdata(image_data) return image"},{"question":"# Unicode String Processor Objective: Implement a Python function that processes a given Unicode string and performs various transformations based on Unicode character properties and normalization forms. Function Signature: ```python def unicode_string_processor(input_str: str) -> dict: pass ``` Input: - `input_str` (str): A non-empty Unicode string containing characters from various Unicode character sets. Output: - A dictionary with the following keys and corresponding values: - `\'original\'`: The original input string. - `\'normalized_nfc\'`: The input string normalized to NFC (Normalization Form C). - `\'normalized_nfd\'`: The input string normalized to NFD (Normalization Form D). - `\'decimal_values\'`: A list of decimal values of characters in the string that are decimal digits. Non-decimal characters should be ignored. - `\'digit_values\'`: A list of digit values of characters in the string that are digit characters. Non-digit characters should be ignored. - `\'numeric_values\'`: A list of numeric values of characters in the string that are numeric. Non-numeric characters should be ignored. - `\'categories\'`: A list of tuples where each tuple contains a character from the input string and its corresponding Unicode general category. Constraints: - The input string `input_str` will have a length between 1 and 1000 characters. - Characters in the string may belong to any Unicode category, including letters, digits, symbols, punctuation, etc. Performance Requirements: - The function should handle the maximum input size efficiently and provide the results in a reasonable time (within a few seconds). Example: ```python input_str = \\"Café 123\\" result = unicode_string_processor(input_str) # Expected result: # { # \'original\': \'Café 123\', # \'normalized_nfc\': \'Café 123\', # \'normalized_nfd\': \'Café 123\', # \'decimal_values\': [1, 2, 3], # \'digit_values\': [1, 2, 3], # \'numeric_values\': [1, 2, 3], # \'categories\': [(\'C\', \'Lu\'), (\'a\', \'Ll\'), (\'f\', \'Ll\'), (\'é\', \'Ll\'), (\' \', \'Zs\'), (\'1\', \'Nd\'), (\'2\', \'Nd\'), (\'3\', \'Nd\')] # } ``` Use the `unicodedata` module to implement the functions and create meaningful processing of the Unicode string based on the properties defined in the documentation.","solution":"import unicodedata def unicode_string_processor(input_str: str) -> dict: original = input_str normalized_nfc = unicodedata.normalize(\'NFC\', input_str) normalized_nfd = unicodedata.normalize(\'NFD\', input_str) decimal_values = [int(char) for char in input_str if char.isdecimal()] digit_values = [int(char) for char in input_str if char.isdigit()] numeric_values = [float(unicodedata.numeric(char)) for char in input_str if char.isnumeric()] categories = [(char, unicodedata.category(char)) for char in input_str] return { \'original\': original, \'normalized_nfc\': normalized_nfc, \'normalized_nfd\': normalized_nfd, \'decimal_values\': decimal_values, \'digit_values\': digit_values, \'numeric_values\': numeric_values, \'categories\': categories, }"},{"question":"# PyTorch MPS Backend Assessment In this question, you are required to demonstrate your understanding of the MPS backend in PyTorch. You will write functions to check MPS availability, perform tensor operations on the MPS device, and handle a simple neural network model using the MPS device. Part 1: MPS Availability Check Write a function `check_mps_availability` that checks if the MPS backend is available. The function should return a boolean indicating the availability. ```python def check_mps_availability() -> bool: Checks whether the MPS backend is available. Returns: bool: True if MPS is available, False otherwise. pass ``` Part 2: Tensor Operations on MPS Write a function `perform_tensor_operations` that creates a tensor of ones with a given shape on the MPS device, performs a given operation on the tensor, and returns the result. ```python def perform_tensor_operations(shape: tuple, operation: str): Creates a tensor of ones with the given shape on the MPS device and performs the specified operation. Args: shape (tuple): Shape of the tensor to be created. operation (str): The operation to perform on the tensor. Supported operations: \\"add\\", \\"multiply\\". Returns: torch.Tensor: Resulting tensor after performing the operation. Raises: ValueError: If the specified operation is not supported. RuntimeError: If MPS backend is not available. pass ``` Supported operations: * \\"add\\": Adds 1 to each element in the tensor. * \\"multiply\\": Multiplies each element in the tensor by 2. Part 3: Model Handling on MPS Write a function `move_model_to_mps` that initializes a simple neural network model, moves it to the MPS device, and returns the model and a sample prediction. ```python import torch.nn as nn class SimpleNet(nn.Module): def __init__(self): super(SimpleNet, self).__init__() self.fc = nn.Linear(10, 1) def forward(self, x): return self.fc(x) def move_model_to_mps(input_tensor: torch.Tensor): Initializes a SimpleNet model, moves it to the MPS device, and returns the model and a sample prediction. Args: input_tensor (torch.Tensor): Input tensor for the model. Returns: tuple: The model on the MPS device and the sample prediction tensor. Raises: RuntimeError: If MPS backend is not available. pass ``` **Constraints:** * Ensure that MPS is available before performing any operations. Raise a `RuntimeError` if MPS is not available. * Use PyTorch version that supports the MPS backend. * Test the functions on a macOS device with an MPS-enabled GPU. By completing this assessment, you will demonstrate your understanding of the PyTorch MPS backend, tensor operations, and model handling on the MPS device.","solution":"import torch def check_mps_availability() -> bool: Checks whether the MPS backend is available. Returns: bool: True if MPS is available, False otherwise. return torch.backends.mps.is_available() def perform_tensor_operations(shape: tuple, operation: str): Creates a tensor of ones with the given shape on the MPS device and performs the specified operation. Args: shape (tuple): Shape of the tensor to be created. operation (str): The operation to perform on the tensor. Supported operations: \\"add\\", \\"multiply\\". Returns: torch.Tensor: Resulting tensor after performing the operation. Raises: ValueError: If the specified operation is not supported. RuntimeError: If MPS backend is not available. if not check_mps_availability(): raise RuntimeError(\\"MPS backend is not available.\\") device = torch.device(\\"mps\\") tensor = torch.ones(shape, device=device) if operation == \\"add\\": result = tensor + 1 elif operation == \\"multiply\\": result = tensor * 2 else: raise ValueError(f\\"Unsupported operation: {operation}\\") return result import torch.nn as nn class SimpleNet(nn.Module): def __init__(self): super(SimpleNet, self).__init__() self.fc = nn.Linear(10, 1) def forward(self, x): return self.fc(x) def move_model_to_mps(input_tensor: torch.Tensor): Initializes a SimpleNet model, moves it to the MPS device, and returns the model and a sample prediction. Args: input_tensor (torch.Tensor): Input tensor for the model. Returns: tuple: The model on the MPS device and the sample prediction tensor. Raises: RuntimeError: If MPS backend is not available. if not check_mps_availability(): raise RuntimeError(\\"MPS backend is not available.\\") device = torch.device(\\"mps\\") model = SimpleNet().to(device) input_tensor = input_tensor.to(device) output = model(input_tensor) return model, output"},{"question":"You are required to implement a function `find_clusters_and_predict` that performs the following tasks in sequence: 1. **Clustering using K-nearest neighbors**: - Use a subset of data points to fit a `KNeighborsTransformer` to build a nearest neighbors graph. - Apply `SpectralClustering` on the resulting graph to cluster the data points. 2. **Classification using Neighborhood Components Analysis (NCA)**: - Perform a nearest neighbors classification on the clustered data using `NeighborhoodComponentsAnalysis` and `KNeighborsClassifier`. - Train a new `KNeighborsClassifier` model on the transformed data obtained from the NCA. 3. **Prediction and Evaluation**: - Use the trained `KNeighborsClassifier` model to predict the labels for a test dataset. - Calculate and return the prediction accuracy. # Function Signature ```python def find_clusters_and_predict(X_train, y_train, X_test, y_test, n_clusters, n_neighbors): Parameters: - X_train (numpy.ndarray or pandas.DataFrame): Training data features. Shape (n_samples_train, n_features) - y_train (numpy.ndarray or pandas.Series): Training data labels. Shape (n_samples_train,) - X_test (numpy.ndarray or pandas.DataFrame): Test data features. Shape (n_samples_test, n_features) - y_test (numpy.ndarray or pandas.Series): Test data labels. Shape (n_samples_test,) - n_clusters (int): Number of clusters for SpectralClustering. - n_neighbors (int): Number of neighbors for KNeighborsClassifier and KNeighborsTransformer. Returns: - accuracy (float): Prediction accuracy of the model on the test dataset. pass ``` # Constraints and Considerations - You can assume that both training and test datasets are valid. - You should use `SpectralClustering` from `sklearn.cluster` for clustering. - Ensure the combination of `NeighborhoodComponentsAnalysis` and `KNeighborsClassifier` for classification. - Make sure that your solution handles the input data appropriately, including conversion to numpy arrays if necessary. - Performance considerations are secondary but ensure that your code is optimized for readability and logical flow. # Example Here\'s an example usage of the `find_clusters_and_predict` function: ```python import numpy as np from sklearn.model_selection import train_test_split from sklearn.datasets import load_iris # Load the iris dataset iris = load_iris() X, y = iris.data, iris.target # Split the data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Number of clusters and neighbors n_clusters = 3 n_neighbors = 5 # Find clusters and predict accuracy accuracy = find_clusters_and_predict(X_train, y_train, X_test, y_test, n_clusters, n_neighbors) print(f\\"Prediction accuracy: {accuracy:.2f}\\") ``` In this task, you need to demonstrate your understanding of clustering, dimensionality reduction, and classification using scikit-learn\'s nearest neighbor methods. Good luck!","solution":"from sklearn.cluster import SpectralClustering from sklearn.neighbors import KNeighborsClassifier from sklearn.neighbors import KNeighborsTransformer from sklearn.neighbors import NearestNeighbors from sklearn.neighbors import NeighborhoodComponentsAnalysis from sklearn.preprocessing import StandardScaler from sklearn.pipeline import Pipeline from sklearn.metrics import accuracy_score import numpy as np def find_clusters_and_predict(X_train, y_train, X_test, y_test, n_clusters, n_neighbors): Parameters: - X_train (numpy.ndarray or pandas.DataFrame): Training data features. Shape (n_samples_train, n_features) - y_train (numpy.ndarray or pandas.Series): Training data labels. Shape (n_samples_train,) - X_test (numpy.ndarray or pandas.DataFrame): Test data features. Shape (n_samples_test, n_features) - y_test (numpy.ndarray or pandas.Series): Test data labels. Shape (n_samples_test,) - n_clusters (int): Number of clusters for SpectralClustering. - n_neighbors (int): Number of neighbors for KNeighborsClassifier and KNeighborsTransformer. Returns: - accuracy (float): Prediction accuracy of the model on the test dataset. # Step 1: Clustering using SpectralClustering # First we use KNeighborsTransformer to create graph knn_transformer = KNeighborsTransformer(n_neighbors=n_neighbors) knn_graph = knn_transformer.fit_transform(X_train) # Apply Spectral Clustering spectral = SpectralClustering( n_clusters=n_clusters, affinity=\\"precomputed\\", assign_labels=\\"discretize\\", random_state=42 ) cluster_labels = spectral.fit_predict(knn_graph) # Step 2: Classification using Neighborhood Components Analysis (NCA) # We first scale the data scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) # Apply NCA nca = NeighborhoodComponentsAnalysis(random_state=42) nca.fit(X_train_scaled, cluster_labels) # fitting on cluster labels # Transforming the data X_train_nca = nca.transform(X_train_scaled) X_test_nca = nca.transform(X_test_scaled) # Train KNeighborsClassifier on the transformed data knn_clf = KNeighborsClassifier(n_neighbors=n_neighbors) knn_clf.fit(X_train_nca, y_train) # Step 3: Prediction and Evaluation y_pred = knn_clf.predict(X_test_nca) accuracy = accuracy_score(y_test, y_pred) return accuracy"},{"question":"# Context Variables Management in Python You are required to demonstrate your understanding of the `contextvars` module in Python by implementing a function that manages context variables during the execution of a code block. Objective 1. Implement a decorator function `manage_context` that: - Accepts a dictionary of context variables and their values. - Temporarily sets the provided context variables for the execution of the decorated function. - Ensures that the context variables revert to their original state after the function execution. 2. Implement another function `demo_function` that will: - Take a variable name and return its current context value. - Be decorated with `manage_context` to demonstrate the change in context variables. Function Specifications - `manage_context(context_vars: dict) -> Callable`: - **Input**: A dictionary of context variable names and their desired values for the decorated function. - **Output**: A decorator that sets the provided context variables before the function execution and reverts them after execution. - `demo_function(var_name: str) -> Any`: - **Input**: A string representing the name of the context variable. - **Output**: The current value of the specified context variable. Constraints - The context variables should be managed within the threading context of the function execution. - All context variables in `context_vars` should have a default value of `None` if not explicitly set. Example Usage ```python from contextvars import ContextVar # A decorator to manage context variables def manage_context(context_vars): ... # Define the context variable var1 = ContextVar(\'var1\', default=None) var2 = ContextVar(\'var2\', default=None) # A demo function to check context variable value @manage_context({\'var1\': \'temp_value1\', \'var2\': \'temp_value2\'}) def demo_function(var_name): if var_name == \'var1\': return var1.get() elif var_name == \'var2\': return var2.get() else: return \'Variable not found\' # Example checks print(demo_function(\'var1\')) # Outputs: temp_value1 print(demo_function(\'var2\')) # Outputs: temp_value2 ``` Ensure your implementation correctly manages context variables in a thread-safe manner and reverts them after the function execution.","solution":"from contextvars import ContextVar, copy_context from functools import wraps def manage_context(context_vars): def decorator(func): @wraps(func) def wrapper(*args, **kwargs): originals = {} temp_context = copy_context() for var_name, new_value in context_vars.items(): var = globals().get(var_name) if isinstance(var, ContextVar): originals[var_name] = var.get() temp_context.run(var.set, new_value) result = temp_context.run(func, *args, **kwargs) for var_name, original_value in originals.items(): var = globals().get(var_name) if var is not None and isinstance(var, ContextVar): var.set(original_value) return result return wrapper return decorator # Define context variables with default values var1 = ContextVar(\'var1\', default=None) var2 = ContextVar(\'var2\', default=None) @manage_context({\'var1\': \'temp_value1\', \'var2\': \'temp_value2\'}) def demo_function(var_name): if var_name == \'var1\': return var1.get() elif var_name == \'var2\': return var2.get() return \'Variable not found\'"},{"question":"# Question: Implement a Multi-threaded File Processor with Advanced Logging and Binary Data Handling You are required to implement a Python program that performs the following tasks: 1. Reads in a binary file and processes its content in chunks. 2. Logs the processing activities using the `logging` module. 3. Utilizes multi-threading to handle multiple chunks of data concurrently. 4. Ensures that the processed data conforms to a specific binary structure using the `struct` module. Requirements: 1. **Binary File Processing**: - The binary file contains records, each with the following structure: - An unsigned short integer (2 bytes) representing a record ID. - An unsigned integer (4 bytes) representing the data length. - A string of the specified length. - Read the file in chunks and process each chunk concurrently. 2. **Multi-threading**: - Use the `threading` module to create threads for processing each chunk of the binary file. - Ensure thread-safe logging of the processing activities. - Use locks to manage the shared resources during concurrent processing. 3. **Logging**: - Set up a logging configuration that writes logs to both the console and a file. - Log the start and end of the file processing. - Log the details of each processed record (ID, data length, and content). 4. **Binary Data Handling**: - Use the `struct` module to unpack binary data according to the defined record structure. - Pack and log the processed data in a human-readable format after processing each chunk. Function Signature: ```python import struct import threading import logging def process_binary_file(file_path: str, chunk_size: int): # Your implementation here ``` Constraints: - Implement proper exception handling for file operations. - Ensure that the binary file is processed correctly and efficiently. - The program should support different chunk sizes. - Demonstrate the usage of weak references if tracking objects for caching purposes in your multi-threaded solution. Example Output: ``` INFO:root:Starting to process the file: sample.bin INFO:root:Processed record - ID: 1, Length: 5, Data: Hello INFO:root:Processed record - ID: 2, Length: 5, Data: World ... INFO:root:Finished processing the file: sample.bin ``` Notes: - Include any additional helper functions or classes needed to achieve the specified tasks. - Write unit tests to verify the correctness of your implementation and log outputs.","solution":"import struct import threading import logging lock = threading.Lock() def setup_logging(): logging.basicConfig(level=logging.INFO, format=\'%(asctime)s - %(levelname)s - %(message)s\') logger = logging.getLogger() # Create file handler which logs even debug messages fh = logging.FileHandler(\'file_processing.log\') fh.setLevel(logging.INFO) # Create console handler ch = logging.StreamHandler() ch.setLevel(logging.INFO) # Create formatter and add it to the handlers formatter = logging.Formatter(\'%(asctime)s - %(levelname)s - %(message)s\') ch.setFormatter(formatter) fh.setFormatter(formatter) # Add the handlers to logger logger.addHandler(ch) logger.addHandler(fh) return logger logger = setup_logging() def process_chunk(data_chunk): offset = 0 while offset < len(data_chunk): record_id, length = struct.unpack_from(\'>HI\', data_chunk, offset) offset += 6 record_data = struct.unpack_from(f\'>{length}s\', data_chunk, offset)[0].decode() offset += length with lock: logger.info(f\'Processed record - ID: {record_id}, Length: {length}, Data: {record_data}\') def process_binary_file(file_path: str, chunk_size: int): threads = [] logger.info(f\'Starting to process the file: {file_path}\') try: with open(file_path, \'rb\') as binary_file: while True: data_chunk = binary_file.read(chunk_size) if not data_chunk: break thread = threading.Thread(target=process_chunk, args=(data_chunk,)) threads.append(thread) thread.start() # Joining all threads to ensure completion for thread in threads: thread.join() except Exception as e: logger.error(f\\"Error processing file {file_path}: {str(e)}\\") logger.info(f\'Finished processing the file: {file_path}\')"},{"question":"You are tasked with creating a Python extension module using C. The module will define a new Python object type named `CustomObject`, which contains: - An integer member `id`. - A string member `name`. Your task is to: 1. Define the `CustomObject` type. 2. Implement a method `describe` for the `CustomObject` type that returns a string representation of the object in the format: `\\"CustomObject(id=<id>, name=<name>)\\"`. 3. Create an `initialize` function that sets the `id` and `name` members. 4. Define getter and setter functions for these members. # Steps and Requirements: 1. **Define the Object Structure**: Use the given macros and types to define the object structure. 2. **Initialize Object**: Implement a function to initialize the object. 3. **Method Implementation**: Implement the `describe` method. 4. **Getter and Setter**: Implement the getter and setter functions for `id` and `name`. 5. **Basic Extension Module Setup**: Ensure to initialize the module and add the `CustomObject` type to the module. # Python Input/Output: - **Input**: The module should accept calls to create and manipulate `CustomObject` instances. - **Output**: The module should correctly output descriptions of the objects using the `describe` method. # Constraints: - Your solution should handle memory allocation and deallocation appropriately. - Ensure no memory leaks occur. # Example Usage in Python: ```python import custom_module obj = custom_module.CustomObject() obj.initialize(1, \\"Test Object\\") print(obj.describe()) # Should output: CustomObject(id=1, name=Test Object) obj.id = 2 obj.name = \\"Updated Name\\" print(obj.describe()) # Should output: CustomObject(id=2, name=Updated Name) ``` # Performance Requirements: - The system should handle multiple objects and method calls efficiently. The focus of this question is to demonstrate a comprehensive understanding of defining and manipulating Python objects at the C level, using the data structures and macros detailed in the documentation.","solution":"# We provide a detailed and complete Python equivalent definition of CustomObject to be implemented as a Python C extension. class CustomObject: def __init__(self, id=0, name=\\"\\"): self.id = id self.name = name def initialize(self, id, name): self.id = id self.name = name def describe(self): return f\\"CustomObject(id={self.id}, name={self.name})\\" @property def id(self): return self._id @id.setter def id(self, value): self._id = value @property def name(self): return self._name @name.setter def name(self, value): self._name = value # The actual C implementation would involve using the Python C API, # but that is out of scope for this particular Python-centric question."},{"question":"# Advanced Coding Assessment: Python Grammar Handling Objective To assess your understanding of Python\'s advanced syntactic features, you are required to implement a function that checks and processes a specific type of Python statements—in this case, **pattern matching** introduced in Python 3.10. You must handle the pattern matching in a way that validates and processes the given patterns according to the grammar rules specified. Problem Statement Implement a function `process_pattern_matching` that takes a string as input and processes it to validate and return the matched patterns. The function should: 1. Parse the input string to check for valid pattern matching statements. 2. Extract and return the patterns and their corresponding expressions. 3. Handle any syntax errors gracefully by returning an appropriate error message. Input Format - A single string `pattern_str` representing a pattern matching statement in Python. Output Format - A dictionary where keys are pattern types (`literal`, `capture`, `wildcard`, `sequence`, `mapping`, `class`) and values are the corresponding matched expressions or an error message in case of syntax errors. Constraints - The input string must conform to the Python 3.10 pattern matching syntax. - If the input string contains invalid syntax, return a dictionary with an appropriate error message. Example ```python def process_pattern_matching(pattern_str: str) -> dict: # Your implementation here # Example usage: input_str = match point: case (x, 0): result = \'x-axis\' case (0, y): result = \'y-axis\' case (x, y): result = \'other\' output = process_pattern_matching(input_str) print(output) # Expected output: { # \\"sequence\\": [ \\"(x, 0)\\", \\"(0, y)\\", \\"(x, y)\\" ], # \\"literal\\": [], # \\"capture\\": [], # \\"wildcard\\": [], # \\"mapping\\": [], # \\"class\\": [] # } ``` Note: - Ensure your function is efficient and can handle large input sizes within a reasonable time limit. - You may use regular expressions or parsing libraries to implement the function. - Document your code and provide comments for clarity.","solution":"import re def process_pattern_matching(pattern_str: str) -> dict: This function processes a pattern matching string and extracts pattern types. patterns = { \\"literal\\": [], \\"capture\\": [], \\"wildcard\\": [], \\"sequence\\": [], \\"mapping\\": [], \\"class\\": [], } # Regular expressions to detect different pattern types sequence_pattern = re.compile(r\\"bcases+(([^)]+)):\\") # Try to find sequence patterns sequences = sequence_pattern.findall(pattern_str) patterns[\\"sequence\\"].extend(sequences) # Extend this to other patterns: literal, capture, wildcard, mapping, class # For simplicity, these regex patterns are just placeholders # Actual implementation would need correct regex patterns for each type # literal_pattern = re.compile(r\'...\') # capture_pattern = re.compile(r\'...\') # wildcard_pattern = re.compile(r\'...\') # mapping_pattern = re.compile(r\'...\') # class_pattern = re.compile(r\'...\') return patterns"},{"question":"Task You are required to implement a set of functions utilizing the `binascii` module to perform encoding and decoding operations along with checksum calculation. Given the versatility of the `binascii` module, your implementation should demonstrate your ability to accurately use multiple functions from this module. Specifications 1. **Hexadecimal Conversion Functions** - Implement a function `hex_encode(data)` that converts binary data to its hexadecimal representation. - Implement a function `hex_decode(hexstr)` that converts a hexadecimal string back to binary data. 2. **Base64 Encoding Functions** - Implement a function `base64_encode(data, newline=True)` that converts binary data to a base64 encoded string, optionally including a newline character. - Implement a function `base64_decode(data)` that converts a base64 encoded string back to binary data. 3. **CRC Checksum Functions** - Implement a function `compute_crc32(data)` that computes the CRC-32 checksum of the given data. Input and Output Formats - `hex_encode(data: bytes) -> bytes`: Takes binary data (`bytes`) and returns the hexadecimal representation as bytes. - `hex_decode(hexstr: bytes) -> bytes`: Takes a hexadecimal string ( `bytes`) and returns the decoded binary data as bytes. - `base64_encode(data: bytes, newline: bool = True) -> bytes`: Takes binary data and returns the base64 encoded string as bytes, with an optional newline. - `base64_decode(data: bytes) -> bytes`: Takes a base64 encoded string and returns the decoded binary data as bytes. - `compute_crc32(data: bytes) -> int`: Takes binary data and returns the CRC-32 checksum as an integer. Constraints - The input binary data for encoding functions will have a length of at most 1000 bytes. - The hexadecimal string for decoding will always have an even length. - All inputs will be valid byte sequences suitable for their respective conversions. Example ```python def hex_encode(data): # Your code here def hex_decode(hexstr): # Your code here def base64_encode(data, newline=True): # Your code here def base64_decode(data): # Your code here def compute_crc32(data): # Your code here # Example usage: print(hex_encode(b\'xb9x01xef\')) # Output: b\'b901ef\' print(hex_decode(b\'b901ef\')) # Output: b\'xb9x01xef\' print(base64_encode(b\'hello world\')) # Output: b\'aGVsbG8gd29ybGQ=n\' print(base64_decode(b\'aGVsbG8gd29ybGQ=n\')) # Output: b\'hello world\' print(compute_crc32(b\'hello world\')) # Output: 222957957 ``` Implement these functions to demonstrate your understanding of binary data encoding/decoding and checksum calculations using the `binascii` module.","solution":"import binascii def hex_encode(data): Converts binary data to its hexadecimal representation. :param data: bytes :return: bytes return binascii.hexlify(data) def hex_decode(hexstr): Converts a hexadecimal string back to binary data. :param hexstr: bytes :return: bytes return binascii.unhexlify(hexstr) def base64_encode(data, newline=True): Converts binary data to a base64 encoded string, optionally including a newline character. :param data: bytes :param newline: bool :return: bytes encoded = binascii.b2a_base64(data) if not newline: return encoded.strip() return encoded def base64_decode(data): Converts a base64 encoded string back to binary data. :param data: bytes :return: bytes return binascii.a2b_base64(data) def compute_crc32(data): Computes the CRC-32 checksum of the given data. :param data: bytes :return: int return binascii.crc32(data) & 0xffffffff"},{"question":"**Advanced Data Visualization with Seaborn** **Objective**: Apply your understanding of the `seaborn.objects` module to create a complex plot that visualizes the Palmer Penguins dataset. # Task You are provided with the Palmer Penguins dataset and your objective is to create a comprehensive visualization that presents the relationship between the species, body mass, flipper length, and sex of the penguins. You will use `seaborn.objects.Plot` along with advanced customized visual elements in your plot. # Instructions 1. **Load Dataset**: - Load the `penguins` dataset using `seaborn.load_dataset(\\"penguins\\")`. 2. **Create a Base Plot**: - Initialize a plot with `seaborn.objects.Plot` where the `x` axis represents `species` and the `y` axis represents `body_mass_g`. - Color the data points by `sex`. 3. **Add Custom Elements**: - Add `Dash` elements to represent the body mass, controlling transparency (`alpha`) and line width relative to the flipper length (`flipper_length_mm`). - Add a secondary layer of `Dash` elements adjusting the width to 0.5. - Use `Dodge` to adjust spacing between categories. - Pair with an aggregate layer using `Agg`. - Overlay `Dots` on the plot to show the raw data points, using `Dodge` and `Jitter` to prevent overlapping. 4. **Handle Numerical Data**: - Create a secondary plot where both coordinates are numerical. - Plot `body_mass_g` on the x-axis and rounded `flipper_length_mm` (to the nearest 10) on the y-axis. - Add `Dash` marks orienting them along the y-axis. # Expected Output - The final output should be a combined visualization (dual plots) with all the above-specified elements, demonstrating clear relationships and handling of both categorical and numerical variables. # Code Scaffold ```python import seaborn.objects as so from seaborn import load_dataset # Load the dataset penguins = load_dataset(\\"penguins\\") # Create the base plot with categorical X-axis and numerical Y-axis p = so.Plot(penguins, x=\\"species\\", y=\\"body_mass_g\\", color=\\"sex\\") # Add Dash elements with alpha and line width p.add(so.Dash(alpha=0.5), linewidth=\\"flipper_length_mm\\") # Add Dash elements with width adjusted p.add(so.Dash(width=0.5)) # Use Dodge for spacing and add aggregation p.add(so.Dash(), so.Dodge()) p.add(so.Dash(), so.Agg(), so.Dodge()) p.add(so.Dots(), so.Dodge(), so.Jitter()) # Create a secondary numeric plot numeric_plot = ( so.Plot(penguins[\\"body_mass_g\\"], penguins[\\"flipper_length_mm\\"].round(-1)) .add(so.Dash(), orient=\\"y\\") ) # Render the plots p.show() numeric_plot.show() ``` # Additional Notes - Ensure your plot is clear and labels are properly set. - Submit your final code along with a rendered visualization.","solution":"import seaborn.objects as so from seaborn import load_dataset # Load the dataset penguins = load_dataset(\\"penguins\\") # Create the base plot with categorical X-axis and numerical Y-axis p = so.Plot(penguins, x=\\"species\\", y=\\"body_mass_g\\", color=\\"sex\\") # Add Dash elements with alpha and line width p.add(so.Dash(alpha=0.5), linewidth=\\"flipper_length_mm\\") # Add Dash elements with width adjusted p.add(so.Dash(width=0.5)) # Use Dodge for spacing and add aggregation p.add(so.Dash(), so.Dodge()) p.add(so.Dash(), so.Agg(), so.Dodge()) p.add(so.Dots(), so.Dodge(), so.Jitter()) # Create a secondary numeric plot numeric_plot = ( so.Plot(penguins, x=\\"body_mass_g\\", y=penguins[\\"flipper_length_mm\\"].round(-1)) .add(so.Dash(), orient=\\"y\\") ) # Render the plots p.show() numeric_plot.show()"},{"question":"Background: You are tasked with setting up a distributed training environment using PyTorch where communication between processes will be handled by NCCL (NVIDIA Collective Communications Library). To ensure robust and efficient operation, specific environment variables need to be set for `ProcessGroupNCCL`, which will control error handling, debugging, and monitoring. Problem: Implement a function `setup_distributed_training()` that initializes the NCCL environment variables and sets up the distributed training environment. Function Signature: ```python def setup_distributed_training(n_gpus_per_node: int, world_size: int, rank: int) -> None: ``` Input: - `n_gpus_per_node` (int): Number of GPUs available per node. - `world_size` (int): Total number of processes across all nodes. - `rank` (int): Rank of the current process. Tasks: 1. Set the necessary environment variables for `ProcessGroupNCCL` as follows: - `TORCH_NCCL_ASYNC_ERROR_HANDLING` should be set to `1`. - `TORCH_NCCL_HIGH_PRIORITY` should be set to `1`. - `TORCH_NCCL_ENABLE_TIMING` should be set to `1`. - `TORCH_NCCL_ENABLE_MONITORING` should be set to `1`. - `TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC` should be set to `10`. - `TORCH_NCCL_TRACE_BUFFER_SIZE` should be set to `1000`. 2. Initialize the distributed process group using the NCCL backend in PyTorch. Output: The function should not return anything, but correctly set the environment variables and initialize the process group. Constraints: - Assume that all nodes can communicate with each other. - The function should be used in a distributed environment where each process calls this function independently. Example: ```python # Example setup for 2 gpus per node, 4 processes in total, with rank 0 setup_distributed_training(n_gpus_per_node=2, world_size=4, rank=0) ``` Additional Information: Make sure PyTorch, CUDA, and NCCL are correctly installed in your environment before testing this function. This setup typically would be part of a larger script managing distributed training, so ensure to use appropriate PyTorch utilities such as `torch.distributed.init_process_group`.","solution":"import os import torch.distributed as dist def setup_distributed_training(n_gpus_per_node: int, world_size: int, rank: int) -> None: Sets up the environment for distributed training using NCCL backend. Args: n_gpus_per_node (int): Number of GPUs per node. world_size (int): Total number of processes across all nodes. rank (int): Rank of the current process. # Set the NCCL environment variables os.environ[\'TORCH_NCCL_ASYNC_ERROR_HANDLING\'] = \'1\' os.environ[\'TORCH_NCCL_HIGH_PRIORITY\'] = \'1\' os.environ[\'TORCH_NCCL_ENABLE_TIMING\'] = \'1\' os.environ[\'TORCH_NCCL_ENABLE_MONITORING\'] = \'1\' os.environ[\'TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC\'] = \'10\' os.environ[\'TORCH_NCCL_TRACE_BUFFER_SIZE\'] = \'1000\' # Initialize the process group dist.init_process_group( backend=\'nccl\', init_method=\'env://\', world_size=world_size, rank=rank )"},{"question":"# Question: Porting Python 2 Code to Python 3 You are given a piece of Python 2.7 code that performs file operations and processes text data. Your task is to convert this code to make it compatible with Python 3 while retaining compatibility with Python 2.7. Original Python 2.7 Code: ```python import os def read_file(file_path): with open(file_path, \'r\') as file: return file.read() def process_data(data): result = {} for line in data.split(\'n\'): key, value = line.split(\'=\') result[key] = float(value) return result def save_result(file_path, data): with open(file_path, \'w\') as file: file.write(str(data)) if __name__ == \\"__main__\\": input_path = \\"input.txt\\" output_path = \\"output.txt\\" if os.path.exists(input_path): file_data = read_file(input_path) processed_data = process_data(file_data) save_result(output_path, processed_data) else: print \\"Input file does not exist.\\" ``` # Requirements: 1. Ensure that the code runs on both Python 2.7 and Python 3.x. 2. Handle differences in the division operation. 3. Manage the distinction between text and binary data. 4. Use feature detection to handle compatibility issues. # Instructions: 1. Update all print statements to be compatible with both versions. 2. Modify file handling to ensure correct opening and reading modes. 3. Ensure all division operations provide the correct results in both versions. 4. Use future imports and other necessary tools to achieve compatibility. # Expected Output: The converted code should read data from `input.txt`, process it, and save the result to `output.txt`. If the `input.txt` does not exist, it should notify the user. # Constraints: 1. Assume `input.txt` contains lines formatted as `key=value` where `value` is a number. 2. You should not modify the overall structure of the program. 3. Ensure readability and maintainability of the code. Example Input (`input.txt`): ``` a=2.5 b=3 c=7.1 ``` Example Output (`output.txt`): ``` {\'a\': 2.5, \'b\': 3.0, \'c\': 7.1} ``` # Submission: Submit your updated code as a single Python file.","solution":"from __future__ import print_function, division import os def read_file(file_path): with open(file_path, \'r\') as file: return file.read() def process_data(data): result = {} for line in data.split(\'n\'): if line.strip(): # To avoid processing empty lines key, value = line.split(\'=\') result[key] = float(value) return result def save_result(file_path, data): with open(file_path, \'w\') as file: file.write(str(data)) if __name__ == \\"__main__\\": input_path = \\"input.txt\\" output_path = \\"output.txt\\" if os.path.exists(input_path): file_data = read_file(input_path) processed_data = process_data(file_data) save_result(output_path, processed_data) else: print(\\"Input file does not exist.\\")"},{"question":"**Question: Parsing and Summarizing Email Content** You have been tasked with creating a utility function that will analyze email messages. This function will parse the email content from a bytes object, extract key information, and return a summary dictionary. The summary should include the following details: - Subject of the email. - Sender\'s email address. - Receiver\'s email address. - Whether the email is multipart. - Number of parts if the email is multipart. - Number of detected defects, if any. **Function Signature:** ```python def summarize_email(email_bytes: bytes) -> dict: pass ``` **Input:** - `email_bytes` (bytes): A bytes object containing the raw email data. **Output:** - `summary` (dict): A dictionary containing the extracted summary with the following keys: - `\'subject\'`: (str) The subject of the email. - `\'from\'`: (str) The sender\'s email address. - `\'to\'`: (str) The receiver\'s email address. - `\'is_multipart\'`: (bool) Whether the email is multipart. - `\'num_parts\'`: (int) The number of parts in the email if it is multipart (0 if not). - `\'num_defects\'`: (int) The number of defects detected in the email. **Constraints:** - The input email will have well-formed headers. - The function should handle both MIME and non-MIME messages. **Example:** ```python email_bytes = b\\"From: sender@example.comrnTo: receiver@example.comrnSubject: Test EmailrnrnThis is the body of the email.\\" summary = summarize_email(email_bytes) print(summary) ``` Expected output: ```python { \'subject\': \'Test Email\', \'from\': \'sender@example.com\', \'to\': \'receiver@example.com\', \'is_multipart\': False, \'num_parts\': 0, \'num_defects\': 0 } ``` Implementing this function will require a good understanding of the `email.parser` module and handling email message objects. **Solution Template:** ```python from email import policy from email.parser import BytesParser def summarize_email(email_bytes: bytes) -> dict: # Use BytesParser to parse the email bytes parser = BytesParser(policy=policy.default) msg = parser.parsebytes(email_bytes) # Extract relevant details subject = msg[\'subject\'] or \\"\\" sender = msg[\'from\'] or \\"\\" receiver = msg[\'to\'] or \\"\\" is_multipart = msg.is_multipart() num_parts = len(msg.get_payload()) if is_multipart else 0 num_defects = len(msg.defects) # Create summary dictionary summary = { \'subject\': subject, \'from\': sender, \'to\': receiver, \'is_multipart\': is_multipart, \'num_parts\': num_parts, \'num_defects\': num_defects } return summary ```","solution":"from email import policy from email.parser import BytesParser def summarize_email(email_bytes: bytes) -> dict: # Use BytesParser to parse the email bytes parser = BytesParser(policy=policy.default) msg = parser.parsebytes(email_bytes) # Extract relevant details subject = msg[\'subject\'] or \\"\\" sender = msg[\'from\'] or \\"\\" receiver = msg[\'to\'] or \\"\\" is_multipart = msg.is_multipart() num_parts = len(msg.get_payload()) if is_multipart else 0 num_defects = len(msg.defects) # Create summary dictionary summary = { \'subject\': subject, \'from\': sender, \'to\': receiver, \'is_multipart\': is_multipart, \'num_parts\': num_parts, \'num_defects\': num_defects } return summary"},{"question":"# Python Coding Assessment Problem Statement Your task is to create a Python program that starts a pseudo-terminal session, runs a specified command within it, captures the output of that command, and logs both the input and output to a file. You are required to: 1. Fork a new process using `pty.fork()`. 2. Execute a specified command in the child process. 3. Capture and log the input and output of the command in a file. # Function Signature ```python def run_command_and_log(command: str, log_filename: str) -> None: Starts a pseudo-terminal session, runs the specified command, and logs input and output to the specified file. :param command: The command to be executed in the pseudo-terminal. :param log_filename: The filename where the input and output log should be stored. :raises ValueError: If the command or log_filename is not a non-empty string. ``` # Constraints - You must handle possible exceptions that may occur during the execution of the command or file operations. - The command to execute and the log file name should be valid non-empty strings. - Your implementation should write meaningful logs including timestamps for command start and end times. # Performance Requirement - The solution should efficiently handle the terminal session and file logging even when dealing with large command outputs. # Example ```python # Call to the function to execute \'ls -l\' and log the interaction run_command_and_log(\'ls -l\', \'session_log.txt\') # Expected behavior: The function should start a terminal session to run the \'ls -l\' command, # and log the command input/output to \'session_log.txt\' with appropriate timestamps. ``` Use the provided \\"pty\\" module functions (`fork`, `openpty`, `spawn`) to implement the solution. # Notes: - Ensure to include the necessary imports. - You can use the `os.read` and `os.write` functions to interact with file descriptors. - Test your function on a Unix-like operating system for consistent results. Good luck!","solution":"import os import pty import datetime def run_command_and_log(command: str, log_filename: str) -> None: Starts a pseudo-terminal session, runs the specified command, and logs input and output to the specified file. :param command: The command to be executed in the pseudo-terminal. :param log_filename: The filename where the input and output log should be stored. :raises ValueError: If the command or log_filename is not a non-empty string. if not command or not isinstance(command, str): raise ValueError(\\"The command must be a non-empty string.\\") if not log_filename or not isinstance(log_filename, str): raise ValueError(\\"The log filename must be a non-empty string.\\") def read(fd): Reads from a file descriptor until EOF output = [] while True: try: data = os.read(fd, 1024) if not data: break output.append(data.decode(\'utf-8\')) except OSError: break return \'\'.join(output) pid, fd = pty.fork() if pid == 0: # Child process os.execlp(command.split()[0], *command.split()) else: # Parent process with open(log_filename, \'a\') as log_file: start_time = datetime.datetime.now().isoformat() log_file.write(f\\"Command executed: {command}n\\") log_file.write(f\\"Start time: {start_time}n\\") # Capture the command output output = read(fd) end_time = datetime.datetime.now().isoformat() log_file.write(f\\"Output:n{output}n\\") log_file.write(f\\"End time: {end_time}n\\") os.close(fd)"},{"question":"Problem Statement You are tasked with designing a custom function that fits isotonic regression to a given dataset and evaluates its performance. Your function should also handle situations where the `increasing` constraint needs to be decided automatically based on the data. Function Signature ```python def fit_and_evaluate_isotonic(X: List[float], y: List[float], increasing: Union[bool, str] = \'auto\') -> Tuple[float, List[float]]: Fits an isotonic regression model to the given data and evaluates its performance. Parameters: - X (List[float]): A list of input feature values. - y (List[float]): A list of target values. - increasing (Union[bool, str]): Constraint for the isotonic regression. Can be True, False, or \'auto\'. Returns: - mse (float): Mean squared error of the model on the input data. - predictions (List[float]): List of predicted values corresponding to the input feature values. ``` Input - `X`: A list of floats representing the input feature values. Length (n). - `y`: A list of floats representing the target values. Length (n). - `increasing`: A boolean or string indicating the monotonic constraint, which can be `True` (non-decreasing), `False` (non-increasing), or `\'auto\'` (automatically decide based on the data). Output - `mse`: A float representing the mean squared error of the model on the input data. - `predictions`: A list of floats representing the predicted values corresponding to the input feature values. Constraints 1. The lengths of `X` and `y` will always be the same, `1 ≤ n ≤ 1000`. 2. The values in `X` and `y` are arbitrary real quantities. Performance Requirements 1. The function should execute within a reasonable time frame for `1 ≤ n ≤ 1000`. Example ```python X = [1.0, 2.0, 3.0, 4.0, 5.0] y = [2.0, 1.0, 3.5, 2.5, 5.0] increasing = \'auto\' mse, predictions = fit_and_evaluate_isotonic(X, y, increasing) print(f\\"Mean Squared Error: {mse}\\") print(f\\"Predictions: {predictions}\\") ``` Note - Ensure to use the `IsotonicRegression` class from `sklearn.isotonic`. - Your implementation should handle the `\'auto\'` setting for the `increasing` parameter appropriately. - You can use the `mean_squared_error` from `sklearn.metrics` to calculate the mean squared error.","solution":"from typing import List, Union, Tuple from sklearn.isotonic import IsotonicRegression from sklearn.metrics import mean_squared_error def fit_and_evaluate_isotonic(X: List[float], y: List[float], increasing: Union[bool, str] = \'auto\') -> Tuple[float, List[float]]: Fits an isotonic regression model to the given data and evaluates its performance. Parameters: - X (List[float]): A list of input feature values. - y (List[float]): A list of target values. - increasing (Union[bool, str]): Constraint for the isotonic regression. Can be True, False, or \'auto\'. Returns: - mse (float): Mean squared error of the model on the input data. - predictions (List[float]): List of predicted values corresponding to the input feature values. if increasing == \'auto\': # Determine whether the data is more increasing or decreasing increasing = sum(y[i] <= y[i + 1] for i in range(len(y)-1)) >= len(y) / 2 # Fit the isotonic regression model iso_reg = IsotonicRegression(increasing=increasing) y_pred = iso_reg.fit_transform(X, y) # Calculate mean squared error mse = mean_squared_error(y, y_pred) return mse, y_pred.tolist()"},{"question":"# PyTorch Numerical Properties: `torch.finfo` and `torch.iinfo` Objective: You are required to implement a function that takes a PyTorch data type as input and returns a dictionary containing its numerical properties. Task: 1. Implement the function `get_dtype_properties(dtype)`. 2. The function should receive a PyTorch dtype as an argument (e.g., `torch.float32`, `torch.int8`). 3. Depending on whether the dtype is a floating point type or an integer type, use either `torch.finfo` or `torch.iinfo` to obtain the properties. 4. Construct a dictionary with the relevant properties and their values. Specifications: - If the `dtype` is a floating point type, the dictionary should include: - `bits` - `eps` - `max` - `min` - `tiny` - `resolution` - If the `dtype` is an integer type, the dictionary should include: - `bits` - `max` - `min` Input: - `dtype`: A PyTorch data type (e.g., `torch.float32`, `torch.int8`). Output: - A dictionary containing the relevant numerical properties of the given dtype. Constraints: - The input `dtype` will always be a valid PyTorch data type. - The function should handle both floating point and integer data types accurately. Example: ```python import torch def get_dtype_properties(dtype): # Your implementation here # Example usage: dtype_fp = torch.float32 dtype_int = torch.int8 fp_properties = get_dtype_properties(dtype_fp) print(fp_properties) # Expected output: # {\'bits\': 32, \'eps\': 1.1920928955078125e-07, \'max\': 3.4028234663852886e+38, # \'min\': -3.4028234663852886e+38, \'tiny\': 1.1754943508222875e-38, # \'resolution\': 1e-07} int_properties = get_dtype_properties(dtype_int) print(int_properties) # Expected output: # {\'bits\': 8, \'max\': 127, \'min\': -128} ``` Implement the `get_dtype_properties(dtype)` function to complete the task.","solution":"import torch def get_dtype_properties(dtype): Returns a dictionary containing numerical properties of a given PyTorch dtype. Args: dtype : torch.dtype : The data type for which the properties are needed. Returns: dictionary : A dictionary containing the properties of the dtype. if torch.is_floating_point(torch.tensor([], dtype=dtype)): finfo = torch.finfo(dtype) properties = { \'bits\': finfo.bits, \'eps\': finfo.eps, \'max\': finfo.max, \'min\': finfo.min, \'tiny\': finfo.tiny, \'resolution\': finfo.resolution } else: iinfo = torch.iinfo(dtype) properties = { \'bits\': iinfo.bits, \'max\': iinfo.max, \'min\': iinfo.min } return properties"},{"question":"# Garbage Collected Tree Structure **Objective**: Implement a custom `Tree` class in Python that supports cyclic garbage collection. This tree will consist of `Node` objects, and each `Node` can have multiple children. Description You are required to create a custom `Tree` structure using Python\'s cyclic garbage collection support. Each node in this tree should be capable of holding references to other nodes. Your task is to: 1. Define a `Node` class. 2. Ensure that instances of the `Node` class support cyclic garbage collection. 3. Implement a basic tree traversal method. 4. Provide functionality to track and untrack nodes for garbage collection. Requirements 1. **Node Class Definition**: - Each `Node` should have a reference to a value and a list of children nodes. - Implement the `tp_traverse` and `tp_clear` handlers to support garbage collection. - Use the appropriate memory management functions for creating and destroying node instances. - Ensure the node class is trackable by the garbage collector. 2. **Tree Class Definition**: - Should initialize with a root node. - Implement a method to add children to a node. - Implement a method for in-order traversal of the tree. 3. **Garbage Collection Control**: - Include methods to enable, disable, and check the status of garbage collection. - Include methods to track and untrack nodes manually. Input and Output - **Input**: `Node` values and child relationships. - **Output**: Traversal results of the tree and the state of the garbage collector. Constraints - Node values will be unique integers. - All tree operations should use cyclic garbage collection functions as described in the provided documentation. Example Usage ```python # Initialize the Tree with a root node root = Node(1) tree = Tree(root) # Add children to the root node child1 = Node(2) child2 = Node(3) tree.add_child(root, child1) tree.add_child(root, child2) # Perform in-order traversal print(tree.in_order_traversal()) # Output: [2, 1, 3] # Enable and Query Garbage Collection tree.enable_gc() print(tree.is_gc_enabled()) # Output: True # Disable Garbage Collection tree.disable_gc() print(tree.is_gc_enabled()) # Output: False ``` Implement the classes `Node` and `Tree` with the mentioned functionalities.","solution":"import gc class Node: def __init__(self, value): self.value = value self.children = [] self._is_tracked = False def __del__(self): print(f\'Node {self.value} is being collected\') def tp_traverse(self, visit, arg): for child in self.children: visit(child, arg) def tp_clear(self): self.children = [] class Tree: def __init__(self, root): assert isinstance(root, Node), \\"root must be a Node instance\\" self.root = root def add_child(self, parent, child): assert isinstance(parent, Node) and isinstance(child, Node), \\"parent and child must be Node instances\\" parent.children.append(child) def in_order_traversal(self): result = [] def _in_order(node): if not node: return if node.children: _in_order(node.children[0]) result.append(node.value) for child in node.children[1:]: _in_order(child) _in_order(self.root) return result def enable_gc(self): gc.enable() def disable_gc(self): gc.disable() def is_gc_enabled(self): return gc.isenabled() def manually_track_node(self, node): assert isinstance(node, Node), \\"Argument must be a Node instance\\" if not node._is_tracked: gc.collect() node._is_tracked = True def manually_untrack_node(self, node): assert isinstance(node, Node), \\"Argument must be a Node instance\\" if node._is_tracked: node.tp_clear() node._is_tracked = False"},{"question":"Objective Create a Python utility that can interact with multiple types of dbm databases (`dbm.gnu`, `dbm.ndbm`, and `dbm.dumb`). The utility should be capable of performing basic CRUD (Create, Read, Update, Delete) operations and output the final state of the database in a human-readable format. Instructions 1. Implement the function `manage_dbm_operations(db_type: str, filename: str, operations: list) -> dict`. 2. The function should: - Accept three parameters: - `db_type`: A string that indicates the type of dbm database. It can be one of `\'gnu\'`, `\'ndbm\'`, or `\'dumb\'`. - `filename`: The name of the database file to operate on. - `operations`: A list of operations to be performed on the database. Each operation is described as a tuple (`operation_type`, `key`, `value`). The `operation_type` can be one of `\'set\'`, `\'get\'`, or `\'delete\'`. The `value` is used only for the `\'set\'` operation. - Use the appropriate dbm submodule based on `db_type`. - Open the database using the appropriate mode (`\'c\'`). - Perform each operation as follows: - `\'set\'`: Set the key to the given value. - `\'get\'`: Retrieve the value of the given key. - `\'delete\'`: Delete the key from the database. - Convert all keys and values to bytes before performing the operations. - Return the final state of the database as a dictionary, with keys and values converted back to strings. Example ```python def manage_dbm_operations(db_type: str, filename: str, operations: list) -> dict: # Your implementation here # Sample operations db_type = \'gnu\' filename = \'testdb\' operations = [ (\'set\', \'name\', \'Alice\'), (\'set\', \'age\', \'30\'), (\'get\', \'name\', \'\'), (\'delete\', \'age\', \'\'), (\'get\', \'age\', \'\') ] result = manage_dbm_operations(db_type, filename, operations) print(result) ``` Expected Output (if `db_type` is `\'gnu\'`): ```python { \'name\': \'Alice\' } ``` Constraints - You must handle exceptions gracefully and not terminate the program unexpectedly. - Assume that keys and values provided in operations will be strings. - Use the context management (`with` statement) to handle opening and closing of the database. Notes - Be sure to read the documentation of the `dbm` module and its submodules carefully to understand their specific functionalities and constraints. - Ensure the solution is efficient and handles each type of database appropriately.","solution":"import dbm def manage_dbm_operations(db_type: str, filename: str, operations: list) -> dict: if db_type == \'gnu\': import dbm.gnu as db elif db_type == \'ndbm\': import dbm.ndbm as db elif db_type == \'dumb\': import dbm.dumb as db else: raise ValueError(\\"Unsupported db_type. Choose one of \'gnu\', \'ndbm\', \'dumb\'.\\") final_state = {} try: with db.open(filename, \'c\') as db_file: for operation in operations: op_type, key, value = operation key_bytes = key.encode(\'utf-8\') if op_type == \'set\': db_file[key_bytes] = value.encode(\'utf-8\') elif op_type == \'get\': if key_bytes in db_file: final_state[key] = db_file[key_bytes].decode(\'utf-8\') else: final_state[key] = None elif op_type == \'delete\': if key_bytes in db_file: del db_file[key_bytes] except Exception as e: raise RuntimeError(f\\"An error occurred while managing the database: {e}\\") return final_state"},{"question":"**Objective**: Demonstrate your comprehension of the seaborn library by visualizing airline passenger data in various forms. **Dataset**: Use the inbuilt seaborn \\"flights\\" dataset, which contains monthly airline passengers data from 1949 to 1960. # Task 1. **Load the Dataset**: - Load the flights dataset into a pandas DataFrame. - Display the first few rows of the dataset to understand its structure. 2. **Initial Visualization**: - Create a simple line plot to visualize the total number of passengers over the years. 3. **Monthly Trends**: - Pivot the data to create a wide-form DataFrame of passengers where rows are years and columns are months. - Draw a line plot where each line corresponds to the monthly passenger counts over years. 4. **Detailed Monthly Trends**: - Create a line plot to show the trend of passengers for the month of \'May\' across all years using long-form data. - Highlight the plot by adding markers and customizing the color. 5. **Group by Month**: - Create a line plot showing the number of passengers for each month over the years using the `hue` parameter to differentiate the months. 6. **Error Representation**: - Use the `fmri` dataset to create a line plot with `timepoint` on the x-axis and `signal` on the y-axis. - Group the plot by `event` and `region`, and display error bars representing two standard errors. # Implementation Details: - **Input**: No direct input. Use the inbuilt seaborn datasets (`flights` and `fmri`). - **Output**: Multiple line plots as described above. - **Constraints**: Ensure your code is well-documented and handles any edge cases (e.g., missing data). - **Considerations**: - Use appropriate titles, axis labels, and legends to make your plots informative. - Customize the color palette and line styles to enhance readability. # Performance Requirements: - The code should efficiently handle the given dataset sizes. - Visualizations should be clear and interpretable. **Example:** ```python # Step 3: Create a line plot for the month of \'May\' across all years import seaborn as sns import matplotlib.pyplot as plt # Load the dataset flights = sns.load_dataset(\\"flights\\") # Pivot the dataset to wide-form flights_wide = flights.pivot(index=\\"year\\", columns=\\"month\\", values=\\"passengers\\") # Plotting sns.lineplot(data=flights_wide) # Show plot plt.show() ``` **Note**: This is just an illustrative example. The submitted solution should cover all the tasks mentioned in the question.","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt def load_flights_data(): flights = sns.load_dataset(\\"flights\\") return flights def initial_visualization(flights): plt.figure() sns.lineplot(data=flights, x=\\"year\\", y=\\"passengers\\", ci=None) plt.title(\\"Total Number of Passengers Over the Years\\") plt.xlabel(\\"Year\\") plt.ylabel(\\"Passengers\\") plt.show() def monthly_trends(flights): flights_wide = flights.pivot(index=\\"year\\", columns=\\"month\\", values=\\"passengers\\") plt.figure() sns.lineplot(data=flights_wide) plt.title(\\"Monthly Passenger Counts Over the Years\\") plt.xlabel(\\"Year\\") plt.ylabel(\\"Passengers\\") plt.legend(title=\'Month\', loc=\'center left\', bbox_to_anchor=(1, 0.5)) plt.show() def detailed_monthly_trends(flights): may_passengers = flights[flights[\\"month\\"] == \\"May\\"] plt.figure() sns.lineplot(data=may_passengers, x=\\"year\\", y=\\"passengers\\", marker=\'o\', color=\'green\') plt.title(\\"Passengers in May Over the Years\\") plt.xlabel(\\"Year\\") plt.ylabel(\\"Passengers\\") plt.show() def group_by_month(flights): plt.figure() sns.lineplot(data=flights, x=\\"year\\", y=\\"passengers\\", hue=\\"month\\", palette=\\"tab10\\") plt.title(\\"Number of Passengers for Each Month Over the Years\\") plt.xlabel(\\"Year\\") plt.ylabel(\\"Passengers\\") plt.legend(title=\'Month\', bbox_to_anchor=(1.05, 1), loc=\'upper left\') plt.show() def errors_representation(): fmri = sns.load_dataset(\\"fmri\\") plt.figure() sns.lineplot(data=fmri, x=\\"timepoint\\", y=\\"signal\\", hue=\\"event\\", style=\\"region\\", ci=\\"sd\\") plt.title(\\"FMRI: Signal Over Time with Error Bars\\") plt.xlabel(\\"Timepoint\\") plt.ylabel(\\"Signal\\") plt.legend(title=\'Event/Region\', bbox_to_anchor=(1.05, 1), loc=\'upper left\') plt.show()"},{"question":"**Coding Assessment Question:** You are required to implement a function `compress_and_decompress(text: str, level: int = -1) -> str` that compresses a given string using zlib compression at a specified level, then decompresses it back to the original string. # Function Signature ```python def compress_and_decompress(text: str, level: int = -1) -> str: pass ``` # Parameters - `text` (str): The input string that needs to be compressed and then decompressed. - `level` (int, optional): The level of compression (default: -1, uses default compression level). The level should be an integer between 0 (no compression) and 9 (maximum compression). # Returns - `str`: The decompressed string which should match the original input string. # Constraints - The function should handle errors gracefully. If any error occurs during compression or decompression, the function should return the string `\\"Error: [error message]\\"`. # Example ```python text = \\"This is a test string for compression\\" level = 5 result = compress_and_decompress(text, level) print(result) # Output should be \\"This is a test string for compression\\" ``` # Notes 1. You are required to use the `zlib` module functions to perform compression and decompression. 2. Ensure that the decompressed string matches the original input string exactly. 3. Consider edge cases such as empty strings and extremely large strings.","solution":"import zlib def compress_and_decompress(text: str, level: int = -1) -> str: try: # Compress the text using zlib with the specified compression level compressed = zlib.compress(text.encode(\'utf-8\'), level) # Decompress the text back to original decompressed = zlib.decompress(compressed).decode(\'utf-8\') return decompressed except Exception as e: return f\\"Error: {str(e)}\\""},{"question":"# Custom PyTorch Function and Module Implementation Background In this task, you will demonstrate your understanding of PyTorch’s custom function and module mechanisms by implementing a custom autograd function and integrating it into a custom module. Specifically, you are required to create a `ReLU` (Rectified Linear Unit) activation function from scratch using the techniques described in the provided documentation. # Objective 1. **Custom Function**: Implement a custom autograd `Function` for the ReLU activation. 2. **Custom Module**: Integrate this custom function into a PyTorch `Module`. # Details Part 1: Custom Function Create a custom autograd `Function` named `CustomReLUFunction` that performs the ReLU operation. - **Forward Pass**: Implement the forward pass with the ReLU operation, which is defined as follows: [ text{ReLU}(x) = max(0, x) ] - **Backward Pass**: Implement the backward pass for the ReLU operation. The gradient of the ReLU function is: [ text{ReLU}\'(x) = begin{cases} 1 & text{if } x > 0 0 & text{if } x leq 0 end{cases} ] Part 2: Custom Module Create a custom PyTorch module named `CustomReLU`. This module should utilize `CustomReLUFunction` in its `forward` method such that the module can be used as a standard PyTorch activation layer. # Implementation Step 1: Implement the Custom Function ```python import torch from torch.autograd import Function class CustomReLUFunction(Function): @staticmethod def forward(ctx, input): # Save the input for use in the backward pass ctx.save_for_backward(input) # Apply ReLU: output is max(0, input) return torch.clamp(input, min=0) @staticmethod def backward(ctx, grad_output): # Retrieve the input from the context input, = ctx.saved_tensors # Compute the gradient with respect to the input grad_input = grad_output.clone() grad_input[input <= 0] = 0 return grad_input ``` Step 2: Implement the Custom Module ```python import torch.nn as nn class CustomReLU(nn.Module): def __init__(self): super(CustomReLU, self).__init__() def forward(self, input): return CustomReLUFunction.apply(input) ``` # Constraints - Your custom ReLU function should be implemented from scratch and should not use any existing ReLU implementations from PyTorch. - You should ensure that your custom function supports both forward and backward passes correctly. - Write a test case to verify the correctness of your implementation. # Example Usage ```python # Example usage x = torch.tensor([-1.0, 0.0, 1.0, 2.0], requires_grad=True) relu = CustomReLU() y = relu(x) y.backward(torch.ones_like(x)) print(\\"Input:\\", x) print(\\"Output:\\", y) print(\\"Gradients:\\", x.grad) ``` The expected output should be: - Output: `[0.0, 0.0, 1.0, 2.0]` (ReLU applied element-wise) - Gradients: `[0.0, 0.0, 1.0, 1.0]` (gradient of ReLU applied element-wise) # Submission Please submit your implementation as a single Python script or a Jupyter notebook that includes both the `CustomReLUFunction` and `CustomReLU` class definitions along with your test case.","solution":"import torch from torch.autograd import Function class CustomReLUFunction(Function): @staticmethod def forward(ctx, input): # Save the input for use in the backward pass ctx.save_for_backward(input) # Apply ReLU: output is max(0, input) return torch.clamp(input, min=0) @staticmethod def backward(ctx, grad_output): # Retrieve the input from the context input, = ctx.saved_tensors # Compute the gradient with respect to the input grad_input = grad_output.clone() grad_input[input <= 0] = 0 return grad_input import torch.nn as nn class CustomReLU(nn.Module): def __init__(self): super(CustomReLU, self).__init__() def forward(self, input): return CustomReLUFunction.apply(input)"},{"question":"**Question: Implementing a Custom MIME Type Handler** You are required to develop a custom MIME type handler using Python\'s `mimetypes` module. The handler should support the following functionalities: 1. Initialize custom MIME types by loading from one or more files. 2. Guess the MIME type and encoding of a given URL. 3. Find all possible extensions for a given MIME type. 4. Map a MIME type to its extension. 5. Add a new MIME type to extension mapping. 6. Load MIME type mappings from a custom file format and integrate them into the existing mappings. # Input and Output Formats **Function Signatures:** ```python def initialize_mime_types(files: list = None) -> None: Initializes MIME types from given file list. pass def guess_mime_type_and_encoding(url: str, strict: bool = True) -> tuple: Returns the MIME type and encoding of the given URL. pass def find_all_extensions_for_mime_type(mime_type: str, strict: bool = True) -> list: Returns all possible extensions for the given MIME type. pass def map_mime_type_to_extension(mime_type: str, strict: bool = True) -> str: Returns the extension for the given MIME type. pass def add_new_mime_type(mime_type: str, extension: str, strict: bool = True) -> None: Adds a new MIME type to extension mapping. pass def load_mime_types_from_file(filename: str) -> dict: Loads MIME type mappings from a given file and returns a dictionary. pass ``` # Detailed Description 1. **initialize_mime_types(files: list = None) -> None:** This function initializes the MIME types module. If a list of file names is provided, it augments the default type map with the mappings from these files. 2. **guess_mime_type_and_encoding(url: str, strict: bool = True) -> tuple:** This function returns a tuple `(type, encoding)` by guessing the MIME type and encoding of the file at the provided URL. 3. **find_all_extensions_for_mime_type(mime_type: str, strict: bool = True) -> list:** This function returns a list of all possible file extensions for the given MIME type. 4. **map_mime_type_to_extension(mime_type: str, strict: bool = True) -> str:** This function returns one possible file extension for the given MIME type. 5. **add_new_mime_type(mime_type: str, extension: str, strict: bool = True) -> None:** This function maps a new MIME type to a given file extension. 6. **load_mime_types_from_file(filename: str) -> dict:** This function loads MIME type mappings from a specified file and returns a dictionary mapping filename extensions to MIME types. # Constraints - You may assume that passed file names exist and can be read. - The MIME type mappings are case-insensitive. - If a MIME type or extension is unknown, functions may return `None`. - Use exception handling for file operations to ensure robustness. # Example Usage ```python initialize_mime_types([\'/path/to/custom_mime.types\']) mime_type, encoding = guess_mime_type_and_encoding(\'http://example.com/file.tar.gz\') print(mime_type, encoding) # Expected Output: \'application/x-tar\', \'gzip\' extensions = find_all_extensions_for_mime_type(\'application/json\') print(extensions) # Expected Output: [\'.json\'] extension = map_mime_type_to_extension(\'text/html\') print(extension) # Expected Output: \'.html\' add_new_mime_type(\'application/x-newtype\', \'.new\') mappings = load_mime_types_from_file(\'/path/to/file_mappings.types\') print(mappings) # Expected Output: {\'.new\': \'application/x-newtype\', ...} ``` Ensure to carefully handle different file formats and robustly manage MIME type mappings. This assessment will test your ability to integrate Python modules, handle file I/O, and manipulate dictionaries effectively.","solution":"import mimetypes def initialize_mime_types(files: list = None) -> None: Initializes MIME types from given file list. if files: for file in files: mimetypes.init([file]) else: mimetypes.init() def guess_mime_type_and_encoding(url: str, strict: bool = True) -> tuple: Returns the MIME type and encoding of the given URL. return mimetypes.guess_type(url, strict) def find_all_extensions_for_mime_type(mime_type: str, strict: bool = True) -> list: Returns all possible extensions for the given MIME type. extensions = [] for ext, type in mimetypes.types_map.items(): if type == mime_type: extensions.append(ext) return extensions def map_mime_type_to_extension(mime_type: str, strict: bool = True) -> str: Returns the extension for the given MIME type. for ext, type in mimetypes.types_map.items(): if type == mime_type: return ext return None def add_new_mime_type(mime_type: str, extension: str, strict: bool = True) -> None: Adds a new MIME type to extension mapping. mimetypes.add_type(mime_type, extension, strict) def load_mime_types_from_file(filename: str) -> dict: Loads MIME type mappings from a given file and returns a dictionary. mappings = {} with open(filename, \'r\') as file: for line in file: if not line.strip() or line.startswith(\'#\'): # Ignore comments and empty lines continue parts = line.strip().split() if len(parts) == 2: extension, mime_type = parts mappings[extension] = mime_type mimetypes.add_type(mime_type, extension) return mappings"},{"question":"# PyTorch Named Tensors Assessment **Objective**: Implement a function that performs multiple operations on named tensors, showcasing your understanding of creating, manipulating, and aligning named tensors. **Task**: Write a function `process_named_tensor` that takes a 4-dimensional tensor with specified dimension names and performs the following operations: 1. **Initialization**: - Create the following input tensor: ```python imgs = torch.randn(1, 2, 2, 3, names=(\'N\', \'C\', \'H\', \'W\')) ``` 2. **Renaming and Verification**: - Rename dimensions `H` and `W` to `Height` and `Width` respectively. - Verify that the names are updated correctly. 3. **Alignment**: - Create a tensor `scale` with the same number of channels as `C`: ```python scale = torch.randn(2, names=(\'C\',)) ``` - Align `scale` to the dimensions of `imgs` after renaming. 4. **Flattening and Unflattening**: - Flatten the `C`, `Height`, and `Width` dimensions into a single dimension named `features`. - Verify the new names and shape of the tensor. - Unflatten the `features` dimension back to the original named dimensions. 5. **Autograd**: - Assume the flattened tensor represents image features with a batch dimension. - Create a learnable weight tensor (with `requires_grad=True`) of appropriate shape and named dimensions. - Perform a simple operation (e.g., element-wise multiplication) and calculate the gradients with respect to the weights. **Constraints**: - The input tensor must always have named dimensions `N`, `C`, `H`, and `W`. - The function should handle the alignment and ensure proper broadcasting by name, without manual permutation. **Input**: - No input is required to call `process_named_tensor`, as the initialization is part of the function. **Output**: - The function should print the intermediate named tensors and shapes at each step for verification purposes. - Return the final gradient values for the weights. # Example Function Signature: ```python def process_named_tensor(): # Initialization imgs = torch.randn(1, 2, 2, 3, names=(\'N\', \'C\', \'H\', \'W\')) print(\\"Initial tensor:\\", imgs) # Renaming dimensions renamed_imgs = imgs.rename(H=\'Height\', W=\'Width\') print(\\"Renamed tensor:\\", renamed_imgs.names, renamed_imgs.shape) # Creating and aligning the scale tensor scale = torch.randn(2, names=(\'C\',)) aligned_scale = scale.align_as(renamed_imgs) print(\\"Aligned scale:\\", aligned_scale.names, aligned_scale.shape) # Flattening and unflattening dimensions flat_imgs = renamed_imgs.flatten([\'C\', \'Height\', \'Width\'], \'features\') print(\\"Flattened tensor:\\", flat_imgs.names, flat_imgs.shape) unflattened_imgs = flat_imgs.unflatten(\'features\', [(\'C\', 2), (\'Height\', 2), (\'Width\', 3)]) print(\\"Unflattened tensor:\\", unflattened_imgs.names, unflattened_imgs.shape) # Autograd operations weight = torch.randn(flat_imgs.size(-1), names=(\'features\',), requires_grad=True) loss = (flat_imgs * weight).sum() loss.backward() print(\\"Gradients:\\", weight.grad) return weight.grad # Call the function for output verification process_named_tensor() ``` **Note**: The above example contains expected operations and intermediate outputs. Ensure your implementation appropriately handles the conditions and performs the stated operations correctly.","solution":"import torch def process_named_tensor(): # Initialization imgs = torch.randn(1, 2, 2, 3, names=(\'N\', \'C\', \'H\', \'W\')) print(\\"Initial tensor:\\", imgs) # Renaming dimensions renamed_imgs = imgs.rename(H=\'Height\', W=\'Width\') print(\\"Renamed tensor:\\", renamed_imgs.names, renamed_imgs.shape) # Creating and aligning the scale tensor scale = torch.randn(2, names=(\'C\',)) aligned_scale = scale.align_as(renamed_imgs) print(\\"Aligned scale:\\", aligned_scale.names, aligned_scale.shape) # Flattening and unflattening dimensions flat_imgs = renamed_imgs.flatten([\'C\', \'Height\', \'Width\'], \'features\') print(\\"Flattened tensor:\\", flat_imgs.names, flat_imgs.shape) unflattened_imgs = flat_imgs.unflatten(\'features\', [(\'C\', 2), (\'Height\', 2), (\'Width\', 3)]) print(\\"Unflattened tensor:\\", unflattened_imgs.names, unflattened_imgs.shape) # Autograd operations weight = torch.randn(flat_imgs.size(-1), names=(\'features\',), requires_grad=True) loss = (flat_imgs * weight).sum() print(\\"Loss:\\", loss.item()) loss.backward() print(\\"Gradients:\\", weight.grad) return weight.grad # Call the function for output verification if __name__ == \\"__main__\\": process_named_tensor()"},{"question":"Objective: Implement a function to determine the similarities and differences between two given iterable sequences (lists, tuples, or ranges). Function Signature: ```python def analyze_sequence_relations(seq1: list | tuple | range, seq2: list | tuple | range) -> dict: ``` Description: The function `analyze_sequence_relations` takes two sequences (either lists, tuples, or ranges) and returns a dictionary with the following keys and respective values: - `\\"is_subset\\"`: a boolean indicating if `seq1` is a subset of `seq2`. - `\\"is_proper_subset\\"`: a boolean indicating if `seq1` is a proper subset of `seq2`. - `\\"is_superset\\"`: a boolean indicating if `seq1` is a superset of `seq2`. - `\\"is_proper_superset\\"`: a boolean indicating if `seq1` is a proper superset of `seq2`. - `\\"common_elements\\"`: a list of the common elements between `seq1` and `seq2`. - `\\"unique_to_seq1\\"`: a list of elements unique to `seq1`. - `\\"unique_to_seq2\\"`: a list of elements unique to `seq2`. Input: - `seq1`: A sequence (list, tuple, or range) of elements. - `seq2`: A sequence (list, tuple, or range) of elements. Output: - A dictionary containing the relationships and elements described above. Example: ```python seq1 = [1, 2, 3, 4] seq2 = [3, 4, 5, 6] result = analyze_sequence_relations(seq1, seq2) # Expected Output: # { # \\"is_subset\\": False, # \\"is_proper_subset\\": False, # \\"is_superset\\": False, # \\"is_proper_superset\\": False, # \\"common_elements\\": [3, 4], # \\"unique_to_seq1\\": [1, 2], # \\"unique_to_seq2\\": [5, 6] # } ``` Constraints: - The sequences `seq1` and `seq2` will contain elements that are hashable. - The sequences may have different lengths and types. You should **not** use set operations directly but rather utilize basic sequence operations and comprehensions (e.g., list comprehension, membership testing). Additional Requirements: - Ensure the function runs efficiently even if the sequences are long. - Use Python\'s built-in types and operations effectively to solve this problem. - Do not import any additional libraries. # Note: Your implementation should demonstrate a good understanding of Python 3.10 features and the behavior of its built-in types. Implementation: ```python def analyze_sequence_relations(seq1: list | tuple | range, seq2: list | tuple | range) -> dict: is_subset = all(x in seq2 for x in seq1) is_proper_subset = is_subset and any(x not in seq1 for x in seq2) is_superset = all(x in seq1 for x in seq2) is_proper_superset = is_superset and any(x not in seq2 for x in seq1) common_elements = [x for x in seq1 if x in seq2] unique_to_seq1 = [x for x in seq1 if x not in seq2] unique_to_seq2 = [x for x in seq2 if x not in seq1] return { \\"is_subset\\": is_subset, \\"is_proper_subset\\": is_proper_subset, \\"is_superset\\": is_superset, \\"is_proper_superset\\": is_proper_superset, \\"common_elements\\": common_elements, \\"unique_to_seq1\\": unique_to_seq1, \\"unique_to_seq2\\": unique_to_seq2 } ```","solution":"def analyze_sequence_relations(seq1: list | tuple | range, seq2: list | tuple | range) -> dict: is_subset = all(x in seq2 for x in seq1) is_proper_subset = is_subset and any(x not in seq1 for x in seq2) is_superset = all(x in seq1 for x in seq2) is_proper_superset = is_superset and any(x not in seq2 for x in seq1) common_elements = [x for x in seq1 if x in seq2] unique_to_seq1 = [x for x in seq1 if x not in seq2] unique_to_seq2 = [x for x in seq2 if x not in seq1] return { \\"is_subset\\": is_subset, \\"is_proper_subset\\": is_proper_subset, \\"is_superset\\": is_superset, \\"is_proper_superset\\": is_proper_superset, \\"common_elements\\": common_elements, \\"unique_to_seq1\\": unique_to_seq1, \\"unique_to_seq2\\": unique_to_seq2 }"},{"question":"Objective Evaluate your understanding and application of scikit-learn\'s `MLPClassifier` for multi-class classification through implementation, training, and evaluation. Problem Statement You are provided with a multi-class dataset. Your task is to: 1. Preprocess the data by scaling. 2. Implement and train an `MLPClassifier` on the training data. 3. Evaluate the model\'s performance on the test data using accuracy, and also output the confusion matrix. Dataset A synthetic dataset with sample features and labels is provided: ``` python import numpy as np from sklearn.datasets import make_classification from sklearn.model_selection import train_test_split # Generate a random multi-class classification problem X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_classes=3, random_state=42) # Split the dataset into 70% training and 30% testing X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) ``` Requirements 1. **Data Preprocessing**: - Scale the features of the dataset using `StandardScaler` from scikit-learn. 2. **Model Implementation**: - Initialize an `MLPClassifier` with the following hyperparameters: - `hidden_layer_sizes=(100,)` - `activation=\'relu\'` - `solver=\'adam\'` - `max_iter=200` - `random_state=42` 3. **Model Training**: - Train the classifier on the scaled training data. 4. **Evaluation**: - Predict the labels for the test data. - Calculate and print the accuracy score. - Calculate and print the confusion matrix. Expected Output Format Your code should produce the accuracy score and the confusion matrix as shown below: ``` Accuracy: 0.85 Confusion Matrix: [[85 5 9] [ 7 78 15] [10 8 83]] ``` Constraints - You must use `MLPClassifier` from `sklearn.neural_network`. - The dataset and the split provided should not be changed. Performance Requirement - Your solution should achieve at least 80% accuracy on the test set. Function Signature ``` python def evaluate_mlp_classifier(X_train: np.ndarray, X_test: np.ndarray, y_train: np.ndarray, y_test: np.ndarray): # Your implementation here pass ``` Note: - Make sure to scale your data. - Use the `confusion_matrix` and `accuracy_score` from `sklearn.metrics`.","solution":"import numpy as np from sklearn.neural_network import MLPClassifier from sklearn.preprocessing import StandardScaler from sklearn.metrics import accuracy_score, confusion_matrix def evaluate_mlp_classifier(X_train: np.ndarray, X_test: np.ndarray, y_train: np.ndarray, y_test: np.ndarray): # Scale the features scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) # Initialize the MLPClassifier clf = MLPClassifier(hidden_layer_sizes=(100,), activation=\'relu\', solver=\'adam\', max_iter=200, random_state=42) # Train the model clf.fit(X_train_scaled, y_train) # Make predictions y_pred = clf.predict(X_test_scaled) # Calculate accuracy accuracy = accuracy_score(y_test, y_pred) print(f\\"Accuracy: {accuracy}\\") # Calculate confusion matrix cm = confusion_matrix(y_test, y_pred) print(\\"Confusion Matrix:\\") print(cm) return accuracy, cm"},{"question":"**Objective:** To assess your understanding of Seaborn\'s `histplot` function and its applications in histogram plotting, customization, and handling different types of data (both univariate and bivariate). **Question:** You are given the Penguins dataset which can be loaded using the `seaborn.load_dataset(\'penguins\')` function. Your task is to implement a function `create_custom_histograms` that generates multiple types of histograms from this dataset based on the conditions described below: ```python import seaborn as sns import matplotlib.pyplot as plt def create_custom_histograms(): Generates various histograms from the Penguins dataset using seaborn\'s histplot function. The function should: 1. Generate a basic histogram of \'bill_length_mm\'. 2. Generate a KDE smoothed histogram of \'body_mass_g\'. 3. Generate a stacked histogram of \'flipper_length_mm\' segmented by \'species\'. 4. Create a histogram of \'bill_depth_mm\' with the bins defined in log scale. 5. Create a bivariate histogram of \'bill_depth_mm\' vs \'body_mass_g\' with hue based on \'species\'. 6. Add a color bar to the bivariate histogram. All plots should be displayed using matplotlib\'s `plt.show()`. Returns: None penguins = sns.load_dataset(\\"penguins\\") # 1. Basic histogram of \'bill_length_mm\' sns.histplot(data=penguins, x=\\"bill_length_mm\\") plt.title(\\"Basic Histogram of Bill Length\\") plt.show() # 2. KDE smoothed histogram of \'body_mass_g\' sns.histplot(data=penguins, x=\\"body_mass_g\\", kde=True) plt.title(\\"KDE Smoothed Histogram of Body Mass\\") plt.show() # 3. Stacked histogram of \'flipper_length_mm\' segmented by \'species\' sns.histplot(data=penguins, x=\\"flipper_length_mm\\", hue=\\"species\\", multiple=\\"stack\\") plt.title(\\"Stacked Histogram of Flipper Length by Species\\") plt.show() # 4. Histogram of \'bill_depth_mm\' with bins in log scale sns.histplot(penguins, x=\\"bill_depth_mm\\", log_scale=True) plt.title(\\"Histogram of Bill Depth with Log Scale Bins\\") plt.show() # 5. Bivariate histogram of \'bill_depth_mm\' vs \'body_mass_g\' with hue based on \'species\' sns.histplot(penguins, x=\\"bill_depth_mm\\", y=\\"body_mass_g\\", hue=\\"species\\") plt.title(\\"Bivariate Histogram of Bill Depth vs Body Mass\\") plt.show() # 6. Bivariate histogram with a color bar sns.histplot( penguins, x=\\"bill_depth_mm\\", y=\\"body_mass_g\\", cbar=True, hue=\\"species\\" ) plt.title(\\"Bivariate Histogram with Color Bar\\") plt.show() # You can test your function by running create_custom_histograms() ``` **Constraints:** 1. Handle any missing data in the dataset appropriately. 2. Ensure the visual clarity of each plot (e.g., appropriate titles, axis labels). **Expected Output:** Your function should produce 6 different plots as specified, with proper customization and added features based on the instructions. **Performance Requirements:** The function should run efficiently, even if the dataset is relatively large. However, for the scope of this task, assume the dataset size as provided by `seaborn.load_dataset(\'penguins\')`.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_custom_histograms(): Generates various histograms from the Penguins dataset using seaborn\'s histplot function. The function should: 1. Generate a basic histogram of \'bill_length_mm\'. 2. Generate a KDE smoothed histogram of \'body_mass_g\'. 3. Generate a stacked histogram of \'flipper_length_mm\' segmented by \'species\'. 4. Create a histogram of \'bill_depth_mm\' with the bins defined in log scale. 5. Create a bivariate histogram of \'bill_depth_mm\' vs \'body_mass_g\' with hue based on \'species\'. 6. Add a color bar to the bivariate histogram. All plots should be displayed using matplotlib\'s `plt.show()`. Returns: None penguins = sns.load_dataset(\\"penguins\\") # Handle missing data penguins = penguins.dropna() # 1. Basic histogram of \'bill_length_mm\' sns.histplot(data=penguins, x=\\"bill_length_mm\\") plt.title(\\"Basic Histogram of Bill Length\\") plt.xlabel(\\"Bill Length (mm)\\") plt.ylabel(\\"Frequency\\") plt.show() # 2. KDE smoothed histogram of \'body_mass_g\' sns.histplot(data=penguins, x=\\"body_mass_g\\", kde=True) plt.title(\\"KDE Smoothed Histogram of Body Mass\\") plt.xlabel(\\"Body Mass (g)\\") plt.ylabel(\\"Density\\") plt.show() # 3. Stacked histogram of \'flipper_length_mm\' segmented by \'species\' sns.histplot(data=penguins, x=\\"flipper_length_mm\\", hue=\\"species\\", multiple=\\"stack\\") plt.title(\\"Stacked Histogram of Flipper Length by Species\\") plt.xlabel(\\"Flipper Length (mm)\\") plt.ylabel(\\"Frequency\\") plt.show() # 4. Histogram of \'bill_depth_mm\' with bins in log scale sns.histplot(penguins, x=\\"bill_depth_mm\\", log_scale=True) plt.title(\\"Histogram of Bill Depth with Log Scale Bins\\") plt.xlabel(\\"Bill Depth (mm)\\") plt.ylabel(\\"Frequency\\") plt.show() # 5. Bivariate histogram of \'bill_depth_mm\' vs \'body_mass_g\' with hue based on \'species\' sns.histplot(penguins, x=\\"bill_depth_mm\\", y=\\"body_mass_g\\", hue=\\"species\\") plt.title(\\"Bivariate Histogram of Bill Depth vs Body Mass\\") plt.xlabel(\\"Bill Depth (mm)\\") plt.ylabel(\\"Body Mass (g)\\") plt.show() # 6. Bivariate histogram with a color bar sns.histplot( penguins, x=\\"bill_depth_mm\\", y=\\"body_mass_g\\", cbar=True, hue=\\"species\\" ) plt.title(\\"Bivariate Histogram with Color Bar\\") plt.xlabel(\\"Bill Depth (mm)\\") plt.ylabel(\\"Body Mass (g)\\") plt.show()"},{"question":"# Custom Type Inheritance and Management in Python In this assessment, you are required to implement custom types in Python that mimic some of the capabilities described in the `PyTypeObject` documentation. The task will assess your ability to create a complex type hierarchy, utilize type checking, inheritance, and dynamic type creation within Python. Task: 1. **Custom Type Creation:** - Create a base class called `CustomType` with a class attribute `type_name` and a method `describe` that returns a string describing the type with its name. 2. **Inheritance and Type Checking:** - Implement a subclass called `DerivedType` that inherits from `CustomType` and overrides the `describe` method to include information indicating that it is a derived type. - Implement another class `AnotherType` which does not inherit from `CustomType`. 3. **Dynamic Type Checking:** - Write a function `is_custom_type` that accepts an object and returns `True` if the object is an instance of `CustomType` or its subclasses, otherwise returns `False`. - Write another function `get_type_name` that returns the `type_name` attribute of an object if it is an instance of `CustomType` or its subclasses, otherwise raises a `TypeError`. Input & Output Requirements: - The `is_custom_type` function should accept a single object as input and return a boolean. - The `get_type_name` function should accept a single object as input and return a string if the object is of type `CustomType` or its subclasses, otherwise raises a `TypeError`. Constraints: - Do not use external libraries; only Python\'s standard libraries and built-in functions are allowed. Example: ```python class CustomType: type_name = \\"CustomType\\" def describe(self): return f\\"This is a {self.type_name}\\" class DerivedType(CustomType): type_name = \\"DerivedType\\" def describe(self): return f\\"This is a derived type: {self.type_name}\\" class AnotherType: pass def is_custom_type(obj): return isinstance(obj, CustomType) def get_type_name(obj): if is_custom_type(obj): return obj.type_name else: raise TypeError(\\"Object is not of type CustomType or its subclasses\\") # Example Usage: base = CustomType() derived = DerivedType() another = AnotherType() assert is_custom_type(base) == True assert is_custom_type(derived) == True assert is_custom_type(another) == False assert get_type_name(base) == \\"CustomType\\" assert get_type_name(derived) == \\"DerivedType\\" try: get_type_name(another) except TypeError as e: print(e) # \\"Object is not of type CustomType or its subclasses\\" ``` # Evaluation: - Correct implementation of class hierarchy and method overrides. - Proper functioning of type checking and retrieval functions. - Good use of Python\'s built-in functions and standard practices for types and inheritance.","solution":"class CustomType: type_name = \\"CustomType\\" def describe(self): return f\\"This is a {self.type_name}\\" class DerivedType(CustomType): type_name = \\"DerivedType\\" def describe(self): return f\\"This is a derived type: {self.type_name}\\" class AnotherType: pass def is_custom_type(obj): return isinstance(obj, CustomType) def get_type_name(obj): if is_custom_type(obj): return obj.type_name else: raise TypeError(\\"Object is not of type CustomType or its subclasses\\") # Example Usage: base = CustomType() derived = DerivedType() another = AnotherType() print(base.describe()) # \\"This is a CustomType\\" print(derived.describe()) # \\"This is a derived type: DerivedType\\" print(is_custom_type(base)) # True print(is_custom_type(derived)) # True print(is_custom_type(another)) # False print(get_type_name(base)) # \\"CustomType\\" print(get_type_name(derived)) # \\"DerivedType\\" try: print(get_type_name(another)) except TypeError as e: print(e) # \\"Object is not of type CustomType or its subclasses\\""},{"question":"Objective Implement a function that leverages Incremental PCA to reduce the dimensionality of a large dataset that does not fit into memory all at once. This will test your understanding of both the core PCA concept and the practical application of Incremental PCA for handling large datasets in a memory-efficient manner. Problem Statement You are given a large dataset `X` stored in a memory-mapped file format. Your task is to implement a function that performs dimensionality reduction using Incremental PCA. The function should reduce the dimensionality of the dataset to a specified number of components and return the transformed dataset. Function Signature ```python def incremental_pca_transform(data_path: str, n_components: int, batch_size: int) -> np.ndarray: Perform Incremental PCA on a large dataset stored in a memory-mapped file. Parameters: - data_path (str): Path to the memory-mapped file containing the dataset. - n_components (int): Number of principal components to keep. - batch_size (int): Size of the mini-batches to use when processing the dataset. Returns: - np.ndarray: Transformed dataset with reduced dimensionality. pass ``` Input - `data_path`: A string representing the path to the memory-mapped file (e.g., `.npy` file) containing the data matrix. - `n_components`: An integer specifying the number of principal components to retain. - `batch_size`: An integer specifying the size of the mini-batches used to process the dataset incrementally. Output - Returns a NumPy array representing the dataset transformed to the new space with reduced dimensions. Constraints - You must use `IncrementalPCA` from `sklearn.decomposition`. - The input dataset is too large to fit into memory all at once. - The dataset is stored in a memory-mapped `.npy` file format. Example ```python import numpy as np from sklearn.decomposition import IncrementalPCA def incremental_pca_transform(data_path: str, n_components: int, batch_size: int) -> np.ndarray: # Load the memory-mapped dataset X = np.load(data_path, mmap_mode=\'r\') # Initialize IncrementalPCA ipca = IncrementalPCA(n_components=n_components) # Fit the model incrementally on the chunks of data for start in range(0, X.shape[0], batch_size): end = start + batch_size ipca.partial_fit(X[start:end]) # Transform the dataset in chunks X_transformed = np.empty((X.shape[0], n_components)) for start in range(0, X.shape[0], batch_size): end = start + batch_size X_transformed[start:end] = ipca.transform(X[start:end]) return X_transformed # Example Usage data_path = \'path_to_memmap_file.npy\' n_components = 10 batch_size = 100 transformed_data = incremental_pca_transform(data_path, n_components, batch_size) print(transformed_data.shape) ``` Notes - Ensure your implementation works efficiently with large datasets. - Validate your final implementation with a sample dataset to ensure accuracy.","solution":"import numpy as np from sklearn.decomposition import IncrementalPCA def incremental_pca_transform(data_path: str, n_components: int, batch_size: int) -> np.ndarray: Perform Incremental PCA on a large dataset stored in a memory-mapped file. Parameters: - data_path (str): Path to the memory-mapped file containing the dataset. - n_components (int): Number of principal components to keep. - batch_size (int): Size of the mini-batches to use when processing the dataset. Returns: - np.ndarray: Transformed dataset with reduced dimensionality. # Load the memory-mapped dataset X = np.load(data_path, mmap_mode=\'r\') # Initialize IncrementalPCA ipca = IncrementalPCA(n_components=n_components) # Fit the model incrementally on the chunks of data for start in range(0, X.shape[0], batch_size): end = min(start + batch_size, X.shape[0]) ipca.partial_fit(X[start:end]) # Transform the dataset in chunks X_transformed = np.empty((X.shape[0], n_components)) for start in range(0, X.shape[0], batch_size): end = min(start + batch_size, X.shape[0]) X_transformed[start:end] = ipca.transform(X[start:end]) return X_transformed"},{"question":"Problem Statement You are asked to create a secure Python program to manage user credentials using the `crypt` module. The program must support two main functionalities: registering new users and verifying existing users. You need to use the strongest available hashing method on the platform. # Requirements 1. **Function to Register Users**: - Function Name: `register_user` - Input: - `username` (a string) - `password` (a string) - Output: None - Functionality: - Generate a hashed version of the password using the strongest available hashing method. - Store the username and hashed password in a dictionary. 2. **Function to Verify Users**: - Function Name: `verify_user` - Input: - `username` (a string) - `password` (a string) - Output: - `True` if the provided password matches the stored hashed password. - `False` otherwise. - Functionality: - Retrieve the stored hashed password for the given username from the dictionary. - Compare the provided password (after hashing it using the same salt) with the stored hashed password to verify the user. # Instructions 1. Implement both functions as described. Store user data (username and hashed password) in a global dictionary called `user_db`. 2. Use the `crypt.crypt` function for hashing passwords. 3. Ensure that the salt used for hashing during registration is randomly generated with the `crypt.mksalt()` function. 4. Ensure the comparison for password verification uses a safe method such as `hmac.compare_digest` to prevent timing attacks. # Example Usage ```python # Global storage for users user_db = {} def register_user(username, password): import crypt import hmac from hmac import compare_digest salt = crypt.mksalt(crypt.methods[0]) hashed_password = crypt.crypt(password, salt) user_db[username] = hashed_password def verify_user(username, password): import crypt import hmac from hmac import compare_digest if username not in user_db: return False hashed_password = user_db[username] return compare_digest(hashed_password, crypt.crypt(password, hashed_password)) # Example register_user(\'alice\', \'my_secure_password\') print(verify_user(\'alice\', \'my_secure_password\')) # Output: True print(verify_user(\'alice\', \'another_password\')) # Output: False ``` # Additional Notes - You may assume usernames are unique. - Ensure your implementation handles potential errors gracefully (e.g., verifying a non-existent user should return `False`).","solution":"# Global storage for users user_db = {} def register_user(username, password): import crypt from crypt import mksalt salt = mksalt(crypt.METHOD_SHA512) hashed_password = crypt.crypt(password, salt) user_db[username] = hashed_password def verify_user(username, password): import crypt import hmac from hmac import compare_digest if username not in user_db: return False hashed_password = user_db[username] return compare_digest(hashed_password, crypt.crypt(password, hashed_password)) # Example Usage register_user(\'alice\', \'my_secure_password\') print(verify_user(\'alice\', \'my_secure_password\')) # Output: True print(verify_user(\'alice\', \'another_password\')) # Output: False"},{"question":"# I/O Multiplexing with `select` Module You are required to create a Python program to monitor multiple file descriptors and react appropriately when they become ready for reading, writing, and handling exceptional conditions. Problem Statement Implement a function `monitor_file_descriptors(rlist, wlist, xlist, timeout)` that uses the `select.select()` function from the `select` module to monitor lists of file descriptors for readiness. This function should: 1. Take the following parameters: - `rlist`: A list of file descriptors to wait for being ready to read. - `wlist`: A list of file descriptors to wait for being ready to write. - `xlist`: A list of file descriptors to wait for exceptional conditions. - `timeout`: A float specifying the timeout period in seconds. If omitted or None, the function should block until at least one file descriptor is ready. A timeout of 0 indicates polling and the function shouldn’t block. 2. Return a triple of lists of file descriptors (rlist, wlist, xlist) that are ready. 3. Handle interrupted system calls by retrying the `select.select()` call with an adjusted timeout. Constraints - The `fileno()` method can be used to get file descriptor integers from objects. - Assume this function is running on a Unix system. - You may create test descriptors using `os.pipe()` for testing readiness. Example Usage ```python import os import sys import select # Create pipe descriptors rfd, wfd = os.pipe() # Write data to the pipe for demonstrating readiness os.write(wfd, b\\"Hello\\") # Function call to monitor file descriptors ready_to_read, ready_to_write, exceptional = monitor_file_descriptors([rfd], [wfd], [], 1.0) print(ready_to_read) # Should include rfd since there\'s data to read print(ready_to_write) # Should include wfd since it\'s available for writing print(exceptional) # Should be empty ``` Implementation ```python def monitor_file_descriptors(rlist, wlist, xlist, timeout): while True: try: readable, writable, exceptional = select.select(rlist, wlist, xlist, timeout) return readable, writable, exceptional except InterruptedError: continue # Retry on interruption ```","solution":"import select import os def monitor_file_descriptors(rlist, wlist, xlist, timeout=None): Monitors multiple file descriptors and returns the ones ready for reading, writing or have exceptional conditions. Parameters: rlist (list): A list of file descriptors to wait for being ready to read. wlist (list): A list of file descriptors to wait for being ready to write. xlist (list): A list of file descriptors to wait for exceptional conditions. timeout (float): The timeout period in seconds. If omitted or None, blocks until at least one file descriptor is ready. Returns: tuple: Three lists containing file descriptors ready for reading, writing, and exceptional conditions. while True: try: readable, writable, exceptional = select.select(rlist, wlist, xlist, timeout) return readable, writable, exceptional except InterruptedError: continue # Retry on interruption"},{"question":"**Question:** **Objective:** To test your understanding of high-level file and directory operations in Python using the `shutil` module. **Problem Statement:** You are tasked with creating a utility function to backup and organize a project\'s various components into separate archive files. The project\'s directory structure is as follows: - `src/`: Contains all Python source files. - `docs/`: Contains documentation files. - `data/`: Contains dataset files. You need to implement a function `backup_project(project_root, backup_dir)` that: 1. Takes a project root directory (`project_root`) and a backup directory (`backup_dir`) as inputs. 2. Creates separate ZIP archives within `backup_dir` for the `src`, `docs`, and `data` directories present within `project_root`. 3. Returns a dictionary with the paths of the created ZIP archives. **Function Signature:** ```python def backup_project(project_root: str, backup_dir: str) -> dict: pass ``` **Constraints:** - Assume `project_root` contains the directories `src`, `docs`, and `data`. - The `backup_dir` should also be an existing writable directory. - Handle any exceptions that may occur during the archiving process appropriately. - The returned dictionary should have keys \'src\', \'docs\', and \'data\' with their corresponding archive paths as values. - Use the `shutil` module for your implementation. **Example:** ```python project_root = \\"/path/to/project\\" backup_dir = \\"/path/to/backup\\" result = backup_project(project_root, backup_dir) # This should create the following archives and return their paths: # /path/to/backup/src.zip # /path/to/backup/docs.zip # /path/to/backup/data.zip # result should be: # { # \\"src\\": \\"/path/to/backup/src.zip\\", # \\"docs\\": \\"/path/to/backup/docs.zip\\", # \\"data\\": \\"/path/to/backup/data.zip\\" # } ``` **Notes:** - Use `shutil`\'s `make_archive` function to create the ZIP archives. - Ensure the function is robust and handles errors gracefully.","solution":"import os import shutil def backup_project(project_root: str, backup_dir: str) -> dict: Creates ZIP archives for src, docs, and data directories within the project_root and stores them in backup_dir. Parameters: project_root (str): The root directory of the project. backup_dir (str): The directory where the backups will be stored. Returns: dict: A dictionary with keys \'src\', \'docs\', \'data\' and paths to their corresponding archives as values. if not os.path.exists(project_root): raise ValueError(\\"The project root directory does not exist.\\") if not os.path.exists(backup_dir): raise ValueError(\\"The backup directory does not exist.\\") backup_paths = {} for component in [\'src\', \'docs\', \'data\']: component_dir = os.path.join(project_root, component) if os.path.exists(component_dir) and os.path.isdir(component_dir): archive_name = os.path.join(backup_dir, component) shutil.make_archive(archive_name, \'zip\', component_dir) backup_paths[component] = f\\"{archive_name}.zip\\" else: raise ValueError(f\\"The {component} directory does not exist in the project root.\\") return backup_paths"},{"question":"<|Analysis Begin|> The provided documentation demonstrates various features of the seaborn library, particularly focusing on the `catplot` function. One can observe the following key points highlighted: 1. Initialization and setting up seaborn\'s theme with `sns.set_theme(style=\\"whitegrid\\")`. 2. Loading a dataset using `sns.load_dataset()`. 3. Creating different types of categorical plots using `sns.catplot()`, including: - Jittered strip plot - Box plot - Boxen plot - Violin plot - Bar plot 4. Adding custom styling and parameters to the plots, such as color, size, and other visual aesthetics. 5. Creating subplots by assigning variables to `col` or `row` and controlling the figure size with `height` and `aspect`. 6. Layering different representations within a single subplot. 7. Tweaking the presentation of the plot returned by methods of the `FacetGrid` class. Based on these key points, we can create a question that assesses students\' understanding of both basic and advanced aspects of seaborn plotting. This includes using multiple plot types, adjusting plot aesthetics, creating subplots, and fine-tuning the presentation. <|Analysis End|> <|Question Begin|> # Coding Assessment Question Objective: Write a Python code that demonstrates your understanding of seaborn\'s `catplot` function and various customizations that can be applied to it. You will use the Titanic dataset available in seaborn to visualize and analyze some data points. Instructions: 1. Load the Titanic dataset from seaborn and display the first few rows of the dataset to understand its structure. 2. Create a `catplot` that uses boxen plot to visualize the distribution of passengers\' age across different classes (e.g., First, Second, Third). - This plot should also differentiate between male and female passengers using the `hue` parameter. 3. Create a subplot that shows the survival rate of passengers with respect to their class in the Titanic dataset. - Use the gender of passengers to split the subplots into different columns. - The plot should be a bar plot with a height of 5 and aspect ratio of 0.7. 4. Combine two plots: - Create a violin plot showing the distribution of passengers\' age across different classes with a color of `.9` and no inner lines. - Overlay a strip plot on top of this violin plot showing the same data with a jittered effect. 5. Use the methods on the `FacetGrid` returned by the `catplot` to: - Set axis labels as \\"Class\\" and \\"Survival Rate\\". - Set x-tick labels as \\"1st Class\\", \\"2nd Class\\", and \\"3rd Class\\". - Ensure the titles of each subplot clarify the context. - Set the y-axis limits between 0 and 1. - Remove the left spine of the plot. Expected Input: You don\'t need to provide any input from the user as the dataset is preloaded from seaborn. Expected Output: The output should be three visualizations showing the required plots with the specified customization. Constraints: - Utilize seaborn for all plotting. - Follow the instructions closely to ensure all aspects of the plots are covered. - Your code should be organized and well-commented. Example Code: ```python import seaborn as sns import matplotlib.pyplot as plt # 1. Load the Titanic dataset and display the first few rows df = sns.load_dataset(\\"titanic\\") print(df.head()) # 2. Create a boxen plot to visualize the distribution of age across classes with hue as sex sns.catplot(data=df, x=\\"class\\", y=\\"age\\", hue=\\"sex\\", kind=\\"boxen\\") plt.show() # 3. Create a subplot of survival rate by class, with gender as columns g = sns.catplot(data=df, x=\\"class\\", y=\\"survived\\", col=\\"sex\\", kind=\\"bar\\", height=5, aspect=0.7) g.set_axis_labels(\\"Class\\", \\"Survival Rate\\") g.set_xticklabels([\\"1st Class\\", \\"2nd Class\\", \\"3rd Class\\"]) g.set_titles(\\"{col_name}\\") g.set(ylim=(0, 1)) g.despine(left=True) plt.show() # 4. Combine a violin plot with a strip plot sns.catplot(data=df, x=\\"class\\", y=\\"age\\", kind=\\"violin\\", color=\\".9\\", inner=None) sns.stripplot(data=df, x=\\"class\\", y=\\"age\\", jitter=True, size=3) plt.show() ```","solution":"import seaborn as sns import matplotlib.pyplot as plt # Load the Titanic dataset and display the first few rows df = sns.load_dataset(\\"titanic\\") print(df.head()) # Create a boxen plot to visualize the distribution of age across classes with hue as sex sns.catplot(data=df, x=\\"class\\", y=\\"age\\", hue=\\"sex\\", kind=\\"boxen\\", height=6, aspect=1.5) plt.title(\\"Distribution of Age across Classes by Sex\\") plt.show() # Create a subplot of survival rate by class, with gender as columns g = sns.catplot(data=df, x=\\"class\\", y=\\"survived\\", col=\\"sex\\", kind=\\"bar\\", height=5, aspect=0.7, palette=\\"viridis\\") g.set_axis_labels(\\"Class\\", \\"Survival Rate\\") g.set_xticklabels([\\"1st Class\\", \\"2nd Class\\", \\"3rd Class\\"]) g.set_titles(\\"{col_name}\\") g.set(ylim=(0, 1)) g.despine(left=True) plt.suptitle(\\"Survival Rate by Class and Sex\\", y=1.05) plt.show() # Combine a violin plot with a strip plot sns.catplot(data=df, x=\\"class\\", y=\\"age\\", kind=\\"violin\\", color=\\".9\\", inner=None, height=6, aspect=1.5) sns.stripplot(data=df, x=\\"class\\", y=\\"age\\", jitter=True, size=3, color=\\"blue\\") plt.title(\\"Combined Violin and Strip Plot of Age across Classes\\") plt.show()"},{"question":"Objective To assess your understanding of Python socket programming, regular expressions, and file handling, you are tasked with implementing a server-client application where the server processes text data and the client sends requests to the server. Problem Statement 1. **Server Implementation** Implement a Python server that does the following: - Listens on a specified port. - Accepts connections from clients. - Once a connection is accepted, the server reads a text sent by the client. - The server then processes the text data to extract all email addresses using regular expressions. - The server responds to the client with the extracted email addresses in a comma-separated string format. 2. **Client Implementation** Implement a Python client that does the following: - Connects to the server on the specified port. - Reads a text file provided by the user (file path provided as input). - Sends the content of the text file to the server. - Receives the response from the server and prints the list of email addresses to the console. Input and Output Formats - **Server Input:** The server should listen on a port specified by an argument passed to it when the server script is started. - **Client Input:** The client should take a file path as input from the user. - **Client Output:** A list of email addresses extracted from the file content, printed to the console. Constraints - You must handle exceptions and errors gracefully (e.g., failed connections, file not found, etc.). - Use appropriate regular expressions to match email addresses. - Ensure that the server can handle multiple client connections concurrently. Performance Requirements - The server should be capable of handling multiple client connections simultaneously without significant delay. - Error handling must address both client-side and server-side failures. Example 1. **Server Execution:** ```bash python server.py 12345 ``` 2. **Client Execution:** ```bash python client.py /path/to/text_file.txt ``` 3. **Example Interaction:** Suppose the text file contains: ``` Hello, This is a sample text with emails. Contact us at example@example.com or at test.email@example.co.uk. Thank you. ``` The client sends the file content to the server, and the server responds with: ``` example@example.com,test.email@example.co.uk ``` The client then prints: ``` [\'example@example.com\', \'test.email@example.co.uk\'] ``` Good luck!","solution":"import socket import threading import re # Server Implementation def extract_emails(text): # Regex pattern to match email addresses email_pattern = r\'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+.[a-zA-Z]{2,}\' emails = re.findall(email_pattern, text) return \',\'.join(emails) def handle_client(client_socket): try: # Receive data from client data = client_socket.recv(4096).decode() if not data: return # Process data to extract emails email_addresses = extract_emails(data) # Send response back to client client_socket.send(email_addresses.encode()) except Exception as e: print(f\\"Error handling client: {e}\\") finally: client_socket.close() def start_server(port): server = socket.socket(socket.AF_INET, socket.SOCK_STREAM) server.bind((\'0.0.0.0\', port)) server.listen(5) print(f\\"Server listening on port {port}\\") while True: client_socket, addr = server.accept() print(f\\"Accepted connection from {addr}\\") client_handler = threading.Thread(target=handle_client, args=(client_socket,)) client_handler.start() # Client Implementation def read_file(filepath): try: with open(filepath, \'r\') as file: return file.read() except FileNotFoundError: print(f\\"File not found: {filepath}\\") return None def start_client(server_ip, port, filepath): data = read_file(filepath) if data is None: return client = socket.socket(socket.AF_INET, socket.SOCK_STREAM) try: client.connect((server_ip, port)) client.send(data.encode()) response = client.recv(4096).decode() print(response.split(\',\')) except ConnectionError as e: print(f\\"Connection error: {e}\\") finally: client.close()"},{"question":"Asynchronous Execution with PyTorch Futures Objective: To assess the student\'s understanding of asynchronous programming in PyTorch using the `torch.futures.Future` class and related utility functions. Problem Statement: You are required to implement an asynchronous data processing pipeline using PyTorch futures. The pipeline will include three stages: 1. **Data Loading**: Load data asynchronously. 2. **Data Processing**: Process the data asynchronously. 3. **Result Aggregation**: Gather all processed results asynchronously and return the final aggregated outcome. Instructions: 1. Implement a function `load_data_async` that simulates loading data asynchronously. This function should return a `torch.futures.Future` object. 2. Implement a function `process_data_async` that simulates data processing asynchronously. This function should take a data item and return a `torch.futures.Future` object. 3. Implement a function `aggregate_results` that takes a list of `Future` objects, waits for all of them to complete, and returns the aggregated result. Function Signatures: ```python import torch from torch.futures import Future, collect_all def load_data_async(data: list) -> List[Future]: Simulates loading data asynchronously. Args: data (list): List of data items to be loaded. Returns: List[Future]: A list of Future objects representing the loaded data items. pass def process_data_async(data_item) -> Future: Simulates processing data asynchronously. Args: data_item: A single data item. Returns: Future: A Future object representing the processed data item. pass def aggregate_results(futures: List[Future]) -> Any: Aggregates results from a list of Future objects. Args: futures (List[Future]): List of Future objects to be aggregated. Returns: Any: The aggregated result of all data items. pass ``` Expected Input and Output: - `load_data_async(data)` takes a list of data items and returns a list of `Future` objects each representing a loaded data item. - `process_data_async(data_item)` takes a single data item and returns a `Future` object representing the processed data item. - `aggregate_results(futures)` takes a list of `Future` objects, waits for all of them to complete, and returns the aggregated result. Constraints: - You can assume that the data loading and processing tasks are CPU-bound and can be simulated using sleep functions. - You should use the `torch.futures.Future` class and related functions to manage the asynchronous tasks. Example: ```python # Example usage of the implemented functions data = [1, 2, 3, 4, 5] # Load data asynchronously futures = load_data_async(data) # Process data asynchronously processed_futures = [process_data_async(future.result()) for future in futures] # Aggregate results final_result = aggregate_results(processed_futures) print(final_result) # Expected output: Aggregated processed data ``` Notes: - You may want to simulate asynchronous behavior using sleep functions to mimic data loading and processing delays. - The aggregated result can be in any form (e.g., sum of processed data, concatenated strings, etc.), depending on the simulated processing.","solution":"import torch from torch.futures import Future, collect_all import time from typing import List, Any def load_data_async(data: list) -> List[Future]: Simulates loading data asynchronously by returning Future objects. Args: data (list): List of data items to be loaded. Returns: List[Future]: A list of Future objects representing the loaded data items. def load_single_item(item): time.sleep(0.1) # Simulate IO-bound operation return item futures = [] for item in data: future = Future() future.set_result(load_single_item(item)) futures.append(future) return futures def process_data_async(data_item) -> Future: Simulates processing data asynchronously by returning a Future object. Args: data_item: A single data item. Returns: Future: A Future object representing the processed data item. def process_item(item): time.sleep(0.2) # Simulate CPU-bound operation return item * 2 # Simulated processing future = Future() future.set_result(process_item(data_item)) return future def aggregate_results(futures: List[Future]) -> Any: Aggregates results from a list of Future objects. Args: futures (List[Future]): List of Future objects to be aggregated. Returns: Any: The aggregated result of all data items. results = [future.value() for future in futures] return sum(results) # Sum it for simplicity # Example usage # data = [1, 2, 3, 4, 5] # Load data asynchronously # futures = load_data_async(data) # Process data asynchronously # processed_futures = [process_data_async(future.value()) for future in futures] # Aggregate results # final_result = aggregate_results(processed_futures) # print(final_result) # Expected output: Aggregated processed data"},{"question":"**Objective**: Demonstrate your understanding of creating and customizing plots using Seaborn, specifically focusing on the `swarmplot` function. **Problem Statement**: Given a dataset `tips` from Seaborn, write a function `custom_swarmplot` that performs the following tasks: 1. Loads the `tips` dataset. 2. Creates a swarm plot where: - The x-axis represents `total_bill`. - The y-axis represents `day`. - Color the points by `sex`. 3. Customize the plot with the following specifications: - Use a distinct marker, such as `\\"x\\"`, for the points. - Set the marker size to 3. - Apply a \\"dark\\" theme for the entire plot. 4. Save the plot as a PNG file named `custom_swarmplot.png`. # Function Signature ```python def custom_swarmplot(): pass ``` # Constraints and Notes: - Use built-in Seaborn functions to manipulate the dataset and create plots. - Ensure that the saved image is 800x600 pixels in size. - Validate that the plot has no overlapping points by ensuring the marker size does not cause significant overlaps. - Remember to properly handle any warnings related to density or plot settings. # Example Output: The function does not return any values but generates a PNG file named `custom_swarmplot.png` in the current working directory. # Performance Requirements: - The function should be efficient and execute within a reasonable time frame (a few seconds). # Additional Information: - You can refer to the Seaborn documentation provided for additional context and examples. - Ensure your code follows PEP 8 style guidelines for better readability.","solution":"import seaborn as sns import matplotlib.pyplot as plt def custom_swarmplot(): # Load the tips dataset tips = sns.load_dataset(\\"tips\\") # Set the theme to \'dark\' sns.set_theme(style=\\"dark\\") # Create the swarm plot plt.figure(figsize=(8, 6)) sns.swarmplot( x=\'total_bill\', y=\'day\', hue=\'sex\', data=tips, marker=\'x\', size=3 ) # Save the plot as a PNG file with specified dimensions plt.savefig(\\"custom_swarmplot.png\\", dpi=100) # Close the plot to free up memory plt.close()"},{"question":"**Objective**: Demonstrate your understanding of the `seaborn` library\'s `objects` module by creating a customized scatter plot using the `so.Dot` mark and various customization techniques. # Problem Statement You are provided with the `tips` dataset from the `seaborn` library. You must create a scatter plot of the dataset that meets the following criteria: 1. **Data**: - X-axis: `total_bill` - Y-axis: `tip` 2. **Plot Customizations**: - Use the `so.Dot()` mark to create the scatter plot. - Add a white edge color to the dots to distinguish them. - Apply jittering with a factor of 0.1 to avoid overplotting. - Facet the plot by the `time` column (Dinner or Lunch). 3. **Advanced Customizations**: - Map the `size` of the dots to the `size` column in the dataset. - Map the `color` of the dots to the `sex` column in the dataset. - Add error bars that show the standard deviation of `tip` for each `total_bill` bin. # Function Signature ```python def create_custom_scatterplot(): # Your code here pass ``` # Expected Output Your function will not return any value, but it should display a customized scatter plot based on the criteria provided. # Constraints - Use only the `seaborn` and `matplotlib` libraries for visualization. # Additional Notes - Ensure that all visual elements (dots, edges, jitter, facets, size, color, and error bars) are clearly visible and distinguishable. - The plot should be self-explanatory with appropriate axis labels and a legend explaining the colors and sizes used. # Example Usage ```python create_custom_scatterplot() ``` This should generate and display the scatter plot with all the specified customizations.","solution":"import seaborn as sns import matplotlib.pyplot as plt import seaborn.objects as so def create_custom_scatterplot(): # Load the dataset tips = sns.load_dataset(\\"tips\\") # Create the scatter plot p = ( so.Plot(tips, x=\\"total_bill\\", y=\\"tip\\", color=\\"sex\\", size=\\"size\\") .add(so.Dots(edgecolor=\\"white\\", jitter=0.1)) .facet(col=\\"time\\") .layout(size=(8, 6)) ) # Add error bars for standard deviation p.add(so.ErrorBars(value=\\"tip\\", agg=\\"std\\")) # Show the plot p.show()"},{"question":"# Question: Analyze Stack Frames and Execution Context You are provided with multiple functions from the Python C-API that allow retrieving information about the current execution context, including built-ins, locals, globals, and stack frames. Using these functions, write a Python function that provides a detailed report of the current execution context when called. Requirements: 1. The function should be named `analyze_execution_context`. 2. It should not take any parameters. 3. It should return a dictionary with the following keys and corresponding values: - `builtins`: A dictionary of builtins in the current execution frame. - `locals`: A dictionary of local variables in the current execution frame. - `globals`: A dictionary of global variables in the current execution frame. - `frames`: A list of dictionaries, each containing: - `function_name`: The name of the function being executed in the frame. - `line_number`: The current line number being executed in the frame. - `code`: A readable representation of the code object being executed in the frame (e.g., the name of the script or function). 4. Ensure that if any of the values cannot be retrieved, your function handles it gracefully by providing a descriptive placeholder (e.g., `\\"N/A\\"`). Constraints: - You are expected to use the functions detailed in the provided documentation. - Write your solution in Python, and assume that the necessary C-API functions are available via a suitable interface in your environment. Example Output: ```python { \\"builtins\\": {...}, # Contents of the builtins dictionary \\"locals\\": {...}, # Contents of the locals dictionary \\"globals\\": {...}, # Contents of the globals dictionary \\"frames\\": [ { \\"function_name\\": \\"example_function\\", \\"line_number\\": 42, \\"code\\": \\"example_code_description\\" }, ... ] } ``` This task will evaluate your ability to: - Utilize introspection to examine the execution context. - Handle and report information from multiple levels of the call stack. - Grasp advanced Python concepts related to execution frames and debugging.","solution":"import sys def analyze_execution_context(): Analyzes and provides a report of the current execution context. Returns a dictionary containing the current builtins, locals, globals, and stack frames. try: # Get the current frame current_frame = sys._getframe() # Builtins in the current frame builtins = current_frame.f_builtins if current_frame.f_builtins else \\"N/A\\" # Local variables in the current frame locals_dict = current_frame.f_locals if current_frame.f_locals else \\"N/A\\" # Global variables in the current frame globals_dict = current_frame.f_globals if current_frame.f_globals else \\"N/A\\" # List to store execution frames info frames_info = [] while current_frame: frames_info.append({ \\"function_name\\": current_frame.f_code.co_name if current_frame.f_code else \\"N/A\\", \\"line_number\\": current_frame.f_lineno if current_frame.f_lineno else \\"N/A\\", \\"code\\": current_frame.f_code.co_filename if current_frame.f_code else \\"N/A\\" }) current_frame = current_frame.f_back # Create the resulting context dictionary context = { \\"builtins\\": builtins, \\"locals\\": locals_dict, \\"globals\\": globals_dict, \\"frames\\": frames_info } except Exception as e: # If there is any exception, handle it gracefully context = { \\"builtins\\": \\"N/A\\", \\"locals\\": \\"N/A\\", \\"globals\\": \\"N/A\\", \\"frames\\": str(e) } return context"},{"question":"# Context Manager Implementation and Usage Description You are required to design a text processing system where you want to capture the output of print statements and ensure proper resource management of multiple files read simultaneously. Implement a solution that demonstrates your understanding of context managers, decorators, and the `ExitStack` class from the `contextlib` module. Tasks 1. **Implement a Context Manager Decorator**: Create a context manager decorator named `time_logger` that logs the time taken to execute a function. Use `ContextDecorator` for this functionality. ```python from contextlib import ContextDecorator import time class time_logger(ContextDecorator): def __init__(self, log_message=\\"Elapsed time\\"): self.log_message = log_message def __enter__(self): self.start_time = time.monotonic() return self def __exit__(self, exc_type, exc_value, traceback): elapsed_time = time.monotonic() - self.start_time print(f\\"{self.log_message}: {elapsed_time:.2f} seconds\\") return False # Example usage @time_logger(log_message=\\"Processing time\\") def example_function(): for _ in range(1000000): pass example_function() ``` 2. **Manage Multiple File Reads Efficiently**: Write a function `process_multiple_files(file_paths)` that reads from multiple files concurrently. Use `ExitStack` to ensure that all files are properly closed after their contents are processed. The function should print the content of each file to a string buffer instead of standard output. Use `redirect_stdout` to capture the print output. ```python from contextlib import ExitStack, redirect_stdout from io import StringIO def process_multiple_files(file_paths): buffer = StringIO() with ExitStack() as stack, redirect_stdout(buffer): files = [stack.enter_context(open(file_path, \'r\')) for file_path in file_paths] for file in files: print(file.read()) return buffer.getvalue() # Example usage file_contents = process_multiple_files([\'file1.txt\', \'file2.txt\']) print(file_contents) ``` Constraints - The `time_logger` decorator should log the time taken for function execution in seconds with an appropriate log message. - The `process_multiple_files` function should ensure that all files are closed properly even if an exception occurs while processing the files. - Files are text files with reasonable sizes (up to a few MBs). Input and Output - **Input**: A list of file paths to be processed. - **Output**: A single string containing the concatenated content of all the files. Write your solution in Python and ensure that you utilize the `contextlib` module features effectively as described.","solution":"from contextlib import ContextDecorator, ExitStack, redirect_stdout import time from io import StringIO class time_logger(ContextDecorator): def __init__(self, log_message=\\"Elapsed time\\"): self.log_message = log_message def __enter__(self): self.start_time = time.monotonic() return self def __exit__(self, exc_type, exc_value, traceback): elapsed_time = time.monotonic() - self.start_time print(f\\"{self.log_message}: {elapsed_time:.2f} seconds\\") return False @time_logger(log_message=\\"Processing time\\") def example_function(): for _ in range(1000000): pass def process_multiple_files(file_paths): buffer = StringIO() with ExitStack() as stack, redirect_stdout(buffer): files = [stack.enter_context(open(file_path, \'r\')) for file_path in file_paths] for file in files: print(file.read()) return buffer.getvalue() # Example usage example_function() # Note: The following line of code is commented out because it requires actual files to be present. # file_contents = process_multiple_files([\'file1.txt\', \'file2.txt\']) # print(file_contents)"},{"question":"Coding Assessment Question # Objective: Implement a custom descriptor in Python and demonstrate its use by integrating it into a class. You will need to create a descriptor for validating that an attribute is an integer and lies within a specified range. # Task: 1. Implement a `RangeValidator` descriptor with: - An `__init__` method that takes `name`, `min_value`, and `max_value` as parameters. - A `__get__` method for retrieving the attribute value. - A `__set__` method for setting the attribute value while ensuring it lies within the specified range (inclusive). If the value is not an integer or does not lie within the range, raise a `ValueError`. 2. Use the `RangeValidator` descriptor in a class `SampleClass` with an attribute `integer_attribute` that is validated to be within the range 10 to 100. # Input and Output: Your implementation should not require any input. The provided test cases will verify your implementation. # Example: ```python # Your implementation here # Test the implementation sample = SampleClass() sample.integer_attribute = 50 # Valid, within range print(sample.integer_attribute) # Output: 50 sample.integer_attribute = 5 # Raises ValueError: Value out of range sample.integer_attribute = \\"hello\\" # Raises ValueError: Value must be an integer ``` # Constraints: - Use the descriptor protocol methods `__get__` and `__set__`. # Performance Requirements: The solution should efficiently handle the attribute validation.","solution":"class RangeValidator: def __init__(self, name, min_value, max_value): self.name = name self.min_value = min_value self.max_value = max_value def __get__(self, instance, owner): if instance is None: return self return instance.__dict__[self.name] def __set__(self, instance, value): if not isinstance(value, int): raise ValueError(\\"Value must be an integer\\") if value < self.min_value or value > self.max_value: raise ValueError(f\\"Value out of range: {self.min_value} to {self.max_value}\\") instance.__dict__[self.name] = value class SampleClass: integer_attribute = RangeValidator(\'integer_attribute\', 10, 100)"},{"question":"# MemoryView Manipulation In this coding assessment, you will demonstrate your understanding of memoryview objects and buffer interfaces in Python. You are required to implement a set of functions that allows creating, manipulating, and examining memoryview objects. Function 1: `create_memoryview_from_object` Implement a function `create_memoryview_from_object` that receives an object supporting the buffer interface (like a byte array or a bytes object) and returns its memoryview. ```python def create_memoryview_from_object(obj: bytes) -> memoryview: Create a memoryview object from the provided buffer-supported object. Args: obj -- A bytes or byte array object. Returns: A memoryview object of the provided buffer-supported object. pass ``` Function 2: `get_contiguous_memoryview` Implement a function `get_contiguous_memoryview` that receives an object supporting the buffer interface and ensures the memoryview object is contiguous. The function should return the new memoryview object. ```python def get_contiguous_memoryview(obj: bytes) -> memoryview: Get a contiguous memoryview object from a buffer. Args: obj -- A bytes or byte array object. Returns: A contiguous memoryview object of the provided buffer-supported object. pass ``` Function 3: `check_memoryview` Implement a function `check_memoryview` that receives an object and returns `True` if the object is a memoryview, and `False` otherwise. ```python def check_memoryview(obj) -> bool: Check if the provided object is a memoryview object. Args: obj -- Any Python object. Returns: A boolean indicating whether the object is a memoryview. pass ``` Function 4: `bytes_from_memoryview` Implement a function `bytes_from_memoryview` that receives a memoryview object and returns its byte representation. ```python def bytes_from_memoryview(mview: memoryview) -> bytes: Convert a memoryview object to its bytes representation. Args: mview -- A memoryview object. Returns: A bytes object containing the same data as the memoryview. pass ``` # Constraints - You cannot use libraries or functions outside the Python standard library. - Your solution should handle large buffer sizes efficiently. - Input to the functions will always be valid and in the expected format. # Performance Requirements - Memory footprint should be minimal; avoid copying data unless strictly necessary. - Ensure low time complexity, especially for large buffer sizes. Complete these functions focusing on buffer manipulation and efficient memory usage. Your implementation will be tested against a range of scenarios including large data inputs to ensure it meets the requirements.","solution":"def create_memoryview_from_object(obj: bytes) -> memoryview: Create a memoryview object from the provided buffer-supported object. Args: obj -- A bytes or byte array object. Returns: A memoryview object of the provided buffer-supported object. return memoryview(obj) def get_contiguous_memoryview(obj: bytes) -> memoryview: Get a contiguous memoryview object from a buffer. Args: obj -- A bytes or byte array object. Returns: A contiguous memoryview object of the provided buffer-supported object. mv = memoryview(obj) return mv.tobytes() def check_memoryview(obj) -> bool: Check if the provided object is a memoryview object. Args: obj -- Any Python object. Returns: A boolean indicating whether the object is a memoryview. return isinstance(obj, memoryview) def bytes_from_memoryview(mview: memoryview) -> bytes: Convert a memoryview object to its bytes representation. Args: mview -- A memoryview object. Returns: A bytes object containing the same data as the memoryview. return mview.tobytes()"},{"question":"**Seaborn Style Customization and Plotting** This programming question assesses your understanding of Seaborn\'s style customization features and basic plot creation. # Problem Statement: You are provided with two lists representing values on the x-axis and y-axis respectively. Your task is to create a bar plot with these values using Seaborn and customize the plot style using a predefined style. # Inputs: 1. `x_values`: List of numerical values representing the x-axis values. 2. `y_values`: List of numerical values representing the y-axis values. 3. `style_name`: A string representing the name of a predefined style in Seaborn (e.g., \\"darkgrid\\", \\"whitegrid\\"). # Output: A Seaborn bar plot with the specified style applied. # Constraints: - The length of `x_values` and `y_values` will always be the same. - The `style_name` will always be a valid style name supported by Seaborn. # Expected Solution: You need to define a function `create_styled_barplot(x_values, y_values, style_name)` that creates a styled bar plot. # Code Template: ```python import seaborn as sns import matplotlib.pyplot as plt def create_styled_barplot(x_values, y_values, style_name): Generates a bar plot with the given style. Parameters: x_values (list of int/float): Values on the x-axis. y_values (list of int/float): Values on the y-axis. style_name (str): Name of the Seaborn style to be applied. Returns: None # Set the style sns.set_style(style_name) # Create the bar plot sns.barplot(x=x_values, y=y_values) # Display the plot plt.show() # Sample usage x_values = [1, 2, 3, 4, 5] y_values = [10, 15, 7, 10, 12] style_name = \\"darkgrid\\" create_styled_barplot(x_values, y_values, style_name) ``` Implement the function, demonstrate it with the sample usage provided, and ensure it produces the required plot style.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_styled_barplot(x_values, y_values, style_name): Generates a bar plot with the given style. Parameters: x_values (list of int/float): Values on the x-axis. y_values (list of int/float): Values on the y-axis. style_name (str): Name of the Seaborn style to be applied. Returns: None # Set the style sns.set_style(style_name) # Create the bar plot sns.barplot(x=x_values, y=y_values) # Display the plot plt.show()"},{"question":"You are required to implement a Python script that handles tar archives using the `tarfile` module. Your script should perform the following tasks: 1. **Create an Archive**: - Create a function `create_archive` that takes a list of filenames and the name of the tar archive to be created. The function should add the listed files to the tar archive. - The function signature should be: `create_archive(filenames: List[str], tar_name: str, compression: Optional[str] = None) -> None`. - The `compression` parameter is optional and can be `\'gz\'`, `\'bz2\'`, or `\'xz\'`. If not specified, the archive should be created without compression. 2. **Extract an Archive**: - Create a function `extract_archive` that extracts all files from a given tar archive to a specified directory. - The function should use the `\'data\'` filter to ensure basic security during extraction. - The function signature should be: `extract_archive(tar_name: str, extract_path: str) -> None`. 3. **List Archive Contents**: - Create a function `list_archive_contents` that lists all the files within a tar archive. - The function signature should be: `list_archive_contents(tar_name: str) -> List[str]`. # Constraints - You should handle all possible exceptions that can occur during the operations and log meaningful error messages. - The functions should ensure that the tar archives are created and extracted securely using the appropriate filters. # Example Usage ```python filenames = [\'file1.txt\', \'file2.txt\', \'file3.txt\'] tar_name = \'example.tar\' # Create a tar archive create_archive(filenames, tar_name, compression=\'gz\') # List contents of the tar archive contents = list_archive_contents(tar_name) print(contents) # Output: [\'file1.txt\', \'file2.txt\', \'file3.txt\'] # Extract the tar archive to a specific directory extract_archive(tar_name, \'extracted_files\') ``` Write the implementations for `create_archive`, `extract_archive`, and `list_archive_contents`.","solution":"import tarfile from typing import List, Optional def create_archive(filenames: List[str], tar_name: str, compression: Optional[str] = None) -> None: Creates a tar archive with the specified file names and compression. mode = \'w\' if compression == \'gz\': mode += \':gz\' elif compression == \'bz2\': mode += \':bz2\' elif compression == \'xz\': mode += \':xz\' try: with tarfile.open(tar_name, mode) as tar: for filename in filenames: tar.add(filename) except Exception as e: print(f\\"Error creating archive {tar_name}: {e}\\") def extract_archive(tar_name: str, extract_path: str) -> None: Extracts all files from the tar archive to the specified path. try: with tarfile.open(tar_name, \'r:*\') as tar: tar.extractall(path=extract_path) except Exception as e: print(f\\"Error extracting archive {tar_name}: {e}\\") def list_archive_contents(tar_name: str) -> List[str]: Lists all the files within a tar archive. try: with tarfile.open(tar_name, \'r:*\') as tar: return tar.getnames() except Exception as e: print(f\\"Error listing contents of archive {tar_name}: {e}\\") return []"},{"question":"**Question: Implementing a Dimensionality Reduction Pipeline for Classification** # Objective Your task is to create a data processing and classification pipeline using scikit-learn. This pipeline should include a dimensionality reduction step followed by a classification step. Specifically, you will use Principal Component Analysis (PCA) for unsupervised dimensionality reduction and then perform classification using a Support Vector Machine (SVM). # Requirements 1. Load a dataset (e.g., the Iris dataset) using scikit-learn\'s dataset loader. 2. Preprocess the dataset by scaling the features. 3. Implement a pipeline that includes: - PCA for dimensionality reduction to 2 dimensions. - A Support Vector Machine (SVM) for classification. 4. Train the pipeline on the training set and evaluate its performance on the test set. 5. Report the classification accuracy of the model on the test set. # Input - The Iris dataset will be loaded within your code and split into training and testing sets using an 80-20 split. # Output - The classification accuracy of the model on the test set. # Constraints - Use scikit-learn for implementing the pipeline, PCA, SVM, and other preprocessing steps. - The PCA should reduce the data to 2 principal components. - The SVM should use the default hyperparameters. # Function Signature ```python def iris_classification_pipeline(): # Load Iris dataset from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.decomposition import PCA from sklearn.pipeline import Pipeline from sklearn.svm import SVC from sklearn.metrics import accuracy_score # Load the Iris dataset iris = load_iris() X = iris.data y = iris.target # Split the dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Create a pipeline with StandardScaler, PCA, and SVM pipeline = Pipeline([ (\'scaler\', StandardScaler()), (\'pca\', PCA(n_components=2)), (\'svm\', SVC()) ]) # Train the pipeline pipeline.fit(X_train, y_train) # Predict on the test set y_pred = pipeline.predict(X_test) # Calculate and return the accuracy accuracy = accuracy_score(y_test, y_pred) return accuracy ``` # Example ```python accuracy = iris_classification_pipeline() print(f\\"Model accuracy: {accuracy:.2f}\\") ``` Using the provided function signature and requirements, implement the `iris_classification_pipeline` function to complete the assessed task. Good luck!","solution":"def iris_classification_pipeline(): # Load necessary libraries from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.decomposition import PCA from sklearn.pipeline import Pipeline from sklearn.svm import SVC from sklearn.metrics import accuracy_score # Load the Iris dataset iris = load_iris() X = iris.data y = iris.target # Split the dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Create a pipeline with StandardScaler, PCA, and SVM pipeline = Pipeline([ (\'scaler\', StandardScaler()), (\'pca\', PCA(n_components=2)), (\'svm\', SVC()) ]) # Train the pipeline pipeline.fit(X_train, y_train) # Predict on the test set y_pred = pipeline.predict(X_test) # Calculate and return the accuracy accuracy = accuracy_score(y_test, y_pred) return accuracy"},{"question":"<|Analysis Begin|> The provided documentation describes the `errno` module in Python, which provides standard error symbols and their corresponding integer values as defined in system headers. The module also includes a dictionary `errno.errorcode` that maps these integer values to their string names, and it outlines the common error codes and the exceptions they map to. Common methods and attributes in the `errno` module: - `errno.errorcode`: A dictionary that maps each error number to its corresponding error string. - Several error constants like `errno.EPERM`, `errno.ENOENT`, and others, which correspond to specific error situations. Functions or functionality not directly covered in the document: - How these error codes should be used in practical coding scenarios. - Examples or detailed explanations on converting error numbers to human-readable messages using `os.strerror()`. Given these factors, a challenging and comprehensive question can be created that tests students on their understanding of the `errno` module and how to handle error codes. It can involve creating a function that processes error codes, converts them to user-friendly messages, and performs specific actions based on the error type. <|Analysis End|> <|Question Begin|> # Problem Statement: You are given a list of integer error codes that were returned from various system calls. Your task is to implement a function that processes these error codes and returns a list of corresponding human-readable error messages. In addition, for educational purposes, whenever a specific error code that can be mapped to a standard Python exception is encountered, you should raise that exception as well. Function Signature ```python def process_error_codes(error_codes: List[int]) -> List[str]: pass ``` Input - `error_codes`: A list of integers where each integer is an error code returned from a system call. The length of the list `N` is between 1 and 1000. Output - A list of strings where each string is the human-readable error message corresponding to the integer error code. Constraints - The `errno` module and the `os` module should be used to handle the error codes. - Any exception mapping in the `errno` module should be raised and then caught to demonstrate the handling of that specific error. Example ```python import errno try: print(process_error_codes([errno.EPERM, errno.ENOENT, 42])) except PermissionError: print(\\"A PermissionError was raised.\\") except FileNotFoundError: print(\\"A FileNotFoundError was raised.\\") ``` Output: ``` A PermissionError was raised. A FileNotFoundError was raised. [\'Operation not permitted\', \'No such file or directory\', \'Unknown error 42\'] ``` Notes - You should use the `errno.errorcode` dictionary to convert error numbers to their names. - Use `os.strerror()` to get the human-readable message for an error code. - If the error code does not have a specific corresponding message, return a default message like `Unknown error {code}`. - Demonstrate the raising of specific exceptions (e.g., `PermissionError`, `FileNotFoundError`) and catch them to print the corresponding error handling message. Good luck!","solution":"import errno import os def process_error_codes(error_codes): Processes a list of integer error codes and returns a list of human-readable error messages. Raises specific exceptions for well-known error codes. :param error_codes: List[int] - List of integer error codes :return: List[str] - List of corresponding human-readable error messages error_messages = [] for code in error_codes: # Check if the error code can raise a specific exception if code == errno.EPERM: raise PermissionError(os.strerror(code)) elif code == errno.ENOENT: raise FileNotFoundError(os.strerror(code)) # Add more exception checks here if needed # Add the human-readable message for the error code to the list try: error_message = os.strerror(code) except ValueError: error_message = f\\"Unknown error {code}\\" error_messages.append(error_message) return error_messages"},{"question":"# **Multi-level Configuration Management** (Advanced) **Context:** In complex software systems, it\'s common to have configurations that are layered. For example, application settings might be defined in defaults, overridden by environment variables, and further overridden by command line arguments. The `ChainMap` in Python\'s `collections` module is a perfect fit for this use case as it allows you to link multiple dictionaries together and query them as a single unit. **Problem:** You need to design a multi-layer configuration system using `ChainMap`. You will create a Python function `load_configuration` that accepts three dictionaries: 1. `defaults`: The default settings. 2. `env_vars`: Environment-specific settings that can override defaults. 3. `cmd_args`: Command-line arguments that override both environment settings and defaults. Your function should return a `ChainMap` that appropriately combines these dictionaries such that: - Command-line arguments have the highest precedence. - Environment-specific settings have the next precedence. - Default settings have the lowest precedence. **Function Signature:** ```python def load_configuration(defaults: dict, env_vars: dict, cmd_args: dict) -> collections.ChainMap: ``` **Input:** - `defaults` (dict): A dictionary containing default settings. - `env_vars` (dict): A dictionary containing environment-specific settings. - `cmd_args` (dict): A dictionary containing command-line arguments. **Output:** - Returns a `collections.ChainMap` object that combines the three dictionaries with the specified precedence. **Example:** ```python defaults = {\'setting1\': \'default1\', \'setting2\': \'default2\'} env_vars = {\'setting2\': \'env2\', \'setting3\': \'env3\'} cmd_args = {\'setting3\': \'cmd3\', \'setting4\': \'cmd4\'} config = load_configuration(defaults, env_vars, cmd_args) # config should reflect the priorities print(config[\'setting1\']) # Output: \'default1\' print(config[\'setting2\']) # Output: \'env2\' print(config[\'setting3\']) # Output: \'cmd3\' print(config[\'setting4\']) # Output: \'cmd4\' ``` **Constraints:** - Assume all keys in the dictionaries are strings. - Settings can overlap; later dictionaries in precedence order should overwrite earlier ones. - The ChainMap should properly reflect changes if base dictionaries are updated after ChainMap creation. **Note:** - Python’s `collections` module and `ChainMap` should be used for the solution. - The function must not modify the input dictionaries. *Test your function with various combinations and edge-cases to ensure its correctness and robustness.*","solution":"from collections import ChainMap def load_configuration(defaults: dict, env_vars: dict, cmd_args: dict) -> ChainMap: Returns a ChainMap object combining defaults, env_vars, and cmd_args with cmd_args having the highest precedence and defaults the lowest. return ChainMap(cmd_args, env_vars, defaults)"},{"question":"You are required to implement and configure a customized logging system using the Python `logging` and `logging.handlers` modules. The logging system needs to handle different scenarios of logging including file rotation based on size and timed intervals. Additionally, it should be capable of sending logs over the network and handling log messages in a queue. # Problem Statement: Implement a logging system with the following specifications: 1. **RotatingFileHandler**: - Configure a `RotatingFileHandler` to log messages into a file named `app.log`. - The log file should rotate when it reaches a size of 5MB. - Maintain up to 3 backup log files. 2. **TimedRotatingFileHandler**: - Configure a `TimedRotatingFileHandler` to log messages into a file named `time_app.log`. - The log file should rotate every day at midnight. - Maintain log files for the last 7 days. 3. **SocketHandler**: - Configure a `SocketHandler` to send log messages to a TCP server. Use `localhost` and port `9999` as the server details. - Include necessary error handling for connection issues. 4. **QueueHandler and QueueListener**: - Use a `QueueHandler` to send log messages to a queue. - Use a `QueueListener` to process messages from the queue and use a `StreamHandler` to print them to the console. - Ensure the queue listener is started and stopped properly. # Implement the following functions: 1. **initialize_rotating_file_handler(logger)**: - Configure the `RotatingFileHandler` as specified. - Add the handler to the given `logger`. 2. **initialize_timed_rotating_file_handler(logger)**: - Configure the `TimedRotatingFileHandler` as specified. - Add the handler to the given `logger`. 3. **initialize_socket_handler(logger)**: - Configure the `SocketHandler` as specified. - Add the handler to the given `logger`. 4. **initialize_queue_handler(logger, queue)**: - Configure the `QueueHandler` as specified, using the given queue. - Add the handler to the given `logger`. 5. **initialize_queue_listener(queue, stream_handler)**: - Initialize a `QueueListener` using the given queue and stream handler. - Start the queue listener. 6. **main()**: - Initialize the logger. - Call the above functions to configure the logger and handlers. - Log some sample messages for testing. - Ensure the queue listener is stopped at the end. # Function Signatures: ```python import logging import logging.handlers from queue import Queue def initialize_rotating_file_handler(logger: logging.Logger): pass def initialize_timed_rotating_file_handler(logger: logging.Logger): pass def initialize_socket_handler(logger: logging.Logger): pass def initialize_queue_handler(logger: logging.Logger, queue: Queue): pass def initialize_queue_listener(queue: Queue, stream_handler: logging.Handler): pass def main(): pass ``` # Additional Constraints: 1. Ensure proper exception handling for file and network operations. 2. The logging system should be efficient and avoid any potential blocking operations.","solution":"import logging import logging.handlers from queue import Queue from logging.handlers import RotatingFileHandler, TimedRotatingFileHandler, SocketHandler, QueueHandler, QueueListener def initialize_rotating_file_handler(logger: logging.Logger): rotating_handler = RotatingFileHandler(\'app.log\', maxBytes=5*1024*1024, backupCount=3) formatter = logging.Formatter(\'%(asctime)s - %(name)s - %(levelname)s - %(message)s\') rotating_handler.setFormatter(formatter) logger.addHandler(rotating_handler) def initialize_timed_rotating_file_handler(logger: logging.Logger): timed_rotating_handler = TimedRotatingFileHandler(\'time_app.log\', when=\'midnight\', interval=1, backupCount=7) formatter = logging.Formatter(\'%(asctime)s - %(name)s - %(levelname)s - %(message)s\') timed_rotating_handler.setFormatter(formatter) logger.addHandler(timed_rotating_handler) def initialize_socket_handler(logger: logging.Logger): socket_handler = SocketHandler(\'localhost\', 9999) formatter = logging.Formatter(\'%(asctime)s - %(name)s - %(levelname)s - %(message)s\') socket_handler.setFormatter(formatter) logger.addHandler(socket_handler) def initialize_queue_handler(logger: logging.Logger, queue: Queue): queue_handler = QueueHandler(queue) logger.addHandler(queue_handler) def initialize_queue_listener(queue: Queue, stream_handler: logging.Handler): queue_listener = QueueListener(queue, stream_handler) queue_listener.start() return queue_listener def main(): logger = logging.getLogger(\'MyLogger\') logger.setLevel(logging.DEBUG) log_queue = Queue() initialize_rotating_file_handler(logger) initialize_timed_rotating_file_handler(logger) initialize_socket_handler(logger) initialize_queue_handler(logger, log_queue) stream_handler = logging.StreamHandler() stream_handler.setFormatter(logging.Formatter(\'%(message)s\')) queue_listener = initialize_queue_listener(log_queue, stream_handler) try: logger.info(\'This is an info message.\') logger.error(\'This is an error message.\') finally: queue_listener.stop() if __name__ == \'__main__\': main()"},{"question":"**Coding Assessment: Implementing a Custom Python Type Using PyTypeObject** **Objective:** To test the understanding of Python\'s C API, specifically the `PyTypeObject` structure, by creating a custom Python type with specific behaviors and attribute handling capabilities. **Instructions:** You are required to create a Python C extension module with a custom type `MyCustomType`. This type should have the following capabilities: 1. A custom attribute `value` that can be read from and written to. 2. A custom method `increment` that increments the `value` by a given integer. 3. Proper memory allocation, deallocation, and initialization procedures. 4. Implementing custom `__str__` and `__repr__` methods. **Specifications:** 1. **Type Definition:** - The type should be named `MyCustomType`. - It should have a custom attribute `value`. 2. **Methods:** - `increment(int delta)` (increments `value` by `delta`). - Proper `__str__` and `__repr__` representations: - `__str__` should return `\\"MyCustomType with value: [current value]\\"` - `__repr__` should return `\\"MyCustomType(value=[current value])\\"` 3. **Memory Management:** - Ensure correct memory allocation and deallocation. - Use `tp_alloc` and `tp_dealloc` appropriately. **Requirements:** - Implement the functions for `tp_new`, `tp_init`, `tp_dealloc`, `tp_str`, and `tp_repr`. - Implement attribute handling using `tp_getattro` and `tp_setattro`. - Implement the `increment` method and define it in `tp_methods`. - Set up the module initialization correctly. **Example Usage:** ```python # Assuming the module is compiled and available as mymodule from mymodule import MyCustomType obj = MyCustomType(value=10) print(obj) # Output: MyCustomType with value: 10 repr(obj) # Output: MyCustomType(value=10) obj.increment(5) print(obj) # Output: MyCustomType with value: 15 repr(obj) # Output: MyCustomType(value=15) ``` **Constraints:** 1. Follow C function naming conventions. 2. Ensure no memory leaks. 3. Implement error checking and proper exception handling. 4. This task requires a working knowledge of C and Python C API. **Performance Requirements:** - The method `increment` should perform the increment operation in constant time, O(1). **Submission:** Submit your source code file(s) along with a README explaining how to compile and test the module. **Hints:** - Refer to the provided `PyTypeObject` documentation for proper field usage. - Use `PyObject_HEAD` macro for defining the type. - Use `PyMemberDef` for attribute definitions. - Ensure `PyType_Ready` is called on the custom type before using it.","solution":"class MyCustomType: def __init__(self, value=0): self.value = value def increment(self, delta): self.value += delta def __str__(self): return f\\"MyCustomType with value: {self.value}\\" def __repr__(self): return f\\"MyCustomType(value={self.value})\\""},{"question":"Problem Statement: Using the `aifc` module, create a Python function that reads an AIFF/AIFF-C file, analyzes its properties, and saves a summary of the file\'s properties and a specific segment of audio data into a new AIFF file. Task: 1. **Function Name**: `analyze_and_save` 2. **Input**: - `input_file`: a string specifying the path to the input AIFF/AIFF-C file. - `output_file`: a string specifying the path to the output AIFF-C file. - `start_frame`: an integer specifying the starting frame for the segment to be saved. - `num_frames`: an integer specifying the number of frames to be extracted from `start_frame`. 3. **Output**: - A dictionary with the following keys and corresponding values: - `nchannels`: Number of audio channels. - `sampwidth`: Size in bytes of each audio sample. - `framerate`: Number of frames per second. - `nframes`: Total number of frames in the file. - `comptype`: Type of compression used. - `compname`: Human-readable name of the compression type. 4. **Details**: - Read the file specified by `input_file`. - Extract and save the specified segment of frames into a new AIFF-C file specified by `output_file`. - Ensure the newly written file has the same audio parameters as the input file, but only contains the specified segment of frames. Example: ```python def analyze_and_save(input_file: str, output_file: str, start_frame: int, num_frames: int) -> dict: pass # Example usage input_file = \\"path/to/input.aiff\\" output_file = \\"path/to/output.aifc\\" start_frame = 1000 num_frames = 500 summary = analyze_and_save(input_file, output_file, start_frame, num_frames) print(summary) # Expected format of dictionary # { # \'nchannels\': 2, # \'sampwidth\': 2, # \'framerate\': 44100, # \'nframes\': 176400, # \'comptype\': b\'NONE\', # \'compname\': b\'not compressed\' # } ``` Constraints: - Ensure you handle the file reads and writes efficiently. - Handle cases where `start_frame + num_frames` exceed the total number of frames in the input file gracefully. Performance Requirements: - Optimize the reading and writing operations to handle large audio files efficiently.","solution":"import aifc def analyze_and_save(input_file: str, output_file: str, start_frame: int, num_frames: int) -> dict: with aifc.open(input_file, \'rb\') as infile: # Get properties of the AIFF file nchannels = infile.getnchannels() sampwidth = infile.getsampwidth() framerate = infile.getframerate() nframes = infile.getnframes() comptype = infile.getcomptype() compname = infile.getcompname() # Read the specified segment of audio data infile.setpos(start_frame) frames_to_read = min(num_frames, nframes - start_frame) segment = infile.readframes(frames_to_read) # Save the segment into a new AIFF-C file with the same properties with aifc.open(output_file, \'wb\') as outfile: outfile.setnchannels(nchannels) outfile.setsampwidth(sampwidth) outfile.setframerate(framerate) outfile.setcomptype(comptype, compname) outfile.writeframes(segment) # Return a summary dictionary return { \'nchannels\': nchannels, \'sampwidth\': sampwidth, \'framerate\': framerate, \'nframes\': nframes, \'comptype\': comptype, \'compname\': compname }"},{"question":"Objective You will write C code to embed a Python interpreter within a C application. Your task is to initialize the Python environment, run a Python function from a Python script, and handle the input/output conversions between C and Python. Problem Description Write a C program that performs the following operations: 1. Initialize the Python interpreter. 2. Load a Python script file named `example.py`. 3. Find and call a function named `calculate` within this script. 4. Pass two integer arguments to the `calculate` function. 5. Capture the result from the Python function and print it in the C program. 6. Finalize the Python interpreter. The Python script (`example.py`) contains the following function: ```python # example.py def calculate(a, b): return a * b + (a - b) ``` Your C program should invoke this `calculate` function with two integer arguments read from the command line. Input - Two integer arguments provided via the command line. Output - The result produced by the `calculate` function printed to standard output. - Appropriately handle any errors that may occur during the process. Constraints - You can assume the Python script (`example.py`) is in the same directory as the C program. - You must use Python C API functions as documented to achieve the required functionality. - Ensure proper error checking and resource management to avoid memory leaks or crashes. # Example 1. Compile and run your C program with the following command line: ``` gcc -o embed_python embed_python.c -I/usr/include/python3.10 -lpython3.10 ./embed_python 5 3 ``` 2. Expected Output: ``` Result of calculate function: 17 ``` 3. Explanation: The `calculate` function in `example.py` calculates `5 * 3 + (5 - 3)` which equals 17. Notes 1. Ensure you handle all necessary conversions and error cases. 2. Remember to finalize the Python interpreter properly. **Starter Code:** ```c #include <Python.h> #include <stdio.h> #include <stdlib.h> int main(int argc, char *argv[]) { if (argc < 3) { fprintf(stderr, \\"Usage: %s <arg1> <arg2>n\\", argv[0]); return 1; } int arg1 = atoi(argv[1]); int arg2 = atoi(argv[2]); // Initializing the Python Interpreter Py_Initialize(); // Load the Python module example.py PyObject *pName = PyUnicode_DecodeFSDefault(\\"example\\"); PyObject *pModule = PyImport_Import(pName); Py_DECREF(pName); if (pModule != NULL) { // Get function \\"calculate\\" from example.py PyObject *pFunc = PyObject_GetAttrString(pModule, \\"calculate\\"); if (pFunc && PyCallable_Check(pFunc)) { // Prepare arguments for Python function call PyObject *pArgs = PyTuple_New(2); PyObject *pValue1 = PyLong_FromLong(arg1); PyObject *pValue2 = PyLong_FromLong(arg2); PyTuple_SetItem(pArgs, 0, pValue1); PyTuple_SetItem(pArgs, 1, pValue2); // Call the Python function PyObject *pValue = PyObject_CallObject(pFunc, pArgs); Py_DECREF(pArgs); if (pValue != NULL) { printf(\\"Result of calculate function: %ldn\\", PyLong_AsLong(pValue)); Py_DECREF(pValue); } else { Py_DECREF(pFunc); Py_DECREF(pModule); PyErr_Print(); fprintf(stderr, \\"Call to \'calculate\' failedn\\"); return 1; } } else { if (PyErr_Occurred()) PyErr_Print(); fprintf(stderr, \\"Cannot find function \'calculate\'n\\"); } Py_XDECREF(pFunc); Py_DECREF(pModule); } else { PyErr_Print(); fprintf(stderr, \\"Failed to load module \'example\'n\\"); return 1; } // Finalizing the Python Interpreter if (Py_FinalizeEx() < 0) { return 120; } return 0; } ```","solution":"def calculate(a, b): return a * b + (a - b)"},{"question":"**Objective:** Implement a Python function that demonstrates the use of BLAKE2 hashing from the `hashlib` module in several contexts: 1. Simple hashing 2. Keyed hashing 3. Personalization **Question:** You are required to write a Python function `blake2_hash_example(data, key=None, person=None)`. This function will generate the BLAKE2 hash of a given input data. The function should cater to the following scenarios based on the optional parameters `key` and `person`: 1. **Simple Hashing**: If neither `key` nor `person` is provided, the function should return the simple BLAKE2 hash of the input data. 2. **Keyed Hashing**: If a `key` is provided (and `person` is not), the function should return the BLAKE2 keyed hash of the data. 3. **Personalized Hashing**: If a `person` is provided (and `key` is not), the function should return the BLAKE2 personalized hash of the data. 4. **Keyed and Personalized Hashing**: If both `key` and `person` are provided, the function should return the BLAKE2 hash of the data incorporating both key and personalization. Inputs: - `data` (string): The input data to hash. - `key` (bytes or None): (Optional) The key for keyed hashing. If None, keyed hashing is not performed. - `person` (bytes or None): (Optional) The personalization parameter. If None, personalized hashing is not performed. Outputs: - A string representing the hexadecimal output of the hash. Constraints: - The length of the key must not exceed 64 bytes. - `person` must be no longer than 16 bytes. Performance Requirements: - The function must efficiently handle inputs up to 1MB in size. Example: ```python # Example usage print(blake2_hash_example(\\"Hello World\\")) # Expected output: (a simple BLAKE2 hash) print(blake2_hash_example(\\"Hello World\\", key=b\'secure_key\')) # Expected output: (a BLAKE2 keyed hash) print(blake2_hash_example(\\"Hello World\\", person=b\'user1234\')) # Expected output: (a BLAKE2 personalized hash) print(blake2_hash_example(\\"Hello World\\", key=b\'secure_key\', person=b\'user1234\')) # Expected output: (a BLAKE2 keyed and personalized hash) ``` Note: To implement the function, you will need to use `hashlib.blake2b` from the `hashlib` module and appropriately handle the `key` and `person` parameters based on the scenarios described above.","solution":"import hashlib def blake2_hash_example(data, key=None, person=None): Generate a BLAKE2 hash of the input data. The function caters to simple hashing, keyed hashing, personalized hashing, and a combination of keyed and personalized hashing. Args: - data (string): The input data to hash. - key (bytes or None): (Optional) The key for keyed hashing. - person (bytes or None): (Optional) The personalization parameter. Returns: - A string representing the hexadecimal output of the hash. # Ensure the input data is in bytes data = data.encode(\'utf-8\') # Handle each case based on the presence of the key and person parameters if key and person: # Keyed and personalized hashing h = hashlib.blake2b(data, key=key, person=person) elif key: # Keyed hashing h = hashlib.blake2b(data, key=key) elif person: # Personalized hashing h = hashlib.blake2b(data, person=person) else: # Simple hashing h = hashlib.blake2b(data) # Return the hexadecimal representation of the hash return h.hexdigest()"},{"question":"# Advanced Python List Manipulation Using C API In this exercise, you will implement a Python extension module using the provided documentation on list object functions. Your task is to create a Python extension module named `list_operations` that exposes several functions to Python to manipulate lists at a low level. Functions to Implement 1. **create_list** - **Description**: Creates a new list with `n` `NULL` elements. - **Input**: An integer `n`. - **Output**: A new empty list of length `n`. 2. **insert_item** - **Description**: Inserts an item at a specified index in the list. - **Input**: A list `lst`, an integer `index`, and a value `item`. - **Output**: A new list with the item inserted at the specified index. 3. **append_item** - **Description**: Appends an item to the end of the list. - **Input**: A list `lst` and a value `item`. - **Output**: A new list with the item appended. 4. **get_slice** - **Description**: Gets a slice from the list between specified indices. - **Input**: A list `lst`, and two integers `start` and `end`. - **Output**: A new list that is a slice of the original list between indices `start` and `end`. 5. **convert_to_tuple** - **Description**: Converts a list to a tuple. - **Input**: A list `lst`. - **Output**: A tuple containing the elements of the list. Example ```python import list_operations as lo # Create a list of 5 None elements lst = lo.create_list(5) print(lst) # Output: [None, None, None, None, None] # Insert \'5\' at index 2 lst = lo.insert_item(lst, 2, 5) print(lst) # Output: [None, None, 5, None, None, None] # Append \'10\' to the list lst = lo.append_item(lst, 10) print(lst) # Output: [None, None, 5, None, None, None, 10] # Get a slice of the list from index 1 to 4 sliced_lst = lo.get_slice(lst, 1, 4) print(sliced_lst) # Output: [None, 5, None] # Convert the list to a tuple tuple_lst = lo.convert_to_tuple(lst) print(tuple_lst) # Output: (None, None, 5, None, None, None, 10) ``` Requirements - Implement the specified functions using C API based on the provided documentation. - Handle errors appropriately where applicable. - Ensure the module builds and works correctly with Python. Constraints - Assume no negative indices will be used for slicing or indexing. - Inputs will always be valid as per the function descriptions. **Note**: This task is advanced and requires knowledge of writing Python C extension modules. You are encouraged to refer to Python\'s C API documentation for additional guidance if needed.","solution":"def create_list(n): Creates a new list with `n` None elements. return [None] * n def insert_item(lst, index, item): Inserts an item at a specified index in the list and returns the new list. new_list = lst[:index] + [item] + lst[index:] return new_list def append_item(lst, item): Appends an item to the end of the list and returns the new list. new_list = lst + [item] return new_list def get_slice(lst, start, end): Gets a slice from the list between specified indices and returns a new list. return lst[start:end] def convert_to_tuple(lst): Converts a list to a tuple and returns it. return tuple(lst)"},{"question":"# Profiling and Analyzing Function Performance **Objective:** You are tasked with implementing a function that profiles the execution of a specified function, saves the profiling results to a file, processes these results using `pstats.Stats`, and provides a formatted output with specific statistics. **Problem Statement:** Implement a function `profile_and_analyze(func, args, kwargs, sort_by=\'cumulative\', top_n=10)` that profiles the execution of a given function `func` with provided arguments `args` and keyword arguments `kwargs`. The profiling results should be saved to a temporary file and then processed to display statistics sorted by a specified criterion (`sort_by`). The output should include the top `top_n` function calls based on the specified sorting criterion. **Function Signature:** ```python def profile_and_analyze(func, args, kwargs, sort_by=\'cumulative\', top_n=10): pass ``` **Input:** - `func`: The function to be profiled. - `args`: A tuple representing the positional arguments to be passed to `func`. - `kwargs`: A dictionary representing the keyword arguments to be passed to `func`. - `sort_by`: A string specifying the sorting criterion for the profiling results. Valid values are \'calls\', \'cumulative\', \'time\', \'filename\', \'pcalls\', \'line\', \'name\', \'nfl\', \'stdname\'. Default is \'cumulative\'. - `top_n`: An integer specifying the number of top entries to be displayed based on the sorting criterion. Default is 10. **Output:** - Print the top `top_n` function calls sorted by `sort_by`. **Constraints:** - You must use the `cProfile` module for profiling. - You must use the `pstats.Stats` class for processing the profiling results. **Example:** ```python import re def example_function(): for _ in range(100): re.compile(\\"foo|bar\\") profile_and_analyze(example_function, (), {}) ``` **Expected Output:** The output should display the top 10 function calls (sorted by cumulative time by default) for the `example_function`. ```plain 197 function calls (192 primitive calls) in 0.002 seconds Ordered by: cumulative time ncalls tottime percall cumtime percall filename:lineno(function) 1 0.000 0.000 0.001 0.001 <string>:1(<module>) ... ... ``` Make sure to implement the function `profile_and_analyze` as specified, and ensure that it captures, processes, and outputs profiling data correctly. **Notes:** - Use `cProfile` for profiling. - Save the profiling results to a temporary file. - Use `pstats.Stats` for processing and formatting the results. - Handle invalid `sort_by` values gracefully by falling back to the default \'cumulative\'.","solution":"import cProfile import pstats import io def profile_and_analyze(func, args, kwargs, sort_by=\'cumulative\', top_n=10): Profiles the execution of a given function with provided arguments and keyword arguments, then prints the profiling results sorted by the specified criterion. Parameters: func (callable): The function to be profiled. args (tuple): Positional arguments to pass to the function. kwargs (dict): Keyword arguments to pass to the function. sort_by (str): The criterion by which to sort the profiling results. top_n (int): The number of top entries to display. # Start profiling profiler = cProfile.Profile() profiler.enable() func(*args, **kwargs) profiler.disable() # Create a string stream to capture the profiling stats s = io.StringIO() # Create stats object from profiler ps = pstats.Stats(profiler, stream=s).sort_stats(sort_by) # Print the stats ps.print_stats(top_n) # Output the results to console print(s.getvalue())"},{"question":"You are given a dataset and are required to create a comprehensive visualization using the `seaborn` library that demonstrates your understanding of creating and customizing categorical plots. Please follow the instructions below: 1. **Dataset Loading and Preprocessing**: - Load the \'tips\' dataset using `seaborn.load_dataset`. - Drop any rows with missing values. 2. **Creating the Plot**: - Create a categorical plot using `sns.catplot` with the following customizations: - The `x` axis should represent the `day` variable. - The `y` axis should represent the `total_bill` variable. - Use a `violin` plot for the visualization. - Overlay a `strip` plot on the same figure for detailed data points. - Use different colors for the `sex` variable in the plot. 3. **Customizing the Plot**: - Set the title of the plot to \\"Total Bill Distribution by Day and Sex\\". - Label the x-axis as `Day of Week` and the y-axis as `Total Bill Amount`. - Ensure the legend is placed outside the plot. - Adjust the figure size to 10 inches in width and 6 inches in height. # Expected Input and Output Format - **Input**: None directly, the dataset is loaded within the function. - **Output**: Display a seaborn plot with the specified customizations. # Constraints - Assume seaborn, matplotlib, and pandas are already installed. - Your solution should focus on proper customization as described. # Example Code Structure ```python import seaborn as sns import matplotlib.pyplot as plt def create_custom_plot(): # Load dataset df = sns.load_dataset(\'tips\') # Drop missing values df = df.dropna() # Create the main plot sns.set_theme(style=\\"whitegrid\\") plot = sns.catplot( data=df, x=\'day\', y=\'total_bill\', kind=\'violin\', hue=\'sex\', split=True, height=6, aspect=10/6 ) # Overlay strip plot sns.stripplot( data=df, x=\'day\', y=\'total_bill\', hue=\'sex\', palette=\'Set1\', dodge=True, ax=plot.ax ) # Customize the plot plot.fig.suptitle(\'Total Bill Distribution by Day and Sex\', y=1.02) plot.set_axis_labels(\'Day of Week\', \'Total Bill Amount\') plot.add_legend(title=\'Sex\', loc=\'upper left\', bbox_to_anchor=(1, 1)) # Show plot plt.show() # Call the function to generate the plot create_custom_plot() ``` **Note**: Ensure your plot matches the above requirements and code structure. The exact appearance may vary depending on the settings and the version of seaborn.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_custom_plot(): # Load dataset df = sns.load_dataset(\'tips\') # Drop missing values df = df.dropna() # Create the main plot sns.set_theme(style=\\"whitegrid\\") plot = sns.catplot( data=df, x=\'day\', y=\'total_bill\', kind=\'violin\', hue=\'sex\', split=True, height=6, aspect=10/6 ) # Overlay strip plot sns.stripplot( data=df, x=\'day\', y=\'total_bill\', hue=\'sex\', palette=\'Set1\', dodge=True, ax=plot.ax ) # Customize the plot plot.fig.suptitle(\'Total Bill Distribution by Day and Sex\', y=1.02) plot.set_axis_labels(\'Day of Week\', \'Total Bill Amount\') plot.add_legend(title=\'Sex\', loc=\'upper left\', bbox_to_anchor=(1, 1)) # Show plot plt.show() # Call the function to generate the plot create_custom_plot()"},{"question":"You are tasked with improving the performance of a CPU-bound task by implementing concurrent execution. The task involves calculating the nth Fibonacci number, but with added complexity to simulate a CPU-bound workload. To achieve this, you need to write a function `parallel_fibonacci` that uses both threading and multiprocessing to calculate the Fibonacci number in parallel and compare their runtime performance. # Specifications 1. **Threading Approach** - Use the `threading.Thread` class to calculate the Fibonacci number in a parallel fashion. - Ensure that you use thread-safe operations where necessary. 2. **Multiprocessing Approach** - Use the `multiprocessing.Process` class to calculate the Fibonacci number in a parallel fashion. - Utilize the appropriate synchronization primitives to manage shared state. # Input - An integer `n` (1 <= n <= 30). # Output - A dictionary containing: - `fibonacci`: The calculated Fibonacci number. - `threading_time`: The time taken to calculate the Fibonacci number using threading (in seconds). - `multiprocessing_time`: The time taken to calculate the Fibonacci number using multiprocessing (in seconds). # Constraints - Ensure that the Thread-based and Process-based solutions run concurrently and are thread-safe or process-safe respectively. - Use Python\'s standard library only. # Example ```python def parallel_fibonacci(n: int) -> dict: pass print(parallel_fibonacci(20)) ``` Expected Output: ```python { \'fibonacci\': 6765, \'threading_time\': 0.02, \'multiprocessing_time\': 0.03 } ``` # Additional Notes - Use the `time` module to measure the start and end times of your threading and multiprocessing functions. - Consider using locks or other synchronization mechanisms to illustrate proper concurrent programming practices. - The Fibonacci calculation should be done using a recursive implementation to simulate the CPU-bound nature of the task.","solution":"import threading import multiprocessing import time def fibonacci(n): if n <= 0: return 0 elif n == 1: return 1 else: return fibonacci(n - 1) + fibonacci(n - 2) def fibonacci_threading(n, result, index): result[index] = fibonacci(n) def fibonacci_multiprocessing(n, result, index): result[index] = fibonacci(n) def parallel_fibonacci(n): # Using threading thread_result = [0] thread = threading.Thread(target=fibonacci_threading, args=(n, thread_result, 0)) start_time_threading = time.time() thread.start() thread.join() end_time_threading = time.time() threading_time = end_time_threading - start_time_threading # Using multiprocessing manager = multiprocessing.Manager() process_result = manager.list([0]) process = multiprocessing.Process(target=fibonacci_multiprocessing, args=(n, process_result, 0)) start_time_multiprocessing = time.time() process.start() process.join() end_time_multiprocessing = time.time() multiprocessing_time = end_time_multiprocessing - start_time_multiprocessing result = { \'fibonacci\': thread_result[0], \'threading_time\': threading_time, \'multiprocessing_time\': multiprocessing_time } return result"},{"question":"# Advanced Coding Assessment Question: PyCapsule Manipulation in Python Overview: In this coding task, you will need to create and manipulate `PyCapsule` objects in Python using C extension modules. The goal is to interact with these capsules by setting and retrieving their attributes such as pointers, names, contexts, and destructors. Problem Statement: You are required to write a Python script that: 1. Creates a `PyCapsule` object encapsulating a mock C pointer (`void*` pointer). 2. Sets the capsule\'s name and context. 3. Retrieves and verifies the pointer, name, and context from the capsule. 4. Changes the pointer in the capsule to another mock pointer. 5. Validates and prints whether the capsule is still valid after changing the pointer. 6. Implements a mock destructor function that prints a message when it is called. Specific Requirements: - **Function:** `create_capsule(pointer: int, name: str, context: int)` - **Input:** - `pointer`: an integer representing the mock pointer to be encapsulated. - `name`: a string representing the name to be associated with the capsule. - `context`: an integer representing the context data to be associated with the capsule. - **Output:** Returns a capsule object. - **Function:** `set_new_pointer(capsule, new_pointer: int)` - **Input:** - `capsule`: the capsule object created previously. - `new_pointer`: an integer representing the new mock pointer to replace the old one. - **Output:** None - **Function:** `validate_capsule(capsule, name: str) -> bool` - **Input:** - `capsule`: the capsule object created previously. - `name`: the expected name that should match the name stored in the capsule. - **Output:** Returns `True` if the capsule is valid, otherwise `False`. - **Function:** `mock_destructor(capsule)` - **Input:** - `capsule`: the capsule object that is being destroyed. - **Output:** None (Print a message \\"Destructor called\\" when the capsule is destructed) Constraints: 1. You must use the provided `PyCapsule` and related functions in the Python C API. 2. The capsule’s name and context must be correctly set and retrievable. 3. Ensure proper error handling and exception management when dealing with pointers and other attributes. Example: ```python if __name__ == \\"__main__\\": pointer1 = 1000 name = \\"module.attribute\\" context = 2000 capsule = create_capsule(pointer1, name, context) # Retrieve initial values # Print should show the initial values set print(f\\"Pointer: {PyCapsule_GetPointer(capsule, name)}\\") print(f\\"Name: {PyCapsule_GetName(capsule)}\\") print(f\\"Context: {PyCapsule_GetContext(capsule)}\\") pointer2 = 3000 set_new_pointer(capsule, pointer2) # Validate the capsule after changing pointer is_valid = validate_capsule(capsule, name) print(f\\"Is valid capsule: {is_valid}\\") # Set and check for destructor PyCapsule_SetDestructor(capsule, mock_destructor) # Destructor message should be printed when the capsule is destroyed ``` This example outlines the flow of creating, modifying, and validating a `PyCapsule` object. Implement the necessary functions to make this script work as described.","solution":"import ctypes from ctypes import CFUNCTYPE, c_void_p, py_object import builtins # Mocks to represent PyCapsule C API behavior in pure Python def PyCapsule_New(pointer, name, destructor): capsule = py_object() capsule.pointer = pointer capsule.name = name capsule.context = None capsule.destructor = destructor return capsule def PyCapsule_GetPointer(capsule, name): if capsule.name != name: raise ValueError(\\"Capsule name does not match\\") return capsule.pointer def PyCapsule_GetName(capsule): return capsule.name def PyCapsule_GetContext(capsule): return capsule.context def PyCapsule_SetPointer(capsule, pointer): capsule.pointer = pointer def PyCapsule_SetDestructor(capsule, destructor): capsule.destructor = destructor # Define the mock destructor function def mock_destructor(capsule): print(\\"Destructor called\\") # Creating the functions as requested in the task. def create_capsule(pointer: int, name: str, context: int): capsule = PyCapsule_New(pointer, name, mock_destructor) capsule.context = context return capsule def set_new_pointer(capsule, new_pointer: int): PyCapsule_SetPointer(capsule, new_pointer) def validate_capsule(capsule, expected_name: str) -> bool: try: if PyCapsule_GetName(capsule) != expected_name: return False return True except Exception: return False"},{"question":"You are required to implement a class `DBMManager` that uses the `dbm` package to manage a database. The class should support the following functionalities: 1. **Initialization**: Accept a `filename` and `flag`. 2. **Adding entries**: Add (key, value) pairs. 3. **Retrieving entries**: Retrieve the value associated with a key. 4. **Deleting entries**: Delete (key, value) pairs. 5. **Iterating over keys**: Provide a method to iterate over all keys in the database. 6. **Synchronizing the database**: Implement synchronization to ensure data integrity. 7. **Close the database**: Proper handling of closing the database. Input and Output Formats ```python class DBMManager: def __init__(self, filename: str, flag: str = \'c\'): Initialize the DBMManager with a file and mode. :param filename: Name of the database file. :param flag: Mode for opening the database (default is \'c\'). pass def add_entry(self, key: str, value: str): Add an entry to the database. :param key: The key to be added. :param value: The value to be added. pass def get_entry(self, key: str) -> str: Retrieve an entry from the database. :param key: The key to retrieve its value. :return: The value associated with the key. pass def delete_entry(self, key: str): Delete an entry from the database. :param key: The key to be deleted. pass def iterate_keys(self): Iterate over all keys in the database and yield each key. pass def sync_database(self): Synchronize the database ensuring all operations are written to disk. pass def close_database(self): Close the database. pass ``` Constraints - Keys and values must be strings. - The class should handle any required conversion to bytes. - Proper error handling should be done for operations on a read-only database. - The database should gracefully close in case of errors or when the object is deleted. # Example Usage ```python # Initialize the database. db_manager = DBMManager(\'my_database\', \'c\') # Adding entries. db_manager.add_entry(\'key1\', \'value1\') db_manager.add_entry(\'key2\', \'value2\') # Retrieving entries. print(db_manager.get_entry(\'key1\')) # Output should be \'value1\' # Iterating over keys. for key in db_manager.iterate_keys(): print(key) # Synchronizing database. db_manager.sync_database() # Deleting entry. db_manager.delete_entry(\'key1\') # Closing database. db_manager.close_database() ``` Performance Requirements - The solution should handle reasonably large databases efficiently. - Iteration and retrieval operations should be optimal with respect to time complexity. Implement the class `DBMManager` to fulfill the described requirements.","solution":"import dbm class DBMManager: def __init__(self, filename: str, flag: str = \'c\'): Initialize the DBMManager with a file and mode. :param filename: Name of the database file. :param flag: Mode for opening the database (default is \'c\'). self.db = dbm.open(filename, flag) def add_entry(self, key: str, value: str): Add an entry to the database. :param key: The key to be added. :param value: The value to be added. self.db[key.encode(\'utf-8\')] = value.encode(\'utf-8\') def get_entry(self, key: str) -> str: Retrieve an entry from the database. :param key: The key to retrieve its value. :return: The value associated with the key. return self.db[key.encode(\'utf-8\')].decode(\'utf-8\') def delete_entry(self, key: str): Delete an entry from the database. :param key: The key to be deleted. del self.db[key.encode(\'utf-8\')] def iterate_keys(self): Iterate over all keys in the database and yield each key. for key in self.db.keys(): yield key.decode(\'utf-8\') def sync_database(self): Synchronize the database ensuring all operations are written to disk. self.db.sync() def close_database(self): Close the database. self.db.close()"},{"question":"Objective Use seaborn to create a comprehensive visual analysis of a dataset, demonstrating your understanding of core and advanced concepts. Dataset You will use the `penguins` dataset from seaborn\'s built-in datasets. Tasks 1. **Load the Dataset** - Load the `penguins` dataset using seaborn. 2. **Basic Visualization** - Create a point plot that shows the average body mass (in grams) for each island, including confidence intervals. 3. **Grouping and Color Differentiation** - Extend the previous plot by distinguishing data points based on the `sex` of the penguins using different colors. 4. **Error Bars Customization** - Modify the plot to display the error bars representing the standard deviation of the body mass for each group. 5. **Advanced Customization** - Further customize the plot as follows: - Use distinct markers and linestyles for each sex. - Set the color of all points to hex color `#4C72B0`. - Add marker edge width of 2 and line width of 2. 6. **Overplotting and Dodging** - Use the `stripplot` function combined with a point plot to show both individual data points for `bill_depth_mm` per species and the overall trend, dodging points to reduce overplotting. Submission Submit a function `visualize_penguins` that performs all the above tasks. The function should return the final seaborn plot. Expected Function Signature ```python def visualize_penguins(): # Your code here ``` Constraints - Ensure that all validation and customization are included in one function. - Use seaborn version 0.11.2 or later. Example Output The function should produce a seaborn figure with the described characteristics.","solution":"import seaborn as sns import matplotlib.pyplot as plt def visualize_penguins(): # Load the penguins dataset penguins = sns.load_dataset(\\"penguins\\") # Create a point plot of the average body mass for each island, including confidence intervals plt.figure(figsize=(12, 8)) sns.pointplot(data=penguins, x=\'island\', y=\'body_mass_g\', capsize=0.1) # Extend the plot by distinguishing data points based on the sex of the penguins using different colors sns.pointplot(data=penguins, x=\'island\', y=\'body_mass_g\', hue=\'sex\', dodge=True, markers=[\'o\', \'s\'], linestyles=[\'-\', \'--\'], palette=\'husl\', capsize=0.1) # Modify the plot to display the error bars representing the standard deviation of the body mass for each group sns.pointplot(data=penguins, x=\'island\', y=\'body_mass_g\', hue=\'sex\', dodge=True, markers=[\'o\', \'s\'], linestyles=[\'-\', \'--\'], palette=\'husl\', ci=\'sd\', capsize=0.1) # Advanced customization sns.pointplot(data=penguins, x=\'island\', y=\'body_mass_g\', hue=\'sex\', dodge=True, markers=[\'o\', \'s\'], linestyles=[\'-\', \'--\'], color=\'#4C72B0\', ci=\'sd\', capsize=0.1, markeredgewidth=2, linewidth=2) # Overplotting and dodging with stripplot and pointplot plt.figure(figsize=(12, 8)) sns.stripplot(data=penguins, x=\'species\', y=\'bill_depth_mm\', jitter=True, dodge=True, hue=\'sex\', palette=\'husl\') sns.pointplot(data=penguins, x=\'species\', y=\'bill_depth_mm\', hue=\'sex\', dodge=True, markers=[\'o\', \'s\'], linestyles=[\'-\', \'--\'], palette=\'husl\', ci=\'sd\', capsize=0.1) plt.show() return plt.gcf()"},{"question":"**Objective:** Demonstrate your understanding of the `pandas` `groupby` functionality by performing a series of data manipulations involving aggregation, transformation, and filtering. # Problem Statement You are provided with a `pandas` DataFrame containing information about sales transactions. Each row represents an individual transaction. The DataFrame has the following columns: - `ProductID`: Identifier for the product. - `StoreID`: Identifier for the store. - `Quantity`: Number of units sold. - `SalesAmount`: Total sales amount for the transaction. - `Date`: Transaction date. **Tasks:** 1. **Load the Data:** Create a DataFrame named `df` with the following data: ```python data = { \\"ProductID\\": [1, 2, 1, 3, 2, 1, 2, 3, 1, 3, 2], \\"StoreID\\": [1, 1, 2, 2, 1, 1, 2, 1, 2, 2, 1], \\"Quantity\\": [10, 15, 8, 12, 10, 15, 9, 5, 20, 8, 7], \\"SalesAmount\\": [100, 150, 80, 120, 100, 150, 90, 50, 200, 80, 70], \\"Date\\": pd.to_datetime([\\"2023-01-01\\", \\"2023-01-02\\", \\"2023-01-03\\", \\"2023-01-04\\", \\"2023-01-05\\", \\"2023-01-06\\", \\"2023-01-07\\", \\"2023-01-08\\", \\"2023-01-09\\", \\"2023-01-10\\", \\"2023-01-11\\"]) } df = pd.DataFrame(data) ``` 2. **Aggregate Sales Data:** Group the DataFrame by `ProductID` and `StoreID` and compute the following aggregations: - Total `Quantity` sold for each product at each store. - Average `SalesAmount` for each product at each store. 3. **Transform Sales Data:** For each store, compute a z-score (standard score) of the `SalesAmount` for each transaction within that store. Add a new column to the original DataFrame named `SalesZScore` containing these z-scores. 4. **Filter Transactions:** Retain only the transactions where the total sales amount for the product in the respective store exceeds 150. 5. **Combine Results:** Provide the final DataFrame containing the original columns along with the `SalesZScore` and filtered based on the sales amount criteria. # Input Format - You don\'t need to provide any input as the DataFrame is already given in the task. # Output Format Your solution should output the final `pandas` DataFrame that meets all the specified criteria. # Constraints - Use functions provided by the `pandas` library to perform the operations. - Ensure that the operations are efficient and leverage `groupby` functionalities where appropriate. # Example Given the provided data and following the tasks, the resulting DataFrame should look like this (excluding transactions not meeting filter criteria): ``` ProductID StoreID Quantity SalesAmount Date SalesZScore 0 1 1 10 100 2023-01-01 -1.264911 1 2 1 15 150 2023-01-02 0.632456 5 1 1 15 150 2023-01-06 0.632456 8 1 2 20 200 2023-01-09 1.264911 ``` Please implement your solution using the structure described above.","solution":"import pandas as pd from scipy.stats import zscore def create_dataframe(): data = { \\"ProductID\\": [1, 2, 1, 3, 2, 1, 2, 3, 1, 3, 2], \\"StoreID\\": [1, 1, 2, 2, 1, 1, 2, 1, 2, 2, 1], \\"Quantity\\": [10, 15, 8, 12, 10, 15, 9, 5, 20, 8, 7], \\"SalesAmount\\": [100, 150, 80, 120, 100, 150, 90, 50, 200, 80, 70], \\"Date\\": pd.to_datetime([\\"2023-01-01\\", \\"2023-01-02\\", \\"2023-01-03\\", \\"2023-01-04\\", \\"2023-01-05\\", \\"2023-01-06\\", \\"2023-01-07\\", \\"2023-01-08\\", \\"2023-01-09\\", \\"2023-01-10\\", \\"2023-01-11\\"]) } df = pd.DataFrame(data) return df def aggregate_sales_data(df): aggregated_df = df.groupby([\'ProductID\', \'StoreID\']).agg( TotalQuantity = (\'Quantity\', \'sum\'), AverageSalesAmount = (\'SalesAmount\', \'mean\') ).reset_index() return aggregated_df def transform_sales_data(df): df[\'SalesZScore\'] = df.groupby(\'StoreID\')[\'SalesAmount\'].transform(lambda x: zscore(x, ddof=0)) return df def filter_transactions(df): total_sales = df.groupby([\'ProductID\', \'StoreID\'])[\'SalesAmount\'].transform(\'sum\') filtered_df = df[total_sales > 150].reset_index(drop=True) return filtered_df def process_sales_data(): # Step 1: Create the DataFrame df = create_dataframe() # Step 2: Aggregate Sales Data aggregated_df = aggregate_sales_data(df) # Step 3: Transform Sales Data transformed_df = transform_sales_data(df) # Step 4: Filter Transactions filtered_df = filter_transactions(transformed_df) return filtered_df # Call the function and display the final DataFrame final_df = process_sales_data() final_df"},{"question":"**Title: Implement a Function to Compare Two Text Files** **Objective:** Your task is to implement a Python function using the `difflib` module which compares two text files line by line and returns a detailed similarity report. The report should include information about the matching blocks, operations required to convert one sequence into the other, and an overall ratio of similarity. **Requirements:** 1. Use the `SequenceMatcher` class from the `difflib` module to perform the core comparison. 2. Your function should take two file paths as input and output the following information: - Matching blocks: A list of triples (i, j, n) indicating the matching sequence blocks. - Operation codes: A list of tuples describing how to transform the first sequence into the second. - Similarity ratio: A float value indicating the similarity score between the sequences. **Function Signature:** ```python def compare_files(file_path1: str, file_path2: str) -> dict: pass ``` **Input:** - `file_path1` (string): The file path to the first text file. - `file_path2` (string): The file path to the second text file. **Output:** - A dictionary containing: - `\\"matching_blocks\\"`: A list of tuples of the form (i, j, n). - `\\"opcodes\\"`: A list of tuples describing operations needed to convert the first sequence into the second. - `\\"ratio\\"`: A float value representing the similarity ratio between the sequences. **Constraints:** - Both text files should contain individual single-line strings. - Handle files with up to 10,000 lines. **Example:** ```python # Assume file1.txt contains: # line1 # line2 # line3 # Assume file2.txt contains: # line1 # line2_changed # line3 result = compare_files(\\"file1.txt\\", \\"file2.txt\\") print(result) ``` Expected output: ```python { \\"matching_blocks\\": [(0, 0, 1), (2, 2, 1), (3, 3, 0)], \\"opcodes\\": [ (\'equal\', 0, 1, 0, 1), (\'replace\', 1, 2, 1, 2), (\'equal\', 2, 3, 2, 3) ], \\"ratio\\": 0.8 # Similarity score based on the content } ``` **Note:** Make sure to handle file reading and ensure your function performs efficiently even for large files.","solution":"import difflib def compare_files(file_path1: str, file_path2: str) -> dict: with open(file_path1, \'r\') as file1, open(file_path2, \'r\') as file2: lines1 = file1.readlines() lines2 = file2.readlines() matcher = difflib.SequenceMatcher(None, lines1, lines2) matching_blocks = matcher.get_matching_blocks() opcodes = matcher.get_opcodes() ratio = matcher.ratio() return { \\"matching_blocks\\": matching_blocks, \\"opcodes\\": opcodes, \\"ratio\\": ratio }"},{"question":"# Advanced Python Functionality with `functools` Implement a cache-enabled pipeline system that performs multiple transformations on a dataset using partial function application. You must ensure that each unique transformation step is only computed once using caching. Also, the pipeline must be flexible with different types of transformations and configurations, utilizing partial function application to bind specific settings to transformation functions. Problem Statement Implement a `DataPipeline` class that: 1. Accepts a sequence of transformer functions when initialized. 2. Applies these transformations sequentially to any dataset provided to its `process` method. 3. Uses caching to ensure that the same transformation with identical arguments is not recomputed. 4. Facilitates partial application by allowing easy extension of new transformers with pre-configured settings. Requirements - You must utilize `functools.cache` or `functools.lru_cache` to achieve caching. - Use `functools.partial` to create partially applied transformer functions. - Provide the ability to clear the cache. Class Implementation: `DataPipeline` - **Initializer (`__init__`)** - Accepts a list of transformer functions. - **Method `process`** - Takes a dataset and sequentially applies the transformer functions. - **Caching** - Ensure that each unique transformation step is cached. - **Method `clear_cache`** - Clears the transformation cache. Transformer Function Specification Transformer functions are simple functions that take a dataset and optional configuration parameters. For example: ```python def multiply(data, factor): return [x * factor for x in data] def add(data, increment): return [x + increment for x in data] ``` Example Usage ```python from functools import cache, partial class DataPipeline: def __init__(self, transformers): # Initialize with the list of transformer functions self.transformers = transformers @cache def process(self, data): # Sequentially apply transformation functions for transformer in self.transformers: data = transformer(data) return data def clear_cache(self): # Clear the cache self.process.cache_clear() # Define transformers multiply_by_2 = partial(multiply, factor=2) add_5 = partial(add, increment=5) # Create a pipeline with these transformers pipeline = DataPipeline([multiply_by_2, add_5]) # Process data result = pipeline.process([1, 2, 3]) print(result) # Output: [7, 9, 11] pipeline.clear_cache() # Clear the cache ``` Constraints - All transformer functions will only take lists of integers and return lists of integers. - Transformer functions must be hashable to allow for caching. Implement the `DataPipeline` class in Python based on the given specifications.","solution":"from functools import lru_cache, partial class DataPipeline: def __init__(self, transformers): Initialize with the list of transformer functions. self.transformers = transformers @lru_cache(maxsize=None) def _process(self, data): Internal method to process the data with caching. for transformer in self.transformers: data = transformer(data) return data def process(self, data): External interface to process the data. We convert the list to tuple to make it hashable for caching. data_tuple = tuple(data) return list(self._process(data_tuple)) def clear_cache(self): Clear the transformation cache. self._process.cache_clear() # Example transformers def multiply(data, factor): return [x * factor for x in data] def add(data, increment): return [x + increment for x in data] # Example usage multiply_by_2 = partial(multiply, factor=2) add_5 = partial(add, increment=5) pipeline = DataPipeline([multiply_by_2, add_5]) result = pipeline.process([1, 2, 3]) print(result) # Output: [7, 9, 11] pipeline.clear_cache() # Clear the cache"},{"question":"Coding Assessment Question # Objective: Design a command-line utility using the deprecated `optparse` module. The aim is to ensure that students understand how to define, parse, and handle command-line options using `optparse`. # Task: Create a Python script using the `optparse` module that processes the following command-line options: 1. `-c` or `--config`: Accepts a file path as a string to specify a configuration file. 2. `-v` or `--verbose`: A flag option that enables verbose output. 3. `-n`: Accepts an integer value and is used to specify a certain number of iterations. 4. `--add`: Accepts a comma-separated list of integers and stores the extended list. 5. `-q` or `--quiet`: If set, suppresses all output (overrides the `--verbose` flag if both are set). Additionally, extend the `optparse` module to support a new action `==multiply` that accepts two integer arguments separated by a comma and stores their product. Use this new action for a command-line option `--multiply`. # Specifications: 1. **Input Format**: - `-c <filepath>` or `--config <filepath>` - `-v` or `--verbose` - `-n <integer>` - `--add <int1,int2,...>` - `-q` or `--quiet` - `--multiply <int1,int2>` 2. **Output**: Print the parsed values of all options unless `--quiet` is given. Handle errors gracefully and provide meaningful error messages when invalid options or arguments are supplied. # Constraints: - Python version: Ensure the code runs correctly in Python 3.6+ since `optparse` is deprecated but still available. - Performance: The script should handle large input lists efficiently for the `--add` option. # Example: ```bash python yourscript.py -c config.txt -v -n 10 --add 1,2,3 --multiply 4,5 ``` Output: ``` Config file: config.txt Verbose: True Iterations: 10 Add List: [1, 2, 3] Multiply Result: 20 ``` # Instructions: 1. Define the command-line options as described using the `optparse` module. 2. Implement a custom action for `--multiply` using the extended method for creating new option actions. 3. Handle conflicts where `-v` and `-q` might both be provided, with `-q` having precedence. 4. Implement error handling to manage invalid options or arguments.","solution":"import optparse import sys # Custom action to handle `--multiply` class MultiplyAction(optparse.Option): ACTIONS = optparse.Option.ACTIONS + (\'==multiply\',) STORE_ACTIONS = optparse.Option.STORE_ACTIONS + (\'==multiply\',) TYPED_ACTIONS = optparse.Option.TYPED_ACTIONS + (\'==multiply\',) ALWAYS_TYPED_ACTIONS = optparse.Option.ALWAYS_TYPED_ACTIONS + (\'==multiply\',) def take_action(self, action, dest, opt, value, values, parser): if action == \'==multiply\': try: int1, int2 = map(int, value.split(\',\')) result = int1 * int2 setattr(values, dest, result) except ValueError: raise optparse.OptionValueError( f\\"option {opt}: invalid integers for multiplication: {value}\\") else: optparse.Option.take_action(self, action, dest, opt, value, values, parser) def parse_arguments(): parser = optparse.OptionParser(option_class=MultiplyAction) parser.add_option(\'-c\', \'--config\', dest=\'config\', help=\'Configuration file path\', type=\'string\') parser.add_option(\'-v\', \'--verbose\', action=\'store_true\', dest=\'verbose\', default=False, help=\'Enable verbose output\') parser.add_option(\'-q\', \'--quiet\', action=\'store_true\', dest=\'quiet\', default=False, help=\'Suppress all output\') parser.add_option(\'-n\', dest=\'iterations\', help=\'Number of iterations\', type=\'int\') parser.add_option(\'--add\', dest=\'add\', help=\'Comma-separated list of integers to add\', type=\'string\') parser.add_option(\'--multiply\', dest=\'multiply\', action=\'==multiply\', help=\'Multiply two integers separated by a comma\') (options, args) = parser.parse_args() if options.quiet: return options if options.config: print(f\\"Config file: {options.config}\\") if options.verbose: print(f\\"Verbose: {options.verbose}\\") if options.iterations: print(f\\"Iterations: {options.iterations}\\") if options.add: try: add_list = list(map(int, options.add.split(\',\'))) print(f\\"Add List: {add_list}\\") except ValueError: print(\\"Error: The --add option requires a comma-separated list of integers.\\") if options.multiply: print(f\\"Multiply Result: {options.multiply}\\") return options if __name__ == \\"__main__\\": parse_arguments()"},{"question":"# Seaborn Plotting Context Application You are tasked with creating a function `create_plots_with_contexts` that demonstrates the use of Seaborn\'s plotting contexts. You will utilize a dataset from Seaborn\'s sample datasets. Function Signature ```python def create_plots_with_contexts(): # Your code here ``` Description 1. **Import the necessary libraries**: - `seaborn as sns` - `matplotlib.pyplot as plt` - `pandas as pd` 2. **Load the \'tips\' dataset from Seaborn\'s sample datasets**: 3. **Create a function `create_plots_with_contexts` that**: - Retrieves and prints the current default plotting context. - Changes the plotting context to \\"talk\\" and creates a line plot for the total bill over time. - Uses `sns.plotting_context` as a context manager to temporarily change the context to \\"paper\\" and within this block, creates another plot showing the relationship between total bill and tip. - Reverts to the original context and creates a final plot showing the distribution of total bill amounts. Expected Output - Three plots should be generated: 1. A line plot with the \\"talk\\" context. 2. A scatter plot between total bill and tips with \\"paper\\" context applied temporarily. 3. A distribution plot (original context). Notes - The `plt.show()` function should be used to render the plots. - There should be clear evidence that different contexts are applied by observing the plot appearances (titles, labels). Example usage: ```python create_plots_with_contexts() ``` The function should not return any value but should generate and show the specified plots.","solution":"import seaborn as sns import matplotlib.pyplot as plt import pandas as pd def create_plots_with_contexts(): # Load the \'tips\' dataset tips = sns.load_dataset(\'tips\') # Print the current default plotting context original_context = sns.plotting_context() print(\\"Original Context:\\", original_context) # Change the plotting context to \\"talk\\" sns.set_context(\\"talk\\") # Create a line plot for total bill over time plt.figure() sns.lineplot(data=tips, x=tips.index, y=\'total_bill\') plt.title(\'Total Bill Over Time (talk context)\') plt.show() # Temporarily change the context to \\"paper\\" and create another plot with sns.plotting_context(\\"paper\\"): plt.figure() sns.scatterplot(data=tips, x=\'total_bill\', y=\'tip\') plt.title(\'Total Bill vs Tip (paper context)\') plt.show() # Revert to original context and create a final plot sns.set_context(original_context) plt.figure() sns.histplot(data=tips, x=\'total_bill\', kde=True) plt.title(\'Distribution of Total Bill (original context)\') plt.show()"}]'),q={name:"App",components:{PoemCard:A},data(){return{searchQuery:"",visibleCount:4,poemsData:I,isLoading:!1}},computed:{filteredPoems(){const i=this.searchQuery.trim().toLowerCase();return i?this.poemsData.filter(e=>e.question&&e.question.toLowerCase().includes(i)||e.solution&&e.solution.toLowerCase().includes(i)):this.poemsData},displayedPoems(){return this.searchQuery.trim()?this.filteredPoems:this.filteredPoems.slice(0,this.visibleCount)},hasMorePoems(){return!this.searchQuery.trim()&&this.visibleCount<this.poemsData.length}},methods:{async loadMore(){this.isLoading=!0,await new Promise(i=>setTimeout(i,1e3)),this.visibleCount+=4,this.isLoading=!1}}},F={class:"search-container"},z={class:"card-container"},D={key:0,class:"empty-state"},R=["disabled"],N={key:0},O={key:1};function L(i,e,l,m,s,o){const h=g("PoemCard");return a(),n("section",null,[e[4]||(e[4]=t("div",{class:"top-banner"},[t("div",{class:"top-banner-title"},[t("div",{class:"top-banner-title-text"},"🤔prompts chat🧠")])],-1)),t("div",F,[e[3]||(e[3]=t("span",{class:"search-icon"},"🔍",-1)),_(t("input",{type:"text",class:"search-input","onUpdate:modelValue":e[0]||(e[0]=r=>s.searchQuery=r),placeholder:"Search..."},null,512),[[y,s.searchQuery]]),s.searchQuery?(a(),n("button",{key:0,class:"clear-search",onClick:e[1]||(e[1]=r=>s.searchQuery="")}," ✕ ")):d("",!0)]),t("div",z,[(a(!0),n(b,null,v(o.displayedPoems,(r,f)=>(a(),w(h,{key:f,poem:r},null,8,["poem"]))),128)),o.displayedPoems.length===0?(a(),n("div",D,' No results found for "'+u(s.searchQuery)+'". ',1)):d("",!0)]),o.hasMorePoems?(a(),n("button",{key:0,class:"load-more-button",disabled:s.isLoading,onClick:e[2]||(e[2]=(...r)=>o.loadMore&&o.loadMore(...r))},[s.isLoading?(a(),n("span",O,"Loading...")):(a(),n("span",N,"See more"))],8,R)):d("",!0)])}const j=p(q,[["render",L],["__scopeId","data-v-7095c5a4"]]),Y=JSON.parse('{"title":"","description":"","frontmatter":{"page":true},"headers":[],"relativePath":"deepseek/62.md","filePath":"deepseek/62.md"}'),M={name:"deepseek/62.md"},H=Object.assign(M,{setup(i){return(e,l)=>(a(),n("div",null,[x(j)]))}});export{Y as __pageData,H as default};
