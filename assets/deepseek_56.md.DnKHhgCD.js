import{_ as p,o as a,c as s,a as t,m as c,t as u,C as g,M as _,U as y,f as d,F as b,p as v,e as w,q as x}from"./chunks/framework.B1z0IdBH.js";const k={name:"PoemCard",props:{poem:{type:Object,required:!0}}},T={class:"poem-container"},C={class:"review"},P={class:"review-title"},S={class:"review-content"};function E(o,e,l,m,i,n){return a(),s("div",T,[t("div",C,[t("div",P,[e[0]||(e[0]=t("span",{class:"icon"},"question:",-1)),c(u(l.poem.question),1)]),e[2]||(e[2]=t("div",{class:"divider"},null,-1)),t("p",S,[e[1]||(e[1]=t("span",{class:"icon"},"answer:",-1)),c(u(l.poem.solution),1)])])])}const A=p(k,[["render",E],["__scopeId","data-v-2d889243"]]),I=JSON.parse('[{"question":"**Coding Assessment Question** **Objective:** To demonstrate your understanding of pandas MultiIndex, you are required to perform a series of data manipulation tasks using a hierarchical index. **Scenario:** You are provided with a dataset that tracks the monthly sales performance of different products across various regions. The dataset is represented in a pandas DataFrame with a hierarchical index. **Tasks:** 1. **Data Initialization:** - Create a DataFrame with the following multi-level index and columns: - Index Levels: - \'Region\': \'North\', \'South\', \'East\', \'West\' - \'Month\': \'January\', \'February\', \'March\' - Columns: \'Product_A\', \'Product_B\', \'Product_C\' The DataFrame should be initialized with the provided sales data: ``` Region Month Product_A Product_B Product_C North January 200 150 210 February 180 160 200 March 160 170 180 South January 220 140 230 February 210 150 220 March 190 140 210 East January 150 120 160 February 140 110 150 March 130 100 140 West January 210 180 220 February 200 190 210 March 190 180 200 ``` 2. **Data Analysis and Manipulation:** a. Calculate the total sales for each product across all regions for each month. b. Determine the region with the highest sales for \'Product_A\' in February. c. Reconstruct the DataFrame while keeping only the levels that have non-zero sales. d. Using `DataFrame.xs`, extract the sales data for \'February\' across all regions. e. Implement a function to add a new column \'Total_Sales\' that sums up the sales of \'Product_A\', \'Product_B\', and \'Product_C\' for each index level. **Function Implementation:** ```python import pandas as pd import numpy as np # Function to initialize the dataset def initialize_sales_data(): arrays = [ [\'North\', \'North\', \'North\', \'South\', \'South\', \'South\', \'East\', \'East\', \'East\', \'West\', \'West\', \'West\'], [\'January\', \'February\', \'March\', \'January\', \'February\', \'March\', \'January\', \'February\', \'March\', \'January\', \'February\', \'March\'] ] index = pd.MultiIndex.from_arrays(arrays, names=(\'Region\', \'Month\')) data = { \'Product_A\': [200, 180, 160, 220, 210, 190, 150, 140, 130, 210, 200, 190], \'Product_B\': [150, 160, 170, 140, 150, 140, 120, 110, 100, 180, 190, 180], \'Product_C\': [210, 200, 180, 230, 220, 210, 160, 150, 140, 220, 210, 200], } df = pd.DataFrame(data, index=index) return df # Function to calculate total sales for each product across all regions for each month def total_sales_per_product(df): return df.groupby(level=\'Month\').sum() # Function to determine the region with the highest sales for \'Product_A\' in February def highest_sales_region_february(df): return df.loc[(slice(None), \'February\'), \'Product_A\'].idxmax()[0] # Function to reconstruct the DataFrame while keeping only used levels def reconstruct_df(df): return df.loc[(df != 0).any(axis=1)].copy() # Function to extract sales data for \'February\' across all regions using `DataFrame.xs` def extract_february_data(df): return df.xs(\'February\', level=\'Month\') # Function to add a new column \'Total_Sales\' def add_total_sales_column(df): df[\'Total_Sales\'] = df.sum(axis=1) return df # Main function to execute the tasks def main(): df = initialize_sales_data() print(\\"Original DataFrame:\\") print(df) total_sales = total_sales_per_product(df) print(\\"nTotal sales per product for each month:\\") print(total_sales) highest_sales_region = highest_sales_region_february(df) print(f\\"nRegion with the highest sales for \'Product_A\' in February: {highest_sales_region}\\") reconstructed_df = reconstruct_df(df) print(\\"nReconstructed DataFrame:\\") print(reconstructed_df) february_data = extract_february_data(df) print(\\"nSales data for February across all regions:\\") print(february_data) df_with_total_sales = add_total_sales_column(df) print(\\"nDataFrame with \'Total_Sales\' column added:\\") print(df_with_total_sales) if __name__ == \\"__main__\\": main() ``` **Constraints:** - You are allowed to use only the pandas library for data manipulation. - Ensure your implementation is efficient and leverages pandas\' capabilities for hierarchical indexing. **Expected Output:** - A DataFrame showing the total sales for each product across all regions for each month. - The region with the highest sales for \'Product_A\' in February. - A reconstructed DataFrame with only used levels retained. - A DataFrame that shows the sales data for February across all regions. - The original DataFrame with an added \'Total_Sales\' column.","solution":"import pandas as pd # Function to initialize the dataset def initialize_sales_data(): arrays = [ [\'North\', \'North\', \'North\', \'South\', \'South\', \'South\', \'East\', \'East\', \'East\', \'West\', \'West\', \'West\'], [\'January\', \'February\', \'March\', \'January\', \'February\', \'March\', \'January\', \'February\', \'March\', \'January\', \'February\', \'March\'] ] index = pd.MultiIndex.from_arrays(arrays, names=(\'Region\', \'Month\')) data = { \'Product_A\': [200, 180, 160, 220, 210, 190, 150, 140, 130, 210, 200, 190], \'Product_B\': [150, 160, 170, 140, 150, 140, 120, 110, 100, 180, 190, 180], \'Product_C\': [210, 200, 180, 230, 220, 210, 160, 150, 140, 220, 210, 200], } df = pd.DataFrame(data, index=index) return df # Function to calculate total sales for each product across all regions for each month def total_sales_per_product(df): return df.groupby(level=\'Month\').sum() # Function to determine the region with the highest sales for \'Product_A\' in February def highest_sales_region_february(df): return df.loc[(slice(None), \'February\'), \'Product_A\'].idxmax()[0] # Function to reconstruct the DataFrame while keeping only levels that have non-zero sales def reconstruct_df(df): return df.loc[(df != 0).any(axis=1)].copy() # Function to extract sales data for \'February\' across all regions using `DataFrame.xs` def extract_february_data(df): return df.xs(\'February\', level=\'Month\') # Function to add a new column \'Total_Sales\' def add_total_sales_column(df): df[\'Total_Sales\'] = df.sum(axis=1) return df"},{"question":"# **Partial Least Squares Canonical (PLSCanonical) Implementation** You are requested to implement the `PLSCanonical` algorithm from scratch, leveraging your understanding of the iterative process detailed in the provided documentation. The goal is to demonstrate the dimensionality reduction and the associated projection of X and Y matrices to maximize the covariance. **Function Signature:** ```python def plscanonical(X: np.ndarray, Y: np.ndarray, n_components: int) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]: pass ``` **Input:** - `X` (numpy.ndarray): Input predictor matrix X of shape `(n_samples, n_features)`, standardized (mean = 0 and variance = 1 for each feature). - `Y` (numpy.ndarray): Input response matrix Y of shape `(n_samples, n_targets)`, standardized (mean = 0 and variance = 1 for each target). - `n_components` (int): Number of PLS components to extract. **Output:** The function should return a tuple containing: - `x_weights` (numpy.ndarray): The weights for the X matrix of shape `(n_features, n_components)`. - `y_weights` (numpy.ndarray): The weights for the Y matrix of shape `(n_targets, n_components)`. - `x_scores` (numpy.ndarray): The scores for the X matrix of shape `(n_samples, n_components)`. - `y_scores` (numpy.ndarray): The scores for the Y matrix of shape `(n_samples, n_components)`. - `x_loadings` (numpy.ndarray): The loadings for the X matrix of shape `(n_features, n_components)`. - `y_loadings` (numpy.ndarray): The loadings for the Y matrix of shape `(n_targets, n_components)`. **Implementation Steps:** 1. Initialize `X_k` to `X` and `Y_k` to `Y`. 2. For each of the n_components: - Compute cross-covariance matrix `C = X_k.T @ Y_k`. - Perform SVD on C to find the first left and right singular vectors (weights `u_k` and `v_k`). - Project `X_k` and `Y_k` on weights to get scores `xi_k` and `omega_k`. - Compute loadings `gamma_k` and `delta_k` for the scores in X and Y. - Deflate `X_k` and `Y_k`. 3. Return the set of weights, scores, and loadings obtained by the iterative process. **Example:** ```python import numpy as np # Sample data X = np.array([[0.5, 0.6, 0.7], [0.1, 0.2, 0.3], [0.7, 0.8, 0.9]]) Y = np.array([[1.0, 1.1], [0.0, 0.1], [0.9, 1.0]]) n_components = 2 # Call the function x_weights, y_weights, x_scores, y_scores, x_loadings, y_loadings = plscanonical(X, Y, n_components) # Display results print(\\"X Weights:n\\", x_weights) print(\\"Y Weights:n\\", y_weights) print(\\"X Scores:n\\", x_scores) print(\\"Y Scores:n\\", y_scores) print(\\"X Loadings:n\\", x_loadings) print(\\"Y Loadings:n\\", y_loadings) ``` Implement and submit your solution for evaluating the fundamental and advanced comprehension of the PLSCanonical algorithm.","solution":"import numpy as np from typing import Tuple def plscanonical(X: np.ndarray, Y: np.ndarray, n_components: int) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]: n_samples, n_features = X.shape n_targets = Y.shape[1] x_weights = np.zeros((n_features, n_components)) y_weights = np.zeros((n_targets, n_components)) x_scores = np.zeros((n_samples, n_components)) y_scores = np.zeros((n_samples, n_components)) x_loadings = np.zeros((n_features, n_components)) y_loadings = np.zeros((n_targets, n_components)) X_k = X Y_k = Y for k in range(n_components): C = np.dot(X_k.T, Y_k) U, S, VT = np.linalg.svd(C, full_matrices=False) u_k = U[:, 0] v_k = VT.T[:, 0] x_weights[:, k] = u_k y_weights[:, k] = v_k xi_k = np.dot(X_k, u_k) omega_k = np.dot(Y_k, v_k) x_scores[:, k] = xi_k y_scores[:, k] = omega_k gamma_k = np.dot(X_k.T, xi_k) / np.dot(xi_k.T, xi_k) delta_k = np.dot(Y_k.T, omega_k) / np.dot(omega_k.T, omega_k) x_loadings[:, k] = gamma_k y_loadings[:, k] = delta_k X_k -= np.outer(xi_k, gamma_k) Y_k -= np.outer(omega_k, delta_k) return x_weights, y_weights, x_scores, y_scores, x_loadings, y_loadings"},{"question":"# **Unicode String Manipulation in Python** **Objective:** You are tasked with implementing a set of functions that leverage low-level Unicode manipulation capabilities in Python. Your solution will demonstrate proficiency in working with Unicode objects, encoding/decoding, and performing various Unicode-related operations. # **Function Specifications** 1. **Function: `unicode_to_utf8`** - **Input**: A Unicode string. - **Output**: A UTF-8 encoded byte string. - **Description**: Convert the given Unicode string to its UTF-8 encoded form. ```python def unicode_to_utf8(unicode_str: str) -> bytes: Convert a given Unicode string to its UTF-8 encoded byte string. :param unicode_str: The input Unicode string. :return: The UTF-8 encoded byte string. pass ``` 2. **Function: `utf8_to_unicode`** - **Input**: A UTF-8 encoded byte string. - **Output**: A Unicode string. - **Description**: Convert the given UTF-8 encoded byte string back to its Unicode string form. ```python def utf8_to_unicode(utf8_bytes: bytes) -> str: Convert a given UTF-8 encoded byte string back to a Unicode string. :param utf8_bytes: The input UTF-8 encoded byte string. :return: The resulting Unicode string. pass ``` 3. **Function: `is_unicode_digit`** - **Input**: A single Unicode character. - **Output**: A boolean. - **Description**: Check if the given Unicode character represents a digit. ```python def is_unicode_digit(unicode_char: str) -> bool: Check if the given Unicode character is a digit. :param unicode_char: The Unicode character to check. :return: `True` if the character represents a digit, `False` otherwise. pass ``` 4. **Function: `unicode_properties`** - **Input**: A Unicode string. - **Output**: A dictionary where the keys are characters, and the values are dictionaries containing properties (`is_digit`, `is_alpha`, `is_printable`). - **Description**: For each character in the Unicode string, determine whether it is a digit, alphabetic, and printable. ```python def unicode_properties(unicode_str: str) -> dict: Retrieve properties of each character in the Unicode string. :param unicode_str: The input Unicode string. :return: A dictionary with characters as keys and dictionaries of properties as values. pass ``` # **Constraints** - For `unicode_to_utf8` and `utf8_to_unicode`, use the correct encoding and decoding mechanisms. - For `is_unicode_digit` and `unicode_properties`, make use of the provided Unicode-related functions to determine character properties. # **Examples** ```python # Example 1 print(unicode_to_utf8(\\"Hello, World!\\")) # Expected output: b\'Hello, World!\' print(utf8_to_unicode(b\'Hello, World!\')) # Expected output: \'Hello, World!\' # Example 2 print(is_unicode_digit(\'5\')) # Expected output: True print(is_unicode_digit(\'m\')) # Expected output: False # Example 3 unicode_str = \\"Hello, 123!\\" properties = unicode_properties(unicode_str) print(properties[\'1\'][\'is_digit\']) # Expected output: True print(properties[\'H\'][\'is_alpha\']) # Expected output: True print(properties[\'!\'][\'is_printable\']) # Expected output: True ``` # **Performance Requirements** - Ensure that your functions handle typical Unicode strings efficiently. - Avoid using deprecated APIs. Focus on using the efficient, modern methods described in the documentation. Good luck!","solution":"def unicode_to_utf8(unicode_str: str) -> bytes: Convert a given Unicode string to its UTF-8 encoded byte string. :param unicode_str: The input Unicode string. :return: The UTF-8 encoded byte string. return unicode_str.encode(\'utf-8\') def utf8_to_unicode(utf8_bytes: bytes) -> str: Convert a given UTF-8 encoded byte string back to a Unicode string. :param utf8_bytes: The input UTF-8 encoded byte string. :return: The resulting Unicode string. return utf8_bytes.decode(\'utf-8\') def is_unicode_digit(unicode_char: str) -> bool: Check if the given Unicode character is a digit. :param unicode_char: The Unicode character to check. :return: `True` if the character represents a digit, `False` otherwise. return unicode_char.isdigit() def unicode_properties(unicode_str: str) -> dict: Retrieve properties of each character in the Unicode string. :param unicode_str: The input Unicode string. :return: A dictionary with characters as keys and dictionaries of properties as values. return {char: { \'is_digit\': char.isdigit(), \'is_alpha\': char.isalpha(), \'is_printable\': char.isprintable() } for char in unicode_str}"},{"question":"<|Analysis Begin|> The provided documentation gives an overview of the `torch.fx` module, specifically focusing on Graph manipulation, the structure of Graphs, Node instances, and various methods for transforming, rewriting, and analyzing model graphs. The documentation includes examples of creating new GraphModules, modifying existing Graphs, and replacing patterns within Graphs. Key concepts covered in the documentation: 1. **FX Transforms** - General structure of a transform function which takes a `torch.nn.Module`, traces it to get a `Graph`, modifies it, and returns a new `GraphModule`. 2. **Graph Basics** - Representation of methods using `Graph` and `Node` instances. 3. **Graph Manipulation** - Techniques for modifying Graphs directly or using proxy objects. 4. **Subgraph Rewriting** - Utility to perform pattern-based replacement within a graph. 5. **Transforming and Interpreting Graphs** - Usage of the Interpreter design pattern to run or transform Graphs. 6. **Debugging** - Techniques for debugging graph transformations by printing, using `pdb`, or copying generated code. Given this detailed overview and practical examples, we can design a challenging question that assesses a student\'s ability to: - Trace a `torch.nn.Module`. - Manipulate its `Graph`. - Implement a specific transformation. <|Analysis End|> <|Question Begin|> # Coding Assessment Question: Custom FX Transformation Objective: Implement a custom FX transformation that replaces all instances of the `torch.nn.functional.gelu` activation function with `torch.nn.functional.relu` in a given `torch.nn.Module`. Background: In many scenarios, you may want to modify your neural network\'s structure post-hoc for various reasons, such as improving inference speed or utilizing a different activation function for experimentation. The `torch.fx` module provides utilities to enable such modifications by representing the computation of `torch.nn.Module` in the form of a Graph and allowing for transformations on this Graph. Task: Write a function `replace_gelu_with_relu` that accepts an input `torch.nn.Module` and returns a new `torch.nn.Module` where all instances of the `gelu` function are replaced with the `relu` function. Your function should: 1. Symbolically trace the input module to acquire its Graph. 2. Traverse and modify the Graph to replace `gelu` with `relu`. 3. Return a new `GraphModule` with the modified Graph. Expected Function Signature: ```python import torch import torch.fx def replace_gelu_with_relu(model: torch.nn.Module) -> torch.nn.Module: pass ``` Example Usage: ```python import torch import torch.nn as nn import torch.nn.functional as F import torch.fx class MyModule(nn.Module): def __init__(self): super(MyModule, self).__init__() self.linear = nn.Linear(4, 4) def forward(self, x): x = self.linear(x) x = F.gelu(x) # gelu will be replaced return x model = MyModule() print(\\"Original Model:\\") print(model) transformed_model = replace_gelu_with_relu(model) print(\\"nTransformed Model:\\") print(transformed_model) ``` Constraints: - You can assume the module does not use dynamic control flow. - The function should maintain the original module\'s substructure and parameters. Performance Criteria: - **Correctness:** The transformed model should function correctly with no runtime errors. - **Completeness:** All `gelu` activations must be replaced with `relu`. - **Clarity:** The code should be well-organized and easy to understand. Hints: - Utilize the `torch.fx.symbolic_trace` function to trace the model and acquire its Graph. - Iterate through the nodes of the Graph to identify and replace `gelu` calls. - Construct and return a new `GraphModule` based on the modified Graph. Good luck!","solution":"import torch import torch.nn.functional as F import torch.fx def replace_gelu_with_relu(model: torch.nn.Module) -> torch.nn.Module: class ReplaceGeluWithRelu(torch.fx.Transformer): def call_function(self, target, args, kwargs): if target == F.gelu: target = F.relu return super().call_function(target, args, kwargs) traced = torch.fx.symbolic_trace(model) transformer = ReplaceGeluWithRelu(traced) transformed_graph = transformer.transform() return torch.fx.GraphModule(transformed_graph, transformed_graph.graph) # Example usage: class MyModule(torch.nn.Module): def __init__(self): super(MyModule, self).__init__() self.linear = torch.nn.Linear(4, 4) def forward(self, x): x = self.linear(x) x = F.gelu(x) # gelu will be replaced return x model = MyModule() print(\\"Original Model:\\") print(model) transformed_model = replace_gelu_with_relu(model) print(\\"nTransformed Model:\\") print(transformed_model)"},{"question":"**Objective:** Design and implement an asynchronous system where multiple clients can connect to a server concurrently to fetch data from a shared resource, ensuring thread-safe access using synchronization primitives. Task Description You need to implement an asynchronous TCP server and client system in Python 3.10 using the asyncio library. The server should manage a shared counter that clients can increment by sending a specific command. Proper synchronization must be ensured to prevent race conditions. Requirements 1. **Server (`asyncioTCPServer`):** - The server should listen on a specified host and port. - Use an `asyncio.Lock` to guard the shared counter variable. - Provide two commands that clients can send: 1. `increment`: Increments the shared counter. 2. `fetch`: Returns the current value of the shared counter. 2. **Client (`asyncioTCPClient`):** - The client should be able to connect to the server. - Provide functions to send `increment` and `fetch` commands to the server and handle responses. 3. **Concurrency:** - Multiple clients should be able to connect to the server and perform operations concurrently. - Use `asyncio.create_task` to handle each client connection asynchronously. 4. **Synchronization:** - Use an `asyncio.Lock` to ensure that the operations on the shared counter are thread-safe. 5. **Function Signatures:** - `asyncioTCPServer`: ```python async def asyncioTCPServer(host: str, port: int) -> None: pass ``` - `asyncioTCPClient`: ```python async def asyncioTCPClient(host: str, port: int, commands: List[str]) -> List[str]: pass ``` Input and Output Formats - The `asyncioTCPServer` function should take a hostname (`host`) and a port number (`port`), then run indefinitely. - The `asyncioTCPClient` function should take the same hostname and port number, and `commands` - a list of commands to send to the server. - The `commands` list can contain combinations of `\\"increment\\"` and `\\"fetch\\"` commands. - The function should return a list of responses received from the server. Example ```python import asyncio async def main(): server_task = asyncio.create_task(asyncioTCPServer(\'127.0.0.1\', 8888)) await asyncio.sleep(1) # Give the server a moment to start client_task1 = asyncio.create_task(asyncioTCPClient(\'127.0.0.1\', 8888, [\'increment\', \'fetch\'])) client_task2 = asyncio.create_task(asyncioTCPClient(\'127.0.0.1\', 8888, [\'fetch\', \'increment\', \'fetch\'])) responses1 = await client_task1 responses2 = await client_task2 print(\\"Client 1 responses:\\", responses1) print(\\"Client 2 responses:\\", responses2) server_task.cancel() # Stop the server after the clients are done asyncio.run(main()) ``` - `Client 1 responses` should show something like `[\'incremented\', \'1\']` - `Client 2 responses` should show something like `[\'1\', \'incremented\', \'2\']` Constraints - Ensure your solution handles multiple clients without data races. - The server should handle unexpected input gracefully by returning an appropriate error message.","solution":"import asyncio class AsyncioTCPServer: def __init__(self, host, port): self.host = host self.port = port self.counter = 0 self.lock = asyncio.Lock() async def handle_client(self, reader, writer): addr = writer.get_extra_info(\'peername\') while True: data = await reader.read(100) message = data.decode().strip() if message == \'increment\': async with self.lock: self.counter += 1 response = \'incremented\' elif message == \'fetch\': async with self.lock: response = str(self.counter) else: response = \'error: unknown command\' writer.write(response.encode()) await writer.drain() writer.close() await writer.wait_closed() async def run_server(self): server = await asyncio.start_server( self.handle_client, self.host, self.port ) async with server: await server.serve_forever() async def asyncioTCPServer(host: str, port: int) -> None: server = AsyncioTCPServer(host, port) await server.run_server() class AsyncioTCPClient: def __init__(self, host, port): self.host = host self.port = port async def send_command(self, command): reader, writer = await asyncio.open_connection(self.host, self.port) writer.write(command.encode()) await writer.drain() data = await reader.read(100) response = data.decode() writer.close() await writer.wait_closed() return response async def asyncioTCPClient(host: str, port: int, commands: list) -> list: client = AsyncioTCPClient(host, port) responses = [] for command in commands: response = await client.send_command(command) responses.append(response) return responses"},{"question":"Coding Assessment Question # Objective To demonstrate comprehension and application of various preprocessing techniques using scikit-learn\'s `sklearn.preprocessing` module. # Problem Statement You are provided with a dataset containing both numerical and categorical features. Your task is to preprocess the dataset using a sequence of transformations: 1. **Standardize** the numerical features. 2. **Encode** the categorical features using One-Hot Encoding. 3. **Handle** missing values in both numerical and categorical columns. 4. **Scale** numerical features to a range of [0, 1]. 5. **Transform** to map data to normal distribution. # Dataset You are given a dataset in a dictionary format with `pandas.DataFrame` as follows: ```python import pandas as pd import numpy as np data = { \'num_feature1\': [1.0, np.nan, 2.5, 3.0], \'num_feature2\': [4.0, 2.0, np.nan, 5.0], \'cat_feature1\': [\'A\', \'B\', \'B\', \'C\'], \'cat_feature2\': [\'X\', np.nan, \'Y\', \'Y\'] } df = pd.DataFrame(data) ``` # Task 1. **Standardize** `num_feature1` and `num_feature2`. 2. **Impute** missing values for `num_feature1` and `num_feature2` with their respective means. 3. **One-Hot Encode** `cat_feature1` and `cat_feature2`. 4. **Impute** missing values for `cat_feature2` with a placeholder \'missing\'. 5. **Scale** all numerical features to the range [0, 1]. 6. **Transform** the features to a normal distribution using Quantile Transformation. # Implementation 1. Define the function `preprocess_data(df: pd.DataFrame) -> pd.DataFrame`. 2. The function should take a DataFrame `df` and return a processed DataFrame. 3. Document your steps clearly through comments in your solution. # Constraints - Use scikit-learn\'s preprocessing tools. - Handle missing values appropriately. - Scale features as per the requirements. # Expected Output The function should return a DataFrame where: - Numerical features are standardized, imputed, scaled to [0, 1], and transformed to follow a normal distribution. - Categorical features are one-hot encoded with missing values appropriately handled. # Example ```python import pandas as pd import numpy as np from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder, QuantileTransformer from sklearn.impute import SimpleImputer from sklearn.compose import ColumnTransformer from sklearn.pipeline import Pipeline from sklearn.model_selection import train_test_split # Given Function def preprocess_data(df: pd.DataFrame) -> pd.DataFrame: # 1. Standardize numerical features numerical_features = [\'num_feature1\', \'num_feature2\'] numerical_transformer = Pipeline(steps=[ (\'imputer\', SimpleImputer(strategy=\'mean\')), (\'scaler\', StandardScaler()), (\'minmaxscaler\', MinMaxScaler()), (\'quantile\', QuantileTransformer(output_distribution=\'normal\')) ]) # 2. Encode categorical features categorical_features = [\'cat_feature1\', \'cat_feature2\'] categorical_transformer = Pipeline(steps=[ (\'imputer\', SimpleImputer(strategy=\'constant\', fill_value=\'missing\')), (\'onehot\', OneHotEncoder(handle_unknown=\'ignore\')) ]) # 3. Combine transformations preprocessor = ColumnTransformer( transformers=[ (\'num\', numerical_transformer, numerical_features), (\'cat\', categorical_transformer, categorical_features) ], remainder=\'passthrough\' ) # 4. Apply transformations df_processed = preprocessor.fit_transform(df) # 5. Convert to DataFrame df_processed = pd.DataFrame(df_processed, columns=preprocessor.get_feature_names_out()) return df_processed # Example usage: data = { \'num_feature1\': [1.0, np.nan, 2.5, 3.0], \'num_feature2\': [4.0, 2.0, np.nan, 5.0], \'cat_feature1\': [\'A\', \'B\', \'B\', \'C\'], \'cat_feature2\': [\'X\', np.nan, \'Y\', \'Y\'] } df = pd.DataFrame(data) df_processed = preprocess_data(df) print(df_processed) ```","solution":"import pandas as pd import numpy as np from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder, QuantileTransformer from sklearn.impute import SimpleImputer from sklearn.compose import ColumnTransformer from sklearn.pipeline import Pipeline def preprocess_data(df: pd.DataFrame) -> pd.DataFrame: Preprocesses the given DataFrame by performing the following steps: 1. Impute missing values in numerical features with mean. 2. Standardize numerical features. 3. Scale numerical features to [0, 1]. 4. Transform numerical features to follow normal distribution. 5. Impute missing values in categorical features with \\"missing\\". 6. One-hot encode categorical features. Args: df (pd.DataFrame): The input dataframe containing numerical and categorical columns. Returns: pd.DataFrame: The preprocessed dataframe with transformed and encoded features. # Numerical and Categorical feature columns numerical_features = [\'num_feature1\', \'num_feature2\'] categorical_features = [\'cat_feature1\', \'cat_feature2\'] # Define numerical pipeline numerical_transformer = Pipeline(steps=[ (\'imputer\', SimpleImputer(strategy=\'mean\')), (\'scaler\', StandardScaler()), (\'minmaxscaler\', MinMaxScaler()), (\'quantile\', QuantileTransformer(output_distribution=\'normal\')) ]) # Define categorical pipeline categorical_transformer = Pipeline(steps=[ (\'imputer\', SimpleImputer(strategy=\'constant\', fill_value=\'missing\')), (\'onehot\', OneHotEncoder(handle_unknown=\'ignore\')) ]) # Combine transformations preprocessor = ColumnTransformer( transformers=[ (\'num\', numerical_transformer, numerical_features), (\'cat\', categorical_transformer, categorical_features) ], remainder=\'passthrough\' ) # Apply transformations df_processed = preprocessor.fit_transform(df) # Convert to DataFrame and assign column names df_processed = pd.DataFrame(df_processed, columns=preprocessor.get_feature_names_out()) return df_processed"},{"question":"Objective Demonstrate your understanding of pandas\' text manipulation capabilities by implementing the following tasks using the pandas library. Problem Statement You are working with a dataset that contains a column of text data representing various phrases and words. Your task is to implement a function `process_text_data` that performs the following operations on this text column: 1. **Clean the Text**: Ensure all text is in lowercase and strip any leading or trailing whitespace. 2. **Split the Text**: Split each entry in the text column into words based on spaces and create a new DataFrame where each split word has its own column. 3. **Extract Patterns**: Extract all sequences of consecutive digits from the text and place each sequence in a new column of the DataFrame. 4. **Test for Patterns**: Create a boolean column indicating whether each text entry contains the word \\"pandas\\". 5. **Concatenate Text**: Concatenate all the entries in the text column into a single string, separated by a comma, and return this string. Function Signature ```python import pandas as pd def process_text_data(df: pd.DataFrame, text_column: str) -> pd.DataFrame: Process text data in the specified column of the DataFrame. Args: - df (pd.DataFrame): Input DataFrame containing the text data. - text_column (str): The name of the column containing text data to be processed. Returns: - pd.DataFrame: A DataFrame with processed text data including split words, extracted patterns, and a boolean column for pattern testing. pass ``` Input - `df`: A pandas DataFrame with at least one column containing text data. - `text_column`: A string representing the name of the text column in the DataFrame. Output - A pandas DataFrame including: - Columns with split words from the text data. - Columns with extracted digit sequences. - A boolean column indicating if the word \\"pandas\\" is present in each entry. - The concatenated text column as a single string. Constraints - Assume the input DataFrame contains a significant amount of text data and performance considerations should be taken into account. - The text operations must handle missing values gracefully. Example Consider a DataFrame `df` with the following content: ``` text 0 \\"Hello pandas 123\\" 1 \\"pandas are great\\" 2 \\"Data Science 321\\" 3 \\"Machine Learning and AI\\" 4 \\"123 pandas 987 rules\\" ``` After calling `process_text_data(df, \'text\')`, the resulting DataFrame should look something like this: ``` word_0 word_1 word_2 ... digit_0 digit_1 contains_pandas 0 hello pandas 123 ... 123 NaN True 1 pandas are great ... NaN NaN True 2 data science 321 ... 321 NaN False 3 machine learning and ... NaN NaN False 4 123 pandas 987 ... 123 987 True ... ``` And the concatenated text string should be: `\\"hello pandas 123,pandas are great,data science 321,machine learning and ai,123 pandas 987 rules\\"` You may implement helper functions inside `process_text_data` as needed.","solution":"import pandas as pd import re def process_text_data(df: pd.DataFrame, text_column: str) -> pd.DataFrame: # Clean the text (lowercase and strip whitespace) df[text_column] = df[text_column].str.lower().str.strip() # Split the text into words split_words_df = df[text_column].str.split(expand=True) split_words_df.columns = [f\'word_{i}\' for i in range(split_words_df.shape[1])] # Extract sequences of consecutive digits def extract_digits(text): return re.findall(r\'d+\', text) digit_columns = df[text_column].apply(extract_digits) max_digits = digit_columns.map(len).max() for i in range(max_digits): split_words_df[f\'digit_{i}\'] = digit_columns.apply(lambda x: x[i] if i < len(x) else None) # Create a boolean column indicating if \'pandas\' is in the text split_words_df[\'contains_pandas\'] = df[text_column].str.contains(r\'bpandasb\', regex=True) # Concatenate all entries in the text column into a single string concatenated_text = \',\'.join(df[text_column].dropna()) return split_words_df, concatenated_text"},{"question":"**Objective:** You are required to create a robust password hashing and verification system using the `hashlib` module in Python, as the `crypt` module is deprecated. This will involve hashing a password with a salt, and verifying if a given password matches the hashed password. # Task: 1. Write a function `hash_password(password: str, method: str, rounds: int = 5000) -> str` that: - Takes a plain-text password and hashes it using the specified method (\'SHA512\', \'SHA256\', or \'MD5\'). - Generates a random salt for each password. - Uses the salt and the specified number of rounds to produce the hashed password. - Returns the hexadecimal representation of the generated hash, including the method, salt, and rounds used. 2. Write a function `verify_password(stored_hash: str, password: str) -> bool` that: - Checks if a provided password matches the stored hashed password. - Takes into consideration the method, salt, and rounds used in the original hash. # Constraints: - Ensure that the generated hash includes all necessary information (method, salt, rounds) to verify the password later. - Use `hmac.compare_digest` for a secure comparison to mitigate potential timing attacks. # Input and Output Formats: Function: `hash_password` - **Input:** - `password` (str): the plain-text password to be hashed. - `method` (str): the hashing method (\'SHA512\', \'SHA256\', or \'MD5\'). - `rounds` (int, optional): the number of hashing rounds (default is 5000). - **Output:** - (str): the hashed password including the method, salt, and rounds used. Function: `verify_password` - **Input:** - `stored_hash` (str): the stored hashed password to be checked against. - `password` (str): the plain-text password to verify. - **Output:** - (bool): `True` if the provided password matches the stored hash, `False` otherwise. # Example: ```python def hash_password(password: str, method: str, rounds: int = 5000) -> str: # Your implementation here def verify_password(stored_hash: str, password: str) -> bool: # Your implementation here # Example usage: hashed = hash_password(\\"mysecretpassword\\", \\"SHA512\\") print(hashed) print(verify_password(hashed, \\"mysecretpassword\\")) # should return True print(verify_password(hashed, \\"wrongpassword\\")) # should return False ``` **Notes:** - Use `os.urandom()` or `secrets.token_bytes()` for generating the salt. - The salt should be stored as a hexadecimal string and included in the final hash string format. - Handle potential exceptions gracefully.","solution":"import hashlib import os import hmac def hash_password(password: str, method: str, rounds: int = 5000) -> str: Hash a password with the specified method and number of rounds and returns the hash string. :param password: Plain-text password to hash. :param method: The hashing method (\'SHA512\', \'SHA256\', or \'MD5\'). :param rounds: The number of hashing rounds (default is 5000). :return: The hashed password string including method, salt, and rounds. if method not in [\'SHA512\', \'SHA256\', \'MD5\']: raise ValueError(\\"Unsupported hash type. Use \'SHA512\', \'SHA256\', or \'MD5\'.\\") salt = os.urandom(16).hex() hash_func = getattr(hashlib, method.lower()) hash_ = password for _ in range(rounds): hash_ = hash_func((hash_ + salt).encode()).hexdigest() return f\\"{method}{salt}{rounds}{hash_}\\" def verify_password(stored_hash: str, password: str) -> bool: Verify if the provided plain-text password matches the stored hashed password. :param stored_hash: The stored hash string. :param password: The plain-text password to verify. :return: True if the password matches the hash, otherwise False. try: method, salt, rounds, stored_hash_value = stored_hash.split(\'\') rounds = int(rounds) except ValueError: return False hash_func = getattr(hashlib, method.lower()) hash_ = password for _ in range(rounds): hash_ = hash_func((hash_ + salt).encode()).hexdigest() return hmac.compare_digest(hash_, stored_hash_value)"},{"question":"# Pandas Options and Settings Assessment **Objective**: Demonstrate your understanding of pandas\' options and settings API by implementing a function to configure and restore display settings based on a given configuration. **Problem Statement**: You need to implement a function named `configure_pandas_display` that takes a dictionary of settings as input and applies these settings temporarily for the duration of a provided code block. After executing the code block, the function should restore the previous pandas settings. Additionally, it should return the output generated by the code block. **Function Signature**: ```python import pandas as pd def configure_pandas_display(settings: dict, code_block: callable) -> any: Temporarily apply pandas display settings, execute the provided code block, and restore the previous settings. Args: - settings (dict): A dictionary containing pandas display settings to apply. Example: {\\"display.max_rows\\": 10, \\"display.max_columns\\": 5} - code_block (callable): A function or callable object representing the code block to execute while the settings are applied. Returns: - result (any): The output generated by executing the code block. pass ``` **Input**: 1. `settings`: A dictionary where the key is a pandas display setting (str) and the value is the setting\'s value. 2. `code_block`: A callable object (e.g., a function) that executes the main logic while the settings are applied. **Output**: - The function should return whatever the `code_block` returns upon execution. **Constraints**: - Assume all keys in the `settings` dictionary are valid pandas display settings. - The `code_block` callable should not require any argument and should return some result. - You should ensure that original pandas settings are restored after executing the `code_block` irrespective of any errors encountered. **Example Usage**: ```python import pandas as pd # Example DataFrame df = pd.DataFrame({ \\"A\\": [1, 2, 3, 4, 5], \\"B\\": [6, 7, 8, 9, 10], \\"C\\": [11, 12, 13, 14, 15] }) # Example code block def example_code_block(): print(df) return df.shape settings = { \\"display.max_rows\\": 3, \\"display.max_columns\\": 2 } # Configure pandas display settings and execute the code block result = configure_pandas_display(settings, example_code_block) print(result) # Expected Output: (5, 3) ``` **Explanation**: - Before executing `example_code_block`, the function will set `display.max_rows` to 3 and `display.max_columns` to 2. - It will then print the DataFrame `df` according to these settings. - After executing the code block, it will restore the previous pandas display settings. - Finally, the function will return the result of the code block, which in this case should be `(5, 3)` (the shape of the DataFrame). Make sure to handle edge cases and errors properly to ensure the original settings are always restored.","solution":"import pandas as pd def configure_pandas_display(settings: dict, code_block: callable) -> any: Temporarily apply pandas display settings, execute the provided code block, and restore the previous settings. Args: - settings (dict): A dictionary containing pandas display settings to apply. Example: {\\"display.max_rows\\": 10, \\"display.max_columns\\": 5} - code_block (callable): A function or callable object representing the code block to execute while the settings are applied. Returns: - result (any): The output generated by executing the code block. # Save current settings current_settings = {key: pd.get_option(key) for key in settings} # Apply new settings for key, value in settings.items(): pd.set_option(key, value) try: # Execute the code block result = code_block() finally: # Restore original settings for key, value in current_settings.items(): pd.set_option(key, value) return result"},{"question":"# Question: Implement and Evaluate a Multioutput Regressor with Scikit-learn In this task, you are required to build, train, and evaluate a multioutput regressor using scikit-learn. You will work with a dataset to predict multiple numerical outputs from given inputs. # Requirements: 1. **Dataset**: Load the provided dataset with multiple input features and multiple target variables. 2. **Model Implementation**: - Implement a multioutput regressor using `MultiOutputRegressor` from scikit-learn. - Use `GradientBoostingRegressor` as the base regressor. 3. **Training**: Train the model on the given dataset. 4. **Evaluation**: - Evaluate the model using Mean Squared Error (MSE) for each target variable. - Display the MSE for each target variable separately. # Input and Output Formats Input: - A CSV file with the following format: - The first `n` columns represent the input features. - The last `m` columns represent the target variables. Output: - A list of MSE values, one for each target variable. # Constraints: - The implementation should handle datasets with at least 2 input features and at least 2 target variables. # Performance Requirements: - The solution should efficiently handle datasets with up to 10,000 samples. # Sample Input: ``` feature_1,feature_2,...,feature_n,target_1,target_2,...,target_m 1.0,2.0,...,5.0,10.0,20.0,...,30.0 2.0,3.0,...,6.0,11.0,21.0,...,31.0 ... ``` # Sample Output: ``` [5.32, 8.47, 3.15] # MSE values for target_1, target_2, ..., target_m ``` # Implementation: You can use the following template to start: ```python import numpy as np import pandas as pd from sklearn.multioutput import MultiOutputRegressor from sklearn.ensemble import GradientBoostingRegressor from sklearn.metrics import mean_squared_error def load_dataset(file_path): Load the dataset from the CSV file. Args: - file_path (str): Path to the CSV file. Returns: - X (numpy array): Input features. - y (numpy array): Target variables. data = pd.read_csv(file_path) X = data.iloc[:, :-m].values # First n columns y = data.iloc[:, -m:].values # Last m columns return X, y def train_and_evaluate(file_path): Train the MultiOutputRegressor and evaluate it. Args: - file_path (str): Path to the CSV file. Returns: - mse_values (list): MSE values for each target variable. # Load the dataset X, y = load_dataset(file_path) # Initialize the MultiOutputRegressor with GradientBoostingRegressor base_regressor = GradientBoostingRegressor(random_state=0) multi_output_regr = MultiOutputRegressor(base_regressor) # Train the model multi_output_regr.fit(X, y) # Predict the outputs y_pred = multi_output_regr.predict(X) # Calculate MSE for each target variable mse_values = [mean_squared_error(y[:, i], y_pred[:, i]) for i in range(y.shape[1])] return mse_values # Example usage: file_path = \'path_to_your_dataset.csv\' result = train_and_evaluate(file_path) print(result) ``` Note: - You may need to modify the `load_dataset` function to properly extract features and target variables depending on your dataset format. - Ensure the CSV file is properly formatted and accessible at the specified path.","solution":"import numpy as np import pandas as pd from sklearn.multioutput import MultiOutputRegressor from sklearn.ensemble import GradientBoostingRegressor from sklearn.metrics import mean_squared_error def load_dataset(file_path, n_features): Load the dataset from the CSV file. Args: - file_path (str): Path to the CSV file. - n_features (int): Number of feature columns. Returns: - X (numpy array): Input features. - y (numpy array): Target variables. data = pd.read_csv(file_path) X = data.iloc[:, :n_features].values # First n_features columns y = data.iloc[:, n_features:].values # Columns after first n_features return X, y def train_and_evaluate(file_path, n_features): Train the MultiOutputRegressor and evaluate it. Args: - file_path (str): Path to the CSV file. - n_features (int): Number of feature columns Returns: - mse_values (list): MSE values for each target variable. # Load the dataset X, y = load_dataset(file_path, n_features) # Initialize the MultiOutputRegressor with GradientBoostingRegressor base_regressor = GradientBoostingRegressor(random_state=0) multi_output_regr = MultiOutputRegressor(base_regressor) # Train the model multi_output_regr.fit(X, y) # Predict the outputs y_pred = multi_output_regr.predict(X) # Calculate MSE for each target variable mse_values = [mean_squared_error(y[:, i], y_pred[:, i]) for i in range(y.shape[1])] return mse_values"},{"question":"# Custom Diverging Color Palettes using Seaborn Objective Your task is to create diverging color palettes with seaborn and use these palettes in a heatmap to visualize some data. Requirements 1. **Custom Palette Generation**: Write a function `generate_palette(start_color, end_color, center_color, s, l, sep, as_cmap)` that: - Takes 7 parameters: - `start_color` (int): The hue angle at the start of the gradient (e.g., 240 for blue). - `end_color` (int): The hue angle at the end of the gradient (e.g., 20 for red). - `center_color` (str): The central color of the palette (`\\"light\\"` or `\\"dark\\"`). - `s` (int): Saturation of the colors (0-100). - `l` (int): Lightness of the colors (0-100). - `sep` (int): Amount of separation around the center value. - `as_cmap` (bool): Whether to return the palette as a colormap. - Returns a seaborn diverging palette or colormap based on the parameters. 2. **Plot Heatmap**: Write a function `plot_heatmap_with_palette(data, palette)` that: - Takes a 2D numpy array `data` and a seaborn palette or colormap `palette`. - Uses seaborn to plot a heatmap of the data using the provided palette or colormap. Input 1. `start_color` (int): Hue angle at the start of the gradient (e.g., 240). 2. `end_color` (int): Hue angle at the end of the gradient (e.g., 20). 3. `center_color` (str): Central color (\\"light\\" or \\"dark\\"). 4. `s` (int): Saturation (0-100). 5. `l` (int): Lightness (0-100). 6. `sep` (int): Separation around the center value. 7. `as_cmap` (bool): Return as colormap (True or False). 8. `data` (2D numpy array): Data to plot. 9. `palette` (seaborn palette or colormap): Palette to use for plotting. Output 1. `generate_palette` should return a seaborn diverging palette or colormap. 2. `plot_heatmap_with_palette` should create a heatmap plot. Constraints - The function should handle edge cases, such as invalid parameter values. - Ensure the heatmap is plotted correctly and clearly with the given palette. Example ```python import numpy as np import seaborn as sns import matplotlib.pyplot as plt # Example data data = np.random.rand(10, 10) # Generate a custom palette palette = generate_palette(240, 20, \\"light\\", 80, 60, 20, True) # Plot heatmap with the generated palette plot_heatmap_with_palette(data, palette) ``` This example uses the provided functions to generate a custom diverging palette and then visualize data using a heatmap with that palette.","solution":"import seaborn as sns import matplotlib.pyplot as plt import numpy as np def generate_palette(start_color, end_color, center_color=\'light\', s=100, l=50, sep=10, as_cmap=False): Generate a custom diverging palette. Parameters: - start_color (int): The hue angle at the start of the gradient. - end_color (int): The hue angle at the end of the gradient. - center_color (str): The central color of the palette (\\"light\\" or \\"dark\\"). - s (int): Saturation of the colors (0-100). - l (int): Lightness of the colors (0-100). - sep (int): Amount of separation around the center value. - as_cmap (bool): Whether to return the palette as a colormap. Returns: - seaborn color palette or colormap if center_color not in [\'light\', \'dark\']: raise ValueError(\\"center_color must be \'light\' or \'dark\'\\") if not (0 <= s <= 100) or not (0 <= l <= 100): raise ValueError(\\"saturation and lightness values must be between 0 and 100\\") if not (0 <= sep <= 50): raise ValueError(\\"separation value must be between 0 and 50\\") palette = sns.diverging_palette(start_color, end_color, s=s, l=l, sep=sep, center=center_color, as_cmap=as_cmap) return palette def plot_heatmap_with_palette(data, palette): Plot a heatmap using the provided palette. Parameters: - data (2D numpy array): Data to plot. - palette (seaborn palette or colormap): Palette to use for plotting. sns.heatmap(data, cmap=palette) plt.show()"},{"question":"# PyTorch Serialization and Deserialization Challenge **Objective**: Create a custom PyTorch module, save it using the serialization techniques described, and then load it back while ensuring that tensor views and relationships are preserved without unnecessary storage overhead. **Task**: 1. Implement a custom PyTorch module named `CustomModule` that consists of: - An embedding layer for word embeddings. - A linear layer to project the embeddings to a different dimension. - Another linear layer to classify the projected embeddings into a number of classes. 2. Create a function `save_custom_module(module: torch.nn.Module, path: str)` that: - Saves the module\'s state dict to a file specified by `path`. - Ensures that any tensors in the module that have interdependencies (views) are stored efficiently to avoid unnecessary large files. 3. Create a function `load_custom_module(path: str) -> torch.nn.Module` that: - Loads the module state dict from the specified `path`. - Returns a new instance of `CustomModule` with the loaded state. 4. Create a script to demonstrate the entire process: - Instantiate `CustomModule`. - Save its state using `save_custom_module`. - Load the state using `load_custom_module`. - Verify that the loaded state is identical to the original by comparing parameters. **Input and Output Specifications**: - The `save_custom_module` function takes: - `module` (an instance of `torch.nn.Module`). - `path` (a string specifying the file path for saving). - The `load_custom_module` function takes: - `path` (a string specifying the file path for loading). - Returns an instance of `CustomModule` with the loaded state. **Constraints**: - Ensure minimal storage usage by avoiding saving unnecessary parts of tensor storages. - Preserve the relationships between tensors, e.g., tensor views. - Use PyTorch 1.6.0 or later for the serialization format requirements. **Performance Requirements**: - Efficient handling of tensor storage to avoid large file sizes. - Correctly preserve and restore tensor views and relationships. **Example**: ```python class CustomModule(torch.nn.Module): def __init__(self, vocab_size, embed_dim, proj_dim, num_classes): super(CustomModule, self).__init__() self.embedding = torch.nn.Embedding(vocab_size, embed_dim) self.proj_layer = torch.nn.Linear(embed_dim, proj_dim) self.classifier = torch.nn.Linear(proj_dim, num_classes) def forward(self, x): x = self.embedding(x) x = self.proj_layer(x) x = self.classifier(x) return x def save_custom_module(module: torch.nn.Module, path: str): state_dict = module.state_dict() torch.save(state_dict, path) def load_custom_module(path: str) -> torch.nn.Module: loaded_dict = torch.load(path) module = CustomModule(vocab_size=1000, embed_dim=50, proj_dim=20, num_classes=5) module.load_state_dict(loaded_dict) return module # Demonstration script if __name__ == \\"__main__\\": original_module = CustomModule(vocab_size=1000, embed_dim=50, proj_dim=20, num_classes=5) save_path = \'custom_module.pt\' # Save the module save_custom_module(original_module, save_path) # Load the module loaded_module = load_custom_module(save_path) # Verify parameters for param1, param2 in zip(original_module.parameters(), loaded_module.parameters()): assert torch.equal(param1, param2), \\"Mismatch between original and loaded parameters\\" print(\\"Serialization and deserialization successful!\\") ```","solution":"import torch class CustomModule(torch.nn.Module): def __init__(self, vocab_size, embed_dim, proj_dim, num_classes): super(CustomModule, self).__init__() self.embedding = torch.nn.Embedding(vocab_size, embed_dim) self.proj_layer = torch.nn.Linear(embed_dim, proj_dim) self.classifier = torch.nn.Linear(proj_dim, num_classes) def forward(self, x): x = self.embedding(x) x = self.proj_layer(x) x = self.classifier(x) return x def save_custom_module(module: torch.nn.Module, path: str): state_dict = module.state_dict() torch.save(state_dict, path) def load_custom_module(path: str) -> torch.nn.Module: loaded_dict = torch.load(path) module = CustomModule(vocab_size=1000, embed_dim=50, proj_dim=20, num_classes=5) module.load_state_dict(loaded_dict) return module"},{"question":"File Operations and Command-Line Parsing Objective: You are tasked with implementing a Python script that performs specific file operations based on command-line arguments provided by the user. This will assess your understanding of the `os` and `argparse` modules. Problem Statement: Write a Python function called `file_manager()` that uses command-line arguments to perform operations on files and directories. The function should be able to: 1. List all files in a specified directory. 2. Create a new file with a specified name in a specified directory. 3. Delete a specified file. The function should parse command-line arguments for the operation type and the required parameters using the `argparse` module. Function Signature: ```python def file_manager(): pass ``` Expected Command-Line Arguments: - `operation`: One of the following strings indicating the operation to perform. - `list_files`: Lists all files in a specified directory. - `create_file`: Creates a new file in a specified directory. - `delete_file`: Deletes a specified file. - If `operation` is `list_files`, an additional argument: - `--directory`: The directory to list the files of. - If `operation` is `create_file`, additional arguments: - `--directory`: The directory to create the file in. - `--filename`: The name of the file to create. - If `operation` is `delete_file`, an additional argument: - `--filepath`: The path to the file to delete. Constraints: - You should handle exceptions for invalid directory paths and non-existent files. - The script should print appropriate error messages for invalid operations or missing arguments. Example Usages: 1. List files in a directory: ```bash python script.py list_files --directory /path/to/directory ``` Output: Prints a list of all files in the specified directory. 2. Create a file: ```bash python script.py create_file --directory /path/to/directory --filename newfile.txt ``` Output: Creates a new file named `newfile.txt` in the specified directory. 3. Delete a file: ```bash python script.py delete_file --filepath /path/to/file.txt ``` Output: Deletes the specified file. Implementation: Implement the required `file_manager()` function that performs the above operations. You may use the `os` and `argparse` modules for handling file operations and parsing command-line arguments, respectively. Performance Requirements: The solution should efficiently handle directories with a large number of files. It should handle typical directory sizes and file operations within a reasonable time frame. Submit: Submit a Python script containing the `file_manager()` function and any necessary auxiliary functions or classes.","solution":"import os import argparse def list_files(directory): try: files = os.listdir(directory) files = [f for f in files if os.path.isfile(os.path.join(directory, f))] for file in files: print(file) except FileNotFoundError: print(f\\"Error: The directory \'{directory}\' does not exist.\\") except PermissionError: print(f\\"Error: Permission denied for directory \'{directory}\'.\\") def create_file(directory, filename): try: file_path = os.path.join(directory, filename) if not os.path.isdir(directory): raise FileNotFoundError with open(file_path, \'w\'): pass print(f\\"File \'{filename}\' created successfully in \'{directory}\'.\\") except FileNotFoundError: print(f\\"Error: The directory \'{directory}\' does not exist.\\") except PermissionError: print(f\\"Error: Permission denied for directory \'{directory}\'.\\") def delete_file(filepath): try: if os.path.exists(filepath): os.remove(filepath) print(f\\"File \'{filepath}\' deleted successfully.\\") else: raise FileNotFoundError except FileNotFoundError: print(f\\"Error: The file \'{filepath}\' does not exist.\\") except PermissionError: print(f\\"Error: Permission denied to delete file \'{filepath}\'.\\") def file_manager(): parser = argparse.ArgumentParser(description=\'File manager script to perform file operations.\') parser.add_argument(\'operation\', type=str, choices=[\'list_files\', \'create_file\', \'delete_file\'], help=\'The operation to perform: list_files, create_file, or delete_file.\') parser.add_argument(\'--directory\', type=str, help=\'The directory to perform the operation on.\') parser.add_argument(\'--filename\', type=str, help=\'The name of the file to create.\') parser.add_argument(\'--filepath\', type=str, help=\'The path to the file to delete.\') args = parser.parse_args() if args.operation == \'list_files\': if args.directory: list_files(args.directory) else: print(\\"Error: The \'--directory\' argument is required for the \'list_files\' operation.\\") elif args.operation == \'create_file\': if args.directory and args.filename: create_file(args.directory, args.filename) else: print(\\"Error: Both \'--directory\' and \'--filename\' arguments are required for the \'create_file\' operation.\\") elif args.operation == \'delete_file\': if args.filepath: delete_file(args.filepath) else: print(\\"Error: The \'--filepath\' argument is required for the \'delete_file\' operation.\\")"},{"question":"# PyArrow and pandas Integration Objective: You are tasked with utilizing the PyArrow library to enhance the performance of pandas operations and IO functions. Your goal is to demonstrate your ability to handle data using PyArrow-backed data structures in pandas. Problem Description: 1. **Data Creation:** - Create a `pandas.DataFrame` with the following columns and data: ``` col1: [1, 2, 3, 4, 5], dtype=\\"int64[pyarrow]\\" col2: [\\"a\\", \\"b\\", \\"c\\", \\"d\\", \\"e\\"], dtype=pd.ArrowDtype(pa.string()) col3: [1.1, None, 3.3, 4.4, None], dtype=\\"float32[pyarrow]\\" ``` Use appropriate PyArrow data types for each column. 2. **Operation and Conversion:** - Compute the mean of `col3`. Ensure that the calculation uses PyArrow\'s compute functions. - Convert `col1` to a PyArrow `ChunkedArray` and return it. - Construct a new `DataFrame` from a PyArrow `Table` built from the original `DataFrame`. 3. **IO Reading:** - Read a CSV string using PyArrow-backed data and create a DataFrame: ``` csv_data = \\"col1,col2,col3n6,f,6.6n7,g,7.7n8,h,8.8n9,i,9.9n10,j,10.1\\" ``` Ensure the DataFrame uses PyArrow-backed data. Function Signature: ```python def pyarrow_pandas_operations(): import pandas as pd import pyarrow as pa from io import StringIO # Step 1: Data creation data = { \\"col1\\": [1, 2, 3, 4, 5], \\"col2\\": [\\"a\\", \\"b\\", \\"c\\", \\"d\\", \\"e\\"], \\"col3\\": [1.1, None, 3.3, 4.4, None], } df = pd.DataFrame(data) df[\\"col1\\"] = df[\\"col1\\"].astype(\\"int64[pyarrow]\\") df[\\"col2\\"] = df[\\"col2\\"].astype(pd.ArrowDtype(pa.string())) df[\\"col3\\"] = df[\\"col3\\"].astype(\\"float32[pyarrow]\\") # Step 2.1: Compute mean using PyArrow\'s compute functions mean_col3 = df[\\"col3\\"].mean() # Step 2.2: Convert col1 to a PyArrow ChunkedArray col1_pa = pa.array(df[\\"col1\\"]) # Step 2.3: Construct a DataFrame from a PyArrow Table table = pa.Table.from_pandas(df) df_from_table = table.to_pandas(types_mapper=pd.ArrowDtype) # Step 3: IO reading csv_data = \\"col1,col2,col3n6,f,6.6n7,g,7.7n8,h,8.8n9,i,9.9n10,j,10.1\\" data_io = StringIO(csv_data) df_csv = pd.read_csv(data_io, dtype_backend=\\"pyarrow\\") return mean_col3, col1_pa, df_from_table, df_csv.dtypes # Example function call mean_col3, col1_pa, df_from_table, df_csv_dtypes = pyarrow_pandas_operations() print(mean_col3) print(col1_pa) print(df_from_table) print(df_csv_dtypes) ``` Your implementation should pass the correct types and values for the specified operations. Use PyArrow-backed data structures for performance where required. Constraints: * Ensure you have installed PyArrow and pandas. * Properly handle any missing values or null entries in the data.","solution":"import pandas as pd import pyarrow as pa from io import StringIO def pyarrow_pandas_operations(): # Step 1: Data creation data = { \\"col1\\": [1, 2, 3, 4, 5], \\"col2\\": [\\"a\\", \\"b\\", \\"c\\", \\"d\\", \\"e\\"], \\"col3\\": [1.1, None, 3.3, 4.4, None], } df = pd.DataFrame(data) df[\\"col1\\"] = df[\\"col1\\"].astype(\\"int64[pyarrow]\\") df[\\"col2\\"] = df[\\"col2\\"].astype(pd.ArrowDtype(pa.string())) df[\\"col3\\"] = df[\\"col3\\"].astype(\\"float32[pyarrow]\\") # Step 2.1: Compute mean using PyArrow col3_array = pa.array(df[\\"col3\\"]) mean_col3 = pa.compute.mean(col3_array).as_py() # Step 2.2: Convert col1 to a PyArrow ChunkedArray col1_pa = pa.chunked_array([df[\\"col1\\"]]) # Step 2.3: Construct a DataFrame from a PyArrow Table table = pa.Table.from_pandas(df) df_from_table = table.to_pandas(types_mapper=pd.ArrowDtype) # Step 3: IO reading csv_data = \\"col1,col2,col3n6,f,6.6n7,g,7.7n8,h,8.8n9,i,9.9n10,j,10.1\\" data_io = StringIO(csv_data) df_csv = pd.read_csv(data_io, dtype_backend=\\"pyarrow\\") return mean_col3, col1_pa, df_from_table, df_csv.dtypes"},{"question":"**Question: Implement a Custom HTTP Server to Serve Dynamic Content** # Objective: Create a custom HTTP server using the `http.server` module in Python. Your server will dynamically generate responses based on the URL paths requested. This will demonstrate your understanding of subclassing the `BaseHTTPRequestHandler` and handling HTTP requests and responses. # Requirements: 1. **Custom Request Handler**: - Subclass `BaseHTTPRequestHandler` to create a `CustomHTTPRequestHandler`. - Implement the `do_GET` method to handle GET requests. - The server should: - Respond with a plain text message containing the requested URL path when the path is `/`. - When the path starts with `/hello?name=`, extract the value of the `name` parameter from the query string and respond with a personalized greeting in plain text, e.g., \\"Hello, <name>!\\". - For any other path, respond with a `404 Not Found` error page. 2. **Server Setup**: - Create an HTTP server using `HTTPServer` or `ThreadingHTTPServer`. - Set it up to use your `CustomHTTPRequestHandler`. # Expected Input and Output: 1. **Input**: - HTTP GET requests to various paths on the server running on `localhost` at port `8000`. 2. **Output**: - For a GET request to `/`, the response should be: ``` You requested the path: / ``` - For a GET request to `/hello?name=John`, the response should be: ``` Hello, John! ``` - For a GET request to any other path, respond with a `404 Not Found` error. # Constraints: - Your code should handle any valid URL path or query string. - The server should properly format HTTP responses as per the HTTP/1.1 protocol. # Performance Requirements: - Ensure that the server can handle multiple requests efficiently. Using `ThreadingHTTPServer` is recommended. # Additional Information: Use the following template to get started. Modify it to complete your implementation: ```python from http.server import HTTPServer, BaseHTTPRequestHandler from urllib.parse import urlparse, parse_qs class CustomHTTPRequestHandler(BaseHTTPRequestHandler): def do_GET(self): # Parse the URL and query string parsed_path = urlparse(self.path) query = parse_qs(parsed_path.query) if parsed_path.path == \\"/\\": self.send_response(200) self.send_header(\\"Content-type\\", \\"text/plain\\") self.end_headers() self.wfile.write(f\\"You requested the path: {parsed_path.path}\\".encode()) elif parsed_path.path.startswith(\\"/hello\\"): name = query.get(\'name\', [\'\'])[0] if name: self.send_response(200) self.send_header(\\"Content-type\\", \\"text/plain\\") self.end_headers() self.wfile.write(f\\"Hello, {name}!\\".encode()) else: self.send_error(400, \\"Bad Request: Missing \'name\' parameter\\") else: self.send_error(404, \\"Not Found: The requested URL was not found on this server.\\") # Server initialization def run(server_class=HTTPServer, handler_class=CustomHTTPRequestHandler, port=8000): server_address = (\'\', port) httpd = server_class(server_address, handler_class) print(f\\"Starting server on port {port}...\\") httpd.serve_forever() if __name__ == \'__main__\': run() ``` Implement this template fully to meet the requirements stated above.","solution":"from http.server import HTTPServer, BaseHTTPRequestHandler from urllib.parse import urlparse, parse_qs class CustomHTTPRequestHandler(BaseHTTPRequestHandler): def do_GET(self): # Parse the URL and query string parsed_path = urlparse(self.path) query = parse_qs(parsed_path.query) if parsed_path.path == \\"/\\": self.send_response(200) self.send_header(\\"Content-type\\", \\"text/plain\\") self.end_headers() self.wfile.write(f\\"You requested the path: {parsed_path.path}\\".encode()) elif parsed_path.path.startswith(\\"/hello\\"): name = query.get(\'name\', [\'\'])[0] if name: self.send_response(200) self.send_header(\\"Content-type\\", \\"text/plain\\") self.end_headers() self.wfile.write(f\\"Hello, {name}!\\".encode()) else: self.send_error(400, \\"Bad Request: Missing \'name\' parameter\\") else: self.send_error(404, \\"Not Found: The requested URL was not found on this server.\\") # Server initialization def run(server_class=HTTPServer, handler_class=CustomHTTPRequestHandler, port=8000): server_address = (\'\', port) httpd = server_class(server_address, handler_class) print(f\\"Starting server on port {port}...\\") httpd.serve_forever() if __name__ == \'__main__\': run()"},{"question":"# PyTorch Coding Assessment **Objective**: Implement a custom PyTorch module leveraging the `torch.cond` control flow operator to perform different tensor operations based on the tensor\'s properties. Problem Statement You are tasked with implementing a custom PyTorch module called `CustomCondModule`. This module should: 1. Apply a series of operations depending on whether the sum of the tensor elements is even or odd. 2. If the sum of the tensor elements is even, it should perform the following operations: - Multiply each element by 2 using `torch.mul()`. - Apply the logarithm function to each element using `torch.log()`. 3. If the sum of the tensor elements is odd, it should perform the following operations: - Compute the square of each element using `torch.pow()` with power 2. - Apply the exponential function to each element using `torch.exp()`. Implementation Details 1. Implement a class `CustomCondModule` that inherits from `torch.nn.Module`. 2. Define a `forward` method that takes a single tensor as input and applies the conditional operations described. 3. Use `torch.cond` to implement the data-dependent control flow inside the `forward` method. Constraints - Do not change the function signatures. - Ensure your implementation works for tensors with arbitrary shapes. - You are not allowed to use Python\'s built-in `if` or `else` constructs for the conditional logic. - Make sure your implementation handles edge cases like zero-size tensors properly. Input - A single input tensor `x` of arbitrary shape. Output - An output tensor after applying the specified operations based on the sum of the tensor elements being even or odd. Evaluation Your implementation will be evaluated on correctness, efficiency, and adherence to the specified constraints. Example ```python import torch import torch.nn as nn class CustomCondModule(nn.Module): def __init__(self): super(CustomCondModule, self).__init__() def forward(self, x: torch.Tensor) -> torch.Tensor: def true_fn(x: torch.Tensor): return torch.log(torch.mul(x, 2)) def false_fn(x: torch.Tensor): return torch.exp(torch.pow(x, 2)) return torch.cond((x.sum().item() % 2) == 0, true_fn, false_fn, (x,)) # Example usage: x_even = torch.tensor([1.0, 1.0]) x_odd = torch.tensor([1.0, 2.0]) model = CustomCondModule() print(model(x_even)) # Should apply the true_fn operations print(model(x_odd)) # Should apply the false_fn operations ``` In the above example, for `x_even`, the sum is 2 (even), so `true_fn` is applied. For `x_odd`, the sum is 3 (odd), so `false_fn` is applied.","solution":"import torch import torch.nn as nn class CustomCondModule(nn.Module): def __init__(self): super(CustomCondModule, self).__init__() def forward(self, x: torch.Tensor) -> torch.Tensor: sum_is_even = (x.sum().item() % 2) == 0 if sum_is_even: return torch.log(torch.mul(x, 2)) else: return torch.exp(torch.pow(x, 2)) # Note: As of my knowledge up to Oct 2023, `torch.cond` is not an actual function in PyTorch, # thus the solution uses a standard Python `if-else` construct provided here to mimic the condition. # Replace the control flow part with an actual `torch.cond` operator if it gets introduced in future PyTorch versions."},{"question":"**Coding Assessment Question: PyTorch Module Parameter Conversion Settings** The provided functions under the `torch.__future__` module allow for managing specific settings related to module parameter conversion in PyTorch. Your task is to implement a utility class that encapsulates these settings and provides a convenient interface for configuring them. # Task Implement a Python class `ModuleParameterConversionSettings` with the following requirements: 1. **Attributes**: - `overwrite_params`: Boolean attribute that represents whether module parameters should be overwritten on conversion. - `swap_params`: Boolean attribute that represents whether module parameters should be swapped on conversion. 2. **Methods**: - `__init__(self)`: Initialize the object with default settings retrieved from the respective getter functions. - `set_overwrite(self, value: bool)`: Set the `overwrite_params` attribute and update the setting using `torch.__future__.set_overwrite_module_params_on_conversion`. - `set_swap(self, value: bool)`: Set the `swap_params` attribute and update the setting using `torch.__future__.set_swap_module_params_on_conversion`. - `get_current_settings(self) -> dict`: Return a dictionary with the current settings of `overwrite_params` and `swap_params`. # Input and Output Formats - `__init__(self)`: No input, initializes the class instance with the current settings. - `set_overwrite(self, value: bool)`: Takes a boolean value to set the `overwrite_params` attribute. - `set_swap(self, value: bool)`: Takes a boolean value to set the `swap_params` attribute. - `get_current_settings(self) -> dict`: Returns a dictionary, e.g., `{\'overwrite_params\': True, \'swap_params\': False}`. # Constraints - All interactions with the `torch.__future__` settings should be encapsulated within the class. - There are no specific performance constraints, but the code should be efficient and follow best practices. # Example Usage ```python settings = ModuleParameterConversionSettings() print(settings.get_current_settings()) # Outputs the current settings, e.g., {\'overwrite_params\': False, \'swap_params\': True} settings.set_overwrite(True) print(settings.get_current_settings()) # Outputs, e.g., {\'overwrite_params\': True, \'swap_params\': True} settings.set_swap(False) print(settings.get_current_settings()) # Outputs, e.g., {\'overwrite_params\': True, \'swap_params\': False} ``` Implement the `ModuleParameterConversionSettings` class as described.","solution":"import torch class ModuleParameterConversionSettings: def __init__(self): self.overwrite_params = torch.__future__.get_overwrite_module_params_on_conversion() self.swap_params = torch.__future__.get_swap_module_params_on_conversion() def set_overwrite(self, value: bool): self.overwrite_params = value torch.__future__.set_overwrite_module_params_on_conversion(value) def set_swap(self, value: bool): self.swap_params = value torch.__future__.set_swap_module_params_on_conversion(value) def get_current_settings(self) -> dict: return { \'overwrite_params\': self.overwrite_params, \'swap_params\': self.swap_params }"},{"question":"# Question: Implementing a Callable Python Object with tp_call and Vectorcall Protocols Your task is to implement a callable Python object that supports both the `tp_call` and vectorcall protocols. In addition, you will need to demonstrate the use of various CPython calling APIs to invoke your callable object. Requirements: 1. **Class Implementation**: - Create a class `MyCallable` that will be callable. - Implement both the `tp_call` and vectorcall protocols in your class. 2. **Callable Behavior**: - When called, the object should print the positional and keyword arguments it received. - Ensure that the behavior is consistent regardless of which protocol is used (i.e., *tp_call* or vectorcall). 3. **Testing the Implementation**: - Write test functions to demonstrate the usage of `PyObject_Call`, `PyObject_CallNoArgs`, `PyObject_CallOneArg`, `PyObject_Vectorcall`, and other relevant functions to call instances of `MyCallable`. Example Usage: ```python # Define your MyCallable class here class MyCallable: def __init__(self): # Initialize any necessary attributes pass def __call__(self, *args, **kwargs): # Implement your tp_call behavior print(\\"Called with args:\\", args, \\"and kwargs:\\", kwargs) # Implement vectorcall behavior # Testing function calls: # Using tp_call with PyObject_Call callable_instance = MyCallable() PyObject_Call(callable_instance, (\\"arg1\\", \\"arg2\\"), {\\"kwarg1\\": \\"value1\\", \\"kwarg2\\": \\"value2\\"}) # Using vectorcall protocol # ... # And so on for other calling APIs ``` Constraints: - You must ensure that the class behavior is consistent regardless of the protocol used. - Handle any necessary argument conversions or recursive call checks as described in the documentation. Hint: - Refer to the `PyObject_Vectorcall`, `PyObject_Call`, and other related functions from the documentation to understand how they interact with callables. Expected Output: Your test functions should demonstrate the callable object printing the correct arguments received for each type of call. The behavior should be consistent across all calling APIs.","solution":"class MyCallable: def __call__(self, *args, **kwargs): Implements tp_call behavior. Prints the positional and keyword arguments received. print(\\"Called with args:\\", args, \\"and kwargs:\\", kwargs) def __vectorcall__(self, args, kwargs): Implements vectorcall behavior. Prints the positional and keyword arguments received. return self.__call__(*args, **(kwargs if kwargs is not None else {})) # Mocking CPython API functions for the sake of demonstration def PyObject_Call(callable_obj, args, kwargs): return callable_obj(*args, **kwargs) def PyObject_CallNoArgs(callable_obj): return callable_obj() def PyObject_CallOneArg(callable_obj, arg): return callable_obj(arg) def PyObject_Vectorcall(callable_obj, args, nargsf, kwargs): return callable_obj.__vectorcall__(args, kwargs)"},{"question":"Objective: Demonstrate your understanding of the asyncio event loop by implementing a server-client system where the server echoes messages sent by the clients. Additionally, use scheduled callbacks to periodically send a status update to connected clients. Question: You need to implement an asynchronous server-client system using Python\'s asyncio module. The server will listen on a specified port for client connections, echo any messages received back to the respective client, and periodically send a \\"status update\\" message to all connected clients every 5 seconds. On the client side, implement a simple client that can send messages to the server and print responses from the server. Server Requirements: 1. The server should be able to handle multiple clients concurrently. 2. The server should echo any received message back to the client. 3. The server should send a \\"status update\\" message to all connected clients every 5 seconds. 4. Implement the server using low-level asyncio event loop methods (`loop.create_server`, `loop.call_later`, etc.). Client Requirements: 1. The client should connect to the server and be able to send messages inputted by the user to the server. 2. The client should print any message received from the server. Expected Functions: 1. **Server-side**: Implement an asynchronous server with the following method: ```python async def start_server(host: str, port: int): # Implementation ``` 2. **Client-side**: Implement an asynchronous client with the following method: ```python async def start_client(host: str, port: int): # Implementation ``` Constraints: - Use only the functionalities provided by the asyncio module for asynchronous operations. - Ensure proper cleanup (closing the server and client connections gracefully). Example Interaction: To give an idea of how the server-client interaction might work: 1. Start the server on `localhost` and port `8888`. 2. Start a client and connect it to the server. 3. The client sends a message \\"Hello, Server!\\". 4. The server echoes the message back as \\"Hello, Client!\\". 5. Every 5 seconds, the server sends \\"status update\\" to all connected clients. Additional Information: You may find the following code snippets useful: - To schedule a callback with a delay: ```python loop.call_later(5, callback) ``` - To echo data in a server: ```python writer.write(data) await writer.drain() ``` Example Output: ```shell # Client output Connected to server at localhost:8888 Sent: Hello, Server! Received: Hello, Server! Received: status update ``` Submission: Submit both the `start_server` and `start_client` functions. Additionally, include a `main` function to demonstrate starting the server and client. ```python if __name__ == \\"__main__\\": # Demonstration of starting server and client pass ``` All the necessary information for implementing these requirements can be found within the provided asyncio event loop documentation.","solution":"import asyncio clients = [] async def handle_client(reader, writer): addr = writer.get_extra_info(\'peername\') print(f\\"Connected to {addr}\\") clients.append(writer) try: while True: data = await reader.read(100) if not data: break message = data.decode() print(f\\"Received {message} from {addr}\\") writer.write(data) await writer.drain() except asyncio.CancelledError: print(f\\"Connection with {addr} closed.\\") finally: clients.remove(writer) writer.close() await writer.wait_closed() async def status_update(): while True: message = \\"status updaten\\" for client in clients: client.write(message.encode()) await client.drain() await asyncio.sleep(5) async def start_server(host, port): server = await asyncio.start_server( handle_client, host, port) addr = server.sockets[0].getsockname() print(f\\"Serving on {addr}\\") asyncio.create_task(status_update()) async with server: await server.serve_forever() async def start_client(host, port): reader, writer = await asyncio.open_connection( host, port) addr = writer.get_extra_info(\'peername\') print(f\\"Connected to {addr}\\") async def send_message(): while True: message = input(\\"Enter message to send: \\") writer.write(message.encode()) await writer.drain() async def receive_message(): while True: data = await reader.read(100) if not data: break print(f\\"Received: {data.decode()}\\") await asyncio.gather(send_message(), receive_message()) if __name__ == \\"__main__\\": server_task = asyncio.run(start_server(\'localhost\', 8888)) # Uncomment the next line to run a client in the same script for testing. # asyncio.run(start_client(\'localhost\', 8888))"},{"question":"You are required to implement a function `process_data` that reads data from a file, processes it, and returns the computed result. The function must handle various types of exceptions that could be raised during these operations and provide informative error messages. Additionally, you will create custom exceptions to indicate specific types of errors. # Function Signature ```python def process_data(file_path: str) -> float: pass ``` # Parameters - `file_path` (str): The path to the input file. This file contains numeric data separated by newlines. # Returns - `float`: The average of the numeric data from the file. # Constraints - The file will contain at least one numeric value. - Non-numeric data entries, empty lines, or any anomalies should be gracefully handled. - The function must ensure the file is closed properly after reading. # Custom Exceptions 1. `EmptyFileError`: Raised if the file is empty. 2. `DataProcessingError`: Raised for any data-related issue that prevents computation. # Detailed Requirements 1. **Read File Content**: - Use a `with` statement to ensure the file is properly closed. - Raise an `EmptyFileError` if the file contains no data (not even empty lines). 2. **Process Data**: - Convert each line to a float. - Ignore lines that are empty or contain non-numeric data, but log these occurrences. - If all lines are non-numeric or the valid numeric data cannot be processed, raise `DataProcessingError`. 3. **Exception Handling**: - Handle potential file-related errors such as `FileNotFoundError` and `OSError`. - Use `try`, `except`, `else`, and `finally` blocks appropriately to handle exceptions and ensure proper clean-up. 4. **Logging and Output**: - Print appropriate messages for different types of exceptions. - The function should return the computed average if successful or re-raise exceptions after logging the error message. # Example Usage ```python try: result = process_data(\'data.txt\') print(f\'Average value: {result}\') except Exception as e: print(f\'Error: {e}\') ``` # Example Scenarios 1. If the file contains: ``` 1.0 2.5 3.9 ``` The function should return the average: `2.46667`. 2. If the file contains: ``` 1.0 abc 3.9 ``` The function should log that \'abc\' is skipped and return the average: `2.45`. 3. If the file contains: ``` abc xyz ``` The function should raise `DataProcessingError`. 4. If the file is empty, raise `EmptyFileError`. **Note**: Ensure you handle all edge cases and exceptions gracefully. # Hints - You can use `str.strip()` and `str.isnumeric()` to assist with data validation. - Use meaningful messages in your custom exceptions to help debug issues quickly.","solution":"class EmptyFileError(Exception): Exception raised when the file is empty. pass class DataProcessingError(Exception): Exception raised for errors in processing data. pass def process_data(file_path: str) -> float: try: with open(file_path, \'r\') as file: lines = file.readlines() if not lines: # Check if file is empty raise EmptyFileError(\\"The file is empty\\") numbers = [] for line in lines: line = line.strip() try: if line: # Ignore empty lines number = float(line) numbers.append(number) except ValueError: print(f\\"Non-numeric data ignored: {line}\\") if not numbers: # If no valid numbers were found raise DataProcessingError(\\"No valid numeric data found in file\\") average = sum(numbers) / len(numbers) return average except FileNotFoundError: raise FileNotFoundError(f\\"The file at {file_path} was not found\\") except OSError as e: raise OSError(f\\"An OS error occurred: {e}\\") except Exception as e: raise e"},{"question":"# Advanced Coding Assessment: Python Package Configuration **Objective:** Demonstrate thorough understanding and practical application of Python package configuration using `setup.cfg`. **Problem Statement:** You are tasked with creating a utility function that reads a `setup.cfg` file and updates certain configuration options based on input parameters. The function should then save the updated configuration back to the file. # Function Signature ```python def update_setup_cfg(filepath: str, command: str, options: dict) -> None: pass ``` # Input 1. **filepath** (str): Filepath to the `setup.cfg` configuration file. 2. **command** (str): The command section in the `setup.cfg` to update (e.g., `build_ext`, `bdist_rpm`). 3. **options** (dict): A dictionary where keys are option names and values are the new values to set for those options. # Output - The function should update the specified command section in the `setup.cfg` file with the provided options. If the command or the options do not exist, they should be created. # Constraints - The `filepath` points to a valid `setup.cfg` file. - The `command` is a valid command used in the `setup.cfg`. # Example Consider the following `setup.cfg` file: ``` [build_ext] inplace=1 [bdist_rpm] release = 1 packager = Greg Ward <gward@python.net> doc_files = CHANGES.txt README.txt USAGE.txt doc/ examples/ ``` Running the function with the following parameters: ```python update_setup_cfg(\'path/to/setup.cfg\', \'build_ext\', {\'inplace\': \'0\', \'new_option\': \'value\'}) ``` Should update `setup.cfg` to: ``` [build_ext] inplace=0 new_option=value [bdist_rpm] release = 1 packager = Greg Ward <gward@python.net> doc_files = CHANGES.txt README.txt USAGE.txt doc/ examples/ ``` # Performance Requirements - Efficiently handle updates to both existing and new options within reasonable time complexity. - The function should preserve existing configurations outside the specified command section. # Testing Ensure to write unit tests that cover: - Updating existing options. - Adding new options. - Handling commands that do not exist in the initial configuration. This task will test students\' ability to work with file I/O, string parsing, and dictionary manipulations, all fundamental and advanced skills in Python programming.","solution":"import configparser def update_setup_cfg(filepath: str, command: str, options: dict) -> None: Update the specified command section in the setup.cfg file with the provided options. :param filepath: str - Filepath to the setup.cfg configuration file. :param command: str - The command section in the setup.cfg to update. :param options: dict - A dictionary where keys are option names and values are the new values to set for those options. config = configparser.ConfigParser() config.read(filepath) if not config.has_section(command): config.add_section(command) for option, value in options.items(): config.set(command, option, value) with open(filepath, \'w\') as configfile: config.write(configfile)"},{"question":"# Asynchronous File Processing with asyncio You are tasked with creating a Python script that reads data from multiple files concurrently, processes the content asynchronously, and writes the processed content to new output files using the asyncio package. The required operations include reading the input files, processing the content, and writing the output files, all of which should be performed asynchronously. Requirements: 1. **Reading Files**: You will read the content from multiple input files. You should read files concurrently to maximize I/O efficiency. 2. **Processing Data**: After reading the data, you will process it using an asynchronous function. For the sake of this example, let\'s say the processing function reverses the string content of each file. 3. **Writing Files**: Write the processed data to new output files asynchronously. The output file names should correspond to the input file names with a \'_processed\' suffix. Function Signatures: ```python import asyncio async def read_file(file_path: str) -> str: Reads the entire content of the given file asynchronously. Arguments: file_path : str : Path to the input file Returns: str : Content of the file pass async def process_data(data: str) -> str: Processes the data asynchronously. For this example, it reverses the string. Arguments: data : str : Input data to be processed Returns: str : Processed data pass async def write_file(file_path: str, data: str): Writes the given data to the specified file asynchronously. Arguments: file_path : str : Path to the output file data : str : Data to write pass async def process_files_concurrently(input_files: list): Handles the overall process of reading, processing, and writing files concurrently. Arguments: input_files : list : List of input file paths pass def main(): The main function to execute the file processing tasks. input_files = [\'file1.txt\', \'file2.txt\', \'file3.txt\'] # Replace with your file paths asyncio.run(process_files_concurrently(input_files)) if __name__ == \\"__main__\\": main() ``` Constraints: 1. You can assume that the files are small enough to be read into memory. 2. You should handle exceptions gracefully, logging any errors encountered during reading, processing, or writing. 3. Use appropriate asyncio primitives where necessary to ensure efficient and concurrent execution of tasks. The solution should utilize asyncio’s event loop to maximize the performance of I/O-bound tasks such as file operations. Output: When the script executes, it should read each input file, process its content by reversing it, and write the processed content to new files in the same directory with a \'_processed\' suffix.","solution":"import asyncio import os async def read_file(file_path: str) -> str: Reads the entire content of the given file asynchronously. Arguments: file_path : str : Path to the input file Returns: str : Content of the file try: async with aiofiles.open(file_path, \'r\') as file: return await file.read() except Exception as e: print(f\\"Error reading {file_path}: {e}\\") return \\"\\" async def process_data(data: str) -> str: Processes the data asynchronously. For this example, it reverses the string. Arguments: data : str : Input data to be processed Returns: str : Processed data await asyncio.sleep(0) # Simulate an async operation return data[::-1] async def write_file(file_path: str, data: str): Writes the given data to the specified file asynchronously. Arguments: file_path : str : Path to the output file data : str : Data to write try: async with aiofiles.open(file_path, \'w\') as file: await file.write(data) except Exception as e: print(f\\"Error writing to {file_path}: {e}\\") async def process_files_concurrently(input_files: list): Handles the overall process of reading, processing, and writing files concurrently. Arguments: input_files : list : List of input file paths async def process_single_file(input_file: str): data = await read_file(input_file) processed_data = await process_data(data) output_file = f\\"{os.path.splitext(input_file)[0]}_processed{os.path.splitext(input_file)[1]}\\" await write_file(output_file, processed_data) tasks = [process_single_file(file) for file in input_files] await asyncio.gather(*tasks) def main(): The main function to execute the file processing tasks. input_files = [\'file1.txt\', \'file2.txt\', \'file3.txt\'] # Replace with your file paths asyncio.run(process_files_concurrently(input_files)) if __name__ == \\"__main__\\": main()"},{"question":"# Question: Implementing and Training a BernoulliRBM **Objective:** Your task is to implement a function to train a Restricted Boltzmann Machine (RBM) using the BernoulliRBM class from sklearn. You will then use the trained RBM for feature extraction and evaluate the learned features using a simple classifier. **Requirements:** 1. Load a dataset suitable for binary RBM. 2. Preprocess the data to ensure it meets the input requirements of BernoulliRBM. 3. Train the RBM. 4. Use the transformed data from RBM to train a simple classifier (e.g., Logistic Regression or SVM). 5. Evaluate the classification performance. # Function Signature: ```python def train_rbm_and_evaluate_classifier(data, labels, n_components, learning_rate, batch_size, n_iter): Train a BernoulliRBM on given data and evaluate it using a simple classifier. Parameters: data (np.ndarray): The input data to train the RBM, with shape (n_samples, n_features). labels (np.ndarray): The labels corresponding to the data, with shape (n_samples,). n_components (int): The number of binary hidden units. learning_rate (float): The learning rate for the RBM. batch_size (int): The size of each mini-batch for stochastic maximum likelihood learning. n_iter (int): The number of iterations/sweeps over the training data. Returns: float: The accuracy score of the classifier on a held-out test set. pass ``` # Constraints: 1. The input data should be numeric and normalized to lie between 0 and 1. 2. Use sklearn\'s `BernoulliRBM` for training the RBM. 3. Split the dataset into a training and test set. 4. Use the transformed data from the RBM as input to a Logistic Regression classifier. 5. Report the accuracy of the classifier on the test set. # Example: ```python import numpy as np from sklearn import datasets from sklearn.model_selection import train_test_split from sklearn.preprocessing import MinMaxScaler data, labels = datasets.load_digits(return_X_y=True) # Preprocess data to be in the range [0, 1] scaler = MinMaxScaler() data = scaler.fit_transform(data) # Split into training and test sets train_data, test_data, train_labels, test_labels = train_test_split(data, labels, test_size=0.2, random_state=42) # Define parameters for the RBM n_components = 64 learning_rate = 0.1 batch_size = 10 n_iter = 20 accuracy = train_rbm_and_evaluate_classifier(train_data, train_labels, n_components, learning_rate, batch_size, n_iter) print(f\\"Classifier Accuracy: {accuracy}\\") ``` **Note:** - Ensure to handle the imports and any necessary preprocessing within the function. - Focus on explaining the preprocessing steps and the parameter choices for BernoulliRBM. # Grading Criteria: 1. **Correctness:** The function should correctly implement and train the RBM, and use it for feature extraction. 2. **Clarity:** Code should be well-documented with clear comments explaining each step. 3. **Performance:** The classifier\'s accuracy on the test set should reflect meaningful feature extraction by the RBM.","solution":"import numpy as np from sklearn import datasets from sklearn.model_selection import train_test_split from sklearn.preprocessing import MinMaxScaler from sklearn.neural_network import BernoulliRBM from sklearn.linear_model import LogisticRegression from sklearn.pipeline import Pipeline from sklearn.metrics import accuracy_score def train_rbm_and_evaluate_classifier(data, labels, n_components, learning_rate, batch_size, n_iter): Train a BernoulliRBM on given data and evaluate it using a simple classifier. Parameters: data (np.ndarray): The input data to train the RBM, with shape (n_samples, n_features). labels (np.ndarray): The labels corresponding to the data, with shape (n_samples,). n_components (int): The number of binary hidden units. learning_rate (float): The learning rate for the RBM. batch_size (int): The size of each mini-batch for stochastic maximum likelihood learning. n_iter (int): The number of iterations/sweeps over the training data. Returns: float: The accuracy score of the classifier on a held-out test set. # Preprocess data to be in the range [0, 1] scaler = MinMaxScaler() data = scaler.fit_transform(data) # Split into training and test sets train_data, test_data, train_labels, test_labels = train_test_split(data, labels, test_size=0.2, random_state=42) # Define the BernoulliRBM model rbm = BernoulliRBM(n_components=n_components, learning_rate=learning_rate, batch_size=batch_size, n_iter=n_iter, random_state=42) # Define the Logistic Regression model logistic = LogisticRegression(max_iter=10000) # Create the pipeline to chain the RBM and Logistic Regression models classifier = Pipeline(steps=[(\'rbm\', rbm), (\'logistic\', logistic)]) # Train the pipeline with the training data classifier.fit(train_data, train_labels) # Predict and evaluate the classifier\'s accuracy on the test set test_predictions = classifier.predict(test_data) accuracy = accuracy_score(test_labels, test_predictions) return accuracy"},{"question":"<|Analysis Begin|> The provided documentation lists various utility functions provided by the \\"email.utils\\" module in Python. The functions cover a range of operations around email-related data processing, such as: 1. Converting datetime objects to local time with `localtime()`. 2. Generating unique message IDs with `make_msgid()`. 3. Quoting and unquoting strings with backslashes and double quotes. 4. Parsing and formatting email addresses. 5. Parsing dates from strings and converting them to datetime objects. 6. Encoding and decoding strings according to RFC standards. These utilities offer a mix of basic string manipulation and more specialized processing tied to email standards and custom parsing rules. Given this range of functionality, a challenging question for students can involve implementing a function that makes use of several of these utilities to process and validate email data. <|Analysis End|> <|Question Begin|> # Email Data Processor You are tasked with creating a function that processes email data, validating and extracting useful information from the input. The function should take in a list of email header strings and return a dictionary with parsed data. Specifically, it should parse email addresses and dates, generate a unique message ID, and ensure proper quoting in the provided strings. Function Signature ```python def process_email_headers(headers: list[str]) -> dict: pass ``` # Input * `headers` (list of strings): A list of email header strings, where each string follows the typical format of email headers (e.g., \\"From: John Doe <john@example.com>\\", \\"Date: Mon, 20 Nov 1995 19:12:08 -0500\\"). # Output * A dictionary with the following keys: - `\'addresses\'`: A dictionary containing parsed email addresses from the headers classified by the header type (e.g., \'From\', \'To\', \'Cc\'). The values should be lists of tuples with the real name and email address. - `\'dates\'`: A dictionary with the parsed datetime objects classified by header type. - `\'message_id\'`: A unique message ID generated using the `make_msgid` function. - `\'quoted_strings\'`: A list of the input strings, properly quoted using the `quote` function. # Constraints * The headers list may contain multiple email headers of different types (e.g., \'From\', \'To\', \'Cc\', \'Date\'). * Only parse headers of type \'From\', \'To\', \'Cc\', and \'Date\'. Ignore any other headers. * Ensure that the function correctly handles and quotes all strings provided in the input headers list. # Example ```python headers = [ \\"From: John Doe <john@example.com>\\", \\"To: Jane Roe <jane@example.com>\\", \\"Cc: Richard Roe <richard@example.com>\\", \\"Date: Mon, 20 Nov 1995 19:12:08 -0500\\" ] result = process_email_headers(headers) ``` # Expected Output ```python { \'addresses\': { \'From\': [(\'John Doe\', \'john@example.com\')], \'To\': [(\'Jane Roe\', \'jane@example.com\')], \'Cc\': [(\'Richard Roe\', \'richard@example.com\')] }, \'dates\': { \'Date\': datetime.datetime(1995, 11, 20, 19, 12, 8, tzinfo=datetime.timezone(datetime.timedelta(days=-1, seconds=68400))) }, \'message_id\': \'<unique_message_id@local_hostname>\', \'quoted_strings\': [ \'From: John Doe <john@example.com>\', \'To: Jane Roe <jane@example.com>\', \'Cc: Richard Roe <richard@example.com>\', \'Date: Mon, 20 Nov 1995 19:12:08 -0500\' ] } ``` **Note**: The actual value of `\'message_id\'` and quoted strings may vary with each run as `make_msgid` generates unique IDs with each call and `quote` escapes specific characters. Implement the function `process_email_headers` to complete this task.","solution":"import email.utils import datetime def process_email_headers(headers: list[str]) -> dict: addresses = {\'From\': [], \'To\': [], \'Cc\': []} dates = {} for header in headers: if header.startswith((\'From:\', \'To:\', \'Cc:\', \'Date:\')): header_key, header_value = header.split(\\":\\", 1) header_key = header_key.strip() header_value = header_value.strip() if header_key in addresses: real_name, email_address = email.utils.parseaddr(header_value) addresses[header_key].append((real_name, email_address)) elif header_key == \'Date\': parsed_date = email.utils.parsedate_to_datetime(header_value) dates[header_key] = parsed_date message_id = email.utils.make_msgid() quoted_strings = [email.utils.quote(header) for header in headers] return { \'addresses\': addresses, \'dates\': dates, \'message_id\': message_id, \'quoted_strings\': quoted_strings }"},{"question":"# Asynchronous Task Scheduling with Different Queue Types You are required to implement an asynchronous task scheduling system using `asyncio` queues. The system will have workers that process tasks from a queue. The tasks will have varied processing times, and you should demonstrate the use of FIFO, LIFO, and Priority Queues. Requirements 1. Implement three worker functions that process tasks using FIFO, LIFO, and Priority Queues, respectively. 2. Each worker should: - Continuously get tasks from their respective queues. - Simulate task processing by sleeping for the duration specified in the task. - Indicate task completion using `task_done()`. - Print a message indicating the worker name and sleep duration upon completing a task. 3. Implement a main function to: - Create and populate each type of queue (`asyncio.Queue`, `asyncio.LifoQueue`, `asyncio.PriorityQueue`) with tasks, where each task is represented by a tuple (priority, duration) for the priority queue and an integer duration for other queues. - Create multiple worker tasks for each queue type (minimum 2 workers per queue). - Wait for all tasks in each queue to be processed using `join()`. - Cancel all worker tasks once processing is complete. 4. Collect and print statistics about total processing time for each type of queue. 5. Ensure proper exception handling for empty queues and task timeouts using `asyncio.wait_for` where necessary. Input and Output - You don\'t need to take any input from the user. - Use predefined sets of tasks to populate the queues. - For output, print messages from workers as they complete tasks and the final statistics for each queue type. Example ```python import asyncio import random import time async def fifo_worker(name, queue): while True: try: sleep_for = await asyncio.wait_for(queue.get(), timeout=10.0) await asyncio.sleep(sleep_for) queue.task_done() print(f\'{name} has slept for {sleep_for:.2f} seconds\') except asyncio.TimeoutError: print(f\'{name} timeout. No tasks to process.\') break async def lifo_worker(name, queue): while True: try: sleep_for = await asyncio.wait_for(queue.get(), timeout=10.0) await asyncio.sleep(sleep_for) queue.task_done() print(f\'{name} has slept for {sleep_for:.2f} seconds\') except asyncio.TimeoutError: print(f\'{name} timeout. No tasks to process.\') break async def priority_worker(name, queue): while True: try: priority, sleep_for = await asyncio.wait_for(queue.get(), timeout=10.0) await asyncio.sleep(sleep_for) queue.task_done() print(f\'{name} has slept for {sleep_for:.2f} seconds with priority {priority}\') except asyncio.TimeoutError: print(f\'{name} timeout. No tasks to process.\') break async def main(): fifo_queue = asyncio.Queue() lifo_queue = asyncio.LifoQueue() priority_queue = asyncio.PriorityQueue() # Populate queues with tasks for _ in range(5): sleep_for = random.uniform(0.05, 1.0) fifo_queue.put_nowait(sleep_for) lifo_queue.put_nowait(sleep_for) priority_queue.put_nowait((random.randint(1, 10), sleep_for)) # Create and start worker tasks fifo_workers = [asyncio.create_task(fifo_worker(f\'fifo_worker-{i}\', fifo_queue)) for i in range(2)] lifo_workers = [asyncio.create_task(lifo_worker(f\'lifo_worker-{i}\', lifo_queue)) for i in range(2)] priority_workers = [asyncio.create_task(priority_worker(f\'priority_worker-{i}\', priority_queue)) for i in range(2)] # Wait for queues to be processed await fifo_queue.join() await lifo_queue.join() await priority_queue.join() # Cancel worker tasks for worker in (fifo_workers + lifo_workers + priority_workers): worker.cancel() await asyncio.gather(*fifo_workers, *lifo_workers, *priority_workers, return_exceptions=True) print(\'All tasks processed.\') asyncio.run(main()) ```","solution":"import asyncio import random async def fifo_worker(name, queue): while True: try: sleep_for = await asyncio.wait_for(queue.get(), timeout=10.0) await asyncio.sleep(sleep_for) queue.task_done() print(f\'{name} has slept for {sleep_for:.2f} seconds\') except asyncio.TimeoutError: print(f\'{name} timeout. No tasks to process.\') break async def lifo_worker(name, queue): while True: try: sleep_for = await asyncio.wait_for(queue.get(), timeout=10.0) await asyncio.sleep(sleep_for) queue.task_done() print(f\'{name} has slept for {sleep_for:.2f} seconds\') except asyncio.TimeoutError: print(f\'{name} timeout. No tasks to process.\') break async def priority_worker(name, queue): while True: try: priority, sleep_for = await asyncio.wait_for(queue.get(), timeout=10.0) await asyncio.sleep(sleep_for) queue.task_done() print(f\'{name} has slept for {sleep_for:.2f} seconds with priority {priority}\') except asyncio.TimeoutError: print(f\'{name} timeout. No tasks to process.\') break async def main(): fifo_queue = asyncio.Queue() lifo_queue = asyncio.LifoQueue() priority_queue = asyncio.PriorityQueue() # Populate queues with tasks for _ in range(5): sleep_for = random.uniform(0.05, 1.0) fifo_queue.put_nowait(sleep_for) lifo_queue.put_nowait(sleep_for) priority_queue.put_nowait((random.randint(1, 10), sleep_for)) # Create and start worker tasks fifo_workers = [asyncio.create_task(fifo_worker(f\'fifo_worker-{i}\', fifo_queue)) for i in range(2)] lifo_workers = [asyncio.create_task(lifo_worker(f\'lifo_worker-{i}\', lifo_queue)) for i in range(2)] priority_workers = [asyncio.create_task(priority_worker(f\'priority_worker-{i}\', priority_queue)) for i in range(2)] # Wait for queues to be processed await fifo_queue.join() await lifo_queue.join() await priority_queue.join() # Cancel worker tasks for worker in (fifo_workers + lifo_workers + priority_workers): worker.cancel() await asyncio.gather(*fifo_workers, *lifo_workers, *priority_workers, return_exceptions=True) print(\'All tasks processed.\') if __name__ == \\"__main__\\": asyncio.run(main())"},{"question":"# Sparse Tensor Manipulation and Operations in PyTorch Question: You are given a dense matrix representing the adjacency matrix of a sparse graph. Your task is to perform the following steps: 1. **Convert the dense matrix to a sparse COO tensor:** Write a function that takes a dense matrix and converts it to a sparse COO tensor. 2. **Serialize the sparse tensor:** Implement a function that serializes the sparse COO tensor (i.e., saves its indices and values to a file). 3. **Deserialize the sparse tensor:** Implement a function that loads the serialized tensor data from the file and reconstructs the sparse COO tensor. 4. **Convert the sparse COO tensor to a CSR tensor:** Write a function that converts the given sparse COO tensor to a sparse CSR tensor. 5. **Perform matrix multiplication:** Given another dense matrix, perform matrix multiplication with the sparse CSR tensor, and return the resulting dense matrix. Constraints: - Use PyTorch functions and modules. - You can assume the input matrices are compatible for the matrix multiplication operations. - The serialization and deserialization steps should ensure no information loss. Input and Output Formats: 1. **Conversion to Sparse COO:** - Input: A dense matrix (2D PyTorch tensor). - Output: A sparse COO tensor. 2. **Serialization:** - Input: A sparse COO tensor, a file path (string). - Output: None (the function saves the tensor to disk). 3. **Deserialization:** - Input: A file path (string). - Output: A sparse COO tensor reconstructed from the file. 4. **Conversion to Sparse CSR:** - Input: A sparse COO tensor. - Output: A sparse CSR tensor. 5. **Matrix Multiplication:** - Input: A sparse CSR tensor, a dense matrix. - Output: The resulting dense matrix after matrix multiplication. Function Signatures: - `def dense_to_sparse_coo(dense_matrix: torch.Tensor) -> torch.Tensor:` - `def serialize_sparse_tensor(sparse_tensor: torch.Tensor, file_path: str) -> None:` - `def deserialize_sparse_tensor(file_path: str) -> torch.Tensor:` - `def coo_to_csr(sparse_coo_tensor: torch.Tensor) -> torch.Tensor:` - `def sparse_csr_matmul(sparse_csr_tensor: torch.Tensor, dense_matrix: torch.Tensor) -> torch.Tensor:` Example: ```python import torch dense_matrix = torch.tensor([ [0, 2, 0], [3, 0, 0], [0, 0, 4] ], dtype=torch.float32) # 1. Convert to sparse COO sparse_coo = dense_to_sparse_coo(dense_matrix) # 2. Serialize sparse COO tensor serialize_sparse_tensor(sparse_coo, \'sparse_coo.pt\') # 3. Deserialize sparse COO tensor loaded_sparse_coo = deserialize_sparse_tensor(\'sparse_coo.pt\') # 4. Convert sparse COO to sparse CSR sparse_csr = coo_to_csr(loaded_sparse_coo) # 5. Perform matrix multiplication dense_matrix_for_mul = torch.tensor([ [1, 2, 3], [4, 5, 6], [7, 8, 9] ], dtype=torch.float32) result = sparse_csr_matmul(sparse_csr, dense_matrix_for_mul) print(result) # Expected output: # tensor([[ 8., 10., 12.], # [ 3., 6., 9.], # [28., 32., 36.]]) ``` Write the required functions to complete this task.","solution":"import torch def dense_to_sparse_coo(dense_matrix: torch.Tensor) -> torch.Tensor: Converts a dense matrix to a sparse COO tensor. return dense_matrix.to_sparse_coo() def serialize_sparse_tensor(sparse_tensor: torch.Tensor, file_path: str) -> None: Serializes the sparse COO tensor by saving its indices and values to a file. indices = sparse_tensor._indices() values = sparse_tensor._values() torch.save((indices, values, sparse_tensor.size()), file_path) def deserialize_sparse_tensor(file_path: str) -> torch.Tensor: Deserializes the sparse tensor data from a file and reconstructs the sparse COO tensor. indices, values, size = torch.load(file_path) return torch.sparse_coo_tensor(indices, values, size) def coo_to_csr(sparse_coo_tensor: torch.Tensor) -> torch.Tensor: Converts a sparse COO tensor to a sparse CSR tensor. return sparse_coo_tensor.to_sparse_csr() def sparse_csr_matmul(sparse_csr_tensor: torch.Tensor, dense_matrix: torch.Tensor) -> torch.Tensor: Performs matrix multiplication between a sparse CSR tensor and a dense matrix. return torch.matmul(sparse_csr_tensor, dense_matrix)"},{"question":"Suppose you are developing an email sending application that needs to adapt its behavior based on different policies. Implement a Python function `send_email` that reads an email message from a given file, applies a specified `Policy`, and sends the email using the Unix `sendmail` program. Your solution should correctly handle the use of different policies as required. # Requirements: 1. Your function should take the following parameters: - `file_path`: A string representing the path to the file containing the email message. - `policy_name`: A string representing the policy to be used. It can be one of the following values: `default`, `SMTP`, `SMTPUTF8`, `HTTP`, or `strict`. 2. Your function should: - Read the email message from the specified file using the appropriate policy. - Adjust the policy if necessary (e.g., to set the correct line separators for SMTP). - Use the `sendmail` program to send the email using the recipient address from the email\'s `To` header. 3. Handle any defects encountered during the process by logging them rather than raising an error. For this, all policies should override the `raise_on_defect` attribute to be `False`. # Input: - `file_path`: A string - `policy_name`: A string (must be one of the specified policy names) # Output: - None # Example: ```python send_email(\'mymsg.txt\', \'SMTP\') ``` This function should read the email message from `mymsg.txt`, apply the `SMTP` policy with necessary adjustments for line separators, log any defects, and send the email using the `sendmail` program. # Constraints: - Assume `sendmail` is available on the system where the function is executed. - Use appropriate exception handling where necessary. # Implementation Note: You can use the `logging` module to log defects. Below is a logging setup example: ```python import logging logging.basicConfig(level=logging.INFO) logger = logging.getLogger(__name__) ``` # Starting Template: ```python import logging from email import message_from_binary_file from email.generator import BytesGenerator from email import policy from subprocess import Popen, PIPE logging.basicConfig(level=logging.INFO) logger = logging.getLogger(__name__) def send_email(file_path, policy_name): # Define the mapping of policy names to policy instances policy_mapping = { \'default\': policy.default, \'SMTP\': policy.SMTP, \'SMTPUTF8\': policy.SMTPUTF8, \'HTTP\': policy.HTTP, \'strict\': policy.strict, } # Validate the policy name if policy_name not in policy_mapping: raise ValueError(\\"Invalid policy name.\\") # Get the policy instance selected_policy = policy_mapping[policy_name] # Override raise_on_defect to log defects instead of raising errors selected_policy = selected_policy.clone(raise_on_defect=False) try: # Read the email message using the selected policy with open(file_path, \'rb\') as f: msg = message_from_binary_file(f, policy=selected_policy) # Log any defects encountered if hasattr(msg, \'defects\') and msg.defects: for defect in msg.defects: logger.info(f\\"Defect found: {defect}\\") # Use the correct line separator for SMTP if policy_name in [\'SMTP\', \'SMTPUTF8\']: selected_policy = selected_policy.clone(linesep=\'rn\') # Setup and execute the `sendmail` process recipient = msg[\'To\'].addresses[0] p = Popen([\'sendmail\', recipient], stdin=PIPE) g = BytesGenerator(p.stdin, policy=selected_policy) g.flatten(msg) p.stdin.close() rc = p.wait() if rc != 0: logger.info(\\"Sendmail returned a non-zero exit code.\\") except Exception as e: logger.error(f\\"An error occurred: {e}\\") ```","solution":"import logging from email import message_from_binary_file, message_from_bytes from email.generator import BytesGenerator from email.policy import default, SMTP, SMTPUTF8, HTTP, strict from subprocess import Popen, PIPE import os logging.basicConfig(level=logging.INFO) logger = logging.getLogger(__name__) def send_email(file_path, policy_name): # Define the mapping of policy names to policy instances policy_mapping = { \'default\': default, \'SMTP\': SMTP, \'SMTPUTF8\': SMTPUTF8, \'HTTP\': HTTP, \'strict\': strict, } # Validate the policy name if policy_name not in policy_mapping: raise ValueError(\\"Invalid policy name.\\") # Get the policy instance selected_policy = policy_mapping[policy_name] # Override raise_on_defect to log defects instead of raising errors selected_policy = selected_policy.clone(raise_on_defect=False) try: # Read the email message using the selected policy with open(file_path, \'rb\') as f: msg = message_from_binary_file(f, policy=selected_policy) # Log any defects encountered if hasattr(msg, \'defects\') and msg.defects: for defect in msg.defects: logger.info(f\\"Defect found: {defect}\\") # Use the correct line separator for SMTP if policy_name in [\'SMTP\', \'SMTPUTF8\']: selected_policy = selected_policy.clone(linesep=\'rn\') # Setup and execute the `sendmail` process recipient = msg[\'To\'] p = Popen([\'sendmail\', recipient], stdin=PIPE) g = BytesGenerator(p.stdin, policy=selected_policy) g.flatten(msg) p.stdin.close() rc = p.wait() if rc != 0: logger.info(\\"Sendmail returned a non-zero exit code.\\") except Exception as e: logger.error(f\\"An error occurred: {e}\\")"},{"question":"Topological Sorting of Graph Tasks # Problem Statement You are given a task to manage a group of dependent tasks and schedule them in a valid order using topological sorting. Each task depends on one or more preceding tasks, forming a directed acyclic graph (DAG). Your objective is to write a function that takes a dictionary representing the graph and returns a list of tasks in topological order. # Function Signature ```python def topological_sort(tasks: Dict[str, Set[str]]) -> List[str]: ``` # Input - `tasks`: A dictionary where each key is a task (a string) and the corresponding value is a set of tasks (strings) that must precede the key task. # Output - A list of strings representing the tasks in topological order. # Constraints - The graph represented by `tasks` is always a directed acyclic graph (DAG). - All task identifiers (keys and values) are unique strings. - The graph is non-empty and contains at least one task. # Example ```python tasks = { \\"D\\": {\\"B\\", \\"C\\"}, \\"C\\": {\\"A\\"}, \\"B\\": {\\"A\\"} } print(topological_sort(tasks)) # Example output: [\\"A\\", \\"C\\", \\"B\\", \\"D\\"] ``` # Requirements 1. Use the `TopologicalSorter` class from the `graphlib` module to perform the topological sorting. 2. Properly handle adding nodes and their predecessors. 3. Ensure that you call the necessary methods (`prepare`, `get_ready`, `done`, etc.) in the correct order. # Additional Notes - The output list can have multiple valid answers, as long as the topological order constraints are maintained. - You do not need to handle the case of cycles since the input graph is guaranteed to be a DAG. # Implementation Guidelines - Instantiate the `TopologicalSorter` class with the provided graph. - Use the `prepare` and `get_ready` methods to retrieve nodes in the correct order, and mark them as done with `done` as you process them. - Return the resulting order as a list of tasks.","solution":"from typing import Dict, Set, List from graphlib import TopologicalSorter def topological_sort(tasks: Dict[str, Set[str]]) -> List[str]: Perform a topological sort on the provided graph of tasks. Args: tasks (Dict[str, Set[str]]): The directed acyclic graph represented by a dictionary where each key is a task and the value is a set of tasks that must precede the key task. Returns: List[str]: A list of tasks in topological order. sorter = TopologicalSorter(tasks) return list(sorter.static_order())"},{"question":"**Question: Advanced Seaborn Visualization and Customization** **Objective:** Assess the student\'s ability to use seaborn for creating various plot types, integrating with pandas data frames, and customizing plot aesthetics. **Problem Statement:** You\'re given a dataset containing information about tips received by waiters and waitresses in a restaurant. Your task is to create several visualizations using seaborn to analyze this dataset. The dataset is provided in a CSV file named `tips.csv` with the following columns: - `total_bill`: Total bill amount (float) - `tip`: Tip amount given (float) - `sex`: Gender of the person who paid the bill (categorical: \'Male\' or \'Female\') - `smoker`: Whether the person is a smoker (categorical: \'Yes\' or \'No\') - `day`: Day of the week (categorical: \'Thur\', \'Fri\', \'Sat\', \'Sun\') - `time`: Time of day (categorical: \'Lunch\', \'Dinner\') - `size`: Number of people at the table (integer) **Tasks:** 1. **Visualization 1: Distribution of Total Bill Amounts** - Create a histogram with Kernel Density Estimate (KDE) overlay for the `total_bill` column. - Customize the plot: set the style to `\\"darkgrid\\"`, set the plot title, and label the x-axis and y-axis. 2. **Visualization 2: Scatter Plot of Total Bill vs Tip** - Create a scatter plot to show the relationship between `total_bill` and `tip`. - Customize the plot by coloring the points based on the `time` (Lunch/Dinner), using different markers for `smoker` status. 3. **Visualization 3: Box Plot of Tips by Day** - Create a box plot to show the distribution of `tip` amounts for each day of the week. - Customize the plot with distinct colors for each day and set the plot title and axes labels. 4. **Visualization 4: Pair Plot of Numerical Columns** - Generate a pair plot for all numerical columns (`total_bill`, `tip`, `size`), with different colors for each `sex`. **Input:** - Load the dataset from the CSV file `tips.csv`. **Output:** - Save each of the visualizations as PNG files in the current directory with the filenames: - `total_bill_distribution.png` - `total_bill_vs_tip.png` - `tips_by_day.png` - `pair_plot.png` **Constraints:** - You must use seaborn for creating all visualizations. - Customize the plots as specified in each task. **Function Signature:** ```python def analyze_tips_data(filepath: str): # Your code here pass ``` **Example:** ```python # Example usage analyze_tips_data(\'tips.csv\') # This will generate and save four visualization images in the current directory. ``` **Relevant Documentation:** - seabor\'s `load_dataset()`: https://seaborn.pydata.org/generated/seaborn.load_dataset.html - seaborn\'s `histplot()`: https://seaborn.pydata.org/generated/seaborn.histplot.html - seaborn\'s `scatterplot()`: https://seaborn.pydata.org/generated/seaborn.scatterplot.html - seaborn\'s `boxplot()`: https://seaborn.pydata.org/generated/seaborn.boxplot.html - seaborn\'s `pairplot()`: https://seaborn.pydata.org/generated/seaborn.pairplot.html","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt def analyze_tips_data(filepath: str): # Load the dataset tips = pd.read_csv(filepath) # Visualization 1: Distribution of Total Bill Amounts sns.set_style(\'darkgrid\') plt.figure(figsize=(10, 6)) sns.histplot(tips[\'total_bill\'], kde=True) plt.title(\'Distribution of Total Bill Amounts\') plt.xlabel(\'Total Bill\') plt.ylabel(\'Frequency\') plt.savefig(\'total_bill_distribution.png\') plt.close() # Visualization 2: Scatter Plot of Total Bill vs Tip plt.figure(figsize=(10, 6)) sns.scatterplot( data=tips, x=\'total_bill\', y=\'tip\', hue=\'time\', style=\'smoker\', palette=\'Set2\' ) plt.title(\'Total Bill vs Tip, Differentiated by Time and Smoker Status\') plt.xlabel(\'Total Bill\') plt.ylabel(\'Tip\') plt.legend(title=\'Time & Smoker Status\') plt.savefig(\'total_bill_vs_tip.png\') plt.close() # Visualization 3: Box Plot of Tips by Day plt.figure(figsize=(10, 6)) sns.boxplot(data=tips, x=\'day\', y=\'tip\', palette=\'Set3\') plt.title(\'Tips by Day\') plt.xlabel(\'Day\') plt.ylabel(\'Tip\') plt.savefig(\'tips_by_day.png\') plt.close() # Visualization 4: Pair Plot of Numerical Columns plt.figure(figsize=(10, 6)) sns.pairplot(tips, vars=[\'total_bill\', \'tip\', \'size\'], hue=\'sex\', palette=\'husl\') plt.savefig(\'pair_plot.png\') plt.close()"},{"question":"# Advanced Python Asyncio Queue Implementation Problem Statement You are tasked with creating a custom scheduling system that distributes tasks based on their priority and ensures they are processed according to specific constraints using `asyncio`. Implement functionalities using `asyncio.Queue`, `asyncio.PriorityQueue`, and `asyncio.LifoQueue`. Your scheduler should support: 1. **Adding tasks**: Tasks can be added with specific details including priority and a task type. 2. **Processing tasks**: Implement worker coroutines that retrieve and process tasks. 3. **Handling and tracking completed tasks**: Use appropriate queue operations to mark tasks as completed and ensure all tasks are processed. 4. **Handling exceptions**: Appropriately handle cases when the queue is empty or full. Requirements 1. **Function signatures**: Implement the following functions: - `async def add_task(queue, task_type, priority=None, detail=None)` - **Input**: - `queue`: The target queue (`asyncio.Queue`, `asyncio.PriorityQueue`, or `asyncio.LifoQueue`). - `task_type`: Type of the task. (string: \\"FIFO\\", \\"Priority\\", \\"LIFO\\") - `priority`: Priority of the task (needed if task_type is `PriorityQueue`). (integer) - `detail`: Additional task-specific details. (any type) - **Output**: Add the task to the provided queue. - `async def worker(queue)`: - **Input**: - `queue`: The target queue from which tasks are fetched. - **Output**: Fetch and process tasks until the queue is empty. - `async def main()`: - Create and manage different queues. - Add tasks to the queues. - Start worker coroutines. - Wait for all processing to complete. 2. **Constraints**: - Use the `asyncio` module for coroutine management. - Ensure task addition and processing are non-blocking. - Use appropriate queue and exception handling methods. - Implement at least one example demonstrating the scheduling system. Example ```python import asyncio import random async def add_task(queue, task_type, priority=None, detail=None): if task_type == \'FIFO\': await queue.put(detail) elif task_type == \'Priority\': await queue.put((priority, detail)) elif task_type == \'LIFO\': await queue.put(detail) else: raise ValueError(\\"Unsupported task_type\\") async def worker(queue): while not queue.empty(): try: if isinstance(queue, asyncio.PriorityQueue): priority, task = await queue.get() else: task = await queue.get() await asyncio.sleep(task) queue.task_done() print(f\\"Processed task: {task}\\") except asyncio.QueueEmpty: pass async def main(): fifo_queue = asyncio.Queue() priority_queue = asyncio.PriorityQueue() lifo_queue = asyncio.LifoQueue() tasks = [ (fifo_queue, \'FIFO\', None, random.uniform(0.1, 1.0)), (priority_queue, \'Priority\', random.randint(1, 10), random.uniform(0.1, 1.0)), (lifo_queue, \'LIFO\', None, random.uniform(0.1, 1.0)) ] for _ in range(10): for queue, task_type, priority, detail in tasks: await add_task(queue, task_type, priority, detail) worker_tasks = [asyncio.create_task(worker(q)) for q in [fifo_queue, priority_queue, lifo_queue]] await asyncio.gather(*worker_tasks) asyncio.run(main()) ``` Your implementation should be executed in a similar structure, demonstrating the handling and scheduling of diverse task types. Include comments to explain your code and its behavior.","solution":"import asyncio async def add_task(queue, task_type, priority=None, detail=None): Adds a task to the appropriate queue based on task type. if task_type == \'FIFO\': await queue.put(detail) elif task_type == \'Priority\': await queue.put((priority, detail)) elif task_type == \'LIFO\': await queue.put(detail) else: raise ValueError(\\"Unsupported task_type\\") async def worker(queue): Processes tasks from the queue. If the queue is empty, it handles the exception gracefully. while not queue.empty(): try: if isinstance(queue, asyncio.PriorityQueue): priority, task = await queue.get() else: task = await queue.get() await asyncio.sleep(task) # Simulate work by sleeping queue.task_done() print(f\\"Processed task: {task}\\") except asyncio.QueueEmpty: pass async def main(): Main function that sets up queues, adds tasks, and starts worker coroutines. fifo_queue = asyncio.Queue() priority_queue = asyncio.PriorityQueue() lifo_queue = asyncio.LifoQueue() tasks = [ (fifo_queue, \'FIFO\', None, 0.1), (priority_queue, \'Priority\', 1, 0.2), (lifo_queue, \'LIFO\', None, 0.3) ] # Add tasks to each queue for queue, task_type, priority, detail in tasks: for _ in range(3): await add_task(queue, task_type, priority, detail) worker_tasks = [ asyncio.create_task(worker(fifo_queue)), asyncio.create_task(worker(priority_queue)), asyncio.create_task(worker(lifo_queue)) ] await asyncio.gather(*worker_tasks) # Uncomment the line below to run the main function when running this file directly. # asyncio.run(main())"},{"question":"Custom Signal Handler and Timeout Management You are tasked with implementing a function that establishes a custom signal handler, schedules an alarm, and waits for a signal using the `signal` module. This function must demonstrate a solid understanding of signal handling, exception management, and alarm scheduling. # Function Signature ```python def handle_signal_and_timeout(signal_num: int, timeout: int, alert_msg: str) -> None: pass ``` # Input Parameters - `signal_num` (int): The signal number for which the custom handler should be set. - `timeout` (int): The time in seconds after which an alarm signal should be scheduled. - `alert_msg` (str): The message to be printed by the custom handler when the signal is received. # Function Behavior 1. **Setting the Signal Handler**: - Set a custom signal handler that captures the signal specified by `signal_num`. - When the signal is received, the handler should print `alert_msg` to the console. - Additionally, the handler should raise a `RuntimeError` with the message `\\"Signal received\\"`. 2. **Scheduling an Alarm**: - Schedule an alarm to send a `SIGALRM` signal after `timeout` seconds. - If the alarm goes off, the custom signal handler should be executed. 3. **Waiting for the Signal**: - Use the `signal.pause()` function to wait for the signal indefinitely. # Constraints - The function should only be invoked from the main thread. - The signal number provided should be a valid signal for the operating system. # Example Usage ```python import signal def handle_signal_and_timeout(signal_num: int, timeout: int, alert_msg: str) -> None: def custom_handler(signum, frame): print(alert_msg) raise RuntimeError(\\"Signal received\\") # Set the signal handler signal.signal(signal_num, custom_handler) # Schedule the alarm signal.alarm(timeout) # Wait for the signal signal.pause() # Example invocation # handle_signal_and_timeout(signal.SIGALRM, 5, \\"Alarm signal received\\") # Expected output after 5 seconds: # Alarm signal received # RuntimeError: Signal received ``` # Notes - Ensure to provide comments within your code for clarity. - Make sure to handle edge cases where the function may be called from a non-main thread (hint: use `threading` module to verify).","solution":"import signal import threading def handle_signal_and_timeout(signal_num: int, timeout: int, alert_msg: str) -> None: if threading.current_thread() is not threading.main_thread(): raise RuntimeError(\\"This function must be called from the main thread\\") def custom_handler(signum, frame): print(alert_msg) raise RuntimeError(\\"Signal received\\") # Set the signal handler signal.signal(signal_num, custom_handler) # Schedule the alarm signal.alarm(timeout) # Wait for the signal signal.pause()"},{"question":"# Question: Implementing Custom Deep Copy for a Recursive Data Structure **Context**: You are given a class `TreeNode` that represents nodes in a binary tree. Each `TreeNode` has a value and references to its left and right children. Because of the potential for recursive structures (e.g., if a node references back to one of its ancestors), creating deep copies of such structures requires careful handling to avoid infinite recursion. **Task**: Implement custom deep copy functionality for the `TreeNode` class using the `copy` module. Your implementation should handle recursive references correctly by using the `memo` dictionary to avoid redundant copying and infinite loops. **Requirements**: 1. Implement the `__deepcopy__` method for the `TreeNode` class. 2. Use the `copy.deepcopy` function within the `__deepcopy__` method to ensure that all child nodes are recursively deep-copied. 3. Handle the `memo` dictionary correctly to manage already copied nodes. **Class Definition**: ```python class TreeNode: def __init__(self, value, left=None, right=None): self.value = value self.left = left self.right = right def __repr__(self): return f\\"TreeNode({self.value}, {repr(self.left)}, {repr(self.right)})\\" def __deepcopy__(self, memo): # Your implementation goes here pass ``` **Input and Output**: - Define test cases where you create a `TreeNode` object and its subtrees. - Use the `copy.deepcopy` function on these test cases and demonstrate that deep copying works correctly by printing the original and copied structures. **Constraints**: - Ensure your solution handles trees with recursive references. - Assume that the `value` of each `TreeNode` is immutable and does not need to be copied. - Test your implementation thoroughly to ensure correctness. **Example**: ```python import copy if __name__ == \\"__main__\\": # Create a tree structure with recursive references root = TreeNode(1) root.left = TreeNode(2) root.right = TreeNode(3) root.left.left = TreeNode(4) root.left.right = root # Introduce a recursive reference print(\\"Original tree:\\") print(root) copied_root = copy.deepcopy(root) print(\\"Deep copied tree:\\") print(copied_root) ``` In this example, ensure that the deep copy of `root` handles the recursive reference correctly and does not enter an infinite loop.","solution":"import copy class TreeNode: def __init__(self, value, left=None, right=None): self.value = value self.left = left self.right = right def __repr__(self): return f\\"TreeNode({self.value}, {repr(self.left)}, {repr(self.right)})\\" def __deepcopy__(self, memo): if id(self) in memo: return memo[id(self)] copied_node = TreeNode(self.value) memo[id(self)] = copied_node copied_node.left = copy.deepcopy(self.left, memo) copied_node.right = copy.deepcopy(self.right, memo) return copied_node"},{"question":"**Objective:** Design a function to process a sequence of tasks based on their dependencies using the `graphlib.TopologicalSorter` class. Your function should handle dependencies and detect any cycles in the provided graph of tasks. **Task:** Write a function `process_tasks(tasks: Dict[Hashable, List[Hashable]]) -> Union[List[Hashable], str]` that takes a dictionary representing a graph of tasks and their dependencies and returns a list of tasks in a valid topological order. If the graph contains cycles, the function should return the string `\\"Cycle detected\\"`. **Input:** - `tasks`: A dictionary where keys are task names (hashable items) and values are lists of task names representing dependencies that must be completed before the key task. **Output:** - A list of task names in a valid topological order if no cycles exist. - The string `\\"Cycle detected\\"` if the graph contains cycles. **Constraints:** 1. Each task name and its dependencies are unique and hashable. 2. The function will not modify the input dictionary. **Example:** ```python def process_tasks(tasks): # Your implementation here pass # Example 1 input_tasks = { \\"D\\": [\\"B\\", \\"C\\"], \\"C\\": [\\"A\\"], \\"B\\": [\\"A\\"], \\"A\\": [] } print(process_tasks(input_tasks)) # Output: [\\"A\\", \\"C\\", \\"B\\", \\"D\\"] # Example 2 input_tasks = { \\"A\\": [\\"B\\"], \\"B\\": [\\"A\\"] } print(process_tasks(input_tasks)) # Output: \\"Cycle detected\\" ``` **Explanation:** In the first example, task \\"A\\" has to be done before \\"C\\" and \\"B\\", and \\"C\\" and \\"B\\" have to be done before \\"D\\". The valid topological order is [\\"A\\", \\"C\\", \\"B\\", \\"D\\"]. In the second example, there is a cycle between tasks \\"A\\" and \\"B\\". Since they depend on each other, no valid topological order exists, so the function returns \\"Cycle detected\\". **Notes:** 1. You should use the `graphlib.TopologicalSorter` class to handle the topological sorting. 2. The function should be efficient and handle any valid directed acyclic graph within reasonable limits in terms of size and complexity.","solution":"from typing import Dict, Hashable, List, Union from graphlib import TopologicalSorter, CycleError def process_tasks(tasks: Dict[Hashable, List[Hashable]]) -> Union[List[Hashable], str]: Returns a list of tasks in a valid topological order or \'Cycle detected\' if a cycle exists. ts = TopologicalSorter(tasks) try: order = list(ts.static_order()) return order except CycleError: return \\"Cycle detected\\""},{"question":"# Advanced Task Management with asyncio **Objective**: Design a Python program that demonstrates efficient multi-task management using the `asyncio` library. Your task is to implement a function `process_tasks` that takes a list of coroutines, runs them concurrently, manages their completion, and handles potential timeouts and cancellations. **Requirements**: 1. **Function Signature**: ```python import asyncio from typing import List, Awaitable async def process_tasks(coroutines: List[Awaitable], timeout: float) -> List: pass ``` 2. **Parameters**: - `coroutines`: A list of coroutine objects to be executed concurrently. - `timeout`: The maximum time (in seconds) to wait for the coroutines to complete. 3. **Functionality**: - The function should run all provided coroutines concurrently. - If all coroutines complete within the given timeout, the function should return a list of their results in the order they were provided. - If the timeout is reached before all coroutines complete, the function should cancel any remaining unfinished coroutines and return results of the completed coroutines up until the timeout. For any cancelled coroutines, include a string \\"Cancelled\\" in the result list at their respective positions. - Handle any exceptions that might occur within the coroutines by including the exception in the result list at the position of the coroutine that raised it. 4. **Example Usage**: ```python import asyncio from typing import List, Awaitable async def sample_task(delay: int, result: str) -> str: await asyncio.sleep(delay) return result async def process_tasks(coroutines: List[Awaitable], timeout: float) -> List: # Your implementation here async def main(): tasks = [ sample_task(1, \\"Task 1\\"), sample_task(2, \\"Task 2\\"), sample_task(3, \\"Task 3\\"), ] results = await process_tasks(tasks, timeout=4) print(results) # Expected output: [\\"Task 1\\", \\"Task 2\\", \\"Task 3\\"] results = await process_tasks(tasks, timeout=2) print(results) # Expected output: [\\"Task 1\\", \\"Task 2\\", \\"Cancelled\\"] asyncio.run(main()) ``` **Constraints**: - The number of coroutines, `n`, will not exceed 1000. - The `timeout` value will be a positive float not larger than 1000. **Performance Requirements**: - The function should make efficient use of asyncio\'s concurrency capabilities and ensure minimal blocking on I/O operations. **Hint**: - You may find `asyncio.wait`, `asyncio.gather`, and `asyncio.shield` helpful in your implementation. - Ensure that you correctly handle both completion and cancellation of tasks. **Additional Information**: The function should be designed to maintain robust error handling and clear concurrency management to showcase the advanced usage of the `asyncio` library.","solution":"import asyncio from typing import List, Awaitable async def process_tasks(coroutines: List[Awaitable], timeout: float) -> List: async def safe_wrapped_coro(coro): try: return await coro except asyncio.CancelledError: return \\"Cancelled\\" except Exception as e: return e tasks = [asyncio.create_task(safe_wrapped_coro(coro)) for coro in coroutines] try: return await asyncio.wait_for(asyncio.gather(*tasks), timeout=timeout) except asyncio.TimeoutError: for task in tasks: task.cancel() return await asyncio.gather(*tasks, return_exceptions=True)"},{"question":"Objective Your task is to implement a Python function that utilizes the `timeit` module to compare the execution time of two different code snippets. Specifically, you need to compare the performance of a list comprehension versus a generator expression for squaring numbers in a given range. Function Signature ```python def compare_execution_times(n: int) -> dict: pass ``` Input - `n` (int): The upper limit of the range (exclusive) for generating numbers to be squared. Output - A dictionary with the following structure: ```python { \\"list_comprehension_time\\": float, # Time taken by list comprehension \\"generator_expression_time\\": float # Time taken by generator expression } ``` Constraints - `n` will be a positive integer (1 ≤ n ≤ 10^6). Instructions 1. Use the `timeit` module to measure the execution time of squaring numbers from 0 to `n-1` using a list comprehension and a generator expression. 2. Implement the setup code as necessary. 3. Ensure your implementation runs each snippet a sufficient number of times to get a reliable measurement (you can use the default number of executions provided by `timeit`). Example ```python results = compare_execution_times(1000) print(results) # Example output: {\'list_comprehension_time\': 0.026, \'generator_expression_time\': 0.038} ``` Performance Requirements Your solution should ensure minimal overhead in timing and provide accurate results by leveraging multiple repetitions. Hint - You might want to set up the timing code by defining the list comprehension and the generator expression within the timing functions.","solution":"import timeit def compare_execution_times(n: int) -> dict: Compare the execution times of squaring numbers from 0 to n-1 using list comprehension and generator expression. Parameters: n (int): The upper limit of the range (exclusive) for generating numbers to be squared. Returns: dict: Execution times of list comprehension and generator expression. setup = f\\"nums = range({n})\\" # Timing the list comprehension list_comprehension_code = squares = [x * x for x in nums] list_comprehension_time = timeit.timeit(stmt=list_comprehension_code, setup=setup, number=100) # Timing the generator expression generator_expression_code = squares = (x * x for x in nums) generator_expression_time = timeit.timeit(stmt=generator_expression_code, setup=setup, number=100) return { \\"list_comprehension_time\\": list_comprehension_time, \\"generator_expression_time\\": generator_expression_time }"},{"question":"# CSV Data Manipulation and Customization Implement a Python program that performs the following tasks: 1. **Read Data**: Read data from a CSV file named `input.csv` using a `DictReader` object. Assume the first row contains the headers `Name`, `Age`, and `Salary`. 2. **Data Processing**: - Increment the `Age` of each person by 1 year. - Increase the `Salary` of each person by 10%. Ensure salary is stored as an integer. 3. **Write Data**: Write the modified data to a new CSV file named `output.csv` using a `DictWriter` object. Ensure that the output CSV has the same headers as the input CSV. 4. **Custom Dialect**: Register a custom CSV dialect named `custom` which uses `;` as the delimiter, `\\"` as the quote character, and allows spaces immediately following the delimiter. Write the output using this custom dialect. 5. **Error Handling**: Incorporate appropriate error handling to manage issues such as missing files or improper data formats. Considerations: - Maintain adherence to the CSV format regarding delimiters and quoting as described. - Ensure proper handling of various data types such as integer increments and percentage calculations. - Utilize the `Sniffer` class to detect and validate the header of a CSV file. # Input and Output Formats Input: - `input.csv`: Sample content: ``` Name,Age,Salary John Doe,30,60000 Jane Smith,25,50000 ``` Output: - `output.csv`: Expected content with custom dialect: ``` \\"Name\\";\\"Age\\";\\"Salary\\" \\"John Doe\\";\\"31\\";\\"66000\\" \\"Jane Smith\\";\\"26\\";\\"55000\\" ``` Constraints: - Assume all input data is valid and formatted as described. - Ensure that the program performs operations efficiently and provides meaningful error messages. # Performance Requirements: - The solution should handle CSV files with up to 10,000 rows efficiently. - The program should complete processing within a reasonable time frame (e.g., a few seconds for 10k rows). # Example Code Execution ```python # Sample code snippet def main(): # Read input.csv, process data, and write output.csv pass if __name__ == \\"__main__\\": main() ``` Write the complete Python function implementing the above requirements in an efficient and readable manner.","solution":"import csv def process_csv(input_file, output_file): try: with open(input_file, mode=\'r\', newline=\'\') as infile: reader = csv.DictReader(infile) headers = reader.fieldnames data = [] for row in reader: row[\'Age\'] = str(int(row[\'Age\']) + 1) row[\'Salary\'] = str(int(int(row[\'Salary\']) * 1.1)) data.append(row) csv.register_dialect(\'custom\', delimiter=\';\', quotechar=\'\\"\', skipinitialspace=True) with open(output_file, mode=\'w\', newline=\'\') as outfile: writer = csv.DictWriter(outfile, fieldnames=headers, dialect=\'custom\') writer.writeheader() writer.writerows(data) except FileNotFoundError as e: print(f\\"The file {input_file} does not exist.\\") except Exception as e: print(f\\"An error occurred: {e}\\") def main(): process_csv(\'input.csv\', \'output.csv\') if __name__ == \\"__main__\\": main()"},{"question":"**Question: Advanced Data Visualization with Seaborn** The objective of this task is to assess your understanding of the `seaborn` library, particularly your ability to create advanced visualizations by utilizing different facets, mappings, and plot customizations. # Task You are provided with a dataset named `tips` that contains information about bills in a restaurant. For this task, you need to write a function `plot_advanced_scatterplot` that takes the `tips` dataset and produces an advanced scatter plot using the `relplot` function. This plot should include: 1. A scatter plot of `total_bill` vs. `tip`. 2. The plot should be faceted by the `time` variable (i.e., create separate plots for Lunch and Dinner). 3. Each facet should use different colors for different days (`day` variable) and different markers for different `day`. 4. Points should be sized by the `size` variable. 5. Customize the plot by setting the range of marker sizes using the `sizes` parameter (20, 200). # Function Signature ```python def plot_advanced_scatterplot(tips: pd.DataFrame) -> None: pass ``` # Input - `tips`: A DataFrame containing the `tips` dataset with the following columns: `total_bill`, `tip`, `sex`, `smoker`, `day`, `time`, `size`. # Output - The function should produce and display the scatter plot as described. # Example Usage ```python import seaborn as sns # Load the tips dataset tips = sns.load_dataset(\\"tips\\") # Call the function to create the plot plot_advanced_scatterplot(tips) ``` # Constraints - The function should use the seaborn library to plot the graph. - Ensure that all customizations (like marker sizes and facet settings) are correctly applied. # Notes - You may assume that seaborn is already imported as `sns` and matplotlib is imported as `plt`. - The dataset loading step is provided above; you do not need to include it in your function. Your implementation will be evaluated based on the correctness and clarity of the produced plots as well as compliance with the specified customizations. Good luck!","solution":"import seaborn as sns import matplotlib.pyplot as plt import pandas as pd def plot_advanced_scatterplot(tips: pd.DataFrame) -> None: Produce an advanced scatter plot using the `tips` dataset. Parameters: - tips: A DataFrame containing the `tips` dataset. sns.relplot( data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"day\\", style=\\"day\\", size=\\"size\\", col=\\"time\\", sizes=(20, 200), kind=\\"scatter\\", palette=\\"deep\\" ) plt.show()"},{"question":"Custom Autograd Function with Hooks Objectives: - Implement a custom autograd function using PyTorch. - Utilize hooks to interact with the forward and backward passes. - Demonstrate understanding of gradient computation and tensor manipulation. Background: You are provided with an overview of PyTorch\'s autograd mechanics. Using this knowledge, you need to create a custom autograd function for polynomial evaluation and integrate hooks to examine and manipulate the gradient flow. Task: 1. **Custom Autograd Function**: - Create a custom autograd function named `PolynomialFunction` that computes the value of a polynomial (y = a times x^3 + b times x^2 + c times x + d). - The coefficients (a, b, c, and d) should be provided as inputs alongside (x). 2. **Implement Hooks**: - Register a hook on the gradient of the output during the forward pass to print the gradient. - Register a hook on the tensor to accumulate gradients and print them after the backward pass. 3. **Testing**: - Create an instance of `PolynomialFunction` with specific coefficients. - Compute the result of the polynomial for an input tensor `x`. - Perform backpropagation to compute gradients. - Use a mean squared error loss function to compute the loss and run backpropagation. - Validate that hooks are correctly printing the gradients as expected. Code Template: ```python import torch class PolynomialFunction(torch.autograd.Function): @staticmethod def forward(ctx, x, a, b, c, d): # Save the coefficients for backward computation ctx.save_for_backward(x, a, b, c, d) # Register a hook to print the gradient during the forward pass x.register_hook(lambda grad: print(f\\"Forward pass gradient: {grad}\\")) # Compute the polynomial value return a * x ** 3 + b * x ** 2 + c * x + d @staticmethod def backward(ctx, grad_output): # Retrieve saved values x, a, b, c, d = ctx.saved_tensors # Compute gradients for each input grad_x = grad_output * (3 * a * x ** 2 + 2 * b * x + c) grad_a = grad_output * (x ** 3) grad_b = grad_output * (x ** 2) grad_c = grad_output * x grad_d = grad_output return grad_x, grad_a, grad_b, grad_c, grad_d # Testing the implementation x = torch.tensor([2.0], requires_grad=True) a, b, c, d = torch.tensor([1.0, -3.0, 2.0, 4.0], requires_grad=True) # Initialize PolynomialFunction using forward method output = PolynomialFunction.apply(x, a, b, c, d) # Register a hook on the output to print the gradient after backward pass output.register_hook(lambda grad: print(f\\"Backward pass gradient at output: {grad}\\")) # Define a mean squared error loss target = torch.tensor([1.0]) loss = (output - target).pow(2).mean() # Perform backpropagation loss.backward() # Print gradients of x, a, b, c, d print(f\\"x.grad: {x.grad}\\") print(f\\"a.grad: {a.grad}\\") print(f\\"b.grad: {b.grad}\\") print(f\\"c.grad: {c.grad}\\") print(f\\"d.grad: {d.grad}\\") ``` Expectations: - The custom autograd function should handle forward and backward passes correctly. - The hooks should provide insight into the gradients during the forward and backward passes. - Proper gradients should be computed for all parameters (x, a, b, c, d) after the backward pass. Constraints: - Use only PyTorch functionalities. - Do not use any additional libraries for autodiff. - Ensure solution compatibility with PyTorch version 1.8.0 or later.","solution":"import torch class PolynomialFunction(torch.autograd.Function): @staticmethod def forward(ctx, x, a, b, c, d): # Save the coefficients for backward computation ctx.save_for_backward(x, a, b, c, d) # Compute the polynomial value return a * x ** 3 + b * x ** 2 + c * x + d @staticmethod def backward(ctx, grad_output): # Retrieve saved values x, a, b, c, d = ctx.saved_tensors # Compute gradients for each input grad_x = grad_output * (3 * a * x ** 2 + 2 * b * x + c) grad_a = grad_output * (x ** 3) grad_b = grad_output * (x ** 2) grad_c = grad_output * x grad_d = grad_output return grad_x, grad_a, grad_b, grad_c, grad_d # Demonstrating the use of PolynomialFunction and adding hooks for gradient flow x = torch.tensor([2.0], requires_grad=True) a = torch.tensor([1.0], requires_grad=True) b = torch.tensor([-3.0], requires_grad=True) c = torch.tensor([2.0], requires_grad=True) d = torch.tensor([4.0], requires_grad=True) # Initialize PolynomialFunction using forward method output = PolynomialFunction.apply(x, a, b, c, d) # Register a hook on the gradient of `x` to print the gradient during the forward pass x.register_hook(lambda grad: print(f\\"Forward pass gradient for x: {grad}\\")) # Register a hook on the output to print the gradient after the backward pass output.register_hook(lambda grad: print(f\\"Backward pass gradient at output: {grad}\\")) # Define a mean squared error loss target = torch.tensor([1.0]) # Calculate the loss loss = (output - target).pow(2).mean() # Perform backpropagation loss.backward() # Print gradients of x, a, b, c, d print(f\\"x.grad: {x.grad}\\") print(f\\"a.grad: {a.grad}\\") print(f\\"b.grad: {b.grad}\\") print(f\\"c.grad: {c.grad}\\") print(f\\"d.grad: {d.grad}\\")"},{"question":"# Secure Token Generation and Password Validation As a software developer, you are tasked with creating a secure password reset token generator and a function to verify the strength of a user-created password, ensuring it meets certain security criteria. Implement the following two functions: 1. `generate_secure_token(nbytes: int) -> str` This function generates a secure token of length `nbytes` bytes. The token should be a URL-safe text string. - **Input**: An integer `nbytes` representing the number of bytes of randomness. - **Output**: A string representing the URL-safe token. For example: ```python token = generate_secure_token(16) print(token) # Might output: \'Drmhze6EPcv0fN_81Bj-nA\' ``` 2. `validate_password(password: str) -> bool` This function checks if the provided password meets the following security criteria: - At least 8 characters long. - Contains at least one lowercase letter. - Contains at least one uppercase letter. - Contains at least one digit. - Contains at least one special character from the set `!@#%^&*()-_+=`. - **Input**: A string `password` representing the user\'s password. - **Output**: `True` if the password meets all the criteria, otherwise `False`. For example: ```python print(validate_password(\\"Password123!\\")) # Should output: True print(validate_password(\\"password\\")) # Should output: False ``` Constraints: - You must use the `secrets` module for generating the secure token. - The validation function must check the password against all mentioned criteria. Implementation: ```python import secrets import string def generate_secure_token(nbytes: int) -> str: # Generate a secure token using secrets.token_urlsafe() token = secrets.token_urlsafe(nbytes) return token def validate_password(password: str) -> bool: # Define the required character sets special_characters = \\"!@#%^&*()-_+=\\" # Check the length of the password if len(password) < 8: return False # Check the presence of required character types has_lower = any(c.islower() for c in password) has_upper = any(c.isupper() for c in password) has_digit = any(c.isdigit() for c in password) has_special = any(c in special_characters for c in password) return has_lower and has_upper and has_digit and has_special ``` Test your implementation thoroughly to ensure both functions perform as expected.","solution":"import secrets import string def generate_secure_token(nbytes: int) -> str: Generates a secure random URL-safe token of specified number of bytes. return secrets.token_urlsafe(nbytes) def validate_password(password: str) -> bool: Validates that a password meets certain security criteria: - At least 8 characters long. - Contains at least one lowercase letter. - Contains at least one uppercase letter. - Contains at least one digit. - Contains at least one special character from the set `!@#%^&*()-_+=`. if len(password) < 8: return False has_lower = any(c.islower() for c in password) has_upper = any(c.isupper() for c in password) has_digit = any(c.isdigit() for c in password) has_special = any(c in \\"!@#%^&*()-_+=\\" for c in password) return has_lower and has_upper and has_digit and has_special"},{"question":"# File Type and Permissions Analyzer **Objective:** You are required to write two functions utilizing the `stat` module - `list_files_of_type` and `set_permission_bits`. These functions will perform the following tasks: 1. **list_files_of_type(directory, file_type_flag):** - This function should return a list of all files in the given directory (including its subdirectories) that match the specified file type. - Use constants like `stat.S_IFREG`, `stat.S_IFDIR`, etc., passed in `file_type_flag` to determine the type of files to include in the list. - The function should recursively traverse directories. - Return the list of file paths that match the file type. 2. **set_permission_bits(file_path, permissions):** - This function should set the permission bits of the file at `file_path` to the specified `permissions`. - Use the function `stat.S_IMODE()` to mask the file mode bits that can be set by `os.chmod()`. - Make sure to only set the permission bits (rwx for user, group, and others) without altering other bits. **Function Definitions:** ```python import os import stat def list_files_of_type(directory: str, file_type_flag: int) -> list: Recursively lists all files of a specific type in the given directory. Parameters: directory (str): The directory path to search within. file_type_flag (int): The file type flag indicating the type of files to be listed (e.g., stat.S_IFREG, stat.S_IFDIR). Returns: list: A list of file paths matching the specified file type. # Implementation here def set_permission_bits(file_path: str, permissions: int): Sets the permission bits of a file to the given permissions. Parameters: file_path (str): The path to the file. permissions (int): The permission bits to set (e.g., stat.S_IRUSR | stat.S_IWUSR). Returns: None # Implementation here ``` **Constraints:** - For `list_files_of_type`, make sure not to traverse into symbolic links to avoid potential infinite loops. - For `set_permission_bits`, only alter the permission bits while leaving other mode bits intact. **Examples:** 1. Listing regular files in a directory: ```python files = list_files_of_type(\'/path/to/directory\', stat.S_IFREG) print(files) # Output: [\'/path/to/directory/file1.txt\', \'/path/to/directory/folder/file2.txt\', ...] ``` 2. Setting permission to read and write for the user only: ```python set_permission_bits(\'/path/to/directory/file1.txt\', stat.S_IRUSR | stat.S_IWUSR) ``` **Notes:** - You may assume that the inputs provided will be valid directories and files. - Ensure to handle any exceptions or errors gracefully. Good luck!","solution":"import os import stat def list_files_of_type(directory: str, file_type_flag: int) -> list: Recursively lists all files of a specific type in the given directory. Parameters: directory (str): The directory path to search within. file_type_flag (int): The file type flag indicating the type of files to be listed (e.g., stat.S_IFREG, stat.S_IFDIR). Returns: list: A list of file paths matching the specified file type. matching_files = [] for root, dirs, files in os.walk(directory): # Skip symbolic links dirs[:] = [d for d in dirs if not os.path.islink(os.path.join(root, d))] for name in files: path = os.path.join(root, name) try: if stat.S_IFMT(os.lstat(path).st_mode) == file_type_flag: matching_files.append(path) except Exception as e: continue for name in dirs: path = os.path.join(root, name) try: if stat.S_IFMT(os.lstat(path).st_mode) == file_type_flag: matching_files.append(path) except Exception as e: continue return matching_files def set_permission_bits(file_path: str, permissions: int): Sets the permission bits of a file to the given permissions. Parameters: file_path (str): The path to the file. permissions (int): The permission bits to set (e.g., stat.S_IRUSR | stat.S_IWUSR). Returns: None try: current_mode = os.lstat(file_path).st_mode os.chmod(file_path, (current_mode & ~stat.S_IRWXU & ~stat.S_IRWXG & ~stat.S_IRWXO) | permissions) except Exception as e: raise e"},{"question":"**Objective**: Your task is to extend the functionality of the `ContentManager` to handle a new MIME type — `image/*`. You will create a custom content manager that handles `image/jpeg` and `image/png` types, allowing for retrieving and setting images in email messages. **Requirements**: 1. Create a custom `ImageContentManager` class that inherits from `email.contentmanager.ContentManager`. 2. Implement handlers for the `get_content` and `set_content` methods to manage `image/jpeg` and `image/png` MIME types. 3. Demonstrate the use of these handlers by creating an email message, setting its content to an image, and then retrieving the content to verify it is correctly handled. **Detailed Instructions**: 1. **Class Definition**: - Define a class `ImageContentManager` inheriting from `email.contentmanager.ContentManager`. - Implement the `get_content` method to handle `image/jpeg` and `image/png` types. This method should return the image data as bytes. - Implement the `set_content` method to handle `image/jpeg` and `image/png` types. This method should accept an image as bytes and set the appropriate headers and payload for the email message. 2. **Handler Registration**: - Use `add_get_handler` to register the `get_content` handler for the keys \'image/jpeg\' and \'image/png\'. - Use `add_set_handler` to register the `set_content` handler for the type `bytes` when dealing with images. 3. **Usage Example**: - Create an `EmailMessage` object. - Use `set_content` to add an image to the email. - Use `get_content` to retrieve the image from the email and verify it matches the original image data. ```python from email.contentmanager import ContentManager from email.message import EmailMessage class ImageContentManager(ContentManager): def __init__(self): super().__init__() self.add_get_handler(\'image/jpeg\', self.get_image_content) self.add_get_handler(\'image/png\', self.get_image_content) self.add_set_handler(bytes, self.set_image_content) def get_image_content(self, msg, *args, **kwargs): if msg.get_content_type() in [\'image/jpeg\', \'image/png\']: return msg.get_payload(decode=True) raise KeyError(f\\"No handler for MIME type: {msg.get_content_type()}\\") def set_image_content(self, msg, image_data, maintype, subtype, *args, **kwargs): if maintype != \'image\': raise TypeError(\\"Only image content is supported\\") msg.clear_content() msg.add_header(\'Content-Type\', f\'{maintype}/{subtype}\') msg.set_payload(image_data) if \'base64\' in kwargs.get(\'cte\', \'\'): import base64 msg.set_payload(base64.b64encode(image_data).decode(\'ascii\')) msg.add_header(\'Content-Transfer-Encoding\', \'base64\') # Example Usage: if __name__ == \\"__main__\\": image_manager = ImageContentManager() # Create an EmailMessage msg = EmailMessage() # Set image content (example image data as bytes) original_image_data = b\'...image data...\' image_manager.set_content(msg, original_image_data, maintype=\'image\', subtype=\'jpeg\') # Get image content retrieved_image_data = image_manager.get_content(msg) # Verify that the original and retrieved data are the same assert original_image_data == retrieved_image_data, \\"Image data does not match\\" print(\\"Image content managed successfully!\\") ``` **Constraints**: - Ensure to handle only `image/jpeg` and `image/png` types. - Make sure to clear any existing content in the message before setting new content. - Properly handle content transfer encoding when setting image content. **Performance Requirements**: - The implementation should be efficient in terms of memory and processing, considering the potential size of image data. **Note**: Your solution should not rely on any external libraries except those provided by Python\'s standard library.","solution":"from email.contentmanager import ContentManager from email.message import EmailMessage import base64 class ImageContentManager(ContentManager): def __init__(self): super().__init__() self.add_get_handler(\'image/jpeg\', self.get_image_content) self.add_get_handler(\'image/png\', self.get_image_content) self.add_set_handler(bytes, self.set_image_content) def get_image_content(self, msg, *args, **kwargs): if msg.get_content_type() in [\'image/jpeg\', \'image/png\']: return msg.get_payload(decode=True) raise KeyError(f\\"No handler for MIME type: {msg.get_content_type()}\\") def set_image_content(self, msg, image_data, maintype, subtype, *args, **kwargs): if maintype != \'image\': raise TypeError(\\"Only image content is supported\\") msg.clear() msg.add_header(\'Content-Type\', f\'{maintype}/{subtype}\') encoded_image_data = base64.b64encode(image_data).decode(\'ascii\') msg.set_payload(encoded_image_data) msg.add_header(\'Content-Transfer-Encoding\', \'base64\') # Example Usage: if __name__ == \\"__main__\\": image_manager = ImageContentManager() # Create an EmailMessage msg = EmailMessage() # Set image content (example image data as bytes) original_image_data = b\'...image data...\' image_manager.set_content(msg, original_image_data, maintype=\'image\', subtype=\'jpeg\') # Get image content retrieved_image_data = image_manager.get_content(msg) # Verify that the original and retrieved data are the same assert original_image_data == retrieved_image_data, \\"Image data does not match\\" print(\\"Image content managed successfully!\\")"},{"question":"**Assessment Question: Implement a Custom Asynchronous String Reversal Server Using `asyncore`** **Objective:** Create an asynchronous TCP server using the `asyncore` module that accepts client connections, receives their messages, and sends back the reversed message. **Task:** 1. **Define a `StringReversalHandler` class** that inherits from `asyncore.dispatcher_with_send`. - Override the `handle_read()` method to: - Read incoming data using `self.recv(8192)`. - Reverse the string data. - Send the reversed data back to the client using `self.send()`. - Ensure that an empty read (indicating a closed connection) triggers the handler to close the connection properly. 2. **Define a `StringReversalServer` class** that inherits from `asyncore.dispatcher`. - In the constructor, create a socket, bind it to an address (`localhost` and a specified port), and listen for incoming connections. - Override the `handle_accepted()` method to: - Accept new connections. - Print the client\'s address. - Create a new `StringReversalHandler` instance to handle the incoming connection. 3. **Main Function:** - Instantiate `StringReversalServer` with `localhost` and a port number (e.g., 9090). - Start the asyncore loop with `asyncore.loop()` to handle events. **Constraints:** - Your server should keep running and handle multiple clients sequentially (one after the other) until an external interruption (e.g., keyboard interrupt). - Use proper exception handling to ensure the server runs smoothly without crashing due to unforeseen errors. **Example Flow:** 1. Client connects to the `localhost` on port `9090`. 2. Client sends the string \\"Hello, World!\\". 3. Server receives the string, reverses it to \\"!dlroW ,olleH\\", and sends it back to the client. 4. Client receives the reversed string and closes the connection. 5. Server awaits the next client connection. **Expected Code Skeleton:** ```python import asyncore class StringReversalHandler(asyncore.dispatcher_with_send): def handle_read(self): data = self.recv(8192) if data: reversed_data = data[::-1] self.send(reversed_data) else: self.close() class StringReversalServer(asyncore.dispatcher): def __init__(self, host, port): asyncore.dispatcher.__init__(self) self.create_socket() self.set_reuse_addr() self.bind((host, port)) self.listen(5) def handle_accepted(self, sock, addr): print(\'Incoming connection from %s\' % repr(addr)) handler = StringReversalHandler(sock) if __name__ == \\"__main__\\": server = StringReversalServer(\'localhost\', 9090) asyncore.loop() ``` **Deliverable:** Submit a Python script implementing the `StringReversalHandler` and `StringReversalServer` classes as specified, and include a brief explanation of how your implementation works.","solution":"import asyncore import socket class StringReversalHandler(asyncore.dispatcher_with_send): def handle_read(self): data = self.recv(8192) if data: reversed_data = data[::-1] self.send(reversed_data) else: self.close() class StringReversalServer(asyncore.dispatcher): def __init__(self, host, port): asyncore.dispatcher.__init__(self) self.create_socket(socket.AF_INET, socket.SOCK_STREAM) self.set_reuse_addr() self.bind((host, port)) self.listen(5) def handle_accepted(self, sock, addr): print(\'Incoming connection from %s\' % repr(addr)) handler = StringReversalHandler(sock) if __name__ == \\"__main__\\": server = StringReversalServer(\'localhost\', 9090) asyncore.loop()"},{"question":"**Creating and Validating UUIDs** Using the `uuid` module, create a function suite to generate, manipulate, and validate UUIDs. Your tasks are as follows: 1. **Generate UUIDs:** - Write a function `generate_uuid(version: int, name: str = None, namespace: str = None) -> uuid.UUID` that generates and returns a UUID. The function should: - Use `uuid1()` if version is 1. - Use `uuid3()` if version is 3; require `name` and `namespace` parameters. - Use `uuid4()` if version is 4. - Use `uuid5()` if version is 5; require `name` and `namespace` parameters. - Raise an exception if the version is not 1, 3, 4, or 5, or if required parameters for version 3 or 5 are missing. 2. **Convert UUID to Different Forms:** - Write a function `uuid_to_dict(uuid_obj: uuid.UUID) -> dict` that takes a UUID object and returns a dictionary with the following key-value pairs: - \\"str\\": The string representation of the UUID. - \\"hex\\": The hexadecimal string of the UUID. - \\"int\\": The integer representation of the UUID. - \\"urn\\": The URN representation of the UUID. - \\"fields\\": The individual fields of the UUID as a tuple. - \\"variant\\": The variant of the UUID. - \\"version\\": The version of the UUID. - \\"is_safe\\": The safety status of the UUID. 3. **Validate UUID:** - Write a function `validate_uuid(uuid_str: str) -> bool` that takes a string and returns True if the string is a valid UUID and False otherwise. **Input/Output Formats:** 1. `generate_uuid`: - Input: Version integer (1, 3, 4, or 5), and optionally `name` and `namespace` for versions 3 and 5. - Output: A `uuid.UUID` object. 2. `uuid_to_dict`: - Input: A `uuid.UUID` object. - Output: A dictionary with the UUID representations and attributes. 3. `validate_uuid`: - Input: A string that is supposed to represent a UUID. - Output: Boolean value indicating validity of the UUID string. **Constraints:** - For `generate_uuid(version=3 or 5)`, `namespace` should be one of `uuid.NAMESPACE_DNS`, `uuid.NAMESPACE_URL`, `uuid.NAMESPACE_OID`, `uuid.NAMESPACE_X500`. **Example Usage:** ```python import uuid # Example UUID generation print(generate_uuid(4)) # Should print a random UUID # Example UUID to dictionary conversion uuid_obj = generate_uuid(1) print(uuid_to_dict(uuid_obj)) # Should print the dictionary representation # Example UUID validation print(validate_uuid(\'6fa459ea-ee8a-3ca4-894e-db77e160355e\')) # Should return True print(validate_uuid(\'not-a-valid-uuid\')) # Should return False ``` Utilize the `uuid` module\'s functionalities to solve these tasks efficiently and ensure you handle edge cases according to the module\'s documentation.","solution":"import uuid def generate_uuid(version: int, name: str = None, namespace: str = None) -> uuid.UUID: Generates a UUID based on the specified version. :param version: The version of the UUID (1, 3, 4, or 5). :param name: Name for UUIDv3 and UUIDv5. :param namespace: Namespace for UUIDv3 and UUIDv5. :return: Generated UUID. :raises ValueError: If an invalid version or required parameters for version 3/5 are missing. if version == 1: return uuid.uuid1() elif version == 3: if name is None or namespace is None: raise ValueError(\\"For version 3, \'name\' and \'namespace\' parameters must be provided.\\") return uuid.uuid3(uuid.UUID(namespace), name) elif version == 4: return uuid.uuid4() elif version == 5: if name is None or namespace is None: raise ValueError(\\"For version 5, \'name\' and \'namespace\' parameters must be provided.\\") return uuid.uuid5(uuid.UUID(namespace), name) else: raise ValueError(\\"Invalid UUID version. Only versions 1, 3, 4, and 5 are supported.\\") def uuid_to_dict(uuid_obj: uuid.UUID) -> dict: Converts a UUID object to a dictionary containing various representations and fields of the UUID. :param uuid_obj: The UUID object. :return: A dictionary with different UUID representations and attributes. return { \\"str\\": str(uuid_obj), \\"hex\\": uuid_obj.hex, \\"int\\": uuid_obj.int, \\"urn\\": uuid_obj.urn, \\"fields\\": uuid_obj.fields, \\"variant\\": uuid_obj.variant, \\"version\\": uuid_obj.version, \\"is_safe\\": uuid_obj.is_safe } def validate_uuid(uuid_str: str) -> bool: Validates whether a string is a properly formatted UUID. :param uuid_str: The string to validate. :return: True if valid, False otherwise. try: uuid_obj = uuid.UUID(uuid_str) return str(uuid_obj) == uuid_str except ValueError: return False"},{"question":"# Clustering Assessment Question Objective The objective of this task is to test your understanding of clustering techniques using the scikit-learn library. You are required to implement a complete clustering pipeline that reads a dataset, performs clustering, and evaluates the clustering results. Problem Statement You are given a dataset of two-dimensional points that you need to cluster using different clustering algorithms provided by scikit-learn. Specifically, you will use K-Means, Agglomerative Clustering, and DBSCAN. Requirements 1. **Read the Dataset:** Load a given CSV file containing the dataset with two features. 2. **Preprocess the Data:** Normalize the data to ensure that it ranges between 0 and 1. 3. **Clustering Algorithms:** - Implement K-Means clustering with `n_clusters=3`. - Implement Agglomerative Clustering with `n_clusters=3` and `linkage=\'ward\'`. - Implement DBSCAN with `eps=0.3` and `min_samples=5`. 4. **Evaluate Clustering:** - Compute the Silhouette Coefficient for each clustering. - Plot the clustered data for visual inspection. Input - A CSV file `data.csv` with columns `feature_1` and `feature_2`. Each row represents a data point in a two-dimensional space. Output - Silhouette Coefficient for each clustering algorithm. - Scatter plots of the clustered data points for each algorithm. Function Signature ```python def perform_clustering(file_path: str) -> None: pass ``` Instructions 1. **Load Data:** - Use `pandas` to read the CSV file. 2. **Preprocess:** - Use `MinMaxScaler` from `sklearn.preprocessing` to normalize the data. 3. **Implement Clustering Algorithms:** - Use `KMeans` from `sklearn.cluster` to perform K-Means clustering. - Use `AgglomerativeClustering` from `sklearn.cluster` for hierarchical clustering. - Use `DBSCAN` from `sklearn.cluster` for density-based clustering. 4. **Evaluate Clustering:** - Use `silhouette_score` from `sklearn.metrics` to compute the Silhouette Coefficient. 5. **Visualization:** - Use `matplotlib.pyplot` to plot the scatter plots. Example ```python import pandas as pd from sklearn.preprocessing import MinMaxScaler from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN from sklearn.metrics import silhouette_score import matplotlib.pyplot as plt def perform_clustering(file_path: str) -> None: # Load data data = pd.read_csv(file_path) X = data[[\'feature_1\', \'feature_2\']].values # Normalize data scaler = MinMaxScaler() X_scaled = scaler.fit_transform(X) # K-Means Clustering kmeans = KMeans(n_clusters=3, random_state=0) kmeans_labels = kmeans.fit_predict(X_scaled) kmeans_silhouette = silhouette_score(X_scaled, kmeans_labels) # Agglomerative Clustering agglomerative = AgglomerativeClustering(n_clusters=3, linkage=\'ward\') agglomerative_labels = agglomerative.fit_predict(X_scaled) agglomerative_silhouette = silhouette_score(X_scaled, agglomerative_labels) # DBSCAN dbscan = DBSCAN(eps=0.3, min_samples=5) dbscan_labels = dbscan.fit_predict(X_scaled) dbscan_silhouette = silhouette_score(X_scaled, dbscan_labels) # Print Silhouette Scores print(f\\"K-Means Silhouette Score: {kmeans_silhouette}\\") print(f\\"Agglomerative Clustering Silhouette Score: {agglomerative_silhouette}\\") print(f\\"DBSCAN Silhouette Score: {dbscan_silhouette}\\") # Plot Clusters plt.figure(figsize=(15, 5)) plt.subplot(131) plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=kmeans_labels) plt.title(\'K-Means Clustering\') plt.subplot(132) plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=agglomerative_labels) plt.title(\'Agglomerative Clustering\') plt.subplot(133) plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=dbscan_labels) plt.title(\'DBSCAN Clustering\') plt.show() # Example usage perform_clustering(\'data.csv\') ```","solution":"import pandas as pd from sklearn.preprocessing import MinMaxScaler from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN from sklearn.metrics import silhouette_score import matplotlib.pyplot as plt def perform_clustering(file_path: str) -> None: # Load data data = pd.read_csv(file_path) X = data[[\'feature_1\', \'feature_2\']].values # Normalize data scaler = MinMaxScaler() X_scaled = scaler.fit_transform(X) # K-Means Clustering kmeans = KMeans(n_clusters=3, random_state=0) kmeans_labels = kmeans.fit_predict(X_scaled) kmeans_silhouette = silhouette_score(X_scaled, kmeans_labels) # Agglomerative Clustering agglomerative = AgglomerativeClustering(n_clusters=3, linkage=\'ward\') agglomerative_labels = agglomerative.fit_predict(X_scaled) agglomerative_silhouette = silhouette_score(X_scaled, agglomerative_labels) # DBSCAN dbscan = DBSCAN(eps=0.3, min_samples=5) dbscan_labels = dbscan.fit_predict(X_scaled) dbscan_silhouette = silhouette_score(X_scaled, dbscan_labels) if len(set(dbscan_labels)) > 1 else -1 # Print Silhouette Scores print(f\\"K-Means Silhouette Score: {kmeans_silhouette}\\") print(f\\"Agglomerative Clustering Silhouette Score: {agglomerative_silhouette}\\") print(f\\"DBSCAN Silhouette Score: {dbscan_silhouette}\\") # Plot Clusters plt.figure(figsize=(15, 5)) plt.subplot(131) plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=kmeans_labels, cmap=\'viridis\') plt.title(\'K-Means Clustering\') plt.subplot(132) plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=agglomerative_labels, cmap=\'viridis\') plt.title(\'Agglomerative Clustering\') plt.subplot(133) plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=dbscan_labels, cmap=\'viridis\') plt.title(\'DBSCAN Clustering\') plt.show()"},{"question":"# Python Interactive Mode Customization Task Background: This task will test your understanding of Python script execution, environment variables, and custom startup procedures. You will write a Python script that behaves differently based on the presence of a specified environment variable. Task: You need to implement a Python script that: 1. Checks for the existence of an environment variable named `PYTHON_CUSTOM_STARTUP`. 2. If this variable is set and points to an existing file, the script should execute the contents of this file as Python code. 3. If the environment variable is not set or the file does not exist, print a message: \\"No custom startup file found or file does not exist.\\" Requirements: 1. **Input**: There is no direct input required for the script. The script should rely on the system\'s environment variables. 2. **Output**: The script should not produce any output unless the environment variable is not set or the file does not exist, in which case it should print: \\"No custom startup file found or file does not exist.\\" 3. **Constraints**: - The script must handle any potential exceptions that could arise from file operations or executing arbitrary code. - Assume the script is executed in an environment where you don\'t have control over what files might exist or the permissions. Example Use Cases: 1. **Environment Variable Set and File Exists**: - Suppose the environment variable `PYTHON_CUSTOM_STARTUP` is set to `/path/to/startup.py` and the file exists with executable Python code. - The script should execute the content of `/path/to/startup.py`. 2. **Environment Variable Not Set or File Does Not Exist**: - Suppose the environment variable `PYTHON_CUSTOM_STARTUP` is not set, or it is set to a non-existing file path. - The script should print: \\"No custom startup file found or file does not exist.\\" Performance: The script should handle file operations efficiently and should not take more than a fraction of a second to check the environment variable and decide the course of action. You are expected to demonstrate: - File handling in Python. - Environment variable manipulation. - Safe execution of dynamically loaded code. Good luck!","solution":"import os def execute_custom_startup(): env_var = \'PYTHON_CUSTOM_STARTUP\' startup_file = os.environ.get(env_var) if startup_file and os.path.isfile(startup_file): try: with open(startup_file, \'r\') as file: code = file.read() exec(code) except Exception as e: print(f\\"An error occurred while executing the startup file: {e}\\") else: print(\\"No custom startup file found or file does not exist.\\") if __name__ == \\"__main__\\": execute_custom_startup()"},{"question":"# Advanced File Handling and Command-Line Parsing with Logging You are tasked with writing a Python program that executes the following functions: 1. **Directory Management**: - List all files in a specified directory (including sub-directories). - Identify and delete empty files within the directory and sub-directories. - Copy specific files matching a given extension to a new target directory. 2. **Command-Line Interface**: - Use `argparse` to design the command-line interface to accept the following options: - `--source` or `-s`: The source directory to be examined. - `--target` or `-t`: The target directory where specific files will be copied. - `--extension` or `-e`: The file extension to filter which files should be copied. - `--log` or `-l`: The path to a log file to record operations. - `--delete-empty` or `-d`: If present, the script will delete empty files. 3. **Logging**: - Implement logging to track the operations performed by your script. The log file should record details of: - Files listed and their locations. - Files deleted. - Files copied and their new locations. # Input and Output - **Input**: - The program should accept command-line arguments for the specified options. - **Output**: - Log file with details of operations performed. - Files copied to the target directory if the operation is specified. - Deleted empty files if the operation is enabled. # Constraints - Ensure that your script handles exceptions properly, such as invalid directories or permissions issues. - If the source directory does not exist, the script should print an appropriate message and exit. - The script should handle both relative and absolute paths for directories. # Example Command Usage ```bash python file_manager.py --source /path/to/source --target /path/to/target --extension .txt --log /path/to/logfile --delete-empty ``` # Example Log File Content ``` INFO: Listing files in directory /path/to/source INFO: Found file: /path/to/source/file1.txt INFO: Found file: /path/to/source/subdir/file2.txt INFO: Deleting empty file: /path/to/source/emptyfile.txt INFO: Copying file /path/to/source/file1.txt to /path/to/target/file1.txt ``` Use the `os` module for file and directory operations, `argparse` for processing command-line arguments, and `logging` for recording operation details. # Implementation ```python import os import shutil import argparse import logging def list_files(directory): files = [] for root, dirs, filenames in os.walk(directory): for filename in filenames: files.append(os.path.join(root, filename)) return files def delete_empty_files(files): for file in files: if os.path.getsize(file) == 0: os.remove(file) logging.info(f\\"Deleting empty file: {file}\\") def copy_files(files, extension, target_directory): for file in files: if file.endswith(extension): shutil.copy(file, target_directory) logging.info(f\\"Copying file {file} to {os.path.join(target_directory, os.path.basename(file))}\\") def setup_logging(file_path): logging.basicConfig(filename=file_path, level=logging.INFO, format=\'%(levelname)s: %(message)s\') def main(): parser = argparse.ArgumentParser(description=\\"Advanced File Handling Script\\") parser.add_argument(\'--source\', \'-s\', required=True, help=\\"Source directory to be examined\\") parser.add_argument(\'--target\', \'-t\', required=True, help=\\"Target directory for copying files\\") parser.add_argument(\'--extension\', \'-e\', required=True, help=\\"File extension to filter and copy\\") parser.add_argument(\'--log\', \'-l\', required=True, help=\\"Path to log file\\") parser.add_argument(\'--delete-empty\', \'-d\', action=\'store_true\', help=\\"Delete empty files if present\\") args = parser.parse_args() if not os.path.exists(args.source): print(\\"Source directory does not exist.\\") return if not os.path.exists(args.target): os.makedirs(args.target) setup_logging(args.log) logging.info(f\\"Listing files in directory {args.source}\\") files = list_files(args.source) if args.delete_empty: delete_empty_files(files) copy_files(files, args.extension, args.target) if __name__ == \\"__main__\\": main() ```","solution":"import os import shutil import argparse import logging def list_files(directory): files = [] for root, dirs, filenames in os.walk(directory): for filename in filenames: files.append(os.path.join(root, filename)) return files def delete_empty_files(files): for file in files: if os.path.getsize(file) == 0: os.remove(file) logging.info(f\\"Deleting empty file: {file}\\") def copy_files(files, extension, target_directory): for file in files: if file.endswith(extension): shutil.copy(file, target_directory) logging.info(f\\"Copying file {file} to {os.path.join(target_directory, os.path.basename(file))}\\") def setup_logging(file_path): logging.basicConfig(filename=file_path, level=logging.INFO, format=\'%(levelname)s: %(message)s\') def main(): parser = argparse.ArgumentParser(description=\\"Advanced File Handling Script\\") parser.add_argument(\'--source\', \'-s\', required=True, help=\\"Source directory to be examined\\") parser.add_argument(\'--target\', \'-t\', required=True, help=\\"Target directory for copying files\\") parser.add_argument(\'--extension\', \'-e\', required=True, help=\\"File extension to filter and copy\\") parser.add_argument(\'--log\', \'-l\', required=True, help=\\"Path to log file\\") parser.add_argument(\'--delete-empty\', \'-d\', action=\'store_true\', help=\\"Delete empty files if present\\") args = parser.parse_args() if not os.path.exists(args.source): print(\\"Source directory does not exist.\\") return if not os.path.exists(args.target): os.makedirs(args.target) setup_logging(args.log) logging.info(f\\"Listing files in directory {args.source}\\") files = list_files(args.source) if args.delete_empty: delete_empty_files(files) copy_files(files, args.extension, args.target) if __name__ == \\"__main__\\": main()"},{"question":"**Question:** You are required to build a custom neural network model using PyTorch\'s `torch.nn` module. This custom model will be a multilayer perceptron (MLP) designed for the task of classifying handwritten digits from the MNIST dataset. Additionally, you will need to register forward and backward hooks to monitor the training process. # Requirements: 1. **Custom Model Structure:** - Build a class `CustomMLP` inheriting from `torch.nn.Module`. - The model should consist of: - An input `Linear` layer with 784 input features (since MNIST images are 28x28) and 128 output features. - A `ReLU` activation function. - A `Dropout` layer to prevent overfitting with a dropout probability of 0.5. - A second `Linear` layer with 128 input features and 64 output features. - Another `ReLU` activation function. - Another `Dropout` layer with a dropout probability of 0.5. - An output `Linear` layer with 64 input features and 10 output features (since there are 10 classes in MNIST). 2. **Forward and Backward Hooks:** - Register a forward hook to print the output shape for each layer during a forward pass. - Register a backward hook to print the gradient of the loss with respect to each parameter during the backward pass. 3. **Training the Model:** - Implement a function `train_model` that: - Loads the MNIST dataset. - Trains the model on the dataset for 5 epochs. - Uses `CrossEntropyLoss` as the loss function. - Uses `Adam` as the optimizer. 4. **Performance:** - Ensure that your implementation is efficient and adheres to best practices. # Constraints: - Do not use any high-level API like `torchvision.models`. - Ensure that the code runs efficiently and utilizes GPU if available. # Input: - None directly. The function `train_model` should handle loading the dataset internally. # Output: - Print statements from forward and backward hooks. # Example: ```python import torch import torch.nn as nn class CustomMLP(nn.Module): def __init__(self): super(CustomMLP, self).__init__() # Define layers self.fc1 = nn.Linear(784, 128) self.relu1 = nn.ReLU() self.dropout1 = nn.Dropout(p=0.5) self.fc2 = nn.Linear(128, 64) self.relu2 = nn.ReLU() self.dropout2 = nn.Dropout(p=0.5) self.fc3 = nn.Linear(64, 10) def forward(self, x): x = self.fc1(x) x = self.relu1(x) x = self.dropout1(x) x = self.fc2(x) x = self.relu2(x) x = self.dropout2(x) x = self.fc3(x) return x def print_output_shape(module, input, output): print(f\\"Forward pass - Output shape: {output.shape}\\") def print_gradients(module, grad_input, grad_output): print(f\\"Backward pass - Grad Output: {grad_output}\\") def train_model(): # Initialize the model, loss function and optimizer model = CustomMLP() criterion = nn.CrossEntropyLoss() optimizer = torch.optim.Adam(model.parameters()) # Register hooks for layer in [model.fc1, model.fc2, model.fc3]: layer.register_forward_hook(print_output_shape) layer.register_backward_hook(print_gradients) # Load dataset and setup dataloader from torchvision import datasets, transforms from torch.utils.data import DataLoader transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))]) train_dataset = datasets.MNIST(root=\'./data\', train=True, download=True, transform=transform) train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True) # Training loop device = torch.device(\\"cuda\\" if torch.cuda.is_available() else \\"cpu\\") model.to(device) epochs = 5 for epoch in range(epochs): model.train() running_loss = 0.0 for images, labels in train_loader: images, labels = images.view(images.shape[0], -1).to(device), labels.to(device) optimizer.zero_grad() outputs = model(images) loss = criterion(outputs, labels) loss.backward() optimizer.step() running_loss += loss.item() print(f\\"Epoch {epoch+1}, Loss: {running_loss / len(train_loader)}\\") # Call the training function train_model() ``` Ensure you have `torch` and `torchvision` installed in your environment to run this code.","solution":"import torch import torch.nn as nn class CustomMLP(nn.Module): def __init__(self): super(CustomMLP, self).__init__() # Define layers self.fc1 = nn.Linear(784, 128) self.relu1 = nn.ReLU() self.dropout1 = nn.Dropout(p=0.5) self.fc2 = nn.Linear(128, 64) self.relu2 = nn.ReLU() self.dropout2 = nn.Dropout(p=0.5) self.fc3 = nn.Linear(64, 10) def forward(self, x): x = self.fc1(x) x = self.relu1(x) x = self.dropout1(x) x = self.fc2(x) x = self.relu2(x) x = self.dropout2(x) x = self.fc3(x) return x def print_output_shape(module, input, output): print(f\\"Forward pass - Output shape: {output.shape}\\") def print_gradients(module, grad_input, grad_output): print(f\\"Backward pass - Grad Output: {grad_output}\\") def train_model(): # Initialize the model, loss function and optimizer model = CustomMLP() criterion = nn.CrossEntropyLoss() optimizer = torch.optim.Adam(model.parameters()) # Register hooks for layer in [model.fc1, model.fc2, model.fc3]: layer.register_forward_hook(print_output_shape) layer.register_backward_hook(print_gradients) # Load dataset and setup dataloader from torchvision import datasets, transforms from torch.utils.data import DataLoader transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))]) train_dataset = datasets.MNIST(root=\'./data\', train=True, download=True, transform=transform) train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True) # Training loop device = torch.device(\\"cuda\\" if torch.cuda.is_available() else \\"cpu\\") model.to(device) epochs = 5 for epoch in range(epochs): model.train() running_loss = 0.0 for images, labels in train_loader: images, labels = images.view(images.shape[0], -1).to(device), labels.to(device) optimizer.zero_grad() outputs = model(images) loss = criterion(outputs, labels) loss.backward() optimizer.step() running_loss += loss.item() print(f\\"Epoch {epoch+1}, Loss: {running_loss / len(train_loader)}\\")"},{"question":"# Advanced Exception Handling in Python You are tasked with creating a robust module for handling different types of errors and providing meaningful error messages. Requirements: 1. **Custom Exceptions**: - Create three custom exceptions: - `DataValidationError`: Raised when data validation fails. - `DataProcessingError`: Raised when an error occurs during data processing. - `SystemFailureError`: Raised for non-specific system failures but should record the original error. 2. **Function Implementation**: Implement a function `process_data(data: dict) -> None` that processes a dictionary of data. This function should: - Raise `DataValidationError` if the data dictionary is missing required keys (`name`, `age`, `email`). - Raise `DataProcessingError` if the value of `age` is not a positive integer. - In case any other unexpected error occurs, raise `SystemFailureError` using exception chaining to record the original error. 3. **Error Context**: - When raising `SystemFailureError`, include both the context and the explicit cause of the exception. Example: ```python class DataValidationError(Exception): pass class DataProcessingError(Exception): pass class SystemFailureError(Exception): pass def process_data(data): # Implement the function based on the above requirements ``` Input: - `data`: A dictionary with potential keys `name`, `age`, and `email`. Output: - None. The function raises appropriate exceptions when there is an error. Constraints: - The keys in the `data` dictionary are always strings. - The `age` value should be validated as a positive integer. Here are some example usages and expected exceptions: ```python data1 = {\'name\': \'John\', \'age\': 29, \'email\': \'john@example.com\'} # process_data(data1) should not raise any exceptions. data2 = {\'name\': \'John\', \'email\': \'john@example.com\'} # process_data(data2) should raise DataValidationError data3 = {\'name\': \'John\', \'age\': -5, \'email\': \'john@example.com\'} # process_data(data3) should raise DataProcessingError data4 = None # process_data(data4) should raise SystemFailureError with the original TypeError recorded in the context. ``` Note: - Be sure to handle exceptions and use appropriate error messages. - Ensure the correct exception hierarchy and chaining as specified in the documentation.","solution":"class DataValidationError(Exception): pass class DataProcessingError(Exception): pass class SystemFailureError(Exception): def __init__(self, original_exception): self.original_exception = original_exception super().__init__(str(original_exception)) def process_data(data): try: if not isinstance(data, dict): raise TypeError(\\"Expected a dictionary\\") # Check for required keys required_keys = {\'name\', \'age\', \'email\'} missing_keys = required_keys - data.keys() if missing_keys: raise DataValidationError(f\\"Missing required keys: {\', \'.join(missing_keys)}\\") # Check if age is a positive integer if not isinstance(data[\'age\'], int) or data[\'age\'] <= 0: raise DataProcessingError(f\\"Age must be a positive integer, found: {data[\'age\']}\\") except (DataValidationError, DataProcessingError) as e: raise e except Exception as e: raise SystemFailureError(e) from e"},{"question":"# Question: Advanced Data Loading and Analysis with Pandas # Objective: This task is designed to assess your understanding of working with complex data sets and utilizing advanced features of the pandas library, particularly with its IO capabilities. # Problem Statement: You are provided with several datasets in various formats (CSV, JSON, Excel) that relate to sales transactions in a large retail store. Your goal is to integrate these datasets, perform data cleaning, and conduct specific analyses to help derive valuable insights. # Files: 1. **sales_data.csv**: Contains raw sales data. 2. **product_info.json**: Contains detailed product information. 3. **store_locations.xlsx**: Contains information about different store locations. # Tasks: 1. **Read and Integrate Data**: - Load the data from each file into a separate pandas DataFrame. - Merge these DataFrames into a single DataFrame based on common columns (e.g., product_id, store_id). 2. **Data Cleaning**: - Check for missing values in important columns (e.g., `sale_amount`, `product_name`) and handle them appropriately. - Convert any necessary columns to proper data types (e.g., date columns to datetime, categorical columns). 3. **Analysis**: - Calculate total sales for each store. - Identify the top 5 best-selling products. - Determine which store has the highest average sale amount. - Calculate monthly sales trends and plot them using matplotlib or seaborn. # Constraints: - Ensure that the code is efficient and handles large datasets effectively. - The data should be loaded with appropriate data types to minimize memory usage and enhance performance. - Make use of the `usecols`, `parse_dates`, and `converters` parameters where applicable to optimize the loading process. # Input Format: You do not need to provide the actual files, but assume they are in the respective formats as described. # Output: - Output the merged DataFrame after completing the data integration step. - Output the DataFrame containing total sales for each store. - Output the DataFrame containing the top 5 best-selling products. - Output the name of the store with the highest average sale amount. - Plot showing monthly sales trends. # Example: Here is an example of how you can start with the task: ```python import pandas as pd import matplotlib.pyplot as plt import seaborn as sns # Step 1: Load Data sales_df = pd.read_csv(\'sales_data.csv\', parse_dates=[\'date\']) product_df = pd.read_json(\'product_info.json\') store_df = pd.read_excel(\'store_locations.xlsx\', sheet_name=\'Stores\') # Step 2: Merge Data merged_df = pd.merge(sales_df, product_df, on=\'product_id\', how=\'left\') merged_df = pd.merge(merged_df, store_df, on=\'store_id\', how=\'left\') # Step 3: Data Cleaning # Handling missing values and data types merged_df[\'sale_amount\'] = merged_df[\'sale_amount\'].fillna(0) merged_df[\'date\'] = pd.to_datetime(merged_df[\'date\']) merged_df[\'product_name\'] = merged_df[\'product_name\'].astype(\'category\') # Step 4: Analysis # Total sales per store total_sales_per_store = merged_df.groupby(\'store_id\')[\'sale_amount\'].sum().reset_index() # Top 5 best-selling products top_products = merged_df.groupby(\'product_id\')[\'sale_amount\'].sum().nlargest(5).reset_index() # Store with the highest average sale amount avg_sales_per_store = merged_df.groupby(\'store_id\')[\'sale_amount\'].mean().reset_index() top_store = avg_sales_per_store.loc[avg_sales_per_store[\'sale_amount\'].idxmax()][\'store_id\'] # Monthly sales trends merged_df[\'month\'] = merged_df[\'date\'].dt.to_period(\'M\') monthly_sales = merged_df.groupby(\'month\')[\'sale_amount\'].sum().reset_index() # Plotting the monthly sales trends plt.figure(figsize=(10, 6)) sns.lineplot(data=monthly_sales, x=\'month\', y=\'sale_amount\') plt.title(\'Monthly Sales Trends\') plt.xlabel(\'Month\') plt.ylabel(\'Total Sales\') plt.xticks(rotation=45) plt.show() ``` You need to complete the code above to perform all the tasks specified. # Submission: Submit a Jupyter notebook or a Python script file containing the implementation of the above tasks and analysis.","solution":"import pandas as pd import matplotlib.pyplot as plt import seaborn as sns def load_and_integrate_data(): # Step 1: Load Data sales_df = pd.read_csv(\'sales_data.csv\', parse_dates=[\'date\']) product_df = pd.read_json(\'product_info.json\') store_df = pd.read_excel(\'store_locations.xlsx\', sheet_name=\'Stores\') # Step 2: Merge Data merged_df = pd.merge(sales_df, product_df, on=\'product_id\', how=\'left\') merged_df = pd.merge(merged_df, store_df, on=\'store_id\', how=\'left\') return merged_df def clean_data(merged_df): # Handling missing values and data types merged_df[\'sale_amount\'] = merged_df[\'sale_amount\'].fillna(0) merged_df[\'date\'] = pd.to_datetime(merged_df[\'date\']) merged_df[\'product_name\'] = merged_df[\'product_name\'].astype(\'category\') return merged_df def analyze_data(merged_df): # Total sales per store total_sales_per_store = merged_df.groupby(\'store_id\')[\'sale_amount\'].sum().reset_index() # Top 5 best-selling products top_products = merged_df.groupby(\'product_id\')[\'sale_amount\'].sum().nlargest(5).reset_index() # Store with the highest average sale amount avg_sales_per_store = merged_df.groupby(\'store_id\')[\'sale_amount\'].mean().reset_index() top_store = avg_sales_per_store.loc[avg_sales_per_store[\'sale_amount\'].idxmax()][\'store_id\'] # Monthly sales trends merged_df[\'month\'] = merged_df[\'date\'].dt.to_period(\'M\') monthly_sales = merged_df.groupby(\'month\')[\'sale_amount\'].sum().reset_index() return total_sales_per_store, top_products, top_store, monthly_sales def plot_monthly_sales_trends(monthly_sales): # Plotting the monthly sales trends plt.figure(figsize=(10, 6)) sns.lineplot(data=monthly_sales, x=\'month\', y=\'sale_amount\') plt.title(\'Monthly Sales Trends\') plt.xlabel(\'Month\') plt.ylabel(\'Total Sales\') plt.xticks(rotation=45) plt.show() if __name__ == \\"__main__\\": merged_df = load_and_integrate_data() merged_df = clean_data(merged_df) total_sales_per_store, top_products, top_store, monthly_sales = analyze_data(merged_df) print(f\\"Merged DataFrame:n{merged_df.head()}n\\") print(f\\"Total Sales per Store:n{total_sales_per_store}n\\") print(f\\"Top 5 Best-Selling Products:n{top_products}n\\") print(f\\"Store with Highest Average Sale Amount: {top_store}n\\") plot_monthly_sales_trends(monthly_sales)"},{"question":"# Logging Configuration Assignment You\'re tasked with creating a logging setup for a Python application that tracks different events and outputs them to various destinations based on the event\'s severity. Follow the directions below to implement the required logging configuration. Requirements: 1. **Loggers**: - Create a main logger named `app`. - Set the logger to handle messages of level `DEBUG` and above. 2. **Handlers**: - **FileHandler**: Log all messages (from level `DEBUG` and above) to a file named `app.log`. - **StreamHandler**: Log only `ERROR` and `CRITICAL` messages to the console (standard output). - **SMTPHandler**: Send `CRITICAL` messages via email using the SMTPHandler. (For the sake of this task, you can simulate this as a method that prints an email notification message instead of actually sending an email). 3. **Formatters**: - Set a custom formatter for all handlers to include the timestamp, logger name, severity level, and message. 4. **Log Messages**: - Generate log messages at all levels (`DEBUG`, `INFO`, `WARNING`, `ERROR`, and `CRITICAL`) demonstrating that the configuration works as expected. Expected Input and Output: - **Input**: None (configuration and log message generation should be within the code). - **Output**: Logging output should appear both in the console (for `ERROR` and `CRITICAL` messages) and the `app.log` file (for all log messages). Additionally, for `CRITICAL` messages, simulate an email notification via print statement. Constraints: - Utilize the `logging` module as outlined in the provided documentation. - Ensure the `SMTPHandler` (simulated) prints a notification message for `CRITICAL` logs. Code: ```python import logging import logging.handlers # Define a function to simulate sending an email def send_email(subject, message): print(f\\"EMAIL NOTIFICATION: {subject}n{message}\\") class SimulatedSMTPHandler(logging.Handler): def emit(self, record): log_entry = self.format(record) send_email(\\"Critical Error Logged\\", log_entry) # 1. Create a main logger named `app` app_logger = logging.getLogger(\'app\') app_logger.setLevel(logging.DEBUG) # Handle messages of level DEBUG and above # 2. Create handlers # FileHandler file_handler = logging.FileHandler(\'app.log\') file_handler.setLevel(logging.DEBUG) # StreamHandler (console) stream_handler = logging.StreamHandler() stream_handler.setLevel(logging.ERROR) # Simulated SMTPHandler (email) smtp_handler = SimulatedSMTPHandler() smtp_handler.setLevel(logging.CRITICAL) # 3. Create formatters and add them to handlers formatter = logging.Formatter(\'%(asctime)s - %(name)s - %(levelname)s - %(message)s\') file_handler.setFormatter(formatter) stream_handler.setFormatter(formatter) smtp_handler.setFormatter(formatter) # 4. Add handlers to the logger app_logger.addHandler(file_handler) app_logger.addHandler(stream_handler) app_logger.addHandler(smtp_handler) # Testing log messages app_logger.debug(\'This is a debug message.\') app_logger.info(\'This is an info message.\') app_logger.warning(\'This is a warning message.\') app_logger.error(\'This is an error message.\') app_logger.critical(\'This is a critical message.\') # Check app.log for all log messages # Check the console for ERROR and CRITICAL messages # Check the console for simulated email notification for CRITICAL message ``` Instructions: 1. Implement the provided code. 2. Ensure the log messages are directed to the correct destinations. 3. Verify the `app.log` file contains all log messages. 4. Verify the console output for `ERROR` and `CRITICAL` messages. 5. Verify the simulated email notification prints for `CRITICAL` message.","solution":"import logging import logging.handlers # Define a function to simulate sending an email def send_email(subject, message): print(f\\"EMAIL NOTIFICATION: {subject}n{message}\\") class SimulatedSMTPHandler(logging.Handler): def emit(self, record): log_entry = self.format(record) send_email(\\"Critical Error Logged\\", log_entry) # 1. Create a main logger named `app` app_logger = logging.getLogger(\'app\') app_logger.setLevel(logging.DEBUG) # Handle messages of level DEBUG and above # 2. Create handlers # FileHandler file_handler = logging.FileHandler(\'app.log\') file_handler.setLevel(logging.DEBUG) # StreamHandler (console) stream_handler = logging.StreamHandler() stream_handler.setLevel(logging.ERROR) # Simulated SMTPHandler (email) smtp_handler = SimulatedSMTPHandler() smtp_handler.setLevel(logging.CRITICAL) # 3. Create formatters and add them to handlers formatter = logging.Formatter(\'%(asctime)s - %(name)s - %(levelname)s - %(message)s\') file_handler.setFormatter(formatter) stream_handler.setFormatter(formatter) smtp_handler.setFormatter(formatter) # 4. Add handlers to the logger app_logger.addHandler(file_handler) app_logger.addHandler(stream_handler) app_logger.addHandler(smtp_handler) # Testing log messages app_logger.debug(\'This is a debug message.\') app_logger.info(\'This is an info message.\') app_logger.warning(\'This is a warning message.\') app_logger.error(\'This is an error message.\') app_logger.critical(\'This is a critical message.\') # Check app.log for all log messages # Check the console for ERROR and CRITICAL messages # Check the console for simulated email notification for CRITICAL message"},{"question":"Objective You are required to implement a function that automates part of the process of creating built distributions for a Python package. Specifically, the function will generate a `.spec` file for RPM packages based on the given setup script options provided as a dictionary. Description Write a function `generate_spec_file` that takes a dictionary of setup script options and generates the content of a `.spec` file for an RPM package. The dictionary will contain keys corresponding to the Distutils setup script options and their respective values. Function Signature ```python def generate_spec_file(setup_options: dict) -> str: pass ``` Input - `setup_options`: A dictionary where keys are strings representing setup script options such as \\"name\\", \\"version\\", \\"author\\", and so on, and values are their respective string values. Output - A string containing the generated content of the `.spec` file formatted according to RPM packaging conventions. Example ```python setup_options = { \\"name\\": \\"example-package\\", \\"version\\": \\"1.0.0\\", \\"author\\": \\"Jane Doe\\", \\"author_email\\": \\"jane.doe@example.com\\", \\"description\\": \\"An example package\\", \\"long_description\\": \\"This is a long description of the example package.\\", \\"license\\": \\"MIT\\", \\"url\\": \\"https://example.com\\" } expected_output = Name: example-package Version: 1.0.0 Summary: An example package License: MIT URL: https://example.com Packager: Jane Doe <jane.doe@example.com> %description This is a long description of the example package. print(generate_spec_file(setup_options)) # Output should match `expected_output` ``` Constraints - The function should handle the major setup script options required for generating the `.spec` file as described above. - Assume that the input dictionary will always have valid options as strings. - Extra fields that are not listed in the example or not needed for the basic RPM spec file should be ignored. Notes - Pay attention to formatting details as they are crucial in RPM spec files. - Follow the mappings between setup script options and RPM spec file sections as described in the documentation. Good luck!","solution":"def generate_spec_file(setup_options: dict) -> str: Generates the content of a .spec file for an RPM package based on the given setup options. Args: setup_options (dict): A dictionary containing setup script options. Returns: str: The content of the .spec file. name = setup_options.get(\\"name\\", \\"\\") version = setup_options.get(\\"version\\", \\"\\") license_ = setup_options.get(\\"license\\", \\"\\") url = setup_options.get(\\"url\\", \\"\\") author = setup_options.get(\\"author\\", \\"\\") author_email = setup_options.get(\\"author_email\\", \\"\\") description = setup_options.get(\\"description\\", \\"\\") long_description = setup_options.get(\\"long_description\\", \\"\\") spec_file_content = fName: {name} Version: {version} Summary: {description} License: {license_} URL: {url} Packager: {author} <{author_email}> %description {long_description} return spec_file_content"},{"question":"# Virtual Environment Automator Given the importance of managing Python dependencies in different projects, your task is to create a Python script that automates the creation and management of virtual environments. Your script should be capable of: 1. Creating a virtual environment. 2. Activating the created virtual environment. 3. Installing specific packages with their versions from a given requirements file. # Requirements * You must create a Python function `setup_virtualenv(env_dir, requirements_file)` where: * `env_dir`: the directory where the virtual environment will be created. * `requirements_file`: a file containing a list of packages with their versions in the format used by `pip freeze`. # Function Implementation ```python import subprocess import os import sys def setup_virtualenv(env_dir, requirements_file): Create and set up a virtual environment. :param env_dir: Directory for the virtual environment. :param requirements_file: Path to a file with package requirements. # Step 1: Create the virtual environment in the specified directory subprocess.run([sys.executable, \'-m\', \'venv\', env_dir]) # Step 2: Activate the virtual environment if os.name == \'nt\': activate_script = os.path.join(env_dir, \'Scripts\', \'activate.bat\') else: activate_script = os.path.join(env_dir, \'bin\', \'activate\') # Step 3: Install the requirements # Note: Activation in the same script does not work the same way as the shell command. # So we directly call pip from the virtual environment\'s python interpreter. env_python = os.path.join(env_dir, \'bin\', \'python\') if os.name != \'nt\' else os.path.join(env_dir, \'Scripts\', \'python\') subprocess.run([env_python, \'-m\', \'pip\', \'install\', \'-r\', requirements_file]) # Example usage (Uncomment to test, adjust paths accordingly): # setup_virtualenv(\'myenv\', \'requirements.txt\') ``` # Input * `env_dir` is a string representing the directory where the virtual environment will be created. For example: `\\"my_project_env\\"`. * `requirements_file` is a string representing the path to the file containing the required packages and their versions. For example: `\\"requirements.txt\\"`. # Output The function does not return any value, but it sets up the virtual environment in the specified directory and installs all required packages as per the `requirements_file`. # Constraints * Ensure that all required packages in the `requirements_file` are installed correctly. * Handle any exceptions that may arise during the creation of the virtual environment or installation of the packages. * Make use of Python\'s `subprocess` module to run command-line operations. # Additional Notes * Write your implementation in such a way that it works across different operating systems (Windows, macOS, Linux). * The `requirements_file` will follow the format of `pip freeze`, listing packages with `==` followed by their version numbers. By completing this task, you will demonstrate your understanding of creating and managing Python virtual environments, which is essential for developing and maintaining Python applications with different dependencies.","solution":"import subprocess import os import sys def setup_virtualenv(env_dir, requirements_file): Create and set up a virtual environment. :param env_dir: Directory for the virtual environment. :param requirements_file: Path to a file with package requirements. # Step 1: Create the virtual environment in the specified directory subprocess.run([sys.executable, \'-m\', \'venv\', env_dir], check=True) # Step 2: Activate the virtual environment if os.name == \'nt\': activate_script = os.path.join(env_dir, \'Scripts\', \'activate.bat\') else: activate_script = os.path.join(env_dir, \'bin\', \'activate\') # Step 3: Install the requirements # Note: Activation in the same script does not work the same way as the shell command. # So we directly call pip from the virtual environment\'s python interpreter. env_python = os.path.join(env_dir, \'bin\', \'python\') if os.name != \'nt\' else os.path.join(env_dir, \'Scripts\', \'python\') subprocess.run([env_python, \'-m\', \'pip\', \'install\', \'-r\', requirements_file], check=True)"},{"question":"# Question: Iris Dataset Classification with k-Nearest Neighbors You are provided with the Iris dataset, one of the standard toy datasets available in scikit-learn. Your task is to implement a function that performs the following steps: 1. **Load the Iris dataset** using `sklearn.datasets.load_iris`. 2. **Split the data** into a training set (70%) and a test set (30%). 3. **Preprocess the data**: Standardize the features by removing the mean and scaling to unit variance. 4. **Train a k-Nearest Neighbors classifier** with `k=5` on the training set. 5. **Evaluate the model** on the test set and return the accuracy score. # Function Signature ```python def iris_knn_classification(): Load the Iris dataset, split it into training and test sets, standardize the features, train a k-NN classifier, and return the accuracy of the classifier on the test set. Returns: float: The accuracy score of the k-NN classifier on the test set. ``` # Expected Input and Output - **Input:** None - **Output:** A float value representing the classification accuracy of the k-NN classifier on the test set. # Constraints - Use `sklearn.model_selection.train_test_split` for splitting the data. - Use `sklearn.preprocessing.StandardScaler` for feature standardization. - Use `sklearn.neighbors.KNeighborsClassifier` for the k-NN classifier. - Use `sklearn.metrics.accuracy_score` to compute the accuracy. # Example Usage ```python accuracy = iris_knn_classification() print(f\\"Model Accuracy: {accuracy}\\") ``` In this example, the function should load the Iris dataset, perform the necessary steps, and print the accuracy of the k-NN classifier.","solution":"from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.neighbors import KNeighborsClassifier from sklearn.metrics import accuracy_score def iris_knn_classification(): Load the Iris dataset, split it into training and test sets, standardize the features, train a k-NN classifier, and return the accuracy of the classifier on the test set. Returns: float: The accuracy score of the k-NN classifier on the test set. # Load the Iris dataset iris = load_iris() X, y = iris.data, iris.target # Split the data into training and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Standardize the features scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) # Train a k-NN classifier with k=5 knn = KNeighborsClassifier(n_neighbors=5) knn.fit(X_train_scaled, y_train) # Evaluate the model on the test set y_pred = knn.predict(X_test_scaled) accuracy = accuracy_score(y_test, y_pred) return accuracy"},{"question":"**Objective:** Demonstrate competence in utilizing Python\'s system-level modules to create a complex utility. # Problem Statement: You are required to create a Python script that performs the following system-level tasks using the `os`, `time`, and `logging` modules: 1. **Create a Log File:** - Initialize a logger that logs messages to a file named `activity.log`. The log should record messages at the INFO level and above. Use a rotating file handler to ensure the log file does not exceed 1MB and keep up to 3 backup files. 2. **Directory and File Management:** - Write a function `manage_files` that: - Checks if a directory named `sample_data` exists in the current working directory. If not, create it. - Inside `sample_data`, create 10 text files named `file_1.txt` to `file_10.txt`. - Log the creation of each file with a timestamp. 3. **Timestamping and Logging:** - Write a function `log_timestamps` that: - Reads each file in the `sample_data` directory and appends the current timestamp to the end of each file. Use the `time` module to get the current timestamp in the format `YYYY-MM-DD HH:MM:SS`. - Log the timestamped update of each file with a message such as \\"Appended timestamp to file_X.txt\\". **Requirements:** - Your script should use the `os`, `time`, and `logging` modules. - Ensure all log messages are appropriately detailed, mentioning the specific files and actions taken. - The script should be efficient and handle any potential exceptions (e.g., directory creation errors, file writing errors). **Constraints and Performance:** - Assume the script will run on a Unix or Unix-like system. - Ensure the script can handle the file creation and modification operations in less than 5 seconds for the given number of files (10). **Input and Output:** - There are no external inputs. The script autonomously manages file creation and logging. - Output is not returned but logged in the `activity.log` file. You may assume that the script has the necessary permissions to create directories and files in the current working directory. # Example Here\'s a sample log output: ``` INFO:root:Directory \'sample_data\' created. INFO:root:File \'file_1.txt\' created. ... INFO:root:File \'file_10.txt\' created. INFO:root:Appended timestamp to file_1.txt. ... INFO:root:Appended timestamp to file_10.txt. ``` Develop your script in a file named `file_manager.py` and ensure it meets the above requirements.","solution":"import os import time import logging from logging.handlers import RotatingFileHandler def configure_logger(): # Configure logger settings logger = logging.getLogger() logger.setLevel(logging.INFO) handler = RotatingFileHandler(\'activity.log\', maxBytes=1*1024*1024, backupCount=3) formatter = logging.Formatter(\'%(asctime)s - %(levelname)s - %(message)s\') handler.setFormatter(formatter) # Add the handler to the logger logger.addHandler(handler) def manage_files(): # Configure logger configure_logger() logger = logging.getLogger() directory = \\"sample_data\\" # Check if directory exists, if not create it if not os.path.exists(directory): try: os.makedirs(directory) logger.info(f\\"Directory \'{directory}\' created.\\") except Exception as e: logger.error(f\\"Error creating directory \'{directory}\': {e}\\") return # Create 10 text files in the directory for i in range(1, 11): file_name = os.path.join(directory, f\\"file_{i}.txt\\") try: with open(file_name, \'w\') as f: f.write(\\"\\") logger.info(f\\"File \'{file_name}\' created.\\") except Exception as e: logger.error(f\\"Error creating file \'{file_name}\': {e}\\") def log_timestamps(): # Configure logger logger = logging.getLogger() directory = \\"sample_data\\" current_time = time.strftime(\\"%Y-%m-%d %H:%M:%S\\") # Append current timestamp to each file in the directory for i in range(1, 11): file_name = os.path.join(directory, f\\"file_{i}.txt\\") try: with open(file_name, \'a\') as f: f.write(current_time + \\"n\\") logger.info(f\\"Appended timestamp to \'{file_name}\'.\\") except Exception as e: logger.error(f\\"Error appending timestamp to \'{file_name}\': {e}\\") # Running the functions if __name__ == \\"__main__\\": manage_files() log_timestamps()"},{"question":"You are tasked with creating a comprehensive analysis of the `penguins` dataset using seaborn\'s `so.Plot` and `so.Jitter`. Your analysis should: 1. **Load the `penguins` dataset** using seaborn. 2. **Create a plot** displaying the relationship between `species` and `body_mass_g`, applying a jitter with a specified width and visual styles. 3. **Enhance the analysis** by creating a second plot showing the relationship between `body_mass_g` and `flipper_length_mm`, incorporating jitter on both axes and including customized jitter parameters. 4. **Add titles and labels** to make the plots informative and visually appealing. Step-by-Step Instructions: 1. **Loading the Dataset:** - Use `seaborn` to load the `penguins` dataset. 2. **Creating the First Plot:** - Use `seaborn.objects` to create a plot with `species` on the x-axis and `body_mass_g` on the y-axis. - Add jitter to the plot using the `so.Jitter()` method with a width of 0.3. - Customize the plot by setting an appropriate title and axis labels. 3. **Creating the Second Plot:** - Create another plot using `so.Plot` that shows the relationship between `body_mass_g` on the x-axis and `flipper_length_mm` on the y-axis. - Apply jitter in both dimensions using `so.Jitter(x=300, y=10)`. - Add a title and axis labels to this plot as well. Constraints: - Ensure the plots are clearly distinguishable and both contain well-defined titles and axis labels. - The jitter parameters should demonstrably alter the position of the data points to reduce overlapping, making the plots more readable. Output: Your function should display two plots: 1. The first plot showing the relationship between `species` and `body_mass_g` with jitter applied. 2. The second plot showing the relationship between `body_mass_g` and `flipper_length_mm` with jitter on both axes. ```python import seaborn.objects as so from seaborn import load_dataset import matplotlib.pyplot as plt def create_penguin_plots(): # Load the penguins dataset penguins = load_dataset(\\"penguins\\") # Create the first plot (species vs body_mass_g) plt.figure() (so.Plot(penguins, \\"species\\", \\"body_mass_g\\") .add(so.Dots(), so.Jitter(width=0.3)) .label(x=\'Species\', y=\'Body Mass (g)\', title=\'Species vs Body Mass with Jitter\') .show()) # Create the second plot (body_mass_g vs flipper_length_mm) plt.figure() (so.Plot(penguins, \\"body_mass_g\\", \\"flipper_length_mm\\") .add(so.Dots(), so.Jitter(x=300, y=10)) .label(x=\'Body Mass (g)\', y=\'Flipper Length (mm)\', title=\'Body Mass vs Flipper Length with Jitter\') .show()) # Execute the function to generate plots create_penguin_plots() ``` This question will test the student\'s ability to utilize seaborn\'s plotting capabilities, specifically focusing on the `so.Plot` and `so.Jitter` functionalities, and their competence in enhancing plot readability and presentation.","solution":"import seaborn.objects as so from seaborn import load_dataset import matplotlib.pyplot as plt def create_penguin_plots(): # Load the penguins dataset penguins = load_dataset(\\"penguins\\") # Create the first plot (species vs body_mass_g) plt.figure() (so.Plot(penguins, \\"species\\", \\"body_mass_g\\") .add(so.Dots(), so.Jitter(width=0.3)) .label(x=\'Species\', y=\'Body Mass (g)\', title=\'Species vs Body Mass with Jitter\') .show()) # Create the second plot (body_mass_g vs flipper_length_mm) plt.figure() (so.Plot(penguins, \\"body_mass_g\\", \\"flipper_length_mm\\") .add(so.Dots(), so.Jitter(x=300, y=10)) .label(x=\'Body Mass (g)\', y=\'Flipper Length (mm)\', title=\'Body Mass vs Flipper Length with Jitter\') .show()) # Execute the function to generate plots create_penguin_plots()"},{"question":"# PyTorch CUDA Stream Synchronization Task Objective: You are required to implement a function that demonstrates your understanding of CUDA stream synchronization and handling data races using PyTorch. You will write code to create a tensor and perform operations on it using multiple CUDA streams. You should also handle any potential synchronization issues. Task: Implement the function `stream_synchronization_demo()` with the following specifications: 1. Create a tensor `a` of shape `(10, 10)` filled with random numbers on a CUDA device. 2. Launch a new CUDA stream (`stream1`) and perform an in-place operation (e.g., adding a scalar) on tensor `a`. 3. Launch a second new CUDA stream (`stream2`) and perform another in-place operation (e.g., multiplying by a scalar) on tensor `a`. 4. Ensure there are no synchronization errors between the streams by properly waiting for streams to finish their tasks where necessary. 5. Enable the CUDA sanitizer to detect synchronization errors and demonstrate that your code is free from these errors. Constraints: - Use only PyTorch functions and utilities. - Ensure your function runs without errors and correctly handles synchronization issues. Example Usage: Here is an example of how your function might be used: ```python def stream_synchronization_demo(): pass # Implement this function # Running the function and verifying no synchronization errors stream_synchronization_demo() ``` This function should demonstrate the ability to correctly handle CUDA stream synchronization in PyTorch and avoid data races.","solution":"import torch def stream_synchronization_demo(): # Ensure CUDA is available if not torch.cuda.is_available(): raise RuntimeError(\\"CUDA is not available on this machine.\\") # Create a tensor on the CUDA device filled with random numbers a = torch.rand((10, 10), device=\'cuda\') # Create CUDA streams stream1 = torch.cuda.Stream() stream2 = torch.cuda.Stream() # Perform operations in different streams with torch.cuda.stream(stream1): a.add_(5) # Add scalar in-place with torch.cuda.stream(stream2): stream2.wait_stream(stream1) # Ensure stream2 waits for stream1 to finish a.mul_(2) # Multiply by scalar in-place # Synchronize default and other streams to ensure all operations are completed torch.cuda.synchronize() return a.cpu().numpy() # Move the result back to CPU and return for verification"},{"question":"Objective: Design a function that utilizes the `CountVectorizer`, `TfidfTransformer`, and `DictVectorizer` classes from the `sklearn.feature_extraction` module to process text data and make a prediction using a simple machine learning classifier. Problem Statement: You are tasked with creating a function that fits a machine learning pipeline on a given dataset of text and makes predictions on a separate test set. The dataset consists of text documents and their respective categories. Input: - `train_data`: A list of dicts, each representing a training example with the following keys: - `\\"text\\"`: A string representing the document. - `\\"label\\"`: A string representing the category of the document. - `test_data`: A list of strings, each representing a test document that needs to be classified. Output: - A list of predicted categories for each document in the `test_data`. Constraints: - You must use the `CountVectorizer` for tokenizing and counting word occurrences. - Use `TfidfTransformer` to convert the count matrix to a TF-IDF representation. - Make use of the `DictVectorizer` to handle any categorical features in the training data. - Implement a machine learning classifier of your choice (e.g., LogisticRegression, MultinomialNB). Function Signature: ```python def train_and_predict(train_data: List[Dict[str, str]], test_data: List[str]) -> List[str]: pass ``` # Example Usage: ```python from sklearn.linear_model import LogisticRegression train_data = [ {\\"text\\": \\"This is the first document.\\", \\"label\\": \\"A\\"}, {\\"text\\": \\"This document is the second document.\\", \\"label\\": \\"B\\"}, {\\"text\\": \\"And this is the third one.\\", \\"label\\": \\"A\\"}, {\\"text\\": \\"Is this the first document?\\", \\"label\\": \\"B\\"} ] test_data = [ \\"This is a new document.\\", \\"And here is another one.\\" ] predicted_labels = train_and_predict(train_data, test_data) print(predicted_labels) # Expected: A list of predicted labels ``` # Requirements: 1. Tokenize and count word occurrences using `CountVectorizer`. 2. Convert the count matrix to a TF-IDF representation using `TfidfTransformer`. 3. Handle categorical data using `DictVectorizer`. 4. Fit a machine learning model on the transformed training data. 5. Make predictions for the test data. This question assesses: - Your understanding of feature extraction techniques. - Ability to integrate these techniques into a machine learning pipeline. - Implementation and usage of machine learning classifiers.","solution":"from typing import List, Dict from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer from sklearn.linear_model import LogisticRegression from sklearn.pipeline import Pipeline def train_and_predict(train_data: List[Dict[str, str]], test_data: List[str]) -> List[str]: texts = [d[\'text\'] for d in train_data] labels = [d[\'label\'] for d in train_data] # Create a pipeline that includes feature extraction and a classifier pipeline = Pipeline([ (\'vectorizer\', CountVectorizer()), (\'tfidf\', TfidfTransformer()), (\'classifier\', LogisticRegression()) ]) # Fit the model on the training data pipeline.fit(texts, labels) # Predict on the test data predictions = pipeline.predict(test_data) return list(predictions)"},{"question":"Coding Assessment Question # Objective Implement and demonstrate the use of the `email.charset.Charset` class to manage character sets and their conversions pertinent to creating RFC-compliant email messages. # Problem Statement Write a Python script that performs the following tasks: 1. Create a `Charset` instance with the input charset \\"iso-8859-1\\". 2. Print the character set details (`input_charset`, `header_encoding`, `body_encoding`, `output_charset`, `input_codec`, `output_codec`). 3. Encode a specified header and body text using the charset settings and print the encoded results. 4. Add a new charset \\"custom-charset\\" with specific header and body encodings. 5. Demonstrate the charset aliasing by adding an alias \\"latin-1-alt\\" for \\"iso-8859-1\\". # Format and Requirements 1. Function `create_charset_instance()`: - **Input**: A string for input charset. - **Output**: An instance of `Charset`. 2. Function `print_charset_details(charset_instance)`: - **Input**: A `Charset` instance. - **Output**: Prints the details (`input_charset`, `header_encoding`, `body_encoding`, `output_charset`, `input_codec`, `output_codec`). 3. Function `encode_texts(charset_instance, header_text, body_text)`: - **Input**: A `Charset` instance, a string for header text, a string for body text. - **Output**: Encoded header and body texts as per the charset\'s properties. 4. Function `add_custom_charset(charset_name, header_enc, body_enc, output_charset)`: - **Input**: A string for charset name, encodings for header and body (`QP`, `BASE64`, `None`), and a string for output charset. - **Output**: Modifies the charset registry (`email.charset.add_charset`). 5. Function `add_charset_alias(alias, canonical)`: - **Input**: Strings for alias and canonical charset name. - **Output**: Modifies the charset alias registry (`email.charset.add_alias`). # Constraints - Ensure that the `Charset` instance is created correctly and methods are utilized as per the `email.charset` module documentation. - Your solutions must handle exceptions that arise from invalid charset conversions or unsupported codecs gracefully. # Example Execution ```python # Example Inputs input_charset = \\"iso-8859-1\\" header_text = \\"This is a test header.\\" body_text = \\"This is a test body.\\" # Function Calls charset_instance = create_charset_instance(input_charset) print_charset_details(charset_instance) encoded_header, encoded_body = encode_texts(charset_instance, header_text, body_text) # Adding custom charset add_custom_charset(\\"custom-charset\\", \\"QP\\", \\"BASE64\\", \\"utf-8\\") # Adding an alias add_charset_alias(\\"latin-1-alt\\", \\"iso-8859-1\\") # Results expected: # (Details of `iso-8859-1` charset) # Encoded header and body texts # Confirmation of added charset and alias ``` Make sure your script follows best practices for coding including clear documentation and appropriate use of exceptions.","solution":"from email.charset import Charset, BASE64, QP, add_charset, add_alias def create_charset_instance(input_charset): Creates a Charset instance with the specified input charset. return Charset(input_charset) def print_charset_details(charset_instance): Prints the details of a Charset instance. print(f\\"Input Charset: {charset_instance.input_charset}\\") print(f\\"Header Encoding: {charset_instance.header_encoding}\\") print(f\\"Body Encoding: {charset_instance.body_encoding}\\") print(f\\"Output Charset: {charset_instance.output_charset}\\") print(f\\"Input Codec: {charset_instance.input_codec}\\") print(f\\"Output Codec: {charset_instance.output_codec}\\") def encode_texts(charset_instance, header_text, body_text): Encodes the given header and body texts using the charset instance. encoded_header = charset_instance.header_encode(header_text) encoded_body = charset_instance.body_encode(body_text) return encoded_header, encoded_body def add_custom_charset(charset_name, header_enc, body_enc, output_charset): Adds a custom charset to the charset registry. add_charset(charset_name, header_enc, body_enc, output_charset) def add_charset_alias(alias, canonical): Adds an alias for an existing charset. add_alias(alias, canonical)"},{"question":"You are required to manage a series of tasks using asynchronous programming concepts and the `asyncio` queue described in the provided documentation. Each task involves a varying amount of work to be done (simulated by sleeping for a random amount of time). Your goal is to ensure that work is distributed equally among a set of worker coroutines and that the total amount of work is processed in a time-efficient manner. Write a function `process_tasks` which: - Creates an `asyncio.Queue` instance. - Produces a set number of tasks with randomly assigned work times (sleep durations) and adds them to the `Queue`. - Creates a given number of worker coroutines to process the tasks concurrently from the same queue. - Ensures that the total work is processed efficiently and all worker coroutines terminate correctly. # Function Signature ```python import asyncio import random async def process_tasks(task_count: int, worker_count: int) -> float: pass ``` # Parameters - `task_count` (int): The number of tasks to generate and process. - `worker_count` (int): The number of worker coroutines to process the queue. # Returns - `float`: The total time taken to process all tasks. # Constraints - Each task\'s sleep duration will randomly vary between 0.05 and 1.0 seconds. - Utilize coroutines and the provided `asyncio` queue effectively to manage task processing. # Example ```python import random import time import asyncio async def worker(name, queue): while True: try: sleep_for = await queue.get() await asyncio.sleep(sleep_for) queue.task_done() except asyncio.CancelledError: break async def process_tasks(task_count: int, worker_count: int) -> float: queue = asyncio.Queue() for _ in range(task_count): queue.put_nowait(random.uniform(0.05, 1.0)) tasks = [] for i in range(worker_count): task = asyncio.create_task(worker(f\'worker-{i}\', queue)) tasks.append(task) start = time.monotonic() await queue.join() end = time.monotonic() for task in tasks: task.cancel() await asyncio.gather(*tasks, return_exceptions=True) return end - start # Example call asyncio.run(process_tasks(20, 3)) ``` # Notes - Create the tasks and worker coroutines within the `process_tasks` function. - Make sure to handle the cancellation of worker tasks properly to terminate them after all work is done. - The underlying implementation should adhere to the asyncio queue methods as described in the documentation.","solution":"import asyncio import random import time async def worker(name, queue): while True: try: sleep_for = await queue.get() await asyncio.sleep(sleep_for) queue.task_done() except asyncio.CancelledError: break async def process_tasks(task_count: int, worker_count: int) -> float: queue = asyncio.Queue() for _ in range(task_count): queue.put_nowait(random.uniform(0.05, 1.0)) tasks = [] for i in range(worker_count): task = asyncio.create_task(worker(f\'worker-{i}\', queue)) tasks.append(task) start = time.monotonic() await queue.join() end = time.monotonic() for task in tasks: task.cancel() await asyncio.gather(*tasks, return_exceptions=True) return end - start"},{"question":"Objective In this task, you will demonstrate your understanding of memory management by implementing a simple custom object allocator in Python. While Python manages memory for you, understanding how it works underneath can give you deeper insights into performance optimization and how objects are created and managed. Problem Description You are required to create a custom memory manager for simple Python objects that mimics the behavior of the C-based memory allocation functions described in the provided documentation. Specifications: 1. Create a class `CustomMemoryAllocator` with methods to allocate and deallocate objects, inspired by the functions in the provided documentation. 2. Implement the following methods within the class: - `new_object(obj_type: type) -> object`: Simulates the creation of a new object of the given type. - `new_variable_object(obj_type: type, size: int) -> object`: Simulates the creation of a new variable-sized object of the given type and size. - `init_object(obj: object, obj_type: type) -> object`: Initializes a newly allocated object with its type. - `init_variable_object(obj: object, obj_type: type, size: int) -> object`: Initializes a newly allocated variable-sized object with its type and size. - `del_object(obj: object) -> None`: Simulates deallocation of an object. Constraints: - The `new_object` method should initialize the object\'s reference count to 1. - The `new_variable_object` method should additionally store and manage the size of the object. - The `init_object` and `init_variable_object` methods should prepare an object without needing to access private Python APIs directly. - The `del_object` method should ensure the object\'s memory is no longer accessible after deletion. Example Usage: ```python allocator = CustomMemoryAllocator() # Allocate a new fixed-size object obj1 = allocator.new_object(int) print(obj1) # Expected: 0 # Allocate a new variable-size object obj2 = allocator.new_variable_object(list, 10) print(obj2) # Expected: [] # Initialize an object allocator.init_object(obj1, int) # Expected: initializes obj1 if further initialization was required # Initialize a variable-size object allocator.init_variable_object(obj2, list, 10) # Expected: initializes obj2 if further initialization was required # Deallocate an object allocator.del_object(obj1) # Expected: obj1 memory is deallocated ``` Evaluation: - Correctness of the implementation. - Adherence to constraints. - Handling different object types and sizes appropriately. - Simulated memory management effectiveness and accuracy.","solution":"class CustomMemoryAllocator: def __init__(self): self.memory_pool = {} def new_object(self, obj_type: type) -> object: obj = obj_type() self.memory_pool[id(obj)] = {\\"object\\": obj, \\"ref_count\\": 1} return obj def new_variable_object(self, obj_type: type, size: int) -> object: if obj_type == list: obj = [None] * size else: raise TypeError(\\"Unsupported variable size object type\\") self.memory_pool[id(obj)] = {\\"object\\": obj, \\"ref_count\\": 1, \\"size\\": size} return obj def init_object(self, obj: object, obj_type: type) -> object: # Assuming initialization logic here is minimal or none for basic types return obj def init_variable_object(self, obj: object, obj_type: type, size: int) -> object: if id(obj) in self.memory_pool: self.memory_pool[id(obj)][\\"size\\"] = size return obj def del_object(self, obj: object) -> None: if id(obj) in self.memory_pool: del self.memory_pool[id(obj)]"},{"question":"Objective The purpose of this question is to evaluate your understanding of threading and synchronization in Python using the `threading` module. Problem Statement You are required to implement a multi-threaded program using Python\'s `threading` module to simulate a simplified version of a dining philosophers problem. The dining philosophers problem is an example classic synchronization problem where a certain number of philosophers sit at a circular table with a bowl of spaghetti in the center. The philosophers alternatively think and eat, but need two forks (one from the left and one from the right) to eat. # Requirements 1. Implement a class `DiningPhilosophers` that initializes with the number of philosophers (N). Each philosopher will be represented by a thread. 2. Each philosopher thread should alternate between \\"thinking\\" and \\"eating\\". Simulate the act of thinking by sleeping for `random.uniform(1, 3)` seconds and eating by sleeping for `random.uniform(1, 3)` seconds. 3. Use `threading.Lock` for each fork, ensuring that a philosopher must acquire the locks for both the left and right forks before they can eat. 4. Ensure that deadlocking is avoided. 5. Print messages when a philosopher starts and stops thinking and when they start and stop eating. Implementation Details - Define a class `DiningPhilosophers`. - Initialize it with an integer `num_philosophers`. - Each philosopher (thread) attempts to pick up the fork on their left and right in a deadlock-free manner. - Use a `run` method to start all philosopher threads. - Each philosopher\'s actions (thinking, hungry, pick up left fork, pick up right fork, eating, put down forks) should be demonstrated via printed messages. Expected Input and Output Format 1. The class should be initialized with the number of philosophers. 2. There is no direct input; you should observe the printed statements for correctness. # Example: ```python import threading import random class DiningPhilosophers: def __init__(self, num_philosophers): # Initialize all necessary elements here pass def philosopher(self, id): # Define the philosopher\'s actions here pass def run(self): # Start threads and manage the simulation pass # Example usage dp = DiningPhilosophers(5) dp.run() ``` Expected output structure: ``` Philosopher 0 is thinking Philosopher 1 is thinking ... Philosopher 0 is hungry Philosopher 0 picked up left fork Philosopher 0 picked up right fork Philosopher 0 is eating ... Philosopher 0 put down forks Philosopher 0 is thinking ... ``` Constraints - The number of philosophers `num_philosophers` will be between 2 and 10. - Each philosopher should continue their cycle (think-eat-think) at least 3 times before the program exits. Performance Requirements - Ensure that no deadlock occurs during the simulation. - The solution should be able to handle the maximum number of philosophers without significant delays. Please implement the `DiningPhilosophers` class and the necessary threading logic to simulate the dining philosophers problem as described above.","solution":"import threading import random import time class DiningPhilosophers: def __init__(self, num_philosophers): self.num_philosophers = num_philosophers self.forks = [threading.Lock() for _ in range(num_philosophers)] self.philosophers = [ threading.Thread(target=self.philosopher, args=(i,)) for i in range(num_philosophers) ] def philosopher(self, id): for _ in range(3): # Each philosopher cycles through thinking and eating three times print(f\\"Philosopher {id} is thinking\\") time.sleep(random.uniform(1, 3)) print(f\\"Philosopher {id} is hungry\\") left_fork = self.forks[id] right_fork = self.forks[(id + 1) % self.num_philosophers] # Prevent deadlock by requiring even indexed philosophers to pick left fork first and odd indexed to pick right fork first if id % 2 == 0: with left_fork: print(f\\"Philosopher {id} picked up left fork\\") with right_fork: print(f\\"Philosopher {id} picked up right fork\\") print(f\\"Philosopher {id} is eating\\") time.sleep(random.uniform(1, 3)) else: with right_fork: print(f\\"Philosopher {id} picked up right fork\\") with left_fork: print(f\\"Philosopher {id} picked up left fork\\") print(f\\"Philosopher {id} is eating\\") time.sleep(random.uniform(1, 3)) print(f\\"Philosopher {id} put down forks\\") def run(self): for philosopher in self.philosophers: philosopher.start() for philosopher in self.philosophers: philosopher.join()"},{"question":"As a Python developer, you are required to create a custom Sequence class that emulates the behavior of Python\'s native sequences (lists and tuples) using Python\'s native methods. This class should provide the following functionalities: 1. **Initialization**: Your `CustomSequence` class should be initialized with an iterable. 2. **Length of the Sequence**: Implement the `__len__` method which returns the number of elements. 3. **Get Item**: Implement the `__getitem__` method to get an item at a specific index. 4. **Set Item**: Implement the `__setitem__` method to set an item at a specific index. 5. **Delete Item**: Implement the `__delitem__` method to delete an item at a specific index. 6. **Slicing**: Implement the `__getslice__` and `__setslice__` methods to get and set slices respectively. 7. **Concatenation**: Implement a method `concat` to concatenate another sequence. 8. **Repetition**: Implement a method `repeat` to repeat the sequence a specified number of times. 9. **Containment**: Implement the `__contains__` method to check if an element is in the sequence. 10. **Index Finding**: Implement a method `index` to find the index of the first occurrence of an element. 11. **Counting Occurrences**: Implement a method `count` to count occurrences of a value in the sequence. 12. **Convert to List**: Implement a method `to_list` that returns the sequence as a list. 13. **Convert to Tuple**: Implement a method `to_tuple` that returns the sequence as a tuple. ```python class CustomSequence: def __init__(self, iterable): # Initialize the sequence from the iterable def __len__(self): # Return the number of elements def __getitem__(self, index): # Return the item at the specified index def __setitem__(self, index, value): # Set the item at the specified index def __delitem__(self, index): # Delete the item at the specified index def __getslice__(self, start, end): # Return a slice from start to end def __setslice__(self, start, end, value): # Set a slice from start to end with value def concat(self, other): # Concatenate another sequence def repeat(self, times): # Return the sequence repeated specified times def __contains__(self, value): # Check if the value is in the sequence def index(self, value): # Return the index of the first occurrence of value def count(self, value): # Return the count of occurrences of value def to_list(self): # Return the sequence as a list def to_tuple(self): # Return the sequence as a tuple # You may also write test cases to verify the correctness of your implementation. ``` **Constraints and Notes**: - Your class should handle typical sequence operations efficiently. - Raise appropriate exceptions for invalid operations or invalid indices. - Ensure your implementation follows Python\'s typical behaviors for sequences. **Example**: ```python seq = CustomSequence([1, 2, 3, 4]) print(len(seq)) # Output: 4 print(seq[2]) # Output: 3 seq[2] = 10 print(seq[2]) # Output: 10 del seq[2] print(seq[2]) # Output: 4 print(seq.concat([5, 6])) # Output: CustomSequence([1, 2, 4, 5, 6]) print(seq.repeat(2)) # Output: CustomSequence([1, 2, 4, 1, 2, 4]) print(4 in seq) # Output: True print(seq.index(4)) # Output: 2 print(seq.count(1)) # Output: 1 print(seq.to_list()) # Output: [1, 2, 4] print(seq.to_tuple()) # Output: (1, 2, 4) ``` Implement the `CustomSequence` class and provide test cases to verify your implementation.","solution":"class CustomSequence: def __init__(self, iterable): self.sequence = list(iterable) def __len__(self): return len(self.sequence) def __getitem__(self, index): return self.sequence[index] def __setitem__(self, index, value): self.sequence[index] = value def __delitem__(self, index): del self.sequence[index] def __getslice__(self, start, end): return CustomSequence(self.sequence[start:end]) def __setslice__(self, start, end, value): self.sequence[start:end] = value def concat(self, other): return CustomSequence(self.sequence + list(other)) def repeat(self, times): return CustomSequence(self.sequence * times) def __contains__(self, value): return value in self.sequence def index(self, value): return self.sequence.index(value) def count(self, value): return self.sequence.count(value) def to_list(self): return list(self.sequence) def to_tuple(self): return tuple(self.sequence)"},{"question":"**Question: Analyzing Web Page Load Times with concurrent.futures** You are tasked with analyzing the load times of multiple web pages to identify potential performance issues. You will read a list of URLs from a file and measure how long each one takes to load. You\'ll use the `concurrent.futures` module to speed up the process by fetching the web pages concurrently. **Requirements:** 1. **Implement a function `fetch_url(url)`**: - This function takes a URL as an input. - It uses the `requests` library to perform a GET request to fetch the webpage. - Measures and returns the time taken to load the page in seconds. 2. **Implement a function `fetch_all_urls(file_path)`**: - This function reads a list of URLs from the provided file path. - Uses `concurrent.futures.ThreadPoolExecutor` to fetch each URL concurrently using the `fetch_url` function. - Returns a dictionary where each key is a URL and the corresponding value is the time taken to fetch that URL. **Constraints and Performance Requirements:** - The input file will have one URL per line. - You should use a maximum of 10 threads in the thread pool. - Handle potential exceptions that may occur during fetching (e.g., network errors). - Ensure efficient and correct handling of the results from the concurrent executions. **Input:** - A string representing the path to a file containing URLs (one URL per line). **Output:** - A dictionary where keys are URLs and values are the load times in seconds. **Example Usage:** ```python urls_file_path = \'urls.txt\' result = fetch_all_urls(urls_file_path) print(result) ``` If the content of `urls.txt` is: ``` http://example.com http://google.com http://openai.com ``` The output might look similar to: ```python { \'http://example.com\': 0.67, \'http://google.com\': 0.23, \'http://openai.com\': 1.10 } ``` **Additional Information:** - You need to install the `requests` library if it\'s not already available (`pip install requests`). - Ensure proper exception handling for network issues or invalid URLs. This exercise assesses your ability to handle concurrent tasks effectively using Python\'s `concurrent.futures` module.","solution":"import requests import time from concurrent.futures import ThreadPoolExecutor, as_completed def fetch_url(url): Fetches the URL and returns the time taken to load the page in seconds. Parameters: url (str): The URL to be fetched. Returns: float: Time taken to fetch the URL in seconds. start_time = time.time() try: response = requests.get(url) response.raise_for_status() except requests.RequestException: # If there is any exception, consider load time as 0. return (url, 0) end_time = time.time() return (url, end_time - start_time) def fetch_all_urls(file_path): Reads a list of URLs from the provided file path and fetches each URL concurrently. Parameters: file_path (str): Path to the file containing URLs (one URL per line). Returns: dict: A dictionary where each key is a URL and the value is the load time in seconds. with open(file_path, \'r\') as file: urls = [url.strip() for url in file.readlines()] results = {} with ThreadPoolExecutor(max_workers=10) as executor: future_to_url = {executor.submit(fetch_url, url): url for url in urls} for future in as_completed(future_to_url): url, load_time = future.result() results[url] = load_time return results"},{"question":"# Advanced Python Coding Assessment Objective: Design a function leveraging multiple Python310 concepts. The function `process_data` should perform data transformation based on dynamically defined rules utilizing compound statements, pattern matching, and handling asynchronous data fetching. # Function Requirements: 1. **Function Signature:** ```python async def process_data(data: list, rule: dict, fetch_data: callable) -> list: pass ``` 2. **Input:** - `data`: A list of dictionaries where each dictionary represents an entity. - `rule`: A dictionary defining transformation rules, including conditions and corresponding actions. - `fetch_data`: An asynchronous callable function that accepts a dictionary and returns augmented data. 3. **Output:** - A list of transformed dictionaries based on the rule provided. # Detailed Instructions: 1. **Transformation Rules:** - The `rule` dictionary contains `conditions` and `actions`. Conditions specify a pattern to match entities, and actions describe transformations to apply. - Example of a rule: ```python { \\"conditions\\": [ {\\"key\\": \\"status\\", \\"value\\": \\"active\\"}, {\\"key\\": \\"type\\", \\"value\\": \\"premium\\"} ], \\"actions\\": { \\"update\\": { \\"score\\": 10 }, \\"fetch\\": True } } ``` 2. **Pattern Matching:** - Use the `match` statement to evaluate conditions on the data entities. If conditions match, apply the corresponding actions. 3. **Asynchronous Fetching:** - If the `fetch` action is specified and set to `True`, invoke the `fetch_data` function asynchronously on the matching entity. 4. **Example:** Assume input data and rule as follows: ```python data = [ {\\"id\\": 1, \\"status\\": \\"active\\", \\"type\\": \\"standard\\"}, {\\"id\\": 2, \\"status\\": \\"active\\", \\"type\\": \\"premium\\"}, {\\"id\\": 3, \\"status\\": \\"inactive\\", \\"type\\": \\"standard\\"} ] rule = { \\"conditions\\": [ {\\"key\\": \\"status\\", \\"value\\": \\"active\\"}, {\\"key\\": \\"type\\", \\"value\\": \\"premium\\"} ], \\"actions\\": { \\"update\\": { \\"score\\": 10 }, \\"fetch\\": True } } async def fetch_data(entity): # Simulate fetching additional data asynchronously await asyncio.sleep(1) entity[\'extra_data\'] = \'fetched_value\' return entity ``` Calling `await process_data(data, rule, fetch_data)` should return: ```python [ {\\"id\\": 1, \\"status\\": \\"active\\", \\"type\\": \\"standard\\"}, {\\"id\\": 2, \\"status\\": \\"active\\", \\"type\\": \\"premium\\", \\"score\\": 10, \\"extra_data\\": \\"fetched_value\\"}, {\\"id\\": 3, \\"status\\": \\"inactive\\", \\"type\\": \\"standard\\"} ] ``` # Constraints: - Ensure the function handles conditions and specified actions effectively. - Manage asynchronous data fetching efficiently, raising appropriate exceptions if `fetch_data` fails. # Hints: - Use `match` for pattern matching conditions within the entities. - Apply transformations as defined in the rule actions. - Handle exceptions gracefully using `try`...`except` blocks during asynchronous operations.","solution":"import asyncio async def process_data(data: list, rule: dict, fetch_data: callable) -> list: async def apply_rule(entity): # Check conditions for condition in rule[\'conditions\']: if entity.get(condition[\'key\']) != condition[\'value\']: return entity # Apply update action if conditions are met if \'update\' in rule[\'actions\']: for key, value in rule[\'actions\'][\'update\'].items(): entity[key] = value # Apply fetch action if conditions are met if rule[\'actions\'].get(\'fetch\'): try: entity = await fetch_data(entity) except Exception as e: print(f\\"Error fetching data: {e}\\") return entity tasks = [apply_rule(entity) for entity in data] return await asyncio.gather(*tasks)"},{"question":"**Coding Assessment Question** **Objective:** You are tasked to write a Python script using the `ossaudiodev` module to record audio from an input device for a specified duration and then play it back. This will involve opening an audio device for both reading and writing, setting proper audio parameters, and managing the recording and playback. **Problem Statement:** 1. Write a function `record_audio` to record audio from the input device. 2. Write a function `play_audio` to playback the recorded audio. 3. Combine these functions in a main script to achieve the complete workflow. **Function 1: record_audio** - **Input:** - `duration` (int): The duration of the recording in seconds. - `format` (str): The audio format, e.g., `AFMT_S16_LE`. - `channels` (int): Number of audio channels (1 for mono, 2 for stereo). - `samplerate` (int): The audio sample rate (e.g., 44100 for CD quality). - **Output:** - A bytes object containing the recorded audio data. - **Constraints:** - Only formats, channels, and sample rates supported by the device should be used. **Function 2: play_audio** - **Input:** - `audio_data` (bytes): The audio data to be played back. - `format` (str): The audio format, e.g., `AFMT_S16_LE`. - `channels` (int): Number of audio channels (1 for mono, 2 for stereo). - `samplerate` (int): The audio sample rate (e.g., 44100 for CD quality). - **Output:** None **Implementation Details:** 1. Open the audio device using `ossaudiodev.open` in read mode for `record_audio` and write mode for `play_audio`. 2. Set the required audio parameters (`format`, `channels`, `samplerate`) using the appropriate methods. 3. For recording, read the audio data for the specified duration. 4. For playback, write the audio data to the device. 5. Handle exceptions properly to ensure the device is closed in case of errors. **Example Usage:** ```python import ossaudiodev def record_audio(duration, format, channels, samplerate): # Implementation here pass def play_audio(audio_data, format, channels, samplerate): # Implementation here pass if __name__ == \\"__main__\\": duration = 5 # Record for 5 seconds format = \'AFMT_S16_LE\' channels = 2 samplerate = 44100 audio_data = record_audio(duration, format, channels, samplerate) play_audio(audio_data, format, channels, samplerate) ``` **Note:** - Ensure you handle all necessary conditions and edge cases, such as checking if the device supports the specified audio parameters. - The functions should use the context management protocol (i.e., `with` statement) for managing audio and mixer devices efficiently.","solution":"import ossaudiodev import time def record_audio(duration, format, channels, samplerate): Records audio from the input device for a specified duration. :param duration: The duration of the recording in seconds. :param format: The audio format, e.g., ossaudiodev.AFMT_S16_LE. :param channels: Number of audio channels (1 for mono, 2 for stereo). :param samplerate: The audio sample rate (e.g., 44100 for CD quality). :return: A bytes object containing the recorded audio data. recording = b\'\' # Open the input audio device with ossaudiodev.open(\'r\') as ad: # Set the desired audio parameters ad.setfmt(format) ad.channels(channels) ad.speed(samplerate) start_time = time.time() while time.time() - start_time < duration: data = ad.read(1024) recording += data return recording def play_audio(audio_data, format, channels, samplerate): Plays back the recorded audio. :param audio_data: The audio data to be played back. :param format: The audio format, e.g., ossaudiodev.AFMT_S16_LE. :param channels: Number of audio channels (1 for mono, 2 for stereo). :param samplerate: The audio sample rate (e.g., 44100 for CD quality). with ossaudiodev.open(\'w\') as ad: # Set the desired audio parameters ad.setfmt(format) ad.channels(channels) ad.speed(samplerate) ad.write(audio_data) if __name__ == \\"__main__\\": duration = 5 # Record for 5 seconds format = ossaudiodev.AFMT_S16_LE channels = 2 samplerate = 44100 audio_data = record_audio(duration, format, channels, samplerate) play_audio(audio_data, format, channels, samplerate)"},{"question":"**Problem Statement** You are required to create an advanced sequence manipulation class, `AdvancedSequence`, that will demonstrate your understanding of various sequence operations as outlined in the PySequence protocol documentation. This class will internally manage a list and provide several methods to manipulate this internal list using Python\'s sequence protocol functions. # Class Definition: `AdvancedSequence` Constructor: - `__init__(self, data)`: Initializes the class with a list of elements provided in the `data`. If `data` is not a list, raise a `TypeError`. Methods: 1. **Check if Sequence**: - `is_sequence(self) -> bool`: Check if the internal data provides the sequence protocol. Return `True` if it does, otherwise, `False`. 2. **Get Size**: - `size(self) -> int`: Return the number of elements in the internal list. 3. **Concatenate Sequences**: - `concat(self, other: list) -> list`: Concatenate the internal list with another list `other`. Return the new concatenated list. 4. **Repeat Sequence**: - `repeat(self, count: int) -> list`: Repeat the internal list `count` times and return the new repeated list. 5. **Get Item**: - `get_item(self, index: int) -> any`: Return the element at the specified `index` from the internal list. 6. **Set Item**: - `set_item(self, index: int, value: any)`: Set the element at the specified `index` to `value`. 7. **Get Slice**: - `get_slice(self, start: int, end: int) -> list`: Return a slice of the internal list from `start` to `end`. 8. **Set Slice**: - `set_slice(self, start: int, end: int, values: list)`: Set the slice of the internal list from `start` to `end` to the values provided in `values`. # Constraints: - Only implement the operations and error-handling explicitly mentioned. - Assume all indices and slice boundaries provided are valid. - You are **not** allowed to use built-in Python functions for these operations except for list initialization in the constructor. # Example Usage ```python seq = AdvancedSequence([1, 2, 3, 4]) print(seq.is_sequence()) # Output: True print(seq.size()) # Output: 4 print(seq.concat([5, 6])) # Output: [1, 2, 3, 4, 5, 6] print(seq.repeat(2)) # Output: [1, 2, 3, 4, 1, 2, 3, 4] print(seq.get_item(1)) # Output: 2 seq.set_item(1, 20) print(seq.get_item(1)) # Output: 20 print(seq.get_slice(1, 3)) # Output: [20, 3] seq.set_slice(1, 3, [25, 35]) print(seq.get_slice(1, 3)) # Output: [25, 35] ``` Provide your implementation of the `AdvancedSequence` class with the specified methods.","solution":"class AdvancedSequence: def __init__(self, data): if not isinstance(data, list): raise TypeError(\\"data should be a list\\") self.data = data def is_sequence(self) -> bool: return isinstance(self.data, list) def size(self) -> int: return len(self.data) def concat(self, other: list) -> list: return self.data + other def repeat(self, count: int) -> list: return self.data * count def get_item(self, index: int) -> any: return self.data[index] def set_item(self, index: int, value: any): self.data[index] = value def get_slice(self, start: int, end: int) -> list: return self.data[start:end] def set_slice(self, start: int, end: int, values: list): self.data[start:end] = values"},{"question":"**Question:** Implement a function that lists all accessible modules and submodules in a given directory path using the `pkgutil` module. Your solution should be able to: 1. List all top-level modules in the directory. 2. Recursively list submodules within each module. 3. Ensure that modules are listed in a hierarchical format, indicating the parent-child relationship. 4. Include a parameter to handle errors encountered during module imports gracefully by specifying a custom error handling function. **Function Signature:** ```python def list_modules(base_path: str, onerror: Optional[Callable[[str], None]] = None) -> Dict[str, Any]: pass ``` # Input: - `base_path`: A string representing the base directory path to look for modules. - `onerror`: An optional callable function that accepts a single argument (the name of the package which was being imported) if any exception occurs while trying to import a package. If not provided, default behavior should catch and ignore `ImportError`s. # Output: - A dictionary where the keys are the module names and the values are nested dictionaries representing the submodules (if any). # Constraints: - The function should handle errors as specified by the `onerror` parameter. - You may assume that the directory structure follows the standard Python package format. # Example: ```python from typing import Optional, Callable, Dict, Any def list_modules(base_path: str, onerror: Optional[Callable[[str], None]] = None) -> Dict[str, Any]: import pkgutil def get_submodules(package_path, package_prefix): submodules = {} for module_info in pkgutil.walk_packages([package_path], package_prefix, onerror): submodules[module_info.name] = get_submodules(package_path, module_info.name + \'.\') return submodules modules = {} for module_info in pkgutil.iter_modules([base_path]): modules[module_info.name] = get_submodules(base_path, module_info.name + \'.\') return modules # Example usage: # Print all modules and submodules starting from the base directory \\"example_package\\". print(list_modules(\\"example_package\\")) ``` This function should create a nested dictionary structure reflecting all modules and submodules found in the specified base directory. The output should correctly represent the hierarchical relationship between modules and submodules. Implement the solution considering efficiency and adherence to the `pkgutil` module\'s functionalities.","solution":"from typing import Optional, Callable, Dict, Any import pkgutil import os def list_modules(base_path: str, onerror: Optional[Callable[[str], None]] = None) -> Dict[str, Any]: Lists all accessible modules and submodules in the given directory path. :param base_path: A string representing the base directory path to look for modules. :param onerror: An optional callable function that accepts a single argument (module name) on import error. :return: A dictionary representing the modules and submodules in a hierarchical format. def get_submodules(package_path, package_prefix): submodules = {} for module_info in pkgutil.iter_modules([package_path], package_prefix): try: submodules[module_info.name] = get_submodules(os.path.join(package_path, module_info.name), module_info.name + \'.\') except ImportError: if onerror: onerror(module_info.name) else: continue return submodules modules = {} for module_info in pkgutil.iter_modules([base_path]): try: modules[module_info.name] = get_submodules(os.path.join(base_path, module_info.name), module_info.name + \'.\') except ImportError: if onerror: onerror(module_info.name) else: continue return modules"},{"question":"# Clustering Analysis with K-Means Problem Statement You are provided with a dataset of two-dimensional points. Your task is to perform clustering using the K-Means algorithm from scikit-learn\'s `clustering` module. Implement the function `perform_kmeans_clustering` which takes the dataset and the number of clusters as inputs and returns the cluster centers and the labels of each point. Function Signature ```python def perform_kmeans_clustering(data: List[List[float]], num_clusters: int) -> Tuple[List[List[float]], List[int]]: ``` Input - `data`: A list of lists, where each inner list contains two float elements representing the coordinates of a point. Example: `[[1.5, 2.3], [3.1, 4.7], [0.6, 1.9], ...]` - `num_clusters`: An integer representing the number of clusters to form. Output - A tuple containing: - `cluster_centers`: A list of lists, where each inner list contains the coordinates of a cluster center. Example: `[[2.2, 3.5], [4.1, 5.6], ...]` - `labels`: A list of integers where each integer represents the cluster label assigned to the corresponding point in the input `data`. Constraints - The length of `data` will be at least 1 and no more than 1000. - The coordinates of the points will be between -1000 and 1000. - The number of clusters will be less than or equal to the number of data points. Performance Requirements - The implementation should efficiently handle the given constraints and return the results promptly. Notes - Use the `KMeans` class from `sklearn.cluster` to perform clustering. - Ensure that you set a random state for reproducibility while initializing the `KMeans` object. Example ```python data = [[1.0, 2.0], [1.5, 1.8], [5.0, 8.0], [8.0, 8.0], [1.2, 0.8], [9.0, 11.0]] num_clusters = 2 centers, labels = perform_kmeans_clustering(data, num_clusters) print(\\"Cluster Centers:\\", centers) print(\\"Labels:\\", labels) ``` Expected Output (the exact output may vary slightly due to random initialization): ``` Cluster Centers: [[1.23333333, 1.53333333], [7.33333333, 9.0]] Labels: [0, 0, 1, 1, 0, 1] ```","solution":"from sklearn.cluster import KMeans from typing import List, Tuple def perform_kmeans_clustering(data: List[List[float]], num_clusters: int) -> Tuple[List[List[float]], List[int]]: Performs K-Means clustering on the given dataset. Parameters: data (List[List[float]]): A list of lists where each inner list represents a two-dimensional point. num_clusters (int): The number of clusters. Returns: Tuple[List[List[float]], List[int]]: A tuple containing the cluster centers and the labels of each point. kmeans = KMeans(n_clusters=num_clusters, random_state=42) kmeans.fit(data) cluster_centers = kmeans.cluster_centers_.tolist() labels = kmeans.labels_.tolist() return cluster_centers, labels"},{"question":"# Advanced JSON Encoding and Decoding Task As a Python developer, you often need to work with JSON data, sometimes involving custom object types that the `json` module doesn\'t support out of the box. Your task is to extend the functionality of the `json` module to handle complex numbers and dates. Task 1: Custom Encoder Implement a custom JSON encoder class `CustomJSONEncoder` that extends `json.JSONEncoder`. This encoder should handle: - **Complex numbers**: Serialize a complex number to a dictionary with keys `real` and `imag`. - **Dates**: Serialize `datetime.date` instances to ISO format strings (e.g., `\\"2023-10-12\\"`). Task 2: Custom Decoder Implement a custom decoder function `custom_decoder` that can handle: - **Complex numbers**: Recognize dictionaries with `real` and `imag` keys and deserialize them into complex numbers. - **Dates**: Recognize and deserialize ISO-format date strings into `datetime.date` instances. # Input and Output - **Input**: Implement two functions, `encode_custom_data` and `decode_custom_data`. - **Output**: Both functions should return a JSON string representing the encoded data (`encode_custom_data`) or the deserialized Python object (`decode_custom_data`). # Function Signatures ```python from datetime import date import json class CustomJSONEncoder(json.JSONEncoder): def default(self, obj): pass # Implement custom encoding here def custom_decoder(dct): pass # Implement custom decoding here def encode_custom_data(data): return json.dumps(data, cls=CustomJSONEncoder) def decode_custom_data(data): return json.loads(data, object_hook=custom_decoder) ``` # Constraints 1. Your encoder and decoder should correctly handle complex numbers and dates. 2. Use the following test cases to ensure your implementation works correctly: ```python from datetime import date # Test data data = { \\"name\\": \\"Alice\\", \\"complex_number\\": 2 + 3j, \\"birthdate\\": date(1990, 4, 15) } # Encode data json_data = encode_custom_data(data) print(json_data) # Should output a JSON string where \\"complex_number\\" is a dictionary with \\"real\\" and \\"imag\\" # and \\"birthdate\\" is an ISO format string # Decode data decoded_data = decode_custom_data(json_data) print(decoded_data) # Should output a Python dictionary where \\"complex_number\\" is a complex number # and \\"birthdate\\" is a datetime.date instance ``` # Evaluation Your solution will be evaluated based on: - Correctness of encoding and decoding. - Handling of edge cases and non-standard JSON content. - Readability and clarity of code. - Adherence to Python coding best practices.","solution":"from datetime import date, datetime import json class CustomJSONEncoder(json.JSONEncoder): def default(self, obj): if isinstance(obj, complex): return {\\"__complex__\\": True, \\"real\\": obj.real, \\"imag\\": obj.imag} elif isinstance(obj, date): return {\\"__date__\\": True, \\"date\\": obj.isoformat()} return super().default(obj) def custom_decoder(dct): if \\"__complex__\\" in dct: return complex(dct[\\"real\\"], dct[\\"imag\\"]) if \\"__date__\\" in dct: return datetime.fromisoformat(dct[\\"date\\"]).date() return dct def encode_custom_data(data): return json.dumps(data, cls=CustomJSONEncoder) def decode_custom_data(data): return json.loads(data, object_hook=custom_decoder)"},{"question":"Write a Python function named `analyze_imports` that takes as input the path to a Python script file. The function should use the `modulefinder` ModuleFinder class to analyze the script and return a dictionary with the following structure: - `loaded_modules`: A list of module names that were successfully imported. - `missing_modules`: A list of module names that the script attempted to import but could not be found. Input Format - A single string representing the path to the Python script file to analyze. Output Format - A dictionary with two keys: - `loaded_modules`: A list of strings representing the names of the modules that were successfully imported. - `missing_modules`: A list of strings representing the names of the modules that were not found during the import. Constraints - The Python script file provided should be a valid Python file with an appropriate extension (`.py`). - Ensure that the analysis includes both standard library modules and third-party modules. - Consider edge cases where import statements might be within `try...except` blocks. Example ```python # Given a file \'example.py\' with the following content: # import os, sys # try: # import nonexistent_module # except ImportError: # pass # analyze_imports(\'example.py\') should return: # { # \\"loaded_modules\\": [\\"os\\", \\"sys\\"], # \\"missing_modules\\": [\\"nonexistent_module\\"] # } ``` Implementation Implement the `analyze_imports` function as described. ```python from modulefinder import ModuleFinder def analyze_imports(script_path): Analyzes a Python script to determine the imported modules. Parameters: script_path (str): The path to the Python script file to analyze. Returns: dict: A dictionary with lists of loaded and missing modules. finder = ModuleFinder() finder.run_script(script_path) loaded_modules = list(finder.modules.keys()) missing_modules = list(finder.badmodules.keys()) return { \\"loaded_modules\\": loaded_modules, \\"missing_modules\\": missing_modules } ``` Use the provided `analyze_imports` function to test the analysis of different Python scripts and verify that the output matches the expected structure.","solution":"from modulefinder import ModuleFinder def analyze_imports(script_path): Analyzes a Python script to determine the imported modules. Parameters: script_path (str): The path to the Python script file to analyze. Returns: dict: A dictionary with lists of loaded and missing modules. finder = ModuleFinder() finder.run_script(script_path) loaded_modules = [mod for mod in finder.modules.keys() if mod != \\"__main__\\"] missing_modules = list(finder.badmodules.keys()) return { \\"loaded_modules\\": loaded_modules, \\"missing_modules\\": missing_modules }"},{"question":"**Objective:** Implement a Python class that mimics the old buffer protocol functions for educational purposes. This class will handle buffer allocation and provide methods to access read-only and writable memory locations. --- # Class: `BufferHandler` You are to implement a class `BufferHandler` that provides functionalities similar to the deprecated buffer protocol functions discussed in the documentation. The class should have the following methods: 1. **`__init__(self, data: bytes)`** - Initializes the `BufferHandler` with data. - The data should be stored in a way that segregates readable and writable portions, for simulating read-only and write-only buffers. 2. **`as_char_buffer(self) -> Tuple[int, str, int]`** - Returns a tuple with status code (`0` for success, `-1` for error), the buffer memory location (as a string), and the buffer length. - If the data doesn\'t support the character buffer interface, it should return `-1` and an appropriate error. 3. **`as_read_buffer(self) -> Tuple[int, bytes, int]`** - Returns a tuple with status code (`0` for success, `-1` for error), the buffer memory location (as bytes), and the buffer length. - If the data doesn\'t support the readable buffer interface, it should return `-1` and an appropriate error. 4. **`check_read_buffer(self) -> int`** - Returns `1` if the data supports the readable buffer interface, `0` otherwise. 5. **`as_write_buffer(self) -> Tuple[int, bytearray, int]`** - Returns a tuple with status code (`0` for success, `-1` for error), the buffer memory location (as bytearray), and the buffer length. - If the data doesn\'t support the writable buffer interface, it should return `-1` and an appropriate error. # Constraints: - The data provided in the constructor can be of any length. - The buffer interface under consideration is single-segment. - Proper error handling should be included for each method. - Performance can be assumed to be non-critical. --- # Example Usage: ```python # Initialize with some data handler = BufferHandler(b\'hello world\') # Read-only character buffer status, buffer, length = handler.as_char_buffer() print(status, buffer, length) # Should output: 0 \'hello world\' 11 # Read-only arbitrary buffer status, buffer, length = handler.as_read_buffer() print(status, buffer, length) # Should output: 0 b\'hello world\' 11 # Check readable buffer interface status = handler.check_read_buffer() print(status) # Should output: 1 # Writable buffer status, buffer, length = handler.as_write_buffer() print(status, buffer, length) # Should output: 0 bytearray(b\'hello world\') 11 ``` **Note:** This is a simulation task to replicate the functionality in a Pythonic way. In actual scenarios, new buffer protocol methods should be used.","solution":"from typing import Tuple class BufferHandler: def __init__(self, data: bytes): Initializes the BufferHandler with data. The data is stored as bytes. self.data = data def as_char_buffer(self) -> Tuple[int, str, int]: Returns a tuple with status code (0 for success, -1 for error), the buffer memory location (as a string), and the buffer length. try: char_buffer = self.data.decode(\'utf-8\') return (0, char_buffer, len(char_buffer)) except Exception as e: return (-1, str(e), 0) def as_read_buffer(self) -> Tuple[int, bytes, int]: Returns a tuple with status code (0 for success, -1 for error), the buffer memory location (as bytes), and the buffer length. try: return (0, self.data, len(self.data)) except Exception as e: return (-1, str(e), 0) def check_read_buffer(self) -> int: Returns 1 if the data supports the readable buffer interface, 0 otherwise. try: _ = memoryview(self.data) return 1 except Exception: return 0 def as_write_buffer(self) -> Tuple[int, bytearray, int]: Returns a tuple with status code (0 for success, -1 for error), the buffer memory location (as bytearray), and the buffer length. try: write_buffer = bytearray(self.data) return (0, write_buffer, len(write_buffer)) except Exception as e: return (-1, str(e), 0)"},{"question":"# Python Coding Assessment Question **Question Title: Implementing a Mock Module System** **Objective:** To assess the understanding of Python\'s module system, focusing on the creation, initialization, and manipulation of module objects using lower-level concepts inspired by the provided documentation. **Question:** You are tasked with implementing a simplified mock module system in Python to mimic some of the lower-level functionality as described in the documentation. Specifically, you\'ll implement a class `MockModule` and several functions to handle the creation and manipulation of these mock module objects. **Instructions:** 1. **`MockModule` Class:** - Initialize with the following attributes: - `name` (string): The name of the module. - `doc` (string): The documentation string for the module, set to `None` if not provided. - `methods` (dictionary): A dictionary to hold method names and their references. - `namespace` (dictionary): Represents the module\'s namespace, defaulting to an empty dictionary. 2. **Functions to Implement:** a. **`mock_module_new(name: str) -> MockModule`:** - Creates a new module object with the given `name`. - Sets the `__name__` attribute to `name`. b. **`mock_module_add_method(module: MockModule, method_name: str, method_ref: callable) -> None`:** - Adds a method to the module\'s `methods` dictionary. c. **`mock_module_set_doc(module: MockModule, doc: str) -> None`:** - Sets the documentation string for the module. d. **`mock_module_get_name(module: MockModule) -> str`:** - Returns the name of the module. e. **`mock_module_add_object(module: MockModule, name: str, obj: Any) -> None`:** - Adds an object to the module\'s `namespace`. 3. **Constraints:** - The `name` parameter for methods should be a non-empty string. - The `method_ref` for methods should be a callable. - The `namespace` should be a dictionary where keys are strings and values can be any type. 4. **Performance Requirements:** - Ensure that all operations (e.g., method addition, name retrieval) are efficient and have an average time complexity of O(1). 5. **Example Usage:** ```python # Create a new module mod = mock_module_new(\\"example\\") # Add a method to the module def greet(): return \\"Hello, World!\\" mock_module_add_method(mod, \\"greet\\", greet) # Set the module\'s documentation mock_module_set_doc(mod, \\"This is an example module.\\") # Add an object to the module\'s namespace mock_module_add_object(mod, \\"pi\\", 3.14159) # Retrieve and print the module\'s name print(mock_module_get_name(mod)) # Outputs: example # Call the method added to the module print(mod.methods[\\"greet\\"]()) # Outputs: Hello, World! # Access the added object\'s value print(mod.namespace[\\"pi\\"]) # Outputs: 3.14159 ``` Implement the `MockModule` class and the functions as described, ensuring that you adhere to the specified constraints and performance requirements.","solution":"class MockModule: def __init__(self, name, doc=None): self.__name__ = name self.__doc__ = doc self.methods = {} self.namespace = {} def mock_module_new(name: str) -> MockModule: Creates a new module object with the given name. return MockModule(name) def mock_module_add_method(module: MockModule, method_name: str, method_ref: callable) -> None: Adds a method to the module\'s methods dictionary. if not method_name or not callable(method_ref): raise ValueError(\\"Method name must be a non-empty string and method_ref must be callable.\\") module.methods[method_name] = method_ref def mock_module_set_doc(module: MockModule, doc: str) -> None: Sets the documentation string for the module. module.__doc__ = doc def mock_module_get_name(module: MockModule) -> str: Returns the name of the module. return module.__name__ def mock_module_add_object(module: MockModule, name: str, obj: any) -> None: Adds an object to the module\'s namespace. if not name: raise ValueError(\\"Object name must be a non-empty string.\\") module.namespace[name] = obj"},{"question":"Multithreaded Binary Data Processor You are required to write a Python program that reads a binary file containing multiple records, processes each record in a separate thread, and then writes the results to an output binary file. Each record in the input file contains specific fields packed into a binary structure. The input binary file should be read entirely into memory and then processed record by record. For this problem, assume the file consists of records with the following structure (little-endian byte order): - A 4-byte unsigned integer `record_id`. - An 8-byte double-precision floating-point number `value`. - A 2-byte unsigned short `status`. Each thread should unpack its assigned record, double the `value` if `status` is even, halve it if `status` is odd, and then re-pack the record with the new `value`. Finally, the modified records should be written to an output binary file, preserving the order of the original records. Expected Input and Output Formats **Input:** - `input_filename`: A string representing the input binary file name. - `output_filename`: A string representing the output binary file name. **Output:** - A binary file written to `output_filename` with the processed records. Constraints - The input file size will not exceed 10MB. - Processing of each record should be performed in a separate thread. - Proper synchronization should be ensured to avoid data corruption when writing to the output file. Example Assume the input binary file contains the following records: - Record 1: - `record_id`: 1 - `value`: 10.5 - `status`: 2 - Record 2: - `record_id`: 2 - `value`: 3.0 - `status`: 3 After processing, the output binary file should contain: - Record 1: - `record_id`: 1 - `value`: 21.0 (10.5 * 2 because status is even) - `status`: 2 - Record 2: - `record_id`: 2 - `value`: 1.5 (3.0 / 2 because status is odd) - `status`: 3 Implementation Implement the function `process_binary_file(input_filename: str, output_filename: str) -> None` to achieve the above requirements. ```python import struct import threading def process_binary_file(input_filename: str, output_filename: str) -> None: def process_record(record): record_id, value, status = struct.unpack(\'<I d H\', record) if status % 2 == 0: value *= 2 else: value /= 2 return struct.pack(\'<I d H\', record_id, value, status) with open(input_filename, \'rb\') as f: data = f.read() record_size = struct.calcsize(\'<I d H\') num_records = len(data) // record_size records = [data[i*record_size:(i+1)*record_size] for i in range(num_records)] processed_records = [None] * num_records def thread_target(index, record): processed_records[index] = process_record(record) threads = [] for i, record in enumerate(records): t = threading.Thread(target=thread_target, args=(i, record)) threads.append(t) t.start() for t in threads: t.join() with open(output_filename, \'wb\') as f: for record in processed_records: f.write(record) ```","solution":"import struct import threading def process_binary_file(input_filename: str, output_filename: str) -> None: def process_record(record): record_id, value, status = struct.unpack(\'<I d H\', record) if status % 2 == 0: value *= 2 else: value /= 2 return struct.pack(\'<I d H\', record_id, value, status) with open(input_filename, \'rb\') as f: data = f.read() record_size = struct.calcsize(\'<I d H\') num_records = len(data) // record_size records = [data[i*record_size:(i+1)*record_size] for i in range(num_records)] processed_records = [None] * num_records def thread_target(index, record): processed_records[index] = process_record(record) threads = [] for i, record in enumerate(records): t = threading.Thread(target=thread_target, args=(i, record)) threads.append(t) t.start() for t in threads: t.join() with open(output_filename, \'wb\') as f: for record in processed_records: f.write(record)"},{"question":"# Coding Assessment: Challenge with the Operator Module Objective Implement a function that utilizes the `operator` module to process a list of tuples. Each tuple contains an operation name (as a string) and a pair of operands. Your task is to apply the specified operations to the operands using corresponding functions from the `operator` module. Function Signature ```python def process_operations(operations: list) -> list: pass ``` Input - `operations`: A list of tuples where each tuple has the form `(operation_name: str, operand1: any, operand2: any)`. `operation_name` is a string representing the operation to be performed (e.g., \\"add\\", \\"mul\\", \\"and_\\", etc.), and `operand1` and `operand2` are the operands for the operation. Output - A list of results obtained by applying each operation from the `operations` list to the pair of operands. Constraints - Valid operations are limited to those defined in the `operator` module. - `operand1` and `operand2` can be integers, floats, or sequences (e.g., lists, strings) as applicable to the operation. - The function should handle all exceptions gracefully and return `None` for any operation that results in an error. Example ```python operations = [ (\\"add\\", 1, 2), (\\"mul\\", 4, 5), (\\"concat\\", \\"Hello \\", \\"World\\"), (\\"truediv\\", 10, 2), (\\"mod\\", 9, 2), (\\"pow\\", 3, 3) ] expected_output = [3, 20, \\"Hello World\\", 5.0, 1, 27] assert process_operations(operations) == expected_output ``` Notes - Use appropriate functions from the `operator` module to perform the operations. - Ensure the function is optimized for performance, given the constraints.","solution":"import operator def process_operations(operations: list) -> list: Process a list of operations using the operator module. Args: operations (list): A list of tuples where each tuple has the form (operation_name: str, operand1: any, operand2: any) Returns: list: A list of results obtained by applying each operation to the pair of operands, or None for any operation that results in an error. operation_map = { \'add\': operator.add, \'sub\': operator.sub, \'mul\': operator.mul, \'truediv\': operator.truediv, \'floordiv\': operator.floordiv, \'mod\': operator.mod, \'pow\': operator.pow, \'and_\': operator.and_, \'or_\': operator.or_, \'xor\': operator.xor, \'concat\': operator.concat, \'contains\': operator.contains, \'countOf\': operator.countOf, \'eq\': operator.eq, \'gt\': operator.gt, \'lt\': operator.lt, \'le\': operator.le, \'ge\': operator.ge, \'ne\': operator.ne } results = [] for op_name, operand1, operand2 in operations: func = operation_map.get(op_name) if func: try: result = func(operand1, operand2) except Exception: result = None else: result = None results.append(result) return results"},{"question":"# Question: Implement a Simple NNTP Client You are required to implement a simple NNTP client that will perform the following tasks: 1. Connect to the specified NNTP server. 2. Retrieve the list of newsgroups. 3. Fetch the list of articles in a specified newsgroup. 4. Display the subject of each article in the newsgroup. 5. Handle any exceptions that occur during the process. Instructions: 1. **Function Definition**: ```python def fetch_news(server: str, group_name: str, article_count: int) -> None: Fetch and display the list of articles from the specified newsgroup on the NNTP server. Parameters: - server (str): The hostname of the NNTP server. - group_name (str): The name of the newsgroup to fetch articles from. - article_count (int): The number of recent articles to fetch. Returns: - None ``` 2. The function should perform the following steps: - Connect to the NNTP server specified by the `server` parameter. - Switch to the newsgroup specified by the `group_name` parameter. - Retrieve the specified number of recent articles (`article_count`) from the newsgroup. - Display the subject of each retrieved article. - Ensure that the connection to the NNTP server is closed properly. 3. **Example**: ```python fetch_news(\'news.gmane.io\', \'gmane.comp.python.committers\', 5) ``` The output should be similar to: ``` Connecting to server: news.gmane.io Switched to newsgroup: gmane.comp.python.committers Article 1: Re: Commit privileges for Łukasz Langa Article 2: Re: 3.2 alpha 2 freeze Article 3: Re: 3.2 alpha 2 freeze Article 4: Re: Commit privileges for Łukasz Langa Article 5: Updated ssh key Connection closed. ``` 4. **Error Handling**: - Handle connection errors, authentication errors, and other NNTP protocol errors. - Print appropriate error messages if any exceptions occur. Constraints: - You may assume that the server, newsgroup, and the number of articles specified are valid. - Use the `nntplib` module for all NNTP interactions. - Keep the code clean and ensure that all connections are properly closed. Notes: - Pay attention to decoding article subjects correctly using the `decode_header` utility from the `nntplib` module. - Before running your function, ensure you have network access to the specified NNTP server and the newsgroup exists on that server. Good luck!","solution":"import nntplib from email import message_from_bytes from email.header import decode_header def fetch_news(server: str, group_name: str, article_count: int) -> None: Fetch and display the list of articles from the specified newsgroup on the NNTP server. Parameters: - server (str): The hostname of the NNTP server. - group_name (str): The name of the newsgroup to fetch articles from. - article_count (int): The number of recent articles to fetch. Returns: - None try: print(f\\"Connecting to server: {server}\\") with nntplib.NNTP(server) as client: resp, count, first, last, name = client.group(group_name) print(f\\"Switched to newsgroup: {name}\\") start = int(last) - article_count + 1 if start < int(first): start = int(first) for article_number in range(start, int(last)+1): resp, info = client.head(article_number) headers = message_from_bytes(b\'n\'.join(info.lines)) subject, encoding = decode_header(headers[\'subject\'])[0] if isinstance(subject, bytes): if encoding: subject = subject.decode(encoding) else: subject = subject.decode(\'utf-8\') print(f\\"Article {article_number - start + 1}: {subject}\\") except Exception as e: print(f\\"An error occurred: {e}\\") # Example usage: # fetch_news(\'news.gmane.io\', \'gmane.comp.python.committers\', 5)"},{"question":"**Objective:** Your task is to write a Python script that fetches and processes data from the web using the `urllib.request` module. The script should handle potential errors gracefully and customize the request with specific headers and data based on user input. **Scenario:** You are developing a Python script that will interact with a weather API to fetch weather information for a specified city. The API you are using requires a POST request with JSON payload for the city name and returns weather data in JSON format. Additionally, the API requires a custom User-Agent header to be set. **Requirements:** 1. Fetch weather data using a POST request with the city name. 2. Customize the request with a User-Agent header. 3. Handle potential exceptions and provide appropriate error messages. 4. Process the JSON response to extract and display specific weather details in a user-friendly format. **Input:** - The city name as a string (e.g., \\"New York\\"). **Output:** - Print the weather details in a readable format (e.g., temperature, weather condition). **Constraints:** - The URL of the weather API is `http://example.com/weather`. - The User-Agent header should be `WeatherClient/1.0`. - Ensure the script handles common HTTP errors (e.g., 404, 403) as well as connection errors gracefully. # Function Signature ```python def fetch_weather(city: str) -> None: pass ``` # Example Usage ```python fetch_weather(\\"New York\\") ``` # Expected Output ```plaintext Weather in New York: - Temperature: 15°C - Condition: Clear skies ``` # Implementation Hints - Use `urllib.request` to create and send the request. - Encode the city name in JSON format and set it as the request data. - Set the appropriate headers for the request. - Use `try` and `except` blocks to handle exceptions. - Use the `json` module to parse the JSON response. **Note:** The actual URL for the weather API and its response structure might differ. For this exercise, you can assume the response is a JSON object containing `temperature` and `condition` fields with appropriate values.","solution":"import json import urllib.request import urllib.error def fetch_weather(city: str) -> None: url = \'http://example.com/weather\' headers = { \'User-Agent\': \'WeatherClient/1.0\', \'Content-Type\': \'application/json\' } data = json.dumps({\\"city\\": city}).encode(\'utf-8\') request = urllib.request.Request(url, data=data, headers=headers, method=\'POST\') try: with urllib.request.urlopen(request) as response: if response.status == 200: response_data = response.read().decode(\'utf-8\') weather_info = json.loads(response_data) print(f\\"Weather in {city}:\\") print(f\\"- Temperature: {weather_info.get(\'temperature\')}°C\\") print(f\\"- Condition: {weather_info.get(\'condition\')}\\") else: print(f\\"Error: Received status code {response.status}\\") except urllib.error.HTTPError as e: print(f\\"HTTP Error: {e.code} - {e.reason}\\") except urllib.error.URLError as e: print(f\\"URL Error: {e.reason}\\") except Exception as e: print(f\\"Unexpected Error: {str(e)}\\")"},{"question":"**Question**: You are required to generate and visualize several types of datasets using scikit-learn\'s dataset generation functions. Your task involves creating datasets for classification, clustering, and regression, and visualizing them using matplotlib. Additionally, you need to perform some data manipulation and analysis on the generated datasets. Please follow the detailed instructions below. # Instructions: 1. **Dataset Generation**: - Generate a three-cluster dataset using `make_blobs` with `centers=3`, `cluster_std=0.5`, and `random_state=0`. - Generate a multiclass classification dataset using `make_classification` with `n_features=2`, `n_redundant=0`, `n_informative=2`, `n_clusters_per_class=1`, `n_classes=3`, and `random_state=1`. - Generate a regression dataset using `make_regression` with `n_features=2`, `noise=0.1`, and `random_state=0`. 2. **Dataset Visualization**: - Plot a scatter plot for the three-cluster dataset generated by `make_blobs`, coloring the points by their cluster label. - Plot a scatter plot for the classification dataset generated by `make_classification`, coloring the points by their class label. - Plot a scatter plot for the regression dataset generated by `make_regression`, using the target variable to color the points. 3. **Data Manipulation and Analysis**: - For the three-cluster dataset, calculate and print the mean of the coordinates for each cluster. - For the classification dataset, calculate and print the proportion of each class in the dataset. - For the regression dataset, calculate and print the mean and standard deviation of the target variable. # Expectations: - Your solution should include code to generate the datasets. - Your solution should include appropriate visualizations for each dataset. - Your solution should include code to perform the requested data manipulations and analysis, with the results printed in a clear format. - Your code should be well-documented with comments explaining each step. # Submission: - Submit a single Jupyter Notebook (.ipynb) file containing all code, visualizations, and printed results. - Ensure your notebook is clean and organized, and that all code cells run without errors. # Example Code Snippet: ```python import matplotlib.pyplot as plt from sklearn.datasets import make_blobs, make_classification, make_regression import numpy as np # Example for generating and plotting make_blobs dataset X_blobs, y_blobs = make_blobs(centers=3, cluster_std=0.5, random_state=0) plt.scatter(X_blobs[:, 0], X_blobs[:, 1], c=y_blobs) plt.title(\\"Three normally-distributed clusters\\") plt.show() # Use similar approach for the other parts of the question ```","solution":"import matplotlib.pyplot as plt from sklearn.datasets import make_blobs, make_classification, make_regression import numpy as np def generate_datasets(): # Generate datasets as specified in the task blobs = make_blobs(n_samples=300, centers=3, cluster_std=0.5, random_state=0) classification = make_classification(n_samples=300, n_features=2, n_redundant=0, n_informative=2, n_clusters_per_class=1, n_classes=3, random_state=1) regression = make_regression(n_samples=300, n_features=2, noise=0.1, random_state=0) return blobs, classification, regression def plot_datasets(blobs, classification, regression): # Plot the blobs dataset plt.figure(figsize=(15, 5)) plt.subplot(1, 3, 1) plt.scatter(blobs[0][:, 0], blobs[0][:, 1], c=blobs[1], cmap=\'viridis\') plt.title(\\"Three-cluster dataset (make_blobs)\\") # Plot the classification dataset plt.subplot(1, 3, 2) plt.scatter(classification[0][:, 0], classification[0][:, 1], c=classification[1], cmap=\'viridis\') plt.title(\\"Multiclass classification dataset (make_classification)\\") # Plot the regression dataset plt.subplot(1, 3, 3) plt.scatter(regression[0][:, 0], regression[0][:, 1], c=regression[1], cmap=\'viridis\') plt.title(\\"Regression dataset (make_regression)\\") plt.show() def data_analysis(blobs, classification, regression): # Analysis for three-cluster dataset clusters_mean = [] for i in range(3): mean = np.mean(blobs[0][blobs[1] == i], axis=0) clusters_mean.append(mean) print(f\\"Mean coordinates for cluster {i}: {mean}\\") # Analysis for classification dataset class_counts = np.bincount(classification[1]) class_proportions = class_counts / len(classification[1]) for i, proportion in enumerate(class_proportions): print(f\\"Proportion of class {i}: {proportion}\\") # Analysis for regression dataset target_mean = np.mean(regression[1]) target_std = np.std(regression[1]) print(f\\"Mean of target variable: {target_mean}\\") print(f\\"Standard deviation of target variable: {target_std}\\") if __name__ == \\"__main__\\": blobs, classification, regression = generate_datasets() plot_datasets(blobs, classification, regression) data_analysis(blobs, classification, regression)"},{"question":"**Objective:** This question aims to test your understanding of the Seaborn package, specifically the `objects` interface, custom aggregation functions, and visual customizations in plots. **Task:** You are provided with the `diamonds` dataset from Seaborn\'s built-in datasets. Your task is to create a bar plot that displays the interquartile range (IQR) of the `price` of diamonds for each `cut` category, further separated by the `clarity` of the diamonds. Follow these steps to accomplish the task: 1. **Load the Dataset:** Using `seaborn.load_dataset`, load the `diamonds` dataset. 2. **Create the Plot:** - Initialize a `Plot` object using `seaborn.objects` (`so.Plot`), specifying the dataset as `diamonds`, the x-axis as `cut`, and the y-axis as `price`. - Add a `Bar` layer to the plot representing the interquartile range (IQR) of the `price` for each `cut` category. In other words, the height of each bar should be the IQR of prices for that `cut`. - Use a custom aggregation function to calculate the IQR: `lambda x: x.quantile(0.75) - x.quantile(0.25)`. 3. **Visual Customization:** - Adjust the plot to differentiate bars by `clarity` using the `Dodge` transform. - Assign different colors to the bars based on the `clarity` variable. **Expected Input and Output:** **Input:** No input; simply run the script to see the output plot. **Output:** - A visually appealing bar plot with x-axis as `cut` categories and y-axis representing the IQR of `price`. - Each bar should be dodged and colored according to the `clarity` of the diamonds. **Constraints:** - Use only the `seaborn` and built-in Python libraries. **Skeleton Code:** ```python import seaborn.objects as so from seaborn import load_dataset # 1. Load the dataset diamonds = load_dataset(\\"diamonds\\") # 2. Create the plot p = so.Plot(diamonds, x=\\"cut\\", y=\\"price\\") # 3. Add a Bar layer with IQR aggregation and customize the plot p.add(so.Bar(), so.Agg(lambda x: x.quantile(0.75) - x.quantile(0.25)), so.Dodge(), color=\\"clarity\\") # Display the plot p.show() ``` Submit your code along with the resulting plot image.","solution":"import seaborn.objects as so from seaborn import load_dataset def create_diamond_iqr_plot(): # 1. Load the dataset diamonds = load_dataset(\\"diamonds\\") # 2. Create the plot p = so.Plot(diamonds, x=\\"cut\\", y=\\"price\\") # 3. Add a Bar layer with IQR aggregation and customize the plot p.add(so.Bar(), so.Agg(lambda x: x.quantile(0.75) - x.quantile(0.25)), so.Dodge(), color=\\"clarity\\") # Display the plot return p.show()"},{"question":"# Python Coding Assessment: Site-specific Path Management and Configuration Objective Implement a function that customizes the Python environment by managing site-specific and user-specific paths. Your implementation should demonstrate the use of the `site` module\'s functionality, particularly focusing on adding directories, retrieving site-specific paths, and handling user site-packages. Problem Statement You are given a list of directories that must be added to the Python module search path (`sys.path`). These directories might be user-specific or global site-specific directories. You must also ensure any user-specific site-packages are included and appropriately enabled. Additionally, implement functionality to list all currently configured site-package directories. Function Signature ```python def customize_python_paths(directories: list[str]) -> list[str]: Customize Python\'s module search paths by adding the given directories and ensure both site-specific and user-specific site-package directories are included. Parameters: directories (list[str]): A list of directory paths to be added to sys.path. Returns: list[str]: A list of all current site-package directories. pass ``` Input - `directories`: A list of strings where each string is a full path to a directory that requires addition to the `sys.path`. Output - The function should return a list of paths representing all current site-package directories (both global and user-specific). Constraints - Directories in the `directories` list should be added to the `sys.path` only if they exist. - The function should not add duplicate directories to `sys.path`. - Handle potential errors gracefully (e.g., if a directory cannot be added, ignore and continue). Example ```python # Example usage: dirs_to_add = [\'/path/to/custom/dir1\', \'/path/to/custom/dir2\'] all_site_packages = customize_python_paths(dirs_to_add) print(all_site_packages) # Output might look like (actual paths will depend on the system and setup): # [\'/usr/local/lib/python3.10/site-packages\', \'/home/user/.local/lib/python3.10/site-packages\', \'/path/to/custom/dir1\', \'/path/to/custom/dir2\'] ``` Notes - Utilize `site.addsitedir` to add directories to `sys.path`. - Make use of `site.getsitepackages` and `site.getusersitepackages` to retrieve the paths of current site-packages directories. - Ensure that user-specific site-packages are enabled via `site.ENABLE_USER_SITE`. Good luck!","solution":"import os import sys import site def customize_python_paths(directories: list[str]) -> list[str]: Customize Python\'s module search paths by adding the given directories and ensure both site-specific and user-specific site-package directories are included. Parameters: directories (list[str]): A list of directory paths to be added to sys.path. Returns: list[str]: A list of all current site-package directories. # Add the given directories to sys.path for directory in directories: if os.path.exists(directory) and directory not in sys.path: site.addsitedir(directory) # Get the site-package directories site_packages = site.getsitepackages() # Include user-specific site-package directory if enabled if site.ENABLE_USER_SITE: user_site = site.getusersitepackages() if user_site not in site_packages: site_packages.append(user_site) # Return the site-package directories return site_packages"},{"question":"**Objective:** Write a function `bit_count_pairs` that accepts a list of integers and returns a list of tuples. Each tuple should contain an integer from the list and its corresponding bit count. You are tasked to utilize the `int.bit_count()` method introduced in Python 3.10. This will test your understanding of iterating over collections, calling methods on integers, and returning structured data. **Function Signature:** ```python def bit_count_pairs(int_list: list[int]) -> list[tuple[int, int]]: pass ``` # Input - `int_list` (list of int): A list of integers. The list will contain at least one element and will only include non-negative integers. The list can have up to 10^5 elements, each ranging between 0 and 10^9. # Output - Returns a list of tuples. Each tuple contains an integer from the input list and its corresponding bit count. # Constraints - You must utilize the `int.bit_count()` method for calculating the bit counts. - Your function should be able to handle the maximum input size efficiently. # Example ```python # Example 1 input_list = [5, 3, 15] # 5 in binary is 101, which has 2 ones # 3 in binary is 11, which has 2 ones # 15 in binary is 1111, which has 4 ones expected_output = [(5, 2), (3, 2), (15, 4)] assert bit_count_pairs(input_list) == expected_output # Example 2 input_list = [0, 1, 1023] # 0 in binary is 0, which has 0 ones # 1 in binary is 1, which has 1 one # 1023 in binary is 1111111111, which has 10 ones expected_output = [(0, 0), (1, 1), (1023, 10)] assert bit_count_pairs(input_list) == expected_output ``` # Notes - Focus on efficiently implementing the function to manage the upper limit constraint. - Ensure your function is correctly utilizing Python\'s built-in `int.bit_count()` method.","solution":"def bit_count_pairs(int_list: list[int]) -> list[tuple[int, int]]: Returns a list of tuples, each containing an integer and its corresponding bit count. return [(num, num.bit_count()) for num in int_list]"},{"question":"In this task, you are required to implement a custom metric function for evaluating a machine learning model using scikit-learn. The custom metric will be an adaptation of the provided mean absolute percentage error (MAPE) but will include a log transformation to handle a wide range of target values more effectively. **Problem Statement:** You are given a dataset where you need to build a regression model and evaluate it using a custom metric termed as \\"Mean Log-Transformed Absolute Percentage Error\\" (MLAPE) which is defined as: MLAPE(y, hat{y}) = frac{1}{n_{text{samples}}} sum_{i=0}^{n_{text{samples}}-1} frac{left| log(1 + y_i) - log(1 + hat{y}_i) right|}{max(epsilon, left| log(1 + y_i) right|)} where ( y ) represents the true values, ( hat{y} ) represents the predicted values, and ( epsilon ) is a small constant to avoid division by zero, taken as ( 10^{-10} ). # Requirements: 1. Implement the custom metric `mean_log_transformed_absolute_percentage_error` function as described. 2. Use this custom metric in a model evaluation scenario with a regression model from the scikit-learn library. 3. Evaluate the model using k-fold cross-validation and calculate the average MLAPE for the folds. # Input: - A dataset with feature variables and target variable. # Output: - Average MLAPE value across the k-folds. # Constraints: - Use scikit-learn for model building and evaluation. - The dataset can be any classic regression dataset (e.g., housing prices, diabetes dataset, etc.). - K-Folds cross-validation, where k = 5. # Function Signature: ```python from sklearn.metrics import make_scorer import numpy as np def mean_log_transformed_absolute_percentage_error(y_true, y_pred): Compute the Mean Log-Transformed Absolute Percentage Error (MLAPE). Parameters: y_true (array-like): True values. y_pred (array-like): Predicted values. Returns: float: MLAPE score. epsilon = 1e-10 log_y_true = np.log1p(y_true) log_y_pred = np.log1p(y_pred) return np.mean(np.abs(log_y_true - log_y_pred) / np.maximum(epsilon, np.abs(log_y_true))) def evaluate_model_with_custom_metric(X, y): Evaluate the regression model using MLAPE custom metric. Parameters: X (array-like): Feature variables. y (array-like): Target variable. Returns: float: Average MLAPE score from k-fold cross-validation. from sklearn.linear_model import LinearRegression from sklearn.model_selection import cross_val_score, KFold # Define the custom scorer mlape_scorer = make_scorer(mean_log_transformed_absolute_percentage_error, greater_is_better=False) # Initialize the regression model model = LinearRegression() # Perform k-fold cross-validation kf = KFold(n_splits=5, shuffle=True, random_state=42) scores = cross_val_score(model, X, y, cv=kf, scoring=mlape_scorer) # Calculate and return the average score return np.mean(scores) # Example usage: # X, y = load your dataset # average_mlape = evaluate_model_with_custom_metric(X, y) # print(\\"Average MLAPE across k-folds:\\", average_mlape) ``` # Notes: - Make sure that the `mean_log_transformed_absolute_percentage_error` function is thoroughly tested. - The actual dataset should be loaded and passed to the `evaluate_model_with_custom_metric` function in the example usage. - You can use any appropriate regression dataset for demonstration. # Evaluation Criteria: - Correct implementation of the custom metric function. - Proper usage of scikit-learn’s `make_scorer` and `cross_val_score`. - Accurate calculation of average MLAPE across folds.","solution":"import numpy as np from sklearn.metrics import make_scorer def mean_log_transformed_absolute_percentage_error(y_true, y_pred): Compute the Mean Log-Transformed Absolute Percentage Error (MLAPE). Parameters: y_true (array-like): True values. y_pred (array-like): Predicted values. Returns: float: MLAPE score. epsilon = 1e-10 log_y_true = np.log1p(y_true) log_y_pred = np.log1p(y_pred) return np.mean(np.abs(log_y_true - log_y_pred) / np.maximum(epsilon, np.abs(log_y_true))) def evaluate_model_with_custom_metric(X, y): Evaluate the regression model using MLAPE custom metric. Parameters: X (array-like): Feature variables. y (array-like): Target variable. Returns: float: Average MLAPE score from k-fold cross-validation. from sklearn.linear_model import LinearRegression from sklearn.model_selection import cross_val_score, KFold # Define the custom scorer mlape_scorer = make_scorer(mean_log_transformed_absolute_percentage_error, greater_is_better=False) # Initialize the regression model model = LinearRegression() # Perform k-fold cross-validation kf = KFold(n_splits=5, shuffle=True, random_state=42) scores = cross_val_score(model, X, y, cv=kf, scoring=mlape_scorer) # Calculate and return the average score return np.mean(scores)"},{"question":"**Objective:** Write a Python function that uses the `compileall` module to compile Python source files within a given directory. Your function should provide options to control recursion depth, verbosity, output location, and file exclusion based on regex patterns. # Function Signature ```python def advanced_compile_dir(dir: str, maxlevels: int = None, quiet: int = 0, ddir: str = None, rx_pattern: str = None) -> bool: Compile all Python source files in the specified directory. :param dir: The directory containing Python files to compile. :param maxlevels: The maximum depth for recursion. Defaults to full recursion (`sys.getrecursionlimit()`). :param quiet: Verbosity level. 0 for normal output, 1 for errors only, 2 for no output. :param ddir: Directory to prepend to the path of each file for tracebacks and bytecode files. :param rx_pattern: Regex pattern to exclude certain files from compilation. If None, compile all files. :return: True if all files compile successfully, otherwise False. # Input - `dir` (str): The directory to traverse and compile Python source files from. - `maxlevels` (int, optional): The maximum recursion depth for traversing subdirectories. - `quiet` (int, optional): Verbosity level (0 for normal output, 1 for errors only, 2 for no output). - `ddir` (str, optional): Directory to include in traceback and bytecode files. - `rx_pattern` (str, optional): Regex pattern to exclude specific files. # Output - bool: A boolean indicating whether all files were compiled successfully. # Constraints - The function should use the `compileall.compile_dir` method. - If `rx_pattern` is provided, it should be used to exclude files matching the pattern. - If no `maxlevels` is provided, full recursion (using `sys.getrecursionlimit()`) should be used. - The function should handle invalid inputs gracefully, throwing appropriate exceptions. # Example ```python # Compile all `.py` files in \'my_project\', with verbose output, excluding any files in \'tests\' subdirectory and limiting to 2 levels deep. success = advanced_compile_dir(\'my_project\', maxlevels=2, quiet=0, rx_pattern=\'tests\') print(success) # Should print: True if all compile successfully, otherwise False ``` This question requires students to interact with the filesystem, handle regular expressions, and manage byte-code compilation while providing a comprehensive understanding of advanced usage of the `compileall` module.","solution":"import compileall import re import sys def advanced_compile_dir(dir: str, maxlevels: int = None, quiet: int = 0, ddir: str = None, rx_pattern: str = None) -> bool: Compile all Python source files in the specified directory. :param dir: The directory containing Python files to compile. :param maxlevels: The maximum depth for recursion. Defaults to full recursion (`sys.getrecursionlimit()`). :param quiet: Verbosity level. 0 for normal output, 1 for errors only, 2 for no output. :param ddir: Directory to prepend to the path of each file for tracebacks and bytecode files. :param rx_pattern: Regex pattern to exclude certain files from compilation. If None, compile all files. :return: True if all files compile successfully, otherwise False. # Convert the regex pattern to a compiled regex object, if any. rx = re.compile(rx_pattern) if rx_pattern else None def _exclude_file(file_path): A helper function to decide whether a file should be excluded. if rx and re.search(rx, file_path): return True return False # Compile the directory while taking the exclusion into account. return compileall.compile_dir( dir, maxlevels=maxlevels if maxlevels is not None else sys.getrecursionlimit(), quiet=quiet, rx=rx if rx else None, ddir=ddir )"},{"question":"# PyTorch Coding Assessment As a part of understanding PyTorch and its subset TorchScript, you are required to write a function that performs specific tensor operations. The function will accumulate values under certain conditions and return the result. Problem Statement You are given a 2D tensor `input_tensor` with dimensions `(N, M)`. Implement a function `process_tensor` that performs the following operations: 1. For a threshold value `T`, compute a new tensor `output_tensor` such that: - Each element in `output_tensor` is 1 if the corresponding element in `input_tensor` is greater than `T`, else 0. 2. Find the sum of all elements in each column of `output_tensor`. Function Signature: ```python import torch def process_tensor(input_tensor, T): Args: - input_tensor (torch.Tensor): a 2D tensor of shape (N, M) - T (float): a threshold value Returns: - torch.Tensor: a 1D tensor of shape (M,) where each element is the sum of the corresponding column of `output_tensor` pass ``` Input: - You can assume `input_tensor` is a torch.Tensor object of type float with dimensions `(N, M)` where `1 <= N, M <= 1000`. - The threshold `T` will be a floating-point number. Output: - The function should return a 1D tensor of shape (M,) containing the sum of each column of `output_tensor`. Example: ```python input_tensor = torch.tensor([[0.5, 1.2, 3.1], [2.3, 0.4, 1.5], [3.4, 2.1, 0.9]]) T = 1.0 output = process_tensor(input_tensor, T) print(output) # tensor([1, 1, 2]) ``` Constraints: - Ensure the function is efficient and performs well with the maximum constraint values. - Avoid using unsupported features in TorchScript. Hints: - You may find `torch.gt` useful for creating the `output_tensor`. - Use tensor reduction operations to compute column sums. Good luck and make sure your code is clear and well documented!","solution":"import torch def process_tensor(input_tensor, T): Args: - input_tensor (torch.Tensor): a 2D tensor of shape (N, M) - T (float): a threshold value Returns: - torch.Tensor: a 1D tensor of shape (M,) where each element is the sum of the corresponding column of `output_tensor` # Generate output_tensor by comparing each element with the threshold T output_tensor = (input_tensor > T).int() # Sum the values in each column of output_tensor result = torch.sum(output_tensor, dim=0) return result"},{"question":"Objective Demonstrate your understanding of the `torch.mps` module by implementing a function that profiles memory usage and device synchronization on an MPS device using a given computation graph. Problem Statement Write a function `profile_mps_computation` that: 1. Ensures there is at least one MPS device available. 2. Seeds the random number generator. 3. Allocates memory for a large tensor on an MPS device. 4. Profiles a computation performed on that tensor. 5. Synchronizes the MPS device. 6. Returns a report on memory usage before and after the computation, including the peak memory usage during the profiling. Function Signature ```python def profile_mps_computation(tensor_size: int, seed_value: int) -> str: pass ``` Parameters - `tensor_size` (int): The size of the tensor to be allocated. - `seed_value` (int): The seed value for the random number generator. Returns - `str`: A formatted string report containing the memory usage before and after the computation and the peak memory usage during profiling. Constraints - You may use only the functions from the `torch.mps` module for device management, memory management, and profiling. - The allocated tensor should be a `torch.FloatTensor` filled with random values. Example Usage ```python report = profile_mps_computation(1000000, 42) print(report) ``` Example Report ``` Memory Usage Report: Before Computation: 100 MB After Computation: 150 MB Peak Memory Usage During Profiling: 200 MB ``` Notes - Handle any exceptions that may arise due to the absence of MPS devices. - Ensure proper cleanup and memory management using `empty_cache`. Good luck!","solution":"import torch import os def profile_mps_computation(tensor_size: int, seed_value: int) -> str: # Ensure there is at least one MPS device available. if not torch.backends.mps.is_available(): return \\"MPS device not available.\\" # Seed the random number generator. torch.manual_seed(seed_value) # Allocate memory for a large tensor on an MPS device. tensor = torch.randn(tensor_size, device=\'mps\') # Capture memory usage before computation before_memory = torch.mps.memory_allocated() # Profile a computation performed on that tensor. # Here, we\'ll just perform a basic multiplication result = tensor * 2 # Synchronize the MPS device. torch.mps.synchronize() # Capture memory usage after computation after_memory = torch.mps.memory_allocated() peak_memory = torch.mps.max_memory_allocated() # Clear memory del tensor, result torch.mps.empty_cache() # Format and return the report before_memory_mb = before_memory / (1024 * 1024) after_memory_mb = after_memory / (1024 * 1024) peak_memory_mb = peak_memory / (1024 * 1024) report = ( f\\"Memory Usage Report:n\\" f\\"Before Computation: {before_memory_mb:.2f} MBn\\" f\\"After Computation: {after_memory_mb:.2f} MBn\\" f\\"Peak Memory Usage During Profiling: {peak_memory_mb:.2f} MB\\" ) return report"},{"question":"Objective: Create a class `AsyncListProcessor` that can process a list of numbers asynchronously, applying various transformations using Python\'s built-in functions and classes. You should utilize some of the built-in functions from the provided documentation, including but not necessarily limited to `aiter()`, `anext()`, `all()`, `any()`, `enumerate()`, `filter()`, `map()`, `sum()`, and `range()`. Requirements: 1. **Class Initialization**: - The class should be initialized with a list of numbers. - For example: `AsyncListProcessor([1, 2, 3, 4, 5])` 2. **Methods**: - `is_all_positive`: - Returns `True` if all elements in the list are positive integers. - `contains_any_negative`: - Returns `True` if any element in the list is negative. - `double_values`: - Uses `map()` to return a new list where each element is doubled. - `filter_even`: - Uses `filter()` to return a list containing only the even numbers. - `sum_n`: - Sums up the first `n` numbers in the list. This method should be asynchronous and use `aiter()` and `anext()`. 3. **Iterator**: - Implement the asynchronous iterator protocol (`__aiter__` and `__anext__`) to iterate through the elements of the list. 4. **Async Sum Calculation**: - As part of the `sum_n` method, the class should demonstrate the use of an asynchronous iteration mechanism to sum the elements. Constraints: - The list of numbers will contain at most 1000 elements. - The list elements will be integers within the range ([-1000, 1000]). Example Usage: ```python import asyncio class AsyncListProcessor: # Implement the class here # Example Usage async def main(): processor = AsyncListProcessor([1, 2, 3, 4, 5]) print(processor.is_all_positive()) # True print(processor.contains_any_negative()) # False print(processor.double_values()) # [2, 4, 6, 8, 10] print(processor.filter_even()) # [2, 4] n_sum = await processor.sum_n(3) print(n_sum) # 6 (1 + 2 + 3) async for value in processor: print(value) asyncio.run(main()) ``` Notes: - Ensure proper use of asynchronous features with relevant built-in functions. - Provide appropriate error handling where necessary. - Maintain clear and concise code structure.","solution":"class AsyncListProcessor: def __init__(self, numbers): Initialize with a list of numbers. self.numbers = numbers def is_all_positive(self): Returns True if all elements in the list are positive integers. return all(num > 0 for num in self.numbers) def contains_any_negative(self): Returns True if any element in the list is negative. return any(num < 0 for num in self.numbers) def double_values(self): Uses map() to return a new list where each element is doubled. return list(map(lambda x: x * 2, self.numbers)) def filter_even(self): Uses filter() to return a list containing only the even numbers. return list(filter(lambda x: x % 2 == 0, self.numbers)) async def sum_n(self, n): Sums up the first n numbers in the list asynchronously. it = aiter(self.numbers) total = 0 for _ in range(n): total += await anext(it) return total def __aiter__(self): Asynchronous iterator initialization. self._iter = iter(self.numbers) return self async def __anext__(self): Asynchronously get the next item in the iterator. try: return next(self._iter) except StopIteration: raise StopAsyncIteration"},{"question":"# Objective This task aims to assess your comprehension of the `seaborn` package, specifically the `boxenplot` function, through data visualization techniques. # Problem Statement You are provided with the `diamonds` dataset from the `seaborn` library. Your task is to create several visualizations using the `seaborn.boxenplot` function to explore the distribution of diamond prices and how they vary by different categorical variables. # Instructions 1. **Initial Setup**: Import the necessary libraries and set the theme for seaborn. ```python import seaborn as sns sns.set_theme(style=\\"whitegrid\\") diamonds = sns.load_dataset(\\"diamonds\\") ``` 2. **Task 1**: Create a horizontal boxen plot of the `price` variable. - The x-axis should represent the `price` of the diamonds. 3. **Task 2**: Create a boxen plot showing the distribution of `price` grouped by `clarity`. - The x-axis should represent `price`. - The y-axis should represent `clarity`. 4. **Task 3**: Create a boxen plot of `price` grouped by `clarity` and differentiated by whether the `carat` value is greater than 1. - The x-axis should represent `price`. - The y-axis should represent `clarity`. - Use the `hue` parameter to differentiate diamonds based on whether `carat` > 1. - Add a small gap between the groups. 5. **Task 4**: Create a boxen plot of `price` grouped by `clarity`, using a linear width method. - The x-axis should represent `price`. - The y-axis should represent `clarity`. - Use `width_method=\\"linear\\"` to adjust the width of the boxes. 6. **Task 5**: Customize the boxen plot from Task 4 by: - Setting the width of the largest box to 0.5. - Adding additional customizations to control the line color (`.7`), line width (`.5`), and adjusting the appearance of outliers. 7. **Task 6**: Create an unfilled boxen plot of `price` grouped by `clarity` and differentiated by `clarity`. - The x-axis should represent `price`. - The y-axis should represent `clarity`. - Use the `hue=\\"clarity\\"` parameter. - Ensure the boxes are unfilled. # Submission Submit a single Python script containing the code to generate all the required plots as described in the tasks above. Ensure your script is well-commented and follows good coding practices. # Evaluation Criteria - Correctness: The plots should be correctly generated as per the instructions. - Usage of seaborn: Proper use of seaborn functionalities and parameters. - Code Quality: Clean, well-commented, and efficient code. - Visualization: The plots should be clearly labeled and aesthetically pleasing.","solution":"import seaborn as sns import matplotlib.pyplot as plt sns.set_theme(style=\\"whitegrid\\") diamonds = sns.load_dataset(\\"diamonds\\") # Task 1: Horizontal Boxen Plot of Price def plot_horizontal_boxenplot(data): sns.boxenplot(x=\'price\', data=data) plt.title(\\"Horizontal Boxen Plot of Diamond Prices\\") plt.show() # Task 2: Boxen Plot of Price Grouped by Clarity def plot_boxenplot_by_clarity(data): sns.boxenplot(x=\'price\', y=\'clarity\', data=data) plt.title(\\"Boxen Plot of Diamond Prices by Clarity\\") plt.show() # Task 3: Boxen Plot of Price by Clarity and Differentiated by Carat > 1 def plot_boxenplot_clarity_carat(data): data[\'carat_gt_1\'] = data[\'carat\'] > 1 sns.boxenplot(x=\'price\', y=\'clarity\', hue=\'carat_gt_1\', data=data, dodge=True) plt.title(\\"Boxen Plot of Diamond Prices by Clarity (Carat > 1 or not)\\") plt.show() # Task 4: Boxen Plot of Price Grouped by Clarity with Linear Width def plot_boxenplot_linear_width(data): sns.boxenplot(x=\'price\', y=\'clarity\', data=data, width_method=\\"linear\\") plt.title(\\"Boxen Plot of Diamond Prices by Clarity with Linear Width\\") plt.show() # Task 5: Customized Boxen Plot from Task 4 def plot_customized_boxenplot(data): sns.boxenplot( x=\'price\', y=\'clarity\', data=data, width_method=\\"linear\\", k_depth=\'full\', linewidth=0.5, color=\'.7\' ) plt.title(\\"Customized Boxen Plot of Diamond Prices by Clarity\\") plt.show() # Task 6: Unfilled Boxen Plot by Clarity def plot_unfilled_boxenplot(data): sns.boxenplot(x=\'price\', y=\'clarity\', hue=\'clarity\', data=data, linewidth=1.5, palette=\\"dark:.3\\", fill=False) plt.title(\\"Unfilled Boxen Plot of Diamond Prices by Clarity\\") plt.show()"},{"question":"# Dynamic Class Creation and Coroutine Transformation **Problem Description**: Using the `types` module, write a function `create_dynamic_coroutine_class` that dynamically creates a class with the following specifications: 1. The class should be named `DynamicCoroutineClass`. 2. The class should have a single method named `async_method` which should be a coroutine function. 3. The `async_method` should yield the string `\\"Coroutine Example\\"` when awaited. **Constraints**: - You must use `types.new_class` to create the class. - The `async_method` should be defined within the class as an asynchronous generator function. - The class should not inherit from any other class. **Input**: None **Output**: - A class object with the specified name and method. **Example**: ```python # Function to implement def create_dynamic_coroutine_class(): # Your implementation here DynamicCoroutineClass = create_dynamic_coroutine_class() # Checking the class name print(DynamicCoroutineClass.__name__) # Output: DynamicCoroutineClass # Creating an instance instance = DynamicCoroutineClass() # Getting the coroutine object coroutine = instance.async_method() # Running the coroutine inside an event loop import asyncio result = asyncio.run(coroutine) # Output: Coroutine Example print(result) ``` Your task is to implement the function `create_dynamic_coroutine_class`.","solution":"import types def create_dynamic_coroutine_class(): # Define the coroutine method async def async_method(self): return \\"Coroutine Example\\" # Create the class using types.new_class DynamicCoroutineClass = types.new_class( \'DynamicCoroutineClass\', (), {}, lambda ns: ns.update({\'async_method\': async_method}) ) return DynamicCoroutineClass # Check class name and method existence DynamicCoroutineClass = create_dynamic_coroutine_class() print(DynamicCoroutineClass.__name__) # Output: DynamicCoroutineClass instance = DynamicCoroutineClass() coroutine = instance.async_method()"},{"question":"# Problem: Generating Infinite Primes Background A generator in Python is a function that allows you to iterate over a sequence of values lazily, meaning it yields items one at a time and only computes subsequent items when requested. For this problem, you will need to implement a generator function that produces an infinite sequence of prime numbers. A prime number is a natural number greater than 1 that has no positive divisors other than 1 and itself. Task Implement a generator function `generate_primes()` that yields an infinite sequence of prime numbers. # Function Signature ```python def generate_primes(): # your code here ``` # Constraints - The generator must be capable of producing an infinite sequence of prime numbers. - You may use any helper functions required, but the primary logic should be implemented within the `generate_primes` generator function. # Performance Requirements - Your solution should efficiently generate prime numbers, leveraging the lazy evaluation properties of generators. - Avoid recalculating primes from scratch after each yield—make use of previously calculated primes to optimize performance. # Example Usage ```python # Example of how the generator might be used: primes = generate_primes() for _ in range(10): print(next(primes)) # This will print the first 10 prime numbers: 2, 3, 5, 7, 11, 13, 17, 19, 23, 29 ``` Additional Notes - You do not need to implement input validation. - Ensure your solution leverages Python\'s generator mechanics.","solution":"def generate_primes(): A generator function that yields an infinite sequence of prime numbers. def is_prime(n, primes): for p in primes: if p * p > n: break if n % p == 0: return False return True primes = [] candidate = 2 while True: if is_prime(candidate, primes): primes.append(candidate) yield candidate candidate += 1"},{"question":"<|Analysis Begin|> The documentation provided offers an extensive look into time series and date functionalities within the pandas framework. It details various operations such as parsing, generating sequences, manipulating, resampling, and converting date times with timezone information. It introduces fundamental concepts such as Timestamp, Timedelta, and Period, alongside their representations in pandas objects like DatetimeIndex and PeriodIndex. It also covers more advanced features like handling DST (Daylight Saving Time) transitions, ambiguous and non-existent times, and custom business day calendars. <|Analysis End|> <|Question Begin|> In this exercise, you are required to demonstrate your understanding and proficiency with time series operations in pandas. You will need to implement, manipulate, and convert time-related data. **Problem Statement:** You are provided with a series of timestamps of event logs occurring over several weeks. Your task is to perform the following operations using pandas: 1. **Generate Timestamps:** - Create a DataFrame called `events` with two columns: - `timestamp`: A list of timestamps ranging from \'2023-01-01 08:00:00\' to \'2023-02-01 08:00:00\' with an interval of 60 minutes each. - `event_id`: A list of integers from 1 to the length of `timestamp`. 2. **Timezone Conversion:** - Convert the `timestamp` column to the US/Pacific timezone. 3. **Resampling and Aggregation:** - Resample the `events` DataFrame to daily frequency, aggregating the event count for each day. - Name this resulting DataFrame `daily_events` and ensure it includes: - `date`: The date of the events. - `event_count`: The total number of events that occurred on that date. 4. **Handling Ambiguous and Non-existent Times:** - Introduce Daylight Saving Time changes and handle any ambiguous and non-existent times by appropriately correcting the times and ensuring your DataFrame does not raise errors during localization. - For this specific case, assume a custom time translation that shifts in Forward by 1 hour if there\'s an overlap or non-existent time. 5. **Custom Business Days:** - Define a custom business calendar that excludes weekends and US holidays (use `USFederalHolidayCalendar`). - Resample the `events` DataFrame based on this custom business day frequency and count the events on valid business days. **Function Signature:** ```python import pandas as pd def process_time_series_events(): # Step 1: Generate Timestamps events = generate_timestamps() # Step 2: Timezone Conversion events = convert_timezone(events) # Step 3: Resampling and Aggregation daily_events = resample_aggregation(events) # Step 4: Handling Ambiguous and Non-existent Times events = handle_ambiguous_nonexistent_times(events) # Step 5: Custom Business Days business_day_counts = custom_business_day_resampling(events) return daily_events, business_day_counts def generate_timestamps(): # Implementation here pass def convert_timezone(events_df): # Implementation here pass def resample_aggregation(events_df): # Implementation here pass def handle_ambiguous_nonexistent_times(events_df): # Implementation here pass def custom_business_day_resampling(events_df): # Implementation here pass # You can also add helper functions if needed. ``` **Input/Output Example:** - Input: - No direct input, the function works by generating timestamps internally. - Output: - `daily_events`: A DataFrame containing daily aggregated event counts. - `business_day_counts`: A DataFrame containing event counts for each valid business day under custom business day rules. **Constraints:** - The time series should handle UTC and timezone conversions accurately. - Ensure no errors for ambiguous or non-existent times. - Must correctly apply custom business day rules, excluding weekends and recognized holidays. Ensure your code is efficient, handles edge cases, and is well-documented.","solution":"import pandas as pd import numpy as np from pandas.tseries.holiday import USFederalHolidayCalendar from datetime import datetime, timedelta def process_time_series_events(): # Step 1: Generate Timestamps events = generate_timestamps() # Step 2: Timezone Conversion events = convert_timezone(events) # Step 3: Resampling and Aggregation daily_events = resample_aggregation(events) # Step 4: Handling Ambiguous and Non-existent Times events = handle_ambiguous_nonexistent_times(events) # Step 5: Custom Business Days business_day_counts = custom_business_day_resampling(events) return daily_events, business_day_counts def generate_timestamps(): rng = pd.date_range(start=\'2023-01-01 08:00:00\', end=\'2023-02-01 08:00:00\', freq=\'60T\') events = pd.DataFrame({\'timestamp\': rng}) events[\'event_id\'] = np.arange(1, len(events) + 1) return events def convert_timezone(events_df): events_df[\'timestamp\'] = pd.to_datetime(events_df[\'timestamp\']).dt.tz_localize(\'UTC\').dt.tz_convert(\'US/Pacific\') return events_df def resample_aggregation(events_df): events_df[\'date\'] = events_df[\'timestamp\'].dt.date daily_events = events_df.groupby(\'date\').size().reset_index(name=\'event_count\') return daily_events def handle_ambiguous_nonexistent_times(events_df): # US/Pacific timezone policy: DST changes try: events_df[\'timestamp\'] = events_df[\'timestamp\'].dt.tz_localize(\'US/Pacific\', ambiguous=\'NaT\') except Exception as e: pass return events_df def custom_business_day_resampling(events_df): cal = USFederalHolidayCalendar() bdays = pd.offsets.CustomBusinessDay(weekmask=\'Mon Tue Wed Thu Fri\', holidays=cal.holidays()) events_df[\'timestamp\'] = pd.to_datetime(events_df[\'timestamp\']) events_df = events_df.set_index(\'timestamp\').asfreq(bdays) business_day_counts = events_df.resample(bdays).size().reset_index(name=\'event_count\') return business_day_counts # Solution Ends Here"},{"question":"# Task You are tasked with developing a function that monitors the system processes and retrieves specific information about each process. The function should be able to return a list of dictionaries, each containing information about a running process. # Requirements 1. **Function Name**: `get_system_process_info` 2. **Input**: The function does not take any input parameters. 3. **Output**: The function returns a list of dictionaries. - Each dictionary represents a running system process and should contain the following key-value pairs: - `pid`: Process ID (integer) - `name`: Process name (string) - `cpu_usage`: Percentage of CPU usage (float) - `memory_usage`: Memory usage in bytes (integer) 4. **Constraints**: - The function must handle exceptions gracefully and provide meaningful error messages. - The function should ensure that the data gathered is current and reflects the latest state of the system processes. 5. **Performance**: The function needs to efficiently retrieve and construct the list of processes. Avoid unnecessary computations and aim to complete the task in a reasonable timeframe. # Example: ```python def get_system_process_info(): # Your implementation goes here # Example output: [ { \\"pid\\": 134, \\"name\\": \\"python\\", \\"cpu_usage\\": 15.2, \\"memory_usage\\": 23552000 }, { \\"pid\\": 102, \\"name\\": \\"chrome\\", \\"cpu_usage\\": 5.6, \\"memory_usage\\": 142000000 } # More processes... ] ``` # Hints: - You may need to use system utilities to get the process information. - Consider using Python modules like `psutil` for accessing system process information. - Use reflection or similar techniques if needed to extract and format the necessary information.","solution":"import psutil def get_system_process_info(): Returns a list of dictionaries containing information about each running process. Each dictionary contains the process id (pid), process name, cpu_usage percentage, and memory usage in bytes. process_info_list = [] try: for process in psutil.process_iter([\'pid\', \'name\', \'cpu_percent\', \'memory_info\']): info = process.info process_info = { \'pid\': info[\'pid\'], \'name\': info[\'name\'], \'cpu_usage\': info[\'cpu_percent\'], \'memory_usage\': info[\'memory_info\'].rss } process_info_list.append(process_info) except (psutil.NoSuchProcess, psutil.AccessDenied, psutil.ZombieProcess) as e: print(f\\"An error occurred while retrieving process information: {e}\\") return process_info_list"},{"question":"**Question: Advanced Seaborn Plotting** You are provided with a dataset containing observations of tips received by servers in a restaurant, known as the `tips` dataset. Using Seaborn\'s `objects` module, your task is to create a detailed bar plot that showcases the distribution of total bills (`total_bill`) given different days of the week, distinguished additionally by gender (`sex`). **Requirements:** 1. Use the `seaborn.objects` module. 2. Load the `tips` dataset using `seaborn.load_dataset(\\"tips\\")`. 3. Create a bar plot with `total_bill` on the `y`-axis and `day` on the `x`-axis. 4. Distinguish the bars by `sex` (use different colors for male and female). 5. Include a legend to label colors by `sex`. 6. Provide proper axis labels and a title for the plot. **Input:** Utilize the provided `tips` dataset; no additional input is required. **Output:** Save the plot as an image file named `total_bill_distribution.png`. **Constraints and Notes:** - You must use the `so.Plot` class and other relevant objects from `seaborn.objects`. - Ensure that your plot is clear and informative, adhering to good visualization practices. **Performance:** The plotting code should execute within reasonable time limits, typically a few seconds. ```python import seaborn.objects as so from seaborn import load_dataset # Load the dataset tips = load_dataset(\\"tips\\") # Your code here to create the specified plot # ... # Save plot to file # plot.save(\\"total_bill_distribution.png\\") ```","solution":"import seaborn.objects as so from seaborn import load_dataset def create_total_bill_distribution_plot(): # Load the dataset tips = load_dataset(\\"tips\\") # Create the plot plot = ( so.Plot(tips, x=\\"day\\", y=\\"total_bill\\", color=\\"sex\\") .add(so.Bar(), so.Dodge()) .label( title=\\"Distribution of Total Bills by Day and Gender\\", x=\\"Day of the Week\\", y=\\"Total Bill\\", color=\\"Gender\\", ) ) # Save the plot to a file plot.save(\\"total_bill_distribution.png\\") # Call the function to create and save the plot create_total_bill_distribution_plot()"},{"question":"As a Python developer, you are tasked with writing a program that performs various operations on set and frozenset objects. Your task is to implement a function `set_operations` that takes two iterable objects and performs a series of operations using the Python C-API methods described below. The iterable objects will be used to create set and frozenset objects. Your function should return a dictionary with the results of these operations. Function Signature ```python def set_operations(iterable1, iterable2): pass ``` Input - **iterable1**: An iterable object (e.g., list, tuple, string) used to create the first set. - **iterable2**: An iterable object (e.g., list, tuple, string) used to create the second set. Output - Returns a dictionary with the following keys and their associated results: - `\\"union\\"`: The union of the two sets. - `\\"intersection\\"`: The intersection of the two sets. - `\\"difference\\"`: The difference of the two sets (first set - second set). - `\\"issubset\\"`: A boolean indicating whether the first set is a subset of the second set. - `\\"issuperset\\"`: A boolean indicating whether the first set is a superset of the second set. Constraints - The iterables provided will not contain any unhashable types. - The function should handle cases where the input iterables are empty. Example ```python set_operations([1, 2, 3], [3, 4, 5]) ``` Output: ```python { \\"union\\": {1, 2, 3, 4, 5}, \\"intersection\\": {3}, \\"difference\\": {1, 2}, \\"issubset\\": False, \\"issuperset\\": False } ``` Additional Notes - You can use Python\'s built-in set operations to implement the desired functionality. - Make sure to handle edge cases, such as when one or both iterables are empty. Implement the `set_operations` function to complete the task.","solution":"def set_operations(iterable1, iterable2): Perform various set operations on two iterables. Args: iterable1 (iterable): An iterable to create the first set. iterable2 (iterable): An iterable to create the second set. Returns: dict: A dictionary with the results of set operations. set1 = set(iterable1) set2 = set(iterable2) result = { \\"union\\": set1.union(set2), \\"intersection\\": set1.intersection(set2), \\"difference\\": set1.difference(set2), \\"issubset\\": set1.issubset(set2), \\"issuperset\\": set1.issuperset(set2) } return result"},{"question":"# Email Header Manipulation with Custom Policies You are tasked with implementing a function that reads an email message from a file, modifies certain headers using different policy configurations, and writes the modified email message back to another file. The function should also handle any defects encountered during this process according to the policy settings. Function Signature ```python def manipulate_email_headers(input_file: str, output_file: str) -> None: pass ``` Description 1. **Read the Email Message:** - Read an email message from a binary file specified by `input_file`. - Use the default policy `email.policy.default` when reading the message. 2. **Modify Headers:** - Clone the default policy to create a custom policy with the following attributes: - `max_line_length` set to 100. - `linesep` set to `rn`. - `cte_type` set to `8bit`. - `raise_on_defect` set to `True`. - Use this custom policy to modify the headers as follows: 1. Add a header `X-Custom-Header` with the value `CustomValue`. 2. Modify an existing header `Subject` to append the string ` - Modified`. 3. **Write the Modified Message:** - Serialize the modified message with the new policy settings. - Write the serialized message to a binary file specified by `output_file`. 4. **Handle Defects:** - If any defects occur during header modification or serialization, raise them as exceptions since the `raise_on_defect` flag is set to `True`. Input - `input_file` (str): Path to the input binary file containing the original email message. - `output_file` (str): Path to the output binary file where the modified email message will be written. Output - The function does not return any value. It writes the modified email message to `output_file`. Example Suppose `input_file` contains an email with the following headers: ``` From: example@example.com To: recipient@example.com Subject: Original Subject ``` The function should generate an output file with the headers: ``` From: example@example.com To: recipient@example.com Subject: Original Subject - Modified X-Custom-Header: CustomValue ``` Also, ensure that the modified message adheres to the custom policy settings and handles any defects accordingly. Notes - Use appropriate classes and methods from the `email` package to parse, modify, and serialize the email message. - Make sure you handle different possibilities such as missing headers gracefully according to the policy settings. Constraints - The `input_file` and `output_file` paths will be valid paths to existing writable files. - The email message in the `input_file` will conform to basic email format standards but may have attributes that could potentially raise defects.","solution":"import email from email import policy from email.parser import BytesParser from email.generator import BytesGenerator from email.policy import Policy def manipulate_email_headers(input_file: str, output_file: str) -> None: # Read the original email message from the binary input file with open(input_file, \'rb\') as file: original_message = BytesParser(policy=policy.default).parse(file) # Create a custom policy for manipulating the email headers custom_policy = policy.default.clone(max_line_length=100, linesep=\'rn\', cte_type=\'8bit\', raise_on_defect=True) # Modify headers according to the requirements original_message.add_header(\'X-Custom-Header\', \'CustomValue\') if \'Subject\' in original_message: original_subject = original_message[\'Subject\'] new_subject = original_subject + \' - Modified\' original_message.replace_header(\'Subject\', new_subject) # Write the modified email message to the binary output file with open(output_file, \'wb\') as file: generator = BytesGenerator(file, policy=custom_policy) generator.flatten(original_message)"},{"question":"**Objective:** Implement a class structure for a `Library` system. This system should incorporate various aspects of Python class mechanics including class variables, instance variables, inheritance, and the iterator protocol. **Instructions:** 1. **Define a Base Class (`Book`)**: - This class should represent a book with the following attributes: - Title (string) - Author (string) - ISBN (string) - Implement a method `__str__()` that returns a string representation of the book in the format: `\\"Title by Author (ISBN: ISBN)\\"`. 2. **Define a Derived Class (`EBook`)**: - Inherit from `Book`. - This class should add the following attribute: - File size (in MB, float) - Override the `__str__()` method to include the file size in the string representation: `\\"Title by Author (ISBN: ISBN) - File size: X MB\\"`. 3. **Define a `Library` class**: - This class should manage a collection of `Book` (and `EBook`) objects using a list. - Implement methods: - `add_book(book)`: Adds a book to the library collection. - `remove_book(isbn)`: Removes a book with the given ISBN from the library collection. - `find_book(isbn)`: Returns the book with the given ISBN or `None` if not found. - Implement the iterator protocol for the `Library` class to iterate over its collection of books. 4. **Usage of the Library Class**: - Demonstrate the functionality of the `Library` class by creating an instance, adding several `Book` and `EBook` objects, removing one, and iterating over the collection to print their string representations. **Constraints:** - Ensure proper encapsulation and use of instance and class variables where appropriate. - Demonstrate the use of inheritance and method overriding correctly. **Example Usage:** ```python # Define the Book class class Book: def __init__(self, title, author, isbn): self.title = title self.author = author self.isbn = isbn def __str__(self): return f\\"{self.title} by {self.author} (ISBN: {self.isbn})\\" # Define the EBook class inheriting from Book class EBook(Book): def __init__(self, title, author, isbn, file_size): super().__init__(title, author, isbn) self.file_size = file_size def __str__(self): return f\\"{self.title} by {self.author} (ISBN: {self.isbn}) - File size: {self.file_size} MB\\" # Define the Library class class Library: def __init__(self): self.collection = [] def add_book(self, book): self.collection.append(book) def remove_book(self, isbn): self.collection = [book for book in self.collection if book.isbn != isbn] def find_book(self, isbn): for book in self.collection: if book.isbn == isbn: return book return None def __iter__(self): return iter(self.collection) # Demonstrate the functionality if __name__ == \\"__main__\\": library = Library() book1 = Book(\\"1984\\", \\"George Orwell\\", \\"9780451524935\\") ebook1 = EBook(\\"Digital Fortress\\", \\"Dan Brown\\", \\"9780312944926\\", 2.4) library.add_book(book1) library.add_book(ebook1) print(\\"Books in library:\\") for book in library: print(book) library.remove_book(\\"9780451524935\\") print(\\"nBooks in library after removal:\\") for book in library: print(book) found_book = library.find_book(\\"9780312944926\\") print(\\"nFound book:\\", found_book) ``` This problem will require students to demonstrate their understanding of Python classes, inheritance, method overriding, and the iterator protocol, making it a comprehensive task.","solution":"class Book: def __init__(self, title, author, isbn): self.title = title self.author = author self.isbn = isbn def __str__(self): return f\\"{self.title} by {self.author} (ISBN: {self.isbn})\\" class EBook(Book): def __init__(self, title, author, isbn, file_size): super().__init__(title, author, isbn) self.file_size = file_size def __str__(self): return f\\"{self.title} by {self.author} (ISBN: {self.isbn}) - File size: {self.file_size} MB\\" class Library: def __init__(self): self.collection = [] def add_book(self, book): self.collection.append(book) def remove_book(self, isbn): self.collection = [book for book in self.collection if book.isbn != isbn] def find_book(self, isbn): for book in self.collection: if book.isbn == isbn: return book return None def __iter__(self): return iter(self.collection)"},{"question":"Problem Statement **Objective:** The goal of this assessment is to evaluate your ability to create different types of seaborn visualizations and interpret the results. # Task Given the Titanic dataset, you are required to perform the following tasks: 1. Load the Titanic dataset using seaborn. 2. Create a count plot to visualize the number of passengers in each class. 3. Enhance the count plot by showing the counts grouped by whether the passengers survived or not. 4. Normalize the counts to show percentages instead of raw counts. 5. Create a bar plot to show the average age of passengers in each class and differentiate the bars by the `sex` of the passengers. 6. Customize the bar plot by adding titles, axis labels and adjusting the style for better readability. # Input There are no explicit input parameters. You will be working directly with the Titanic dataset provided by seaborn. # Output 1. Display the count plot of the number of passengers in each class. 2. Display the enhanced count plot grouped by survival status. 3. Display the normalized percentage count plot. 4. Display the bar plot of average age, grouped by class and sex. 5. Customize the aesthetics of the bar plot for clarity. # Constraints - Use seaborn and its built-in functionalities to accomplish the tasks. - Make sure to set a theme for seaborn to ensure all plots follow a consistent style. # Example ```python # Import seaborn and set the theme import seaborn as sns sns.set_theme(style=\\"whitegrid\\") # Load the Titanic dataset titanic = sns.load_dataset(\\"titanic\\") # 1. Create a count plot for the number of passengers in each class sns.countplot(data=titanic, x=\\"class\\") # 2. Enhance the count plot by grouping by \'survived\' sns.countplot(data=titanic, x=\\"class\\", hue=\\"survived\\") # 3. Normalize the counts to show percentages sns.countplot(data=titanic, x=\\"class\\", hue=\\"survived\\", stat=\\"percent\\") # 4. Create a bar plot of the average age of passengers in each class, differentiated by \'sex\' sns.barplot(data=titanic, x=\\"class\\", y=\\"age\\", hue=\\"sex\\") # 5. Customize the bar plot import matplotlib.pyplot as plt plt.title(\\"Average Age of Passengers by Class and Sex\\") plt.xlabel(\\"Class\\") plt.ylabel(\\"Average Age\\") plt.show() ``` This example will help you understand the steps involved in the task. Your final implementation should handle each of these steps and create the required plots effectively.","solution":"import seaborn as sns import matplotlib.pyplot as plt # Set the seaborn theme for consistent styling sns.set_theme(style=\\"whitegrid\\") # Load the Titanic dataset titanic = sns.load_dataset(\\"titanic\\") def create_count_plot(): Creates and displays a count plot for the number of passengers in each class. plt.figure(figsize=(10, 6)) sns.countplot(data=titanic, x=\\"class\\") plt.title(\'Number of Passengers in Each Class\') plt.xlabel(\'Class\') plt.ylabel(\'Count\') plt.show() def create_enhanced_count_plot(): Creates and displays an enhanced count plot for the number of passengers in each class, grouped by survival status. plt.figure(figsize=(10, 6)) sns.countplot(data=titanic, x=\\"class\\", hue=\\"survived\\") plt.title(\'Number of Passengers in Each Class Grouped by Survival Status\') plt.xlabel(\'Class\') plt.ylabel(\'Count\') plt.legend(title=\'Survived\', loc=\'upper right\') plt.show() def create_normalized_count_plot(): Creates and displays a normalized count plot for the number of passengers in each class, showing percentages instead of raw counts. plt.figure(figsize=(10, 6)) total = titanic[\'class\'].value_counts().sum() sns.countplot( data=titanic, x=\\"class\\", hue=\\"survived\\", stat=\\"percent\\" ) plt.title(\'Percentage of Passengers in Each Class Grouped by Survival Status\') plt.xlabel(\'Class\') plt.ylabel(\'Percentage\') plt.legend(title=\'Survived\', loc=\'upper right\') plt.show() def create_bar_plot(): Creates and displays a bar plot showing the average age of passengers in each class, differentiated by the sex of the passengers. Customizes the plot aesthetics. plt.figure(figsize=(10, 6)) sns.barplot(data=titanic, x=\\"class\\", y=\\"age\\", hue=\\"sex\\") plt.title(\'Average Age of Passengers by Class and Sex\') plt.xlabel(\'Class\') plt.ylabel(\'Average Age\') plt.legend(title=\'Sex\', loc=\'upper right\') plt.show() create_count_plot() create_enhanced_count_plot() create_normalized_count_plot() create_bar_plot()"},{"question":"**Command-Line Tool for To-Do List Management** You have been assigned to create a simple command-line interface (CLI) for managing a to-do list. The program will support adding tasks, listing all tasks, deleting tasks, and marking tasks as completed. Your task is to implement the CLI using the `argparse` module. Your program should support the following functionalities: 1. **Adding a new task**: This should add a task to the list with a description provided by the user. 2. **Listing all tasks**: This should list all tasks with their status (completed or not completed). 3. **Deleting a task**: This should delete a task based on its ID. 4. **Marking a task as completed**: This should update the status of a task as completed based on its ID. **Implementation Details:** - Create a Python script using the `argparse` module. - Use a file named `todo.txt` to store tasks persistently. Each task should be stored with an ID, description, and status (completed or not). - Handle errors gracefully, such as trying to delete or complete a task that does not exist. - Provide helpful usage and help messages for the user. **Requirements:** 1. **Add Task:** - Command: `add` - Argument: `description` (string) - Example: `python todo.py add \\"Buy groceries\\"` 2. **List All Tasks:** - Command: `list` - No arguments - Example: `python todo.py list` 3. **Delete Task:** - Command: `delete` - Argument: `task_id` (integer) - Example: `python todo.py delete 1` 4. **Complete Task:** - Command: `complete` - Argument: `task_id` (integer) - Example: `python todo.py complete 2` **Sample Usage:** ```bash python todo.py add \\"Walk the dog\\" Task added successfully. python todo.py list 1. [ ] Walk the dog python todo.py complete 1 Task marked as completed. python todo.py list 1. [x] Walk the dog python todo.py delete 1 Task deleted successfully. ``` **Constraints:** 1. Ensure that each task ID is unique and increments automatically. 2. Ensure that the task descriptions are stored in a readable format within the `todo.txt` file. **Grading Criteria:** - Correct use of `argparse` for command-line argument parsing. - Proper reading from and writing to the `todo.txt` file. - Handling of edge cases and errors (e.g., non-existent task IDs). - Clear and user-friendly help and usage messages. - Clean and well-documented code. **Note:** - You are allowed to use standard Python libraries. - Avoid using external libraries for this task. Good luck!","solution":"import argparse import os TODO_FILE = \'todo.txt\' def read_tasks(): if not os.path.exists(TODO_FILE): return [] with open(TODO_FILE, \'r\') as file: lines = file.readlines() tasks = [] for line in lines: parts = line.strip().split(\';\', 2) task_id, status, description = parts tasks.append({\'id\': int(task_id), \'status\': status == \'True\', \'description\': description}) return tasks def write_tasks(tasks): with open(TODO_FILE, \'w\') as file: for task in tasks: file.write(f\\"{task[\'id\']};{task[\'status\']};{task[\'description\']}n\\") def add_task(description): tasks = read_tasks() task_id = 1 if not tasks else tasks[-1][\'id\'] + 1 new_task = {\'id\': task_id, \'status\': False, \'description\': description} tasks.append(new_task) write_tasks(tasks) print(\\"Task added successfully.\\") def list_tasks(): tasks = read_tasks() for task in tasks: status = \'[x]\' if task[\'status\'] else \'[ ]\' print(f\\"{task[\'id\']}. {status} {task[\'description\']}\\") def delete_task(task_id): tasks = read_tasks() tasks = [task for task in tasks if task[\'id\'] != task_id] write_tasks(tasks) print(\\"Task deleted successfully.\\") def complete_task(task_id): tasks = read_tasks() for task in tasks: if task[\'id\'] == task_id: task[\'status\'] = True write_tasks(tasks) print(\\"Task marked as completed.\\") def main(): parser = argparse.ArgumentParser(description=\\"To-Do List Management\\") subparsers = parser.add_subparsers(dest=\'command\', required=True) parser_add = subparsers.add_parser(\'add\', help=\'Add a new task\') parser_add.add_argument(\'description\', type=str, help=\'Description of the task\') parser_list = subparsers.add_parser(\'list\', help=\'List all tasks\') parser_delete = subparsers.add_parser(\'delete\', help=\'Delete a task by ID\') parser_delete.add_argument(\'task_id\', type=int, help=\'ID of the task to delete\') parser_complete = subparsers.add_parser(\'complete\', help=\'Mark a task as completed by ID\') parser_complete.add_argument(\'task_id\', type=int, help=\'ID of the task to mark as completed\') args = parser.parse_args() if args.command == \'add\': add_task(args.description) elif args.command == \'list\': list_tasks() elif args.command == \'delete\': delete_task(args.task_id) elif args.command == \'complete\': complete_task(args.task_id) if __name__ == \\"__main__\\": main()"},{"question":"**Objective:** The goal of this task is to demonstrate your proficiency with pandas `Index` objects and their various subclasses by implementing and manipulating indices in a DataFrame. **Question:** You are given a DataFrame containing sales data with the following columns: `order_id`, `product`, `category`, `order_date`, `quantity`, and `price`. ```python import pandas as pd data = { \'order_id\': [1, 2, 3, 4, 5], \'product\': [\'A\', \'B\', \'C\', \'A\', \'B\'], \'category\': [\'Electronics\', \'Furniture\', \'Electronics\', \'Electronics\', \'Furniture\'], \'order_date\': pd.to_datetime([\'2021-07-01\', \'2021-07-02\', \'2021-07-03\', \'2021-07-04\', \'2021-07-05\']), \'quantity\': [2, 1, 1, 3, 2], \'price\': [200, 450, 300, 200, 450] } sales_df = pd.DataFrame(data) ``` **Tasks:** 1. **Creating MultiIndex:** - Convert the `sales_df` DataFrame so that it uses `order_date` and `category` as a MultiIndex. Name this new DataFrame `multi_sales_df`. 2. **Index Operations:** - Using `multi_sales_df`, write a function `select_data_by_date_and_category` that accepts a start date, end date, and a category as arguments and returns the subset of data for that category within the date range. 3. **Time-Specific Operations:** - Generate a `DatetimeIndex` for the `order_date` column. Shift this index by 1 day and append it to the existing DataFrame as a new column `shifted_order_date`. 4. **Performance Analysis:** - Create a function `get_unique_products` that extracts and returns a list of unique products for a given category using `CategoricalIndex`. 5. **Aggregation and Transformation:** - Write a function to calculate the total revenue for each category leveraging the `MultiIndex`. Return a DataFrame indexed by `category` with a column `total_revenue`. **Function Signatures:** ```python def create_multiindex_sales_df(sales_df: pd.DataFrame) -> pd.DataFrame: pass def select_data_by_date_and_category(multi_sales_df: pd.DataFrame, start_date: str, end_date: str, category: str) -> pd.DataFrame: pass def add_shifted_order_date(sales_df: pd.DataFrame) -> pd.DataFrame: pass def get_unique_products(sales_df: pd.DataFrame, category: str) -> list: pass def calculate_total_revenue(multi_sales_df: pd.DataFrame) -> pd.DataFrame: pass ``` **Constraints:** - Dates are provided in the format \'YYYY-MM-DD\'. - The category parameter for `select_data_by_date_and_category` and `get_unique_products` will always be a valid category present in the DataFrame. - The DataFrame may grow larger, and performance should be considered when manipulating large DataFrames. **Example Output:** ```python # Creating the multi-index DataFrame multi_sales_df = create_multiindex_sales_df(sales_df) print(multi_sales_df) # Selecting data by date and category filtered_data = select_data_by_date_and_category(multi_sales_df, \'2021-07-01\', \'2021-07-03\', \'Electronics\') print(filtered_data) # Adding shifted order date shifted_sales_df = add_shifted_order_date(sales_df) print(shifted_sales_df) # Getting unique products in \'Electronics\' category unique_products = get_unique_products(sales_df, \'Electronics\') print(unique_products) # Calculating total revenue per category total_revenue_df = calculate_total_revenue(multi_sales_df) print(total_revenue_df) ``` Ensure your solutions are well-tested with the provided data and are able to handle edge cases such as missing values or empty DataFrames.","solution":"import pandas as pd def create_multiindex_sales_df(sales_df: pd.DataFrame) -> pd.DataFrame: Convert the sales_df DataFrame so that it uses order_date and category as a MultiIndex. multi_sales_df = sales_df.set_index([\'order_date\', \'category\']) return multi_sales_df def select_data_by_date_and_category(multi_sales_df: pd.DataFrame, start_date: str, end_date: str, category: str) -> pd.DataFrame: Return a subset of data for the specified category within the date range. idx = pd.IndexSlice filtered_data = multi_sales_df.loc[idx[start_date:end_date, category], :] return filtered_data def add_shifted_order_date(sales_df: pd.DataFrame) -> pd.DataFrame: Shift the order_date column by 1 day and add it as a new column shifted_order_date. sales_df[\'shifted_order_date\'] = sales_df[\'order_date\'] + pd.Timedelta(days=1) return sales_df def get_unique_products(sales_df: pd.DataFrame, category: str) -> list: Extract and return a list of unique products for a given category. unique_products = sales_df[sales_df[\'category\'] == category][\'product\'].unique() return unique_products.tolist() def calculate_total_revenue(multi_sales_df: pd.DataFrame) -> pd.DataFrame: Calculate the total revenue for each category. Return a DataFrame indexed by category with a column total_revenue. multi_sales_df[\'revenue\'] = multi_sales_df[\'quantity\'] * multi_sales_df[\'price\'] total_revenue_df = multi_sales_df.groupby(\'category\')[[\'revenue\']].sum().rename(columns={\'revenue\': \'total_revenue\'}) return total_revenue_df"},{"question":"# Custom Precision-Recall Curve Display In this coding assessment, you will implement a custom visualization class for displaying Precision-Recall (PR) curves using scikit-learn\'s plotting API. The PR curve is a useful tool for understanding the trade-off between precision and recall for different threshold settings of a binary classifier. Your Task 1. Implement a `PRCurveDisplay` class following the structure provided in the documentation. Include methods for: - `__init__`: Store the calculated data needed for plotting. - `plot`: Handle the visualization logic with optional customization. - `from_estimator`: Compute the required values from an existing estimator and data. - `from_predictions`: Compute the required values from true and predicted labels. 2. Ensure that the `plot` method can handle both single and multiple axes using matplotlib\'s `GridSpecFromSubplotSpec`. Class Definition ```python import matplotlib.pyplot as plt from sklearn.metrics import precision_recall_curve, auc class PRCurveDisplay: def __init__(self, precision, recall, pr_auc, estimator_name): Initialize the PRCurveDisplay with precision, recall, and AUC values. Arguments: - precision: Array of precision values. - recall: Array of recall values. - pr_auc: Area under the precision-recall curve. - estimator_name: Name of the estimator. self.precision = precision self.recall = recall self.pr_auc = pr_auc self.estimator_name = estimator_name @classmethod def from_estimator(cls, estimator, X, y): Create a PRCurveDisplay from an estimator and data. Arguments: - estimator: The trained estimator. - X: Feature matrix. - y: True labels. Returns: A PRCurveDisplay object. y_pred = estimator.predict_proba(X)[:, 1] return cls.from_predictions(y, y_pred, estimator.__class__.__name__) @classmethod def from_predictions(cls, y, y_pred, estimator_name): Create a PRCurveDisplay from true and predicted labels. Arguments: - y: True labels. - y_pred: Predicted probabilities. - estimator_name: Name of the estimator. Returns: A PRCurveDisplay object. precision, recall, _ = precision_recall_curve(y, y_pred) pr_auc = auc(recall, precision) viz = cls(precision, recall, pr_auc, estimator_name) return viz.plot() def plot(self, ax=None, name=None, **kwargs): Plot the precision-recall curve. Arguments: - ax: Matplotlib axes object. - name: Optional name for the plot. - **kwargs: Additional plotting parameters. if ax is None: fig, ax = plt.subplots() ax.plot(self.recall, self.precision, lw=2, label=f\'{self.estimator_name} (AUC = {self.pr_auc:.2f}) {name}\') ax.set_xlabel(\'Recall\') ax.set_ylabel(\'Precision\') ax.set_title(\'Precision-Recall Curve\') ax.legend(loc=\\"best\\") self.ax_ = ax self.figure_ = ax.figure_ return self # Example usage: # from sklearn.linear_model import LogisticRegression # from sklearn.datasets import make_classification # from sklearn.model_selection import train_test_split # X, y = make_classification(random_state=0) # X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0) # clf = LogisticRegression().fit(X_train, y_train) # pr_display = PRCurveDisplay.from_estimator(clf, X_test, y_test) # pr_display.plot() ``` Constraints - You should not use any additional libraries other than scikit-learn and matplotlib. - Ensure that your implementation supports multiple axes using `GridSpecFromSubplotSpec`. Performance Requirements - Your implementation should handle datasets with up to 100,000 samples efficiently.","solution":"import matplotlib.pyplot as plt from sklearn.metrics import precision_recall_curve, auc class PRCurveDisplay: def __init__(self, precision, recall, pr_auc, estimator_name): Initialize the PRCurveDisplay with precision, recall, and AUC values. Arguments: - precision: Array of precision values. - recall: Array of recall values. - pr_auc: Area under the precision-recall curve. - estimator_name: Name of the estimator. self.precision = precision self.recall = recall self.pr_auc = pr_auc self.estimator_name = estimator_name @classmethod def from_estimator(cls, estimator, X, y): Create a PRCurveDisplay from an estimator and data. Arguments: - estimator: The trained estimator. - X: Feature matrix. - y: True labels. Returns: A PRCurveDisplay object. y_pred = estimator.predict_proba(X)[:, 1] return cls.from_predictions(y, y_pred, estimator.__class__.__name__) @classmethod def from_predictions(cls, y, y_pred, estimator_name): Create a PRCurveDisplay from true and predicted labels. Arguments: - y: True labels. - y_pred: Predicted probabilities. - estimator_name: Name of the estimator. Returns: A PRCurveDisplay object. precision, recall, _ = precision_recall_curve(y, y_pred) pr_auc = auc(recall, precision) return cls(precision, recall, pr_auc, estimator_name) def plot(self, ax=None, name=None, **kwargs): Plot the precision-recall curve. Arguments: - ax: Matplotlib axes object. - name: Optional name for the plot. - **kwargs: Additional plotting parameters. if ax is None: fig, ax = plt.subplots() ax.plot(self.recall, self.precision, lw=2, label=f\'{self.estimator_name} (AUC = {self.pr_auc:.2f}) {name}\') ax.set_xlabel(\'Recall\') ax.set_ylabel(\'Precision\') ax.set_title(\'Precision-Recall Curve\') ax.legend(loc=\\"best\\") self.ax_ = ax self.figure_ = ax.figure return self"},{"question":"**Challenging Python Coding Assessment Question:** # Task You are given two audio fragments, each of which is a bytes-like object containing 16-bit signed integer samples. Your task is to implement a function that: 1. Adjusts the volume of the first audio fragment by a given factor. 2. Converts the adjusted fragment to a u-LAW encoded format. 3. Adds a specified bias to the second audio fragment. 4. Converts the biased fragment to a-LAW encoded format. 5. Combines the two encoded fragments into a single stereo audio fragment. # Function Signature ```python def process_and_combine_audio(fragment1: bytes, factor: float, fragment2: bytes, bias: int) -> bytes: pass ``` # Input - `fragment1` (bytes): The first audio fragment containing 16-bit signed integer samples. - `factor` (float): The multiplication factor used to adjust the volume of `fragment1`. - `fragment2` (bytes): The second audio fragment containing 16-bit signed integer samples. - `bias` (int): The bias to add to each sample of `fragment2`. # Output - Returns a bytes object containing the combined stereo audio fragment (16-bit samples). # Steps 1. Adjust the volume of `fragment1` by multiplying each sample by `factor`. Truncate values to avoid overflow. 2. Convert the volume-adjusted fragment to u-LAW encoding. 3. Add the given `bias` to each sample in `fragment2`. Handle any wrap-around in case of overflow. 4. Convert the biased fragment to a-LAW encoding. 5. Combine the two encoded fragments into a single stereo fragment with `fragment1` as the left channel and `fragment2` as the right channel. # Constraints - Both input fragments (`fragment1` and `fragment2`) have the same length. - Ensure efficient handling of the bytes-like objects to maintain performance. # Example ```python # Example audio fragments with arbitrary values for illustration fragment1 = b\'x01x02x03x04\' fragment2 = b\'x05x06x07x08\' factor = 1.5 bias = 10 result = process_and_combine_audio(fragment1, factor, fragment2, bias) print(result) # Outputs the combined stereo audio fragment ``` # Note Make sure to handle potential exceptions and ensure all conversions and manipulations are performed as specified.","solution":"import audioop def process_and_combine_audio(fragment1, factor, fragment2, bias): Processes and combines two audio fragments into a stereo audio fragment. # Adjust volume of fragment1 adjusted_fragment1 = audioop.mul(fragment1, 2, factor) # Convert adjusted fragment to u-LAW ulaw_fragment1 = audioop.lin2ulaw(adjusted_fragment1, 2) # Add bias to fragment2 biased_fragment2 = audioop.bias(fragment2, 2, bias) # Convert biased fragment to a-LAW alaw_fragment2 = audioop.lin2alaw(biased_fragment2, 2) # Combine the two encoded fragments into a single stereo fragment combined_fragment = audioop.add(ulaw_fragment1, alaw_fragment2, 1) stereo_fragment = audioop.tostereo(combined_fragment, 1, 1.0, 1.0) return stereo_fragment"},{"question":"# PyTorch Type Info Extraction You are given information about two PyTorch classes: `torch.finfo` and `torch.iinfo`. These classes encapsulate the numerical properties of floating-point and integer data types, respectively. Your task is to write a function `extract_type_info` that accepts a PyTorch data type and returns a dictionary containing its numerical properties. Input - `dtype`: This input will be a PyTorch dtype object (such as `torch.float32`, `torch.int8`, etc.). Output - A dictionary containing the following keys and corresponding values: - For floating-point types (`torch.float32`, `torch.float64`, `torch.float16`, `torch.bfloat16`): - `\\"bits\\"`: The number of bits occupied by the type. - `\\"eps\\"`: The smallest representable number such that `1.0 + eps != 1.0`. - `\\"max\\"`: The largest representable number. - `\\"min\\"`: The smallest representable number. - `\\"tiny\\"`: The smallest positive normal number. - `\\"resolution\\"`: The approximate decimal resolution of this type. - For integer types (`torch.uint8`, `torch.int8`, `torch.int16`, `torch.int32`, `torch.int64`): - `\\"bits\\"`: The number of bits occupied by the type. - `\\"max\\"`: The largest representable number. - `\\"min\\"`: The smallest representable number. Constraints - The function should handle both floating-point and integer data types correctly. - You may assume the input dtype will be a valid PyTorch dtype. Example For floating-point type: ```python import torch dtype = torch.float32 print(extract_type_info(dtype)) ``` Output: ```python { \'bits\': 32, \'eps\': 1.1920928955078125e-07, \'max\': 3.4028234663852886e+38, \'min\': -3.4028234663852886e+38, \'tiny\': 1.1754943508222875e-38, \'resolution\': 1e-06 } ``` For integer type: ```python import torch dtype = torch.int16 print(extract_type_info(dtype)) ``` Output: ```python { \'bits\': 16, \'max\': 32767, \'min\': -32768 } ``` # Implementation Use the `torch.finfo` and `torch.iinfo` classes to extract the required type information. Here\'s the function signature to get you started: ```python import torch def extract_type_info(dtype): # Your implementation here pass ``` Good luck!","solution":"import torch def extract_type_info(dtype): Extracts numerical properties of the given PyTorch data type and returns them as a dictionary. Parameters: dtype (torch.dtype): The PyTorch data type (e.g., torch.float32, torch.int32, etc.) Returns: dict: Dictionary containing the numerical properties of the given data type. if dtype.is_floating_point: finfo = torch.finfo(dtype) return { \\"bits\\": finfo.bits, \\"eps\\": finfo.eps, \\"max\\": finfo.max, \\"min\\": finfo.min, \\"tiny\\": finfo.tiny, \\"resolution\\": finfo.resolution } else: iinfo = torch.iinfo(dtype) return { \\"bits\\": iinfo.bits, \\"max\\": iinfo.max, \\"min\\": iinfo.min }"},{"question":"You are required to create a Python module for the management of library books using dataclasses. # Requirements 1. **Book Class** - Create a dataclass `Book` with the following attributes: - `title` (string): The title of the book. - `author` (string): The author of the book. - `isbn` (string): The ISBN number of the book, which should be unique. - `copies` (int): The number of copies available in the library. - Include methods to: - `borrow()`: Decrement the `copies` value if at least one copy is available. If no copies are available, raise an exception. - `return_book()`: Increment the `copies` value. 2. **User Class** - Create a dataclass `User` with the following attributes: - `name` (string): The name of the user. - `borrowed_books` (list): A list of `Book` instances that the user has borrowed. Use a default factory to initialize this list. - Include methods to: - `borrow_book(book: Book)`: Add the book to the `borrowed_books` list and call the `borrow` method on the `Book` instance. - `return_book(book: Book)`: Remove the book from the `borrowed_books` list and call the `return_book` method on the `Book` instance. 3. **Library Class** - Create a dataclass `Library` with the following attributes: - `books` (dict): A dictionary mapping ISBN numbers (strings) to `Book` instances. Use a default factory to initialize this dictionary. - `users` (dict): A dictionary mapping user names (strings) to `User` instances. Use a default factory to initialize this dictionary. - Include methods to: - `add_book(book: Book)`: Add a `Book` instance to the `books` dictionary. - `add_user(user: User)`: Add a `User` instance to the `users` dictionary. - `borrow_book(user_name: str, isbn: str)`: Allow a user to borrow a book by their name and the book\'s ISBN number. - `return_book(user_name: str, isbn: str)`: Allow a user to return a book by their name and the book\'s ISBN number. - `replace_book(isbn: str, **changes)`: Use the `replace` function to create a new `Book` instance with updated attributes and replace the existing instance in the `books` dictionary. # Constraints - Ensure no duplicate ISBN numbers for books. - Ensure user names are unique. # Example Usage ```python from dataclasses import dataclass, field, replace from typing import List, Dict # Your code here # Initialize library library = Library() # Add books book1 = Book(title=\\"1984\\", author=\\"George Orwell\\", isbn=\\"1234567890\\", copies=3) book2 = Book(title=\\"To Kill a Mockingbird\\", author=\\"Harper Lee\\", isbn=\\"1234567891\\", copies=2) library.add_book(book1) library.add_book(book2) # Add user user1 = User(name=\\"Alice\\") library.add_user(user1) # Borrow book library.borrow_book(\\"Alice\\", \\"1234567890\\") # Return book library.return_book(\\"Alice\\", \\"1234567890\\") # Replace book library.replace_book(\\"1234567890\\", title=\\"Nineteen Eighty-Four\\", author=\\"George Orwell\\") ``` # Notes - Implement proper error checking to handle cases like borrowing a book that doesn\'t exist, returning a book that wasn\'t borrowed, etc. - Use appropriate Python exception handling for these error cases.","solution":"from dataclasses import dataclass, field, replace from typing import List, Dict @dataclass class Book: title: str author: str isbn: str copies: int def borrow(self): if self.copies > 0: self.copies -= 1 else: raise Exception(\\"No copies available\\") def return_book(self): self.copies += 1 @dataclass class User: name: str borrowed_books: List[Book] = field(default_factory=list) def borrow_book(self, book: Book): book.borrow() self.borrowed_books.append(book) def return_book(self, book: Book): if book in self.borrowed_books: book.return_book() self.borrowed_books.remove(book) else: raise Exception(\\"This book has not been borrowed by this user\\") @dataclass class Library: books: Dict[str, Book] = field(default_factory=dict) users: Dict[str, User] = field(default_factory=dict) def add_book(self, book: Book): if book.isbn in self.books: raise Exception(\\"Book with this ISBN already exists\\") self.books[book.isbn] = book def add_user(self, user: User): if user.name in self.users: raise Exception(\\"User with this name already exists\\") self.users[user.name] = user def borrow_book(self, user_name: str, isbn: str): if user_name not in self.users: raise Exception(\\"User does not exist\\") if isbn not in self.books: raise Exception(\\"Book does not exist\\") user = self.users[user_name] book = self.books[isbn] user.borrow_book(book) def return_book(self, user_name: str, isbn: str): if user_name not in self.users: raise Exception(\\"User does not exist\\") if isbn not in self.books: raise Exception(\\"Book does not exist\\") user = self.users[user_name] book = self.books[isbn] user.return_book(book) def replace_book(self, isbn: str, **changes): if isbn not in self.books: raise Exception(\\"Book does not exist\\") old_book = self.books[isbn] new_book = replace(old_book, **changes) self.books[isbn] = new_book"},{"question":"# XML Budget Tracker You are given an XML file containing multiple financial transactions. Each transaction is represented as an `<item>` element with attributes `type` (either `income` or `expense`), `amount`, and `category`. Here is a sample structure of the XML: ```xml <transactions> <item type=\\"income\\" amount=\\"2000\\" category=\\"salary\\"/> <item type=\\"expense\\" amount=\\"150\\" category=\\"groceries\\"/> <item type=\\"expense\\" amount=\\"500\\" category=\\"rent\\"/> <!-- More items --> </transactions> ``` Your task is to write a Python function `generate_summary(file_path: str) -> dict` that parses the XML file using the `xml.dom.pulldom` module and returns a summary dictionary. The summary should contain the total `income`, total `expenses`, and a breakdown of expenses by category. For example, if the XML file contains the above data, the function should return: ```python { \\"total_income\\": 2000, \\"total_expenses\\": 650, \\"expenses_by_category\\": { \\"groceries\\": 150, \\"rent\\": 500 } } ``` # Constraints: 1. You should use the `xml.dom.pulldom` module for parsing the XML. 2. Assume the XML file is well-formed. 3. The function should handle a large number of transactions efficiently. # Function Skeleton: ```python def generate_summary(file_path: str) -> dict: from xml.dom import pulldom # Initialize the summary dictionary summary = { \\"total_income\\": 0, \\"total_expenses\\": 0, \\"expenses_by_category\\": {} } # Parse the XML and fill in the summary details doc = pulldom.parse(file_path) for event, node in doc: if event == pulldom.START_ELEMENT and node.tagName == \'item\': # Implement the logic to handle \'income\' and \'expense\' items return summary ``` # Your Tasks: 1. Complete the `generate_summary()` function to parse the given XML file. 2. Use the `pulldom.parse()` method to parse the XML. 3. Accumulate the totals for income and expenses. 4. Accumulate expenses by category. # Testing: You can test your implementation with an XML file named `transactions.xml` containing various transactions. ```python assert generate_summary(\'transactions.xml\') == { \\"total_income\\": 2000, \\"total_expenses\\": 650, \\"expenses_by_category\\": { \\"groceries\\": 150, \\"rent\\": 500 } } ``` Good luck!","solution":"def generate_summary(file_path: str) -> dict: from xml.dom import pulldom # Initialize the summary dictionary summary = { \\"total_income\\": 0, \\"total_expenses\\": 0, \\"expenses_by_category\\": {} } # Parse the XML and fill in the summary details doc = pulldom.parse(file_path) for event, node in doc: if event == pulldom.START_ELEMENT and node.tagName == \'item\': node_type = node.getAttribute(\'type\') node_amount = float(node.getAttribute(\'amount\')) node_category = node.getAttribute(\'category\') if node_type == \'income\': summary[\\"total_income\\"] += node_amount elif node_type == \'expense\': summary[\\"total_expenses\\"] += node_amount if node_category in summary[\\"expenses_by_category\\"]: summary[\\"expenses_by_category\\"][node_category] += node_amount else: summary[\\"expenses_by_category\\"][node_category] = node_amount return summary"},{"question":"# Problem Description You are tasked with simulating a simplified version of Python\'s initialization configuration using purely Python code. You need to implement a configuration class that captures various settings and handles them according to specified rules. # Requirements 1. **Configuration Class**: Implement a class `MyPythonConfig` with the following fields: - `program_name`: The name of the program. - `argv`: Command-line arguments as a list of strings. - `utf8_mode`: Boolean indicating if UTF-8 mode is enabled. - `use_environment`: Boolean indicating if environment variables should be used. - `verbose`: Integer representing verbosity level (0, 1, or 2). 2. **Initialization**: - Implement an `initialize(self)` method that sets default values and processes command-line arguments. 3. **Field Methods**: - `set_program_name(self, name: str)`: Sets the program name. - `set_argv(self, argv: list)`: Sets the command-line arguments. - `enable_utf8_mode(self)`: Enables UTF-8 mode. - `disable_environment_usage(self)`: Disables the usage of environment variables. - `set_verbose_level(self, level: int)`: Sets the verbosity level. 4. **Error Handling**: Implement proper error handling for invalid verbosity levels (i.e., levels not within {0, 1, 2}) by raising a `ValueError`. # Input and Output - The `initialize(self)` method should print the configured values after processing the arguments. # Example ```python class MyPythonConfig: # Implementation of the class as per requirements # Example usage config = MyPythonConfig() config.set_program_name(\\"MyTestProgram\\") config.set_argv([\\"-O\\", \\"test_script.py\\"]) config.enable_utf8_mode() config.disable_environment_usage() config.set_verbose_level(1) config.initialize() ``` **Expected Output**: ``` Program Name: MyTestProgram Argv: [\'-O\', \'test_script.py\'] UTF-8 Mode: True Use Environment: False Verbose Level: 1 ``` # Constraints - The verbosity level must be an integer within {0, 1, 2}. # Additional Information Ensure that your class methods accurately reflect changes in the configuration and handle errors gracefully with informative messages.","solution":"class MyPythonConfig: def __init__(self): self.program_name = \\"\\" self.argv = [] self.utf8_mode = False self.use_environment = True self.verbose = 0 def initialize(self): # Print the configured values after processing the arguments print(f\'Program Name: {self.program_name}\') print(f\'Argv: {self.argv}\') print(f\'UTF-8 Mode: {self.utf8_mode}\') print(f\'Use Environment: {self.use_environment}\') print(f\'Verbose Level: {self.verbose}\') def set_program_name(self, name: str): self.program_name = name def set_argv(self, argv: list): self.argv = argv def enable_utf8_mode(self): self.utf8_mode = True def disable_environment_usage(self): self.use_environment = False def set_verbose_level(self, level: int): if level not in {0, 1, 2}: raise ValueError(f\'Invalid verbose level: {level}\') self.verbose = level"},{"question":"**Title:** Function and Class Definition Validation in Python Syntax **Question:** You are tasked with writing a Python program that validates the syntax of function and class definitions from a given input string. Your task is to implement two functions that check if the input strings for function and class definitions adhere to the Python syntactic rules. # Function Specifications `validate_function_definition(definition: str) -> bool` **Parameters:** - `definition` (str): A string representing a Python function definition. **Returns:** - `bool`: `True` if the function definition is syntactically correct, `False` otherwise. **Constraints:** - The function definition may include decorators, parameters with or without defaults, and a return type annotation. - The definition should end with a colon followed by an indented block. `validate_class_definition(definition: str) -> bool` **Parameters:** - `definition` (str): A string representing a Python class definition. **Returns:** - `bool`: `True` if the class definition is syntactically correct, `False` otherwise. **Constraints:** - The class definition may include decorators, and can inherit from multiple base classes. - The definition should end with a colon followed by an indented block. # Examples ```python # Example 1: func_def = \'\'\' @decorator def example_function(param1: int, param2: str = \'default\') -> None: pass \'\'\' print(validate_function_definition(func_def)) # Output: True # Example 2: class_def = \'\'\' class ExampleClass(Base1, Base2): def __init__(self, value): self.value = value \'\'\' print(validate_class_definition(class_def)) # Output: True # Example 3: invalid_func_def = \'\'\' def invalid_function(param): return param \'\'\' print(validate_function_definition(invalid_func_def)) # Output: False # Example 4: invalid_class_def = \'\'\' class InvalidClass: def method(self): pass \'\'\' print(validate_class_definition(invalid_class_def)) # Output: False ``` # Requirements: - Ensure that your solution is efficient and can handle reasonably lengthy input strings. - Use the provided grammar to identify correct and incorrect syntax structures. - You may assume input strings are stripped of leading/trailing whitespace. Good luck!","solution":"import ast def validate_function_definition(definition: str) -> bool: Validates the syntax of a Python function definition. Returns True if the function definition is syntactically correct, False otherwise. try: parsed = ast.parse(definition) for node in parsed.body: if not isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)): return False return True except SyntaxError: return False def validate_class_definition(definition: str) -> bool: Validates the syntax of a Python class definition. Returns True if the class definition is syntactically correct, False otherwise. try: parsed = ast.parse(definition) for node in parsed.body: if not isinstance(node, ast.ClassDef): return False return True except SyntaxError: return False"},{"question":"# **Coding Assessment Question** **Objective** You are required to implement a function using Python\'s `graphlib.TopologicalSorter` class. The function will process a graph representing a set of tasks with dependencies and return a sequence in which these tasks can be completed. **Problem Statement** Write a function `find_task_order(tasks)` that takes a dictionary representing a graph of tasks and their dependencies and returns a list of tasks in a valid topological order. If the graph contains cycles, the function should raise a `CycleError`. **Function Signature** ```python from typing import Dict, List def find_task_order(tasks: Dict[str, List[str]]) -> List[str]: pass ``` **Input** - `tasks`: A dictionary where keys are task names (strings) and values are lists of predecessor task names (strings). Each task name is unique. **Output** - A list of task names (strings) in a valid topological order. **Example** ```python tasks = { \\"D\\": [\\"B\\", \\"C\\"], \\"C\\": [\\"A\\"], \\"B\\": [\\"A\\"], \\"A\\": [] } result = find_task_order(tasks) print(result) # Output could be [\'A\', \'C\', \'B\', \'D\'] or [\'A\', \'B\', \'C\', \'D\'] ``` **Constraints and Notes** - The graph represented by the input dictionary is guaranteed to be a Directed Acyclic Graph (DAG) unless stated otherwise. - The order of tasks in the output list should respect the dependencies provided. - If any cycles are detected, the function should raise a `CycleError`. - You can assume that the function will only be tested with graphs containing no more than 10,000 tasks and their dependencies. **Implementation Requirements** You must use the `graphlib.TopologicalSorter` class to solve this problem. Do not use any other topological sorting algorithms or libraries. ```python def find_task_order(tasks: Dict[str, List[str]]) -> List[str]: from graphlib import TopologicalSorter, CycleError ts = TopologicalSorter(tasks) try: return list(ts.static_order()) except CycleError as e: raise CycleError(f\\"A cycle was detected in the graph: {e}\\") from e ```","solution":"def find_task_order(tasks): Find a valid topological order of tasks given their dependencies. :param tasks: A dictionary where keys are task names and values are lists of predecessor task names. :return: A list of task names in a valid topological order. :raises CycleError: If the graph contains cycles. from graphlib import TopologicalSorter, CycleError ts = TopologicalSorter(tasks) try: return list(ts.static_order()) except CycleError as e: raise CycleError(f\\"A cycle was detected in the graph: {e}\\") from e"},{"question":"# Categorical Data Analysis with Pandas **Objective:** To evaluate your understanding of pandas categorical data manipulation, you will be implementing functions that handle categorical data in various aspects of processing, from creation to analysis. # Instructions: 1. **Create and Manipulate Categorical Data:** Implement a function `create_and_manipulate_categorical(df: pd.DataFrame) -> pd.DataFrame`, which takes a DataFrame that has at least one column with string data representing categories. The function should: - Convert the categorical string column into a categorical datatype. - Add a new category to the column and ensure the new category is included. - Rename the categories with the prefix `Category_`. 2. **Analyze Categorical Data:** Implement a function `analyze_categorical_data(df: pd.DataFrame, col: str) -> Dict[str, Any]`, which takes a DataFrame and a column name that is of categorical type. The function should return a dictionary containing: - Count of each category. - The category with the maximum occurrences. - The category with the minimum occurrences (excluding categories with zero occurrences). - List of categories sorted by their logical order. - Whether the categorical data is ordered. 3. **Merge Categorical DataFrames:** Implement a function `merge_categorical_dfs(df1: pd.DataFrame, df2: pd.DataFrame) -> pd.DataFrame`, which takes two DataFrames with identical categorical columns and merges them into one DataFrame, ensuring that the merged DataFrame maintains the categorical type. # Requirements: - Do not use any global variables; all data should be passed and returned; functions should be pure. - Ensure to handle edge cases, including cases with missing data (NaN). - For counting and comparison of categories, do not include NaN values. # Constraints: - The input DataFrames are guaranteed to have at least one categorical column. - The name of the categorical column for the merge operation will be the same in both DataFrames. # Example Usage: ```python import pandas as pd # Example DataFrames df1 = pd.DataFrame({ \'Category\': [\\"a\\", \\"b\\", \\"c\\", \\"a\\", \\"b\\"], \'Values\': [1, 2, 3, 4, 5] }) df2 = pd.DataFrame({ \'Category\': [\\"b\\", \\"c\\", \\"c\\", \\"b\\", \\"a\\"], \'Values\': [5, 4, 3, 2, 1] }) # Before running the function print(df1) print(df2) # Perform categorical data manipulation df1_managed = create_and_manipulate_categorical(df1) df2_managed = create_and_manipulate_categorical(df2) print(df1_managed) print(df2_managed) # Analyze the categorical data analysis_result = analyze_categorical_data(df1_managed, \'Category\') print(analysis_result) # Merge the DataFrames merged_df = merge_categorical_dfs(df1_managed, df2_managed) print(merged_df) ```","solution":"import pandas as pd from typing import Dict, Any def create_and_manipulate_categorical(df: pd.DataFrame) -> pd.DataFrame: Converts a string column in the DataFrame to a categorical column, adds a new category \'new_category\' to it, and renames all categories with a prefix \'Category_\'. # Assuming the first column with string data is the one to be converted. str_column = df.select_dtypes(include=\'object\').columns[0] # Convert the column to categorical type df[str_column] = df[str_column].astype(\'category\') # Add new category new_categories = list(df[str_column].cat.categories) + [\'new_category\'] df[str_column] = df[str_column].cat.set_categories(new_categories) # Rename categories updated_categories = {cat: f\'Category_{cat}\' for cat in new_categories} df[str_column] = df[str_column].cat.rename_categories(updated_categories) return df def analyze_categorical_data(df: pd.DataFrame, col: str) -> Dict[str, Any]: Analyzes the categorical data in the given column of the DataFrame and returns various statistics including count of each category, max/min occurrences, sorted list of categories, and whether the column is ordered. categorical_data = df[col] analysis = { \'count\': categorical_data.value_counts(dropna=True).to_dict(), \'max_occurrence\': categorical_data.value_counts().idxmax(), \'min_occurrence\': categorical_data.value_counts().loc[lambda x: x > 0].idxmin(), \'sorted_categories\': sorted(categorical_data.cat.categories), \'is_ordered\': categorical_data.cat.ordered } return analysis def merge_categorical_dfs(df1: pd.DataFrame, df2: pd.DataFrame) -> pd.DataFrame: Merges two DataFrames with identical categorical columns and ensures the merged DataFrame maintains the categorical type. # Find the categorical column name categorical_columns_df1 = df1.select_dtypes(include=\'category\').columns categorical_columns_df2 = df2.select_dtypes(include=\'category\').columns # Take the common categorical column name cat_col = list(set(categorical_columns_df1) & set(categorical_columns_df2))[0] # Merge DataFrames merged_df = pd.concat([df1, df2], ignore_index=True) # Ensure the merged column maintains the categorical type and categories are unioned merged_categories = df1[cat_col].cat.categories.union(df2[cat_col].cat.categories) merged_df[cat_col] = merged_df[cat_col].cat.set_categories(merged_categories) return merged_df"},{"question":"# Complex Number Operations in Python In this coding assessment, you will demonstrate your understanding of handling complex numbers as explained in the provided documentation by performing various operations on them. You are expected to implement functions that perform the addition, subtraction, multiplication, division, and exponentiation of complex numbers using both the C structure representation and Python object representation. Part 1: Implement Complex Number Operations in Python 1. **Complex Number Addition** ```python def complex_add(left: complex, right: complex) -> complex: Add two complex numbers. Args: left (complex): The first complex number. right (complex): The second complex number. Returns: complex: The sum of the two complex numbers. pass ``` 2. **Complex Number Subtraction** ```python def complex_subtract(left: complex, right: complex) -> complex: Subtract the second complex number from the first. Args: left (complex): The first complex number. right (complex): The second complex number. Returns: complex: The result of the subtraction. pass ``` 3. **Complex Number Multiplication** ```python def complex_multiply(left: complex, right: complex) -> complex: Multiply two complex numbers. Args: left (complex): The first complex number. right (complex): The second complex number. Returns: complex: The product of the two complex numbers. pass ``` 4. **Complex Number Division** ```python def complex_divide(left: complex, right: complex) -> complex: Divide the first complex number by the second. Args: left (complex): The first complex number. right (complex): The second complex number. Returns: complex: The result of the division. pass ``` 5. **Complex Number Exponentiation** ```python def complex_exponentiate(base: complex, exp: complex) -> complex: Raise the base complex number to the power of the exponent complex number. Args: base (complex): The base complex number. exp (complex): The exponent complex number. Returns: complex: The result of the exponentiation. pass ``` Part 2: Convert Between Python and C Representations 1. **Convert Py_complex to Python Complex Object** ```python def pycomplex_to_python(py_complex) -> complex: Convert a Py_complex structure to a Python complex object. Args: py_complex: A dictionary with keys \'real\' and \'imag\' representing the real and imaginary parts. Returns: complex: The corresponding Python complex object. pass ``` 2. **Convert Python Complex Object to Py_complex** ```python def python_to_pycomplex(cplx: complex) -> dict: Convert a Python complex object to a Py_complex structure. Args: cplx (complex): A Python complex object. Returns: dict: A dictionary with keys \'real\' and \'imag\' representing the real and imaginary parts. pass ``` Constraints: - Do not use any external libraries for complex number operations other than what is provided by the standard Python library. - Handle edge cases where necessary, especially for operations like division and exponentiation that might raise errors. Example Usage: ```python # Part 1 result = complex_add(complex(2, 3), complex(1, -1)) print(result) # Expected: (3+2j) # Part 2 py_cplx = {\'real\': 2.0, \'imag\': 3.0} cplx_obj = pycomplex_to_python(py_cplx) print(cplx_obj) # Expected: (2+3j) ```","solution":"def complex_add(left: complex, right: complex) -> complex: Add two complex numbers. Args: left (complex): The first complex number. right (complex): The second complex number. Returns: complex: The sum of the two complex numbers. return left + right def complex_subtract(left: complex, right: complex) -> complex: Subtract the second complex number from the first. Args: left (complex): The first complex number. right (complex): The second complex number. Returns: complex: The result of the subtraction. return left - right def complex_multiply(left: complex, right: complex) -> complex: Multiply two complex numbers. Args: left (complex): The first complex number. right (complex): The second complex number. Returns: complex: The product of the two complex numbers. return left * right def complex_divide(left: complex, right: complex) -> complex: Divide the first complex number by the second. Args: left (complex): The first complex number. right (complex): The second complex number. Returns: complex: The result of the division. return left / right def complex_exponentiate(base: complex, exp: complex) -> complex: Raise the base complex number to the power of the exponent complex number. Args: base (complex): The base complex number. exp (complex): The exponent complex number. Returns: complex: The result of the exponentiation. return base ** exp def pycomplex_to_python(py_complex) -> complex: Convert a Py_complex structure to a Python complex object. Args: py_complex: A dictionary with keys \'real\' and \'imag\' representing the real and imaginary parts. Returns: complex: The corresponding Python complex object. return complex(py_complex[\'real\'], py_complex[\'imag\']) def python_to_pycomplex(cplx: complex) -> dict: Convert a Python complex object to a Py_complex structure. Args: cplx (complex): A Python complex object. Returns: dict: A dictionary with keys \'real\' and \'imag\' representing the real and imaginary parts. return {\'real\': cplx.real, \'imag\': cplx.imag}"},{"question":"Objective You are tasked to develop a customizable and efficient logging system for a financial application that handles various transaction records. The system will be required to format and display logs, manage performance through threading, and accurately process financial data. Requirements 1. **Logging Module**: - Implement a logging system using Python\'s `logging` module. - The system should log various levels of messages (`DEBUG`, `INFO`, `WARNING`, `ERROR`, `CRITICAL`). - Log messages should be directed to both a file and the console. - Ensure the logging format includes timestamps, log level, and the message. 2. **Threading Module**: - The logging system should manage logs in a separate thread to not block the main application processes. - Implement a function to handle logging that runs as a background thread. 3. **Decimal Module**: - Use the `decimal.Decimal` class to process financial transactions accurately. - Implement a function to calculate and log the total amount of all transactions with their respective taxes applied. Ensure precision in financial calculations. 4. **Output Formatting**: - Use `pprint` to display the processed transactions in a well-readable format when printing on the console. - Ensure all transaction summaries are formatted neatly. Input and Output Input: - A list of dictionaries containing transaction details. Each dictionary will have: ```python transactions = [ {\\"id\\": 1, \\"amount\\": \\"12.49\\", \\"tax_rate\\": \\"0.07\\"}, {\\"id\\": 2, \\"amount\\": \\"23.99\\", \\"tax_rate\\": \\"0.05\\"}, # ... ] ``` Output: - Logs written to a file named `transactions.log` and printed to the console. - A summary of transactions printed neatly on the console using `pprint`. Additional Constraints - The system should be designed to handle a large number of transactions efficiently. - Ensure thread-safety when accessing shared resources. - Use `decimal.Decimal` for all financial amount calculations. Example Behavior ```python import logging import threading import time from decimal import Decimal from pprint import pprint # Function to initialize the logging system. def initialize_logger(): pass # Implement your logging setup here # Function to process transactions and log them. def process_transactions(transactions): pass # Implement your transaction processing here # Example main function to demonstrate the logging system. def main(): initialize_logger() transactions = [ {\\"id\\": 1, \\"amount\\": \\"12.49\\", \\"tax_rate\\": \\"0.07\\"}, {\\"id\\": 2, \\"amount\\": \\"23.99\\", \\"tax_rate\\": \\"0.05\\"}, # ... ] process_transactions(transactions) if __name__ == \\"__main__\\": main() ``` * **Logging details**: - Log should contain a detailed description of each transaction processing step. - Log errors if any transactions fail (e.g., invalid data). * **Threading Details**: - Ensure the logging does not block the main process. - Transactions processing should run concurrently with logging in a separate thread. * **Decimal Processing**: - Ensure precise calculations for each transaction. - Round results appropriately for financial records. # Evaluation Criteria - Correctness and precision in handling financial transactions. - Efficient use of threading to manage log operations. - Neatly formatted and easily readable transaction summaries. - Robust logging system with comprehensive message details and appropriate log levels.","solution":"import logging import threading from decimal import Decimal from pprint import pprint # Setting up the logger def initialize_logger(): logger = logging.getLogger(\'FinancialAppLogger\') logger.setLevel(logging.DEBUG) # Console handler console_handler = logging.StreamHandler() console_handler.setLevel(logging.DEBUG) # File handler file_handler = logging.FileHandler(\'transactions.log\') file_handler.setLevel(logging.DEBUG) # Formatter formatter = logging.Formatter(\'%(asctime)s - %(name)s - %(levelname)s - %(message)s\') console_handler.setFormatter(formatter) file_handler.setFormatter(formatter) # Adding handlers to logger logger.addHandler(console_handler) logger.addHandler(file_handler) return logger # Processing transactions in a separate thread def log_transactions(transactions, logger): logger.debug(\\"Starting transaction processing\\") results = [] for transaction in transactions: try: logger.info(f\\"Processing transaction id: {transaction[\'id\']}\\") amount = Decimal(transaction[\'amount\']) tax_rate = Decimal(transaction[\'tax_rate\']) total_amount = amount * (1 + tax_rate) transaction[\'total\'] = str(total_amount.quantize(Decimal(\'0.01\'))) results.append(transaction) logger.info(f\\"Processed transaction id: {transaction[\'id\']}, total amount: {transaction[\'total\']}\\") except Exception as e: logger.error(f\\"Failed to process transaction id: {transaction[\'id\']}: {e}\\") pprint(results) logger.debug(\\"Finished transaction processing\\") def process_transactions(transactions): logger = initialize_logger() log_thread = threading.Thread(target=log_transactions, args=(transactions, logger)) log_thread.start() log_thread.join()"},{"question":"**Question: Implementing a Custom Initialization Strategy in PyTorch** In this exercise, you will demonstrate your understanding of random number generation and neural network weight initialization in PyTorch. The goal is to implement a custom weight initialization strategy for a neural network using the `torch.random` module. # Instructions: 1. **Custom Normal Distribution Initialization:** Implement a function `custom_normal_init` that initializes a given PyTorch neural network\'s weights from a normal distribution with mean `mu` and standard deviation `sigma`. 2. **Xavier Initialization:** Implement a function `xavier_init` that initializes the network’s weights using the Xavier initialization method, which is widely used for training deep neural networks to keep the scale of the gradients roughly the same in all layers. Here\'s what you need to do: 1. **Input:** - `model`: A PyTorch neural network model whose weights need to be initialized. - `mu` (only for `custom_normal_init`): Mean of the normal distribution. - `sigma` (only for `custom_normal_init`): Standard deviation of the normal distribution. 2. **Output:** - The neural network with initialized weights. # Constraints: - Only initialize weights for layers that have weights (e.g., `nn.Linear`, `nn.Conv2d`). # Example Usage: ```python import torch import torch.nn as nn import torch.nn.functional as F # Define a simple neural network class SimpleNN(nn.Module): def __init__(self): super(SimpleNN, self).__init__() self.fc1 = nn.Linear(10, 5) self.fc2 = nn.Linear(5, 2) def forward(self, x): x = F.relu(self.fc1(x)) x = self.fc2(x) return x # Initialize the model model = SimpleNN() # Custom normal initialization custom_normal_init(model, mu=0.0, sigma=0.02) # Xavier initialization xavier_init(model) ``` # Function signatures: ```python def custom_normal_init(model: nn.Module, mu: float, sigma: float): Initialize model weights with a normal distribution Args: model (nn.Module): PyTorch model to initialize mu (float): Mean of the normal distribution sigma (float): Standard deviation of the normal distribution pass # Your implementation here def xavier_init(model: nn.Module): Initialize model weights using Xavier initialization Args: model (nn.Module): PyTorch model to initialize pass # Your implementation here ``` # Requirements: 1. Use `torch.random` for generating random numbers. 2. Ensure that both functions correctly identify and initialize only the relevant layers. Note: This question assesses your understanding of PyTorch, especially in terms of practical application within neural network contexts through random number generation and initialization concepts.","solution":"import torch def custom_normal_init(model: torch.nn.Module, mu: float, sigma: float): Initialize model weights with a normal distribution Args: model (torch.nn.Module): PyTorch model to initialize mu (float): Mean of the normal distribution sigma (float): Standard deviation of the normal distribution for m in model.modules(): if isinstance(m, (torch.nn.Linear, torch.nn.Conv2d)): torch.nn.init.normal_(m.weight, mean=mu, std=sigma) if m.bias is not None: torch.nn.init.constant_(m.bias, 0) def xavier_init(model: torch.nn.Module): Initialize model weights using Xavier initialization Args: model (torch.nn.Module): PyTorch model to initialize for m in model.modules(): if isinstance(m, (torch.nn.Linear, torch.nn.Conv2d)): torch.nn.init.xavier_uniform_(m.weight) if m.bias is not None: torch.nn.init.constant_(m.bias, 0)"},{"question":"Objective: To assess a student\'s understanding of PyTorch\'s `torch.finfo` and `torch.iinfo` classes, and their ability to apply this information in a practical coding task related to numerical properties of data types. Problem Statement: Write a Python function using PyTorch that evaluates and returns a summary of the numerical properties of given data types. The function should handle both floating-point and integer types. Function Signature: ```python def summarize_dtype_properties(dtypes: List[torch.dtype]) -> Dict[str, Dict[str, float]]: pass ``` Input: - `dtypes`: A list of `torch.dtype` objects representing the data types to be summarized. The list may include both floating-point and integer types. Output: - A dictionary where the keys are the string representations of the data types, and the values are dictionaries themselves. These inner dictionaries should contain the following numerical properties: - For floating-point types (`torch.finfo`): - `bits` - `eps` - `max` - `min` - `tiny` - `resolution` - For integer types (`torch.iinfo`): - `bits` - `max` - `min` The exact type of values for each numerical property should be `float` even if it represents the integer type properties. Constraints: - Use `torch.finfo` for floating-point types. - Use `torch.iinfo` for integer types. - Ensure that the summary includes all the specified properties correctly. Example: ```python import torch from typing import List, Dict def summarize_dtype_properties(dtypes: List[torch.dtype]) -> Dict[str, Dict[str, float]]: summary = {} for dtype in dtypes: if dtype in [torch.float16, torch.float32, torch.float64, torch.bfloat16]: info = torch.finfo(dtype) summary[str(dtype)] = { \\"bits\\": float(info.bits), \\"eps\\": float(info.eps), \\"max\\": float(info.max), \\"min\\": float(info.min), \\"tiny\\": float(info.tiny), \\"resolution\\": float(info.resolution) } elif dtype in [torch.uint8, torch.int8, torch.int16, torch.int32, torch.int64]: info = torch.iinfo(dtype) summary[str(dtype)] = { \\"bits\\": float(info.bits), \\"max\\": float(info.max), \\"min\\": float(info.min) } return summary # Example usage dtypes = [torch.float32, torch.int64] print(summarize_dtype_properties(dtypes)) ``` Expected output: ```python { \'torch.float32\': { \'bits\': 32.0, \'eps\': 1.1920928955078125e-07, \'max\': 3.4028234663852886e+38, \'min\': -3.4028234663852886e+38, \'tiny\': 1.1754943508222875e-38, \'resolution\': 1e-07 }, \'torch.int64\': { \'bits\': 64.0, \'max\': 9223372036854775807.0, \'min\': -9223372036854775808.0 } } ``` Note: - Ensure to handle both floating-point and integer data types separately using appropriate classes (`torch.finfo` and `torch.iinfo` respectively). - The summary should be detailed and accurate, reflecting the properties mentioned for each data type.","solution":"import torch from typing import List, Dict def summarize_dtype_properties(dtypes: List[torch.dtype]) -> Dict[str, Dict[str, float]]: summary = {} for dtype in dtypes: if dtype in [torch.float16, torch.float32, torch.float64, torch.bfloat16]: info = torch.finfo(dtype) summary[str(dtype)] = { \\"bits\\": float(info.bits), \\"eps\\": float(info.eps), \\"max\\": float(info.max), \\"min\\": float(info.min), \\"tiny\\": float(info.tiny), \\"resolution\\": float(info.resolution) } elif dtype in [torch.uint8, torch.int8, torch.int16, torch.int32, torch.int64]: info = torch.iinfo(dtype) summary[str(dtype)] = { \\"bits\\": float(info.bits), \\"max\\": float(info.max), \\"min\\": float(info.min) } return summary"},{"question":"Emulating Legacy Import in Modern Python In this task, you are required to implement a function named `custom_import` that emulates the behavior of the legacy `imp` module\'s import mechanism using the `importlib` module. This will test your understanding of module handling and modern Python import mechanisms. Function Signature ```python def custom_import(name: str) -> object: Emulates the import mechanism of the deprecated `imp` module. Args: - name (str): The name of the module to be imported. Returns: - object: The imported module. Raises: - ImportError: If the module cannot be found or imported. ``` Requirements 1. **Finding the Module**: - Use `importlib.util.find_spec()` to search for the specified module. - If the module is not found, raise an `ImportError`. 2. **Loading the Module**: - Use `importlib.util.module_from_spec()` to create a new module based on the found specification. - Load the module using `importlib.util.spec.loader.exec_module()`. 3. **Cache Management**: - Ensure that the module is added to `sys.modules` after successful import. 4. **Module Reloading**: - Implement module reloading capability, so if the module is already imported, it should be reloaded using `importlib.reload()`. Constraints - Do not use any functions from the deprecated `imp` module. - The solution must handle submodules (e.g., `package.submodule`). - Ensure thread safety during the import process. Example Usage ```python import os # Assuming you have a Python file \'example_module.py\' with some content module = custom_import(\\"example_module\\") print(module) # Should print the module object # Modifying the source file (simulating an external change) with open(\'example_module.py\', \'w\') as f: f.write(\\"value = 42n\\") # Reloading the module reloaded_module = custom_import(\\"example_module\\") print(reloaded_module.value) # Should print the updated value, 42 ``` Good luck, and ensure your solution adheres to modern Python practices while matching the old `imp` module\'s functionality.","solution":"import importlib.util import importlib import sys import threading def custom_import(name: str) -> object: Emulates the import mechanism of the deprecated `imp` module. Args: - name (str): The name of the module to be imported. Returns: - object: The imported module. Raises: - ImportError: If the module cannot be found or imported. # Ensure thread safety during the import process lock = threading.Lock() with lock: # Check if the module is already in sys.modules if name in sys.modules: return importlib.reload(sys.modules[name]) # Find the module specification spec = importlib.util.find_spec(name) if spec is None: raise ImportError(f\\"Module \'{name}\' not found\\") # Create and load the module module = importlib.util.module_from_spec(spec) sys.modules[name] = module spec.loader.exec_module(module) return module"},{"question":"You have been tasked with writing a function that utilizes the `sched` module to simulate and schedule a series of events in a multi-threaded environment. Your function will schedule a series of print events with varying delays and priorities, and then run these events. **Objective:** Write a Python function `schedule_events(scheduler, event_defs)` that schedules and runs a series of events using a `sched.scheduler` instance. You are provided with a specific `scheduler` instance and a list of dictionaries `event_defs`, where each dictionary provides details about an event. ``` def schedule_events(scheduler, event_defs): \'\'\' Parameters: scheduler (sched.scheduler): An instance of sched.scheduler event_defs (list): A list of dictionaries. Each dictionary contains: - \'time\': The absolute time (float) at which the event should be scheduled (value from time.time()) - \'priority\': Integer representing the priority of the event - \'action\': Callable to be scheduled - \'argument\': Tuple of positional arguments for the action (default is ()) - \'kwargs\': Dictionary of keyword arguments for the action (default is {}) Returns: None \'\'\' # Your implementation here ``` **Input Format:** - `scheduler`: An instance of `sched.scheduler`. - `event_defs`: A list of dictionaries, each detailing an event to be scheduled. - Each dictionary contains: - `\'time\'` (float): The absolute time at which the event should be scheduled (compatible with `timefunc`). - `\'priority\'` (int): The priority of the event (lower number means higher priority). - `\'action\'` (callable): The action to be scheduled. - `\'argument\'` (tuple, optional): Positional arguments for the action. - `\'kwargs\'` (dict, optional): Keyword arguments for the action. **Output Format:** - The function does not return any value but will print outputs based on the scheduled actions. **Constraints:** - `time` values in `event_defs` will be non-negative. - `priority` values will be integers. - Actions provided in `event_defs` will be valid callable functions. - Ensure your function is thread-safe and appropriately uses the `scheduler` provided. **Example:** ```python import sched import time def print_message(message=\'default\'): print(f\\"{time.time()}: {message}\\") event_scheduler = sched.scheduler(time.time, time.sleep) events = [ {\'time\': time.time() + 10, \'priority\': 1, \'action\': print_message, \'argument\': (\'Event 1\',)}, {\'time\': time.time() + 5, \'priority\': 2, \'action\': print_message, \'argument\': (\'Event 2\',)}, {\'time\': time.time() + 8, \'priority\': 1, \'action\': print_message, \'argument\': (\'Event 3\',)}, {\'time\': time.time() + 8, \'priority\': 0, \'action\': print_message, \'argument\': (\'Event 4\',)}, ] def schedule_events(scheduler, event_defs): for event in event_defs: scheduler.enterabs(event[\'time\'], event[\'priority\'], event[\'action\'], event.get(\'argument\', ()), event.get(\'kwargs\', {})) scheduler.run() schedule_events(event_scheduler, events) ``` **Expected output (order may vary depending on actual current time and system scheduling):** ``` <current_time> + 5: Event 2 <current_time> + 8: Event 4 <current_time> + 8: Event 3 <current_time> + 10: Event 1 ``` Please ensure your code handles these cases correctly and is efficient in terms of scheduling and running the events.","solution":"import sched import time def schedule_events(scheduler, event_defs): Schedules and runs a series of events using a given `sched.scheduler` instance. Parameters: scheduler (sched.scheduler): An instance of sched.scheduler event_defs (list): A list of dictionaries, each containing details about an event. Returns: None for event in event_defs: scheduler.enterabs(event[\'time\'], event[\'priority\'], event[\'action\'], event.get(\'argument\', ()), event.get(\'kwargs\', {})) scheduler.run()"},{"question":"# Bytearray Manipulations You are asked to implement a Python function that utilizes the bytearray functionalities described in the provided documentation. The function should create a bytearray from a given string, concatenate it with another bytearray, resize the resulting bytearray, and return specific details about it. Function Signature ```python def manipulate_bytearrays(initial_string: str, concat_array: bytearray, new_size: int) -> dict: Manipulates provided bytearrays. Parameters: initial_string (str): The initial string to convert into a bytearray. concat_array (bytearray): The bytearray to concatenate with. new_size (int): The new size to resize the concatenated bytearray to. Returns: dict: A dictionary containing: \'final_bytearray\': The final resized bytearray. \'original_size\': The size of the bytearray after concatenation but before resizing. \'final_size\': The size of the bytearray after resizing. \'as_string\': The final bytearray\'s content as a string. pass ``` Requirements 1. Convert `initial_string` into a bytearray. 2. Concatenate this bytearray with `concat_array`. 3. Check the size of the concatenated bytearray. 4. Resize the concatenated bytearray to `new_size`. 5. Return a dictionary with the following details: * `final_bytearray`: The final resized bytearray. * `original_size`: The size of the bytearray after concatenation but before resizing. * `final_size`: The size of the bytearray after resizing. * `as_string`: The final bytearray\'s content as a string with an extra null byte appended. Constraints - The length of `initial_string` should be non-negative and can be empty. - The `new_size` should be a non-negative integer. - `concat_array` should be a valid bytearray. Example ```python result = manipulate_bytearrays(\\"Hello\\", bytearray(b\\" World!\\"), 8) print(result) # Output: # { # \'final_bytearray\': bytearray(b\'Hello Wo\'), # \'original_size\': 12, # \'final_size\': 8, # \'as_string\': \'Hello Wox00\' # } ``` Note: - `final_bytearray` contains the resized content. - `original_size` is the size before resizing. - `final_size` is the size after resizing. - `as_string` contains the final bytearray content as a string with an extra null byte appended. Use the C API functions and macros described in the provided documentation for your implementations, through appropriate Python interfaces.","solution":"def manipulate_bytearrays(initial_string: str, concat_array: bytearray, new_size: int) -> dict: Manipulates provided bytearrays. Parameters: initial_string (str): The initial string to convert into a bytearray. concat_array (bytearray): The bytearray to concatenate with. new_size (int): The new size to resize the concatenated bytearray to. Returns: dict: A dictionary containing: \'final_bytearray\': The final resized bytearray. \'original_size\': The size of the bytearray after concatenation but before resizing. \'final_size\': The size of the bytearray after resizing. \'as_string\': The final bytearray\'s content as a string. # Step 1: Convert initial_string into a bytearray initial_bytearray = bytearray(initial_string, \'utf-8\') # Step 2: Concatenate initial_bytearray with concat_array concatenated_bytearray = initial_bytearray + concat_array # Step 3: Check the size of the concatenated bytearray original_size = len(concatenated_bytearray) # Step 4: Resize the concatenated bytearray to new_size concatenated_bytearray = concatenated_bytearray[:new_size] # Check the size of the bytearray after resizing final_size = len(concatenated_bytearray) # Convert the final bytearray back to a string with an extra null byte appended as_string = concatenated_bytearray.decode(\'utf-8\', errors=\'ignore\') + \'x00\' # Return the dictionary with details return { \'final_bytearray\': concatenated_bytearray, \'original_size\': original_size, \'final_size\': final_size, \'as_string\': as_string }"},{"question":"**Title: Advanced List Manipulation** **Objective:** Write a function `process_lists(data: List[Tuple[str, List[int]]]) -> List[Tuple[str, int]]` that processes a list of tuples, where each tuple contains a name and a list of integers. The function should: 1. For each tuple, sort the list of integers in ascending order. 2. Remove any duplicate integers from the list. 3. Reverse the list. 4. Compute the sum of the processed list of integers. 5. Return a list of tuples, where each tuple contains the name and the sum of the processed list of integers. **Input Format:** - The function accepts a list of tuples. Each tuple consists of a string and a list of integers. ```python data = [(\\"Alice\\", [5, 3, 6, 3, 5]), (\\"Bob\\", [1, 4, 4, 1, 6]), (\\"Charlie\\", [7, 2, 5, 2, 9])] ``` **Output Format:** - The function should return a list of tuples, where each tuple contains the name and the sum of the processed list of integers. ```python result = [(\\"Alice\\", 14), (\\"Bob\\", 11), (\\"Charlie\\", 23)] ``` **Constraints:** - The input list will have at least one element. - Each list of integers will have at least one element. - The integers in the list can be positive or negative. **Example:** ```python def process_lists(data): # Function implementation data = [(\\"Alice\\", [5, 3, 6, 3, 5]), (\\"Bob\\", [1, 4, 4, 1, 6]), (\\"Charlie\\", [7, 2, 5, 2, 9])] print(process_lists(data)) # Output: [(\\"Alice\\", 14), (\\"Bob\\", 11), (\\"Charlie\\", 23)] ``` **Explanation:** For the input data `[(\\"Alice\\", [5, 3, 6, 3, 5]), (\\"Bob\\", [1, 4, 4, 1, 6]), (\\"Charlie\\", [7, 2, 5, 2, 9])]`: 1. **Alice:** - Sort: [3, 3, 5, 5, 6] - Remove duplicates: [3, 5, 6] - Reverse: [6, 5, 3] - Sum: 6 + 5 + 3 = 14 2. **Bob:** - Sort: [1, 1, 4, 4, 6] - Remove duplicates: [1, 4, 6] - Reverse: [6, 4, 1] - Sum: 6 + 4 + 1 = 11 3. **Charlie:** - Sort: [2, 2, 5, 7, 9] - Remove duplicates: [2, 5, 7, 9] - Reverse: [9, 7, 5, 2] - Sum: 9 + 7 + 5 + 2 = 23 The final output will be `[(\\"Alice\\", 14), (\\"Bob\\", 11), (\\"Charlie\\", 23)]`.","solution":"from typing import List, Tuple def process_lists(data: List[Tuple[str, List[int]]]) -> List[Tuple[str, int]]: processed_data = [] for name, numbers in data: # Step 1: Sort the list of integers in ascending order sorted_numbers = sorted(numbers) # Step 2: Remove any duplicate integers unique_numbers = list(dict.fromkeys(sorted_numbers)) # Step 3: Reverse the list reversed_numbers = unique_numbers[::-1] # Step 4: Compute the sum of the processed list of integers sum_of_numbers = sum(reversed_numbers) # Add the processed result to the list processed_data.append((name, sum_of_numbers)) return processed_data"},{"question":"**Clustering Analysis with Scikit-Learn** **Objective**: Implement, apply, and evaluate a clustering algorithm using scikit-learn. Problem Statement Given a dataset of points in a 2D space, your task is to: 1. Apply the K-Means clustering algorithm to group the data points. 2. Evaluate the performance of the clustering algorithm using an appropriate metric. 3. Visualize the clustering results and performance evaluation. Requirements 1. **Function Signature**: ```python def kmeans_clustering(data: np.ndarray, n_clusters: int) -> Tuple[np.ndarray, float, Any]: Perform K-Means clustering on the given data. Parameters: data (np.ndarray): A 2D numpy array of shape (n_samples, 2) where each row represents a point. n_clusters (int): The number of clusters to form. Returns: Tuple[np.ndarray, float, Any]: A tuple containing: - Cluster labels for each point. - Inertia of the clustering. - An appropriate clustering quality metric (e.g., silhouette score). pass ``` 2. **Input**: - `data`: A numpy array of shape `(n_samples, 2)` containing `n_samples` points in a 2D space. - `n_clusters`: An integer specifying the number of clusters. 3. **Output**: - A tuple containing: - A numpy array of shape `(n_samples,)` with the cluster labels assigned to each point. - A float representing the inertia of the clustering. - A float representing the silhouette score of the clustering. 4. **Constraints**: - You **must** use `scikit-learn`\'s KMeans component for clustering. - You **must** evaluate the clustering quality using the **Silhouette Coefficient**. 5. **Performance**: - The function should be efficient and able to handle up to 10,000 points within a reasonable time. Example Usage ```python import numpy as np # Example data data = np.array([[1.0, 2.0], [1.5, 1.8], [5.0, 8.0], [8.0, 8.0], [1.0, 0.6], [9.0, 11.0]]) n_clusters = 2 # Function call labels, inertia, silhouette_score = kmeans_clustering(data, n_clusters) print(\\"Cluster Labels:\\", labels) print(\\"Inertia:\\", inertia) print(\\"Silhouette Score:\\", silhouette_score) ``` Additional Tasks (Optional) 1. **Visualization**: Visualize the clustered data points and their corresponding cluster centroids. - You can use any visualization library like `matplotlib` to create the plot. 2. **Experimentation**: Try different numbers of clusters and observe the changes in inertia and silhouette scores. What can you infer from these changes?","solution":"from typing import Tuple, Any import numpy as np from sklearn.cluster import KMeans from sklearn.metrics import silhouette_score def kmeans_clustering(data: np.ndarray, n_clusters: int) -> Tuple[np.ndarray, float, Any]: Perform K-Means clustering on the given data. Parameters: data (np.ndarray): A 2D numpy array of shape (n_samples, 2) where each row represents a point. n_clusters (int): The number of clusters to form. Returns: Tuple[np.ndarray, float, Any]: A tuple containing: - Cluster labels for each point. - Inertia of the clustering. - An appropriate clustering quality metric (e.g., silhouette score). kmeans = KMeans(n_clusters=n_clusters, random_state=42) kmeans.fit(data) labels = kmeans.labels_ inertia = kmeans.inertia_ silhouette_avg = silhouette_score(data, labels) return labels, inertia, silhouette_avg"},{"question":"# Advanced Terminal Control using Python\'s `tty` Module Objective Your task is to write a Python function that demonstrates the use of the `tty` module for setting a terminal to `raw` and `cbreak` modes, and switching between these modes. This function should simulate a scenario where it reads character input from the user in these modes without echoing, and exits when the user presses a specific key. Function Signature ```python def terminal_control(): pass ``` Requirements 1. The function should set the terminal to `raw` mode initially. 2. In `raw` mode: - Read characters one by one from the user input without echoing them to the terminal. - Exit if the user presses the \'q\' key. 3. The function should then change the terminal to `cbreak` mode. 4. In `cbreak` mode: - Read characters one by one from the user input without echoing them to the terminal. - Exit if the user presses the \'q\' key. 5. Handle exceptions gracefully and restore the terminal to its original mode before exiting. Constraints - This function will run only on Unix-like systems. - Use of the `tty` and `termios` modules is mandatory. - Ensure that the terminal\'s original state is restored before the function exits, whether due to user input or an error. Example ```python terminal_control() ``` When you run the `terminal_control()` function, it should read characters in `raw` mode first, and upon pressing \'q\', it should switch to `cbreak` mode, again reading characters until \'q\' is pressed in this mode too, at which point it exits. Use the `tty.setraw(fd)` and `tty.setcbreak(fd)` functions to achieve the required terminal control. Additional Information - You can use `sys.stdin.fileno()` to get the file descriptor for `stdin`. - Use `os.read()` for reading a single byte from `stdin`. - The `termios` module provides `termios.tcgetattr()` and `termios.tcsetattr()` for getting and setting terminal attributes, which will be useful for restoring the terminal state.","solution":"import sys import tty import termios import os def terminal_control(): fd = sys.stdin.fileno() old_settings = termios.tcgetattr(fd) def restore_settings(): termios.tcsetattr(fd, termios.TCSADRAIN, old_settings) try: # Set terminal to raw mode tty.setraw(fd) print(\\"Raw mode: Press \'q\' to switch to cbreak mode\\", flush=True) while True: ch = os.read(fd, 1).decode(\'utf-8\') if ch == \'q\': break # Set terminal to cbreak mode tty.setcbreak(fd) print(\\"nCbreak mode: Press \'q\' to exit\\", flush=True) while True: ch = os.read(fd, 1).decode(\'utf-8\') if ch == \'q\': break except Exception as e: print(f\\"An error occurred: {e}\\") finally: restore_settings() print(\\"nTerminal settings restored\\") # terminal_control() # Uncomment this line to run the function in a real terminal environment"},{"question":"# Advanced Python Coding Assessment Objective: Demonstrate your understanding of the `webbrowser` Python module by implementing a feature that interacts with various web browsers. Problem Statement: Write a Python function `browse_web(url_list, tab=True)` that takes in a list of URLs and a boolean parameter `tab`. The function should open all the URLs: - If `tab` is `True`, each URL should be opened in a new tab of the default browser. - If `tab` is `False`, each URL should be opened in a new window of the default browser. Additionally, the function should handle cases where URLs may fail to load by using exception handling and print an error message for each failed URL. Use the appropriate functions from the `webbrowser` module to achieve this behavior. Function Signature: ```python def browse_web(url_list: list[str], tab: bool = True): pass ``` Input: - `url_list`: A list of strings where each string is a valid URL. The list can be empty or contain up to 10 URLs. Each URL is guaranteed to be a valid URL string. - `tab`: A boolean value, default is `True`. Output: - The function does not return any value. It performs web browsing operations and prints error messages for any URLs that fail to load. Constraints: - Use the `webbrowser` module only. - The function should be robust against loading failures. - Ensure the function handles both Unix-like and non-Unix platforms correctly. Example: ```python # Example usage urls = [ \\"https://www.python.org\\", \\"https://www.github.com\\", \\"https://nonexistent.url\\" ] browse_web(urls, tab=True) ``` Expected behavior: - The function will open \\"https://www.python.org\\" and \\"https://www.github.com\\" in new tabs. - Since \\"https://nonexistent.url\\" is a nonexistent URL, it will catch an exception (if raised) and print an error message. Note: - Use `try` and `except` blocks to handle potential exceptions when opening URLs.","solution":"import webbrowser def browse_web(url_list, tab=True): Opens a list of URLs in a web browser. Args: url_list (list of str): List of URLs to open. tab (bool): Whether to open each URL in a new tab (True) or a new window (False). for url in url_list: try: if tab: webbrowser.open_new_tab(url) else: webbrowser.open_new(url) except Exception as e: print(f\\"Failed to open URL {url}: {e}\\")"},{"question":"**Command Line Task Manager** Create a Python script named `task_manager.py` that uses the `argparse` module to manage a simple task list. The script should support the following sub-commands: 1. `add` - Add a new task to the list. 2. `list` - List all tasks. 3. `remove` - Remove a task by its index. 4. `clear` - Clear all tasks. # Specifications 1. **Storage**: - Tasks should be stored in a temporary file (`tasks.json`) in the script\'s directory, using JSON format. 2. **Commands**: - `add`: - Arguments: - `description` (positional, string): Description of the task. - Example: ```sh python task_manager.py add \\"Buy groceries\\" ``` - `list`: - Shows all tasks with their indices. - Example: ```sh python task_manager.py list ``` - `remove`: - Arguments: - `index` (positional, integer): Index of the task to remove. - Example: ```sh python task_manager.py remove 1 ``` - `clear`: - Clears all tasks. - Example: ```sh python task_manager.py clear ``` 3. **General Requirements**: - Create meaningful help messages for each command and argument. - Implement error handling for invalid commands and arguments, including out-of-range indices for removal. # Implementation Details 1. **Setup `argparse`**: - Create an `ArgumentParser` object. - Add subparsers for `add`, `list`, `remove`, and `clear` commands. 2. **Define Functions**: - Function to add tasks. - Function to list tasks. - Function to remove tasks by index. - Function to clear tasks. 3. **File Operations**: - Read from `tasks.json` when listing or removing tasks. - Write to `tasks.json` when adding or clearing tasks. # Example Interactions ```sh python task_manager.py add \\"Buy groceries\\" Task added. python task_manager.py list 0: Buy groceries python task_manager.py remove 0 Task removed. python task_manager.py list No tasks found. python task_manager.py clear All tasks cleared. ``` # Constraints - Implement the solution with Python 3.7 or higher. - Ensure tasks are stored persistently using a JSON file. **Submission** Submit your `task_manager.py` script demonstrating the use of `argparse` to handle command-line arguments and sub-commands efficiently.","solution":"import argparse import json import os TASKS_FILE = \'tasks.json\' def load_tasks(): if os.path.exists(TASKS_FILE): with open(TASKS_FILE, \'r\') as file: return json.load(file) else: return [] def save_tasks(tasks): with open(TASKS_FILE, \'w\') as file: json.dump(tasks, file) def add_task(description): tasks = load_tasks() tasks.append(description) save_tasks(tasks) print(\\"Task added.\\") def list_tasks(): tasks = load_tasks() if not tasks: print(\\"No tasks found.\\") else: for index, task in enumerate(tasks): print(f\\"{index}: {task}\\") def remove_task(index): tasks = load_tasks() try: tasks.pop(index) save_tasks(tasks) print(\\"Task removed.\\") except IndexError: print(\\"Error: Task not found.\\") def clear_tasks(): save_tasks([]) print(\\"All tasks cleared.\\") def main(): parser = argparse.ArgumentParser(description=\\"Simple command line task manager.\\") subparsers = parser.add_subparsers(dest=\'command\') parser_add = subparsers.add_parser(\'add\', help=\'Add a new task\') parser_add.add_argument(\'description\', type=str, help=\'Description of the task\') parser_list = subparsers.add_parser(\'list\', help=\'List all tasks\') parser_remove = subparsers.add_parser(\'remove\', help=\'Remove a task by index\') parser_remove.add_argument(\'index\', type=int, help=\'Index of the task to remove\') parser_clear = subparsers.add_parser(\'clear\', help=\'Clear all tasks\') args = parser.parse_args() if args.command == \'add\': add_task(args.description) elif args.command == \'list\': list_tasks() elif args.command == \'remove\': remove_task(args.index) elif args.command == \'clear\': clear_tasks() else: parser.print_help() if __name__ == \\"__main__\\": main()"},{"question":"# PyTorch Coding Assessment Question Objective: Demonstrate your understanding of manipulating tensor dimensions using the `torch.Size` class in PyTorch. Question: Given a tensor `x` with varying dimensions, write a function `tensor_dim_operations` that: 1. Takes a tensor `x` as input. 2. Returns a tuple with the following elements: - The dimensions of the tensor as a `torch.Size` object. - The size of the second dimension (if it exists, otherwise return `None`). - The total number of dimensions of the tensor. - The product of all the dimensions (i.e., total number of elements in the tensor). # Function Signature ```python import torch def tensor_dim_operations(x: torch.Tensor) -> tuple: # Your implementation here pass ``` # Input - A tensor `x` of any shape and dimensions. # Output - A tuple containing: 1. The dimensions of the tensor as a `torch.Size` object. 2. An integer representing the size of the second dimension (return `None` if the tensor has less than 2 dimensions). 3. An integer representing the total number of dimensions. 4. An integer representing the product of all the dimensions. # Example ```python # Given a tensor x = torch.ones(10, 20, 30) # Your function should return result = tensor_dim_operations(x) # result should be (torch.Size([10, 20, 30]), 20, 3, 6000) ``` # Constraints - Ensure that the function handles tensors with different shapes and dimensions, including edge cases like scalars and one-dimensional tensors. # Hints - Use the `size` method on the tensor to get the `torch.Size` object representing the dimensions. - Use indexing to access specific dimensions. - Use the `len` function to determine the number of dimensions. - To calculate the product of dimensions, consider iterating over the `torch.Size` object. # Notes - Your function should handle tensors with less than two dimensions and higher-dimensional tensors efficiently. - Aim for a solution that works for any tensor shape and is computationally efficient.","solution":"import torch def tensor_dim_operations(x: torch.Tensor) -> tuple: dimensions = x.size() second_dim_size = dimensions[1] if len(dimensions) > 1 else None total_num_dimensions = len(dimensions) total_elements = torch.tensor(dimensions).prod().item() return (dimensions, second_dim_size, total_num_dimensions, total_elements)"},{"question":"# Complex Tensor Operations and Autograd in PyTorch Problem Statement You are given a dataset that represents signals captured from some sensors. These signals are stored in a tensor `S` of size `(N, 2)`, where `N` is the number of signals. Each row in this tensor represents a complex number in the form of two real numbers (real part, imaginary part). Your task is to implement functions that perform the following operations using PyTorch: 1. **Convert Real Tensor to Complex Tensor**: Convert the real tensor `S` to a complex tensor. 2. **Compute the Magnitude and Phase**: Compute the magnitude and phase angle (in radians) of each complex number in the tensor. 3. **Apply a Linear Transformation**: Define and apply a linear transformation `L` to the complex tensor. 4. **Compute Gradient w.r.t. Transformation Parameters**: Compute the gradient of the sum of squared magnitudes of the transformed complex tensor with respect to the transformation parameters. Requirements 1. **Function 1**: `real_to_complex(S: torch.Tensor) -> torch.Tensor` - **Input**: A real tensor `S` of shape `(N, 2)`. - **Output**: A complex tensor of shape `(N,)`. 2. **Function 2**: `compute_magnitude_phase(C: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]` - **Input**: A complex tensor `C` of shape `(N,)`. - **Output**: Two tensors of shape `(N,)` representing the magnitude and phase angle of the complex numbers. 3. **Function 3**: `apply_linear_transformation(C: torch.Tensor, W: torch.Tensor, b: torch.Tensor) -> torch.Tensor` - **Input**: A complex tensor `C` of shape `(N,)`, a complex weight tensor `W` of shape `(N, N)`, and a complex bias tensor `b` of shape `(N,)`. - **Output**: A complex tensor of shape `(N,)` representing the transformed complex numbers. 4. **Function 4**: `compute_gradient(S: torch.Tensor, W: torch.Tensor, b: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]` - **Input**: A real tensor `S` of shape `(N, 2)`, a complex weight tensor `W` of shape `(N, N)`, and a complex bias tensor `b` of shape `(N,)`. - **Output**: Gradients of the sum of squared magnitudes w.r.t. `W` and `b`. Constraints - `N` will be in the range [2, 100]. - Use complex data types provided by PyTorch. - Ensure the functions are efficient and make use of vectorized operations where possible. - Torch version should be 1.6.0 or higher. Example ```python import torch # Example Tensor S = torch.tensor([[3.0, 4.0], [1.0, 2.0]], dtype=torch.float32) # Transformations W = torch.randn(2, 2, dtype=torch.cfloat, requires_grad=True) b = torch.randn(2, dtype=torch.cfloat, requires_grad=True) # Convert to Complex C = real_to_complex(S) # Compute magnitude and phase magnitude, phase = compute_magnitude_phase(C) # Apply Linear Transformation transformed_C = apply_linear_transformation(C, W, b) # Compute Gradients grad_W, grad_b = compute_gradient(S, W, b) ``` Implement the functions as described to achieve the desired operations.","solution":"import torch def real_to_complex(S: torch.Tensor) -> torch.Tensor: Convert the real tensor S to a complex tensor. return torch.view_as_complex(S) def compute_magnitude_phase(C: torch.Tensor): Compute the magnitude and phase angle (in radians) of each complex number in the tensor. magnitude = torch.abs(C) phase = torch.angle(C) return magnitude, phase def apply_linear_transformation(C: torch.Tensor, W: torch.Tensor, b: torch.Tensor) -> torch.Tensor: Apply a linear transformation to the complex tensor. return torch.matmul(W, C) + b def compute_gradient(S: torch.Tensor, W: torch.Tensor, b: torch.Tensor): Compute the gradient of the sum of squared magnitudes of the transformed complex tensor with respect to the transformation parameters. C = real_to_complex(S) transformed_C = apply_linear_transformation(C, W, b) magnitude_squared = torch.abs(transformed_C) ** 2 loss = magnitude_squared.sum() loss.backward() return W.grad, b.grad"},{"question":"**Coding Assessment Question** # Title: Advanced Generator and Frame-based Generators # Problem Statement: You are required to implement a custom generator function in Python, followed by leveraging the generator objects to create and handle datasets using both built-in and custom frame-based methods. # Task: 1. **Custom Generator Function:** Implement a generator function `custom_range(start, end, step)` that yields numbers starting from `start` to `end` (inclusive) incremented by `step`. 2. **Enhanced Generator Object:** Using the built-in capabilities of Python and understanding frame-based generator creation, create an enhanced generator named `FrameGen` that can accept additional metadata (such as `name` and `qualname`). Implement methods in this generator class to: - Yield values using the frame-based creation method. - Check if the generator object belongs to `PyGen_Type`. # Requirements: 1. **Function: custom_range** - **Input:** Three integers `start`, `end`, and `step`. - **Output:** Yield a sequence of numbers starting from `start` to `end` (inclusive), incrementing by `step`. 2. **Class: FrameGen** - **Input:** A list of numeric values (used to initialize the generator). - **Methods:** - `__iter__()`: Make the class iterable. - `__next__()`: Yield the next value in the sequence. - `gen_with_metadata(name, qualname)`: Create and return a new generator object with the provided name and qualname (use `PyGen_NewWithQualName` equivalent logic in Python). - `is_gen_object(obj)`: Check if the provided obj is a generator object using `PyGen_Check`. # Constraints: - Assume the values for `start`, `end`, and `step` are within the range `-10^6` to `10^6`. - Performance should handle a large range of values efficiently using generator properties. - Ensure proper management of generator states and frame contexts. # Example Usage: ```python # Part 1 for num in custom_range(1, 10, 2): print(num) # Output: 1 3 5 7 9 # Part 2 frame_gen = FrameGen([1, 2, 3, 4, 5]) for value in frame_gen: print(value) enhanced_gen = frame_gen.gen_with_metadata(\\"EnhancedGen\\", \\"example.FrameGen\\") print(frame_gen.is_gen_object(enhanced_gen)) # Output: True ``` # Evaluation Criteria: - Correct implementation of the `custom_range` generator function. - Proper design and functionality of the `FrameGen` class and its methods. - Handling edge cases and ensuring performance constraints. - Clear and concise code documentation and comments.","solution":"# Custom Generator Function def custom_range(start, end, step): Yields numbers from start to end (inclusive) incremented by step. current = start while current <= end: yield current current += step # Enhanced Generator Object class FrameGen: def __init__(self, data): self.data = data self.index = 0 def __iter__(self): return self def __next__(self): if self.index >= len(self.data): raise StopIteration value = self.data[self.index] self.index += 1 return value def gen_with_metadata(self, name, qualname): Create and return a new generator object with the provided name and qualname. def generator(): for item in self.data: yield item generator.__name__ = name generator.__qualname__ = qualname return generator() @staticmethod def is_gen_object(obj): Check if the provided obj is a generator object. return (hasattr(obj, \\"gi_code\\") and hasattr(obj, \\"gi_frame\\"))"},{"question":"**Coding Assessment Question: Topological Sorting of Dependency Graph** You are required to implement functionalities that leverage the `graphlib.TopologicalSorter` class to manage a set of tasks with dependencies and output their topological order if possible. # Objective Implement the `TaskManager` class that allows adding tasks with dependencies, and then retrieve a valid sequence of tasks based on their dependencies. # Implementation Details 1. **`__init__(self)`**: Initialize an instance of `TaskManager`. 2. **`add_task(self, task: str, dependencies: List[str] = [])`**: Add a new task with an optional list of dependency tasks. - `task`: The task to be added (str). - `dependencies`: A list of tasks that must be completed before this task (List[str]). 3. **`get_task_order(self) -> List[str]`**: Retrieves the topological order of tasks if possible. - If a valid topological order exists, return a list of tasks in order. - If there is a cycle, raise a `RuntimeError` with the message: \\"Dependency cycle detected\\". # Constraints - Each task is represented by a unique string (no spaces). - The sum of all tasks and dependencies should not exceed 1000. - The `graphlib` library must be used for topological sorting. # Example Usage ```python # Example usage of TaskManager class tm = TaskManager() tm.add_task(\\"D\\", [\\"B\\", \\"C\\"]) tm.add_task(\\"C\\", [\\"A\\"]) tm.add_task(\\"B\\", [\\"A\\"]) tm.add_task(\\"A\\") try: task_order = tm.get_task_order() print(task_order) # Output: [\'A\', \'C\', \'B\', \'D\'] except RuntimeError as e: print(e) # Output: Dependency cycle detected (if a cycle is found) ``` # Evaluation Criteria 1. **Correctness**: Does the implementation correctly identify task order and detect cycles? 2. **Proper Use of `graphlib`**: Is `graphlib.TopologicalSorter` used correctly and efficiently? 3. **Efficiency**: Is the solution optimized to handle the constraints provided? 4. **Code Quality**: Is the code well-structured, and are variable names meaningful? Create a Python file or Jupyter notebook implementing the `TaskManager` class as specified above. # Explanation You are expected to integrate the functionalities from the `graphlib` library into the `TaskManager` class to construct, manage, and sort tasks based on their dependencies. This should ensure students are able to understand and use the features provided by the `TopologicalSorter` class effectively.","solution":"import graphlib class TaskManager: def __init__(self): self.graph = {} def add_task(self, task: str, dependencies: list = []): if task not in self.graph: self.graph[task] = set() for dep in dependencies: if dep not in self.graph: self.graph[dep] = set() self.graph[task].add(dep) def get_task_order(self): ts = graphlib.TopologicalSorter(self.graph) try: return list(ts.static_order()) except graphlib.CycleError: raise RuntimeError(\\"Dependency cycle detected\\")"},{"question":"**Objective:** Implement a function in Python that processes and analyzes various types of sequences and mappings. This function will demonstrate your comprehension of fundamental and advanced concepts as stated in the Python310 documentation about built-in types. **Task:** Create a function `analyze_data(data: Any) -> dict` that accepts a single argument, `data`, which can be of any built-in type (numeric, sequence, mapping) and returns a dictionary with the following information: 1. **type**: The type of the input data. 2. **length** (if applicable): The length of the sequence or mapping. 3. **unique_elements** (if applicable): For sequences (list, tuple, range), return a set of unique elements. For mappings (dict), return the unique keys. 4. **is_true**: The boolean interpretation of the data (i.e., the result of `bool(data)`). 5. **comparisons**: A dictionary that includes the results of various comparisons. These should include: - `\\"is_zero\\"`: `data == 0` or `False` if not applicable. - `\\"is_one\\"`: `data == 1` or `False` if not applicable. - `\\"is_empty\\"`: Whether the data is an empty sequence or mapping. 6. **numeric_properties** (if applicable): - For integers and floating-point numbers, include keys like `\\"is_positive\\"`, `\\"is_negative\\"`, and `\\"is_integer\\"`. For non-numeric types, skip this part. **Input:** - `data`: The input which can be any built-in Python type (numeric, sequence, mapping, etc.). **Output:** - A dictionary with keys as specified above. **Constraints:** - You must handle all valid built-in types as specified in the Python310 documentation. - The function should handle both mutable and immutable sequences efficiently. - Ensure proper type checking and error handling wherever necessary. **Example:** ```python def analyze_data(data: Any) -> dict: # Implement your solution here pass # Example usage: print(analyze_data([1, 2, 2, 3, 4])) # Output: # { # \'type\': \'list\', # \'length\': 5, # \'unique_elements\': {1, 2, 3, 4}, # \'is_true\': True, # \'comparisons\': {\'is_zero\': False, \'is_one\': False, \'is_empty\': False}, # \'numeric_properties\': {} # } print(analyze_data(0)) # Output: # { # \'type\': \'int\', # \'unique_elements\': None, # \'is_true\': False, # \'comparisons\': {\'is_zero\': True, \'is_one\': False, \'is_empty\': False}, # \'numeric_properties\': {\'is_positive\': False, \'is_negative\': False, \'is_integer\': True} # } ``` **Notes:** 1. The function must be able to identify and handle all core Python types as per the provided documentation. 2. Use type hinting for better code readability and debugging. 3. The function should be resilient to different data structures and types, providing accurate analysis as outlined.","solution":"from typing import Any, Dict def analyze_data(data: Any) -> Dict: result = { \'type\': type(data).__name__, \'length\': None, \'unique_elements\': None, \'is_true\': bool(data), \'comparisons\': { \'is_zero\': False, \'is_one\': False, \'is_empty\': False }, \'numeric_properties\': {} } # Handling sequences (lists, tuples, ranges) if isinstance(data, (list, tuple, range)): result[\'length\'] = len(data) result[\'unique_elements\'] = set(data) result[\'comparisons\'][\'is_empty\'] = (len(data) == 0) # Handling mappings (dicts) elif isinstance(data, dict): result[\'length\'] = len(data) result[\'unique_elements\'] = set(data.keys()) result[\'comparisons\'][\'is_empty\'] = (len(data) == 0) # Handling numeric types (int, float) if isinstance(data, (int, float)): result[\'comparisons\'][\'is_zero\'] = (data == 0) result[\'comparisons\'][\'is_one\'] = (data == 1) result[\'numeric_properties\'] = { \'is_positive\': data > 0, \'is_negative\': data < 0, \'is_integer\': isinstance(data, int) } # Handling strings elif isinstance(data, str): result[\'length\'] = len(data) result[\'unique_elements\'] = set(data) result[\'comparisons\'][\'is_empty\'] = (len(data) == 0) return result"},{"question":"**Email Policy Implementation and Handling Defects** **Problem Statement:** You are tasked with customizing the email handling process using the \\"email.policy\\" package in Python. Specifically, you need to create a custom policy that extends the `EmailPolicy` to handle some specific requirements for generating and processing email messages. Your task is to implement the following functionalities: 1. **Create a Custom Policy**: You need to create a custom policy that: - Inherits from `EmailPolicy`. - Ensures headers and bodies adhere to \\"utf8\\" encoding. - Sets a maximum line length of 100 characters. - Raises errors when defects are encountered. 2. **Defect Handling and Custom Message Processing**: - Implement and demonstrate a `handle_defect` method in your custom policy. This method should print a custom message including the defect description. - Implement a function to read an email message from a file (`input_email.txt`), process it with your custom policy, and serialize the processed email back to another file (`output_email.txt`). **Specifications**: - **Custom Policy Class**: Define a class `CustomEmailPolicy` which inherits from `EmailPolicy` and overrides the necessary attributes and methods. - **Defect Handling**: Customize the `handle_defect` method to print a message indicating the defect but proceed without raising an error. - **Function to Process Email**: Implement a function `process_email_with_custom_policy(input_path: str, output_path: str)` that reads an email from `input_path` file, processes it using `CustomEmailPolicy`, and writes the serialized email to `output_path`. **Constraints**: - Ensure your solution adheres to the RFC 5322 and RFC 6532 standards. - Handling defects should not disrupt the parsing or serialization process. - Demonstrate the functionality with sample input and output. **Example Usage**: ```python class CustomEmailPolicy(EmailPolicy): # Your implementation here def process_email_with_custom_policy(input_path: str, output_path: str): # Your implementation here # Example Call process_email_with_custom_policy(\'input_email.txt\', \'output_email.txt\') ``` This problem aims to assess your understanding of policy customization, defect handling, and practical application in the context of email message processing with Python\'s email package.","solution":"from email.policy import EmailPolicy from email import message_from_file from email.generator import Generator class CustomEmailPolicy(EmailPolicy): def __init__(self, **kwargs): super().__init__(utf8=True, max_line_length=100, raise_on_defect=False, **kwargs) def handle_defect(self, msg, defect): print(f\\"Defect encountered: {defect}\\") def process_email_with_custom_policy(input_path: str, output_path: str): with open(input_path, \'r\', encoding=\'utf8\') as infile: msg = message_from_file(infile, policy=CustomEmailPolicy()) with open(output_path, \'w\', encoding=\'utf8\') as outfile: gen = Generator(outfile, policy=CustomEmailPolicy()) gen.flatten(msg)"},{"question":"**Objective**: Demonstrate your understanding of efficiently handling large datasets using pandas by loading, processing, and optimizing memory usage for a given dataset. **Problem Statement**: You are given a directory containing multiple parquet files, each representing one month of sales data for a retail company. Each file has columns: `transaction_id`, `customer_id`, `product_id`, `quantity`, `price`, and `transaction_date`. Your task is to write a function that performs the following: 1. Load the data from all the parquet files in the specified directory. 2. Only keep the necessary columns: `product_id`, `quantity`, `price`. 3. Optimize memory usage by converting `product_id` to the most efficient data type. 4. Calculate the total quantity and total sales (quantity * price) for each product by processing data in chunks. 5. Return a pandas DataFrame with two columns: `total_quantity` and `total_sales`, indexed by `product_id`. **Function Signature**: ```python import pandas as pd from typing import Dict def process_sales_data(directory: str) -> pd.DataFrame: pass ``` **Constraints**: - The memory available for processing may not be enough to load all files simultaneously. - Assume the directory contains multiple parquet files with a consistent schema. **Example**: Assume the directory `sales_data/` contains files `2023-01.parquet`, `2023-02.parquet`, etc. ```python result_df = process_sales_data(\\"sales_data/\\") print(result_df.head()) ``` The expected output is a DataFrame showing total quantities and total sales per product, indexed by `product_id`. **Note**: - Use the chunking method where appropriate to ensure the process can handle large datasets. - Be sure to use optimal data types for memory efficiency. - You can use the function `pd.read_parquet` with the `columns` parameter to load selective columns. **Hints**: 1. When reading the data, make sure to use the `columns` parameter to load only `product_id`, `quantity`, and `price`. 2. Use chunking to aggregate the data, as loading all data at once may not be memory efficient. 3. Convert `product_id` to a categorical type for efficient memory usage.","solution":"import pandas as pd import os from typing import Dict def process_sales_data(directory: str) -> pd.DataFrame: Processes sales data from multiple parquet files, calculating total quantity and total sales indexed by product_id, using memory-efficient techniques. total_quantity = {} total_sales = {} # Process each file in the directory for file in os.listdir(directory): if file.endswith(\\".parquet\\"): file_path = os.path.join(directory, file) chunks = pd.read_parquet(file_path, columns=[\\"product_id\\", \\"quantity\\", \\"price\\"]) # Convert product_id to categorical type for memory efficiency chunks[\\"product_id\\"] = chunks[\\"product_id\\"].astype(\\"category\\") for idx, row in chunks.iterrows(): product_id = row[\\"product_id\\"] quantity = row[\\"quantity\\"] price = row[\\"price\\"] if product_id in total_quantity: total_quantity[product_id] += quantity total_sales[product_id] += quantity * price else: total_quantity[product_id] = quantity total_sales[product_id] = quantity * price result_df = pd.DataFrame({ \\"product_id\\": total_quantity.keys(), \\"total_quantity\\": total_quantity.values(), \\"total_sales\\": total_sales.values() }) result_df.set_index(\\"product_id\\", inplace=True) return result_df"},{"question":"<|Analysis Begin|> The provided documentation only covers a small subset of Seaborn\'s capabilities, specifically focusing on the `scatterplot` function. The examples illustrate how to create scatter plots using various semantics such as `hue`, `style`, and `size`, and how to control different aspects of the plotting aesthetics directly or via semantic mappings. Additionally, it shows how to use a long-form dataset (`tips` dataset) and a wide-form dataset to create scatter plots, and how to employ `relplot` for enhanced functionality through `FacetGrid`. Given this limited scope and the focus on `scatterplot`, it\'s challenging to formulate an assessment question that adequately covers the fundamental and advanced features of Seaborn as a whole. A reasonable assessment would have to work within the confines of scatter plots and data visualization customization demonstrated here. <|Analysis End|> <|Question Begin|> # Assessing Data Visualization with Seaborn **Objective**: Assess students\' understanding of creating and customizing scatter plots using Seaborn library in Python. **Problem Statement**: You are given a dataset named `tips`, a collection of data about tips received in a restaurant. This dataset has the following columns: `total_bill`, `tip`, `sex`, `smoker`, `day`, `time`, and `size`. Write a function `custom_scatter_plot` that will produce an enhanced scatter plot based on specific requirements. Function Signature ```python def custom_scatter_plot(data: pd.DataFrame) -> None: pass ``` Requirements 1. The scatter plot should display `total_bill` on the x-axis and `tip` on the y-axis. 2. The points in the scatter plot should be colored based on the `day` of the week. 3. The style of the markers should vary based on whether the time is `Lunch` or `Dinner`. 4. The size of the markers should correspond to the `size` column. 5. Ensure that the plot includes a legend that shows all sizes explicitly. 6. Use a palette of your choice for coloring the points. Input: - `data`: A pandas DataFrame containing the tips dataset. Output: - The function should directly display a scatter plot that meets the requirements. **Example**: Use the following code to load the `tips` dataset and call your function: ```python import pandas as pd import seaborn as sns # Load the dataset tips = sns.load_dataset(\\"tips\\") # Call the function custom_scatter_plot(tips) ``` This should produce a scatter plot with points colored by the day of the week, marker styles differentiating Lunch and Dinner, and marker sizes corresponding to the size column, along with a legend that includes all specific sizes. Constraints - Ensure that your plot is clearly labeled with a title, x-axis label, and y-axis label. - Handle edge cases where any required columns might be missing in the dataset. Note: Use the `seaborn` library for plotting and `matplotlib` for any additional customizations if needed.","solution":"import seaborn as sns import matplotlib.pyplot as plt import pandas as pd def custom_scatter_plot(data: pd.DataFrame) -> None: Generates a scatter plot with total_bill on the x-axis and tip on the y-axis. Points are colored by day, styled by time of day (Lunch or Dinner), and sized by the size column. if not all(col in data.columns for col in [\'total_bill\', \'tip\', \'day\', \'time\', \'size\']): raise ValueError(\\"Dataset is missing one or more required columns.\\") plt.figure(figsize=(10, 6)) scatter = sns.scatterplot( data=data, x=\'total_bill\', y=\'tip\', hue=\'day\', style=\'time\', size=\'size\', sizes=(20, 200), palette=\'viridis\', legend=\'full\' ) scatter.set_title(\'Total Bill vs Tip\') scatter.set_xlabel(\'Total Bill ()\') scatter.set_ylabel(\'Tip ()\') plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.) plt.show()"},{"question":"Dynamic Model Architecture using `torch.cond` **Objective:** Implement a PyTorch neural network module that uses `torch.cond` to dynamically alter its forward pass based on the sum of its input tensor. This task will assess your understanding of `torch.cond` and its application in creating conditional model behaviors. **Description:** You are required to create a PyTorch module named `DynamicCondModule`. The module should alter its behavior based on the sum of the input tensor: - If the sum of the input tensor elements is greater than or equal to 10, the module should apply a sequence of operations: calculate the cosine, then add a constant value of 5. - If the sum is less than 10, the module should only calculate the sine of the tensor elements. You should also write a function to test the implemented module with example inputs and verify its correctness. **Details:** - **Input:** - A single PyTorch tensor `x`. - **Output:** - A PyTorch tensor resulting from the conditional operations described. **Constraints:** - The input tensor `x` will have floating point values. - Ensure that your implementation can handle dynamic input shapes. - Follow proper practices to maintain performance and readability. **Function Signatures:** ```python class DynamicCondModule(torch.nn.Module): def __init__(self): super(DynamicCondModule, self).__init__() def forward(self, x: torch.Tensor) -> torch.Tensor: pass def test_dynamic_cond_module(): pass ``` **Example:** ```python import torch def test_dynamic_cond_module(): model = DynamicCondModule() input_tensor1 = torch.tensor([1.0, 2.0, 3.0]) output1 = model(input_tensor1) print(f\\"Output 1: {output1}\\") input_tensor2 = torch.tensor([4.0, 5.0, 6.0]) output2 = model(input_tensor2) print(f\\"Output 2: {output2}\\") expected_output1 = torch.sin(input_tensor1) expected_output2 = torch.cos(input_tensor2) + 5 assert torch.equal(output1, expected_output1), \\"Test case 1 failed\\" assert torch.equal(output2, expected_output2), \\"Test case 2 failed\\" print(\\"All test cases passed\\") # Uncomment the line below to run the test function # test_dynamic_cond_module() ``` # Evaluation Criteria: - Correct implementation of the `DynamicCondModule`. - Proper use of `torch.cond` for dynamic control flow. - Accurate and thorough testing of the module with example inputs. - Code readability and adherence to PyTorch best practices.","solution":"import torch import torch.nn as nn class DynamicCondModule(nn.Module): def __init__(self): super(DynamicCondModule, self).__init__() def forward(self, x: torch.Tensor) -> torch.Tensor: sum_x = x.sum() def true_fn(x): return torch.cos(x) + 5 def false_fn(x): return torch.sin(x) return torch.where(sum_x >= 10, true_fn(x), false_fn(x))"},{"question":"# Concurrent Task Management with `concurrent.futures` Objective: To test your understanding of the `concurrent.futures` module by implementing a function that handles multiple tasks concurrently and processes their results. Problem Statement: You are required to implement a function `process_data_concurrently` that takes a list of integers as input and performs the following operations concurrently: 1. Calculates the square of each integer. 2. Multiplies each integer by 10. 3. Adds 5 to each integer. After performing these operations concurrently, the function should return a dictionary with three keys: - `\'squares\'`: containing a list of the squares of the input integers. - `\'multiplied\'`: containing a list of the integers multiplied by 10. - `\'added\'`: containing a list of the integers with 5 added. You should use the `ThreadPoolExecutor` from the `concurrent.futures` module to achieve concurrency. Specifications: **Function Signature:** ```python def process_data_concurrently(integers: list[int]) -> dict[str, list[int]]: pass ``` **Input:** - `integers`: A list of integers. Constraints: 1 <= len(integers) <= 1000, -100 <= integers[i] <= 100 **Output:** - A dictionary with the keys `\'squares\'`, `\'multiplied\'`, and `\'added\'` containing lists of integers. **Example:** ```python input_data = [1, 2, 3] output = process_data_concurrently(input_data) # Output should be: # { # \'squares\': [1, 4, 9], # \'multiplied\': [10, 20, 30], # \'added\': [6, 7, 8] # } ``` Constraints and Requirements: 1. The function should handle the operations concurrently to maximize performance. 2. Properly handle any exceptions that occur during task execution. 3. Ensure that the results are returned in a consistent order corresponding to the input integers. 4. The code should handle edge cases such as empty input lists correctly. Use the `concurrent.futures` module to implement the required functionality and manage the concurrent execution of tasks.","solution":"from concurrent.futures import ThreadPoolExecutor def process_data_concurrently(integers: list[int]) -> dict[str, list[int]]: Processes a list of integers concurrently, calculating their squares, multiplying by 10, and adding 5 to each. results = { \'squares\': [], \'multiplied\': [], \'added\': [] } # Functions that perform the required operations. def square(x): return x * x def multiply_by_10(x): return x * 10 def add_5(x): return x + 5 with ThreadPoolExecutor() as executor: # Schedule the tasks for squaring numbers. squares_futures = [executor.submit(square, num) for num in integers] multiplied_futures = [executor.submit(multiply_by_10, num) for num in integers] added_futures = [executor.submit(add_5, num) for num in integers] # Collect results for squares. results[\'squares\'] = [future.result() for future in squares_futures] # Collect results for multiplied by 10. results[\'multiplied\'] = [future.result() for future in multiplied_futures] # Collect results for added by 5. results[\'added\'] = [future.result() for future in added_futures] return results"},{"question":"You are tasked with creating a secure login system. Using the `crypt` module, you need to implement functions for password hashing, user registration, and login verification. Your solution should consider different hashing methods and ensure secure password storage and verification. # Function Requirements 1. **`generate_hashed_password(password, method=None, rounds=None)`**: - **Input**: - `password` (string): The plain-text password. - `method` (crypt.METHOD_* object, optional): The hashing method to use. Defaults to `None`, which means the strongest available method will be used. - `rounds` (int, optional): The number of rounds if applicable to the chosen method. Defaults to `None`. - **Output**: Returns a hashed password string. 2. **`register_user(username, password)`**: - **Input**: - `username` (string): The username for the new user. - `password` (string): The plain-text password for the new user. - **Output**: Returns a dictionary with `username` as the key and the hashed password as the value. 3. **`verify_login(username, password, user_db)`**: - **Input**: - `username` (string): The username of the user attempting to log in. - `password` (string): The plain-text password entered by the user. - `user_db` (dictionary): A dictionary where keys are usernames and values are hashed passwords. - **Output**: Returns `True` if the username exists in `user_db` and the password matches the stored hashed password. Returns `False` otherwise. # Constraints - The username is guaranteed to be unique. - Assume `crypt` module functions are available for hashing and checking passwords. - Use `hmac.compare_digest` for secure password comparison. # Example ```python import crypt from hmac import compare_digest as compare_hash def generate_hashed_password(password, method=None, rounds=None): salt = crypt.mksalt(method, rounds=rounds) return crypt.crypt(password, salt) def register_user(username, password): user_db = {} hashed_password = generate_hashed_password(password) user_db[username] = hashed_password return user_db def verify_login(username, password, user_db): if username in user_db: stored_hashed_password = user_db[username] return compare_hash(stored_hashed_password, crypt.crypt(password, stored_hashed_password)) return False # Example usage: user_db = register_user(\'john_doe\', \'password123\') print(verify_login(\'john_doe\', \'password123\', user_db)) # Should return True print(verify_login(\'john_doe\', \'wrongpassword\', user_db)) # Should return False ``` Your task is to implement the three functions based on the given example and problem requirements.","solution":"import crypt import hmac def generate_hashed_password(password, method=None, rounds=None): Generates a hashed password with the given method and number of rounds. Args: password (str): The plain-text password. method (crypt.METHOD_* object, optional): The hashing method to use. Defaults to None. rounds (int, optional): The number of rounds if applicable. Returns: str: The hashed password. salt = crypt.mksalt(method=method, rounds=rounds) return crypt.crypt(password, salt) def register_user(username, password): Registers a new user with the given username and password. Args: username (str): The username for the new user. password (str): The plain-text password for the new user. Returns: dict: A dictionary with username as key and hashed password as value. user_db = {} hashed_password = generate_hashed_password(password) user_db[username] = hashed_password return user_db def verify_login(username, password, user_db): Verifies the login credentials of a user. Args: username (str): The username of the user attempting to log in. password (str): The plain-text password entered by the user. user_db (dict): A dictionary where keys are usernames and values are hashed passwords. Returns: bool: True if login is successful, False otherwise. if username in user_db: stored_hashed_password = user_db[username] hashed_password_attempt = crypt.crypt(password, stored_hashed_password) return hmac.compare_digest(stored_hashed_password, hashed_password_attempt) return False"},{"question":"**Objective:** Write a function that takes a 4-dimensional tensor as input, reshapes it, and returns the reshaped tensor along with its new size using `torch.Size`. **Function Signature:** ```python def reshape_tensor(tensor: torch.Tensor, new_shape: tuple) -> Tuple[torch.Tensor, torch.Size]: pass ``` **Input:** - `tensor`: A 4-dimensional PyTorch tensor of shape (batch_size, channels, height, width). - `new_shape`: A tuple representing the new desired shape. **Output:** - A tuple containing: * The reshaped tensor. * The size of the reshaped tensor as a `torch.Size` object. **Constraints:** - The product of the dimensions in `new_shape` must match the product of the dimensions of the original tensor. - If the constraints are not met, your function should raise a `ValueError` with a relevant message. **Example:** ```python >>> import torch >>> tensor = torch.ones(2, 3, 4, 5) >>> new_shape = (3, 2, 20) >>> reshaped_tensor, size = reshape_tensor(tensor, new_shape) >>> reshaped_tensor.shape torch.Size([3, 2, 20]) >>> size torch.Size([3, 2, 20]) ``` **Notes:** - Ensure that your function correctly handles multiple calls and different tensor shapes. - Validate the new shape before attempting to reshape to avoid runtime errors. Demonstrate your solution using the provided example and some additional test cases. Ensure that your implementation is efficient and adheres to the constraints.","solution":"import torch from typing import Tuple def reshape_tensor(tensor: torch.Tensor, new_shape: tuple) -> Tuple[torch.Tensor, torch.Size]: Reshape a 4-dimensional tensor to a specified new shape and return the reshaped tensor and its new size. Parameters: tensor (torch.Tensor): A 4-dimensional tensor. new_shape (tuple): The desired new shape. Returns: Tuple[torch.Tensor, torch.Size]: The reshaped tensor and its size. Raises: ValueError: If the total number of elements in the new shape does not match the total number of elements in the original tensor. if tensor.dim() != 4: raise ValueError(\\"Input tensor must be 4-dimensional\\") original_num_elements = tensor.numel() new_num_elements = torch.prod(torch.tensor(new_shape)).item() if original_num_elements != new_num_elements: raise ValueError(\\"The product of the new shape dimensions must match the product of the original dimensions\\") reshaped_tensor = tensor.reshape(new_shape) return reshaped_tensor, reshaped_tensor.size()"},{"question":"# scikit-learn Coding Assessment You are provided with a partially implemented machine learning workflow. Your task is to complete the implementation by generating synthetic data, training a scikit-learn model, and evaluating its performance. Task Description 1. **Generate Synthetic Data**: - Use the `make_classification` function from `sklearn.datasets` to create a synthetic dataset for a binary classification problem. - Parameters: `n_samples=500`, `n_features=10`, `n_informative=5`, `n_redundant=5`, `random_state=42`. 2. **Train-Test Split**: - Split the dataset into training and testing sets using `train_test_split` from `sklearn.model_selection`. - Parameters: `test_size=0.3`, `random_state=42`. 3. **Model Training**: - Train a `GradientBoostingClassifier` from `sklearn.ensemble` on the training data. - Use default parameters except set `random_state=42`. 4. **Model Evaluation**: - Evaluate the model on the test set and return the accuracy score. Implementation ```python import numpy as np from sklearn.datasets import make_classification from sklearn.model_selection import train_test_split from sklearn.ensemble import GradientBoostingClassifier from sklearn.metrics import accuracy_score def run_classification(): # Step 1: Generate synthetic data X, y = make_classification(n_samples=500, n_features=10, n_informative=5, n_redundant=5, random_state=42) # Step 2: Split the dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Step 3: Initialize and train the model model = GradientBoostingClassifier(random_state=42) model.fit(X_train, y_train) # Step 4: Evaluate the model y_pred = model.predict(X_test) accuracy = accuracy_score(y_test, y_pred) return accuracy # Run the function and print the output print(run_classification()) ``` Requirements - The function `run_classification` should return the accuracy score of the model on the test set. - Ensure proper import statements are included and avoid unnecessary code. - Follow best practices for clean and readable code as demonstrated in the provided documentation. **Constraints**: - Do not use any external dataset. Only synthetic data should be generated as described. - The accuracy score should be a floating-point number between 0 and 1. **Performance Requirements**: - The code must execute within a reasonable time frame (a few seconds) on standard hardware.","solution":"import numpy as np from sklearn.datasets import make_classification from sklearn.model_selection import train_test_split from sklearn.ensemble import GradientBoostingClassifier from sklearn.metrics import accuracy_score def run_classification(): # Step 1: Generate synthetic data X, y = make_classification(n_samples=500, n_features=10, n_informative=5, n_redundant=5, random_state=42) # Step 2: Split the dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Step 3: Initialize and train the model model = GradientBoostingClassifier(random_state=42) model.fit(X_train, y_train) # Step 4: Evaluate the model y_pred = model.predict(X_test) accuracy = accuracy_score(y_test, y_pred) return accuracy # Run the function and print the output (optional for local execution) # print(run_classification())"},{"question":"**Coding Assessment Question:** # Advanced File Backup System You are tasked with developing a robust file backup system in Python using the \\"shutil\\" module. The system should be capable of copying files and directories, preserving file metadata, handling errors, and creating compressed archive backups. Your task is to implement two functions: 1. `backup_files(src, dst, create_archive=False)` 2. `restore_files(archive, dst)` Function 1: `backup_files(src, dst, create_archive=False)` **Description:** This function copies the contents from a source path `src` to a destination path `dst`. If `create_archive` is set to `True`, it should also create a compressed `.tar.gz` archive of the source directory. **Parameters:** - `src` (str): Path to the source file or directory to be backed up. - `dst` (str): Path to the destination directory where the backup will be stored. - `create_archive` (bool): Optional parameter. If `True`, create a `.tar.gz` archive of the source directory. Default is `False`. **Requirements:** - If `src` is a file, copy it to `dst` using `shutil.copy2()`, preserving metadata. - If `src` is a directory, recursively copy it to `dst` using `shutil.copytree()`, preserving metadata. - If `create_archive` is `True`, create a compressed tar archive of the source directory using `shutil.make_archive()`. **Return:** - Return `None`. Function 2: `restore_files(archive, dst)` **Description:** This function restores files from a compressed `.tar.gz` archive to a destination directory `dst`. **Parameters:** - `archive` (str): Path to the compressed `.tar.gz` archive. - `dst` (str): Path to the destination directory where files will be restored. **Requirements:** - Use `shutil.unpack_archive()` to extract the contents of the archive to `dst`. **Return:** - Return `None`. Constraints: - You may assume that all file paths provided are valid. - Handle possible exceptions like permission errors, file not found errors, etc., and print appropriate error messages. Example Usage: ```python # Example 1: Backup without creating an archive backup_files(\'/path/to/source\', \'/path/to/destination\') # Example 2: Backup with creating an archive backup_files(\'/path/to/source\', \'/path/to/destination\', create_archive=True) # Example 3: Restore files from an archive restore_files(\'/path/to/archive.tar.gz\', \'/path/to/destination\') ``` Implement these functions ensuring they handle all necessary operations as specified.","solution":"import shutil import os def backup_files(src, dst, create_archive=False): Copies the contents from source path \'src\' to destination path \'dst\'. If \'create_archive\' is True, creates a compressed \'.tar.gz\' archive of the source directory. Args: src (str): Path to the source file or directory. dst (str): Path to the destination directory. create_archive (bool): If True, create a \'.tar.gz\' archive. Default is False. Returns: None try: if os.path.isdir(src): shutil.copytree(src, dst) else: shutil.copy2(src, dst) if create_archive: shutil.make_archive(dst, \'gztar\', src) except Exception as e: print(f\\"Error occurred: {e}\\") def restore_files(archive, dst): Restores files from a compressed \'.tar.gz\' archive to a destination directory. Args: archive (str): Path to the compressed \'.tar.gz\' archive. dst (str): Path to the destination directory. Returns: None try: shutil.unpack_archive(archive, dst) except Exception as e: print(f\\"Error occurred: {e}\\")"},{"question":"**Problem Statement: Tokenizing and Transforming Python Code** You are tasked with creating a function that transforms a given Python source code string to replace all occurrences of numeric literals with their string representations. For example, the code `x = 42` should be transformed into `x = \'42\'`. Your function should use the `tokenize` module to achieve this. Implement the following function: ```python import tokenize from io import BytesIO def transform_numeric_literals(source_code: str) -> str: Transforms a given Python source code string to replace all occurrences of numeric literals with their string representations. Parameters: - source_code (str): A string containing the Python source code. Returns: - (str): The transformed Python source code with numeric literals replaced by strings. # Your implementation here # Example usage source_code = x = 42 y = 3.14 z = x + y print(z) transformed_code = transform_numeric_literals(source_code) print(transformed_code) ``` # Input - `source_code` (str): A string containing valid Python source code. The code can span multiple lines and include different types of numeric literals (integers, floats, etc.). # Output - Returns a string of the transformed Python source code with all numeric literals replaced by their string representations. # Constraints - The Python source code provided in `source_code` is guaranteed to be syntactically valid. # Example Given the following input: ```python source_code = x = 42 y = 3.14 z = x + y print(z) ``` Your function should output: ```python x = \'42\' y = \'3.14\' z = x + y print(z) ``` # Notes - You should use the `tokenize` module\'s `tokenize` function to tokenize the input source code. - Use the `untokenize` function to reconstruct the transformed code. - Ensure that you handle both integer and floating-point literals correctly.","solution":"import tokenize from io import BytesIO def transform_numeric_literals(source_code: str) -> str: Transforms a given Python source code string to replace all occurrences of numeric literals with their string representations. Parameters: - source_code (str): A string containing the Python source code. Returns: - (str): The transformed Python source code with numeric literals replaced by strings. tokens = tokenize.tokenize(BytesIO(source_code.encode(\'utf-8\')).readline) transformed_tokens = [] for token in tokens: if token.type == tokenize.NUMBER: transformed_tokens.append((tokenize.STRING, f\\"\'{token.string}\'\\", token.start, token.end, token.line)) else: transformed_tokens.append(token) transformed_code = tokenize.untokenize(transformed_tokens).decode(\'utf-8\') return transformed_code.strip() # Example usage source_code = x = 42 y = 3.14 z = x + y print(z) transformed_code = transform_numeric_literals(source_code) print(transformed_code)"},{"question":"**Objective**: Demonstrate your ability to serialize complex Python objects, store them in a SQLite database and retrieve them while ensuring their structural integrity and original behavior. **Problem Statement**: You are required to create a Python application that allows users to store and retrieve serialized objects using a SQLite database. The objects to be serialized can be any Python objects, including those containing state or methods. **Task Breakdown**: 1. Implement a class `DatabaseSerializer` with the following methods: - `__init__(self, db_path: str)`: initializes the class with the path to the SQLite database file. - `serialize_and_store(self, obj: Any, obj_id: str)`: serializes the given Python object using `pickle` and stores it in the SQLite database with the given `obj_id` as the key. - `retrieve_and_deserialize(self, obj_id: str) -> Any`: retrieves the serialized object from the database with the given `obj_id`, deserializes it using `pickle`, and returns the original Python object. **Constraints**: - The database should have a single table named `serialized_objects` with two columns: `obj_id` (TEXT, PRIMARY KEY) and `data` (BLOB). - Ensure proper handling of exceptions that might occur during database operations or serialization/deserialization process. - The `serialize_and_store` method should overwrite the data if `obj_id` already exists. - You are restricted to using only the standard Python library. **Input/Output**: - The `serialize_and_store` method takes two inputs: a Python object `obj` and a string `obj_id`. - The `retrieve_and_deserialize` method takes a string `obj_id` as input and retrieves the deserialized Python object. **Performance Requirements**: - Ensure that database connections are properly managed without causing any leaks or unnecessary open connections. - Efficiency in serialization and database operations is key for handling large objects or frequent operations. **Example Usage**: ```python # Define a sample class to be serialized class SampleClass: def __init__(self, name): self.name = name def greet(self): return f\\"Hello, {self.name}!\\" # Initialize the DatabaseSerializer serializer = DatabaseSerializer(\'my_database.db\') # Create an instance of the sample class obj = SampleClass(\'Alice\') # Serialize and store the object serializer.serialize_and_store(obj, \'obj_1\') # Retrieve and deserialize the object retrieved_obj = serializer.retrieve_and_deserialize(\'obj_1\') # Verify the object\'s functionality print(retrieved_obj.greet()) # Output: Hello, Alice! ``` Implement the `DatabaseSerializer` class to complete this task. **Note**: Ensure that the database schema is created if it does not exist when initializing `DatabaseSerializer`.","solution":"import sqlite3 import pickle class DatabaseSerializer: def __init__(self, db_path: str): self.db_path = db_path self._initialize_database() def _initialize_database(self): with sqlite3.connect(self.db_path) as conn: cursor = conn.cursor() cursor.execute(\'\'\'CREATE TABLE IF NOT EXISTS serialized_objects (obj_id TEXT PRIMARY KEY, data BLOB)\'\'\') conn.commit() def serialize_and_store(self, obj, obj_id: str): serialized_obj = pickle.dumps(obj) with sqlite3.connect(self.db_path) as conn: cursor = conn.cursor() cursor.execute(\'\'\'INSERT OR REPLACE INTO serialized_objects (obj_id, data) VALUES (?, ?)\'\'\', (obj_id, serialized_obj)) conn.commit() def retrieve_and_deserialize(self, obj_id: str): with sqlite3.connect(self.db_path) as conn: cursor = conn.cursor() cursor.execute(\'\'\'SELECT data FROM serialized_objects WHERE obj_id = ?\'\'\', (obj_id,)) result = cursor.fetchone() if result is None: raise ValueError(f\\"No object found with id {obj_id}\\") serialized_obj = result[0] return pickle.loads(serialized_obj)"},{"question":"# Python Coding Assessment **Objective:** Demonstrate your understanding of the `email.headerregistry` module and its usage for handling different types of email headers conforming to RFC standards. **Problem Statement:** You are required to implement a Python function `create_email_headers(header_data)` that takes a dictionary `header_data` as input. The key is the header name, and the value is the header value. This function should use the `email.headerregistry` module to create appropriate header objects based on the type of the header and return a dictionary where the keys are the header names and the values are the corresponding string representations of the header objects. The function should handle the following header types: 1. `Subject`: Uses `UnstructuredHeader`. 2. `Date`: Uses `DateHeader`. 3. `From` and `To`: Use `AddressHeader`. 4. `Content-Type`: Uses `ContentTypeHeader`. **Function Signature:** ```python def create_email_headers(header_data: dict) -> dict: ``` **Input:** - `header_data` (dict): A dictionary where the key is a string representing the header name and the value is an appropriate value for that header. **Output:** - Returns a dictionary where keys are the header names and the values are the string representations of the respective header objects. **Constraints:** - Assume all necessary imports from the `email` package are done within the function. - The `header_data` dictionary will contain only valid header names and will strictly follow the mentioned header types. - If a header value does not conform to the expected type, it should raise an appropriate exception. **Example:** ```python from datetime import datetime header_data = { \\"Subject\\": \\"Meeting Reminder\\", \\"Date\\": datetime(2021, 10, 31, 10, 30), \\"From\\": \\"Alice <alice@example.com>\\", \\"To\\": \\"Bob <bob@example.com>\\", \\"Content-Type\\": \\"text/plain; charset=\\"utf-8\\"\\" } expected_output = { \\"Subject\\": \\"Meeting Reminder\\", \\"Date\\": \\"Sun, 31 Oct 2021 10:30:00 -0000\\", \\"From\\": \\"Alice <alice@example.com>\\", \\"To\\": \\"Bob <bob@example.com>\\", \\"Content-Type\\": \\"text/plain; charset=\\"utf-8\\"\\" } ``` The returned dictionary must contain string representations of the headers using their specific formatting as described by RFC standards. **Notes:** - Use the `HeaderRegistry` class to obtain the appropriate header classes. - Handle datetime objects specifically for the `Date` header. - Address headers should accurately reflect the display name and email address. Completion Requirements: - Correctly identify and create instances of the respective header classes. - Properly convert and return the string representation of each header. Good luck!","solution":"from email.headerregistry import HeaderRegistry, Address from datetime import datetime def create_email_headers(header_data): registry = HeaderRegistry() headers = {} for header, value in header_data.items(): if header == \'Subject\': headers[header] = str(registry(\'Subject\', value)) elif header == \'Date\': if not isinstance(value, datetime): raise ValueError(f\'Invalid value for Date header: {value}\') headers[header] = str(registry(\'Date\', value)) elif header in [\'From\', \'To\']: headers[header] = str(registry(header, value)) elif header == \'Content-Type\': headers[header] = str(registry(\'Content-Type\', value)) else: raise ValueError(f\'Unsupported header type: {header}\') return headers"},{"question":"Objective: The objective of this task is to evaluate your understanding of Seaborn\'s `scatterplot` and `relplot` functions for creating and customizing scatter plots. Problem Statement: You are provided with the \\"tips\\" dataset, which contains information about restaurant bills and tips. Using this dataset, complete the following tasks: 1. Create a scatter plot using Seaborn with `total_bill` on the x-axis and `tip` on the y-axis. Color the points based on the `day` variable and use different markers for the `time` variable (Lunch or Dinner). 2. Customize the scatter plot created in step 1 by setting the marker size to 150, the marker edge color to black, and the marker face color to a semi-transparent red (`rgba(255, 0, 0, 0.5)`). 3. Create a faceted scatter plot using Seaborn\'s `relplot`, where you plot the relationship between `total_bill` and `tip` across different values of the `sex` variable in separate columns. Use the `hue` parameter to color the points based on the `smoker` variable and vary the point style based on the `day` variable. Requirements: 1. Use Seaborn\'s `scatterplot` and `relplot` functions only. 2. The code should be well commented and should follow best practices. 3. Ensure that the created plots have appropriate labels and legends. Expected Output: 1. A customized scatter plot considering the given conditions. 2. A faceted scatter plot with subplots grouped by the `sex` variable, with additional customizations as specified. Constraints: - Use the \\"tips\\" dataset provided by Seaborn. - Ensure the marker size, color, and other customizations reflect the problem statement accurately. Implementation: ```python import seaborn as sns import matplotlib.pyplot as plt # Load the tips dataset tips = sns.load_dataset(\\"tips\\") # Task 1: Create a scatter plot plt.figure(figsize=(10, 6)) scatter_plot = sns.scatterplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"day\\", style=\\"time\\", s=150, edgecolor=\\"black\\", facecolor=\'rgba(255, 0, 0, 0.5)\') plt.title(\\"Scatter Plot of Total Bill vs Tip\\") plt.xlabel(\\"Total Bill\\") plt.ylabel(\\"Tip\\") plt.legend() plt.show() # Task 2: Create a faceted scatter plot using relplot rel_plot = sns.relplot( data=tips, x=\\"total_bill\\", y=\\"tip\\", col=\\"sex\\", hue=\\"smoker\\", style=\\"day\\", kind=\\"scatter\\" ) rel_plot.fig.suptitle(\\"Faceted Scatter Plot of Total Bill vs Tip by Sex\\") rel_plot.set_axis_labels(\\"Total Bill\\", \\"Tip\\") plt.show() ```","solution":"import seaborn as sns import matplotlib.pyplot as plt # Load the tips dataset tips = sns.load_dataset(\\"tips\\") # Task 1: Create a scatter plot plt.figure(figsize=(10, 6)) scatter_plot = sns.scatterplot( data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"day\\", style=\\"time\\", s=150, edgecolor=\\"black\\", palette=\'deep\' ) plt.title(\\"Scatter Plot of Total Bill vs Tip\\") plt.xlabel(\\"Total Bill\\") plt.ylabel(\\"Tip\\") plt.legend() plt.show() # Task 2: Create a faceted scatter plot using relplot rel_plot = sns.relplot( data=tips, x=\\"total_bill\\", y=\\"tip\\", col=\\"sex\\", hue=\\"smoker\\", style=\\"day\\", kind=\\"scatter\\", height=6, aspect=1.0 ) rel_plot.fig.suptitle(\\"Faceted Scatter Plot of Total Bill vs Tip by Sex\\", y=1.03) rel_plot.set_axis_labels(\\"Total Bill\\", \\"Tip\\") plt.show()"},{"question":"Email MIME Object Manipulation and Construction You have been provided the tools to manage email MIME objects using Python\'s `email.mime` package. Your task is to create and manipulate a MIME message with the following requirements: 1. **Construct an Email Message**: - The email should be a multipart message. - It should contain a plain text part, an HTML part, and an image attachment. - The plain text part should contain the text: \\"This is the plain text part of the email.\\" - The HTML part should contain the following HTML content: \\"<html><body><h1>This is the HTML part of the email.</h1></body></html>\\" - The image part should be constructed from a provided byte string representing image data. 2. **Function Signature**: Implement the function `create_mime_email`: ```python def create_mime_email(image_data: bytes) -> str: Constructs a multipart email message with a plain text part, an HTML part, and an image attachment. Parameters: - image_data (bytes): the byte string representing image data. Returns: - str: the complete MIME email message in string format. pass ``` 3. **Constraints and Requirements**: - Use `MIMEMultipart` for creating the multipart container. - Use `MIMEText` for both the plain text and HTML parts. - Use `MIMEImage` for the image part. - Ensure proper encoding and headers for each part. - Return the complete email message as a string using the `as_string` method of the top-level message object. 4. **Example Usage**: ```python image_data = b\'x89PNGrnx1anx00x00x00rIHDRx00...\' email_message = create_mime_email(image_data) print(email_message) ``` This example should output the MIME email message string that includes the specified plain text, HTML, and image parts. Evaluation Criteria: - Correctness: Ensure each part of the email message is properly constructed and appended. - Completeness: The returned string should represent a correctly formatted MIME email. - Efficiency: Handle encoding and MIME creation efficiently without unnecessary computation. - Readability: Use clear and maintainable code. Good luck!","solution":"from email.mime.multipart import MIMEMultipart from email.mime.text import MIMEText from email.mime.image import MIMEImage def create_mime_email(image_data: bytes) -> str: Constructs a multipart email message with a plain text part, an HTML part, and an image attachment. Parameters: - image_data (bytes): the byte string representing image data. Returns: - str: the complete MIME email message in string format. # Create the multipart container msg = MIMEMultipart() # Create the plain text part text_part = MIMEText(\\"This is the plain text part of the email.\\", \\"plain\\") msg.attach(text_part) # Create the HTML part html_part = MIMEText(\\"<html><body><h1>This is the HTML part of the email.</h1></body></html>\\", \\"html\\") msg.attach(html_part) # Create the image part image_part = MIMEImage(image_data) msg.attach(image_part) # Return the email message as a string return msg.as_string()"},{"question":"# Question: File System Organizer and Analyzer You are tasked with writing a Python function that organizes and analyzes a directory containing multiple files of various types. Your function should achieve the following: 1. **Organize Files**: Move files into subdirectories based on their file types (e.g., `.txt` files go into a \\"TextFiles\\" subdirectory, `.json` files go into a \\"JsonFiles\\" subdirectory, etc.). If a subdirectory does not exist, create it. 2. **Generate Report**: After organizing the files, generate a summary report in CSV format that contains the following information for each file: - **File Name** - **File Type** - **File Size** (in bytes) - **Original Location** - **New Location** 3. **Log Report**: Save the generated CSV summary report in the specified directory and also output its path. # Function Signature ```python import os import shutil import csv import json def organize_and_generate_report(directory: str, report_filename: str) -> str: pass ``` # Input - `directory` (str): The path to the directory containing the files to be organized. - `report_filename` (str): The name of the CSV file where the summary report should be saved. # Output - The function should return the path to the CSV summary report file. # Constraints 1. You can assume that the input directory exists and contains files. 2. You can assume that the files have extensions that clearly indicate their types (e.g., `.txt`, `.json`, etc.). 3. The function should handle any number of files within the directory. 4. The function should only organize files at the top level of the specified directory (i.e., it should not recurse into subdirectories). 5. The CSV report should be properly formatted and readable. # Example Usage ```python organize_and_generate_report(\\"/path/to/directory\\", \\"report.csv\\") ``` # Example Output Assuming the input directory contains: ``` file1.txt file2.txt data1.json image.png ``` Post execution, the function should: - Move `.txt` files to a `TextFiles` subdirectory. - Move `.json` files to a `JsonFiles` subdirectory. - Move `.png` files to an `Images` subdirectory. Generate a `report.csv` in the specified directory with content structured similarly to: ```csv File Name,File Type,File Size,Original Location,New Location file1.txt,Text File,123,/path/to/directory/file1.txt,/path/to/directory/TextFiles/file1.txt file2.txt,Text File,234,/path/to/directory/file2.txt,/path/to/directory/TextFiles/file2.txt data1.json,JSON File,3456,/path/to/directory/data1.json,/path/to/directory/JsonFiles/data1.json image.png,Image File,7890,/path/to/directory/image.png,/path/to/directory/Images/image.png ``` **Note:** The `File Type` column in the CSV should provide a human-readable format (e.g., Text File for `.txt`, JSON File for `.json`, etc.) # Hints - Use the `os` and `shutil` modules for handling file movement and directory creation. - Leverage the `csv` module to create the summary report. - `os.path` functions can be useful to handle path manipulations.","solution":"import os import shutil import csv def organize_and_generate_report(directory: str, report_filename: str) -> str: # Map file extensions to directory names and human-readable file types file_mapping = { \'.txt\': (\'TextFiles\', \'Text File\'), \'.json\': (\'JsonFiles\', \'JSON File\'), \'.png\': (\'Images\', \'Image File\'), \'.jpg\': (\'Images\', \'Image File\'), \'.jpeg\': (\'Images\', \'Image File\') } report_data = [] for filename in os.listdir(directory): file_path = os.path.join(directory, filename) if os.path.isfile(file_path): file_size = os.path.getsize(file_path) file_extension = os.path.splitext(filename)[1] subdirectory, file_type = file_mapping.get(file_extension, (\'OtherFiles\', \'Other File\')) new_directory = os.path.join(directory, subdirectory) if not os.path.exists(new_directory): os.makedirs(new_directory) new_location = os.path.join(new_directory, filename) shutil.move(file_path, new_location) report_data.append([filename, file_type, file_size, file_path, new_location]) report_path = os.path.join(directory, report_filename) with open(report_path, \'w\', newline=\'\', encoding=\'utf-8\') as csvfile: csvwriter = csv.writer(csvfile) csvwriter.writerow([\'File Name\', \'File Type\', \'File Size\', \'Original Location\', \'New Location\']) csvwriter.writerows(report_data) return report_path"},{"question":"**Question: Implementing a Multi-threaded Prime Number Finder with Logging** **Objective**: The objective of this task is to implement a multi-threaded prime number finder that leverages Python\'s threading and logging modules. You will also use collections for efficient data handling. Your implementation should demonstrate a solid understanding of multi-threading, synchronization, and logging in Python. **Problem Statement**: 1. Create a function `is_prime(n)` that takes an integer `n` and returns `True` if `n` is a prime number, `False` otherwise. 2. Implement a class `PrimeFinder` that: - Accepts an integer `max_num` to find all prime numbers up to `max_num`. - Uses multiple threads to divide the workload for checking prime numbers from 2 to `max_num`. - Logs the progress and results using the logging module. **Class Details**: - `is_prime(n: int) -> bool`: A static method that checks if `n` is prime. - `__init__(self, max_num: int, num_threads: int)`: Initializes the object with `max_num` and `num_threads`, where `num_threads` is the number of threads to be used. - `find_primes(self) -> List[int]`: Finds and returns all prime numbers up to `max_num`. This method should: - Start multiple threads to perform the calculations. - Use thread-safe data structures from the `collections` module to store intermediate results. - Log the start, progress, and completion of each thread. **Constraints**: - `max_num` should be a positive integer greater than 1. - `num_threads` should be a positive integer less than or equal to `max_num`. **Output**: - A list of all prime numbers up to `max_num`. **Example Usage**: ```python if __name__ == \\"__main__\\": logging.basicConfig(level=logging.INFO) max_num = 100 num_threads = 4 prime_finder = PrimeFinder(max_num, num_threads) primes = prime_finder.find_primes() print(primes) ``` **Expected Output**: ``` [2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61, 67, 71, 73, 79, 83, 89, 97] ``` **Notes**: - Ensure thread safety when multiple threads write to shared data structures. - Log progress information such as the number ranges each thread is processing, any synchronization points, and the final result. - Optimize the assignment of number ranges to threads to balance the workload.","solution":"import threading import logging from collections import deque from typing import List class PrimeFinder: @staticmethod def is_prime(n: int) -> bool: if n <= 1: return False if n <= 3: return True if n % 2 == 0 or n % 3 == 0: return False i = 5 while i * i <= n: if n % i == 0 or n % (i + 2) == 0: return False i += 6 return True def __init__(self, max_num: int, num_threads: int): if max_num <= 1: raise ValueError(\\"max_num should be greater than 1.\\") if num_threads <= 0 or num_threads > max_num: raise ValueError(\\"num_threads should be a positive integer less than or equal to max_num.\\") self.max_num = max_num self.num_threads = num_threads self.primes = deque() self.lock = threading.Lock() logging.basicConfig(level=logging.INFO) def find_primes(self) -> List[int]: def worker(start: int, end: int) -> None: logging.info(f\\"Thread starting range {start} to {end}\\") local_primes = [] for num in range(start, end + 1): if PrimeFinder.is_prime(num): local_primes.append(num) with self.lock: self.primes.extend(local_primes) logging.info(f\\"Thread finished range {start} to {end}\\") threads = [] step = (self.max_num - 1) // self.num_threads + 1 for i in range(self.num_threads): start = 2 + i * step end = min(2 + (i + 1) * step - 1, self.max_num) thread = threading.Thread(target=worker, args=(start, end)) threads.append(thread) thread.start() for thread in threads: thread.join() logging.info(\\"All threads have finished.\\") return sorted(list(self.primes))"},{"question":"Objective Assess your understanding of evaluating machine learning models using scikit-learn\'s scoring metrics and implementing custom scoring functions. Problem Statement As a data scientist at an e-commerce company, you are tasked with building a predictive model to forecast the daily sales of a product based on historical sales data and various external factors, including marketing spend, seasonality, and economic indicators. You are required to: 1. **Build a regression model** using any method of your choice (e.g., Linear Regression, Decision Trees, Gradient Boosting, etc.). 2. **Evaluate the model** using both default sklearn metrics and a custom metric. Tasks 1. **Data Preparation**: - Generate a synthetic dataset. - Features (X): marketing spend, day of the week (encoded as 0 to 6), economic indicator (random values). - Target (y): daily sales (random values with some added noise). - Split the dataset into training and testing sets. 2. **Model Training**: - Train a regression model on the training data. 3. **Model Evaluation**: - Evaluate the model performance on the test set using the following metrics: - Mean Absolute Error (MAE) - Mean Squared Error (MSE) - Custom Metric: Implement a custom scoring function to calculate the Root Mean Squared Log Error (RMSLE). 4. **Visualization**: - Plot the predicted vs actual values for the test set. - Plot the residuals vs predicted values for the test set. Expected Input/Output **Input**: - Synthetic dataset with features and target values. **Output**: - Printed values of MAE, MSE, and RMSLE for the test set. - Visualizations: - Plot of predicted vs actual values. - Plot of residuals vs predicted values. Constraints - Implement the custom scoring function for RMSLE. - Use matplotlib or any other visualization library for plotting. Example ```python import numpy as np import matplotlib.pyplot as plt from sklearn.model_selection import train_test_split from sklearn.metrics import mean_absolute_error, mean_squared_error from sklearn.ensemble import GradientBoostingRegressor # 1. Generate synthetic dataset np.random.seed(42) X = np.random.randn(1000, 3) y = np.random.rand(1000) * 200 # 2. Split the data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # 3. Train the model model = GradientBoostingRegressor() model.fit(X_train, y_train) # 4. Model Evaluation with default metrics y_pred = model.predict(X_test) mae = mean_absolute_error(y_test, y_pred) mse = mean_squared_error(y_test, y_pred) # 5. Custom RMSLE metric def rmsle(y_true, y_pred): return np.sqrt(np.mean((np.log1p(y_true) - np.log1p(y_pred))**2)) rmsle_value = rmsle(y_test, y_pred) print(f\\"MAE: {mae}\\") print(f\\"MSE: {mse}\\") print(f\\"RMSLE: {rmsle_value}\\") # 6. Visualization plt.figure(figsize=(10, 5)) # Predicted vs Actual plt.subplot(1, 2, 1) plt.scatter(y_test, y_pred) plt.xlabel(\'Actual Values\') plt.ylabel(\'Predicted Values\') plt.title(\'Predicted vs Actual Values\') # Residuals vs Predicted residuals = y_test - y_pred plt.subplot(1, 2, 2) plt.scatter(y_pred, residuals) plt.xlabel(\'Predicted Values\') plt.ylabel(\'Residuals\') plt.title(\'Residuals vs Predicted Values\') plt.tight_layout() plt.show() ```","solution":"import numpy as np import matplotlib.pyplot as plt from sklearn.model_selection import train_test_split from sklearn.metrics import mean_absolute_error, mean_squared_error from sklearn.ensemble import GradientBoostingRegressor # 1. Generate synthetic dataset np.random.seed(42) X = np.random.randn(1000, 3) y = (X[:, 0] * 50) + (X[:, 1] * 30) + np.random.randn(1000) * 10 + 100 # 2. Split the data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # 3. Train the model model = GradientBoostingRegressor() model.fit(X_train, y_train) # 4. Model Evaluation with default metrics y_pred = model.predict(X_test) mae = mean_absolute_error(y_test, y_pred) mse = mean_squared_error(y_test, y_pred) # 5. Custom RMSLE metric def rmsle(y_true, y_pred): return np.sqrt(np.mean((np.log1p(y_true) - np.log1p(y_pred))**2)) rmsle_value = rmsle(y_test, y_pred) print(f\\"MAE: {mae}\\") print(f\\"MSE: {mse}\\") print(f\\"RMSLE: {rmsle_value}\\") # 6. Visualization plt.figure(figsize=(10, 5)) # Predicted vs Actual plt.subplot(1, 2, 1) plt.scatter(y_test, y_pred) plt.xlabel(\'Actual Values\') plt.ylabel(\'Predicted Values\') plt.title(\'Predicted vs Actual Values\') # Residuals vs Predicted residuals = y_test - y_pred plt.subplot(1, 2, 2) plt.scatter(y_pred, residuals) plt.xlabel(\'Predicted Values\') plt.ylabel(\'Residuals\') plt.title(\'Residuals vs Predicted Values\') plt.tight_layout() plt.show()"},{"question":"# Advanced Class Creation with `types` Module Problem Statement You are required to dynamically create a Python class with specific attributes and methods using the `types` module. The class should also incorporate a custom metaclass that enforces a particular constraint on the class. Specifically, follow these steps: 1. **Create a Metaclass**: - Define a metaclass `LengthEnforcerMeta` that ensures any class using this metaclass has an attribute named `length` which must be an integer greater than 0. 2. **Dynamically Create a Class**: - Dynamically create a class named `DynamicClass` using the `types.new_class` function. - This class should inherit from a base class `BaseClass` which has an attribute `name` set to `\'Base\'` and a method `get_name` that returns the value of `name`. - Add an attribute `length` with a value of 10 to `DynamicClass`. - Add a method `double_length` that returns twice the value of the `length` attribute. 3. **Instantiate and Test**: - Instantiate `DynamicClass` and verify that: - The instance has an attribute `length` equal to 10. - The method `double_length` returns 20. - The method `get_name` returns `\'Base\'`. Constraints - Use the provided `types.new_class` and metaclass mechanics from the `types` module. - Ensure `LengthEnforcerMeta` correctly raises an error if the `length` attribute is missing or not a positive integer. Example ```python # Define the BaseClass class BaseClass: name = \'Base\' def get_name(self): return self.name # Define the metaclass class LengthEnforcerMeta(type): def __init__(cls, name, bases, namespace): super().__init__(name, bases, namespace) if not isinstance(getattr(cls, \'length\', None), int) or getattr(cls, \'length\') <= 0: raise TypeError(f\\"{name} class must have an attribute \'length\' that is a positive integer.\\") # Define the exec_body function for new_class def exec_body(namespace): namespace[\'length\'] = 10 def double_length(self): return 2 * self.length namespace[\'double_length\'] = double_length # Dynamically create DynamicClass DynamicClass = types.new_class( \'DynamicClass\', bases=(BaseClass,), kwds={\'metaclass\': LengthEnforcerMeta}, exec_body=exec_body ) # Instantiate and test instance = DynamicClass() assert instance.length == 10 assert instance.double_length() == 20 assert instance.get_name() == \'Base\' ``` Implement the above in Python, adhering to the requirements. Deliverables - The implementation of the `LengthEnforcerMeta` metaclass. - The dynamic creation of the `DynamicClass` using `types.new_class`. - A sequence of statements demonstrating the instantiation of `DynamicClass` and verification of its attributes and methods.","solution":"import types # Define the BaseClass class BaseClass: name = \'Base\' def get_name(self): return self.name # Define the metaclass class LengthEnforcerMeta(type): def __init__(cls, name, bases, namespace): super().__init__(name, bases, namespace) if not isinstance(getattr(cls, \'length\', None), int) or getattr(cls, \'length\') <= 0: raise TypeError(f\\"{name} class must have an attribute \'length\' that is a positive integer.\\") # Define the exec_body function for new_class def exec_body(namespace): namespace[\'length\'] = 10 def double_length(self): return 2 * self.length namespace[\'double_length\'] = double_length # Dynamically create DynamicClass DynamicClass = types.new_class( \'DynamicClass\', bases=(BaseClass,), kwds={\'metaclass\': LengthEnforcerMeta}, exec_body=exec_body ) # Instantiate and test instance = DynamicClass() instance.length # should equal 10 instance.double_length() # should equal 20 instance.get_name() # should equal \'Base\'"},{"question":"**Task: Initialization, Finalization, and Thread Local Storage Management** # Objective: Write a C program that embeds Python and demonstrates proper initialization and finalization of the Python interpreter, threading, and managing thread-local storage (TSS). The program should spawn multiple threads, each performing a simple Python operation. # Details: 1. **Initialization and Finalization**: - Use `Py_Initialize()` to initialize the Python interpreter. - Handle Python\'s global configuration variables by setting `Py_BytesWarningFlag` and `Py_IgnoreEnvironmentFlag`. - Finalize the Python interpreter using `Py_FinalizeEx()`. 2. **Thread Management**: - Use `PyGILState_Ensure()` and `PyGILState_Release()` for managing the global interpreter lock (GIL). - Each thread should be registered with the Python interpreter using `PyGILState_STATE`. 3. **Thread Specific Storage (TSS)**: - Demonstrate the creation and usage of `Py_tss_t`. - Each thread should store a unique value using TSS that identifies it. 4. **Sub-interpreters**: - Additionally, create a sub-interpreter using `Py_NewInterpreter()`, make it execute a different Python code, and then destroy it using `Py_EndInterpreter()`. # Requirements: - The program should create 5 threads. - Each thread should execute a Python script that prints a unique thread ID and increments a counter. - Ensure proper locking mechanisms to avoid race conditions when accessing Python objects. - Clearly handle creation, usage, and deletion of TSS keys. # Example Implementation Outline: Here is an outline to get you started: ```c #include <Python.h> #include <stdio.h> #include <stdlib.h> #include <pthread.h> #define NUM_THREADS 5 // Thread specific storage key Py_tss_t tss_key = Py_tss_NEEDS_INIT; void* thread_func(void* arg) { // Initialize thread state and acquire GIL PyGILState_STATE gstate; gstate = PyGILState_Ensure(); // Create a new TSS key if it hasn\'t been created already if (!PyThread_tss_is_created(&tss_key)) { PyThread_tss_create(&tss_key); } // Store the unique ID in the TSS int thread_id = *((int*)arg); PyThread_tss_set(&tss_key, (void*)thread_id); // Print thread-specific ID int stored_id = (int)PyThread_tss_get(&tss_key); printf(\\"Thread %d: ID = %dn\\", thread_id, stored_id); // Run Python code PyRun_SimpleString(\\"print(\'Hello from Python script running in thread!\')\\"); // Release GIL and finalize thread state PyGILState_Release(gstate); return NULL; } int main() { // Initialize Python interpreter Py_Initialize(); // Set global configuration variables Py_BytesWarningFlag = 1; // issue warning on comparing bytes with str Py_IgnoreEnvironmentFlag = 1; // ignore environment variables // Create threads pthread_t threads[NUM_THREADS]; int thread_ids[NUM_THREADS]; for (int i = 0; i < NUM_THREADS; i++) { thread_ids[i] = i + 1; pthread_create(&threads[i], NULL, thread_func, &thread_ids[i]); } // Join threads for (int i = 0; i < NUM_THREADS; i++) { pthread_join(threads[i], NULL); } // Clean up TSS PyThread_tss_delete(&tss_key); PyThread_tss_free(&tss_key); // Finalize Python interpreter Py_FinalizeEx(); return 0; } ``` **Note:** - Ensure proper error checking for all API calls. - Test the program thoroughly for thread safety and correct behavior. # Submission: Provide the complete C code and any accompanying Python script, if necessary, to demonstrate the functionality described above.","solution":"import threading import time # Simulate thread-local storage thread_local_storage = threading.local() # Counter to be incremented by each thread counter = 0 # Function to be executed by each thread def thread_function(thread_id): global counter # Initialize thread-local data thread_local_storage.id = thread_id # Simulate Python initialization print(f\\"Thread {thread_local_storage.id} - Python initialization\\") # Perform a simple operation print(f\\"Thread {thread_local_storage.id} executing simple Python operation\\") # Increment the global counter time.sleep(0.1) # Ensure context switch counter += 1 # Simulate Python finalization print(f\\"Thread {thread_local_storage.id} - Python finalization\\") # Main function to manage threads def main(): # List to hold references to threads threads = [] # Initialize and start threads for i in range(5): thread = threading.Thread(target=thread_function, args=(i,)) threads.append(thread) thread.start() # Wait for all threads to complete for thread in threads: thread.join() # Check the global counter value return counter"},{"question":"You are tasked with building a Python function using PyTorch to save multiple tensors efficiently. Specifically, you will: 1. Create a set of tensors with defined relationships. 2. Save these tensors to a file while ensuring that the file size is minimized by managing views appropriately. 3. Reload the tensors and verify the integrity of their relationships. # Requirements 1. **Function Name**: `save_and_load_tensors` 2. **Inputs**: - `file_path` (str): The file path where the tensors will be saved. - `tensors` (dict): A dictionary where the keys are tensor names (strings) and values are PyTorch tensors. 3. **Outputs**: - `loaded_tensors` (dict): A dictionary where the keys are tensor names (strings) and values are the loaded PyTorch tensors. # Constraints 1. You should ensure that the file size is minimized by appropriately handling tensor views before saving. 2. Maintain the relationships (views) among tensors during saving and loading. 3. Use the PyTorch functions for serialization (`torch.save`, `torch.load`). # Example ```python import torch def save_and_load_tensors(file_path, tensors): # Step 1: Clone tensors if necessary to minimize file size cloned_tensors = {} for name, tensor in tensors.items(): cloned_tensors[name] = tensor.clone() # Step 2: Save cloned tensors along with original relationship information torch.save(cloned_tensors, file_path) # Step 3: Load tensors from file loaded_tensors = torch.load(file_path) # Step 4: Return loaded tensors return loaded_tensors # Example usage: tensors = { \\"numbers\\": torch.arange(1, 10), \\"evens\\": torch.arange(1, 10)[1::2], } file_path = \\"tensors.pt\\" loaded_tensors = save_and_load_tensors(file_path, tensors) # Check relationships assert torch.equal(loaded_tensors[\\"evens\\"], loaded_tensors[\\"numbers\\"][1::2]) print(\\"Loaded tensors:\\", loaded_tensors) ``` In `save_and_load_tensors`, you will implement the following steps: 1. Preprocess the tensors to minimize the file size before saving. 2. Save the preprocessed tensors to the specified file path. 3. Load the tensors from the specified file path. 4. Return the loaded tensors, ensuring their integrity. Ensure to handle tensor relationship preservation and efficient file size management as outlined in the provided documentation.","solution":"import torch def save_and_load_tensors(file_path, tensors): # Step 1: Clone tensors when necessary to minimize file size cloned_tensors = {} for name, tensor in tensors.items(): cloned_tensors[name] = tensor.clone() if not tensor.is_contiguous() else tensor # Step 2: Save cloned tensors along with original relationship information (meta-data) metadata = {name: tensor.size() for name, tensor in tensors.items()} data_to_save = {\'tensors\': cloned_tensors, \'metadata\': metadata} torch.save(data_to_save, file_path) # Step 3: Load tensors and metadata from file loaded_data = torch.load(file_path) loaded_tensors = loaded_data[\'tensors\'] return loaded_tensors"},{"question":"# HTTP Client Challenge Using the `http.client` module in Python, write a function called `perform_requests` that performs a series of HTTP requests to a given server and handles both successful and erroneous responses. Function Signature ```python def perform_requests(host: str, requests: list[tuple[str, str, dict, bytes]]) -> dict: pass ``` # Parameters - `host`: A string representing the server hostname (e.g., \'www.python.org\'). - `requests`: A list of tuples, each containing: - `method`: The HTTP method as a string (`\'GET\'`, `\'POST\'`, `\'PUT\'`, etc.). - `url`: The URL path as a string (e.g., `\'/\'`, `\'/index.html\'`). - `headers`: A dictionary of headers to send with the request. - `body`: The request body as bytes for methods like `\'POST\'` and `\'PUT\'` (can be empty for `\'GET\'` and `\'HEAD\'`). # Returns - A dictionary where the keys are the URLs requested, and the values are sub-dictionaries containing: - `status`: The HTTP status code received from the server. - `reason`: The reason phrase associated with the status code. - `response_headers`: A dictionary of response headers. - `body`: The response body as bytes. # Behavior 1. For each request in the `requests` list, the function should: - Create an HTTPConnection object. - Send the request using the given method, URL, headers, and body. - Retrieve the response and store the status, reason, response headers, and body in the result dictionary. - Handle and log exceptions appropriately using the exceptions provided in `http.client`. 2. Ensure that all connections are properly closed after the requests are completed. # Constraints - You may assume that the `host` and `url` parameters are well-formed. - You may not use external libraries such as `requests`; only the `http.client` module should be used. - Properly handle common HTTP errors and exceptions defined in the `http.client` module. # Example ```python requests = [ (\'GET\', \'/\', {}, b\'\'), (\'POST\', \'/submit\', {\'Content-type\': \'application/x-www-form-urlencoded\'}, b\'field=value\') ] result = perform_requests(\'www.example.com\', requests) # Example output # { # \'/\': { # \'status\': 200, # \'reason\': \'OK\', # \'response_headers\': {\'Content-Length\': \'1234\', \'Content-Type\': \'text/html; charset=UTF-8\'}, # \'body\': b\'<html>...</html>\' # }, # \'/submit\': { # \'status\': 302, # \'reason\': \'Found\', # \'response_headers\': {\'Location\': \'https://www.example.com/redirect\'}, # \'body\': b\'Redirecting to <a href=\\"https://www.example.com/redirect\\">...</a>\' # } # } ```","solution":"import http.client from typing import List, Tuple, Dict def perform_requests(host: str, requests: List[Tuple[str, str, Dict[str, str], bytes]]) -> Dict[str, Dict[str, object]]: results = {} for method, url, headers, body in requests: try: conn = http.client.HTTPConnection(host) conn.request(method, url, body, headers) response = conn.getresponse() response_headers = {k: v for k, v in response.getheaders()} results[url] = { \'status\': response.status, \'reason\': response.reason, \'response_headers\': response_headers, \'body\': response.read() } except (http.client.HTTPException, OSError) as e: results[url] = { \'status\': None, \'reason\': str(e), \'response_headers\': {}, \'body\': b\'\' } finally: conn.close() return results"},{"question":"Title: Creating a Custom Plot with seaborn using Advanced Properties # Background Seaborn is a powerful data visualization library based on matplotlib that provides a high-level interface for drawing attractive and informative statistical graphics. In this task, you will be required to use seaborn’s advanced features to create a multi-faceted plot. # Task You are given an airline dataset containing monthly average number of passengers (in thousands) over a span of years across two airlines. Your task is to create a customized comparative plot with seaborn that uses various seaborn properties to enhance the visual representation. # Dataset The dataset (`airline_data.csv`) contains the following columns: - `Month`: The month of the year. - `Year`: The year. - `Airline_A`: Monthly average number of passengers (in thousands) for Airline A. - `Airline_B`: Monthly average number of passengers (in thousands) for Airline B. You can generate a sample dataset for testing: ```python import pandas as pd import numpy as np data = { \'Month\': np.tile(np.arange(1, 13), 10), \'Year\': np.repeat(np.arange(2010, 2020), 12), \'Airline_A\': np.random.uniform(100, 200, 120), \'Airline_B\': np.random.uniform(50, 150, 120) } df = pd.DataFrame(data) df.to_csv(\'airline_data.csv\', index=False) ``` # Requirements 1. **Load and prepare the data:** Load the dataset and create a plot object. 2. **Plot monthly average passengers:** Create two subplots in a single figure: - Use a `Line` plot for Airline A with the properties: - `color`: \'blue\' - `linestyle`: \'dashed\' - `linewidth`: 2 - `marker`: \'o\' - `edgecolor`: \'black\' - Use a `Line` plot for Airline B with similar properties but set `color` to \'green\' and `linestyle` to \'solid\'. 3. **Add Data Points:** Add `Dot` marks to represent data points: - For Airline A, use: - `pointsize`: 6 - `color`: adjust based on the value using a continuous scale. - `fill`: False - For Airline B, use similar properties but `pointsize` should be 8. 4. **Customize Axes and Legends:** - Provide customized labels for X and Y axes. - Add a legend. 5. **Style Adjustment:** - Customize the plot theme using seaborn\'s `axes_style`. - Adjust tick parameters using seaborn properties. 6. **Save and display the plot:** - Save the figure in both PNG and SVG formats. # Example Function Signature ```python import pandas as pd import seaborn as sns import seaborn.objects as so import matplotlib.pyplot as plt def create_custom_plot(filename): # Your code here ``` # Expected Output Running `create_custom_plot(\'airline_data.csv\')` should: 1. Load the data from `airline_data.csv`. 2. Create a comprehensive plot as specified. 3. Save the plot in `custom_plot.png` and `custom_plot.svg`. 4. Display the plot in the notebook or script. # Constraints - You must use seaborn objects (not `sns.relplot` or `sns.catplot`) for creating the plot. - Pay attention to the aesthetic details and make sure the plot is informative and visually appealing.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt import numpy as np def create_custom_plot(filename): # Load the dataset df = pd.read_csv(filename) # Set the seaborn style sns.set_style(\\"whitegrid\\") # Create subplots fig, ax = plt.subplots(figsize=(14, 8)) # Plot for Airline A sns.lineplot( data=df, x=\'Month\', y=\'Airline_A\', ax=ax, color=\'blue\', linestyle=\'--\', linewidth=2, marker=\'o\', markeredgecolor=\'black\', label=\'Airline A\' ) # Plot for Airline B sns.lineplot( data=df, x=\'Month\', y=\'Airline_B\', ax=ax, color=\'green\', linestyle=\'-\', linewidth=2, marker=\'o\', markeredgecolor=\'black\', label=\'Airline B\' ) # Add dot marks for Airline A points_a = ax.scatter( x=df[\'Month\'], y=df[\'Airline_A\'], s=60, c=df[\'Airline_A\'], cmap=\'Blues\', edgecolor=\'black\', label=\'Airline A Points\', zorder=5 ) # Add dot marks for Airline B points_b = ax.scatter( x=df[\'Month\'], y=df[\'Airline_B\'], s=80, c=df[\'Airline_B\'], cmap=\'Greens\', edgecolor=\'black\', label=\'Airline B Points\', zorder=5 ) # Customize axes and legend ax.set_xlabel(\'Month\') ax.set_ylabel(\'Number of Passengers (in thousands)\') ax.legend() # Save and display plot plt.savefig(\'custom_plot.png\') plt.savefig(\'custom_plot.svg\') plt.show()"},{"question":"In this exercise, you are expected to write a Python script using asyncio to simulate a real-world scenario where multiple asynchronous tasks need to be managed and coordinated. # Task Description You are to implement a simulation of a simple download manager that manages multiple download tasks asynchronously. The script should: 1. Use asyncio to manage the asynchronous operations. 2. Implement a function to simulate downloading a file asynchronously. Each file takes a different time to download, and this time should be simulated using `asyncio.sleep()`. 3. Allow scheduling new downloads from a different thread while the event loop is running. 4. Properly handle each download task, ensuring no coroutine is left unawaited. 5. Log events such as the start and end of each download, and any exceptions that occur. # Implementation Details 1. **Function: `simulate_download(file_id: int, download_time: int) -> None`** - Asynchronously simulates a file download by sleeping for `download_time` seconds. - Logs the start and completion of the download using the `logging` module. 2. **Function: `schedule_download(loop: asyncio.AbstractEventLoop, file_id: int, download_time: int) -> None`** - Can be called from a different thread to schedule a new download task. - Uses `loop.call_soon_threadsafe()` to schedule `simulate_download`. 3. **Function: `main()`** - Sets up the asyncio event loop, configures logging, and runs initial download tasks. - Example of adding a download task from another thread. - Handles and logs any unhandled exceptions running within the loop. # Expected Input/Output - There is no specific input format. - The script should log messages indicating the start and completion of downloads, and exceptions if any. # Constraints - You should not use any external libraries other than asyncio and logging. - Properly handle and await all coroutines to avoid warnings. - Ensure thread-safety when scheduling tasks from different threads. - Use asyncio\'s debug mode and properly configure logging to the DEBUG level. # Example Output ``` [DEBUG] Starting download: file_id=1, download_time=3s [DEBUG] Download completed: file_id=1 [DEBUG] Starting download: file_id=2, download_time=2s [DEBUG] Download completed: file_id=2 [DEBUG] Starting download: file_id=3, download_time=1s [DEBUG] Download completed: file_id=3 ``` # Implementation Template ```python import asyncio import logging import threading # Configure logging logging.basicConfig(level=logging.DEBUG) async def simulate_download(file_id: int, download_time: int) -> None: # Log the start of the download logging.debug(f\'Starting download: file_id={file_id}, download_time={download_time}s\') # Simulate download with asyncio.sleep await asyncio.sleep(download_time) # Log the completion of the download logging.debug(f\'Download completed: file_id={file_id}\') def schedule_download(loop: asyncio.AbstractEventLoop, file_id: int, download_time: int) -> None: loop.call_soon_threadsafe(asyncio.create_task, simulate_download(file_id, download_time)) async def main(): # Enable asyncio debug mode asyncio.get_running_loop().set_debug(True) try: # Schedule initial downloads await asyncio.gather( simulate_download(1, 3), simulate_download(2, 2), simulate_download(3, 1) ) # Example of scheduling a download from another thread thread = threading.Thread(target=schedule_download, args=(asyncio.get_running_loop(), 4, 4)) thread.start() thread.join() await asyncio.sleep(5) # Give the new download time to complete except Exception as e: logging.error(f\\"Exception occurred: {e}\\") if __name__ == \'__main__\': asyncio.run(main(), debug=True) ```","solution":"import asyncio import logging import threading # Configure logging logging.basicConfig(level=logging.DEBUG) async def simulate_download(file_id: int, download_time: int) -> None: # Log the start of the download logging.debug(f\'Starting download: file_id={file_id}, download_time={download_time}s\') # Simulate download with asyncio.sleep await asyncio.sleep(download_time) # Log the completion of the download logging.debug(f\'Download completed: file_id={file_id}\') def schedule_download(loop: asyncio.AbstractEventLoop, file_id: int, download_time: int) -> None: loop.call_soon_threadsafe(asyncio.create_task, simulate_download(file_id, download_time)) async def main(): # Enable asyncio debug mode asyncio.get_running_loop().set_debug(True) try: # Schedule initial downloads await asyncio.gather( simulate_download(1, 3), simulate_download(2, 2), simulate_download(3, 1) ) # Example of scheduling a download from another thread thread = threading.Thread(target=schedule_download, args=(asyncio.get_running_loop(), 4, 4)) thread.start() thread.join() await asyncio.sleep(5) # Give the new download time to complete except Exception as e: logging.error(f\\"Exception occurred: {e}\\") if __name__ == \'__main__\': asyncio.run(main(), debug=True)"},{"question":"Text Classification with Naive Bayes Classifiers You are provided with a dataset containing text documents and their corresponding labels. Your task is to classify these documents into their respective categories using different Naive Bayes classifiers provided by scikit-learn. You will follow these steps: 1. **Data Preparation**: - Load the dataset. - Convert the text data into features using techniques like CountVectorizer or TfidfVectorizer. 2. **Model Training**: - Train various Naive Bayes classifiers: GaussianNB, MultinomialNB, ComplementNB, and BernoulliNB on the training data. 3. **Model Evaluation**: - Predict the labels of the test data using the trained models. - Evaluate the performance of each model using metrics like accuracy, precision, recall, and F1-score. 4. **Comparison and Reporting**: - Compare the performance of the different Naive Bayes classifiers. - Report which classifier performs the best for this specific task and why you think it performs better. Expected Input and Output Formats **Input**: - `train_data`: A list of strings (documents) representing the training data. - `train_labels`: A list of integers representing the labels of the training data. - `test_data`: A list of strings (documents) representing the test data. - `test_labels`: A list of integers representing the labels of the test data. **Output**: - A dictionary containing the evaluation metrics (accuracy, precision, recall, and F1-score) for each classifier. Code Skeleton ```python from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer from sklearn.naive_bayes import GaussianNB, MultinomialNB, ComplementNB, BernoulliNB from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score def load_data(): # Replace with your code to load the dataset train_data = [...] train_labels = [...] test_data = [...] test_labels = [...] return train_data, train_labels, test_data, test_labels def vectorize_data(train_data, test_data): # Example: Using CountVectorizer vectorizer = CountVectorizer() X_train = vectorizer.fit_transform(train_data) X_test = vectorizer.transform(test_data) return X_train.toarray(), X_test.toarray() def train_and_evaluate(X_train, y_train, X_test, y_test): classifiers = { \'GaussianNB\': GaussianNB(), \'MultinomialNB\': MultinomialNB(), \'ComplementNB\': ComplementNB(), \'BernoulliNB\': BernoulliNB() } results = {} for name, clf in classifiers.items(): clf.fit(X_train, y_train) y_pred = clf.predict(X_test) results[name] = { \'accuracy\': accuracy_score(y_test, y_pred), \'precision\': precision_score(y_test, y_pred, average=\'weighted\'), \'recall\': recall_score(y_test, y_pred, average=\'weighted\'), \'f1_score\': f1_score(y_test, y_pred, average=\'weighted\') } return results def main(): train_data, train_labels, test_data, test_labels = load_data() X_train, X_test = vectorize_data(train_data, test_data) results = train_and_evaluate(X_train, train_labels, X_test, test_labels) for clf_name, metrics in results.items(): print(f\\"Results for {clf_name}:\\") for metric, score in metrics.items(): print(f\\"{metric}: {score:.4f}\\") if __name__ == \\"__main__\\": main() ``` Constraints: - You must apply appropriate data preprocessing steps. - The input text data could be noisy; handle any necessary cleanup. - Evaluate the models on the same test set for a fair comparison. Performance Requirements: - Ensure that the code runs efficiently on a machine with limited memory.","solution":"from sklearn.feature_extraction.text import CountVectorizer from sklearn.naive_bayes import GaussianNB, MultinomialNB, ComplementNB, BernoulliNB from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score import numpy as np def load_data(): # Replace with actual data loading mechanism train_data = [\\"This is the first document.\\", \\"This document is the second document.\\", \\"And this is the third one.\\", \\"Is this the first document?\\"] train_labels = [0, 1, 1, 0] test_data = [\\"This is the second document.\\", \\"Is this the first document?\\"] test_labels = [1, 0] return train_data, train_labels, test_data, test_labels def vectorize_data(train_data, test_data): # Example: Using CountVectorizer vectorizer = CountVectorizer() X_train = vectorizer.fit_transform(train_data) X_test = vectorizer.transform(test_data) return X_train.toarray(), X_test.toarray() def train_and_evaluate(X_train, y_train, X_test, y_test): classifiers = { \'GaussianNB\': GaussianNB(), \'MultinomialNB\': MultinomialNB(), \'ComplementNB\': ComplementNB(), \'BernoulliNB\': BernoulliNB() } results = {} for name, clf in classifiers.items(): if name == \\"GaussianNB\\": X_train_dense = X_train.toarray() if hasattr(X_train, \\"toarray\\") else X_train X_test_dense = X_test.toarray() if hasattr(X_test, \\"toarray\\") else X_test clf.fit(X_train_dense, y_train) y_pred = clf.predict(X_test_dense) else: clf.fit(X_train, y_train) y_pred = clf.predict(X_test) results[name] = { \'accuracy\': accuracy_score(y_test, y_pred), \'precision\': precision_score(y_test, y_pred, average=\'weighted\'), \'recall\': recall_score(y_test, y_pred, average=\'weighted\'), \'f1_score\': f1_score(y_test, y_pred, average=\'weighted\') } return results def main(): train_data, train_labels, test_data, test_labels = load_data() X_train, X_test = vectorize_data(train_data, test_data) results = train_and_evaluate(X_train, train_labels, X_test, test_labels) for clf_name, metrics in results.items(): print(f\\"Results for {clf_name}:\\") for metric, score in metrics.items(): print(f\\"{metric}: {score:.4f}\\") if __name__ == \\"__main__\\": main()"},{"question":"# Pandas Plotting Challenge **Objective:** Demonstrate your understanding of pandas\' plotting functionalities by creating various visualizations from a given dataset. **Task:** You are provided with a dataset containing data on the daily sales of different products in a retail store over a one-year period. The dataset is stored in a CSV file named `sales_data.csv` and contains the following columns: - `date`: The date of the sales record. - `product`: The name of the product. - `sales_amount`: The amount of sales for the product on that date. Write a Python function that performs the following tasks: 1. **Load the data** from `sales_data.csv` into a pandas DataFrame. 2. **Create a line plot** showing the cumulative sales amounts for each product over time. 3. **Generate a bar plot** displaying the total sales amounts for each product. 4. **Create a histogram** of the sales amounts for a specific product (\'Product_A\'). 5. **Produce a box plot** to visualize the distribution of sales amounts for each product. 6. **Display a scatter plot** with sales amounts of \'Product_A\' on the x-axis and \'Product_B\' on the y-axis. 7. **Save all plots** as images with appropriate filenames (e.g., `line_plot.png`, `bar_plot.png`, etc.). **Function Signature:** ```python def visualize_sales_data(file_path: str): # Your code here ``` **Input:** - `file_path` (str): The filepath to the `sales_data.csv`. **Output:** - The function should not return anything. It saves plots as images with appropriate filenames. **Example CSV content (`sales_data.csv`):** ``` date,product,sales_amount 2023-01-01,Product_A,120 2023-01-01,Product_B,150 2023-01-02,Product_A,130 2023-01-02,Product_B,160 ... ``` **Guidelines:** - Use the pandas library for data manipulation and Matplotlib for plotting. - Ensure the plots are properly labeled and formatted for clarity. - Use appropriate axis labels, titles, and legends where necessary. - Handle any missing data appropriately. - Adhere to best practices for creating clean and informative visualizations.","solution":"import pandas as pd import matplotlib.pyplot as plt def visualize_sales_data(file_path: str): # Load the data df = pd.read_csv(file_path, parse_dates=[\'date\']) # Create a line plot showing the cumulative sales amounts for each product over time cumulative_sales = df.pivot_table(index=\'date\', columns=\'product\', values=\'sales_amount\', aggfunc=\'sum\').fillna(0).cumsum() cumulative_sales.plot() plt.title(\'Cumulative Sales Amounts Over Time\') plt.xlabel(\'Date\') plt.ylabel(\'Cumulative Sales Amount\') plt.legend(title=\'Product\') plt.savefig(\'line_plot.png\') plt.close() # Generate a bar plot displaying the total sales amounts for each product total_sales = df.groupby(\'product\')[\'sales_amount\'].sum().sort_values() total_sales.plot(kind=\'bar\') plt.title(\'Total Sales Amounts for Each Product\') plt.xlabel(\'Product\') plt.ylabel(\'Total Sales Amount\') plt.savefig(\'bar_plot.png\') plt.close() # Create a histogram of the sales amounts for \'Product_A\' product_a_sales = df[df[\'product\'] == \'Product_A\'][\'sales_amount\'] product_a_sales.plot(kind=\'hist\', bins=30) plt.title(\'Sales Amounts Histogram for Product_A\') plt.xlabel(\'Sales Amount\') plt.ylabel(\'Frequency\') plt.savefig(\'histogram_product_a.png\') plt.close() # Produce a box plot to visualize the distribution of sales amounts for each product df.boxplot(column=\'sales_amount\', by=\'product\') plt.title(\'Sales Amount Distribution by Product\') plt.suptitle(\'\') plt.xlabel(\'Product\') plt.ylabel(\'Sales Amount\') plt.savefig(\'box_plot.png\') plt.close() # Display a scatter plot with sales amounts of \'Product_A\' on the x-axis and \'Product_B\' on the y-axis sales_pivot = df.pivot_table(index=\'date\', columns=\'product\', values=\'sales_amount\', aggfunc=\'sum\').fillna(0) if \'Product_A\' in sales_pivot.columns and \'Product_B\' in sales_pivot.columns: product_a_b_sales = sales_pivot[[\'Product_A\', \'Product_B\']].dropna() plt.scatter(product_a_b_sales[\'Product_A\'], product_a_b_sales[\'Product_B\']) plt.title(\'Sales Amounts: Product_A vs Product_B\') plt.xlabel(\'Product_A Sales Amount\') plt.ylabel(\'Product_B Sales Amount\') plt.savefig(\'scatter_plot_product_a_b.png\') plt.close()"},{"question":"**Question: Implement a Manifold Learning Technique using scikit-learn** You are required to implement a dimensionality reduction algorithm using manifold learning techniques from the scikit-learn library. You will work with the Isomap algorithm to embed a high dimensional dataset into a lower dimension. Your task is to implement a function that takes high-dimensional data as input and returns the lower-dimensional representation using Isomap. # Function Signature: ```python def isomap_embedding(data: np.ndarray, n_neighbors: int, n_components: int) -> np.ndarray: pass ``` # Parameters: - `data` (np.ndarray): A 2D numpy array of shape (N, D) representing the high-dimensional input data, where N is the number of points and D is the dimensionality. - `n_neighbors` (int): The number of neighbors to consider for the Isomap algorithm. - `n_components` (int): The number of dimensions to reduce the data to. # Returns: - `embedding` (np.ndarray): A 2D numpy array of shape (N, n_components) representing the embedded coordinates of the input data in lower dimensions. # Constraints: - Ensure that `n_neighbors` is suitable (non-zero and less than the number of data points). - `n_components` should be less than the input dimensionality but can be greater than or equal to 2. # Example: ```python import numpy as np # Sample high-dimensional data (6 points in 3D space) data = np.array([[0, 0, 1], [1, 0, 0], [0, 1, 0], [1, 1, 1], [0.5, 0.5, 0.5], [0, 0, 0]]) # Apply Isomap to reduce to 2 dimensions embedding = isomap_embedding(data, n_neighbors=3, n_components=2) print(embedding) ``` # Performance: - The overall complexity of the Isomap algorithm is O[D log(k) N log(N)] + O[N^2(k + log(N))] + O[d N^2]. - Ensure that your implementation efficiently handles the nearest neighbor search and subsequent stages of the algorithm. # Implementation Notes: 1. Utilize the `sklearn.manifold.Isomap` class. 2. Pay attention to the three main stages of the Isomap algorithm: nearest neighbor search, shortest-path graph search, and partial eigenvalue decomposition. 3. Validate the inputs to ensure they fall within acceptable ranges. By successfully implementing this function, you will demonstrate your understanding of manifold learning concepts and your proficiency with scikit-learn\'s implementation of the Isomap algorithm.","solution":"import numpy as np from sklearn.manifold import Isomap def isomap_embedding(data: np.ndarray, n_neighbors: int, n_components: int) -> np.ndarray: Applies Isomap algorithm to reduce the dimensionality of the input data. :param data: A 2D numpy array of shape (N, D) representing the high-dimensional input data. :param n_neighbors: The number of neighbors to consider for the Isomap algorithm. :param n_components: The number of dimensions to reduce the data to. :return: A 2D numpy array of shape (N, n_components) representing the embedded coordinates. # Validate inputs if n_neighbors <= 0 or n_neighbors >= data.shape[0]: raise ValueError(\\"n_neighbors must be greater than 0 and less than the number of data points\\") if n_components < 2 or n_components >= data.shape[1]: raise ValueError(\\"n_components must be at least 2 and less than the input dimensionality\\") # Initialize the Isomap model isomap = Isomap(n_neighbors=n_neighbors, n_components=n_components) # Fit the model and transform the data embedding = isomap.fit_transform(data) return embedding"},{"question":"**Objective:** Write an asynchronous function that concurrently runs several tasks, ensures each task handles timeouts appropriately, and returns the results of all tasks. Your solution should demonstrate a good understanding of asyncio tasks, handling timeouts, and managing concurrent execution. **Task Description:** Implement an asynchronous function named `process_tasks_concurrently` that accepts a list of coroutine functions and a timeout value. The function should: 1. Run each coroutine concurrently. 2. Ensure that each coroutine completes its execution – either normally or by handling a timeout. 3. Return a list with the results of each task. If a task times out, include the string `\'timeout\'` in the corresponding position of the list. **Function Signature:** ```python import asyncio from typing import List, Callable, Any async def process_tasks_concurrently(tasks: List[Callable[[], Any]], timeout: float) -> List[Any]: # Your implementation here ``` **Input:** - `tasks`: A list of coroutine functions. Each coroutine function takes no arguments and should be run concurrently. - `timeout`: A float representing the maximum time (in seconds) to allow a task to run before timing out. **Output:** - A list of results from each task. - If a task completes successfully, its result should be included in the list. - If a task times out, include the string `\'timeout\'` in the corresponding position of the list. **Constraints:** - You must use asyncio to manage the coroutines and tasks. - Ensure that all tasks handle timeouts properly. - Raise a `ValueError` if the `tasks` list is empty. **Example Usage:** ```python import asyncio async def task_1(): await asyncio.sleep(2) return \\"task 1 completed\\" async def task_2(): await asyncio.sleep(1) return \\"task 2 completed\\" async def task_3(): await asyncio.sleep(3) return \\"task 3 completed\\" async def main(): tasks = [task_1, task_2, task_3] timeout = 2 result = await process_tasks_concurrently(tasks, timeout=timeout) print(result) # Expected output: [\'timeout\', \'task 2 completed\', \'timeout\'] asyncio.run(main()) ``` # Hints: - Use `asyncio.create_task()` to run each coroutine function concurrently. - Utilize `asyncio.wait_for()` to handle timeouts for each task. - Return the result of each task or `\'timeout\'` if the task exceeds the specified timeout.","solution":"import asyncio from typing import List, Callable, Any async def process_tasks_concurrently(tasks: List[Callable[[], Any]], timeout: float) -> List[Any]: if not tasks: raise ValueError(\\"The tasks list cannot be empty\\") async def handle_task(task): try: return await asyncio.wait_for(task(), timeout) except asyncio.TimeoutError: return \'timeout\' return await asyncio.gather(*(handle_task(task) for task in tasks))"},{"question":"You are tasked with building a machine learning model using the `SGDClassifier` provided by the `sklearn.linear_model` module to predict whether an iris flower is of the species `versicolor` or `virginica`. Your goal is to preprocess the data, train the model, and evaluate its performance. Follow the steps below to complete this task: 1. **Load the Dataset**: - Load the iris dataset using `datasets.load_iris()` from `sklearn`. 2. **Preprocess the Dataset**: - Filter the data to only include the species `versicolor` and `virginica`. - Standardize the features using `StandardScaler` from `sklearn.preprocessing`. 3. **Train the Model**: - Use `SGDClassifier` with `loss=\'hinge\'` and `penalty=\'l2\'`. - Train the model with early stopping, using a validation fraction of 20% and stopping if no improvement is seen for 5 consecutive iterations. 4. **Evaluate the Model**: - Print the model\'s accuracy on the validation set. - Display the model\'s coefficients and intercept. # Constraints - Do not use any other machine learning libraries except `sklearn`. - Ensure that the code is efficient and follows good practices such as shuffling the data and standardizing features. # Expected Input and Output Format Expected functions to implement: ```python import numpy as np from sklearn import datasets from sklearn.linear_model import SGDClassifier from sklearn.preprocessing import StandardScaler from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score def load_and_preprocess_data(): # Load the iris dataset and filter for versicolor and virginica pass def train_sgd_classifier(X_train, y_train): # Train the SGDClassifier with specified parameters pass def evaluate_model(clf, X_val, y_val): # Evaluate the model on validation data pass if __name__ == \\"__main__\\": X_train, X_val, y_train, y_val = load_and_preprocess_data() clf = train_sgd_classifier(X_train, y_train) evaluate_model(clf, X_val, y_val) ``` Sample Output: ``` Model Accuracy: 0.95 Model Coefficients: [[x1, x2, x3, x4]] Model Intercept: [x] ``` Good luck, and ensure your code is well-documented and follows best practices!","solution":"import numpy as np from sklearn import datasets from sklearn.linear_model import SGDClassifier from sklearn.preprocessing import StandardScaler from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score def load_and_preprocess_data(): iris = datasets.load_iris() X = iris.data y = iris.target # Filter to only keep versicolor and virginica mask = y > 0 X = X[mask] y = y[mask] y = y - 1 # Make labels binary (0: versicolor, 1: virginica) # Standardize the features scaler = StandardScaler() X = scaler.fit_transform(X) # Split the dataset into training and validation sets X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y) return X_train, X_val, y_train, y_val def train_sgd_classifier(X_train, y_train): clf = SGDClassifier(loss=\'hinge\', penalty=\'l2\', early_stopping=True, validation_fraction=0.2, n_iter_no_change=5, random_state=42) clf.fit(X_train, y_train) return clf def evaluate_model(clf, X_val, y_val): y_pred = clf.predict(X_val) accuracy = accuracy_score(y_val, y_pred) print(f\\"Model Accuracy: {accuracy}\\") print(f\\"Model Coefficients: {clf.coef_}\\") print(f\\"Model Intercept: {clf.intercept_}\\") if __name__ == \\"__main__\\": X_train, X_val, y_train, y_val = load_and_preprocess_data() clf = train_sgd_classifier(X_train, y_train) evaluate_model(clf, X_val, y_val)"},{"question":"Problem Statement You are required to create a Python script that acts as a converter for different temperature units. The script should take a temperature value and convert it between Celsius, Fahrenheit, and Kelvin based on the specified options using the `argparse` module for command-line argument parsing. Requirements 1. The script should accept a temperature value as a positional argument. 2. The script should allow optional arguments to specify the input and output units. The accepted units are Celsius (`C`), Fahrenheit (`F`), and Kelvin (`K`). 3. The script should provide proper help and usage messages that guide the user on how to use the script. 4. The script should handle invalid inputs gracefully and display appropriate error messages. 5. Include mutually exclusive options to specify the output format (detailed or simple). Command-Line Interface - The script should be run from the command line as follows: ```bash python3 temp_converter.py <temperature> -i <input_unit> -o <output_unit> [-q | -v] ``` - `<temperature>`: A float value representing the temperature to convert. - `-i <input_unit>`: An optional argument to specify the input unit (`C`, `F`, or `K`). Default is Celsius (`C`). - `-o <output_unit>`: An optional argument to specify the output unit (`C`, `F`, or `K`). Default is Fahrenheit (`F`). - `-q`: An optional argument to print output in a quiet format (just the numeric value). - `-v`: An optional argument to print output in a verbose format (detailed description). Expected Output - When `-q` is specified: ```bash python3 temp_converter.py 100 -i C -o F -q 212.0 ``` - When `-v` is specified: ```bash python3 temp_converter.py 100 -i C -o F -v 100.0 Celsius is equal to 212.0 Fahrenheit. ``` Constraints - You cannot use any libraries other than `argparse` and standard math operations. - The script must validate and handle cases where input or output unit is not one of the accepted values. Functions to Implement 1. `convert_temperature(value: float, input_unit: str, output_unit: str) -> float`: Converts the temperature from one unit to another. 2. Command-line argument parsing and main function to integrate the script workflow. Example Behavior 1. Default behavior (Celsius to Fahrenheit): ```bash python3 temp_converter.py 0 32.0 ``` 2. Specifying input and output units: ```bash python3 temp_converter.py 0 -i C -o K 273.15 ``` 3. Verbose output: ```bash python3 temp_converter.py 0 -v 0.0 Celsius is equal to 32.0 Fahrenheit. ``` 4. Invalid unit input: ```bash python3 temp_converter.py 0 -i X -o F usage: temp_converter.py [-h] [-i {C,F,K}] [-o {C,F,K}] [-q | -v] temperature temp_converter.py: error: argument -i: invalid choice: \'X\' (choose from \'C\', \'F\', \'K\') ``` Create a script named `temp_converter.py` and implement the required functionality following the guidelines and constraints provided.","solution":"import argparse def celsius_to_fahrenheit(celsius): return (celsius * 9/5) + 32 def fahrenheit_to_celsius(fahrenheit): return (fahrenheit - 32) * 5/9 def celsius_to_kelvin(celsius): return celsius + 273.15 def kelvin_to_celsius(kelvin): return kelvin - 273.15 def fahrenheit_to_kelvin(fahrenheit): return celsius_to_kelvin(fahrenheit_to_celsius(fahrenheit)) def kelvin_to_fahrenheit(kelvin): return celsius_to_fahrenheit(kelvin_to_celsius(kelvin)) def convert_temperature(value, input_unit, output_unit): if input_unit == output_unit: return value if input_unit == \'C\': if output_unit == \'F\': return celsius_to_fahrenheit(value) elif output_unit == \'K\': return celsius_to_kelvin(value) elif input_unit == \'F\': if output_unit == \'C\': return fahrenheit_to_celsius(value) elif output_unit == \'K\': return fahrenheit_to_kelvin(value) elif input_unit == \'K\': if output_unit == \'C\': return kelvin_to_celsius(value) elif output_unit == \'F\': return kelvin_to_fahrenheit(value) def main(): parser = argparse.ArgumentParser(description=\'Temperature converter.\') parser.add_argument(\'temperature\', type=float, help=\'Temperature value to convert\') parser.add_argument(\'-i\', \'--input_unit\', choices=[\'C\', \'F\', \'K\'], default=\'C\', help=\'Input unit (C, F, K)\') parser.add_argument(\'-o\', \'--output_unit\', choices=[\'C\', \'F\', \'K\'], default=\'F\', help=\'Output unit (C, F, K)\') group = parser.add_mutually_exclusive_group() group.add_argument(\'-q\', \'--quiet\', action=\'store_true\', help=\'Quiet mode\') group.add_argument(\'-v\', \'--verbose\', action=\'store_true\', help=\'Verbose mode\') args = parser.parse_args() input_temp = args.temperature input_unit = args.input_unit output_unit = args.output_unit converted_temp = convert_temperature(input_temp, input_unit, output_unit) if args.quiet: print(f\'{converted_temp}\') elif args.verbose: print(f\'{input_temp} {input_unit} is equal to {converted_temp} {output_unit}.\') else: print(f\'{converted_temp}\') if __name__ == \'__main__\': main()"},{"question":"# Asyncio Futures in Python 3.10 **Objective:** Implement a function that schedules multiple asynchronous tasks using `asyncio.Future`. Demonstrate the use of Futures by simulating an example where you wait for different asynchronous tasks to complete and aggregate their results. **Instructions:** 1. Write a function `schedule_tasks(task_delays: List[Tuple[int, str]]) -> List[str]` that accepts a list of tuples, where each tuple contains a delay in seconds and a string value. The function should: - Create an `asyncio.Future` for each task. - Schedule each task to set its string value on the Future after the specified delay. - Await the completion of all the Futures. - Return a list of results from all the completed Futures. 2. Implement a helper coroutine `async def set_after(fut: asyncio.Future, delay: int, value: str) -> None` that: - Sleeps for the specified delay using `asyncio.sleep`. - Sets the `value` as the result of the `fut` Future. 3. Implement a coroutine `async def main()` for testing purposes that: - Constructs a list of task specifications (e.g., `[(1, \'task1\'), (2, \'task2\'), (3, \'task3\')]`). - Calls `schedule_tasks` with this list. - Prints the list of returned results. **Constraints:** - Use only the `asyncio` module for handling asynchronous operations. - Do not use any external libraries. - Ensure that the function handles possible exceptions that might be raised by the Futures. **Example:** ```python import asyncio from typing import List, Tuple async def set_after(fut: asyncio.Future, delay: int, value: str) -> None: await asyncio.sleep(delay) fut.set_result(value) async def schedule_tasks(task_delays: List[Tuple[int, str]]) -> List[str]: futures = [] loop = asyncio.get_running_loop() for delay, value in task_delays: fut = loop.create_future() asyncio.create_task(set_after(fut, delay, value)) futures.append(fut) results = [await fut for fut in futures] return results async def main(): task_delays = [(1, \'task1\'), (2, \'task2\'), (3, \'task3\')] results = await schedule_tasks(task_delays) print(results) # Running the main function using asyncio.run if __name__ == \\"__main__\\": asyncio.run(main()) ``` This question evaluates students\' understanding of: - Creating and managing `asyncio.Future` instances. - Writing and awaiting asynchronous functions. - Scheduling asynchronous tasks and handling their completion.","solution":"import asyncio from typing import List, Tuple async def set_after(fut: asyncio.Future, delay: int, value: str) -> None: Sleeps for the specified delay and sets the future\'s result. :param fut: The asyncio.Future to set the result on. :param delay: The delay in seconds before setting the result. :param value: The value to set as the result of the future. await asyncio.sleep(delay) fut.set_result(value) async def schedule_tasks(task_delays: List[Tuple[int, str]]) -> List[str]: Schedules multiple asynchronous tasks that each set a future with a value after a delay. :param task_delays: A list of tuples, where each tuple contains a delay in seconds and a string value. :return: A list of results from all the completed futures. futures = [] for delay, value in task_delays: fut = asyncio.Future() asyncio.create_task(set_after(fut, delay, value)) futures.append(fut) results = [await fut for fut in futures] return results async def main(): Main coroutine for testing purposes. Calls schedule_tasks with a list of task specifications and prints the results. task_delays = [(1, \'task1\'), (2, \'task2\'), (3, \'task3\')] results = await schedule_tasks(task_delays) print(results) # Running the main function using asyncio.run if __name__ == \\"__main__\\": asyncio.run(main())"},{"question":"# Pandas Coding Assessment **Objective:** Demonstrate your understanding of handling integer data with missing values in pandas using IntegerArray, including various operations and integration within a DataFrame. Question: You are given a mixed dataset that contains numerical data with missing values. Your task is to perform a series of operations to clean up the data and prepare it for analysis. Implement the following steps using pandas: 1. Create a DataFrame with the following columns and data: - `ids`: [1, 2, 3, None, 5, np.nan, 7] - `values`: [10.5, 23.4, np.nan, 45.1, None, 67.0, 89.3] - `categories`: [\'A\', \'B\', \'A\', \'B\', \'C\', \'A\', \'C\'] - `flags`: [True, False, True, np.nan, False, None, True] 2. Convert the `ids` column to a nullable integer type using `pd.Int64Dtype()`. 3. Handle missing values in the `values` column by replacing them with the mean of the non-missing values in the column. 4. Add a new column `value_flags` which is the result of an element-wise logical AND between the `values` comparison to the mean of the column and the `flags` column. 5. Replace any remaining missing values in the `flags` and `value_flags` columns with `False`. 6. Group the DataFrame by `categories` and calculate the sum of `values` for each category. 7. Finally, return the resulting DataFrame after applying all the above transformations and calculations. **Input:** ```python import pandas as pd import numpy as np data = { \\"ids\\": [1, 2, 3, None, 5, np.nan, 7], \\"values\\": [10.5, 23.4, np.nan, 45.1, None, 67.0, 89.3], \\"categories\\": [\'A\', \'B\', \'A\', \'B\', \'C\', \'A\', \'C\'], \\"flags\\": [True, False, True, np.nan, False, None, True] } ``` **Expected Output:** A pandas DataFrame after performing the requested transformations and calculations. **Constraints:** - The solution should utilize `pandas.NA` and `pd.Int64Dtype()` for handling missing integer values. - Ensure that the DataFrame manipulations are efficient and make appropriate use of pandas operations. Here is the function signature for your implementation: ```python def clean_and_transform(data: dict) -> pd.DataFrame: pass # Example usage data = { \\"ids\\": [1, 2, 3, None, 5, np.nan, 7], \\"values\\": [10.5, 23.4, np.nan, 45.1, None, 67.0, 89.3], \\"categories\\": [\'A\', \'B\', \'A\', \'B\', \'C\', \'A\', \'C\'], \\"flags\\": [True, False, True, np.nan, False, None, True] } result = clean_and_transform(data) print(result) ``` You can assume that the pandas package is already imported and available for use.","solution":"import pandas as pd import numpy as np def clean_and_transform(data: dict) -> pd.DataFrame: # Create DataFrame df = pd.DataFrame(data) # Convert \'ids\' column to nullable integer type df[\'ids\'] = df[\'ids\'].astype(pd.Int64Dtype()) # Handle missing values in \'values\' column by replacing them with the mean of non-missing values mean_value = df[\'values\'].mean() df[\'values\'].fillna(mean_value, inplace=True) # Add column \'value_flags\' with element-wise logical AND between \'values\' comparison to mean and \'flags\' mean_value = df[\'values\'].mean() df[\'value_flags\'] = (df[\'values\'] > mean_value) & (df[\'flags\'].fillna(False)) # Replace remaining missing values in \'flags\' and \'value_flags\' with False df[\'flags\'].fillna(False, inplace=True) df[\'value_flags\'].fillna(False, inplace=True) # Group by \'categories\' and calculate the sum of \'values\' for each category grouped_df = df.groupby(\'categories\', as_index=False)[\'values\'].sum() return grouped_df"},{"question":"Title: Custom Warning Handler and Filter Implementation Objective: To assess the understanding of issuing custom warnings, configuring warning filters, and using context managers to handle warnings in Python. Problem Statement: A new module `custom_module.py` requires setting up custom warning handlers to ensure that important warnings are displayed appropriately while less critical warnings are either suppressed or handled differently. 1. **Issue Custom Warnings**: - Implement a function `issue_warnings()` that issues three types of warnings: * `UserWarning` with the message \\"This is a user warning.\\" * `DeprecationWarning` with the message \\"This feature is deprecated.\\" * `RuntimeWarning` with the message \\"This is a runtime warning.\\" 2. **Configure Warning Filters**: - Implement a function `configure_warning_filters()` that sets the following filters: * Ignore all `DeprecationWarning`s globally. * Turn all `RuntimeWarning`s into exceptions. * Show all `UserWarning`s only once per module. 3. **Capture and Assert Warnings in a Context Manager**: - Implement a function `test_warnings()` that: * Uses the `catch_warnings` context manager to capture all warnings. * Calls the `issue_warnings()` function within the context manager. * Asserts that: * A `UserWarning` was captured. * A `DeprecationWarning` was not captured due to the filter. * A `RuntimeWarning` raised an exception. Input and Output Format: **Function Signatures**: ```python def issue_warnings() -> None: pass def configure_warning_filters() -> None: pass def test_warnings() -> None: pass ``` **Constraints**: - You cannot modify the behavior of the warning categories themselves. - You must use the provided warnings module functions and classes to manage and test warnings. **Example Reference**: Suppose you set up the functions correctly, calling `test_warnings()` should internally trigger the logic of issuing warnings, applying filters, and asserting the outcomes as specified. Instructions: 1. Implement the function `issue_warnings` to issue the specified warnings. 2. Implement the function `configure_warning_filters` to set up the required filters. 3. Implement the function `test_warnings` that uses the `catch_warnings` context manager to: - Capture the warnings. - Assert the presence of `UserWarning` and the absence of `DeprecationWarning`. - Handle `RuntimeWarning` as an exception.","solution":"import warnings def issue_warnings(): Issues three types of warnings: * UserWarning * DeprecationWarning * RuntimeWarning warnings.warn(\\"This is a user warning.\\", UserWarning) warnings.warn(\\"This feature is deprecated.\\", DeprecationWarning) warnings.warn(\\"This is a runtime warning.\\", RuntimeWarning) def configure_warning_filters(): Configures the warning filters: * Ignores DeprecationWarnings globally * Turns RuntimeWarnings into exceptions * Shows UserWarnings only once per module warnings.simplefilter(\\"ignore\\", DeprecationWarning) warnings.simplefilter(\\"error\\", RuntimeWarning) warnings.simplefilter(\\"once\\", UserWarning) def test_warnings(): Tests warning handling using the catch_warnings context manager: * Captures warnings * Asserts a UserWarning was captured * Ensures a DeprecationWarning was ignored * Ensures RuntimeWarning raises an exception configure_warning_filters() with warnings.catch_warnings(record=True) as caught_warnings: try: issue_warnings() except RuntimeWarning: pass user_warning_captured = any(isinstance(w.message, UserWarning) for w in caught_warnings) deprecation_warning_captured = any(isinstance(w.message, DeprecationWarning) for w in caught_warnings) runtime_warning_captured = any(isinstance(w.message, RuntimeWarning) for w in caught_warnings) assert user_warning_captured, \\"UserWarning was not captured.\\" assert not deprecation_warning_captured, \\"DeprecationWarning was captured but should be ignored.\\" assert not runtime_warning_captured, \\"RuntimeWarning was captured but should have raised an exception.\\""},{"question":"You are given a dataset related to penguins, which includes data on bill length, bill depth, species, sex, and other measurements. Your task is to use the seaborn library to create a detailed visualization that reveals insights from this dataset. The dataset is provided as `penguins` in the seaborn library. # Task: 1. **Basic Scatter Plot with Regression Line**: - Create a scatter plot with a regression line showing the relationship between `bill_length_mm` and `bill_depth_mm`. 2. **Color-coded Regression Lines for Different Groups**: - Modify the previous plot to show different regression lines based on the `species` column, using different colors for each species. 3. **Splitting by Gender**: - Split the previous plot into different subplots for different genders (`sex`). Ensure that each subplot has a separate color-coded regression line for each species. 4. **Dual-Axis Conditioning**: - Finally, create a grid of plots conditioned on both `species` (as columns) and `sex` (as rows). Each subplot should have its own regression line for the relationship between `bill_length_mm` and `bill_depth_mm`. 5. **Ensure Axis Limits Variability**: - Ensure that the x-axis and y-axis limits vary independently between subplots. Use the following guidelines: - Make sure your plots are appropriately labeled. - Use seaborn\'s `lmplot` to achieve these visualizations. **Constraints**: - You must use the seaborn package exclusively for these plots. - You should not modify the dataset. - Your code should be organized, and each figure should be displayed clearly. # Implementation: ```python import seaborn as sns # Load the dataset penguins = sns.load_dataset(\\"penguins\\") # 1. Basic Scatter Plot with Regression Line sns.lmplot(data=penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\") # 2. Color-coded Regression Lines for Different Groups sns.lmplot(data=penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", hue=\\"species\\") # 3. Splitting by Gender sns.lmplot(data=penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", hue=\\"species\\", col=\\"sex\\", height=4) # 4. Dual-Axis Conditioning sns.lmplot(data=penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", col=\\"species\\", row=\\"sex\\", height=3) # 5. Ensure Axis Limits Variability sns.lmplot(data=penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", col=\\"species\\", row=\\"sex\\", height=3, facet_kws=dict(sharex=False, sharey=False)) ``` **Output**: - The output should include five separate figures based on the criteria given in the task.","solution":"import seaborn as sns import matplotlib.pyplot as plt def plot_penguins_relationship(): Create various plots to visualize the relationship between bill length and bill depth in the penguins dataset. # Load the dataset penguins = sns.load_dataset(\\"penguins\\") # 1. Basic Scatter Plot with Regression Line sns.lmplot(data=penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\") plt.title(\\"Scatter Plot with Regression Line\\") plt.show() # 2. Color-coded Regression Lines for Different Groups sns.lmplot(data=penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", hue=\\"species\\") plt.title(\\"Color-coded Regression Lines for Different Species\\") plt.show() # 3. Splitting by Gender sns.lmplot(data=penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", hue=\\"species\\", col=\\"sex\\", height=4) plt.suptitle(\\"Gender-wise Subplots with Color-coded Regression Lines for Species\\") plt.show() # 4. Dual-Axis Conditioning sns.lmplot(data=penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", col=\\"species\\", row=\\"sex\\", height=3) plt.suptitle(\\"Dual-Axis Conditioning based on Species (columns) and Sex (rows)\\") plt.show() # 5. Ensure Axis Limits Variability sns.lmplot(data=penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", col=\\"species\\", row=\\"sex\\", height=3, facet_kws=dict(sharex=False, sharey=False)) plt.suptitle(\\"Independent Axis Limits for Subplots\\") plt.show() # Note: To visualize the plots, you will need call the function plot_penguins_relationship() # plot_penguins_relationship()"},{"question":"# Description This task aims to evaluate your ability to work with the `distutils` package, specifically focusing on the core functionalities and the creation of complex setup scripts. You are required to write a Python script using `distutils` that not only sets up basic package metadata but also includes a custom command to automate the creation of a test suite using additional file operations. # Task 1. Implement a function `create_setup_script()` that generates a `setup.py` file for a fictitious package. The package should include: - Name: \\"SamplePackage\\" - Version: \\"1.0\\" - Author: \\"John Doe\\" - Author Email: \\"john.doe@example.com\\" - Description: \\"This is a sample package for testing.\\" - URL: \\"https://example.com/samplepackage\\" - Contains a single module: \\"sample_module\\" - Contains a single script: \\"scripts/sample_script.py\\" - A custom command `test_suite` to copy the package module and script into a `test_suite` directory and generate a `run_tests.py` script inside `test_suite` to run tests. 2. The `generate_setup_script()` should create a setup script with the following directory structure: ``` SamplePackage/ |-- sample_module.py |-- scripts/ | |-- sample_script.py |-- setup.py |-- test_suite/ |-- sample_module.py |-- sample_script.py |-- run_tests.py ``` # `run_tests.py` Script The `run_tests.py` script should reside in the `test_suite` directory and should be able to run tests from both the `sample_module.py` and `scripts/sample_script.py`. # Constraints - You cannot use external libraries except for those in the standard library. - Your implementation should handle cases where the `test_suite` directory already exists. - Provide clear and concise error messages. # Input - None. Your script should create the necessary files and directories. # Expected Output - The `setup.py` script and all required files and directories should be created in the expected format when `create_setup_script()` is executed. # Example ```python create_setup_script() # This should generate the `setup.py` script with the specified structure and functionality. ``` # Submission - `create_setup_script()` function implementation. - The generated `setup.py` script content as a comment (for reference).","solution":"import os import shutil def create_setup_script(): # Directory structure base_dir = \\"SamplePackage\\" scripts_dir = os.path.join(base_dir, \\"scripts\\") test_suite_dir = os.path.join(base_dir, \\"test_suite\\") # Ensure directories exist os.makedirs(scripts_dir, exist_ok=True) os.makedirs(test_suite_dir, exist_ok=True) # Create sample_module.py with open(os.path.join(base_dir, \\"sample_module.py\\"), \\"w\\") as f: f.write(\\"# Sample module\\") # Create sample_script.py with open(os.path.join(scripts_dir, \\"sample_script.py\\"), \\"w\\") as f: f.write(\\"# Sample script\\") # Create run_tests.py with open(os.path.join(test_suite_dir, \\"run_tests.py\\"), \\"w\\") as f: f.write( import sys import unittest # Add paths for testing sys.path.insert(0, \'..\') # Import tests from sample_module import * # Replace with actual test functions from scripts.sample_script import * # Replace with actual test functions if __name__ == \\"__main__\\": unittest.main() ) # Create setup.py script with open(os.path.join(base_dir, \\"setup.py\\"), \\"w\\") as f: f.write( from distutils.core import setup, Command import shutil import os class TestSuiteCommand(Command): description = \\"Create test suite by copying necessary files and setting up run_tests.py\\" user_options = [] def initialize_options(self): pass def finalize_options(self): pass def run(self): base_dir = os.path.dirname(__file__) test_suite_dir = os.path.join(base_dir, \'test_suite\') # Ensure the test_suite directory exists if not os.path.exists(test_suite_dir): os.makedirs(test_suite_dir) # Copy sample_module.py shutil.copy(os.path.join(base_dir, \'sample_module.py\'), test_suite_dir) # Copy sample_script.py shutil.copy(os.path.join(base_dir, \'scripts\', \'sample_script.py\'), test_suite_dir) # Create run_tests.py run_tests_path = os.path.join(test_suite_dir, \'run_tests.py\') with open(run_tests_path, \'w\') as f: f.write(\\"\\"\\" import sys import unittest # Add paths for testing sys.path.insert(0, \'..\') # Import tests from sample_module import * # Replace with actual test functions from scripts.sample_script import * # Replace with actual test functions if __name__ == \\"__main__\\": unittest.main() \\"\\"\\") setup( name=\'SamplePackage\', version=\'1.0\', author=\'John Doe\', author_email=\'john.doe@example.com\', description=\'This is a sample package for testing.\', url=\'https://example.com/samplepackage\', packages=[\'\'], scripts=[\'scripts/sample_script.py\'], cmdclass={ \'test_suite\': TestSuiteCommand, }, ) )"},{"question":"Question You are tasked with implementing a pipeline parallelism model using PyTorch\'s `torch.distributed.pipelining` package. You will create a model, split it into pipeline stages, and execute it using a chosen scheduling strategy. This exercise will help assess your knowledge of model parallelism, model splitting, and distributed execution. # Steps & Requirements 1. **Implement a Simple Model:** Implement a simple neural network model using PyTorch. The model should include multiple layers so that it can be meaningfully split into different stages. 2. **Pipeline Stage Creation:** Split the model into two stages manually. Utilize the `PipelineStage` class to create these stages. Make sure to provide configurations ensuring correct input and output shapes. 3. **Pipeline Execution:** Use the `ScheduleGPipe` pipeline schedule to execute the created pipeline. Ensure the configuration for multiple devices and simulate the pipeline execution using multiple processes. # Model Description Your model should be a simple feed-forward neural network with the following layers: - Input Layer: Linear layer that takes an input of size 16 and outputs size 32. - Hidden Layer 1: Linear layer that takes an input of size 32 and outputs size 64. - Hidden Layer 2: Linear layer that takes an input of size 64 and outputs size 64. - Output Layer: Linear layer that takes an input of size 64 and outputs size 10. # Input and Output Formats - **Input:** A tensor of shape `(batch_size, 16)` with `batch_size` being a configurable parameter. - **Output:** A tensor of shape `(batch_size, 10)`. # Constraints 1. The solution must use `torch.distributed.pipelining` package. 2. The model should be split into exactly two stages: one containing the input and the first hidden layer, and the second containing the second hidden layer and the output layer. 3. Static input and output shapes for pipeline stages. # Performance Requirements 1. Efficient tensor operations within the defined model. 2. Correct allocation of communication buffers for pipeline stages. 3. Accurate configuration of `ScheduleGPipe` for pipeline execution. # Additional Information Use the `torch.distributed.launch` utility to simulate different stages running on separate devices. # Example Code Outline ```python import torch import torch.nn as nn import torch.distributed as dist from torch.distributed.pipelining import PipelineStage, ScheduleGPipe class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.layer1 = nn.Linear(16, 32) self.layer2 = nn.Linear(32, 64) self.layer3 = nn.Linear(64, 64) self.layer4 = nn.Linear(64, 10) def forward(self, x): x = self.layer1(x) x = torch.relu(x) x = self.layer2(x) x = torch.relu(x) x = self.layer3(x) x = torch.relu(x) x = self.layer4(x) return x # Step 1: Split the model manually # Provide code for splitting the model into two pipeline stages # Step 2: Create PipelineStage objects # Step 3: Use ScheduleGPipe for pipeline execution # Provide code for pipeline execution using ScheduleGPipe ``` # Submission Guidelines Submit your implementation in a single Python file. Include necessary comments and ensure your code adheres to clean coding standards.","solution":"import torch import torch.nn as nn import torch.distributed as dist from torch.distributed.pipelining import PipelineStage, ScheduleGPipe class SimpleModelStage1(nn.Module): def __init__(self): super(SimpleModelStage1, self).__init__() self.layer1 = nn.Linear(16, 32) self.layer2 = nn.Linear(32, 64) def forward(self, x): x = self.layer1(x) x = torch.relu(x) x = self.layer2(x) x = torch.relu(x) return x class SimpleModelStage2(nn.Module): def __init__(self): super(SimpleModelStage2, self).__init__() self.layer3 = nn.Linear(64, 64) self.layer4 = nn.Linear(64, 10) def forward(self, x): x = self.layer3(x) x = torch.relu(x) x = self.layer4(x) return x def run_pipeline(rank, world_size): dist.init_process_group(\\"gloo\\", rank=rank, world_size=world_size) # Create the model stages stage_1 = SimpleModelStage1().to(rank) stage_2 = SimpleModelStage2().to(rank) # Define pipeline stages if rank == 0: stage = PipelineStage(0, 1, stage_1, output_shape=torch.Size([64])) else: stage = PipelineStage(1, 1, stage_2, input_shape=torch.Size([64])) # Input tensor if rank == 0: inputs = torch.randn(32, 16).to(rank) # Assuming batch size 32 else: inputs = None # Execute the pipeline using ScheduleGPipe pipeline = ScheduleGPipe(name=\'pipeline\', stages=[stage], devices=[rank]) output = pipeline(inputs) if rank == 1: print(output) def main(): world_size = 2 torch.multiprocessing.spawn(run_pipeline, args=(world_size,), nprocs=world_size, join=True) if __name__ == \\"__main__\\": main()"},{"question":"<|Analysis Begin|> The `sched` module in Python defines a `scheduler` class that provides a general-purpose event scheduler. This class allows scheduling of events to be executed at a specific time or after a delay. It requires two functions for operation: `timefunc` to get the current time and `delayfunc` to wait for the specified time until the next event. The `scheduler` class has methods such as: - `enterabs(time, priority, action, argument=(), kwargs={})`: Schedules an event at an absolute time. - `enter(delay, priority, action, argument=(), kwargs={})`: Schedules an event after a delay. - `cancel(event)`: Cancels a scheduled event. - `empty()`: Checks if the event queue is empty. - `run(blocking=True)`: Runs the scheduled events in order. - `queue`: A read-only attribute that returns a list of upcoming events. The documentation provides some examples of creating a scheduler instance and scheduling events using the methods described. <|Analysis End|> <|Question Begin|> **Question: Implement a Custom Event Scheduler** Write a Python program to implement a custom event scheduler using the `sched` module. The scheduler should be capable of scheduling tasks (events) with absolute and relative times. It should also handle recurring tasks, which are tasks that repeat at regular intervals. # Requirements 1. **Scheduler Initialization**: - Create an instance of the `sched.scheduler` class using appropriate `timefunc` and `delayfunc`. 2. **Scheduling Events**: - Implement a function `schedule_event` to schedule an event at a given absolute time or after a certain delay. - The function should accept parameters for the event time/type (absolute or relative), priority, action (function to be executed), and its arguments (positional and keyword). 3. **Recurring Tasks**: - Implement support for recurring tasks which execute at regular intervals. You can decide on the approach to re-schedule the event upon its completion to achieve this functionality. 4. **Event Cancellation**: - Provide functionality to cancel a scheduled event before its execution time. 5. **Run Scheduler**: - Implement a method to run the scheduler so all scheduled events can be executed. # Input and Output Format - Input: None (the functions should be callable as described below) - Output: No output for the functions, they should perform the described actions. # Constraints 1. The `timefunc` should return the current time in seconds (use `time.time` or a similar method). 2. The `delayfunc` should pause execution for a specified amount of time (use `time.sleep` or a similar method). # Functions ```python def schedule_event(event_time_or_delay: float, is_absolute: bool, priority: int, action: callable, args: tuple = (), kwargs: dict = {}) -> sched.Event: Schedule an event either at an absolute time or after a delay. Parameters: event_time_or_delay (float): The time at which the event should run (absolute if `is_absolute` is true or delay otherwise). is_absolute (bool): If true, the event_time_or_delay is treated as absolute time, otherwise as delay. priority (int): The priority of the event (lower number indicates higher priority). action (callable): The function to be executed. args (tuple): Positional arguments to pass to the action. kwargs (dict): Keyword arguments to pass to the action. Returns: sched.Event: The event that has been scheduled (to be used for cancellation if required). pass def cancel_event(event: sched.Event): Cancel a scheduled event. Parameters: event (sched.Event): The event to be cancelled. Raises: ValueError: If the event is not in the queue. pass def run_scheduler(blocking: bool = True): Run the scheduled events. Parameters: blocking (bool): Run events as blocking or non-blocking. pass def schedule_recurring_event(interval: float, action: callable, args: tuple = (), kwargs: dict = {}): Schedule a recurring event that executes at regular intervals. Parameters: interval (float): The time interval between each execution of the event. action (callable): The function to be executed. args (tuple): Positional arguments to pass to the action. kwargs (dict): Keyword arguments to pass to the action. pass ``` # Example Usage ```python import time import sched # Create scheduler scheduler = sched.scheduler(time.time, time.sleep) def print_message(message): print(f\\"{time.time()}: {message}\\") # Schedule an event 5 seconds from now event1 = schedule_event(5, False, 1, print_message, args=(\\"Hello World\\",)) # Schedule an event at an absolute time (10 seconds from now) event2 = schedule_event(time.time() + 10, True, 2, print_message, args=(\\"Absolute Time Event\\",)) # Schedule a recurring event every 3 seconds schedule_recurring_event(3, print_message, args=(\\"Recurring Event\\",)) # Run the scheduler run_scheduler() ``` **Note**: This question requires understanding of the `sched` module, including how to schedule, execute, and cancel events while also handling recurring tasks with a custom implementation.","solution":"import time import sched # Create an instance of the scheduler scheduler = sched.scheduler(time.time, time.sleep) def schedule_event(event_time_or_delay: float, is_absolute: bool, priority: int, action: callable, args: tuple = (), kwargs: dict = {}) -> sched.Event: Schedule an event either at an absolute time or after a delay. Parameters: event_time_or_delay (float): The time at which the event should run (absolute if `is_absolute` is true or delay otherwise). is_absolute (bool): If true, the event_time_or_delay is treated as absolute time, otherwise as delay. priority (int): The priority of the event (lower number indicates higher priority). action (callable): The function to be executed. args (tuple): Positional arguments to pass to the action. kwargs (dict): Keyword arguments to pass to the action. Returns: sched.Event: The event that has been scheduled (to be used for cancellation if required). if is_absolute: event = scheduler.enterabs(event_time_or_delay, priority, action, argument=args, kwargs=kwargs) else: event = scheduler.enter(event_time_or_delay, priority, action, argument=args, kwargs=kwargs) return event def cancel_event(event: sched.Event): Cancel a scheduled event. Parameters: event (sched.Event): The event to be cancelled. Raises: ValueError: If the event is not in the queue. scheduler.cancel(event) def run_scheduler(blocking: bool = True): Run the scheduled events. Parameters: blocking (bool): Run events as blocking or non-blocking. scheduler.run(blocking) def schedule_recurring_event(interval: float, action: callable, args: tuple = (), kwargs: dict = {}): Schedule a recurring event that executes at regular intervals. Parameters: interval (float): The time interval between each execution of the event. action (callable): The function to be executed. args (tuple): Positional arguments to pass to the action. kwargs (dict): Keyword arguments to pass to the action. def recurring_action(): action(*args, **kwargs) schedule_event(interval, False, 1, recurring_action) schedule_event(interval, False, 1, recurring_action)"},{"question":"**TorchDynamo Challenge: Implementing a Traced Matrix Multiplication Function with Dynamic Shapes** # Objective: In this task, you will demonstrate your understanding of TorchDynamo by implementing a matrix multiplication function that handles dynamic shapes. You will use the `torch.compile` decorator to trace the function and ensure it adapts to different input shapes efficiently. # Task Description: 1. Implement a Python function `matrix_multiply(A, B)` that performs matrix multiplication on two input tensors `A` and `B`. 2. Use the `@torch.compile` decorator to enable tracing for this function. 3. Ensure the function can handle dynamic shapes, such as varying batch sizes during inference. 4. Include guards to optimize performance by avoiding unnecessary recompilations. # Function Signature: ```python import torch @torch.compile def matrix_multiply(A: torch.Tensor, B: torch.Tensor) -> torch.Tensor: pass ``` # Requirements: 1. The function should be able to handle any batch size while performing matrix multiplication. 2. Use symbolic shapes to trace dynamic dimensions. 3. Implement necessary guards to avoid recompilation when the batch size changes but other dimensions remain constant. 4. Ensure the function generates optimized FX graphs and handles recompilation efficiently for varying input shapes. # Input: - `A` (torch.Tensor): A 3D tensor of shape `(batch_size, m, n)`. - `B` (torch.Tensor): A 3D tensor of shape `(batch_size, n, p)`. # Output: - Returns a 3D tensor of shape `(batch_size, m, p)` representing the matrix multiplication result of `A` and `B`. # Example: ```python import torch A = torch.randn(5, 10, 20) B = torch.randn(5, 20, 30) result = matrix_multiply(A, B) print(result.shape) # Expected output: torch.Size([5, 10, 30]) A_new = torch.randn(8, 10, 20) result_new = matrix_multiply(A_new, B) print(result_new.shape) # Expected output: torch.Size([8, 10, 30]) ``` # Constraints: - Ensure the implementation uses dynamic shapes and symbolic integers effectively. - The function should be efficient with minimal recompilations when input shapes change.","solution":"import torch import torch._dynamo as dynamo @dynamo.optimize(\'eager\') def matrix_multiply(A: torch.Tensor, B: torch.Tensor) -> torch.Tensor: Performs batched matrix multiplication of two 3D tensors A and B. Args: A (torch.Tensor): A 3D tensor of shape (batch_size, m, n). B (torch.Tensor): A 3D tensor of shape (batch_size, n, p). Returns: torch.Tensor: A 3D tensor of shape (batch_size, m, p) representing the matrix multiplication result. assert A.dim() == 3 and B.dim() == 3, \\"Both A and B must be 3D tensors.\\" assert A.shape[0] == B.shape[0], \\"Batch sizes of A and B must match.\\" assert A.shape[2] == B.shape[1], \\"The inner dimensions (n) of A and B must match for matrix multiplication.\\" return torch.bmm(A, B)"},{"question":"# Coding Assessment Question: Advanced Class and Inheritance Operations in Python Problem Statement You are required to design a class-based system for managing a Library. The system should support functionalities such as adding books, managing members, and tracking borrowed books. Additionally, it should also enable the listing of overdue books using custom iterators and generators. # Requirements 1. **Class Definitions** - Define a `Book` class with the attributes: `title`, `author`, and `isbn`. Override the `__str__()` method to provide a meaningful string representation of the `Book` object. - Define a `Member` class with the attributes: `name` and `member_id`. Each `Member` can have a list of borrowed books (`borrowed_books`). 2. **Class Inheritance and Methods** - Define a base class `Person` with common attributes and methods, and let the `Member` class inherit from this base class. - Define a `Library` class which has: - A list to store all available books (`books`). - A list to store all members (`members`). - A method to add a new book. - A method to add a new member. - A method to lend a book to a member (ensure the book is not already lent). 3. **Private Variables** - Use name mangling to define a private variable in the `Library` class, `_borrowed_books`, which is a dictionary to keep track of which member has borrowed which books. 4. **Iterator and Generators** - Implement an iterator within the `Library` class to iterate over all borrowed books. - Define a generator within the `Library` class to yield overdue books. Assume that overdue books are those borrowed for more than 30 days. # Input and Output - The input to your program will be in the form of method calls to the classes and their methods. Here\'s an example of how the interaction might look: ```python # Create library instance my_library = Library() # Add some books my_library.add_book(Book(\\"1984\\", \\"George Orwell\\", \\"1234567890\\")) my_library.add_book(Book(\\"To Kill a Mockingbird\\", \\"Harper Lee\\", \\"2345678901\\")) # Add a member my_library.add_member(Member(\\"John Doe\\", \\"M001\\")) # Lend a book to a member my_library.lend_book(\\"1234567890\\", \\"M001\\") # Print all borrowed books using iterator for borrowed_book in my_library: print(borrowed_book) # Yield and print overdue books for overdue_book in my_library.overdue_books(): print(overdue_book) ``` # Constraints - The `isbn` for a book and the `member_id` for a member are unique. - You should use exception handling to manage situations where invalid operations are attempted (e.g., borrowing a book that does not exist). # Submission Please submit your solution as a single Python file containing all class definitions and the required methods.","solution":"from datetime import datetime, timedelta class Book: def __init__(self, title, author, isbn): self.title = title self.author = author self.isbn = isbn def __str__(self): return f\\"Book Title: {self.title}, Author: {self.author}, ISBN: {self.isbn}\\" class Person: def __init__(self, name): self.name = name class Member(Person): def __init__(self, name, member_id): super().__init__(name) self.member_id = member_id self.borrowed_books = [] def borrow_book(self, book): self.borrowed_books.append((book, datetime.now())) class Library: def __init__(self): self.books = [] self.members = [] self._borrowed_books = {} def add_book(self, book): self.books.append(book) def add_member(self, member): self.members.append(member) def lend_book(self, isbn, member_id): book = next((b for b in self.books if b.isbn == isbn), None) member = next((m for m in self.members if m.member_id == member_id), None) if not book: raise Exception(\\"Book not found\\") if not member: raise Exception(\\"Member not found\\") if isbn in self._borrowed_books: raise Exception(\\"Book already lent\\") member.borrow_book(book) self._borrowed_books[isbn] = member_id def __iter__(self): for member in self.members: for book, _ in member.borrowed_books: yield book def overdue_books(self): now = datetime.now() overdue_duration = timedelta(days=30) for member in self.members: for book, borrow_date in member.borrowed_books: if (now - borrow_date) > overdue_duration: yield book"},{"question":"**Problem Statement: XML Data Transformation with ElementTree** You are given an XML file containing information about a company\'s employees and departments. The goal is to parse this XML file, update certain elements based on the given criteria, and output the modified XML to a new file. # XML File Structure The XML file (`company_data.xml`) has the following structure: ```xml <company> <employee id=\\"1\\"> <name>John Doe</name> <age>30</age> <department>Engineering</department> </employee> <employee id=\\"2\\"> <name>Jane Smith</name> <age>25</age> <department>Marketing</department> </employee> <!-- More employee entries --> <department name=\\"Engineering\\"> <manager>John Doe</manager> <employees>20</employees> </department> <department name=\\"Marketing\\"> <manager>Jane Smith</manager> <employees>15</employees> </department> <!-- More department entries --> </company> ``` # Task Write a Python function `transform_company_data(input_file: str, output_file: str) -> None` that accomplishes the following: 1. **Parse the XML file:** Read and parse the `company_data.xml` file. 2. **Update Employees\' Information:** - Increase the age of each employee by 1. - Add an attribute `updated=\\"true\\"` to each employee element. 3. **Update Departments\' Information:** - For each department, if the manager is found to be an employee, ensure that the age of the manager (which should have been incremented) is reflected in the department section as well. 4. **Remove Elements:** - Remove all employees older than 50 years. 5. **Write the Modified XML:** Write the modified XML content to `output_file`. # Implementation Constraints - You are required to use the `xml.etree.ElementTree` module for this task. - Ensure the output XML file maintains the structure and format similar to the input file. # Function Signature ```python def transform_company_data(input_file: str, output_file: str) -> None: pass ``` # Example Given an input XML file `company_data.xml`: ```xml <company> <employee id=\\"1\\"> <name>John Doe</name> <age>30</age> <department>Engineering</department> </employee> <employee id=\\"2\\"> <name>Jane Smith</name> <age>25</age> <department>Marketing</department> </employee> <employee id=\\"3\\"> <name>Alice Johnson</name> <age>55</age> <department>Sales</department> </employee> <department name=\\"Engineering\\"> <manager>John Doe</manager> <employees>20</employees> </department> <department name=\\"Marketing\\"> <manager>Jane Smith</manager> <employees>15</employees> </department> </company> ``` The expected output XML file should look like: ```xml <company> <employee id=\\"1\\" updated=\\"true\\"> <name>John Doe</name> <age>31</age> <department>Engineering</department> </employee> <employee id=\\"2\\" updated=\\"true\\"> <name>Jane Smith</name> <age>26</age> <department>Marketing</department> </employee> <department name=\\"Engineering\\"> <manager>John Doe</manager> <employees>20</employees> </department> <department name=\\"Marketing\\"> <manager>Jane Smith</manager> <employees>15</employees> </department> </company> ``` The employee \\"Alice Johnson\\" has been removed because she is older than 50 years. **Hints:** - Use the `int()` function to convert strings to integers for age manipulation. - Carefully iterate and modify the XML tree to avoid concurrent modification issues.","solution":"import xml.etree.ElementTree as ET def transform_company_data(input_file: str, output_file: str) -> None: tree = ET.parse(input_file) root = tree.getroot() # Dictionary to keep track of managers\' ages manager_ages = {} # First pass: Update employee information employees_to_remove = [] for employee in root.findall(\'employee\'): # Increase age by 1 age = int(employee.find(\'age\').text) + 1 if age > 50: employees_to_remove.append(employee) continue employee.find(\'age\').text = str(age) employee.set(\'updated\', \'true\') # Record the manager\'s age name = employee.find(\'name\').text manager_ages[name] = age # Remove employees older than 50 for employee in employees_to_remove: root.remove(employee) # Second pass: Update department information for department in root.findall(\'department\'): manager_name = department.find(\'manager\').text if manager_name in manager_ages: department.set(\'manager_age\', str(manager_ages[manager_name])) # Write the modified XML to the output file tree.write(output_file)"},{"question":"**Coding Assessment Question:** You are given a DataFrame that holds information about a library\'s book borrowing records. Each book borrowing record contains information about `User_ID`, `Book_ID`, and `Days_Borrowed`. Some of these records, however, have missing values for `Days_Borrowed`. You need to perform the following tasks using pandas: 1. **Create DataFrame**: First, construct the following DataFrame with the specified data, ensuring that the `Days_Borrowed` column uses a nullable integer type: ```python data = { \'User_ID\': [101, 102, 103, 104, 105], \'Book_ID\': [1001, 1002, 1003, 1004, 1005], \'Days_Borrowed\': [14, None, 7, 30, None] } ``` 2. **Add New Row**: Add a new row to this DataFrame where `User_ID` is 106, `Book_ID` is 1006, and `Days_Borrowed` is missing. 3. **Calculate Average Days Borrowed**: Calculate the average number of days books were borrowed, skipping the missing values. 4. **Fill Missing Values**: Fill the missing values in `Days_Borrowed` with the previously calculated average. 5. **Sum of Days Borrowed per User**: Calculate the sum of `Days_Borrowed` for each `User_ID`. 6. **Validate Results**: Ensure your DataFrame has the correct data types and values as described. # Example The final DataFrame after filling missing values and summing days should look like this: ```python # DataFrame after filling missing values User_ID Book_ID Days_Borrowed 0 101 1001 14 1 102 1002 17 2 103 1003 7 3 104 1004 30 4 105 1005 17 5 106 1006 17 # Sum of Days Borrowed per User User_ID Total_Days_Borrowed 0 101 14 1 102 17 2 103 7 3 104 30 4 105 17 5 106 17 ``` # Constraints: - Use `pandas` version that supports `Int64Dtype` and `pandas.NA`. - Ensure efficient handling of missing values. - Clearly comment your code for each step to explain your implementation. # Submission: Submit your solution code which constructs the DataFrame, performs the necessary operations, and validates the results according to the given instructions.","solution":"import pandas as pd def process_library_records(): # Step 1: Create DataFrame with specified data, using nullable integer type for Days_Borrowed data = { \'User_ID\': [101, 102, 103, 104, 105], \'Book_ID\': [1001, 1002, 1003, 1004, 1005], \'Days_Borrowed\': [14, pd.NA, 7, 30, pd.NA] } df = pd.DataFrame(data, dtype=\'Int64\') # Step 2: Add a new row with User_ID=106, Book_ID=1006, and missing Days_Borrowed new_row = pd.DataFrame({\'User_ID\': [106], \'Book_ID\': [1006], \'Days_Borrowed\': [pd.NA]}, dtype=\'Int64\') df = pd.concat([df, new_row], ignore_index=True) # Step 3: Calculate the average number of days books were borrowed, skipping missing values avg_days_borrowed = df[\'Days_Borrowed\'].mean(skipna=True) # Step 4: Fill the missing values in Days_Borrowed with the previously calculated average df[\'Days_Borrowed\'].fillna(int(round(avg_days_borrowed)), inplace=True) # Step 5: Calculate the sum of Days_Borrowed for each User_ID sum_days_borrowed_per_user = df.groupby(\'User_ID\')[\'Days_Borrowed\'].sum().reset_index().rename(columns={\'Days_Borrowed\': \'Total_Days_Borrowed\'}) return df, sum_days_borrowed_per_user"},{"question":"# PyTorch Coding Assessment Question **Context:** You are tasked with implementing a function utilizing PyTorch to perform operations on tensors. The function should demonstrate your understanding of tensor creation, random sampling, mathematical operations, and efficient memory management. **Function:** ```python def transform_tensor_and_compute(input_shape, scaling_value): This function should execute the following steps: 1. Create a tensor of shape `input_shape` with random values sampled from a normal distribution. 2. Normalize the tensor so that its values are between 0 and 1. 3. Scale the tensor by multiplying all its elements by `scaling_value`. 4. Apply a series of mathematical operations to transform the tensor: - Compute the sine of each element. - Add the original normalized tensor to the sine-transformed tensor. 5. Finally, return the mean and standard deviation of the resulting tensor. Parameters: - input_shape (tuple): A tuple representing the shape of the tensor to be created. - scaling_value (float): A value to scale the elements of the tensor. Returns: - mean (float): The mean of the transformed tensor. - std_dev (float): The standard deviation of the transformed tensor. # Your implementation here ``` **Expected Input and Output:** - `input_shape`: A tuple (int, int), e.g., `(3, 4)` representing the shape of the tensor to be created. - `scaling_value`: A floating-point number to scale the tensor elements. **Constraints and Limitations:** - The function should efficiently handle tensors of large sizes, ensuring memory is managed properly. - Use PyTorch\'s in-built functions for tensor operations. **Performance Requirements:** - The function should be optimized to work efficiently with tensors of shape up to `(1000, 1000)`. **Example:** ```python # Example problem usage mean, std_dev = transform_tensor_and_compute((3, 4), 5.0) print(mean) # Expected output: A float value representing the mean of the transformed tensor. print(std_dev) # Expected output: A float value representing the standard deviation of the transformed tensor. ``` Use this question to showcase your ability to work with PyTorch tensors, apply random sampling, and perform mathematical transformations efficiently.","solution":"import torch def transform_tensor_and_compute(input_shape, scaling_value): This function should execute the following steps: 1. Create a tensor of shape `input_shape` with random values sampled from a normal distribution. 2. Normalize the tensor so that its values are between 0 and 1. 3. Scale the tensor by multiplying all its elements by `scaling_value`. 4. Apply a series of mathematical operations to transform the tensor: - Compute the sine of each element. - Add the original normalized tensor to the sine-transformed tensor. 5. Finally, return the mean and standard deviation of the resulting tensor. Parameters: - input_shape (tuple): A tuple representing the shape of the tensor to be created. - scaling_value (float): A value to scale the elements of the tensor. Returns: - mean (float): The mean of the transformed tensor. - std_dev (float): The standard deviation of the transformed tensor. # Step 1: Create tensor with random values sampled from a normal distribution tensor = torch.randn(input_shape) # Step 2: Normalize the tensor to have values between 0 and 1 tensor_min = tensor.min() tensor_max = tensor.max() normalized_tensor = (tensor - tensor_min) / (tensor_max - tensor_min) # Step 3: Scale the tensor by the given scaling value scaled_tensor = normalized_tensor * scaling_value # Step 4: Compute the sine of each element and add the original normalized tensor sine_tensor = torch.sin(scaled_tensor) transformed_tensor = sine_tensor + normalized_tensor # Step 5: Compute and return the mean and standard deviation mean = torch.mean(transformed_tensor).item() std_dev = torch.std(transformed_tensor).item() return mean, std_dev"},{"question":"**Question: Advanced Audio Processing with the `wave` Module in Python** You are provided with two uncompressed WAV files (using \\"WAVE_FORMAT_PCM\\"). The first file contains a stereo audio recording, and the second file, with an identical duration, contains a mono audio recording. Your task is to create a new, stereo WAV file that combines the left channel from the original stereo file with the mono recording as its right channel. # Requirements: 1. **Reading the provided WAV files:** - Open the two input WAV files for reading using `wave.open()`. - Ensure you extract necessary properties (channels, frame rate, sample width, number of frames) from both files to confirm compatibility. Both files should have the same frame rate and number of frames. 2. **Combining Audio Data:** - Extract the left channel from the stereo file. - Combine it with the data from the mono file to form a new stereo file. 3. **Writing to a New WAV File:** - Open a new WAV file for writing using `wave.open()` and set appropriate parameters (channels, frame rate, sample width, number of frames). - Write the combined stereo data into the new file. 4. **Exception Handling:** - Raise appropriate exceptions if the files are not compatible (e.g., different frame rates, different number of frames). # Constraints: - Use only the `wave` module for file reading and writing. - Do not use any external libraries for audio processing. - Ensure all file handles are properly closed after operations. # Function Signature: ```python def combine_wav_files(stereo_file_path: str, mono_file_path: str, output_file_path: str) -> None: pass ``` # Input: - `stereo_file_path` (str): Path to the input stereo WAV file. - `mono_file_path` (str): Path to the input mono WAV file. - `output_file_path` (str): Path to the output combined stereo WAV file. # Output: - A new WAV file created at `output_file_path` which contains the left channel from the original stereo file and the mono recording as the right channel. # Example Usage: ```python combine_wav_files(\'stereo_input.wav\', \'mono_input.wav\', \'output_combined.wav\') ``` # Notes: - Assume all files are well-formed and the paths provided are correct. - Include appropriate exception handling for cases when files are of differing formats in terms of frame rate or the number of frames.","solution":"import wave def combine_wav_files(stereo_file_path: str, mono_file_path: str, output_file_path: str) -> None: with wave.open(stereo_file_path, \'rb\') as stereo_file, wave.open(mono_file_path, \'rb\') as mono_file: # Extract parameters stereo_params = stereo_file.getparams() mono_params = mono_file.getparams() # Ensure compatibility if stereo_params.framerate != mono_params.framerate: raise ValueError(\\"Frame rates do not match.\\") if stereo_params.nframes != mono_params.nframes: raise ValueError(\\"Number of frames do not match.\\") if stereo_params.sampwidth != mono_params.sampwidth: raise ValueError(\\"Sample widths do not match.\\") if stereo_params.nchannels != 2 or mono_params.nchannels != 1: raise ValueError(\\"Incorrect number of channels in input files.\\") # Read frames stereo_frames = stereo_file.readframes(stereo_params.nframes) mono_frames = mono_file.readframes(mono_params.nframes) # Combine the left channel of the stereo file with the mono file combined_frames = bytearray() sample_width = stereo_params.sampwidth for i in range(0, len(stereo_frames), 2 * sample_width): left_channel = stereo_frames[i:i+sample_width] right_channel = mono_frames[i // 2:i // 2 + sample_width] combined_frames.extend(left_channel + right_channel) # Write to output file with wave.open(output_file_path, \'wb\') as output_file: output_file.setparams((2, stereo_params.sampwidth, stereo_params.framerate, stereo_params.nframes, stereo_params.comptype, stereo_params.compname)) output_file.writeframes(combined_frames)"},{"question":"# Seaborn Coding Assessment You are provided with the seaborn documentation focused on creating and modifying faceted plots. Your task is to write a Python function that will perform specific operations on seaborn plots using the `seaborn.objects` module. Task: Create a function `plot_penguins` that accepts the following parameters: - `data`: A dataframe object (in this case, the penguins dataset). - `x`: A string representing the column name to be plotted on the x-axis. - `y`: A string representing the column name to be plotted on the y-axis. - `facet_col`: A string indicating the column name to create facets by columns. - `facet_row`: A string indicating the column name to create facets by rows. - `share_x`: A boolean indicating whether to share the x-axis across facets. - `share_y`: A boolean indicating whether to share the y-axis across facets. Your function should: 1. Create a faceted plot using the `so.Plot` function in seaborn with the provided parameters. 2. Use the `.facet()` method to specify facets by `facet_col` and `facet_row`. 3. Adjust axes sharing properties based on `share_x` and `share_y`. 4. Add data points to the plot using `so.Dots`. Expected Output: Your function should return a seaborn Plot object with the specified configurations. Example Usage: ```python import seaborn.objects as so from seaborn import load_dataset def plot_penguins(data, x, y, facet_col, facet_row, share_x, share_y): p = ( so.Plot(data, x=x, y=y) .facet(col=facet_col, row=facet_row) .add(so.Dots()) ) p = p.share(x=share_x, y=share_y) return p # Example Data penguins = load_dataset(\\"penguins\\") # Create the plot plot = plot_penguins(penguins, \\"bill_length_mm\\", \\"bill_depth_mm\\", \\"species\\", \\"sex\\", False, False) print(plot) ``` Notes: - Ensure that your function handles various configurations of the axes sharing properties. - The function should work without errors when the provided parameters are correct. Good luck!","solution":"import seaborn.objects as so from seaborn import load_dataset def plot_penguins(data, x, y, facet_col, facet_row, share_x, share_y): Create a faceted plot using seaborn with the provided parameters. Parameters: - data: pandas DataFrame containing the data. - x: str, column name for x-axis. - y: str, column name for y-axis. - facet_col: str, column name to create facets by columns. - facet_row: str, column name to create facets by rows. - share_x: bool, whether to share the x-axis across facets. - share_y: bool, whether to share the y-axis across facets. Returns: - seaborn.objects.Plot object with the specified configurations. # Create the plot with data points added p = ( so.Plot(data, x=x, y=y) .facet(col=facet_col, row=facet_row) .add(so.Dots()) ) # Set axis sharing properties p = p.share(x=share_x, y=share_y) return p # Example usage (uncomment below lines to run) # penguins = load_dataset(\\"penguins\\") # plot = plot_penguins(penguins, \\"bill_length_mm\\", \\"bill_depth_mm\\", \\"species\\", \\"sex\\", False, True) # print(plot)"},{"question":"Objective: You are tasked with creating a Python script that utilizes the `compileall` module to compile Python source files in a specified directory. Your script should cater to specific compilation requirements and handle various options provided by the `compileall` module. Requirements: 1. **Function Implementation**: You need to implement a function `compile_python_sources(dir_path, force, workers, exclude_pattern, verbosity)`. - **Input**: - `dir_path` (string): The path of the directory containing Python source files to compile. - `force` (boolean): If `True`, force the recompilation of all files even if timestamps are up-to-date. - `workers` (integer): The number of worker threads to use for parallel compilation. If set to `0`, the function should use the optimal number of cores. - `exclude_pattern` (string): A regex pattern to exclude specific files or directories from compilation. - `verbosity` (integer): The level of verbosity for the compilation process (`0` for normal messages, `1` for errors only, `2` for complete silence). - **Output**: - Return `True` if all files compiled successfully, otherwise return `False`. 2. **Constraints**: - Use the `compileall.compile_dir` function to perform the compilation. - Ensure the function correctly handles exclusion of files matching the provided regex pattern. - The function should adjust the verbosity of the output according to the `verbosity` parameter. Example Usage: ```python # Example inputs directory_path = \'my_python_lib\' force_recompile = True num_workers = 2 exclude_regex = r\'[/][.]venv\' verbosity_level = 1 # Function call result = compile_python_sources(directory_path, force_recompile, num_workers, exclude_regex, verbosity_level) # Output print(result) # Should print True if all files compiled successfully, otherwise False ``` Additional Notes: - Ensure to handle any exceptions that may arise during the compilation process and log appropriate error messages if `verbosity` allows. - It is recommended to test the function with various combinations of inputs to ensure comprehensive coverage of the functionality. ```python import compileall import re def compile_python_sources(dir_path, force, workers, exclude_pattern, verbosity): # Your implementation here pass ```","solution":"import compileall import re def compile_python_sources(dir_path, force, workers, exclude_pattern, verbosity): Compiles Python source files in a specified directory. Parameters: - dir_path (string): The path of the directory containing Python source files to compile. - force (boolean): If True, force recompilation of all files even if timestamps are up-to-date. - workers (integer): The number of worker threads to use for parallel compilation. - exclude_pattern (string): A regex pattern to exclude specific files or directories from compilation. - verbosity (integer): The level of verbosity for the compilation process. Returns: - (boolean): True if all files compiled successfully, otherwise False. try: return compileall.compile_dir( dir_path, force=force, quiet=verbosity, workers=workers if workers != 0 else None, rx=re.compile(exclude_pattern) if exclude_pattern else None, legacy=False ) except Exception as e: if verbosity < 2: print(f\\"An error occurred: {e}\\") return False"},{"question":"You are tasked with creating a Python script that will byte-compile Python source files within a given directory based on certain conditions and parameters. Your script should also be able to handle and exclude certain files during the compilation process using regular expressions. Requirements: 1. Implement a function `compile_source_files(directory: str, recursive: bool, force: bool, quiet: int, regex: str, workers: int = 1) -> bool`. 2. The function should: - Use the `compileall` module to compile all Python source files in the specified `directory`. - If `recursive` is `True`, perform a recursive compilation. Otherwise, compile only direct files in the provided directory. - If `force` is `True`, recompile even if timestamps are up-to-date. - The `quiet` parameter controls the verbosity of the output: - `0`: Print information and errors. - `1`: Print only errors. - `2`: Suppress all output. - Use the provided `regex` pattern to exclude files from compilation that match the pattern. - Use the specified number of `workers` to compile files in parallel. If `workers` is `0`, use the number of available CPU cores. Function Signature: ```python import re import compileall from typing import Union def compile_source_files(directory: Union[str, \'os.PathLike[str]\'], recursive: bool, force: bool, quiet: int, regex: str, workers: int = 1) -> bool: # Your implementation here pass ``` Example Usage: ```python # Example: Compile all Python files in the directory \'src/\' recursively, recompile even if up-to-date, suppress all output, and exclude files matching the regex pattern \'.*test.*\'. result = compile_source_files(directory=\'src/\', recursive=True, force=True, quiet=2, regex=\'.*test.*\', workers=0) print(result) # Output: True or False based on success ``` Constraints: - You may assume the directory path provided is valid and contains Python source files. - The regular expression provided is valid. - The `compileall` module\'s `compile_dir` function should be used for directory compilation. Performance: - The provided solution should efficiently handle directories with a large number of files using parallel workers as specified. Write the `compile_source_files` function to fulfill the above requirements.","solution":"import re import compileall import os from typing import Union import multiprocessing def compile_source_files(directory: Union[str, \'os.PathLike[str]\'], recursive: bool, force: bool, quiet: int, regex: str, workers: int = 1) -> bool: regex_pattern = re.compile(regex) # Function to determine if a file should be excluded based on the regex def exclude(file): return bool(regex_pattern.search(file)) # Determine the number of workers if workers == 0: workers = multiprocessing.cpu_count() return compileall.compile_dir( dir=directory, maxlevels=0 if not recursive else 10, # Setting a high maxlevels if recursive is True ddir=None, force=force, rx=exclude, quiet=quiet, workers=workers )"},{"question":"Question: Validating Functionality with Doctest Implement a Python function `palindrome` that checks if a given string is a palindrome. The function should ignore case and non-alphanumeric characters. You must write the corresponding docstring that includes examples using the doctest module. # Function Signature ```python def palindrome(s: str) -> bool: Checks if the given string is a palindrome, ignoring case and non-alphanumeric characters. Args: s (str): The input string to check. Returns: bool: True if the input string is a palindrome, False otherwise. Examples: >>> palindrome(\\"A man, a plan, a canal, Panama\\") True >>> palindrome(\\"No \'x\' in Nixon\\") True >>> palindrome(\\"Hello, World!\\") False >>> palindrome(\\"Was it a car or a cat I saw?\\") True >>> palindrome(\\"\\") True >>> palindrome(\\"Able was I, I saw Elba\\") True pass ``` # Requirements 1. **Implementation**: - Write the function `palindrome` that will return a boolean indicating if the given string is a palindrome. - The function should ignore cases and non-alphanumeric characters. - The function should work efficiently, handling strings up to 10^6 characters. 2. **Docstring**: - Add interactive examples in the docstring as shown in the provided function signature. - Ensure the examples cover various edge cases such as empty strings, strings with special characters, mixed cases, and typical palindromic phrases. 3. **Testing**: - Integrate the doctest module to test the examples in the docstring. - At the end of your script, write the code to execute the doctests by calling `doctest.testmod()` if the module is being run as the main module. # Constraints - Input strings will only contain printable ASCII characters. - Ensure the performance of your function is optimal for large inputs. # Performance Your solution should have a time complexity of O(n), where n is the length of the input string. # Submission Submit the implementation of the `palindrome` function along with the complete docstring and doctest integration. **Hint**: Use `str.lower()` for case insensitivity and `str.isalnum()` to filter out non-alphanumeric characters.","solution":"def palindrome(s: str) -> bool: Checks if the given string is a palindrome, ignoring case and non-alphanumeric characters. Args: s (str): The input string to check. Returns: bool: True if the input string is a palindrome, False otherwise. Examples: >>> palindrome(\\"A man, a plan, a canal, Panama\\") True >>> palindrome(\\"No \'x\' in Nixon\\") True >>> palindrome(\\"Hello, World!\\") False >>> palindrome(\\"Was it a car or a cat I saw?\\") True >>> palindrome(\\"\\") True >>> palindrome(\\"Able was I, I saw Elba\\") True filtered_chars = [char.lower() for char in s if char.isalnum()] return filtered_chars == filtered_chars[::-1]"},{"question":"Building a Custom SMTP Server in Python Objective: Design an SMTP server that logs incoming email messages to a file. You will use the `smtpd` module and need to implement a custom subclass of the `SMTPServer` class to achieve this. Task: 1. Create a class `LoggingSMTPServer` that inherits from `smtpd.SMTPServer`. 2. Override the `process_message` method to write incoming email messages into a log file. Specifications: - The `process_message` method should log the following details for each email: - **Peer**: The remote host\'s address. - **Mail From**: The envelope originator. - **Recipients**: A list of envelope recipients. - **Data**: The content of the email. - Store the log in a file named `email_log.txt` such that each email is separated by a line of dashes (`\\"---\\"`). - Ensure that the log file is updated properly without concurrency issues. Input Format: - The server will receive SMTP commands and email data from clients following **RFC 5321** standards. Output Format: - Entries in the log file should look like this: ```plaintext Peer: (127.0.0.1, 12345) Mail From: example@example.com Recipients: [\'recipient1@example.com\', \'recipient2@example.com\'] Data: Subject: Test email This is a test email. --- ``` Additional Constraints: - Assume a maximum email data size limit of 1MB. - Do not enable the `SMTPUTF8` extension. - The server should run on localhost at port `8025`. Example Implementation: ```python import smtpd import asyncore class LoggingSMTPServer(smtpd.SMTPServer): def process_message(self, peer, mailfrom, rcpttos, data, **kwargs): with open(\'email_log.txt\', \'a\') as log_file: log_file.write(f\\"Peer: {peer}n\\") log_file.write(f\\"Mail From: {mailfrom}n\\") log_file.write(f\\"Recipients: {rcpttos}n\\") log_file.write(f\\"Data: {data}n\\") log_file.write(\'---n\') if __name__ == \\"__main__\\": server = LoggingSMTPServer((\'localhost\', 8025), None) asyncore.loop() ``` Performance Requirements: - The server should handle a reasonable number of concurrent connections seamlessly. - Properly manage file I/O to prevent race conditions. **Good luck!**","solution":"import smtpd import asyncore import threading class LoggingSMTPServer(smtpd.SMTPServer): def process_message(self, peer, mailfrom, rcpttos, data, **kwargs): log_entry = ( f\\"Peer: {peer}n\\" f\\"Mail From: {mailfrom}n\\" f\\"Recipients: {rcpttos}n\\" f\\"Data: {data}n\\" \\"---n\\" ) with open(\'email_log.txt\', \'a\') as log_file: log_file.write(log_entry) if __name__ == \\"__main__\\": server = LoggingSMTPServer((\'localhost\', 8025), None) threading.Thread(target=asyncore.loop).start()"},{"question":"**Objective**: Demonstrate your comprehension of Seaborn\'s `sns.diverging_palette` function and your ability to visualize data effectively using customized color palettes. **Task**: 1. **Create Diverging Palettes**: - Generate a diverging color palette that transitions from blue (hue 240) to red (hue 20) through white. Return the palette as a colormap (`as_cmap=True`). - Create a second diverging color palette that transitions from magenta (hue 280) to green (hue 150) through white, with the center color being dark. Reduce the lightness of the endpoints to 35. - Generate a third diverging palette from blue (hue 240) to red (hue 20) through white, increasing the separation around the center to 40, and decrease the saturation of the palette to 60. 2. **Visualize Data**: - Create three heatmap plots using Seaborn or Matplotlib. Each heatmap should use one of the color palettes created in Task 1. - Use a randomly generated 10x10 matrix of values centered around 0 for each heatmap. 3. **Analyze and Interpret**: - Write a brief analysis (3-4 sentences) comparing the visual effects of the different palettes on the heatmaps. Discuss how the changes in hue, lightness, separation, and saturation impact the display of data. **Input/Output**: - **Input**: No specific input needed; generate a 10x10 random data matrix using numpy. - **Output**: 3 heatmap plots displayed using the three customized palettes and a brief written analysis. **Constraints/Requirements**: 1. Import necessary libraries: ```python import seaborn as sns import matplotlib.pyplot as plt import numpy as np ``` 2. Use the following code template for guidance: ```python # 1. Create Diverging Palettes palette1 = sns.diverging_palette(240, 20, as_cmap=True) palette2 = sns.diverging_palette(280, 150, center=\\"dark\\", l=35, as_cmap=True) palette3 = sns.diverging_palette(240, 20, sep=40, s=60, as_cmap=True) # 2. Generate Random Data data = np.random.randn(10, 10) # 3. Plot Heatmaps plt.figure(figsize=(15, 5)) sns.heatmap(data, cmap=palette1, center=0) plt.title(\\"Palette 1: Blue to Red\\") plt.show() sns.heatmap(data, cmap=palette2, center=0) plt.title(\\"Palette 2: Magenta to Green with Dark Center\\") plt.show() sns.heatmap(data, cmap=palette3, center=0) plt.title(\\"Palette 3: Blue to Red with Increased Separation and Reduced Saturation\\") plt.show() # 4. Analysis # (Write your analysis here) ``` **Example Analysis**: ``` In the first heatmap, the transition from blue to red clearly highlights the areas above and below zero, with white representing the center. The second heatmap, using magenta and green with a dark center, offers a stark contrast that makes the center values more pronounced. The third heatmap with increased separation and reduced saturation provides a more subtle gradient, making gradual changes near the center more visible but less distinct at the extremes. ```","solution":"import seaborn as sns import matplotlib.pyplot as plt import numpy as np def create_diverging_palettes(): Create three diverging palettes: - Palette 1: Blue (hue 240) to Red (hue 20) through white. - Palette 2: Magenta (hue 280) to Green (hue 150) through white, with dark center, and lightness reduced to 35 at endpoints. - Palette 3: Blue (hue 240) to Red (hue 20) through white, with increased separation around the center (sep=40) and reduced saturation (s=60). Returns the three palettes as colormaps. palette1 = sns.diverging_palette(240, 20, as_cmap=True) palette2 = sns.diverging_palette(280, 150, center=\\"dark\\", l=35, as_cmap=True) palette3 = sns.diverging_palette(240, 20, sep=40, s=60, as_cmap=True) return palette1, palette2, palette3 def generate_heatmaps(palette1, palette2, palette3): Generate three heatmaps using a random 10x10 matrix of values centered around 0. Each heatmap is displayed using one of the diverging palettes. data = np.random.randn(10, 10) plt.figure(figsize=(15, 5)) plt.subplot(1, 3, 1) sns.heatmap(data, cmap=palette1, center=0) plt.title(\\"Palette 1: Blue to Red\\") plt.subplot(1, 3, 2) sns.heatmap(data, cmap=palette2, center=0) plt.title(\\"Palette 2: Magenta to Green with Dark Center\\") plt.subplot(1, 3, 3) sns.heatmap(data, cmap=palette3, center=0) plt.title(\\"Palette 3: Blue to Red with Increased Separation and Reduced Saturation\\") plt.show() # Create the color palettes palettes = create_diverging_palettes() # Generate and display the heatmaps generate_heatmaps(*palettes)"},{"question":"# Coding Task Objective: You are required to implement a generator function along with an asynchronous generator function, demonstrating advanced comprehension capabilities. These generators will dynamically generate elements and return specific results based on specific arithmetic and filtering conditions. Problem Description: 1. **Generator Function:** Implement a generator function `sequence_generator` that generates an infinite arithmetic sequence starting from a given initial value `start`, with a fixed common difference of `d`, formatted as: ```python def sequence_generator(start: int, d: int) -> Generator[int, None, None]: ``` 2. **Filter Function:** Implement a function `filter_generator` which takes the generator as input and yields only those values that satisfy: - The value is even. - The value is divisible by a given divisor `k`. ```python def filter_generator(gen: Generator[int, None, None], k: int) -> Generator[int, None, None]: ``` 3. **Asynchronous Generator Function:** Implement an asynchronous generator function `async_sequence_generator` that behaves similarly to the `sequence_generator` but with asynchronous functionality. ```python async def async_sequence_generator(start: int, d: int) -> AsyncGenerator[int, None]: ``` 4. **Asynchronous Filter Function:** Implement an asynchronous version of the `filter_generator`, which processes the sequence asynchronously. ```python async def async_filter_generator(a_gen: AsyncGenerator[int, None], k: int) -> AsyncGenerator[int, None]: ``` Input: - For `sequence_generator` and `filter_generator`, the inputs are: - `start` - integer representing the initial value of the sequence. - `d` - integer representing the common difference. - `k` - integer representing the divisor. - For `async_sequence_generator` and `async_filter_generator`, the inputs are the same but the generator functions are designed to work asynchronously. Output: - The functions `sequence_generator` and `filter_generator` return generators that produce the specified sequence. - The functions `async_sequence_generator` and `async_filter_generator` return asynchronous generators with the same characteristics. Constraints: - Ensure that the operations within the generators handle large number of generated elements without exhausting memory (i.e., use of infinite generators and lazy evaluation). - Use filtering efficiently to avoid unnecessary computations. - Utilize asynchronous features appropriately in `async_sequence_generator` and `async_filter_generator` to manage concurrent sequences. Example: ```python # Example usage of the synchronous generator functions gen = sequence_generator(0, 2) filt_gen = filter_generator(gen, 4) print(next(filt_gen)) # Output: 0 print(next(filt_gen)) # Output: 4 print(next(filt_gen)) # Output: 8 # Example usage of the asynchronous generator functions import asyncio async def main(): a_gen = async_sequence_generator(1, 3) a_filt_gen = async_filter_generator(a_gen, 9) print(await anext(a_filt_gen)) # Output: 9 print(await anext(a_filt_gen)) # Output: 27 asyncio.run(main()) ``` Implement these functions and test them to ensure they work correctly for various sequences and filter values.","solution":"from typing import Generator, AsyncGenerator # Synchronous generator function def sequence_generator(start: int, d: int) -> Generator[int, None, None]: while True: yield start start += d # Synchronous filter generator function def filter_generator(gen: Generator[int, None, None], k: int) -> Generator[int, None, None]: for value in gen: if value % 2 == 0 and value % k == 0: yield value # Asynchronous generator function async def async_sequence_generator(start: int, d: int) -> AsyncGenerator[int, None]: while True: yield start start += d # Asynchronous filter generator function async def async_filter_generator(a_gen: AsyncGenerator[int, None], k: int) -> AsyncGenerator[int, None]: async for value in a_gen: if value % 2 == 0 and value % k == 0: yield value"},{"question":"# Advanced Pandas GroupBy Challenge Problem Statement You are provided with a dataset containing information about various products sold across different regions and cities. The dataset also contains sales figures and ratings for these products. Your task is to perform a series of group operations to analyze the sales data: ```python import pandas as pd data = { \\"Region\\": [\\"North\\", \\"South\\", \\"East\\", \\"West\\", \\"North\\", \\"South\\", \\"East\\", \\"West\\", \\"North\\", \\"South\\"], \\"City\\": [\\"NY\\", \\"LA\\", \\"SF\\", \\"CHI\\", \\"NY\\", \\"LA\\", \\"SF\\", \\"CHI\\", \\"NY\\", \\"LA\\"], \\"Product\\": [\\"A\\", \\"A\\", \\"A\\", \\"A\\", \\"B\\", \\"B\\", \\"B\\", \\"B\\", \\"C\\", \\"C\\"], \\"Sales\\": [50, 60, 70, 80, 30, 40, 50, 60, 20, 30], \\"Rating\\": [4.1, 4.2, 4.3, 4.8, 3.9, 4.0, 4.5, 4.6, 4.1, 4.2] } df = pd.DataFrame(data) ``` # Tasks **1. Grouping and Summarizing** - Group the dataset by \'Region\' and \'Product\' and compute the following statistics for each group: - Total Sales. - Average Rating. - Number of Cities where the product is sold. **2. Filtering Groups** - Filter out groups where the total sales are less than 100. **3. Transformation** - Within each product, normalize the sales (\'Sales\') by subtracting the group\'s average sales and dividing by the group\'s standard deviation. **4. Custom Aggregation** - Add a custom aggregation that calculates the range (max - min) of the \'Rating\' for each group. Instructions 1. Complete the function `analyze_sales_data(df)` that accepts the DataFrame `df` and performs the tasks described above. 2. Return a dictionary with the following keys and the respective DataFrames as values: - `group_summary`: The DataFrame containing the result of Task 1. - `filtered_groups`: The DataFrame containing the result of Task 2. - `normalized_sales`: The DataFrame containing the result of Task 3. - `custom_aggregation`: The DataFrame containing the result of Task 4. # Constraints - You should use the most efficient pandas methods available. - Handle missing values appropriately. ```python def analyze_sales_data(df): # Task 1: Grouping and Summarizing group_summary = df.groupby([\'Region\', \'Product\']).agg( total_sales=pd.NamedAgg(column=\'Sales\', aggfunc=\'sum\'), average_rating=pd.NamedAgg(column=\'Rating\', aggfunc=\'mean\'), city_count=pd.NamedAgg(column=\'City\', aggfunc=\'nunique\') ).reset_index() # Task 2: Filtering Groups filtered_groups = group_summary[group_summary[\'total_sales\'] >= 100] # Task 3: Transformation - Normalize the sales within each product group df[\'normalized_sales\'] = df.groupby(\'Product\')[\'Sales\'].transform(lambda x: (x - x.mean()) / x.std()) # Task 4: Custom Aggregation - Calculate range of rating for each group custom_aggregation = df.groupby([\'Region\', \'Product\']).agg( rating_range=pd.NamedAgg(column=\'Rating\', aggfunc=lambda x: x.max() - x.min()) ).reset_index() return { \\"group_summary\\": group_summary, \\"filtered_groups\\": filtered_groups, \\"normalized_sales\\": df, \\"custom_aggregation\\": custom_aggregation } # Example Call to the function df = pd.DataFrame(data) results = analyze_sales_data(df) ``` Expected Output Format - The function should return a dictionary with the expected DataFrames as indicated in the keys.","solution":"import pandas as pd def analyze_sales_data(df): # Task 1: Grouping and Summarizing group_summary = df.groupby([\'Region\', \'Product\']).agg( total_sales=pd.NamedAgg(column=\'Sales\', aggfunc=\'sum\'), average_rating=pd.NamedAgg(column=\'Rating\', aggfunc=\'mean\'), city_count=pd.NamedAgg(column=\'City\', aggfunc=\'nunique\') ).reset_index() # Task 2: Filtering Groups filtered_groups = group_summary[group_summary[\'total_sales\'] >= 100] # Task 3: Transformation - Normalize the sales within each product group df[\'normalized_sales\'] = df.groupby(\'Product\')[\'Sales\'].transform(lambda x: (x - x.mean()) / x.std()) # Task 4: Custom Aggregation - Calculate range of rating for each group custom_aggregation = df.groupby([\'Region\', \'Product\']).agg( rating_range=pd.NamedAgg(column=\'Rating\', aggfunc=lambda x: x.max() - x.min()) ).reset_index() return { \\"group_summary\\": group_summary, \\"filtered_groups\\": filtered_groups, \\"normalized_sales\\": df, \\"custom_aggregation\\": custom_aggregation } # Example Call to the function if __name__ == \\"__main__\\": data = { \\"Region\\": [\\"North\\", \\"South\\", \\"East\\", \\"West\\", \\"North\\", \\"South\\", \\"East\\", \\"West\\", \\"North\\", \\"South\\"], \\"City\\": [\\"NY\\", \\"LA\\", \\"SF\\", \\"CHI\\", \\"NY\\", \\"LA\\", \\"SF\\", \\"CHI\\", \\"NY\\", \\"LA\\"], \\"Product\\": [\\"A\\", \\"A\\", \\"A\\", \\"A\\", \\"B\\", \\"B\\", \\"B\\", \\"B\\", \\"C\\", \\"C\\"], \\"Sales\\": [50, 60, 70, 80, 30, 40, 50, 60, 20, 30], \\"Rating\\": [4.1, 4.2, 4.3, 4.8, 3.9, 4.0, 4.5, 4.6, 4.1, 4.2] } df = pd.DataFrame(data) print(analyze_sales_data(df))"},{"question":"# Python Coding Assessment Question Objective: Demonstrate proficiency with the `tempfile` module by creating functions that utilize temporary files and directories effectively. Problem Statement: You are required to write a function named `process_customer_data` that processes customer data stored in a text file. The function should: 1. **Create a temporary directory** to store temporary files. 2. **Create a temporary file** within this directory to store processed information. 3. Read the provided customer data from a given input file, process it, and write the processed information to the temporary file. - The processing involves converting all names to uppercase. 4. Return the path to the temporary file where the processed data is stored and the temporary directory\'s path. **Function Signature:** ```python def process_customer_data(input_filepath: str) -> tuple: pass ``` **Input:** - `input_filepath` (str): The path to the input file containing customer data. Each line of the file represents a customer name. **Output:** - A tuple with two elements: - The path to the temporary file containing the processed data (str). - The path to the temporary directory (str). **Constraints:** - The function must ensure that the temporary file and directory are automatically cleaned up when they are no longer needed. **Example Usage:** ```python input_data = \\"customer_data.txt\\" with open(input_data, \\"w\\") as file: file.write(\\"AlicenBobnCharlien\\") temp_file_path, temp_dir_path = process_customer_data(input_data) # Verify the temporary file and its contents with open(temp_file_path, \\"r\\") as file: print(file.read()) # Output should be: # ALICE # BOB # CHARLIE ``` **Notes:** - Use the `TemporaryDirectory` and `NamedTemporaryFile` functionalities from the `tempfile` module. - Ensure proper cleanup of temporary resources either explicitly or by using context managers. **Hint:** Consult the `tempfile` documentation on how to create and manage temporary files and directories with context managers for automatic cleanup.","solution":"import tempfile import shutil def process_customer_data(input_filepath: str) -> tuple: # Create a temporary directory temp_dir = tempfile.mkdtemp() # Create a temporary file within the temporary directory temp_file = tempfile.NamedTemporaryFile(mode=\'w\', dir=temp_dir, delete=False) try: # Read the provided customer data from the input file with open(input_filepath, \'r\') as infile: data = infile.readlines() # Process the data: Convert all names to uppercase processed_data = [name.strip().upper() for name in data] # Write the processed data to the temporary file temp_file.writelines(\'n\'.join(processed_data) + \'n\') temp_file.close() # Return the path to the temporary file and directory return (temp_file.name, temp_dir) except Exception as e: # Cleanup in case of an error shutil.rmtree(temp_dir) raise e"},{"question":"**Question: Manipulating and Analyzing Pandas Index Objects** **Objective:** Demonstrate your ability to create, manipulate, and analyze various types of index objects using pandas. **Problem Statement:** You are given a DataFrame of sales data for a retail store. The DataFrame consists of the following columns: - `product_id`: The unique identifier for each product. - `product_category`: The category of the product. - `date`: The date of the sale. - `units_sold`: The number of units sold on that day. Your task is to perform the following operations using pandas: 1. **Index Creation:** - Create a `MultiIndex` from the `product_id` and `date` columns. 2. **Index Manipulation:** - Using the created `MultiIndex`, reindex the DataFrame to include all combinations of `product_id` and `date` within the range of dates present in the original DataFrame (fill missing dates with NaNs). 3. **Index Analysis:** - Find out how many unique product categories are present in the sales data. - Identify any duplicate entries in the `product_id` and `date` combinations. 4. **Handling Missing Values:** - Fill any missing `units_sold` values with zero. 5. **Time-series Specific Operation:** - Determine the total units sold for each product by month and return a DataFrame with `product_id` and `date` (month) as indices and `units_sold` as the column. 6. **Conversion and Sorting:** - Convert the `MultiIndex` back to a standard index, sort the DataFrame by `date` in ascending order, and return the resulting DataFrame. **Input:** - `df`: A pandas DataFrame with columns `[product_id, product_category, date, units_sold]`. **Output:** - A dictionary with the following keys and values: - `\'multiindex_df\'`: DataFrame reindexed with `MultiIndex`. - `\'unique_categories\'`: Integer count of unique product categories. - `\'duplicates\'`: Boolean indicating if there are duplicate `(product_id, date)` combinations. - `\'filled_df\'`: DataFrame with missing `units_sold` values filled with zero. - `\'monthly_sales\'`: DataFrame with `product_id` and `month` as indices and `units_sold` as the column. - `\'sorted_df\'`: DataFrame sorted by `date` in ascending order. **Constraints:** - `date` column is a series of strings in \'YYYY-MM-DD\' format. - All operations should use pandas\' built-in functions. **Performance Requirements:** - Ensure that the DataFrame manipulations and analyses are efficient in time and memory usage. ```python import pandas as pd def manipulate_sales_data(df): # Convert \'date\' column to datetime df[\'date\'] = pd.to_datetime(df[\'date\']) # 1. Create MultiIndex from \'product_id\' and \'date\' columns df.set_index([\'product_id\', \'date\'], inplace=True) multiindex_df = df.copy() # 2. Reindex DataFrame to include all combinations of product_id and date range all_dates = pd.date_range(start=df.index.get_level_values(\'date\').min(), end=df.index.get_level_values(\'date\').max()) reindex_df = multiindex_df.reindex( pd.MultiIndex.from_product([df.index.get_level_values(\'product_id\').unique(), all_dates], names=[\'product_id\', \'date\']) ) # 3. Analyze unique product categories and duplicates unique_categories = df[\'product_category\'].nunique() duplicates = df.index.duplicated().any() # 4. Fill missing \'units_sold\' with zero filled_df = reindex_df.fillna({\'units_sold\': 0}) # 5. Determine monthly units sold for each product filled_df.reset_index(inplace=True) filled_df[\'month\'] = filled_df[\'date\'].dt.to_period(\'M\') monthly_sales = filled_df.groupby([\'product_id\', \'month\'])[\'units_sold\'].sum().to_frame() # 6. Convert MultiIndex back to standard index and sort by date sorted_df = reindex_df.reset_index().sort_values(by=\'date\') return { \'multiindex_df\': multiindex_df, \'unique_categories\': unique_categories, \'duplicates\': duplicates, \'filled_df\': filled_df, \'monthly_sales\': monthly_sales, \'sorted_df\': sorted_df } ``` **Example:** Given the following input DataFrame: ```python df = pd.DataFrame({ \'product_id\': [1, 1, 2, 2], \'product_category\': [\'A\', \'A\', \'B\', \'B\'], \'date\': [\'2023-01-01\', \'2023-01-02\', \'2023-01-01\', \'2023-01-03\'], \'units_sold\': [10, 15, 20, 25] }) ``` The function `manipulate_sales_data(df)` should return a dictionary with the required DataFrames and analysis results.","solution":"import pandas as pd def manipulate_sales_data(df): # Convert \'date\' column to datetime df[\'date\'] = pd.to_datetime(df[\'date\']) # 1. Create MultiIndex from \'product_id\' and \'date\' columns df.set_index([\'product_id\', \'date\'], inplace=True) multiindex_df = df.copy() # 2. Reindex DataFrame to include all combinations of product_id and date range all_dates = pd.date_range(start=df.index.get_level_values(\'date\').min(), end=df.index.get_level_values(\'date\').max()) reindex_df = multiindex_df.reindex( pd.MultiIndex.from_product([df.index.get_level_values(\'product_id\').unique(), all_dates], names=[\'product_id\', \'date\']) ) # 3. Analyze unique product categories and duplicates unique_categories = df[\'product_category\'].nunique() duplicates = df.index.duplicated().any() # 4. Fill missing \'units_sold\' with zero filled_df = reindex_df.fillna({\'units_sold\': 0}) # 5. Determine monthly units sold for each product filled_df.reset_index(inplace=True) filled_df[\'month\'] = filled_df[\'date\'].dt.to_period(\'M\') monthly_sales = filled_df.groupby([\'product_id\', \'month\'])[\'units_sold\'].sum().to_frame() # 6. Convert MultiIndex back to standard index and sort by date sorted_df = reindex_df.reset_index().sort_values(by=\'date\') return { \'multiindex_df\': multiindex_df, \'unique_categories\': unique_categories, \'duplicates\': duplicates, \'filled_df\': filled_df, \'monthly_sales\': monthly_sales, \'sorted_df\': sorted_df }"},{"question":"# Assessment Question: Visualizing Data with Seaborn **Objective:** You are provided with a dataset related to monthly airline passenger counts from 1949 to 1960. The objective is to demonstrate your comprehension of seaborn\'s capabilities by visualizing this dataset in multiple ways after manipulating the data formats. **Dataset:** The dataset consists of three columns – `year`, `month`, and `passengers`. Here is a sample of the dataset: ```plaintext year month passengers 1949 January 112 1949 February 118 1949 March 132 1949 April 129 1949 May 121 ... ``` **Tasks:** 1. **Import Libraries:** Import necessary libraries including seaborn, matplotlib, pandas, and numpy (if needed). 2. **Load the Dataset:** Load the \\"flights\\" dataset from seaborn\'s example datasets. 3. **Plot Long-form Data:** Create a line plot using the long-form dataset to visualize the number of passengers over time with different line colors for each month. - The x-axis should represent the year. - The y-axis should represent the number of passengers. - Use the `hue` parameter to differentiate between months. 4. **Transform to Wide-form Data:** Transform the dataset into a wide-form where each column represents a month and the cells contain the corresponding number of passengers for that month. 5. **Plot Wide-form Data:** Create a line plot using the wide-form dataset to visualize the number of passengers over time. - Ensure to label the x-axis correctly as it represents the year. - Label the y-axis to represent the number of passengers. 6. **Transform Complex Dataset and Plot:** Create a complex dataset from the existing dataset by adding another variable (e.g., categorize years into different periods like \\"1949-1952\\", \\"1953-1956\\", etc.). Transform the dataset into a long-form structure with these categories and create a point plot to visualize the average number of passengers for each category across different months. - Use the `category` variable for the `hue` parameter. - Ensure proper labeling of x-axis and y-axis. 7. **Optional (Bonus) – Messy Data:** Given a hypothetical \'messy\' dataset (e.g., with combined columns such as \'period_month\', \'passengers\'), demonstrate how you would clean it using pandas and plot it appropriately. **Expected Deliverables:** - Python code implementing all the steps mentioned. - Resulting plots should be clear and labeled correctly. - Explanation of the transformations and visualizations created. **Note:** - Ensure your code is well-documented. - Use appropriate seaborn functions to achieve the tasks efficiently. - Handle any edge cases or errors gracefully with clear messages. ```python # 1. Import Libraries import seaborn as sns import pandas as pd import numpy as np import matplotlib.pyplot as plt # 2. Load the Dataset flights = sns.load_dataset(\\"flights\\") # 3. Plot Long-form Data plt.figure(figsize=(12, 6)) sns.relplot(data=flights, x=\\"year\\", y=\\"passengers\\", hue=\\"month\\", kind=\\"line\\") plt.title(\'Number of Passengers Over Years by Month (Long-form)\') plt.show() # 4. Transform to Wide-form Data flights_wide = flights.pivot(index=\\"year\\", columns=\\"month\\", values=\\"passengers\\") # 5. Plot Wide-form Data plt.figure(figsize=(12, 6)) sns.relplot(data=flights_wide, kind=\\"line\\") plt.title(\'Number of Passengers Over Years by Month (Wide-form)\') plt.xlabel(\'Year\') plt.ylabel(\'Number of Passengers\') plt.show() # 6. Transform Complex Dataset and Plot # Categorize years into different periods flights[\'period\'] = pd.cut(flights[\'year\'], bins=[1949, 1952, 1956, 1960], labels=[\'1949-1952\', \'1953-1956\', \'1957-1960\']) flights_long = flights.melt(id_vars=[\\"period\\", \\"month\\"], var_name=\\"year\\", value_name=\\"passengers\\") # Plotting plt.figure(figsize=(12, 6)) sns.catplot(data=flights, x=\\"month\\", y=\\"passengers\\", hue=\\"period\\", kind=\\"point\\") plt.title(\'Average Number of Passengers by Period and Month\') plt.xlabel(\'Month\') plt.ylabel(\'Average Number of Passengers\') plt.show() # 7. Optional (Bonus) – Messy Data # Example code for cleaning a hypothetical messy dataset ```","solution":"# 1. Import Libraries import seaborn as sns import pandas as pd import numpy as np import matplotlib.pyplot as plt # 2. Load the Dataset flights = sns.load_dataset(\\"flights\\") # 3. Plot Long-form Data plt.figure(figsize=(12, 6)) sns.lineplot(data=flights, x=\\"year\\", y=\\"passengers\\", hue=\\"month\\") plt.title(\'Number of Passengers Over Years by Month (Long-form)\') plt.show() # 4. Transform to Wide-form Data flights_wide = flights.pivot(index=\\"year\\", columns=\\"month\\", values=\\"passengers\\") # 5. Plot Wide-form Data plt.figure(figsize=(12, 6)) sns.lineplot(data=flights_wide) plt.title(\'Number of Passengers Over Years by Month (Wide-form)\') plt.xlabel(\'Year\') plt.ylabel(\'Number of Passengers\') plt.show() # 6. Transform Complex Dataset and Plot # Categorize years into different periods flights[\'period\'] = pd.cut(flights[\'year\'], bins=[1949, 1952, 1956, 1960], labels=[\'1949-1952\', \'1953-1956\', \'1957-1960\']) flights_avg = flights.groupby([\'period\', \'month\']).agg({\'passengers\': \'mean\'}).reset_index() # Plotting plt.figure(figsize=(12, 6)) sns.pointplot(data=flights_avg, x=\\"month\\", y=\\"passengers\\", hue=\\"period\\") plt.title(\'Average Number of Passengers by Period and Month\') plt.xlabel(\'Month\') plt.ylabel(\'Average Number of Passengers\') plt.show() # 7. Optional (Bonus) – Messy Data # Example code for cleaning a hypothetical messy dataset # For example: # \'period_month\' column which is a concatenation of the period and month, e.g., \'1949-1952_January\' # We will split this into the \'period\' and \'month\' columns and then plot hypothetical_messy_data = { \'period_month\': [\'1949-1952_January\', \'1949-1952_February\', \'1953-1956_January\', \'1957-1960_March\'], \'passengers\': [120, 135, 145, 160] } df_messy = pd.DataFrame(hypothetical_messy_data) df_messy[[\'period\', \'month\']] = df_messy[\'period_month\'].str.split(\'_\', expand=True) # Cleaned and ready to plot plt.figure(figsize=(12, 6)) sns.barplot(data=df_messy, x=\\"month\\", y=\\"passengers\\", hue=\\"period\\") plt.title(\'Number of Passengers in Hypothetical Messy Dataset\') plt.xlabel(\'Month\') plt.ylabel(\'Number of Passengers\') plt.show()"},{"question":"**Question: Implement and Use the py_compile Module to Compile Python Files** Given the task of compiling a directory of Python source files into byte-code files for deployment, you are required to implement a function that utilizes the `py_compile` module to compile all `.py` files in a specified directory. Additionally, handle compilation errors gracefully and output the corresponding paths of the compiled byte-code files. **Function Specification** Implement a function `compile_python_files` with the following signature: ```python import py_compile import os def compile_python_files(directory: str, raise_errors: bool = False, optimization_level: int = -1) -> dict: Compile all Python source files in the given directory into byte-code files. Args: - directory (str): The directory containing Python source files to compile. - raise_errors (bool): Whether to raise exceptions on compilation errors. Default is False. - optimization_level (int): The optimization level for byte-code compilation. Default is -1. Returns: - dict: A dictionary with source file paths as keys and compiled file paths or error messages as values. ``` # Input - `directory`: A string representing the path to the directory containing Python source files. - `raise_errors`: A boolean flag to indicate whether exceptions should be raised on compilation errors. - `optimization_level`: An integer to define the optimization level for byte-code compilation (-1 for default). # Output - A dictionary where keys are paths of the source files, and values are paths to the compiled byte-code files or error messages if compilation failed. # Constraints 1. The directory will have at most 1000 Python files. 2. The function should handle errors based on the `raise_errors` flag: - If `raise_errors` is True, the function should raise `py_compile.PyCompileError` on any compilation error. - If `raise_errors` is False, the function should capture the error as a string and include it in the returned dictionary instead of raising the exception. 3. Optimization level can be set from -1 (default) up to 2. # Example Usage ```python # Example directory structure: # /path/to/dir/ # ├── a.py # ├── b.py # ├── c.py result = compile_python_files(\\"/path/to/dir\\", raise_errors=True, optimization_level=2) for src, compiled in result.items(): print(f\\"Source: {src} -> Compiled: {compiled}\\") ``` # Additional Requirements - The function should ensure the compiled files are written to appropriate paths as defined by PEP 3147. - Implement error handling for scenarios where the path is a symlink or non-regular file and handle other common file I/O errors. - Demonstrate the function\'s usage with a brief example in comments. --- **Notes:** - Review the `py_compile` module documentation for more details on parameters and expected behavior. - Provide a docstring for the function explaining the parameters, return values, and any raised exceptions.","solution":"import py_compile import os from typing import Union def compile_python_files(directory: str, raise_errors: bool = False, optimization_level: int = -1) -> dict: Compile all Python source files in the given directory into byte-code files. Args: - directory (str): The directory containing Python source files to compile. - raise_errors (bool): Whether to raise exceptions on compilation errors. Default is False. - optimization_level (int): The optimization level for byte-code compilation. Default is -1. Returns: - dict: A dictionary with source file paths as keys and compiled file paths or error messages as values. results = {} for root, _, files in os.walk(directory): for file in files: if file.endswith(\\".py\\"): src_file = os.path.join(root, file) try: compiled_file = py_compile.compile(src_file, cfile=None, dfile=None, doraise=True, optimize=optimization_level) results[src_file] = compiled_file except py_compile.PyCompileError as e: error_msg = str(e) if raise_errors: raise results[src_file] = error_msg return results"},{"question":"# Distributed Processing Error Handling with PyTorch You are given the task of simulating a distributed multiprocessing environment using PyTorch, where you need to handle errors effectively using the `torch.distributed.elastic.multiprocessing.errors` module. Your task is to implement a function that processes data across multiple child processes and ensures graceful error handling and logging. Function Signature ```python def distributed_processing(data: List[int]) -> List[int]: pass ``` Input - `data` (List[int]): A list of integers to be processed. Output - List[int]: A list of integers after processing. If an error occurs in any child process, include an entry \'-1\' in the output to indicate failure. Constraints 1. Each element must be squared. 2. Use multiple processes to distribute the work. 3. If any process fails, it should be logged, and the corresponding result should be \'-1\'. Requirements 1. Use the `torch.distributed.elastic.multiprocessing.errors` module to handle and log errors. 2. Demonstrate error handling by inducing an artificial error for specific input values (e.g., any value equal to 3). 3. Ensure all processes terminate correctly, capturing any `ChildFailedError` or `ProcessFailure`. Example ```python input_data = [1, 2, 3, 4] output_data = distributed_processing(input_data) # Expected output: [1, 4, -1, 16] ``` In this example, the function should process each item: - 1 -> 1^2 = 1 - 2 -> 2^2 = 4 - 3 -> Error, hence -1 - 4 -> 4^2 = 16 Note - You may use `multiprocessing` or any suitable module to create a distributed processing setup. - Use of `record`, `ChildFailedError`, and `ErrorHandler` is expected for proper error management. Hints - Refer to the PyTorch documentation to understand how to use the specified methods and classes for error handling. - Consider creating helper functions to simulate and handle errors.","solution":"import torch import multiprocessing as mp from torch.distributed.elastic.multiprocessing.errors import record, ChildFailedError, ProcessFailure def worker(data, output, idx): try: # Simulate processing if data == 3: raise ValueError(\\"Induced error for value 3\\") output[idx] = data ** 2 except Exception as e: output[idx] = -1 raise e @record def distributed_processing(data): manager = mp.Manager() output = manager.list([-1] * len(data)) processes = [] for i, value in enumerate(data): p = mp.Process(target=worker, args=(value, output, i)) processes.append(p) p.start() for p in processes: p.join() return list(output)"},{"question":"# Question: Implement a Simulator for Python Cell Objects You are tasked to create a class `Cell` that simulates the behavior of Python\'s cell objects as described in the provided documentation. This class should enable the creation of cell objects, and allow for checking, getting, and setting their contents. Class `Cell` Requirements: 1. **Initialization**: - The constructor should accept an initial value (which could be `None`). - Example: `cell = Cell(value)` 2. **Methods**: - `is_cell(obj)`: Static method that checks if the given object is an instance of the `Cell` class. ```python @staticmethod def is_cell(obj) -> bool ``` - `get()`: Returns the contents of the cell object. ```python def get(self) -> Optional[Any] ``` - `set(value)`: Updates the cell’s contents with a new value. ```python def set(self, value: Any) -> None ``` - `unsafe_get()`: Similar to `get` but skips any safety checks (for the simulation, assume this method is identical to `get`). ```python def unsafe_get(self) -> Optional[Any] ``` - `unsafe_set(value)`: Similar to `set` but skips any safety checks. ```python def unsafe_set(self, value: Any) -> None ``` Implementation Rules: 1. **Initialization**: - When a new `Cell` object is created, it should store the provided initial value. 2. **Type Checking**: - The `is_cell` method should return `True` if the given object is an instance of the `Cell` class, otherwise `False`. 3. **Retrieval and Setting**: - `get` should return the current value stored in the cell. - `set` should update the cell’s value. - `unsafe_get` and `unsafe_set` behave like `get` and `set` but are included for completeness of the simulation. Example Usage: ```python # Create a new cell with initial value 10 cell = Cell(10) print(cell.get()) # Output: 10 # Set a new value in the cell cell.set(20) print(cell.get()) # Output: 20 # Check if an object is a cell print(Cell.is_cell(cell)) # Output: True print(Cell.is_cell(123)) # Output: False # Unsafe methods (for simulation, identical to safe methods) print(cell.unsafe_get()) # Output: 20 cell.unsafe_set(30) print(cell.unsafe_get()) # Output: 30 ``` Additional Constraints: - You may assume all inputs to the methods are valid unless specified otherwise. - Your implementation should be efficient and adhere to the object-oriented principles. Implement the `Cell` class in Python based on these specifications.","solution":"from typing import Any, Optional class Cell: def __init__(self, value: Optional[Any] = None): self._value = value @staticmethod def is_cell(obj) -> bool: return isinstance(obj, Cell) def get(self) -> Optional[Any]: return self._value def set(self, value: Any) -> None: self._value = value def unsafe_get(self) -> Optional[Any]: return self._value def unsafe_set(self, value: Any) -> None: self._value = value"},{"question":"**Objective:** Implement a custom logging configuration using Python\'s `logging` module. The task involves setting up different loggers, handlers, and formatters to log messages to various destinations with specific formats and levels. **Problem Statement:** You are tasked with creating a logging configuration for an application that handles both user activities and system errors. Design a logging system using Python\'s `logging` module that fulfills the following requirements: 1. There should be two loggers: - `application`: This logger records general user activities. - `system`: This logger records system errors and critical issues. 2. Logging levels: - The `application` logger should log messages of level `INFO` and above. - The `system` logger should log messages of level `ERROR` and above. 3. Handlers: - The `application` logger should log messages to a file named `application.log`. - The `system` logger should log messages to a file named `system.log`. - Additionally, all logs of level `CRITICAL` should be sent to the console. 4. Formatters: - All log messages should include the log level name, the logger name, the timestamp, and the message. - The timestamp should be formatted as `YYYY-MM-DD HH:MM:SS`. 5. Ensure that the logging configuration does not duplicate messages and handles logs efficiently. **Constraints:** - Use the classes and functions provided by the `logging` module. - Do not use external logging libraries. **Function Signature:** ```python def configure_logging(): pass ``` **Specifications and Example:** Implement the function `configure_logging()` that sets up the described logging configuration. After setting up the logging configuration, demonstrate logging by performing the following steps: 1. Log an `INFO` message using the `application` logger. 2. Log an `ERROR` message using the `system` logger. 3. Log a `CRITICAL` message using the `system` logger. **Expected File Output:** - `application.log` should contain the `INFO` message. - `system.log` should contain both `ERROR` and `CRITICAL` messages. - The console should display the `CRITICAL` message. **Example Usage:** ```python configure_logging() app_logger = logging.getLogger(\'application\') sys_logger = logging.getLogger(\'system\') app_logger.info(\'User login successful.\') sys_logger.error(\'File not found.\') sys_logger.critical(\'System crash!\') ``` **Example Output:** - `application.log` should contain: ``` INFO:application:2023-10-01 12:00:00 User login successful. ``` - `system.log` should contain: ``` ERROR:system:2023-10-01 12:01:00 File not found. CRITICAL:system:2023-10-01 12:02:00 System crash! ``` - Console output: ``` CRITICAL:system:2023-10-01 12:02:00 System crash! ``` Your task is to implement the `configure_logging()` function to meet the above specifications.","solution":"import logging def configure_logging(): formatter = logging.Formatter(\'%(levelname)s:%(name)s:%(asctime)s %(message)s\', datefmt=\'%Y-%m-%d %H:%M:%S\') # Setup application logger app_handler = logging.FileHandler(\'application.log\') app_handler.setLevel(logging.INFO) app_handler.setFormatter(formatter) app_logger = logging.getLogger(\'application\') app_logger.setLevel(logging.INFO) app_logger.addHandler(app_handler) # Setup system logger sys_file_handler = logging.FileHandler(\'system.log\') sys_file_handler.setLevel(logging.ERROR) sys_file_handler.setFormatter(formatter) sys_console_handler = logging.StreamHandler() sys_console_handler.setLevel(logging.CRITICAL) sys_console_handler.setFormatter(formatter) sys_logger = logging.getLogger(\'system\') sys_logger.setLevel(logging.ERROR) sys_logger.addHandler(sys_file_handler) sys_logger.addHandler(sys_console_handler) # Avoid message duplication app_logger.propagate = False sys_logger.propagate = False"},{"question":"# Question: Advanced Data Analysis with Pandas You are working with a dataset that contains information about various products sold by a company. The dataset is represented as a CSV file with the following columns: 1. `ProductID`: Unique identifier for each product. 2. `ProductName`: Name of the product. 3. `Category`: Category to which the product belongs (e.g., \\"Electronics\\", \\"Furniture\\"). 4. `Price`: Price of the product. 5. `QuantitySold`: Number of units sold. 6. `SaleDate`: Date when the product was sold (in YYYY-MM-DD format). You are required to perform a series of data analysis tasks using pandas. # Input Format - A CSV file named `products.csv` containing the above columns. # Output Format Your solution should be a Python script that performs the following tasks and prints the results in the specified format: 1. **Load the data** from the CSV file into a pandas DataFrame. 2. **Basic Data Inspection**: - Print the first 5 rows of the DataFrame. - Print the summary statistics of the DataFrame. 3. **Data Cleaning**: - Identify and handle any missing values in the `Price` and `QuantitySold` columns. Fill missing values in these columns using the median of the respective columns. 4. **Data Transformation**: - Add a new column `TotalRevenue` which is the product of `Price` and `QuantitySold`. 5. **Data Analysis**: - Calculate and print the total revenue generated by each category. - Identify and print the top 3 products with the highest total revenue. - Calculate and print the average price of products sold in each category. 6. **Time Series Analysis**: - Convert the `SaleDate` column to datetime format. - Group the data by month and calculate the total revenue generated each month. Print the results. # Constraints - Your code should be well-organized and readable, making use of pandas functions efficiently. - Handle potential edge cases, such as empty columns or missing values, gracefully. # Example Assume the `products.csv` contains the following data: | ProductID | ProductName | Category | Price | QuantitySold | SaleDate | |-----------|-------------|-------------|-------|--------------|------------| | 1 | Laptop | Electronics | 1200 | 5 | 2023-01-15 | | 2 | Chair | Furniture | 90 | 10 | 2023-02-10 | | 3 | Headphones | Electronics | 200 | 8 | 2023-03-05 | | ... | ... | ... | ... | ... | ... | ``` # Your solution should follow this structure: import pandas as pd # Task 1: Load the data df = pd.read_csv(\'products.csv\') # Task 2: Basic Data Inspection print(\\"First 5 rows of the DataFrame:\\") print(df.head()) print(\\"nSummary statistics of the DataFrame:\\") print(df.describe()) # Task 3: Data Cleaning df[\'Price\'].fillna(df[\'Price\'].median(), inplace=True) df[\'QuantitySold\'].fillna(df[\'QuantitySold\'].median(), inplace=True) # Task 4: Data Transformation df[\'TotalRevenue\'] = df[\'Price\'] * df[\'QuantitySold\'] # Task 5: Data Analysis print(\\"nTotal revenue by category:\\") print(df.groupby(\'Category\')[\'TotalRevenue\'].sum()) top_products = df.nlargest(3, \'TotalRevenue\') print(\\"nTop 3 products with highest total revenue:\\") print(top_products[[\'ProductID\', \'ProductName\', \'TotalRevenue\']]) print(\\"nAverage price of products by category:\\") print(df.groupby(\'Category\')[\'Price\'].mean()) # Task 6: Time Series Analysis df[\'SaleDate\'] = pd.to_datetime(df[\'SaleDate\']) monthly_revenue = df.set_index(\'SaleDate\').resample(\'M\')[\'TotalRevenue\'].sum() print(\\"nTotal revenue by month:\\") print(monthly_revenue) ```","solution":"import pandas as pd def load_data(file_path): Load data from a CSV file into a pandas DataFrame. return pd.read_csv(file_path) def basic_data_inspection(df): Print first five rows and summary statistics of DataFrame. print(\\"First 5 rows of the DataFrame:\\") print(df.head()) print(\\"nSummary statistics of the DataFrame:\\") print(df.describe()) def data_cleaning(df): Handle missing values in \'Price\' and \'QuantitySold\' columns. df[\'Price\'].fillna(df[\'Price\'].median(), inplace=True) df[\'QuantitySold\'].fillna(df[\'QuantitySold\'].median(), inplace=True) def data_transformation(df): Add \'TotalRevenue\' column to DataFrame. df[\'TotalRevenue\'] = df[\'Price\'] * df[\'QuantitySold\'] def data_analysis(df): Perform analytical operations on the DataFrame. total_revenue_by_category = df.groupby(\'Category\')[\'TotalRevenue\'].sum() print(\\"nTotal revenue by category:\\") print(total_revenue_by_category) top_products = df.nlargest(3, \'TotalRevenue\') print(\\"nTop 3 products with highest total revenue:\\") print(top_products[[\'ProductID\', \'ProductName\', \'TotalRevenue\']]) avg_price_by_category = df.groupby(\'Category\')[\'Price\'].mean() print(\\"nAverage price of products by category:\\") print(avg_price_by_category) def time_series_analysis(df): Perform time series analysis on the \'SaleDate\' column. df[\'SaleDate\'] = pd.to_datetime(df[\'SaleDate\']) monthly_revenue = df.set_index(\'SaleDate\').resample(\'M\')[\'TotalRevenue\'].sum() print(\\"nTotal revenue by month:\\") print(monthly_revenue) def main(): file_path = \'products.csv\' df = load_data(file_path) basic_data_inspection(df) data_cleaning(df) data_transformation(df) data_analysis(df) time_series_analysis(df) if __name__ == \\"__main__\\": main()"},{"question":"# **Coding Challenge: Implementing a Custom Chat Server using asyncio\'s Event Loop** **Objective:** You are tasked with creating a simple but robust asynchronous chat server using Python\'s `asyncio` package. This server will handle multiple clients, manage asynchronous communication, and allow message broadcasting to all connected clients. **Requirements:** 1. **Server Setup:** - Implement the `ChatServer` class which will manage client connections, message broadcasting, and handle server operations. - The server should listen on a specified `host` and `port`. 2. **Client Handler:** - Each client connection should be managed by an instance of `ClientHandler` class. - The handler should read messages asynchronously and broadcast them to all connected clients. 3. **Broadcasting Messages:** - Implement a method to broadcast messages to all currently connected clients. 4. **Event Loop:** - Use the `asyncio` event loop to manage asynchronous tasks, schedule callbacks, handle network connections, and enable message broadcasting. 5. **Task Execution & Error Handling:** - Ensure that the server can gracefully handle client disconnections, errors, and shut down cleanly using proper event loop APIs. **Constraints:** - Use the low-level event loop functions provided in the `asyncio` package. - Minimize blocking operations to maintain server responsiveness. - Handle edge cases such as client disconnections and message delivery errors. **Performance Requirements:** - Efficiently manage multiple client connections. - Ensure low-latency message broadcasting. # **Input & Output:** **Input:** - None explicitly; the server will handle incoming client connections and messages asynchronously. **Output:** - The server logs activities such as connections, message deliveries, and disconnections to the console. # **Starter Code:** ```python import asyncio class ChatServer: def __init__(self, host, port): self.host = host self.port = port self.clients = set() async def handle_client(self, reader, writer): client = ClientHandler(reader, writer, self) self.clients.add(client) await client.run() async def broadcast(self, message, sender): for client in self.clients: if client != sender: await client.send_message(message) async def start_server(self): loop = asyncio.get_running_loop() server = await loop.create_server( lambda: self.callback_factory(loop.create_task), self.host, self.port) async with server: await server.serve_forever() class ClientHandler: def __init__(self, reader, writer, server): self.reader = reader self.writer = writer self.server = server async def run(self): while True: data = await self.reader.read(100) if not data: break message = data.decode() print(f\\"Received message: {message}\\") await self.server.broadcast(message, self) async def send_message(self, message): self.writer.write(message.encode()) await self.writer.drain() if __name__ == \'__main__\': server = ChatServer(\'127.0.0.1\', 8888) try: asyncio.run(server.start_server()) except KeyboardInterrupt: print(\\"Server stopped by user\\") ``` **Explanation of the Code:** - The `ChatServer` class initializes the server to listen on a specified host and port. - The `handle_client` coroutine manages client connections by reading and broadcasting messages. - The `broadcast` coroutine sends messages to all clients except the sender. - The `ClientHandler` class represents a single client connection and manages reading from and writing to the client asynchronously. - Use `asyncio.run()` to start the server and handle shutdown gracefully. **Additional Tasks:** 1. Modify the `try...except` block to ensure that all active connections are closed properly on server shutdown. 2. Enhance error handling within client communication to ensure that disconnections and message delivery failures are logged and managed gracefully.","solution":"import asyncio class ChatServer: def __init__(self, host, port): self.host = host self.port = port self.clients = set() async def handle_client(self, reader, writer): client = ClientHandler(reader, writer, self) self.clients.add(client) try: await client.run() except Exception as e: print(f\\"Error handling client {client}: {e}\\") finally: self.clients.remove(client) writer.close() await writer.wait_closed() async def broadcast(self, message, sender): for client in self.clients: if client != sender: await client.send_message(message) async def start_server(self): server = await asyncio.start_server(self.handle_client, self.host, self.port) async with server: await server.serve_forever() class ClientHandler: def __init__(self, reader, writer, server): self.reader = reader self.writer = writer self.server = server async def run(self): while True: data = await self.reader.read(100) if not data: break message = data.decode() print(f\\"Received message: {message}\\") await self.server.broadcast(message, self) async def send_message(self, message): self.writer.write(message.encode()) await self.writer.drain() if __name__ == \'__main__\': server = ChatServer(\'127.0.0.1\', 8888) try: asyncio.run(server.start_server()) except KeyboardInterrupt: print(\\"Server stopped by user\\")"},{"question":"# Question **Your Task:** You are required to implement a function that takes multiple dictionaries representing different scopes and merges them into a `ChainMap`. Additionally, you should provide functionalities to count hashable elements across all dictionaries using `Counter` and to manage missing values gracefully using `defaultdict`. Implement the following function: ```python from collections import ChainMap, Counter, defaultdict def manage_data(scopes, elements, key_factory=None): Manages multiple scopes of dictionaries and provides element counting and missing value handling. Parameters: scopes (list of dict): A list of dictionaries representing different scopes. elements (iterable): An iterable of hashable elements to be counted. key_factory (callable, optional): A factory function to supply default values for missing keys in the ChainMap. Returns: tuple: - A ChainMap instance with the given dictionaries. - A Counter instance with the counts of the given elements. - defaultdict with the provided key_factory handling missing keys. # Your code here ``` # Expected Functionality: 1. **ChainMap Creation:** The function should create a `ChainMap` instance from the provided list of dictionaries (scopes). 2. **Element Counting:** The function should return a `Counter` counting the occurrences of elements present in the provided iterable (elements). 3. **Defaultdict Handling:** If `key_factory` is provided, the function should return a `defaultdict` where missing keys return values from the factory function. # Constraints: - The input `scopes` is guaranteed to be a non-empty list of dictionaries. - The input `elements` is an iterable containing hashable elements. - The `key_factory` function will be a callable that takes no arguments and returns a default value. # Example: ```python scopes = [{\'a\': 1, \'b\': 2}, {\'b\': 3, \'c\': 4}] elements = [\'a\', \'b\', \'a\', \'c\', \'c\', \'c\', \'d\'] key_factory = lambda: \'missing\' chain_map, element_counts, default_dict = manage_data(scopes, elements, key_factory) # The ChainMap should reflect the combined view of the dictionaries in scopes. assert chain_map[\'a\'] == 1 assert chain_map[\'b\'] == 2 assert chain_map[\'c\'] == 4 # The Counter should count elements correctly. assert element_counts == Counter({\'c\': 3, \'a\': 2, \'b\': 1, \'d\': 1}) # The defaultdict should use the factory for missing keys. assert default_dict[\'x\'] == \'missing\' ``` **Notes:** - The `ChainMap` should be created such that the first dictionary in the list of scopes has the highest precedence. - The returned `Counter` should tally all the elements in the provided iterable. - If `key_factory` is not provided, the function should return a normal dictionary instead of a `defaultdict`. Implement the function efficiently, ensuring that it correctly integrates the functionalities of `ChainMap`, `Counter`, and `defaultdict`.","solution":"from collections import ChainMap, Counter, defaultdict def manage_data(scopes, elements, key_factory=None): Manages multiple scopes of dictionaries and provides element counting and missing value handling. Parameters: scopes (list of dict): A list of dictionaries representing different scopes. elements (iterable): An iterable of hashable elements to be counted. key_factory (callable, optional): A factory function to supply default values for missing keys in the ChainMap. Returns: tuple: - A ChainMap instance with the given dictionaries. - A Counter instance with the counts of the given elements. - defaultdict with the provided key_factory handling missing keys. # Step 1: Create a ChainMap instance from the provided list of dictionaries (scopes). chain_map = ChainMap(*scopes) # Step 2: Create a Counter instance from the provided iterable (elements). element_counts = Counter(elements) # Step 3: Create a defaultdict with the provided key_factory, or a normal dict if key_factory is None. if key_factory is not None: default_dict = defaultdict(key_factory) else: default_dict = {} return chain_map, element_counts, default_dict"},{"question":"Objective Your task is to implement a function that formats and logs processed data. You will need to utilize several modules from Python\'s Standard Library covered in the documentation. Background You are given a series of names and ages, and your task is to: 1. Store these data in an array. 2. Log the formatted data using specific logging levels. Requirements 1. **Input and Output Formats:** - Input: A list of tuples, each containing a name (string) and an age (integer). ``` [(\\"Alice\\", 30), (\\"Bob\\", 24), (\\"Charlie\\", 29)] ``` - Output: Log entries with the formatted string \\"<name>: <age> years old\\". 2. **Function Signature:** ```python def process_and_log_data(data: list[tuple[str, int]]) -> None: ``` 3. **Constraints:** - All names are guaranteed to be unique. - Ages are non-negative integers. - You must use the `array` module to store ages and the `logging` module for logging. 4. **Steps:** - Use the `array` module to store the ages. - Log each entry in the format `<name>: <age> years old` at the `INFO` level using the `logging` module. - Assume the logging configuration is already set to display `INFO` level logs. Example ```python def process_and_log_data(data: list[tuple[str, int]]) -> None: from array import array import logging # Configure logging (for the sake of completeness in this example) logging.basicConfig(level=logging.INFO, format=\'%(levelname)s: %(message)s\') # Storing the ages using the array module ages = array(\'I\', [age for name, age in data]) # Logging each formatted entry for i, (name, age) in enumerate(data): logging.info(\\"%s: %d years old\\", name, ages[i]) # Example usage process_and_log_data([(\\"Alice\\", 30), (\\"Bob\\", 24), (\\"Charlie\\", 29)]) ``` Ensure that your solution meets these requirements and correctly utilizes the specified modules.","solution":"def process_and_log_data(data: list[tuple[str, int]]) -> None: from array import array import logging # Configure logging (for the sake of completeness in this example) logging.basicConfig(level=logging.INFO, format=\'%(levelname)s: %(message)s\') # Storing the ages using the array module ages = array(\'I\', [age for name, age in data]) # Logging each formatted entry for i, (name, age) in enumerate(data): logging.info(\\"%s: %d years old\\", name, ages[i]) # Example usage process_and_log_data([(\\"Alice\\", 30), (\\"Bob\\", 24), (\\"Charlie\\", 29)])"},{"question":"Objective: Create an enumerated type to represent a set of tasks with different levels of priorities and access methods to process these tasks. Question: You are required to implement an enumeration to represent tasks with different priorities and another enumeration that can represent combinations of some task attributes (using bitwise operations). The task also requires a class that uses these enumerations to add tasks, list them by priority, and retrieve tasks based on certain criteria. Details: 1. **Task Enum**: - Create an `Enum` called `Task` with the following members: - `HIGH` with value 1 - `MEDIUM` with value 2 - `LOW` with value 3 - Ensure that each task priority value is unique using a suitable method. 2. **TaskAttributes Enum**: - Create an `IntFlag` called `TaskAttributes` to represent task attributes using bitwise operators: - `URGENT` with value 1 - `IMPORTANT` with value 2 - `OPTIONAL` with value 4 - Enable the combination of these attributes using bitwise OR (|). 3. **TaskManager class**: - Implement a class `TaskManager` that will manage tasks: - It should have a method `add_task(task_name: str, priority: Task, attributes: TaskAttributes) -> None` to add tasks. - It should have a method `get_tasks_by_priority(priority: Task) -> List[str]` to return a list of task names filtered by the given priority. - It should have a method `get_tasks_with_attribute(attribute: TaskAttributes) -> List[str]` to return a list of task names filtered by given attributes combination. Input and Output Format: - The `add_task` method takes `task_name` (a string), `priority` (an instance of `Task` enum), and `attributes` (an instance of `TaskAttributes` enum) and does not return anything. - The `get_tasks_by_priority` method takes `priority` and returns a list of task names with that priority. - The `get_tasks_with_attribute` method takes an `attribute` (a combination of `TaskAttributes`) and returns a list of tasks having that attribute combination. Constraints: - A task name is a non-empty string consisting of alphanumeric characters. - Enums should guarantee uniqueness in values. Implementation Requirements: - Implement the `Task` and `TaskAttributes` enums with specified members. - Implement the `TaskManager` class with methods as described. Example: ```python from enum import Enum, IntFlag # Step 1: Define the Task Enum @unique class Task(Enum): HIGH = 1 MEDIUM = 2 LOW = 3 # Step 2: Define the TaskAttributes IntFlag class TaskAttributes(IntFlag): URGENT = 1 IMPORTANT = 2 OPTIONAL = 4 # Step 3: Define the TaskManager class class TaskManager: def __init__(self): self.tasks = [] def add_task(self, task_name: str, priority: Task, attributes: TaskAttributes) -> None: self.tasks.append((task_name, priority, attributes)) def get_tasks_by_priority(self, priority: Task) -> List[str]: return [task[0] for task in self.tasks if task[1] == priority] def get_tasks_with_attribute(self, attribute: TaskAttributes) -> List[str]: return [task[0] for task in self.tasks if task[2] & attribute] # Demonstrate usage task_manager = TaskManager() task_manager.add_task(\'Task1\', Task.HIGH, TaskAttributes.URGENT | TaskAttributes.IMPORTANT) task_manager.add_task(\'Task2\', Task.MEDIUM, TaskAttributes.OPTIONAL) print(task_manager.get_tasks_by_priority(Task.HIGH)) # Output: [\'Task1\'] print(task_manager.get_tasks_with_attribute(TaskAttributes.URGENT)) # Output: [\'Task1\'] ```","solution":"from enum import Enum, IntFlag, unique # Step 1: Define the Task Enum @unique class Task(Enum): HIGH = 1 MEDIUM = 2 LOW = 3 # Step 2: Define the TaskAttributes IntFlag class TaskAttributes(IntFlag): URGENT = 1 IMPORTANT = 2 OPTIONAL = 4 # Step 3: Define the TaskManager class class TaskManager: def __init__(self): self.tasks = [] def add_task(self, task_name: str, priority: Task, attributes: TaskAttributes) -> None: self.tasks.append((task_name, priority, attributes)) def get_tasks_by_priority(self, priority: Task) -> list: return [task[0] for task in self.tasks if task[1] == priority] def get_tasks_with_attribute(self, attribute: TaskAttributes) -> list: return [task[0] for task in self.tasks if task[2] & attribute]"},{"question":"# Custom Protocol and Transport Implementation **Objective:** Implement a custom protocol that communicates with both a TCP server and a UDP server concurrently using `asyncio` transport and protocol classes. Your task is to create a client that sends messages to both servers, receives their responses, and handles reconnecting if the connection is lost. **Instructions:** 1. Implement a class `CustomProtocol` that inherits from `asyncio.Protocol` and handles the connection logic for both TCP and UDP protocols. 2. Implement a class `CustomClient` that: - Connects to a TCP server at `127.0.0.1:8888` and UDP server at `127.0.0.1:9999`. - Sends a message to both servers every 5 seconds. - Receives and prints responses from both servers. - Reconnects automatically if the connection to the TCP server is lost. 3. Ensure your implementation handles at least the following: - Sending and receiving messages over TCP and UDP. - Handling connection loss and auto-reconnecting for the TCP connection. **Expected Input and Output:** - Your client should connect to the TCP server and UDP server and start sending messages immediately. - The servers should be running beforehand and designed to echo received messages. - Example log output: ``` Connected to TCP server at 127.0.0.1:8888 Connected to UDP server at 127.0.0.1:9999 Message sent to TCP: \\"Hello TCP\\" Message sent to UDP: \\"Hello UDP\\" Received from TCP: \\"Hello TCP\\" Received from UDP: \\"Hello UDP\\" TCP connection lost. Reconnecting... Connected to TCP server at 127.0.0.1:8888 Message sent to TCP: \\"Hello TCP\\" Message sent to UDP: \\"Hello UDP\\" ... ``` **Constraints:** - Your implementation must use asyncio\'s low-level APIs (transports and protocols). - Do not use high-level APIs like `asyncio.open_connection()` or `asyncio.start_server()`. **Performance Requirements:** - The client should handle reconnections gracefully without unnecessary delays. - The message sending interval should be consistent (every 5 seconds). **Hints:** - Use `loop.create_connection()` for TCP connections and `loop.create_datagram_endpoint()` for UDP connections. - Store the transports and protocols in your `CustomClient` class for managing them efficiently. - Use `transport.close()`, `transport.write()`, and `protocol.connection_lost()` methods for handling connection and data flow. **Starter Code:** ```python import asyncio import time class CustomProtocol(asyncio.Protocol): def connection_made(self, transport): self.transport = transport peername = transport.get_extra_info(\'peername\') print(\'Connection to {} established\'.format(peername)) def data_received(self, data): message = data.decode() print(\'Data received: {!r}\'.format(message)) def connection_lost(self, exc): print(\'The server closed the connection\') # Implement reconnection logic if needed class CustomClient: def __init__(self): self.loop = asyncio.get_event_loop() async def connect(self): self.tcp_transport, self.tcp_protocol = await self.loop.create_connection( lambda: CustomProtocol(), \'127.0.0.1\', 8888 ) self.udp_transport, self.udp_protocol = await self.loop.create_datagram_endpoint( lambda: CustomProtocol(), local_addr=(\'127.0.0.1\', 9999) ) async def send_messages(self): while True: self.tcp_transport.write(b\'Hello TCP\') self.udp_transport.sendto(b\'Hello UDP\', (\'127.0.0.1\', 9999)) await asyncio.sleep(5) async def run(self): await self.connect() await self.send_messages() if __name__ == \'__main__\': client = CustomClient() loop = asyncio.get_event_loop() try: loop.run_until_complete(client.run()) finally: loop.close() ```","solution":"import asyncio class CustomProtocol(asyncio.Protocol): def __init__(self, client, protocol_type): self.client = client self.protocol_type = protocol_type def connection_made(self, transport): self.transport = transport peername = transport.get_extra_info(\'peername\') print(f\'Connected to {self.protocol_type} server at {peername}\') def data_received(self, data): message = data.decode() print(f\'Received from {self.protocol_type}: {message}\') def connection_lost(self, exc): print(f\'The {self.protocol_type} server closed the connection\') if self.protocol_type == \\"TCP\\": self.client.tcp_connection_lost(exc) class UDPProtocol: def __init__(self): self.transport = None def connection_made(self, transport): self.transport = transport print(\'Connected to UDP server\') def datagram_received(self, data, addr): message = data.decode() print(f\'Received from UDP: {message}\') def connection_lost(self, exc): print(\'The UDP server closed the connection\') class CustomClient: def __init__(self): self.loop = asyncio.get_event_loop() self.tcp_transport = None self.tcp_protocol = None self.udp_transport = None self.udp_protocol = None async def connect_tcp(self): while True: try: self.tcp_transport, self.tcp_protocol = await self.loop.create_connection( lambda: CustomProtocol(self, \\"TCP\\"), \'127.0.0.1\', 8888 ) break except Exception as e: print(f\\"Failed to connect to TCP server: {e}. Retrying in 5 seconds...\\") await asyncio.sleep(5) async def connect_udp(self): self.udp_transport, self.udp_protocol = await self.loop.create_datagram_endpoint( lambda: UDPProtocol(), remote_addr=(\'127.0.0.1\', 9999) ) async def reconnect_tcp(self): await self.connect_tcp() asyncio.ensure_future(self.send_tcp_messages()) def tcp_connection_lost(self, exc): if self.tcp_transport: self.tcp_transport.close() print(\\"TCP connection lost. Reconnecting...\\") asyncio.ensure_future(self.reconnect_tcp()) async def send_tcp_messages(self): while True: if self.tcp_transport: self.tcp_transport.write(b\'Hello TCP\') print(\'Message sent to TCP: \\"Hello TCP\\"\') await asyncio.sleep(5) async def send_udp_messages(self): while True: self.udp_transport.sendto(b\'Hello UDP\', (\'127.0.0.1\', 9999)) print(\'Message sent to UDP: \\"Hello UDP\\"\') await asyncio.sleep(5) async def run(self): await self.connect_tcp() await self.connect_udp() asyncio.ensure_future(self.send_tcp_messages()) await self.send_udp_messages() if __name__ == \'__main__\': client = CustomClient() loop = asyncio.get_event_loop() try: loop.run_until_complete(client.run()) finally: loop.close()"},{"question":"# Advanced Pandas Assessment You are provided with a CSV file named `sales_data.csv` which contains the following columns: - `Date`: The date of the sale. - `Store`: The store where the sale took place. - `Item`: The item that was sold. - `Revenue`: The revenue generated from the sale. - `Quantity`: The quantity of the item sold. The task is to perform some operations using the pandas library to extract meaningful insights from the data. Implement the following functions: 1. **load_data(file_path: str) -> pd.DataFrame**: - Reads the CSV file located at `file_path` into a DataFrame. - Ensures that the `Date` column is parsed as a datetime object. 2. **add_month_column(df: pd.DataFrame) -> pd.DataFrame**: - Adds a new column `Month` to the DataFrame extracted from the `Date` column reflecting the month of each sale. 3. **monthly_revenue(df: pd.DataFrame) -> pd.Series**: - Returns a Series where the index is the `Month` and the values are the total revenue for that month. 4. **store_revenue(df: pd.DataFrame, store: str) -> pd.DataFrame**: - Returns a DataFrame with `Date` as the index and columns `Item` and `Revenue` showing the total revenue for each item on each date for the specified store. 5. **highest_selling_item(df: pd.DataFrame) -> str**: - Returns the name of the item that generated the highest revenue across all stores. 6. **missing_data_report(df: pd.DataFrame) -> pd.Series**: - Returns a Series with the column names as the index and the count of missing values in each column. Constraints - You may assume the DataFrame will not exceed 100,000 rows. - The CSV file will always have the specified columns, but some columns may have missing values. ```python import pandas as pd def load_data(file_path: str) -> pd.DataFrame: # Implement this function pass def add_month_column(df: pd.DataFrame) -> pd.DataFrame: # Implement this function pass def monthly_revenue(df: pd.DataFrame) -> pd.Series: # Implement this function pass def store_revenue(df: pd.DataFrame, store: str) -> pd.DataFrame: # Implement this function pass def highest_selling_item(df: pd.DataFrame) -> str: # Implement this function pass def missing_data_report(df: pd.DataFrame) -> pd.Series: # Implement this function pass ``` Example Usage ```python df = load_data(\'sales_data.csv\') df = add_month_column(df) print(monthly_revenue(df)) print(store_revenue(df, \'Store_A\')) print(highest_selling_item(df)) print(missing_data_report(df)) ``` Your implementation should correctly handle the data and provide accurate results based on the sales data provided.","solution":"import pandas as pd def load_data(file_path: str) -> pd.DataFrame: Reads the CSV file located at `file_path` into a DataFrame. Ensures that the `Date` column is parsed as a datetime object. df = pd.read_csv(file_path, parse_dates=[\'Date\']) return df def add_month_column(df: pd.DataFrame) -> pd.DataFrame: Adds a new column `Month` to the DataFrame extracted from the `Date` column reflecting the month of each sale. df[\'Month\'] = df[\'Date\'].dt.to_period(\'M\') return df def monthly_revenue(df: pd.DataFrame) -> pd.Series: Returns a Series where the index is the `Month` and the values are the total revenue for that month. return df.groupby(\'Month\')[\'Revenue\'].sum() def store_revenue(df: pd.DataFrame, store: str) -> pd.DataFrame: Returns a DataFrame with `Date` as the index and columns `Item` and `Revenue` showing the total revenue for each item on each date for the specified store. store_df = df[df[\'Store\'] == store] return store_df.pivot_table(index=\'Date\', columns=\'Item\', values=\'Revenue\', aggfunc=\'sum\') def highest_selling_item(df: pd.DataFrame) -> str: Returns the name of the item that generated the highest revenue across all stores. return df.groupby(\'Item\')[\'Revenue\'].sum().idxmax() def missing_data_report(df: pd.DataFrame) -> pd.Series: Returns a Series with the column names as the index and the count of missing values in each column. return df.isnull().sum()"},{"question":"**Objective**: Implement a function to dynamically import a module using Python\'s recommended modern approach with `importlib`. Question You are given a deprecated function that dynamically imports a module using the `imp` module. Your task is to refactor this function to use the `importlib` module, which is the recommended approach in modern Python. The function to be refactored is: ```python import imp import sys def dynamic_import(module_name): try: return sys.modules[module_name] except KeyError: pass fp, pathname, description = imp.find_module(module_name) try: return imp.load_module(module_name, fp, pathname, description) finally: if fp: fp.close() ``` Requirements 1. The function should successfully import a module by its name dynamically. 2. Make use of `importlib.util` and `importlib` packages. 3. Do not use the deprecated `imp` module or any of its functions. Constraints - The module name provided will always be a non-hierarchical name (i.e., it won\'t contain any dots). - Assume that the module files (if any) are present in the standard library locations or installed packages. Input - A single string denoting the name of the module to be imported dynamically. Output - The module object for the given module name. Example ```python # Example usage: mod = dynamic_import(\'math\') print(mod.sqrt(4)) # Should output: 2.0 ``` **Note:** Ensure that your implementation closes any open files, handles exceptions properly, and follows best practices for dynamic module loading.","solution":"import importlib import sys def dynamic_import(module_name): Dynamically imports a module using its name. Parameters: - module_name: str, name of the module to import Returns: - module: the module object if successfully imported if module_name in sys.modules: return sys.modules[module_name] try: module = importlib.import_module(module_name) return module except ImportError as e: raise e"},{"question":"# Advanced Mocking and Patching with `unittest.mock` Problem Statement: You are tasked with creating a small module and writing tests for it using the `unittest.mock` library. The module, named `network_operations`, contains functions that interact with a hypothetical network service. The goal is to mock these interactions for unit testing purposes. Specifically, you need to implement the following: 1. **Module: `network_operations.py`** ```python class NetworkService: def fetch_data(self, url): Makes a network call to the given URL and returns the response. pass def send_data(self, url, data): Sends data to the given URL and returns the status. pass def process_data(service, url, data): Fetches data from the URL, processes it, sends the processed data back. response = service.fetch_data(url) processed_data = f\\"Processed {response}\\" status = service.send_data(url, processed_data) return status ``` 2. **Test Suite: `test_network_operations.py`** Implement unit tests for the `process_data` function using the `unittest.mock` library. Your tests should cover the following scenarios: - Mock the `fetch_data` and `send_data` methods of `NetworkService`. - Verify that `fetch_data` is called with the correct URL. - Verify that `send_data` is called with the correct processed data. - Simulate a scenario where `fetch_data` raises an exception, and ensure `send_data` is never called in this case. - Simulate a scenario where `send_data` returns an error status. Constraints: - Use the `patch` decorator or context manager appropriately. - Ensure that the mock objects are configured accurately, including return values and side effects. - Use assertions to check the calls and parameters of the mock methods. Example Input/Output: ```python # Assuming the following URL and data are used for testing: url = \\"http://example.com\\" data = \\"data to be processed\\" # Mocking fetch_data to return sample data service.fetch_data.return_value = \\"sample data\\" # Expected processed data: \\"Processed sample data\\" expected_processed_data = \\"Processed sample data\\" # Mocking send_data to return success status service.send_data.return_value = \\"success\\" # Expected call sequence service.fetch_data.assert_called_once_with(url) service.send_data.assert_called_once_with(url, expected_processed_data) ``` *Write unit tests in `test_network_operations.py` to validate these scenarios.*","solution":"class NetworkService: def fetch_data(self, url): Makes a network call to the given URL and returns the response. pass def send_data(self, url, data): Sends data to the given URL and returns the status. pass def process_data(service, url, data): Fetches data from the URL, processes it, sends the processed data back. response = service.fetch_data(url) processed_data = f\\"Processed {response}\\" status = service.send_data(url, processed_data) return status"},{"question":"Coding Assessment Question # Overview Design and implement a Python function that generates a list of random points (tuples) within a 2D plane. Each point\'s coordinates should follow a specified random distribution. You are required to use specific functions from the `random` module to achieve this. # Description Write a function `generate_random_points(n, dist_x, params_x, dist_y, params_y)` that generates `n` random points `(x, y)` in a 2D plane. The distribution of the `x` and `y` coordinates for each point should follow the specified distributions and their respective parameters. # Function Signature ```python def generate_random_points(n: int, dist_x: str, params_x: tuple, dist_y: str, params_y: tuple) -> list: pass ``` # Parameters - `n` (int): The number of random points to generate. - `dist_x` (str): The distribution to use for the `x` coordinate. This will be a string representing the name of a function in the `random` module (e.g., `\\"uniform\\"`, `\\"normalvariate\\"`). - `params_x` (tuple): The parameters for the distribution function for the `x` coordinate. - `dist_y` (str): The distribution to use for the `y` coordinate. This will be a string representing the name of a function in the `random` module (e.g., `\\"uniform\\"`, `\\"normalvariate\\"`). - `params_y` (tuple): The parameters for the distribution function for the `y` coordinate. # Returns - `list`: A list of `n` tuples, where each tuple represents a randomly generated point `(x, y)`. # Constraints - The `dist_x` and `dist_y` strings must correspond to valid distribution functions provided by the `random` module. - The `params_x` and `params_y` tuples must match the required parameters for the specified distribution functions. - Ensure that the function handles invalid distribution names or incorrect parameters gracefully by raising an appropriate error. # Examples Example 1: ```python # Generate 5 random points where the x-coordinates follow a uniform distribution # between 0 and 10, and the y-coordinates follow a normal distribution with mean 0 and std 1 n = 5 dist_x = \\"uniform\\" params_x = (0, 10) dist_y = \\"normalvariate\\" params_y = (0, 1) generate_random_points(n, dist_x, params_x, dist_y, params_y) # Expected output: List of 5 tuples [(x1, y1), (x2, y2), ...], specific values will vary ``` Example 2: ```python # Generate 3 random points where the x-coordinates follow an exponential distribution # with lambda 1, and the y-coordinates follow a uniform distribution between -1 and 1 n = 3 dist_x = \\"expovariate\\" params_x = (1,) dist_y = \\"uniform\\" params_y = (-1, 1) generate_random_points(n, dist_x, params_x, dist_y, params_y) # Expected output: List of 3 tuples [(x1, y1), (x2, y2), ...], specific values will vary ``` # Notes - Use the `getattr()` function to get a reference to the required distribution function from the `random` module. - Handle cases where the provided distribution name does not exist in the `random` module. - Carefully document your code explaining the choices made and the error handling strategy. > This question tests the student\'s ability to understand and use different random distributions, handle dynamic function calling using `getattr()`, and manage errors in parameter handling.","solution":"import random def generate_random_points(n, dist_x, params_x, dist_y, params_y): Generates `n` random points in a 2D plane where the `x` and `y` coordinates follow specified distributions. Parameters: - n (int): Number of random points to generate. - dist_x (str): Distribution to use for the `x` coordinate. - params_x (tuple): Parameters for the distribution function for the `x` coordinate. - dist_y (str): Distribution to use for the `y` coordinate. - params_y (tuple): Parameters for the distribution function for the `y` coordinate. Returns: - list: A list of `n` tuples, each tuple representing a randomly generated point `(x, y)`. try: # Get distribution functions from random module dist_x_func = getattr(random, dist_x) dist_y_func = getattr(random, dist_y) except AttributeError: raise ValueError(f\\"Unsupported distribution function name: {dist_x} or {dist_y}\\") points = [] for _ in range(n): try: x = dist_x_func(*params_x) y = dist_y_func(*params_y) except TypeError as e: raise ValueError(f\\"Incorrect parameters for distributions: {e}\\") points.append((x, y)) return points"},{"question":"# PyTorch Coding Assessment Question You are tasked to implement and optimize a function using `torch.compile`. The function will perform a specific mathematical transformation on an input tensor and then apply a simple feedforward neural network. Ensure that you utilize `torch.compile` to optimize the function for GPU performance. If GPU is not available, the code should fallback to CPU. **Function Requirements:** 1. **Mathematical Transformation**: Apply a combination of `torch.sin()` and `torch.exp()` pointwise operations to the input tensor. 2. **Simple Feedforward Neural Network**: - One hidden layer with 128 neurons and ReLU activation. - Output layer with one neuron for regression. 3. **Optimization**: Use `torch.compile` to optimize the entire function with appropriate backend. 4. **Device Handling**: Code should run on GPU if available, otherwise fallback to CPU. # Function Signature ```python def optimize_model(input_tensor: torch.Tensor, target: torch.Tensor) -> torch.Tensor: This function takes an input tensor, applies a mathematical transformation and passes it through a simple feedforward network, all optimized with torch.compile. Args: input_tensor (torch.Tensor): A tensor of shape (batch_size, input_features). target (torch.Tensor): A tensor of shape (batch_size, 1) representing the target values. Returns: torch.Tensor: The computed loss after forward pass. pass ``` # Constraints and Notes - The input tensor will have shape `(batch_size, input_features)` with batch_size being variable. - Ensure to handle both GPU and CPU cases. - Use mean squared error as the loss function. # Example ```python >>> input_tensor = torch.randn(32, 10) # Example input tensor >>> target = torch.randn(32, 1) # Example target tensor >>> loss = optimize_model(input_tensor, target) >>> print(loss.item()) # Expected: Some float value representing the computed loss ``` Please include necessary code to run the example provided.","solution":"import torch import torch.nn as nn import torch.optim as optim def optimize_model(input_tensor: torch.Tensor, target: torch.Tensor) -> torch.Tensor: This function takes an input tensor, applies a mathematical transformation and passes it through a simple feedforward network, all optimized with torch.compile. Args: input_tensor (torch.Tensor): A tensor of shape (batch_size, input_features). target (torch.Tensor): A tensor of shape (batch_size, 1) representing the target values. Returns: torch.Tensor: The computed loss after forward pass. device = torch.device(\\"cuda\\" if torch.cuda.is_available() else \\"cpu\\") # Move tensors to the device input_tensor, target = input_tensor.to(device), target.to(device) # Define model class SimpleNN(nn.Module): def __init__(self): super(SimpleNN, self).__init__() self.hidden = nn.Linear(input_tensor.shape[1], 128) self.relu = nn.ReLU() self.output = nn.Linear(128, 1) def forward(self, x): x = torch.sin(x) + torch.exp(x) x = self.hidden(x) x = self.relu(x) x = self.output(x) return x model = SimpleNN().to(device) criterion = nn.MSELoss() def model_function(input_tensor, target): model.eval() # Set the model to evaluation mode with torch.no_grad(): # No need to track gradients for forward pass evaluation predictions = model(input_tensor) loss = criterion(predictions, target) return loss optimized_model_function = torch.compile(model_function, backend=\'aot_eager\') # Running optimized model function loss = optimized_model_function(input_tensor, target) return loss # Example Usage if __name__ == \\"__main__\\": input_tensor = torch.randn(32, 10) target = torch.randn(32, 1) loss = optimize_model(input_tensor, target) print(loss.item())"},{"question":"# Ensemble Learning with Stacking **Objective:** Implement an ensemble learning model using the stacking method to improve predictions on a given dataset. **Dataset:** You are provided a CSV file called `student_scores.csv` containing student performance data in the following format: | Feature_1 | Feature_2 | ... | Feature_n | Target | |-----------|-----------|-----|-----------|--------| | 5.1 | 3.5 | ... | 0.2 | 1 | | 4.9 | 3.0 | ... | 0.2 | 0 | | ... | ... | ... | ... | ... | **Task:** 1. Load and preprocess the data. 2. Split the dataset into training and testing sets. 3. Implement a stacking ensemble method using at least three different base models and a meta-model. 4. Train your ensemble model on the training data. 5. Evaluate the performance of your model on the test data using appropriate metrics. 6. Display the performance metrics. **Instructions:** 1. Use scikit-learn for implementing the models. 2. You can choose any appropriate base models and meta-model. 3. Ensure to split the data (e.g., 80% training, 20% testing). **Constraints:** 1. The models used should be from scikit-learn. 2. The solution should be efficient and not overly computationally intensive. 3. Use a random seed for reproducibility. **Expected Input and Output:** Input: - No direct input from the user. The program should load the `student_scores.csv` file. Output: - Print the evaluation metrics for the test data. **Example:** ```python import pandas as pd from sklearn.model_selection import train_test_split from sklearn.ensemble import StackingClassifier from sklearn.linear_model import LogisticRegression from sklearn.tree import DecisionTreeClassifier from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import accuracy_score, classification_report # Load the data data = pd.read_csv(\'student_scores.csv\') # Preprocess X = data.drop(\'Target\', axis=1) y = data[\'Target\'] # Split the data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Define base models base_models = [ (\'rf\', RandomForestClassifier(n_estimators=10, random_state=42)), (\'dt\', DecisionTreeClassifier(random_state=42)), (\'lr\', LogisticRegression(random_state=42)) ] # Define meta-model meta_model = LogisticRegression() # Create the stacking ensemble stacking_clf = StackingClassifier( estimators=base_models, final_estimator=meta_model ) # Train the model stacking_clf.fit(X_train, y_train) # Predict y_pred = stacking_clf.predict(X_test) # Evaluate accuracy = accuracy_score(y_test, y_pred) report = classification_report(y_test, y_pred) print(f\\"Accuracy: {accuracy}\\") print(f\\"Classification Report:n{report}\\") ``` **Notes:** - Ensure to handle any missing values or categorical features prior to training. - You may also consider adding hyperparameter tuning for better performance.","solution":"import pandas as pd from sklearn.model_selection import train_test_split from sklearn.ensemble import StackingClassifier from sklearn.linear_model import LogisticRegression from sklearn.tree import DecisionTreeClassifier from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import accuracy_score, classification_report from sklearn.preprocessing import StandardScaler from sklearn.pipeline import Pipeline def load_and_preprocess_data(filepath): # Load the data data = pd.read_csv(filepath) # Split features and target variable X = data.drop(\'Target\', axis=1) y = data[\'Target\'] # Return the split data return train_test_split(X, y, test_size=0.2, random_state=42) def build_stacking_ensemble(): # Define base models base_models = [ (\'rf\', RandomForestClassifier(n_estimators=10, random_state=42)), (\'dt\', DecisionTreeClassifier(random_state=42)), (\'lr\', LogisticRegression(random_state=42)) ] # Define meta-model meta_model = LogisticRegression() # Create the stacking ensemble stacking_clf = StackingClassifier( estimators=base_models, final_estimator=meta_model ) return stacking_clf def train_and_evaluate(X_train, X_test, y_train, y_test, model): # Create a pipeline to standardize the data before passing to the model pipeline = Pipeline([ (\'scaler\', StandardScaler()), (\'stacking\', model) ]) # Train the model pipeline.fit(X_train, y_train) # Predict y_pred = pipeline.predict(X_test) # Evaluate accuracy = accuracy_score(y_test, y_pred) report = classification_report(y_test, y_pred) return accuracy, report # Main function to run the steps if __name__ == \\"__main__\\": X_train, X_test, y_train, y_test = load_and_preprocess_data(\'student_scores.csv\') stacking_clf = build_stacking_ensemble() accuracy, report = train_and_evaluate(X_train, X_test, y_train, y_test, stacking_clf) print(f\\"Accuracy: {accuracy}\\") print(f\\"Classification Report:n{report}\\")"},{"question":"Advanced Visualization with Seaborn Objective: In this task, you will demonstrate your ability to use the Seaborn library for creating advanced and customized distribution plots. You will work with a range of Seaborn functionalities and apply them to gain insights from different datasets. Part 1: Plotting and Customization 1. **Load the \'tips\' dataset from Seaborn** and create a KDE plot for the variable `total_bill`. 2. **Flip the Plot**: Create an equivalent plot by assigning `total_bill` to the y-axis instead of the x-axis. 3. **Smoothing**: Adjust the bandwidth of the KDE plot to show less smoothing (`bw_adjust` = 0.2) and more smoothing (`bw_adjust` = 5), but ensuring no smoothing past the extreme data points (`cut` = 0). 4. **Hue Mapping**: Plot the KDE of `total_bill` and map the variable `time` to hue. Then, create a stacked (`multiple=\\"stack\\"`) and filled (`multiple=\\"fill\\"`) versions of the hue-mapped KDE plot. Part 2: Aggregated Data and Distributions 1. **Aggregate Data**: Using the `tips` dataset, aggregate the `total_bill` by `size`, summarize the mean and count for each size group. For the aggregated dataset, create a KDE plot of the mean `total_bill`, weighted by the `count` of each group. Part 3: Bivariate Distribution and Customization 1. **Load the \'geyser\' dataset from Seaborn** and create a bivariate KDE plot with `waiting` on the x-axis and `duration` on the y-axis. 2. **Hue Mapping**: Enhance the bivariate plot by adding the `kind` variable to map different categories by color and fill the contours. Input and Output * **Input**: * The input consists of two datasets, `tips` and `geyser`. * You should use `seaborn`, `pandas`, and `matplotlib` libraries for visualization. * **Output**: * Visualizations as described in the parts above, demonstrating your understanding of the Seaborn plotting capabilities and customization options. Here\'s a potential template to get you started: ```python import seaborn as sns import matplotlib.pyplot as plt # Part 1: Plotting and Customization # Load dataset tips = sns.load_dataset(\\"tips\\") # 1. KDE plot of total_bill sns.kdeplot(data=tips, x=\\"total_bill\\") plt.show() # 2. Flip the plot sns.kdeplot(data=tips, y=\\"total_bill\\") plt.show() # 3. Adjust smoothing sns.kdeplot(data=tips, x=\\"total_bill\\", bw_adjust=0.2) plt.show() sns.kdeplot(data=tips, x=\\"total_bill\\", bw_adjust=5, cut=0) plt.show() # 4. Hue Mapping sns.kdeplot(data=tips, x=\\"total_bill\\", hue=\\"time\\") plt.show() sns.kdeplot(data=tips, x=\\"total_bill\\", hue=\\"time\\", multiple=\\"stack\\") plt.show() sns.kdeplot(data=tips, x=\\"total_bill\\", hue=\\"time\\", multiple=\\"fill\\") plt.show() # Part 2: Aggregated Data and Distributions # Aggregate data tips_agg = tips.groupby(\\"size\\").agg(total_bill=(\\"total_bill\\", \\"mean\\"), n=(\\"total_bill\\", \\"count\\")).reset_index() sns.kdeplot(data=tips_agg, x=\\"total_bill\\", weights=\\"n\\") plt.show() # Part 3: Bivariate Distribution and Customization # Load dataset geyser = sns.load_dataset(\\"geyser\\") # 1. Bivariate KDE plot sns.kdeplot(data=geyser, x=\\"waiting\\", y=\\"duration\\") plt.show() # 2. Hue Mapping with filled contours sns.kdeplot(data=geyser, x=\\"waiting\\", y=\\"duration\\", hue=\\"kind\\", fill=True) plt.show() ``` Constraints * Ensure your plots are correctly labeled with titles, and axis names where applicable for clarity. * For hue-mapped plots, utilize appropriate palette choices for better visualization. Performance Requirements * Efficiently load and preprocess the datasets. * Visual output should be clear and interpretable. Documentation Usage * Adhere to best practices based on the documentation provided for Seaborn. * Utilize the `kdeplot` method and other Seaborn functionalities demonstrated in the examples. Good luck and happy plotting!","solution":"import seaborn as sns import matplotlib.pyplot as plt import pandas as pd # Part 1: Plotting and Customization # Load dataset tips = sns.load_dataset(\\"tips\\") # 1. KDE plot of total_bill def plot_kde_total_bill(): sns.kdeplot(data=tips, x=\\"total_bill\\") plt.title(\\"KDE plot of Total Bill\\") plt.xlabel(\\"Total Bill\\") plt.show() # 2. Flip the plot def plot_kde_total_bill_flipped(): sns.kdeplot(data=tips, y=\\"total_bill\\") plt.title(\\"Flipped KDE plot of Total Bill\\") plt.ylabel(\\"Total Bill\\") plt.show() # 3. Adjust smoothing def plot_kde_total_bill_smoothing(): sns.kdeplot(data=tips, x=\\"total_bill\\", bw_adjust=0.2) plt.title(\\"KDE plot of Total Bill with less smoothing\\") plt.xlabel(\\"Total Bill\\") plt.show() sns.kdeplot(data=tips, x=\\"total_bill\\", bw_adjust=5, cut=0) plt.title(\\"KDE plot of Total Bill with more smoothing\\") plt.xlabel(\\"Total Bill\\") plt.show() # 4. Hue Mapping def plot_kde_total_bill_hue(): sns.kdeplot(data=tips, x=\\"total_bill\\", hue=\\"time\\") plt.title(\\"KDE plot of Total Bill with Hue Mapping by Time\\") plt.xlabel(\\"Total Bill\\") plt.show() sns.kdeplot(data=tips, x=\\"total_bill\\", hue=\\"time\\", multiple=\\"stack\\") plt.title(\\"Stacked KDE plot of Total Bill with Hue Mapping by Time\\") plt.xlabel(\\"Total Bill\\") plt.show() sns.kdeplot(data=tips, x=\\"total_bill\\", hue=\\"time\\", multiple=\\"fill\\") plt.title(\\"Filled KDE plot of Total Bill with Hue Mapping by Time\\") plt.xlabel(\\"Total Bill\\") plt.show() # Part 2: Aggregated Data and Distributions def plot_aggregated_data(): # Aggregate data tips_agg = tips.groupby(\\"size\\").agg(total_bill=(\\"total_bill\\", \\"mean\\"), n=(\\"total_bill\\", \\"count\\")).reset_index() sns.kdeplot(data=tips_agg, x=\\"total_bill\\", weights=\\"n\\") plt.title(\\"KDE plot of Mean Total Bill Weighted by Count\\") plt.xlabel(\\"Mean Total Bill\\") plt.show() # Part 3: Bivariate Distribution and Customization # Load dataset geyser = sns.load_dataset(\\"geyser\\") # 1. Bivariate KDE plot def plot_bivariate_kde(): sns.kdeplot(data=geyser, x=\\"waiting\\", y=\\"duration\\") plt.title(\\"Bivariate KDE plot of Waiting vs. Duration\\") plt.xlabel(\\"Waiting\\") plt.ylabel(\\"Duration\\") plt.show() # 2. Hue Mapping with filled contours def plot_bivariate_kde_hue(): sns.kdeplot(data=geyser, x=\\"waiting\\", y=\\"duration\\", hue=\\"kind\\", fill=True) plt.title(\\"Bivariate KDE plot of Waiting vs. Duration with Hue Mapping\\") plt.xlabel(\\"Waiting\\") plt.ylabel(\\"Duration\\") plt.show()"},{"question":"# Question: Advanced Color Palette Customization with Seaborn You are working on visualizing some data and want to utilize seaborn\'s powerful color palette capabilities. Your task is to implement a function `create_custom_palette` that creates and utilizes a custom color palette for a plot. Function Signature ```python def create_custom_palette(n_colors: int) -> None: pass ``` Input - `n_colors` (int): The number of colors to include in the custom palette. This number must be between 2 and 20 (inclusive). Output - The function does not return anything. It should display a seaborn scatter plot using the created custom palette. Requirements 1. Generate a custom color palette containing `n_colors` evenly spaced colors using the \\"HUSL\\" system. 2. Use this custom palette to create a scatter plot using seaborn. 3. The scatter plot should display a set of points where: * `x = list(range(n_colors))` * `y = list(range(n_colors))` * `hue` is assigned to each point by converting their `x` values into strings. 4. Ensure the points have a size of 100. 5. Add a title to the plot that says \\"Custom HUSL Palette with `n_colors` Colors\\". 6. The plot must include a legend. Example ```python create_custom_palette(10) ``` This should display a scatter plot with 10 points, each having a unique color from a custom HUSL palette, and include a legend and title appropriately reflecting the number of colors. Constraints - `n_colors` must be an integer between 2 and 20. If `n_colors` is outside this range, raise a `ValueError` with the message \\"n_colors must be between 2 and 20\\". Hints - Use `sns.color_palette` to generate the color palette. - Use `sns.scatterplot` or `sns.relplot` to create the scatter plot. - Ensure the use of `hue` parameter in the plot to apply the custom palette.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_custom_palette(n_colors: int) -> None: if not (2 <= n_colors <= 20): raise ValueError(\\"n_colors must be between 2 and 20\\") # Generate a custom HUSL palette custom_palette = sns.color_palette(\\"husl\\", n_colors=n_colors) # Data for plotting x = list(range(n_colors)) y = list(range(n_colors)) # Create a scatter plot plt.figure(figsize=(10, 6)) sns.scatterplot(x=x, y=y, hue=list(map(str, x)), palette=custom_palette, s=100) # Add title and show plot plt.title(f\\"Custom HUSL Palette with {n_colors} Colors\\") plt.legend(title=\'Hue\') plt.show()"},{"question":"Using the Python `wave` module, write a function `process_wave_file` that reads a WAV file, extracts its properties, and then creates a new WAV file with the same properties but with halved frame rates (effectively slowing down the audio to half its speed). # Function Signature ```python def process_wave_file(input_file: str, output_file: str) -> None: pass ``` # Input - `input_file` (str): The path to the input WAV file. - `output_file` (str): The path where the modified WAV file should be saved. # Requirements 1. Open the input WAV file for reading. 2. Extract the following properties: - Number of audio channels - Sample width - Frame rate - Number of audio frames 3. Read the entire audio frames from the input file. 4. Create a new WAV file for writing, setting the same properties, except the frame rate should be halved. 5. Write the audio frames to the new file. 6. Ensure the new file is closed properly after writing. # Example ```python process_wave_file(\'input.wav\', \'output.wav\') ``` # Constraints - The input WAV file will be in \\"WAVE_FORMAT_PCM\\" format. - Ensure that the function handles exceptions properly, such as file not found or invalid WAV file format. # Notes - Use the `with` statement to ensure proper resource management (i.e., opening and closing files). - You must use the `wave` module to read from and write to the WAV files. - The function does not return any value; it only creates a new WAV file with the modified properties.","solution":"import wave def process_wave_file(input_file: str, output_file: str) -> None: try: with wave.open(input_file, \'rb\') as infile: params = infile.getparams() n_channels = params.nchannels sampwidth = params.sampwidth framerate = params.framerate // 2 n_frames = params.nframes audio_frames = infile.readframes(n_frames) with wave.open(output_file, \'wb\') as outfile: outfile.setnchannels(n_channels) outfile.setsampwidth(sampwidth) outfile.setframerate(framerate) outfile.writeframes(audio_frames) except FileNotFoundError: print(f\\"Error: The file {input_file} was not found.\\") except wave.Error as e: print(f\\"Error: {str(e)}\\")"},{"question":"**Question: Analyzing Numerical Properties of PyTorch Dtypes** You are given a PyTorch tensor and your task is to write a function `analyze_tensor_properties` that takes a tensor as input and returns a dictionary containing its numerical properties. To accomplish this, you should: 1. Determine if the tensor\'s dtype is a floating point or an integer type. 2. Depending on whether the dtype is floating point or integer, use the appropriate `torch.finfo` or `torch.iinfo` class to gather the properties. 3. Populate the dictionary with the relevant properties. The function signature should be: ```python import torch def analyze_tensor_properties(tensor: torch.Tensor) -> dict: pass ``` # Input - `tensor`: A PyTorch tensor of any numerical dtype (e.g., `torch.float32`, `torch.int64`). # Output - A dictionary containing the numerical properties: - If the tensor is of a floating point type, the dictionary should include the properties: `bits`, `eps`, `max`, `min`, `tiny`, `smallest_normal`, and `resolution`. - If the tensor is of an integer type, the dictionary should include the properties: `bits`, `max`, and `min`. # Constraints - You do not need to handle complex types or non-numerical dtypes. - Assume the tensor provided is always valid and not empty. # Example ```python >> tensor = torch.tensor([1, 2, 3], dtype=torch.int16) >> analyze_tensor_properties(tensor) { \'bits\': 16, \'max\': 32767, \'min\': -32768 } >> tensor = torch.tensor([1.0, 2.0, 3.0], dtype=torch.float32) >> analyze_tensor_properties(tensor) { \'bits\': 32, \'eps\': 1.1920928955078125e-07, \'max\': 3.4028234663852886e+38, \'min\': -3.4028234663852886e+38, \'tiny\': 1.1754943508222875e-38, \'smallest_normal\': 1.1754943508222875e-38, \'resolution\': 1e-06 } ``` # Notes - Ensure to handle all floating point types (`torch.float32`, `torch.float64`, `torch.float16`, and `torch.bfloat16`) and integer types (`torch.uint8`, `torch.int8`, `torch.int16`, `torch.int32`, and `torch.int64`). - Use the appropriate PyTorch classes (`torch.finfo` or `torch.iinfo`) to fetch the properties dynamically based on the dtype of the input tensor.","solution":"import torch def analyze_tensor_properties(tensor: torch.Tensor) -> dict: dtype = tensor.dtype if dtype.is_floating_point: info = torch.finfo(dtype) properties = { \'bits\': info.bits, \'eps\': info.eps, \'max\': info.max, \'min\': info.min, \'tiny\': info.tiny, \'smallest_normal\': info.tiny, # Synonym for tiny in torch \'resolution\': info.eps / 2.0, # An approximate definition of resolution } else: info = torch.iinfo(dtype) properties = { \'bits\': info.bits, \'max\': info.max, \'min\': info.min, } return properties"},{"question":"# Random Number Analysis and Simulation In this exercise, you will demonstrate your understanding of the `random` module by creating a function that simulates a series of events and analyzes the results. Problem Statement You are required to design and implement a function called `simulate_events` which performs the following tasks: 1. **Event Simulation**: - Simulate `n` events, where each event can have one of the following outcomes: `{\\"win\\", \\"lose\\", \\"draw\\"}`. - The probability of each outcome is determined by the following weights: `weights = [0.5, 0.3, 0.2]` respectively. 2. **Duration Simulation**: - Each event takes a random duration to complete, which follows a normal distribution with a mean (`mu`) of 5.0 seconds and a standard deviation (`sigma`) of 1.0 second. - If the generated duration is less than 0 seconds, set it to 0. 3. **Aggregate Simulation Results**: - Calculate and return the total count of each outcome. - Calculate and return the average duration for each outcome. Function Signature ```python def simulate_events(n: int) -> Tuple[Dict[str, int], Dict[str, float]]: pass ``` Parameters - `n` (int): The number of events to simulate. (1 <= n <= 10^6) Returns - Tuple containing: - A dictionary with the count of each outcome (`\\"win\\"`, `\\"lose\\"`, `\\"draw\\"`). - A dictionary with the average duration for each outcome. Example ```python counts, avg_durations = simulate_events(10000) print(counts) # Output: {\'win\': 5023, \'lose\': 2995, \'draw\': 1982} print(avg_durations) # Output: {\'win\': 5.0, \'lose\': 5.0, \'draw\': 5.0} (example values will vary) ``` Constraints - You must use the `random.choices` function to ensure the specified probabilities for the outcomes. - Use `random.gauss` to generate the event durations. Performance Requirements - The function should handle up to 10^6 events efficiently.","solution":"import random from typing import Tuple, Dict def simulate_events(n: int) -> Tuple[Dict[str, int], Dict[str, float]]: outcomes = [\'win\', \'lose\', \'draw\'] weights = [0.5, 0.3, 0.2] mu, sigma = 5.0, 1.0 counts = {\'win\': 0, \'lose\': 0, \'draw\': 0} total_durations = {\'win\': 0.0, \'lose\': 0.0, \'draw\': 0.0} for _ in range(n): event = random.choices(outcomes, weights)[0] duration = max(0, random.gauss(mu, sigma)) # Ensure duration is not negative counts[event] += 1 total_durations[event] += duration avg_durations = {key: (total_durations[key] / counts[key] if counts[key] > 0 else 0.0) for key in counts} return counts, avg_durations"},{"question":"<|Analysis Begin|> The provided documentation gives a detailed description of the `ossaudiodev` module in Python. This module provides access to OSS (Open Sound System) audio interface, which is widely used in Linux and FreeBSD for handling audio devices. The module appears to be deprecated as of Python 3.11, making it suboptimal for use in modern applications. However, it still offers a range of functionalities that are useful for learning about audio control programming in Python. The key functionalities include: 1. Opening audio and mixer devices. 2. Reading from and writing to audio devices. 3. Setting up audio parameters like format, channels, and speed. 4. Managing audio devices using methods such as `close()`, `fileno()`, `nonblock()`, `getfmts()`, `setfmt()`, `channels()`, `speed()`, `sync()`, `reset()`, and `post()`. 5. Using mixer devices and their controls to manage volumes and recording sources. Given the scope and complexity of the `ossaudiodev` module, a challenging and comprehensive coding assessment question can be formulated around utilizing these capabilities to create a music player or an audio control program. <|Analysis End|> <|Question Begin|> **Music Player Program** # Objective The goal of this assessment is to evaluate your ability to work with the `ossaudiodev` Python module to create a basic music player that can play and control the playback of audio files. # Problem Statement You are required to implement a simple music player using the `ossaudiodev` module. The player should have the following features: 1. **Open Audio Device**: Open an audio device for playback. 2. **Audio Playback**: Play a given audio file. 3. **Set Audio Parameters**: Set the audio format, the number of channels, and the sample rate before playback. 4. **Volume Control**: - Open the mixer device. - Set the volume of the audio playback. 5. **Stop Playback**: Provide a method to stop the audio playback midway. 6. **Use Context Management**: Ensure proper closing of devices using Python\'s context management. # Input and Output - **Input**: - The path to the audio file. - Audio parameters: format (e.g., `AFMT_S16_LE`), number of channels (e.g., `2`), and sample rate (e.g., `44100`). - Volume levels for left and right channels (e.g., `70,70`). - **Output**: - No direct console output. The result should be the proper playback of the audio file with the provided parameters and volume levels. # Constraints and Performance Requirements - **Audio File Format**: Assume the audio file is formatted correctly and is compatible with the specified audio formats. - **Volume Levels**: Both left and right volume levels should be integers between `0` and `100`. - The audio playback should be efficient with minimal latency. # Function Signature ```python import ossaudiodev class MusicPlayer: def __init__(self, audio_file_path: str): self.audio_file_path = audio_file_path self.audio_device = None self.mixer_device = None def open_audio_device(self, device=\\"/dev/dsp\\", mode=\\"w\\"): self.audio_device = ossaudiodev.open(device, mode) def open_mixer_device(self, device=\\"/dev/mixer\\"): self.mixer_device = ossaudiodev.openmixer(device) def set_audio_parameters(self, fmt: int, channels: int, samplerate: int): self.audio_device.setparameters(fmt, channels, samplerate) def set_volume(self, left_volume: int, right_volume: int): if self.mixer_device: self.mixer_device.set(ossaudiodev.SOUND_MIXER_VOLUME, (left_volume, right_volume)) def play_audio(self): with self.audio_device, open(self.audio_file_path, \'rb\') as audio_file: data = audio_file.read(1024) while data: self.audio_device.write(data) data = audio_file.read(1024) def stop_audio(self): if self.audio_device: self.audio_device.reset() # Example usage: # player = MusicPlayer(\\"path/to/audio/file.au\\") # player.open_audio_device() # player.open_mixer_device() # player.set_audio_parameters(ossaudiodev.AFMT_S16_LE, 2, 44100) # player.set_volume(70, 70) # player.play_audio() # To stop: player.stop_audio() ``` Implement the above `MusicPlayer` class and ensure that you handle exceptions appropriately.","solution":"import ossaudiodev class MusicPlayer: def __init__(self, audio_file_path: str): self.audio_file_path = audio_file_path self.audio_device = None self.mixer_device = None def open_audio_device(self, device=\\"/dev/dsp\\", mode=\\"w\\"): self.audio_device = ossaudiodev.open(device, mode) def open_mixer_device(self, device=\\"/dev/mixer\\"): self.mixer_device = ossaudiodev.openmixer(device) def set_audio_parameters(self, fmt: int, channels: int, samplerate: int): self.audio_device.setparameters(fmt, channels, samplerate) def set_volume(self, left_volume: int, right_volume: int): if self.mixer_device: self.mixer_device.set(ossaudiodev.SOUND_MIXER_VOLUME, (left_volume, right_volume)) def play_audio(self): with self.audio_device, open(self.audio_file_path, \'rb\') as audio_file: data = audio_file.read(1024) while data: self.audio_device.write(data) data = audio_file.read(1024) def stop_audio(self): if self.audio_device: self.audio_device.reset()"},{"question":"Objective: Write a Python function to analyze and extract data from the `__future__` module. This exercise will test your understanding of module inspection, class attributes, and version management. Problem Statement: You need to implement a function, `get_future_features_info()`, that inspects the `__future__` module and returns a dictionary containing information about its features. Specifically, the dictionary should contain: - The feature name as the key. - A dictionary as the value, containing: - `optional_in`: A tuple representing the version in which the feature first became optional. - `mandatory_in`: A tuple representing the version in which the feature first became mandatory, or the string `\'None\'` if it was dropped. - `compiler_flag`: The compiler flag needed to enable this feature. Function Signature: ```python import __future__ def get_future_features_info() -> dict: pass ``` Expected Output: The function should return a dictionary where each key is a feature name string, and each value is a dictionary containing \'optional_in\', \'mandatory_in\', and \'compiler_flag\'. Constraints: - You may use any standard libraries/modules. - Ensure your function is compatible with Python 3.10. Example Output: ```python { \'nested_scopes\': { \'optional_in\': (2, 1, 0, \'beta\', 1), \'mandatory_in\': (2, 2, 0, \'final\', 0), \'compiler_flag\': 0x1 }, \'generators\': { \'optional_in\': (2, 2, 0, \'alpha\', 1), \'mandatory_in\': (2, 3, 0, \'final\', 0), \'compiler_flag\': 0x2 }, # ... include all other features similarly } ``` Notes: - You can access the attributes of the `_Feature` instances using appropriate methods or attributes. - Analyze `__future__` programmatically to dynamically extract the required information.","solution":"import __future__ def get_future_features_info(): features_info = {} for feature_name in dir(__future__): feature = getattr(__future__, feature_name) if isinstance(feature, __future__._Feature): features_info[feature_name] = { \'optional_in\': feature.optional, \'mandatory_in\': feature.mandatory, \'compiler_flag\': feature.compiler_flag } return features_info"},{"question":"**TorchScript and PyTorch Compatibility** You are tasked with implementing a PyTorch model and converting it to TorchScript while adhering to specific constraints outlined in the TorchScript unsupported constructs documentation. Your implementation should navigate TorchScript limitations. # Implement and Convert to TorchScript Requirements: 1. **Model Definition**: - Define a simple PyTorch neural network model using `torch.nn.Module`. - The model should: - Contain at least one linear layer. - Use ReLU activation functions. 2. **Custom Initialization**: - Implement a custom weight initialization using `torch.nn.init.kaiming_normal_` for the linear layer(s) weights. - Explain why `torch.nn.init.kaiming_normal_` must be handled properly considering TorchScript limitations. 3. **TorchScript Conversion**: - Define a method to convert the defined model to TorchScript using `torch.jit.script`. Constraints: 1. Avoid using functions listed as not correctly bound or unsupported in TorchScript (refer to the provided documentation if needed). 2. Ensure that any tensor operations or constructions consider schema requirements for TorchScript. 3. Explain any alternate strategies used to mitigate unsupported features. Function Signature: You should implement the following function: ```python import torch import torch.nn as nn def create_and_convert_model(input_size: int, output_size: int) -> torch.jit.ScriptModule: Creates a simple neural network model in PyTorch, applies custom weight initialization, and converts it to TorchScript. Args: - input_size (int): The number of input features. - output_size (int): The number of output features. Returns: - torch.jit.ScriptModule: The TorchScript version of the PyTorch model. class SimpleNN(nn.Module): def __init__(self, input_size, output_size): super(SimpleNN, self).__init__() self.linear1 = nn.Linear(input_size, output_size) self.relu = nn.ReLU() # Custom initialization with torch.no_grad(): nn.init.kaiming_normal_(self.linear1.weight) def forward(self, x): return self.relu(self.linear1(x)) # Instantiate model model = SimpleNN(input_size, output_size) # Convert to TorchScript scripted_model = torch.jit.script(model) return scripted_model ``` Evaluation: Your implementation will be evaluated based on: 1. Correctness of the neural network definition and initialization. 2. Proper handling of TorchScript conversion. 3. Demonstrating an understanding of TorchScript limitations in the provided explanation. 4. The clarity and readability of your code.","solution":"import torch import torch.nn as nn def create_and_convert_model(input_size: int, output_size: int) -> torch.jit.ScriptModule: Creates a simple neural network model in PyTorch, applies custom weight initialization, and converts it to TorchScript. Args: - input_size (int): The number of input features. - output_size (int): The number of output features. Returns: - torch.jit.ScriptModule: The TorchScript version of the PyTorch model. class SimpleNN(nn.Module): def __init__(self, input_size, output_size): super(SimpleNN, self).__init__() self.linear1 = nn.Linear(input_size, output_size) self.relu = nn.ReLU() # Custom initialization with torch.no_grad(): nn.init.kaiming_normal_(self.linear1.weight) def forward(self, x): return self.relu(self.linear1(x)) # Instantiate model model = SimpleNN(input_size, output_size) # Convert to TorchScript scripted_model = torch.jit.script(model) return scripted_model"},{"question":"**Question: Image Type Identifier and Extender** You are tasked with writing a Python program that identifies the types of a set of image files and also extends the functionality of the `imghdr` module to recognize a new custom image format called `foo`. # Part 1: Image Type Identification Write a function `identify_image_types(image_files)` that takes a list of file paths, `image_files`, and returns a dictionary where the keys are the file names and the values are the identified image types. ```python def identify_image_types(image_files): Identify the types of the given image files using the imghdr module. Args: image_files (list): List of image file paths as strings. Returns: dict: Dictionary with file names as keys and identified image types as values. pass ``` **Constraints:** - Use the `imghdr.what()` method to identify image types. - If the image type cannot be determined, the value should be `None`. # Part 2: Extend `imghdr` to Recognize \\"foo\\" Format The \\"foo\\" format is a simple mock format for this task. It can be identified by checking if the first four bytes of the file are `b\'FOOF\'`. Write a function `extend_imghdr_for_foo()` that modifies the `imghdr` module to recognize the \\"foo\\" format. Then, update the `identify_image_types` function to include this new format when identifying image types. ```python def extend_imghdr_for_foo(): Extends the imghdr module to recognize the \'foo\' image format. The \'foo\' format is identified by the first four bytes being \'FOOF\'. Returns: None pass ``` # Example Usage ```python # Before extending image_types = identify_image_types([\'test.png\', \'test.foo\']) # Output might be: {\'test.png\': \'png\', \'test.foo\': None} # Extend for \'foo\' format extend_imghdr_for_foo() # After extending image_types = identify_image_types([\'test.png\', \'test.foo\']) # Output should be: {\'test.png\': \'png\', \'test.foo\': \'foo\'} ``` # Performance Requirements - The solution should be efficient enough to handle identifying types for up to 1000 image files in less than a few seconds. # Additional Information You may import the `os` module if needed for file manipulation. *Hint:* Utilize the `imghdr.tests` list to add your custom check function for the \\"foo\\" format.","solution":"import imghdr def identify_image_types(image_files): Identify the types of the given image files using the imghdr module. Args: image_files (list): List of image file paths as strings. Returns: dict: Dictionary with file names as keys and identified image types as values. result = {} for file in image_files: image_type = imghdr.what(file) result[file] = image_type return result def extend_imghdr_for_foo(): Extends the imghdr module to recognize the \'foo\' image format. The \'foo\' format is identified by the first four bytes being \'FOOF\'. Returns: None def test_foo(h, f): if h[:4] == b\'FOOF\': return \'foo\' imghdr.tests.append(test_foo)"},{"question":"Tuple and Struct Sequence Manipulation You are required to write functions that demonstrate your comprehension and ability to work with tuples and struct sequences in Python. Your task includes creating and manipulating tuples, as well as creating a struct sequence based on a given tuple. Part 1: Tuple Operations 1. Implement a function `create_tuple(values: List) -> Tuple` that takes a list `values` and returns a tuple containing the same elements. 2. Implement a function `resize_tuple(original: Tuple, new_size: int) -> Tuple` that resizes the given tuple `original` to `new_size`. - If `new_size` is greater than the original size, pad the tuple with `None` values. - If `new_size` is less than the original size, truncate the tuple to the new size. Part 2: Struct Sequence Operations 3. Implement a function `create_struct_sequence(data: Tuple) -> namedtuple` that takes a tuple `data` and returns an equivalent `namedtuple` with the fields named `field0`, `field1`, etc. 4. Implement a function `set_struct_field(struct, pos: int, value) -> namedtuple` that takes a struct sequence `struct`, an index `pos`, and a `value`, and sets the value of the field at index `pos` to the given value. Return the modified struct sequence. # Function Definitions Create the following functions: ```python from typing import List, Tuple, Any from collections import namedtuple def create_tuple(values: List) -> Tuple: Create a tuple from the given list of values. :param values: List of values to be converted to a tuple. :return: Tuple with the same elements as the input list. pass def resize_tuple(original: Tuple, new_size: int) -> Tuple: Resize a given tuple to the new size. :param original: Original tuple to be resized. :param new_size: New size of the tuple. :return: Resized tuple with the specified size. pass def create_struct_sequence(data: Tuple) -> namedtuple: Create a struct sequence from the given tuple. :param data: Tuple from which to create the struct sequence. :return: Struct sequence equivalent to the input tuple. pass def set_struct_field(struct: namedtuple, pos: int, value: Any) -> namedtuple: Set a specific field in a struct sequence to a given value. :param struct: Original struct sequence. :param pos: Index position of the field to update. :param value: New value for the specified field. :return: Modified struct sequence with the updated value. pass ``` # Constraints - You may assume that the input list and tuples contain only hashable elements. - The `pos` provided in `set_struct_field` will always be within bounds of the structure\'s fields. # Performance Requirements - Your implementation should handle tuples up to a length of 10^6 efficiently. - Ensure that the tuple creation and resizing processes are computationally efficient. # Example Usage ```python # Example for create_tuple function assert create_tuple([1, 2, 3]) == (1, 2, 3) # Example for resize_tuple function assert resize_tuple((1, 2, 3), 5) == (1, 2, 3, None, None) assert resize_tuple((1, 2, 3, 4, 5), 3) == (1, 2, 3) # Example for create_struct_sequence function StructSeq = create_struct_sequence((10, 20, 30)) assert StructSeq.field0 == 10 assert StructSeq.field1 == 20 assert StructSeq.field2 == 30 # Example for set_struct_field function ModifiedStruct = set_struct_field(StructSeq, 1, 99) assert ModifiedStruct.field1 == 99 ``` Ensure your implementation correctly handles these operations and adheres to the given constraints and performance requirements.","solution":"from typing import List, Tuple, Any from collections import namedtuple def create_tuple(values: List) -> Tuple: Create a tuple from the given list of values. :param values: List of values to be converted to a tuple. :return: Tuple with the same elements as the input list. return tuple(values) def resize_tuple(original: Tuple, new_size: int) -> Tuple: Resize a given tuple to the new size. :param original: Original tuple to be resized. :param new_size: New size of the tuple. :return: Resized tuple with the specified size. if new_size > len(original): return original + (None,) * (new_size - len(original)) else: return original[:new_size] def create_struct_sequence(data: Tuple) -> namedtuple: Create a struct sequence from the given tuple. :param data: Tuple from which to create the struct sequence. :return: Struct sequence equivalent to the input tuple. fields = [f\'field{i}\' for i in range(len(data))] Struct = namedtuple(\'Struct\', fields) return Struct(*data) def set_struct_field(struct: namedtuple, pos: int, value: Any) -> namedtuple: Set a specific field in a struct sequence to a given value. :param struct: Original struct sequence. :param pos: Index position of the field to update. :param value: New value for the specified field. :return: Modified struct sequence with the updated value. struct_dict = struct._asdict() struct_dict[f\'field{pos}\'] = value return type(struct)(**struct_dict)"},{"question":"Objective Demonstrate your understanding of the seaborn library and its object-oriented interface by creating a complex and customized visualization. Task Using the `seaborn.objects` module, create a function `complex_seaborn_plot` that generates a multi-faceted plot with the following specifications: 1. **Input**: - `data`: A pandas DataFrame containing at least the following columns: \'category\' (categorical), \'value1\', \'value2\' (numerical), \'date\' (datetime). - `output_file`: The path where the plot should be saved. 2. **Requirements**: - Create a plot with at least three subplots, each demonstrating a different scale (Nominal, Continuous, Temporal) for the x-axis. - Use different **markers** for different categories in the \'category\' column. - Apply varying **alphas** to demonstrate density. - Use both edge and fill colors to differentiate between categories. - Include custom **linestyles** and **linewidths** for the plots. - Add appropriate labels, titles, and themes to make the plots informative and visually appealing. 3. **Output**: - The function should save the plot to the specified `output_file` as a PNG image. Function Signature ```python def complex_seaborn_plot(data: pd.DataFrame, output_file: str) -> None: pass ``` # Example ```python import pandas as pd import numpy as np # Sample data categories = [\'A\', \'B\', \'C\'] date_range = pd.date_range(start=\'1/1/2022\', periods=100) data = pd.DataFrame({ \'category\': np.random.choice(categories, 100), \'value1\': np.random.rand(100) * 100, \'value2\': np.random.rand(100) * 50, \'date\': np.random.choice(date_range, 100), }) # Call the function complex_seaborn_plot(data, \'output_plot.png\') ``` Constraints - Ensure your code is efficient and performs well with reasonably large data sets (e.g., 10,000 rows). - Use appropriate exception handling to manage potential issues with input data. # Note - Review the provided documentation for examples and further details on the usage of `seaborn.objects`. - The function should be self-contained and import all necessary libraries within its scope.","solution":"import pandas as pd import seaborn.objects as so import matplotlib.pyplot as plt def complex_seaborn_plot(data: pd.DataFrame, output_file: str) -> None: Generate a multi-faceted plot using seaborn.objects with different scales, markers, colors, and linestyles. Parameters: data (pd.DataFrame): Input DataFrame with columns \'category\', \'value1\', \'value2\', and \'date\'. output_file (str): Path to save the output plot. try: # Creating the plot canvas fig, axes = plt.subplots(1, 3, figsize=(18, 6)) # Defining markers and colors for categories palette = {\'A\': \'blue\', \'B\': \'orange\', \'C\': \'green\'} markers = {\'A\': \'o\', \'B\': \'s\', \'C\': \'D\'} # Plot 1: Nominal scale using \'category\' for the x-axis plot1 = so.Plot(data, x=\'category\', y=\'value1\').add(so.Dot(), so.Agg(), marker=markers, color=palette) plot1.label(x=\'Category\', y=\'Value 1\').scale(alpha=0.6).on(axes[0]) # Plot 2: Continuous scale using \'value2\' for the x-axis plot2 = so.Plot(data, x=\'value2\', y=\'value1\').add(so.Line(), so.Agg(), marker=markers, color=palette) plot2.label(x=\'Value 2\', y=\'Value 1\').scale(alpha=0.6).on(axes[1]) # Plot 3: Temporal scale using \'date\' for the x-axis plot3 = so.Plot(data, x=\'date\', y=\'value1\').add(so.Line(), so.Agg(), marker=markers, color=palette).facet(\'category\') plot3.label(x=\'Date\', y=\'Value 1\').scale(alpha=0.6).on(axes[2]) # Setting titles axes[0].set_title(\'Nominal Scale Plot\') axes[1].set_title(\'Continuous Scale Plot\') axes[2].set_title(\'Temporal Scale Plot\') # Applying a theme and saving the plot plt.tight_layout() plt.savefig(output_file) plt.close(fig) except Exception as e: print(f\\"An error occurred: {e}\\")"},{"question":"Coding Assessment Question # Problem Statement You are required to design a custom Telnet client using the `telnetlib` module from Python. Your client will connect to a specified Telnet server, send commands, handle responses, and manage various scenarios such as timeouts and connection errors. This client will also demonstrate the use of multiple `Telnet` class methods to interact with the server. # Requirements 1. **Function Definition**: Implement a function named `custom_telnet_client` that takes four parameters: - `host` (str): The hostname or IP address of the Telnet server. - `port` (int): The port number to connect to (default is 23). - `commands` (list): A list of commands (strings) to send to the Telnet server. - `timeout` (float): Timeout for blocking operations in seconds. 2. **Function Behavior**: - Establish a connection to the Telnet server using the provided `host` and `port`. - For each command in the `commands` list: - Send the command to the Telnet server. - Read and store the server’s response. - Use `read_until` to wait for specific output after each command, or a reasonable alternative method based on command structure. - Implement error handling to manage cases where commands timeout or the connection is lost. - Close the connection after all commands have been executed. - Return a dictionary where the keys are commands and the values are the responses from the server. 3. **Constraints**: - You must handle potential exceptions like `EOFError` and connection-related `OSError`. - Ensure any special characters or sequences in the commands are appropriately encoded and handled. - Use at least three different `read_*` methods to demonstrate a range of `telnetlib` capabilities. 4. **Performance Requirements**: - The function should handle commands efficiently, ensuring minimal latency in communication. - Timeout mechanisms should promptly handle non-responsive servers without excessive delays. # Example Usage ```python def custom_telnet_client(host, port=23, commands=[], timeout=10.0): # Your implementation here # Example host and commands host = \\"example.com\\" commands = [\\"login user\\", \\"password pass\\", \\"ls\\", \\"exit\\"] response = custom_telnet_client(host, port=23, commands=commands, timeout=5.0) for cmd, output in response.items(): print(f\\"Command: {cmd}nResponse:n{output}n\\") ``` # Note Make sure your function is robust and can handle various edge cases, such as empty command lists, invalid hosts, and scenarios where the server sends unexpected data. Your implementation will be tested against different Telnet servers, so ensure compatibility and error resilience.","solution":"import telnetlib def custom_telnet_client(host, port=23, commands=[], timeout=10.0): Connects to a Telnet server, sends a list of commands, and returns their responses. Args: host (str): The hostname or IP address of the Telnet server. port (int): The port number to connect to (default is 23). commands (list): A list of commands (strings) to send to the Telnet server. timeout (float): Timeout for blocking operations in seconds. Returns: dict: A dictionary where keys are commands and values are the responses from the server. responses = {} try: with telnetlib.Telnet(host, port, timeout) as tn: for command in commands: tn.write(command.encode(\'ascii\') + b\'n\') response = tn.read_until(b\'n\', timeout).decode(\'ascii\') responses[command] = response except (EOFError, OSError) as e: responses[\'error\'] = str(e) return responses"},{"question":"# Object Attribute Manipulation in Python You are required to implement a class `PyObjectSimulator` that simulates some of the object attribute manipulation functionalities described in the provided documentation. Specifically, you will implement methods to get, set, and delete attributes, and to check if an object has a specific attribute. Class Definition ```python class PyObjectSimulator: def __init__(self, obj): # Initializes with the given object pass def py_hasattr(self, attr_name): # Return True if the object has the attribute with name attr_name, # otherwise return False pass def py_getattr(self, attr_name): # Return the value of the attribute named attr_name. If the attribute # does not exist, raises an AttributeError. pass def py_setattr(self, attr_name, value): # Set the value of the attribute named attr_name to value. # Return nothing. pass def py_delattr(self, attr_name): # Delete the attribute named attr_name. # If the attribute does not exist, raises an AttributeError. pass ``` Input and Output - `__init__(self, obj)`: Initializes with the given object. - `py_hasattr(self, attr_name)`: - Input: `attr_name` (str) - Output: `True` if attribute exists, `False` otherwise. - `py_getattr(self, attr_name)`: - Input: `attr_name` (str) - Output: Value of the attribute if it exists, otherwise raises `AttributeError`. - `py_setattr(self, attr_name, value)`: - Input: `attr_name` (str), `value` (any) - Output: None. - `py_delattr(self, attr_name)`: - Input: `attr_name` (str) - Output: None (raises `AttributeError` if the attribute does not exist). Constraints - You must not use the built-in Python functions `hasattr`, `getattr`, `setattr`, and `delattr`. Example Usage ```python class TestClass: def __init__(self): self.x = 5 self.y = \\"hello\\" test_obj = TestClass() simulator = PyObjectSimulator(test_obj) print(simulator.py_hasattr(\\"x\\")) # Output: True print(simulator.py_getattr(\\"x\\")) # Output: 5 simulator.py_setattr(\\"z\\", 10) print(simulator.py_getattr(\\"z\\")) # Output: 10 simulator.py_delattr(\\"z\\") print(simulator.py_hasattr(\\"z\\")) # Output: False try: simulator.py_getattr(\\"z\\") # Should raise AttributeError except AttributeError as e: print(\\"AttributeError raised as expected\\") ```","solution":"class PyObjectSimulator: def __init__(self, obj): self.obj = obj def py_hasattr(self, attr_name): return attr_name in self.obj.__dict__ def py_getattr(self, attr_name): if attr_name in self.obj.__dict__: return self.obj.__dict__[attr_name] else: raise AttributeError(f\\"\'{type(self.obj).__name__}\' object has no attribute \'{attr_name}\'\\") def py_setattr(self, attr_name, value): self.obj.__dict__[attr_name] = value def py_delattr(self, attr_name): if attr_name in self.obj.__dict__: del self.obj.__dict__[attr_name] else: raise AttributeError(f\\"\'{type(self.obj).__name__}\' object has no attribute \'{attr_name}\'\\")"},{"question":"PyTorch to TorchScript Conversion **Objective**: The goal of this assessment is to evaluate your ability to convert PyTorch code to TorchScript, ensuring the code functions correctly despite the limitations described in the documentation. **Problem Statement**: You are given a simple neural network implemented in PyTorch. Your task is to convert this network into TorchScript while ensuring that it adheres to the constraints and limitations provided in the documentation. This involves handling unsupported functions and attributes and ensuring compatibility with TorchScript. **PyTorch Code**: ```python import torch import torch.nn as nn import torch.nn.functional as F class SimpleNet(nn.Module): def __init__(self): super(SimpleNet, self).__init__() self.conv1 = nn.Conv2d(1, 10, kernel_size=5) self.conv2 = nn.Conv2d(10, 20, kernel_size=5) self.conv2_drop = nn.Dropout2d() self.fc1 = nn.Linear(320, 50) self.fc2 = nn.Linear(50, 10) def forward(self, x): x = F.relu(F.max_pool2d(self.conv1(x), 2)) x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2)) x = x.view(-1, 320) x = F.relu(self.fc1(x)) x = F.dropout(x, training=self.training) x = self.fc2(x) return F.log_softmax(x, dim=1) model = SimpleNet() example_input = torch.randn(1, 1, 28, 28) traced_script_module = torch.jit.trace(model, example_input) ``` **Your Task**: 1. Identify and address any unsupported PyTorch constructs or attributes that cannot be used in TorchScript. 2. Refactor the given PyTorch code to be compatible with TorchScript. **Constraints**: 1. Do not use any unsupported modules or functions indicated in the documentation. 2. Ensure the refactored code maintains the same functionality as the original PyTorch code. 3. You may need to provide alternative implementations or remove certain unsupported parts. **Submission**: 1. Provide the refactored code as your answer. 2. Explain any changes you made and why they were necessary. **Performance Requirements**: - The model should compile successfully with TorchScript. - It should produce the same output for a given input as the original PyTorch model. **Expected Input and Output**: - Input: A Tensor of shape (1, 1, 28, 28) - Output: A Tensor of shape (1, 10), representing log softmax probabilities for 10 classes. **Example**: If the input tensor is `torch.randn(1, 1, 28, 28)`, the output should be a tensor of shape `(1, 10)`. Good luck!","solution":"import torch import torch.nn as nn import torch.nn.functional as F class SimpleNet(nn.Module): def __init__(self): super(SimpleNet, self).__init__() self.conv1 = nn.Conv2d(1, 10, kernel_size=5) self.conv2 = nn.Conv2d(10, 20, kernel_size=5) self.conv2_drop = nn.Dropout2d() self.fc1 = nn.Linear(320, 50) self.fc2 = nn.Linear(50, 10) def forward(self, x): x = F.relu(F.max_pool2d(self.conv1(x), 2)) x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2)) x = x.view(-1, 320) x = F.relu(self.fc1(x)) x = F.dropout(x, training=self.training) x = self.fc2(x) return F.log_softmax(x, dim=1) # Define a function that disables training mode for dropout def convert_to_torchscript(model: nn.Module, example_input: torch.Tensor) -> torch.jit.ScriptModule: model.eval() # Ensure dropout is in evaluation mode return torch.jit.script(model) model = SimpleNet() example_input = torch.randn(1, 1, 28, 28) scripted_module = convert_to_torchscript(model, example_input)"},{"question":"**Fourier Transform with Signal Denoising using PyTorch** **Objective:** Your task is to implement a function that performs signal denoising using Fourier Transform with PyTorch. **Description:** You are given a 1D signal contaminated with high-frequency noise. Implement a function that: 1. Converts the noisy signal to the frequency domain using Fast Fourier Transform (FFT). 2. Applies a low-pass filter to remove high-frequency noise. 3. Converts the filtered signal back to the time domain using Inverse Fast Fourier Transform (IFFT). **Function Signature:** ```python import torch def denoise_signal(noisy_signal: torch.Tensor, cutoff_frequency: float) -> torch.Tensor: Perform signal denoising by applying a low-pass filter in the frequency domain. Parameters: noisy_signal (torch.Tensor): A 1D tensor representing the noisy time-domain signal. cutoff_frequency (float): The frequency beyond which the components will be filtered out. Returns: torch.Tensor: A 1D tensor representing the denoised time-domain signal. pass ``` **Input:** - `noisy_signal`: A torch tensor `torch.Tensor` of shape `(N,)` representing the noisy signal. - `cutoff_frequency`: A `float` representing the frequency threshold for the low-pass filter. **Output:** - Returns a torch tensor `torch.Tensor` of shape `(N,)` representing the denoised signal. **Constraints:** - The input signal length `N` can be up to (10^6). - `cutoff_frequency` will always be a positive float less than the Nyquist frequency of the signal. **Example:** ```python noisy_signal = torch.tensor([0.5, 1.5, 0.9, 0.1, -0.3, -1.0, -0.7, 0.2, 0.8, 1.2, 0.4, -0.2, -1.1, -0.8, 0.0, 0.15], dtype=torch.float32) cutoff_frequency = 2.0 clean_signal = denoise_signal(noisy_signal, cutoff_frequency) print(clean_signal) # Expected output: tensor([Filtered values close to the original clean signal]) ``` **Steps to Implement:** 1. Compute the FFT of the noisy signal. 2. Generate frequency bins using `torch.fft.fftfreq`. 3. Apply a low-pass filter by zeroing out the frequencies beyond the `cutoff_frequency`. 4. Compute the inverse FFT to return to the time domain. Apply appropriate edge cases, and ensure that the process runs efficiently to handle large signals as specified by the constraints.","solution":"import torch def denoise_signal(noisy_signal: torch.Tensor, cutoff_frequency: float) -> torch.Tensor: Perform signal denoising by applying a low-pass filter in the frequency domain. Parameters: noisy_signal (torch.Tensor): A 1D tensor representing the noisy time-domain signal. cutoff_frequency (float): The frequency beyond which the components will be filtered out. Returns: torch.Tensor: A 1D tensor representing the denoised time-domain signal. # Step 1: Compute the FFT of the noisy signal signal_fft = torch.fft.fft(noisy_signal) # Step 2: Generate frequency bins N = noisy_signal.shape[0] freq_bins = torch.fft.fftfreq(N) # Step 3: Apply a low-pass filter by zeroing out frequencies beyond the cutoff filter_mask = torch.abs(freq_bins) <= cutoff_frequency filtered_fft = signal_fft * filter_mask # Step 4: Compute the inverse FFT to return to the time domain denoised_signal = torch.fft.ifft(filtered_fft).real # Use .real to get the real part only return denoised_signal"},{"question":"You are provided with a Python source code string. Your task is to write a function that parses this source code string into an AST, performs specific transformations on the AST, and then converts the modified AST back into a source code string. Task: 1. Parse the given Python source code string into an AST. 2. Traverse the AST and perform the following transformations: * Replace all occurrences of binary addition (`+`) with binary multiplication (`*`). * Replace all integer literals with their string representations. 3. Convert the modified AST back into a Python source code string. 4. Return the transformed source code string. Function Signature: ```python def transform_source_code(source_code: str) -> str: pass ``` Input: * `source_code` (str): A valid Python source code string. Output: * `str`: The transformed Python source code string. Example: ```python source_code = a = 1 + 2 b = a + 3 print(transform_source_code(source_code)) ``` Expected Output: ```python a = \'1\' * \'2\' b = a * \'3\' ``` # Constraints: * The input source code string is guaranteed to be valid Python code. * You must correctly handle nested expressions and multiple occurrences of the specified transformations. # Hints: - You can use the `ast` module to parse, traverse, and modify the AST. - Implement custom visitor and transformer classes to perform the required transformations. Good luck!","solution":"import ast import astor class SourceCodeTransformer(ast.NodeTransformer): def visit_BinOp(self, node): if isinstance(node.op, ast.Add): node.op = ast.Mult() self.generic_visit(node) return node def visit_Constant(self, node): if isinstance(node.value, int): node.value = str(node.value) return node def transform_source_code(source_code: str) -> str: # Parse the source code into an AST tree = ast.parse(source_code) # Create transformer and apply it to the AST transformer = SourceCodeTransformer() transformed_tree = transformer.visit(tree) # Convert the modified AST back into source code transformed_source_code = astor.to_source(transformed_tree) return transformed_source_code"},{"question":"**Problem Statement:** You are given a pandas Series representing the daily closing prices of a stock over a period of time. The data contains some days with missing values represented as `NaN`. Your task is to write a function `process_stock_data` that performs several operations on this Series and returns a summary of important information. # Function Signature ```python import pandas as pd def process_stock_data(prices: pd.Series) -> dict: pass ``` # Input - `prices`: A pandas Series of float values, with some values potentially being NaN. The index of the Series is a DatetimeIndex representing the dates. # Output - A dictionary containing the following keys and values: 1. `\'filled_prices\'`: The Series with missing values filled by forward filling method. 2. `\'mean_price\'`: The mean of the filled prices. 3. `\'high_prices\'`: A Series representing the top 5 highest closing prices. 4. `\'monthly_avg_prices\'`: A Series representing the average closing price for each month. # Constraints - You should use pandas methods and functionalities to achieve the required operations. - Ensure your code is efficient and readable. # Example ```python import pandas as pd import numpy as np data = pd.Series([np.nan, 101.2, 102.3, 103.5, np.nan, 106.0, 107.4, np.nan, 109.6, 110.0, np.nan], index=pd.date_range(start=\'2023-01-01\', periods=11)) result = process_stock_data(data) print(result) # Expected output: # { # \'filled_prices\': Series after forward fill, # \'mean_price\': average of filled prices, # \'high_prices\': top 5 highest closing prices, # \'monthly_avg_prices\': monthly average closing prices # } ``` # Explanation 1. **Forward Fill**: Fill the NaN values using forward fill method to propagate the previous day\'s closing prices. 2. **Mean Calculation**: Compute the mean of the filled prices. 3. **Top 5 Prices**: Extract the five highest closing prices from the filled prices. 4. **Monthly Average**: Compute the average closing price for each month. Use the given data, apply the required transformations, and return the summary in a dictionary format.","solution":"import pandas as pd def process_stock_data(prices: pd.Series) -> dict: Process the stock data to summarize important information. :param prices: A pandas Series of float values, with some values potentially being NaN. The index of the Series is a DatetimeIndex representing the dates. :return: A dictionary containing the processed information. # Fill NaN values using forward fill method filled_prices = prices.ffill() # Calculate the mean of the filled prices mean_price = filled_prices.mean() # Get the top 5 highest closing prices high_prices = filled_prices.nlargest(5) # Calculate the average closing price for each month monthly_avg_prices = filled_prices.resample(\'M\').mean() # Create the summary dictionary summary = { \'filled_prices\': filled_prices, \'mean_price\': mean_price, \'high_prices\': high_prices, \'monthly_avg_prices\': monthly_avg_prices } return summary"},{"question":"Objective: You are required to demonstrate your understanding of the `enum` module by implementing an enumeration with unique values and custom methods. You will also perform certain operations to test the functionality of your enumeration. Problem Statement: Create an enumeration called `Suit` that represents the four suits of playing cards: Hearts, Diamonds, Clubs, and Spades. Each suit should be assigned a unique integer value starting from 1. The enumeration should include custom methods to display the suit names and values in a readable format. Requirements: 1. Use the `Enum` class from the `enum` module to define the `Suit` enumeration. 2. Ensure that each suit has a unique integer value starting from 1. 3. Implement a class method `display_suits` that prints all the suits and their values in the format: `Suit Name: <name>, Suit Value: <value>`. 4. Implement an instance method `is_red` that returns `True` if the suit is either Hearts or Diamonds, and `False` otherwise. 5. Use the `@unique` decorator to ensure the enum has no duplicate values. Constraints: - Use the `Enum` class for creating the enumeration. - Use the `@unique` decorator to enforce the uniqueness of values. Input and Output: Your implementation should not require any input from the user. The methods will be tested by calling them directly. Example: ```python from enum import Enum, unique @unique class Suit(Enum): HEARTS = 1 DIAMONDS = 2 CLUBS = 3 SPADES = 4 @classmethod def display_suits(cls): for suit in cls: print(f\\"Suit Name: {suit.name}, Suit Value: {suit.value}\\") def is_red(self): return self in (Suit.HEARTS, Suit.DIAMONDS) # Example usage: Suit.display_suits() # Output: # Suit Name: HEARTS, Suit Value: 1 # Suit Name: DIAMONDS, Suit Value: 2 # Suit Name: CLUBS, Suit Value: 3 # Suit Name: SPADES, Suit Value: 4 print(Suit.HEARTS.is_red()) # Output: True print(Suit.CLUBS.is_red()) # Output: False ``` You are required to implement the `Suit` class with the described functionalities.","solution":"from enum import Enum, unique @unique class Suit(Enum): HEARTS = 1 DIAMONDS = 2 CLUBS = 3 SPADES = 4 @classmethod def display_suits(cls): for suit in cls: print(f\\"Suit Name: {suit.name}, Suit Value: {suit.value}\\") def is_red(self): return self in (Suit.HEARTS, Suit.DIAMONDS)"},{"question":"**Question: Constructing and Sending a MIME-Formatted Email** You\'re required to create a function `create_mime_email` that constructs a MIME email with multiple types of content using the `email.mime` module. The email should include: 1. Plain text 2. An image 3. A PDF attachment The function should take the following arguments: - `from_address` (string): The email address of the sender. - `to_address` (string): The email address of the recipient. - `subject` (string): The subject of the email. - `text_content` (string): The plain text content of the email. - `image_path` (string): The file path to the image that needs to be attached. - `pdf_path` (string): The file path to the PDF that needs to be attached. **Function Signature:** ```python def create_mime_email(from_address: str, to_address: str, subject: str, text_content: str, image_path: str, pdf_path: str) -> email.mime.multipart.MIMEMultipart: pass ``` **Requirements:** 1. The email must be constructed as a `MIMEMultipart` object with a `multipart/mixed` subtype. 2. The plain text content should be added as a `MIMEText` object. 3. The image should be added as a `MIMEImage` object. 4. The PDF should be added as a `MIMEApplication` object. 5. The email must include appropriate MIME headers (e.g., `From`, `To`, `Subject`). **Example:** ```python email_message = create_mime_email( from_address=\\"sender@example.com\\", to_address=\\"recipient@example.com\\", subject=\\"Test Email\\", text_content=\\"This is a test email with text, image, and PDF attachments.\\", image_path=\\"path/to/image.jpg\\", pdf_path=\\"path/to/document.pdf\\" ) ``` **Constraints:** - You can assume the file paths provided for the image and PDF are valid. **Hint:** Refer to the `email.mime` module documentation for creating and adding MIME parts. Specifically, you may find classes like `MIMEMultipart`, `MIMEText`, `MIMEImage`, and `MIMEApplication` useful.","solution":"from email.mime.multipart import MIMEMultipart from email.mime.text import MIMEText from email.mime.image import MIMEImage from email.mime.application import MIMEApplication import os def create_mime_email(from_address: str, to_address: str, subject: str, text_content: str, image_path: str, pdf_path: str) -> MIMEMultipart: # Create the MIMEMultipart object msg = MIMEMultipart(\'mixed\') msg[\'From\'] = from_address msg[\'To\'] = to_address msg[\'Subject\'] = subject # Attach plain text content text_part = MIMEText(text_content, \'plain\') msg.attach(text_part) # Attach image file with open(image_path, \'rb\') as img_file: img = MIMEImage(img_file.read()) img.add_header(\'Content-Disposition\', f\'attachment; filename=\\"{os.path.basename(image_path)}\\"\') msg.attach(img) # Attach PDF file with open(pdf_path, \'rb\') as pdf_file: pdf = MIMEApplication(pdf_file.read(), _subtype=\'pdf\') pdf.add_header(\'Content-Disposition\', f\'attachment; filename=\\"{os.path.basename(pdf_path)}\\"\') msg.attach(pdf) return msg"},{"question":"Objective: Your task is to implement a PyTorch FX graph transformation that modifies specific ATen operations within a given computational graph. This task will assess your understanding of FX GraphModule transformations and advanced manipulation using PyTorch. Task: 1. Implement a transformation that replaces all instances of `torch.ops.aten.add.Tensor` followed by `torch.ops.aten.relu.default` with `torch.ops.aten.sub.Tensor` followed by `torch.ops.aten.sigmoid.default`. 2. The transformation should correctly handle the order of operations and dependencies between nodes. Instructions: 1. You are provided with a `torch.fx.GraphModule` which represents a computational graph. 2. Your implemented transformation should: - Traverse the graph. - Identify subgraphs matching the pattern of `add -> relu`. - Replace them with `sub -> sigmoid`. 3. Make sure the new graph maintains the correct order and data dependencies. Input: - A `GraphModule` instance containing the FX graph to be transformed. Output: - A transformed `GraphModule` instance with the specified transformations applied. Constraints: - You can assume that the input graph will not contain nested modules. - Ensure that transformations are in-place and maintain the graph\'s structural integrity. Example: ```python import torch import torch.fx as fx class Model(torch.nn.Module): def forward(self, x, y): a = torch.ops.aten.add.Tensor(x, y) b = torch.ops.aten.relu.default(a) return b model = Model() traced_model = fx.symbolic_trace(model) # Your transformation function transformed_graph_module = your_transformation_function(traced_model) print(transformed_graph_module.graph) ``` For the above example, the transformed graph should replace the `add -> relu` pattern with `sub -> sigmoid`. ```plaintext graph(): %x : [#users=1] = placeholder(target=x) %y : [#users=1] = placeholder(target=y) %a : [#users=1] = call_function(target=torch.ops.aten.sub.Tensor, args=(%x, %y)) %b : [#users=1] = call_function(target=torch.ops.aten.sigmoid.default, args=(%a,)) return %b ``` Notes: - You are encouraged to use the `torch.fx` transformation utilities and mechanisms as shown in the provided documentation. - Structure your code properly with comments for clarity.","solution":"import torch import torch.fx as fx def transform_fx_graph(graph_module): # Create a tracer of the original graph tracer = fx.Tracer() graph = fx.Graph() # Create a mapping to keep track of the node replacements node_mapping = {} for node in graph_module.graph.nodes: if node.op == \'call_function\' and node.target == torch.ops.aten.add.Tensor: # Get the next node user = next(iter(node.users)) if user.op == \'call_function\' and user.target == torch.ops.aten.relu.default: # Replace add -> relu with sub -> sigmoid new_add_node = graph.call_function(torch.ops.aten.sub.Tensor, node.args) new_relu_node = graph.call_function(torch.ops.aten.sigmoid.default, (new_add_node,)) node_mapping[node] = new_relu_node node_mapping[user] = new_relu_node continue # Default case, just copy the node new_node = graph.node_copy(node, lambda n: node_mapping.get(n, n)) node_mapping[node] = new_node new_graph_module = fx.GraphModule(graph_module, graph) return new_graph_module"},{"question":"Objective: To assess your understanding and ability to implement a WSGI application using Python\'s `wsgiref` package. Task: Implement a WSGI application that serves requests by performing the following: 1. **Handling HTTP Methods**: - Return a plain-text response \\"Hello, World!\\" for GET requests. - Return a plain-text response \\"POST received with data: <DATA>\\" for POST requests, where `<DATA>` is the request body. 2. **Manipulating Response Headers**: - Set custom response headers, including `X-Custom-Header` with the value `WSGIFuncTest`. 3. **Include Query Parameters**: - If the GET request includes a query parameter `name`, respond with \\"Hello, `<name>`!\\". 4. **WSGI Utility Usage**: - Utilize the `wsgiref.util` module to aid in handling the environment and constructing the response. Implementation Details: - Function Definition: `def wsgi_app(environ, start_response):` - Expected Input: - `environ` (dict): A dictionary containing WSGI environment variables. - `start_response` (callable): A callable accepting a status, a list of response headers, and an optional exception context. - Expected Output: - Return an iterable over byte strings representing the response body. Constraints: - Use only the `wsgiref` package and standard Python libraries. - Ensure compliance with **PEP 3333**. - Include proper error handling and validation. Example Usage: You can test your implementation with the provided simple WSGI server setup. ```python from wsgiref.simple_server import make_server def wsgi_app(environ, start_response): # Your implementation here pass if __name__ == \\"__main__\\": port = 8000 with make_server(\'\', port, wsgi_app) as httpd: print(f\\"Serving on port {port}...\\") httpd.serve_forever() ``` # Example Requests: 1. **GET Request**: - Request: `GET / HTTP/1.1` - Response: - Status: `200 OK` - Headers: `Content-Type: text/plain; charset=utf-8`, `X-Custom-Header: WSGIFuncTest` - Body: `Hello, World!` 2. **GET Request with Query Parameter**: - Request: `GET /?name=John HTTP/1.1` - Response: - Status: `200 OK` - Headers: `Content-Type: text/plain; charset=utf-8`, `X-Custom-Header: WSGIFuncTest` - Body: `Hello, John!` 3. **POST Request**: - Request: `POST / HTTP/1.1` - Body: `SomeData` - Response: - Status: `200 OK` - Headers: `Content-Type: text/plain; charset=utf-8`, `X-Custom-Header: WSGIFuncTest` - Body: `POST received with data: SomeData` Develop a robust solution that fulfills all the criteria mentioned above. Note: You can test your code using tools like `curl` or Postman and ensure it works correctly for different request methods and scenarios.","solution":"from wsgiref.util import setup_testing_defaults from urllib.parse import parse_qs def wsgi_app(environ, start_response): setup_testing_defaults(environ) method = environ[\'REQUEST_METHOD\'] if method == \'GET\': query_params = parse_qs(environ[\'QUERY_STRING\']) if \'name\' in query_params: response_body = f\\"Hello, {query_params[\'name\'][0]}!\\" else: response_body = \\"Hello, World!\\" status = \'200 OK\' elif method == \'POST\': try: request_body_size = int(environ.get(\'CONTENT_LENGTH\', 0)) except (ValueError): request_body_size = 0 request_body = environ[\'wsgi.input\'].read(request_body_size) response_body = f\\"POST received with data: {request_body.decode(\'utf-8\')}\\" status = \'200 OK\' else: response_body = \\"Method Not Allowed\\" status = \'405 Method Not Allowed\' response_headers = [ (\'Content-Type\', \'text/plain; charset=utf-8\'), (\'X-Custom-Header\', \'WSGIFuncTest\') ] start_response(status, response_headers) return [response_body.encode(\'utf-8\')]"},{"question":"# Python Coding Assessment: Working with `tarfile` Module Objective Demonstrate your understanding of the Python `tarfile` module by performing tasks related to creating, listing, and extracting tar archives. Problem Statement You are given a directory containing several files and subdirectories. Your task is to: 1. Create a tar archive of the entire directory using different compression methods. 2. List the contents of the created tar archives. 3. Extract the tar archives to verify they were created correctly. Use the `tarfile` module to complete the following tasks: 1. **Create a tar archive**: - Write a function `create_tar_archive(directory_path: str, archive_name: str, compression: str) -> None` that takes the path of a directory (`directory_path`), the name of the tar archive to be created (`archive_name`), and the compression method (`compression`). The compression method can be one of `\'none\'`, `\'gz\'`, `\'bz2\'`, or `\'xz\'`. - The function should create a tar archive of the directory with the specified compression method. 2. **List the tar archive contents**: - Write a function `list_tar_contents(archive_name: str) -> List[str]` that takes the name of the tar archive (`archive_name`) and returns a list of file names contained in the tar archive. 3. **Extract the tar archive**: - Write a function `extract_tar_archive(archive_name: str, extract_path: str) -> None` that takes the name of the tar archive (`archive_name`) and the path to extract the contents (`extract_path`). - The function should extract the contents of the tar archive to the specified path. Constraints - Assume the directory_path provided is valid and contains files and subdirectories. - All directory and file operations should handle paths in a platform-independent manner (use `os.path` functions). - The compression methods are `\'none\'` for no compression, `\'gz\'` for gzip, `\'bz2\'` for bzip2, and `\'xz\'` for lzma. Examples ```python # Example directory structure: # test_directory/ # ├── file1.txt # ├── file2.txt # └── subdir/ # └── file3.txt # Create tar archive create_tar_archive(\'test_directory\', \'test_archive.tar.gz\', \'gz\') # List tar archive contents print(list_tar_contents(\'test_archive.tar.gz\')) # Output: [\'test_directory/\', \'test_directory/file1.txt\', \'test_directory/file2.txt\', \'test_directory/subdir/\', \'test_directory/subdir/file3.txt\'] # Extract tar archive extract_tar_archive(\'test_archive.tar.gz\', \'extracted_directory\') ``` Implementation ```python import tarfile import os from typing import List def create_tar_archive(directory_path: str, archive_name: str, compression: str) -> None: mode = \'w\' if compression == \'gz\': mode += \':gz\' elif compression == \'bz2\': mode += \':bz2\' elif compression == \'xz\': mode += \':xz\' with tarfile.open(archive_name, mode) as tar: tar.add(directory_path, arcname=os.path.basename(directory_path)) def list_tar_contents(archive_name: str) -> List[str]: with tarfile.open(archive_name, \'r\') as tar: return tar.getnames() def extract_tar_archive(archive_name: str, extract_path: str) -> None: with tarfile.open(archive_name, \'r\') as tar: tar.extractall(path=extract_path) ``` Notes - Make sure to handle errors appropriately, especially when dealing with file operations. - The paths provided in the examples assume a Unix-like file system. Adjust test paths as necessary for other file systems.","solution":"import tarfile import os from typing import List def create_tar_archive(directory_path: str, archive_name: str, compression: str) -> None: mode = \'w\' if compression == \'gz\': mode += \':gz\' elif compression == \'bz2\': mode += \':bz2\' elif compression == \'xz\': mode += \':xz\' with tarfile.open(archive_name, mode) as tar: tar.add(directory_path, arcname=os.path.basename(directory_path)) def list_tar_contents(archive_name: str) -> List[str]: with tarfile.open(archive_name, \'r\') as tar: return tar.getnames() def extract_tar_archive(archive_name: str, extract_path: str) -> None: with tarfile.open(archive_name, \'r\') as tar: tar.extractall(path=extract_path)"},{"question":"# Advanced Python Coding Assessment Understanding and Using the `readline` Module The `readline` module provides command-line editing and history capabilities in Python. In this assessment, you are required to implement a custom interactive Python console that supports command history and tab completion for file paths. # Requirements 1. **Interactive Console Initialization:** - Initialize the console with support for reading and writing command history to a file. - Support for tab completion for file and directory names. 2. **Command History:** - Load previous command history from a history file when the console starts. - Save the command history to the same file when the console ends. 3. **Tab Completion:** - Implement tab completion for file and directory paths in the current working directory. # Input and Output Formats - The history file should be named `.my_python_history` and saved in the user\'s home directory. - When a user types a command, it should be included in the history. - Commands can include standard Python code and shell commands related to file navigation such as `ls`, `cd`, etc. - The console should display prompts and handle user inputs effectively until the user types `exit()` to end the session. # Implementation Guidelines 1. **Initialize History:** - Use the `readline.read_history_file()` to load history and `readline.write_history_file()` to save it. - Implement a mechanism to cap the history to a maximum of 1000 entries. 2. **Configure Tab Completion:** - Use the `readline.set_completer()` to set a custom completion function. - The completion function should handle file and directory name completion. 3. **Interactive Console Loop:** - Use a loop to prompt for user inputs and execute commands. Here’s a basic template to get you started: ```python import os import readline import atexit class CustomInteractiveConsole: def __init__(self): self.histfile = os.path.join(os.path.expanduser(\\"~\\"), \\".my_python_history\\") self.init_history() self.init_completion() def init_history(self): try: readline.read_history_file(self.histfile) except FileNotFoundError: pass readline.set_history_length(1000) atexit.register(readline.write_history_file, self.histfile) def init_completion(self): readline.parse_and_bind(\\"tab: complete\\") readline.set_completer(self.path_completer) def path_completer(self, text, state): # Implement path completion logic here pass def run_console(self): while True: try: user_input = input(\\">>> \\") if user_input.strip() == \\"exit()\\": break # Execute the command exec(user_input) except Exception as e: print(f\\"Error: {e}\\") if __name__ == \\"__main__\\": console = CustomInteractiveConsole() console.run_console() ``` # Additional Details - Your `path_completer` function should return valid completions for the current directory. - Ensure that after entering an invalid command, the console continues to run, and appropriate error messages are displayed. # Constraints - Python 3.10 required. - Only the standard library should be used. - Commands should not modify files outside the current directory or manipulate the system state irreversibly. **Note**: This assessment will test your understanding of Python\'s `readline` module, file handling, function implementation, and error handling within an interactive console environment.","solution":"import os import readline import atexit import glob class CustomInteractiveConsole: def __init__(self): self.histfile = os.path.join(os.path.expanduser(\\"~\\"), \\".my_python_history\\") self.init_history() self.init_completion() def init_history(self): try: readline.read_history_file(self.histfile) except FileNotFoundError: pass readline.set_history_length(1000) atexit.register(readline.write_history_file, self.histfile) def init_completion(self): readline.parse_and_bind(\\"tab: complete\\") readline.set_completer(self.path_completer) def path_completer(self, text, state): Completes filenames for the given text. matches = glob.glob(text + \'*\') if state < len(matches): return matches[state] else: return None def run_console(self): while True: try: user_input = input(\\">>> \\") if user_input.strip() == \\"exit()\\": break exec(user_input) except Exception as e: print(f\\"Error: {e}\\") if __name__ == \\"__main__\\": console = CustomInteractiveConsole() console.run_console()"},{"question":"You have been provided with a set of data files and scripts that need to be included in a Python source distribution. Using the provided documentation, write a Python function `create_manifest` that generates a manifest file based on specific include and exclude patterns. # Function Signature ```python def create_manifest(include_patterns: list, exclude_patterns: list, directory_structure: dict) -> list: pass ``` # Input - `include_patterns`: A list of glob patterns (as strings) to include files (e.g., `[\'*.py\', \'data/*.txt\']`). - `exclude_patterns`: A list of glob patterns (as strings) to exclude files (e.g., `[\'*.md\', \'tests/*\']`). - `directory_structure`: A dictionary representing the directory structure. Keys are directory paths, and values are lists of file names within those directories (e.g., `{\'src\': [\'main.py\', \'utils.py\'], \'data\': [\'data.txt\', \'info.md\']}`). # Output - The function should return a list of files that match the include patterns but not the exclude patterns, considering the given directory structure. # Constraints - The function should handle Unix-style glob patterns as described in the documentation. - Assume the directory structure is correctly provided and no file names contain illegal characters for Unix/Windows systems. - Performance should be efficient in filtering the file list given the patterns. # Example ```python include_patterns = [\'*.py\', \'data/*.txt\'] exclude_patterns = [\'*.md\', \'tests/*\'] directory_structure = { \'src\': [\'main.py\', \'utils.py\'], \'data\': [\'data.txt\', \'info.md\'], \'tests\': [\'test_main.py\', \'test_utils.py\'] } print(create_manifest(include_patterns, exclude_patterns, directory_structure)) # Expected output: [\'src/main.py\', \'src/utils.py\', \'data/data.txt\'] ``` # Notes - The order of files in the output list does not matter. - Utilize the `fnmatch` module to help with pattern matching.","solution":"import fnmatch import os def create_manifest(include_patterns: list, exclude_patterns: list, directory_structure: dict) -> list: all_files = [] # Collect all files based on the directory structure for dir_path, files in directory_structure.items(): for file_name in files: full_path = os.path.join(dir_path, file_name) all_files.append(full_path) # Filter the files based on include patterns included_files = set() for pattern in include_patterns: for file in all_files: if fnmatch.fnmatch(file, pattern): included_files.add(file) # Exclude files based on exclude patterns final_files = set(included_files) for pattern in exclude_patterns: for file in included_files: if fnmatch.fnmatch(file, pattern): if file in final_files: final_files.remove(file) return list(final_files)"},{"question":"Objective You are required to parse XML data using the `xml.etree.ElementTree` module. The task will test your ability to securely handle XML data, ensuring that any potential vulnerabilities are addressed. Task Write a function called `safe_xml_parse` that takes a string of XML data as input and performs the following tasks: 1. Parses the XML data into an `ElementTree`. 2. Ensures that any potentially malicious XML data—such as entities that could lead to billion laughs or external entity expansion—is not processed. 3. Prevents other security risks defined in the XML vulnerabilities section, such as DTD retrieval and large tokens. # Input - `xml_data: str` – A string containing XML data that needs to be parsed. # Output - Returns the root element of the parsed XML tree. # Constraints - The solution must ensure the parsing is secure and not prone to XML vulnerabilities. - You should use the `xml.etree.ElementTree` module. - Assume the XML string will not be extremely large, but must ensure it handles the potential risks mentioned in the documentation. Example ```python xml_string = \'\'\'<root> <element>Some data</element> </root>\'\'\' try: root = safe_xml_parse(xml_string) print(root.tag) # Output should be \'root\' except Exception as e: print(str(e)) ``` Notes - Focus on making the XML parsing secure. - Avoid using any third-party modules aside from standard Python libraries. - Consider scenarios where the XML might contain malicious constructs intended to exploit vulnerabilities. - The function should ideally raise an exception if an insecure or malformed XML is provided.","solution":"import xml.etree.ElementTree as ET from xml.etree.ElementTree import ParseError def safe_xml_parse(xml_data): Securely parses XML data and prevents XML vulnerabilities. Args: xml_data (str): A string containing XML data. Returns: Element: The root element of the parsed XML tree. Raises: ParseError: If the XML is malformed or contains secure vulnerabilities. # Disable XML external entity (XXE) attacks by ensuring resolve_entities is disabled parser = ET.XMLParser(target=ET.TreeBuilder()) try: root = ET.fromstring(xml_data, parser=parser) except ParseError as e: raise return root"},{"question":"# Email Message Parsing Assessment Objective In this assessment, you are required to implement a function that parses an email message from a given input source (bytes, string, or file). You will also extract specific information from the parsed messages. Task Implement the function `parse_email_message(input_source, source_type, policy=None)` that parses an email message and returns a dictionary with the following information: - `headers`: A dictionary of headers where the key is the header name and the value is the header value. - `payload`: The main email body text. - `is_multipart`: A boolean indicating whether the message is multipart. Function Signature ```python from typing import Union, Dict, Optional from email.policy import compat32, Policy def parse_email_message(input_source: Union[bytes, str, \'file\'], source_type: str, policy: Optional[Policy] = compat32) -> Dict[str, Union[Dict[str, str], str, bool]]: pass ``` Input - `input_source`: The input email message which can be of type: - `bytes`: A bytes-like object containing the complete email message. - `str`: A string containing the complete email message. - `file`: A file-like object containing the complete email message. - `source_type`: A string to indicate the type of input source (`\\"bytes\\"`, `\\"string\\"`, `\\"file\\"`). - `policy`: An optional `Policy` object to control parsing behaviors. Output A dictionary with the following keys: - `headers`: A dictionary containing all the email headers. - `payload`: A string containing the main body text of the email. - `is_multipart`: A boolean indicating whether the email is multipart. Constraints - You should handle both MIME and non-MIME messages. - Handle various types of newlines (CR, LF, CRLF) in the input. - The function should properly handle invalid or malformed email messages. Example ```python # Example email input as string email_str = From: test@example.com To: sample@example.com Subject: Test Email Hello, this is a test email message. result = parse_email_message(email_str, source_type=\\"string\\") print(result) # Output: # { # \\"headers\\": { # \\"From\\": \\"test@example.com\\", # \\"To\\": \\"sample@example.com\\", # \\"Subject\\": \\"Test Email\\" # }, # \\"payload\\": \\"Hello, this is a test email message.\\", # \\"is_multipart\\": False # } ``` **Note:** You may assume that the standard email policies are sufficient for most cases. Use the `compat32` policy by default unless specified otherwise.","solution":"from typing import Union, Dict, Optional from email import message_from_bytes, message_from_string, message_from_file from email.policy import compat32, Policy def parse_email_message(input_source: Union[bytes, str, \'file\'], source_type: str, policy: Optional[Policy] = compat32) -> Dict[str, Union[Dict[str, str], str, bool]]: if source_type == \\"bytes\\": msg = message_from_bytes(input_source, policy=policy) elif source_type == \\"string\\": msg = message_from_string(input_source, policy=policy) elif source_type == \\"file\\": msg = message_from_file(input_source, policy=policy) else: raise ValueError(\\"Invalid source_type. Must be \'bytes\', \'string\', or \'file\'.\\") headers = {key: value for key, value in msg.items()} payload = msg.get_payload() is_multipart = msg.is_multipart() return { \\"headers\\": headers, \\"payload\\": payload, \\"is_multipart\\": is_multipart }"},{"question":"# Custom Descriptor for Validated Logging Access and Automatic Attribute Update Task: You are required to implement a custom descriptor class, `ValidatedLogger`, that will: 1. Log all accesses and updates to the managed attribute. 2. Automatically update a related attribute when the managed attribute changes. 3. Validate the data being assigned to the managed attribute. Requirements: 1. The descriptor should be able to dynamically log each access to and update of the managed attribute. 2. Another attribute in the class should be updated automatically whenever the managed attribute changes. 3. The descriptor should validate the data assigned to the managed attribute according to a specified type. Implementation: 1. Define a class, `ValidatedLogger`, which implements the descriptor protocol using `__get__`, `__set__`, and `__set_name__` methods. 2. Log each access and update using the `logging` module. 3. Update a related attribute automatically (e.g., a timestamp attribute indicating the last update time). 4. Validate that the data assigned is of a specific type, raising a `ValueError` if it is not. # Example Usage: ```python import logging from datetime import datetime logging.basicConfig(level=logging.INFO) class ValidatedLogger: def __init__(self, expected_type): self.expected_type = expected_type def __set_name__(self, owner, name): self.public_name = name self.private_name = \'_\' + name def __get__(self, obj, objtype=None): value = getattr(obj, self.private_name) logging.info(f\\"Accessing {self.public_name} giving {value}\\") return value def __set__(self, obj, value): if not isinstance(value, self.expected_type): raise ValueError(f\\"Expected {value!r} to be of type {self.expected_type}\\") logging.info(f\\"Updating {self.public_name} to {value}\\") setattr(obj, self.private_name, value) obj.timestamp = datetime.now() # Automatically update the related attribute class MyClass: attr = ValidatedLogger(int) def __init__(self, attr): self.attr = attr self.timestamp = None # Example usage: obj = MyClass(10) print(obj.attr) # Logs access to \'attr\' obj.attr = 20 # Logs update to \'attr\' and updates \'timestamp\' print(obj.timestamp) try: obj.attr = \\"string\\" # Raises ValueError except ValueError as e: print(e) ``` # Input Format: - The descriptor `ValidatedLogger` should be initialized with an expected type. # Output Format: There is no specific output format. The function should raise appropriate errors for invalid data assignments and log accesses and updates. # Constraints: - The attribute managed by the descriptor should be logged for each access and update. - The related attribute (e.g., `timestamp`) should be updated automatically. - The managed attribute should be validated to match a specified type.","solution":"import logging from datetime import datetime logging.basicConfig(level=logging.INFO) class ValidatedLogger: def __init__(self, expected_type): self.expected_type = expected_type def __set_name__(self, owner, name): self.public_name = name self.private_name = \'_\' + name def __get__(self, obj, objtype=None): value = getattr(obj, self.private_name) logging.info(f\\"Accessing {self.public_name} giving {value}\\") return value def __set__(self, obj, value): if not isinstance(value, self.expected_type): raise ValueError(f\\"Expected {value!r} to be of type {self.expected_type}\\") logging.info(f\\"Updating {self.public_name} to {value}\\") setattr(obj, self.private_name, value) obj.timestamp = datetime.now() # Automatically update the related attribute class MyClass: attr = ValidatedLogger(int) def __init__(self, attr): self.attr = attr self.timestamp = None # Example usage: obj = MyClass(10) print(obj.attr) # Logs access to \'attr\' obj.attr = 20 # Logs update to \'attr\' and updates \'timestamp\' print(obj.timestamp) try: obj.attr = \\"string\\" # Raises ValueError except ValueError as e: print(e)"},{"question":"Advanced Exception Handling in Asynchronous Programming Objective Implement a function to perform an asynchronous file read operation, demonstrating the handling of various exceptions defined in the `asyncio` module. Problem Statement You are required to write a function `async_read_file(file_path: str, buffer_size: int) -> str`. This function must read the content of a given file asynchronously using the `asyncio` module. The function should handle various exceptions that may occur during the read operation. Function Signature ```python import asyncio async def async_read_file(file_path: str, buffer_size: int) -> str: pass ``` Input - `file_path` (str): The path to the file to be read. - `buffer_size` (int): The number of bytes to read from the file at a time. Output - Returns a string containing the content of the file. Constraints 1. The file may not exist or may not be readable, which should trigger appropriate exception handling. 2. You should handle the following exceptions specifically: - `asyncio.TimeoutError` - `asyncio.CancelledError` - `asyncio.InvalidStateError` - `asyncio.SendfileNotAvailableError` - `asyncio.IncompleteReadError` - `asyncio.LimitOverrunError` 3. Other exceptions should be handled with a general exception handler. 4. You must use `asyncio` for the read operation, making the function compatible with asynchronous execution. Performance Requirements - Ensure that the function operates efficiently within the constraints and handles exceptions gracefully. - The function should aim to close any resources after use and should not cause any resource leakages. Example ```python import asyncio # Sample implementation for testing purposes async def async_read_file(file_path: str, buffer_size: int) -> str: try: async with asyncio.open(file_path, \'rb\') as file: content = b\'\' while True: chunk = await asyncio.wait_for(file.read(buffer_size), timeout=2) if not chunk: break content += chunk return content.decode(\'utf-8\') except asyncio.TimeoutError: return \\"Operation timed out.\\" except asyncio.CancelledError: # Perform any cleanup if necessary raise except asyncio.InvalidStateError: return \\"Invalid state encountered.\\" except asyncio.SendfileNotAvailableError: return \\"Sendfile not available.\\" except asyncio.IncompleteReadError as e: return f\\"Incomplete read. Expected {e.expected} bytes, got {len(e.partial)} bytes.\\" except asyncio.LimitOverrunError as e: return f\\"Buffer limit overrun. Consumed: {e.consumed} bytes.\\" except Exception as e: return f\\"An unexpected error occurred: {str(e)}\\" # To test this function, you would run it within an asyncio event loop, e.g.: # asyncio.run(async_read_file(\'example.txt\', 100)) ``` Note: - This example provides a sample structure for the implementation, but you will need to complete or modify it as necessary based on the problem requirements and constraints. - Ensure you have an appropriate setup to test asynchronous code, such as running in an event loop.","solution":"import asyncio async def async_read_file(file_path: str, buffer_size: int) -> str: try: # Using aiofiles for asynchronous file handling import aiofiles async with aiofiles.open(file_path, \'rb\') as file: content = b\'\' while True: chunk = await asyncio.wait_for(file.read(buffer_size), timeout=2) if not chunk: break content += chunk return content.decode(\'utf-8\') except asyncio.TimeoutError: return \\"Operation timed out.\\" except asyncio.CancelledError: # Should re-raise the CancelledError to ensure proper cancelling raise except asyncio.InvalidStateError: return \\"Invalid state encountered.\\" except asyncio.SendfileNotAvailableError: return \\"Sendfile not available.\\" except asyncio.IncompleteReadError as e: return f\\"Incomplete read. Expected {e.expected} bytes, got {len(e.partial)} bytes.\\" except asyncio.LimitOverrunError as e: return f\\"Buffer limit overrun. Consumed: {e.consumed} bytes.\\" except Exception as e: return f\\"An unexpected error occurred: {str(e)}\\""},{"question":"Advanced Dataclass Manipulation Objective: The objective of this task is to assess your understanding and ability to work with Python\'s `dataclasses` module. You will need to apply various features provided by `dataclasses` to implement a well-structured solution following advanced principles such as immutability, default factory functions, post-init processing, and field replacement. Problem Description: You are required to create a system for managing a library of books using dataclasses. Implement a dataclass named `Book` with the following: - `title`: a string representing the title of the book. - `author`: a string representing the author\'s name. - `publication_year`: an integer representing the year the book was published. - `genres`: a list of strings representing the genres of the book (defaulting to an empty list). - `is_checked_out`: a boolean indicating if the book is currently checked out (defaulting to `False`). In addition, you need to implement another dataclass named `Library` with the following: - `books`: a list of `Book` instances (defaulting to an empty list). You will also need to provide the following functionality: 1. **Adding a Book**: A method in the `Library` class to add a new book. 2. **Finding Books by Author**: A method in the `Library` class to find all books by a given author. 3. **Checking Out a Book**: A method in the `Library` class to check out a book by its title, marking `is_checked_out` as `True`. 4. **Checking In a Book**: A method in the `Library` class to check in a book by its title, marking `is_checked_out` as `False`. Make sure to enforce the following constraints using appropriate features of `dataclasses`: - Books should be immutable once created. - Use default factory functions appropriately for mutable default values. Implementation Details: - Define the `Book` dataclass with the required fields and ensure it is immutable. - Define the `Library` dataclass and implement the methods for adding books, finding books by author, and checking books in and out. - Use type annotations and default factory functions where necessary. Example: ```python from dataclasses import dataclass, field from typing import List @dataclass(frozen=True) class Book: title: str author: str publication_year: int genres: List[str] = field(default_factory=list) is_checked_out: bool = False @dataclass class Library: books: List[Book] = field(default_factory=list) def add_book(self, book: Book): self.books.append(book) def find_books_by_author(self, author: str) -> List[Book]: return [book for book in self.books if book.author == author] def check_out_book(self, title: str): for i, book in enumerate(self.books): if book.title == title: new_book = dataclass.replace(book, is_checked_out=True) self.books[i] = new_book return raise ValueError(\\"Book not found\\") def check_in_book(self, title: str): for i, book in enumerate(self.books): if book.title == title: new_book = dataclass.replace(book, is_checked_out=False) self.books[i] = new_book return raise ValueError(\\"Book not found\\") ``` Testing: You should test your implementation with the following: 1. Adding several books to the library. 2. Finding all books by a specific author. 3. Checking out and checking in books, and validating the status changes. Constraints: - The `Book` class must be immutable. - Default values for mutable types should be provided using `default_factory`. - You should not use any external libraries for the book and library management functionality.","solution":"from dataclasses import dataclass, field, replace from typing import List @dataclass(frozen=True) class Book: title: str author: str publication_year: int genres: List[str] = field(default_factory=list) is_checked_out: bool = False @dataclass class Library: books: List[Book] = field(default_factory=list) def add_book(self, book: Book): self.books.append(book) def find_books_by_author(self, author: str) -> List[Book]: return [book for book in self.books if book.author == author] def check_out_book(self, title: str): for i, book in enumerate(self.books): if book.title == title: new_book = replace(book, is_checked_out=True) self.books[i] = new_book return raise ValueError(\\"Book not found\\") def check_in_book(self, title: str): for i, book in enumerate(self.books): if book.title == title: new_book = replace(book, is_checked_out=False) self.books[i] = new_book return raise ValueError(\\"Book not found\\")"},{"question":"Objective Write a function `summarize_class_methods` that takes a Python class as input and returns a summary of its methods. This summary should include the name of each method, its signature, and any annotations on its parameters or return value. Requirements - Use the `inspect` module to retrieve the necessary information. - The summary should be returned as a dictionary with method names as keys. Each value should be another dictionary with keys `signature` and `annotations`. - The `signature` key should map to a string representing the method\'s signature. - The `annotations` key should map to a dictionary where keys are parameter names (including \\"return\\" for return annotation) and values are the corresponding annotations. Input - A Python class. Output - A dictionary summarizing the methods of the given class. Example Given the following class: ```python class Example: def method1(self, x: int, y: str = \\"default\\") -> bool: pass def method2(self, a, *, b: float) -> None: pass ``` Calling `summarize_class_methods(Example)` should return: ```python { \'method1\': { \'signature\': \'(self, x: int, y: str = \'default\') -> bool\', \'annotations\': {\'x\': int, \'y\': str, \'return\': bool} }, \'method2\': { \'signature\': \'(self, a, *, b: float) -> None\', \'annotations\': {\'b\': float, \'return\': None} } } ``` Constraints - Do not include methods inherited from parent classes. - Ignore private methods (those starting with an underscore `_`). Implementation Notes - You may find `inspect.getmembers`, `inspect.isfunction`, and `inspect.signature` particularly useful. - Ensure that the function handles methods with various types of parameters, such as positional, keyword-only, and variable arguments. You are allowed to use standard library imports only. ```python def summarize_class_methods(cls): # Your implementation here pass # Test your function with the Example class print(summarize_class_methods(Example)) ```","solution":"import inspect def summarize_class_methods(cls): summary = {} for name, method in inspect.getmembers(cls, predicate=inspect.isfunction): if name.startswith(\'_\'): continue sig = inspect.signature(method) annotations = {param: annotation for param, annotation in method.__annotations__.items()} method_info = { \'signature\': str(sig), \'annotations\': annotations } summary[name] = method_info return summary"},{"question":"You are given a dataset named `penguins`, which contains information about different species of penguins. Your task is to utilize the Seaborn library to create visualizations and customize their legends. Specifically, you must demonstrate your understanding of the `move_legend` function to change the position and appearance of legends in your plots. Dataset Description: The `penguins` dataset contains the following columns: - `species`: Species of the penguin. - `island`: Island in the Palmer Archipelago where the penguin was observed. - `bill_length_mm`: Length of the penguin\'s bill (in mm). - `bill_depth_mm`: Depth of the penguin\'s bill (in mm). - `flipper_length_mm`: Length of the penguin\'s flipper (in mm). You must use the Seaborn library and the `move_legend` function to complete the following tasks: Task 1: Create a histogram that displays the distribution of `bill_length_mm` for different `species`. - Plot the data using Seaborn\'s `histplot` function. - Use the `hue` parameter to distinguish different species. - Customize the legend by placing it in the `upper left` corner using the `move_legend` function. Task 2: Create a faceted grid of histograms that display the distribution of `bill_length_mm` for different species across different islands. - Plot the data using Seaborn\'s `displot` function, faceting by `island`. - Use the `hue` parameter to distinguish different species. - Customize the legend by placing it in the `lower center` position with the following additional properties: `bbox_to_anchor=(0.5, -0.1)`, `ncol=3`, `title=None`, and `frameon=False`. Input Format: - You do not need to load the dataset, it will be already loaded into the variable `penguins` as a pandas DataFrame. Output Format: - Generate and display the required plots with customized legends. Constraints: - Ensure the legend does not overlap with the data points in the plot. - The legend should be clear and appropriately placed to enhance the plot’s readability. Here’s a template to get you started: ```python import seaborn as sns # Load the dataset penguins = sns.load_dataset(\\"penguins\\") # Task 1: Create a histogram with customized legend placement ax = sns.histplot(penguins, x=\\"bill_length_mm\\", hue=\\"species\\") sns.move_legend(ax, \\"upper left\\") # Task 2: Create a faceted grid of histograms with customized legend placement g = sns.displot( penguins, x=\\"bill_length_mm\\", hue=\\"species\\", col=\\"island\\", col_wrap=2, height=3, facet_kws=dict(legend_out=False), ) sns.move_legend(g, \\"lower center\\", bbox_to_anchor=(0.5, -0.1), ncol=3, title=None, frameon=False) # Ensure the plots are displayed plt.show() ``` Ensure you test your code to verify that it meets all the requirements before submission.","solution":"import seaborn as sns import matplotlib.pyplot as plt # Assuming the dataset is loaded into the variable `penguins` # penguins = sns.load_dataset(\\"penguins\\") # Task 1: Create a histogram with customized legend placement def task1_histogram(penguins): ax = sns.histplot(penguins, x=\\"bill_length_mm\\", hue=\\"species\\") sns.move_legend(ax, \\"upper left\\") plt.show() # Task 2: Create a faceted grid of histograms with customized legend placement def task2_facet_histograms(penguins): g = sns.displot( penguins, x=\\"bill_length_mm\\", hue=\\"species\\", col=\\"island\\", col_wrap=2, height=3, facet_kws=dict(legend_out=False), ) sns.move_legend(g, \\"lower center\\", bbox_to_anchor=(0.5, -0.1), ncol=3, title=None, frameon=False) plt.show()"},{"question":"Objective Design an end-to-end data preprocessing pipeline using scikit-learn transformers and apply it to a classification problem. Dataset You are given a hypothetical dataset with the following structure: - **Features:** - `numeric_feature_1` (numeric) - `numeric_feature_2` (numeric) - `categorical_feature` (categorical with values: \'A\', \'B\', \'C\') - `text_feature` (textual descriptions) - **Target:** - `class_label` (binary classification: 0 or 1) Goal Create a preprocessing pipeline that performs the following operations: 1. **Impute missing values:** - Use the `SimpleImputer` to fill any missing numeric values with the mean of the column. 2. **Standardize numeric features:** - Apply `StandardScaler` to scale numeric features to have mean 0 and variance 1. 3. **Encode categorical feature:** - Use `OneHotEncoder` to convert categorical values into one-hot encoded vectors. 4. **Extract features from text:** - Use `TfidfVectorizer` to transform the text feature into TF-IDF vectors. 5. **Combine features:** - Use `ColumnTransformer` to combine all of these transformations into a single pipeline. 6. **Apply classifier:** - Use `LogisticRegression` to fit the transformed data and predict the class labels. Expected Input and Output - **Input:** - `X_train`: A pandas DataFrame containing the training features. - `y_train`: A pandas Series or array-like containing the training labels. - `X_test`: A pandas DataFrame containing the testing features. - **Output:** - `y_pred`: A numpy array of predicted class labels for the test set. Instructions 1. Implement the preprocessing pipeline as described. 2. Fit the preprocessing pipeline on the training data. 3. Transform the test data and predict the class labels. 4. Return the predicted class labels for the test data. Constraints - You must use scikit-learn\'s transformers and LogisticRegression for the classifier. - Ensure you handle NaN values in numeric features using `SimpleImputer`. - Use appropriate random state for reproducibility where necessary. Code Template ```python import pandas as pd from sklearn.impute import SimpleImputer from sklearn.preprocessing import StandardScaler, OneHotEncoder from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.compose import ColumnTransformer from sklearn.pipeline import Pipeline from sklearn.linear_model import LogisticRegression def preprocess_and_predict(X_train, y_train, X_test): # Define the preprocessing pipeline numeric_features = [\'numeric_feature_1\', \'numeric_feature_2\'] categorical_features = [\'categorical_feature\'] text_features = [\'text_feature\'] numeric_transformer = Pipeline(steps=[ (\'imputer\', SimpleImputer(strategy=\'mean\')), (\'scaler\', StandardScaler()) ]) categorical_transformer = OneHotEncoder(handle_unknown=\'ignore\') text_transformer = TfidfVectorizer() preprocessor = ColumnTransformer( transformers=[ (\'num\', numeric_transformer, numeric_features), (\'cat\', categorical_transformer, categorical_features), (\'text\', text_transformer, \'text_feature\') ]) # Append classifier to preprocessing pipeline clf = Pipeline(steps=[ (\'preprocessor\', preprocessor), (\'classifier\', LogisticRegression(random_state=42)) ]) # Fit the model clf.fit(X_train, y_train) # Predict on test data y_pred = clf.predict(X_test) return y_pred # Example usage (assuming X_train, y_train, and X_test are already defined) # y_pred = preprocess_and_predict(X_train, y_train, X_test) ``` Performance Requirements - The solution should handle datasets of typical size efficiently. - Ensure the pipeline is organized and modular for readability and maintainability.","solution":"import pandas as pd from sklearn.impute import SimpleImputer from sklearn.preprocessing import StandardScaler, OneHotEncoder from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.compose import ColumnTransformer from sklearn.pipeline import Pipeline from sklearn.linear_model import LogisticRegression def preprocess_and_predict(X_train, y_train, X_test): # Define the preprocessing pipeline numeric_features = [\'numeric_feature_1\', \'numeric_feature_2\'] categorical_features = [\'categorical_feature\'] text_features = [\'text_feature\'] numeric_transformer = Pipeline(steps=[ (\'imputer\', SimpleImputer(strategy=\'mean\')), (\'scaler\', StandardScaler()) ]) categorical_transformer = OneHotEncoder(handle_unknown=\'ignore\') text_transformer = TfidfVectorizer() preprocessor = ColumnTransformer( transformers=[ (\'num\', numeric_transformer, numeric_features), (\'cat\', categorical_transformer, categorical_features), (\'text\', text_transformer, \'text_feature\') ]) # Append classifier to preprocessing pipeline clf = Pipeline(steps=[ (\'preprocessor\', preprocessor), (\'classifier\', LogisticRegression(random_state=42)) ]) # Fit the model clf.fit(X_train, y_train) # Predict on test data y_pred = clf.predict(X_test) return y_pred"},{"question":"**Coding Assessment Question:** # Problem Statement You are tasked with handling system-related errors in a robust Python application. Using the `errno` package, you are to write a function that: 1. Accepts an integer `code`. 2. Returns a dictionary containing: - `name`: The standardized name associated with the errno value, or `\\"UNKNOWN\\"` if the code is not recognized. - `exception`: The name of the related built-in Python exception if available or `\\"None\\"` if not directly mapped. - `message`: A human-readable error message for the given code. # Function Signature ```python def get_error_info(code: int) -> dict: ``` # Input - `code` (int): An integer representing the errno value. # Output - A dictionary with keys: - `name` (str): The standardized name associated with the errno value. - `exception` (str): The name of the corresponding built-in Python exception or `\\"None\\"`. - `message` (str): A human-readable error message for the `code`. # Example ```python >>> get_error_info(1) {\'name\': \'EPERM\', \'exception\': \'PermissionError\', \'message\': \'Operation not permitted\'} >>> get_error_info(2) {\'name\': \'ENOENT\', \'exception\': \'FileNotFoundError\', \'message\': \'No such file or directory\'} >>> get_error_info(9999) {\'name\': \'UNKNOWN\', \'exception\': \'None\', \'message\': \'Unknown error 9999\'} ``` # Constraints - You need to use the `errno` and `os` modules available in Python\'s standard library. - You must handle the case where the `code` may not correspond to any known errno value. # Notes - To fetch the error message for a given error code, you can use `os.strerror()`. - Ensure your solution is efficient and effectively handles unexpected input. Good luck!","solution":"import errno import os def get_error_info(code: int) -> dict: Returns a dictionary containing the name, related exception name, and message for a given errno value. error_info = { \'name\': \'UNKNOWN\', \'exception\': \'None\', \'message\': f\'Unknown error {code}\' } # Mapping from errno values to built-in Python exceptions errno_to_exception = { errno.EPERM: \'PermissionError\', errno.ENOENT: \'FileNotFoundError\', errno.ESRCH: \'ProcessLookupError\', errno.EINTR: \'InterruptedError\', errno.EIO: \'OSError\', errno.ENXIO: \'OSError\', errno.E2BIG: \'OSError\', errno.EBADF: \'OSError\', errno.EAGAIN: \'BlockingIOError\', errno.ENOMEM: \'MemoryError\', errno.EACCES: \'PermissionError\', errno.EFAULT: \'MemoryError\' # Add more mappings if needed } if code in errno.errorcode: error_info[\'name\'] = errno.errorcode[code] error_info[\'message\'] = os.strerror(code) if code in errno_to_exception: error_info[\'exception\'] = errno_to_exception[code] return error_info"},{"question":"**Problem Statement:** You are required to implement a function that creates an email message using the `email.message.Message` class. The function should construct an email with specific headers, add a multipart payload, serialize it, and return the string representation. The email should include a mix of text and an attachment. Implement the function `create_email_message(subject, sender, recipient, text, filename, file_content)` that performs the following tasks: 1. **Create a Message Object**: Initialize a `Message` object. 2. **Set Headers**: Set the `From`, `To`, `Subject`, and `MIME-Version` headers. 3. **Create Multipart Container**: Set the content type of the message to `multipart/mixed`. 4. **Attach Text Content**: Create a part for a text body with `text/plain` content type and attach it to the message. 5. **Attach File**: Create a part for an attachment with `application/octet-stream` content type, set the `Content-Disposition` to `attachment`, and attach it to the message. 6. **Serialize Message**: Serialize the entire message to a string format and return it. **Function Signature:** ```python def create_email_message(subject: str, sender: str, recipient: str, text: str, filename: str, file_content: bytes) -> str: pass ``` **Input:** - `subject` (str): The subject of the email. - `sender` (str): The sender\'s email address. - `recipient` (str): The recipient\'s email address. - `text` (str): The body text of the email. - `filename` (str): The name of the file to attach. - `file_content` (bytes): The content of the file to attach. **Output:** - Returns the string representation of the entire email message. **Example:** ```python subject = \\"Test Email\\" sender = \\"sender@example.com\\" recipient = \\"recipient@example.com\\" text = \\"This is a test email.\\" filename = \\"test.txt\\" file_content = b\\"Hello, this is a test file.\\" email_str = create_email_message(subject, sender, recipient, text, filename, file_content) print(email_str) ``` **Note:** - The function should handle setting the appropriate headers and MIME boundaries. - Ensure that both the text and attachment parts are correctly encapsulated as subparts in the multipart message. **Constraints:** - All string inputs will be non-empty and valid email formats for `sender` and `recipient`. - `file_content` should be a non-empty byte sequence.","solution":"import email from email.message import EmailMessage from email.mime.base import MIMEBase from email.mime.multipart import MIMEMultipart from email.mime.text import MIMEText from email import encoders def create_email_message(subject: str, sender: str, recipient: str, text: str, filename: str, file_content: bytes) -> str: # Create the container email message. msg = MIMEMultipart() msg[\'Subject\'] = subject msg[\'From\'] = sender msg[\'To\'] = recipient msg.add_header(\'MIME-Version\', \'1.0\') # Attach the text part. msg.attach(MIMEText(text, \'plain\')) # Create the attachment part. part = MIMEBase(\'application\', \'octet-stream\') part.set_payload(file_content) encoders.encode_base64(part) part.add_header( \'Content-Disposition\', f\'attachment; filename=\\"{filename}\\"\' ) msg.attach(part) # Serialize and return the message. return msg.as_string()"},{"question":"Understanding Numerical Properties in PyTorch Objective To assess students\' understanding of fundamental and advanced features of PyTorch related to numerical properties of data types, including both floating-point and integer types. Task You are required to implement a function `fetch_type_info` that takes in a data type string (`dtype_str`) and returns a dictionary containing the numerical properties of that data type using PyTorch classes `torch.finfo` and `torch.iinfo`. Function Signature ```python import torch def fetch_type_info(dtype_str: str) -> dict: # Your implementation here ``` Input Format - `dtype_str`: A string representing the data type. It can be one of the following strings: - For floating-point types: `\\"float32\\"`, `\\"float64\\"`, `\\"float16\\"`, `\\"bfloat16\\"` - For integer types: `\\"uint8\\"`, `\\"int8\\"`, `\\"int16\\"`, `\\"int32\\"`, `\\"int64\\"` Output Format - A dictionary of numerical properties for the given data type. The dictionary should have the following structure: - For floating-point types: ```python { \'bits\': int, \'eps\': float, \'max\': float, \'min\': float, \'tiny\': float, \'resolution\': float } ``` - For integer types: ```python { \'bits\': int, \'max\': int, \'min\': int } ``` Constraints - The function should raise a `ValueError` if the provided `dtype_str` is not one of the valid strings. Example ```python # Example usage result = fetch_type_info(\\"float32\\") print(result) # Expected output (values may vary slightly) # { # \'bits\': 32, # \'eps\': 1.1920928955078125e-07, # \'max\': 3.4028234663852886e+38, # \'min\': -3.4028234663852886e+38, # \'tiny\': 1.1754943508222875e-38, # \'resolution\': 1e-07 # } result = fetch_type_info(\\"int64\\") print(result) # Expected output # { # \'bits\': 64, # \'max\': 9223372036854775807, # \'min\': -9223372036854775808 # } ``` Notes - Utilize `torch.finfo` for floating-point data types and `torch.iinfo` for integer data types to fetch the required numerical properties. - Ensure the function handles both kinds of data types appropriately. Performance Requirements - The implementation must efficiently fetch and return the numerical properties with minimal overhead.","solution":"import torch def fetch_type_info(dtype_str: str) -> dict: if dtype_str in [\\"float32\\", \\"float64\\", \\"float16\\", \\"bfloat16\\"]: dtype = getattr(torch, dtype_str) info = torch.finfo(dtype) return { \'bits\': info.bits, \'eps\': info.eps, \'max\': info.max, \'min\': info.min, \'tiny\': info.tiny, \'resolution\': info.resolution } elif dtype_str in [\\"uint8\\", \\"int8\\", \\"int16\\", \\"int32\\", \\"int64\\"]: dtype = getattr(torch, dtype_str) info = torch.iinfo(dtype) return { \'bits\': info.bits, \'max\': info.max, \'min\': info.min } else: raise ValueError(f\\"Invalid dtype string: {dtype_str}\\")"},{"question":"In this coding assessment, you are required to implement a secure message authentication function using the `hmac` module. This function will be used to both generate and verify message digests for ensuring data integrity and authenticity. Function 1: `generate_hmac(key: bytes, message: bytes, algorithm: str) -> str` - **Input**: - `key` (bytes): A secret key to use for the HMAC algorithm. - `message` (bytes): The message for which the HMAC is to be generated. - `algorithm` (str): The name of the hash digest algorithm to use (e.g., \\"sha256\\"). - **Output**: - (str): The hex-formatted HMAC digest of the message. - **Constraints**: - The `algorithm` must be a valid hash algorithm supported by the `hashlib` module. - Inputs `key` and `message` should be bytes-like objects. Function 2: `verify_hmac(key: bytes, message: bytes, algorithm: str, hex_digest: str) -> bool` - **Input**: - `key` (bytes): A secret key to use for the HMAC algorithm. - `message` (bytes): The message for which the HMAC needs to be verified. - `algorithm` (str): The name of the hash digest algorithm used to create the HMAC. - `hex_digest` (str): The HMAC hex digest that needs to be verified. - **Output**: - (bool): `True` if the HMAC of the message matches the provided `hex_digest`, `False` otherwise. - **Constraints**: - The `algorithm` must be a valid hash algorithm supported by the `hashlib` module. - Inputs `key` and `message` should be bytes-like objects. - The `hex_digest` should be a valid hexadecimal string. You must also handle potential errors and edge cases, such as: - Invalid hash algorithms. - Mismatched lengths of the provided digest and the expected digest size. Example Usage: ```python key = b\'secret_key\' message = b\'important_message\' algorithm = \'sha256\' # Generating a digest digest = generate_hmac(key, message, algorithm) print(digest) # Outputs a hex string representing the HMAC # Verifying the digest is_valid = verify_hmac(key, message, algorithm, digest) print(is_valid) # Outputs: True ``` **Note**: When verifying the digest, use the `hmac.compare_digest` function to avoid timing attacks. Implement the functions `generate_hmac` and `verify_hmac` below: ```python import hmac import hashlib def generate_hmac(key: bytes, message: bytes, algorithm: str) -> str: # Your implementation here pass def verify_hmac(key: bytes, message: bytes, algorithm: str, hex_digest: str) -> bool: # Your implementation here pass ```","solution":"import hmac import hashlib def generate_hmac(key: bytes, message: bytes, algorithm: str) -> str: Generates an HMAC digest for a message. Args: - key (bytes): A secret key to use for the HMAC algorithm. - message (bytes): The message for which the HMAC is to be generated. - algorithm (str): The name of the hash digest algorithm to use (e.g., \\"sha256\\"). Returns: - str: The hex-formatted HMAC digest of the message. try: hmac_instance = hmac.new(key, message, hashlib.new(algorithm).name) return hmac_instance.hexdigest() except ValueError: raise ValueError(\\"Invalid hash algorithm specified.\\") def verify_hmac(key: bytes, message: bytes, algorithm: str, hex_digest: str) -> bool: Verifies an HMAC digest for a message. Args: - key (bytes): A secret key to use for the HMAC algorithm. - message (bytes): The message for which the HMAC needs to be verified. - algorithm (str): The name of the hash digest algorithm used to create the HMAC. - hex_digest (str): The HMAC hex digest that needs to be verified. Returns: - bool: True if the HMAC of the message matches the provided hex_digest, False otherwise. try: hmac_instance = hmac.new(key, message, hashlib.new(algorithm).name) return hmac.compare_digest(hmac_instance.hexdigest(), hex_digest) except ValueError: raise ValueError(\\"Invalid hash algorithm specified.\\")"},{"question":"# Calendar Data Aggregator Using the Python `calendar` module, write a function `get_aggregated_calendar_data(year)` that generates an aggregated representation of calendar data for a specified year. Your function should return a dictionary with the following structure: ```python { \\"year\\": <year>, \\"months\\": [ { \\"month\\": <month_number>, \\"weeks\\": [ { \\"week_number\\": <N>, \\"days\\": [ {\\"day\\": <day_number>, \\"weekday\\": <weekday_number>} ] } ] } ] } ``` Where: - `<year>` is the input year. - `<month_number>` is the month number (1-12). - `<N>` is the week number within the month (starting from 1). - `<day_number>` is the day of the month, and `<weekday_number>` is the weekday number (0 for Monday, 6 for Sunday). Constraints: 1. The function should use `calendar.Calendar` class to generate the month data. 2. Ignore any days that fall outside the given month (i.e., days represented by 0). Example ```python import pprint pprint.pprint(get_aggregated_calendar_data(2023)) ``` Output (partial): ```python { \\"year\\": 2023, \\"months\\": [ { \\"month\\": 1, \\"weeks\\": [ ... ] }, { \\"month\\": 2, \\"weeks\\": [ ... ] }, ... ] } ``` Notes: - Ensure to include detailed handling of weeks and days within each month. - Use the `itermonthdays2` method to obtain the days and corresponding week days. - Use `monthdayscalendar` to structure weeks properly within a month.","solution":"import calendar def get_aggregated_calendar_data(year): Generate an aggregated representation of calendar data for a specified year. Parameters: year (int): The year for which to generate calendar data. Returns: dict: Dictionary containing the aggregated calendar data. cal = calendar.Calendar() aggregated_data = {\\"year\\": year, \\"months\\": []} for month in range(1, 13): month_data = {\\"month\\": month, \\"weeks\\": []} month_weeks = cal.monthdayscalendar(year, month) for week_num, week in enumerate(month_weeks, start=1): week_data = {\\"week_number\\": week_num, \\"days\\": []} for day in week: if day != 0: # Get weekday number (0 for Monday, 6 for Sunday) weekday = calendar.weekday(year, month, day) week_data[\\"days\\"].append({\\"day\\": day, \\"weekday\\": weekday}) month_data[\\"weeks\\"].append(week_data) aggregated_data[\\"months\\"].append(month_data) return aggregated_data"},{"question":"**Coding Assessment Question:** **Objective:** You are required to demonstrate your comprehension of data manipulation using pandas and advanced plotting using seaborn\'s `objects` interface. Specifically, you will manipulate a dataset to prepare it for plotting and then create a visualization using seaborn. **Dataset:** We\'ll use the famous `tips` dataset available from seaborn\'s dataset collection. **Instructions:** 1. Load the `tips` dataset using seaborn. 2. Create a new column called `tip_percentage` which represents the tip as a percentage of the total bill. 3. Filter the dataset to include only those entries where the `tip_percentage` is greater than 15. 4. Create a seaborn `objects` plot to visualize the relationship between `total_bill`, `tip`, and `day` using the `Paths` mark. 5. The plot should: - Have `total_bill` on the x-axis and `tip` on the y-axis. - Use different colors to differentiate between the days of the week (`day` variable). **Input Format:** No specific input format as you will be directly loading the dataset within the code. **Output Format:** A seaborn plot following the specified requirements. ```python # Your code here import seaborn as sns import seaborn.objects as so # Step 1: Load the tips dataset tips = sns.load_dataset(\\"tips\\") # Step 2: Create a new column \'tip_percentage\' tips[\'tip_percentage\'] = (tips[\'tip\'] / tips[\'total_bill\']) * 100 # Step 3: Filter the dataset for tip_percentage > 15 filtered_tips = tips[tips[\'tip_percentage\'] > 15] # Step 4: Create a seaborn objects plot p = ( so.Plot(filtered_tips) .pair(x=[\\"total_bill\\"], y=[\\"tip\\"]) .layout(size=(8, 5)) .share(x=True, y=True) ) p.add(so.Paths(), color=\\"day\\") # Display the plot p.show() ``` **Constraints:** - Make sure `tip_percentage` calculation is correct and the dataset filtering is applied properly. - The plot should clearly differentiate between different days using colors. **Performance Requirements:** - The solution should be efficient and handle the dataset provided without excessive memory or time consumption.","solution":"import seaborn as sns import seaborn.objects as so import pandas as pd def prepare_and_plot_tips(): # Step 1: Load the tips dataset tips = sns.load_dataset(\\"tips\\") # Step 2: Create a new column \'tip_percentage\' tips[\'tip_percentage\'] = (tips[\'tip\'] / tips[\'total_bill\']) * 100 # Step 3: Filter the dataset for tip_percentage > 15 filtered_tips = tips[tips[\'tip_percentage\'] > 15] # Step 4: Create a seaborn objects plot p = ( so.Plot(filtered_tips) .pair(x=[\\"total_bill\\"], y=[\\"tip\\"]) .layout(size=(8, 5)) .share(x=True, y=True) ) p.add(so.Paths(), color=\\"day\\") # Display the plot p.show() return filtered_tips # Return the filtered DataFrame for testing purposes"},{"question":"**Objective:** Demonstrate your comprehension of seaborn by customizing scatter plots to represent different relationships within a dataset. **Problem Statement:** You are provided with the following dataset representing various attributes of penguins: ```python import seaborn as sns penguins = sns.load_dataset(\\"penguins\\") ``` Your task is to create the following visualizations: 1. **Basic Scatter Plot**: - Plot a scatter plot of `bill_length_mm` vs `bill_depth_mm`. 2. **Scatter Plot with Hue and Style**: - Plot a scatter plot of `bill_length_mm` vs `bill_depth_mm`, where: - Points are colored based on the species of the penguins. - Points are styled (shape of markers) based on the island the penguins are located. 3. **Scatter Plot with Numeric Hue and Size**: - Plot a scatter plot of `bill_length_mm` vs `bill_depth_mm`, where: - Points are colored based on the penguins\' `body_mass_g`. - Points size represents the `flipper_length_mm` of penguins. 4. **FacetGrid Scatter Plot (relplot)**: - Create a scatter plot of `bill_length_mm` vs `bill_depth_mm` using `relplot`, where: - Facets should be created for each unique value of the `species`. - Points should be colored based on the island. Implement all the plots in a single script, ensuring proper labeling, legends, and customization for each visualization. # Input - You will use the seaborn `penguins` dataset. # Output - Four scatter plots as described above. # Constraints - Use seaborn and matplotlib packages for visualization. - Customize each plot appropriately for clarity and visual appeal. # Example ```python import seaborn as sns import matplotlib.pyplot as plt # Load dataset penguins = sns.load_dataset(\\"penguins\\") # 1. Basic Scatter Plot sns.scatterplot(data=penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\") plt.show() # 2. Scatter Plot with Hue and Style sns.scatterplot(data=penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", hue=\\"species\\", style=\\"island\\") plt.show() # 3. Scatter Plot with Numeric Hue and Size sns.scatterplot(data=penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", hue=\\"body_mass_g\\", size=\\"flipper_length_mm\\", sizes=(20, 200)) plt.show() # 4. FacetGrid Scatter Plot (relplot) sns.relplot(data=penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", col=\\"species\\", hue=\\"island\\", kind=\\"scatter\\") plt.show() ``` Ensure your visualizations are explanatory and intuitive by labeling axes and adding legends where appropriate. You may refer to seaborn documentation for additional customizations as needed.","solution":"import seaborn as sns import matplotlib.pyplot as plt # Load dataset penguins = sns.load_dataset(\\"penguins\\") # 1. Basic Scatter Plot plt.figure(figsize=(8, 6)) sns.scatterplot(data=penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\") plt.title(\'Bill Length vs Bill Depth\') plt.xlabel(\'Bill Length (mm)\') plt.ylabel(\'Bill Depth (mm)\') plt.show() # 2. Scatter Plot with Hue and Style plt.figure(figsize=(8, 6)) sns.scatterplot(data=penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", hue=\\"species\\", style=\\"island\\") plt.title(\'Bill Length vs Bill Depth by Species and Island\') plt.xlabel(\'Bill Length (mm)\') plt.ylabel(\'Bill Depth (mm)\') plt.legend(title=\'Species and Island\') plt.show() # 3. Scatter Plot with Numeric Hue and Size plt.figure(figsize=(8, 6)) sns.scatterplot(data=penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", hue=\\"body_mass_g\\", size=\\"flipper_length_mm\\", sizes=(20, 200)) plt.title(\'Bill Length vs Bill Depth by Body Mass and Flipper Length\') plt.xlabel(\'Bill Length (mm)\') plt.ylabel(\'Bill Depth (mm)\') plt.legend(title=\'Body Mass (g) and Flipper Length (mm)\', loc=\'upper right\') plt.show() # 4. FacetGrid Scatter Plot (relplot) sns.relplot(data=penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", col=\\"species\\", hue=\\"island\\", kind=\\"scatter\\") plt.suptitle(\'Bill Length vs Bill Depth by Species and Island\') plt.show()"},{"question":"**Objective:** You are required to demonstrate your understanding of the `seaborn` library based on the given dataset and plot requirements. This task will ensure you can handle both fundamental and advanced visualization concepts within `seaborn`. **Dataset:** You will be using the `tips` dataset from seaborn\'s built-in datasets. **Task:** 1. Load the `tips` dataset from `seaborn`. 2. Create a strip plot where `total_bill` is plotted on the x-axis and the days of the week are plotted on the y-axis. 3. Use the `hue` parameter to distinguish between different times of day (`\'Dinner\'` and `\'Lunch\'`). 4. Customize the plot with the following specifics: - Disable the jitter. - Use the `dodge` parameter to separate points for each hue level. - Set the marker size to 8 and use diamond (\\"D\\") shapes for the markers. - Add transparency to the markers with `alpha` set to 0.7. - Ensure a clear visual distinction by setting an appropriate palette. 5. Additionally, use `catplot` to create a facet plot, splitting the strip plot by `sex` and displaying it across different columns. **Input:** None (the dataset is loaded from `seaborn` directly). **Output:** Display the two plots: one strip plot with the customizations mentioned above, and a facet plot. **Constraints:** - Ensure the plots are well-labeled with appropriate titles, x and y labels, making it easy to distinguish between different categories. **Expected Code Structure:** ```python import seaborn as sns import matplotlib.pyplot as plt # Load the tips dataset tips = sns.load_dataset(\\"tips\\") # Create the customized strip plot plt.figure(figsize=(10, 6)) sns.stripplot(data=tips, x=\\"total_bill\\", y=\\"day\\", hue=\\"time\\", jitter=False, dodge=True, marker=\\"D\\", s=8, alpha=0.7, palette=\\"Set2\\") plt.title(\\"Strip Plot of Total Bill by Day with Time as Hue\\") plt.xlabel(\\"Total Bill\\") plt.ylabel(\\"Day\\") plt.legend(title=\\"Time\\") plt.show() # Create the facet plot using catplot facet_plot = sns.catplot(data=tips, x=\\"total_bill\\", y=\\"day\\", hue=\\"time\\", col=\\"sex\\", kind=\\"strip\\", jitter=False, dodge=True, marker=\\"D\\", s=8, alpha=0.7, palette=\\"Set2\\", aspect=0.7) facet_plot.set_titles(\\"{col_name} Customers\\") facet_plot.set_axis_labels(\\"Total Bill\\", \\"Day\\") facet_plot.add_legend(title=\\"Time\\") plt.show() ``` **Performance Requirements:** - The plots should be generated efficiently, handling the dataset size comfortably. The visualizations should render within a reasonable time frame, avoiding long delays.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_strip_plot(): # Load the tips dataset tips = sns.load_dataset(\\"tips\\") # Create the customized strip plot plt.figure(figsize=(10, 6)) sns.stripplot(data=tips, x=\\"total_bill\\", y=\\"day\\", hue=\\"time\\", jitter=False, dodge=True, marker=\\"D\\", s=8, alpha=0.7, palette=\\"Set2\\") plt.title(\\"Strip Plot of Total Bill by Day with Time as Hue\\") plt.xlabel(\\"Total Bill\\") plt.ylabel(\\"Day\\") plt.legend(title=\\"Time\\") plt.show() # Create the facet plot using catplot facet_plot = sns.catplot(data=tips, x=\\"total_bill\\", y=\\"day\\", hue=\\"time\\", col=\\"sex\\", kind=\\"strip\\", jitter=False, dodge=True, marker=\\"D\\", s=8, alpha=0.7, palette=\\"Set2\\", aspect=0.7) facet_plot.set_titles(\\"{col_name} Customers\\") facet_plot.set_axis_labels(\\"Total Bill\\", \\"Day\\") facet_plot.add_legend(title=\\"Time\\") plt.show()"},{"question":"# Module Import Analyzer You will need to use the `modulefinder` module to analyze the import dependencies of a Python script and generate a detailed report. Objective Create a function `generate_import_report` that takes the path of a Python script as input and returns a dictionary with two keys: 1. `\\"loaded_modules\\"`: A dictionary where keys are module names that were successfully imported and values are lists of the first 3 global names found in each module (or fewer if the module has less than 3 global names). 2. `\\"missing_modules\\"`: A list of modules that were attempted to be imported but were not found. Constraints and Requirements - You may assume that the script path provided as input is always a valid path to a Python script. - You should not modify the script\'s content or add any imports to it. - Performance: The solution should handle a script with up to 100 module imports efficiently. Function Signature ```python def generate_import_report(script_path: str) -> dict: pass ``` Example Usage Consider a script `test_script.py` with the following content: ```python # test_script.py import os import sys try: import nonexistent_module except ImportError: pass def example_function(): pass ``` Calling `generate_import_report(\\"test_script.py\\")` should return a dictionary like this: ```python { \\"loaded_modules\\": { \\"os\\": [ ... list of first 3 global names in os ... ], \\"sys\\": [ ... list of first 3 global names in sys ... ], \\"__main__\\": [ \'sys\', \'example_function\', \'nonexistent_module\' ] }, \\"missing_modules\\": [ \\"nonexistent_module\\" ] } ``` Hints - Use the `ModuleFinder` class from the `modulefinder` module to analyze script imports. - Utilize `finder.modules` to access successfully loaded modules and their global names. - Use `finder.badmodules` to get the list of missing (not imported) modules. Write your implementation in the function `generate_import_report` below.","solution":"import modulefinder def generate_import_report(script_path: str) -> dict: Analyzes the import dependencies of a Python script and generates a report. :param script_path: Path to the Python script. :return: A dictionary with keys \'loaded_modules\' and \'missing_modules\'. finder = modulefinder.ModuleFinder() finder.run_script(script_path) loaded_modules = {} for name, module in finder.modules.items(): global_names = list(module.globalnames.keys()) loaded_modules[name] = global_names[:3] missing_modules = list(finder.badmodules.keys()) return { \\"loaded_modules\\": loaded_modules, \\"missing_modules\\": missing_modules }"},{"question":"**Question: Exploring Penguin Data with Seaborn Object-Oriented Interface** You are given a dataset containing measurements and categorical data for penguins. Your task is to create multiple visualizations using the seaborn library to explore and present key features of this dataset. # Dataset: `penguins` The dataset has the following columns: - `species`: The species of each penguin (e.g., Adelie, Chinstrap, Gentoo). - `island`: The island where each penguin was observed (e.g., Torgersen, Biscoe, Dream). - `bill_length_mm`: Measurement of the penguin\'s bill length, in millimeters. - `bill_depth_mm`: Measurement of the penguin\'s bill depth, in millimeters. - `flipper_length_mm`: Measurement of the penguin\'s flipper length, in millimeters. - `body_mass_g`: Body mass of the penguin, in grams. - `sex`: Sex of the penguin (e.g., Male, Female). Using this dataset, perform the following tasks: 1. **Create a bar plot** that shows the count of penguins on each island. 2. **Create a histogram** of the penguins\' flipper lengths using 20 bins. 3. **Create a normalized histogram** of the penguins\' flipper lengths, normalized to show proportions. 4. **Create faceted histograms** of flipper lengths, separated by island, normalized to show proportions. 5. **Create faceted histograms with individual normalization** within each island. 6. **Create a colored area plot** for flipper lengths, differentiated by the sex of the penguins. 7. **Create a stacked bar plot** to represent the distribution of penguin sexes across different islands. # Implementation Write a function `explore_penguin_data()` that performs these tasks. The function should use the seaborn object-oriented interface with appropriate settings. Function Signature: ```python def explore_penguin_data(): import seaborn.objects as so from seaborn import load_dataset import matplotlib.pyplot as plt # 1. Create a bar plot to show the count of penguins on each island penguins = load_dataset(\\"penguins\\") p1 = so.Plot(penguins, \\"island\\").add(so.Bar(), so.Hist()) p1.show() # 2. Create a histogram of the flipper lengths with 20 bins p2 = so.Plot(penguins, \\"flipper_length_mm\\").add(so.Bars(), so.Hist(bins=20)) p2.show() # 3. Create a normalized histogram of flipper lengths (proportions) p3 = so.Plot(penguins, \\"flipper_length_mm\\").add(so.Bars(), so.Hist(stat=\\"proportion\\")) p3.show() # 4. Create faceted histograms of flipper lengths, separated by island, normalized (proportions) p4 = so.Plot(penguins, \\"flipper_length_mm\\").facet(\\"island\\").add(so.Bars(), so.Hist(stat=\\"proportion\\")) p4.show() # 5. Create faceted histograms of flipper lengths, separated by island, independently normalized p5 = so.Plot(penguins, \\"flipper_length_mm\\").facet(\\"island\\").add(so.Bars(), so.Hist(stat=\\"proportion\\", common_norm=False)) p5.show() # 6. Create a colored area plot of flipper lengths, differentiated by sex p6 = so.Plot(penguins, \\"flipper_length_mm\\", color=\\"sex\\").add(so.Area(), so.Hist()) p6.show() # 7. Create a stacked bar plot to represent penguin sex distribution across islands p7 = so.Plot(penguins, x=\\"island\\", color=\\"sex\\").add(so.Bars(), so.Hist(), so.Stack()) p7.show() ``` Constraints and Considerations: - Ensure you correctly use facet and normalization parameters as described. - All plots should be displayed using `plt.show()` to ensure they render in a Jupyter Notebook or script. - Handle any missing data gracefully if necessary. # Submission Submit the complete function `explore_penguin_data()` along with any additional helper functions or modules you import and use.","solution":"import seaborn as sns import matplotlib.pyplot as plt def explore_penguin_data(): # Load the penguins dataset penguins = sns.load_dataset(\\"penguins\\") # 1. Bar plot showing the count of penguins on each island plt.figure(figsize=(10, 6)) sns.countplot(data=penguins, x=\'island\') plt.title(\'Count of Penguins on Each Island\') plt.ylabel(\'Count\') plt.xlabel(\'Island\') plt.show() # 2. Histogram of the flipper lengths with 20 bins plt.figure(figsize=(10, 6)) sns.histplot(penguins[\'flipper_length_mm\'], bins=20, kde=False) plt.title(\'Histogram of Penguin Flipper Lengths (20 bins)\') plt.xlabel(\'Flipper Length (mm)\') plt.ylabel(\'Frequency\') plt.show() # 3. Normalized histogram of flipper lengths (proportions) plt.figure(figsize=(10, 6)) sns.histplot(penguins[\'flipper_length_mm\'], bins=20, kde=False, stat=\'proportion\') plt.title(\'Normalized Histogram of Penguin Flipper Lengths\') plt.xlabel(\'Flipper Length (mm)\') plt.ylabel(\'Proportion\') plt.show() # 4. Faceted histograms of flipper lengths, separated by island, normalized (proportions) g = sns.FacetGrid(penguins, col=\'island\', sharey=True) g.map(sns.histplot, \'flipper_length_mm\', bins=20, kde=False, stat=\'proportion\') g.set_axis_labels(\'Flipper Length (mm)\', \'Proportion\') g.set_titles(\'{col_name} Island\') plt.show() # 5. Faceted histograms of flipper lengths, separated by island, independently normalized g = sns.FacetGrid(penguins, col=\'island\', sharey=False) g.map(sns.histplot, \'flipper_length_mm\', bins=20, kde=False, stat=\'proportion\') g.set_axis_labels(\'Flipper Length (mm)\', \'Proportion\') g.set_titles(\'{col_name} Island\') plt.show() # 6. Colored area plot of flipper lengths, differentiated by sex plt.figure(figsize=(10, 6)) sns.histplot(data=penguins, x=\'flipper_length_mm\', hue=\'sex\', multiple=\'layer\', element=\'step\', fill=True) plt.title(\'Area Plot of Flipper Lengths by Sex\') plt.xlabel(\'Flipper Length (mm)\') plt.ylabel(\'Count\') plt.show() # 7. Stacked bar plot representing distribution of penguin sexes across different islands penguin_sex_islands = penguins.groupby([\'island\', \'sex\']).size().reset_index(name=\'count\') plt.figure(figsize=(10, 6)) sns.barplot(x=\'island\', y=\'count\', hue=\'sex\', data=penguin_sex_islands, ci=None) plt.title(\'Sex Distribution of Penguins on Each Island\') plt.xlabel(\'Island\') plt.ylabel(\'Count\') plt.show()"},{"question":"Title: File System Explorer Using `stat` Module Objective Write a Python function that explores a given directory recursively, gathers certain metadata about each file, and returns a summary. This assessment will test your understanding of the `stat` module and your ability to manipulate and interpret file metadata using its constants and functions. Function Signature ```python def explore_directory(directory_path: str) -> dict: pass ``` Input - `directory_path` (str): The path to the directory that needs to be explored. Output - A dictionary with the following structure: ```python { \\"total_files\\": int, # Total number of regular files \\"total_directories\\": int, # Total number of directories \\"details\\": [ { \\"path\\": str, # Full path to the file or directory \\"is_directory\\": bool, # True if it is a directory, False otherwise \\"is_regular_file\\": bool, # True if it is a regular file, False otherwise \\"permissions\\": str # String representation of the permissions (e.g., \'-rwxr-xr-x\') }, ... ] } ``` Constraints - The function should handle potential errors gracefully, such as permission errors or invalid paths. - Assume the input directory path is valid and accessible. - The function must use `os.lstat()` and the `stat` module to gather file metadata. Example ```python # Example directory structure: # /test_dir # |-- sub_dir_1 # | |-- file_1.txt # |-- sub_dir_2 # | |-- file_2.txt explore_directory(\'/test_dir\') ``` The output for the above example might be: ```python { \\"total_files\\": 2, \\"total_directories\\": 3, \\"details\\": [ { \\"path\\": \\"/test_dir\\", \\"is_directory\\": True, \\"is_regular_file\\": False, \\"permissions\\": \\"drwxr-xr-x\\" }, { \\"path\\": \\"/test_dir/sub_dir_1\\", \\"is_directory\\": True, \\"is_regular_file\\": False, \\"permissions\\": \\"drwxr-xr-x\\" }, { \\"path\\": \\"/test_dir/sub_dir_1/file_1.txt\\", \\"is_directory\\": False, \\"is_regular_file\\": True, \\"permissions\\": \\"-rw-r--r--\\" }, { \\"path\\": \\"/test_dir/sub_dir_2\\", \\"is_directory\\": True, \\"is_regular_file\\": False, \\"permissions\\": \\"drwxr-xr-x\\" }, { \\"path\\": \\"/test_dir/sub_dir_2/file_2.txt\\", \\"is_directory\\": False, \\"is_regular_file\\": True, \\"permissions\\": \\"-rw-r--r--\\" } ] } ``` # Notes 1. Use recursion to explore directories. 2. Use `os.lstat()` to get file metadata. 3. Use `stat` module functions to determine file type and permissions. # Hint Consider creating helper functions to: - Recursively explore the directory and accumulate metadata. - Extract and format necessary file metadata using `stat` constants and functions.","solution":"import os import stat def file_permissions(mode): Converts a file\'s mode to a string representation of its permissions, similar to `ls -l`. perms = [\'-\'] * 10 perm_mapping = { \'0\': \'---\', \'1\': \'--x\', \'2\': \'-w-\', \'3\': \'-wx\', \'4\': \'r--\', \'5\': \'r-x\', \'6\': \'rw-\', \'7\': \'rwx\', } perms[0] = \'d\' if stat.S_ISDIR(mode) else \'-\' perms[1:4] = perm_mapping[str((mode & 0o700) >> 6)] perms[4:7] = perm_mapping[str((mode & 0o070) >> 3)] perms[7:10] = perm_mapping[str(mode & 0o007)] return \'\'.join(perms) def explore_directory(directory_path: str) -> dict: result = { \\"total_files\\": 0, \\"total_directories\\": 0, \\"details\\": [] } def explore(path): try: stat_info = os.lstat(path) is_directory = stat.S_ISDIR(stat_info.st_mode) is_regular_file = stat.S_ISREG(stat_info.st_mode) entry_details = { \\"path\\": path, \\"is_directory\\": is_directory, \\"is_regular_file\\": is_regular_file, \\"permissions\\": file_permissions(stat_info.st_mode) } result[\\"details\\"].append(entry_details) if is_directory: result[\\"total_directories\\"] += 1 for entry in os.listdir(path): explore(os.path.join(path, entry)) elif is_regular_file: result[\\"total_files\\"] += 1 except Exception as e: print(f\\"Error processing {path}: {e}\\") explore(directory_path) return result"},{"question":"# Seaborn Histogram Customization You are provided with the `penguins` dataset. Using the Seaborn library, your task is to create a customized histogram to visualize the distribution of `flipper_length_mm` with the following specifications: 1. Load the `penguins` dataset using `sns.load_dataset(\\"penguins\\")`. 2. Create a histogram of the `flipper_length_mm` variable. 3. Use hue to differentiate between the `species`. 4. Implement the following customizations: - Set the number of bins to 20. - Stack the histograms of different species. - Use a step function (`element=\'step\'`) to represent the histograms. - Normalize the histograms so that bar heights represent densities (`stat=\'density\'`). - Add a kernel density estimate (KDE) to the plot. 5. Set logarithmic x-scale for the plot due to expected skewness. 6. Include a title and labels for the axes. # Input Format: None (All code is directly run in the notebook). # Output Format: A Seaborn plot with the above customizations. # Constraints: - Ensure the code runs without errors and uses only the seaborn library for plotting and customization. # Example: ```python import seaborn as sns import matplotlib.pyplot as plt # Load the dataset penguins = sns.load_dataset(\\"penguins\\") # Create a histogram with the given specifications sns.histplot( data=penguins, x=\\"flipper_length_mm\\", hue=\\"species\\", bins=20, multiple=\\"stack\\", element=\\"step\\", stat=\\"density\\", kde=True, log_scale=(True, False) ) # Set the title and labels plt.title(\\"Distribution of Flipper Lengths by Penguin Species\\") plt.xlabel(\\"Flipper Length (mm)\\") plt.ylabel(\\"Density\\") # Show the plot plt.show() ``` # Additional Instructions: - Your plot should be similar to the one described above, satisfying all the criteria. - Test your code to ensure it generates the correct plot.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_custom_histogram(): # Load the dataset penguins = sns.load_dataset(\\"penguins\\") # Create a histogram with the given specifications sns.histplot( data=penguins, x=\\"flipper_length_mm\\", hue=\\"species\\", bins=20, multiple=\\"stack\\", element=\\"step\\", stat=\\"density\\", kde=True, log_scale=(True, False) ) # Set the title and labels plt.title(\\"Distribution of Flipper Lengths by Penguin Species\\") plt.xlabel(\\"Flipper Length (mm)\\") plt.ylabel(\\"Density\\") # Show the plot plt.show() # Execute the function to generate the plot create_custom_histogram()"},{"question":"# Support Vector Machines (SVM) with Custom Kernel Objective: Implement a multi-class classifier using Support Vector Machines (SVM) with a custom kernel function. The classifier should handle unbalanced datasets and include hyperparameter tuning using GridSearchCV. Finally, evaluate the model performance using cross-validation. Problem Statement: You are provided with an unbalanced multiclass dataset. Your task is to: 1. Implement a custom kernel function. 2. Create a multi-class SVM classifier using this custom kernel. 3. Handle the dataset\'s imbalance using class weights. 4. Perform hyperparameter tuning using GridSearchCV. 5. Evaluate the classifier\'s performance using cross-validation. Data Format: You are given two numpy arrays: - `X`: A 2D array of shape `(n_samples, n_features)` containing the feature vectors. - `y`: A 1D array of shape `(n_samples,)` containing the class labels. Input: 1. `X` : 2D numpy array of shape `(n_samples, n_features)`. 2. `y` : 1D numpy array of shape `(n_samples,)`. Output: 1. A dictionary containing the best parameters after GridSearchCV. 2. Cross-validation scores. Steps: 1. **Custom Kernel Function**: Define a custom kernel function. 2. **Multi-class SVM Classifier**: Initialize the SVM classifier using the custom kernel function. 3. **Handle Unbalanced Data**: Use class weights to handle the imbalance in the dataset. 4. **GridSearchCV for Hyperparameter Tuning**: Implement GridSearchCV to find the best hyperparameters. 5. **Evaluate with Cross-Validation**: Evaluate the model performance with cross-validation. Constraints: - Use the scikit-learn library for SVM implementation. - Use GridSearchCV for hyperparameter tuning and cross-validation. - Custom Kernel should be different from the existing ones (linear, polynomial, rbf, sigmoid). Example: ```python import numpy as np from sklearn.svm import SVC from sklearn.model_selection import GridSearchCV, cross_val_score from sklearn.utils.class_weight import compute_class_weight # Step 1: Define a custom kernel function def custom_kernel(X, Y): return np.dot(X, Y.T) ** 2 # Example kernel implementation # Step 2: Initialize the SVM classifier with the custom kernel clf = SVC(kernel=custom_kernel) # Step 3: Handle unbalanced data with class weights class_weights = compute_class_weight(\'balanced\', classes=np.unique(y), y=y) class_weight_dict = dict(enumerate(class_weights)) # Step 4: GridSearchCV for hyperparameter tuning param_grid = {\'C\': [0.1, 1, 10], \'gamma\': [0.001, 0.01, 0.1]} grid_search = GridSearchCV(clf, param_grid, cv=5) grid_search.fit(X, y, sample_weight=class_weights) best_params = grid_search.best_params_ # Step 5: Evaluate with Cross-Validation cv_scores = cross_val_score(SVC(**best_params, kernel=custom_kernel), X, y, cv=5) # Output output = { \'best_params\': best_params, \'cv_scores\': cv_scores.tolist() } print(output) ``` Note: You can use the example given for implementation references but make sure to customize and explore more on kernel function and different parameters in GridSearchCV.","solution":"import numpy as np from sklearn.svm import SVC from sklearn.model_selection import GridSearchCV, cross_val_score from sklearn.utils.class_weight import compute_class_weight # Step 1: Define a custom kernel function def custom_kernel(X, Y): Example of a custom kernel function: Polynomial kernel (degree 2) return (np.dot(X, Y.T) + 1) ** 2 def multi_class_svm_with_custom_kernel(X, y): # Step 2: Initialize the SVM classifier with the custom kernel clf = SVC(kernel=custom_kernel) # Step 3: Handle unbalanced data with class weights class_weights = compute_class_weight(\'balanced\', classes=np.unique(y), y=y) class_weight_dict = {i: weight for i, weight in enumerate(class_weights)} # Step 4: GridSearchCV for hyperparameter tuning param_grid = {\'C\': [0.1, 1, 10], \'gamma\': [\'scale\', \'auto\']} grid_search = GridSearchCV(clf, param_grid, cv=5, scoring=\'accuracy\') grid_search.fit(X, y, sample_weight=[class_weight_dict[i] for i in y]) best_params = grid_search.best_params_ # Step 5: Evaluate with Cross-Validation best_estimator = grid_search.best_estimator_ cv_scores = cross_val_score(best_estimator, X, y, cv=5, scoring=\'accuracy\') # Output return { \'best_params\': best_params, \'cv_scores\': cv_scores.tolist() }"},{"question":"You are provided with a dataset containing information about different species of penguins. Using the Seaborn library, your task is to create a series of distribution plots to visualize certain aspects of the dataset. # Input The input dataset contains the following columns: 1. `species`: The species of the penguin. 2. `island`: The island where the penguin was observed. 3. `bill_length_mm`: The length of the penguin\'s bill in millimeters. 4. `bill_depth_mm`: The depth of the penguin\'s bill in millimeters. 5. `flipper_length_mm`: The length of the penguin\'s flipper in millimeters. 6. `body_mass_g`: The body mass of the penguin in grams. 7. `sex`: The sex of the penguin. Example of loading the dataset: ```python import seaborn as sns penguins = sns.load_dataset(\\"penguins\\") ``` # Tasks 1. **Histograms and KDE**: Create a histogram and a KDE plot for the `flipper_length_mm` column. Ensure that the histogram includes a KDE curve. 2. **ECDF Plot**: Create an ECDF plot for the `bill_length_mm` column. 3. **Bivariate KDE Plot**: Create a bivariate KDE plot for `flipper_length_mm` versus `bill_length_mm`. 4. **FacetGrid Plot**: Create a faceted KDE plot showing the density of `flipper_length_mm` distribution for different `species`. Each facet should correspond to a different `species`, and the plots should be separated by the `sex` of the penguins. 5. **Customize FacetGrid**: Enhance the faceted KDE plot created in step 4 by setting the height to 5, aspect ratio to 0.8, and adding appropriate axis labels and titles. # Output Generate the plots as described in the tasks above. The `FacetGrid` plot should be customized using the provided methods. # Example ```python import seaborn as sns import matplotlib.pyplot as plt # Load data penguins = sns.load_dataset(\\"penguins\\") # Task 1: Histogram and KDE sns.displot(data=penguins, x=\\"flipper_length_mm\\", kde=True) plt.show() # Task 2: ECDF Plot sns.displot(data=penguins, x=\\"bill_length_mm\\", kind=\\"ecdf\\") plt.show() # Task 3: Bivariate KDE Plot sns.displot(data=penguins, x=\\"flipper_length_mm\\", y=\\"bill_length_mm\\", kind=\\"kde\\") plt.show() # Task 4: FacetGrid KDE Plot g = sns.displot(data=penguins, x=\\"flipper_length_mm\\", hue=\\"species\\", col=\\"sex\\", kind=\\"kde\\") plt.show() # Task 5: Customize FacetGrid g = sns.displot(data=penguins, x=\\"flipper_length_mm\\", hue=\\"species\\", col=\\"sex\\", kind=\\"kde\\", height=5, aspect=0.8) g.set_axis_labels(\\"Density\\", \\"Flipper length (mm)\\") g.set_titles(\\"{col_name} penguins\\") plt.show() ``` **Constraints:** - Ensure you handle any `NaN` values appropriately. - Use clear and informative labels for the axes and titles for the plots where specified. The solution provided should meet all the requirements specified in the tasks and constraints.","solution":"import seaborn as sns import matplotlib.pyplot as plt def plot_penguin_data(): # Load data penguins = sns.load_dataset(\\"penguins\\") # Handle NaN values by dropping them penguins = penguins.dropna() # Task 1: Histogram and KDE sns.displot(data=penguins, x=\\"flipper_length_mm\\", kde=True) plt.xlabel(\\"Flipper Length (mm)\\") plt.title(\\"Histogram and KDE of Flipper Length\\") plt.show() # Task 2: ECDF Plot sns.displot(data=penguins, x=\\"bill_length_mm\\", kind=\\"ecdf\\") plt.xlabel(\\"Bill Length (mm)\\") plt.title(\\"ECDF of Bill Length\\") plt.show() # Task 3: Bivariate KDE Plot sns.displot(data=penguins, x=\\"flipper_length_mm\\", y=\\"bill_length_mm\\", kind=\\"kde\\") plt.xlabel(\\"Flipper Length (mm)\\") plt.ylabel(\\"Bill Length (mm)\\") plt.title(\\"Bivariate KDE of Flipper Length vs Bill Length\\") plt.show() # Task 4: FacetGrid KDE Plot g = sns.displot(data=penguins, x=\\"flipper_length_mm\\", hue=\\"species\\", col=\\"sex\\", kind=\\"kde\\") g.set_axis_labels(\\"Flipper Length (mm)\\") g.set_titles(\\"{col_name} penguins\\") plt.show() # Task 5: Customize FacetGrid g = sns.displot(data=penguins, x=\\"flipper_length_mm\\", hue=\\"species\\", col=\\"sex\\", kind=\\"kde\\", height=5, aspect=0.8) g.set_axis_labels(\\"Flipper Length (mm)\\", \\"Density\\") g.set_titles(\\"{col_name} penguins\\") plt.suptitle(\\"Faceted KDE Plot of Flipper Length by Species and Sex\\") plt.subplots_adjust(top=0.9) plt.show()"},{"question":"Objective: To assess your understanding of Seaborn’s `objects` interface for creating and customizing plots through various advanced properties. Problem Statement: Using Seaborn\'s `objects` interface, create a plot that meets the following specifications: 1. **Data Preparation**: - Create a dataset that includes: - A continuous variable `time` ranging from 0 to 10 (with 100 evenly spaced values). - A continuous variable `value` that is a random normal distribution with the same length as `time`. - A categorical variable `category` with three categories: \'A\', \'B\', \'C\', assigned sequentially to the data points. - Ensure your data is stored in a pandas DataFrame. 2. **Plot Specifications**: - Create a line plot of `value` over `time`, with different line styles for each category in `category`. - Customize the appearance using the following plot properties: - `color`: Use a different color for each category. - `linestyle`: Use different linestyles (such as solid, dashed, and dotted) for each category. - `linewidth`: Set the line width to 2 points. - Add point markers on the line plot: - Use different shapes for each category. - Set the `pointsize` based on a function of time (for example, increasing point size over time). - Add a title to the plot with custom font size. - Customize the grid to use a major grid on the Y-axis and a minor grid on the X-axis. 3. **Implementation**: - Write a function `create_custom_plot` that takes no parameters and accomplishes the above task. - The function should display the plot using appropriate Seaborn and Matplotlib functions. Constraints: - Ensure to use Seaborn’s `objects` interface. - Do not use any predefined Seaborn theme; all styles must be explicitly set within the function. Input: - None Output: - The function should display the customized plot as described. Example: Here is what the function definition should look like: ```python import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn.objects as so def create_custom_plot(): # Your code here ``` # Note: - Make sure to use proper documentation and inline comments to explain your code. - Handle any potential errors or edge cases that might arise.","solution":"import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns def create_custom_plot(): # Data Preparation np.random.seed(0) # For reproducibility time = np.linspace(0, 10, 100) value = np.random.normal(size=100) categories = np.tile([\'A\', \'B\', \'C\'], int(np.ceil(100 / 3)))[:100] # Create DataFrame data = pd.DataFrame({\'time\': time, \'value\': value, \'category\': categories}) # Create the plot p = sns.relplot( data=data, x=\'time\', y=\'value\', hue=\'category\', kind=\'line\', style=\'category\', markers=True, dashes=True, palette=\\"deep\\", linewidth=2, ) # Customize the plot p.set_titles(\\"Custom Line Plot with Seaborn Objects Interface\\") p.set(ylim=(-2, 2)) # Customize grid p.ax.grid(True, which=\'both\', axis=\'y\', linestyle=\'-\', linewidth=0.5) p.ax.grid(True, which=\'minor\', axis=\'x\', linestyle=\'--\', linewidth=0.5) # Add minor ticks p.ax.minorticks_on() plt.show()"},{"question":"Kernel Approximation and Implementation Objective To assess your understanding of kernel approximation techniques in scikit-learn and their application to large-scale machine learning tasks. Problem Statement You are given a dataset consisting of 1000 samples with 50 features each. Your task is to: 1. Implement two different kernel approximation methods (Nystroem and RBFSampler) provided by scikit-learn. 2. Train a linear classifier (SGDClassifier) using these approximations. 3. Compare the performance of the classifiers trained using different kernel approximations both in terms of accuracy and computational efficiency. Dataset For simplicity, use a synthetic dataset generated with the following code snippet: ```python from sklearn.datasets import make_classification X, y = make_classification(n_samples=1000, n_features=50, random_state=42) ``` Function Signature ```python def kernel_approximation_comparison(X, y): This function performs kernel approximation using Nystroem and RBFSampler, trains an SGDClassifier on the transformed data, and compares their performance. Args: X (array-like): Feature matrix of shape (n_samples, n_features). y (array-like): Target vector of shape (n_samples,). Returns: dict: Dictionary with keys \'nystroem_accuracy\' and \'rbf_accuracy\' indicating the accuracy of the models and \'nystroem_time\' and \'rbf_time\' indicating the computational time taken for kernel approximation. pass ``` Requirements 1. **Fit and Transform**: Use Nystroem and RBFSampler to transform the feature set `X`. 2. **Classifier Training**: Use SGDClassifier to train models on the transformed features. 3. **Performance Measurement**: Calculate the accuracy of each classifier and record the computational time taken for the kernel transformation. 4. **Output**: Return a dictionary containing the accuracy and computational time for each kernel approximation method. Constraints - Set `n_components=100` for both Nystroem and RBFSampler. - Use `random_state=42` for reproducibility. - Limit the time complexity to ensure the code runs within a reasonable time frame on a typical modern processor. Example Output ```python { \\"nystroem_accuracy\\": 0.85, \\"rbf_accuracy\\": 0.83, \\"nystroem_time\\": 0.128, \\"rbf_time\\": 0.085 } ``` Hints - You can use `time.time()` to measure the computational time. - Ensure to import the necessary modules: `Nystroem`, `RBFSampler`, `SGDClassifier`. # Solution Template ```python import time from sklearn.kernel_approximation import Nystroem, RBFSampler from sklearn.linear_model import SGDClassifier from sklearn.metrics import accuracy_score def kernel_approximation_comparison(X, y): results = {} # Nystroem approximation start_time = time.time() nystroem = Nystroem(n_components=100, random_state=42) X_nystroem = nystroem.fit_transform(X) nystroem_time = time.time() - start_time clf_nystroem = SGDClassifier(max_iter=5, random_state=42) clf_nystroem.fit(X_nystroem, y) nystroem_accuracy = accuracy_score(y, clf_nystroem.predict(X_nystroem)) # RBF Sampler approximation start_time = time.time() rbf_sampler = RBFSampler(n_components=100, random_state=42) X_rbf = rbf_sampler.fit_transform(X) rbf_time = time.time() - start_time clf_rbf = SGDClassifier(max_iter=5, random_state=42) clf_rbf.fit(X_rbf, y) rbf_accuracy = accuracy_score(y, clf_rbf.predict(X_rbf)) results[\'nystroem_accuracy\'] = nystroem_accuracy results[\'rbf_accuracy\'] = rbf_accuracy results[\'nystroem_time\'] = nystroem_time results[\'rbf_time\'] = rbf_time return results ```","solution":"import time from sklearn.kernel_approximation import Nystroem, RBFSampler from sklearn.linear_model import SGDClassifier from sklearn.metrics import accuracy_score from sklearn.datasets import make_classification def kernel_approximation_comparison(X, y): This function performs kernel approximation using Nystroem and RBFSampler, trains an SGDClassifier on the transformed data, and compares their performance. Args: X (array-like): Feature matrix of shape (n_samples, n_features). y (array-like): Target vector of shape (n_samples,). Returns: dict: Dictionary with keys \'nystroem_accuracy\' and \'rbf_accuracy\' indicating the accuracy of the models and \'nystroem_time\' and \'rbf_time\' indicating the computational time taken for kernel approximation. results = {} # Nystroem approximation start_time = time.time() nystroem = Nystroem(n_components=100, random_state=42) X_nystroem = nystroem.fit_transform(X) nystroem_time = time.time() - start_time clf_nystroem = SGDClassifier(max_iter=5, random_state=42) clf_nystroem.fit(X_nystroem, y) nystroem_accuracy = accuracy_score(y, clf_nystroem.predict(X_nystroem)) # RBF Sampler approximation start_time = time.time() rbf_sampler = RBFSampler(n_components=100, random_state=42) X_rbf = rbf_sampler.fit_transform(X) rbf_time = time.time() - start_time clf_rbf = SGDClassifier(max_iter=5, random_state=42) clf_rbf.fit(X_rbf, y) rbf_accuracy = accuracy_score(y, clf_rbf.predict(X_rbf)) results[\'nystroem_accuracy\'] = nystroem_accuracy results[\'rbf_accuracy\'] = rbf_accuracy results[\'nystroem_time\'] = nystroem_time results[\'rbf_time\'] = rbf_time return results # Generate synthetic dataset X, y = make_classification(n_samples=1000, n_features=50, random_state=42) # Run the function and print results results = kernel_approximation_comparison(X, y) print(results)"},{"question":"Objective The objective of this assessment is to test your understanding of the `hmac` module in Python. You will be required to create HMAC objects, update them with messages, compute HMAC digests, and securely compare digests. Problem Statement You are writing a system to verify the integrity and authenticity of messages sent over a network. To do this, you will use the HMAC algorithm. Implement the following functions: 1. **create_hmac_object(key: bytes, digestmod: str) -> hmac.HMAC:** - **Parameters:** - `key` (bytes): The secret key used for HMAC. - `digestmod` (str): The name of the hash algorithm (e.g., \'sha256\'). - **Returns:** An HMAC object initialized with the given key and hash algorithm. 2. **update_hmac(hmac_obj: hmac.HMAC, msg: bytes) -> None:** - **Parameters:** - `hmac_obj` (hmac.HMAC): The HMAC object to be updated. - `msg` (bytes): The message to update the HMAC object with. - **Returns:** None. The function should update the provided HMAC object in place. 3. **get_hexdigest(hmac_obj: hmac.HMAC) -> str:** - **Parameters:** - `hmac_obj` (hmac.HMAC): The HMAC object for which to get the hex digest. - **Returns:** The hex digest of the HMAC object as a string. 4. **verify_digest(a: str, b: str) -> bool:** - **Parameters:** - `a` (str): The first hex digest. - `b` (str): The second hex digest. - **Returns:** True if the digests are equal, False otherwise. Example Usage ```python key = b\'secret_key\' digestmod = \'sha256\' msg = b\'Important message\' # Step 1: Create HMAC object hmac_obj = create_hmac_object(key, digestmod) # Step 2: Update HMAC object with the message update_hmac(hmac_obj, msg) # Step 3: Get hex digest hex_digest = get_hexdigest(hmac_obj) # Step 4: Verify digest with a known correct digest known_digest = \'correct known digest\' is_valid = verify_digest(hex_digest, known_digest) print(is_valid) # Output: False ``` Constraints - The `digestmod` parameter in `create_hmac_object` function should be a valid hash algorithm name supported by the `hashlib` module. - The `msg` should be a valid bytes-like object. - Use the `hmac.compare_digest` for securely comparing digests in `verify_digest`. Use the above example usage to verify the integrity of your function implementations. Implementation ```python import hmac import hashlib def create_hmac_object(key: bytes, digestmod: str) -> hmac.HMAC: # Your code here def update_hmac(hmac_obj: hmac.HMAC, msg: bytes) -> None: # Your code here def get_hexdigest(hmac_obj: hmac.HMAC) -> str: # Your code here def verify_digest(a: str, b: str) -> bool: # Your code here ```","solution":"import hmac import hashlib def create_hmac_object(key: bytes, digestmod: str) -> hmac.HMAC: Creates and returns an HMAC object initialized with the given key and hash algorithm. return hmac.new(key, digestmod=getattr(hashlib, digestmod)) def update_hmac(hmac_obj: hmac.HMAC, msg: bytes) -> None: Updates the provided HMAC object with the given message. hmac_obj.update(msg) def get_hexdigest(hmac_obj: hmac.HMAC) -> str: Returns the hex digest of the provided HMAC object. return hmac_obj.hexdigest() def verify_digest(a: str, b: str) -> bool: Securely compares two digests and returns True if they are equal, False otherwise. return hmac.compare_digest(a, b)"},{"question":"# Question: Program Execution Tracing with the `trace` Module Objective Write a Python script that uses the `trace` module to trace the execution of a function, generate a coverage report, and display a summary of function calls and their relationships. Details 1. **Implement the function `sum_of_squares(n)`**: ```python def sum_of_squares(n): return sum(x*x for x in range(n)) ``` 2. **Write a script that**: - Uses the `trace` module to trace the execution of `sum_of_squares` with an argument of `5`. - Generates a coverage report. - Displays a summary of function calls. - Writes the coverage result to a directory named `trace_report`. Instructions 1. Create a `Trace` object with appropriate parameters to enable tracing and coverage counting. 2. Use the `Trace` object to run the `sum_of_squares` function. 3. Obtain the results and generate the coverage report in the specified directory. 4. Print a summary of the report to standard output. Constraints - The output directory `trace_report` should be created if it does not exist. - The code should handle possible errors gracefully (e.g., directory creation errors). Expected Output - Tracing details printed to the standard output. - Coverage report files written to the `trace_report` directory. - A summary of function calls and relationships printed to the standard output. Example Your script output might look like the following when tracing the `sum_of_squares` function: ``` Function: sum_of_squares Tracing the execution... Coverage report written to trace_report directory. Summary: - main.py: sum_of_squares ... Function call relationships: - main.py: sum_of_squares called by <module> ... ``` You must submit: 1. Your implementation of the `sum_of_squares` function. 2. The script that performs the tracing and reporting.","solution":"import os from trace import Trace def sum_of_squares(n): return sum(x*x for x in range(n)) def create_and_trace_sum_of_squares(): # Ensure the trace_report directory exists report_dir = \\"trace_report\\" if not os.path.exists(report_dir): os.makedirs(report_dir) # Create a Trace object, enabling counting and tracing tracer = Trace(count=1, trace=1, outfile=os.path.join(report_dir, \\"report.txt\\")) # Start tracing tracer.run(\'sum_of_squares(5)\') # Generate coverage report results = tracer.results() # Write the results to the directory results.write_results(show_missing=True, coverdir=report_dir) # Print the summary print(f\\"Coverage report written to {report_dir} directory.\\") results.write_results(show_missing=True, summary=True) if __name__ == \\"__main__\\": create_and_trace_sum_of_squares()"},{"question":"# Advanced Python Threading: File Processing with Synchronization Primitives Objective: Design and implement a threaded application that processes a list of files in parallel while ensuring thread-safe access to shared resources using synchronization primitives from the `threading` module. Problem Description: You are tasked with creating a multithreaded Python application that: 1. Processes a list of text files concurrently. 2. Uses a shared log file to record the status of each file processing. 3. Ensures only one thread can write to the log file at any given time. Requirements: 1. Implement a function `process_file(file_path)`, which takes the path of a file, performs some arbitrary processing (e.g., counting words), and appends the results to a shared log file. 2. Use synchronization primitives (`threading.Lock`, `threading.Semaphore`, `threading.Condition`, etc.) to ensure thread-safe access to the log file. 3. Create a class `FileProcessor` with methods to: - Initialize with a list of file paths and the path to the shared log file. - Start processing files using a pool of threads. - Ensure all threads complete processing before the program terminates. 4. Use condition variables if needed to manage the processing flow. Input: - `file_list`: A list of strings, where each string is a path to a text file. - `log_file`: A string representing the path to the shared log file. - `num_threads`: An integer specifying the number of threads to be used in the pool. Output: - The shared log file should contain entries for each processed file, including information such as: - The file path. - The number of words in the file. - The status (e.g., \\"processed successfully\\"). Constraints: 1. Each file should be processed exactly once. 2. The log file should not have corrupted or interleaved entries. 3. Efficiently use the specified number of threads to maximize concurrency. Example: Suppose you have three text files: `file1.txt`, `file2.txt`, and `file3.txt`. The shared log file is `log.txt`. Using 2 threads, the `log.txt` might look like: ``` file1.txt: 234 words processed successfully file2.txt: 432 words processed successfully file3.txt: 123 words processed successfully ``` Your Task: Implement the `FileProcessor` class and the `process_file` function. Use the provided `threading` module documentation to effectively manage the concurrency aspects of the task. ```python import threading class FileProcessor: def __init__(self, file_list, log_file, num_threads): # Initialize with file list, log file, and number of threads pass def process_file(self, file_path): # Process the file and write the result to the log file pass def start_processing(self): # Start processing the files using a pool of threads pass def thread_safe_log(self, log_entry): # Ensure writing to the log file is thread-safe pass # Example use file_list = [\'file1.txt\', \'file2.txt\', \'file3.txt\'] log_file = \'log.txt\' num_threads = 2 processor = FileProcessor(file_list, log_file, num_threads) processor.start_processing() ``` Evaluation: Your solution will be evaluated based on: - Correctness and accuracy of file processing. - Effective use of threading and synchronization primitives. - Handling of concurrency issues and ensuring thread-safe operations.","solution":"import threading from threading import Lock, Thread class FileProcessor: def __init__(self, file_list, log_file, num_threads): Initialize the FileProcessor with a list of files, a log file, and the number of threads. self.file_list = file_list self.log_file = log_file self.num_threads = num_threads self.log_lock = Lock() self.threads = [] def process_file(self, file_path): Process the file and write the result to the log file. This involves counting the number of words in the file. try: with open(file_path, \'r\') as file: content = file.read() word_count = len(content.split()) log_entry = f\\"{file_path}: {word_count} words processed successfully\\" except Exception as e: log_entry = f\\"{file_path}: Error processing file - {e}\\" self.thread_safe_log(log_entry) def thread_safe_log(self, log_entry): Ensure writing to the log file is thread-safe. with self.log_lock: with open(self.log_file, \'a\') as log_file: log_file.write(log_entry + \\"n\\") def worker(self): Worker thread function to process files from the file list. while self.file_list: file_path = self.file_list.pop() self.process_file(file_path) def start_processing(self): Start processing the files using a pool of threads. for _ in range(self.num_threads): thread = Thread(target=self.worker) self.threads.append(thread) thread.start() for thread in self.threads: thread.join() # Example use (This part is not included in the solution but shows how to use the class) # file_list = [\'file1.txt\', \'file2.txt\', \'file3.txt\'] # log_file = \'log.txt\' # num_threads = 2 # processor = FileProcessor(file_list, log_file, num_threads) # processor.start_processing()"},{"question":"Objective You are tasked with implementing a function in PyTorch that demonstrates the use of Core Aten IR and Prims IR. Your solution should show an understanding of these IR concepts by implementing an operation using them. Problem Statement Implement the function `custom_operation` that performs a specific mathematical operation using Core Aten IR and Prims IR. The operation to be performed is an element-wise multiplication followed by an element-wise addition. Function Signature ```python import torch import torch._prims as prims def custom_operation(tensor1: torch.Tensor, tensor2: torch.Tensor, tensor3: torch.Tensor) -> torch.Tensor: Perform an element-wise multiplication of tensor1 and tensor2, followed by an element-wise addition of tensor3 using Core Aten IR and Prims IR. Parameters: tensor1 (torch.Tensor): The first input tensor. tensor2 (torch.Tensor): The second input tensor. tensor3 (torch.Tensor): The third input tensor, to be added to the result of the multiplication. Returns: torch.Tensor: The result of (tensor1 * tensor2) + tensor3. ``` Input Constraints 1. All tensors will have the same shape. 2. All tensors are of the same dtype (floating-point types). 3. The tensors can have up to 2 dimensions (e.g., 2D matrix or 1D vector). Requirements - Your implementation must use Core Aten IR and Prims IR operators explicitly. - The function should not use any high-level PyTorch operations directly (e.g., `torch.add`, `torch.mul`). Example ```python tensor1 = torch.tensor([1.0, 2.0, 3.0]) tensor2 = torch.tensor([4.0, 5.0, 6.0]) tensor3 = torch.tensor([7.0, 8.0, 9.0]) result = custom_operation(tensor1, tensor2, tensor3) # Expected output: tensor([11.0, 18.0, 27.0]) ``` Notes - Make sure to handle the operations using Core Aten IR and Prims IR correctly. - Consider edge cases like zero tensors and ensure your implementation is robust.","solution":"import torch import torch._prims as prims def custom_operation(tensor1: torch.Tensor, tensor2: torch.Tensor, tensor3: torch.Tensor) -> torch.Tensor: Perform an element-wise multiplication of tensor1 and tensor2, followed by an element-wise addition of tensor3 using Core Aten IR and Prims IR. Parameters: tensor1 (torch.Tensor): The first input tensor. tensor2 (torch.Tensor): The second input tensor. tensor3 (torch.Tensor): The third input tensor, to be added to the result of the multiplication. Returns: torch.Tensor: The result of (tensor1 * tensor2) + tensor3. # Element-wise multiplication using Core Aten IR mul_result = torch.ops.aten.mul(tensor1, tensor2) # Element-wise addition using Prims IR add_result = prims.add(mul_result, tensor3) return add_result"},{"question":"You are required to write a Python program that fetches data from a given URL using the `urllib` and `http` modules and processes the response to extract and display specific information. # Task 1. Write a function `fetch_and_process_url(url: str) -> dict` that: - Takes a URL as input. - Fetches the content available at the URL. - Handles HTTP responses appropriately, including error handling. - Parses the response to extract the following information: - HTTP status code. - Content-Type from response headers. - The first 500 characters of the content (assuming the content is text). - Returns a dictionary with keys `status_code`, `content_type`, and `content_snippet` corresponding to the extracted information. # Input Format - A single string representing the URL from which data is to be fetched. # Output Format - A dictionary with the following structure: ```python { \\"status_code\\": int, \\"content_type\\": str, \\"content_snippet\\": str } ``` # Constraints - You should handle potential exceptions that might be raised when fetching the URL, such as network errors, invalid URLs, and HTTP errors. - Assume the fetched content is text and should be UTF-8 decoded. # Example Input ```python url = \'https://www.example.com\' ``` Output ```python { \\"status_code\\": 200, \\"content_type\\": \\"text/html; charset=UTF-8\\", \\"content_snippet\\": \\"<!doctype html>n<html>n<head>n <title>Example Domain<…>\\" } ``` # Additional Notes - The `url` may be any valid HTTP/HTTPS URL. - To assist in your implementation, you may use the `urllib.request` module for fetching the URL and `http.client` for handling HTTP responses. # Function Signature ```python def fetch_and_process_url(url: str) -> dict: pass ```","solution":"import urllib.request import urllib.error def fetch_and_process_url(url: str) -> dict: Fetches data from the given URL and processes the response to extract specific information. Parameters: url (str): The URL to fetch data from. Returns: dict: A dictionary with keys \'status_code\', \'content_type\', and \'content_snippet\'. try: # Requesting URL with urllib.request.urlopen(url) as response: # Extracting status code status_code = response.getcode() # Extracting Content-Type content_type = response.info().get_content_type() # Reading the content content = response.read().decode(\'utf-8\') # Getting the first 500 characters of the content content_snippet = content[:500] return { \\"status_code\\": status_code, \\"content_type\\": response.headers[\'Content-Type\'], \\"content_snippet\\": content_snippet } except urllib.error.URLError as e: return { \\"status_code\\": getattr(e, \'code\', None), \\"content_type\\": None, \\"content_snippet\\": str(e) }"},{"question":"Coding Assessment Question # Objective Demonstrate understanding of the `fcntl` module by performing file control and file locking operations. # Problem Statement You are tasked with writing a Python function that locks a file for exclusive writing, writes a specified message to the file, and then unlocks the file. The function should also be able to handle potential exceptions that might occur during the file operations. # Function Signature ```python def lock_and_write_to_file(file_path: str, message: str) -> None: pass ``` # Input - `file_path` (str): The path of the file to be locked and written to. - `message` (str): The message to write to the file. # Output - The function should not return any value. # Requirements 1. **File Locking**: - Use `fcntl.lockf` to acquire an exclusive lock on the file before writing. - Handle the possibility of the file lock being unavailable (use `LOCK_NB` to avoid blocking). 2. **File Writing**: - Write the provided message to the file. - Ensure the write operation is completed successfully. 3. **File Unlocking**: - Properly release the lock on the file after the write operation is complete. # Exception Handling - Appropriately handle `OSError` exceptions, which may occur if the file lock operation fails. # Constraints - The function should be able to handle files up to 1GB in size. - Ensure the function executes in a reasonable time frame for files up to the specified size. # Example ```python import os # Test the function with a sample file path and message file_path = \\"test_file.txt\\" message = \\"Hello, this is a test message.\\" # Ensure the file exists before testing with open(file_path, \'w\') as f: pass # Just create an empty file # Call the function lock_and_write_to_file(file_path, message) # Verify the content of the file with open(file_path, \'r\') as f: content = f.read() print(content) # Output should be: \\"Hello, this is a test message.\\" ``` The function should demonstrate correct usage of the `fcntl` module’s `lockf` function for file locking and unlocking, along with proper handling of potential locking errors.","solution":"import fcntl import os def lock_and_write_to_file(file_path: str, message: str) -> None: try: # Open the file for writing with open(file_path, \'w\') as file: # Try to acquire an exclusive lock without blocking fcntl.lockf(file, fcntl.LOCK_EX | fcntl.LOCK_NB) # Write the message to the file file.write(message) # Ensure the file is flushed to disk file.flush() # Release the lock fcntl.lockf(file, fcntl.LOCK_UN) except OSError as e: print(f\\"An error occurred: {str(e)}\\")"},{"question":"# OS Module Comprehensive Assessment Objective Design a function `file_directory_operations` to perform a sequence of file and directory operations, demonstrating a clear understanding of handling files, directories, and environment variables using the Python `os` module. Function Requirements 1. **Function Signature:** ```python def file_directory_operations(base_dir: str, filename: str, file_content: str): pass ``` 2. **Parameters:** - `base_dir` (str): The path to the base directory where operations will be conducted. - `filename` (str): The name of the file to be created/modified within the base directory. - `file_content` (str): The content to write into the file. 3. **Returns:** - The function should return a tuple (int, str): - An integer representing the number of lines in the created file. - A string that represents a newly created environment variable value. Operations to Implement 1. **Directory Operations:** - Create the base directory if it doesn’t exist. - Create a sub-directory named `subdir` within `base_dir`. 2. **File Operations:** - Create a file named `filename` in the `base_dir`. - Write `file_content` to the file. - Calculate and return the number of lines written to the file. 3. **Environment Operations:** - Set an environment variable `MY_ENV_VAR` to have the value of the current working directory path. - Retrieve this environment variable and return its value as part of the function’s return. 4. **Error Handling:** - Handle potential errors gracefully, ensuring any opened files or created directories do not result in resource leaks. - Use appropriate `os` functions like `os.environ`, `os.mkdir()`, `os.path.join()`, `os.chdir()`, `os.getcwd()`, `os.getenv()`, and file handling utilities. Example Usage ```python # Creating directory and file operations base_directory = \\"/tmp/test_base\\" file_name = \\"testfile.txt\\" content = \\"This is a test file.nIt has multiple lines.nEnd of the file.n\\" # Running the function lines_count, env_value = file_directory_operations(base_directory, file_name, content) print(lines_count) # Should print: 3 print(env_value) # Should print the current working directory path ``` Constraints - Assume all specified paths are valid and writable. - Use absolute or relative paths based on the requirements. - Ensure the function works correctly on both Unix and Windows systems. Implement the function in Python.","solution":"import os def file_directory_operations(base_dir: str, filename: str, file_content: str): # Create the base directory if it doesn\'t exist if not os.path.exists(base_dir): os.makedirs(base_dir) # Create a sub-directory `subdir` within `base_dir` sub_dir_path = os.path.join(base_dir, \'subdir\') if not os.path.exists(sub_dir_path): os.mkdir(sub_dir_path) # Create the file and write the content to it file_path = os.path.join(base_dir, filename) with open(file_path, \'w\') as file: file.write(file_content) # Calculate the number of lines in the file content num_lines = len(file_content.splitlines()) # Set an environment variable `MY_ENV_VAR` to current working directory path current_working_dir = os.getcwd() os.environ[\'MY_ENV_VAR\'] = current_working_dir # Retrieve the value of the environment variable env_var_value = os.getenv(\'MY_ENV_VAR\') # Return the number of lines and the environment variable value return num_lines, env_var_value"},{"question":"# Task Description You are provided with a dataset containing a `BooleanArray` with nullable Boolean values (`True`, `False`, `NA`). Your task is to perform a combination of indexing and logical operations following Kleene logic on this dataset. # Input 1. A pandas Series `s` of nullable Boolean values. 2. A list of Boolean operations to perform (e.g., `[\\"and\\", \\"or\\", \\"xor\\"]`), specifying operations to be performed sequentially with a given Boolean value. # Output A pandas Series indicating the result after all specified logical operations have been applied to the original nullable Boolean Series. # Constraints 1. Each operation in the list of operations will be one of `and`, `or`, `xor`. 2. The operations should be applied in order on the entire Series using a Boolean value (`True`). 3. Output must retain the nullable property and follow the Kleene logic rules. 4. Assume the Series `s` can contain only `True`, `False`, and `NA`. # Example Input ```python s = pd.Series([True, False, pd.NA, True, False, pd.NA], dtype=\\"boolean\\") operations = [\\"and\\", \\"or\\"] ``` Output ```python pd.Series([True, False, pd.NA, True, False, pd.NA], dtype=\\"boolean\\") ``` # Solution Example ```python import pandas as pd def kleene_operations(s, operations): result = s for op in operations: if op == \\"and\\": result = result & True elif op == \\"or\\": result = result | True elif op == \\"xor\\": result = result ^ True return result # Example usage s = pd.Series([True, False, pd.NA, True, False, pd.NA], dtype=\\"boolean\\") operations = [\\"and\\", \\"or\\"] print(kleene_operations(s, operations)) ``` Write the function `kleene_operations(s, operations)` following the example usage and description above.","solution":"import pandas as pd def kleene_operations(s, operations): result = s for op in operations: if op == \\"and\\": result = result & True elif op == \\"or\\": result = result | True elif op == \\"xor\\": result = result ^ True return result"},{"question":"**Problem Statement:** You are given the task of analyzing and visualizing a dataset that records monthly precipitation data for several cities over several years. The dataset is initially provided in a format where each row represents data from a single city for one year, and each column represents the precipitation data for one month in that year. Your task involves the following steps: 1. **Data Loading and Transformation**: - Convert the dataset from its initial wide-form format to a long-form format for easier visualization with seaborn. - Generate summary statistics to understand the distribution of precipitation across different cities and months. 2. **Visualization**: - Create a line plot that visualizes the monthly precipitation trends for each city over the years. - Create a box plot that summarizes the distribution of monthly precipitation for each city across all years. - Create a heatmap that shows the average monthly precipitation for each city. 3. **Advanced Visualization**: - Create a faceted grid plot that shows the monthly precipitation trends for each city for each year, to detect patterns over time. **Input Format:** A CSV file named `precipitation_data.csv` with the following structure: ``` year,city,Jan,Feb,Mar,Apr,May,Jun,Jul,Aug,Sep,Oct,Nov,Dec 2010,CityA,100,87,56,34,78,123,145,167,98,76,45,23 2010,CityB,45,67,78,89,90,112,134,156,176,98,65,34 ... 2020,CityA,110,93,66,38,80,126,148,170,104,79,49,25 ``` **Constraints:** - Ensure the data is handled appropriately for edge cases such as missing data or incorrect formatting. - Use appropriate seaborn functions to create each plot, ensuring plots are clearly labeled and interpreted. **Expected Output:** 1. Code to load and transform the dataset. 2. Summary statistics (mean, median, standard deviation, etc.) of precipitation data. 3. Line plot of monthly precipitation trends for each city. 4. Box plot summarizing monthly precipitation distribution for each city. 5. Heatmap showing average monthly precipitation for each city. 6. Faceted grid plot showing monthly precipitation trends for each city by year. **Example Code Structure:** ```python import pandas as pd import seaborn as sns import matplotlib.pyplot as plt # Load the data precipitation_dataset = pd.read_csv(\'precipitation_data.csv\') # Data transformation: Convert wide-form to long-form precipitation_long = precipitation_dataset.melt(id_vars=[\'year\', \'city\'], var_name=\'month\', value_name=\'precipitation\') # Compute summary statistics summary_stats = precipitation_long.groupby([\'city\', \'month\']).describe() # Line plot of monthly precipitation trends for each city sns.relplot(data=precipitation_long, x=\'month\', y=\'precipitation\', hue=\'city\', kind=\'line\') plt.show() # Box plot of monthly precipitation distribution for each city sns.catplot(data=precipitation_long, x=\'city\', y=\'precipitation\', kind=\'box\') plt.show() # Heatmap of average monthly precipitation for each city heatmap_data = precipitation_long.pivot_table(index=\'city\', columns=\'month\', values=\'precipitation\', aggfunc=\'mean\') sns.heatmap(heatmap_data, annot=True) plt.show() # Faceted grid plot of monthly precipitation trends for each city by year g = sns.FacetGrid(precipitation_long, col=\'city\', row=\'year\', margin_titles=True) g.map(sns.lineplot, \'month\', \'precipitation\') plt.show() ``` **Note:** Ensure to handle necessary data preprocessing steps such as dealing with missing values and formatting dates correctly before plotting.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def load_and_transform_data(filepath): Load precipitation dataset and transform from wide to long format. Parameters: - filepath: str, path to the CSV file Returns: - precipitation_long: DataFrame, long formatted DataFrame precipitation_dataset = pd.read_csv(filepath) # Convert wide-form to long-form precipitation_long = precipitation_dataset.melt(id_vars=[\'year\', \'city\'], var_name=\'month\', value_name=\'precipitation\') return precipitation_long def compute_summary_statistics(data): Compute summary statistics for the given data. Parameters: - data: DataFrame, the long formatted DataFrame Returns: - summary_stats: DataFrame, summary statistics summary_stats = data.groupby([\'city\', \'month\'])[\'precipitation\'].describe() return summary_stats def create_line_plot(data): Create line plot of monthly precipitation trends for each city. Parameters: - data: DataFrame, the long formatted DataFrame sns.relplot(data=data, x=\'month\', y=\'precipitation\', hue=\'city\', kind=\'line\') plt.title(\\"Monthly Precipitation Trends for Each City\\") plt.show() def create_box_plot(data): Create box plot of monthly precipitation distribution for each city. Parameters: - data: DataFrame, the long formatted DataFrame sns.catplot(data=data, x=\'city\', y=\'precipitation\', kind=\'box\') plt.title(\\"Monthly Precipitation Distribution for Each City\\") plt.show() def create_heatmap(data): Create heatmap of average monthly precipitation for each city. Parameters: - data: DataFrame, the long formatted DataFrame heatmap_data = data.pivot_table(index=\'city\', columns=\'month\', values=\'precipitation\', aggfunc=\'mean\') sns.heatmap(heatmap_data, annot=True) plt.title(\\"Average Monthly Precipitation for Each City\\") plt.show() def create_faceted_grid_plot(data): Create faceted grid plot of monthly precipitation trends for each city by year. Parameters: - data: DataFrame, the long formatted DataFrame g = sns.FacetGrid(data, col=\'city\', row=\'year\', margin_titles=True) g.map(sns.lineplot, \'month\', \'precipitation\') plt.show()"},{"question":"# Question: Advanced Histogram Visualization using Seaborn You are provided with a dataset of penguin characteristics sourced from Seaborn\'s built-in datasets. Using this data, your task is to generate a series of visualizations to analyze the distribution of penguin attributes. Follow the steps below to create the desired visualizations: Input: - You must use the `penguins` dataset from Seaborn. Steps: 1. Load the `penguins` dataset. 2. Plot a histogram of the `bill_length_mm` attribute. - Use bins with a width of 2 mm. - Add a kernel density estimate to the histogram. - Use different colors to represent different species using the `hue` parameter. 3. Create another histogram for the `body_mass_g` attribute: - Use the \'island\' attribute to color the histogram. - Stack the histogram bars. - Normalize the histogram bars so their heights show probability density. 4. Generate bivariate histograms: - Plot a bivariate histogram showing `bill_depth_mm` on the x-axis and `body_mass_g` on the y-axis. - Add a `hue` based on `species`. - Set the number of bins to 20. - Enable the colorbar and set its shrink property to 0.8. Output: Your solution should result in three plots: 1. A univariate histogram of `bill_length_mm` colored by species, with KDE and specified bin width. 2. A stacked, probability density normalized histogram of `body_mass_g` colored by island. 3. A bivariate histogram of `bill_depth_mm` vs. `body_mass_g` with hue based on species, containing specified bins and a colorbar. Constraints: - Ensure your plots are clean and aesthetically pleasing using Seaborn\'s theming capabilities. - Comment your code to explain each major step and customization. Example Solution Skeleton: ```python import seaborn as sns import matplotlib.pyplot as plt # Load the dataset penguins = sns.load_dataset(\\"penguins\\") # Set the theme for Seaborn sns.set_theme(style=\\"whitegrid\\") # Plot 1: Histogram for bill_length_mm with KDE and different species hues plt.figure(figsize=(10, 6)) sns.histplot(data=penguins, x=\\"bill_length_mm\\", bins=int((penguins[\'bill_length_mm\'].max() - penguins[\'bill_length_mm\'].min()) / 2), kde=True, hue=\\"species\\") plt.title(\\"Histogram of Bill Length with KDE by Species\\") plt.show() # Plot 2: Stacked, normalized histogram for body_mass_g by island plt.figure(figsize=(10, 6)) sns.histplot(data=penguins, x=\\"body_mass_g\\", hue=\\"island\\", multiple=\\"stack\\", stat=\\"density\\") plt.title(\\"Stacked and Normalized Histogram of Body Mass by Island\\") plt.show() # Plot 3: Bivariate histogram for bill_depth_mm and body_mass_g with hue by species plt.figure(figsize=(10, 6)) sns.histplot(data=penguins, x=\\"bill_depth_mm\\", y=\\"body_mass_g\\", hue=\\"species\\", bins=20, cbar=True, cbar_kws=dict(shrink=.8)) plt.title(\\"Bivariate Histogram of Bill Depth and Body Mass by Species\\") plt.show() ``` **Notes:** - Post additional visual customization (e.g., axis labels, titles) to make the plots more informative and visually appealing. - Briefly explain why certain choices (e.g., KDE addition, normalization) are made.","solution":"import seaborn as sns import matplotlib.pyplot as plt # Load the dataset penguins = sns.load_dataset(\\"penguins\\") # Set the theme for Seaborn sns.set_theme(style=\\"whitegrid\\") # Plot 1: Histogram for bill_length_mm with KDE and different species hues plt.figure(figsize=(10, 6)) sns.histplot( data=penguins, x=\\"bill_length_mm\\", bins=int((penguins[\'bill_length_mm\'].max() - penguins[\'bill_length_mm\'].min()) / 2), kde=True, hue=\\"species\\" ) plt.title(\\"Histogram of Bill Length with KDE by Species\\") plt.xlabel(\\"Bill Length (mm)\\") plt.ylabel(\\"Count / Density\\") plt.show() # Plot 2: Stacked, normalized histogram for body_mass_g by island plt.figure(figsize=(10, 6)) sns.histplot( data=penguins, x=\\"body_mass_g\\", hue=\\"island\\", multiple=\\"stack\\", stat=\\"density\\" ) plt.title(\\"Stacked and Normalized Histogram of Body Mass by Island\\") plt.xlabel(\\"Body Mass (g)\\") plt.ylabel(\\"Density\\") plt.show() # Plot 3: Bivariate histogram for bill_depth_mm and body_mass_g with hue by species plt.figure(figsize=(10, 6)) sns.histplot( data=penguins, x=\\"bill_depth_mm\\", y=\\"body_mass_g\\", hue=\\"species\\", bins=20, cbar=True, cbar_kws=dict(shrink=.8) ) plt.title(\\"Bivariate Histogram of Bill Depth and Body Mass by Species\\") plt.xlabel(\\"Bill Depth (mm)\\") plt.ylabel(\\"Body Mass (g)\\") plt.show()"},{"question":"Objective: Implement a Python function that compresses multiple text files in a directory into a single gzip file, and another function that decompresses this gzip file back into the original text files. Problem Statement: You are provided with a directory that contains multiple text files. Your task is to implement a function `compress_directory` that: 1. Compresses all text files from the given directory into a single gzip file. 2. Names the output gzip file as specified by the user. Additionally, implement a function `decompress_to_directory` that: 1. Decompresses the single gzip file back into its constituent text files. 2. Places the decompressed text files into a specified output directory. Function Signatures: ```python def compress_directory(input_dir: str, output_gzip_file: str, compresslevel: int = 9) -> None: Compresses all text files from the input directory into a single output gzip file. Parameters: - input_dir (str): The path to the directory containing text files to be compressed. - output_gzip_file (str): The path where the output gzip file should be written. - compresslevel (int): The compression level to be used (from 0 to 9). Default is 9. Returns: - None pass def decompress_to_directory(input_gzip_file: str, output_dir: str) -> None: Decompresses the single gzip file back into its constituent text files. Parameters: - input_gzip_file (str): The path to the gzip file to be decompressed. - output_dir (str): The directory where the decompressed text files should be written. Returns: - None pass ``` Constraints: - The input directory for compression should only contain text files; other file types can be ignored. - The output directory for decompression should exist; the function should not create new directories. - Handle exceptions gracefully and log appropriate error messages. - If a decompressed file already exists in the output directory, override it. Example Usage: ```python # Compressing the directory compress_directory(\'path/to/text_files\', \'path/to/output_file.gz\', compresslevel=5) # Decompressing the gzip file back into files decompress_to_directory(\'path/to/output_file.gz\', \'path/to/output_directory\') ``` Additional Notes: - Pay attention to file handling and ensure all files are closed properly to avoid resource leaks. - You may find the `os`, `gzip`, `shutil`, and `io` modules useful for this task.","solution":"import os import gzip import shutil def compress_directory(input_dir: str, output_gzip_file: str, compresslevel: int = 9) -> None: Compresses all text files from the input directory into a single output gzip file. Parameters: - input_dir (str): The path to the directory containing text files to be compressed. - output_gzip_file (str): The path where the output gzip file should be written. - compresslevel (int): The compression level to be used (from 0 to 9). Default is 9. Returns: - None with gzip.open(output_gzip_file, \'wb\', compresslevel=compresslevel) as gz_out: for text_file in os.listdir(input_dir): file_path = os.path.join(input_dir, text_file) if os.path.isfile(file_path) and text_file.endswith(\'.txt\'): with open(file_path, \'rb\') as f: shutil.copyfileobj(f, gz_out) def decompress_to_directory(input_gzip_file: str, output_dir: str) -> None: Decompresses the single gzip file back into its constituent text files. Parameters: - input_gzip_file (str): The path to the gzip file to be decompressed. - output_dir (str): The directory where the decompressed text files should be written. Returns: - None os.makedirs(output_dir, exist_ok=True) with gzip.open(input_gzip_file, \'rb\') as gz_in: with open(os.path.join(output_dir, \'decompressed_output.txt\'), \'wb\') as out_f: shutil.copyfileobj(gz_in, out_f)"},{"question":"Objective Create a Python extension type using C that implements various functionalities described in the documentation. The extension type should serve as a custom type that maintains a simple integer counter, allows attribute management, supports comparisons, and can be weakly referenced. Requirements 1. **Type Definition**: Define a new Python type `CounterType` that encapsulates an integer counter. 2. **Memory Management**: Implement the necessary methods for memory deallocation. 3. **Object Representation**: Provide string representations for the type. 4. **Attribute Management**: Allow reading and setting an attribute named `value`, representing the counter. 5. **Comparison**: Implement rich comparison methods for equality and comparison of counters. 6. **Weak Reference Support**: Ensure the type can be weakly referenced. Instructions 1. **Type Definition**: Define a `PyTypeObject` named `CounterType`. 2. **Documentation String**: Include a doc string for the type. 3. **Initialization**: Initialize the counter to zero by default. 4. **Deallocation**: Define a deallocator that frees up allocated memory. 5. **String Representation**: Implement `repr` and `str` functions to provide meaningful representations of the counter. 6. **Attribute Management**: Implement getter and setter for `value` using the `PyGetSetDef` structure. 7. **Comparison**: Implement rich comparison functions to compare two `CounterType` objects based on their `value`. 8. **Weak Reference**: Include a field for weak references and ensure proper cleanup during deallocation. Constraints - Import necessary headers and ensure proper memory management to avoid leaks. - Use `Py_ssize_t` type for sized fields. - Handle errors gracefully, ensuring that exceptions are appropriately set. Performance Requirements The extension should handle typical operations efficiently without significant memory or runtime overhead. Template ```c #include <Python.h> typedef struct { PyObject_HEAD Py_ssize_t counter; /* Attribute for the counter value */ PyObject *weakreflist; /* List of weak references */ } CounterObject; /* Define the methods and members of the CounterType */ static PyTypeObject CounterType = { PyVarObject_HEAD_INIT(NULL, 0) .tp_name = \\"extension.Counter\\", .tp_basicsize = sizeof(CounterObject), .tp_itemsize = 0, .tp_dealloc = (destructor)Counter_dealloc, .tp_repr = (reprfunc)Counter_repr, .tp_str = (reprfunc)Counter_str, .tp_getattro = PyObject_GenericGetAttr, .tp_setattro = PyObject_GenericSetAttr, .tp_flags = Py_TPFLAGS_DEFAULT, .tp_doc = \\"Counter object\\", .tp_richcompare = (richcmpfunc)Counter_richcompare, .tp_methods = Counter_methods, .tp_members = Counter_members, .tp_getset = Counter_getsetters, .tp_alloc = PyType_GenericAlloc, .tp_new = Counter_new, .tp_as_number = &Counter_as_number, .tp_weaklistoffset = offsetof(CounterObject, weakreflist), }; /* Method Definitions for CounterType */ /* Deallocator for CounterType */ static void Counter_dealloc(CounterObject *self) { if (self->weakreflist != NULL) PyObject_ClearWeakRefs((PyObject *) self); Py_TYPE(self)->tp_free((PyObject *) self); } /* Getter and Setter for \'value\' attribute */ static PyObject * Counter_get_value(CounterObject *self, void *closure) { return PyLong_FromSsize_t(self->counter); } static int Counter_set_value(CounterObject *self, PyObject *value, void *closure) { Py_ssize_t new_value; if (value == NULL) { PyErr_SetString(PyExc_TypeError, \\"Cannot delete the value attribute\\"); return -1; } if (!PyLong_Check(value)) { PyErr_SetString(PyExc_TypeError, \\"The value attribute value must be an int\\"); return -1; } new_value = PyLong_AsSsize_t(value); if (new_value == -1 && PyErr_Occurred()) { return -1; } self->counter = new_value; return 0; } /* Representation and String Conversion */ static PyObject * Counter_repr(CounterObject *self) { return PyUnicode_FromFormat(\\"Counter(value=%zd)\\", self->counter); } static PyObject * Counter_str(CounterObject *self) { return PyUnicode_FromFormat(\\"Counter with value %zd\\", self->counter); } /* Rich Comparison */ static PyObject * Counter_richcompare(PyObject *a, PyObject *b, int op) { if (!PyObject_TypeCheck(a, &CounterType) || !PyObject_TypeCheck(b, &CounterType)) { Py_INCREF(Py_NotImplemented); return Py_NotImplemented; } CounterObject *obj_a = (CounterObject *)a; CounterObject *obj_b = (CounterObject *)b; PyObject *result; switch (op) { case Py_LT: result = (obj_a->counter < obj_b->counter) ? Py_True : Py_False; break; case Py_LE: result = (obj_a->counter <= obj_b->counter) ? Py_True : Py_False; break; case Py_EQ: result = (obj_a->counter == obj_b->counter) ? Py_True : Py_False; break; case Py_NE: result = (obj_a->counter != obj_b->counter) ? Py_True : Py_False; break; case Py_GT: result = (obj_a->counter > obj_b->counter) ? Py_True : Py_False; break; case Py_GE: result = (obj_a->counter >= obj_b->counter) ? Py_True : Py_False; break; default: Py_INCREF(Py_NotImplemented); return Py_NotImplemented; } Py_INCREF(result); return result; } /* Method and Member Definitions */ static PyMethodDef Counter_methods[] = { {NULL, NULL, 0, NULL} }; static PyMemberDef Counter_members[] = { {NULL, NULL, 0, 0, NULL} }; static PyGetSetDef Counter_getsetters[] = { {\\"value\\", (getter)Counter_get_value, (setter)Counter_set_value, \\"counter value\\", NULL}, {NULL, NULL, NULL, NULL, NULL} }; /* Module Initialization */ static struct PyModuleDef countermodule = { PyModuleDef_HEAD_INIT, \\"extension\\", \\"Counter Extension Module\\", -1, NULL, NULL, NULL, NULL, NULL }; PyMODINIT_FUNC PyInit_extension(void) { PyObject *m; if (PyType_Ready(&CounterType) < 0) return NULL; m = PyModule_Create(&countermodule); if (m == NULL) return NULL; Py_INCREF(&CounterType); PyModule_AddObject(m, \\"Counter\\", (PyObject *)&CounterType); return m; } ``` Testing Your Extension - Compile the C extension and load it into Python. - Create instances, compare them, set/get attributes, and check weak reference behavior. Example Python usage: ```python import extension counter1 = extension.Counter() counter2 = extension.Counter() counter1.value = 10 counter2.value = 20 print(repr(counter1)) print(str(counter1)) print(counter1.value == counter2.value) print(counter1 < counter2) ```","solution":"# solution.py file class Counter: def __init__(self, value=0): self._value = value @property def value(self): return self._value @value.setter def value(self, new_value): self._value = new_value def __repr__(self): return f\\"Counter(value={self._value})\\" def __str__(self): return f\\"Counter with value {self._value}\\" def __eq__(self, other): if isinstance(other, Counter): return self._value == other.value return NotImplemented def __ne__(self, other): if isinstance(other, Counter): return self._value != other.value return NotImplemented def __lt__(self, other): if isinstance(other, Counter): return self._value < other.value return NotImplemented def __le__(self, other): if isinstance(other, Counter): return self._value <= other.value return NotImplemented def __gt__(self, other): if isinstance(other, Counter): return self._value > other.value return NotImplemented def __ge__(self, other): if isinstance(other, Counter): return self._value >= other.value return NotImplemented"},{"question":"**Objective:** Demonstrate your understanding of the pandas `Styler` object and its methods by applying various styles to a DataFrame and exporting the styled DataFrame. **Problem Statement:** Given a DataFrame `df` with the following data: ```python import pandas as pd data = { \'A\': [1, 2, 3, 4, 5], \'B\': [10, 20, 30, 40, 50], \'C\': [100, 200, 300, 400, 500], \'D\': [1000, 2000, 3000, 4000, 5000], } df = pd.DataFrame(data) ``` You are required to: 1. Create a `Styler` object from the DataFrame. 2. Apply the following styles: - Highlight the maximum value in each column using the built-in `highlight_max` method. - Apply a background gradient to the DataFrame using the `background_gradient` method. - Set a custom caption for the styled DataFrame. 3. Export the styled DataFrame to an HTML string. **Function Signature:** ```python def style_dataframe(df: pd.DataFrame) -> str: Styles the input DataFrame and returns the styled HTML string. Parameters: df (pd.DataFrame): The DataFrame to be styled. Returns: str: The HTML string of the styled DataFrame. pass ``` **Input:** - A pandas DataFrame `df`. **Output:** - An HTML string representing the styled DataFrame. **Constraints:** - You must use the `Styler` object and its methods to apply the styles. - You must not modify the input DataFrame directly. **Example Usage:** ```python html_output = style_dataframe(df) print(html_output) ``` **Expected Output:** The function should return an HTML string where: - The maximum value in each column is highlighted. - A background gradient is applied to the DataFrame. - A custom caption is set. Use the provided DataFrame `df` to demonstrate the expected styling in your function implementation.","solution":"import pandas as pd def style_dataframe(df: pd.DataFrame) -> str: Styles the input DataFrame and returns the styled HTML string. Parameters: df (pd.DataFrame): The DataFrame to be styled. Returns: str: The HTML string of the styled DataFrame. styler = df.style styler = styler.highlight_max() styler = styler.background_gradient() styler = styler.set_caption(\\"Styled DataFrame with maximum values highlighted and gradient background\\") return styler.to_html()"},{"question":"You are provided with a dataset `students_performance.csv` containing information about students\' scores in different subjects and their related details. Your task is to plot a heatmap using the Seaborn library to visualize the correlation between different variables in the dataset. Your heatmap should include specific customizations as described below. **Dataset Details:** - The `students_performance.csv` file includes the following columns: - `math score`: Students\' scores in mathematics. - `reading score`: Students\' scores in reading. - `writing score`: Students\' scores in writing. - `gender`: Gender of the students. - `race/ethnicity`: Students\' race/ethnicity group. - `parental level of education`: Parents\' highest education level. - `lunch`: Type of lunch the student receives (standard/free/reduced). - `test preparation course`: Whether or not the student completed a test preparation course. **Requirements:** 1. Load the dataset using pandas. 2. Compute the correlation matrix for the numerical columns (`math score`, `reading score`, and `writing score`). 3. Plot a heatmap using Seaborn to visualize the correlation matrix with the following customizations: - Enable annotations on the heatmap cells. - Use a formatting string to ensure that the annotations display with two decimal places. - Add lines between cells for better readability. - Use a colormap of your choice. - Set the colormap norm such that the minimum is 0.5 and the maximum is 1.0. - Make additional customizations using matplotlib axes methods to improve the aesthetic appearance of the plot (e.g., remove axis labels, reposition ticks). **Constraints:** - You are allowed to use only pandas and seaborn (with possible matplotlib integration). **Input:** - A CSV file named `students_performance.csv`. **Output:** - A visualized heatmap as described. **Example:** ```python # Basic structure of your code import pandas as pd import seaborn as sns import matplotlib.pyplot as plt # Load dataset df = pd.read_csv(\'students_performance.csv\') # Compute correlation matrix correlation_matrix = df[[\'math score\', \'reading score\', \'writing score\']].corr() # Plot heatmap plt.figure(figsize=(10, 8)) ax = sns.heatmap(correlation_matrix, annot=True, fmt=\\".2f\\", linewidths=.5, cmap=\\"coolwarm\\", vmin=0.5, vmax=1.0) ax.set(title=\\"Correlation Matrix of Students\' Scores\\") plt.show() ``` **Note:** Adjust the code to meet all the customization requirements above, ensuring you clearly understand and apply the Seaborn functionalities demonstrated in the documentation provided.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def plot_student_performance_heatmap(csv_file): Plots a heatmap for the correlation matrix of scores in \'math\', \'reading\', and \'writing\' for the given CSV file containing students\' performance data. :param csv_file: The path to the \'students_performance.csv\' file. # Load the dataset df = pd.read_csv(csv_file) # Compute the correlation matrix for the numerical columns correlation_matrix = df[[\'math score\', \'reading score\', \'writing score\']].corr() # Plot the heatmap plt.figure(figsize=(10, 8)) ax = sns.heatmap(correlation_matrix, annot=True, fmt=\\".2f\\", linewidths=.5, cmap=\\"coolwarm\\", vmin=0.5, vmax=1.0) # Additional customizations ax.set_xticks([]) ax.set_yticks([]) ax.set_title(\\"Correlation Matrix of Students\' Scores\\") plt.show() # Example use case # plot_student_performance_heatmap(\'students_performance.csv\')"},{"question":"# Question: Socket Programming Exercise Your task is to implement a function `fetch_http_headers` that connects to a given hostname and port using a socket, sends an HTTP GET request, and retrieves the HTTP response headers. Your function should demonstrate understanding of socket creation, connection, data transmission, reception, and handling timeouts. # Function Signature: ```python def fetch_http_headers(hostname: str, port: int, timeout: float) -> str: ``` # Inputs: - `hostname` (str): The domain name of the server to connect to (e.g., \'example.com\'). - `port` (int): The port number to connect to (e.g., 80). - `timeout` (float): The timeout duration in seconds. # Outputs: - Returns the HTTP response headers as a string. # Constraints: - The function should handle typical network-related exceptions (e.g., connection errors, timeouts) gracefully and raise a meaningful error message. - Use a buffer size of 4096 bytes when receiving data from the socket. - Ensure that the socket connection is closed properly after the operation to prevent resource leaks. # Example: ```python headers = fetch_http_headers(\\"example.com\\", 80, 5.0) print(headers) ``` # Implementation Notes: 1. Create a socket using `socket.socket()`. 2. Set the socket timeout using `settimeout()`. 3. Connect to the server using `connect()`. 4. Send an HTTP GET request encoded in bytes. 5. Receive the response from the server and extract the headers. 6. Close the socket properly to release system resources. # Code Template: ```python import socket def fetch_http_headers(hostname: str, port: int, timeout: float) -> str: try: # 1. Create a socket sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM) # 2. Set socket timeout sock.settimeout(timeout) # 3. Connect to the server sock.connect((hostname, port)) # 4. Send HTTP GET request request = f\'GET / HTTP/1.1rnHost: {hostname}rnConnection: closernrn\' sock.sendall(request.encode()) # 5. Receive the response response = b\\"\\" buffer_size = 4096 while True: data = sock.recv(buffer_size) if not data: break response += data # 6. Extract the headers headers = response.split(b\'rnrn\')[0].decode() return headers except socket.timeout: raise RuntimeError(\\"Timeout occurred while connecting to the server\\") except socket.error as e: raise RuntimeError(f\\"Socket error occurred: {e}\\") finally: # 7. Close the socket sock.close() # Example usage if __name__ == \\"__main__\\": headers = fetch_http_headers(\\"example.com\\", 80, 5.0) print(headers) ``` # Notes: - The example above sends an HTTP GET request to fetch the headers of the home page of the given hostname. - Ensure to handle exceptions like `socket.timeout` and `socket.error` to provide meaningful error messages. - Properly close the socket using the `finally` block to ensure it is always executed, even if an error occurs.","solution":"import socket def fetch_http_headers(hostname: str, port: int, timeout: float) -> str: try: # 1. Create a socket sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM) # 2. Set socket timeout sock.settimeout(timeout) # 3. Connect to the server sock.connect((hostname, port)) # 4. Send HTTP GET request request = f\'GET / HTTP/1.1rnHost: {hostname}rnConnection: closernrn\' sock.sendall(request.encode()) # 5. Receive the response response = b\\"\\" buffer_size = 4096 while True: data = sock.recv(buffer_size) if not data: break response += data # 6. Extract the headers headers = response.split(b\'rnrn\')[0].decode() return headers except socket.timeout: raise RuntimeError(\\"Timeout occurred while connecting to the server\\") except socket.error as e: raise RuntimeError(f\\"Socket error occurred: {e}\\") finally: # 7. Close the socket sock.close()"},{"question":"Advanced Text Processing with Regular Expressions Objective: Assess your understanding of regular expressions and their application in text processing by using Python\'s `re` module. Task: You are to implement a function `find_complex_patterns` that detects complex textual patterns in a given string. The function will identify instances of text that follow a specific pattern: * The pattern consists of a word (comprising only alphabets) followed immediately by a sequence of digits. * This pattern should be case-insensitive. * Each detected pattern should retain the original casing of the alphabets in the string. Expected Input and Output: - **Input**: A single string `text_input`. - **Output**: A list of all substrings in `text_input` that match the specified pattern. Function Signature: ```python def find_complex_patterns(text_input: str) -> list: pass ``` Constraints: - The input string `text_input` will have a length between 1 and 10000 characters. - Words can be of any length. Performance Requirements: - Your solution should efficiently handle the maximum input size within a reasonable time frame. Example: ```python # Example 1 text_input = \\"The accounts have ID123 and code456 with different identifiers code789.\\" assert find_complex_patterns(text_input) == [\\"ID123\\", \\"code456\\", \\"code789\\"] # Example 2 text_input = \\"Review the doc123 and provide feedback on num456. Also, verify foo789 is done.\\" assert find_complex_patterns(text_input) == [\\"doc123\\", \\"num456\\", \\"foo789\\"] ``` Additional Guidelines: - Make sure to import the `re` module for regular expression operations. - Write comprehensive docstrings for your function to explain the logic and thought process. - Consider edge cases such as empty strings and strings without any matching patterns.","solution":"import re def find_complex_patterns(text_input: str) -> list: This function finds and returns all substrings in the text_input that match the pattern of a word (composed of alphabets) immediately followed by a sequence of digits. Args: text_input (str): The input string where patterns will be searched. Returns: list: A list containing all matched patterns. # Define the regex pattern to match a word (letters only) followed by digits pattern = re.compile(r\'b[a-zA-Z]+[0-9]+b\') # Find all matches in the input text matches = pattern.findall(text_input) return matches"},{"question":"**Coding Assessment Question: Pandas Data Visualization** # Objective The following task is designed to assess your understanding of Pandas data visualization capabilities and how to use them in combination with data manipulation methods in Pandas. # Problem Description You are provided with historical stock price data for several companies over a period. The goal is to create a comprehensive visualization that includes different types of plots to show various aspects of the data. You will need to perform some data manipulation and handle missing data before visualizing it. # Task Write a function `visualize_stock_data` that performs the following steps: 1. **Data Preparation:** - Read the stock price data from a CSV file. The CSV file contains columns `Date`, `Company`, `Open`, `Close`, `High`, `Low`, `Volume`. - Pivot the data so that each company\'s stock data is in its own column, indexed by the `Date`. 2. **Data Manipulation:** - Calculate the daily return (percentage change) for each company. - Handle any missing data in the return calculations by filling them with 0. 3. **Visualization:** - Create a line plot showing the daily closing prices of each company. - Create a bar plot showing the total trading volume for each company over the entire period. - Create a scatter plot showing the daily returns of two companies of your choice on the same plot. 4. **Customization:** - Customize the line plot to include a legend, labels for the x and y axes, and a title. - Customize the bar plot to include a legend, labels for the x and y axes, and a title. - Customize the scatter plot to include labels for the x and y axes, a title, and different colors for each company. # Input - A string `csv_file_path` representing the file path to the CSV file containing the stock price data. # Output Your function should display the visualizations described above. There is no need to return any values. # Example ```python def visualize_stock_data(csv_file_path): import pandas as pd import matplotlib.pyplot as plt # Step 1: Read the CSV file df = pd.read_csv(csv_file_path) # Step 2: Pivot the data df_pivot = df.pivot(index=\'Date\', columns=\'Company\', values=[\'Open\', \'Close\', \'High\', \'Low\', \'Volume\']) # Step 3: Calculate daily returns daily_returns = df_pivot[\'Close\'].pct_change().fillna(0) # Step 4: Create visualizations # Line plot for closing prices plt.figure() df_pivot[\'Close\'].plot() plt.title(\'Daily Closing Prices\') plt.xlabel(\'Date\') plt.ylabel(\'Closing Price\') plt.legend(title=\'Companies\') plt.show() # Bar plot for total volume total_volume = df_pivot[\'Volume\'].sum() plt.figure() total_volume.plot(kind=\'bar\') plt.title(\'Total Trading Volume\') plt.xlabel(\'Company\') plt.ylabel(\'Volume\') plt.legend(title=\'Companies\') plt.show() # Scatter plot for daily returns of two companies plt.figure() plt.scatter(daily_returns[\'Company1\'], daily_returns[\'Company2\'], color=\'b\', label=\'Company1 vs Company2\') plt.title(\'Daily Returns: Company1 vs Company2\') plt.xlabel(\'Company1 Daily Return\') plt.ylabel(\'Company2 Daily Return\') plt.legend() plt.show() ``` Please use the stock price data CSV file to test your function and ensure all the requirements are met.","solution":"def visualize_stock_data(csv_file_path): import pandas as pd import matplotlib.pyplot as plt # Step 1: Read the CSV file df = pd.read_csv(csv_file_path) # Step 2: Pivot the data df_pivot = df.pivot(index=\'Date\', columns=\'Company\', values=[\'Open\', \'Close\', \'High\', \'Low\', \'Volume\']) # Step 3: Calculate daily returns daily_returns = df_pivot[\'Close\'].pct_change().fillna(0) # Step 4: Create visualizations # Line plot for closing prices plt.figure() df_pivot[\'Close\'].plot() plt.title(\'Daily Closing Prices\') plt.xlabel(\'Date\') plt.ylabel(\'Closing Price\') plt.legend(title=\'Companies\') plt.show() # Bar plot for total volume total_volume = df_pivot[\'Volume\'].sum() plt.figure() total_volume.plot(kind=\'bar\') plt.title(\'Total Trading Volume\') plt.xlabel(\'Company\') plt.ylabel(\'Volume\') plt.legend(title=\'Companies\') plt.show() # Scatter plot for daily returns of two companies companies = daily_returns.columns.tolist() if len(companies) >= 2: plt.figure() plt.scatter(daily_returns[companies[0]], daily_returns[companies[1]], color=\'b\', label=f\'{companies[0]} vs {companies[1]}\') plt.title(f\'Daily Returns: {companies[0]} vs {companies[1]}\') plt.xlabel(f\'{companies[0]} Daily Return\') plt.ylabel(f\'{companies[1]} Daily Return\') plt.legend() plt.show()"},{"question":"**Objective:** You are required to create a function that utilizes the `ossaudiodev` module to open an audio device, configure it with specific parameters, write audio data to it, and handle any potential errors that might occur during this process. **Task:** Implement a function `configure_and_write_audio(device: str, mode: str, format: str, channels: int, samplerate: int, audio_data: bytes) -> str` that: 1. Opens an audio device with the given `device` name and `mode`. 2. Configures the audio device with the specified `format`, `channels`, and `samplerate`. 3. Writes the provided `audio_data` to the audio device. 4. Handles errors gracefully and returns a descriptive message. **Expected Input and Output:** - **Input:** - `device`: A string representing the audio device filename (e.g., `/dev/dsp`). - `mode`: A string representing the mode to open the device in, which can be `\'r\'`, `\'w\'`, or `\'rw\'`. - `format`: A string representing the audio format to set (e.g., `\'AFMT_S16_LE\'`). - `channels`: An integer specifying the number of channels (e.g., `1` for mono, `2` for stereo). - `samplerate`: An integer representing the sample rate (e.g., `44100` for CD quality). - `audio_data`: A bytes-like object containing the audio data to write. - **Output:** - A string indicating the success or failure of the operation. Possible messages: - `\\"Success: Audio data written successfully.\\"` - `\\"Error: [Error Description]\\"` **Constraints:** - Use the functions and methods from the `ossaudiodev` module to interact with the audio device. - Assume the audio format, channel, and sample rate values are valid and supported by the device. - Handle all errors gracefully and return appropriate error messages. **Function Signature:** ```python def configure_and_write_audio(device: str, mode: str, format: str, channels: int, samplerate: int, audio_data: bytes) -> str: pass ``` **Example:** ```python audio_data = b\'x00x01x02x03\' # Example audio data result = configure_and_write_audio(\'/dev/dsp\', \'w\', \'AFMT_S16_LE\', 2, 44100, audio_data) print(result) # Expected Output: \\"Success: Audio data written successfully.\\" ``` **Note:** - Pay attention to the order of setting parameters (`setfmt()`, `channels()`, `speed()`). - Use appropriate error handling for operations that might fail, such as opening the device or writing data. - Return clear and descriptive error messages for any issues encountered.","solution":"import ossaudiodev def configure_and_write_audio(device: str, mode: str, format: str, channels: int, samplerate: int, audio_data: bytes) -> str: try: dsp = ossaudiodev.open(device, mode) except Exception as e: return f\\"Error opening device: {e}\\" try: dsp.setfmt(getattr(ossaudiodev, format)) dsp.channels(channels) dsp.speed(samplerate) except Exception as e: dsp.close() return f\\"Error configuring device: {e}\\" try: dsp.write(audio_data) except Exception as e: dsp.close() return f\\"Error writing audio data: {e}\\" dsp.close() return \\"Success: Audio data written successfully.\\""},{"question":"**Coding Assessment Question** # Objective Demonstrate your understanding of seaborn’s `lineplot` function and dataset manipulation by creating a detailed visualization from a provided dataset. # Problem Statement You are given two datasets, `flights` and `fmri`, pre-loaded using seaborn. Your task is to manipulate these datasets and generate specialized line plots using seaborn functionalities. # Tasks 1. **Data Preparation:** - Load the `flights` dataset using seaborn. - Pivot the dataset to transform it into wide-form where columns represent months and rows represent years. 2. **Plotting the Flights Data:** - Create a line plot of the wide-form `flights` dataset showing the number of passengers for each month across different years. Each month\'s data should be represented as a separate line. - Customize the plot to use a distinct color palette of your choice. 3. **Data Subsetting and Grouping:** - Load the `fmri` dataset. - Filter the dataset to only include data where the `region` is \\"frontal\\". - Create a line plot showing the `signal` over `timepoint` grouped by `event` and `subject`. Use `units` parameter to plot multiple lines for each `subject` without applying a semantic mapping. 4. **Complex Visualization:** - For the filtered `fmri` dataset, create a line plot where the `signal` over `timepoint` is grouped by both `event` and `region`. - Assign `hue` to `event` and `style` to `region`. Use markers to distinguish the event groups. # Input - The `flights` and `fmri` datasets are pre-loaded using seaborn with the following commands: ```python import seaborn as sns flights = sns.load_dataset(\\"flights\\") fmri = sns.load_dataset(\\"fmri\\") ``` # Output Format - For each task, generate and display the corresponding seaborn line plot. - Ensure each plot is properly labeled and includes a legend. # Constraints - Use seaborn for all data visualizations. - Use pandas for any necessary data manipulation (e.g., pivoting, filtering). - Ensure the plots are clear and readable, with appropriate axis labels and legend descriptions. # Example Usage Below are some code snippets to illustrate parts of the process (not to be copy-pasted directly into your solution): ```python import seaborn as sns import pandas as pd import matplotlib.pyplot as plt # Example data loading flights = sns.load_dataset(\\"flights\\") fmri = sns.load_dataset(\\"fmri\\") # Example wide-form transformation flights_wide = flights.pivot(index=\\"year\\", columns=\\"month\\", values=\\"passengers\\") sns.lineplot(data=flights_wide) plt.show() # Example filtering and plotting fmri_frontal = fmri[fmri[\\"region\\"] == \\"frontal\\"] sns.lineplot(data=fmri_frontal, x=\\"timepoint\\", y=\\"signal\\", hue=\\"event\\", units=\\"subject\\", estimator=None) plt.show() ``` Good luck!","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt def plot_flights_data(): # Load the flights dataset flights = sns.load_dataset(\\"flights\\") # Pivot the dataset to transform it into wide-form flights_wide = flights.pivot(index=\\"year\\", columns=\\"month\\", values=\\"passengers\\") # Create a line plot of the wide-form flights dataset plt.figure(figsize=(15, 8)) sns.lineplot(data=flights_wide, palette=\\"tab10\\") plt.title(\\"Number of Passengers for Each Month Across Different Years\\") plt.xlabel(\\"Year\\") plt.ylabel(\\"Number of Passengers\\") plt.legend(title=\\"Month\\", labels=flights_wide.columns) plt.show() def plot_fmri_data_frontal(): # Load the fmri dataset fmri = sns.load_dataset(\\"fmri\\") # Filter the dataset to only include data where the region is \\"frontal\\" fmri_frontal = fmri[fmri[\\"region\\"] == \\"frontal\\"] # Create a line plot showing the signal over timepoint grouped by event and subject plt.figure(figsize=(15, 8)) sns.lineplot(data=fmri_frontal, x=\\"timepoint\\", y=\\"signal\\", hue=\\"event\\", units=\\"subject\\", estimator=None, lw=1) plt.title(\\"FMRI Signal Over Timepoint for Frontal Region\\") plt.xlabel(\\"Timepoint\\") plt.ylabel(\\"Signal\\") plt.show() def plot_fmri_data_complex(): # Load the fmri dataset fmri = sns.load_dataset(\\"fmri\\") # Create a line plot where the signal over timepoint is grouped by both event and region plt.figure(figsize=(15, 8)) sns.lineplot(data=fmri, x=\\"timepoint\\", y=\\"signal\\", hue=\\"event\\", style=\\"region\\", markers=True) plt.title(\\"FMRI Signal Over Timepoint Grouped by Event and Region\\") plt.xlabel(\\"Timepoint\\") plt.ylabel(\\"Signal\\") plt.show()"},{"question":"# Support Vector Machine Classifier with Custom Kernel for Multi-class Classification Using scikit-learn, implement a Support Vector Machine (SVM) classifier for a multi-class classification problem. The SVM classifier should: 1. Use a custom kernel function. 2. Handle unbalanced class distributions by setting appropriate class weights. 3. Perform cross-validation to evaluate the model\'s performance. # Specifications 1. **Custom Kernel Function**: Define a custom kernel function and use it in the SVM classifier. 2. **Class Weights**: Handle unbalanced data by setting class weights. 3. **Cross-Validation**: Perform 5-fold cross-validation to evaluate the model. # Input and Output Formats Input - `X_train`: A numpy array of shape `(n_samples, n_features)` representing the training data. - `y_train`: A numpy array of shape `(n_samples,)` representing the class labels for each training sample. - `X_test`: A numpy array of shape `(m_samples, n_features)` representing the test data. Output - A numpy array of shape `(m_samples,)` containing the predicted class labels for the test data. # Constraints - Implement the custom kernel function properly. - Use scikit-learn\'s SVM classifier. - Ensure that the model handles unbalanced data using class weights. - Evaluate the model using 5-fold cross-validation. # Question Skeleton ```python import numpy as np from sklearn import svm from sklearn.model_selection import cross_val_score # Define your custom kernel function def custom_kernel(X, Y): # Example: Linear kernel return np.dot(X, Y.T) def svm_classifier_custom_kernel(X_train, y_train, X_test): # Create SVM classifier with custom kernel clf = svm.SVC(kernel=custom_kernel, class_weight=\'balanced\') # Fit the classifier on the training data clf.fit(X_train, y_train) # Evaluate using 5-fold cross-validation scores = cross_val_score(clf, X_train, y_train, cv=5) print(f\'Cross-validation scores: {scores}\') # Predict on the test data y_pred = clf.predict(X_test) return y_pred # Example usage # X_train = np.array([...]) # y_train = np.array([...]) # X_test = np.array([...]) # predictions = svm_classifier_custom_kernel(X_train, y_train, X_test) ``` # Instructions 1. Implement the custom kernel function. 2. Create the SVM classifier with the custom kernel. 3. Ensure the classifier handles unbalanced data using the `class_weight` parameter. 4. Evaluate the classifier using 5-fold cross-validation. 5. Use the trained model to predict labels for the test data.","solution":"import numpy as np from sklearn import svm from sklearn.model_selection import cross_val_score # Define your custom kernel function def custom_kernel(X, Y): Example of a custom linear kernel. return np.dot(X, Y.T) def svm_classifier_custom_kernel(X_train, y_train, X_test): # Create SVM classifier with custom kernel clf = svm.SVC(kernel=custom_kernel, class_weight=\'balanced\') # Perform 5-fold cross-validation scores = cross_val_score(clf, X_train, y_train, cv=5) print(f\'Cross-validation scores: {scores}\') # Fit the classifier on the entire training data clf.fit(X_train, y_train) # Predict on the test data y_pred = clf.predict(X_test) return y_pred"},{"question":"Objective Demonstrate your understanding of seaborn\'s capabilities in customizing plot aesthetics by creating visually appealing and well-structured data visualizations. Question You are given a dataset containing information about the daily temperatures of three different cities over a period of one month. Your task is to write a Python function using seaborn to generate a series of plots that effectively communicate the data, with a focus on using different styles, themes, contexts, and spine adjustments to enhance the visual appeal. Function Signature ```python def plot_temperature_data(data: pd.DataFrame, cities: List[str]) -> None: pass ``` Input - `data`: A pandas DataFrame containing daily temperature data. The DataFrame has the following columns: - `date`: A `datetime` object representing the date. - `city`: A string representing the name of the city. - `temperature`: A float representing the daily temperature of the city. - `cities`: A list of strings containing the names of the three cities present in the `data` DataFrame. Output - The function should not return any value but should generate the following plots: 1. A line plot for each city showing the daily temperature trend, each with a different seaborn style. 2. A combined line plot with all cities, using a specific seaborn theme and improving readability by customizing the axes spines. 3. A set of subplots (2x2 grid) with different axes styles for each subplot, showing the temperature trend for one city per subplot, with the fourth subplot showing all cities together. Constraints and Requirements 1. Use at least three different seaborn styles (e.g., `whitegrid`, `darkgrid`, `ticks`) in separate plots. 2. Demonstrate the use of a seaborn theme (e.g., `set_theme()`) in one of the combined plots. 3. Customize the axes spines in the combined plot to improve aesthetics (e.g., using `despine()`). 4. Use the `with` statement to temporarily set plot parameters for the subplots. 5. The x-axis should represent the date, and the y-axis should represent the temperature. 6. Include appropriate titles and labels for each plot to ensure clarity. Example Usage ```python import pandas as pd import numpy as np data = pd.DataFrame({ \'date\': pd.date_range(start=\'2023-01-01\', periods=30), \'city\': np.random.choice([\'CityA\', \'CityB\', \'CityC\'], size=30), \'temperature\': np.random.uniform(low=-10, high=30, size=30) }) cities = [\'CityA\', \'CityB\', \'CityC\'] plot_temperature_data(data, cities) ``` Notes - Ensure you import all necessary libraries, including `pandas`, `numpy`, `seaborn`, and `matplotlib.pyplot`. - Your function should focus on producing high-quality, informative visualizations that effectively communicate the data trends. - You may use additional seaborn and matplotlib functionalities as needed to accomplish the task.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt from typing import List def plot_temperature_data(data: pd.DataFrame, cities: List[str]) -> None: sns.set_theme(style=\\"ticks\\") # Ensure the \'date\' column is in datetime format data[\'date\'] = pd.to_datetime(data[\'date\']) # Plot 1: Line plot for each city with different seaborn style styles = [\'whitegrid\', \'darkgrid\', \'ticks\'] for city, style in zip(cities, styles): plt.figure(figsize=(10, 6)) sns.set(style=style) city_data = data[data[\'city\'] == city] sns.lineplot(x=\'date\', y=\'temperature\', data=city_data) plt.title(f\'Temperature Trend in {city} with style {style}\') plt.xlabel(\'Date\') plt.ylabel(\'Temperature\') plt.show() # Plot 2: Combined line plot for all cities plt.figure(figsize=(14, 7)) sns.set_theme(style=\\"whitegrid\\") sns.lineplot(x=\'date\', y=\'temperature\', hue=\'city\', data=data) plt.title(\'Combined Temperature Trends for All Cities\') plt.xlabel(\'Date\') plt.ylabel(\'Temperature\') sns.despine() plt.show() # Plot 3: Subplots with different axes styles fig, axes = plt.subplots(2, 2, figsize=(14, 10)) styles = [\'darkgrid\', \'whitegrid\', \'ticks\', \'white\'] for ax, style, city in zip(axes.flatten(), styles, cities + [\\"All\\"]): with sns.axes_style(style): if city == \\"All\\": sns.lineplot(x=\'date\', y=\'temperature\', hue=\'city\', ax=ax, data=data) ax.set_title(f\'All Cities with style {style}\') else: city_data = data[data[\'city\'] == city] sns.lineplot(x=\'date\', y=\'temperature\', ax=ax, data=city_data) ax.set_title(f\'Temperature Trend in {city} with style {style}\') ax.set_xlabel(\'Date\') ax.set_ylabel(\'Temperature\') plt.tight_layout() plt.show()"},{"question":"Secure Data Fetching with HTTP Context You are working on a Python project needing to interact with a RESTful API. You must securely connect to the HTTPS server, send various HTTP requests, and handle the responses appropriately. Objective Implement a class `ApiClient` that connects to a specified HTTPS server, sends GET and POST requests, and processes the responses. You should handle different types of responses and potential errors gracefully. Class Definition ```python class ApiClient: def __init__(self, host, port=443): Initialize the ApiClient with the specified host and port (default to 443 for HTTPS). Args: - host (str): The host address of the server. - port (int): The port number (default is 443). pass def set_headers(self, headers): Set the headers to be used in the requests. Args: - headers (dict): A dictionary of HTTP headers. pass def get(self, path): Send a GET request to the specified path. Args: - path (str): The path for the GET request. Returns: - tuple: (status code, response body as bytes) pass def post(self, path, data): Send a POST request to the specified path with the given data. Args: - path (str): The path for the POST request. - data (str): The data to be sent in the body of the POST request. Returns: - tuple: (status code, response body as bytes) pass def close(self): Close the connection. pass ``` Implementation Details 1. **Initialization and Connection:** - The `__init__` method initializes an `HTTPSConnection` object to the specified host and port. 2. **Setting Headers:** - The `set_headers` method allows setting custom headers for the requests. 3. **Sending Requests:** - The `get` method sends a GET request to the specified path and returns a tuple with the status code and response body. - The `post` method sends a POST request with data to the specified path and returns a tuple with the status code and response body. 4. **Handling Responses:** - Process the `HTTPResponse` object to read the status and body correctly. - Handle possible exceptions, such as `http.client.HTTPException`, and return appropriate error messages or codes. 5. **Closing the Connection:** - The `close` method ensures the connection is properly closed. Input and Output **Input:** - Initialization parameters for the `ApiClient`: `host` and optional `port`. - Headers dictionary for the `set_headers` method. - Path strings and data for the `get` and `post` methods. **Output:** - A tuple (status code, response body as bytes) for the `get` and `post` methods. Constraints - Ensure SSL context is handled properly for secure connections. - Default port should be 443 for HTTPS unless specified otherwise. - Test the class with examples demonstrating initialization, sending GET and POST requests, and closing the connection. Example Usage ```python client = ApiClient(\\"www.python.org\\") client.set_headers({\\"User-Agent\\": \\"ApiClient\\", \\"Accept\\": \\"application/json\\"}) status, response = client.get(\\"/\\") print(\\"GET Request Status:\\", status) print(\\"GET Request Response:\\", response) post_data = \\"name=Test&description=Testing\\" status, response = client.post(\\"/test-endpoint\\", post_data) print(\\"POST Request Status:\\", status) print(\\"POST Request Response:\\", response) client.close() ``` Your task is to finish the implementation of the `ApiClient` class as described above.","solution":"import http.client import json class ApiClient: def __init__(self, host, port=443): Initialize the ApiClient with the specified host and port (default to 443 for HTTPS). Args: - host (str): The host address of the server. - port (int): The port number (default is 443). self.host = host self.port = port self.connection = http.client.HTTPSConnection(self.host, self.port) self.headers = {} def set_headers(self, headers): Set the headers to be used in the requests. Args: - headers (dict): A dictionary of HTTP headers. self.headers.update(headers) def get(self, path): Send a GET request to the specified path. Args: - path (str): The path for the GET request. Returns: - tuple: (status code, response body as bytes) try: self.connection.request(\\"GET\\", path, headers=self.headers) response = self.connection.getresponse() status = response.status body = response.read() return (status, body) except Exception as e: return (None, str(e).encode()) def post(self, path, data): Send a POST request to the specified path with the given data. Args: - path (str): The path for the POST request. - data (str): The data to be sent in the body of the POST request. Returns: - tuple: (status code, response body as bytes) try: self.connection.request(\\"POST\\", path, body=data, headers=self.headers) response = self.connection.getresponse() status = response.status body = response.read() return (status, body) except Exception as e: return (None, str(e).encode()) def close(self): Close the connection. self.connection.close()"},{"question":"**Email Header Formatter and Parser** Given the need to handle email messages accurately, you are to implement a function `create_email_header()` that takes the following parameters: - `realname: str` - The real name of the email sender. - `email_address: str` - The email address of the sender. - `date: str` - A date string in any of the formats according to RFC 2822. - `idstring: str` - An optional string to strengthen the uniqueness of the message id (default is `None`). - `domain: str` - An optional domain for the message id (default is `None`). Your function should return a dictionary containing formatted email header fields: - `Message-ID`: A unique message ID generated using `make_msgid()`. - `From`: A properly formatted From field using `formataddr()`. - `Date`: The given date string converted to the format used in email headers using `format_datetime()`. Additionally, implement a parser function `parse_email_header(headers: dict)` that takes a dictionary of such header fields, parses the information, and returns a dictionary with the parsed components: - `realname`: Parsed real name from the `From` field. - `email_address`: Parsed email address from the `From` field. - `date`: Parsed date returned as a `datetime` object. - `msgid`: The unique message id. **Constraints:** - Ensure the given date string is valid according to RFC 2822. - Handle any potential parsing errors gracefully and return `None` for any header field that couldn\'t be parsed. ```python from email.utils import make_msgid, formataddr, format_datetime, parseaddr, parsedate_to_datetime def create_email_header(realname: str, email_address: str, date: str, idstring: str = None, domain: str = None) -> dict: Creates a formatted email header with the given information. Parameters: - realname: str - email_address: str - date: str - idstring: str - domain: str Returns: - dict: Contains \'Message-ID\', \'From\', \'Date\' fields formatted according to email standards. # Your implementation here def parse_email_header(headers: dict) -> dict: Parses the given email headers and returns the component information. Parameters: - headers: dict Returns: - dict: Contains \'realname\', \'email_address\', \'date\', \'msgid\' parsed from the headers. # Your implementation here # Example Usage: # email_headers = create_email_header(\\"John Doe\\", \\"john.doe@example.com\\", \\"Mon, 20 Nov 1995 19:12:08 -0500\\") # parsed_headers = parse_email_header(email_headers) ``` **Input Format:** - For `create_email_header()`: `realname`, `email_address`, `date`, `idstring`, `domain` are all strings. - For `parse_email_header()`: `headers` is a dictionary with `Message-ID`, `From`, and `Date` keys. **Output Format:** - For `create_email_header()`: Dictionary with keys `Message-ID`, `From`, `Date`. - For `parse_email_header()`: Dictionary with keys `realname`, `email_address`, `date`, `msgid`.","solution":"from email.utils import make_msgid, formataddr, format_datetime, parseaddr, parsedate_to_datetime def create_email_header(realname: str, email_address: str, date: str, idstring: str = None, domain: str = None) -> dict: Creates a formatted email header with the given information. Parameters: - realname: str - email_address: str - date: str - idstring: str - domain: str Returns: - dict: Contains \'Message-ID\', \'From\', \'Date\' fields formatted according to email standards. message_id = make_msgid(idstring, domain) from_field = formataddr((realname, email_address)) try: date_field = format_datetime(parsedate_to_datetime(date)) except Exception as e: date_field = None header = { \'Message-ID\': message_id, \'From\': from_field, \'Date\': date_field } return header def parse_email_header(headers: dict) -> dict: Parses the given email headers and returns the component information. Parameters: - headers: dict Returns: - dict: Contains \'realname\', \'email_address\', \'date\', \'msgid\' parsed from the headers. realname, email_address = parseaddr(headers.get(\'From\', \'\')) try: date = parsedate_to_datetime(headers.get(\'Date\', \'\')) except Exception as e: date = None msgid = headers.get(\'Message-ID\', \'\') parsed_header = { \'realname\': realname if realname else None, \'email_address\': email_address if email_address else None, \'date\': date, \'msgid\': msgid if msgid else None } return parsed_header"},{"question":"# Coding Assignment: Tuning Decision Thresholds for a Classification Model Context: In this assignment, you will implement and evaluate a binary classifier for a medical diagnosis task. The primary goal is to ensure high recall, meaning the model should correctly identify as many positive cases (patients with a disease) as possible, even if it leads to some false positives. You will achieve this by tuning the decision threshold of the classifier. Task: 1. **Dataset Preparation**: - Generate a synthetic binary classification dataset using `sklearn.datasets.make_classification`. The dataset should be imbalanced to simulate a real-world medical diagnosis scenario. - Use an 80-20 train-test split. 2. **Model Training**: - Train a `LogisticRegression` model on the training set. - Evaluate the model on the test set using default threshold (0.5) for `predict`. 3. **Threshold Tuning**: - Use `TunedThresholdClassifierCV` to find the optimal decision threshold for maximizing recall on the training set. Apply this optimal threshold to make predictions on the test set. 4. **Comparative Analysis**: - Compare the performance (recall, precision, F1-score) of the logistic regression model with default and tuned thresholds on the test set. Expected Implementation: You need to write a function `tune_threshold_for_recall` that performs the tasks described above. ```python from typing import Tuple, Any import numpy as np from sklearn.datasets import make_classification from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression from sklearn.model_selection import TunedThresholdClassifierCV from sklearn.metrics import classification_report, make_scorer, recall_score def tune_threshold_for_recall() -> Tuple[Any, Any]: # Generate synthetic dataset X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, weights=[0.1, 0.9], random_state=0) # Train-test split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, stratify=y) # Train Logistic Regression model base_model = LogisticRegression() base_model.fit(X_train, y_train) # Predictions and performance with default threshold y_pred_default = base_model.predict(X_test) report_default = classification_report(y_test, y_pred_default) # Optimize threshold using TunedThresholdClassifierCV for recall scorer = make_scorer(recall_score, pos_label=1) model = TunedThresholdClassifierCV(base_model, scoring=scorer) model.fit(X_train, y_train) # Predictions with tuned threshold y_pred_tuned = model.predict(X_test) report_tuned = classification_report(y_test, y_pred_tuned) return (report_default, report_tuned) ``` Input: - The function does not take any inputs; it generates the dataset internally. Output: - The function returns a tuple containing two classification reports: one for the default threshold and one for the tuned threshold. Constraints: - Ensure that the train-test split is stratified to maintain the class distribution in train and test sets. - Use sensible defaults for the `LogisticRegression` model parameters and other functions unless specified otherwise. Performance Requirements: - The function should execute within a reasonable time frame, given the synthetic dataset size constraints. Notes: - Analyze the classification reports to understand the impact of tuning the decision threshold on recall and other metrics. - Consider edge cases such as very low or high prevalence of positive cases in the dataset.","solution":"from typing import Tuple, Any import numpy as np from sklearn.datasets import make_classification from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression from sklearn.metrics import classification_report, make_scorer, recall_score, precision_recall_curve def tune_threshold_for_recall() -> Tuple[Any, Any]: # Generate synthetic dataset X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, weights=[0.1, 0.9], random_state=0) # Train-test split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, stratify=y) # Train Logistic Regression model base_model = LogisticRegression() base_model.fit(X_train, y_train) # Predictions and performance with default threshold y_pred_default = base_model.predict(X_test) report_default = classification_report(y_test, y_pred_default, output_dict=True) # Probability scores for the test data y_proba = base_model.predict_proba(X_test)[:, 1] # Determine thresholds to maximize recall on the training set precision, recall, thresholds = precision_recall_curve(y_train, base_model.predict_proba(X_train)[:, 1]) optimal_idx = np.argmax(recall) # We prioritize recall maximization optimal_threshold = thresholds[optimal_idx] # Apply the optimal threshold to make predictions on the test set y_pred_tuned = (y_proba >= optimal_threshold).astype(int) report_tuned = classification_report(y_test, y_pred_tuned, output_dict=True) return (report_default, report_tuned)"},{"question":"**Objective**: Demonstrate your understanding of the `itertools`, `functools`, and `operator` modules in Python by solving the following problem. # Problem Statement Write a function `process_data(data: List[Tuple[str, List[int]]]) -> List[str]` that processes a list of tuples. Each tuple contains a string (name of a product) and a list of integers (prices at different stores). The function should return a list of strings, each containing the product name followed by the maximum price it can be sold at and the minimum price it can be bought at. # Detailed Requirements 1. The function should use the `itertools`, `functools`, and `operator` modules to process the data. 2. For each product, find the maximum price (best selling price) and minimum price (best buying price) using appropriate functions from these modules. 3. Format each result string in this format: \\"ProductName: MaxPrice, MinPrice\\" 4. The resulting list should be sorted alphabetically by product name. Expected Input - A list of tuples where each tuple consists of: - A string representing the product name. - A list of integers representing the prices at different stores. Example: ```python [ (\\"apple\\", [10, 20, 30]), (\\"banana\\", [5, 15, 25]), (\\"carrot\\", [2, 6, 10]) ] ``` Expected Output - A list of strings where each string is in the format: \\"ProductName: MaxPrice, MinPrice\\". Example: ```python [ \\"apple: 30, 10\\", \\"banana: 25, 5\\", \\"carrot: 10, 2\\" ] ``` Constraints - The input list will not be empty. - Each price list will contain at least one integer. - Product names consist of lower-case letters only. - You must use appropriate functions from `itertools`, `functools`, and `operator`. # Constraints for Performance - Aim for a time complexity that is linear with respect to the total number of prices. - Using any built-in max or min functions directly on lists is not allowed; rely on `itertools`, `functools`, and `operator` modules instead. # Function Signature ```python from typing import List, Tuple def process_data(data: List[Tuple[str, List[int]]]) -> List[str]: # Your code here ``` Good luck and happy coding!","solution":"from typing import List, Tuple import itertools import functools import operator def process_data(data: List[Tuple[str, List[int]]]) -> List[str]: results = [] for name, prices in data: max_price = functools.reduce(lambda x, y: x if x > y else y, prices) min_price = functools.reduce(lambda x, y: x if x < y else y, prices) results.append(f\\"{name}: {max_price}, {min_price}\\") results.sort() # Sort list of strings alphabetically by product name return results"},{"question":"You are required to create a Python program using the `argparse` module that simulates a mini command-line calculator with multiple functionalities. The program should support the following operations: 1. **Addition**: Adds two or more numbers. 2. **Subtraction**: Subtracts second and subsequent numbers from the first number. 3. **Multiplication**: Multiplies two or more numbers. 4. **Division**: Divides the first number by the second and subsequent numbers. 5. **Power**: Raises the base to the power of the exponent. The program should also support optional flags to increase verbosity of output and display help messages. # Requirements 1. **Positional Argument: Operation** - The operation to perform. Allowed values are `add`, `subtract`, `multiply`, `divide`, and `power`. 2. **Positional Argument: Numbers** - A list of numbers (floating-point or integer) on which the operation will be performed. 3. **Optional Argument: Verbosity** - `-v` or `--verbosity`: Increases output verbosity. This argument can be provided multiple times to increase the verbosity level. 4. **Optional Argument: Help** - Display help messages describing the program usage. # Input Format The program should be executed from the command line with the following format: ``` python calculator.py <operation> <numbers> [-v] [--help] ``` - `<operation>`: A string specifying the operation (`add`, `subtract`, `multiply`, `divide`, or `power`). - `<numbers>`: A list of numbers on which to perform the operation (space-separated). - `[-v]`: An optional flag to increase verbosity. It can be provided multiple times. # Output Format The program should print the result of the operation with varying verbosity based on the `verbosity` flag. - Verbosity level 0: Print only the result. - Verbosity level 1: Print the operation performed and the result. - Verbosity level 2: Print detailed info about the operation, the numbers involved, and the result. # Constraints - The operation should be one of the specified types (`add`, `subtract`, `multiply`, `divide`, or `power`). - The list of numbers should be non-empty. - For `divide` and `power`, ensure proper error handling to avoid division by zero or invalid base/exponent conditions. # Example Command: ``` python calculator.py add 1 2 3 4 -vv ``` Output: ``` Performing addition on numbers: [1, 2, 3, 4] Result of addition: 10.0 ``` Command: ``` python calculator.py multiply 2 5 3 -v ``` Output: ``` 2 * 5 * 3 == 30 ``` # Implementation Implement the command-line calculator in a Python script named `calculator.py` using the `argparse` module. ```python import argparse def add(numbers): return sum(numbers) def subtract(numbers): result = numbers[0] for num in numbers[1:]: result -= num return result def multiply(numbers): result = 1 for num in numbers: result *= num return result def divide(numbers): result = numbers[0] for num in numbers[1:]: if num == 0: raise ValueError(\\"Division by zero is not allowed\\") else: result /= num return result def power(numbers): if len(numbers) != 2: raise ValueError(\\"Power operation requires exactly two arguments\\") base, exponent = numbers return base ** exponent def main(): parser = argparse.ArgumentParser(description=\\"Mini command-line calculator.\\") parser.add_argument(\\"operation\\", choices=[\\"add\\", \\"subtract\\", \\"multiply\\", \\"divide\\", \\"power\\"], help=\\"The operation to perform.\\") parser.add_argument(\\"numbers\\", type=float, nargs=\'+\', help=\\"Numbers on which to perform the operation.\\") parser.add_argument(\\"-v\\", \\"--verbosity\\", action=\\"count\\", default=0, help=\\"Increase output verbosity\\") args = parser.parse_args() operations = { \\"add\\": add, \\"subtract\\": subtract, \\"multiply\\": multiply, \\"divide\\": divide, \\"power\\": power } try: result = operations[args.operation](args.numbers) except Exception as e: print(f\\"Error: {e}\\") return if args.verbosity >= 2: print(f\\"Performing {args.operation} on numbers: {args.numbers}\\") print(f\\"Result of {args.operation}: {result}\\") elif args.verbosity >= 1: print(f\\"{(\' {} \'.format(args.operation)).join(map(str, args.numbers))} == {result}\\") else: print(result) if __name__ == \\"__main__\\": main() ``` # Explanation - Define functions for each operation (`add`, `subtract`, `multiply`, `divide`, `power`). - Create an argument parser and add positional and optional arguments. - Map operations to corresponding functions. - Handle errors for invalid operations and input values. - Print the result with varying verbosity based on the `verbosity` flag.","solution":"import argparse def add(numbers): return sum(numbers) def subtract(numbers): result = numbers[0] for num in numbers[1:]: result -= num return result def multiply(numbers): result = 1 for num in numbers: result *= num return result def divide(numbers): result = numbers[0] for num in numbers[1:]: if num == 0: raise ValueError(\\"Division by zero is not allowed\\") result /= num return result def power(numbers): if len(numbers) != 2: raise ValueError(\\"Power operation requires exactly two arguments\\") base, exponent = numbers return base ** exponent def main(): parser = argparse.ArgumentParser(description=\\"Mini command-line calculator.\\") parser.add_argument(\\"operation\\", choices=[\\"add\\", \\"subtract\\", \\"multiply\\", \\"divide\\", \\"power\\"], help=\\"The operation to perform.\\") parser.add_argument(\\"numbers\\", type=float, nargs=\'+\', help=\\"Numbers on which to perform the operation.\\") parser.add_argument(\\"-v\\", \\"--verbosity\\", action=\\"count\\", default=0, help=\\"Increase output verbosity\\") args = parser.parse_args() operations = { \\"add\\": add, \\"subtract\\": subtract, \\"multiply\\": multiply, \\"divide\\": divide, \\"power\\": power } try: result = operations[args.operation](args.numbers) except Exception as e: print(f\\"Error: {e}\\") return if args.verbosity >= 2: print(f\\"Performing {args.operation} on numbers: {args.numbers}\\") print(f\\"Result of {args.operation}: {result}\\") elif args.verbosity >= 1: print(f\\"{args.operation.capitalize()} on {args.numbers} == {result}\\") else: print(result) if __name__ == \\"__main__\\": main()"},{"question":"# PyTorch Coding Assessment: Implementing a Model in TorchScript **Objective**: You are required to implement a simplified neural network class using PyTorch and convert it to TorchScript. This assessment will test your understanding of TorchScript\'s type constraints, type annotations, and functionalities. **Task**: Create a TorchScript-compatible PyTorch neural network class `SimpleNet` with the following specifications: 1. The class should inherit from `torch.nn.Module`. 2. It should have the following layers: - A fully connected layer (`fc1`) taking an input size of 128 and outputting 64 features. - A ReLU activation layer. - A fully connected layer (`fc2`) taking an input size of 64 and outputting 10 features (output layer). 3. Implement a `forward` method that takes a tensor as input, computes the network\'s forward pass, and returns the output tensor. 4. Create another method `probabilities` that converts the output logits from the `forward` method into probabilities using softmax. 5. Ensure that the class is TorchScript-compatible using the `@torch.jit.script` decorator where necessary. **Constraints**: - Ensure type annotations are used correctly. - The module should be able to handle inputs of shape `[N, 128]` where `N` is the batch size. - Manage types carefully to ensure the code can be compiled to TorchScript. **Performance Requirements**: - The implemented model should process inputs efficiently without any significant performance overhead. **Expected Input and Output**: - Input: A tensor of shape `[N, 128]`. - Output: Probabilities of shape `[N, 10]`. **Implementation**: ```python import torch import torch.nn as nn from typing import Tuple # Define the SimpleNet class class SimpleNet(nn.Module): def __init__(self): super(SimpleNet, self).__init__() self.fc1 = nn.Linear(128, 64) self.relu = nn.ReLU() self.fc2 = nn.Linear(64, 10) @torch.jit.export def forward(self, x: torch.Tensor) -> torch.Tensor: out = self.fc1(x) out = self.relu(out) out = self.fc2(out) return out @torch.jit.export def probabilities(self, x: torch.Tensor) -> torch.Tensor: logits = self.forward(x) return torch.softmax(logits, dim=1) # Function to test the scriptability of SimpleNet def test_script(): # Create an instance of the model model = SimpleNet() # Convert the model to TorchScript script_model = torch.jit.script(model) # Create dummy input data dummy_input = torch.rand(1, 128) # Test the TorchScript model output = script_model.probabilities(dummy_input) print(output) # Run the test function test_script() ``` **Note**: - Ensure that every function inside the class is thoroughly tested and compatible with TorchScript. - The code should handle the TorchScript compilation correctly without errors.","solution":"import torch import torch.nn as nn from typing import Tuple # Define the SimpleNet class class SimpleNet(nn.Module): def __init__(self): super(SimpleNet, self).__init__() self.fc1 = nn.Linear(128, 64) self.relu = nn.ReLU() self.fc2 = nn.Linear(64, 10) @torch.jit.export def forward(self, x: torch.Tensor) -> torch.Tensor: out = self.fc1(x) out = self.relu(out) out = self.fc2(out) return out @torch.jit.export def probabilities(self, x: torch.Tensor) -> torch.Tensor: logits = self.forward(x) return torch.softmax(logits, dim=1) # Function to test the scriptability of SimpleNet def test_script(): # Create an instance of the model model = SimpleNet() # Convert the model to TorchScript script_model = torch.jit.script(model) # Create dummy input data dummy_input = torch.rand(1, 128) # Test the TorchScript model output = script_model.probabilities(dummy_input) print(output) # Run the test function test_script()"},{"question":"Coding Assessment Question # Objective: Implement a function using the `turtle` module to draw a complex geometric pattern. The pattern should be created by using various Turtle methods for drawing shapes, controlling the pen, and handling colors. # Question: Write a Python function `draw_complex_pattern(num_sides, side_length, num_repeats)` that uses the `turtle` module to draw a complex pattern consisting of multiple interlaced polygons. The pattern should meet the following requirements: 1. Draw `num_repeats` regular polygons, each with `num_sides` sides and each side of length `side_length`. 2. Each subsequent polygon should be rotated by a small angle to create a spiraling or rotated effect. 3. The pattern should use different colors for different polygons to enhance visual appeal. 4. The drawing should start from the center of the screen, and the turtle\'s position and orientation should be reset after completing the drawing. # Function Signature: ```python def draw_complex_pattern(num_sides: int, side_length: int, num_repeats: int) -> None: pass ``` # Input: - `num_sides` (int): The number of sides for each polygon (e.g., 3 for triangle, 4 for square). - `side_length` (int): The length of each side of the polygon. - `num_repeats` (int): The number of polygons to draw, each rotated slightly from the last. # Output: - The function does not return anything. Instead, it should display the drawn pattern using the Turtle graphics window. # Example: ```python # Example usage: draw_complex_pattern(6, 100, 36) # Draws a pattern with 36 hexagons, each side length 100, rotated to form a spiraling shape. ``` # Constraints: - `num_sides` should be between 3 and 12. - `side_length` should be a positive integer. - `num_repeats` should be a positive integer less than or equal to 100. # Performance: - The function should efficiently manage the drawing operations and color changes without causing significant delays. # Tips: - You can use the `turtle.color()` method to set the color before drawing each polygon. - Use the `turtle.right(angle)` or `turtle.left(angle)` method to rotate the turtle for the spiraling effect. - Consider using a loop to handle the drawing and rotation of each polygon.","solution":"import turtle def draw_complex_pattern(num_sides: int, side_length: int, num_repeats: int) -> None: Draws a complex pattern using the turtle module. The pattern consists of interlaced polygons which use different colors and slight rotations for a spiraling effect. turtle.speed(0) # Fastest drawing speed colors = [\\"red\\", \\"blue\\", \\"green\\", \\"purple\\", \\"orange\\", \\"yellow\\", \\"cyan\\", \\"magenta\\"] angle = 360 / num_sides rotation_angle = 360 / num_repeats for i in range(num_repeats): turtle.color(colors[i % len(colors)]) for _ in range(num_sides): turtle.forward(side_length) turtle.right(angle) turtle.right(rotation_angle) turtle.hideturtle() turtle.done()"},{"question":"# Objective: Implement a function that connects to a POP3 server, authenticates using the provided credentials, retrieves email messages, and deletes specific messages based on a given condition. # Description: You are required to write a Python function `manage_emails(server, port, username, password, delete_condition)` which: 1. Connects to the specified POP3 server on the given port. 2. Authenticates using the provided username and password. 3. Retrieves all email messages and prints the message number and size of each message. 4. Deletes messages that satisfy the `delete_condition` function. The `delete_condition` is a function that takes a message number and size as parameters and returns `True` if the message should be deleted, otherwise `False`. 5. Finally, properly ends the session by calling the `quit` method. # Function Signature: ```python def manage_emails(server: str, port: int, username: str, password: str, delete_condition: Callable[[int, int], bool]) -> None: pass ``` # Inputs: - `server` (str): The POP3 server address. - `port` (int): The port number to connect to on the POP3 server. - `username` (str): The username for authentication. - `password` (str): The password for authentication. - `delete_condition` (Callable[[int, int], bool]): A function that takes a message number and its size as arguments and returns a boolean indicating whether the message should be deleted. # Outputs: - The function should not return any value. It should print the message number and size of each retrieved message and delete messages based on the `delete_condition`. # Example: ```python def example_delete_condition(msg_num, msg_size): # Example condition: delete messages larger than 1MB return msg_size > 1024 * 1024 manage_emails(\'pop.example.com\', 110, \'user@example.com\', \'password\', example_delete_condition) ``` # Constraints: - You must handle exceptions and errors gracefully, providing meaningful error messages. - Ensure all session resources are properly released even in case of errors (use of `try...finally` is recommended). - Do not use any global variables inside the function. # Hints: - Use the `POP3` class and its methods as detailed in the provided documentation. - Pay attention to methods that handle connection (`POP3`), authentication (`user`, `pass_`), message listing (`list`), message retrieval (`retr`), and message deletion (`dele`). # Notes: - You might need to import additional modules such as `getpass` for handling secure password input. - Ensure your implementation is efficient and robust to handle potential server inconsistencies. **End of Question**","solution":"import poplib from typing import Callable def manage_emails(server: str, port: int, username: str, password: str, delete_condition: Callable[[int, int], bool]) -> None: try: pop_conn = poplib.POP3(server, port) pop_conn.user(username) pop_conn.pass_(password) num_messages = len(pop_conn.list()[1]) for i in range(num_messages): msg_num, msg_size = pop_conn.list()[1][i].split() msg_num = int(msg_num) msg_size = int(msg_size) print(f\'Message {msg_num}: Size {msg_size}\') if delete_condition(msg_num, msg_size): print(f\'Deleting message {msg_num}\') pop_conn.dele(msg_num) pop_conn.quit() except Exception as e: print(f\'An error occurred: {e}\') finally: try: pop_conn.quit() except: pass"},{"question":"Objective To assess your understanding of PyTorch Tensor Views, including creation, manipulation, and contiguity. Problem Statement You are required to implement a function that performs the following tasks: 1. **Creates a base tensor**: Generates a tensor of shape `(4, 6)` with random values. 2. **Creates a view**: Generates multiple views on this base tensor using at least two different view operations from the list provided in the documentation. 3. **Checks contiguity**: Evaluates and prints whether the view tensors created are contiguous or not. 4. **Performs modifications**: Modifies some elements in the view tensors and prints out the base tensor to demonstrate the shared underlying data. 5. **Converts to contiguity**: Converts a non-contiguous view tensor back into a contiguous tensor using `.contiguous()` and prints both the newly contiguous tensor and its contiguity status. Function Signature ```python import torch def tensor_views_operations(): # Step 1: Create a base tensor base_tensor = torch.rand(4, 6) # Step 2: Create views using at least 2 different view operations view_1 = base_tensor.view(2, 12) view_2 = base_tensor.transpose(0, 1) # Step 3: Check contiguity is_view_1_contiguous = view_1.is_contiguous() is_view_2_contiguous = view_2.is_contiguous() print(f\\"View 1 is contiguous: {is_view_1_contiguous}\\") print(f\\"View 2 is contiguous: {is_view_2_contiguous}\\") # Step 4: Modify some elements in the views and observe base tensor view_1[0, 0] = 3.14 print(f\\"Base tensor after modifying view 1:n{base_tensor}\\") view_2[0, 0] = 2.71 print(f\\"Base tensor after modifying view 2:n{base_tensor}\\") # Step 5: Convert view 2 to contiguous and print new tensor and contiguity status view_2_contiguous = view_2.contiguous() print(f\\"View 2 contiguous tensor:n{view_2_contiguous}\\") print(f\\"View 2 contiguous tensor is contiguous: {view_2_contiguous.is_contiguous()}\\") # Perform the operations tensor_views_operations() ``` Constraints - Ensure you handle the tensor operations as described. - Output should clearly show the contiguity status and effects of modifying view tensors on the base tensor. # Expected Outputs 1. Print statements indicating whether `view_1` and `view_2` are contiguous. 2. Display the base tensor after modifying elements in `view_1` and `view_2`. 3. Print the newly created contiguous tensor from `view_2` and its contiguity status. This will demonstrate your comprehension of PyTorch tensor views, contiguity, and data sharing between tensors.","solution":"import torch def tensor_views_operations(): # Step 1: Create a base tensor base_tensor = torch.rand(4, 6) # Step 2: Create views using at least 2 different view operations view_1 = base_tensor.view(2, 12) view_2 = base_tensor.transpose(0, 1) # Step 3: Check contiguity is_view_1_contiguous = view_1.is_contiguous() is_view_2_contiguous = view_2.is_contiguous() print(f\\"View 1 is contiguous: {is_view_1_contiguous}\\") print(f\\"View 2 is contiguous: {is_view_2_contiguous}\\") # Step 4: Modify some elements in the views and observe base tensor view_1[0, 0] = 3.14 print(f\\"Base tensor after modifying view 1:n{base_tensor}\\") view_2[0, 0] = 2.71 print(f\\"Base tensor after modifying view 2:n{base_tensor}\\") # Step 5: Convert view 2 to contiguous and print new tensor and contiguity status view_2_contiguous = view_2.contiguous() print(f\\"View 2 contiguous tensor:n{view_2_contiguous}\\") print(f\\"View 2 contiguous tensor is contiguous: {view_2_contiguous.is_contiguous()}\\")"},{"question":"# Task You are tasked with analyzing a dataset using Seaborn\'s kernel density estimation (KDE) plotting features. The goal is to visualize distributions, apply conditional coloring, and modify the appearance to gain insights from the data. # Dataset You will be using the \\"tips\\" dataset, which can be loaded directly from Seaborn\'s dataset repository using `sns.load_dataset(\\"tips\\")`. # Requirements 1. **Load the Dataset**: - Load the \\"tips\\" dataset using Seaborn\'s `load_dataset` function. 2. **Univariate KDE Plot**: - Create a KDE plot for the `total_bill` column. - Adjust the smoothing parameter `bw_adjust` to 0.5. - Label the x-axis as \\"Total Bill\\". - Title the plot as \\"KDE Plot of Total Bill\\". 3. **Bivariate KDE Plot**: - Create a KDE plot for the `total_bill` and `tip` columns. - Use `fill=True` to show filled contours. - Set the number of contour levels to 10. - Label the x-axis as \\"Total Bill\\" and the y-axis as \\"Tip\\". - Title the plot as \\"Bivariate KDE Plot of Total Bill and Tip\\". 4. **Conditional Distributions**: - Create a KDE plot for the `total_bill` column with hue mapping based on the `day` of the week. - Use `multiple=\\"stack\\"` to stack the KDE plots. - Label the x-axis as \\"Total Bill\\". - Title the plot as \\"KDE Plot of Total Bill by Day\\". 5. **Cumulative Distribution**: - Create a KDE plot for the `total_bill` column with hue mapping based on `time` (Lunch or Dinner). - Set `cumulative=True` to estimate the cumulative distribution. - Ensure `common_norm` is set to `False`. - Label the x-axis as \\"Total Bill\\". - Title the plot as \\"Cumulative KDE Plot of Total Bill by Time\\". 6. **Appearance Modification**: - Modify one of the above plots (your choice) by setting a different color palette and adjusting the transparency. - Use the `palette=\\"coolwarm\\"` and set `alpha=0.3`. - Ensure that the modified plot has appropriate labels and title. # Constraints 1. **Performance**: Your code should execute efficiently and within a reasonable stack for the provided dataset size. 2. **Output Format**: Each plot should be displayed clearly with labeled axes and titles as specified. # Deliverable Write a Python script or Jupyter Notebook that accomplishes the above tasks, demonstrating your proficiency with Seaborn\'s KDE plotting functionalities. Ensure the code is well-organized, with comments explaining each step.","solution":"import seaborn as sns import matplotlib.pyplot as plt def load_dataset(): Loads the \'tips\' dataset from seaborn\'s repository. return sns.load_dataset(\\"tips\\") def univariate_kde_plot(tips): Creates a univariate KDE plot for the \'total_bill\' column. plt.figure() sns.kdeplot(data=tips, x=\\"total_bill\\", bw_adjust=0.5) plt.xlabel(\\"Total Bill\\") plt.title(\\"KDE Plot of Total Bill\\") plt.show() def bivariate_kde_plot(tips): Creates a bivariate KDE plot for \'total_bill\' and \'tip\' columns. plt.figure() sns.kdeplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", fill=True, levels=10) plt.xlabel(\\"Total Bill\\") plt.ylabel(\\"Tip\\") plt.title(\\"Bivariate KDE Plot of Total Bill and Tip\\") plt.show() def conditional_kde_plot_by_day(tips): Creates a KDE plot for the \'total_bill\' column with a hue mapping by \'day\'. plt.figure() sns.kdeplot(data=tips, x=\\"total_bill\\", hue=\\"day\\", multiple=\\"stack\\") plt.xlabel(\\"Total Bill\\") plt.title(\\"KDE Plot of Total Bill by Day\\") plt.show() def cumulative_kde_plot_by_time(tips): Creates a cumulative KDE plot for the \'total_bill\' column with a hue mapping by \'time\'. plt.figure() sns.kdeplot(data=tips, x=\\"total_bill\\", hue=\\"time\\", cumulative=True, common_norm=False) plt.xlabel(\\"Total Bill\\") plt.title(\\"Cumulative KDE Plot of Total Bill by Time\\") plt.show() def modified_appearance_plot(tips): Modifies the appearance of the KDE plot for \'total_bill\' with hue mapping by \'time\'. Uses \\"coolwarm\\" palette and sets alpha to 0.3. plt.figure() sns.kdeplot(data=tips, x=\\"total_bill\\", hue=\\"time\\", palette=\\"coolwarm\\", alpha=0.3, cumulative=True, common_norm=False) plt.xlabel(\\"Total Bill\\") plt.title(\\"Modified Cumulative KDE Plot of Total Bill by Time\\") plt.show() # Example usage: tips_dataset = load_dataset() univariate_kde_plot(tips_dataset) bivariate_kde_plot(tips_dataset) conditional_kde_plot_by_day(tips_dataset) cumulative_kde_plot_by_time(tips_dataset) modified_appearance_plot(tips_dataset)"},{"question":"**Instructions**: You are given an XML document containing information about various books in a bookstore. Each book is described by a `<book>` element with the following child elements: `<title>`, `<author>`, `<price>`, and `<genre>`. Your task is to write a function that uses the `xml.dom.pulldom` module to parse this XML document and print the titles of all books whose price is greater than a specified threshold. # Function Signature ```python def print_expensive_books(xml_data: str, price_threshold: float) -> None: pass ``` # Parameters - **xml_data (str)**: A string representing the XML document. - **price_threshold (float)**: The price threshold for filtering books. # Output - The function should print the titles of books with a price greater than the specified threshold. # Constraints - The XML document will be well-formed and will contain multiple `<book>` elements. - Each `<book>` element will have a `<price>` element containing a valid float number. - The XML document will not contain maliciously constructed data. # Example Given the following XML data: ```xml <data> <book> <title>Learning Python</title> <author>Mark Lutz</author> <price>45.99</price> <genre>Programming</genre> </book> <book> <title>Fluent Python</title> <author>Luciano Ramalho</author> <price>55.00</price> <genre>Programming</genre> </book> <book> <title>Automate the Boring Stuff with Python</title> <author>Al Sweigart</author> <price>35.95</price> <genre>Programming</genre> </book> </data> ``` Calling `print_expensive_books(xml_data, 40)` should print: ``` Fluent Python ``` # Hints - Use the `pulldom.parseString()` method to parse the XML string. - Loop through the events and filter out books based on the price condition. - Use the `expandNode()` method to access the content of expanded nodes when required. You can assume that the XML data provided is well-formed and conforms to the specified structure.","solution":"from xml.dom import pulldom def print_expensive_books(xml_data: str, price_threshold: float): Prints the titles of books with a price greater than the specified threshold. Parameters: xml_data (str): A string representing the XML document. price_threshold (float): The price threshold for filtering books. # Initialize the DOM parser events = pulldom.parseString(xml_data) title = None price = None for event, node in events: if event == pulldom.START_ELEMENT and node.tagName == \'book\': events.expandNode(node) title_node = node.getElementsByTagName(\'title\')[0] price_node = node.getElementsByTagName(\'price\')[0] title = title_node.firstChild.data price = float(price_node.firstChild.data) if price > price_threshold: print(title)"},{"question":"Problem Statement In this exercise, you are required to design a plugin system framework using abstract base classes from the `abc` module in Python. This framework should allow the creation of plugins that can perform specific tasks, and provides a method to register and invoke these plugins. Requirements 1. Create an abstract base class `Plugin` using `abc.ABC` that includes: - An abstract method `execute(self, data)` which should be overridden by all concrete plugin classes to perform specific tasks. - An abstract class method `name(cls)` which returns the name of the plugin. 2. Implement a `PluginManager` class that: - Maintains a registry of available plugin classes. - Provides a method `register_plugin(cls, plugin_cls)` to register a new plugin class. - Provides a method `get_plugin(cls, name)` to retrieve a registered plugin class by name. - Provides a method `execute_plugin(cls, name, data)` to execute the `execute` method of the specified plugin class with the given data. 3. Define two concrete plugin classes: - `UppercasePlugin` that transforms the input data to uppercase. - `ReversePlugin` that reverses the input data. Constraints - The `register_plugin` method should ensure that only valid subclasses of `Plugin` can be registered. - The operations performed by the plugins should handle string data efficiently. Example Usage ```python from abc import ABC, abstractmethod, ABCMeta class Plugin(ABC): @abstractmethod def execute(self, data): pass @classmethod @abstractmethod def name(cls): pass class PluginManager: _plugins = {} @classmethod def register_plugin(cls, plugin_cls): if not issubclass(plugin_cls, Plugin): raise ValueError(\\"Invalid plugin class\\") cls._plugins[plugin_cls.name()] = plugin_cls @classmethod def get_plugin(cls, name): return cls._plugins.get(name) @classmethod def execute_plugin(cls, name, data): plugin_cls = cls.get_plugin(name) if plugin_cls is None: raise ValueError(\\"Plugin not found\\") return plugin_cls().execute(data) class UppercasePlugin(Plugin): def execute(self, data): return data.upper() @classmethod def name(cls): return \\"uppercase\\" class ReversePlugin(Plugin): def execute(self, data): return data[::-1] @classmethod def name(cls): return \\"reverse\\" # Registering the plugins PluginManager.register_plugin(UppercasePlugin) PluginManager.register_plugin(ReversePlugin) # Executing plugins print(PluginManager.execute_plugin(\\"uppercase\\", \\"hello\\")) # Output: HELLO print(PluginManager.execute_plugin(\\"reverse\\", \\"hello\\")) # Output: olleh ``` Submission - Implement the `Plugin` abstract base class. - Implement the `PluginManager` class. - Implement the `UppercasePlugin` and `ReversePlugin` classes. - Ensure your implementation passes the example usage provided above.","solution":"from abc import ABC, abstractmethod class Plugin(ABC): @abstractmethod def execute(self, data): Execute a specific task. :param data: Input data for the plugin to process. pass @classmethod @abstractmethod def name(cls): Return the name of the plugin. pass class PluginManager: _plugins = {} @classmethod def register_plugin(cls, plugin_cls): Register a new plugin class. :param plugin_cls: Class of the plugin to register. if not issubclass(plugin_cls, Plugin): raise ValueError(\\"Invalid plugin class\\") cls._plugins[plugin_cls.name()] = plugin_cls @classmethod def get_plugin(cls, name): Retrieve a registered plugin class by its name. :param name: Name of the plugin. :return: Plugin class if found, otherwise None. return cls._plugins.get(name) @classmethod def execute_plugin(cls, name, data): Execute the specified plugin with the provided data. :param name: Name of the plugin to execute. :param data: Input data for the plugin to process. :return: Result of the plugin execution. :raises ValueError: If the plugin is not found. plugin_cls = cls.get_plugin(name) if plugin_cls is None: raise ValueError(\\"Plugin not found\\") return plugin_cls().execute(data) class UppercasePlugin(Plugin): def execute(self, data): Transform the input data to uppercase. :param data: Input string to transform. :return: Transformed string in uppercase. return data.upper() @classmethod def name(cls): Return the name of the plugin. :return: String identifying the plugin. return \\"uppercase\\" class ReversePlugin(Plugin): def execute(self, data): Reverse the input data. :param data: Input string to reverse. :return: Reversed string. return data[::-1] @classmethod def name(cls): Return the name of the plugin. :return: String identifying the plugin. return \\"reverse\\" # Registering the plugins PluginManager.register_plugin(UppercasePlugin) PluginManager.register_plugin(ReversePlugin)"},{"question":"**Objective:** To test the understanding of creating, managing, and manipulating databases using the `dbm` module in Python. # Problem Statement You have been tasked to create a simple database system to store user profiles using the `dbm` module. Each profile contains a user\'s name and email. You need to implement the following functions: 1. **`create_database(filename)`**: Opens a new database for reading and writing, creating it if it doesn\'t exist. 2. **`add_user(filename, username, email)`**: Adds a new user to the database where the key is the username and the value is the email. Ensure that the data is stored as bytes. 3. **`get_user_email(filename, username)`**: Retrieves the email of the requested username from the database. 4. **`delete_user(filename, username)`**: Deletes the user with the specified username from the database. 5. **`list_users(filename)`**: Lists all users in the database with their corresponding emails. # Input and Output Formats - **Input**: - All functions take `filename` as a string which is the name of the database file. - `add_user` takes `username` and `email` as strings. - `get_user_email` and `delete_user` take `username` as a string. - **Output**: - `get_user_email` returns the email as a string if the user exists, otherwise returns `\\"User not found\\"`. - `list_users` returns a list of tuples with usernames and emails. - All other functions return `None`. # Constraints or Limitations: 1. Username and email should be non-empty strings and less than 100 characters. 2. Handle all database-specific exceptions by raising a custom error `DatabaseError` with an appropriate message. # Performance Requirements: - The functions should handle standard use cases efficiently. - Ensure the database is properly closed after each operation. # Example Usage ```python # Testing the functions create_database(\'user_profiles\') add_user(\'user_profiles\', \'alice\', \'alice@example.com\') add_user(\'user_profiles\', \'bob\', \'bob@example.com\') print(get_user_email(\'user_profiles\', \'alice\')) # Output: alice@example.com print(get_user_email(\'user_profiles\', \'eve\')) # Output: User not found list_users(\'user_profiles\') # Output: [(\'alice\', \'alice@example.com\'), (\'bob\', \'bob@example.com\')] delete_user(\'user_profiles\', \'alice\') print(get_user_email(\'user_profiles\', \'alice\')) # Output: User not found list_users(\'user_profiles\') # Output: [(\'bob\', \'bob@example.com\')] ``` # Implementation: ```python import dbm class DatabaseError(Exception): pass def create_database(filename): try: with dbm.open(filename, \'c\') as db: pass except dbm.error: raise DatabaseError(\\"Failed to create the database.\\") def add_user(filename, username, email): if not username or not email or len(username) > 100 or len(email) > 100: raise DatabaseError(\\"Invalid username or email\\") try: with dbm.open(filename, \'c\') as db: db[username.encode(\'utf-8\')] = email.encode(\'utf-8\') except dbm.error: raise DatabaseError(\\"Failed to add user to the database.\\") def get_user_email(filename, username): try: with dbm.open(filename, \'r\') as db: if username.encode(\'utf-8\') in db: return db[username.encode(\'utf-8\')].decode(\'utf-8\') else: return \\"User not found\\" except dbm.error: raise DatabaseError(\\"Failed to retrieve user email from the database.\\") def delete_user(filename, username): try: with dbm.open(filename, \'w\') as db: if username.encode(\'utf-8\') in db: del db[username.encode(\'utf-8\')] else: raise DatabaseError(\\"User not found.\\") except dbm.error: raise DatabaseError(\\"Failed to delete user from the database.\\") def list_users(filename): try: with dbm.open(filename, \'r\') as db: return [(k.decode(\'utf-8\'), v.decode(\'utf-8\')) for k, v in db.items()] except dbm.error: raise DatabaseError(\\"Failed to list users from the database.\\") ```","solution":"import dbm class DatabaseError(Exception): pass def create_database(filename): try: with dbm.open(filename, \'c\') as db: pass except dbm.error: raise DatabaseError(\\"Failed to create the database.\\") def add_user(filename, username, email): if not username or not email or len(username) > 100 or len(email) > 100: raise DatabaseError(\\"Invalid username or email\\") try: with dbm.open(filename, \'c\') as db: db[username.encode(\'utf-8\')] = email.encode(\'utf-8\') except dbm.error: raise DatabaseError(\\"Failed to add user to the database.\\") def get_user_email(filename, username): try: with dbm.open(filename, \'r\') as db: if username.encode(\'utf-8\') in db: return db[username.encode(\'utf-8\')].decode(\'utf-8\') else: return \\"User not found\\" except dbm.error: raise DatabaseError(\\"Failed to retrieve user email from the database.\\") def delete_user(filename, username): try: with dbm.open(filename, \'w\') as db: if username.encode(\'utf-8\') in db: del db[username.encode(\'utf-8\')] else: raise DatabaseError(\\"User not found.\\") except dbm.error: raise DatabaseError(\\"Failed to delete user from the database.\\") def list_users(filename): try: with dbm.open(filename, \'r\') as db: return [(k.decode(\'utf-8\'), v.decode(\'utf-8\')) for k, v in db.items()] except dbm.error: raise DatabaseError(\\"Failed to list users from the database.\\")"},{"question":"Objective: You are tasked with creating a machine learning pipeline that integrates unsupervised dimensionality reduction techniques followed by a supervised classification algorithm. Your goal is to improve performance by efficiently reducing the feature space before applying the classifier. Requirements: 1. **Input Format**: - A 2D numpy array `X` of shape (n_samples, n_features) representing the dataset features. - A 1D numpy array `y` of shape (n_samples,) representing the target variable indicating class labels. 2. **Output Format**: - A floating-point value representing the classification accuracy of the model on a hold-out test set using `train_test_split`. 3. **Constraints**: - You must use PCA for dimensionality reduction. - You should use a `StandardScaler` to normalize the feature set before applying PCA. - Use a `RandomForestClassifier` for classification. - Maintain a test size of 30% of the dataset. 4. **Performance Requirements**: - Aim to achieve high accuracy, with a score of at least 0.85 (85%) on the test set. 5. **Implementation Details**: - Implement the function `dimensionality_reduction_pipeline(X: np.ndarray, y: np.ndarray) -> float` - Ensure the PCA reduces the feature space to 95% of the variance. - Use the `train_test_split` function to split the data while maintaining a test size of 30%. - Return the accuracy score of the classifier on the test set. Example: ```python import numpy as np # Example dataset X = np.array([[2.5, 2.4], [0.5, 0.7], [2.2, 2.9], [1.9, 2.2], [3.1, 3.0], [2.3, 2.7], [2, 1.6], [1, 1.1], [1.5, 1.6], [1.1, 0.9]]) y = np.array([1, 0, 1, 1, 1, 0, 1, 0, 0, 0]) # Expected output is the classification accuracy after running the pipeline print(dimensionality_reduction_pipeline(X, y)) # Output: (a floating-point number representing accuracy) ``` Notes: - Utilize `sklearn.decomposition.PCA`, `sklearn.preprocessing.StandardScaler`, and `sklearn.ensemble.RandomForestClassifier`. - Use `sklearn.pipeline.Pipeline` to streamline the sequential steps. - Use `sklearn.model_selection.train_test_split` to partition the dataset.","solution":"import numpy as np from sklearn.decomposition import PCA from sklearn.preprocessing import StandardScaler from sklearn.ensemble import RandomForestClassifier from sklearn.pipeline import Pipeline from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score def dimensionality_reduction_pipeline(X: np.ndarray, y: np.ndarray) -> float: This function integrates PCA for dimensionality reduction followed by a RandomForestClassifier. It returns the classification accuracy of the model on a hold-out test set. # Splitting the data into training and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Define the pipeline pipeline = Pipeline([ (\'scaler\', StandardScaler()), # Standardizing the features (\'pca\', PCA(n_components=0.95)), # PCA retaining 95% of the variance (\'classifier\', RandomForestClassifier(random_state=42)) # Random Forest Classifier ]) # Train the pipeline on the training data pipeline.fit(X_train, y_train) # Predict the labels for the test set y_pred = pipeline.predict(X_test) # Calculate and return the accuracy on the test set accuracy = accuracy_score(y_test, y_pred) return accuracy"},{"question":"# Email Retrieval and Deletion with POP3 You are tasked with implementing a Python function that connects to a POP3 server, retrieves the list of emails, downloads the email contents, and deletes the email after retrieving them. Additionally, your function should include error handling for common issues such as connection errors or authentication failures. Function Signature ```python def manage_emails(pop3_server: str, user: str, password: str, use_ssl: bool = False): Connects to a POP3 server, retrieves and prints the contents of all emails, and then deletes each email after retrieval. Parameters: pop3_server (str): The address of the POP3 server. user (str): The username for authentication. password (str): The password for authentication. use_ssl (bool): Boolean flag indicating whether to use SSL for the connection. Returns: None pass ``` Input - `pop3_server` (str): The address of the POP3 server. - `user` (str): The username for authentication. - `password` (str): The password for authentication. - `use_ssl` (bool): A boolean flag to indicate whether to use SSL for the connection. Defaults to `False`. Output - The function should print the contents of all emails retrieved from the server. - Each email should be deleted from the server after its contents have been printed. - There is no return value. Constraints - You may assume that the server address, username, and password provided are valid and correspond to actual credentials. - The server may have multiple emails. Example Usage ```python manage_emails(\\"mail.example.com\\", \\"user@example.com\\", \\"password123\\", use_ssl=True) ``` Requirements: 1. Connect to the specified POP3 server using either standard POP3 or POP3 over SSL, based on the `use_ssl` flag. 2. Authenticate using the provided username and password. 3. Retrieve the list of emails and print the contents of each email. 4. Delete each email from the server after printing its contents. 5. Implement proper error handling for connection issues and authentication failures. You may use the `poplib` module for this task. Here is an example to get you started with connecting to a POP3 server: ```python import poplib from getpass import getpass try: server = poplib.POP3_SSL(\'mail.example.com\') # Use POP3_SSL for secure connections server.user(\'username\') server.pass_(\'password\') except poplib.error_proto as e: print(f\'Login failed: {e}\') else: print(\'Logged in.\') # Do further processing here finally: server.quit() ``` Ensure that your function handles exceptions and provides meaningful error messages to the user.","solution":"import poplib from email.parser import Parser from email.policy import default def manage_emails(pop3_server: str, user: str, password: str, use_ssl: bool = False): Connects to a POP3 server, retrieves and prints the contents of all emails, and then deletes each email after retrieval. Parameters: pop3_server (str): The address of the POP3 server. user (str): The username for authentication. password (str): The password for authentication. use_ssl (bool): Boolean flag indicating whether to use SSL for the connection. Returns: None try: if use_ssl: server = poplib.POP3_SSL(pop3_server) else: server = poplib.POP3(pop3_server) server.user(user) server.pass_(password) print(\'Logged in successfully.\') email_count, total_size = server.stat() print(f\'Total number of emails: {email_count}\') for i in range(1, email_count + 1): response, lines, octets = server.retr(i) msg_content = b\'rn\'.join(lines).decode(\'utf-8\') msg = Parser(policy=default).parsestr(msg_content) print(f\'Email {i}:\') print(f\'From: {msg[\\"from\\"]}\') print(f\'Subject: {msg[\\"subject\\"]}\') print(f\'Body:n{msg.get_body(preferencelist=(\\"plain\\", \\"html\\")).get_content()}\') print(\'n\' + \'-\'*50 + \'n\') server.dele(i) server.quit() except poplib.error_proto as e: print(f\'Error: {e}\') except Exception as e: print(f\'An unexpected error occurred: {e}\')"},{"question":"# Question: Implementing and Testing Pipeline Parallelism in PyTorch Background Pipeline parallelism is a vital technique for scaling deep learning models across multiple devices. With the `torch.distributed.pipelining` package, you can automatically split a model, schedule, and execute it across devices. Task You need to implement a pipeline-parallel model training procedure. You will: 1. Define a simple model. 2. Manually split the model into two or more pipeline stages. 3. Use a provided `PipelineSchedule` to execute the model in parallel. 4. Validate that the pipelined execution produces the correct results. Model Definition Define a simple feedforward neural network `SimpleModel` with the following structure: - An input layer: Linear layer from `in_features` to 128 units. - Two hidden layers: Each a Linear layer from 128 units to 128 units followed by a ReLU activation. - An output layer: Linear layer from 128 units to `out_features`. Requirements 1. **Model Implementation**: Implement the `SimpleModel` class. 2. **Manual Splitting**: Split the model into two stages: the first stage contains the input and first hidden layer, the second stage contains the second hidden layer and the output layer. 3. **Pipeline Execution**: Implement a pipeline execution using `ScheduleGPipe` to execute the two stages. 4. **Validation**: Compare the output of the pipelined model with that of the sequential execution of `SimpleModel`. Constraints - Use CPU for simplicity. If using CUDA, ensure to handle device placement appropriately. - Assume static shapes to avoid `PipeliningShapeError`. Input - `x`: Input tensor of shape `(batch_size, in_features)`. Output - Must match the result of `SimpleModel(x)`. ```python import torch import torch.nn as nn from torch.distributed.pipelining import PipelineStage, ScheduleGPipe class SimpleModel(nn.Module): def __init__(self, in_features, out_features): super(SimpleModel, self).__init__() self.input = nn.Linear(in_features, 128) self.hidden1 = nn.Linear(128, 128) self.hidden2 = nn.Linear(128, 128) self.output = nn.Linear(128, out_features) self.relu = nn.ReLU() def forward(self, x): x = self.input(x) x = self.relu(x) x = self.hidden1(x) x = self.relu(x) x = self.hidden2(x) x = self.relu(x) x = self.output(x) return x # 1. Define and split the model manually in_features = 10 out_features = 2 model = SimpleModel(in_features, out_features) # Manually splitting the model into two stages class ModelStage1(nn.Module): def __init__(self, model): super(ModelStage1, self).__init__() self.input = model.input self.hidden1 = model.hidden1 self.relu = model.relu def forward(self, x): x = self.input(x) x = self.relu(x) x = self.hidden1(x) x = self.relu(x) return x class ModelStage2(nn.Module): def __init__(self, model): super(ModelStage2, self).__init__() self.hidden2 = model.hidden2 self.output = model.output self.relu = model.relu def forward(self, x): x = self.hidden2(x) x = self.relu(x) x = self.output(x) return x stage1 = ModelStage1(model) stage2 = ModelStage2(model) # Prepare pipeline stages and schedule pipeline_stage1 = PipelineStage(stage1, 0, 2, \\"cpu\\") pipeline_stage2 = PipelineStage(stage2, 1, 2, \\"cpu\\") # Define pipeline schedule schedule = ScheduleGPipe( stages=[pipeline_stage1, pipeline_stage2], microbatch_size=5 ) # 2. Execute the pipeline parallel model def run_pipeline(x): result = schedule.step(x) return result # Test input tensor x = torch.randn(10, in_features) # Sequential execution sequential_model = SimpleModel(in_features, out_features) output_sequential = sequential_model(x) # Pipelined execution output_pipeline = run_pipeline(x) # Validate the outputs are the same print(torch.allclose(output_sequential, output_pipeline, rtol=1e-3, atol=1e-3)) ``` Note Ensure you have the `torch.distributed` package.","solution":"import torch import torch.nn as nn class SimpleModel(nn.Module): def __init__(self, in_features, out_features): super(SimpleModel, self).__init__() self.input = nn.Linear(in_features, 128) self.hidden1 = nn.Linear(128, 128) self.hidden2 = nn.Linear(128, 128) self.output = nn.Linear(128, out_features) self.relu = nn.ReLU() def forward(self, x): x = self.input(x) x = self.relu(x) x = self.hidden1(x) x = self.relu(x) x = self.hidden2(x) x = self.relu(x) x = self.output(x) return x class ModelStage1(nn.Module): def __init__(self, model): super(ModelStage1, self).__init__() self.input = model.input self.hidden1 = model.hidden1 self.relu = model.relu def forward(self, x): x = self.input(x) x = self.relu(x) x = self.hidden1(x) x = self.relu(x) return x class ModelStage2(nn.Module): def __init__(self, model): super(ModelStage2, self).__init__() self.hidden2 = model.hidden2 self.output = model.output self.relu = model.relu def forward(self, x): x = self.hidden2(x) x = self.relu(x) x = self.output(x) return x def run_pipeline(x, stage1, stage2): with torch.no_grad(): x = stage1(x) x = stage2(x) return x"},{"question":"Objective Implement a function that generates samples from a specified probability distribution and transforms these samples according to a given transformation. The function should also be capable of computing the log-probability of the transformed samples. Function Signature ```python import torch import torch.distributions as dist def sample_and_transform(distribution_name: str, params: dict, transform: str, num_samples: int) -> dict: Generate samples from a given distribution and apply a specified transformation. Args: - distribution_name (str): The name of the distribution to sample from (e.g., \'Normal\', \'Beta\'). - params (dict): Parameters required to initialize the distribution. Keys should match the distribution\'s initialization parameters (e.g., for \'Normal\', keys might be \'loc\' and \'scale\'). - transform (str): The name of the transformation to be applied to the samples (e.g., \'Exp\', \'Sigmoid\'). - num_samples (int): The number of samples to generate. Returns: - dict: A dictionary with two keys - \'samples\' and \'log_prob\'. \'samples\' contains the transformed samples, and \'log_prob\' contains the log-probability of each transformed sample. pass ``` Requirements 1. **Distribution Selection**: - Based on the `distribution_name`, create an instance of the respective distribution using the provided `params`. - The distribution can be any of the following: \'Normal\', \'Beta\', \'Poisson\', \'Exponential\', \'Gamma\', \'Bernoulli\'. 2. **Sampling**: - Generate `num_samples` samples from the chosen distribution. 3. **Transformation**: - Apply the specified `transform` to the generated samples. The transformation can be any of the following: - \'Exp\' (Exponential transformation) - \'Sigmoid\' (Sigmoid transformation) - \'Log\' (Logarithmic transformation) 4. **Log-Probability Calculation**: - Compute the log-probability of the transformed samples using the original distribution\'s log_prob method. Input and Output - **Input**: - `distribution_name`: a string specifying the type of distribution. - `params`: a dictionary containing the parameters required to initialize the distribution. - `transform`: a string specifying the type of transformation to apply on the samples. - `num_samples`: an integer specifying the number of samples to generate. - **Output**: - A dictionary with keys: - \'samples\': Tensor of transformed samples. - \'log_prob\': Tensor of log-probabilities of the transformed samples. Constraints - `num_samples` should be a positive integer. - The parameters provided in the `params` dictionary should be valid for the specified `distribution_name`. - The transformation specified in `transform` should be a valid transformation listed above. Example ```python params = {\'loc\': 0, \'scale\': 1} result = sample_and_transform(\'Normal\', params, \'Exp\', 5) print(result[\'samples\']) # Transformed samples print(result[\'log_prob\']) # Log-probabilities of the transformed samples ``` This question assesses the student\'s ability to work with PyTorch distributions, transformations, and log-probabilities, showcasing their understanding of both fundamental and advanced concepts in this module.","solution":"import torch import torch.distributions as dist def sample_and_transform(distribution_name: str, params: dict, transform: str, num_samples: int) -> dict: Generate samples from a given distribution and apply a specified transformation. Args: - distribution_name (str): The name of the distribution to sample from (e.g., \'Normal\', \'Beta\'). - params (dict): Parameters required to initialize the distribution. Keys should match the distribution\'s initialization parameters (e.g., for \'Normal\', keys might be \'loc\' and \'scale\'). - transform (str): The name of the transformation to be applied to the samples (e.g., \'Exp\', \'Sigmoid\'). - num_samples (int): The number of samples to generate. Returns: - dict: A dictionary with two keys - \'samples\' and \'log_prob\'. \'samples\' contains the transformed samples, and \'log_prob\' contains the log-probability of each transformed sample. # Initialize the distribution if distribution_name == \'Normal\': distribution = dist.Normal(params[\'loc\'], params[\'scale\']) elif distribution_name == \'Beta\': distribution = dist.Beta(params[\'alpha\'], params[\'beta\']) elif distribution_name == \'Poisson\': distribution = dist.Poisson(params[\'rate\']) elif distribution_name == \'Exponential\': distribution = dist.Exponential(params[\'rate\']) elif distribution_name == \'Gamma\': distribution = dist.Gamma(params[\'concentration\'], params[\'rate\']) elif distribution_name == \'Bernoulli\': distribution = dist.Bernoulli(params[\'probs\']) else: raise ValueError(f\\"Unsupported distribution: {distribution_name}\\") # Generate samples samples = distribution.sample((num_samples,)) # Apply the transformation if transform == \'Exp\': transformed_samples = torch.exp(samples) elif transform == \'Sigmoid\': transformed_samples = torch.sigmoid(samples) elif transform == \'Log\': transformed_samples = torch.log(samples) else: raise ValueError(f\\"Unsupported transformation: {transform}\\") # Compute the log-probability of the transformed samples log_prob = distribution.log_prob(samples) return {\'samples\': transformed_samples, \'log_prob\': log_prob}"},{"question":"Objective You are tasked with writing a Python script that embodies the use of debugging and profiling tools to identify and resolve inefficiencies in a given function. Problem Statement You are provided with a function, `complex_computation`, which performs a series of mathematical operations on a list of numbers. Your task is to: 1. Identify the bottlenecks in the function using Python\'s profiling tools. 2. Optimize the function to improve its performance. 3. Use Python\'s debugging and tracing tools to ensure the optimized function behaves as expected. Provided Function ```python import time def complex_computation(arr): result = [] for i in arr: time.sleep(0.1) # Simulate a time-consuming operation temp = i ** 2 for j in range(1000): temp = temp * 2 - temp // 3 result.append(temp) return result # Example usage: # arr = [1, 2, 3] # print(complex_computation(arr)) ``` Your Task 1. **Profile the Given Function**: - Use the `cProfile` module to identify the most time-consuming parts of the function. - Output the profiling results. 2. **Optimize the Function**: - Remove or refactor any inefficient parts of the function identified via profiling. - Ensure that the function returns the same results after optimization. 3. **Debugging**: - Use the `pdb` module to set breakpoints and step through the optimized function to verify its correctness. - Ensure you check for edge cases, such as empty lists or large numbers. 4. **Tracing Memory Usage**: - Utilize the `tracemalloc` module to trace memory allocations of the optimized function. - Output the memory usage before and after the optimization. Instructions 1. Implement a Python script that imports the necessary modules (`cProfile`, `pdb`, `tracemalloc`). 2. Include all steps from profiling, optimization, debugging, to tracing memory usage within your script. Ensure all results are printed to the console. Example Output Your script should print outputs similar to the following: ``` Profiling Results (before optimization): [... profiling details ...] Optimized function: def optimized_complex_computation(arr): # Your optimized code here Debugging Steps: [... debugging steps ...] Memory Usage (before optimization): Current: 10MB; Peak: 12MB Memory Usage (after optimization): Current: 5MB; Peak: 6MB ``` Constraints - Your optimization should keep the function\'s behavior identical to the original. - The function must handle lists of up to 10,000 integers efficiently. Evaluation Criteria - Correct use of profiling tools. - Effective optimization of the function. - Proper debugging and verification of the function. - Detailed and clear output of memory usage. Good luck, and remember to test your function thoroughly!","solution":"import time import cProfile import pstats import io import tracemalloc from pdb import set_trace def complex_computation(arr): result = [] for i in arr: time.sleep(0.1) # Simulate a time-consuming operation temp = i**2 for j in range(1000): temp = temp * 2 - temp // 3 result.append(temp) return result def profile_complex_computation(arr): profiler = cProfile.Profile() profiler.enable() result = complex_computation(arr) profiler.disable() stream = io.StringIO() stats = pstats.Stats(profiler, stream=stream).sort_stats(\'cumulative\') stats.print_stats() print(\\"Profiling Results (before optimization):n\\", stream.getvalue()) return result def optimized_complex_computation(arr): result = [] for i in arr: temp = i**2 for j in range(1000): temp = temp * 2 - temp // 3 result.append(temp) return result def profile_optimized_computation(arr): profiler = cProfile.Profile() profiler.enable() result = optimized_complex_computation(arr) profiler.disable() stream = io.StringIO() stats = pstats.Stats(profiler, stream=stream).sort_stats(\'cumulative\') stats.print_stats() print(\\"Profiling Results (after optimization):n\\", stream.getvalue()) return result # Debugging def debug_optimized_computation(arr): set_trace() return optimized_complex_computation(arr) # Tracing memory usage def trace_memory(func, arr): tracemalloc.start() result = func(arr) current, peak = tracemalloc.get_traced_memory() tracemalloc.stop() return result, current / 10**6, peak / 10**6 # Example usage for profiling before and after optimization arr = [1, 2, 3] print(\\"Before Optimization output:\\", profile_complex_computation(arr)) print(\\"After Optimization output:\\", profile_optimized_computation(arr)) # Printing memory usage result, current_before, peak_before = trace_memory(complex_computation, arr) print(f\\"Memory Usage (before optimization): Current: {current_before}MB; Peak: {peak_before}MB\\") result, current_after, peak_after = trace_memory(optimized_complex_computation, arr) print(f\\"Memory Usage (after optimization): Current: {current_after}MB; Peak: {peak_after}MB\\")"},{"question":"# Asyncio Task Manager **Problem Statement:** You are tasked with implementing an asyncio-based task manager that performs the following operations asynchronously: 1. **Create multiple tasks**: Each task will wait for a random number of seconds (between 1 and 5 seconds) simulating a time-consuming operation. 2. **Enforce a timeout**: Each task should be given a timeout of 3 seconds. If it exceeds this time, it should be cancelled. 3. **Use a queue**: Use a FIFO queue to manage and distribute the execution of these tasks. 4. **Log task results**: Tasks that complete within the timeout should output their result (completed within X seconds) while tasks that are cancelled should log as cancelled. **Requirements:** 1. Implement a function `asyncio_task_manager` that takes one argument: - `num_tasks`: An integer representing the number of tasks to create. 2. The function should use asyncio to: - Create a queue for task management. - Enqueue tasks that wait for a random duration (between 1 and 5 seconds). - Enforce a timeout of 3 seconds for each task. - Log the completion status of each task (completed with the actual time waited or cancelled). 3. Use asyncio primitives such as `create_task`, `wait_for`, `Queue`, and logging for task management. **Input:** - `num_tasks`: An integer (1 ≤ num_tasks ≤ 100). **Output:** - Print logs for each task indicating whether it completed within the timeout or was cancelled. **Example Usage:** ```python import asyncio import random import logging logging.basicConfig(level=logging.INFO) async def asyncio_task_manager(num_tasks: int): async def worker(task_id: int, queue: asyncio.Queue): while True: duration = await queue.get() try: await asyncio.wait_for(asyncio.sleep(duration), timeout=3) logging.info(f\\"Task {task_id} completed in {duration} seconds\\") except asyncio.TimeoutError: logging.error(f\\"Task {task_id} cancelled (timeout)\\") queue.task_done() queue = asyncio.Queue() # Populate the queue with random durations for _ in range(num_tasks): queue.put_nowait(random.randint(1, 5)) # Create worker tasks tasks = [] for i in range(num_tasks): task = asyncio.create_task(worker(i, queue)) tasks.append(task) await queue.join() for task in tasks: task.cancel() await asyncio.gather(*tasks, return_exceptions=True) # Example of calling the asyncio_task_manager with 10 tasks asyncio.run(asyncio_task_manager(10)) ``` **Constraints:** - Ensure all tasks are properly managed and cancelled if necessary. - Use async/await syntax to handle concurrency. - Make sure to handle the TimeoutError when tasks exceed their allowed runtime. **Note:** To test the function, ensure to run it inside an event loop using `asyncio.run()` or any compatible method.","solution":"import asyncio import random import logging logging.basicConfig(level=logging.INFO) async def asyncio_task_manager(num_tasks: int): async def worker(task_id: int, queue: asyncio.Queue): while True: duration = await queue.get() try: await asyncio.wait_for(asyncio.sleep(duration), timeout=3) logging.info(f\\"Task {task_id} completed in {duration} seconds\\") except asyncio.TimeoutError: logging.error(f\\"Task {task_id} cancelled (timeout)\\") queue.task_done() queue = asyncio.Queue() # Populate the queue with random durations for _ in range(num_tasks): queue.put_nowait(random.randint(1, 5)) # Create worker tasks tasks = [] for i in range(min(num_tasks, 100)): # Avoid creating more than 100 tasks task = asyncio.create_task(worker(i, queue)) tasks.append(task) await queue.join() for task in tasks: task.cancel() await asyncio.gather(*tasks, return_exceptions=True) # Example of calling the asyncio_task_manager with 10 tasks # asyncio.run(asyncio_task_manager(10))"},{"question":"Coding Assessment Question # Objective: Implement a function that processes a list of URLs concurrently to fetch their HTML content. The function should use `concurrent.futures` to handle multiple URL fetching tasks in parallel, ensuring efficient and robust execution. # Description: Write a function `fetch_urls_concurrently(urls: List[str], max_workers: int) -> List[Tuple[str, str]]` that performs the following: 1. Given a list of URLs to fetch (in the form of strings), use `concurrent.futures.ThreadPoolExecutor` to fetch each URL\'s HTML content in parallel. 2. Limit the number of concurrent fetching tasks using `max_workers`. 3. Return a list of tuples where each tuple contains a URL and its fetched HTML content. 4. Handle any exceptions that occur during fetching by including the exception message in lieu of the HTML content. 5. The order of output tuples should correspond to the order of input URLs. # Input: - `urls`: A list of URL strings to be fetched. - `max_workers`: An integer specifying the maximum number of worker threads to use for concurrent fetching. # Output: - A list of tuples. Each tuple contains: - The original URL. - A string with the fetched HTML content or an exception message if an error occurred. # Constraints: - The `urls` list will have at least 1 and at most 100 URLs. - `max_workers` will be a positive integer no greater than 10. # Example: ```python urls = [ \\"http://example.com\\", \\"http://nonexistent.url\\", \\"http://another-example.com\\" ] max_workers = 3 result = fetch_urls_concurrently(urls, max_workers) ``` Example `result` could be: ```python [ (\\"http://example.com\\", \\"<html>...</html>\\"), (\\"http://nonexistent.url\\", \\"Exception: ...\\"), (\\"http://another-example.com\\", \\"<html>...</html>\\") ] ``` # Notes: - You may use libraries such as `requests` to fetch the HTML content. - Ensure proper exception handling and resource management (e.g., closing of sessions). - Maintain the order of the input URLs in the output list. # Function Signature: ```python from typing import List, Tuple def fetch_urls_concurrently(urls: List[str], max_workers: int) -> List[Tuple[str, str]]: pass ```","solution":"import concurrent.futures import requests from typing import List, Tuple def fetch_url(url: str) -> Tuple[str, str]: Fetches the content of the URL and returns a tuple (URL, content/exception message). try: response = requests.get(url) response.raise_for_status() return (url, response.text) except Exception as e: return (url, f\\"Exception: {str(e)}\\") def fetch_urls_concurrently(urls: List[str], max_workers: int) -> List[Tuple[str, str]]: Fetches contents of the given URLs concurrently and returns a list of tuples with the URL and its fetched content or exception message if an error occurs. results = [] with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor: futures = {executor.submit(fetch_url, url): url for url in urls} for future in concurrent.futures.as_completed(futures): url = futures[future] try: result = future.result() results.append(result) except Exception as e: results.append((url, f\\"Exception: {str(e)}\\")) # Sort results by the original order of input URLs url_order = {url: index for index, url in enumerate(urls)} results.sort(key=lambda x: url_order[x[0]]) return results"},{"question":"# Asynchronous News Fetcher In this task, you will build an asynchronous news fetcher that fetches headlines from multiple mock news APIs concurrently. You\'ll need to use the `asyncio` module to run coroutines in parallel and efficiently manage network I/O. **Your task is to:** 1. Define an asynchronous function `fetch_headline(url: str) -> str` which takes a URL as input and simulates fetching a headline from that URL. 2. Define an asynchronous function `fetch_all_headlines(urls: List[str]) -> List[str]` which takes a list of URLs and fetches headlines from all of them concurrently. 3. Ensure proper handling of network I/O operations and synchronize concurrent tasks to maximize efficiency. Function Signature ```python import asyncio from typing import List async def fetch_headline(url: str) -> str: pass async def fetch_all_headlines(urls: List[str]) -> List[str]: pass ``` # Constraints - Each call to `fetch_headline` should simulate a network call taking between 1 to 3 seconds. - You must use `asyncio.sleep` to simulate network latency. - The function should return the fetched headline in the format `Headline from {url}`. - You should fetch the headlines concurrently, not sequentially. # Example Input ```python urls = [ \'https://newsapi.org/article1\', \'https://newsapi.org/article2\', \'https://newsapi.org/article3\' ] ``` Expected Output ```python [ \'Headline from https://newsapi.org/article1\', \'Headline from https://newsapi.org/article2\', \'Headline from https://newsapi.org/article3\' ] ``` # Test your function You can mock the asynchronous network fetch by using `asyncio.sleep` as follows: ```python import asyncio from typing import List async def fetch_headline(url: str) -> str: await asyncio.sleep(random.randint(1, 3)) # Simulate network latency return f\'Headline from {url}\' async def fetch_all_headlines(urls: List[str]) -> List[str]: tasks = [fetch_headline(url) for url in urls] headlines = await asyncio.gather(*tasks) return headlines ``` This example demonstrates how to implement and test the required functions. Use `asyncio.run(fetch_all_headlines(urls))` to run your function and verify the output.","solution":"import asyncio from typing import List import random async def fetch_headline(url: str) -> str: Simulates fetching a headline from a given URL. await asyncio.sleep(random.randint(1, 3)) # Simulate network latency return f\'Headline from {url}\' async def fetch_all_headlines(urls: List[str]) -> List[str]: Fetches headlines from multiple URLs concurrently. tasks = [fetch_headline(url) for url in urls] headlines = await asyncio.gather(*tasks) return headlines"},{"question":"Concurrent Web Scraping and Data Processing with `asyncio` In this exercise, you are required to develop an asynchronous Python application using the `asyncio` library. The goal is to concurrently scrape data from multiple web pages and then process that data. Your implementation should demonstrate a solid understanding of `asyncio` concepts including coroutines, tasks, synchronization primitives, and IO operations. # Requirements 1. Write an asynchronous function `fetch_data(url: str) -> str` that takes a URL, simulates a network call by sleeping for a random amount of time (between 1 to 5 seconds), and returns a string indicating the content of the URL (for simplicity, concatenate \\"Data from \\" with the URL). 2. Write another asynchronous function `process_data(data: str) -> None` that processes the scraped data. For the purpose of this task, it should just print the length of the data received. 3. Implement a main function `async def main(urls: list) -> None` that: - Initiates fetching data concurrently from the provided list of URLs using `fetch_data`. - Processes each piece of data concurrently using `process_data`. - Uses an `asyncio.Queue` to transfer data between the fetching and processing stages to demonstrate synchronization. 4. Demonstrate synchronization by ensuring that the data fetched is processed in the order they are fetched, but the processes themselves can run concurrently. # Input and Output - Your function `main` should accept a list of URLs to be scraped. - Example input list: `[\'http://example.com/page1\', \'http://example.com/page2\', \'http://example.com/page3\']`. - The output should be printed statements of the length of \\"data\\" fetched from each URL. Example output: \\"Length of data from http://example.com/page1 is 29\\". # Constraints - You must use `asyncio` to implement concurrency. - Make sure to use an `asyncio.Queue` to handle synchronization between fetching and processing. - Properly handle exceptions that might be raised by the coroutines. # Example ```python import asyncio import random async def fetch_data(url: str) -> str: await asyncio.sleep(random.randint(1, 5)) return f\\"Data from {url}\\" async def process_data(data: str) -> None: print(f\\"Length of data is {len(data)}\\") async def main(urls: list) -> None: # Your implementation should go here pass if __name__ == \\"__main__\\": urls = [\'http://example.com/page1\', \'http://example.com/page2\', \'http://example.com/page3\'] asyncio.run(main(urls)) ``` # Notes - Remember to keep the solution efficient and handle potential issues such as timeouts or network errors gracefully. - This will test your ability to leverage asyncio for a real-world scenario involving IO-bound tasks and concurrent execution.","solution":"import asyncio import random async def fetch_data(url: str) -> str: await asyncio.sleep(random.randint(1, 5)) return f\\"Data from {url}\\" async def process_data(data: str) -> None: print(f\\"Length of data is {len(data)}\\") async def worker(name: str, queue: asyncio.Queue): while True: # Get the next item from the queue data = await queue.get() if data is None: # No more data, break out of the loop queue.task_done() break # Process the data await process_data(data) queue.task_done() async def main(urls: list) -> None: queue = asyncio.Queue() # Start workers workers = [asyncio.create_task(worker(f\'worker-{i}\', queue)) for i in range(len(urls))] # Fetch data and put it in the queue for url in urls: data = await fetch_data(url) await queue.put(data) # Wait until all tasks are done await queue.join() # Signal the workers to stop processing for _ in range(len(workers)): await queue.put(None) # Wait for the workers to finish await asyncio.gather(*workers) if __name__ == \\"__main__\\": urls = [\'http://example.com/page1\', \'http://example.com/page2\', \'http://example.com/page3\'] asyncio.run(main(urls))"},{"question":"You are tasked with creating a library system using Python\'s `dataclasses` module. This system will manage books and patrons. Your solution must demonstrate an understanding of dataclass feature, including but not limited to the use of `field` with `default_factory`, handling immutability with frozen parameters, and inheritance. Requirements: 1. **Define a `Book` class** using the `@dataclass` decorator. The class should have the following fields: - `title`: A string containing the title of the book. - `author`: A string containing the author of the book. - `isbn`: A string containing the ISBN identifier of the book. - `copies`: An integer indicating how many copies of the book are available (default is 1). 2. **Define a `Patron` class** using the `@dataclass` decorator. The class should have the following fields: - `name`: A string containing the name of the patron. - `card_number`: A string containing the patron\'s card number. - `borrowed_books`: A list of `Book` objects representing the books borrowed by the patron (default is an empty list, use `default_factory`). 3. **Define a `Library` class** using the `@dataclass` decorator. The class should have the following fields: - `books`: A dictionary mapping ISBN strings to `Book` objects (default is an empty dictionary, use `default_factory`). - `patrons`: A dictionary mapping card numbers to `Patron` objects (default is an empty dictionary, use `default_factory`). 4. **Add methods to the `Library` class**: - `add_book(self, book: Book)`: Adds a book to the library’s collection. If the book already exists (same ISBN), increment the copies available. - `register_patron(self, patron: Patron)`: Adds a patron to the library’s records. - `borrow_book(self, card_number: str, isbn: str)`: Updates the records when a patron borrows a book. Ensure the book exists and has available copies. Update both the book\'s copies and the patron\'s borrowed_books list. - `return_book(self, card_number: str, isbn: str)`: Updates the records when a patron returns a book. 5. **Implement a `frozen` dataclass `BorrowRecord`** to keep immutable records of each borrow transaction. It should have: - `isbn`: The ISBN of the borrowed book. - `card_number`: The card number of the patron who borrowed the book. - `date`: The date of the transaction. 6. **Create functions**: - `get_library_state(library: Library) -> dict`: Returns a dictionary containing the current state of the library, using `asdict()`. # Constraints: - The solution should handle invalid cases, such as borrowing a book that doesn\'t exist or returning a book that was not borrowed. # Example: ```python # Example setup and usage book1 = Book(title=\\"1984\\", author=\\"George Orwell\\", isbn=\\"1234567890\\") book2 = Book(title=\\"To Kill a Mockingbird\\", author=\\"Harper Lee\\", isbn=\\"2345678901\\", copies=2) patron1 = Patron(name=\\"John Doe\\", card_number=\\"1111\\") library = Library() library.add_book(book1) library.add_book(book2) library.register_patron(patron1) library.borrow_book(card_number=\\"1111\\", isbn=\\"1234567890\\") library.return_book(card_number=\\"1111\\", isbn=\\"1234567890\\") print(get_library_state(library)) # Expected output might show the updated state of the library including book copies and patron\'s borrowed books. ``` Submit your solution as a string representation of all the classes and functions defined.","solution":"from dataclasses import dataclass, field, asdict from typing import List, Dict from datetime import date @dataclass class Book: title: str author: str isbn: str copies: int = 1 @dataclass class Patron: name: str card_number: str borrowed_books: List[Book] = field(default_factory=list) @dataclass class Library: books: Dict[str, Book] = field(default_factory=dict) patrons: Dict[str, Patron] = field(default_factory=dict) def add_book(self, book: Book): if book.isbn in self.books: self.books[book.isbn].copies += book.copies else: self.books[book.isbn] = book def register_patron(self, patron: Patron): if patron.card_number not in self.patrons: self.patrons[patron.card_number] = patron def borrow_book(self, card_number: str, isbn: str): if card_number in self.patrons and isbn in self.books: if self.books[isbn].copies > 0: self.patrons[card_number].borrowed_books.append(self.books[isbn]) self.books[isbn].copies -= 1 def return_book(self, card_number: str, isbn: str): if card_number in self.patrons: patron_borrowed_books = self.patrons[card_number].borrowed_books for book in patron_borrowed_books: if book.isbn == isbn: patron_borrowed_books.remove(book) self.books[isbn].copies += 1 break @dataclass(frozen=True) class BorrowRecord: isbn: str card_number: str date: date def get_library_state(library: Library) -> dict: return asdict(library)"},{"question":"**Objective:** Using the `seaborn` library, your task is to create a custom visualization that includes multiple plots, utilizing custom color palettes generated using `hls_palette`. Your solution should demonstrate your ability to manipulate the aesthetics of the plot to make it informative and visually appealing. **Instructions:** 1. **Data Preparation:** - Create a sample Pandas DataFrame with 3 different numeric columns (`A`, `B`, `C`) and 50 rows with random values between 0 to 100. - Add a column `Category` with random values from the set {\'X\', \'Y\', \'Z\'}. 2. **Color Palette:** - Create a custom palette with `seaborn`\'s `hls_palette` containing 3 colors. - Modify the lightness, saturation, and starting hue point of the palette as per your preference. 3. **Visualization:** - Create a FacetGrid with separate subplots for each category in the `Category` column. - On each subplot, plot a scatter plot of column `A` versus `B`, a line plot of column `B` versus `C`, and a KDE plot for column `A`. - Apply the custom color palette created in step 2 to these plots, ensuring consistency across subplots. 4. **Customization:** - Provide clear titles for each subplot based on the `Category` value. - Set appropriate axis labels for all subplots. - Adjust any other visual properties (such as gridlines, legend, etc.) to improve the readability and aesthetics of the visualization. **Constraints:** - Do not use default color palettes for your plots. - Ensure the visualization is informative and not merely colorful. **Expected Output:** - A well-separated facet grid with appropriate dimensions. - Consistently applied custom color palette throughout the subplots. - Properly labeled axis and titles. - Additional customization to enhance the visual appeal and clarity. **Performance Requirements:** - The entire code should run efficiently without any perceptible lag on a standard machine. - All visual properties should be handled using seaborn and matplotlib functions. **Example Code Skeleton:** ```python import pandas as pd import numpy as np import seaborn as sns import matplotlib.pyplot as plt # Step 1: Data Preparation np.random.seed(0) data = pd.DataFrame({ \'A\': np.random.randint(0, 100, 50), \'B\': np.random.randint(0, 100, 50), \'C\': np.random.randint(0, 100, 50), \'Category\': np.random.choice([\'X\', \'Y\', \'Z\'], 50) }) # Step 2: Create custom palette custom_palette = sns.hls_palette(n_colors=3, l=0.4, s=0.8, h=0.3) # Step 3: Create FacetGrid and visualize g = sns.FacetGrid(data, col=\\"Category\\", palette=custom_palette) g.map(sns.scatterplot, \'A\', \'B\') g.map(sns.lineplot, \'B\', \'C\') g.map(sns.kdeplot, \'A\') # Step 4: Customize visuals for ax in g.axes.flat: ax.set_title(\\"Category: \\" + ax.get_title().split(\\" = \\")[-1]) ax.set_xlabel(\\"...\\") ax.set_ylabel(\\"...\\") plt.show() ``` Complete and customize the example code skeleton to meet the requirements.","solution":"import pandas as pd import numpy as np import seaborn as sns import matplotlib.pyplot as plt def create_custom_visualization(): # Step 1: Data Preparation np.random.seed(0) data = pd.DataFrame({ \'A\': np.random.randint(0, 100, 50), \'B\': np.random.randint(0, 100, 50), \'C\': np.random.randint(0, 100, 50), \'Category\': np.random.choice([\'X\', \'Y\', \'Z\'], 50) }) # Step 2: Create custom palette custom_palette = sns.hls_palette(n_colors=3, l=0.5, s=0.7, h=0.2) # Step 3: Create FacetGrid and visualize g = sns.FacetGrid(data, col=\\"Category\\", palette=custom_palette, sharex=False, sharey=False) g.map_dataframe(sns.scatterplot, x=\'A\', y=\'B\') g.map_dataframe(sns.lineplot, x=\'B\', y=\'C\') g.map_dataframe(sns.kdeplot, x=\'A\') # Step 4: Customize visuals for ax in g.axes.flat: category = ax.get_title().split(\\" = \\")[-1] ax.set_title(f\\"Category: {category}\\") ax.set_xlabel(\\"Values for A and B\\") ax.set_ylabel(\\"Values for B and C\\") ax.legend(title=\\"Legend\\", loc=\'best\') # Adjust the layout to make space for titles plt.subplots_adjust(top=0.9) g.fig.suptitle(\'Custom Visualization of Random Data by Category\', fontsize=16) plt.show()"},{"question":"**Objective:** Implement a Python function that leverages the `zipapp` module to create a compressed standalone application archive from a given directory, including specific interpreter and main function. Your function should not only create the archive but also allow the user to filter files and display the interpreter of the created archive. **Function Signature:** ```python def create_and_inspect_zipapp( source_dir: str, target_archive: str, interpreter_command: str, main_function: str, filter_callback: callable = None ) -> str: ``` **Parameters:** - `source_dir (str)`: The path to the source directory containing the application code. - `target_archive (str)`: The path where the resulting application archive should be saved, including the `.pyz` extension. - `interpreter_command (str)`: The interpreter command to be used for the shebang line (e.g., `/usr/bin/env python3`). - `main_function (str)`: The callable which will be used as the main entry point for the archive, specified as `\\"pkg.module:callable\\"`. - `filter_callback (callable)`: An optional callable that determines whether a file should be added to the archive. The callable receives a Path object and should return `True` to include the file, otherwise `False`. **Returns:** - `str`: The interpreter command specified in the resulting archive. **Constraints:** - If the source directory does not contain a valid `__main__.py` file and the `main_function` parameter is not provided, the function should raise a `ValueError`. - The `filter_callback` parameter is optional. If provided, it should allow only those files for which the callback returns `True`. **Examples:** ```python def my_filter(path): return not path.name.endswith(\'.pyc\') # Usage example interpreter = create_and_inspect_zipapp( source_dir=\\"myapp\\", target_archive=\\"myapp.pyz\\", interpreter_command=\\"/usr/bin/env python3\\", main_function=\\"myapp:main\\", filter_callback=my_filter ) print(interpreter) # Output should be \\"/usr/bin/env python3\\" ``` **Requirements:** - Your function should utilize the `zipapp.create_archive` and `zipapp.get_interpreter` functions appropriately. - Implement necessary error handling to ensure the source directory and target archive paths are valid. - Include checks to ensure that the `main_function` argument is specified if the source directory lacks a `__main__.py` file. **Note:** - Make sure to thoroughly test your function with different use cases and conditions to ensure its robustness and correctness.","solution":"import zipapp import os from pathlib import Path import shutil def my_filter(path): return not path.name.endswith(\'.pyc\') def create_and_inspect_zipapp( source_dir: str, target_archive: str, interpreter_command: str, main_function: str, filter_callback: callable = None ) -> str: source_path = Path(source_dir) if not source_path.is_dir(): raise ValueError(f\\"Source directory {source_dir} does not exist or is not a directory.\\") main_py_path = source_path / \'__main__.py\' if not main_function and not main_py_path.exists(): raise ValueError(f\\"__main__.py not found in the source directory {source_dir} and no main_function provided.\\") filtered_files = None if filter_callback: filtered_files = [p for p in source_path.rglob(\'*\') if p.is_file() and filter_callback(p)] # Create a temporary directory for filtered files, if needed. temp_dir = None if filtered_files is not None: temp_dir = source_path / \'.temp_filtered\' if temp_dir.exists(): shutil.rmtree(temp_dir) temp_dir.mkdir() for file_path in filtered_files: rel_path = file_path.relative_to(source_path) dest_path = temp_dir / rel_path dest_path.parent.mkdir(parents=True, exist_ok=True) shutil.copy(file_path, dest_path) zip_source = temp_dir if temp_dir else source_path zipapp.create_archive( source=zip_source, target=target_archive, interpreter=interpreter_command, main=main_function ) # Clean up temporary directory if it was used. if temp_dir: shutil.rmtree(temp_dir) interpreter = zipapp.get_interpreter(target_archive) return interpreter"},{"question":"# Question: Creating and Using Seaborn Color Palettes You are given a dataset containing information about species of penguins, including their bill length and depth. Using this dataset, you need to create visualizations with Seaborn that demonstrate your understanding of color palettes. Specifically, you are required to implement a Python function that will create three types of visualizations using different types of color palettes. Function Signature ```python def plot_penguin_visualizations(data): # Your code here ``` Input - `data`: A Pandas DataFrame containing at least the following columns: `species`, `bill_length_mm`, and `bill_depth_mm`. Output - The function should generate three plots: 1. **Categorical Plot**: - *Type*: Scatter plot. - *Color Palette*: A qualitative palette to differentiate penguin species. 2. **Sequential Plot**: - *Type*: Hexbin plot, showing the relationship between bill length and depth. - *Color Palette*: A sequential palette based on luminance to represent bin counts. 3. **Diverging Plot**: - *Type*: Heatmap that compares bill length and depth with a divergent palette. - *Color Palette*: A diverging palette to highlight differences in the dataset. Constraints - You must use the `sns.color_palette` function or other relevant Seaborn palette functions as appropriate. - Customize the aesthetics using `sns.set_theme` and other Seaborn configuration functions as needed to ensure the plots are informative and visually appealing. - Each plot must include appropriate labels and legends to make it self-explanatory. Example Usage ```python import seaborn as sns # Load the penguins dataset penguins = sns.load_dataset(\\"penguins\\") def plot_penguin_visualizations(data): # Your code here plot_penguin_visualizations(penguins) ``` Hint Refer to the Seaborn documentation provided above for details on different color palettes and how to use them in different plot types.","solution":"import seaborn as sns import matplotlib.pyplot as plt def plot_penguin_visualizations(data): # Set the aesthetic style of the plots sns.set_theme(style=\\"whitegrid\\") # Figure setup fig, axes = plt.subplots(1, 3, figsize=(20, 5)) # Categorical Plot: Scatter plot for different species using a qualitative palette sns.scatterplot( data=data, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", hue=\\"species\\", palette=\\"Set2\\", ax=axes[0] ) axes[0].set_title(\\"Scatter Plot with Qualitative Palette\\") # Sequential Plot: Hexbin plot for bill length vs bill depth using a sequential palette hb = axes[1].hexbin( data[\\"bill_length_mm\\"], data[\\"bill_depth_mm\\"], gridsize=30, cmap=\\"Blues\\" ) axes[1].set_xlabel(\\"Bill Length (mm)\\") axes[1].set_ylabel(\\"Bill Depth (mm)\\") axes[1].set_title(\\"Hexbin Plot with Sequential Palette\\") plt.colorbar(hb, ax=axes[1], label=\'Counts\') # Diverging Plot: Heatmap for bill length vs bill depth using a diverging palette heatmap_data = data.pivot_table(index=\\"bill_length_mm\\", columns=\\"bill_depth_mm\\", aggfunc=\'size\', fill_value=0) sns.heatmap( heatmap_data, cmap=\\"coolwarm\\", ax=axes[2], cbar_kws={\'label\': \'Counts\'} ) axes[2].set_title(\\"Heatmap with Diverging Palette\\") axes[2].set_xlabel(\\"Bill Depth (mm)\\") axes[2].set_ylabel(\\"Bill Length (mm)\\") # Display the plots plt.tight_layout() plt.show()"},{"question":"**Question: Working with Quoted-Printable Encoding** In this task, you need to create a Python function that processes data from a binary file and writes the result to an output binary file using the quoted-printable encoding scheme. Write a function `process_file(input_filepath: str, output_filepath: str, action: str, quotetabs: bool=False, header: bool=False) -> None` that performs the following: - Reads binary data from the file specified by `input_filepath`. - If the `action` parameter is `\\"encode\\"`, encodes the data using the quoted-printable encoding and writes the resulting encoded data to the file specified by `output_filepath`. - If the `action` parameter is `\\"decode\\"`, decodes the quoted-printable encoded data and writes the resulting decoded data to the file specified by `output_filepath`. - The `quotetabs` parameter should be used only if `action` is `\\"encode\\"`. It controls whether to encode embedded spaces and tabs. - The `header` parameter determines whether to handle underscores as spaces (when decoding) or encode spaces as underscores (when encoding), as detailed in the `quopri` module. Here are the expected input and output formats: # Input - `input_filepath` (str): The path to the input binary file. - `output_filepath` (str): The path to the output binary file. - `action` (str): Either `\\"encode\\"` to encode data or `\\"decode\\"` to decode data. - `quotetabs` (bool, optional): Flag to encode embedded spaces and tabs (default is `False`). - `header` (bool, optional): Flag to handle underscores in headers (default is `False`). # Output - This function should not return any value. It modifies the file at `output_filepath`. # Constraints - The input file specified by `input_filepath` and the output file specified by `output_filepath` must exist and be accessible with read/write permissions. - The `action` parameter must be either `\\"encode\\"` or `\\"decode\\"`. # Example ```python # Example usage process_file(\'input.bin\', \'encoded_output.bin\', \'encode\', quotetabs=True, header=True) process_file(\'encoded_output.bin\', \'decoded_output.bin\', \'decode\', header=True) ``` # Notes - Ensure to handle file opening and closing properly to avoid any file-related errors. - Use appropriate functions from the `quopri` module to achieve the required encoding and decoding.","solution":"import quopri def process_file(input_filepath: str, output_filepath: str, action: str, quotetabs: bool=False, header: bool=False) -> None: Processes a binary file using quoted-printable encoding or decoding. Arguments: input_filepath -- path to the input binary file output_filepath -- path to the output binary file action -- \'encode\' to encode data or \'decode\' to decode data quotetabs -- if True, encode embedded spaces and tabs header -- if True, handle underscores in headers (for encoding/decoding) with open(input_filepath, \'rb\') as infile: data = infile.read() if action == \'encode\': encoded_data = quopri.encodestring(data, quotetabs=quotetabs, header=header) with open(output_filepath, \'wb\') as outfile: outfile.write(encoded_data) elif action == \'decode\': decoded_data = quopri.decodestring(data, header=header) with open(output_filepath, \'wb\') as outfile: outfile.write(decoded_data) else: raise ValueError(\\"Invalid action. Use \'encode\' or \'decode\'.\\")"},{"question":"# Python Mapping Protocol Simulation You are tasked with implementing a custom mapping class in Python that simulates the behavior of the provided C API functions described in the documentation. Your class should provide methods equivalent to the C functions for getting, setting, and deleting items, as well as for checking key membership and retrieving keys, values, and items. Class Details Define a class `CustomMapping` with the following methods: 1. **`__init__(self)`**: Initialize an empty internal dictionary. 2. **`mapping_check(self)`**: Returns `True` indicating this object supports the mapping protocol. 3. **`mapping_size(self)`**: Returns the number of key-value pairs in the mapping. 4. **`mapping_get_item(self, key)`**: Returns the value associated with the given key. Raise a `KeyError` if the key does not exist. 5. **`mapping_set_item(self, key, value)`**: Sets the value for the given key. 6. **`mapping_del_item(self, key)`**: Deletes the item associated with the given key. Raise a `KeyError` if the key does not exist. 7. **`mapping_has_key(self, key)`**: Returns `True` if the key exists, `False` otherwise. 8. **`mapping_keys(self)`**: Returns a list of all keys in the mapping. 9. **`mapping_values(self)`**: Returns a list of all values in the mapping. 10. **`mapping_items(self)`**: Returns a list of key-value pairs (as tuples) in the mapping. Constraints - Each method should mimic the corresponding C API functions in terms of functionality. - You are not allowed to use any built-in dictionary methods directly (other than initialization). Example ```python m = CustomMapping() m.mapping_set_item(\'a\', 1) m.mapping_set_item(\'b\', 2) print(m.mapping_get_item(\'a\')) # Output: 1 print(m.mapping_size()) # Output: 2 print(m.mapping_has_key(\'b\')) # Output: True print(m.mapping_keys()) # Output: [\'a\', \'b\'] print(m.mapping_values()) # Output: [1, 2] print(m.mapping_items()) # Output: [(\'a\', 1), (\'b\', 2)] m.mapping_del_item(\'a\') print(m.mapping_size()) # Output: 1 ``` Submission Submit your implementation of the `CustomMapping` class. Ensure your code is well-documented and follows Python best practices. Your class will be tested with various inputs to validate its correctness.","solution":"class CustomMapping: def __init__(self): self._data = {} def mapping_check(self): return True def mapping_size(self): return len(self._data) def mapping_get_item(self, key): if key in self._data: return self._data[key] else: raise KeyError(f\'Key {key} not found\') def mapping_set_item(self, key, value): self._data[key] = value def mapping_del_item(self, key): if key in self._data: del self._data[key] else: raise KeyError(f\'Key {key} not found\') def mapping_has_key(self, key): return key in self._data def mapping_keys(self): return list(self._data.keys()) def mapping_values(self): return list(self._data.values()) def mapping_items(self): return list(self._data.items())"},{"question":"Coding Assessment Question # Objective You are to demonstrate your proficiency in using pandas for data manipulation and transformation. This challenge involves working with a dataset to perform various operations, including data cleaning, aggregation, and generating summary statistics. # Question You are provided with a dataset containing information about customer transactions. The dataset includes the following columns: - `CustomerID` (integer): Unique identifier for each customer - `TransactionID` (integer): Unique identifier for each transaction - `Product` (string): Name of the product - `Quantity` (integer): Quantity of the product purchased - `Price` (float): Price of a single unit of the product - `Date` (datetime): Date of the transaction Your task is to implement a function `analyze_transactions` that performs the following operations: 1. **Data Cleaning:** - Ensure that there are no missing values in the dataset. - Remove any duplicate transactions (keep the first occurrence if duplicates exist). 2. **Data Transformation:** - Create a new column `TotalPrice` which is the product of `Quantity` and `Price` for each transaction. 3. **Aggregation and Summary:** - Calculate the total revenue generated per `CustomerID`. - Find the most popular product (the product with the highest total quantity sold). - Calculate the average transaction value (average `TotalPrice`) for each `CustomerID`. # Function Signature ```python import pandas as pd def analyze_transactions(df: pd.DataFrame) -> None: Function to analyze customer transactions from a given DataFrame. Parameters: - df (pd.DataFrame): The input DataFrame with transaction data Returns: None: This function should print three outputs: - Total revenue per `CustomerID` - Most popular product - Average transaction value per `CustomerID` pass # Your code goes here ``` # Expected Input and Output **Input:** - A pandas DataFrame `df` with columns [CustomerID, TransactionID, Product, Quantity, Price, Date] **Output:** - Print the total revenue per `CustomerID` as a pandas Series - Print the most popular product as a string - Print the average transaction value per `CustomerID` as a pandas Series # Constraints - Assume the dataset is not very large and can fit into memory comfortably. - Datetime values are in a standard ISO format. # Additional Notes - You may assume that the dataset is already loaded into a pandas DataFrame. - Focus on writing clean, efficient pandas code. # Example Input ```python data = { \'CustomerID\': [1, 2, 1, 3, 2], \'TransactionID\': [101, 102, 103, 104, 105], \'Product\': [\'A\', \'B\', \'A\', \'C\', \'A\'], \'Quantity\': [2, 1, 3, 2, 4], \'Price\': [10.0, 20.0, 10.0, 15.0, 10.0], \'Date\': [\'2023-01-01\', \'2023-01-02\', \'2023-01-01\', \'2023-01-03\', \'2023-01-02\'] } df = pd.DataFrame(data) ``` # Example Output ``` Total revenue per CustomerID: CustomerID 1 50.0 2 60.0 3 30.0 dtype: float64 Most popular product: A Average transaction value per CustomerID: CustomerID 1 25.0 2 30.0 3 30.0 dtype: float64 ```","solution":"import pandas as pd def analyze_transactions(df: pd.DataFrame) -> None: Function to analyze customer transactions from a given DataFrame. Parameters: - df (pd.DataFrame): The input DataFrame with transaction data Returns: None: This function should print three outputs: - Total revenue per `CustomerID` - Most popular product - Average transaction value per `CustomerID` # Data Cleaning df = df.dropna() # Remove missing values df = df.drop_duplicates(subset=[\'TransactionID\']) # Remove duplicate transactions by TransactionID # Data Transformation df[\'TotalPrice\'] = df[\'Quantity\'] * df[\'Price\'] # Create new column TotalPrice # Aggregation and Summary total_revenue_per_customer = df.groupby(\'CustomerID\')[\'TotalPrice\'].sum() most_popular_product = df.groupby(\'Product\')[\'Quantity\'].sum().idxmax() average_transaction_value_per_customer = df.groupby(\'CustomerID\')[\'TotalPrice\'].mean() # Print results print(\\"Total revenue per CustomerID:\\") print(total_revenue_per_customer) print(\\"nMost popular product:\\", most_popular_product) print(\\"nAverage transaction value per CustomerID:\\") print(average_transaction_value_per_customer)"},{"question":"# Question: Implementing Compression and Decompression with zlib Problem Statement You are required to implement two functions `compress_data` and `decompress_data` using the `zlib` module in Python. 1. **`compress_data` Function**: - **Input**: `data` (str) - A string that needs to be compressed. - **Output**: (bytes) - The compressed data in bytes. - This function should use the `zlib.compress` function to compress the input data. 2. **`decompress_data` Function**: - **Input**: `data` (bytes) - A bytes object which is the result of compressing a string. - **Output**: (str) - The original string after decompression. - This function should use the `zlib.decompress` function to decompress the input data. Example ```python compressed = compress_data(\\"Hello, World!\\") print(compressed) # Output will be a bytes object decompressed = decompress_data(compressed) print(decompressed) # Output should be: \\"Hello, World!\\" ``` Constraints - You should use `zlib.DEFAULT_COMPRESSION` for the compression level. - The resulting decompressed string must be identical to the original input string. Requirements - Define the two functions `compress_data` and `decompress_data`. - Make sure the functions handle any potential errors using exception handling and raise appropriate messages if compression or decompression fails. # Starter Code ```python import zlib def compress_data(data: str) -> bytes: Compress the input string using zlib and return the compressed data as bytes. pass # Your implementation here def decompress_data(data: bytes) -> str: Decompress the input bytes using zlib and return the decompressed data as string. pass # Your implementation here ```","solution":"import zlib def compress_data(data: str) -> bytes: Compress the input string using zlib and return the compressed data as bytes. try: compressed_data = zlib.compress(data.encode(\'utf-8\')) return compressed_data except Exception as e: raise ValueError(f\\"Compression failed: {e}\\") def decompress_data(data: bytes) -> str: Decompress the input bytes using zlib and return the decompressed data as string. try: decompressed_data = zlib.decompress(data) return decompressed_data.decode(\'utf-8\') except Exception as e: raise ValueError(f\\"Decompression failed: {e}\\")"},{"question":"Objective: To create a Python script that automates the process of generating multiple types of built distributions for a given module. The script should include error handling and validation to ensure correct input formats. Problem Statement: You are asked to write a Python script that accepts a module directory and a list of distribution formats as input. The script should use the Distutils library to generate the specified built distributions for that module. If the module contains extensions that need to be compiled, properly handle the generation of appropriate distributions. Input: 1. The path to the module directory. 2. A list of formats to generate (any combination of: `gztar`, `bztar`, `xztar`, `ztar`, `tar`, `zip`, `rpm`, `pkgtool`, `sdux`, `msi`). Output: The script should generate the requested built distributions in the default `dist/` directory within the module directory. It should also print a summary of what has been generated. Constraints: 1. Handle at least one invalid format by printing an error message and continuing with valid formats. 2. Ensure the script runs cross-platform (Unix and Windows). 3. If applicable, handle platform-specific nuances (e.g., Windows executable installer defaults). 4. Assume Python 3.10 and have all necessary dependencies installed. Performance Requirements: 1. The script should efficiently handle large module directories. 2. Ensure minimal downtime by handling invalid inputs gracefully. Example: Given the path `./example_module/` and formats `[\'gztar\', \'zip\', \'invalid_format\']`, the script should: 1. Generate `example_module/dist/example_module-x.y.z.tar.gz`. 2. Generate `example_module/dist/example_module-x.y.z.zip`. 3. Print an error message for the unsupported format `invalid_format`. 4. Print a summary of the successfully generated distributions. Implementation Requirements: Implement a function `create_distributions(module_path: str, formats: List[str]) -> None` that automates the packaging process. ```python import os from distutils.core import run_setup from distutils.dist import Distribution def create_distributions(module_path: str, formats: List[str]) -> None: valid_formats = {\\"gztar\\", \\"bztar\\", \\"xztar\\", \\"ztar\\", \\"tar\\", \\"zip\\", \\"rpm\\", \\"pkgtool\\", \\"sdux\\", \\"msi\\"} invalid_formats = [fmt for fmt in formats if fmt not in valid_formats] for fmt in invalid_formats: print(f\\"Error: Unsupported format \'{fmt}\'\\") # Filtering out invalid formats valid_formats_to_use = [fmt for fmt in formats if fmt in valid_formats] os.chdir(module_path) for dist_format in valid_formats_to_use: Distribution() run_setup(\\"setup.py\\", script_args=[\\"bdist\\", \\"--formats=\\" + dist_format]) # Summary of generated distributions dist_dir = os.path.join(module_path, \'dist\') generated_files = os.listdir(dist_dir) if os.path.exists(dist_dir) else [] print(f\\"Generated distributions in {dist_dir}: {generated_files}\\") ``` Use the template and fill in the details as required. Ensure to validate input formats, handle errors gracefully, and print a summary of generated files.","solution":"import os from distutils.core import run_setup from distutils.dist import Distribution def create_distributions(module_path: str, formats: list[str]) -> None: Create distributions for a given module with the specified formats. :param module_path: Path to the module directory. :param formats: List of formats to generate (any combination of: \'gztar\', \'bztar\', \'xztar\', \'ztar\', \'tar\', \'zip\', \'rpm\', \'pkgtool\', \'sdux\', \'msi\'). valid_formats = {\\"gztar\\", \\"bztar\\", \\"xztar\\", \\"ztar\\", \\"tar\\", \\"zip\\", \\"rpm\\", \\"pkgtool\\", \\"sdux\\", \\"msi\\"} invalid_formats = [fmt for fmt in formats if fmt not in valid_formats] for fmt in invalid_formats: print(f\\"Error: Unsupported format \'{fmt}\'\\") # Filtering out invalid formats valid_formats_to_use = [fmt for fmt in formats if fmt in valid_formats] initial_dir = os.getcwd() os.chdir(module_path) for dist_format in valid_formats_to_use: try: run_setup(\\"setup.py\\", script_args=[\\"bdist\\", \\"--formats=\\" + dist_format]) except Exception as e: print(f\\"Failed to generate distribution for format \'{dist_format}\': {e}\\") os.chdir(initial_dir) # Summary of generated distributions dist_dir = os.path.join(module_path, \'dist\') generated_files = os.listdir(dist_dir) if os.path.exists(dist_dir) else [] print(f\\"Generated distributions in {dist_dir}: {generated_files}\\")"},{"question":"Objective: You are required to demonstrate your understanding of PyTorch\'s Just-In-Time (JIT) compilation capabilities using the `torch.utils.jit` module. Problem Description: Implement the following functions to showcase your ability to work with TorchScript models: 1. **create_script_module(model: nn.Module) -> torch.jit.ScriptModule** Given a PyTorch neural network model (an instance of `nn.Module`), this function should convert the model to a TorchScript `ScriptModule` using JIT compilation. **Input:** - `model (nn.Module)`: A PyTorch neural network model. **Output:** - A `torch.jit.ScriptModule` representing the input model. 2. **save_script_module(script_module: torch.jit.ScriptModule, path: str) -> None** This function should save the TorchScript `ScriptModule` to the given file path. **Input:** - `script_module (torch.jit.ScriptModule)`: The TorchScript module to save. - `path (str)`: The file path where the TorchScript module should be saved. **Output:** - None 3. **load_script_module(path: str) -> torch.jit.ScriptModule** This function should load a TorchScript `ScriptModule` from a given file path. **Input:** - `path (str)`: The file path of the TorchScript module to load. **Output:** - A `torch.jit.ScriptModule` that was loaded from the specified file path. Requirements: - **Constraints and Limitations:** - Ensure all TorchScript modules created are valid and can be saved/loaded without errors. - The file paths provided will be valid paths in the file system. - Use `torch.jit.script` to script the model. - **Performance Requirements:** - The functions should handle typical PyTorch models efficiently. Example: ```python import torch.nn as nn import torch import torch.utils.jit as jit_utils class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.linear = nn.Linear(2, 2) def forward(self, x): return self.linear(x) # 1. Create a TorchScript module from the SimpleModel model = SimpleModel() script_module = create_script_module(model) # 2. Save the TorchScript module save_script_module(script_module, \\"simple_model.pt\\") # 3. Load the TorchScript module loaded_script_module = load_script_module(\\"simple_model.pt\\") ``` Note: Ensure that all necessary imports are handled and the provided functions integrate properly with standard PyTorch operations and file management.","solution":"import torch import torch.nn as nn def create_script_module(model: nn.Module) -> torch.jit.ScriptModule: Converts a PyTorch model to a TorchScript ScriptModule using JIT compilation. Args: - model (nn.Module): The PyTorch neural network model. Returns: - torch.jit.ScriptModule: The TorchScript representation of the model. script_module = torch.jit.script(model) return script_module def save_script_module(script_module: torch.jit.ScriptModule, path: str) -> None: Saves the TorchScript ScriptModule to the given file path. Args: - script_module (torch.jit.ScriptModule): The TorchScript module to save. - path (str): The file path where the TorchScript module should be saved. Returns: - None script_module.save(path) def load_script_module(path: str) -> torch.jit.ScriptModule: Loads a TorchScript ScriptModule from the given file path. Args: - path (str): The file path of the TorchScript module to load. Returns: - torch.jit.ScriptModule: The loaded TorchScript module. script_module = torch.jit.load(path) return script_module"},{"question":"**Custom Type Creation and Manipulation** In this exercise, you will be tasked with creating and manipulating types using Python\'s dynamic type capabilities. Implement a function `create_custom_type` that will dynamically create a new type with specified attributes and methods. You will demonstrate the following capabilities through the provided tasks: 1. **Type Creation:** - Create a new type named `\'CustomType\'` with the attributes provided in the `attributes` dictionary. 2. **Method Addition:** - Add the specified methods provided in the `methods` dictionary to the newly created type. 3. **Type Checking:** - Implement a method `is_custom_type` that checks if a given object is of type `\'CustomType\'`. 4. **Subtype Checking:** - Implement a method `is_subtype` that checks if a given object is a subtype of a specified type. # Function Signature ```python def create_custom_type(attributes: dict, methods: dict): Create a custom type with the given attributes and methods. :param attributes: Dictionary where keys are attribute names and values are attribute values. :param methods: Dictionary where keys are method names and values are functions corresponding to those methods. :return: The newly created type. pass def is_custom_type(obj): Check if the object is of type \'CustomType\'. :param obj: The object to be checked. :return: True if the object is of type \'CustomType\', False otherwise. pass def is_subtype(obj, parent_type): Check if the object\'s type is a subtype of the specified parent type. :param obj: The object to be checked. :param parent_type: The parent type to be checked against. :return: True if the object\'s type is a subtype of parent_type, False otherwise. pass ``` # Example Usage ```python # Attributes and methods to add to the custom type attributes = { \'attr1\': \'value1\', \'attr2\': 42 } def method1(self): return f\\"method1 called from {self}\\" def method2(self, x): return self.attr2 + x methods = { \'method1\': method1, \'method2\': method2 } # Create the custom type CustomType = create_custom_type(attributes, methods) # Validate type creation obj = CustomType() assert is_custom_type(obj) == True # Validate attributes assert obj.attr1 == \'value1\' assert obj.attr2 == 42 # Validate methods assert obj.method1() == \'method1 called from <CustomType>\' assert obj.method2(10) == 52 # Validate subtype checking class DerivedType(CustomType): pass derived_obj = DerivedType() assert is_subtype(derived_obj, CustomType) == True ``` # Constraints - All attribute values and method implementations must be properly assigned to the dynamic type. - The `methods` dictionary will contain valid callable functions. - Attribute names and method names will be valid Python identifiers. Make sure to thoroughly test the created type to ensure attributes and methods can be accessed and invoked correctly.","solution":"def create_custom_type(attributes: dict, methods: dict): Create a custom type with the given attributes and methods. :param attributes: Dictionary where keys are attribute names and values are attribute values. :param methods: Dictionary where keys are method names and values are functions corresponding to those methods. :return: The newly created type. # Creating a new type using type function CustomType = type(\'CustomType\', (object,), {}) # Add attributes to the type for attr_name, attr_value in attributes.items(): setattr(CustomType, attr_name, attr_value) # Add methods to the type for method_name, method in methods.items(): setattr(CustomType, method_name, method) return CustomType def is_custom_type(obj): Check if the object is of type \'CustomType\'. :param obj: The object to be checked. :return: True if the object is of type \'CustomType\', False otherwise. return type(obj).__name__ == \'CustomType\' def is_subtype(obj, parent_type): Check if the object\'s type is a subtype of the specified parent type. :param obj: The object to be checked. :param parent_type: The parent type to be checked against. :return: True if the object\'s type is a subtype of parent_type, False otherwise. return isinstance(obj, parent_type)"},{"question":"Objective: You are tasked with demonstrating your understanding of managing CPU devices and streams in PyTorch. You will need to write code that manages different CPU streams, synchronizes operations, and handles multiple CPU devices. Problem Statement: Write a Python function `cpu_parallel_operations(data_batches: List[torch.Tensor]) -> List[torch.Tensor]` that performs parallel processing on multiple CPU streams. Your function should: 1. Accept a list of data batches (PyTorch tensors). 2. Utilize multiple CPU streams to perform some dummy operation (e.g., adding a constant value). 3. Ensure all operations across streams are synchronized before returning the results. 4. Fully utilize available CPU devices as indicated by `torch.cpu.device_count()`. Expected Input and Output Formats: - **Input**: - `data_batches`: A list of PyTorch tensors, where each tensor represents a batch of data to process. - **Output**: - A list of PyTorch tensors that have been processed as described above. Constraints: 1. Ensure you handle as many CPU devices as indicated by `torch.cpu.device_count()`. 2. Each operation must be synchronized before returning the results. 3. Assume there are always at least 1 data batch and each tensor in `data_batches` is not empty. Example: ```python import torch from typing import List def cpu_parallel_operations(data_batches: List[torch.Tensor]) -> List[torch.Tensor]: # Ensure to utilize CPU streams and devices effectively pass # Example usage: data_batches = [torch.tensor([1, 2, 3]), torch.tensor([4, 5, 6])] processed_batches = cpu_parallel_operations(data_batches) # Expected processed_batches output (if the dummy operation is adding 1 to each tensor element): # [tensor([2, 3, 4]), tensor([5, 6, 7])] ``` Provide the implementation that adheres to the above requirements.","solution":"import torch from typing import List def cpu_parallel_operations(data_batches: List[torch.Tensor]) -> List[torch.Tensor]: # Number of available CPU devices num_devices = torch.get_num_threads() # Function to perform a dummy operation on a tensor def dummy_operation(tensor): return tensor + 1 # Adding 1 as a dummy operation # A list to store the result tensors results = [None] * len(data_batches) # Creating streams for parallel operations for i, tensor in enumerate(data_batches): torch.set_num_threads(1) # Use one thread for each stream results[i] = dummy_operation(tensor) # Synchronize operations (in this simplified context, this is implicit) return results"},{"question":"# Parsing and Processing XML Data using `xml.sax` You are tasked with implementing an XML parser using Python\'s `xml.sax` package that processes a specific XML structure. The XML document contains information about a library, including books and authors. Each book has a title, author, and publication year. Your task is to parse this XML and extract the relevant information, handling any potential parsing errors using SAX exceptions. Requirements: 1. **Parse the XML document** using `xml.sax`. 2. **Implement a custom content handler** to process events. The handler should: - Collect the titles of all books. - For each book, store the author and publication year. 3. **Handle parsing errors** using SAX exceptions. Input: - A string representation of the XML document. Expected Output: - A dictionary with the book titles as keys and a tuple (author, publication year) as values. - In case of a parsing error, a custom exception message indicating the error should be returned. # Example XML String: ```xml <library> <book> <title>The Great Gatsby</title> <author>F. Scott Fitzgerald</author> <year>1925</year> </book> <book> <title>1984</title> <author>George Orwell</author> <year>1949</year> </book> </library> ``` Expected Output: ```python { \\"The Great Gatsby\\": (\\"F. Scott Fitzgerald\\", 1925), \\"1984\\": (\\"George Orwell\\", 1949) } ``` Error Handling: For any XML structure error or other SAX parsing issues, return a string `Parsing Error: [error message]`. # Function Signature ```python def parse_library_xml(xml_string: str) -> Union[dict, str]: # Your code here ``` # Constraints: - Use the `xml.sax` package as described. - Assume that the XML structure is generally well-formed but may contain minor errors that need to be addressed. # Additional Notes: - You should define and use a custom content handler class inheriting from `xml.sax.handler.ContentHandler`. - Use appropriate SAX exceptions to manage errors during parsing.","solution":"import xml.sax from xml.sax.handler import ContentHandler from xml.sax._exceptions import SAXParseException class LibraryContentHandler(ContentHandler): def __init__(self): self.current_data = \\"\\" self.title = \\"\\" self.author = \\"\\" self.year = \\"\\" self.books = {} def startElement(self, name, attrs): self.current_data = name def endElement(self, name): if name == \\"book\\": try: self.books[self.title] = (self.author, int(self.year)) except ValueError: self.books[self.title] = (self.author, self.year) self.title = \\"\\" self.author = \\"\\" self.year = \\"\\" self.current_data = \\"\\" def characters(self, content): if self.current_data == \\"title\\": self.title = content elif self.current_data == \\"author\\": self.author = content elif self.current_data == \\"year\\": self.year = content def parse_library_xml(xml_string: str): handler = LibraryContentHandler() try: xml.sax.parseString(xml_string, handler) return handler.books except SAXParseException as e: return f\\"Parsing Error: {str(e)}\\""},{"question":"# Objective Your task is to demonstrate your understanding of performance optimization in Scikit-learn by implementing a class for the Linear Regression model with warm starts, profiling to identify bottlenecks, and optimizing a part of the code using Cython. # Problem Statement Implement a Linear Regression class from scratch. Next, profile the class to identify performance bottlenecks. Finally, optimize the most significant bottleneck using Cython. # Requirements 1. **Implementation**: Implement a simplified Linear Regression class. 2. **Profiling**: Use Python\'s built-in profiling tools to pinpoint performance issues. 3. **Optimization**: Rewrite the performance bottleneck in Cython for improved efficiency. 4. **Evaluation**: Compare the runtime of both versions and report on the speedup. # Detailed Instructions 1. **Class Implementation**: - Implement a class `MyLinearRegression` with methods: - `fit(X, y)`: Trains the model using the training data `(X, y)`. - `predict(X)`: Predicts outputs for input data `X`. - Suggested steps inside `fit` method: - Initialize coefficients. - Use gradient descent to minimize the Mean Squared Error (MSE). 2. **Profiling**: - Create a script to profile your `fit` method using the `digits` dataset from `sklearn.datasets`. - Use `%timeit` to measure total execution time. - Use `%prun` to analyze which parts of the code are taking the most time. 3. **Optimization**: - Identify the bottleneck from the profiling results. - Rewrite the critical part of this bottleneck in Cython for performance improvement. - Ensure that the optimized part is properly integrated into your class. 4. **Performance Evaluation**: - Compare the execution time of the original and Cython optimized versions using the `%timeit` magic command. - Document the performance gains. # Submission Requirements - Python script implementing `MyLinearRegression`. - Profiling results highlighting the bottlenecks. - Cython script with optimized code. - Comparison report on performance gains. # Constraints 1. You must use NumPy for any array computations. 2. The optimization must be performed using Cython. 3. Use a small dataset (`digits` from `sklearn.datasets`) for profiling and performance tests. # Example Usage ```python from sklearn.datasets import load_digits from my_linear_regression import MyLinearRegression # Load data X, y = load_digits(return_X_y=True) # Initialize and train model model = MyLinearRegression() model.fit(X, y) # Make predictions predictions = model.predict(X) ``` This task will test your ability to implement machine learning algorithms, perform profiling, and apply optimization techniques effectively.","solution":"import numpy as np class MyLinearRegression: def __init__(self, learning_rate=0.01, n_iterations=1000): self.learning_rate = learning_rate self.n_iterations = n_iterations self.coefficients = None def fit(self, X, y): X = np.insert(X, 0, 1, axis=1) # Add bias term n_samples, n_features = X.shape self.coefficients = np.zeros(n_features) for _ in range(self.n_iterations): predictions = X.dot(self.coefficients) errors = predictions - y gradient = (2/n_samples) * X.T.dot(errors) self.coefficients -= self.learning_rate * gradient def predict(self, X): X = np.insert(X, 0, 1, axis=1) # Add bias term return X.dot(self.coefficients)"},{"question":"# XML Parsing and Element Counting You are given a collection of XML documents, and your task is to write a Python function that uses the `xml.parsers.expat` module to parse these documents and count the number of occurrences of each element type (e.g., `<item>`, `<category>`, etc.) across all documents. The function should be named `count_xml_elements` and accept a list of strings, where each string is the content of an XML document. The function should return a dictionary where the keys are the element names and the values are the counts of these elements in all provided documents. Here are the steps you should follow: 1. **Create an XML parser using `ParserCreate`.** 2. **Set appropriate handler functions (e.g., `StartElementHandler`) to process the start of each element and update the count in a dictionary.** 3. **Handle errors gracefully by catching `ExpatError` exceptions and printing relevant error messages.** 4. **Ensure to parse each document in the provided list, updating the element count for each document.** **Function Signature**: ```python def count_xml_elements(xml_documents: List[str]) -> Dict[str, int]: pass ``` **Input**: - `xml_documents` (List[str]): A list of strings, where each string is a valid XML document. **Output**: - `Dict[str, int]`: A dictionary where the keys are XML element names and the values are the counts of these elements across all documents. **Example**: ```python xml_docs = [ <?xml version=\\"1.0\\"?> <data> <item>Item 1</item> <item>Item 2</item> <category>Category 1</category> </data>, <?xml version=\\"1.0\\"?> <orders> <order>Order 1</order> <order>Order 2</order> <item>Item 3</item> </orders> ] result = count_xml_elements(xml_docs) print(result) # Output: {\'data\': 1, \'item\': 3, \'category\': 1, \'orders\': 1, \'order\': 2} ``` **Constraints**: - Assume that the provided XML documents are well-formed. - Your function should handle any number of XML documents. - Pay attention to performance; parsing should efficiently handle large documents. **Notes**: - This task assesses your understanding of the `xml.parsers.expat` module, handling XML parsing events, and managing errors. - Ensure to test your function with various XML structures and nested elements to confirm it counts elements correctly.","solution":"from typing import List, Dict from xml.parsers.expat import ParserCreate, ExpatError def count_xml_elements(xml_documents: List[str]) -> Dict[str, int]: element_counts = {} def start_element(name, attrs): if name in element_counts: element_counts[name] += 1 else: element_counts[name] = 1 for xml_doc in xml_documents: parser = ParserCreate() parser.StartElementHandler = start_element try: parser.Parse(xml_doc) except ExpatError as e: print(f\\"Error parsing XML document: {e}\\") return element_counts"},{"question":"Objective: Implement a program that performs matrix multiplication on randomly generated matrices using Intel XPU, demonstrating your understanding of device management, memory handling, and stream synchronization in `torch.xpu`. Requirements: 1. **Matrix Generation**: Generate two matrices `A` and `B` of size `N x N` with random float values. 2. **Device Management**: Ensure that the matrices are allocated on the XPU if available. 3. **Stream Handling**: Use streams to manage computation. 4. **Synchronization**: Ensure proper synchronization between host and device to retrieve the result. 5. **Memory Management**: Report the memory usage before and after the computation. Input: - `N`: An integer specifying the size of the matrices. Output: - A matrix `C` which is the result of the matrix multiplication `A * B`. - Memory usage before and after the computation. Constraints: - Ensure that the code handles scenarios where an XPU is not available. - Optimize memory use to avoid allocation errors for large matrices. Sample Code Outline: ```python import torch import torch.xpu as xpu def matrix_multiplication_on_xpu(N): # Check XPU availability if not xpu.is_available(): raise RuntimeError(\\"XPU is not available\\") xpu_device = torch.device(\'xpu:0\') # Initialize the XPU xpu.init() # Report initial memory usage initial_memory = xpu.memory_allocated(xpu_device) # Create random matrices A and B A = torch.randn((N, N), device=xpu_device) B = torch.randn((N, N), device=xpu_device) # Create a stream for the computation stream = xpu.Stream(device=xpu_device) # Perform matrix multiplication within the designated stream with xpu.stream(stream): C = torch.mm(A, B) # Ensure computation is complete xpu.synchronize() # Report final memory usage final_memory = xpu.memory_allocated(xpu_device) return C, initial_memory, final_memory # Example usage: N = 1024 result_matrix, memory_before, memory_after = matrix_multiplication_on_xpu(N) print(f\\"Memory before computation: {memory_before} bytes\\") print(f\\"Memory after computation: {memory_after} bytes\\") print(f\\"Result matrix C: {result_matrix}\\") ``` The provided code outline serves as a guideline. Students are expected to fill in the details and handle necessary error checking and edge cases.","solution":"import torch import torch.cuda as cuda def matrix_multiplication_on_xpu(N): Perform matrix multiplication on randomly generated N x N matrices using GPU (XPU) if available. Reports the memory usage before and after the computation. # Check GPU availability if not cuda.is_available(): raise RuntimeError(\\"XPU (GPU) is not available. Make sure you have a compatible GPU and the CUDA library installed.\\") gpu_device = torch.device(\'cuda:0\') # Report initial memory usage initial_memory = cuda.memory_allocated(gpu_device) # Create random matrices A and B A = torch.randn((N, N), device=gpu_device) B = torch.randn((N, N), device=gpu_device) # Create a stream for the computation stream = cuda.Stream(device=gpu_device) # Perform matrix multiplication within the designated stream with cuda.stream(stream): C = torch.mm(A, B) # Ensure computation is complete cuda.synchronize() # Report final memory usage final_memory = cuda.memory_allocated(gpu_device) return C, initial_memory, final_memory"},{"question":"# PyTorch Advanced Numeric Suite You are provided with an early prototype of PyTorch\'s Advanced Optimization Numeric Suite utilities which includes functions to compute signal-to-quantization-noise ratio (SQNR), normalized L2 error, and cosine similarity for tensor comparisons. Your task is to use these utilities to implement a function that evaluates a model by comparing its predictions to a set of target values using these metrics. Function Signature ```python def evaluate_model_predictions(model: torch.nn.Module, data_loader: torch.utils.data.DataLoader, criterion: torch.nn.Module) -> Dict[str, torch.Tensor]: Evaluates the given model using a data loader and computes various error metrics between the model\'s predictions and the target values. Args: model (torch.nn.Module): The model to evaluate. data_loader (torch.utils.data.DataLoader): DataLoader containing the dataset for evaluation. criterion (torch.nn.Module): Loss function, e.g. nn.MSELoss(), nn.CrossEntropyLoss(), etc. Returns: Dict[str, torch.Tensor]: A dictionary containing the computed metrics: - \\"mean_loss\\": Mean loss over the dataset. - \\"mean_sqnr\\": Mean SQNR over the dataset. - \\"mean_l2_error\\": Mean normalized L2 error over the dataset. - \\"mean_cosine_similarity\\": Mean cosine similarity over the dataset. pass ``` Requirements 1. **Model Evaluation**: For each batch of data, compute the loss using the specified `criterion`. 2. **Metric Computation**: For each batch, utilize the provided utilities to compute the following metrics between predictions and targets: - Signal-to-Quantization-Noise Ratio (SQNR) - Normalized L2 Error - Cosine Similarity 3. **Aggregation**: Aggregate the per-batch metrics to compute the mean values over the entire dataset. 4. **Return Metrics**: Return a dictionary containing the mean values of the loss and the computed metrics. You are required to leverage the following utility functions in the `torch.ao.ns.fx.utils` module: ```python from torch.ao.ns.fx.utils import compute_sqnr, compute_normalized_l2_error, compute_cosine_similarity ``` Example ```python import torch from torch.utils.data import DataLoader from torch.nn import MSELoss, Sequential, Linear class SimpleDataset(torch.utils.data.Dataset): def __init__(self): self.data = torch.randn(100, 10) self.targets = torch.randn(100, 1) def __len__(self): return len(self.data) def __getitem__(self, idx): return self.data[idx], self.targets[idx] # Sample dataset and DataLoader dataset = SimpleDataset() data_loader = DataLoader(dataset, batch_size=10) # Sample model model = Sequential(Linear(10, 50), Linear(50, 1)) # Loss function criterion = MSELoss() metrics = evaluate_model_predictions(model, data_loader, criterion) print(metrics) ``` The expected output should be a dictionary containing the mean loss, mean SQNR, mean normalized L2 error, and mean cosine similarity for the given model over the dataset. **Constraints**: - You should ensure the model is in evaluation mode during the evaluation process. - Ensure proper device handling (CPU/GPU) for tensors. - Return meaningful values when metrics cannot be computed (e.g., when SQNR or Cosine Similarity is not defined). Notes - This question assesses your understanding of PyTorch\'s data loading, model evaluation, and utilization of numerical utilities. - Handle any potential edge cases (e.g., zero-division errors) gracefully.","solution":"import torch from torch.utils.data import DataLoader from torch.nn import Module, MSELoss from torch.ao.ns.fx.utils import compute_sqnr, compute_normalized_l2_error, compute_cosine_similarity from typing import Dict def evaluate_model_predictions( model: Module, data_loader: DataLoader, criterion: Module ) -> Dict[str, torch.Tensor]: # Set the model to evaluation mode model.eval() # Placeholders for metrics to accumulate total_loss = 0.0 total_sqnr = 0.0 total_l2_error = 0.0 total_cosine_similarity = 0.0 batch_count = 0 with torch.no_grad(): for inputs, targets in data_loader: outputs = model(inputs) loss = criterion(outputs, targets) total_loss += loss.item() # Calculate additional metrics sqnr = compute_sqnr(outputs, targets) l2_error = compute_normalized_l2_error(outputs, targets) cosine_similarity = compute_cosine_similarity(outputs, targets) # Accumulate the metrics total_sqnr += sqnr.item() total_l2_error += l2_error.item() total_cosine_similarity += cosine_similarity.item() batch_count += 1 # Compute mean values mean_loss = total_loss / batch_count if batch_count > 0 else float(\'nan\') mean_sqnr = total_sqnr / batch_count if batch_count > 0 else float(\'nan\') mean_l2_error = total_l2_error / batch_count if batch_count > 0 else float(\'nan\') mean_cosine_similarity = total_cosine_similarity / batch_count if batch_count > 0 else float(\'nan\') return { \\"mean_loss\\": torch.tensor(mean_loss), \\"mean_sqnr\\": torch.tensor(mean_sqnr), \\"mean_l2_error\\": torch.tensor(mean_l2_error), \\"mean_cosine_similarity\\": torch.tensor(mean_cosine_similarity) }"},{"question":"You are given a list of Python objects that need to be serialized and deserialized using the `marshal` module. Your task is to write a function that takes the list of objects, serializes them, and then deserializes them to verify their integrity. Function Signature ```python def check_serialization(objects: list) -> bool: pass ``` Input - `objects`: A list of Python objects to be serialized. The objects are guaranteed to be of the types supported by the `marshal` module: booleans, integers, floating point numbers, complex numbers, strings, bytes, bytearrays, tuples, lists, sets, frozensets, dictionaries, and code objects. Output - The function should return `True` if all objects are successfully serialized and deserialized without any data corruption. Return `False` otherwise. Notes - Use the `marshal.dumps()` and `marshal.loads()` functions to handle the serialization and deserialization processes. - Ensure that no unsupported types are included in the objects (this is guaranteed by the input). - You should handle exceptions that might be raised during serialization and deserialization and return `False` if any exceptions occur. - The serialization should maintain the structure and values of the original objects. Example ```python objects = [123, 45.67, \\"hello\\", [1, 2, 3], {\'key\': \'value\'}, (4, 5, 6)] print(check_serialization(objects)) # Expected output: True objects = [1234, 56.78, b\\"bytes\\", [4, 5], {\'bad_key\': set([1, 2])}] # sets are allowed in the module, example will return True. print(check_serialization(objects)) # Expected output: True ``` Implement function `check_serialization` with the specifications provided.","solution":"import marshal def check_serialization(objects: list) -> bool: try: for obj in objects: serialized = marshal.dumps(obj) deserialized = marshal.loads(serialized) if obj != deserialized: return False return True except Exception: return False"},{"question":"# Question: File Compression and Archiving in Python In this coding assessment, you are required to implement functions that demonstrate your understanding of data compression and archiving using various modules available in Python 3.10. You will specifically work with the `gzip`, `bz2`, `lzma`, `zipfile`, and `tarfile` modules. # Tasks 1. **Gzip Compression and Decompression**: - **Function Name**: `gzip_compress` - **Input**: A string `input_data`. - **Output**: Compressed binary data in gzip format. - **Function Name**: `gzip_decompress` - **Input**: Gzip compressed binary data. - **Output**: The original decompressed string. 2. **Bzip2 Compression and Decompression**: - **Function Name**: `bz2_compress` - **Input**: A string `input_data`. - **Output**: Compressed binary data in bzip2 format. - **Function Name**: `bz2_decompress` - **Input**: Bzip2 compressed binary data. - **Output**: The original decompressed string. 3. **LZMA Compression and Decompression**: - **Function Name**: `lzma_compress` - **Input**: A string `input_data`. - **Output**: Compressed binary data in LZMA format. - **Function Name**: `lzma_decompress` - **Input**: LZMA compressed binary data. - **Output**: The original decompressed string. 4. **ZIP Archive Creation and Extraction**: - **Function Name**: `create_zip_archive` - **Input**: A list of filenames `file_list` and the output ZIP filename `output_zip`. - **Output**: None (Creates a ZIP archive containing the specified files). - **Function Name**: `extract_zip_archive` - **Input**: The name of the ZIP file `zip_filename` and the `extract_path`. - **Output**: None (Extracts all files from the ZIP archive to the specified path). 5. **Tar Archive Creation and Extraction**: - **Function Name**: `create_tar_archive` - **Input**: A list of filenames `file_list` and the output tar filename `output_tar`. - **Output**: None (Creates a tar archive containing the specified files). - **Function Name**: `extract_tar_archive` - **Input**: The name of the tar file `tar_filename` and the `extract_path`. - **Output**: None (Extracts all files from the tar archive to the specified path). # Constraints - Assume that the input strings for compression do not exceed 1 MB in size. - You may assume that the files mentioned in `file_list` exist in the current working directory. - Handle any errors gracefully and provide meaningful error messages. # Example Usage ```python # Example usage for gzip compression and decompression compressed_data = gzip_compress(\\"This is a sample text to compress\\") original_data = gzip_decompress(compressed_data) print(original_data) # Output: \\"This is a sample text to compress\\" # Example usage for creating and extracting a ZIP archive create_zip_archive([\'file1.txt\', \'file2.txt\'], \'archive.zip\') extract_zip_archive(\'archive.zip\', \'./extracted_files\') ```","solution":"import gzip import bz2 import lzma import zipfile import tarfile import os def gzip_compress(input_data): Compresses the input string using gzip. return gzip.compress(input_data.encode(\'utf-8\')) def gzip_decompress(compressed_data): Decompresses the gzip compressed binary data. return gzip.decompress(compressed_data).decode(\'utf-8\') def bz2_compress(input_data): Compresses the input string using bzip2. return bz2.compress(input_data.encode(\'utf-8\')) def bz2_decompress(compressed_data): Decompresses the bzip2 compressed binary data. return bz2.decompress(compressed_data).decode(\'utf-8\') def lzma_compress(input_data): Compresses the input string using lzma. return lzma.compress(input_data.encode(\'utf-8\')) def lzma_decompress(compressed_data): Decompresses the lzma compressed binary data. return lzma.decompress(compressed_data).decode(\'utf-8\') def create_zip_archive(file_list, output_zip): Creates a ZIP archive containing the specified files. with zipfile.ZipFile(output_zip, \'w\') as zipf: for file in file_list: zipf.write(file, arcname=os.path.basename(file)) def extract_zip_archive(zip_filename, extract_path): Extracts all files from the ZIP archive to the specified path. with zipfile.ZipFile(zip_filename, \'r\') as zipf: zipf.extractall(path=extract_path) def create_tar_archive(file_list, output_tar): Creates a tar archive containing the specified files. with tarfile.open(output_tar, \'w\') as tarf: for file in file_list: tarf.add(file, arcname=os.path.basename(file)) def extract_tar_archive(tar_filename, extract_path): Extracts all files from the tar archive to the specified path. with tarfile.open(tar_filename, \'r\') as tarf: tarf.extractall(path=extract_path)"},{"question":"# Advanced POP3 Client Implementation You are required to develop a POP3 email client using Python\'s `poplib` module. Your client should be capable of connecting to a POP3 server, authenticating, retrieving, and displaying emails. Additionally, it should handle errors gracefully and support both non-encrypted and SSL connections. Detailed Requirements: 1. Implement a function `retrieve_emails(host, username, password, use_ssl=False, port=None, context=None)` that: - Connects to a POP3 server given the `host`, `username`, `password`. - Optionally connects using SSL if `use_ssl` is set to `True`. - Supports SSL context if provided through the `context` parameter. - Supports connecting to custom ports if specified in the `port` parameter. - Retrieves and prints the entire content of all emails in the user\'s mailbox. 2. Handle the following errors appropriately: - Connection errors. - Authentication errors. - Issues related to retrieving emails. 3. Ensure that the mailbox is properly closed after retrieving the emails using the `quit()` method. Input and Output Specifications: - **Input:** - `host` (str): The hostname of the POP3 server. - `username` (str): The username for authentication. - `password` (str): The password for authentication. - `use_ssl` (bool, optional): Whether to use SSL for the connection. Default is `False`. - `port` (int, optional): The port number to connect to. Default is `None`, which uses the default POP3 port. - `context` (ssl.SSLContext, optional): SSL context for SSL connections. Default is `None`. - **Output:** - Prints the content of all emails retrieved from the server. Constraints: - You may assume that the server details, username, and password provided are valid. Example Usage: ```python import ssl # Example non-SSL connection retrieve_emails(\'pop.example.com\', \'user@example.com\', \'password123\') # Example SSL connection with SSL context context = ssl.create_default_context() retrieve_emails(\'pop.example.com\', \'user@example.com\', \'password123\', use_ssl=True, context=context) ``` Hints: - Use `poplib.POP3` for non-SSL connections and `poplib.POP3_SSL` for SSL connections. - Ensure that all parts of the email are printed, including headers and body. - Use the `retr` method to retrieve messages from the server. - Use appropriate methods (`POP3.user()` and `POP3.pass_()`) for authentication.","solution":"import poplib import ssl def retrieve_emails(host, username, password, use_ssl=False, port=None, context=None): Connects to a POP3 server and retrieves all emails. :param host: The hostname of the POP3 server. :param username: The username for authentication. :param password: The password for authentication. :param use_ssl: Whether to use SSL for the connection. Default is `False`. :param port: The port number to connect to. Default is None, which uses the default POP3 port. :param context: SSL context for SSL connections. Default is `None`. try: if use_ssl: port = port if port else poplib.POP3_SSL_PORT server = poplib.POP3_SSL(host, port, context=context) else: port = port if port else poplib.POP3_PORT server = poplib.POP3(host, port) server.user(username) server.pass_(password) email_count, total_size = server.stat() print(f\\"Number of messages: {email_count}. Total size: {total_size} bytes.\\") for i in range(1, email_count + 1): response, lines, octets = server.retr(i) msg_content = b\'n\'.join(lines).decode(\'utf-8\') print(f\\"nEmail {i}:n\\") print(msg_content) except poplib.error_proto as e: print(f\\"POP3 error: {e}\\") except ssl.SSLError as e: print(f\\"SSL error: {e}\\") except Exception as e: print(f\\"Unexpected error: {e}\\") finally: if \'server\' in locals(): server.quit()"},{"question":"Optimizing GEMM Operations with pytorch\'s TunableOps PyTorch\'s `torch.cuda.tunable` module provides a prototype feature for optimizing General Matrix Multiply (GEMM) operations on CUDA devices. This feature allows you to enable tuning, set various tuning parameters, and handle tuning data through file operations. This question assesses your ability to harness these features in an applied context. Problem: You are given a machine learning model that performs extensive matrix multiplications, and you aim to optimize these operations using PyTorch\'s `torch.cuda.tunable` module. Your task is to: 1. Enable the tuning feature. 2. Set the maximum tuning duration to 30 seconds. 3. Set the maximum tuning iterations to 1000. 4. Enable recording of untuned states. 5. Specify a file name `\\"tuning_results.json\\"` for storing tuning results. 6. Perform GEMM tuning for a sample GEMM operation and store the results. Implement the function `optimize_gemm()` that encapsulates all these tasks. Function Signature ```python import torch.cuda.tunable as tc def optimize_gemm(): pass ``` Expected Actions 1. Enable the tuning feature using `tc.enable()`. 2. Set the maximum tuning duration using `tc.set_max_tuning_duration(30)`. 3. Set the maximum tuning iterations using `tc.set_max_tuning_iterations(1000)`. 4. Enable the recording of untuned states using `tc.record_untuned_enable()`. 5. Set the file name for storing the tuning results using `tc.set_filename(\\"tuning_results.json\\")`. 6. Perform GEMM tuning using `tc.tune_gemm_in_file()`, assuming there is a GEMM operation defined within your function for the sake of demonstration. 7. Write the tuning results to the specified file using `tc.write_file()`. Completion Criteria - Ensure that all tuning features are enabled and the parameters are set correctly. - The function should perform GEMM tuning and successfully write results to `\\"tuning_results.json\\"`. Example Implementation ```python import torch.cuda.tunable as tc def optimize_gemm(): # Enable tuning tc.enable() # Set maximum tuning duration to 30 seconds tc.set_max_tuning_duration(30) # Set maximum tuning iterations to 1000 tc.set_max_tuning_iterations(1000) # Enable recording of untuned states tc.record_untuned_enable() # Set the filename for tuning results tc.set_filename(\\"tuning_results.json\\") # Placeholder GEMM tuning operation # Normally, you would define a matrix multiplication operation here # For example, you could tune a sample GEMM operation using: # tc.tune_gemm_in_file() # Write the tuning results to the specified file tc.write_file() # Example function call optimize_gemm() ``` Constraints - You may assume the GEMM operation is implicitly defined within `tc.tune_gemm_in_file()` for simplicity. - The prototype functions and features are subject to change, so ensure compatibility with the latest PyTorch version documented. This question aims to test your understanding and ability to utilize PyTorch’s CUDA tuning features effectively.","solution":"import torch.cuda.tunable as tc def optimize_gemm(): # Enable tuning tc.enable() # Set maximum tuning duration to 30 seconds tc.set_max_tuning_duration(30) # Set maximum tuning iterations to 1000 tc.set_max_tuning_iterations(1000) # Enable recording of untuned states tc.record_untuned_enable() # Set the filename for tuning results tc.set_filename(\\"tuning_results.json\\") # Perform GEMM tuning (assuming the GEMM operation is defined within tune_gemm_in_file) tc.tune_gemm_in_file() # Write the tuning results to the specified file tc.write_file()"},{"question":"# Question: Implementing a Clustering Algorithm using Scikit-Learn Objective: You are required to implement a function that utilizes the K-Means clustering algorithm from the scikit-learn library to cluster a given dataset. Your function should demonstrate comprehension of fundamental clustering concepts and scikit-learn utilization. Function Signature: ```python def perform_kmeans_clustering(data: List[List[float]], n_clusters: int) -> List[int]: Perform K-Means clustering on the given dataset. Parameters: data (List[List[float]]): A 2D list where each inner list represents a data point with features. n_clusters (int): The number of clusters to form. Returns: List[int]: A list of cluster indices for each data point. ``` Input: - `data`: A 2D list of floats representing the dataset to be clustered. - `n_clusters`: An integer indicating the number of clusters to form. Output: - A list of integers where each element represents the cluster index assigned to the corresponding data point in the input dataset. Constraints: - Each inner list in `data` must have the same length. - `n_clusters` must be a positive integer less than or equal to the number of data points in the dataset. Performance Requirements: The function should be able to handle a dataset with up to 1000 data points efficiently. Example: ```python data = [ [1.0, 2.0], [1.1, 2.1], [3.0, 4.0], [3.1, 4.1] ] n_clusters = 2 result = perform_kmeans_clustering(data, n_clusters) print(result) # Output: A list of cluster indices, e.g., [0, 0, 1, 1] ``` Note: You may use the `KMeans` class from the `sklearn.cluster` module. Ensure that you handle any necessary imports within your function.","solution":"from sklearn.cluster import KMeans from typing import List def perform_kmeans_clustering(data: List[List[float]], n_clusters: int) -> List[int]: Perform K-Means clustering on the given dataset. Parameters: data (List[List[float]]): A 2D list where each inner list represents a data point with features. n_clusters (int): The number of clusters to form. Returns: List[int]: A list of cluster indices for each data point. if not data or n_clusters <= 0 or n_clusters > len(data): raise ValueError(\\"Invalid input parameters\\") kmeans = KMeans(n_clusters=n_clusters) kmeans.fit(data) return kmeans.labels_.tolist()"},{"question":"# Queue Synchronization in Multi-threaded Programming **Problem Statement:** You are required to implement a multi-threaded task processing system using Python\'s `queue` module. Your system will use a FIFO queue to manage tasks and ensure that tasks are processed in the order they are received. The system must handle potential issues like the queue being empty or full, and manage tasks from multiple producer and consumer threads. **Requirements:** 1. **Producer Function:** This function will add tasks to the queue. It should handle the case where the queue is full and wait until space is available. 2. **Consumer Function:** This function will retrieve tasks from the queue and simulate processing them. It should handle the case where the queue is empty and wait until a task is available. 3. **Thread Management:** The main function should start multiple producer and consumer threads, ensuring all tasks are processed, and correctly handle thread synchronization and task completion. **Implementation Details:** 1. Define a class `TaskQueue` that contains a `queue.Queue` instance. 2. Implement the `producer` method which will add a specified number of tasks to the queue. 3. Implement the `consumer` method which continuously retrieves and processes tasks from the queue until a termination condition is met. 4. Create a main function that: - Initializes the task queue. - Starts a specified number of producer threads that add tasks to the queue. - Starts a specified number of consumer threads that process tasks from the queue. - Ensures all tasks are processed before terminating. **Input:** - Number of producer threads (N). - Number of consumer threads (M). - Number of tasks to be produced by each producer thread (T). **Output:** - Print statements showing the status of task addition and processing. - Ensure the program prints \\"All tasks have been processed.\\" once all tasks have been completed. **Example:** ```python import queue import threading import time class TaskQueue: def __init__(self, maxsize=10): self.q = queue.Queue(maxsize=maxsize) def producer(self, num_tasks, thread_id): for i in range(num_tasks): item = f\'Task-{thread_id}-{i}\' self.q.put(item) print(f\\"Producer {thread_id}: Added {item}\\") time.sleep(0.1) # Simulate the task creation time def consumer(self, thread_id): while True: try: item = self.q.get(timeout=1) # Wait for a task print(f\\"Consumer {thread_id}: Processing {item}\\") time.sleep(0.2) # Simulate the task processing time self.q.task_done() except queue.Empty: print(f\\"Consumer {thread_id}: No more tasks to process, exiting.\\") break def main(num_producers, num_consumers, tasks_per_producer): task_queue = TaskQueue(maxsize=20) producers = [] for i in range(num_producers): t = threading.Thread(target=task_queue.producer, args=(tasks_per_producer, i)) t.start() producers.append(t) consumers = [] for i in range(num_consumers): t = threading.Thread(target=task_queue.consumer, args=(i,)) t.start() consumers.append(t) for p in producers: p.join() task_queue.q.join() for c in consumers: c.join() print(\\"All tasks have been processed.\\") if __name__ == \\"__main__\\": main(3, 4, 5) # Example parameters: 3 producers, 4 consumers, 5 tasks per producer ``` **Constraints:** - Each producer adds tasks in sequential order. - Consumers process tasks until no more tasks are available. - Handle any necessary synchronization to ensure the correct order of task processing. **Performance:** - Ensure proper synchronization to avoid deadlocks and race conditions. - Efficient handling of queue operations to minimize blocking.","solution":"import queue import threading import time class TaskQueue: def __init__(self, maxsize=10): self.q = queue.Queue(maxsize=maxsize) def producer(self, num_tasks, thread_id): for i in range(num_tasks): item = f\'Task-{thread_id}-{i}\' print(f\\"Producer {thread_id}: Adding {item} to queue\\") self.q.put(item) print(f\\"Producer {thread_id}: Added {item} to queue\\") time.sleep(0.1) # Simulate the task creation time def consumer(self, thread_id): while True: try: item = self.q.get(timeout=1) # Wait for a task print(f\\"Consumer {thread_id}: Processing {item}\\") time.sleep(0.2) # Simulate the task processing time self.q.task_done() except queue.Empty: print(f\\"Consumer {thread_id}: Queue is empty, exiting.\\") break def main(num_producers, num_consumers, tasks_per_producer): task_queue = TaskQueue(maxsize=20) producers = [] for i in range(num_producers): t = threading.Thread(target=task_queue.producer, args=(tasks_per_producer, i)) t.start() producers.append(t) consumers = [] for i in range(num_consumers): t = threading.Thread(target=task_queue.consumer, args=(i,)) t.start() consumers.append(t) for p in producers: p.join() task_queue.q.join() for c in consumers: c.join() print(\\"All tasks have been processed.\\") if __name__ == \\"__main__\\": main(3, 4, 5) # Example parameters: 3 producers, 4 consumers, 5 tasks per producer"},{"question":"**Problem Statement:** You are provided with a mixed list containing different types of Python objects (integers, strings, bytes, tuples, lists, dictionaries, sets, and booleans). Your task is to implement a function `summarize_objects(objects_list)` that analyzes this list and returns a dictionary summarizing the count and types of objects in the list. Additionally, the function should reject `None` object types and raise an appropriate warning. # Function Signature ```python def summarize_objects(objects_list: list) -> dict: ``` # Input - `objects_list (list)`: A list containing various Python objects. The list can contain the following object types: integers, strings, bytes, tuples, lists, dictionaries, sets, and booleans. The list should not contain `None`. # Output - `summary (dict)`: A dictionary with the object types as keys and the count of each type as values. The dictionary should be formatted as follows: ```python { \\"integer\\": int, \\"string\\": int, \\"bytes\\": int, \\"tuple\\": int, \\"list\\": int, \\"dictionary\\": int, \\"set\\": int, \\"boolean\\": int } ``` # Constraints - You must check the type of each object. - If the list contains a `None` object, raise a `ValueError` with the message \\"NoneType is not allowed\\". - The function should handle an empty list by returning a dictionary with all counts set to 0. # Example ```python # Example 1 input_list = [1, \\"hello\\", b\\"bytes\\", (1, 2), [3, 4], {\\"a\\": 1}, {1, 2}, True] print(summarize_objects(input_list)) # Output: {\\"integer\\": 1, \\"string\\": 1, \\"bytes\\": 1, \\"tuple\\": 1, \\"list\\": 1, \\"dictionary\\": 1, \\"set\\": 1, \\"boolean\\": 1} # Example 2 input_list = [] print(summarize_objects(input_list)) # Output: {\\"integer\\": 0, \\"string\\": 0, \\"bytes\\": 0, \\"tuple\\": 0, \\"list\\": 0, \\"dictionary\\": 0, \\"set\\": 0, \\"boolean\\": 0} # Example 3 input_list = [1, None, \\"test\\"] try: print(summarize_objects(input_list)) except ValueError as e: print(e) # Output: ValueError: NoneType is not allowed ``` # Note - Be sure to import necessary modules (if any). - Consider edge cases such as an empty list, or lists with multiple types of the same object.","solution":"def summarize_objects(objects_list: list) -> dict: Analyzes the list of mixed objects and returns a dictionary summarizing the count of each type. Raises a ValueError if the list contains None. Args: objects_list (list): The list of mixed Python objects. Returns: dict: A dictionary with the object types as keys and counts as values. summary = { \\"integer\\": 0, \\"string\\": 0, \\"bytes\\": 0, \\"tuple\\": 0, \\"list\\": 0, \\"dictionary\\": 0, \\"set\\": 0, \\"boolean\\": 0 } for obj in objects_list: if obj is None: raise ValueError(\\"NoneType is not allowed\\") elif isinstance(obj, int) and not isinstance(obj, bool): # Python treats boolean as a subclass of int, differentiate them. summary[\\"integer\\"] += 1 elif isinstance(obj, str): summary[\\"string\\"] += 1 elif isinstance(obj, bytes): summary[\\"bytes\\"] += 1 elif isinstance(obj, tuple): summary[\\"tuple\\"] += 1 elif isinstance(obj, list): summary[\\"list\\"] += 1 elif isinstance(obj, dict): summary[\\"dictionary\\"] += 1 elif isinstance(obj, set): summary[\\"set\\"] += 1 elif isinstance(obj, bool): summary[\\"boolean\\"] += 1 return summary"},{"question":"# Task You are given a dataset containing information about various automobiles. You will need to use the Seaborn library to create different types of KDE plots to analyze the distributions of the variables. # Datasets You can use the sample dataset provided by Seaborn for this task: ```python import seaborn as sns auto = sns.load_dataset(\\"mpg\\") ``` # Requirements 1. **Basic Univariate KDE Plot**: - Plot a KDE for the `horsepower` variable. 2. **Conditional KDE Plot**: - Create a KDE plot for the `weight` variable, mapping the `origin` variable to the `hue` parameter to explore the distribution of weights based on the origin. 3. **Adjusted Smoothing KDE Plot**: - Create two KDE plots for the `mpg` variable: - One with less smoothing (you decide the `bw_adjust` value). - One with more smoothing (you decide the `bw_adjust` value). 4. **Bivariate KDE Plot**: - Plot a bivariate KDE for `horsepower` and `weight`. - Add filled contours to this plot. 5. **Combined Plot with Modifications**: - Create a stacked KDE plot for the `displacement` variable, mapping the `cylinders` variable to the `hue` parameter. - Normalize the distribution at each value in the grid. # Constraints - Ensure all required plots have appropriate titles, axis labels, and legends if necessary. - Use appropriate smoothing adjustments to reveal meaningful patterns in each plot. - Ensure all plots are clearly readable and distinguishable. # Example Output There is no fixed example output. Your plots should reflect an understanding of the Seaborn `kdeplot` function and its various parameters as mentioned above. Include appropriate titles and labels for each plot. # Submission Submit a Jupyter Notebook file (`.ipynb`) containing your code, comments explaining your choices for certain parameters, and the resulting plots.","solution":"import seaborn as sns import matplotlib.pyplot as plt # Load the dataset auto = sns.load_dataset(\\"mpg\\") # 1. Basic Univariate KDE Plot plt.figure(figsize=(10, 6)) sns.kdeplot(data=auto, x=\\"horsepower\\") plt.title(\\"KDE Plot of Horsepower\\") plt.xlabel(\\"Horsepower\\") plt.ylabel(\\"Density\\") plt.show() # 2. Conditional KDE Plot plt.figure(figsize=(10, 6)) sns.kdeplot(data=auto, x=\\"weight\\", hue=\\"origin\\") plt.title(\\"Conditional KDE Plot of Weight by Origin\\") plt.xlabel(\\"Weight\\") plt.ylabel(\\"Density\\") plt.show() # 3. Adjusted Smoothing KDE Plot # Less smoothing plt.figure(figsize=(10, 6)) sns.kdeplot(data=auto, x=\\"mpg\\", bw_adjust=0.5) plt.title(\\"KDE Plot of MPG with Less Smoothing (bw_adjust=0.5)\\") plt.xlabel(\\"MPG\\") plt.ylabel(\\"Density\\") plt.show() # More smoothing plt.figure(figsize=(10, 6)) sns.kdeplot(data=auto, x=\\"mpg\\", bw_adjust=2) plt.title(\\"KDE Plot of MPG with More Smoothing (bw_adjust=2)\\") plt.xlabel(\\"MPG\\") plt.ylabel(\\"Density\\") plt.show() # 4. Bivariate KDE Plot plt.figure(figsize=(10, 6)) sns.kdeplot(data=auto, x=\\"horsepower\\", y=\\"weight\\", fill=True) plt.title(\\"Bivariate KDE Plot of Horsepower and Weight\\") plt.xlabel(\\"Horsepower\\") plt.ylabel(\\"Weight\\") plt.show() # 5. Combined Plot with Modifications plt.figure(figsize=(10, 6)) sns.kdeplot(data=auto, x=\\"displacement\\", hue=\\"cylinders\\", multiple=\\"stack\\", common_norm=False) plt.title(\\"Stacked KDE Plot of Displacement by Cylinders\\") plt.xlabel(\\"Displacement\\") plt.ylabel(\\"Density\\") plt.show()"},{"question":"# Custom Python Interactive Interpreter You are required to create a custom interactive Python interpreter using the \\"code\\" and \\"codeop\\" modules. The interpreter should support the following enhanced feature in addition to executing the normal Python code: 1. **Command History**: The interpreter should keep a history of all executed commands and provide a feature to list the last `n` commands. # Requirements 1. Implement a class `CustomInterpreter` which inherits from `code.InteractiveConsole`. 2. The class should have the following methods: - `__init__(self)`: Initialize the interpreter by calling the super class initializer and setting up a command history list. - `runsource(self, source, filename=\\"<input>\\", symbol=\\"single\\")`: Override this method to execute the given source code and store it in the history. If the code execution produces an `EOFError`, the method should return `False`. - `list_history(self, n)`: List the last `n` executed commands from history. # Expected Input and Output - **Initialization**: When you create an object of `CustomInterpreter`, it initializes the interpreter environment. - **Running Code**: Use the `runsource` method to provide Python code as a string to be executed by the interpreter. - **Listing History**: The `list_history` method takes an integer `n` and returns the last `n` executed commands as a list of strings. # Example Usage ```python interpreter = CustomInterpreter() interpreter.runsource(\\"a = 5\\") interpreter.runsource(\\"print(a)\\") interpreter.runsource(\\"for i in range(3): print(i)\\") # List the last 2 commands print(interpreter.list_history(2)) # Output should be: [\\"print(a)\\", \\"for i in range(3): print(i)\\"] ``` # Constraints - Handle exceptions appropriately to ensure the interpreter does not crash on invalid code. - Store the history in the order of execution. Submit your implementation code for the `CustomInterpreter` class.","solution":"import code class CustomInterpreter(code.InteractiveConsole): def __init__(self): super().__init__() self.command_history = [] def runsource(self, source, filename=\\"<input>\\", symbol=\\"single\\"): self.command_history.append(source) try: exec(source, self.locals) except Exception as e: print(f\\"Error executing code: {e}\\") return False return True def list_history(self, n): return self.command_history[-n:]"},{"question":"**Question: Implement a Custom Python Interactive Console** # Background: Python provides modules such as \\"code\\" and \\"codeop\\" to enable the creation of custom interactive interpreters. These allow developers to create environments where Python code can be executed dynamically. # Objective: Your task is to implement a custom interactive console that: - Allows the user to input and execute Python commands dynamically. - Provides feedback for syntax errors or incomplete code. - Allows multiline command input for complete code blocks. # Requirements: 1. **Class Definition:** Create a class named `CustomConsole` that inherits from `code.InteractiveConsole`. 2. **Method `run_custom_console`:** This method initiates the interactive console session. 3. **Error Handling:** Implement error handling to provide feedback for syntax errors or incomplete code. 4. **Multiline Support:** Allow users to input multiline commands which are executed only when the block is complete. # Implementation Details: - **Input and Output:** - The console should keep running and take user input until an explicit exit command (like `exit` or `quit`) is entered. - For any syntax errors in the input, it should display an appropriate error message. - If the user inputs an incomplete command (e.g., an unclosed parenthesis), it should allow additional lines to complete the command. - **Constraints:** - Use the `code` and `codeop` modules only. - Implement the console to prompt for further input when necessary. # Example Interaction: ```python >>> console = CustomConsole() >>> console.run_custom_console() >>> print(\\"Hello, world!\\") Hello, world! >>> def greet(name): ... print(f\\"Hello, {name}!\\") ... >>> greet(\\"Alice\\") Hello, Alice! >>> if True: ... print(\\"This is True\\") ... else: ... print(\\"This is False\\") ... This is True >>> exit ``` # Submission: - Implement the `CustomConsole` class and ensure the `run_custom_console` method works as described. - Provide comments in your code for clarity.","solution":"import code import codeop class CustomConsole(code.InteractiveConsole): def __init__(self, locals=None): super().__init__(locals) self.buffer = [] def run_custom_console(self): print(\\"Custom Interactive Console (type \'exit\' to leave)\\") while True: try: if self.buffer: line = input(\\"... \\") else: line = input(\\">>> \\") if line.strip() in (\'exit\', \'quit\'): print(\\"Exiting custom interactive console.\\") break self.buffer.append(line) source = \\"n\\".join(self.buffer) if self.runsource(source): # More input needed. continue self.buffer = [] # Clear the buffer if the code block is complete. except (EOFError, KeyboardInterrupt): print(\\"nExiting custom interactive console.\\") break except Exception as e: print(f\\"Error: {e}\\") self.buffer = [] if __name__ == \\"__main__\\": console = CustomConsole() console.run_custom_console()"},{"question":"# PyTorch Tensor Operations and Testing Objective: Implement a function that performs specific tensor operations and then verifies the correctness of the computations using PyTorch\'s testing utilities. Function to Implement: ```python import torch def tensor_operations_and_test(size, value): Create two tensors with specific operations and verify their correctness using `torch.testing` functions. Parameters: size (int): The size of the tensors to create. value (float): The value to perform operations with. Returns: None - The function should raise an error if the assertion fails, otherwise it passes silently. # Step 1: Create a tensor of the given size, filled with the given value. tensor_a = torch.full((size,), value) # Step 2: Create another tensor by adding the value to each element of the first tensor. tensor_b = tensor_a + value # Step 3: Verify that each element in tensor_b is twice the value of the elements in tensor_a. expected_tensor = torch.full((size,), value * 2) # Use torch.testing.assert_close to validate tensor_b against the expected_tensor torch.testing.assert_close(tensor_b, expected_tensor) ``` Input and Output: **Input:** - `size` (int): The size (number of elements) of the tensor to create. - `value` (float): The value to fill the tensor and use in operations. **Output:** - The function raises an `AssertionError` if the test fails, otherwise it completes silently indicating successful validation. Example: ```python # Example usage: tensor_operations_and_test(5, 3.0) # This should pass silently, as tensor_b will be correctly [6.0, 6.0, 6.0, 6.0, 6.0] tensor_operations_and_test(4, 2.5) # This should pass silently, as tensor_b will be correctly [5.0, 5.0, 5.0, 5.0] ``` Constraints: - The size of the tensor should be a positive integer. - The value should be a float. This question assesses the student\'s ability to create tensors, perform tensor operations, and utilize PyTorch\'s testing utilities to ensure correctness.","solution":"import torch def tensor_operations_and_test(size, value): Create two tensors with specific operations and verify their correctness using `torch.testing` functions. Parameters: size (int): The size of the tensors to create. value (float): The value to perform operations with. Returns: None - The function should raise an error if the assertion fails, otherwise it passes silently. # Step 1: Create a tensor of the given size, filled with the given value. tensor_a = torch.full((size,), value) # Step 2: Create another tensor by adding the value to each element of the first tensor. tensor_b = tensor_a + value # Step 3: Verify that each element in tensor_b is twice the value of the elements in tensor_a. expected_tensor = torch.full((size,), value * 2) # Use torch.testing.assert_close to validate tensor_b against the expected_tensor torch.testing.assert_close(tensor_b, expected_tensor)"},{"question":"Objective: To assess the student\'s understanding of Python\'s `resource` module, specifically their ability to manipulate system resource limits and retrieve resource usage statistics. Problem Statement: You are required to implement a set of functions that demonstrate your understanding of the `resource` module. These functions should be able to: 1. Set a limit on the CPU time for the process. 2. Retrieve and print the current limits on core file size. 3. Retrieve and print resource usage information after performing specific tasks. Function Specifications: 1. **Function Name:** `set_cpu_time_limit` - **Input:** An integer `limit_in_seconds`. - **Output:** None. - **Behavior:** Sets the soft limit on the CPU time (`RLIMIT_CPU`) to `limit_in_seconds`. 2. **Function Name:** `print_core_file_limit` - **Input:** None. - **Output:** Prints the soft and hard limits for core file size (`RLIMIT_CORE`). 3. **Function Name:** `print_resource_usage` - **Input:** None. - **Output:** Prints the resource usage for the current process (`RUSAGE_SELF`) after sleeping for 2 seconds and performing a CPU-intensive task (e.g., computing the sum of a large range of numbers). Implementation Constraints: 1. You should handle any potential errors by catching exceptions and printing appropriate error messages. 2. Ensure that the functions work on most Unix-based systems. If the required resources are not available, provide fallback behavior with a suitable error message. Example Usage: ```python # Setting CPU time limit to 5 seconds set_cpu_time_limit(5) # Printing the core file size limits print_core_file_limit() # Printing resource usage after specific tasks print_resource_usage() ``` Ensure your implementation adheres to the given specifications. Your code will be evaluated on correctness, error handling, and code readability.","solution":"import resource import time def set_cpu_time_limit(limit_in_seconds): Sets the soft limit on the CPU time. Parameters: limit_in_seconds (int): The limit in seconds for the CPU time. try: resource.setrlimit(resource.RLIMIT_CPU, (limit_in_seconds, resource.RLIM_INFINITY)) except Exception as e: print(f\\"Error setting CPU time limit: {e}\\") def print_core_file_limit(): Prints the soft and hard limits for core file size. try: soft, hard = resource.getrlimit(resource.RLIMIT_CORE) print(f\\"Core file size limits - Soft limit: {soft}, Hard limit: {hard}\\") except Exception as e: print(f\\"Error retrieving core file size limits: {e}\\") def print_resource_usage(): Prints the resource usage for the current process after performing specific tasks. try: # Perform specific tasks time.sleep(2) sum(range(10**7)) # CPU-intensive task usage = resource.getrusage(resource.RUSAGE_SELF) print(f\\"User time: {usage.ru_utime} seconds\\") print(f\\"System time: {usage.ru_stime} seconds\\") print(f\\"Maximum resident set size: {usage.ru_maxrss} KB\\") print(f\\"Page faults (not requiring I/O): {usage.ru_minflt}\\") print(f\\"Page faults (requiring I/O): {usage.ru_majflt}\\") print(f\\"Voluntary context switches: {usage.ru_nvcsw}\\") print(f\\"Involuntary context switches: {usage.ru_nivcsw}\\") except Exception as e: print(f\\"Error retrieving resource usage: {e}\\")"},{"question":"# Advanced Coding Assessment: Customizing Plots with Seaborn **Objective:** Demonstrate your understanding of Seaborn\'s theming capabilities and customization by creating a specialized plot. **Task:** 1. Load the built-in Seaborn dataset \'tips\'. 2. Create a bar plot showing the total bill amount for each day of the week. 3. Customize the plot appearance using Seaborn themes and custom parameters: - Set the theme to \'darkgrid\'. - Use a deep color palette. - Style adjustments using custom parameters: - Disable the right and top spines. - Set the gridline width to 0.8. - Set the bar edge color to \'black\'. 4. Ensure that these customizations apply to all subsequent plots in the current session. 5. Display the final plot. **Expected Function and Output:** ```python def custom_barplot(): import seaborn as sns import matplotlib.pyplot as plt # Load the dataset tips = sns.load_dataset(\\"tips\\") # Set the theme with custom parameters custom_params = { \\"axes.spines.right\\": False, \\"axes.spines.top\\": False, \\"grid.linewidth\\": 0.8, } sns.set_theme(style=\\"darkgrid\\", palette=\\"deep\\", rc=custom_params) # Create the bar plot sns.barplot(x=\\"day\\", y=\\"total_bill\\", data=tips, edgecolor=\'black\') # Display the plot plt.show() # Calling the function custom_barplot() ``` **Constraints:** - You must use Seaborn and Matplotlib libraries only. - Your solution should focus on the theme and plot customization capabilities of Seaborn. **Performance Requirements:** - The solution should efficiently apply themes and custom parameters without significant delays.","solution":"def custom_barplot(): import seaborn as sns import matplotlib.pyplot as plt # Load the dataset tips = sns.load_dataset(\\"tips\\") # Set the theme with custom parameters custom_params = { \\"axes.spines.right\\": False, \\"axes.spines.top\\": False, \\"grid.linewidth\\": 0.8, } sns.set_theme(style=\\"darkgrid\\", palette=\\"deep\\", rc=custom_params) # Create the bar plot sns.barplot(x=\\"day\\", y=\\"total_bill\\", data=tips, edgecolor=\'black\') # Display the plot plt.show()"},{"question":"Objective: Implement and compare Linear Discriminant Analysis (LDA) and Quadratic Discriminant Analysis (QDA) using scikit-learn on synthetic data. Additionally, explore LDA for dimensionality reduction and the effects of shrinkage on LDA\'s performance. Instructions: 1. **Data Generation**: - Generate a synthetic dataset with 3 classes and 100 samples per class. Use a multivariate Gaussian distribution to create the data with clearly separable means for each class. 2. **Model Implementation**: - Implement LDA and QDA classifiers using scikit-learn. - Fit both LDA and QDA to the generated data. - Predict the classes for the training data using both models. - Compute and print the accuracy of both classifiers. 3. **Dimensionality Reduction**: - Using LDA, transform the data to a 2D space. - Plot the transformed data points in the 2D space, coloring them by their class labels. 4. **Shrinkage in LDA**: - Implement LDA with and without shrinkage (using `shrinkage=\'auto\'`). - Compute and compare the accuracy of both LDA models. 5. **Performance Analysis**: - Analyze and discuss the differences in performance between LDA and QDA. - Analyze the impact of applying shrinkage to LDA. Code Template: ```python import numpy as np import matplotlib.pyplot as plt from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis from sklearn.metrics import accuracy_score # Step 1: Data Generation np.random.seed(42) n_samples = 100 n_features = 2 n_classes = 3 # Mean vectors for each class means = [np.array([2, 2]), np.array([7, 7]), np.array([13, 13])] cov = np.eye(n_features) # Identity covariance matrix X = np.vstack([np.random.multivariate_normal(mean, cov, n_samples) for mean in means]) y = np.hstack([[i] * n_samples for i in range(n_classes)]) # Step 2: Implement LDA and QDA lda = LinearDiscriminantAnalysis() qda = QuadraticDiscriminantAnalysis() lda.fit(X, y) qda.fit(X, y) y_pred_lda = lda.predict(X) y_pred_qda = qda.predict(X) accuracy_lda = accuracy_score(y, y_pred_lda) accuracy_qda = accuracy_score(y, y_pred_qda) print(f\'LDA Accuracy: {accuracy_lda:.2f}\') print(f\'QDA Accuracy: {accuracy_qda:.2f}\') # Step 3: Dimensionality Reduction using LDA X_lda = lda.transform(X, n_components=2) plt.figure(figsize=(8, 6)) for class_value in np.unique(y): plt.scatter(X_lda[y == class_value, 0], X_lda[y == class_value, 1], label=f\'Class {class_value}\') plt.xlabel(\'Component 1\') plt.ylabel(\'Component 2\') plt.legend() plt.title(\'LDA Dimensionality Reduction to 2D\') plt.show() # Step 4: Shrinkage in LDA lda_shrinkage = LinearDiscriminantAnalysis(shrinkage=\'auto\', solver=\'lsqr\') lda_no_shrinkage = LinearDiscriminantAnalysis() lda_shrinkage.fit(X, y) lda_no_shrinkage.fit(X, y) y_pred_shrinkage = lda_shrinkage.predict(X) y_pred_no_shrinkage = lda_no_shrinkage.predict(X) accuracy_shrinkage = accuracy_score(y, y_pred_shrinkage) accuracy_no_shrinkage = accuracy_score(y, y_pred_no_shrinkage) print(f\'LDA with Shrinkage Accuracy: {accuracy_shrinkage:.2f}\') print(f\'LDA without Shrinkage Accuracy: {accuracy_no_shrinkage:.2f}\') # Step 5: Performance Analysis # Write a short analysis discussing the results. ``` Expected Output: - Classification accuracy for LDA and QDA. - 2D scatter plot showing the transformed data points using LDA. - Classification accuracy for LDA with and without shrinkage. - Analysis discussing the performance differences between LDA and QDA and the impact of shrinkage on LDA. Constraints: - Use only scikit-learn, numpy, and matplotlib for this task. - Ensure your code is readable and well-documented. Notes: - Ensure your environment has the necessary packages installed using `pip install scikit-learn matplotlib numpy`. - Your code should be efficient and handle edge cases gracefully.","solution":"import numpy as np import matplotlib.pyplot as plt from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis from sklearn.metrics import accuracy_score # Step 1: Data Generation np.random.seed(42) n_samples = 100 n_features = 2 n_classes = 3 # Mean vectors for each class means = [np.array([2, 2]), np.array([7, 7]), np.array([13, 13])] cov = np.eye(n_features) # Identity covariance matrix X = np.vstack([np.random.multivariate_normal(mean, cov, n_samples) for mean in means]) y = np.hstack([[i] * n_samples for i in range(n_classes)]) # Step 2: Implement LDA and QDA lda = LinearDiscriminantAnalysis() qda = QuadraticDiscriminantAnalysis() lda.fit(X, y) qda.fit(X, y) y_pred_lda = lda.predict(X) y_pred_qda = qda.predict(X) accuracy_lda = accuracy_score(y, y_pred_lda) accuracy_qda = accuracy_score(y, y_pred_qda) print(f\'LDA Accuracy: {accuracy_lda:.2f}\') print(f\'QDA Accuracy: {accuracy_qda:.2f}\') # Step 3: Dimensionality Reduction using LDA X_lda = lda.transform(X) plt.figure(figsize=(8, 6)) for class_value in np.unique(y): plt.scatter(X_lda[y == class_value, 0], X_lda[y == class_value, 1], label=f\'Class {class_value}\') plt.xlabel(\'Component 1\') plt.ylabel(\'Component 2\') plt.legend() plt.title(\'LDA Dimensionality Reduction to 2D\') plt.show() # Step 4: Shrinkage in LDA lda_shrinkage = LinearDiscriminantAnalysis(shrinkage=\'auto\', solver=\'lsqr\') lda_no_shrinkage = LinearDiscriminantAnalysis() lda_shrinkage.fit(X, y) lda_no_shrinkage.fit(X, y) y_pred_shrinkage = lda_shrinkage.predict(X) y_pred_no_shrinkage = lda_no_shrinkage.predict(X) accuracy_shrinkage = accuracy_score(y, y_pred_shrinkage) accuracy_no_shrinkage = accuracy_score(y, y_pred_no_shrinkage) print(f\'LDA with Shrinkage Accuracy: {accuracy_shrinkage:.2f}\') print(f\'LDA without Shrinkage Accuracy: {accuracy_no_shrinkage:.2f}\') # Step 5: Performance Analysis # LDA vs QDA Performance Analysis: QDA is more flexible and can capture non-linear boundaries better # as it models each class separately and allows for different covariance matrices. LDA assumes # that every class shares the same covariance matrix, making it less flexible but potentially more # robust for linear separations. # Shrinkage effect: # Considering shrinkage, it can improve the performance of LDA in scenarios where the covariance # matrices have high variance. This regularization technique helps in reducing overfitting."},{"question":"Background In machine learning, evaluating the similarity or distance between samples is essential for various tasks such as clustering, classification, and recommendation systems. The `sklearn.metrics.pairwise` module provides several functions for computing pairwise distances and similarities, and understanding how to use these functions is crucial. Task You are given a dataset of points in a 2-dimensional space. Implement a custom RBF (Radial Basis Function) kernel and use it to compute the similarity matrix of the dataset. Furthermore, implement the polynomial kernel with a degree of 3. Use both kernels to evaluate the pairwise similarities in the dataset and compare the results. Specifications 1. Write a function `rbf_kernel_custom(X, gamma)` that computes the RBF kernel similarity matrix. - **Input**: - `X`: A `numpy` array of shape `(n_samples, n_features)` representing the dataset. - `gamma`: A float parameter for the RBF kernel. - **Output**: - A `numpy` array of shape `(n_samples, n_samples)` representing the RBF kernel similarity matrix. 2. Write a function `polynomial_kernel_custom(X, degree, coef0)` that computes the polynomial kernel similarity matrix. - **Input**: - `X`: A `numpy` array of shape `(n_samples, n_features)` representing the dataset. - `degree`: An integer representing the degree of the polynomial kernel. - `coef0`: A float representing the independent term in the polynomial kernel. - **Output**: - A `numpy` array of shape `(n_samples, n_samples)` representing the polynomial kernel similarity matrix. 3. Use the provided `X` dataset and compute the similarity matrices using both the custom RBF and polynomial kernels. 4. Compare the similarity matrices from both kernels. Dataset ```python import numpy as np X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]]) gamma = 0.5 degree = 3 coef0 = 1 ``` Example ```python def rbf_kernel_custom(X, gamma): # Your implementation here def polynomial_kernel_custom(X, degree, coef0): # Your implementation here # Using the given dataset and parameters similarity_matrix_rbf = rbf_kernel_custom(X, gamma) similarity_matrix_poly = polynomial_kernel_custom(X, degree, coef0) print(\\"RBF Kernel Similarity Matrix:\\") print(similarity_matrix_rbf) print(\\"Polynomial Kernel Similarity Matrix:\\") print(similarity_matrix_poly) ``` # Constraints 1. Do not use any `pairwise` functions from `scikit-learn` to directly compute the RBF or polynomial kernels. 2. You may use basic `numpy` functions for matrix operations. # Evaluation Criteria - Correctness of the custom kernel implementations. - Correct calculation of the similarity matrices. - Proper comparison and analysis of the results.","solution":"import numpy as np def rbf_kernel_custom(X, gamma): Computes the RBF kernel similarity matrix. Parameters: X (numpy.ndarray): Dataset of shape (n_samples, n_features). gamma (float): Parameter for RBF kernel. Returns: numpy.ndarray: RBF kernel similarity matrix of shape (n_samples, n_samples). sq_dists = np.sum((X[:, np.newaxis, :] - X[np.newaxis, :, :]) ** 2, axis=2) return np.exp(-gamma * sq_dists) def polynomial_kernel_custom(X, degree, coef0): Computes the polynomial kernel similarity matrix. Parameters: X (numpy.ndarray): Dataset of shape (n_samples, n_features). degree (int): Degree of the polynomial kernel. coef0 (float): Independent term in polynomial kernel. Returns: numpy.ndarray: Polynomial kernel similarity matrix of shape (n_samples, n_samples). return (np.dot(X, X.T) + coef0) ** degree # Using the given dataset and parameters X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]]) gamma = 0.5 degree = 3 coef0 = 1 similarity_matrix_rbf = rbf_kernel_custom(X, gamma) similarity_matrix_poly = polynomial_kernel_custom(X, degree, coef0) print(\\"RBF Kernel Similarity Matrix:\\") print(similarity_matrix_rbf) print(\\"Polynomial Kernel Similarity Matrix:\\") print(similarity_matrix_poly)"},{"question":"# Advanced Coding Assessment: WAV File Manipulation and Color Conversion Objective: Your task is to demonstrate your understanding of the `wave` and `colorsys` modules by writing a Python script that reads a WAV audio file, processes its data, and converts certain aspects of it into a visual representation using color transformations. Description: 1. **Read a WAV file**: - Use the `wave` module to read a WAV file provided as input. - Extract audio frame data from the file. 2. **Process Audio Data**: - Compute the average amplitude of audio data in segments. - Use these amplitude values to create a corresponding list of grayscale color values. 3. **Convert to Color**: - Convert these grayscale values to RGB using the `colorsys` module. 4. **Output**: - Save the original WAV file\'s information along with the computed color values into a new file format (you can choose an appropriate format, e.g., JSON or CSV). Requirements: - Implement the function `process_wav_file(input_wav: str, output_file: str) -> None` where: - `input_wav` is the path to the input WAV file. - `output_file` is the path to the output file containing the original information along with the processed color transformations. Example Usage: ```python process_wav_file(\'example.wav\', \'output.json\') ``` Constraints: - You may assume that all WAV files are standard and not corrupted. - You are free to decide on the segment length for averaging the amplitude. - Efficient handling of potentially large audio files is a plus. Hints: - The `wave` module provides methods to read frames and extract parameters from WAV files. - The `colorsys` module allows you to convert between grayscale and RGB color systems. - Consider using lists or dictionaries to store data before writing it to the output file. Expected Output Format: The output file should contain: - Metadata about the WAV file (e.g., sample rate, number of channels). - A list of RGB color values corresponding to segments of the audio data.","solution":"import wave import colorsys import json import struct def process_wav_file(input_wav: str, output_file: str) -> None: # Open the WAV file with wave.open(input_wav, \'rb\') as wav_file: # Extract parameters params = wav_file.getparams() n_channels, sampwidth, framerate, n_frames, comptype, compname = params # Read frames and convert to amplitude values frames = wav_file.readframes(n_frames) amplitude_values = struct.unpack(\'<\' + (\'h\' * (n_frames * n_channels)), frames) # Convert amplitude to grayscale values (0-255) grayscale_values = [int((abs(amp) / (2 ** (sampwidth * 8 - 1))) * 255) for amp in amplitude_values] # Convert grayscale values to RGB rgb_values = [colorsys.hsv_to_rgb(0, 0, gray / 255.0) for gray in grayscale_values] rgb_values = [(int(r * 255), int(g * 255), int(b * 255)) for r, g, b in rgb_values] # Prepare data for output data = { \'params\': { \'n_channels\': n_channels, \'sampwidth\': sampwidth, \'framerate\': framerate, \'n_frames\': n_frames, \'comptype\': comptype, \'compname\': compname }, \'colors\': rgb_values } # Write to output file in JSON format with open(output_file, \'w\') as out_file: json.dump(data, out_file, indent=4)"},{"question":"# Task You are given a dataset containing numerical features, and your goal is to transform these features using kernel approximations and then train a linear classifier on the transformed data. Specifically, you will need to: 1. Use the `RBFSampler` to transform the input features. 2. Train a stochastic gradient descent classifier (`SGDClassifier`) on the transformed features. 3. Evaluate the classifier\'s accuracy on the testing set. # Input - A training dataset (`X_train`, `y_train`): `X_train` is a 2D list of floating-point values representing the features, and `y_train` is a list of integers representing the labels. - A testing dataset (`X_test`, `y_test`): `X_test` is a 2D list of floating-point values representing the features, and `y_test` is a list of integers representing the labels. # Output - Print the accuracy of the classifier on the testing dataset. # Constraints - Use `gamma = 1` for the `RBFSampler`. - Use `n_components = 100` for the `RBFSampler`. - Use `max_iter = 1000` for the `SGDClassifier`. # Example ```python from sklearn.kernel_approximation import RBFSampler from sklearn.linear_model import SGDClassifier from sklearn.datasets import make_classification from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score # Generate dummy data X, y = make_classification(n_samples=100, n_features=20) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25) # Implement your solution here def kernel_approximation_and_classification(X_train, y_train, X_test, y_test): # Initialize RBFSampler rbf_sampler = RBFSampler(gamma=1, n_components=100, random_state=1) # Fit and transform training data X_train_features = rbf_sampler.fit_transform(X_train) # Transform testing data X_test_features = rbf_sampler.transform(X_test) # Initialize and train SGDClassifier clf = SGDClassifier(max_iter=1000, random_state=1) clf.fit(X_train_features, y_train) # Predict and evaluate y_pred = clf.predict(X_test_features) accuracy = accuracy_score(y_test, y_pred) print(\\"Accuracy:\\", accuracy) # Run example solution kernel_approximation_and_classification(X_train, y_train, X_test, y_test) ``` # Notes - The provided example uses `make_classification` to generate dummy data. In a real-world scenario, you would be provided with actual datasets. - Although the accuracy score may vary due to the randomness in data generation, ensure that your implementation is correct and efficiently handles the approximation and classification tasks.","solution":"from sklearn.kernel_approximation import RBFSampler from sklearn.linear_model import SGDClassifier from sklearn.datasets import make_classification from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score def kernel_approximation_and_classification(X_train, y_train, X_test, y_test): Transforms the input features using RBFSampler and trains a SGDClassifier on the transformed data. Calculates and prints the accuracy of the classifier on the testing dataset. Parameters: - X_train (2D list of floats): Training dataset features. - y_train (list of integers): Training dataset labels. - X_test (2D list of floats): Testing dataset features. - y_test (list of integers): Testing dataset labels. # Initialize RBFSampler rbf_sampler = RBFSampler(gamma=1, n_components=100, random_state=1) # Fit and transform training data X_train_features = rbf_sampler.fit_transform(X_train) # Transform testing data X_test_features = rbf_sampler.transform(X_test) # Initialize and train SGDClassifier clf = SGDClassifier(max_iter=1000, random_state=1) clf.fit(X_train_features, y_train) # Predict and evaluate y_pred = clf.predict(X_test_features) accuracy = accuracy_score(y_test, y_pred) return accuracy def example_usage(): # Generate dummy data X, y = make_classification(n_samples=100, n_features=20, random_state=42) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42) # Get the accuracy of the model accuracy = kernel_approximation_and_classification(X_train, y_train, X_test, y_test) print(\\"Accuracy:\\", accuracy) example_usage()"},{"question":"# PyTorch Attention Mechanism Implementation The `torch.nn.attention.experimental` module contains experimental APIs for attention mechanisms. Your task is to create and validate a simple multi-head attention mechanism using these experimental APIs. Requirements: 1. **Function Definition**: Implement a function `multi_head_attention` that initializes and applies a multi-head attention mechanism to given input tensors. 2. **Inputs**: - `query`: A tensor of shape `(batch_size, query_len, d_model)` representing the query embeddings. - `key`: A tensor of shape `(batch_size, key_len, d_model)` representing the key embeddings. - `value`: A tensor of shape `(batch_size, value_len, d_model)` representing the value embeddings. - `num_heads`: An integer denoting the number of attention heads. 3. **Outputs**: - A tensor of shape `(batch_size, query_len, d_model)` representing the output of the multi-head attention mechanism. Instructions: - You should employ the experimental API within the `torch.nn.attention.experimental` module to create the multi-head attention mechanism. - Validate your implementation with a test case where `batch_size=2`, `query_len=3`, `key_len=3`, `value_len=3`, `d_model=8`, and `num_heads=2`. ```python import torch import torch.nn as nn from torch.nn.attention.experimental import MultiheadAttention def multi_head_attention(query, key, value, num_heads): Applies a multi-head attention mechanism using PyTorch\'s experimental attention module. Parameters: query (torch.Tensor): Query tensor of shape (batch_size, query_len, d_model) key (torch.Tensor): Key tensor of shape (batch_size, key_len, d_model) value (torch.Tensor): Value tensor of shape (batch_size, value_len, d_model) num_heads (int): Number of attention heads Returns: torch.Tensor: Output tensor of shape (batch_size, query_len, d_model) d_model = query.size(-1) attention_layer = MultiheadAttention(d_model, num_heads) output, _ = attention_layer(query, key, value) return output # Example test case batch_size = 2 query_len = 3 key_len = 3 value_len = 3 d_model = 8 num_heads = 2 query = torch.rand(batch_size, query_len, d_model) key = torch.rand(batch_size, key_len, d_model) value = torch.rand(batch_size, value_len, d_model) output = multi_head_attention(query, key, value, num_heads) print(\\"Output Shape:\\", output.shape) # Expected: (2, 3, 8) ``` - Ensure that the implementation is clear, well-commented, and follows PyTorch conventions. - Pay attention to the shapes of the input and output tensors and ensure they align with expectations. - Include additional test cases to verify the robustness of your function.","solution":"import torch import torch.nn as nn def multi_head_attention(query, key, value, num_heads): Applies a multi-head attention mechanism using PyTorch\'s built-in multihead attention module. Parameters: query (torch.Tensor): Query tensor of shape (batch_size, query_len, d_model) key (torch.Tensor): Key tensor of shape (batch_size, key_len, d_model) value (torch.Tensor): Value tensor of shape (batch_size, value_len, d_model) num_heads (int): Number of attention heads Returns: torch.Tensor: Output tensor of shape (batch_size, query_len, d_model) d_model = query.size(-1) assert d_model % num_heads == 0, \\"d_model must be divisible by num_heads\\" attention_layer = nn.MultiheadAttention(d_model, num_heads, batch_first=True) output, _ = attention_layer(query, key, value) return output"},{"question":"**Coding Assessment Question** # Objective: The goal of this coding assessment is to evaluate your understanding of the Principal Component Analysis (PCA) as implemented in scikit-learn\'s decomposition module. You are required to write a function that processes a dataset using PCA and returns the transformed dataset along with the explained variance ratio of the components. # Description: Given a dataset, implement the following function: ```python import numpy as np from sklearn.decomposition import PCA def apply_pca(data: np.ndarray, n_components: int) -> (np.ndarray, np.ndarray): Applies PCA to the provided dataset and returns the transformed data along with the explained variance ratio. Parameters: data (np.ndarray): The input dataset, where each row represents a sample and each column represents a feature. n_components (int): The number of principal components to compute. Returns: transformed_data (np.ndarray): The dataset projected onto the principal components. explained_variance_ratio (np.ndarray): The explained variance ratio of the principal components. # Your code here pass ``` # Constraints: 1. The input `data` will be a valid numpy ndarray with a shape of (m, n) where m is the number of samples and n is the number of features. 2. The parameter `n_components` will be a positive integer less than or equal to the number of features in the data. # Requirements: - Implement the `apply_pca` function to compute the principal components of the dataset. - The function should return the transformed data and the explained variance ratio of the principal components. - Use the PCA class from scikit-learn\'s decomposition module. - Ensure that you center the data by subtracting the mean of each feature before applying PCA. - Verify that the function works correctly with appropriate test cases. # Example Usage: ```python import numpy as np # Example data data = np.array([[2.5, 2.4], [0.5, 0.7], [2.2, 2.9], [1.9, 2.2], [3.1, 3.0], [2.3, 2.7], [2, 1.6], [1, 1.1], [1.5, 1.6], [1.1, 0.9]]) # Apply PCA transformed_data, explained_variance_ratio = apply_pca(data, n_components=2) # Print the results print(\\"Transformed Data:n\\", transformed_data) print(\\"Explained Variance Ratio:n\\", explained_variance_ratio) ``` # Notes: - The transformed data should have the shape (m, n_components). - The explained variance ratio should be a 1D array of length `n_components`. # Submission: Submit your implementation of the `apply_pca` function. Ensure that it follows the specified requirements and passes the example usage test case.","solution":"import numpy as np from sklearn.decomposition import PCA def apply_pca(data: np.ndarray, n_components: int) -> (np.ndarray, np.ndarray): Applies PCA to the provided dataset and returns the transformed data along with the explained variance ratio. Parameters: data (np.ndarray): The input dataset, where each row represents a sample and each column represents a feature. n_components (int): The number of principal components to compute. Returns: transformed_data (np.ndarray): The dataset projected onto the principal components. explained_variance_ratio (np.ndarray): The explained variance ratio of the principal components. # Center the data by subtracting the mean of each feature data_centered = data - np.mean(data, axis=0) # Create PCA object pca = PCA(n_components=n_components) # Fit and transform the data using PCA transformed_data = pca.fit_transform(data_centered) # Get the explained variance ratio explained_variance_ratio = pca.explained_variance_ratio_ return transformed_data, explained_variance_ratio"},{"question":"# Asynchronous Web Scraping using `concurrent.futures` **Description:** You need to implement a function `async_scrape(urls)` that accepts a list of URLs and asynchronously scrapes data from each URL. Use the `concurrent.futures.ThreadPoolExecutor` for this task. The function should return a dictionary where the keys are the URLs and values are the respective HTML content retrieved. # Requirements: 1. **Function Signature:** ```python def async_scrape(urls: List[str]) -> Dict[str, str]: ``` 2. **Input:** - `urls`: A list of strings, where each string is a valid URL. ```python urls = [\\"http://example.com\\", \\"http://example.net\\"] ``` 3. **Output:** - A dictionary with URLs as keys and the fetched HTML content as values. ```python { \\"http://example.com\\": \\"<html> ... </html>\\", \\"http://example.net\\": \\"<html> ... </html>\\" } ``` 4. **Constraints:** - The list of URLs will not exceed 100 items. - Each URL\'s content will be less than 2MB. - Ensure efficient network usage and avoid blocking the main thread. 5. **Performance:** - The function should utilize multithreading to perform I/O-bound tasks efficiently. - Ensure minimal latency while fetching data from multiple URLs. # Implementation Notes: - Use the `concurrent.futures.ThreadPoolExecutor` to manage the threading. - Handle exceptions that may occur during web requests to avoid terminating the entire scraping process. - Use `requests` library to simplify the HTTP requests. # Example: ```python from typing import List, Dict import requests from concurrent.futures import ThreadPoolExecutor, as_completed def fetch_url(url: str) -> str: try: response = requests.get(url) response.raise_for_status() # Ensure we notice bad responses return url, response.text except requests.RequestException as e: return url, str(e) def async_scrape(urls: List[str]) -> Dict[str, str]: results = {} with ThreadPoolExecutor(max_workers=10) as executor: future_to_url = {executor.submit(fetch_url, url): url for url in urls} for future in as_completed(future_to_url): url, result = future.result() results[url] = result return results ``` Test Case: ```python urls = [\\"http://example.com\\", \\"http://example.net\\"] print(async_scrape(urls)) # Expected Output: Dictionary with HTML content for each URL. ``` # Notes: - Ensure your code handles HTTP errors gracefully. - Efficiently manages multiple threads for optimal performance.","solution":"from typing import List, Dict import requests from concurrent.futures import ThreadPoolExecutor, as_completed def fetch_url(url: str) -> tuple: Fetches content from a URL. Args: url (str): The URL to fetch content from. Returns: tuple: A tuple containing the URL and the response text or error message. try: response = requests.get(url) response.raise_for_status() # Ensure we notice bad responses return url, response.text except requests.RequestException as e: return url, str(e) def async_scrape(urls: List[str]) -> Dict[str, str]: Asynchronously scrapes data from a list of URLs. Args: urls (List[str]): A list of URLs to scrape. Returns: Dict[str, str]: A dictionary where keys are URLs and values are the HTML content or error messages. results = {} with ThreadPoolExecutor(max_workers=10) as executor: future_to_url = {executor.submit(fetch_url, url): url for url in urls} for future in as_completed(future_to_url): url, result = future.result() results[url] = result return results"},{"question":"Title: Implementing a Custom Container Type with Special Methods Problem Statement: Create a custom container class, `CustomList`, that emulates the behavior of a Python list. Your `CustomList` should support: - Index-based access - Length calculation - Iteration - Slicing To achieve this, you will need to implement several special methods. Requirements: 1. Initialize the class with a given list of elements. 2. Implement special methods to support indexing, length calculation, iteration, and slicing. Input/Output: - Input: Initialize the class with a list of items (e.g., `custom_list = CustomList([1, 2, 3, 4])`). - Output: - Index-based Access: `custom_list[2]` should return `3`. - Length Calculation: `len(custom_list)` should return `4`. - Iteration: `for item in custom_list: print(item)` should print `1 2 3 4`. - Slicing: `custom_list[1:3]` should return `[2, 3]`. Constraints: - The elements of `CustomList` can be of any type. - Slicing should return a `CustomList`. Performance Requirements: - Your implementation should have time complexity similar to that of Python\'s built-in list for the length calculation, indexing, and slicing operations. # Example: ```python custom_list = CustomList([1, 2, 3, 4]) # Index-based Access print(custom_list[2]) # Output: 3 # Length Calculation print(len(custom_list)) # Output: 4 # Iteration for item in custom_list: print(item) # Output: 1 2 3 4 # Slicing print(custom_list[1:3]) # Output: CustomList([2, 3]) ``` Solution Template: ```python class CustomList: def __init__(self, elements): self.elements = elements def __getitem__(self, index): # Your code here def __len__(self): # Your code here def __iter__(self): # Your code here def __repr__(self): return f\\"{CustomList(self.elements)}\\" def __str__(self): return str(self.elements) # You might need to add other special methods to handle slicing # Testing the class custom_list = CustomList([1, 2, 3, 4]) print(custom_list[2]) # Expected Output: 3 print(len(custom_list)) # Expected Output: 4 for item in custom_list: print(item) # Expected Output: 1 2 3 4 print(custom_list[1:3]) # Expected Output: CustomList([2, 3]) ```","solution":"class CustomList: def __init__(self, elements): self.elements = elements def __getitem__(self, index): # Handle if index is a slice if isinstance(index, slice): return CustomList(self.elements[index]) return self.elements[index] def __len__(self): return len(self.elements) def __iter__(self): return iter(self.elements) def __repr__(self): return f\\"CustomList({self.elements})\\" def __str__(self): return str(self.elements)"},{"question":"**Question: Analyzing Restaurant Tips Data Using Seaborn** Given the seaborn `tips` dataset, we would like to perform a visual analysis of the dataset using Seaborn\'s object-oriented interface. Specifically, we aim to investigate how different variables impact the tips given by customers. Write a Python function `analyze_tips_data` that: 1. Loads the `tips` dataset from seaborn. 2. Creates and displays the following plots: - A bar plot showing the total number of tips given each day. - A grouped bar plot showing the total number of tips given each day, separated by sex of the customer. - A bar plot showing the number of occurrences of each party size. - A bar plot showing the number of occurrences of each party size, but with counts assigned to the y-axis. The function should not return any values but should display the plots using appropriate Seaborn methods. # Constraints: - Use `seaborn.objects` interface to create the plots. - Use the `so.Plot` method to initialize plots and the `add` method for adding plot layers. - Use transformation functions such as `so.Count` to aggregate the data. # Input: None (the function should load the tips dataset internally from seaborn). # Output: The function should display the described plots sequentially. # Example Usage: ```python analyze_tips_data() ``` # Hints: - Refer to the Seaborn documentation if needed, especially focusing on the `so.Plot` and the `add` methods. - Consider the mapping of variables (`x`, `y`, `color`) appropriately for grouping and defining axes. # Implementation: ```python import seaborn.objects as so from seaborn import load_dataset def analyze_tips_data(): # Load the dataset tips = load_dataset(\\"tips\\") # Plot 1: Total number of tips given each day so.Plot(tips, x=\\"day\\").add(so.Bar(), so.Count()).show() # Plot 2: Total number of tips given each day, separated by sex so.Plot(tips, x=\\"day\\", color=\\"sex\\").add(so.Bar(), so.Count(), so.Dodge()).show() # Plot 3: Number of occurrences of each party size so.Plot(tips, x=\\"size\\").add(so.Bar(), so.Count()).show() # Plot 4: Number of occurrences of each party size with counts on the y-axis so.Plot(tips, y=\\"size\\").add(so.Bar(), so.Count()).show() # Example usage analyze_tips_data() ```","solution":"import seaborn.objects as so from seaborn import load_dataset def analyze_tips_data(): # Load the dataset tips = load_dataset(\\"tips\\") # Plot 1: Total number of tips given each day so.Plot(tips, x=\\"day\\").add(so.Bar(), so.Count()).show() # Plot 2: Total number of tips given each day, separated by sex so.Plot(tips, x=\\"day\\", color=\\"sex\\").add(so.Bar(), so.Count(), so.Dodge()).show() # Plot 3: Number of occurrences of each party size so.Plot(tips, x=\\"size\\").add(so.Bar(), so.Count()).show() # Plot 4: Number of occurrences of each party size with counts on the y-axis so.Plot(tips, y=\\"size\\").add(so.Bar(), so.Count()).show() # Example usage analyze_tips_data()"},{"question":"# Custom Python Interactive Console **Objective:** Create a custom Python interactive console using the Python `code` module that can handle multiline input and execute user-provided Python code snippets. **Detailed Description:** 1. Implement a class `CustomConsole` that inherits from `code.InteractiveConsole`. 2. Override the `raw_input` method to handle multiline input until a completable code block is received. 3. The interactive console should: - Read lines of input from the user until a blank line is entered. - Accumulate these lines and attempt to execute them as a single block of code. - Print the result of the execution or any error messages. **Function Signature:** ```python class CustomConsole(code.InteractiveConsole): def __init__(self, locals=None): super().__init__(locals) def raw_input(self, prompt=\\">>> \\"): # Implement your custom raw_input method here pass def run_custom_console(commands): Initialize and run the custom interactive console with a list of commands. Args: commands (list of str): A list of commands to execute. Example: run_custom_console([\\"a = 10\\", \\"b = 20\\", \\"c = a + b\\", \\"print(c)\\"]) Outputs: 30 console = CustomConsole() for cmd in commands: console.push(cmd) ``` **Input:** - A list of strings, where each string represents a line of Python code to be executed. **Output:** - The output should be the result of executing the code. If the code is not complete, the console should continue accumulating input until a complete code block is formed and executed correctly. **Constraints:** - The commands should handle typical Python code including conditionals, loops, function definitions, and expressions. - The custom console should handle exceptions gracefully, displaying meaningful error messages. # Example ```python # Example usage commands = [ \\"def add(a, b):\\", \\" return a + b\\", \\"print(add(2, 3))\\" ] run_custom_console(commands) # Expected Output: # 5 ``` --------- 1. You should use the `code.InteractiveConsole` and consider overriding methods to accumulate and execute the code. 2. Consider how to handle incomplete code blocks and prompt the user for more input until the code block is complete. 3. Ensure the class can handle exceptions and prints error messages gracefully. **Note:** This task requires a deep understanding of how to interact with and extend Python\'s built-in modules for creating an interactive programming environment.","solution":"import code class CustomConsole(code.InteractiveConsole): def __init__(self, locals=None): super().__init__(locals) def raw_input(self, prompt=\\">>> \\"): Override the raw_input method to handle multiline input lines = [] while True: try: line = input(prompt) except EOFError: break lines.append(line) if self.runsource(\'n\'.join(lines)): prompt = \\"... \\" else: break return \'n\'.join(lines) def run_custom_console(commands): Initialize and run the custom interactive console with a list of commands. Args: commands (list of str): A list of commands to execute. Example: run_custom_console([\\"a = 10\\", \\"b = 20\\", \\"c = a + b\\", \\"print(c)\\"]) Outputs: 30 console = CustomConsole() for cmd in commands: result = console.push(cmd) if result: # If command is incomplete, keep accumulating console.push(\\"\\")"},{"question":"**Question: Utilizing Dataset Loaders and Fetchers from `sklearn.datasets`** You are required to implement a function `load_and_describe_dataset` that demonstrates your understanding of the `sklearn.datasets` module. This function will: 1. Load a specified dataset using `sklearn.datasets`. 2. Return specific information about the dataset. # Function Signature ```python def load_and_describe_dataset(dataset_name: str) -> dict: Parameters: - dataset_name: str : The name of the dataset to load. Valid values are: - \'iris\' - \'digits\' - \'wine\' - \'breast_cancer\' Returns: - info: dict : A dictionary containing the following information: - \'data_shape\': Tuple[int, int] : The shape of the data matrix (n_samples, n_features). - \'target_shape\': int : Number of target values (should be n_samples). - \'DESCR\': str : The full description of the dataset. pass ``` # Requirements: 1. **Loading the Dataset**: Use the appropriate loader from `sklearn.datasets` (`load_*` functions). 2. **Extract Shape Information**: Determine the shape of the data matrix and the target array. 3. **Dataset Description**: Include the full dataset description in the returned dictionary. # Example Usage: ```python info = load_and_describe_dataset(\'iris\') print(info) ``` Expected Output: ```python { \'data_shape\': (150, 4), \'target_shape\': 150, \'DESCR\': \\"Iris plants datasetn--------------------...\\" } ``` # Constraints: - You can only use dataset names specified in the prompt (`\'iris\'`, `\'digits\'`, `\'wine\'`, `\'breast_cancer\'`). - Ensure that the `DESCR` field in the output dictionary contains the complete description provided by the dataset loader. # Evaluation Criteria: - Correctly loading the specified dataset. - Accurately extracting and reporting dataset shapes. - Properly including the full dataset description.","solution":"from sklearn.datasets import load_iris, load_digits, load_wine, load_breast_cancer def load_and_describe_dataset(dataset_name: str) -> dict: Parameters: - dataset_name: str : The name of the dataset to load. Valid values are: - \'iris\' - \'digits\' - \'wine\' - \'breast_cancer\' Returns: - info: dict : A dictionary containing the following information: - \'data_shape\': Tuple[int, int] : The shape of the data matrix (n_samples, n_features). - \'target_shape\': int : Number of target values (should be n_samples). - \'DESCR\': str : The full description of the dataset. if dataset_name == \'iris\': data = load_iris() elif dataset_name == \'digits\': data = load_digits() elif dataset_name == \'wine\': data = load_wine() elif dataset_name == \'breast_cancer\': data = load_breast_cancer() else: raise ValueError(\\"Invalid dataset name. Choose from \'iris\', \'digits\', \'wine\', \'breast_cancer\'.\\") info = { \'data_shape\': data.data.shape, \'target_shape\': data.target.shape[0], \'DESCR\': data.DESCR } return info"},{"question":"**Objective:** Demonstrate your understanding of seaborn\'s `so.Plot` and `so.Jitter` functionalities by creating a plot that visualizes the `penguins` dataset with specific jitter configurations. **Problem Statement:** 1. Load the `penguins` dataset from seaborn. 2. Create a function `plot_penguin_jitter` that generates two plots: - A dot plot comparing `species` to `body_mass_g` with a jitter width of 0.5. - A dot plot comparing `body_mass_g` (rounded to the nearest 1000) and `flipper_length_mm` (rounded to the nearest 10) with jitter values of `x=150` and `y=10`. **Requirements:** - Implement a function `plot_penguin_jitter()`. - Ensure the two plots are displayed as part of the output. - Use the `seaborn.objects` submodule as demonstrated in the examples. - Document your code for clarity. **Function Signature:** ```python import seaborn.objects as so from seaborn import load_dataset def plot_penguin_jitter(): # Load the penguins dataset penguins = load_dataset(\\"penguins\\") # Plot 1: species vs. body_mass_g with jitter width 0.5 plot_1 = ( so.Plot(penguins, \\"species\\", \\"body_mass_g\\") .add(so.Dots(), so.Jitter(0.5)) ) # Plot 2: body_mass_g (rounded) vs. flipper_length_mm (rounded) with jitter x=150, y=10 plot_2 = ( so.Plot( penguins[\\"body_mass_g\\"].round(-3), penguins[\\"flipper_length_mm\\"].round(-1), ) .add(so.Dots(), so.Jitter(x=150, y=10)) ) # Display the plots plot_1.show() plot_2.show() ``` **Input:** - No input parameters are required for this function. **Output:** - The function should display two seaborn plots. **Constraints:** - Ensure that the seaborn library is installed. - The dataset may contain missing values; handle them appropriately if necessary for plotting. **Notes:** - Use the `.show()` method from seaborn to display the plots. - Utilize the specified jitter transformations accurately. By completing this task, you will demonstrate your ability to use seaborn for data visualization, specifically in creating dot plots with jitter to handle overplotting in dense datasets.","solution":"import seaborn.objects as so from seaborn import load_dataset def plot_penguin_jitter(): Generates two seaborn plots with jitter adjustments for the penguins dataset: 1. A dot plot comparing species to body_mass_g with a jitter width of 0.5. 2. A dot plot comparing rounded body_mass_g and rounded flipper_length_mm with jitter values of x=150 and y=10. # Load the penguins dataset penguins = load_dataset(\\"penguins\\").dropna() # Plot 1: species vs. body_mass_g with jitter width 0.5 plot_1 = ( so.Plot(penguins, x=\\"species\\", y=\\"body_mass_g\\") .add(so.Dots(), so.Jitter(width=0.5)) ) # Plot 2: body_mass_g (rounded) vs. flipper_length_mm (rounded) with jitter x=150, y=10 plot_2 = ( so.Plot(penguins, x=penguins[\\"body_mass_g\\"].round(-3), y=penguins[\\"flipper_length_mm\\"].round(-1)) .add(so.Dots(), so.Jitter(x=150, y=10)) ) # Display the plots plot_1.show() plot_2.show()"},{"question":"# Custom Codec Implementation **Objective:** Implement a custom codec that performs a simple character substitution, encoding vowels (\\"a\\", \\"e\\", \\"i\\", \\"o\\", \\"u\\") into their uppercase forms and decoding them back to their lowercase forms. Demonstrate understanding of `codecs` base classes by creating a fully functional codec. **Task:** 1. Implement a custom `VowelCodec` class that inherits from `codecs.Codec` and performs the specified encoding and decoding. 2. Create appropriate `StreamWriter` and `StreamReader` classes. 3. Register your custom codec using `codecs.register()`. 4. Demonstrate the use of your custom codec with both encoding and decoding operations, using the `encode()` and `decode()` functions. **Specifications:** - **Input and Output Formats:** - **Encoding Input**: A string containing lowercase vowels. - **Encoding Output**: A string where vowels are substituted with their uppercase forms. - **Decoding Input**: A string containing uppercase vowels. - **Decoding Output**: A string where uppercase vowels are substituted back to their lowercase forms. - **Constraints:** - Your codec should handle both empty and non-empty input strings. - Utilize the `codecs` module for implementation. - **Performance Requirements:** - The encoding and decoding operations should have a time complexity of O(n), where n is the length of the input string. **Example Use Case:** ```python import codecs # Define your codec implementation here class VowelCodec(codecs.Codec): def encode(self, input, errors=\'strict\'): output = input.replace(\'a\', \'A\').replace(\'e\', \'E\').replace(\'i\', \'I\').replace(\'o\', \'O\').replace(\'u\', \'U\') return output, len(input) def decode(self, input, errors=\'strict\'): output = input.replace(\'A\', \'a\').replace(\'E\', \'e\').replace(\'I\', \'i\').replace(\'O\', \'o\').replace(\'U\', \'u\') return output, len(input) class VowelStreamWriter(codecs.StreamWriter): def write(self, object): encoded, _ = self.encode(object) self.stream.write(encoded) class VowelStreamReader(codecs.StreamReader): def read(self, size=-1, chars=-1, firstline=False): data = self.stream.read(size) return self.decode(data)[0] def vowel_search_function(encoding): if encoding == \'vowelcodec\': return codecs.CodecInfo( name=\'vowelcodec\', encode=VowelCodec().encode, decode=VowelCodec().decode, incrementalencoder=None, # Incremental encoding not required for simplicity incrementaldecoder=None, # Incremental decoding not required for simplicity streamreader=VowelStreamReader, streamwriter=VowelStreamWriter, ) return None # Register the codec codecs.register(vowel_search_function) # Using the custom codec encoded = codecs.encode(\'hello world\', \'vowelcodec\') print(encoded) # Output: \'hEllO wOrld\' decoded = codecs.decode(encoded, \'vowelcodec\') print(decoded) # Output: \'hello world\' ``` **Complete the implementation details for `VowelCodec`, `VowelStreamWriter`, and `VowelStreamReader` as specified.**","solution":"import codecs class VowelCodec(codecs.Codec): def encode(self, input, errors=\'strict\'): output = input.replace(\'a\', \'A\').replace(\'e\', \'E\').replace(\'i\', \'I\').replace(\'o\', \'O\').replace(\'u\', \'U\') return output, len(input) def decode(self, input, errors=\'strict\'): output = input.replace(\'A\', \'a\').replace(\'E\', \'e\').replace(\'I\', \'i\').replace(\'O\', \'o\').replace(\'U\', \'u\') return output, len(input) class VowelStreamWriter(codecs.StreamWriter): def write(self, object): encoded, _ = self.encode(object) self.stream.write(encoded) class VowelStreamReader(codecs.StreamReader): def read(self, size=-1, chars=-1, firstline=False): data = self.stream.read(size) return self.decode(data)[0] def vowel_search_function(encoding): if encoding == \'vowelcodec\': return codecs.CodecInfo( name=\'vowelcodec\', encode=VowelCodec().encode, decode=VowelCodec().decode, incrementalencoder=None, # Incremental encoding not required for simplicity incrementaldecoder=None, # Incremental decoding not required for simplicity streamreader=VowelStreamReader, streamwriter=VowelStreamWriter, ) return None # Register the codec codecs.register(vowel_search_function)"},{"question":"**Objective:** Design a program to simulate a simple backup and compression system for text files, implementing functionalities to search for files, compress them, and manage basic operations like logging errors and measuring performance. **Task:** You are required to write a Python program that performs the following tasks: 1. **Search for Files:** Use the `glob` module to search for all `.txt` files in the current directory. 2. **File Handling and Compression**: - Use the `os` and `shutil` modules to create a directory named `backup` and copy all the found `.txt` files into this directory. - Compress each file in the `backup` directory using the `zlib` module. 3. **Command Line Arguments**: Parse command line arguments using the `argparse` module. The program should accept the following arguments: - `--operation`: A mandatory argument specifying the operation to perform (`\'backup\'`, `\'compress\'`). - `--logfile`: An optional argument specifying the path to a log file where warnings and errors will be recorded using `sys.stderr`. 4. **Error Handling**: Log any errors encountered during file operations to the specified log file. 5. **Performance Measurement**: Measure the time taken to perform the entire backup and compression operation using the `timeit` module. 6. **Testing**: Write unit tests using the `unittest` module to ensure the following functionalities: - Correct identification of `.txt` files. - Correct creation and population of the `backup` directory. - Correct compression of files. **Constraints:** - Ensure that you handle file and directory errors gracefully. - Your solution should be efficient and make use of Python\'s standard library modules as documented. **Input and Output:** - **Input**: Command line arguments. - **Output**: Display the list of operations performed and the time taken for the entire operation. Log errors and warnings to the specified log file if provided. ```python import os import shutil import glob import zlib import sys import argparse from timeit import default_timer as timer import unittest # Function to search for .txt files def search_txt_files(): return glob.glob(\'*.txt\') # Function to create backup directory and backup files def backup_files(files): if not os.path.exists(\'backup\'): os.makedirs(\'backup\') for file in files: shutil.copy(file, os.path.join(\'backup\', file)) # Function to compress files def compress_files(files): for file in files: with open(os.path.join(\'backup\', file), \'rb\') as f: data = f.read() compressed_data = zlib.compress(data) with open(os.path.join(\'backup\', file + \'.zlib\'), \'wb\') as f: f.write(compressed_data) # Function to log errors def log_error(message, logfile=None): if logfile: with open(logfile, \'a\') as f: f.write(message + \'n\') else: sys.stderr.write(message + \'n\') # Main function to parse arguments and perform operations def main(): parser = argparse.ArgumentParser(description=\'Backup and compress text files.\') parser.add_argument(\'--operation\', required=True, choices=[\'backup\', \'compress\'], help=\'Operation to perform\') parser.add_argument(\'--logfile\', help=\'Path to log file for warnings and errors\') args = parser.parse_args() start_time = timer() try: files = search_txt_files() if args.operation == \'backup\': backup_files(files) elif args.operation == \'compress\': compress_files(files) except Exception as e: log_error(f\'Error: {e}\', args.logfile) end_time = timer() print(f\'Operation {args.operation} completed in {end_time - start_time} seconds\') # Unit tests class TestBackupSystem(unittest.TestCase): def test_search_txt_files(self): files = search_txt_files() self.assertTrue(all(file.endswith(\'.txt\') for file in files)) def test_backup_files(self): files = [\'test1.txt\', \'test2.txt\'] backup_files(files) self.assertTrue(all(os.path.exists(os.path.join(\'backup\', file)) for file in files)) def test_compress_files(self): files = [\'test1.txt\', \'test2.txt\'] compress_files(files) self.assertTrue(all(os.path.exists(os.path.join(\'backup\', file + \'.zlib\')) for file in files)) if __name__ == \'__main__\': unittest.main(exit=False) main() ``` # Instructions: 1. Implement the provided functions based on the descriptions. 2. Ensure that all functions behave as expected and handle potential errors gracefully. 3. Write unit tests to validate the implemented functions. 4. Test the complete application by running it with different command line arguments.","solution":"import os import shutil import glob import zlib import sys import argparse from timeit import default_timer as timer def search_txt_files(): Returns a list of all .txt files in the current directory. return glob.glob(\'*.txt\') def backup_files(files): Creates a backup directory and copies all specified files into it. if not os.path.exists(\'backup\'): os.makedirs(\'backup\') for file in files: shutil.copy(file, os.path.join(\'backup\', file)) def compress_files(files): Compresses specified files in the backup directory using zlib. for file in files: backup_file_path = os.path.join(\'backup\', file) with open(backup_file_path, \'rb\') as f: data = f.read() compressed_data = zlib.compress(data) with open(backup_file_path + \'.zlib\', \'wb\') as f: f.write(compressed_data) def log_error(message, logfile=None): Logs error messages to the specified log file or stderr. if logfile: with open(logfile, \'a\') as f: f.write(message + \'n\') else: sys.stderr.write(message + \'n\') def main(): parser = argparse.ArgumentParser(description=\'Backup and compress text files.\') parser.add_argument(\'--operation\', required=True, choices=[\'backup\', \'compress\'], help=\'Operation to perform\') parser.add_argument(\'--logfile\', help=\'Path to log file for warnings and errors\') args = parser.parse_args() start_time = timer() try: files = search_txt_files() if args.operation == \'backup\': backup_files(files) elif args.operation == \'compress\': compress_files([file for file in os.listdir(\'backup\') if file.endswith(\'.txt\')]) except Exception as e: log_error(f\'Error: {e}\', args.logfile) end_time = timer() print(f\'Operation {args.operation} completed in {end_time - start_time} seconds\')"},{"question":"# PyTorch Compilation and Graph Manipulation In this exercise, you are tasked with demonstrating your understanding of PyTorch\'s `torch.compiler` API. You will compile a simple neural network model and perform some graph manipulations as described. Instructions 1. **Define a Simple Neural Network:** - Create a basic PyTorch neural network class `SimpleNN` with the following structure: - Input layer with 3 neurons - One hidden layer with 5 neurons and ReLU activation - Output layer with 2 neurons 2. **Compile the Model:** - Write a function `compile_model` that takes an instance of `SimpleNN` and returns the compiled model using `torch.compiler.compile`. 3. **Graph Manipulation:** - Write a function `apply_graph_optimizations` that takes the compiled model and performs the following operations: - Assume the result of the output layer remains constant using `assume_constant_result`. - Substitute a new operation in the graph by adding an additional layer with 2 neurons (ignore activation function for simplicity). 4. **Check Status:** - Ensure that the final model is indeed compiled using `is_compiling` or `is_dynamo_compiling`. Function Signatures ```python import torch import torch.nn as nn import torch.compiler as tc class SimpleNN(nn.Module): def __init__(self): super(SimpleNN, self).__init__() self.input_layer = nn.Linear(3, 5) self.hidden_layer = nn.ReLU() self.output_layer = nn.Linear(5, 2) def forward(self, x): x = self.input_layer(x) x = self.hidden_layer(x) x = self.output_layer(x) return x def compile_model(model: SimpleNN) -> torch.jit.ScriptModule: # Your code to compile the model pass def apply_graph_optimizations(compiled_model: torch.jit.ScriptModule) -> torch.jit.ScriptModule: # Your code to manipulate the graph and add an additional layer pass def check_compilation_status(compiled_model: torch.jit.ScriptModule) -> bool: # Your code to check the compilation status pass ``` Constraints - You may not modify the structure of the `SimpleNN` class except for adding necessary imports or attributes. - The additional layer added in `apply_graph_optimizations` should simply transform the output from 2 to 2 neurons. Example ```python # Example usage model = SimpleNN() compiled_model = compile_model(model) optimized_model = apply_graph_optimizations(compiled_model) assert check_compilation_status(optimized_model) ``` Note - You may assume the availability of sample input tensors to test your network. - Proper error handling and validation of the operations are required. Good luck!","solution":"import torch import torch.nn as nn import torch.compiler as tc class SimpleNN(nn.Module): def __init__(self): super(SimpleNN, self).__init__() self.input_layer = nn.Linear(3, 5) self.hidden_layer = nn.ReLU() self.output_layer = nn.Linear(5, 2) def forward(self, x): x = self.input_layer(x) x = self.hidden_layer(x) x = self.output_layer(x) return x def compile_model(model: SimpleNN) -> torch.jit.ScriptModule: # Compile the PyTorch model using torch.jit.script scripted_model = torch.jit.script(model) return scripted_model def apply_graph_optimizations(compiled_model: torch.jit.ScriptModule) -> torch.jit.ScriptModule: # Assume the result of the output layer is constant # (Note: Assume constant result is an ill-defined operation in TorchScript for simplicity) class OptimizedNN(nn.Module): def __init__(self, original_model): super(OptimizedNN, self).__init__() self.model = original_model self.additional_layer = nn.Linear(2, 2) def forward(self, x): x = self.model(x) x = self.additional_layer(x) return x optimized_model = torch.jit.script(OptimizedNN(compiled_model)) return optimized_model def check_compilation_status(compiled_model: torch.jit.ScriptModule) -> bool: # Check if the model has been compiled return hasattr(compiled_model, \'_c\')"},{"question":"# Advanced Python Coding Assessment Objective The objective of this assessment is to test your understanding and ability to effectively use the `importlib` module to dynamically import modules and manage Python\'s import system. Task You are to implement a custom module importer that lazily imports a module when it is accessed and provides functionality to reload all modules imported by it. Requirements 1. **Implement a Lazy Module Importer Class**: - Class Name: `LazyModuleImporter` - Methods to implement: - `__init__(self, module_names: List[str])`: Initializes the importer with a list of module names to be lazily imported. - `import_modules(self) -> None`: Sets up lazy import for all specified modules. - `reload_all(self) -> None`: Reloads all modules that were imported by the importer. 2. **Usage**: - The class should use `importlib.util.LazyLoader` to lazily load modules. - Provide methods to dynamically import and reload these modules. - Ensure all modules are reloaded when `reload_all` is called. Input - A list of module names to be lazily imported. Expected Output - Modules should be imported lazily when accessed. - Modules should be reloaded when the `reload_all` method is called. Example ```python from typing import List class LazyModuleImporter: def __init__(self, module_names: List[str]): # Initialize the importer with the module names pass def import_modules(self) -> None: # Setup lazy import for the specified modules pass def reload_all(self) -> None: # Reload all lazily imported modules pass # Example of how the class is used if __name__ == \\"__main__\\": modules = [\'os\', \'sys\', \'random\'] importer = LazyModuleImporter(modules) importer.import_modules() import os print(os.name) # Accessing os module should trigger import importer.reload_all() # Reload all imported modules ``` Constraints - Modules must only be imported when they are accessed. - The solution should handle cases where the module names are invalid or the imports fail gracefully. - Ensure all necessary edge cases are considered and handled appropriately. Performance The implementation should handle lazy loading and reloading efficiently without unnecessary overhead. Miscellaneous - Avoid using third-party libraries; only use Python\'s standard library. - Document your code for readability and maintainability.","solution":"import importlib import importlib.util from typing import List class LazyModuleImporter: def __init__(self, module_names: List[str]): self.module_names = module_names self.imported_modules = {} def import_modules(self) -> None: for module_name in self.module_names: module = importlib.import_module(module_name) loader = importlib.util.find_spec(module_name).loader if loader is not None: lazy_module = importlib.util.LazyLoader(loader).load_module(module_name) globals()[module_name] = lazy_module self.imported_modules[module_name] = lazy_module def reload_all(self) -> None: for module_name in self.imported_modules: importlib.reload(self.imported_modules[module_name])"},{"question":"# Python Buffer Management In Python 3, direct manipulation of memory buffers is an advanced concept that permits the handling of byte-oriented data without unnecessary copying. It is essential for performance-focused applications like image processing, scientific computing, and networking. Given this, your task is to implement a function that mimics the deprecated functionality of `PyObject_AsCharBuffer` using the modern Python buffer interface. Specifically, you will create a function that: 1. Takes an object supporting the buffer protocol as input. 2. Returns a read-only `memoryview` of that object. **Function Signature:** ```python def as_char_buffer(obj: object) -> memoryview: pass ``` # Constraints: - The input object must support the buffer protocol, otherwise, an appropriate `TypeError` should be raised. - The returned `memoryview` must provide a read-only view of the input object\'s data. # Examples: Example 1: ```python data = b\\"example\\" buffer_view = as_char_buffer(data) print(buffer_view) # Output: <memory at 0x...> ``` Example 2: ```python try: invalid_obj = \\"invalid string\\" buffer_view = as_char_buffer(invalid_obj) # Expecting a TypeError to be raised except TypeError as e: print(e) # Output: Object does not support the buffer interface ``` # Notes: - To check whether an object supports the buffer protocol, you can use `memoryview(obj)`. If it raises a `TypeError`, the object does not support the buffer interface. - Ensure your function handles these cases gracefully by providing a clear error message. Implement the `as_char_buffer` function and ensure it meets the above specifications.","solution":"def as_char_buffer(obj: object) -> memoryview: Takes an object supporting the buffer protocol as input and returns a read-only memoryview of that object. :param obj: An object that supports the buffer protocol :return: A read-only memoryview of the input object\'s data :raises TypeError: If the object does not support the buffer protocol try: # Create a memoryview of the object mv = memoryview(obj) # Return a read-only view of the memoryview return mv.toreadonly() except TypeError: raise TypeError(\\"Object does not support the buffer interface\\")"},{"question":"You are tasked with implementing and manipulating Python function objects using the native Python features. The goal is to dynamically create function objects, modify their attributes, and retrieve information about them. Problem Statement 1. **Create and Modify Function Objects** Write a function, `create_and_modify_function`, that: - Takes three parameters: `globals_dict` (dictionary), `defaults` (tuple), and `annotations` (dictionary). - Uses these parameters to dynamically create and return a new function object using Python\'s native functionalities. The created function should: - Have a simple code object that adds two numbers (`a` and `b`). - Use `globals_dict` as its global variable scope for execution. - Set the default values for its parameters using `defaults`. - Set the type annotations for its parameters using `annotations`. 2. **Retrieve Function Attributes** Write another function, `retrieve_function_attributes`, that: - Takes a function object as a parameter. - Returns a tuple containing the function’s code object, globals dictionary, default values, and annotations. Example ```python # Example usage: globals_dict = {\'__name__\': \'__main__\'} defaults = (3,) annotations = {\'a\': int, \'b\': int, \'return\': int} # Create a new function with the specified global variables, defaults, and annotations new_func = create_and_modify_function(globals_dict, defaults, annotations) # Now, retrieve attributes from the newly created function code, globals_, defaults_, annotations_ = retrieve_function_attributes(new_func) assert code.co_name == \'add\' # function name should be \'add\' assert globals_ == globals_dict assert defaults_ == defaults assert annotations_ == annotations ``` Constraints - You must use `types.FunctionType` for creating new function objects. - The function `create_and_modify_function` should be able to handle any valid dictionary for globals, tuple for defaults, and dictionary for annotations. - The function `retrieve_function_attributes` should return the correct attributes for any valid Python function object passed to it. Note - Your solution should not use any external libraries outside of Python’s standard library. - Focus on demonstrating a deep understanding of Python function objects and their manipulation. Good luck!","solution":"import types def create_and_modify_function(globals_dict, defaults, annotations): Dynamically creates and returns a new function that adds two numbers. Parameters: globals_dict (dict): Dictionary to use as the global variable scope for execution. defaults (tuple): Default values for the function parameters. annotations (dict): Type annotations for the function parameters. Returns: function: The dynamically created function object. func_name = \'add\' func_code = def {}(a, b): return a + b .format(func_name) exec(func_code, globals_dict) new_func = globals_dict[func_name] new_func.__defaults__ = defaults new_func.__annotations__ = annotations return new_func def retrieve_function_attributes(func): Retrieves and returns a tuple containing the function\'s code object, globals dictionary, default values, and annotations. Parameters: func (function): The function object. Returns: tuple: A tuple containing the function\'s code object, globals dictionary, default values, and annotations. return (func.__code__, func.__globals__, func.__defaults__, func.__annotations__)"},{"question":"# PyTorch Graph Mode Quantization Backend Configuration Task Problem Statement You are tasked with designing a function that allows users to load, examine, and manipulate the quantization backend configuration for FX Graph Mode in PyTorch. For this task, you will be provided with a text file containing the default backend configuration (`default_backend_config.txt`). You need to implement the following functionality in your function: 1. **Load Configuration**: Read the quantization backend configuration from a given file. 2. **Display Configuration**: Print the loaded configuration in a readable format. 3. **Modify Configuration**: Allow users to change specific quantization parameters for a given backend. 4. **Save Configuration**: Save the modified configuration back to a file. Function Specification 1. **Input**: - The path to the configuration file (a string). - The backend name for which the configuration should be modified (a string). - A dictionary containing the parameters to modify along with their new values. 2. **Output**: - None. The function should save the modified configuration back to a file. 3. **Constraints**: - You should ensure that the input dictionary only contains valid keys present in the original configuration for the specified backend. - If the backend name or dictionary keys are invalid, the function should raise an appropriate error. - The configuration should be saved in a format similar to the original configuration file. Function Signature ```python def modify_quantization_backend_config(config_file_path: str, backend_name: str, modifications: dict) -> None: Modifies the quantization backend configuration for a specific backend. Parameters: config_file_path (str): The path to the configuration file. backend_name (str): The backend name whose configuration is to be modified. modifications (dict): A dictionary containing the parameter names and their new values. Returns: None: The function saves the modified configuration back to the file. pass ``` Example Usage Assume the configuration file `default_backend_config.txt` contains: ```plaintext x86: input_scale: 1.0 weight_scale: 0.5 activation_postprocess: ReLU qnnpack: input_scale: 2.0 weight_scale: 1.0 activation_postprocess: Sigmoid ``` ```python # Example usage of the function config_file = \'default_backend_config.txt\' backend = \'x86\' modifications = { \'input_scale\': 1.5, \'activation_postprocess\': \'Tanh\' } modify_quantization_backend_config(config_file, backend, modifications) ``` After execution, the configuration file `default_backend_config.txt` should be updated to: ```plaintext x86: input_scale: 1.5 weight_scale: 0.5 activation_postprocess: Tanh qnnpack: input_scale: 2.0 weight_scale: 1.0 activation_postprocess: Sigmoid ``` Use this question to assess students\' understanding of file operations, dictionary manipulations, and string parsing in Python, in conjunction with their ability to work with PyTorch quantization configurations.","solution":"def modify_quantization_backend_config(config_file_path: str, backend_name: str, modifications: dict) -> None: Modifies the quantization backend configuration for a specific backend. Parameters: config_file_path (str): The path to the configuration file. backend_name (str): The backend name whose configuration is to be modified. modifications (dict): A dictionary containing the parameter names and their new values. Returns: None: The function saves the modified configuration back to the file. # Read the configuration file with open(config_file_path, \'r\') as file: lines = file.readlines() # Parse the configuration and construct a configuration dictionary config = {} current_backend = None for line in lines: line = line.strip() if line.endswith(\':\') and not line.startswith(\' \'): current_backend = line[:-1] config[current_backend] = {} elif current_backend is not None and \':\' in line: key, value = line.split(\':\', 1) config[current_backend][key.strip()] = value.strip() # Check if backend_name is valid if backend_name not in config: raise ValueError(f\\"Backend \'{backend_name}\' not found in the configuration\\") # Validate and apply modifications for key in modifications: if key not in config[backend_name]: raise ValueError(f\\"Key \'{key}\' is not a valid configuration parameter for backend \'{backend_name}\'\\") config[backend_name][key] = str(modifications[key]) # Save the modified configuration back to the file with open(config_file_path, \'w\') as file: for backend, params in config.items(): file.write(f\\"{backend}:n\\") for key, value in params.items(): file.write(f\\" {key}: {value}n\\")"},{"question":"# Asyncio Synchronization Primitives Coding Challenge Problem Statement You are required to implement a logging system that manages access to a shared resource (a log file). Multiple concurrent logger tasks should be able to write messages to this log file in an orderly fashion without corrupting the log data. Each logger task must wait for permission to access the log file to ensure that log entries from different tasks do not intermix. You should use the following asyncio primitives: 1. **Lock** for ensuring mutual exclusion while writing to the log file. 2. **Event** to notify a monitor task that the log file has been updated. 3. **Condition** to manage the modification and checking of a shared status flag. Requirements 1. Implement asynchronous methods for the following functionalities: - Writing to the log file using a Lock to ensure mutual exclusion. - A logger task that writes a given message to the log file. - A monitor task that waits for the Event to be set, signifying that the log file has been updated. - A method to stop the logger task based on Condition. 2. Create a `Logger` class that encapsulates these functionalities. The class should: - Have an asynchronous method `write_log(message: str)` that uses a Lock to write the message to the log file. - Have an asynchronous method `monitor_log()` that waits for the Event indicating log file updates and then processes the update. - Have an asynchronous method `stop_logger()` that uses a Condition to trigger the stopping of logger tasks. 3. Demonstrate the usage of your `Logger` class with multiple logger tasks writing messages to a shared log file and a monitor task processing these updates. Constraints - Use asyncio primitives and functions as provided by the `asyncio` module. - Ensure that the log file writes do not overlap by accurately using `Lock`. - Manage the updating and waiting mechanics using `Event` and `Condition`. Example Usage ```python import asyncio from asyncio import Lock, Event, Condition class Logger: def __init__(self): self.lock = Lock() self.event = Event() self.condition = Condition(lock=self.lock) self.log_file = \\"async_log.txt\\" self.stop_logging = False async def write_log(self, message: str): async with self.lock: with open(self.log_file, \'a\') as file: file.write(message + \\"n\\") self.event.set() await asyncio.sleep(0) # Simulate some delay async def logger_task(self, message: str): while not self.stop_logging: await self.write_log(message) await asyncio.sleep(1) async def monitor_log(self): while not self.stop_logging: await self.event.wait() self.event.clear() print(\\"Log file updated.\\") print(\\"Stopping monitor.\\") async def stop_logger(self): async with self.condition: self.stop_logging = True self.condition.notify_all() async def main(): logger = Logger() log_task1 = asyncio.create_task(logger.logger_task(\\"Logger 1 message\\")) log_task2 = asyncio.create_task(logger.logger_task(\\"Logger 2 message\\")) monitor_task = asyncio.create_task(logger.monitor_log()) await asyncio.sleep(5) await logger.stop_logger() await asyncio.sleep(1) # Give some time to stop await log_task1 await log_task2 await monitor_task asyncio.run(main()) ``` In this example, multiple logger tasks write to a shared log file, monitored by a single monitor task, while managing synchronization using asyncio primitives.","solution":"import asyncio from asyncio import Lock, Event, Condition class Logger: def __init__(self): self.lock = Lock() self.event = Event() self.condition = Condition(lock=self.lock) self.log_file = \\"async_log.txt\\" self.stop_logging = False async def write_log(self, message: str): async with self.lock: with open(self.log_file, \'a\') as file: file.write(message + \\"n\\") self.event.set() await asyncio.sleep(0) # Simulate some delay async def logger_task(self, message: str): while not self.stop_logging: await self.write_log(message) await asyncio.sleep(1) # Simulate periodic logging async def monitor_log(self): while not self.stop_logging: await self.event.wait() self.event.clear() # Here, in an actual application, some more processing can be done. print(\\"Log file updated.\\") print(\\"Stopping monitor.\\") async def stop_logger(self): async with self.condition: self.stop_logging = True self.condition.notify_all() async def main(): logger = Logger() log_task1 = asyncio.create_task(logger.logger_task(\\"Logger 1 message\\")) log_task2 = asyncio.create_task(logger.logger_task(\\"Logger 2 message\\")) monitor_task = asyncio.create_task(logger.monitor_log()) await asyncio.sleep(5) await logger.stop_logger() await asyncio.sleep(1) # Give some time to stop await log_task1 await log_task2 await monitor_task if __name__ == \\"__main__\\": asyncio.run(main())"},{"question":"# Coding Assessment: Advanced Visualization with seaborn Objective: Your task is to demonstrate your understanding of seaborn, pandas, and numpy by performing data manipulations and creating multiple types of visualizations based on given datasets. You will need to handle both long-form and wide-form data, and utilize seaborn for accurate and informative plot generation. Instructions: 1. **Data Loading and Inspection** - Load the \\"flights\\" dataset using seaborn. - Display the first few rows of the dataset. 2. **Long-Form Data Visualization** - Use seaborn to create a line plot showing the number of passengers each year for each month. Each month should be represented by a different line. 3. **Data Transformation** - Convert the \\"flights\\" dataset into a wide-form format where each column represents a month\'s time series over the years. - Display the first few rows of the wide-form dataset. 4. **Wide-Form Data Visualization** - Use the wide-form dataset to create a line plot showing the number of passengers each year for each month. Ensure that each line represents a different month. 5. **Advanced Visualization** - Transform the \\"anagrams\\" dataset into a long-form format. The dataset should include columns for \\"subidr\\" (subject id), \\"attnr\\" (attention), \\"solutions\\" (number of solutions available), and \\"score\\" (memory performance score). - Use seaborn to create a point plot showing the average score as a function of the number of solutions and attention. 6. **Data Aggregation and Plot** - Group the \\"flights\\" dataset by year and calculate the average number of passengers per year. - Use seaborn to plot this aggregated data to show the trend of passengers over the years. Constraints and Specifications: - Utilize pandas and seaborn for data manipulation and visualization. - Ensure all plots are clearly labeled. - Handle any missing data appropriately. - Your solution should be efficient and well-documented. Input Format: - There is no explicit input format provided. Use the datasets available in seaborn. Output Format: - Display the generated plots inline. - Ensure that the transformations and intermediate results of the steps are printed out for verification purposes. Below is an example template for your solution: ```python import seaborn as sns import pandas as pd # Step 1: Load the flights dataset flights = sns.load_dataset(\\"flights\\") # Display the first few rows print(flights.head()) # Step 2: Create a long-form line plot sns.relplot(data=flights, x=\\"year\\", y=\\"passengers\\", hue=\\"month\\", kind=\\"line\\") # Step 3: Convert to wide-form and display flights_wide = flights.pivot(index=\\"year\\", columns=\\"month\\", values=\\"passengers\\") print(flights_wide.head()) # Step 4: Create a wide-form line plot sns.relplot(data=flights_wide, kind=\\"line\\") # Step 5: Transform the anagrams dataset to long-form and create a point plot anagrams = sns.load_dataset(\\"anagrams\\") anagrams_long = anagrams.melt(id_vars=[\\"subidr\\", \\"attnr\\"], var_name=\\"solutions\\", value_name=\\"score\\") print(anagrams_long.head()) sns.catplot(data=anagrams_long, x=\\"solutions\\", y=\\"score\\", hue=\\"attnr\\", kind=\\"point\\") # Step 6: Group by year and plot the average number of passengers per year flights_avg = flights.groupby(\\"year\\").mean(numeric_only=True) print(flights_avg.head()) sns.relplot(data=flights_avg, x=\\"year\\", y=\\"passengers\\", kind=\\"line\\") ``` Make sure to execute your code in a Jupyter notebook or a similar environment to visualize the plots properly.","solution":"import seaborn as sns import pandas as pd import numpy as np import matplotlib.pyplot as plt # Step 1: Load the flights dataset flights = sns.load_dataset(\\"flights\\") # Display the first few rows print(flights.head()) # Step 2: Create a long-form line plot plt.figure(figsize=(12, 6)) sns.lineplot(data=flights, x=\\"year\\", y=\\"passengers\\", hue=\\"month\\") plt.title(\'Number of Passengers Each Year for Each Month\') plt.show() # Step 3: Convert to wide-form and display flights_wide = flights.pivot(index=\\"year\\", columns=\\"month\\", values=\\"passengers\\") print(flights_wide.head()) # Step 4: Create a wide-form line plot plt.figure(figsize=(12, 6)) sns.lineplot(data=flights_wide) plt.title(\'Number of Passengers Each Year for Each Month (Wide Form)\') plt.show() # Step 5: Transform the anagrams dataset to long-form and create a point plot anagrams = sns.load_dataset(\\"anagrams\\") anagrams_long = anagrams.melt(id_vars=[\\"subidr\\", \\"attnr\\"], var_name=\\"solutions\\", value_name=\\"score\\") print(anagrams_long.head()) plt.figure(figsize=(12, 6)) sns.pointplot(data=anagrams_long, x=\\"solutions\\", y=\\"score\\", hue=\\"attnr\\", dodge=True) plt.title(\'Average Score as a Function of Number of Solutions and Attention\') plt.show() # Step 6: Group by year and plot the average number of passengers per year flights_avg = flights.groupby(\\"year\\").mean(numeric_only=True).reset_index() print(flights_avg.head()) plt.figure(figsize=(12, 6)) sns.lineplot(data=flights_avg, x=\\"year\\", y=\\"passengers\\") plt.title(\'Average Number of Passengers Per Year\') plt.show()"},{"question":"# Seaborn Coding Assessment: Custom Colormap Generation **Objective:** You are tasked with creating a custom colormap generator using the `seaborn` library\'s `cubehelix_palette` function. This will demonstrate your understanding of seaborn\'s colormap customization capabilities. **Problem Statement:** Write a Python function `generate_custom_colormap` that returns a seaborn colormap based on input parameters defining its properties. The function should take the following parameters: 1. `mode` (str): The type of colormap to generate. It can be `\\"discrete\\"` for a discrete palette or `\\"continuous\\"` for a continuous colormap. 2. `n_colors` (int, optional): The number of colors to include in the palette. Default is `6`. 3. `start` (float, optional): The start value of the helix. Default is `0.0`. 4. `rot` (float, optional): The amount of rotation for the helix. Default is `0.0`. 5. `gamma` (float, optional): The gamma correction factor. Default is `1.0`. 6. `hue` (float, optional): The saturation of the colors. Default is `0.0`. 7. `dark` (float, optional): The luminance of the darkest color. Default is `0.0`. 8. `light` (float, optional): The luminance of the lightest color. Default is `1.0`. 9. `reverse` (bool, optional): Whether to reverse the direction of the luminance ramp. Default is `False`. **Constraints:** - The `mode` parameter must be either `\\"discrete\\"` or `\\"continuous\\"`. - All other parameters must be within their respective valid ranges for the `cubehelix_palette` function. - The function must validate input parameters and raise appropriate exceptions for invalid values. **Function Signature:** ```python import seaborn as sns def generate_custom_colormap(mode=\'discrete\', n_colors=6, start=0.0, rot=0.0, gamma=1.0, hue=0.0, dark=0.0, light=1.0, reverse=False): pass ``` **Example Usage:** ```python # Example 1: Generate a discrete colormap with default parameters palette1 = generate_custom_colormap() print(palette1) # Example 2: Generate a continuous colormap with a specific rotation and hue palette2 = generate_custom_colormap(mode=\'continuous\', rot=1, hue=0.8) print(palette2) # Example 3: Generate a custom discrete colormap with reversed luminance ramp palette3 = generate_custom_colormap(n_colors=8, reverse=True) print(palette3) ``` Your task is to implement the `generate_custom_colormap` function to meet the specifications and constraints outlined above. **Assessment Criteria:** - Correct initialization and usage of seaborn\'s `cubehelix_palette` function. - Accurate handling of different parameter values and types. - Proper parameter validation and error handling. - Efficiency and clarity of the implemented solution.","solution":"import seaborn as sns def generate_custom_colormap(mode=\'discrete\', n_colors=6, start=0.0, rot=0.0, gamma=1.0, hue=0.0, dark=0.0, light=1.0, reverse=False): Generate a custom colormap using seaborn\'s cubehelix_palette function. Parameters: - mode (str): \'discrete\' for a discrete palette, \'continuous\' for a continuous colormap. - n_colors (int, optional): Number of colors in the palette. Default is 6. - start (float, optional): Start value of the helix. Default is 0.0. - rot (float, optional): Amount of rotation for the helix. Default is 0.0. - gamma (float, optional): Gamma correction factor. Default is 1.0. - hue (float, optional): Saturation of the colors. Default is 0.0. - dark (float, optional): Luminance of the darkest color. Default is 0.0. - light (float, optional): Luminance of the lightest color. Default is 1.0. - reverse (bool, optional): Reverse the direction of luminance ramp. Default is False. Returns: A seaborn color palette or colormap. # Validate mode if mode not in [\'discrete\', \'continuous\']: raise ValueError(\\"mode must be \'discrete\' or \'continuous\'\\") # Generate the palette palette = sns.cubehelix_palette(n_colors=n_colors, start=start, rot=rot, gamma=gamma, hue=hue, dark=dark, light=light, reverse=reverse, as_cmap=(mode == \'continuous\')) return palette"},{"question":"**Problem Statement: Implementing and Managing a Library System** You are required to implement a simplified library management system in Python using Object-Oriented Programming concepts. The system should have the following features: 1. **Base Class: `LibraryItem`** This will be the base class for all items that can be borrowed from the library. - Attributes: - `title`: The title of the item (string). - `author`: The author of the item (string). - `item_id`: A unique identifier for the item (string). - `availability`: A boolean indicating if the item is available for borrowing. - Methods: - `__init__(self, title, author, item_id)`: Initializes the item with title, author, and item_id, and sets availability to True. - `__str__(self)`: Returns a string representing the item details. - `borrow(self)`: Sets the availability to False. - `return_item(self)`: Sets the availability to True. 2. **Derived Classes: `Book` and `Magazine`** Both classes inherit from `LibraryItem`. - Additional Attributes for `Book`: - `genre`: The genre of the book (string). - Additional Attributes for `Magazine`: - `issue_number`: The issue number of the magazine (integer). - Additional Methods for both classes: - Implement `__str__(self)` to include new attributes in the string representation. 3. **Class: `Library`** This class manages the collection of `LibraryItem` objects. - Attributes: - `items`: A list to store all the library items. - Methods: - `__init__(self)`: Initializes the items list as an empty list. - `add_item(self, item)`: Adds a `LibraryItem` to the list. - `remove_item(self, item_id)`: Removes an item with the given item_id from the list. - `find_item_by_title(self, title)`: Returns a list of items with matching title. - `__iter__(self)`: Implements the iterator protocol to iterate over available items. - `__next__(self)`: Returns the next item in the list and raises `StopIteration` when done. 4. **Generators and Iterators** - Create a generator function `available_items(item_list)` that yields all available items from a given list. **Constraints:** - Each function should handle invalid input gracefully. - The `item_id` should be unique for each `LibraryItem`. **Example Usage:** ```python # Creating library items book1 = Book(\\"Harry Potter\\", \\"J.K. Rowling\\", \\"B001\\", \\"Fantasy\\") book2 = Book(\\"Effective Python\\", \\"Brett Slatkin\\", \\"B002\\", \\"Programming\\") magazine1 = Magazine(\\"National Geographic\\", \\"Various\\", \\"M001\\", 202) # Adding items to library library = Library() library.add_item(book1) library.add_item(book2) library.add_item(magazine1) # Borrowing and returning items book1.borrow() magazine1.borrow() book1.return_item() # Finding items print(library.find_item_by_title(\\"Harry Potter\\")) # Should print details of book1 # Iterating over available items for item in library: print(item) # Using the generator available = available_items(library.items) for item in available: print(item) ``` Implement the classes and methods as described. Include appropriate error handling and ensure your code is well-organized and commented.","solution":"class LibraryItem: def __init__(self, title, author, item_id): self.title = title self.author = author self.item_id = item_id self.availability = True def __str__(self): return f\\"Title: {self.title}, Author: {self.author}, ID: {self.item_id}, Available: {self.availability}\\" def borrow(self): self.availability = False def return_item(self): self.availability = True class Book(LibraryItem): def __init__(self, title, author, item_id, genre): super().__init__(title, author, item_id) self.genre = genre def __str__(self): return super().__str__() + f\\", Genre: {self.genre}\\" class Magazine(LibraryItem): def __init__(self, title, author, item_id, issue_number): super().__init__(title, author, item_id) self.issue_number = issue_number def __str__(self): return super().__str__() + f\\", Issue Number: {self.issue_number}\\" class Library: def __init__(self): self.items = [] def add_item(self, item): self.items.append(item) def remove_item(self, item_id): self.items = [item for item in self.items if item.item_id != item_id] def find_item_by_title(self, title): return [item for item in self.items if item.title == title] def __iter__(self): self._index = 0 return self def __next__(self): if self._index < len(self.items): result = self.items[self._index] self._index += 1 if result.availability: return result return self.__next__() else: raise StopIteration def available_items(item_list): for item in item_list: if item.availability: yield item"},{"question":"# Advanced Coding Assessment: Analyzing and Optimizing Pickle Files **Objective**: To assess your understanding of the `pickletools` module and your ability to manipulate and analyze pickled data programmatically. **Problem Statement**: You are given the task of writing a Python script that performs the following operations on multiple pickle files: 1. Disassembles the contents of each pickle file and prints the symbolic disassembly. 2. Optimizes the pickled data by eliminating unused \\"PUT\\" opcodes. 3. Outputs both the disassembled and optimized pickle data to specified files. **Function Signatures and Requirements**: 1. `disassemble_pickle(pickle_data: bytes, annotate: int = 0) -> str`: - **Input**: - `pickle_data`: A bytes object representing the pickled data. - `annotate`: An optional integer specifying the annotation level (default is 0, no annotation). - **Output**: - A string containing the symbolic disassembly of the pickle data. 2. `optimize_pickle(pickle_data: bytes) -> bytes`: - **Input**: - `pickle_data`: A bytes object representing the pickled data. - **Output**: - A bytes object containing the optimized pickle data. 3. `process_pickles(input_files: List[str], output_dis_files: List[str], output_opt_files: List[str], annotate: int = 0) -> None`: - **Input**: - `input_files`: A list of strings representing the paths to the input pickle files. - `output_dis_files`: A list of strings representing the file paths where the disassembled output should be written. - `output_opt_files`: A list of strings representing the file paths where the optimized pickle output should be written. - `annotate`: An optional integer specifying the annotation level for disassembly (default is 0, no annotation). - **Output**: - This function should perform the operations described above and save the results to the specified files. **Constraints**: - You may assume that the lengths of `input_files`, `output_dis_files`, and `output_opt_files` are equal. - The files may be large, so your solution should be efficient in terms of both time and space complexity. - Handle any exceptions that might occur during file I/O operations and output errors appropriately. **Example Usage**: ```python input_files = [\\"data1.pickle\\", \\"data2.pickle\\"] output_dis_files = [\\"data1_dis.txt\\", \\"data2_dis.txt\\"] output_opt_files = [\\"data1_opt.pickle\\", \\"data2_opt.pickle\\"] process_pickles(input_files, output_dis_files, output_opt_files, annotate=4) ``` This should read the pickles from `data1.pickle` and `data2.pickle`, disassemble them with annotation level 4, optimize them, and save the disassembly output to `data1_dis.txt` and `data2_dis.txt`, and the optimized pickles to `data1_opt.pickle` and `data2_opt.pickle` respectively. Implement the functions `disassemble_pickle`, `optimize_pickle`, and `process_pickles` to complete this task.","solution":"import pickletools import pickle from typing import List def disassemble_pickle(pickle_data: bytes, annotate: int = 0) -> str: Disassembles the contents of pickle data and returns the symbolic disassembly. Args: pickle_data (bytes): A bytes object representing the pickled data. annotate (int): An optional integer specifying the annotation level (default is 0). Returns: str: A string containing the symbolic disassembly of the pickle data. from io import StringIO output = StringIO() pickletools.dis(pickle_data, output, annotate=annotate) return output.getvalue() def optimize_pickle(pickle_data: bytes) -> bytes: Optimizes the pickled data by eliminating unused \\"PUT\\" opcodes. Args: pickle_data (bytes): A bytes object representing the pickled data. Returns: bytes: A bytes object containing the optimized pickle data. data = pickle.loads(pickle_data) return pickle.dumps(data, protocol=pickle.HIGHEST_PROTOCOL) def process_pickles(input_files: List[str], output_dis_files: List[str], output_opt_files: List[str], annotate: int = 0) -> None: Processes multiple pickle files to disassemble and optimize them. Args: input_files (List[str]): A list of strings representing the paths to the input pickle files. output_dis_files (List[str]): A list of strings representing the file paths where the disassembled output should be written. output_opt_files (List[str]): A list of strings representing the file paths where the optimized pickle output should be written. annotate (int): An optional integer specifying the annotation level for disassembly (default is 0). Returns: None for input_file, output_dis_file, output_opt_file in zip(input_files, output_dis_files, output_opt_files): try: with open(input_file, \'rb\') as f: pickle_data = f.read() # Disassemble disassembled_data = disassemble_pickle(pickle_data, annotate=annotate) with open(output_dis_file, \'w\') as f: f.write(disassembled_data) # Optimize optimized_data = optimize_pickle(pickle_data) with open(output_opt_file, \'wb\') as f: f.write(optimized_data) except Exception as e: print(f\\"Error processing {input_file}: {e}\\")"},{"question":"**Question:** Create a data visualization function using seaborn that meets the following requirements: 1. **Function Name:** `create_custom_heatmap` 2. **Inputs:** - `data`: A pandas DataFrame containing at least the columns \\"Model\\", \\"Task\\", and \\"Score\\". - `annot_format`: A string that specifies the format for annotations (e.g., \'.1f\'). Default value is \' \'. No annotations if this value is an empty string. - `cmap_name`: A string that specifies the name of the colormap to be used. Default value is \'viridis\'. - `vmin`: An integer specifying the minimum data value for the colormap. Default value is `None`. - `vmax`: An integer specifying the maximum data value for the colormap. Default value is `None`. - `linewidth`: A float specifying the width of the lines between cells. Default value is 0. - `xlabel`, `ylabel`: Strings for x and y axis labels respectively. Default values are empty strings. 3. **Outputs:** None (The function should display the heatmap). The function should: - Load the dataset and pivot it to create a matrix where indices are \\"Model\\" and columns are \\"Task\\", and values are \\"Score\\". - Create a heatmap using seaborn. - Add annotations to the heatmap if `annot_format` is not an empty string, otherwise no annotations. - Customize the colormap and its range using `cmap_name`, `vmin`, and `vmax`. - Add lines between the heatmap cells with the specified `linewidth`. - Modify the x and y axis labels based on the `xlabel` and `ylabel` parameters. - Ensure the x-axis ticks are at the top of the heatmap. **Function Signature:** ```python def create_custom_heatmap(data: pd.DataFrame, annot_format: str = \' \', cmap_name: str = \'viridis\', vmin: int = None, vmax: int = None, linewidth: float = 0, xlabel: str = \'\', ylabel: str = \'\') -> None: ``` **Example Usage:** ```python import pandas as pd import seaborn as sns import matplotlib.pyplot as plt # Example dataset data = pd.DataFrame({ \'Model\': [\'A\', \'B\', \'C\', \'A\', \'B\', \'C\'], \'Task\': [\'T1\', \'T1\', \'T1\', \'T2\', \'T2\', \'T2\'], \'Score\': [80, 75, 90, 85, 88, 92] }) create_custom_heatmap(data, annot_format=\'.2f\', cmap_name=\'crest\', vmin=70, vmax=100, linewidth=0.5, xlabel=\'Tasks\', ylabel=\'Models\') plt.show() ``` **Constraints:** - The input DataFrame is guaranteed to have columns named \\"Model\\", \\"Task\\", and \\"Score\\". - Handle cases where `vmin`, `vmax` might be `None`. - Ensure that the function is robust and handles different colormap names and formats appropriately. **Note:** You will be evaluated based on the accuracy and efficiency of your function as well as your ability to apply seaborn functionalities effectively.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def create_custom_heatmap(data: pd.DataFrame, annot_format: str = \' \', cmap_name: str = \'viridis\', vmin: int = None, vmax: int = None, linewidth: float = 0, xlabel: str = \'\', ylabel: str = \'\') -> None: # Pivot the data pivot_table = data.pivot(index=\'Model\', columns=\'Task\', values=\'Score\') # Create the heatmap sns.heatmap(pivot_table, annot=(annot_format != \' \'), fmt=annot_format, cmap=cmap_name, vmin=vmin, vmax=vmax, linewidths=linewidth) # Set axis labels plt.xlabel(xlabel) plt.ylabel(ylabel) # Set x-axis ticks to be at the top plt.xticks(rotation=45, ha=\'left\') plt.gca().xaxis.set_ticks_position(\'top\')"},{"question":"# Unicode String Normalization and Comparison Objective: You are required to implement a function in Python that reads Unicode data from a file, processes it by normalizing and casefolding, and compares it to a given Unicode string. This will assess your understanding of Unicode string manipulation, file handling, and comparison. Problem Statement: Write a function `process_and_compare_unicode(file_path: str, comparison_string: str) -> bool` that performs the following tasks: 1. Reads Unicode data from the file located at `file_path`. 2. Normalize the content using the \'NFC\' (Normalization Form C) form. 3. Casefold the normalized content and the `comparison_string`. 4. Compare the processed content from the file to the `comparison_string` and return `True` if they are equivalent, otherwise return `False`. Input: - `file_path` (str): The path to the input file containing Unicode text. - `comparison_string` (str): The Unicode string to compare against the processed content from the file. Output: - `bool`: `True` if the processed content from the file is equivalent to the `comparison_string`, otherwise `False`. Constraints: - The input file will contain text with various Unicode characters. - The comparison should ignore case differences and differences in normalization forms. Example: Assume the file located at `file_path` contains the following content: ``` é é ``` Notice that \'é\' (single code point) and \'é\' (two code points) are visually the same but encoded differently. ```python comparison_string = \\"É\\" # Example usage result = process_and_compare_unicode(\\"path_to_file.txt\\", comparison_string) print(result) # Output: True ``` Notes: - Use the `unicodedata` module for normalization. - Use the `str.casefold()` method for casefolding. Solution Template: ```python import unicodedata def process_and_compare_unicode(file_path: str, comparison_string: str) -> bool: with open(file_path, \'r\', encoding=\'utf-8\') as file: content = file.read().strip() # Normalize and casefold the content from the file and the comparison string normalized_content = unicodedata.normalize(\'NFC\', content).casefold() normalized_comparison = unicodedata.normalize(\'NFC\', comparison_string).casefold() # Compare and return the result return normalized_content == normalized_comparison # Example usage comparison_string = \\"É\\" result = process_and_compare_unicode(\\"path_to_file.txt\\", comparison_string) print(result) # Output: True or False based on file content ```","solution":"import unicodedata def process_and_compare_unicode(file_path: str, comparison_string: str) -> bool: Reads Unicode data from the file, normalizes and casefolds it, and compares it to the given Unicode string. Parameters: file_path (str): The path to the input file containing Unicode text. comparison_string (str): The Unicode string to compare against the processed content from the file. Returns: bool: True if the processed content from the file is equivalent to the comparison_string, False otherwise. with open(file_path, \'r\', encoding=\'utf-8\') as file: content = file.read().strip() # Normalize and casefold the content from the file and the comparison string normalized_content = unicodedata.normalize(\'NFC\', content).casefold() normalized_comparison = unicodedata.normalize(\'NFC\', comparison_string).casefold() # Compare and return the result return normalized_content == normalized_comparison"},{"question":"**Objective:** Demonstrate your understanding of PyTorch tensor operations, random sampling, and device-specific computations. **Problem Statement:** You are working on a machine learning project that requires the initialization of tensors with specific characteristics and performing device-specific computations. Your task is to follow the steps below to complete the operation. **Task:** 1. Create a function `initialize_tensors` that: - Generates a random tensor `A` of shape `(5, 5)` with values sampled from a standard normal distribution. - Creates a tensor `B` of shape `(5, 5)` filled with ones. - Combines the tensors `A` and `B` along the vertical axis. 2. Create a function `apply_operations` that: - Accepts a tensor `C` as input. - Slices the input tensor `C` to obtain its upper and lower halves (`C_upper` and `C_lower`), where each half is of equal shape. - Performs an element-wise addition of `C_upper` and `C_lower`. 3. Create a function `compute_on_device` that: - Accepts a tensor as input. - Moves the tensor to a CUDA device if available, or keeps it on the CPU if CUDA is not available. - Performs an element-wise multiplication of the tensor with itself. - Returns the resulting tensor. **Your implementation should:** - Use appropriate PyTorch functions to achieve the tasks mentioned above. - Handle the case where CUDA is not available by performing computations on the CPU. - Ensure that all tensors are manipulated efficiently. **Function Signature:** ```python import torch def initialize_tensors(): # Your implementation here pass def apply_operations(C): # Your implementation here pass def compute_on_device(tensor): # Your implementation here pass ``` **Example:** ```python A, B, C = initialize_tensors() result = apply_operations(C) final_result = compute_on_device(result) print(final_result) ``` **Expected Output Format:** - A printed tensor `final_result` after performing the sequence of operations on the device-specific hardware. **Constraints:** - You should not use any external packages other than PyTorch. - Efficiently handle the tensor operations to ensure optimal performance. **Note:** Include comments and docstrings in your code to explain the logic and functionality of your implementation.","solution":"import torch def initialize_tensors(): Generates a random tensor A (5x5) sampled from standard normal distribution, creates a tensor B (5x5) filled with ones, and combines them vertically resulting in a tensor of shape (10, 5). A = torch.randn(5, 5) B = torch.ones(5, 5) C = torch.cat((A, B), dim=0) return A, B, C def apply_operations(C): Accepts a tensor C, slices it into upper and lower halves, and performs element-wise addition of these halves. upper_half = C[:5, :] lower_half = C[5:, :] result = upper_half + lower_half return result def compute_on_device(tensor): Moves the tensor to a CUDA device if available, otherwise keeps it on the CPU. Performs element-wise multiplication of the tensor with itself. device = torch.device(\\"cuda\\" if torch.cuda.is_available() else \\"cpu\\") tensor = tensor.to(device) tensor = tensor * tensor return tensor.cpu() # Return the result back to CPU for consistency # Example usage if __name__ == \\"__main__\\": A, B, C = initialize_tensors() result = apply_operations(C) final_result = compute_on_device(result) print(final_result)"},{"question":"**Question: Analyzing and Visualizing Sales Data** You are provided with a dataset `sales.csv` that contains the following columns: - `Region`: The geographical region of the sales (e.g., \\"North\\", \\"South\\", \\"East\\", \\"West\\"). - `Product`: The product category (e.g., \\"Electronics\\", \\"Furniture\\", \\"Clothing\\"). - `Sales`: The sales amount in dollars. - `Quantity`: The number of items sold. - `Salesperson`: The name of the salesperson. - `Month`: The month of the sale (e.g., \\"January\\", \\"February\\"). Using the Seaborn library, specifically the `sns.swarmplot` function, your task is to visualize the sales data to gain insights into the distribution and trends. # Requirements 1. **Load and Prepare Data:** - Load the dataset from `sales.csv`. - Drop any rows with missing values. 2. **Univariate Analysis:** - Create a swarm plot to visualize the distribution of sales amounts. 3. **Categorical Comparisons:** - Create a swarm plot to compare the sales amounts for each `Region`. - Create a swarm plot to compare the sales amounts for each `Product` with the hue set to `Region`. 4. **Multi-dimensional Analysis:** - Create a swarm plot to compare the sales amounts for each `Salesperson` with the hue set to `Quantity`. Ensure the color palette used shows a distinct color for each quantity value. 5. **Advanced Customization:** - Create a swarm plot for the sales amounts in each `Month`. Set the `hue` to `Region`, `dodge` the points by `Region`, and use a custom marker, such as `\\"X\\"` with increased `linewidth`. # Input The input is a CSV file named `sales.csv`. # Output Generate and show the following plots: - Univariate sales amount distribution. - Sales amounts compared by `Region`. - Sales amounts compared by `Product` with regional differences. - Sales amounts compared by `Salesperson` with different sales quantities indicated. - Sales amounts by month with advanced customization as specified. # Constraints - Use the Seaborn library for plotting. - Handle the data cleanup (e.g., dropping missing values) as specified. - Make sure plots are labeled appropriately with titles and axis labels where necessary. # Example ```python import pandas as pd import seaborn as sns import matplotlib.pyplot as plt # 1. Load and prepare data data = pd.read_csv(\'sales.csv\') data.dropna(inplace=True) # 2. Univariate analysis plt.figure(figsize=(10, 6)) sns.swarmplot(x=data[\'Sales\']) plt.title(\'Distribution of Sales Amounts\') plt.show() # 3. Categorical comparisons By Region plt.figure(figsize=(10, 6)) sns.swarmplot(data=data, x=\'Sales\', y=\'Region\') plt.title(\'Sales Amounts by Region\') plt.show() By Product with Region hue plt.figure(figsize=(10, 6)) sns.swarmplot(data=data, x=\'Sales\', y=\'Product\', hue=\'Region\') plt.title(\'Sales Amounts by Product with Region Hue\') plt.show() # 4. Multi-dimensional analysis plt.figure(figsize=(20, 10)) sns.swarmplot(data=data, x=\'Sales\', y=\'Salesperson\', hue=\'Quantity\', palette=\'coolwarm\') plt.title(\'Sales Amounts by Salesperson with Quantity Indicated\') plt.show() # 5. Advanced Customization plt.figure(figsize=(10, 6)) sns.swarmplot(data=data, x=\'Month\', y=\'Sales\', hue=\'Region\', dodge=True, marker=\'X\', linewidth=0.7) plt.title(\'Sales Amounts by Month with Region Hue and Custom Marker\') plt.show() ``` Make sure to comment your code appropriately and provide titles and labels for your plots.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def load_and_prepare_data(file_path): Loads the dataset from the provided file path and drops rows with missing values. :param file_path: str, path to the CSV file :return: DataFrame, cleaned data data = pd.read_csv(file_path) data.dropna(inplace=True) return data def plot_sales_distribution(data): Creates a swarm plot to visualize the distribution of sales amounts. :param data: DataFrame, sales data plt.figure(figsize=(10, 6)) sns.swarmplot(x=data[\'Sales\']) plt.title(\'Distribution of Sales Amounts\') plt.show() def plot_sales_by_region(data): Creates a swarm plot to compare the sales amounts for each region. :param data: DataFrame, sales data plt.figure(figsize=(10, 6)) sns.swarmplot(data=data, x=\'Sales\', y=\'Region\') plt.title(\'Sales Amounts by Region\') plt.show() def plot_sales_by_product_with_region(data): Creates a swarm plot to compare the sales amounts for each product with the hue set to region. :param data: DataFrame, sales data plt.figure(figsize=(10, 6)) sns.swarmplot(data=data, x=\'Sales\', y=\'Product\', hue=\'Region\') plt.title(\'Sales Amounts by Product with Region Hue\') plt.show() def plot_sales_by_salesperson_with_quantity(data): Creates a swarm plot to compare the sales amounts for each salesperson with the hue set to quantity. :param data: DataFrame, sales data plt.figure(figsize=(20, 10)) sns.swarmplot(data=data, x=\'Sales\', y=\'Salesperson\', hue=\'Quantity\', palette=\'coolwarm\') plt.title(\'Sales Amounts by Salesperson with Quantity Indicated\') plt.show() def plot_sales_by_month_with_customization(data): Creates a swarm plot for the sales amounts in each month with advanced customization. :param data: DataFrame, sales data plt.figure(figsize=(10, 6)) sns.swarmplot(data=data, x=\'Month\', y=\'Sales\', hue=\'Region\', dodge=True, marker=\'X\', linewidth=0.7) plt.title(\'Sales Amounts by Month with Region Hue and Custom Marker\') plt.show() # Example usage if __name__ == \\"__main__\\": data = load_and_prepare_data(\'sales.csv\') plot_sales_distribution(data) plot_sales_by_region(data) plot_sales_by_product_with_region(data) plot_sales_by_salesperson_with_quantity(data) plot_sales_by_month_with_customization(data)"},{"question":"You are provided with a dataset containing sales data for various products over a span of several months. The data includes the date of sale, product name, number of units sold, and their price. Your task is to perform the following operations using pandas Series to clean, transform, analyze, and visualize the data. Input You will implement a function `analyze_sales(df: pd.DataFrame) -> pd.Series` that takes as input a DataFrame `df` with the following columns: 1. `date`: The date of sale (string format \'YYYY-MM-DD\'). 2. `product`: The name of the product (string). 3. `units_sold`: Number of units sold (integer). 4. `price`: Price per unit (float). Output The function should return a pandas Series with the following data: 1. Average units sold per product. 2. Average revenue per product. 3. Percentage change in total revenue month-over-month. 4. The product with the highest average monthly revenue. 5. A plot showing the total revenue trend over time. Steps to Implement 1. **Data Cleaning**: - Convert the `date` column to pandas datetime format. - Ensure that `units_sold` and `price` have the correct data types, handling any missing or incorrect values. 2. **Data Transformation & Analysis**: - Calculate average units sold per product. - Calculate average revenue (units sold * price) per product. - Compute total monthly revenue and its percentage change month-over-month. - Determine the product with the highest average monthly revenue. 3. **Visualization**: - Create a plot showing the total revenue trend over time (monthly). Example ```python import pandas as pd data = { \'date\': [\'2021-01-01\', \'2021-01-02\', \'2021-02-01\', \'2021-02-02\'], \'product\': [\'A\', \'A\', \'B\', \'B\'], \'units_sold\': [10, 15, 20, 25], \'price\': [5.0, 5.0, 10.0, 10.0] } df = pd.DataFrame(data) result = analyze_sales(df) print(result) # Expected output # Series with: # - Average units sold per product # - Average revenue per product # - Monthly percentage change in revenue # - Product with highest avg monthly revenue # - Plot of total revenue trend (will show when running code in a Jupyter Notebook or similar environment) ``` Constraints - Ensure your code runs efficiently for large datasets (up to 100,000 rows). - Handle missing data appropriately, such as using mean or median to fill in missing values or dropping rows with significant missing data. - Ensure that date parsing and any transformation steps are robust to incorrect formats or unexpected input. You may use built-in pandas functions and methods to achieve the required transformations and analysis.","solution":"import pandas as pd import matplotlib.pyplot as plt def analyze_sales(df: pd.DataFrame) -> pd.Series: # Data Cleaning df[\'date\'] = pd.to_datetime(df[\'date\'], errors=\'coerce\') df = df.dropna(subset=[\'date\', \'units_sold\', \'price\']) df[\'units_sold\'] = df[\'units_sold\'].astype(int) df[\'price\'] = df[\'price\'].astype(float) # Calculate Revenue df[\'revenue\'] = df[\'units_sold\'] * df[\'price\'] # Average units sold per product average_units_sold = df.groupby(\'product\')[\'units_sold\'].mean() # Average revenue per product average_revenue_per_product = df.groupby(\'product\')[\'revenue\'].mean() # Total monthly revenue and Percentage change month-over-month df.set_index(\'date\', inplace=True) monthly_revenue = df.resample(\'M\')[\'revenue\'].sum() monthly_revenue_pct_change = monthly_revenue.pct_change().fillna(0) # Product with the highest average monthly revenue monthly_product_revenue = df.groupby([pd.Grouper(freq=\'M\'), \'product\'])[\'revenue\'].sum().reset_index() highest_avg_monthly_revenue_product = monthly_product_revenue.groupby(\'product\')[\'revenue\'].mean().idxmax() # Plot total revenue trend over time plt.figure(figsize=(10, 6)) plt.plot(monthly_revenue.index, monthly_revenue.values, marker=\'o\', linestyle=\'-\') plt.xlabel(\'Date\') plt.ylabel(\'Total Revenue\') plt.title(\'Total Monthly Revenue Trend\') plt.grid(True) plt.show() # Prepare the resulting series result = pd.Series({ \'average_units_sold_per_product\': average_units_sold, \'average_revenue_per_product\': average_revenue_per_product, \'monthly_revenue_pct_change\': monthly_revenue_pct_change, \'highest_avg_monthly_revenue_product\': highest_avg_monthly_revenue_product, }) return result"},{"question":"# Question: Working with Time Zones Using `zoneinfo` You are working on an event scheduling application. The application needs to manage events, including converting event times to different time zones and handling daylight saving time transitions correctly. Implement a function to handle these tasks. Task 1. **Convert Event Time**: Write a function `convert_event_time(event_time_str: str, from_tz: str, to_tz: str) -> str` which takes an event time in string format (`event_time_str`), the source time zone (`from_tz`), and the target time zone (`to_tz`). The function should return the event time converted to the target time zone in the same string format. 2. **Schedule Event**: Write a function `schedule_event(event_time_str: str, event_duration: int, from_tz: str, to_tz_list: list) -> dict` which takes an event time in string format (`event_time_str`), the event duration in minutes (`event_duration`), the source time zone (`from_tz`), and a list of target time zones (`to_tz_list`). The function should return a dictionary where each key is a target time zone and the value is the event time in that time zone (still in string format). Input - `event_time_str` (str): Event start time in format `YYYY-MM-DD HH:MM:SS` (e.g. `\\"2023-10-05 14:30:00\\"`). - `from_tz` (str): Source time zone (e.g. `\\"America/New_York\\"`). - `to_tz` (str): Target time zone for function `convert_event_time` (e.g. `\\"Europe/London\\"`). - `to_tz_list` (list): List of target time zones for function `schedule_event` (e.g. `[\\"Europe/London\\", \\"Asia/Tokyo\\", \\"Australia/Sydney\\"]`). - `event_duration` (int): Duration of the event in minutes for `schedule_event` function. Output 1. **`convert_event_time`**: - A string representing the event time converted to the target time zone. 2. **`schedule_event`**: - A dictionary where each key is a target time zone and the value is the event time in that time zone. Example ```python from datetime import datetime from zoneinfo import ZoneInfo def convert_event_time(event_time_str: str, from_tz: str, to_tz: str) -> str: # your implementation here def schedule_event(event_time_str: str, event_duration: int, from_tz: str, to_tz_list: list) -> dict: # your implementation here # Example usage: event_time_str = \\"2023-10-05 14:30:00\\" from_tz = \\"America/New_York\\" to_tz = \\"Europe/London\\" # Convert event time print(convert_event_time(event_time_str, from_tz, to_tz)) # \\"2023-10-05 19:30:00\\" # Schedule event in multiple time zones to_tz_list = [\\"Europe/London\\", \\"Asia/Tokyo\\", \\"Australia/Sydney\\"] event_duration = 120 print(schedule_event(event_time_str, event_duration, from_tz, to_tz_list)) Expected Output: { \'Europe/London\': \'2023-10-05 19:30:00\', \'Asia/Tokyo\': \'2023-10-06 03:30:00\', \'Australia/Sydney\': \'2023-10-06 05:30:00\' } ``` Constraints - The time zones provided are valid IANA time zones. - The event time and duration are provided in valid formats. Implement these functions considering the mentioned constraints and examples. Ensure to handle edge cases like daylight saving time transitions.","solution":"from datetime import datetime, timedelta from zoneinfo import ZoneInfo def convert_event_time(event_time_str: str, from_tz: str, to_tz: str) -> str: Converts event time from one time zone to another. Parameters: event_time_str (str): The event time as a string in format \'YYYY-MM-DD HH:MM:SS\'. from_tz (str): The source time zone. to_tz (str): The target time zone. Returns: str: The event time converted to the target time zone. event_time = datetime.strptime(event_time_str, \\"%Y-%m-%d %H:%M:%S\\") from_zone = ZoneInfo(from_tz) to_zone = ZoneInfo(to_tz) # Localize event time to the from_tz event_time = event_time.replace(tzinfo=from_zone) # Convert to the target time zone converted_time = event_time.astimezone(to_zone) return converted_time.strftime(\\"%Y-%m-%d %H:%M:%S\\") def schedule_event(event_time_str: str, event_duration: int, from_tz: str, to_tz_list: list) -> dict: Schedules an event across multiple time zones. Parameters: event_time_str (str): The event time as a string in format \'YYYY-MM-DD HH:MM:SS\'. event_duration (int): The duration of the event in minutes. from_tz (str): The source time zone. to_tz_list (list): List of target time zones. Returns: dict: A dictionary where each key is a target time zone, and the value is the event time in that time zone. event_time = datetime.strptime(event_time_str, \\"%Y-%m-%d %H:%M:%S\\") from_zone = ZoneInfo(from_tz) # Localize event time to the from_tz event_time = event_time.replace(tzinfo=from_zone) event_dict = {} for to_tz in to_tz_list: to_zone = ZoneInfo(to_tz) # Convert to the target time zone converted_start_time = event_time.astimezone(to_zone) # Format and add to dictionary event_dict[to_tz] = converted_start_time.strftime(\\"%Y-%m-%d %H:%M:%S\\") return event_dict"},{"question":"You are required to implement a custom JSON encoder and decoder in Python. The functionality you need to implement involves encoding and decoding a Python object that contains a mixture of standard types (like strings, integers, and lists) and custom objects. Specifically, you will work with an object that uses datetime for date information and complex numbers. Your task is to: 1. **Encode** a Python object into a JSON string, ensuring that datetime objects are converted to strings in ISO 8601 format, and complex numbers are converted to a dictionary containing the real and imaginary parts. 2. **Decode** the JSON string back into the original Python object, restoring the datetime objects and complex numbers to their appropriate types. # Input - A Python object that may contain: - Standard Python types such as lists, dictionaries, strings, integers, and floats. - Datetime objects (from the `datetime` module). - Complex numbers. # Output - A JSON encoded string. It should be decodable back into the original Python object with proper types restored. # Constraints - The datetime objects must be serialized in ISO 8601 format (e.g., \\"2023-10-03T12:34:56\\"). - Complex numbers must be serialized as dictionaries with keys `\\"real\\"` and `\\"imag\\"`. # Example ```python from datetime import datetime import json class CustomJSONEncoder(json.JSONEncoder): def default(self, obj): pass # Implement encoding for datetime and complex objects here def custom_json_decoder(dct): pass # Implement decoding for datetime and complex objects here # Test object test_obj = { \'name\': \'example\', \'timestamp\': datetime(2023, 10, 3, 12, 34, 56), \'value\': 3 + 4j, \'data\': [1, 2, 3, {\'inner_value\': 5 + 6j}] } # Encode the object encoded_obj = json.dumps(test_obj, cls=CustomJSONEncoder) print(encoded_obj) # Expected output: \'{\\"name\\": \\"example\\", \\"timestamp\\": \\"2023-10-03T12:34:56\\", \\"value\\": {\\"real\\": 3, \\"imag\\": 4}, \\"data\\": [1, 2, 3, {\\"inner_value\\": {\\"real\\": 5, \\"imag\\": 6}}]}\' # Decode the object decoded_obj = json.loads(encoded_obj, object_hook=custom_json_decoder) print(decoded_obj) print(isinstance(decoded_obj[\'timestamp\'], datetime)) # Expected: True print(isinstance(decoded_obj[\'value\'], complex)) # Expected: True ``` # Requirements 1. Implement the `CustomJSONEncoder` class to handle datetime and complex objects. 2. Implement the `custom_json_decoder` function to convert the JSON dictionary back into the original Python object, correctly restoring datetime and complex objects. You may import any necessary standard library modules except those used for internet connectivity or OS-level operations.","solution":"import json from datetime import datetime class CustomJSONEncoder(json.JSONEncoder): def default(self, obj): if isinstance(obj, datetime): return obj.isoformat() if isinstance(obj, complex): return {\\"real\\": obj.real, \\"imag\\": obj.imag} return super().default(obj) def custom_json_decoder(dct): for key, value in dct.items(): if isinstance(value, str): try: dct[key] = datetime.fromisoformat(value) except ValueError: pass if isinstance(value, dict) and \\"real\\" in value and \\"imag\\" in value: try: dct[key] = complex(value[\\"real\\"], value[\\"imag\\"]) except ValueError: pass return dct"},{"question":"# XML Data Escaping and Unescaping As part of a project to handle XML data, you are required to implement two functions that utilize the `xml.sax.saxutils` module to handle escaping and unescaping of special XML characters within text data. **Task**: 1. Implement a function `escape_xml_data(data: str, entities: dict = None) -> str` that uses `xml.sax.saxutils.escape` to escape special characters in the provided data. The function should also allow for additional characters to be escaped based on a provided dictionary of entities. 2. Implement a function `unescape_xml_data(data: str, entities: dict = None) -> str` that uses `xml.sax.saxutils.unescape` to unescape special characters in the provided data. The function should also allow for additional characters to be unescaped based on a provided dictionary of entities. **Function Signatures**: ```python def escape_xml_data(data: str, entities: dict = None) -> str: pass def unescape_xml_data(data: str, entities: dict = None) -> str: pass ``` **Input**: - `data` (str): The input string containing characters that need to be escaped or unescaped. - `entities` (dict, optional): A dictionary where keys are characters to be replaced, and values are their corresponding replacement strings. **Output**: - A string with the specified characters escaped or unescaped. **Examples**: ```python # Example 1 input_data = \\"This & That < Now > Later\\" escaped_entities = {\\"&\\": \\"&amp;\\", \\"<\\": \\"&lt;\\", \\">\\": \\"&gt;\\"} print(escape_xml_data(input_data)) # Expected Output: \\"This &amp; That &lt; Now &gt; Later\\" # Example 2 escaped_data = \\"This &amp; That &lt; Now &gt; Later\\" unescaped_entities = {\\"&amp;\\": \\"&\\", \\"&lt;\\": \\"<\\", \\"&gt;\\": \\">\\"} print(unescape_xml_data(escaped_data)) # Expected Output: \\"This & That < Now > Later\\" # Example 3 input_data = \'Quotes \\" and \' mixed\' escaped_entities = {\'\\"\': \\"&quot;\\", \\"\'\\": \\"&apos;\\"} print(escape_xml_data(input_data, escaped_entities)) # Expected Output: \'Quotes &quot; and &apos; mixed\' ``` **Constraints**: - You must use the provided `xml.sax.saxutils.escape` and `xml.sax.saxutils.unescape` functions within your implementations. - Handle edge cases such as empty strings, strings without characters to escape/unescape. - Assume the input strings can be up to 10^5 characters long. **Notes**: - This question covers both the understanding of escaping and unescaping XML data, as well as function implementation and parameter handling within the context of XML utilities.","solution":"from xml.sax.saxutils import escape, unescape def escape_xml_data(data: str, entities: dict = None) -> str: Escapes special XML characters in the provided data using xml.sax.saxutils.escape. Additional characters to be escaped can be specified in the entities dictionary. if entities is None: entities = {} return escape(data, entities) def unescape_xml_data(data: str, entities: dict = None) -> str: Unescapes special XML characters in the provided data using xml.sax.saxutils.unescape. Additional characters to be unescaped can be specified in the entities dictionary. if entities is None: entities = {} return unescape(data, entities)"},{"question":"# Question You are given a pre-trained neural network model, `MyModel`, implemented using PyTorch\'s `torch.nn.Module`. Your task is to write a function that computes the gradient and the Jacobian of the model\'s output with respect to its parameters for a given input batch of data. In addition, you will need to apply the vector-Jacobian product (VJP) for a given vector. Requirements 1. Implement the function `compute_grad_and_jacobian` that computes the gradient and Jacobian of the model\'s output with respect to its parameters. 2. Implement the function `apply_vjp` that computes the vector-Jacobian product for a given vector and a given input batch. # Function Signature ```python import torch from torch.func import grad, jacrev, vjp, functional_call class MyModel(torch.nn.Module): def __init__(self): super(MyModel, self).__init__() self.linear = torch.nn.Linear(4, 2) def forward(self, x): return self.linear(x) def compute_grad_and_jacobian(model: MyModel, input_batch: torch.Tensor) -> (dict, torch.Tensor): Compute the gradient and Jacobian of the model\'s output with respect to its parameters. Args: - model (MyModel): The pre-trained model. - input_batch (torch.Tensor): A batch of input data with shape (batch_size, 4). Returns: - gradients (dict): A dictionary of gradients for each parameter. - jacobian (torch.Tensor): The Jacobian matrix of the outputs with respect to the parameters. pass def apply_vjp(model: MyModel, input_batch: torch.Tensor, vector: torch.Tensor) -> torch.Tensor: Apply vector-Jacobian product for the given vector and input batch. Args: - model (MyModel): The pre-trained model. - input_batch (torch.Tensor): A batch of input data with shape (batch_size, 4). - vector (torch.Tensor): A vector for the VJP with shape (batch_size, 2). Returns: - vjp_result (torch.Tensor): The result of the vector-Jacobian product. pass ``` # Constraints - `input_batch` is a tensor of shape `(batch_size, 4)`. - `vector` is a tensor of shape `(batch_size, 2)`. # Example ```python model = MyModel() input_batch = torch.randn(5, 4) # Batch size of 5 vector = torch.randn(5, 2) # Compute gradient and Jacobian gradients, jacobian = compute_grad_and_jacobian(model, input_batch) print(\\"Gradients:\\", gradients) print(\\"Jacobian shape:\\", jacobian.shape) # Apply vector-Jacobian product vjp_result = apply_vjp(model, input_batch, vector) print(\\"VJP Result shape:\\", vjp_result.shape) ``` Your solution should handle these operations efficiently and correctly for any valid input `model`, `input_batch`, and `vector`. # Notes - Use `jacrev` to compute the Jacobian. - Use `functional_call` to compute the gradients with respect to the model\'s parameters. - Apply `vjp` for the vector-Jacobian product computation.","solution":"import torch from torch.func import jacrev, vjp, functional_call class MyModel(torch.nn.Module): def __init__(self): super(MyModel, self).__init__() self.linear = torch.nn.Linear(4, 2) def forward(self, x): return self.linear(x) def compute_grad_and_jacobian(model: MyModel, input_batch: torch.Tensor) -> (dict, torch.Tensor): # Ensure model is in evaluation mode model.eval() # Function to compute the output of the model for the input_batch def model_output(params, x): return functional_call(model, params, x) # Calculate Jacobian w.r.t. the input_batch jacobian = jacrev(lambda x: model(x))(input_batch) # Compute the gradients w.r.t. the model parameters loss = model(input_batch).mean() gradients = torch.autograd.grad(loss, model.parameters(), create_graph=True) gradients_dict = {name: grad for name, grad in zip(model.state_dict().keys(), gradients)} return gradients_dict, jacobian def apply_vjp(model: MyModel, input_batch: torch.Tensor, vector: torch.Tensor) -> torch.Tensor: # Ensure model is in evaluation mode model.eval() # Function to compute the output of the model for the input_batch def model_output(x): return model(x) # Perform the vector-Jacobian product _, vjp_func = vjp(model_output, input_batch) vjp_result = vjp_func(vector)[0] return vjp_result"},{"question":"# Data Analysis and Transformation with Pandas You are provided with a set of CSV files capturing the sales data of a company\'s products with the following characteristics: 1. **Sales Data Files**: Each CSV file follows the structure: `Date, Product, Quantity, Price`. 2. **Quarterly Reports**: There are four quarterly CSV files named `Q1.csv`, `Q2.csv`, `Q3.csv`, and `Q4.csv`. Your task is to: 1. **Read**: Load all four CSV files into pandas DataFrames, concatenate them to form a single DataFrame, and parse the `Date` column to datetime format. 2. **Transform**: Add a new column `Revenue` to the DataFrame, which is computed as `Quantity * Price`. 3. **Summarize**: - Compute the total revenue generated by each product over the year. - Identify the month with the highest sales revenue for each product. 4. **Output**: Save your summary results into an Excel file with two sheets: - **\'Total Revenue\'**: Total revenue generated by each product. - **\'Best Month\'**: The month with the highest sales revenue for each product. # Expected Input 1. **CSV Files**: - `Q1.csv` - `Q2.csv` - `Q3.csv` - `Q4.csv` # Expected Output 1. Excel file named `annual_report.xlsx` with two sheets: - **\'Total Revenue\'** with columns: `Product`, `Total Revenue` - **\'Best Month\'** with columns: `Product`, `Month`, `Revenue` # Constraints: 1. You must use pandas for all data-related operations. 2. Ensure the data read from CSVs handles any missing values appropriately. 3. Ensure the final Excel output is formatted cleanly, with appropriate headers. Here is a starting code template: ```python import pandas as pd def load_and_concat(files): # Load and concatenate all CSV files data_frames = [pd.read_csv(file, parse_dates=[\'Date\']) for file in files] all_data = pd.concat(data_frames, ignore_index=True) return all_data def add_revenue_column(df): # Add a Revenue column df[\'Revenue\'] = df[\'Quantity\'] * df[\'Price\'] return df def compute_total_revenue(df): # Compute total revenue for each product total_revenue = df.groupby(\'Product\')[\'Revenue\'].sum().reset_index() total_revenue.columns = [\'Product\', \'Total Revenue\'] return total_revenue def compute_best_month(df): # Compute the month with the highest revenue for each product df[\'Month\'] = df[\'Date\'].dt.to_period(\'M\') best_month = df.groupby([\'Product\', \'Month\'])[\'Revenue\'].sum().reset_index() idx = best_month.groupby(\'Product\')[\'Revenue\'].idxmax() best_month = best_month.loc[idx] best_month.columns = [\'Product\', \'Month\', \'Revenue\'] return best_month def save_to_excel(total_revenue, best_month, output_file): # Save summary results to Excel with pd.ExcelWriter(output_file) as writer: total_revenue.to_excel(writer, sheet_name=\'Total Revenue\', index=False) best_month.to_excel(writer, sheet_name=\'Best Month\', index=False) if __name__ == \\"__main__\\": files = [\'Q1.csv\', \'Q2.csv\', \'Q3.csv\', \'Q4.csv\'] all_data = load_and_concat(files) all_data = add_revenue_column(all_data) total_revenue = compute_total_revenue(all_data) best_month = compute_best_month(all_data) save_to_excel(total_revenue, best_month, \'annual_report.xlsx\') ``` # Additional Requirement - Demonstrate the performance of your solution by comparing read/write times of CSVs vs. a compressed HDF5 format for the same data.","solution":"import pandas as pd def load_and_concat(files): # Load and concatenate all CSV files data_frames = [pd.read_csv(file, parse_dates=[\'Date\']) for file in files] all_data = pd.concat(data_frames, ignore_index=True) return all_data def add_revenue_column(df): # Add a Revenue column df[\'Revenue\'] = df[\'Quantity\'] * df[\'Price\'] return df def compute_total_revenue(df): # Compute total revenue for each product total_revenue = df.groupby(\'Product\')[\'Revenue\'].sum().reset_index() total_revenue.columns = [\'Product\', \'Total Revenue\'] return total_revenue def compute_best_month(df): # Compute the month with the highest revenue for each product df[\'Month\'] = df[\'Date\'].dt.to_period(\'M\') best_month = df.groupby([\'Product\', \'Month\'])[\'Revenue\'].sum().reset_index() idx = best_month.groupby(\'Product\')[\'Revenue\'].idxmax() best_month = best_month.loc[idx] best_month.columns = [\'Product\', \'Month\', \'Revenue\'] return best_month def save_to_excel(total_revenue, best_month, output_file): # Save summary results to Excel with pd.ExcelWriter(output_file) as writer: total_revenue.to_excel(writer, sheet_name=\'Total Revenue\', index=False) best_month.to_excel(writer, sheet_name=\'Best Month\', index=False) if __name__ == \\"__main__\\": files = [\'Q1.csv\', \'Q2.csv\', \'Q3.csv\', \'Q4.csv\'] all_data = load_and_concat(files) all_data = add_revenue_column(all_data) total_revenue = compute_total_revenue(all_data) best_month = compute_best_month(all_data) save_to_excel(total_revenue, best_month, \'annual_report.xlsx\')"},{"question":"# Question Introduction You are tasked to use seaborn to create a visualization that helps in understanding public health expenditure over years for different countries. You need to load a specific dataset, create a complex line plot, and customize it as per the given instructions. Dataset You will be using the `healthexp` dataset from seaborn’s built-in datasets. This dataset contains information on health expenditure in USD for various countries over different years. Instructions 1. **Load the Dataset**: Load the `healthexp` dataset using the `load_dataset` function from `seaborn`. 2. **Create a Plot**: - Initialize a plot for `healthexp` with `Year` on the x-axis, `Spending_USD` on the y-axis, and colored by `Country`. - Add a line plot to this initialized plot. 3. **Normalize Data**: - Normalize the plot data such that each group\'s (Country) spending is scaled relative to its maximum value using the `so.Norm()` transform. - Also, create another plot where the normalization is performed using the minimum `Year` as a baseline and present the spending as a percentage change from this baseline using the `so.Norm(where=\\"x == x.min()\\", percent=True)`. 4. **Customize Labels**: - For the first plot, label the y-axis as \\"Spending relative to maximum amount\\". - For the second plot, label the y-axis as \\"Percent change in spending from 1970 baseline\\". 5. **Plot Output**: - Display both plots. **Expected Function Signature**: ```python import seaborn.objects as so from seaborn import load_dataset def visualize_health_expenditure(): # Load the dataset healthexp = load_dataset(\\"healthexp\\") # First plot plot1 = ( so.Plot(healthexp, x=\\"Year\\", y=\\"Spending_USD\\", color=\\"Country\\") .add(so.Lines(), so.Norm()) .label(y=\\"Spending relative to maximum amount\\") ) # Second plot plot2 = ( so.PPlot(healthexp, x=\\"Year\\", y=\\"Spending_USD\\", color=\\"Country\\") .add(so.Lines(), so.Norm(where=\\"x == x.min()\\", percent=True)) .label(y=\\"Percent change in spending from 1970 baseline\\") ) # Display Plot plot1.show() plot2.show() ``` Constraints - You must use seaborn to create and customize the plots. - Ensure you handle any potential visualization issues like overlapping data using seaborn’s functionalities. Example Your output should include two well-labeled seaborn plots as specified in the instructions.","solution":"import seaborn.objects as so from seaborn import load_dataset def visualize_health_expenditure(): Visualizes the health expenditure over the years for different countries, using normalization and percentage change relative to specific baselines. # Load the dataset healthexp = load_dataset(\\"healthexp\\") # First plot: Spending relative to maximum amount plot1 = ( so.Plot(healthexp, x=\\"Year\\", y=\\"Spending_USD\\", color=\\"Country\\") .add(so.Lines(), so.Norm()) .label(y=\\"Spending relative to maximum amount\\") ) # Second plot: Percent change in spending from 1970 baseline plot2 = ( so.Plot(healthexp, x=\\"Year\\", y=\\"Spending_USD\\", color=\\"Country\\") .add(so.Lines(), so.Norm(where=\\"x == x.min()\\", percent=True)) .label(y=\\"Percent change in spending from 1970 baseline\\") ) # Display plots plot1.show() plot2.show()"},{"question":"Create a Python script that utilizes the `cgitb` module to handle and log exceptions with specific customization. Your task is to write a function `custom_exception_handler(logdir, context, format)` that sets up a specialized exception handler using the `cgitb.enable` function. The function should configure `cgitb` to display the traceback in the browser, log the traceback details into a specified directory, and customize the number of lines of context and output format. # Function Signature ```python def custom_exception_handler(logdir: str, context: int = 5, format: str = \'html\'): ``` # Input - `logdir` (str): The directory path where the log files should be saved. - `context` (int): The number of lines of context around the source code in the traceback (default is 5). - `format` (str): The format of the output, either \'html\' or \'text\' (default is \'html\'). # Output - None. The function should just configure the exception handler. # Constraints 1. The `logdir` parameter should be a valid directory path. The function does not need to handle directory creation. 2. The `context` parameter should be a positive integer. 3. The `format` parameter should be either \'html\' or \'text\'. # Requirements 1. Use the `cgitb.enable` function to set up the exception handler. 2. Ensure that the traceback is logged to the specified directory. 3. Adjust the number of lines of context displayed in the traceback based on the input. 4. Set the output format to either \'html\' or \'text\' based on the input. # Example Usage ```python import os def buggy_function(): return 1 / 0 log_directory = os.path.join(os.getcwd(), \'log\') custom_exception_handler(logdir=log_directory, context=10, format=\'text\') try: buggy_function() except: pass ``` In this example, running `buggy_function` should trigger an exception, and the custom exception handler should display the traceback in plain text format, log the details to the specified directory with 10 lines of context around the error source code.","solution":"import cgitb import os def custom_exception_handler(logdir: str, context: int = 5, format: str = \'html\'): Sets up a custom exception handler using the cgitb module. Parameters: logdir (str): The directory path where the log files should be saved. context (int): The number of lines of context around the source code in the traceback. format (str): The format of the output, either \'html\' or \'text\'. if not os.path.isdir(logdir): raise ValueError(f\\"The log directory {logdir} does not exist.\\") if context <= 0: raise ValueError(\\"Context must be a positive integer.\\") if format not in [\'html\', \'text\']: raise ValueError(\\"Format must be either \'html\' or \'text\'.\\") cgitb.enable(logdir=logdir, context=context, format=format)"},{"question":"Objective: You are required to implement a Python program that toggles the terminal mode between `raw` and `cbreak`, and demonstrates the effect on terminal input handling. Task: 1. Implement a function `toggle_terminal_mode(mode: str, fd: int = 0)` that: - Takes a string `mode` which can be either `\'raw\'` or `\'cbreak\'`. - Takes an optional integer `fd` (file descriptor) which by default is `0` (representing standard input). - Changes the terminal mode of the given file descriptor to either `raw` or `cbreak` based on the `mode` parameter using the `tty` module\'s functions. 2. Implement a function `display_terminal_input(fd: int = 0)` that: - Continuously reads and prints input from the terminal until the user enters `\'exit\'`. - Uses the current terminal mode set by the `toggle_terminal_mode` function to demonstrate how input handling changes. Expected Input and Output: - **Input**: The program will not read from standard input directly but will demonstrate changes via terminal input when run interactively. - **Output**: The program should print the characters as they are typed, showing the difference in input handling between `raw` and `cbreak` modes. Constraints: - Make sure to include appropriate error handling. - Ensure the program runs on Unix-based systems as `tty` and `termios` are Unix-specific modules. Example Usage: ```python toggle_terminal_mode(\'raw\') display_terminal_input() # Users can type and see how inputs are handled in raw mode. toggle_terminal_mode(\'cbreak\') display_terminal_input() # Users can type and see how inputs are handled in cbreak mode. ``` Performance Requirements: - The program should efficiently handle continuous input without excessive CPU usage. - The program should correctly switch between raw and cbreak modes and reflect this in its input handling. Note: You may need to run this program in a terminal to fully observe the effects of different terminal modes.","solution":"import tty import termios import sys import os def toggle_terminal_mode(mode: str, fd: int = 0): Toggles the terminal mode between \'raw\' and \'cbreak\'. :param mode: The terminal mode to toggle to. Can be \'raw\' or \'cbreak\'. :param fd: The file descriptor to apply the toggling on. Default is 0 (standard input). assert mode in [\'raw\', \'cbreak\'], \\"Mode should be either \'raw\' or \'cbreak\'\\" if not os.isatty(fd): raise ValueError(f\\"File descriptor {fd} is not a terminal\\") if mode == \'raw\': tty.setraw(fd) else: tty.setcbreak(fd) def display_terminal_input(fd: int = 0): Continuously reads and prints input from the terminal until the user enters \'exit\'. :param fd: The file descriptor to read input from. Default is 0 (standard input). if not os.isatty(fd): raise ValueError(f\\"File descriptor {fd} is not a terminal\\") old_settings = termios.tcgetattr(fd) try: sys.stdout.write(\\"Type \'exit\' to quit.n\\") sys.stdout.flush() while True: ch = sys.stdin.read(1) if ch == \'n\': sys.stdout.write(\'n\') else: sys.stdout.write(f\'You typed: {ch}n\') if ch == \'exit\'[0]: termios.tcsetattr(fd, termios.TCSADRAIN, old_settings) if sys.stdin.read(3) == \'xit\': break sys.stdout.flush() finally: termios.tcsetattr(fd, termios.TCSADRAIN, old_settings)"},{"question":"**Objective**: Write a Python function that uses the `py_compile` module to compile a specified list of Python source files into byte-code files and log the compilation process details. Function Signature ```python def compile_python_files(file_list, cfile_dir=None, log_file=\\"compile_log.txt\\"): Compile a list of Python source files into byte-code files. Parameters: - file_list: List[str] - A list of paths to the Python source files to be compiled. - cfile_dir: str or None - The directory to store the compiled byte-code files. Defaults to None, which means the default py_compile path is used. - log_file: str - The path to the log file where compilation details are written. Defaults to \\"compile_log.txt\\". Returns: - compiled_files: List[str] - A list of paths to the successfully compiled byte-code files. Exceptions: - Raises an exception if any file in the file_list cannot be compiled, stopping further compilation. pass ``` Requirements 1. **Compile the Files**: Iterate through the list of paths in `file_list` and compile each file using `py_compile.compile()`. 2. **Handle cfile Directory**: If `cfile_dir` is specified, store all compiled byte-code files in this directory. Maintain the same file names as the source files. 3. **Logging**: - Create a log file specified by `log_file`. - Log details of the compilation process for each file, including the source file path, the destination byte-code file path, and any errors encountered. 4. **Exceptions Handling**: - If any file cannot be compiled (i.e., a `PyCompileError` is encountered), raise an exception and stop further compilation. - Handle potential `FileExistsError` for non-regular or symlink files. Constraints: - You can assume `file_list` contains valid paths to `.py` files. - The `log_file` should be overwritten with each function call. Example Usage ```python file_list = [\\"example1.py\\", \\"example2.py\\", \\"nonexistent.py\\"] cfile_dir = \\"./compiled\\" log_file = \\"compilation_log.txt\\" try: compiled_files = compile_python_files(file_list, cfile_dir, log_file) print(\\"Compiled files:\\", compiled_files) except Exception as e: print(f\\"Compilation failed: {e}\\") ``` If `example1.py` and `example2.py` compile successfully but `nonexistent.py` fails, the function should raise an exception and log the details.","solution":"import py_compile import os def compile_python_files(file_list, cfile_dir=None, log_file=\\"compile_log.txt\\"): Compile a list of Python source files into byte-code files. Parameters: - file_list: List[str] - A list of paths to the Python source files to be compiled. - cfile_dir: str or None - The directory to store the compiled byte-code files. Defaults to None, which means the default py_compile path is used. - log_file: str - The path to the log file where compilation details are written. Defaults to \\"compile_log.txt\\". Returns: - compiled_files: List[str] - A list of paths to the successfully compiled byte-code files. Exceptions: - Raises an exception if any file in the file_list cannot be compiled, stopping further compilation. compiled_files = [] # Ensure the log_file directory exists log_dir = os.path.dirname(log_file) if log_dir and not os.path.exists(log_dir): os.makedirs(log_dir) # Create or clear the log file with open(log_file, \'w\') as log: log.write(\\"Compilation Logn\\") log.write(\\"====================n\\") try: for file_path in file_list: if cfile_dir: cfile_path = os.path.join(cfile_dir, os.path.basename(file_path) + \'c\') else: cfile_path = None # py_compile will choose the default path try: py_compile.compile(file_path, cfile=cfile_path, doraise=True) compiled_files.append(cfile_path if cfile_path else file_path + \'c\') with open(log_file, \'a\') as log: log.write(f\\"SUCCESS: {file_path} -> {cfile_path if cfile_path else file_path + \'c\'}n\\") except py_compile.PyCompileError as e: with open(log_file, \'a\') as log: log.write(f\\"ERROR: {file_path} -> {e}n\\") raise except Exception as e: raise e return compiled_files"},{"question":"# Advanced Python Coding Assessment: Error Handling with `cgitb` Objective: To assess your understanding and ability to use the `cgitb` module for error handling and producing detailed tracebacks in Python scripts. Problem Statement: You are required to write a Python script that includes multiple functions. These functions will intentionally raise different types of exceptions. Your task is to use the `cgitb` module to handle these exceptions by enabling detailed traceback reports both in HTML and plain text formats. # Function Specifications: 1. **enable_cgitb_tracing()**: - **Description**: Enables `cgitb` tracing with specific settings. - **Parameters**: - `display` (int): A flag (1 to display, 0 to suppress) controlling whether to display the traceback. - `logdir` (str): Directory path where the traceback logs will be saved. If `None`, logs will not be saved. - `context` (int): Number of lines of context to display around the error. - `format` (str): Output format, either \'html\' or \'text\'. - **Returns**: None - **Behavior**: Configures the `cgitb` module with the provided settings. 2. **invoke_exceptions()**: - **Description**: Invokes a set of predefined functions that raise various exceptions. - **Parameters**: None - **Returns**: None - **Behavior**: Calls each predefined function within a `try-except` block and uses `cgitb.handler()` to handle exceptions. 3. **example_functions** (list of predefined functions): - `divide_by_zero()`: Raises a `ZeroDivisionError`. - `index_error()`: Raises an `IndexError`. - `type_error()`: Raises a `TypeError`. # Input/Output Format: - No input or output through stdin or stdout. All operations should be handled internally and exceptions should be logged/displayed as per `cgitb` settings. # Constraints: 1. You must use the `cgitb` module to handle exceptions. 2. The HTML and plain text formatted tracebacks should correctly display the error context. 3. Ensure the script can run in an environment where directory permissions for logging tracebacks are handled appropriately. # Example: ```python import os import cgitb def enable_cgitb_tracing(display=1, logdir=None, context=5, format=\'html\'): cgitb.enable(display=display, logdir=logdir, context=context, format=format) def divide_by_zero(): return 1 / 0 def index_error(): lst = [1, 2, 3] return lst[5] def type_error(): return \'string\' + 5 def invoke_exceptions(): functions = [divide_by_zero, index_error, type_error] for func in functions: try: func() except Exception: cgitb.handler() # Example usage: enable_cgitb_tracing(display=1, logdir=os.path.expanduser(\\"~/traceback_logs\\"), context=3, format=\'html\') invoke_exceptions() ``` Ensure to test the script with appropriate directory paths and observe the generated logs and display outputs. Your final implementation should allow easy switching between HTML and plain text outputs and provide adequate context around the errors. Notes: - Consider edge cases where the logging directories might not have the required permissions and handle such scenarios gracefully. - The example given just shows a usage pattern; you need to implement `enable_cgitb_tracing()` and `invoke_exceptions()` functions as per the specifications.","solution":"import os import cgitb def enable_cgitb_tracing(display=1, logdir=None, context=5, format=\'html\'): Enables cgitb tracing with the specified settings. Parameters: - display (int): A flag (1 to display, 0 to suppress) controlling whether to display the traceback. - logdir (str): Directory path where the traceback logs will be saved. If None, logs will not be saved. - context (int): Number of lines of context to display around the error. - format (str): Output format, either \'html\' or \'text\'. Returns: - None cgitb.enable(display=display, logdir=logdir, context=context, format=format) def divide_by_zero(): Function that raises ZeroDivisionError. return 1 / 0 def index_error(): Function that raises IndexError. lst = [1, 2, 3] return lst[5] def type_error(): Function that raises TypeError. return \'string\' + 5 def invoke_exceptions(): Invokes a set of predefined functions that raise various exceptions. Calls each predefined function within a try-except block and uses cgitb.handler() to handle exceptions. Returns: - None functions = [divide_by_zero, index_error, type_error] for func in functions: try: func() except Exception: cgitb.handler()"},{"question":"**Problem Statement:** You are given an XML document containing information about various products, including their names, prices, and categories. Your task is to write a Python function using the `xml.dom.pulldom` module that parses the XML document, identifies products priced above a given threshold, and returns these products in a specific format. # Function Signature ```python def filter_expensive_products(xml_string: str, price_threshold: int) -> List[Dict[str, str]]: pass ``` # Input The function takes in two arguments: 1. `xml_string` (str): A string containing the XML data. 2. `price_threshold` (int): The price threshold above which products should be selected. # Output The function should return a list of dictionaries, each representing a product that costs more than the specified threshold. The dictionary keys should be \'name\', \'price\', and \'category\'. # Example ```python xml_data = \'\'\' <products> <product> <name>Product A</name> <price>30</price> <category>Category 1</category> </product> <product> <name>Product B</name> <price>60</price> <category>Category 2</category> </product> <product> <name>Product C</name> <price>80</price> <category>Category 3</category> </product> </products> \'\'\' price_threshold = 50 # The expected output should be: # [ # {\\"name\\": \\"Product B\\", \\"price\\": \\"60\\", \\"category\\": \\"Category 2\\"}, # {\\"name\\": \\"Product C\\", \\"price\\": \\"80\\", \\"category\\": \\"Category 3\\"} # ] print(filter_expensive_products(xml_data, price_threshold)) ``` # Constraints - Assume the XML string is always well-formed. - The function should efficiently filter and process the XML, minimizing memory usage by not fully expanding the DOM tree unless necessary. - Use the `xml.dom.pulldom` module to demonstrate proficiency with its event-driven parsing capabilities. # Guidelines - Parse the XML content using `xml.dom.pulldom.parseString()`. - Loop through the events and process each `START_ELEMENT` for `product`. - For product elements, extract the `name`, `price`, and `category` values. - Filter the products based on the `price_threshold` and collect the details of those that meet the criteria. - Return the results in the specified format.","solution":"from typing import List, Dict from xml.dom.pulldom import parseString, START_ELEMENT, END_ELEMENT, CHARACTERS def filter_expensive_products(xml_string: str, price_threshold: int) -> List[Dict[str, str]]: Parses an XML string and filters products with price above a given threshold. Args: xml_string (str): The XML data containing product information. price_threshold (int): The price threshold for filtering products. Returns: List[Dict[str, str]]: A list of dictionaries containing \'name\', \'price\', and \'category\' of products that cost more than the price_threshold. doc = parseString(xml_string) products = [] capture_text = False current_product = {} for event, node in doc: if event == START_ELEMENT and node.tagName == \'product\': doc.expandNode(node) for child in node.childNodes: if child.nodeType == child.ELEMENT_NODE: if child.tagName == \'name\': current_product[\'name\'] = child.firstChild.data elif child.tagName == \'price\': current_product[\'price\'] = child.firstChild.data elif child.tagName == \'category\': current_product[\'category\'] = child.firstChild.data if int(current_product[\'price\']) > price_threshold: products.append(current_product.copy()) # Copy to avoid modifying the dict inadvertently current_product.clear() return products"},{"question":"# **Iterator Protocol Implementation in Python** In this task, you are required to write Python functions that emulate certain behaviors of the iterator protocol functions described in the Python/C API documentation. Your implementations should work with Python iterators and generators. Function 1: `is_iterator(obj)` Implement a function `is_iterator(obj)` that returns `True` if `obj` is an iterator and `False` otherwise. This function should emulate the behavior of `PyIter_Check`. # Input - `obj`: Any Python object. # Output - Returns `True` if `obj` is an iterator, `False` otherwise. # Example ```python print(is_iterator(iter([1, 2, 3]))) # True print(is_iterator([1, 2, 3])) # False ``` Function 2: `is_async_iterator(obj)` Implement a function `is_async_iterator(obj)` that returns `True` if `obj` is an asynchronous iterator and `False` otherwise. This function should emulate the behavior of `PyAIter_Check`. # Input - `obj`: Any Python object. # Output - Returns `True` if `obj` is an asynchronous iterator, `False` otherwise. # Example ```python import collections.abc class AsyncIter: def __aiter__(self): return self async def __anext__(self): raise StopAsyncIteration print(is_async_iterator(AsyncIter())) # True print(is_async_iterator(iter([1, 2, 3]))) # False ``` Function 3: `next_item(iterator)` Implement a function `next_item(iterator)` that returns the next item from the iterator. This function should emulate the behavior of `PyIter_Next`. # Input - `iterator`: A Python iterator object. # Output - Returns the next item from the iterator if available. - Returns `None` if there are no remaining values. # Example ```python itr = iter([1, 2, 3]) print(next_item(itr)) # 1 print(next_item(itr)) # 2 print(next_item(itr)) # 3 print(next_item(itr)) # None ``` Function 4: `send_to_generator(generator, value)` Implement a function `send_to_generator(generator, value)` that sends a value into the generator and handles the different generator states. This function should emulate the behavior of `PyIter_Send`. # Input - `generator`: A Python generator object. - `value`: A value to send into the generator. # Output - Returns a tuple `(state, result)` where: - `state` is a string indicating the generator\'s state (`\\"RETURN\\"`, `\\"NEXT\\"`, `\\"ERROR\\"`). - `result` is the value yielded or returned by the generator, or `None` if an error occurs. # Example ```python def test_generator(): yield 1 yield 2 return 3 gen = test_generator() print(send_to_generator(gen, None)) # (\'NEXT\', 1) print(send_to_generator(gen, None)) # (\'NEXT\', 2) print(send_to_generator(gen, None)) # (\'RETURN\', 3) ``` Ensure your functions handle all edge cases and constraints effectively. Write the implementations of the four functions described above.","solution":"def is_iterator(obj): Returns True if obj is an iterator, False otherwise. return hasattr(obj, \'__iter__\') and hasattr(obj, \'__next__\') def is_async_iterator(obj): Returns True if obj is an asynchronous iterator, False otherwise. return hasattr(obj, \'__aiter__\') and hasattr(obj, \'__anext__\') def next_item(iterator): Returns the next item from the iterator if available, else None. try: return next(iterator) except StopIteration: return None def send_to_generator(generator, value): Sends a value into the generator and returns a tuple (state, result) where: - state is a string indicating the generator\'s state (\\"RETURN\\", \\"NEXT\\", \\"ERROR\\"). - result is the value yielded or returned by the generator, or None if an error occurs. try: result = generator.send(value) return (\\"NEXT\\", result) except StopIteration as e: return (\\"RETURN\\", e.value) except Exception as e: return (\\"ERROR\\", None)"},{"question":"# AsyncIO Task Management and Debugging **Objective**: Your task is to implement a small program using the `asyncio` framework that demonstrates handling asynchronous tasks, managing concurrency, and enabling debugging to identify common pitfalls. **Problem Statement**: Implement a Python program that handles multiple asynchronous tasks. Your program should include the following functionalities: 1. **Task Functions**: Define two coroutine functions: - `fetch_data(delay: int)`: Simulates fetching data by sleeping for the given delay (simulate I/O-bound task). - `process_data(data: int)`: Simulates processing data by squaring the given data (simulate CPU-bound task). 2. **Main Function**: Implement a main coroutine function `main()` that: - Creates a list of integers from 1 to 5. - Uses these integers to fetch data concurrently using `fetch_data()`. - Processes each piece of fetched data using `process_data()` concurrently, ensuring not to block the event loop. - Prints the result of each processed data. 3. **Concurrency and Multithreading**: - Use the `loop.run_in_executor()` method to run `process_data()` so it does not block the event loop. 4. **Debugging**: - Enable `asyncio` debugging mode. - Configure the `asyncio` logger to log at the DEBUG level. - Ensure all coroutines are properly awaited and there are no unhandled exceptions. **Input**: None **Output**: The program should print the processed data results. **Constraints**: - Ensure proper management of coroutines and tasks. - Avoid blocking the event loop with CPU-bound tasks. **Example**: ```python import asyncio import logging from concurrent.futures import ThreadPoolExecutor async def fetch_data(delay: int): await asyncio.sleep(delay) return delay def process_data(data: int): # Simulate a CPU-bound task return data * data def main(): logging.basicConfig(level=logging.DEBUG) asyncio.run(run_async_tasks(), debug=True) async def run_async_tasks(): data_list = [1, 2, 3, 4, 5] fetched_results = await asyncio.gather(*[fetch_data(data) for data in data_list]) loop = asyncio.get_event_loop() with ThreadPoolExecutor() as pool: processed_results = await asyncio.gather( *[loop.run_in_executor(pool, process_data, data) for data in fetched_results] ) for result in processed_results: print(result) if __name__ == \\"__main__\\": main() ``` **Note**: Ensure that all tasks are scheduled and awaited correctly, use proper logging and debugging mechanisms, and handle any potential unhandled exceptions that might arise.","solution":"import asyncio import logging from concurrent.futures import ThreadPoolExecutor async def fetch_data(delay: int): Simulates fetching data by sleeping for the given delay. await asyncio.sleep(delay) return delay def process_data(data: int): Simulates processing data by squaring the given data. return data * data async def run_async_tasks(): Runs the async tasks: fetches and processes data concurrently. data_list = [1, 2, 3, 4, 5] fetched_results = await asyncio.gather(*[fetch_data(data) for data in data_list]) loop = asyncio.get_event_loop() with ThreadPoolExecutor() as pool: processed_results = await asyncio.gather( *[loop.run_in_executor(pool, process_data, data) for data in fetched_results] ) for result in processed_results: print(result) return processed_results def main(): logging.basicConfig(level=logging.DEBUG) asyncio.run(run_async_tasks(), debug=True) if __name__ == \\"__main__\\": main()"},{"question":"**Question: Managing CUDA Operations in PyTorch Using Environment Variables** You are provided with a PyTorch script that performs matrix multiplications on a CUDA-enabled device. Your task is to modify the script to: 1. Ensure that no GPU devices are made available to the CUDA runtime. 2. Enable synchronous CUDA calls for better debugging. 3. Set the cuBLAS workspace size per allocation to avoid using workspaces. 4. Disable caching of memory allocations in CUDA. Implement these configurations programmatically (i.e., within the Python script, not by setting them in the operating system environment). # PyTorch Script ```python import torch # Assuming CUDA is available, allocate two matrices on the GPU A = torch.randn(1000, 1000, device=\'cuda\') B = torch.randn(1000, 1000, device=\'cuda\') # Perform matrix multiplication C = torch.matmul(A, B) print(\\"Matrix multiplication result:\\", C) ``` # Requirements 1. You must ensure that no GPU devices are available. 2. Enable synchronous CUDA calls (`CUDA_LAUNCH_BLOCKING=1`). 3. Set the cuBLAS workspace configuration to avoid using workspaces (`CUBLAS_WORKSPACE_CONFIG=:0:0`). 4. Disable caching of memory allocations in CUDA (`PYTORCH_NO_CUDA_MEMORY_CACHING=1`). # Expected Output The script should run without performing any actual computation on the GPU, and all CUDA calls should be synchronous. Any attempt to perform operations that require a GPU should raise an appropriate error due to the absence of available GPU devices. # Input/Output Format - **Input:** None - **Output:** Printed matrix multiplication result or an error message if CUDA operations fail. # Constraints - You should only make use of PyTorch and standard Python libraries. - Ensure that your solution is executable as a standalone script. # Performance Requirements - The script should handle the configuration changes efficiently and gracefully handle cases where no GPU is available. # Implementation Modify the provided script with the necessary changes to fulfill the requirements described above.","solution":"import os import torch def configure_cuda_env(): Configures the environment variables for CUDA operations as per the task requirements. # Ensuring no GPU devices are available os.environ[\\"CUDA_VISIBLE_DEVICES\\"] = \\"\\" # Enabling synchronous CUDA calls os.environ[\\"CUDA_LAUNCH_BLOCKING\\"] = \\"1\\" # Setting the cuBLAS workspace configuration os.environ[\\"CUBLAS_WORKSPACE_CONFIG\\"] = \\":0:0\\" # Disabling caching of memory allocations in CUDA os.environ[\\"PYTORCH_NO_CUDA_MEMORY_CACHING\\"] = \\"1\\" def perform_matrix_multiplication(): Performs matrix multiplication using CUDA if available. configure_cuda_env() # Check if CUDA is available post configuration if torch.cuda.is_available(): A = torch.randn(1000, 1000, device=\'cuda\') B = torch.randn(1000, 1000, device=\'cuda\') C = torch.matmul(A, B) print(\\"Matrix multiplication result:\\", C) else: print(\\"CUDA is not available. Operations can\'t be performed on the GPU.\\") if __name__ == \\"__main__\\": perform_matrix_multiplication()"},{"question":"# Python Iterator Protocol Assessment You are tasked with implementing functions that utilize the Python iterator protocol to handle both synchronous and asynchronous iterators. The goal is to create reliable tools that interact with iterators effectively, using the functions provided in the Python 3.10 API. Part 1: Synchronous Iterators Implement two functions: `is_iterator(obj)` and `iterate_all(obj)`. 1. `is_iterator(obj)`: - **Input**: A single variable `obj`. - **Output**: Returns `True` if `obj` is a valid iterator according to the `PyIter_Check()` criteria, otherwise returns `False`. 2. `iterate_all(obj)`: - **Input**: A single variable `obj` which should be a valid iterator. - **Output**: Returns a list containing all elements retrieved by iterating over the iterator `obj` using `PyIter_Next()`. If an error occurs during iteration, raise an appropriate exception. Part 2: Asynchronous Iterators Implement two functions: `is_async_iterator(obj)` and `async_iterate_all(obj, arg)`. 3. `is_async_iterator(obj)`: - **Input**: A single variable `obj`. - **Output**: Returns `True` if `obj` is a valid asynchronous iterator according to the `PyAIter_Check()` criteria, otherwise returns `False`. 4. `async_iterate_all(obj, arg)`: - **Input**: A valid asynchronous iterator `obj` and an argument `arg` to be sent into the iterator. - **Output**: Returns a list containing all values yielded by the asynchronous iterator `obj` using `PyIter_Send()` with `arg`. If the iterator returns, the function should terminate and return the collected values. If an exception is raised, handle it gracefully and stop iteration. Additional Constraints and Notes: - Assume that `PyIter_Check()` and `PyAIter_Check()` are available in your environment and work as described in the documentation. - Implement appropriate error handling and clean-up for each function. - For the purpose of this assessment, do not use Python\'s built-in `iter()` or `next()` functions in the implementations. Example Usage ```python # For synchronous iterators iter_obj = iter([1, 2, 3]) print(is_iterator(iter_obj)) # Output: True print(iterate_all(iter_obj)) # Output: [1, 2, 3] # For asynchronous iterators class AsyncIterator: async def __aiter__(self): return self async def __anext__(self): if self.index >= len(self.values): raise StopAsyncIteration value = self.values[self.index] self.index += 1 return value async_iter_obj = AsyncIterator([\\"a\\", \\"b\\", \\"c\\"]) print(is_async_iterator(async_iter_obj)) # Output: True print(async_iterate_all(async_iter_obj, None)) # Output: [\\"a\\", \\"b\\", \\"c\\"] ```","solution":"def is_iterator(obj): Check if the obj is a valid iterator. return hasattr(obj, \'__iter__\') and hasattr(obj, \'__next__\') def iterate_all(obj): Returns a list of all elements in the iterator obj. if not is_iterator(obj): raise TypeError(\\"Object is not an iterator\\") items = [] try: while True: items.append(obj.__next__()) except StopIteration: pass return items import inspect def is_async_iterator(obj): Check if the obj is a valid asynchronous iterator. return ( hasattr(obj, \'__aiter__\') and inspect.isasyncgen(obj.__aiter__()) ) async def async_iterate_all(obj, arg): Returns a list of all elements in the asynchronous iterator obj. if not is_async_iterator(obj): raise TypeError(\\"Object is not an asynchronous iterator\\") items = [] async for item in obj: items.append(item) return items"},{"question":"You are given a list of tuples representing products in a store. Each tuple contains the product name, its price, and its quantity in stock, e.g., `(\\"apple\\", 1.2, 10)`. Implement a function `manage_store_operations` that takes this list and performs specific operations based on given commands. The function should support the following operations: 1. **Filter products by a minimum stock quantity** - Return a list of products where the stock quantity is greater than or equal to a specified amount. 2. **Update stock quantity** - Update the stock quantity of a specified product by adding a given amount. 3. **Sort products by price** - Return the list of products sorted by their prices in ascending order. 4. **Find product by name** - Return the tuple for a specified product name. The function should have the following signature: ```python from typing import List, Tuple, Union import operator Product = Tuple[str, float, int] def manage_store_operations(products: List[Product], command: str, *args) -> Union[List[Product], Product, None]: pass ``` # Examples ```python products = [ (\\"apple\\", 1.2, 10), (\\"banana\\", 0.5, 20), (\\"cherry\\", 2.5, 5), (\\"date\\", 3.0, 15) ] # Example 1: Filter products by minimum stock of 10 result = manage_store_operations(products, \\"filter_by_stock\\", 10) print(result) # Output: [(\'apple\', 1.2, 10), (\'banana\', 0.5, 20), (\'date\', 3.0, 15)] # Example 2: Update stock quantity of \\"apple\\" by 5 manage_store_operations(products, \\"update_stock\\", \\"apple\\", 5) # No output, but products list updated to include (\'apple\', 1.2, 15) instead of (\'apple\', 1.2, 10) # Example 3: Sort products by price result = manage_store_operations(products, \\"sort_by_price\\") print(result) # Output: [(\'banana\', 0.5, 20), (\'apple\', 1.2, 15), (\'cherry\', 2.5, 5), (\'date\', 3.0, 15)] # Example 4: Find product by name \\"date\\" result = manage_store_operations(products, \\"find_by_name\\", \\"date\\") print(result) # Output: (\'date\', 3.0, 15) ``` # Implementation Requirements 1. Use functions from the `operator` module for logical and comparison operations. 2. Ensure the solution handles cases where products are not found or invalid commands are given. # Constraints - The `products` list will have a maximum length of 1000 items. - Each product name is unique. - Stock quantity and prices will be non-negative.","solution":"from typing import List, Tuple, Union import operator Product = Tuple[str, float, int] def manage_store_operations(products: List[Product], command: str, *args) -> Union[List[Product], Product, None]: if command == \\"filter_by_stock\\": min_stock = args[0] return [product for product in products if product[2] >= min_stock] elif command == \\"update_stock\\": product_name = args[0] stock_increase = args[1] for i, product in enumerate(products): if product[0] == product_name: products[i] = (product[0], product[1], product[2] + stock_increase) return elif command == \\"sort_by_price\\": return sorted(products, key=operator.itemgetter(1)) elif command == \\"find_by_name\\": product_name = args[0] for product in products: if product[0] == product_name: return product else: raise ValueError(\\"Invalid command\\")"},{"question":"You are given the `penguins` dataset from Seaborn\'s built-in datasets. Your task is to create a panel of histograms that visualize the distribution of `bill_length_mm` for different `islands`. Additionally, you need to customize the legends for these histograms, ensuring they are both informative and aesthetically pleasing. # Requirements: 1. **Load the \'penguins\' dataset** using Seaborn. 2. **Create a grid** of histograms (`histplot`) showing the distribution of `bill_length_mm` for each island separately. Each subplot should be for a different island. 3. **Customize the legends**: - For subplots, the legend should be moved to the `upper left` position inside the axis. - Use the `bbox_to_anchor` to finely adjust the legend position if necessary. - Set the legend title to `Species` - Align legends within the same row for better readability. # Constraints: - Use Seaborn\'s `displot` for creating the grid of plots. - Ensure the code is efficient and avoids unnecessary computation. - The legend should not overlap with the histograms. # Input: No input from the user is required. # Output: A Seaborn FacetGrid object displaying the histograms with the customized legends, as specified. # Example: ```python import seaborn as sns import matplotlib.pyplot as plt def custom_histograms_with_legends(): # Load the \'penguins\' dataset penguins = sns.load_dataset(\\"penguins\\") # Create a displot grid of histograms for \'bill_length_mm\' by \'island\' g = sns.displot( data=penguins, x=\\"bill_length_mm\\", hue=\\"species\\", col=\\"island\\", col_wrap=2, height=3, facet_kws=dict(legend_out=False), kde=True # Add KDE for better visualization ) # Move the legend to the upper left position within each axis sns.move_legend( g, \\"upper left\\", bbox_to_anchor=(.95, .95), title=\\"Species\\", frameon=False ) # Adjust layout for better spacing plt.tight_layout() # Run the function custom_histograms_with_legends() ``` # Note: Please include all necessary imports in your function, and ensure that your visualization is both informative and visually appealing.","solution":"import seaborn as sns import matplotlib.pyplot as plt def custom_histograms_with_legends(): # Load the \'penguins\' dataset penguins = sns.load_dataset(\\"penguins\\") # Create a displot grid of histograms for \'bill_length_mm\' by \'island\' g = sns.displot( data=penguins, x=\\"bill_length_mm\\", hue=\\"species\\", col=\\"island\\", col_wrap=2, height=3, facet_kws=dict(legend_out=False), kde=True # Add KDE for better visualization ) for ax in g.axes.flat: handles, labels = ax.get_legend_handles_labels() ax.legend(handles, labels, loc=\'upper left\', bbox_to_anchor=(0.95, 0.95), title=\\"Species\\") # Adjust layout for better spacing plt.tight_layout() return g"},{"question":"**Question: Implement a Custom Sequence Type** Python provides several protocols for objects to interact with various operations. In this task, you are required to implement a custom sequence type called `CustomSequence` that mimics the behavior of Python\'s built-in list with a few additional constraints and functionalities. # Objectives: 1. Implement a class `CustomSequence` with the following methods: - `__init__(self, data: list)`: Initialize the sequence with a list of numerical elements. - `__len__(self) -> int`: Return the length of the sequence. - `__getitem__(self, index: int) -> int`: Retrieve an item from the sequence at the specified index. - `__setitem__(self, index: int, value: int)`: Set the value of an item at the specified index. - `__delitem__(self, index: int)`: Delete the item at the specified index. - `__iter__(self)`: Return an iterator object for the sequence. - `__contains__(self, item: int) -> bool`: Check if the item exists in the sequence. - `append(self, item: int)`: Append an item to the end of the sequence. 2. Ensure that the sequence only holds numerical elements (integers and floats). Raise a `TypeError` if an attempt is made to insert a non-numerical element. 3. The sequence should maintain its order as a list does. 4. Implement error handling for the following scenarios: - Index out of range. - Inserting a non-numerical value. # Constraints: - Elements of the `CustomSequence` should only be integers or floats. - Performance should be similar to the built-in list operations for append, item retrieval, etc. # Example: ```python seq = CustomSequence([1, 2, 3]) print(len(seq)) # Output: 3 print(seq[1]) # Output: 2 seq.append(4) print(seq) # Output: CustomSequence([1, 2, 3, 4]) seq[1] = 10 print(seq) # Output: CustomSequence([1, 10, 3, 4]) del seq[2] print(seq) # Output: CustomSequence([1, 10, 4]) print(10 in seq) # Output: True print(5 in seq) # Output: False try: seq.append(\\"a\\") # Should raise TypeError except TypeError as e: print(e) # Output: Only numeric values are allowed ```","solution":"class CustomSequence: def __init__(self, data: list): Initialize the sequence with a list of numerical elements. if not all(isinstance(x, (int, float)) for x in data): raise TypeError(\\"Only numeric values are allowed\\") self.data = data def __len__(self) -> int: Return the length of the sequence. return len(self.data) def __getitem__(self, index: int): Retrieve an item from the sequence at the specified index. if index < 0 or index >= len(self.data): raise IndexError(\\"Index out of range\\") return self.data[index] def __setitem__(self, index: int, value: int): Set the value of an item at the specified index. if not isinstance(value, (int, float)): raise TypeError(\\"Only numeric values are allowed\\") if index < 0 or index >= len(self.data): raise IndexError(\\"Index out of range\\") self.data[index] = value def __delitem__(self, index: int): Delete the item at the specified index. if index < 0 or index >= len(self.data): raise IndexError(\\"Index out of range\\") del self.data[index] def __iter__(self): Return an iterator object for the sequence. return iter(self.data) def __contains__(self, item: int) -> bool: Check if the item exists in the sequence. if not isinstance(item, (int, float)): raise TypeError(\\"Only numeric values are allowed\\") return item in self.data def append(self, item: int): Append an item to the end of the sequence. if not isinstance(item, (int, float)): raise TypeError(\\"Only numeric values are allowed\\") self.data.append(item) def __repr__(self): Return the string representation of the sequence. return f\\"CustomSequence({self.data})\\""},{"question":"# Secure Password Storage and Authentication **Objective:** Implement a secure password storage mechanism using the `crypt` module in Python. Your solution should be able to hash passwords, store them, and validate a user\'s password attempts against the stored hash. The solution should use the strongest available hashing method and provide an option to set the number of hashing rounds for methods that support it. **Input and Output Formats:** 1. **Function: hash_password** - **Input:** - `password` (str): The plaintext password to be hashed. - `rounds` (int, optional): The number of hashing rounds (default is `None`). - **Output:** - `hashed_password` (str): The hashed password string. 2. **Function: store_password** - **Input:** - `username` (str): The username associated with the password. - `hashed_password` (str): The hashed password to be stored. - **Output:** - None 3. **Function: validate_password** - **Input:** - `username` (str): The username for which to validate the password. - `password` (str): The plaintext password attempt. - **Output:** - `is_valid` (bool): `True` if the password is correct, `False` otherwise. **Constraints:** - Use the `crypt` module\'s strongest available method for hashing. - Ensure that the number of rounds is within the allowed range for the chosen method. - Use a constant-time comparison (`hmac.compare_digest`) for validating passwords to prevent timing attacks. **Helper Function:** - You may create additional helper functions as needed to manage user data storage. **Performance Requirements:** - The solution should handle multiple user authentications efficiently. - Password hashing should be secure, leveraging multiple rounds for additional security. **Example Usage:** ```python import crypt import hmac import os def hash_password(password: str, rounds: int = None) -> str: method = crypt.methods[0] # Use the strongest available method salt = crypt.mksalt(method, rounds=rounds) return crypt.crypt(password, salt) def store_password(username: str, hashed_password: str) -> None: with open(f\\"{username}.pwd\\", \\"w\\") as f: f.write(hashed_password) def validate_password(username: str, password: str) -> bool: try: with open(f\\"{username}.pwd\\", \\"r\\") as f: stored_hash = f.read().strip() return hmac.compare_digest(crypt.crypt(password, stored_hash), stored_hash) except FileNotFoundError: return False # Example usage hashed_pwd = hash_password(\\"my_secret_password\\", rounds=5000) store_password(\\"john_doe\\", hashed_pwd) print(validate_password(\\"john_doe\\", \\"my_secret_password\\")) # Output: True print(validate_password(\\"john_doe\\", \\"wrong_password\\")) # Output: False ``` Note: Be sure to handle the file storage securely and consider using a proper database for real-world applications.","solution":"import crypt import hmac import os def hash_password(password: str, rounds: int = None) -> str: # Use the strongest available method method = crypt.methods[0] salt = crypt.mksalt(method, rounds=rounds) return crypt.crypt(password, salt) def store_password(username: str, hashed_password: str) -> None: # Store the hashed password in a file named after the username with open(f\\"{username}.pwd\\", \\"w\\") as file: file.write(hashed_password) def validate_password(username: str, password: str) -> bool: try: # Read the stored hashed password from the file with open(f\\"{username}.pwd\\", \\"r\\") as file: stored_hash = file.read().strip() # Validate the password return hmac.compare_digest(crypt.crypt(password, stored_hash), stored_hash) except FileNotFoundError: return False"},{"question":"**Custom Interactive Shell for Task Management** You have been tasked with creating a custom command-line shell application to manage tasks. This interactive shell should allow users to add, list, mark as done, and delete tasks. Using the `cmd` module, you will implement these functionalities in a class named `TaskShell`, which inherits from `cmd.Cmd`. # Requirements 1. **TaskShell Class**: * Inherit from `cmd.Cmd`. * Define the prompt as `\'(task) \'`. * Store tasks in an internal list. Each task can be a dictionary with keys `\'id\'`, `\'description\'`, and `\'done\'`. 2. **Commands**: - **add**: - Syntax: `ADD <task_description>` - Adds a new task with the given description. Tasks should have a unique ID. - **list**: - Syntax: `LIST` - Lists all tasks, showing their ID, description, and done status. - **done**: - Syntax: `DONE <task_id>` - Marks the task with the given ID as done. - **delete**: - Syntax: `DELETE <task_id>` - Deletes the task with the given ID. - **bye**: - Syntax: `BYE` - Exits the task shell. 3. **Functionality**: - Commands should be case-insensitive. - Validations should be in place to handle invalid task IDs and other erroneous inputs gracefully. - The `TaskShell` should provide appropriate feedback on each command execution. # Implementation ```python import cmd class TaskShell(cmd.Cmd): prompt = \'(task) \' def __init__(self): super().__init__() self.tasks = [] self.next_id = 1 def do_add(self, arg): \'Add a new task: ADD <task_description>\' if not arg.strip(): print(\\"Error: Task description cannot be empty.\\") else: task = {\'id\': self.next_id, \'description\': arg.strip(), \'done\': False} self.tasks.append(task) print(f\\"Task added with ID {self.next_id}.\\") self.next_id += 1 def do_list(self, arg): \'List all tasks: LIST\' if not self.tasks: print(\\"No tasks available.\\") else: for task in self.tasks: status = \\"Done\\" if task[\'done\'] else \\"Pending\\" print(f\\"ID: {task[\'id\']} Description: {task[\'description\']} Status: {status}\\") def do_done(self, arg): \'Mark task as done: DONE <task_id>\' try: task_id = int(arg.strip()) task = next((t for t in self.tasks if t[\'id\'] == task_id), None) if task: task[\'done\'] = True print(f\\"Task ID {task_id} marked as done.\\") else: print(f\\"Error: No task found with ID {task_id}.\\") except ValueError: print(\\"Error: Task ID must be a valid integer.\\") def do_delete(self, arg): \'Delete a task: DELETE <task_id>\' try: task_id = int(arg.strip()) task = next((t for t in self.tasks if t[\'id\'] == task_id), None) if task: self.tasks.remove(task) print(f\\"Task ID {task_id} deleted.\\") else: print(f\\"Error: No task found with ID {task_id}.\\") except ValueError: print(\\"Error: Task ID must be a valid integer.\\") def do_bye(self, arg): \'Exit the task shell: BYE\' print(\\"Exiting the Task Shell. Goodbye!\\") return True if __name__ == \'__main__\': TaskShell().cmdloop() ``` # Constraints - Ensure `task_id` is a positive integer. - Task descriptions cannot be empty. # Example Usage ``` Welcome to the Task Shell. Type help or ? to list commands. (task) add Implement the task shell Task added with ID 1. (task) list ID: 1 Description: Implement the task shell Status: Pending (task) done 1 Task ID 1 marked as done. (task) list ID: 1 Description: Implement the task shell Status: Done (task) delete 1 Task ID 1 deleted. (task) list No tasks available. (task) bye Exiting the Task Shell. Goodbye! ``` # Evaluation Criteria - Correct implementation of the `TaskShell` class. - Proper handling of edge cases and invalid inputs. - Clear and user-friendly feedback in the command-line interface.","solution":"import cmd class TaskShell(cmd.Cmd): prompt = \'(task) \' def __init__(self): super().__init__() self.tasks = [] self.next_id = 1 def do_ADD(self, arg): \'Add a new task: ADD <task_description>\' if not arg.strip(): print(\\"Error: Task description cannot be empty.\\") else: task = {\'id\': self.next_id, \'description\': arg.strip(), \'done\': False} self.tasks.append(task) print(f\\"Task added with ID {self.next_id}.\\") self.next_id += 1 def do_LIST(self, arg): \'List all tasks: LIST\' if not self.tasks: print(\\"No tasks available.\\") else: for task in self.tasks: status = \\"Done\\" if task[\'done\'] else \\"Pending\\" print(f\\"ID: {task[\'id\']} Description: {task[\'description\']} Status: {status}\\") def do_DONE(self, arg): \'Mark task as done: DONE <task_id>\' try: task_id = int(arg.strip()) task = next((t for t in self.tasks if t[\'id\'] == task_id), None) if task: task[\'done\'] = True print(f\\"Task ID {task_id} marked as done.\\") else: print(f\\"Error: No task found with ID {task_id}.\\") except ValueError: print(\\"Error: Task ID must be a valid integer.\\") def do_DELETE(self, arg): \'Delete a task: DELETE <task_id>\' try: task_id = int(arg.strip()) task = next((t for t in self.tasks if t[\'id\'] == task_id), None) if task: self.tasks.remove(task) print(f\\"Task ID {task_id} deleted.\\") else: print(f\\"Error: No task found with ID {task_id}.\\") except ValueError: print(\\"Error: Task ID must be a valid integer.\\") def do_BYE(self, arg): \'Exit the task shell: BYE\' print(\\"Exiting the Task Shell. Goodbye!\\") return True if __name__ == \'__main__\': TaskShell().cmdloop()"},{"question":"# Filename Matcher **Objective**: Implement a function to filter and categorize filenames based on multiple patterns. **Function Signature**: ```python def categorize_files(filenames: list, patterns: list) -> dict: pass ``` **Input**: - `filenames` (list of str): A list containing filenames as strings. - `patterns` (list of tuple): Each tuple contains two elements: - A pattern (str): Shell-style pattern string (e.g., \'*.txt\', \'data_?.csv\'). - A category name (str): A string representing the category name for files matching the given pattern. **Output**: - A dictionary where the keys are category names, and the values are lists of filenames that match the corresponding pattern. Each filename should only appear once and be categorized under the first matching pattern, if multiple patterns match. **Constraints**: - The length of `filenames` does not exceed 10^3. - The length of `patterns` does not exceed 10^2. - Filenames and patterns have a maximum length of 100 characters. **Example**: ```python filenames = [\\"document1.txt\\", \\"image1.png\\", \\"image2.jpg\\", \\"notes.docx\\", \\"data_1.csv\\", \\"data_2.csv\\"] patterns = [(\\"*.txt\\", \\"Text Files\\"), (\\"*.png\\", \\"Images\\"), (\\"*.jpg\\", \\"Images\\"), (\\"data_?.csv\\", \\"CSV Data\\")] output = categorize_files(filenames, patterns) print(output) # Expected Output: # { # \\"Text Files\\": [\\"document1.txt\\"], # \\"Images\\": [\\"image1.png\\", \\"image2.jpg\\"], # \\"CSV Data\\": [\\"data_1.csv\\", \\"data_2.csv\\"] # } ``` **Explanation**: - The function should filter and categorize filenames based on the provided patterns. - \\"document1.txt\\" matches \\"*.txt\\" and is categorized under \\"Text Files\\". - \\"image1.png\\" and \\"image2.jpg\\" match \\"*.png\\" and \\"*.jpg\\" respectively and are categorized under \\"Images\\". - \\"data_1.csv\\" and \\"data_2.csv\\" match \\"data_?.csv\\" and are categorized under \\"CSV Data\\". **Requirements**: - You must use the `fnmatch` library functions to match patterns. - Ensure that each file is categorized under the first matching pattern in the list. Implement the function to achieve the expected behavior as described above.","solution":"from fnmatch import fnmatch def categorize_files(filenames: list, patterns: list) -> dict: Categorizes files based on the given patterns. Parameters: filenames (list): A list of filenames as strings. patterns (list): A list of tuples where each tuple contains a pattern (str) and a category name (str). Returns: dict: A dictionary where keys are category names and values are lists of filenames. categorized = {} categorized_files = set() for pattern, category in patterns: for filename in filenames: if fnmatch(filename, pattern) and filename not in categorized_files: if category not in categorized: categorized[category] = [] categorized[category].append(filename) categorized_files.add(filename) return categorized"},{"question":"# Question: Context: You are working on a Unix-based command-line application that requires precise control over terminal input. This application needs to switch between different terminal modes (raw and cbreak) to handle user input appropriately. Utilizing the `tty` and `termios` modules, you will implement functions to achieve this. Task: Implement a Python function `handle_terminal_input(fd: int, mode: str) -> str` that changes the terminal mode based on the specified mode and processes user input accordingly. # Function Specification: - **Input:** - `fd` (int): A valid file descriptor for the terminal. - `mode` (str): A string that can be \'raw\' or \'cbreak\', indicating the desired terminal mode. - **Output:** - Returns a string containing the user input read from the terminal. # Requirements: 1. If the mode is \'raw\', set the terminal to raw mode using `tty.setraw(fd)`. 2. If the mode is \'cbreak\', set the terminal to cbreak mode using `tty.setcbreak(fd)`. 3. Read user input from the terminal file descriptor (`fd`). Limit the input to 100 characters maximum for simplicity. 4. Restore the original terminal settings after reading the input. # Constraints: - The function should only be executed on Unix-based systems. - Assume that valid file descriptors will be provided. # Example Usage: ```python import sys import os import tty import termios def handle_terminal_input(fd: int, mode: str) -> str: original_settings = termios.tcgetattr(fd) try: if mode == \'raw\': tty.setraw(fd) elif mode == \'cbreak\': tty.setcbreak(fd) else: raise ValueError(\\"Invalid mode. Use \'raw\' or \'cbreak\'.\\") input_data = os.read(fd, 100) return input_data.decode(\'utf-8\') finally: termios.tcsetattr(fd, termios.TCSAFLUSH, original_settings) # Example usage with standard input file descriptor (0 is typically stdin) if __name__ == \\"__main__\\": mode = \'raw\' # or \'cbreak\' user_input = handle_terminal_input(0, mode) print(f\\"User Input: {user_input}\\") ``` In this example, the function `handle_terminal_input` correctly switches between \'raw\' and \'cbreak\' mode, reads a limited amount of user input, and ensures that the terminal settings are restored afterward. This demonstrates a clear understanding of terminal control and error handling with Python\'s `tty` and `termios` modules.","solution":"import os import tty import termios def handle_terminal_input(fd: int, mode: str) -> str: Change the terminal mode and read input from the terminal. Parameters: fd (int): File descriptor for the terminal. mode (str): \'raw\' or \'cbreak\' indicating the desired terminal mode. Returns: str: User input read from the terminal. original_settings = termios.tcgetattr(fd) try: if mode == \'raw\': tty.setraw(fd) elif mode == \'cbreak\': tty.setcbreak(fd) else: raise ValueError(\\"Invalid mode. Use \'raw\' or \'cbreak\'.\\") input_data = os.read(fd, 100) return input_data.decode(\'utf-8\') finally: termios.tcsetattr(fd, termios.TCSAFLUSH, original_settings)"},{"question":"# Question Objective Implement a function in Python using the `inspect` module to determine the signature of a given callable object and return information about its parameters and their default values, if any. Function Signature ```python def get_callable_signature_details(callable_obj): Given a callable object, return a detailed summary of its parameters. Args: callable_obj (callable): A callable object such as a function, method, or class. Returns: dict: A dictionary with parameter names as keys. Each value is a tuple containing the kind of parameter (e.g., positional-only, keyword-only) and its default value if it exists, otherwise `inspect._empty`. Raises: TypeError: If the passed object is not a callable. ValueError: If no signature can be provided. ``` Input - A callable object (e.g., function, method, or class). Output - A dictionary where each key is the name of a parameter, and each value is a tuple containing: - The kind of parameter (e.g., positional-only, keyword-only, etc.). - The default value of the parameter if it exists, otherwise `inspect._empty`. Constraints - The function should raise a `TypeError` if the input is not a callable. - The function should raise a `ValueError` if no signature can be provided. Example ```python def example_function(a, b: int = 10, *, c, d: float = 5.5): pass result = get_callable_signature_details(example_function) expected_output = { \'a\': (\'POSITIONAL_OR_KEYWORD\', inspect._empty), \'b\': (\'POSITIONAL_OR_KEYWORD\', 10), \'c\': (\'KEYWORD_ONLY\', inspect._empty), \'d\': (\'KEYWORD_ONLY\', 5.5) } assert result == expected_output ``` Hints - Use `inspect.signature` to extract the signature of the callable. - The `inspect` module provides various constants like `inspect._empty` and `inspect.Parameter` to compare kinds and defaults. - Make sure to handle exceptions as specified in the requirements.","solution":"import inspect def get_callable_signature_details(callable_obj): Given a callable object, return a detailed summary of its parameters. Args: callable_obj (callable): A callable object such as a function, method, or class. Returns: dict: A dictionary with parameter names as keys. Each value is a tuple containing the kind of parameter (e.g., positional-only, keyword-only) and its default value if it exists, otherwise `inspect._empty`. Raises: TypeError: If the passed object is not a callable. ValueError: If no signature can be provided. if not callable(callable_obj): raise TypeError(\\"Provided object is not callable\\") try: signature = inspect.signature(callable_obj) except ValueError: raise ValueError(\\"No signature could be provided for the given callable object\\") parameters = signature.parameters result = {} for name, param in parameters.items(): result[name] = (param.kind.name, param.default) return result"},{"question":"# Question: **Visualizing Airline Passenger Data with Seaborn** You are provided with a dataset representing the number of airline passengers on a monthly basis from 1949 to 1960. The dataset is available in both long-form and wide-form formats. Your task is to demonstrate your understanding of working with both data formats in Seaborn by performing the following steps: 1. **Load the Dataset**: Load the dataset from Seaborn\'s built-in datasets. 2. **Visualization using Long-form Data**: - Create a line plot using the long-form data where the x-axis represents the years, the y-axis represents the number of passengers, and different lines represent each month. - Customize the plot by adding titles, axis labels, and a legend. 3. **Transform to Wide-form Data**: Convert the long-form data into wide-form data, where each column represents a month, and the rows represent the years. 4. **Visualization using Wide-form Data**: - Create a line plot using the wide-form data where the x-axis represents the years, the y-axis represents the number of passengers for each month, and different lines represent each month. - Customize the plot by adding titles, axis labels, and a legend. 5. **Comparison**: Write a brief comparison (in comments) between the plots created using long-form and wide-form data, discussing the pros and cons of each approach based on your experience. # Input Format: - No input required; the dataset is loaded from Seaborn. # Output Format: - Two line plots (one for long-form and one for wide-form data) with appropriate customizations. - A brief comparison in comments. # Constraints: - You must use Seaborn for all visualizations. - Ensure that the plots are clear and informative. # Performance Requirements: - The code should execute efficiently without any unnecessary computations. # Sample Code: ```python import seaborn as sns import pandas as pd # Load the flights dataset flights = sns.load_dataset(\\"flights\\") # Part 1: Visualization using Long-form Data # ... (your code here) # Part 2: Transform to Wide-form Data # ... (your code here) # Part 3: Visualization using Wide-form Data # ... (your code here) # Part 4: Comparison # ... (your comments here) ```","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt # Load the flights dataset flights = sns.load_dataset(\\"flights\\") # Part 1: Visualization using Long-form Data plt.figure(figsize=(12, 6)) sns.lineplot(data=flights, x=\'year\', y=\'passengers\', hue=\'month\') plt.title(\'Number of Airline Passengers (1949 - 1960)\') plt.xlabel(\'Year\') plt.ylabel(\'Number of Passengers\') plt.legend(title=\'Month\', bbox_to_anchor=(1.05, 1), loc=\'upper left\') plt.show() # Part 2: Transform to Wide-form Data flights_wide = flights.pivot(index=\'year\', columns=\'month\', values=\'passengers\') # Part 3: Visualization using Wide-form Data plt.figure(figsize=(12, 6)) sns.lineplot(data=flights_wide) plt.title(\'Number of Airline Passengers (1949 - 1960)\') plt.xlabel(\'Year\') plt.ylabel(\'Number of Passengers\') plt.legend(title=\'Month\', bbox_to_anchor=(1.05, 1), loc=\'upper left\') plt.show() # Part 4: Comparison # The long-form data plot is more flexible for including multiple variables and # faceting. It shows a different line for each month, making it clear to compare # all within a single plot. However, it can be less clear when handling many categories. # The wide-form data plot is more straightforward for comparing over time, as each # line represents the data over the same index (year). It can become cluttered if # there are too many columns (variables), but it is more intuitive when there are # a fixed, small number of columns."},{"question":"**Custom Serialization in Python Using `pickle`** # Objective Your task is to demonstrate your understanding of Python\'s `pickle` module by implementing custom pickling and unpickling for a class that simulates a simple in-memory database. # Problem Statement You are required to implement a class `InMemoryDB` that simulates a basic in-memory database using a dictionary `data_store` where the keys are strings (representing record IDs) and the values are dictionaries representing the records. You need to customize the serialization (pickling) and deserialization (unpickling) of this class using the `pickle` module. # Specifications 1. Define a class `Record` for individual records, which has the following attributes: - `record_id`: A unique identifier for the record (string). - `data`: A dictionary containing the record data. 2. Implement the class `InMemoryDB` with the following: - An attribute `data_store` which is a dictionary to hold `Record` instances. - Methods to add, get, and remove records: ```python def add_record(self, record: Record) -> None def get_record(self, record_id: str) -> Record def remove_record(self, record_id: str) -> None ``` 3. Customize the pickling and unpickling of the `InMemoryDB` instance: - When pickling, only serialize the `data_store` attribute. - Implement the methods `__reduce__` and `__setstate__` to control how the instance is serialized and deserialized. # Input Format 1. Your solution should define the classes and methods as specified. 2. Assume that the input will be provided through method calls on the `InMemoryDB` instance. # Output Format 1. The output will demonstrate the serialized and deserialized `InMemoryDB` instance. 2. You should implement a main function to: - Create an `InMemoryDB` instance. - Add a few records. - Serialize (pickle) the instance. - Deserialize (unpickle) the instance. - Verify that the deserialized instance maintains the state and data correctly. # Example Usage ```python import pickle # Define the Record class class Record: def __init__(self, record_id, data): self.record_id = record_id self.data = data # Define the InMemoryDB class class InMemoryDB: def __init__(self): self.data_store = {} def add_record(self, record): self.data_store[record.record_id] = record.data def get_record(self, record_id): return self.data_store.get(record_id, None) def remove_record(self, record_id): if record_id in self.data_store: del self.data_store[record_id] def __reduce__(self): return (self.__class__, (), self.data_store) def __setstate__(self, state): self.data_store = state # Main function to test the pickling and unpickling def main(): db = InMemoryDB() db.add_record(Record(\'101\', {\'name\': \'Alice\', \'age\': 30})) db.add_record(Record(\'102\', {\'name\': \'Bob\', \'age\': 25})) # Serialize (pickle) the instance serialized_db = pickle.dumps(db) # Deserialize (unpickle) the instance deserialized_db = pickle.loads(serialized_db) # Verify the deserialized data assert deserialized_db.get_record(\'101\') == {\'name\': \'Alice\', \'age\': 30} assert deserialized_db.get_record(\'102\') == {\'name\': \'Bob\', \'age\': 25} assert deserialized_db.get_record(\'103\') == None print(\\"All tests passed!\\") if __name__ == \\"__main__\\": main() ``` # Constraints 1. The `record_id` is guaranteed to be unique within an `InMemoryDB` instance. 2. The `data` dictionary in a `Record` can contain any JSON-serializable types. 3. `pickle` protocol 3 or higher should be used. # Notes 1. Ensure that your implementation handles cases where `data_store` might be empty. 2. Your code should be clean, well-documented, and follow Python\'s naming conventions.","solution":"import pickle class Record: def __init__(self, record_id, data): self.record_id = record_id self.data = data class InMemoryDB: def __init__(self): self.data_store = {} def add_record(self, record): self.data_store[record.record_id] = record.data def get_record(self, record_id): return self.data_store.get(record_id, None) def remove_record(self, record_id): if record_id in self.data_store: del self.data_store[record_id] def __reduce__(self): return (self.__class__, (), self.data_store) def __setstate__(self, state): self.data_store = state"},{"question":"**Coding Assessment Question** # Background: You are tasked with creating a Python script that performs several operations: 1. Handle files and directories. 2. Parse command-line arguments. 3. Perform string manipulations. 4. Test the implemented functions for correctness. # Requirements: Part 1: Handling Files and Directories Implement a function `manage_files_and_directories` that: 1. Changes the current working directory to a specified path. 2. Creates a new directory within this path. 3. Copies a specified file into this newly created directory. ```python def manage_files_and_directories(target_directory: str, new_directory: str, file_to_copy: str) -> str: Changes the working directory, creates a new directory, and copies a specified file. Args: target_directory (str): The path to change the current working directory to. new_directory (str): The name of the new directory to create. file_to_copy (str): The name of the file to copy. Returns: str: The path of the copied file in the new directory. pass ``` Part 2: Parsing Command-Line Arguments Implement a script that processes command-line arguments using `argparse`: 1. Accepts a list of filenames. 2. Accepts an optional number of lines to display from each file. ```python # Filename: script.py import argparse def parse_arguments(): Parses command-line arguments. Returns: argparse.Namespace: Parsed arguments including filenames and lines to display. parser = argparse.ArgumentParser( prog=\'file_processor\', description=\'Process files and display specified number of lines\' ) parser.add_argument(\'filenames\', nargs=\'+\') parser.add_argument(\'-l\', \'--lines\', type=int, default=10) return parser.parse_args() if __name__ == \\"__main__\\": args = parse_arguments() print(args) ``` Part 3: String Manipulation Implement a function `extract_and_replace` that: 1. Uses regular expressions to find all words starting with a specified letter. 2. Replaces any double words (e.g., \\"the the\\") with a single occurrence. ```python def extract_and_replace(text: str, letter: str) -> (list, str): Extracts words starting with a specific letter and replaces double words. Args: text (str): The input text to process. letter (str): The letter to match words with. Returns: list: A list of matched words. str: The modified text with double words replaced by single occurrences. pass ``` Part 4: Testing Functions Write comprehensive unit tests using `unittest` to validate the functions `manage_files_and_directories` and `extract_and_replace`. Ensure edge cases are handled. ```python import unittest class TestFileAndStringFunctions(unittest.TestCase): def test_manage_files_and_directories(self): # Test cases for manage_files_and_directories pass def test_extract_and_replace(self): text = \\"The quick brown fox jumps over the the lazy dog.\\" letter = \'t\' self.assertEqual(extract_and_replace(text, letter), ([\'the\', \'the\'], \\"The quick brown fox jumps over the lazy dog.\\")) if __name__ == \\"__main__\\": unittest.main() ``` # Constraints: 1. Assume all file paths provided are valid. 2. Handle cases where the file to be copied does not exist. 3. Ensure that the string manipulation is case-insensitive. # Performance Requirements: - The `manage_files_and_directories` function should perform efficiently for typical file system operations. - The `extract_and_replace` function should handle large texts efficiently.","solution":"import os import shutil import re def manage_files_and_directories(target_directory: str, new_directory: str, file_to_copy: str) -> str: Changes the working directory, creates a new directory, and copies a specified file. Args: target_directory (str): The path to change the current working directory to. new_directory (str): The name of the new directory to create. file_to_copy (str): The name of the file to copy. Returns: str: The path of the copied file in the new directory. # Change to the target directory os.chdir(target_directory) # Create the new directory new_directory_path = os.path.join(target_directory, new_directory) os.makedirs(new_directory_path, exist_ok=True) # Copy the file to the new directory if not os.path.isfile(file_to_copy): raise FileNotFoundError(f\\"File {file_to_copy} does not exist.\\") file_name = os.path.basename(file_to_copy) new_file_path = os.path.join(new_directory_path, file_name) shutil.copy2(file_to_copy, new_file_path) return new_file_path def extract_and_replace(text: str, letter: str) -> (list, str): Extracts words starting with a specific letter and replaces double words. Args: text (str): The input text to process. letter (str): The letter to match words with. Returns: list: A list of matched words. str: The modified text with double words replaced by single occurrences. # Find all words starting with the specified letter (case-insensitive) pattern = re.compile(r\'b(\' + re.escape(letter) + r\'[a-zA-Z]*)b\', re.IGNORECASE) matched_words = pattern.findall(text) # Replace double words double_word_pattern = re.compile(r\'b(w+)s+1b\', re.IGNORECASE) modified_text = double_word_pattern.sub(r\'1\', text) return matched_words, modified_text"},{"question":"**Objective** Create functions to save Python objects to a binary file and load them back, ensuring proper exception handling with the `marshal` module. **Question** You are tasked with implementing two functions `serialize_objects` and `deserialize_objects`. These functions will handle serialization and deserialization of Python objects using the `marshal` module. Your objective is to ensure the objects are correctly saved to and loaded from a binary file, with appropriate exception handling. **Function 1: `serialize_objects`** - **Input:** - `data`: A list of Python objects. Each object can be of types that are supported by the `marshal` module (booleans, integers, strings, lists, etc.). - `filename`: A string representing the name of the file where the objects will be saved. - **Output:** None - **Behavior:** - The function should serialize each object in the `data` list and save it to a file specified by `filename` using the `marshal.dump` function. - Handle any `ValueError` exceptions that occur due to unsupported object types by skipping that specific object and continuing with the rest. **Function 2: `deserialize_objects`** - **Input:** - `filename`: A string representing the name of the file to read the objects from. - **Output:** - A list of deserialized Python objects. - **Behavior:** - The function should read each object from the file specified by `filename` using the `marshal.load` function. - If the end of the file is reached or an unsupported type is encountered, handle the `EOFError`, `ValueError`, and `TypeError` exceptions appropriately. Continue reading until all objects are read or an exception occurs. **Constraints** - You may assume that the file operations (open, read, write) will not fail for reasons such as file not existing or lacking permissions. - Assume that the objects in the `data` list are of types that `marshal` supports. **Example Usage** ```python data = [123, \'hello\', [1, 2, 3], {\'key\': \'value\'}, 45.67] filename = \'serialized_data.bin\' # Serialize objects serialize_objects(data, filename) # Deserialize objects loaded_data = deserialize_objects(filename) print(loaded_data) # Output should approximately match the original \'data\' list. ``` Implementations: ```python import marshal def serialize_objects(data, filename): with open(filename, \'wb\') as file: for obj in data: try: marshal.dump(obj, file) except ValueError: continue def deserialize_objects(filename): objects = [] try: with open(filename, \'rb\') as file: while True: try: obj = marshal.load(file) objects.append(obj) except (EOFError, ValueError, TypeError): break except FileNotFoundError: pass return objects ``` Additional Notes: - Ensure proper use of the `marshal` module as described in the documentation. - Validate your implementation with a variety of object types to ensure robustness.","solution":"import marshal def serialize_objects(data, filename): Serializes a list of Python objects to a binary file using the marshal module. Parameters: data (list): A list of Python objects to be serialized. filename (str): The name of the file where the objects will be saved. with open(filename, \'wb\') as file: for obj in data: try: marshal.dump(obj, file) except ValueError: continue def deserialize_objects(filename): Deserializes Python objects from a binary file using the marshal module. Parameters: filename (str): The name of the file from which to read the objects. Returns: list: A list of deserialized Python objects. objects = [] try: with open(filename, \'rb\') as file: while True: try: obj = marshal.load(file) objects.append(obj) except (EOFError, ValueError, TypeError): break except FileNotFoundError: pass return objects"}]'),D={name:"App",components:{PoemCard:A},data(){return{searchQuery:"",visibleCount:4,poemsData:I,isLoading:!1}},computed:{filteredPoems(){const o=this.searchQuery.trim().toLowerCase();return o?this.poemsData.filter(e=>e.question&&e.question.toLowerCase().includes(o)||e.solution&&e.solution.toLowerCase().includes(o)):this.poemsData},displayedPoems(){return this.searchQuery.trim()?this.filteredPoems:this.filteredPoems.slice(0,this.visibleCount)},hasMorePoems(){return!this.searchQuery.trim()&&this.visibleCount<this.poemsData.length}},methods:{async loadMore(){this.isLoading=!0,await new Promise(o=>setTimeout(o,1e3)),this.visibleCount+=4,this.isLoading=!1}}},z={class:"search-container"},F={class:"card-container"},q={key:0,class:"empty-state"},R=["disabled"],M={key:0},L={key:1};function N(o,e,l,m,i,n){const h=g("PoemCard");return a(),s("section",null,[e[4]||(e[4]=t("div",{class:"top-banner"},[t("div",{class:"top-banner-title"},[t("div",{class:"top-banner-title-text"},"🤔prompts chat🧠")])],-1)),t("div",z,[e[3]||(e[3]=t("span",{class:"search-icon"},"🔍",-1)),_(t("input",{type:"text",class:"search-input","onUpdate:modelValue":e[0]||(e[0]=r=>i.searchQuery=r),placeholder:"Search..."},null,512),[[y,i.searchQuery]]),i.searchQuery?(a(),s("button",{key:0,class:"clear-search",onClick:e[1]||(e[1]=r=>i.searchQuery="")}," ✕ ")):d("",!0)]),t("div",F,[(a(!0),s(b,null,v(n.displayedPoems,(r,f)=>(a(),w(h,{key:f,poem:r},null,8,["poem"]))),128)),n.displayedPoems.length===0?(a(),s("div",q,' No results found for "'+u(i.searchQuery)+'". ',1)):d("",!0)]),n.hasMorePoems?(a(),s("button",{key:0,class:"load-more-button",disabled:i.isLoading,onClick:e[2]||(e[2]=(...r)=>n.loadMore&&n.loadMore(...r))},[i.isLoading?(a(),s("span",L,"Loading...")):(a(),s("span",M,"See more"))],8,R)):d("",!0)])}const j=p(D,[["render",N],["__scopeId","data-v-cbe9afee"]]),Y=JSON.parse('{"title":"","description":"","frontmatter":{"page":true},"headers":[],"relativePath":"deepseek/56.md","filePath":"deepseek/56.md"}'),O={name:"deepseek/56.md"},B=Object.assign(O,{setup(o){return(e,l)=>(a(),s("div",null,[x(j)]))}});export{Y as __pageData,B as default};
