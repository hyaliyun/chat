import{_ as p,o as a,c as n,a as t,m as u,t as c,C as _,M as g,U as y,f as d,F as b,p as v,e as w,q as x}from"./chunks/framework.B1z0IdBH.js";const k={name:"PoemCard",props:{poem:{type:Object,required:!0}}},T={class:"poem-container"},E={class:"review"},C={class:"review-title"},P={class:"review-content"};function S(i,e,l,m,s,r){return a(),n("div",T,[t("div",E,[t("div",C,[e[0]||(e[0]=t("span",{class:"icon"},"question:",-1)),u(c(l.poem.question),1)]),e[2]||(e[2]=t("div",{class:"divider"},null,-1)),t("p",P,[e[1]||(e[1]=t("span",{class:"icon"},"answer:",-1)),u(c(l.poem.solution),1)])])])}const I=p(k,[["render",S],["__scopeId","data-v-9945dd55"]]),A=JSON.parse('[{"question":"You are given a dataset containing the columns \'A\', \'B\', \'X\', and \'Y\'. Your task is to create a series of plots using seaborn\'s objects interface. You must demonstrate your ability to control the layout and size of these plots within the figure, as well as use faceting to create subplots. Follow the instructions below: Instructions: 1. **Read the dataset**: Assume that the dataset is a CSV file named `data.csv`. 2. **Create a basic plot**: Instantiate a `seaborn.objects` plot and set its size to `(6, 6)`. 3. **Create faceted subplots**: Use the `facet` method to create a 2x2 grid of subplots, with: - Rows indexed by column \'A\'. - Columns indexed by column \'X\'. 4. **Apply a layout engine**: Use the `constrained` layout engine. 5. **Adjust the extent**: Set the extent of the overall plot within the figure to `[0, 0, 0.9, 1]`. Provide the code to achieve the above tasks, and ensure that the final plot is displayed correctly. Expected Input: - The dataset will be provided as a CSV file named `data.csv`. Expected Output: - A seaborn plot that meets all the specified criteria. ```python # Sample Implementation (Do not include this in the question) import seaborn.objects as so import pandas as pd # 1. Read the dataset data = pd.read_csv(\'data.csv\') # 2. Create a basic plot and set its size p = so.Plot(data=data).layout(size=(6, 6)) # 3. Create faceted subplots p = p.facet(\\"A\\", \\"X\\") # 4. Apply a layout engine p = p.layout(engine=\\"constrained\\") # 5. Adjust the extent p = p.layout(extent=[0, 0, 0.9, 1]) # Show the plot p.show() ``` **Note**: This is a complex problem intended to test your understanding of seaborn’s advanced plotting capabilities, particularly with respect to handling layout and faceting.","solution":"import seaborn as sns import pandas as pd import seaborn.objects as so def create_seaborn_plot(datafile): # 1. Read the dataset data = pd.read_csv(datafile) # 2. Create a basic plot and set its size plot = so.Plot(data=data).layout(size=(6, 6)) # 3. Create faceted subplots plot = plot.facet(\\"A\\", \\"X\\") # 4. Apply a layout engine plot = plot.layout(engine=\\"constrained\\") # 5. Adjust the extent plot = plot.layout(extent=[0, 0, 0.9, 1]) # Show the plot plot.show()"},{"question":"# Coding Assessment: Optimizing a K-Means Implementation Objective Implement and optimize a K-Means clustering algorithm. This task will test your understanding of scikit-learn, Numpy/Scipy, and performance optimization techniques. You will start by implementing the algorithm in Python, profile your code to identify bottlenecks, and then optimize the code based on your profiling results. Instructions 1. **Basic Implementation** - Implement the K-Means algorithm from scratch using Python and Numpy/Scipy. - Your implementation should: - Randomly initialize cluster centroids. - Assign data points to the nearest centroid. - Recompute centroids as the mean of assigned points. - Repeat until convergence or a maximum number of iterations. - Function Signature: ```python def kmeans(X: np.ndarray, n_clusters: int, max_iter: int = 300, tol: float = 1e-4) -> Tuple[np.ndarray, np.ndarray]: Perform K-Means clustering. Parameters: X (np.ndarray): 2D array with shape (n_samples, n_features) containing the data. n_clusters (int): Number of clusters. max_iter (int): Maximum number of iterations. tol (float): Convergence tolerance. Returns: Tuple[np.ndarray, np.ndarray]: (centroids, labels) centroids (np.ndarray): Array of shape (n_clusters, n_features) with the final centroids. labels (np.ndarray): Array of shape (n_samples,) with the cluster assignment for each data point. ``` 2. **Profiling** - Use IPython\'s `%prun` to profile your implementation and identify performance bottlenecks. - Provide a summary of the profiling results. 3. **Optimization** - Optimize your implementation to improve performance. - Consider using vectorized operations in Numpy/Scipy to reduce Python loop overhead. - Optionally, you may use Cython and joblib for further optimization. 4. **Testing and Validation** - Perform the clustering on the provided dataset and verify the results. - Ensure that your optimized implementation produces the same results as the initial implementation. 5. **Submission** - Submit your initial implementation, profiling summary, optimized implementation, and any additional profiling after optimization. Constraints - Input dataset `X` is a 2D Numpy array with shape `(n_samples, n_features)`. - `n_clusters` is a positive integer less than or equal to `n_samples`. - Use standard libraries like Numpy and Scipy without relying on pre-existing scikit-learn implementations for K-Means. - Performance improvement should be significant and justifiable based on profiling results. Example ```python import numpy as np # Example dataset np.random.seed(0) X = np.random.rand(100, 2) # Running the initial implementation centroids, labels = kmeans(X, n_clusters=3) print(\\"Final centroids:\\", centroids) print(\\"Cluster labels:\\", labels) ``` Provide your initial implementation, profiling summary, optimized implementation, and any additional profiling results after optimization.","solution":"import numpy as np def kmeans(X: np.ndarray, n_clusters: int, max_iter: int = 300, tol: float = 1e-4) -> tuple: Perform K-Means clustering. Parameters: X (np.ndarray): 2D array with shape (n_samples, n_features) containing the data. n_clusters (int): Number of clusters. max_iter (int): Maximum number of iterations. tol (float): Convergence tolerance. Returns: tuple(np.ndarray, np.ndarray): (centroids, labels) centroids (np.ndarray): Array of shape (n_clusters, n_features) with the final centroids. labels (np.ndarray): Array of shape (n_samples,) with the cluster assignment for each data point. n_samples, n_features = X.shape rng = np.random.default_rng() centroids = X[rng.choice(n_samples, n_clusters, replace=False)] labels = np.zeros(n_samples, dtype=np.int32) for _ in range(max_iter): # Assign labels to each point for i in range(n_samples): distances = np.linalg.norm(X[i] - centroids, axis=1) labels[i] = np.argmin(distances) # Compute new centroids new_centroids = np.array([X[labels == j].mean(axis=0) for j in range(n_clusters)]) # Convergence check if np.linalg.norm(new_centroids - centroids) < tol: break centroids = new_centroids return centroids, labels"},{"question":"Objective Demonstrate your understanding of the seaborn package\'s capabilities for data visualization and statistical representation by solving the following problem. Question You are given the seaborn `penguins` dataset. Your task is to create a series of plots that provide detailed insights into the penguin data. Specifically, you need to visualize the body mass of penguins grouped by their species and sex, and include error bars to show the standard deviation. Additionally, create a faceted line plot showing the bill depth against body mass for each species, including error intervals. Steps 1. Load the seaborn `penguins` dataset. 2. Create a dot plot of body mass (`body_mass_g`) grouped by species (`species`) and colored by sex (`sex`). Add error bars representing the standard deviation. 3. Create a faceted line plot to show the relationship between body mass (`body_mass_g`) and bill depth (`bill_depth_mm`). Each subplot should represent a different species (`species`), with error intervals displayed. Expected Functions ```python import seaborn.objects as so from seaborn import load_dataset def visualize_penguins(): # Load the dataset penguins = load_dataset(\\"penguins\\") # Dot plot with error bars plot1 = ( so.Plot(penguins, x=\\"body_mass_g\\", y=\\"species\\", color=\\"sex\\") .add(so.Dot(), so.Agg(), so.Dodge()) .add(so.Range(), so.Est(errorbar=\\"sd\\"), so.Dodge()) ) # Faceted line plot with error intervals plot2 = ( so.Plot(penguins, x=\\"body_mass_g\\", y=\\"bill_depth_mm\\", color=\\"sex\\", linestyle=\\"species\\") .facet(\\"species\\") .add(so.Line(marker=\\"o\\"), so.Agg()) .add(so.Range(), so.Est(errorbar=\\"sd\\")) ) return plot1, plot2 ``` Expected Outputs - `plot1`: A dot plot showing body mass grouped by species and colored by sex, with error bars for standard deviation. - `plot2`: A faceted line plot showing the bill depth vs. body mass, faceted by species, with error intervals for standard deviation. Constraints - Use seaborn objects from the `seaborn.objects` module only. - Each plot should provide a clear and accurate representation of the data with appropriate labels and legends. Performance Requirements - Ensure the code is well-optimized to handle the dataset efficiently without unnecessary computations or memory usage.","solution":"import seaborn.objects as so from seaborn import load_dataset def visualize_penguins(): Generates plots to visualize the penguins dataset. Returns: A tuple containing the dot plot and the faceted line plot. # Load the penguins dataset penguins = load_dataset(\\"penguins\\") # Dot plot with error bars plot1 = ( so.Plot(penguins, x=\\"body_mass_g\\", y=\\"species\\", color=\\"sex\\") .add(so.Dot(), so.Agg(), so.Dodge()) .add(so.Range(), so.Est(errorbar=\\"sd\\"), so.Dodge()) .label(title=\\"Body Mass by Species and Sex with Error Bars\\") ) # Faceted line plot with error intervals plot2 = ( so.Plot(penguins, x=\\"body_mass_g\\", y=\\"bill_depth_mm\\", color=\\"sex\\", linestyle=\\"species\\") .facet(\\"species\\") .add(so.Line(marker=\\"o\\"), so.Agg()) .add(so.Range(), so.Est(errorbar=\\"sd\\")) .label(title=\\"Bill Depth vs Body Mass Faceted by Species with Error Intervals\\") ) return plot1, plot2"},{"question":"You are working with a dataset that includes information about customer feedback on a product. The dataset contains: 1. `CustomerID` - An identifier for the customer. 2. `Feedback` - The feedback given by the customer, which can be one of \'Very Poor\', \'Poor\', \'Average\', \'Good\', \'Excellent\'. 3. `FeedbackDate` - The date the feedback was provided. You are required to implement a function that processes this dataset and provides useful insights. # `process_customer_feedback` Parameters - `df` (pd.DataFrame): A DataFrame containing the columns `CustomerID`, `Feedback`, and `FeedbackDate`. Returns - A dictionary with the following keys: - `ordered_feedback`: A pandas Series of the `Feedback` column where the feedback is treated as an ordered categorical type. - `feedback_counts`: A pandas Series containing the count of each category in the `Feedback` column, including any categories that have zero occurrences. - `sorted_feedback_df`: A DataFrame sorted by `FeedbackDate` and `Feedback` (with `Feedback` sorted based on its categorical order). - `missing_feedback_count`: An integer representing the count of records where `Feedback` is missing. Constraints - The function should efficiently handle missing values in the `Feedback` column. - Feedback should be ordered as [\'Very Poor\', \'Poor\', \'Average\', \'Good\', \'Excellent\']. - The function should handle large datasets. # Example ```python import pandas as pd data = { \'CustomerID\': [1, 2, 3, 4, 5], \'Feedback\': [\'Good\', \'Poor\', \'Excellent\', None, \'Average\'], \'FeedbackDate\': [\'2023-01-01\', \'2023-02-01\', \'2023-01-15\', \'2023-03-01\', \'2023-02-20\'] } df = pd.DataFrame(data) result = process_customer_feedback(df) print(result[\'ordered_feedback\']) # Output: # 0 Good # 1 Poor # 2 Excellent # 3 NaN # 4 Average # Name: Feedback, dtype: category # Categories (5, object): [\'Very Poor\' < \'Poor\' < \'Average\' < \'Good\' < \'Excellent\'] print(result[\'feedback_counts\']) # Output: # Very Poor 0 # Poor 1 # Average 1 # Good 1 # Excellent 1 # dtype: int64 print(result[\'sorted_feedback_df\']) # Output: # CustomerID Feedback FeedbackDate # 0 1 Good 2023-01-01 # 2 3 Excellent 2023-01-15 # 1 2 Poor 2023-02-01 # 4 5 Average 2023-02-20 # 3 4 NaN 2023-03-01 print(result[\'missing_feedback_count\']) # Output: # 1 ``` Implement the function `process_customer_feedback` to achieve the desired functionality.","solution":"import pandas as pd def process_customer_feedback(df): Process the customer feedback dataset and provide insights. Parameters: - df (pd.DataFrame): A DataFrame containing the columns `CustomerID`, `Feedback`, and `FeedbackDate`. Returns: - dict: A dictionary containing the requested insights. # Define the order for the feedback categories feedback_order = [\'Very Poor\', \'Poor\', \'Average\', \'Good\', \'Excellent\'] feedback_type = pd.CategoricalDtype(categories=feedback_order, ordered=True) # Convert the Feedback column to the ordered categorical type df[\'Feedback\'] = df[\'Feedback\'].astype(feedback_type) # Count occurrences of each feedback category, including those with zero occurrences feedback_counts = df[\'Feedback\'].value_counts().reindex(feedback_order).fillna(0).astype(int) # Sort the DataFrame by FeedbackDate and Feedback (with `Feedback` sorted based on its categorical order) sorted_feedback_df = df.sort_values(by=[\'FeedbackDate\', \'Feedback\']) # Count records where Feedback is missing missing_feedback_count = df[\'Feedback\'].isna().sum() return { \'ordered_feedback\': df[\'Feedback\'], \'feedback_counts\': feedback_counts, \'sorted_feedback_df\': sorted_feedback_df, \'missing_feedback_count\': missing_feedback_count }"},{"question":"# Kernel Density Estimation: Advanced Implementation Kernel Density Estimation (KDE) is a non-parametric way to estimate the probability density function of a random variable. Task Implement a function `calculate_kde` that performs kernel density estimation on a 2D dataset using the `KernelDensity` class from scikit-learn. Your task is to correctly fit the model with the dataset and return density estimates for a provided set of sample points. Function Signature ```python def calculate_kde(data: np.ndarray, sample_points: np.ndarray, bandwidth: float, kernel: str) -> np.ndarray: Perform kernel density estimation on the given dataset and return the density estimates for the sample points. Parameters: - data (np.ndarray): A 2D array of shape (n_samples, n_features) representing the input data for KDE. - sample_points (np.ndarray): A 2D array of shape (m_samples, n_features) showing the points to evaluate the density estimates. - bandwidth (float): The bandwidth parameter for the KDE. - kernel (str): The kernel to use for KDE. Must be one of [\'gaussian\', \'tophat\', \'epanechnikov\', \'exponential\', \'linear\', \'cosine\']. Returns: - np.ndarray: A 1D array of shape (m_samples,) containing the log density estimates for the sample points. ``` Constraints - The `data` array must have at least 10 samples and at most 1000 samples. - The `sample_points` array must have at least 5 sample points and at most 500 sample points. - The `bandwidth` must be a positive float. - The `kernel` string must be one of the valid kernel types specified ([\'gaussian\', \'tophat\', \'epanechnikov\', \'exponential\', \'linear\', \'cosine\']). Example ```python import numpy as np data = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]]) sample_points = np.array([[0, 0], [1, 1], [2, 2]]) bandwidth = 0.5 kernel = \'gaussian\' density_estimates = calculate_kde(data, sample_points, bandwidth, kernel) print(density_estimates) # Expected Output: An array of log density estimates for the sample points ``` Note - Ensure you handle the appropriate imports and error handling for invalid input parameters. - The return value should be the log density estimates, as provided by the `score_samples` method in the `KernelDensity` class. - Your implementation should be efficient to handle the upper limits of the constraints.","solution":"import numpy as np from sklearn.neighbors import KernelDensity def calculate_kde(data: np.ndarray, sample_points: np.ndarray, bandwidth: float, kernel: str) -> np.ndarray: Perform kernel density estimation on the given dataset and return the density estimates for the sample points. Parameters: - data (np.ndarray): A 2D array of shape (n_samples, n_features) representing the input data for KDE. - sample_points (np.ndarray): A 2D array of shape (m_samples, n_features) showing the points to evaluate the density estimates. - bandwidth (float): The bandwidth parameter for the KDE. - kernel (str): The kernel to use for KDE. Must be one of [\'gaussian\', \'tophat\', \'epanechnikov\', \'exponential\', \'linear\', \'cosine\']. Returns: - np.ndarray: A 1D array of shape (m_samples,) containing the log density estimates for the sample points. # Validate parameters if len(data) < 10 or len(data) > 1000: raise ValueError(\\"data array must have at least 10 samples and at most 1000 samples.\\") if len(sample_points) < 5 or len(sample_points) > 500: raise ValueError(\\"sample_points array must have at least 5 sample points and at most 500 sample points.\\") if bandwidth <= 0: raise ValueError(\\"bandwidth must be a positive float.\\") valid_kernels = [\'gaussian\', \'tophat\', \'epanechnikov\', \'exponential\', \'linear\', \'cosine\'] if kernel not in valid_kernels: raise ValueError(f\\"kernel must be one of {valid_kernels}.\\") # Initialize and fit the KernelDensity model kde = KernelDensity(bandwidth=bandwidth, kernel=kernel) kde.fit(data) # Compute log density estimates for the sample points log_density_estimates = kde.score_samples(sample_points) return log_density_estimates"},{"question":"**Objective:** Implement a mini interactive Python shell with basic tab completion and history management. **Description:** You are required to implement a simplified version of an interactive Python shell. This shell should support: 1. **Tab Completion:** Suggest possible completions for variable and module names when the \\"Tab\\" key is pressed. 2. **History Management:** Store executed commands and allow navigation through the command history using the up and down arrow keys. **Functional Requirements:** 1. **Tab Completion:** - When the user types the beginning of a variable or module name and presses \\"Tab\\", the shell should list all possible completions. - For dotted expressions (e.g., `math.`), it should complete based on the attributes of the resulting object. 2. **History Management:** - Maintain a history of commands entered by the user in a list. - Allow the user to navigate through the history using the up and down arrow keys. **Input and Output Formats:** - **Input:** User-typed commands in the interactive shell. - **Output:** Suggestions for tab completion and execution results in the interactive shell. **Constraints:** - Do not use external libraries other than the standard library. - Assume the presence of basic Python modules like `os`, `sys`, and `re`. **Performance Requirements:** - The shell should respond to user inputs in real-time without noticeable delays for tab completion and history navigation. **Hints:** - You might want to use `input()` for reading user commands. - Consider using a loop to maintain the shell session. - Use Python\'s introspection capabilities (`dir()`, `getattr()`, etc.) for tab completion. - Manage the history in a list and handle navigation using indices. **Example:** ```python # Starting the custom shell >>> import ma[TAB] # Output suggestions: import math, import matplotlib, etc. >>> import math >>> math.si[TAB] # Output suggestions: math.sin, math.sinh, etc. >>> math.sin(45) # Output: 0.8509035245341184 >>> [UP-ARROW] # Restores the previous command: math.sin(45) ``` **Implementation:** Write a Python program fulfilling the above specification.","solution":"import rlcompleter import readline import code # Enable tab completion readline.parse_and_bind(\\"tab: complete\\") class InteractiveShell: def __init__(self): self.history = [] def run(self): while True: try: command = input(\\">>> \\") if command.strip() == \\"exit\\": break self.history.append(command) exec(command, globals(), globals()) except Exception as e: print(f\\"Error: {e}\\") if __name__ == \\"__main__\\": shell = InteractiveShell() shell.run()"},{"question":"**Objective:** Write a function to merge multiple pandas `DataFrame` objects using various merging techniques provided by pandas and return specific results. **Function Signature:** ```python def custom_merge(dfs: list, method: str, additional_params: dict = {}) -> pd.DataFrame: pass ``` **Input:** - `dfs` (list): A list of tuples where each tuple contains a `DataFrame` and its name (string). - `method` (str): The merging method to be used. It can be: - `\\"concat\\"`: Concatenate the DataFrames. - `\\"merge\\"`: Merge DataFrames on common columns. - `\\"join\\"`: Join DataFrames on indexes. - `additional_params` (dict): Additional parameters to be passed to the respective merging method. For example, for `concat`, it could be `{\\"keys\\": [\\"df1\\", \\"df2\\"]}`, and for `merge` or `join`, it could be `{\\"on\\": \\"column_name\\", \\"how\\": \\"inner\\"}`. **Output:** - A merged `DataFrame` object resulting from the specified merge method and parameters. **Constraints:** - Ensure the merging operations handle edge cases such as missing values and non-overlapping indexes gracefully. - Optimize for performance by minimizing unnecessary data copies. **Example:** ```python import pandas as pd def custom_merge(dfs, method, additional_params={}): # Your code here # Sample DataFrames df1 = pd.DataFrame({\\"A\\": [1, 2, 3], \\"B\\": [\\"a\\", \\"b\\", \\"c\\"]}) df2 = pd.DataFrame({\\"A\\": [4, 5, 6], \\"C\\": [\\"d\\", \\"e\\", \\"f\\"]}) df3 = pd.DataFrame({\\"B\\": [\\"g\\", \\"h\\", \\"i\\"], \\"C\\": [7, 8, 9]}) # Example usage: result1 = custom_merge([(df1, \\"df1\\"), (df2, \\"df2\\")], method=\\"concat\\", additional_params={\\"keys\\": [\\"df1\\", \\"df2\\"]}) print(result1) result2 = custom_merge([(df1, \\"df1\\"), (df2, \\"df2\\")], method=\\"merge\\", additional_params={\\"on\\": \\"A\\", \\"how\\": \\"outer\\"}) print(result2) result3 = custom_merge([(df1, \\"df1\\"), (df3, \\"df3\\")], method=\\"join\\", additional_params={\\"how\\": \\"inner\\"}) print(result3) ``` **Expected Result:** 1. For the `concat` example: ``` A B C df1 1.0 a NaN df1 2.0 b NaN df1 3.0 c NaN df2 4.0 NaN d df2 5.0 NaN e df2 6.0 NaN f ``` 2. For the `merge` example: ``` A B C 0 1.0 a NaN 1 2.0 b NaN 2 3.0 c NaN 3 4.0 NaN d 4 5.0 NaN e 5 6.0 NaN f ``` 3. For the `join` example: ``` A C 0 a 7 1 b 8 2 c 9 g 1 d h 2 e i 3 f ``` **Notes:** - The `dfs` list will always contain at least two DataFrames. - Handling of the `additional_params` dictionary is essential for proper function operation. - Your function should be robust and fail gracefully, providing meaningful error messages if the inputs do not conform to expected formats.","solution":"import pandas as pd def custom_merge(dfs, method, additional_params={}): Merge multiple DataFrame objects using various merging techniques provided by pandas and return specific results. Parameters: dfs (list): A list of tuples where each tuple contains a DataFrame and its name (string). method (str): The merging method to be used. It can be: \'concat\', \'merge\', \'join\'. additional_params (dict): Additional parameters to be passed to the respective merging method. Returns: pd.DataFrame: The merged DataFrame. df_list = [df for df, name in dfs] if method == \\"concat\\": return pd.concat(df_list, **additional_params) elif method == \\"merge\\": if \\"on\\" not in additional_params or \\"how\\" not in additional_params: raise ValueError(\\"Parameters \'on\' and \'how\' are required for merge method.\\") merged_df = df_list[0] for df in df_list[1:]: merged_df = pd.merge(merged_df, df, **additional_params) return merged_df elif method == \\"join\\": if \\"how\\" not in additional_params: raise ValueError(\\"Parameter \'how\' is required for join method.\\") merged_df = df_list[0] for df in df_list[1:]: merged_df = merged_df.join(df, **additional_params) return merged_df else: raise ValueError(\\"Invalid method provided. Choose from \'concat\', \'merge\', or \'join\'.\\")"},{"question":"# Cryptographic Services: Secure Message Verification In this coding assessment, you will demonstrate your understanding of the `hashlib` and `hmac` modules by implementing a secure message verification system. # Problem Statement You need to implement two functions to ensure the integrity and authenticity of messages. 1. **`generate_secure_hash(message: str, key: bytes) -> str`**: - This function takes a message (string) and a key (bytes), and returns a hexadecimal string representing the HMAC (Hash-based Message Authentication Code) of the message using the SHA-256 hashing algorithm. - `hmac` module should be used for keyed hashing. 2. **`verify_secure_hash(message: str, key: bytes, hash: str) -> bool`**: - This function takes a message (string), a key (bytes), and an expected hash (string). It should return `True` if the HMAC of the message matches the given hash, and `False` otherwise. # Input and Output Formats - The `message` parameter for both functions is a string of arbitrary length. - The `key` parameter for both functions is a byte sequence. - The `hash` parameter for `verify_secure_hash` is a hexadecimal string. # Constraints - The key should be at least 16 bytes long to ensure a reasonable level of security. - The functions should handle empty messages properly. - Performance is not the primary concern, but your implementation should be efficient enough to handle reasonably large messages (up to 1 MB in size). # Example ```python message = \\"Hello, world!\\" key = b\\"this_is_a_secret_key\\" # Generate secure hash generated_hash = generate_secure_hash(message, key) print(f\\"Generated Hash: {generated_hash}\\") # Verify the hash is_valid = verify_secure_hash(message, key, generated_hash) print(f\\"Is the hash valid? {is_valid}\\") # Output # Generated Hash: (some hexadecimal string) # Is the hash valid? True ``` # Notes - Ensure you are using the `hmac` library for generating and verifying the secure hash. - The hexadecimal representation of the hash should be in lowercase. ```python import hmac import hashlib def generate_secure_hash(message: str, key: bytes) -> str: Generate a secure HMAC hash of the message using the provided key and SHA-256 algorithm. Parameters: message (str): The message to hash. key (bytes): The secret key used for hashing. Returns: str: The hexadecimal representation of the HMAC hash. # Your implementation here def verify_secure_hash(message: str, key: bytes, hash: str) -> bool: Verify a message against a provided HMAC hash using the same key and SHA-256 algorithm. Parameters: message (str): The message to verify. key (bytes): The secret key used for verifying. hash (str): The expected HMAC hash in hexadecimal format. Returns: bool: True if the HMAC hash matches, False otherwise. # Your implementation here ```","solution":"import hmac import hashlib def generate_secure_hash(message: str, key: bytes) -> str: Generate a secure HMAC hash of the message using the provided key and SHA-256 algorithm. Parameters: message (str): The message to hash. key (bytes): The secret key used for hashing. Returns: str: The hexadecimal representation of the HMAC hash. hmac_obj = hmac.new(key, message.encode(), hashlib.sha256) return hmac_obj.hexdigest() def verify_secure_hash(message: str, key: bytes, hash: str) -> bool: Verify a message against a provided HMAC hash using the same key and SHA-256 algorithm. Parameters: message (str): The message to verify. key (bytes): The secret key used for verifying. hash (str): The expected HMAC hash in hexadecimal format. Returns: bool: True if the HMAC hash matches, False otherwise. expected_hash = generate_secure_hash(message, key) return hmac.compare_digest(expected_hash, hash)"},{"question":"# Platform Information Reporter Objective: Develop a Python function that aggregates information about the current platform using the `platform` module and presents it in a structured dictionary format. This function is designed to test your understanding of various platform-related functions and your ability to combine their outputs cohesively. Function Signature: ```python def get_platform_info() -> dict: # Your implementation here ``` Input: None (the function does not take any parameters). Output: A dictionary containing the following keys and their corresponding values: - `\'architecture\'`: A tuple `(bits, linkage)` from `platform.architecture()`. - `\'machine\'`: A string from `platform.machine()`. - `\'node\'`: A string from `platform.node()`. - `\'platform\'`: A string from `platform.platform()`. - `\'processor\'`: A string from `platform.processor()`. - `\'python_build\'`: A tuple `(buildno, builddate)` from `platform.python_build()`. - `\'python_compiler\'`: A string from `platform.python_compiler()`. - `\'python_implementation\'`: A string from `platform.python_implementation()`. - `\'python_version\'`: A string from `platform.python_version()`. - `\'system\'`: A string from `platform.system()`. - `\'uname\'`: A named tuple from `platform.uname()`. Constraints: - Ensure that the function handles cases where values cannot be determined and provides default or empty values as specified in the `platform` module documentation. - Make sure to use the appropriate functions from the `platform` module to obtain each piece of information. Example: If the function is run on a specific platform, the output might look like the following (the actual output may vary depending on the system): ```python { \'architecture\': (\'64bit\', \'ELF\'), \'machine\': \'x86_64\', \'node\': \'hostname.local\', \'platform\': \'Linux-5.4.0-42-generic-x86_64-with-Ubuntu-20.04-focal\', \'processor\': \'x86_64\', \'python_build\': (\'default\', \'Jul 20 2020 13:22:52\'), \'python_compiler\': \'GCC 9.3.0\', \'python_implementation\': \'CPython\', \'python_version\': \'3.10.0\', \'system\': \'Linux\', \'uname\': uname_result(system=\'Linux\', node=\'hostname.local\', release=\'5.4.0-42-generic\', version=\'#46-Ubuntu SMP Fri Jul 10 00:24:02 UTC 2020\', machine=\'x86_64\', processor=\'x86_64\') } ``` This question tests your ability to work with the `platform` module and understand the different types of platform data it can provide.","solution":"import platform def get_platform_info(): Collects and returns detailed platform information as a dictionary. platform_info = { \'architecture\': platform.architecture(), \'machine\': platform.machine(), \'node\': platform.node(), \'platform\': platform.platform(), \'processor\': platform.processor(), \'python_build\': platform.python_build(), \'python_compiler\': platform.python_compiler(), \'python_implementation\': platform.python_implementation(), \'python_version\': platform.python_version(), \'system\': platform.system(), \'uname\': platform.uname() } return platform_info"},{"question":"Python Coding Assessment Question # File System Utility Functions You are required to implement a utility class that encapsulates several commonly needed file and directory operations using the modules described in the given documentation snippet. # Class: `FileSystemUtility` Methods to Implement: 1. **`__init__(self, base_path: str)`** - Initializes the utility with a base directory path. 2. **`list_files(self) -> list`** - Lists all files in the base directory and its subdirectories. 3. **`copy_file(self, src: str, dst: str) -> bool`** - Copies a file from the source path to the destination path within the base directory structure. Returns `True` if the operation is successful and `False` otherwise. 4. **`compare_files(self, path1: str, path2: str) -> bool`** - Compares two files within the base directory for equality. Returns `True` if the files are the same and `False` otherwise. 5. **`create_temp_file(self) -> str`** - Creates a temporary file within the base directory and returns the path to this file. # Constraints - The base path must be a valid directory path. - All file operations should be relative to the base path. - For the sake of simplicity, assume all path strings provided are valid and accessible. # Example ```python fs_util = FileSystemUtility(\'/my/base/directory\') # List all files files = fs_util.list_files() print(files) # Copy a file success = fs_util.copy_file(\'file1.txt\', \'copy_of_file1.txt\') print(\'Copy Success:\', success) # Compare two files are_equal = fs_util.compare_files(\'file1.txt\', \'copy_of_file1.txt\') print(\'Are Files Equal:\', are_equal) # Create a temporary file temp_file_path = fs_util.create_temp_file() print(\'Temp File Created at:\', temp_file_path) ``` # Performance - `list_files` should recurse through the directories efficiently. - `copy_file`, `compare_files`, and `create_temp_file` should handle large files gracefully. Implement the `FileSystemUtility` class with the methods described above, ensuring all operations are reliable and efficient. # Note Utilize the appropriate modules (`pathlib`, `shutil`, `filecmp`, `tempfile`) for implementing each method to demonstrate your understanding and usage of Python\'s file system handling capabilities.","solution":"import os import shutil import filecmp import tempfile from pathlib import Path class FileSystemUtility: def __init__(self, base_path: str): self.base_path = Path(base_path) if not self.base_path.is_dir(): raise ValueError(f\\"Provided base path \'{base_path}\' is not a valid directory.\\") def list_files(self) -> list: Lists all files in the base directory and its subdirectories. all_files = [] for root, _, files in os.walk(self.base_path): for file in files: all_files.append(str(Path(root) / file)) return all_files def copy_file(self, src: str, dst: str) -> bool: Copies a file from the source path to the destination path within the base directory structure. try: src_path = self.base_path / src dst_path = self.base_path / dst shutil.copy2(src_path, dst_path) return True except (FileNotFoundError, IOError): return False def compare_files(self, path1: str, path2: str) -> bool: Compares two files within the base directory for equality. file1 = self.base_path / path1 file2 = self.base_path / path2 return filecmp.cmp(file1, file2, shallow=False) def create_temp_file(self) -> str: Creates a temporary file within the base directory and returns the path to this file. temp_file = tempfile.NamedTemporaryFile(delete=False, dir=self.base_path) temp_file.close() return temp_file.name"},{"question":"**Objective: Understanding and Creating Residual Plots in Seaborn** # Question: You are given a dataset `mpg` from the seaborn library, which contains information about various car models. The dataset includes the following columns: - `mpg`: miles per gallon - `cylinders`: number of cylinders - `displacement`: engine displacement - `horsepower`: engine horsepower - `weight`: weight of the car - `acceleration`: time taken to accelerate from 0 to 60 mph - `model_year`: model year of the car - `origin`: origin of the car - `name`: name of the car # Tasks: 1. Load the dataset `mpg` using Seaborn\'s `load_dataset` function. 2. Create a scatter plot of the residuals after fitting a simple regression model using `sns.residplot`: - Use `weight` as the predictor variable (`x`) and `displacement` as the dependent variable (`y`). - Interpret whether there is any structure in the residual plot indicating a potential violation of the linear regression assumption. 3. Create another residual plot to check if removing higher-order trends stabilizes the residuals: - Use `horsepower` as the predictor variable (`x`) and `mpg` as the dependent variable (`y`). - Use `order=2` to fit a polynomial regression model. - Interpret the structure of the residual plot. 4. Enhance the residual plot from Task 3 by adding a LOWESS curve: - Use the option `lowess=True` and set the line color to red. - Interpret how the LOWESS curve helps in understanding the structure of residuals. # Constraints: - Use only seaborn and matplotlib libraries for plotting. - Ensure the plots are self-contained with appropriate axis labels and titles. - Provide the interpretations as comments in your code. # Expected Input and Output Formats: - **Input:** Code to generate the plots based on the `mpg` dataset. - **Output:** Residual plots as specified and corresponding interpretations as code comments. # Performance Requirements: - Efficiently generate plots with correct aesthetics using seaborn functions. - Appropriately interpret the plots based on the residual structures. # Example Code Template: ```python import seaborn as sns import matplotlib.pyplot as plt # Task 1: Load the dataset mpg = sns.load_dataset(\\"mpg\\") # Task 2: Create a scatter plot of residuals for \'weight\' vs \'displacement\' plt.figure(figsize=(10, 6)) sns.residplot(data=mpg, x=\\"weight\\", y=\\"displacement\\") plt.title(\'Residual Plot: Weight vs Displacement\') plt.xlabel(\'Weight\') plt.ylabel(\'Residuals of Displacement\') # Interpretation: [Your comment here] # Task 3: Create a residual plot with polynomial regression (order=2) for \'horsepower\' vs \'mpg\' plt.figure(figsize=(10, 6)) sns.residplot(data=mpg, x=\\"horsepower\\", y=\\"mpg\\", order=2) plt.title(\'Residual Plot (Polynomial Regression): Horsepower vs MPG\') plt.xlabel(\'Horsepower\') plt.ylabel(\'Residuals of MPG\') # Interpretation: [Your comment here] # Task 4: Add a LOWESS curve to the residual plot from Task 3 and interpret plt.figure(figsize=(10, 6)) sns.residplot(data=mpg, x=\\"horsepower\\", y=\\"mpg\\", order=2, lowess=True, line_kws=dict(color=\\"r\\")) plt.title(\'Residual Plot with LOWESS Curve: Horsepower vs MPG\') plt.xlabel(\'Horsepower\') plt.ylabel(\'Residuals of MPG\') # Interpretation: [Your comment here] plt.show() ```","solution":"import seaborn as sns import matplotlib.pyplot as plt # Task 1: Load the dataset mpg = sns.load_dataset(\\"mpg\\") # Task 2: Create a scatter plot of residuals for \'weight\' vs \'displacement\' plt.figure(figsize=(10, 6)) sns.residplot(data=mpg, x=\\"weight\\", y=\\"displacement\\") plt.title(\'Residual Plot: Weight vs Displacement\') plt.xlabel(\'Weight\') plt.ylabel(\'Residuals of Displacement\') plt.grid(True) plt.show() # Interpretation: # If there is no pattern in the residual plot, it indicates that a linear model is appropriate. # However, if there is a clear structure (such as a curve or trend), it suggests a potential # violation of the linear regression assumption, indicating that a different model might be more appropriate. # Task 3: Create a residual plot with polynomial regression (order=2) for \'horsepower\' vs \'mpg\' plt.figure(figsize=(10, 6)) sns.residplot(data=mpg, x=\\"horsepower\\", y=\\"mpg\\", order=2) plt.title(\'Residual Plot (Polynomial Regression): Horsepower vs MPG\') plt.xlabel(\'Horsepower\') plt.ylabel(\'Residuals of MPG\') plt.grid(True) plt.show() # Interpretation: # The residual plot should be checked for randomness. If the residuals randomly scatter around the # horizontal axis, it means that the polynomial regression of order 2 is a good fit. Otherwise, # further investigation into the model might be needed. # Task 4: Add a LOWESS curve to the residual plot from Task 3 and interpret plt.figure(figsize=(10, 6)) sns.residplot(data=mpg, x=\\"horsepower\\", y=\\"mpg\\", order=2, lowess=True, line_kws=dict(color=\\"r\\")) plt.title(\'Residual Plot with LOWESS Curve: Horsepower vs MPG\') plt.xlabel(\'Horsepower\') plt.ylabel(\'Residuals of MPG\') plt.grid(True) plt.show() # Interpretation: # The LOWESS curve helps in visually assessing the trend in the residuals. If the curve is relatively flat # and close to the horizontal axis, it indicates that the polynomial regression model is appropriate. # If the curve shows trends (e.g., systematic patterns), it indicates a more complex relationship between # the variables that the current model does not capture."},{"question":"**Question: Custom Calendar Filter** You are required to create a function named `filter_calendar_events` that filters out specific days from a given month based on certain criteria. The function will use the `calendar` module to generate the days of the month and filter out days based on whether they fall on weekdays or weekends and whether they are prime numbers. # Function Signature ```python def filter_calendar_events(year: int, month: int, filter_criteria: str) -> list: ``` # Input - `year` (int): The year for which the calendar is to be generated. - `month` (int): The month (1–12) for which the calendar is to be generated. - `filter_criteria` (str): A string specifying the criteria for filtering days. It can be one of the following: - `\'weekdays\'`: Include only weekdays (Monday-Friday). - `\'weekends\'`: Include only weekends (Saturday-Sunday). - `\'prime_days\'`: Include only days where the date is a prime number. # Output - A list of integers representing the filtered days based on the provided criteria. # Constraints - The `month` will always be between 1 and 12. - The `year` will always be a valid Gregorian calendar year. - The function should correctly handle leap years. # Example ```python # Example 1: Filter weekdays print(filter_calendar_events(2023, 10, \'weekdays\')) # Output: [2, 3, 4, 5, 6, 9, 10, 11, 12, 13, 16, 17, 18, 19, 20, 23, 24, 25, 26, 27, 30, 31] # Example 2: Filter weekends print(filter_calendar_events(2023, 10, \'weekends\')) # Output: [1, 7, 8, 14, 15, 21, 22, 28, 29] # Example 3: Filter prime days print(filter_calendar_events(2023, 10, \'prime_days\')) # Output: [2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31] ``` # Notes - You may use the `calendar.Calendar().itermonthdays2()` method to iterate over the days of the specified month and year. - Remember to consider the edge cases, such as months with 30 and 31 days and February in leap years. - Zero values (representing days outside the specified month) should be ignored. # Performance Requirements - The solution should have a time complexity no worse than O(n), where n is the number of days in the month.","solution":"import calendar def is_prime(n): if n <= 1: return False if n <= 3: return True if n % 2 == 0 or n % 3 == 0: return False i = 5 while i * i <= n: if n % i == 0 or n % (i + 2) == 0: return False i += 6 return True def filter_calendar_events(year: int, month: int, filter_criteria: str) -> list: cal = calendar.Calendar() days = [] for day, weekday in cal.itermonthdays2(year, month): if day == 0: continue if filter_criteria == \'weekdays\' and weekday < 5: days.append(day) elif filter_criteria == \'weekends\' and weekday >= 5: days.append(day) elif filter_criteria == \'prime_days\' and is_prime(day): days.append(day) return days"},{"question":"Objective: You are tasked with implementing a custom PyTorch autograd function and using forward-mode automatic differentiation to compute the Jacobian of a given function. This will test your understanding of both fundamental and advanced concepts of PyTorch autograd. Problem Description: 1. **Custom Autograd Function:** Implement a custom autograd function for the element-wise square (f(x) = x^2). 2. **Forward-Mode AD:** Use forward-mode automatic differentiation to compute the Jacobian of the function ( g(x) = [x_1^2, x_2^2, ..., x_n^2] ) with respect to the input tensor ( x ). Requirements: 1. **Custom Autograd Function:** - Define a custom autograd function using `torch.autograd.Function`. - Implement both the `forward` and `backward` methods. - The `backward` method should compute the derivative of ( f(x) = x^2 ). 2. **Jacobian Calculation using Forward-Mode AD:** - Create a function `compute_jacobian` that accepts a tensor ( x ) as input and returns the Jacobian matrix of ( g(x) ) using forward-mode automatic differentiation. - Use the `forward_ad` module for this purpose. Function Signatures: ```python import torch from torch.autograd import Function from torch.autograd import forward_ad class SquareFunction(Function): @staticmethod def forward(ctx, x): # Save context for backward pass ctx.save_for_backward(x) return x ** 2 @staticmethod def backward(ctx, grad_output): # Retrieve saved tensor x, = ctx.saved_tensors grad_input = 2 * x * grad_output return grad_input def compute_jacobian(x): # Your implementation for Jacobian computation using forward-mode AD dual_x = forward_ad.make_dual(x, torch.eye(x.size()[0])) dual_output = SquareFunction.apply(dual_x) jacobian = forward_ad.unpack_dual(dual_output).tangent return jacobian # Example usage: if __name__ == \'__main__\': x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True) jacobian = compute_jacobian(x) print(jacobian) ``` Input: A 1-dimensional tensor ( x ) with ( n ) elements. Output: A 2-dimensional tensor representing the Jacobian matrix of dimension ( n times n ). Constraints: - ( 1 leq n leq 10 ) Performance Requirements: - The solution should efficiently compute the Jacobian without using loops, leveraging PyTorch\'s efficient tensor operations.","solution":"import torch from torch.autograd import Function from torch.autograd.functional import jacobian class SquareFunction(Function): @staticmethod def forward(ctx, x): # Save context for backward pass ctx.save_for_backward(x) return x ** 2 @staticmethod def backward(ctx, grad_output): # Retrieve saved tensor x, = ctx.saved_tensors grad_input = 2 * x * grad_output return grad_input def compute_jacobian(x): def squared_function(x): return SquareFunction.apply(x) return jacobian(squared_function, x) # Example usage: if __name__ == \'__main__\': x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True) jacobian_matrix = compute_jacobian(x) print(jacobian_matrix)"},{"question":"You are provided with a series of physics experiments recording different values, and you need to process the data using various mathematical calculations. Implement a function `process_experiment_data(data)` that performs the following steps: 1. **Normalize the Data**: Normalize the recorded values in the `data` list by converting all angles from degrees to radians. 2. **Calculate Distance**: For each pair of points in the `data`, calculate the Euclidean distance using both direct manual calculation and the `math.dist` function for verification. 3. **Compute Statistical Values**: - Compute the precise floating-point sum of the distances using `math.fsum`. - Find the greatest common divisor (GCD) and least common multiple (LCM) of the integer parts of the distances. The `data` list contains tuples, where each tuple represents the coordinates of a point in (x, y) format. Function Signature ```python def process_experiment_data(data): # Your implementation here pass ``` Input - `data` (List[Tuple[float, float]]): A list of tuples where each tuple represents the coordinates of a point (x, y) with angle values in degrees. Output - The function should output a dictionary containing: - `\\"normalized_data\\"`: List of tuples where angles are converted to radians. - `\\"distances\\"`: List of distances calculated between each pair of points. - `\\"precise_sum_distances\\"`: The precise floating-point sum of the distances. - `\\"gcd_distances\\"`: The GCD of the integer parts of the distances. - `\\"lcm_distances\\"`: The LCM of the integer parts of the distances. Constraints - Each coordinate in `data` is within the range -1000 to 1000. - The number of points should not exceed 100. - Points are given in degrees; convert them to radians for calculations. Example ```python data = [ (200.0, 140.0), (122.0, 60.0), (300.0, 270.0) ] result = process_experiment_data(data) print(result) ``` Expected Output (values may differ due to specific implementation details): ```python { \\"normalized_data\\": [ (3.490658503988659, 2.443460952792061), (2.129301687433082, 1.0471975511965976), (5.235987755982989, 4.71238898038469) ], \\"distances\\": [80.374776, 157.379543, 104.115673], \\"precise_sum_distances\\": 341.870, \\"gcd_distances\\": 1, \\"lcm_distances\\": 157 } ``` Notes - Use the `math.radians` function to convert degrees to radians. - Calculate the Euclidean distance between points (x1, y1) and (x2, y2) using the formula: [ sqrt{(x2 - x1)^2 + (y2 - y1)^2} ] - Ensure your function handles precision and edge cases effectively.","solution":"import math from math import gcd from functools import reduce def lcm(a, b): return abs(a * b) // gcd(a, b) def process_experiment_data(data): # Convert degrees to radians normalized_data = [(math.radians(x), math.radians(y)) for x, y in data] # Calculate Euclidean distances distances = [] for i in range(len(normalized_data)): for j in range(i+1, len(normalized_data)): x1, y1 = normalized_data[i] x2, y2 = normalized_data[j] dist = math.sqrt((x2 - x1)**2 + (y2 - y1)**2) distances.append(dist) # Compute precise sum of distances precise_sum_distances = math.fsum(distances) # Compute GCD and LCM of integer parts of the distances int_distances = [int(dist) for dist in distances] gcd_distances = reduce(gcd, int_distances) lcm_distances = reduce(lcm, int_distances) return { \'normalized_data\': normalized_data, \'distances\': distances, \'precise_sum_distances\': precise_sum_distances, \'gcd_distances\': gcd_distances, \'lcm_distances\': lcm_distances }"},{"question":"# Custom Extension Dtype and ExtensionArray Implementation Objective: Create a custom pandas extension dtype for handling a specialized type of data, and implement an `ExtensionArray` to manage the data. The custom dtype will be for handling IP addresses, stored as strings in \\"xxx.xxx.xxx.xxx\\" format. You are required to implement the custom dtype and array, focusing on a few specific methods. Instructions: 1. **Define Custom Extension Dtype**: - Create a class `IPAddressDtype` that inherits from `pandas.api.extensions.ExtensionDtype`. - Implement the required properties and methods: - `name`: Return a string representing the name of the dtype (e.g., \\"ip_address\\"). - `type`: Return the `str` type. - `kind`: Return a string representing the kind of data (e.g., \\"O\\" for object). - `construct_array_type`: Return the `IPAddressArray` class. 2. **Implement ExtensionArray**: - Create a class `IPAddressArray` that inherits from `pandas.api.extensions.ExtensionArray`. - Implement the methods: - `__init__(self, values)`: Initialize the array with a sequence of IP address strings. - `dtype(self)`: Return an instance of `IPAddressDtype`. - `__len__(self)`: Return the length of the array. - `__getitem__(self, idx)`: Return the IP address at the given index. - `copy(self)`: Return a copy of the array. - `take(self, indices, allow_fill=False, fill_value=None)`: Return a new IP address array containing the elements specified by `indices`. 3. **Testing**: - You are provided with example IP addresses. Implement tests to validate the creation and usage of the custom dtype and array. Example: ```python import pandas as pd from pandas.api.extensions import ExtensionDtype, ExtensionArray class IPAddressDtype(ExtensionDtype): name = \'ip_address\' type = str kind = \'O\' def construct_array_type(self): return IPAddressArray class IPAddressArray(ExtensionArray): def __init__(self, values): self._data = list(values) @property def dtype(self): return IPAddressDtype() def __len__(self): return len(self._data) def __getitem__(self, idx): if isinstance(idx, int): return self._data[idx] else: return [self._data[i] for i in idx] def copy(self): return IPAddressArray(self._data.copy()) def take(self, indices, allow_fill=False, fill_value=None): if allow_fill: result = [self._data[i] if i != -1 else fill_value for i in indices] else: result = [self._data[i] for i in indices] return IPAddressArray(result) # Example Test ip_data = IPAddressArray([\'192.168.1.1\', \'10.0.0.1\', \'172.16.0.1\']) assert len(ip_data) == 3 assert ip_data[0] == \'192.168.1.1\' assert ip_data.take([0, 2])._data == [\'192.168.1.1\', \'172.16.0.1\'] # Register the dtype with pandas pd.api.extensions.register_extension_dtype(IPAddressDtype) ``` You need to implement the complete `IPAddressDtype` and `IPAddressArray` based on the guidelines given and write comprehensive tests to verify their functionality.","solution":"import pandas as pd from pandas.api.extensions import ExtensionDtype, ExtensionArray class IPAddressDtype(ExtensionDtype): name = \'ip_address\' type = str kind = \'O\' def construct_array_type(self): return IPAddressArray class IPAddressArray(ExtensionArray): def __init__(self, values): self._data = list(values) @property def dtype(self): return IPAddressDtype() def __len__(self): return len(self._data) def __getitem__(self, idx): if isinstance(idx, int): return self._data[idx] else: return [self._data[i] for i in idx] def copy(self): return IPAddressArray(self._data.copy()) def take(self, indices, allow_fill=False, fill_value=None): if allow_fill: result = [self._data[i] if i != -1 else fill_value for i in indices] else: result = [self._data[i] for i in indices] return IPAddressArray(result) # Register the dtype with pandas pd.api.extensions.register_extension_dtype(IPAddressDtype)"},{"question":"# Task Using the Seaborn library, write a function `custom_pointplot` that: 1. Loads the \\"penguins\\" dataset. 2. Generates a point plot showing the average body mass of penguins grouped by island and differentiated by species. 3. Customizes the plot to use different markers and linestyles for each species and represents the error bars using standard deviation. 4. Applies a specific dodge value to avoid overplotting. 5. Customizes the appearance of the points to use a diamond marker with a specific color. 6. Adds the values of the average body mass as an annotation above each point. 7. Includes proper labels and a title for the plot. # Function Signature ```python import seaborn as sns import matplotlib.pyplot as plt def custom_pointplot() -> None: pass ``` # Example Output The function should output a plot where: - The x-axis represents the islands. - The y-axis represents the body mass of penguins. - Different species are reflected in the color, marker, and linestyle. - Error bars represent the standard deviation. - The points are dodged to reduce overplotting. - Plot points are drawn using diamond markers. - Each point has an annotated average value above it. - The axes are properly labeled, and the plot has an appropriate title. # Constraints - Use Seaborn and Matplotlib libraries only. - Ensure the function shows the plot but does not return any values. - Refer to the Seaborn documentation for correct usage of the functions and parameters. # Implementation Implement the function `custom_pointplot` to meet the given requirements.","solution":"import seaborn as sns import matplotlib.pyplot as plt def custom_pointplot() -> None: # Load the penguins dataset penguins = sns.load_dataset(\\"penguins\\") # Create the point plot plt.figure(figsize=(10, 6)) pointplot = sns.pointplot( x=\\"island\\", y=\\"body_mass_g\\", hue=\\"species\\", data=penguins, dodge=0.3, markers=[\\"D\\", \\"o\\", \\"s\\"], # Different markers for each species linestyles=[\\"-\\", \\"--\\", \\"-.\\"], # Different linestyles for each species ci=\\"sd\\" # Standard deviation for error bars ) # Customize diamond marker color (all black in this example) for line in pointplot.lines: line.set_markerfacecolor(\'black\') line.set_markeredgecolor(\'black\') # Annotate each point with the average body mass for i, artist in enumerate(pointplot.lines): for j, value in enumerate(artist.get_ydata()): pointplot.text( artist.get_xdata()[j], value, f\'{value:.1f}\', ha=\'center\', va=\'bottom\', color=\'black\' ) # Set labels and title plt.xlabel(\\"Island\\") plt.ylabel(\\"Body Mass (g)\\") plt.title(\\"Average Body Mass of Penguins by Island and Species\\") plt.legend(title=\\"Species\\") # Show the plot plt.show()"},{"question":"# Objective You are to demonstrate your understanding of pandas\' time series functionalities and operations. Specifically, you will create and manipulate time indices, perform resampling, and handle time zone conversions. # Question **Problem Statement:** You are given a dataset of electricity usage measurements. Each row contains a timestamp and the corresponding power usage in kilowatts (kW). Write a Python function using pandas that performs the following tasks: 1. Load the data from the given list of dictionaries (representing rows of the dataset). 2. Convert the timestamps to a timezone-aware datetime index localized to UTC. 3. Convert the UTC times to the \'US/Eastern\' timezone. 4. Resample the data to calculate the daily total power usage. 5. Identify any days where the recorded power usage is greater than a specified threshold (given as input). 6. Handle any ambiguous times by inferring the correct offset and nonexistent times by replacing them with `NaT`. # Input ```python data = [ {\\"timestamp\\": \\"2023-10-27 23:00:00\\", \\"power_usage\\": 5}, {\\"timestamp\\": \\"2023-10-28 00:00:00\\", \\"power_usage\\": 4}, {\\"timestamp\\": \\"2023-10-28 01:00:00\\", \\"power_usage\\": 6}, {\\"timestamp\\": \\"2023-10-28 02:00:00\\", \\"power_usage\\": 3}, {\\"timestamp\\": \\"2023-11-01 10:30:00\\", \\"power_usage\\": 10}, {\\"timestamp\\": \\"2023-11-01 11:30:00\\", \\"power_usage\\": 5}, {\\"timestamp\\": \\"2023-11-05 01:30:00\\", \\"power_usage\\": 7}, {\\"timestamp\\": \\"2023-11-05 02:30:00\\", \\"power_usage\\": 9} ] threshold = 20 ``` # Output A DataFrame containing the days where the daily total power usage exceeds the specified threshold. # Expected Function ```python import pandas as pd def process_power_usage(data, threshold): # Task 1: Load the data into a DataFrame df = pd.DataFrame(data) # Task 2: Convert the timestamps to timezone-aware datetime index localized to UTC df[\'timestamp\'] = pd.to_datetime(df[\'timestamp\']) df = df.set_index(\'timestamp\') df.index = df.index.tz_localize(\'UTC\', ambiguous=\'infer\', nonexistent=\'NaT\') # Task 3: Convert the UTC times to \'US/Eastern\' timezone df.index = df.index.tz_convert(\'US/Eastern\') # Task 4: Resample the data to calculate the daily total power usage daily_usage = df[\'power_usage\'].resample(\'D\').sum() # Task 5: Identify any days where the recorded power usage is greater than the specified threshold high_usage_days = daily_usage[daily_usage > threshold] # Return the days with high power usage return high_usage_days.reset_index() # Test the function with the provided data and threshold result = process_power_usage(data, threshold) print(result) ``` Note: Ensure that you handle both ambiguous and nonexistent times as described using the `ambiguous=\'infer\'` and `nonexistent=\'NaT\'` parameters.","solution":"import pandas as pd def process_power_usage(data, threshold): # Task 1: Load the data into a DataFrame df = pd.DataFrame(data) # Task 2: Convert the timestamps to timezone-aware datetime index localized to UTC df[\'timestamp\'] = pd.to_datetime(df[\'timestamp\']) df = df.set_index(\'timestamp\') df.index = df.index.tz_localize(\'UTC\', ambiguous=\'infer\', nonexistent=\'NaT\') # Task 3: Convert the UTC times to \'US/Eastern\' timezone df.index = df.index.tz_convert(\'US/Eastern\') # Task 4: Resample the data to calculate the daily total power usage daily_usage = df[\'power_usage\'].resample(\'D\').sum() # Task 5: Identify any days where the recorded power usage is greater than the specified threshold high_usage_days = daily_usage[daily_usage > threshold] # Return the days with high power usage return high_usage_days.reset_index()"},{"question":"**Question:** You are given a text consisting of various characters. Your task is to implement a function `analyze_text` that processes this text to output certain Unicode character properties and normalized forms of the text. **Function Signature:** ```python def analyze_text(input_text: str) -> dict: pass ``` **Input:** - `input_text`: A string containing Unicode characters. **Output:** - The function should return a dictionary with the following keys and corresponding values: - `\'character_details\'`: A list of dictionaries, each containing details of a character (excluding spaces) in the input text. Each dictionary should contain: - `\'char\'`: The character itself. - `\'name\'`: The Unicode name of the character. - `\'decimal\'`: The decimal value if the character has a decimal value, else `None`. - `\'category\'`: The general Unicode category of the character. - `\'normalized_forms\'`: A dictionary containing the normalized forms of the input text, with keys as the normalization form names (`\'NFC\'`, `\'NFKC\'`, `\'NFD\'`, `\'NFKD\'`) and values as the respective normalized text. **Constraints:** - The function should handle text with a mix of different character types (letters, digits, symbols, etc.). - Assume the input text length does not exceed 1000 characters. **Example:** ```python input_text = \\"π2ё\\" result = analyze_text(input_text) # Example structure of the result dictionary expected_result = { \'character_details\': [ { \'char\': \'π\', \'name\': \'GREEK SMALL LETTER PI\', \'decimal\': None, \'category\': \'Ll\' }, { \'char\': \'2\', \'name\': \'DIGIT TWO\', \'decimal\': 2, \'category\': \'Nd\' }, { \'char\': \'ё\', \'name\': \'CYRILLIC SMALL LETTER IO\', \'decimal\': None, \'category\': \'Ll\' } ], \'normalized_forms\': { \'NFC\': \\"π2ё\\", \'NFKC\': \\"π2ё\\", \'NFD\': \\"π2ё\\", \'NFKD\': \\"π2ё\\" } } assert result == expected_result ``` **Notes:** - Use the `unicodedata` module functions provided in the documentation to implement your solution. - Handle exceptions, such as characters without a Unicode name or decimal value, appropriately.","solution":"import unicodedata def analyze_text(input_text: str) -> dict: character_details = [] for char in input_text: if char.isspace(): continue char_info = { \'char\': char, \'name\': unicodedata.name(char, \\"UNKNOWN\\"), # get Unicode name or \'UNKNOWN\' if not found \'decimal\': unicodedata.decimal(char, None), # get decimal value or None if not a digit \'category\': unicodedata.category(char) # get Unicode category } character_details.append(char_info) normalized_forms = { \'NFC\': unicodedata.normalize(\'NFC\', input_text), \'NFKC\': unicodedata.normalize(\'NFKC\', input_text), \'NFD\': unicodedata.normalize(\'NFD\', input_text), \'NFKD\': unicodedata.normalize(\'NFKD\', input_text) } return { \'character_details\': character_details, \'normalized_forms\': normalized_forms }"},{"question":"**Question: Password Policy Compliance Checker** Given the `spwd` module which provides access to the Unix shadow password database, your task is to write a function `check_password_compliance(min_days: int, max_days: int, warn_days: int) -> List[str]`. This function will check if user accounts comply with specified password policy requirements defined by the following parameters: - `min_days`: The minimum number of days between password changes. - `max_days`: The maximum number of days between password changes. - `warn_days`: The number of days before password expiration to warn the user. **Function Signature:** ```python def check_password_compliance(min_days: int, max_days: int, warn_days: int) -> List[str]: pass ``` **Parameters:** 1. `min_days` (int): The minimum number of days required between password changes. 2. `max_days` (int): The maximum number of days allowed between password changes. 3. `warn_days` (int): The number of days before password expiration when the user should be warned. **Returns:** - `List[str]`: A list of login names (`sp_namp`) of users who are not in compliance with the specified password policy. **Constraints:** 1. Your function should use the `spwd` module to retrieve the necessary shadow password data. 2. Avoid running the function if you do not have the necessary privileges, and handle such cases by returning an empty list. 3. Ensure that all shadow password entries are checked against the specified policy. **Example:** ```python # Example scenario (Note: This requires root privileges to execute): non_compliant_users = check_password_compliance(7, 90, 14) print(non_compliant_users) # Example Output: [\'user1\', \'user3\'] ``` **Hints:** - Use the `spwd.getspall()` function to retrieve all shadow password entries. - Analyze each entry\'s `sp_min`, `sp_max`, and `sp_warn` attributes to check compliance with the given parameters. Ensure your code handles potential exceptions, particularly those related to permission errors, and write clean and efficient Python code that adheres to best practices.","solution":"import spwd from typing import List def check_password_compliance(min_days: int, max_days: int, warn_days: int) -> List[str]: try: # Retrieve all shadow password entries shadow_entries = spwd.getspall() except PermissionError: # Return an empty list if we don\'t have necessary privileges return [] non_compliant_users = [] for entry in shadow_entries: if (entry.sp_min < min_days or entry.sp_max > max_days or entry.sp_warn < warn_days): non_compliant_users.append(entry.sp_namp) return non_compliant_users"},{"question":"Objective: To demonstrate your understanding of the `logging.config` module in Python by configuring a logging system using a dictionary and implementing a custom logging handler. Problem Statement: You are required to set up the logging configuration of a Python application using the `dictConfig` function from the `logging.config` module. You also need to create a custom logging handler that outputs log messages to both the console and a list. Finally, you must write a function to apply this configuration and test it with various log messages. Requirements: 1. Define the logging configuration using a dictionary. 2. Implement a custom logging handler class, `ListStreamHandler`, which inherits from `logging.StreamHandler` and also stores log messages in a list. 3. Write a function `configure_logging(config)` that takes a logging configuration dictionary and applies it using `dictConfig`. 4. Write tests within a function `test_logging()` to verify the logging configuration and custom handler. Input and Output Formats: - **Input for `configure_logging(config)`**: A dictionary that specifies the logging configuration. - **Output for `configure_logging(config)`**: No return value; however, it should apply the logging configuration. - **Output of `test_logging()`**: The content of the list within the custom handler should be printed, verifying it contains the expected log messages. Constraints: - Use Python\'s `logging` and `logging.config` modules. - The configuration should include: - A formatter. - A logger with the custom handler and a basic stream handler. - Make sure to handle any exceptions that might occur during log configuration setup. Example: ```python import logging import logging.config from typing import List class ListStreamHandler(logging.StreamHandler): def __init__(self): super().__init__() self.log_list = [] def emit(self, record): log_entry = self.format(record) self.log_list.append(log_entry) super().emit(record) def configure_logging(config: dict): logging.config.dictConfig(config) def test_logging(): handler = ListStreamHandler() logging.basicConfig(level=logging.DEBUG, handlers=[handler]) # Example messages logger = logging.getLogger(\\"testLogger\\") logger.debug(\\"This is a debug message\\") logger.info(\\"This is an info message\\") logger.warning(\\"This is a warning message\\") # Print the log list print(handler.log_list) # Example config dictionary (this needs to be completed by students) config = { \'version\': 1, \'formatters\': { \'detailed\': { \'format\': \'%(asctime)s %(name)s %(levelname)s %(message)s\', } }, \'handlers\': { \'listHandler\': { \'class\': \'ListStreamHandler\', # This needs to be correctly handled by students \'formatter\': \'detailed\' }, \'console\': { \'class\': \'logging.StreamHandler\', \'formatter\': \'detailed\' } }, \'loggers\': { \'testLogger\': { \'level\': \'DEBUG\', \'handlers\': [\'listHandler\', \'console\'] } } } # Call the function to apply configuration configure_logging(config) # Test the logging configuration test_logging() ``` Write the missing parts of the `ListStreamHandler`, `configure_logging`, and `test_logging` functions to ensure that the logging configuration is correctly applied and tested.","solution":"import logging import logging.config from typing import List class ListStreamHandler(logging.StreamHandler): def __init__(self): super().__init__() self.log_list = [] def emit(self, record): log_entry = self.format(record) self.log_list.append(log_entry) super().emit(record) def configure_logging(config: dict): logging.config.dictConfig(config) def test_logging(): handler = ListStreamHandler() logger = logging.getLogger(\'testLogger\') logger.addHandler(handler) logger.setLevel(logging.DEBUG) # Example messages logger.debug(\\"This is a debug message\\") logger.info(\\"This is an info message\\") logger.warning(\\"This is a warning message\\") # Return the log list for assertion return handler.log_list # Example config dictionary config = { \'version\': 1, \'formatters\': { \'detailed\': { \'format\': \'%(asctime)s %(name)s %(levelname)s %(message)s\', } }, \'handlers\': { \'listHandler\': { \'()\': ListStreamHandler, \'formatter\': \'detailed\' }, \'console\': { \'class\': \'logging.StreamHandler\', \'formatter\': \'detailed\' } }, \'loggers\': { \'testLogger\': { \'level\': \'DEBUG\', \'handlers\': [\'listHandler\', \'console\'] } } } # Call the function to apply configuration configure_logging(config)"},{"question":"# Dynamic Class Creation and Manipulation In this task, you will create a dynamic class using the `types` module and perform various operations on it. The goal is to demonstrate your understanding of dynamic class creation, type handling, and metaclasses in Python. Task Details: 1. **Create a Dynamic Class**: - Use the `types.new_class` function to create a class named `DynamicPerson`. - This class should have the following structure: - Attributes: `name` (str), `age` (int). - Methods: `greet()` which returns a greeting string based on the name, and `is_adult()` that returns a boolean indicating if age is 18 or greater. 2. **Manipulate the Dynamic Class**: - Instantiate an object of `DynamicPerson` with a name and age. - Modify the `name` and `age` attributes dynamically. - Verify the type and behaviors of the methods. 3. **Type Handling**: - Write a function `display_type_info` that takes an object and prints its type information using the types from the `types` module (such as `FunctionType`, `MethodType`, etc.). Example: ```python # Example Usage dp = DynamicPerson(name=\\"Alice\\", age=30) print(dp.greet()) # Output: \\"Hello, my name is Alice.\\" print(dp.is_adult()) # Output: True dp.name = \\"Bob\\" dp.age = 17 print(dp.greet()) # Output: \\"Hello, my name is Bob.\\" print(dp.is_adult()) # Output: False # Display type information display_type_info(dp) ``` Constraints: - Do not use the `type` function directly for class creation. Use `types.new_class`. - Ensure the methods are defined properly within the dynamic class. - Handle type checking using the `types` module where applicable. Performance Requirements: - The class creation and manipulation should have minimal overhead. - The `display_type_info` function should efficiently retrieve and display type information. Implement the following functions: ```python import types def create_dynamic_person_class(): # Create the DynamicPerson class pass def display_type_info(obj): # Display type information using the types module pass ```","solution":"import types def create_dynamic_person_class(): Creates and returns the DynamicPerson class using types.new_class. def greet(self): return f\\"Hello, my name is {self.name}.\\" def is_adult(self): return self.age >= 18 cls_dict = { \'__init__\': lambda self, name, age: setattr(self, \'name\', name) or setattr(self, \'age\', age), \'greet\': greet, \'is_adult\': is_adult } return types.new_class(\'DynamicPerson\', (), {}, lambda ns: ns.update(cls_dict)) def display_type_info(obj): Displays the type information of the given object. import types print(f\'Type of object: {type(obj)}\') for attr_name in dir(obj): attr = getattr(obj, attr_name) if isinstance(attr, types.FunctionType): print(f\'{attr_name} is a function.\') elif isinstance(attr, types.MethodType): print(f\'{attr_name} is a method.\') else: print(f\'{attr_name} is of type {type(attr)}\')"},{"question":"You are tasked with creating a custom server to handle a simplified version of the SMTP (Simple Mail Transfer Protocol) using the `asynchat` module in Python. The server should accept incoming connections, read SMTP requests, and respond appropriately based on the commands received. # Requirements: 1. Implement a subclass of `asynchat.async_chat` named `SMTPChannel` that handles the following SMTP commands: - `HELO`: Respond with \\"250 Hello\\". - `MAIL FROM:<address>`: Respond with \\"250 OK\\". - `RCPT TO:<address>`: Respond with \\"250 OK\\". - `DATA`: Respond with \\"354 Start mail input; end with <CRLF>.<CRLF>\\" and collect incoming data until a single line containing just a \\".\\" character is received, then respond with \\"250 OK\\". - `QUIT`: Respond with \\"221 Bye\\" and close the connection. 2. Use the `set_terminator()` method to handle command termination based on the following criteria: - Commands are terminated by `\\"rn\\"`. - Data input after receiving `DATA` command is terminated by a line containing just `\\".rn\\"`. 3. Implement the `found_terminator()` and `collect_incoming_data()` methods to process incoming data and respond according to the SMTP protocol. 4. Write a `SMTPServer` class that initializes the `asyncore` server and handles incoming connections by creating `SMTPChannel` instances. # Expected Input and Output Formats: - The server will receive commands from a client over a socket connection. - Responses should be sent back to the client according to the specified responses for each command. # Constraints: - You should manage the asynchronous nature of incoming and outgoing data properly. - Assume a single connection at a time for simplicity, although your implementation should support multiple connections if properly integrated with `asyncore`. # Example: 1. Client sends `HELO localhost` - Server responds with `250 Hello` 2. Client sends `MAIL FROM:<address>` - Server responds with `250 OK` 3. Client sends `RCPT TO:<address>` - Server responds with `250 OK` 4. Client sends `DATA` - Server responds with `354 Start mail input; end with <CRLF>.<CRLF>` 5. Client sends the email body followed by `.rn` - Server responds with `250 OK` 6. Client sends `QUIT` - Server responds with `221 Bye` and closes the connection ```python import asynchat import asyncore import socket class SMTPChannel(asynchat.async_chat): def __init__(self, conn, addr): asynchat.async_chat.__init__(self, conn) self.addr = addr self.ibuffer = [] self.set_terminator(b\'rn\') self.push(b\'220 Welcomern\') def collect_incoming_data(self, data): self.ibuffer.append(data) def found_terminator(self): command = b\'\'.join(self.ibuffer).decode(\'ascii\') self.ibuffer = [] print(f\\"Received: {command}\\") if command.startswith(\\"HELO\\"): self.push(b\\"250 Hellorn\\") elif command.startswith(\\"MAIL FROM:\\"): self.push(b\\"250 OKrn\\") elif command.startswith(\\"RCPT TO:\\"): self.push(b\\"250 OKrn\\") elif command.startswith(\\"DATA\\"): self.push(b\\"354 Start mail input; end with <CRLF>.<CRLF>rn\\") self.set_terminator(b\'rn.rn\') elif command == \\".\\": self.push(b\\"250 OKrn\\") self.set_terminator(b\'rn\') elif command == \\"QUIT\\": self.push(b\\"221 Byern\\") self.close_when_done() else: self.push(b\\"500 Command unrecognizedrn\\") class SMTPServer(asyncore.dispatcher): def __init__(self, host, port): asyncore.dispatcher.__init__(self) self.create_socket(socket.AF_INET, socket.SOCK_STREAM) self.set_reuse_addr() self.bind((host, port)) self.listen(5) print(f\\"Listening on {host}:{port}\\") def handle_accepted(self, conn, addr): print(f\\"Connection from {addr}\\") SMTPChannel(conn, addr) if __name__ == \\"__main__\\": server = SMTPServer(\'localhost\', 8025) asyncore.loop() ``` # Notes: - Make sure your implementation is both robust and efficient. - You may simulate a client and test your server locally to verify the correctness of your implementation.","solution":"import asynchat import asyncore import socket class SMTPChannel(asynchat.async_chat): def __init__(self, conn, addr): asynchat.async_chat.__init__(self, conn) self.addr = addr self.ibuffer = [] self.mail_buffer = [] self.state = \'COMMAND\' self.set_terminator(b\'rn\') self.push(b\'220 Welcomern\') def collect_incoming_data(self, data): if self.state == \'COMMAND\': self.ibuffer.append(data) elif self.state == \'DATA\': self.mail_buffer.append(data) def found_terminator(self): if self.state == \'COMMAND\': command = b\'\'.join(self.ibuffer).decode(\'ascii\') self.ibuffer = [] print(f\\"Received: {command}\\") if command.startswith(\\"HELO\\"): self.push(b\\"250 Hellorn\\") elif command.startswith(\\"MAIL FROM:\\"): self.push(b\\"250 OKrn\\") elif command.startswith(\\"RCPT TO:\\"): self.push(b\\"250 OKrn\\") elif command.startswith(\\"DATA\\"): self.push(b\\"354 Start mail input; end with <CRLF>.<CRLF>rn\\") self.state = \'DATA\' self.set_terminator(b\'rn.rn\') elif command == \\"QUIT\\": self.push(b\\"221 Byern\\") self.close_when_done() else: self.push(b\\"500 Command unrecognizedrn\\") elif self.state == \'DATA\': self.state = \'COMMAND\' self.set_terminator(b\'rn\') self.mail_buffer = [] self.push(b\\"250 OKrn\\") class SMTPServer(asyncore.dispatcher): def __init__(self, host, port): asyncore.dispatcher.__init__(self) self.create_socket(socket.AF_INET, socket.SOCK_STREAM) self.set_reuse_addr() self.bind((host, port)) self.listen(5) print(f\\"Listening on {host}:{port}\\") def handle_accepted(self, conn, addr): print(f\\"Connection from {addr}\\") SMTPChannel(conn, addr) if __name__ == \\"__main__\\": server = SMTPServer(\'localhost\', 8025) asyncore.loop()"},{"question":"# Complex Number Manipulation and Arithmetic **Objective:** Your task is to implement a function that performs a sequence of complex number operations using the Python C API functions described in the documentation. # Problem Description You are required to create a Python function `complex_operations(operations, complex_numbers)`. This function will take in two parameters: 1. `operations`: A list of tuples, where each tuple contains an operation name (as a string) and the indices of the operands in the `complex_numbers` list. - Supported operations are `\\"sum\\"`, `\\"diff\\"`, `\\"prod\\"`, `\\"quot\\"`, `\\"neg\\"`, and `\\"pow\\"`. - For `\\"neg\\"`, only one index is provided. 2. `complex_numbers`: A list of tuples, where each tuple represents a complex number with its real and imaginary parts `(real, imag)`. The function should: 1. Initialize complex numbers from `complex_numbers` list using `PyComplex_FromDoubles`. 2. Perform the sequence of operations specified in the `operations` list. 3. Return the final complex number as a tuple `(real, imag)`. # Example ```python def complex_operations(operations, complex_numbers): pass # Your code here # Example usage: operations = [(\\"sum\\", 0, 1), (\\"prod\\", 2, 1), (\\"neg\\", 0)] complex_numbers = [(3.0, 4.0), (1.0, 2.0), (2.0, -1.0)] result = complex_operations(operations, complex_numbers) print(result) # Should return the resulting complex number as a tuple `(real, imag)` ``` # Specifications 1. **Input:** - `operations`: List of tuples, each tuple containing an operation name and indices. - `complex_numbers`: List of tuples, each tuple containing two floats `(real, imag)`. 2. **Output:** - Final complex number as a tuple `(real, imag)`. 3. **Constraints:** - `0 <= len(operations) <= 100` - `0 <= len(complex_numbers) <= 100` - Real and imaginary parts of complex numbers are floats. - Valid operations in `operations` list are `\\"sum\\"`, `\\"diff\\"`, `\\"prod\\"`, `\\"quot\\"`, `\\"neg\\"`, and `\\"pow\\"`. 4. **Performance Requirements:** - Solution should be efficient in terms of both time and space complexity. # Hints 1. You may use a dictionary to map operation names to corresponding functions. 2. Remember to handle edge cases, such as division by zero when performing the `\\"quot\\"` operation. 3. Ensure proper error handling and use appropriate functions for extracting and converting complex numbers. **Note:** Utilize the C API functions where applicable as described in the provided documentation.","solution":"def complex_operations(operations, complex_numbers): Perform a sequence of complex number operations and return the final result as a complex tuple (real, imag). Parameters: - operations: A list of tuples containing the operation name and indices of the complex numbers. - complex_numbers: A list of tuples where each tuple contains the real and imaginary parts of a complex number. Returns: - A tuple (real, imag) representing the resulting complex number after performing all operations. def to_complex(complex_tuple): Convert a tuple (real, imag) to a complex number. return complex(complex_tuple[0], complex_tuple[1]) def to_tuple(complex_number): Convert a complex number to a tuple (real, imag). return (complex_number.real, complex_number.imag) # Convert the list of complex number tuples to actual complex number objects complex_list = [to_complex(num) for num in complex_numbers] # Dictionary mapping operation names to lambda functions that implement them operations_dict = { \\"sum\\": lambda x, y: x + y, \\"diff\\": lambda x, y: x - y, \\"prod\\": lambda x, y: x * y, \\"quot\\": lambda x, y: x / y if y != 0 else complex(float(\'inf\'), float(\'inf\')), # Handle division by zero appropriately if needed \\"neg\\": lambda x: -x, \\"pow\\": lambda x, y: x ** y } result = complex_list[operations[0][1]] if len(operations) > 0 else complex_numbers[0] for op in operations: op_name = op[0] if op_name == \\"neg\\": idx = op[1] result = operations_dict[op_name](complex_list[idx]) else: idx1 = op[1] idx2 = op[2] result = operations_dict[op_name](complex_list[idx1], complex_list[idx2]) return to_tuple(result)"},{"question":"# Programming Assessment: XML Document Manipulation **Objective:** Implement a function that constructs, manipulates, and queries an XML document using the Document Object Model (DOM) API provided by the `xml.dom` module. This task will test your ability to work with various node types, navigate the DOM tree, and manipulate attributes and elements. # Task: You are given a function signature `def build_and_modify_xml():` which you need to implement to perform the following tasks: 1. **Create an XML Document:** - Build a new `Document` object. - Create a root element `<library>`. - Append the root element to the `Document`. 2. **Add Elements with Attributes:** - Create three child elements `<book>` each having attributes `title` and `author`. - Append these `<book>` elements to the `<library>` root element. 3. **Add Text Nodes:** - For each `<book>` element, add a child `<summary>` element containing text summarizing the book. 4. **Modify Elements:** - Update the `title` attribute of one of the `<book>` elements to a new value. - Remove the `summary` element from one of the `<book>` elements. 5. **Query the Document:** - Implement a function `get_books_by_author(document, author_name)` that returns a list of titles of books written by the specified author. # Constraints: - Each book must have a `title` and `author` attribute. - The `title` attributes must be unique across all `<book>` elements. - The document must conform to standard XML structure and requirements. # Example: ```python from xml.dom.minidom import Document def build_and_modify_xml(): doc = Document() # Create root element <library> library = doc.createElement(\\"library\\") doc.appendChild(library) # Add three <book> elements with attributes books_data = [ {\\"title\\": \\"1984\\", \\"author\\": \\"George Orwell\\", \\"summary\\": \\"Dystopian social science fiction novel\\"}, {\\"title\\": \\"Brave New World\\", \\"author\\": \\"Aldous Huxley\\", \\"summary\\": \\"Science fiction novel\\"}, {\\"title\\": \\"Fahrenheit 451\\", \\"author\\": \\"Ray Bradbury\\", \\"summary\\": \\"Dystopian novel\\"} ] for book in books_data: book_elem = doc.createElement(\\"book\\") book_elem.setAttribute(\\"title\\", book[\\"title\\"]) book_elem.setAttribute(\\"author\\", book[\\"author\\"]) summary_elem = doc.createElement(\\"summary\\") summary_text = doc.createTextNode(book[\\"summary\\"]) summary_elem.appendChild(summary_text) book_elem.appendChild(summary_elem) library.appendChild(book_elem) # Modify the title of one of the <book> elements books = doc.getElementsByTagName(\\"book\\") for book in books: if book.getAttribute(\\"title\\") == \\"1984\\": book.setAttribute(\\"title\\", \\"Nineteen Eighty-Four\\") break # Remove the summary of one of the <book> elements for book in books: if book.getAttribute(\\"title\\") == \\"Brave New World\\": summary_elem = book.getElementsByTagName(\\"summary\\")[0] book.removeChild(summary_elem) break return doc def get_books_by_author(document, author_name): books = document.getElementsByTagName(\\"book\\") titles = [] for book in books: if book.getAttribute(\\"author\\") == author_name: titles.append(book.getAttribute(\\"title\\")) return titles # Example usage doc = build_and_modify_xml() print(get_books_by_author(doc, \\"George Orwell\\")) # Output: [\'Nineteen Eighty-Four\'] ``` # Submission Requirements: - Implement the two functions `build_and_modify_xml()` and `get_books_by_author(document, author_name)`. - Ensure your solution correctly handles the creation, modification, and querying of the XML document. - The code must be executable without errors and conform to the above specifications.","solution":"from xml.dom.minidom import Document def build_and_modify_xml(): Builds and modifies an XML document based on the specified tasks. Returns: Document: The modified XML document. doc = Document() # Create root element <library> library = doc.createElement(\\"library\\") doc.appendChild(library) # Add three <book> elements with attributes books_data = [ {\\"title\\": \\"1984\\", \\"author\\": \\"George Orwell\\", \\"summary\\": \\"Dystopian social science fiction novel\\"}, {\\"title\\": \\"Brave New World\\", \\"author\\": \\"Aldous Huxley\\", \\"summary\\": \\"Science fiction novel\\"}, {\\"title\\": \\"Fahrenheit 451\\", \\"author\\": \\"Ray Bradbury\\", \\"summary\\": \\"Dystopian novel\\"} ] for book in books_data: book_elem = doc.createElement(\\"book\\") book_elem.setAttribute(\\"title\\", book[\\"title\\"]) book_elem.setAttribute(\\"author\\", book[\\"author\\"]) summary_elem = doc.createElement(\\"summary\\") summary_text = doc.createTextNode(book[\\"summary\\"]) summary_elem.appendChild(summary_text) book_elem.appendChild(summary_elem) library.appendChild(book_elem) # Modify the title of one of the <book> elements books = doc.getElementsByTagName(\\"book\\") for book in books: if book.getAttribute(\\"title\\") == \\"1984\\": book.setAttribute(\\"title\\", \\"Nineteen Eighty-Four\\") break # Remove the summary of one of the <book> elements for book in books: if book.getAttribute(\\"title\\") == \\"Brave New World\\": summary_elem = book.getElementsByTagName(\\"summary\\")[0] book.removeChild(summary_elem) break return doc def get_books_by_author(document, author_name): Gets a list of book titles by the specified author from the XML document. Args: document (Document): The XML document to query. author_name (str): The author name to filter books by. Returns: list: A list of book titles by the specified author. books = document.getElementsByTagName(\\"book\\") titles = [] for book in books: if book.getAttribute(\\"author\\") == author_name: titles.append(book.getAttribute(\\"title\\")) return titles"},{"question":"# Coding Assessment Question Objective Create a Python program that uses the `concurrent.futures` module to compute the factorial of multiple numbers in parallel. This will demonstrate your understanding of managing concurrent tasks, handling asynchronous results, and implementing error-handling mechanisms in Python. Question You are tasked with writing a Python function, `compute_factorials(numbers)`, that takes a list of integers and computes the factorial of each integer in parallel using the `concurrent.futures` module. # Function Signature ```python def compute_factorials(numbers: List[int]) -> Dict[int, Union[int, str]]: ``` # Input - `numbers`: A list of non-negative integers for which the factorial needs to be computed. (List[int]) # Output - The function should return a dictionary where the keys are the integers from the input list and the values are their corresponding factorials. If an error occurs during the computation (such as a very large integer leading to a recursion limit error), the value should be an error message. (Dict[int, Union[int, str]]) # Constraints - You must use the `concurrent.futures` module to parallelize the task. - Each factorial calculation should be done asynchronously. - Implement proper error handling to manage large input values that could lead to computation errors. - You can assume that the input list will contain only non-negative integers. - Performance: Your solution should handle lists of up to 10,000 integers efficiently. # Example ```python from typing import List, Dict, Union def compute_factorials(numbers: List[int]) -> Dict[int, Union[int, str]]: pass # Example usage numbers = [5, 7, 10, 20] result = compute_factorials(numbers) print(result) # Expected: {5: 120, 7: 5040, 10: 3628800, 20: 2432902008176640000} ``` # Hint Use `concurrent.futures.ThreadPoolExecutor` or `concurrent.futures.ProcessPoolExecutor` to manage a pool of worker threads or processes. # Additional Information Handling factorials for large numbers might cause long computation times or `RecursionError`. Make sure to include appropriate try-except blocks to handle such cases gracefully and return an appropriate error message instead of crashing the program.","solution":"from typing import List, Dict, Union import concurrent.futures import math def compute_factorials(numbers: List[int]) -> Dict[int, Union[int, str]]: def safe_factorial(n: int) -> Union[int, str]: try: return math.factorial(n) except (RecursionError, OverflowError) as e: return str(e) with concurrent.futures.ThreadPoolExecutor() as executor: future_to_number = {executor.submit(safe_factorial, num): num for num in numbers} results = {} for future in concurrent.futures.as_completed(future_to_number): number = future_to_number[future] try: result = future.result() except Exception as exc: result = str(exc) results[number] = result return results"},{"question":"# Custom Pickling and Copying with `copyreg` You have been given a task to manage a custom-designed class where default pickling and copying mechanisms are insufficient. Your goal is to implement custom pickling and copying logic using the `copyreg` module in Python. # Problem Description Define a class `Person` with the following properties: - `name` (string) - `age` (integer) - `address` (string) Implement custom pickling and copying logic for the `Person` class with the following requirements: 1. **Definition of the `Person` class**: - A constructor that initializes `name`, `age`, and `address`. - A `__str__` method to represent the `Person` object as a string in the format `Name: {name}, Age: {age}, Address: {address}`. 2. **Custom pickling logic**: - Define a function `pickle_person` that takes a `Person` object and returns a tuple containing the class and a tuple of its properties (`name`, `age`, `address`). - Register the `pickle_person` function using `copyreg.pickle`. 3. **Custom copying logic**: - Ensure that when a `Person` object is copied using the `copy` module, the custom pickling function is invoked. # Constraints - Assume the `name` string length is not more than 50 characters. - The `age` is a positive integer not exceeding 120. - The `address` string length is not more than 100 characters. # Input/Output - There is no specific input/output required from standard I/O. Your task is to define the class and ensure the proper functioning of `pickle` and `copy`. # Example ```python import copyreg, copy, pickle # Define the Person class class Person: def __init__(self, name, age, address): self.name = name self.age = age self.address = address def __str__(self): return f\\"Name: {self.name}, Age: {self.age}, Address: {self.address}\\" # Implement the pickling function def pickle_person(person): print(\\"pickling a Person instance...\\") return Person, (person.name, person.age, person.address) # Register the pickling function copyreg.pickle(Person, pickle_person) # Testing the custom pickling and copying p = Person(\\"John Doe\\", 30, \\"123 Main St\\") p_copy = copy.copy(p) print(p_copy) # Should print: pickling a Person instance... followed by Name: John Doe, Age: 30, Address: 123 Main St p_pickle = pickle.dumps(p) # Should print: pickling a Person instance... print(pickle.loads(p_pickle)) # Should recreate the Person object and print Name: John Doe, Age: 30, Address: 123 Main St ``` Ensure every requirement for the custom pickling and copying logic within the `Person` class is met in your implementation.","solution":"import copyreg import copy import pickle class Person: def __init__(self, name, age, address): self.name = name self.age = age self.address = address def __str__(self): return f\\"Name: {self.name}, Age: {self.age}, Address: {self.address}\\" # Implement the pickling function def pickle_person(person): return Person, (person.name, person.age, person.address) # Register the pickling function copyreg.pickle(Person, pickle_person)"},{"question":"# Custom Scikit-Learn Estimator Objective: You are to implement a custom scikit-learn compatible estimator that demonstrates understanding of the fundamental scikit-learn development principles. This task will test your ability to create compliant estimators that can integrate seamlessly with scikit-learn\'s pipeline and model selection tools. Task: Implement a custom classifier named `CustomKNNClassifier` that mimics the behavior of a simple K-Nearest Neighbors (KNN) algorithm. Your custom estimator should: 1. Inherit from `BaseEstimator` and `ClassifierMixin`. 2. Include parameters: - `n_neighbors` (the number of neighbors to use) - `metric` (distance metric to use, default is Euclidean) 3. Implement the following methods: - `__init__(self, n_neighbors=5, metric=\'euclidean\')` - `fit(self, X, y)` - `predict(self, X)` 4. Validate input data using scikit-learn\'s utilities. 5. Handle the random state appropriately if necessary (though not required for KNN). 6. Store learned attributes with trailing underscores. 7. Include comments and docstrings following the numpy docstring standard. Constraints - You must use `sklearn.utils.validation` for input validation. - You must use `scipy.spatial.distance.cdist` for distance computation. Expected Function Signatures: ```python class CustomKNNClassifier(ClassifierMixin, BaseEstimator): def __init__(self, n_neighbors=5, metric=\'euclidean\'): pass def fit(self, X, y): pass def predict(self, X): pass ``` Example Usage: ```python import numpy as np from sklearn.utils.estimator_checks import check_estimator from sklearn.datasets import load_iris # Load dataset data = load_iris() X, y = data.data, data.target # Initialize and fit the custom estimator clf = CustomKNNClassifier(n_neighbors=3) clf.fit(X, y) # Predict using the custom estimator predictions = clf.predict(X) # Perform estimator checks check_estimator(clf) # Should pass all checks if implemented correctly ``` Submission: Submit your implementation of the `CustomKNNClassifier` class. Ensure that your code passes the `check_estimator` validation for scikit-learn compatibility.","solution":"import numpy as np from sklearn.base import BaseEstimator, ClassifierMixin from sklearn.utils.validation import check_X_y, check_array, check_is_fitted from scipy.spatial.distance import cdist class CustomKNNClassifier(ClassifierMixin, BaseEstimator): def __init__(self, n_neighbors=5, metric=\'euclidean\'): Custom K-Nearest Neighbors classifier. Parameters ---------- n_neighbors : int, default=5 Number of neighbors to use for k-neighbors queries. metric : str, default=\'euclidean\' Distance metric to use for computing neighbors. Any metric from the ``scipy.spatial.distance.cdist`` module. self.n_neighbors = n_neighbors self.metric = metric def fit(self, X, y): Fit the model using X as training data and y as target values. Parameters ---------- X : array-like of shape (n_samples, n_features) Training data. y : array-like of shape (n_samples,) Target values. Returns ------- self : CustomKNNClassifier Fitted estimator. # Validate inputs X, y = check_X_y(X, y) # Store the training data and targets self.X_ = X self.y_ = y return self def predict(self, X): Perform classification on samples in X. Parameters ---------- X : array-like of shape (n_samples, n_features) Input samples. Returns ------- y_pred : array of shape (n_samples,) Predicted target values for X. # Check if fit has been called check_is_fitted(self, [\\"X_\\", \\"y_\\"]) # Validate the input X = check_array(X) # Compute distances between X and self.X_ distances = cdist(X, self.X_, metric=self.metric) # Find the indices of the nearest neighbors neighbor_indices = np.argsort(distances, axis=1)[:, :self.n_neighbors] # Neighbor voting y_pred = np.empty(X.shape[0], dtype=self.y_.dtype) for i, indices in enumerate(neighbor_indices): closest_y = self.y_[indices] # Predict the most common class among nearest neighbors y_pred[i] = np.argmax(np.bincount(closest_y)) return y_pred"},{"question":"**Objective:** To assess students\' understanding of seaborn\'s capabilities for creating and customizing heatmaps. **Problem Statement:** You are provided with a dataset `students_scores` which contains the following columns: `Student_ID`, `Subject`, and `Score`. Your task is to implement a function `create_custom_heatmap` which takes the dataset as input and generates a customized heatmap with the following requirements: 1. Pivot the dataset so that the index is `Student_ID`, columns are `Subject`, and values are `Score`. 2. Create a heatmap with the following customizations: - Display cell values as annotations. - Use the `coolwarm` colormap. - Add lines between cells with a linewidth of 0.7. - Set the colormap norm such that the minimum value is 10, and the maximum value is 100. - Use a separate dataframe for annotations displaying the rank of the score within each student (row-wise ranking). - Tweak the plot to move the x-axis ticks to the top. **Function Signature:** ```python import pandas as pd def create_custom_heatmap(students_scores: pd.DataFrame) -> None: pass ``` **Input:** - A pandas DataFrame `students_scores` with the following columns: - `Student_ID` (categorical): IDs of 10 students. - `Subject` (categorical): Names of 5 subjects. - `Score` (numeric): Scores obtained by each student in each subject. **Output:** - The function does not return anything. Instead, it should display the heatmap using seaborn. **Constraints:** - Each student will have scores for all 5 subjects. **Example:** ```python import pandas as pd data = { \'Student_ID\': [\'S1\', \'S1\', \'S1\', \'S1\', \'S1\', \'S2\', \'S2\', \'S2\', \'S2\', \'S2\', \'S3\', \'S3\', \'S3\', \'S3\', \'S3\', \'S4\', \'S4\', \'S4\', \'S4\', \'S4\', \'S5\', \'S5\', \'S5\', \'S5\', \'S5\'], \'Subject\': [\'Math\', \'English\', \'Science\', \'History\', \'Art\', \'Math\', \'English\', \'Science\', \'History\', \'Art\', \'Math\', \'English\', \'Science\', \'History\', \'Art\', \'Math\', \'English\', \'Science\', \'History\', \'Art\', \'Math\', \'English\', \'Science\', \'History\', \'Art\'], \'Score\': [88, 92, 76, 81, 95, 67, 82, 79, 92, 60, 95, 88, 77, 66, 92, 74, 85, 90, 91, 78, 85, 79, 88, 70, 86] } df = pd.DataFrame(data) create_custom_heatmap(df) ``` The output should be a well-formatted heatmap displaying the described custom configurations. # Additional Notes: - Make sure to set `sns.set_theme()` before creating the heatmap for better visual consistency. - Ensure you have the necessary imports in the function, including seaborn and any other required libraries.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt import numpy as np def create_custom_heatmap(students_scores: pd.DataFrame) -> None: # Pivot the dataset heatmap_data = students_scores.pivot(index=\'Student_ID\', columns=\'Subject\', values=\'Score\') # Define the colormap limits vmin, vmax = 10, 100 # Create annotations for the heatmap (rank within each student) annotations = heatmap_data.rank(axis=1).astype(int) # Define the colormap and customizations sns.set_theme() plt.figure(figsize=(10, 8)) ax = sns.heatmap( heatmap_data, annot=annotations, fmt=\\"d\\", cmap=\\"coolwarm\\", linewidths=0.7, vmin=vmin, vmax=vmax, cbar_kws={\'shrink\': .8} ) # Move x-axis ticks to the top ax.xaxis.tick_top() ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha=\'left\') # Display the heatmap plt.show()"},{"question":"**Task: Implement a HTTP Status Checker** Your task is to implement a function `http_status_checker(url: str) -> dict` that takes a URL as input and returns a dictionary with information about the HTTP status of the response. This function should: 1. Make an HTTP GET request to the given URL. 2. Retrieve the HTTP status code from the response. 3. Map the status code to its corresponding `http.HTTPStatus` entry. 4. Return a dictionary containing the following keys: - `status_code`: The numerical status code (e.g., 200). - `status_name`: The name corresponding to the status code (e.g., `\\"OK\\"`). - `status_phrase`: The reason phrase for the status code (e.g., `\\"OK\\"`). - `status_description`: A detailed description of the status code. # Constraints * You must use the `http.client` module for making the HTTP request. * Handle exceptions that may arise during the HTTP request (e.g., network errors). # Performance Requirements * The function should complete the HTTP request and return the status information efficiently. Aim for a solution that completes within a few seconds assuming a normal network connection. # Example Usage ```python from your_solution import http_status_checker url = \\"http://www.example.com\\" result = http_status_checker(url) print(result) # Example Output: # { # \'status_code\': 200, # \'status_name\': \'OK\', # \'status_phrase\': \'OK\', # \'status_description\': \'Request fulfilled, document follows\' # } ``` # Detailed Function Explanation `http_status_checker(url: str) -> dict` * **Input**: - `url` (str): The URL to which the HTTP GET request will be made. * **Output**: - `dict`: A dictionary containing the HTTP status information (`status_code`, `status_name`, `status_phrase`, `status_description`). **Note**: Consider edge cases such as: - Invalid URL format. - Unreachable server. - Unsuccessful HTTP requests (status codes other than 200).","solution":"import http.client from urllib.parse import urlparse from http import HTTPStatus def http_status_checker(url: str) -> dict: Check the HTTP status of a given URL. Parameters: url (str): URL to check Returns: dict: Dictionary containing status information result = {} try: parsed_url = urlparse(url) if parsed_url.scheme == \\"https\\": conn = http.client.HTTPSConnection(parsed_url.netloc) else: conn = http.client.HTTPConnection(parsed_url.netloc) conn.request(\\"GET\\", parsed_url.path or \\"/\\") response = conn.getresponse() status_code = response.status status_info = HTTPStatus(status_code) result[\'status_code\'] = status_code result[\'status_name\'] = status_info.name result[\'status_phrase\'] = status_info.phrase result[\'status_description\'] = status_info.description except Exception as e: result[\'error\'] = str(e) finally: conn.close() return result"},{"question":"# Advanced Python Coding Assessment **Objective**: Demonstrate your understanding of Python\'s `functools` module by solving a problem that requires the use of multiple decorators and higher-order functions. **Problem Statement**: You are tasked with implementing a caching mechanism for a class that performs various mathematical operations. Your goal is to ensure that repeated expensive calculations are cached efficiently. You will also implement a generic function to handle different data types with various operations. 1. Implement a class `MathOperations` with the following specifications: - An `__init__` method that initializes an attribute `data` as an empty list. - A method `add_number` to add a number to the `data` list. - A method `mean` to calculate and return the arithmetic mean of the numbers in `data`. - Use the `cached_property` decorator to cache the result after the first calculation. - A method `variance` to calculate and return the variance of the numbers in `data`. - Use the `lru_cache` decorator to cache the results of calls with the same data. 2. Implement a generic function `compute` using the `singledispatch` decorator with the following specifications: - The main implementation should raise a `NotImplementedError` if called directly. - Register an implementation for `int` type that returns the square of the integer. - Register an implementation for `list` type that returns the sum of elements in the list. - Register an implementation for `dict` type that returns an inverted dictionary (keys become values and values become keys). 3. Implement a function `partial_sum` using the `partial` function that: - Takes an argument `increment` which is added to a given number. - This partial function should have the `increment` default to 10. **Example Usage**: ```python # Part 1: MathOperations class m = MathOperations() m.add_number(4) m.add_number(7) print(m.mean) # Should calculate and cache the mean print(m.mean) # Should return the cached mean print(m.variance()) # Should calculate and cache the variance print(m.variance()) # Should return the cached variance # Part 2: compute function print(compute(5)) # Should return 25 print(compute([1, 2, 3])) # Should return 6 print(compute({\'a\': 1, \'b\': 2})) # Should return {1: \'a\', 2: \'b\'} # Part 3: partial_sum function increment_by_5 = partial_sum(5) print(increment_by_5(10)) # Should return 15 increment_by_default = partial_sum() print(increment_by_default(10)) # Should return 20 ``` **Constraints**: - The `data` list in `MathOperations` will have at most 10,000 numbers. - Numbers in the `data` list are guaranteed to be integers in the range [0, 10^7]. - For the `compute` function, the dictionary keys and values will be strings or integers. **Submission**: Submit a Python file containing your solution. Ensure that your code is well-documented and includes necessary error handling where applicable.","solution":"from functools import lru_cache, cached_property, singledispatch, partial import math class MathOperations: def __init__(self): self.data = [] def add_number(self, number): self.data.append(number) @cached_property def mean(self): if not self.data: return 0 return sum(self.data) / len(self.data) @lru_cache() def variance(self): if not self.data: return 0 mean = self.mean return sum((x - mean) ** 2 for x in self.data) / len(self.data) @singledispatch def compute(val): raise NotImplementedError(\\"Unsupported type\\") @compute.register(int) def _(val): return val ** 2 @compute.register(list) def _(val): return sum(val) @compute.register(dict) def _(val): return {v: k for k, v in val.items()} def partial_sum(number, increment=10): return number + increment"},{"question":"**Coding Assessment Question:** # Objective: Create a series of plots using the seaborn library that demonstrate your understanding of various features and customization options of the `stripplot` function. Additionally, create a multi-facet plot using `catplot`. # Instructions: 1. Load the `tips` dataset using seaborn. 2. Create a strip plot showing the distribution of `total_bill` split by `day`. Customize it with a **deep** color palette. 3. Create another strip plot showing `total_bill` against `day` with a `hue` based on the `sex` column, ensuring the points are not jittered. 4. Create a horizontal strip plot showing the relationship between `total_bill` and `size`, ensuring the `size` variable uses its native scale. 5. Finally, create a multi-facet plot using `catplot` to show `total_bill` against `time` with `hue` based on `sex`, and each facet separated by `day`. # Requirements: - Each plot should have a title describing what it represents. - The plots should be visually distinguishable with appropriate color palettes and markers. - Ensure the multi-facet plot has an aspect ratio of 0.5 for each facet. # Expected Output: You should implement a function named `plot_tips_data` that takes no inputs and outputs the required plots. Here is the function signature: ```python import seaborn as sns import matplotlib.pyplot as plt def plot_tips_data(): # Load `tips` dataset using seaborn tips = sns.load_dataset(\\"tips\\") # 1. Strip plot of `total_bill` split by `day` with deep color palette plt.figure(figsize=(10, 6)) sns.stripplot(data=tips, x=\\"total_bill\\", y=\\"day\\", palette=\\"deep\\") plt.title(\\"Total Bill Distribution by Day with Deep Color Palette\\") plt.show() # 2. Strip plot of `total_bill` vs `day` with `hue` based on `sex` and no jitter plt.figure(figsize=(10, 6)) sns.stripplot(data=tips, x=\\"total_bill\\", y=\\"day\\", hue=\\"sex\\", jitter=False) plt.title(\\"Total Bill vs Day with Hue Based on Sex (No Jitter)\\") plt.show() # 3. Horizontal strip plot of `total_bill` vs `size` with native scale plt.figure(figsize=(10, 6)) sns.stripplot(data=tips, x=\\"total_bill\\", y=\\"size\\", orient=\\"h\\", native_scale=True) plt.title(\\"Total Bill vs Size with Native Scale (Horizontal Plot)\\") plt.show() # 4. Multi-facet plot using `catplot` sns.catplot(data=tips, x=\\"time\\", y=\\"total_bill\\", hue=\\"sex\\", col=\\"day\\", aspect=.5) plt.suptitle(\\"Total Bill vs Time with Hue Based on Sex, Faceted by Day\\") plt.show() # Call the function to generate the plots plot_tips_data() ``` # Constraints: - Ensure your plots are aesthetically pleasing and labels are readable. - Do not use default color mappings; always specify a palette when relevant. - Pay attention to plot orientations as specified. # Submission: Submit your python file containing the implementation of the `plot_tips_data` function. Ensure your code is well-commented and follows PEP 8 guidelines for readability.","solution":"import seaborn as sns import matplotlib.pyplot as plt def plot_tips_data(): # Load `tips` dataset using seaborn tips = sns.load_dataset(\\"tips\\") # 1. Strip plot of `total_bill` split by `day` with deep color palette plt.figure(figsize=(10, 6)) sns.stripplot(data=tips, x=\\"total_bill\\", y=\\"day\\", palette=\\"deep\\") plt.title(\\"Total Bill Distribution by Day with Deep Color Palette\\") plt.show() # 2. Strip plot of `total_bill` vs `day` with `hue` based on `sex` and no jitter plt.figure(figsize=(10, 6)) sns.stripplot(data=tips, x=\\"total_bill\\", y=\\"day\\", hue=\\"sex\\", jitter=False) plt.title(\\"Total Bill vs Day with Hue Based on Sex (No Jitter)\\") plt.show() # 3. Horizontal strip plot of `total_bill` vs `size` with native scale plt.figure(figsize=(10, 6)) sns.stripplot(data=tips, x=\\"total_bill\\", y=\\"size\\", orient=\\"h\\", native_scale=True) plt.title(\\"Total Bill vs Size with Native Scale (Horizontal Plot)\\") plt.show() # 4. Multi-facet plot using `catplot` g = sns.catplot(data=tips, x=\\"time\\", y=\\"total_bill\\", hue=\\"sex\\", col=\\"day\\", aspect=0.5) g.fig.suptitle(\\"Total Bill vs Time with Hue Based on Sex, Faceted by Day\\") plt.show() # Call the function to generate the plots plot_tips_data()"},{"question":"# Python Coding Assessment Question Objective The objective of this question is to assess your understanding of file and directory access in Python, including manipulation and comparison of file paths, working with temporary files, and performing file operations. Problem Statement You are required to write a function that synchronizes the contents of two directories. The function should ensure that both directories contain the same files with the same content. If a file exists in the source directory but not in the destination directory, it should be copied over. If a file exists in the destination directory but not in the source directory or if it is different, it should be updated. Function Signature ```python def sync_directories(src: str, dest: str) -> None: pass ``` Input * `src` (str): The path to the source directory. * `dest` (str): The path to the destination directory. Output The function does not return anything. It updates the destination directory to match the source directory. Constraints * Assume that both `src` and `dest` are valid directories. * The function should recursively synchronize all subdirectories. * Use appropriate modules (`os`, `shutil`, `filecmp`, `pathlib`, etc.) to perform necessary file and directory operations. Performance Requirements * The function should handle large directories with multiple levels of subdirectories efficiently. Example Suppose `src` directory has the following structure: ``` src/ file1.txt file2.txt subdir/ file3.txt ``` And `dest` directory has the following structure: ``` dest/ file1.txt file4.txt subdir/ file3_old.txt ``` After running `sync_directories(\'src\', \'dest\')`, the `dest` directory should be updated to: ``` dest/ file1.txt file2.txt subdir/ file3.txt ``` Guidelines 1. You may find the `os`, `shutil`, `filecmp`, and `pathlib` modules particularly useful for this task. 2. Ensure to handle file overwrites and deletions appropriately. 3. You can use `filecmp.cmp` to compare files.","solution":"import os import shutil import filecmp from pathlib import Path def sync_directories(src: str, dest: str) -> None: src_path = Path(src) dest_path = Path(dest) if not src_path.is_dir() or not dest_path.is_dir(): raise ValueError(\\"Both src and dest must be valid directories\\") def sync(src_dir: Path, dest_dir: Path): for item in src_dir.iterdir(): src_item = src_dir / item.name dest_item = dest_dir / item.name if src_item.is_dir(): if not dest_item.exists(): shutil.copytree(src_item, dest_item) else: sync(src_item, dest_item) else: if not dest_item.exists() or not filecmp.cmp(src_item, dest_item, shallow=False): shutil.copy2(src_item, dest_item) for item in dest_dir.iterdir(): if not (src_dir / item.name).exists(): if item.is_dir(): shutil.rmtree(item) else: item.unlink() sync(src_path, dest_path)"},{"question":"You are working on a distributed machine learning task using PyTorch\'s distributed RPC framework. Your goal is to manage and perform remote operations on tensors distributed across multiple workers. In this task, you will implement a function that performs the following steps: 1. Create a remote tensor on a specified worker. 2. Perform a series of remote updates to this tensor from another worker. 3. Ensure proper management of RRefs, including correct synchronization and notification to prevent premature deletion and data inconsistencies. # Function Specification **Function Name**: `manage_remote_tensor` **Inputs**: 1. `worker_A` (string): The name of the worker where the tensor will be created. 2. `worker_B` (string): The name of the worker that will perform remote operations on the tensor. 3. `initial_value` (torch.Tensor): The initial value of the tensor to be created. **Output**: - Returns the final value of the tensor after all remote operations are completed. **Constraints**: - The code should handle the creation and management of RRefs correctly, ensuring that all remote operations are synchronized. - Ensure that the owner of the tensor (worker A) is properly notified of any updates to the RRefs. - Handle out-of-order message delivery and any necessary synchronization to prevent data inconsistencies. # Example ```python import torch import torch.distributed.rpc as rpc def manage_remote_tensor(worker_A, worker_B, initial_value): # Implementation here pass # Example execution in a distributed environment: # Note: The following code assumes that an RPC framework is already initialized with workers properly set up. rpc.init_rpc(\\"worker_A\\", rank=0, world_size=2) rpc.init_rpc(\\"worker_B\\", rank=1, world_size=2) initial_value = torch.tensor([1.0, 2.0]) result = manage_remote_tensor(\\"worker_A\\", \\"worker_B\\", initial_value) print(\\"Final tensor value on worker A:\\", result) ``` **Explanation**: - The function `manage_remote_tensor` should remotely initialize the tensor on `worker_A` and perform operations on it from `worker_B`. - Ensure that the ownership and reference counting of the RRef are managed correctly so that the tensor remains valid throughout the operations. - Finally, retrieve and return the final value of the tensor. # Notes - Use `rpc.remote` to create the tensor on `worker_A`. - Implement remote updates and ensure proper communication between `worker_A` and `worker_B`. - Make use of RRef lifecycle guarantees to coordinate references and deletion notifications correctly. # Performance Requirements - Ensure that the solution is efficient and leverages the distributed nature of the problem. - Handle proper synchronization and message passing to avoid bottlenecks or race conditions. **Good luck!** Demonstrate your understanding of PyTorch\'s distributed RPC framework and RRef protocol by implementing the `manage_remote_tensor` function.","solution":"import torch import torch.distributed.rpc as rpc def manage_remote_tensor(worker_A, worker_B, initial_value): def create_tensor(initial_value): return initial_value def update_tensor(rref, value): rref.local_value().add_(value) return rref.local_value() # Create a remote tensor on worker_A tensor_rref = rpc.remote(worker_A, create_tensor, args=(initial_value,)) # Perform updates on the tensor from worker_B future = rpc.rpc_sync(worker_B, update_tensor, args=(tensor_rref, torch.tensor([1.0, 1.0]))) future = rpc.rpc_sync(worker_B, update_tensor, args=(tensor_rref, torch.tensor([2.0, 2.0]))) # Return the final value by waiting for the future to complete final_value = tensor_rref.to_here() return final_value # Example execution in a distributed environment: # Note: The following code assumes that an RPC framework is already initialized with workers properly set up. if __name__ == \\"__main__\\": rpc.init_rpc(\\"worker_A\\", rank=0, world_size=2) rpc.init_rpc(\\"worker_B\\", rank=1, world_size=2) initial_value = torch.tensor([1.0, 2.0]) result = manage_remote_tensor(\\"worker_A\\", \\"worker_B\\", initial_value) print(\\"Final tensor value on worker A:\\", result) rpc.shutdown()"},{"question":"# Multi-threading and Synchronization with `threading` Module **Objective:** Implement a multi-threaded program that manages a shared resource using Python\'s `threading` module. Specifically, you will create a system where multiple threads increment a shared counter, ensuring that race conditions are avoided using appropriate synchronization mechanisms. **Task:** 1. Create a class `Counter` that has the following: - An integer attribute `value` initialized to 0. - Methods to increment the counter safely by one using a Lock object to prevent race conditions. 2. Implement a function `increment_counter(counter, n)` that takes a `Counter` object and an integer `n`. This function should increment the counter\'s `value` by 1, `n` times using multiple threads. 3. Create a main function to: - Instantiate a `Counter` object. - Create and start 10 threads, each of which will increment the counter\'s value 1000 times using the `increment_counter` function. - Ensure all threads complete their execution. - Print the final value of the counter. **Constraints:** - Each thread must increment the counter in a thread-safe manner using Python\'s `threading.Lock`. - You must use the `threading` module\'s Thread class to manage threads. - The final value of the counter should be 10000. **Performance Requirements:** - Ensure that threads are properly synchronized to avoid race conditions. **Input Format:** There is no input required from the user; you will directly implement the functions and the execution flow. **Output Format:** Print the final value of the counter. # Example ```python import threading class Counter: def __init__(self): self.value = 0 self.lock = threading.Lock() def increment(self): with self.lock: self.value += 1 def increment_counter(counter, n): for _ in range(n): counter.increment() def main(): counter = Counter() threads = [] for _ in range(10): thread = threading.Thread(target=increment_counter, args=(counter, 1000)) threads.append(thread) thread.start() for thread in threads: thread.join() print(\\"Final counter value:\\", counter.value) if __name__ == \\"__main__\\": main() ``` **Explanation:** - A `Counter` class with a `Lock` to ensure atomic updates to the `value` attribute. - A function `increment_counter` to perform `n` increments using the `Counter`\'s thread-safe increment method. - The `main` function creates 10 threads, each incrementing the counter 1000 times. - The threads are started and then joined to ensure all complete before printing the final counter value. The expected output should be: ``` Final counter value: 10000 ``` Ensure your implementation is correct by comparing the final counter value with the expected output.","solution":"import threading class Counter: def __init__(self): self.value = 0 self.lock = threading.Lock() def increment(self): with self.lock: self.value += 1 def increment_counter(counter, n): for _ in range(n): counter.increment() def main(): counter = Counter() threads = [] for _ in range(10): thread = threading.Thread(target=increment_counter, args=(counter, 1000)) threads.append(thread) thread.start() for thread in threads: thread.join() print(\\"Final counter value:\\", counter.value) if __name__ == \\"__main__\\": main()"},{"question":"**Question: Customizing Seaborn Plots with Faceting and Labels** **Objective:** You are provided with the famous \\"Penguins\\" dataset from Seaborn. Your task is to create a grid of plots that visualizes the relationship between different measurements of penguins, faceted by species and sex, and includes custom labels and legends. **Instructions:** 1. Load the \\"Penguins\\" dataset using `seaborn.load_dataset`. 2. Create a `seaborn.objects.Plot` object with the following specifications: - `x` axis: `bill_length_mm` - `y` axis: `flipper_length_mm` 3. Add a dot plot layer colored by `species`. 4. Facet the plot by `species` and `sex`. 5. Modify the labels: - Title: \\"Penguin Measurements by Species and Sex\\" - X-axis label: \\"Bill Length (mm)\\" - Y-axis label: \\"Flipper Length (mm)\\" - Legend title: \\"Species\\" 6. Ensure each facet has a customized title indicating the species and sex. **Expected Input:** The function does not require any inputs. **Expected Output:** The function should output a Seaborn plot with the described customizations, and it should display correctly within a Jupyter Notebook. **Code Template:** ```python import seaborn.objects as so from seaborn import load_dataset def create_customized_plot(): # Step 1: Load the dataset penguins = load_dataset(\\"penguins\\") # Step 2: Create the Plot object plot = ( so.Plot(penguins, x=\\"bill_length_mm\\", y=\\"flipper_length_mm\\") .add(so.Dot(), color=\\"species\\") .facet(row=\\"species\\", col=\\"sex\\") .label( title=\\"Penguin Measurements by Species and Sex\\", x=\\"Bill Length (mm)\\", y=\\"Flipper Length (mm)\\", legend=\\"Species\\" ) ) # Step 3: Customize facet titles plot = plot.label(title=\\"{species} Penguins ({sex})\\".format) # Step 4: Display the plot return plot.show() # The function will be automatically executed and the plot displayed when running this cell. create_customized_plot() ``` **Constraints:** - Ensure all columns used for faceting and plotting are free of missing values to avoid errors. - Use appropriate data types for all operations. **Performance Requirements:** - The plot should be generated efficiently and should render within a reasonable time for large datasets.","solution":"import seaborn.objects as so from seaborn import load_dataset def create_customized_plot(): # Step 1: Load the dataset penguins = load_dataset(\\"penguins\\").dropna(subset=[\\"bill_length_mm\\", \\"flipper_length_mm\\", \\"sex\\"]) # Ensure no NaN values # Step 2: Create the Plot object plot = ( so.Plot(penguins, x=\\"bill_length_mm\\", y=\\"flipper_length_mm\\") .add(so.Dot(), color=\\"species\\") .facet(row=\\"species\\", col=\\"sex\\") .label( title=\\"Penguin Measurements by Species and Sex\\", x=\\"Bill Length (mm)\\", y=\\"Flipper Length (mm)\\", legend=\\"Species\\" ) ) # Step 3: Customize facet titles plot = plot.label(title=\\"{species} Penguins ({sex})\\") # Step 4: Display the plot plot.show() # Call the function to create and display the plot create_customized_plot()"},{"question":"Problem Statement You are tasked with writing a Python program that fetches weather data from a public API, processes the data, and gracefully handles any potential issues that may arise during the fetching process. Your program should demonstrate the use of the `urllib.request` module for making HTTP requests, handling both GET and POST methods, and managing exceptions properly. # Requirements 1. **Fetch Current Weather Data**: - Use `urllib.request.urlopen` to fetch current weather data for a given city from a public weather API (e.g., OpenWeatherMap, WeatherAPI). - The API URL should be in the format: `http://api.weatherapi.com/v1/current.json?key=YOUR_API_KEY&q=CITY_NAME` - Ensure that the API key and city name are passed dynamically. 2. **Handle Different HTTP Methods**: - Demonstrate both a `GET` request to fetch weather data and a `POST` request to log some dummy data (e.g., temperature) to a fictional endpoint `http://example.com/log`. 3. **HTTP Headers**: - Include custom headers in your request, such as User-Agent. 4. **Exception Handling**: - Properly handle exceptions using `URLError` and `HTTPError`. - Print relevant information when an error occurs (e.g., status code, error message). 5. **Response Processing**: - Parse the JSON response and print the current temperature along with a brief description of the weather condition (e.g., \\"Clear\\", \\"Thunderstorm\\"). # Function Signature ```python import urllib.request import urllib.error import urllib.parse import json def fetch_weather_data(api_key: str, city_name: str) -> None: Fetches current weather data for a given city using a public API and logs the temperature to a fictional endpoint. Parameters: api_key (str): The API key for the weather service. city_name (str): The name of the city to fetch the weather for. Returns: None pass ``` # Constraints - You are required to use the `urllib.request` module. - You must handle exceptions elegantly and provide meaningful error messages. - Use the proper HTTP methods (`GET` and `POST`) for fetching and logging data respectively. # Input and Output Formats - **Input**: Function parameters `api_key` and `city_name`. - **Output**: Print statements indicating the temperature, weather condition, and any errors encountered. # Example Usage ```python fetch_weather_data(\'your_api_key_here\', \'London\') ``` Expected Output ```plaintext Temperature: 15°C Weather: Clear ``` Or in case of an error: ```plaintext The server couldn\'t fulfill the request. Error code: 404 ``` Or: ```plaintext We failed to reach a server. Reason: [Errno 11001] getaddrinfo failed ``` Note: Replace `\'your_api_key_here\'` with a valid API key for the example to work. # Additional Notes - Ensure your code handles both successful requests and typical errors like 404 (Not Found) or network-related issues. - The parsing of the JSON response should be robust to handle potential changes in the API response structure.","solution":"import urllib.request import urllib.error import urllib.parse import json def fetch_weather_data(api_key: str, city_name: str) -> None: Fetches current weather data for a given city using a public API and logs the temperature to a fictional endpoint. Parameters: api_key (str): The API key for the weather service. city_name (str): The name of the city to fetch the weather for. Returns: None base_url = \\"http://api.weatherapi.com/v1/current.json\\" query_string = urllib.parse.urlencode({\'key\': api_key, \'q\': city_name}) url = f\\"{base_url}?{query_string}\\" headers = { \'User-Agent\': \'WeatherApp/1.0\', } try: request = urllib.request.Request(url, headers=headers) with urllib.request.urlopen(request) as response: data = response.read().decode() weather_data = json.loads(data) temperature = weather_data[\'current\'][\'temp_c\'] condition = weather_data[\'current\'][\'condition\'][\'text\'] print(f\\"Temperature: {temperature}°C\\") print(f\\"Weather: {condition}\\") # Simulated logging to a fictional endpoint log_url = \\"http://example.com/log\\" log_data = urllib.parse.urlencode({\'temperature\': temperature}) log_data = log_data.encode(\'utf-8\') log_request = urllib.request.Request(log_url, data=log_data, headers=headers, method=\'POST\') with urllib.request.urlopen(log_request) as log_response: if log_response.status == 200: print(\\"Temperature logged successfully.\\") except urllib.error.HTTPError as e: print(\'The server couldn\'t fulfill the request.\') print(f\'Error code: {e.code}\') except urllib.error.URLError as e: print(\'We failed to reach a server.\') print(f\'Reason: {e.reason}\')"},{"question":"Objective Implement a function that processes a given text file and converts the data to uppercase, reports the frequency of each letter, and saves the results in a new binary file. This function should demonstrate the use of text I/O, binary I/O, and appropriate handling of encodings as described in the \'io\' module. Task Write a function `process_file(input_file: str, output_file: str, encoding: str = \'utf-8\') -> None` that performs the following steps: 1. **Open and read the input file** specified by `input_file` using the provided encoding. 2. **Convert the content to uppercase** and calculate the frequency of each letter (A-Z). 3. **Write two outputs** to `output_file` in binary format: - The content in uppercase. - The letter frequency count as a `dict` serialized to bytes using pickle for storage. Constraints - The `input_file` will be a text file encoded in `utf-8` by default, but the encoding can be specified. - The `output_file` will be a binary file. - You must handle exceptions appropriately, including issues with file access, encoding, and I/O operations. Expected Input and Output - Input: A text file with any string content. - Output: A binary file containing the uppercase text and a serialized dictionary of letter frequencies. Example: ```python # Given input_file contains the text: # \\"Hello World!\\" # The expected binary output file will include: # - The uppercase text: \\"HELLO WORLD!\\" # - The serialized dictionary of letter frequencies: {\'H\': 1, \'E\': 1, \'L\': 3, \'O\': 2, \'W\': 1, \'R\': 1, \'D\': 1} def process_file(input_file: str, output_file: str, encoding: str = \'utf-8\') -> None: import io import pickle # Step 1: Read the input file with specified encoding try: with io.open(input_file, \'r\', encoding=encoding) as file: content = file.read() except Exception as e: print(f\\"Error reading file: {e}\\") return # Step 2: Convert content to uppercase and calculate letter frequencies content_upper = content.upper() letter_frequency = {} for char in content_upper: if char.isalpha(): if char in letter_frequency: letter_frequency[char] += 1 else: letter_frequency[char] = 1 # Step 3: Write the outputs to the binary file try: with io.open(output_file, \'wb\') as binary_file: binary_file.write(content_upper.encode(encoding)) binary_file.write(b\'n\') # Newline as a separator binary_file.write(pickle.dumps(letter_frequency)) except Exception as e: print(f\\"Error writing to binary file: {e}\\") ``` Notes - Use `io.open()` for file operations as it allows efficient handling of different I/O types. - Ensure to handle encodings properly to avoid common pitfalls, especially when dealing with diverse text inputs. - Provide meaningful error messages if any operation fails, ensuring the function does not crash silently.","solution":"import io import pickle def process_file(input_file: str, output_file: str, encoding: str = \'utf-8\') -> None: Process the input file to convert its content to uppercase, count the frequency of each letter, and save the results in a binary file. Args: - input_file (str): Path to the input text file. - output_file (str): Path to the output binary file. - encoding (str): Encoding of the input file. Default is \'utf-8\'. # Step 1: Read the input file with specified encoding try: with io.open(input_file, \'r\', encoding=encoding) as file: content = file.read() except Exception as e: print(f\\"Error reading file: {e}\\") return # Step 2: Convert content to uppercase and calculate letter frequencies content_upper = content.upper() letter_frequency = {} for char in content_upper: if char.isalpha(): if char in letter_frequency: letter_frequency[char] += 1 else: letter_frequency[char] = 1 # Step 3: Write the outputs to the binary file try: with io.open(output_file, \'wb\') as binary_file: binary_file.write(content_upper.encode(encoding)) binary_file.write(b\'n\') # Newline as a separator binary_file.write(pickle.dumps(letter_frequency)) except Exception as e: print(f\\"Error writing to binary file: {e}\\")"},{"question":"Objective: Design a custom Python class `CustomContainer` that demonstrates understanding of: 1. Attribute access methods. 2. Container emulation methods. 3. Context Managers. 4. Iterators. Task: 1. **Initialization**: Implement the `__init__` method to initialize the container with a collection of items. 2. **Attribute Access**: Override `__getattr__`, `__setattr__`, and `__delattr__` methods to manage attribute access. 3. **Container Emulation**: Implement `__getitem__`, `__setitem__`, `__delitem__`, `__iter__`, and `__contains__` to make your class behave like a Python container. 4. **Context Management**: Implement the `__enter__` and `__exit__` methods to manage resource context. 5. **Custom Iterator**: Implement a custom iterator for your container using `__iter__` and `__next__`. Requirements: 1. **Initialization**: - The class should accept any iterable during initialization and store it. ```python def __init__(self, items): # your code here ``` 2. **Attribute Access**: - `__getattr__`: Called when an attribute is not found in the usual places. - `__setattr__`: Called when an attribute assignment is attempted. - `__delattr__`: Called when an attribute deletion is attempted. ```python def __getattr__(self, name): # your code here def __setattr__(self, name, value): # your code here def __delattr__(self, name): # your code here ``` 3. **Container Emulation**: - `__getitem__`: To get an item. - `__setitem__`: To set an item. - `__delitem__`: To delete an item. - `__iter__`: To return an iterator. - `__contains__`: To check if an item is in the container. ```python def __getitem__(self, key): # your code here def __setitem__(self, key, value): # your code here def __delitem__(self, key): # your code here def __iter__(self): # your code here def __contains__(self, item): # your code here ``` 4. **Context Management**: - `__enter__`: To acquire resources. - `__exit__`: To release resources. ```python def __enter__(self): # your code here def __exit__(self, exc_type, exc_val, exc_tb): # your code here ``` 5. **Custom Iterator**: - Implement a nested iterator class within `CustomContainer` which implements `__iter__` and `__next__`. ```python class Iterator: def __init__(self, container): # your code here def __iter__(self): # your code here def __next__(self): # your code here ``` Input: - The constructor will take an iterable. Output: - Various functionalities are demonstrated through the implemented methods. Example Usage: ```python container = CustomContainer([1, 2, 3, 4, 5]) # Demonstrating attribute access: container.new_attr = \'I am new\' print(container.new_attr) # Output: I am new del container.new_attr # Demonstrating container emulation: print(container[1]) # Output: 2 container[1] = 20 print(container[1]) # Output: 20 del container[1] for item in container: print(item) print(3 in container) # Output: True # Demonstrating context management: with container as cnt: # Do something within the context pass ``` Ensure your code follows good practices, is efficient, and well-documented.","solution":"class CustomContainer: def __init__(self, items): self._items = list(items) self._attributes = {} # Attribute Access def __getattr__(self, name): if name in self._attributes: return self._attributes[name] raise AttributeError(f\\"\'{self.__class__.__name__}\' object has no attribute \'{name}\'\\") def __setattr__(self, name, value): if name in {\\"_items\\", \\"_attributes\\"}: super().__setattr__(name, value) else: self._attributes[name] = value def __delattr__(self, name): if name in self._attributes: del self._attributes[name] else: raise AttributeError(f\\"\'{self.__class__.__name__}\' object has no attribute \'{name}\'\\") # Container Emulation def __getitem__(self, key): return self._items[key] def __setitem__(self, key, value): self._items[key] = value def __delitem__(self, key): del self._items[key] def __iter__(self): return self.Iterator(self) def __contains__(self, item): return item in self._items # Context Management def __enter__(self): return self def __exit__(self, exc_type, exc_val, exc_tb): pass # Custom Iterator class Iterator: def __init__(self, container): self._container = container self._index = 0 def __iter__(self): return self def __next__(self): if self._index < len(self._container._items): value = self._container._items[self._index] self._index += 1 return value else: raise StopIteration"},{"question":"# Signal Processing with Window Functions in PyTorch You will implement a function to apply a series of window functions to a signal and analyze the resulting spectrum using the Fast Fourier Transform (FFT). Function Signature ```python import torch def analyze_signal(signal: torch.Tensor, sample_rate: int, window_type: str) -> torch.Tensor: pass ``` Parameters - `signal` (torch.Tensor): A 1-D tensor representing the input signal. - `sample_rate` (int): The sampling rate of the signal in Hz. - `window_type` (str): The type of window function to apply. Select from: \'hann\', \'hamming\', \'blackman\', \'bartlett\'. Returns - A 1-D tensor representing the magnitude of the FFT of the windowed signal. Instructions 1. **Window Application**: Apply the specified window function to the input signal. Normalize the window so that the area under the window is equal to 1. 2. **FFT Calculation**: Compute the FFT of the windowed signal and return the magnitude spectrum. 3. Ensure that the result retains the real part of the FFT result (since it\'s a real signal) and normalize the FFT output. Example ```python import torch # Define a sample input signal and sample rate sample_rate = 1000 t = torch.arange(0, 1.0, 1.0 / sample_rate) signal = torch.sin(2 * torch.pi * 5 * t) + 0.5 * torch.sin(2 * torch.pi * 10 * t) # Analyze the signal with a Hamming window fft_magnitude = analyze_signal(signal, sample_rate, \'hamming\') # Check the result print(fft_magnitude) ``` In this question, demonstrate how you can generate and apply different window functions to the provided signal before performing an FFT. Discuss any effects these windows have on the resultant spectrum in your comments within the code. Constraints - The `signal` tensor length will always be a power of 2. - Use the window functions from `torch.signal.windows`.","solution":"import torch def analyze_signal(signal: torch.Tensor, sample_rate: int, window_type: str) -> torch.Tensor: Analyzes the input signal by applying a window function and computing the FFT. Parameters: - signal (torch.Tensor): A 1-D tensor representing the input signal. - sample_rate (int): The sampling rate of the signal in Hz. - window_type (str): The type of window function to apply. Select from: \'hann\', \'hamming\', \'blackman\', \'bartlett\'. Returns: - A 1-D tensor representing the magnitude of the FFT of the windowed signal. # Define the window function mapping windows = { \'hann\': torch.hann_window, \'hamming\': torch.hamming_window, \'blackman\': torch.blackman_window, \'bartlett\': torch.bartlett_window } # Check for valid window type if window_type not in windows: raise ValueError(f\\"Invalid window type {window_type}. Choose from \'hann\', \'hamming\', \'blackman\', \'bartlett\'.\\") # Generate the window window = windows[window_type](signal.size(0), periodic=False) # Apply the window to the signal windowed_signal = signal * window # Perform the FFT fft_result = torch.fft.fft(windowed_signal, norm=\'forward\') # Compute the magnitude magnitude_spectrum = torch.abs(fft_result) return magnitude_spectrum"},{"question":"<|Analysis Begin|> The provided documentation is for various datetime-related objects and functions in Python, specifically for creating and manipulating date, time, and timezone objects through the `datetime` module in Python. These include creating date, datetime, time, timedelta, and timezone objects and extracting specific fields from those objects. The focus is on using macros for operations involving these objects. From this, we can derive that students should demonstrate their understanding and ability to handle date and time operations using the `datetime` module. This includes creating datetime objects, manipulating them, performing checks on object types, extracting fields, and handling timezones. <|Analysis End|> <|Question Begin|> # Coding Assessment Question **Objective:** Implement a function to process a list of datetime strings, convert them into `datetime` objects, and extract specific date and time information from each object. **Problem:** You are given a list of datetime strings in the format `YYYY-MM-DD HH:MM:SS` (e.g., `\'2023-10-01 12:30:45\'`). Your task is to write a Python function that does the following: 1. Convert each datetime string into a `datetime` object. 2. Extract the year, month, day, hour, minute, and second from each `datetime` object. 3. Return a list of dictionaries, with each dictionary containing the extracted information for one datetime string. **Function Signature:** ```python from typing import List, Dict def process_datetimes(datetime_strings: List[str]) -> List[Dict[str, int]]: ``` **Input:** - `datetime_strings`: A list of datetime strings (`List[str]`). Each string is in the format `YYYY-MM-DD HH:MM:SS`. **Output:** - Returns a list of dictionaries (`List[Dict[str, int]]`). Each dictionary contains the keys `\'year\'`, `\'month\'`, `\'day\'`, `\'hour\'`, `\'minute\'`, `\'second\'` with their respective integer values extracted from the datetime string. **Constraints:** 1. All datetime strings provided in the list are valid and follow the format `YYYY-MM-DD HH:MM:SS`. 2. The list can have up to 1000 datetime strings. **Example:** ```python datetime_strings = [\\"2023-12-25 15:30:00\\", \\"2021-06-10 04:50:20\\"] print(process_datetimes(datetime_strings)) # Output: # [ # {\'year\': 2023, \'month\': 12, \'day\': 25, \'hour\': 15, \'minute\': 30, \'second\': 0}, # {\'year\': 2021, \'month\': 6, \'day\': 10, \'hour\': 4, \'minute\': 50, \'second\': 20} # ] ``` **Important Notes:** - Use the `datetime` module to handle the conversion and extraction processes. - Ensure that your solution handles the conversion and extraction efficiently.","solution":"from typing import List, Dict from datetime import datetime def process_datetimes(datetime_strings: List[str]) -> List[Dict[str, int]]: result = [] for dt_str in datetime_strings: dt = datetime.strptime(dt_str, \'%Y-%m-%d %H:%M:%S\') dt_info = { \'year\': dt.year, \'month\': dt.month, \'day\': dt.day, \'hour\': dt.hour, \'minute\': dt.minute, \'second\': dt.second, } result.append(dt_info) return result"},{"question":"**File and Directory Access with `pathlib`** **Objective:** Demonstrate your understanding of Python\'s `pathlib` module and its functionalities related to file and directory operations. **Problem Statement:** You are tasked with creating a utility function that searches for a specific file type within a given directory and all its subdirectories. This utility will return a list of the absolute paths to all the files with the specified extension. **Function Signature:** ```python def find_files_with_extension(directory: str, extension: str) -> list: Searches for files with a specific extension within the given directory and all its subdirectories. Parameters: directory (str): The path to the directory within which to search for files. extension (str): The file extension to search for (e.g. \'txt\', \'py\', \'jpg\'). Returns: list: A list of absolute paths to all files with the specified extension. ``` **Input:** 1. `directory`: A string representing the path to the directory to search within. 2. `extension`: A string representing the file extension to search for (e.g., \'txt\', \'py\', \'jpg\'). **Output:** A list of absolute paths to the found files. **Constraints:** - The solution must use Python\'s `pathlib` module. - The function should handle both relative and absolute paths for the `directory` parameter. - The function should recursively search all subdirectories. - The `extension` parameter will not include a leading dot (e.g., use \'txt\' instead of \'.txt\'). **Example:** ```python # Example directory structure: # /my_dir # ├── file1.txt # ├── file2.py # └── subdir1 # └── file3.txt # Example call: result = find_files_with_extension(\'/my_dir\', \'txt\') # Expected result: # [\'/my_dir/file1.txt\', \'/my_dir/subdir1/file3.txt\'] ``` **Notes:** - Ensure that your function handles edge cases such as the directory not existing or being empty. - Use the `pathlib.Path` object for path manipulations.","solution":"from pathlib import Path def find_files_with_extension(directory: str, extension: str) -> list: Searches for files with a specific extension within the given directory and all its subdirectories. Parameters: directory (str): The path to the directory within which to search for files. extension (str): The file extension to search for (e.g. \'txt\', \'py\', \'jpg\'). Returns: list: A list of absolute paths to all files with the specified extension. # Convert the directory to a Path object dir_path = Path(directory) # Ensure extension starts with a dot for the suffix method. ext = f\\".{extension}\\" # Initialize an empty list to store paths of found files found_files = [] # Use rglob to recursively search for files with the specified extension for file_path in dir_path.rglob(f\\"*{ext}\\"): if file_path.is_file(): found_files.append(str(file_path.resolve())) return found_files"},{"question":"You are tasked with implementing a function that filters a list of file names based on a given pattern using shell-style wildcards and additionally provides a count of how many files matched the pattern. This function should also provide an option to perform case-sensitive matching. # Function Signature ```python def filter_and_count_files(file_list: list, pattern: str, case_sensitive: bool = False) -> tuple: Filters file names in `file_list` that match the given `pattern` and returns a tuple containing a list of matched file names and the count of matches. Parameters: - file_list (list): A list of file name strings. - pattern (str): The pattern string using Unix shell-style wildcards. - case_sensitive (bool): If True, the matching will be case-sensitive. Returns: - tuple: A tuple where the first element is the list of matched file names and the second element is the count of matched file names. ``` # Input 1. `file_list`: A list of strings representing file names. Example: `[\\"File1.txt\\", \\"file2.TXT\\", \\"document.pdf\\", \\"image.JPEG\\"]`. 2. `pattern`: A shell-style wildcard pattern string. Example: `\\"*.txt\\"`. 3. `case_sensitive`: A boolean flag indicating whether the pattern matching should be case-sensitive. Default is `False`. # Output A tuple containing two elements: 1. A list of strings representing the file names that matched the given pattern. 2. An integer representing the count of matched file names. # Constraints - The function should handle both case-sensitive and case-insensitive matching as specified by the `case_sensitive` parameter. - File names and pattern strings are not empty. - `file_list` will contain at most `10^3` file name strings. - The pattern will contain at most `10^2` characters. # Example ```python file_list = [\\"report1.txt\\", \\"Report2.TXT\\", \\"summary.docx\\", \\"notes.txt\\"] pattern = \\"*.txt\\" case_sensitive = False # Expected output: ([\\"report1.txt\\", \\"Report2.TXT\\", \\"notes.txt\\"], 3) print(filter_and_count_files(file_list, pattern, case_sensitive)) case_sensitive = True # Expected output: ([\\"report1.txt\\", \\"notes.txt\\"], 2) print(filter_and_count_files(file_list, pattern, case_sensitive)) ``` # Note Use the `fnmatch` module to perform the pattern matching with the respective case sensitivity option.","solution":"import fnmatch def filter_and_count_files(file_list, pattern, case_sensitive=False): Filters file names in `file_list` that match the given `pattern` and returns a tuple containing a list of matched file names and the count of matches. Parameters: - file_list (list): A list of file name strings. - pattern (str): The pattern string using Unix shell-style wildcards. - case_sensitive (bool): If True, the matching will be case-sensitive. Returns: - tuple: A tuple where the first element is the list of matched file names and the second element is the count of matched file names. if not case_sensitive: file_list = [file.lower() for file in file_list] pattern = pattern.lower() matched_files = fnmatch.filter(file_list, pattern) return (matched_files, len(matched_files))"},{"question":"**Question:** # Implementing a Periodic Async Job Scheduler You are required to create an asynchronous job scheduler in Python using `asyncio`, which will periodically execute a set of tasks and log their completion times. Each task should be executed at a specific interval, and the execution should be managed in such a way that it does not block the main event loop. # Requirements: 1. **Function Signature:** - `async def periodic_scheduler(tasks: List[Tuple[Callable, float]]) -> None:` 2. **Inputs:** - `tasks`: A list of tuples where each tuple contains: - `Callable`: The task function to be executed. - `float`: The time interval (in seconds) after which the task should be repeated. 3. **Output:** - The function should not return anything. Instead, it should log the message `\'Task {func.__name__} executed at {timestamp}\'` each time a task is executed. 4. **Constraints:** - The tasks should be scheduled and managed using `asyncio` event loop methods. - Ensure the scheduler does not block the event loop. - Handle the logging using an asynchronous method. 5. **Performance:** - The scheduler should efficiently manage the periodic execution of tasks even if there are many tasks with short intervals. # Example: ```python import asyncio import time from typing import Callable, List, Tuple async def task1(): print(f\\"Task1 executed at {time.time()}\\") async def task2(): print(f\\"Task2 executed at {time.time()}\\") tasks = [ (task1, 1.5), (task2, 2.0) ] async def periodic_scheduler(tasks: List[Tuple[Callable, float]]) -> None: # Your implementation here # Running the scheduler if __name__ == \\"__main__\\": asyncio.run(periodic_scheduler(tasks)) ``` In the above example, `task1` should be executed every 1.5 seconds and `task2` should be executed every 2.0 seconds. The log output should look similar to: ``` Task task1 executed at 1634567890.123456 Task task2 executed at 1634567891.123456 Task task1 executed at 1634567891.623456 ... ``` **Hints:** - Use `asyncio.sleep` to create delays between task executions. - You might find it useful to use the `asyncio.call_later` for periodic scheduling of tasks. - Ensure to manage the event loop properly by checking if it’s running and creating tasks accordingly.","solution":"import asyncio import time from typing import Callable, List, Tuple async def log_execution(func: Callable): Log the execution of the function. await func() print(f\\"Task {func.__name__} executed at {time.time()}\\") async def periodic_task(func: Callable, interval: float): while True: await log_execution(func) await asyncio.sleep(interval) async def periodic_scheduler(tasks: List[Tuple[Callable, float]]) -> None: loop = asyncio.get_running_loop() scheduled_tasks = [] for task, interval in tasks: scheduled_task = loop.create_task(periodic_task(task, interval)) scheduled_tasks.append(scheduled_task) await asyncio.gather(*scheduled_tasks)"},{"question":"# Advanced Python Coding Assessment Objective As part of this assessment, you are required to demonstrate your understanding of the `pathlib` module in Python by implementing several functions to perform path manipulations and file operations. Problem Statement You are tasked with implementing a utility class called `FileSystemHelper` using the `pathlib` module. This class should provide functionalities to navigate, query, and manipulate the filesystem as specified below. Class: `FileSystemHelper` **Methods:** 1. **`list_python_files(self, directory: str) -> list`:** List all Python files (`*.py`) in the given directory and all its subdirectories. - **Input:** `directory` (str) - Path to the directory. - **Output:** List of paths to Python files (list of `str`). 2. **`find_largest_file(self, directory: str) -> str`:** Find the largest file in the given directory and its subdirectories. - **Input:** `directory` (str) - Path to the directory. - **Output:** Path to the largest file (str). If there are multiple files of the same maximum size, return any one of them. 3. **`create_backup(self, original_file: str, backup_directory: str) -> str`:** Create a backup of the given file in the specified backup directory. The backup file should have the same name as the original but with a \\".bak\\" extension. - **Input:** `original_file` (str) - Path to the file to be backed up. `backup_directory` (str) - Path to the directory where the backup should be stored. - **Output:** Path to the backup file (str). 4. **`relative_path(self, source: str, destination: str) -> str`:** Compute the relative path from `source` to `destination`. - **Input:** `source` (str) - Source path. `destination` (str) - Destination path. - **Output:** Relative path from `source` to `destination` (str). Example Usage ```python from pathlib import Path from FileSystemHelper import FileSystemHelper helper = FileSystemHelper() # List all Python files python_files = helper.list_python_files(\'/path/to/directory\') print(python_files) # Find the largest file largest_file = helper.find_largest_file(\'/path/to/directory\') print(largest_file) # Create a backup backup_file = helper.create_backup(\'/path/to/file.txt\', \'/path/to/backup\') print(backup_file) # Compute relative path relative = helper.relative_path(\'/path/to/source\', \'/path/to/destination\') print(relative) ``` # Constraints - Use only the functionalities provided by the `pathlib` module. - You can assume that the input paths are valid and exist. - Performance should be considered where applicable. Submission Please implement the `FileSystemHelper` class and its methods as described. Ensure that your implementation handles edge cases and is optimized for performance.","solution":"from pathlib import Path class FileSystemHelper: def list_python_files(self, directory: str) -> list: List all Python files (*.py) in the given directory and all its subdirectories. return [str(file) for file in Path(directory).rglob(\'*.py\')] def find_largest_file(self, directory: str) -> str: Find the largest file in the given directory and its subdirectories. largest_file = max(Path(directory).rglob(\'*\'), key=lambda f: f.stat().st_size, default=None) return str(largest_file) if largest_file else \'\' def create_backup(self, original_file: str, backup_directory: str) -> str: Create a backup of the given file in the specified backup directory. The backup file should have the same name as the original but with a \\".bak\\" extension. original = Path(original_file) backup_dir = Path(backup_directory) backup_file = backup_dir / (original.stem + \'.bak\') backup_file.write_bytes(original.read_bytes()) return str(backup_file) def relative_path(self, source: str, destination: str) -> str: Compute the relative path from source to destination. source_path = Path(source) destination_path = Path(destination) return str(destination_path.relative_to(source_path))"},{"question":"You are required to implement a Python function that creates a backup of a given directory. The backup should include all subdirectories and files, preserving their metadata, and should exclude specific file types based on given glob-style patterns. Finally, the function should compress the backup into a specified archive format (such as \'zip\' or \'gztar\'). # Requirements: 1. Implement the function `create_backup(src, dst, patterns, archive_format)`. 2. **Parameters**: - `src`: The path to the source directory to be backed up (string). - `dst`: The path to the destination directory where the backup archive will be created (string). - `patterns`: A list of glob-style patterns representing file types to exclude from the backup (list of strings). - `archive_format`: The format of the archive to create (string). Can be one of \'zip\', \'tar\', \'gztar\', \'bztar\', or \'xztar\'. 3. **Return**: - The path to the created archive (string). 4. **Constraints**: - `src` must be a valid directory path. - `dst` must be a writable directory path. - The list `patterns` can contain zero or more patterns. - The `archive_format` must be a valid format supported by `shutil.make_archive()`. - Preserve all file metadata (including permission bits and timestamps) during the backup process. # Implementation Details: - Use `shutil.copytree()` to copy the `src` directory to a temporary location, ignoring files that match any of the `patterns`. - Use `shutil.make_archive()` to create the archive in the `dst` directory using the desired `archive_format`. - Clean up any temporary files or directories created during the process. # Example Usage: ```python import os def create_backup(src, dst, patterns, archive_format): # Your implementation here # Example usage src_directory = \'/path/to/source\' dst_directory = \'/path/to/destination\' exclude_patterns = [\'*.tmp\', \'*.log\'] archive_format = \'zip\' archive_path = create_backup(src_directory, dst_directory, exclude_patterns, archive_format) print(f\\"Backup created at: {archive_path}\\") ``` # Notes: - Ensure thorough documentation and handle exceptions that may arise during I/O operations. - You may use the `tempfile` module to handle temporary files and directories if needed.","solution":"import shutil import os import tempfile from fnmatch import fnmatch def create_backup(src, dst, patterns, archive_format): Creates a backup of the given directory, excluding specific file types, and compresses it into the specified archive format. Parameters: - src (str): The path to the source directory to be backed up. - dst (str): The path to the destination directory where the backup archive will be created. - patterns (list of str): A list of glob-style patterns representing file types to exclude from the backup. - archive_format (str): The format of the archive to create. Can be \'zip\', \'tar\', \'gztar\', \'bztar\', or \'xztar\'. Returns: - str: The path to the created archive. if not os.path.isdir(src): raise ValueError(\\"The source path must be a valid directory.\\") if not os.path.isdir(dst): raise ValueError(\\"The destination path must be a valid directory.\\") if archive_format not in [\'zip\', \'tar\', \'gztar\', \'bztar\', \'xztar\']: raise ValueError(\\"The archive_format must be one of \'zip\', \'tar\', \'gztar\', \'bztar\', or \'xztar\'.\\") def ignore_patterns(dir, files): ignore_files = [] for pattern in patterns: ignore_files.extend([f for f in files if fnmatch(f, pattern)]) return set(ignore_files) with tempfile.TemporaryDirectory() as temp_dir: temp_backup_dir = os.path.join(temp_dir, \\"backup\\") shutil.copytree(src, temp_backup_dir, ignore=ignore_patterns) archive_path = shutil.make_archive(os.path.join(dst, \'backup\'), archive_format, temp_dir, \'backup\') return archive_path"},{"question":"Coding Assessment Question # Context In distributed computing, managing errors that arise from individual processes is crucial for robust and resilient systems. PyTorch\'s `torch.distributed.elastic.multiprocessing.errors` module provides mechanisms for capturing and handling such errors. Your task is to implement a function that simulates a distributed task execution, records errors using the provided error handling classes, and summarizes the error information. # Task Implement a function `simulate_distributed_execution` which simulates the execution of multiple child processes performing a task. The function should handle and record errors encountered during the execution of these processes. # Function Signature ```python def simulate_distributed_execution(tasks: List[Callable[[], Any]]) -> Tuple[int, List[str]]: ... ``` # Input - `tasks`: A list of callable functions, each representing a task to be executed in a separate process. # Output - A tuple containing: - `n_failed`: The number of tasks that failed. - `error_messages`: A list of error messages for the failed tasks. # Constraints 1. You may assume that each task in `tasks` can either complete successfully or raise an exception. 2. Use the `torch.distributed.elastic.multiprocessing.errors` module to handle and record errors. Specifically, make use of the `record` function and `ChildFailedError` class. 3. Ensure that your implementation handles the error recording properly and aggregates the error messages for summary. # Example ```python def task_success(): return \\"Success\\" def task_failure(): raise RuntimeError(\\"Failed Task\\") tasks = [task_success, task_failure, task_failure] n_failed, error_messages = simulate_distributed_execution(tasks) # Expected Output # n_failed: 2 # error_messages: [\\"Failed Task\\", \\"Failed Task\\"] ``` # Notes - Ensure that you handle any errors raised by the tasks and appropriately use the `ChildFailedError` class to capture the error information. - Summarize the error messages in the order that the tasks are provided. # Implementation You are required to write the implementation of the `simulate_distributed_execution` function. Ensure that your solution handles all edge cases and follows best practices for error handling in a distributed environment.","solution":"from typing import List, Callable, Any, Tuple def simulate_distributed_execution(tasks: List[Callable[[], Any]]) -> Tuple[int, List[str]]: Simulates the execution of multiple child processes performing a task. Handles and records errors encountered during the execution of these processes. Args: tasks (List[Callable[[], Any]]): A list of callable functions, each representing a task to be executed. Returns: Tuple[int, List[str]]: A tuple containing the number of failed tasks and a list of error messages for the failed tasks. n_failed = 0 error_messages = [] for task in tasks: try: task() except Exception as e: n_failed += 1 error_messages.append(str(e)) return n_failed, error_messages"},{"question":"**Objective**: Demonstrate your understanding of seaborn\'s color palette blending and data visualization by creating a comprehensive matplotlib plot with a custom color map. **Problem Statement**: Write a function `create_custom_heatmap(data, colors)` that takes in a 2D dataset and a list of color specifications. The function should: 1. Blend the provided colors to create a custom colormap. 2. Generate a heatmap of the dataset using seaborn, applying the custom colormap. 3. Display the heatmap with appropriate annotations, titles, and labels. **Function Signature**: ```python def create_custom_heatmap(data, colors): pass ``` # Input: - `data`: A 2D list or a 2D numpy array representing the dataset to visualize. - `colors`: A list of color specifications that seaborn can interpret (hex codes, named colors, etc.). # Output: - The function should display a heatmap using seaborn. # Example: ```python import numpy as np # Sample data data = np.random.rand(10, 12) # Sample color specifications colors = [\\"#45a872\\", \\".8\\", \\"xkcd:golden\\"] #create heatmap create_custom_heatmap(data, colors) ``` **Constraints**: 1. The `data` input should contain at least 5 rows and 5 columns. 2. The `colors` list should contain at least two color specifications. 3. The heatmap must include a color bar. **Performance Requirements**: - The function should efficiently handle datasets up to 1000 rows by 1000 columns. **Note**: Ensure that the plot is self-contained and properly annotated with titles and labels to make it comprehensible without additional context. **Hint**: You may use `sns.heatmap` for generating the heatmap and `sns.blend_palette` for creating the colormap.","solution":"import seaborn as sns import matplotlib.pyplot as plt from matplotlib.colors import LinearSegmentedColormap import numpy as np def create_custom_heatmap(data, colors): Creates and displays a heatmap using a custom colormap made from the provided list of colors. Parameters: data (2D list or 2D numpy array): The dataset to visualize. colors (list): List of color specifications interpretable by seaborn. Returns: None # Blend the provided colors to create a custom colormap custom_cmap = LinearSegmentedColormap.from_list(\\"custom_cmap\\", colors) # Plot the heatmap using seaborn plt.figure(figsize=(10, 8)) ax = sns.heatmap(data, cmap=custom_cmap, annot=True, cbar=True) # Add titles and labels ax.set_title(\\"Custom Heatmap\\") ax.set_xlabel(\\"Columns\\") ax.set_ylabel(\\"Rows\\") # Display the plot plt.show()"},{"question":"Implement a new Python object type in C, called `CustomList`, which is a variable-length list-like object. The `CustomList` should support the following features: 1. **Initialization**: The `CustomList` should be initialized with a given size and default value for its elements. 2. **Method to get an element**: `get(index)` should return the element at the specified index. 3. **Method to set an element**: `set(index, value)` should set the element at the specified index to the given value. 4. **Method to get the size**: `size()` should return the current size of the list. 5. **Attributes**: The object should have a `max_size` attribute that denotes the maximum possible size of the list, and a `current_size` attribute for the current number of elements. Your implementation should include: - The definition of the `CustomList` type using `PyVarObject`. - C functions implementing the `get`, `set`, and `size` methods, following the `PyCFunction` or appropriate calling convention. - A `PyMemberDef` array for the `max_size` and `current_size` attributes. - A method table (`PyMethodDef`) for the callable methods. - Initialization, type setting, and reference management macros. For simplicity, implement this extension type as a pure C module, and ensure that proper error handling and memory management practices are followed. Test your implementation by compiling the module and writing a small Python script that uses the `CustomList` to demonstrate its functionality. **Input**: - Function calls and attribute access in Python. **Output**: - The corresponding results and changes in the custom list. **Constraints**: - The maximum list size should not exceed 1000 elements. - Proper error handling should be in place for index out of range, invalid inputs, and memory allocation issues. Example usage in Python: ```python import customlist # Create a CustomList of size 10 with default value 0 cl = customlist.CustomList(10, 0) # Set value at index 3 cl.set(3, 99) # Get value at index 3 print(cl.get(3)) # Output: 99 # Get the size of the list print(cl.size()) # Output: 10 # Access attributes print(cl.max_size) # Output: 1000 print(cl.current_size) # Output: 10 ```","solution":"# Assume we use a \\"customlist.c\\" file written in C # The following is a conceptual implementation in Python that simulates the CustomList # To be actually implemented in C, translating these concepts to CPython API constructs class CustomList: def __init__(self, size, default_value): self.max_size = 1000 if size > self.max_size: raise ValueError(\\"Size exceeds maximum allowed size\\") self.current_size = size self.data = [default_value] * size def get(self, index): if not 0 <= index < self.current_size: raise IndexError(\\"Index out of range\\") return self.data[index] def set(self, index, value): if not 0 <= index < self.current_size: raise IndexError(\\"Index out of range\\") self.data[index] = value def size(self): return self.current_size"},{"question":"**Question: Implement a custom sorted collection using the `bisect` module** You are required to implement a custom sorted collection class, `SortedCollection`, which maintains a list of records in sorted order using the `bisect` module. The class should support efficient insertions and lookups based on a key. It should provide the following functionalities: 1. **Initialization**: - `SortedCollection(iterable=None, key=None)`: Initializes the collection with an optional iterable of records and an optional key function to extract the sorting key from records. If no `key` is provided, elements are compared directly. 2. **Insert**: - `insert(record)`: Inserts a new `record` into the collection while maintaining sorted order. 3. **Find**: - `find(key_value)`: Finds and returns the record with the given `key_value`. If multiple records have the same key, return the leftmost one. Raises `ValueError` if the key is not found. 4. **Find Less Than**: - `find_lt(key_value)`: Finds and returns the rightmost record with a key less than `key_value`. Raises `ValueError` if no such record exists. 5. **Find Greater Than**: - `find_gt(key_value)`: Finds and returns the leftmost record with a key greater than `key_value`. Raises `ValueError` if no such record exists. 6. **Example Usage**: ```python from operator import itemgetter records = [(\'red\', 5), (\'blue\', 1), (\'yellow\', 8), (\'black\', 0)] col = SortedCollection(records, key=itemgetter(1)) col.insert((\'green\', 3)) print(col.find(3)) # Output: (\'green\', 3) print(col.find_lt(3)) # Output: (\'blue\', 1) print(col.find_gt(3)) # Output: (\'red\', 5) print(col.find(9)) # Raises ValueError ``` **Requirements**: - You must use the `bisect` module functions (`bisect_left`, `bisect_right`, `insort_left`, `insort_right`) where appropriate. - Your implementation should handle the optional key function to support complex records. - Ensure that searching and inserting records is efficient and leverages the bisection algorithm for optimal performance. **Constraints**: - The `key` function, if provided, will always return a comparable value. - The records will be comparable or have a key function applied to them that returns comparable values. Implement the `SortedCollection` class below: ```python from bisect import bisect_left, bisect_right, insort_left, insort_right class SortedCollection: def __init__(self, iterable=None, key=None): # Initialize the collection with an optional iterable and key function pass def insert(self, record): # Insert a record into the collection maintaining sorted order pass def find(self, key_value): # Find and return the record with the given key_value pass def find_lt(self, key_value): # Find and return the rightmost record with a key less than key_value pass def find_gt(self, key_value): # Find and return the leftmost record with a key greater than key_value pass ```","solution":"from bisect import bisect_left, bisect_right, insort_left class SortedCollection: def __init__(self, iterable=None, key=None): self._key = key self._items = sorted(iterable, key=key) if iterable else [] def insert(self, record): key = self._key(record) if self._key else record insort_left(self._items, record, key=lambda x: self._key(x) if self._key else x) def find(self, key_value): keys = [self._key(item) if self._key else item for item in self._items] i = bisect_left(keys, key_value) if i != len(self._items) and keys[i] == key_value: return self._items[i] raise ValueError(\'No record found with key:\', key_value) def find_lt(self, key_value): keys = [self._key(item) if self._key else item for item in self._items] i = bisect_left(keys, key_value) if i: return self._items[i - 1] raise ValueError(\'No record found with key less than:\', key_value) def find_gt(self, key_value): keys = [self._key(item) if self._key else item for item in self._items] i = bisect_right(keys, key_value) if i != len(self._items): return self._items[i] raise ValueError(\'No record found with key greater than:\', key_value)"},{"question":"# Pandas Coding Assessment Question Objective: You are required to write a function that will perform multiple operations to handle missing data within a DataFrame. This task will assess your ability to understand and manipulate missing data using pandas. Question: Given a pandas DataFrame, perform the following operations: 1. Detect and list all columns that contain any missing values. 2. For columns with missing values, replace the missing values using the following strategies: - If the column is numeric (integer or float), replace missing values with the mean value of the column. - If the column is of any other type (e.g., object, boolean), replace missing values with the most frequent value in the column. If there is a tie, use the lexicographically smallest one. 3. Add a boolean indicator column for each column that had missing data, where the new column indicates `True` if the original column had a missing value at that row, and `False` otherwise. The new column should be named as the original column name appended with `_was_missing`. 4. Return the modified DataFrame. Function Signature ```python import pandas as pd def handle_missing_data(df: pd.DataFrame) -> pd.DataFrame: Detects and handles missing data in the DataFrame. Parameters: df (pd.DataFrame): A DataFrame possibly containing missing data. Returns: pd.DataFrame: Modified DataFrame with missing data handled and indicator columns added. pass ``` Example: ```python data = { \'A\': [1, 2, None, 4, 5], \'B\': [np.nan, \'B\', \'C\', \'B\', np.nan], \'C\': [True, False, np.nan, False, True] } df = pd.DataFrame(data) result_df = handle_missing_data(df) print(result_df) ``` Expected output: ``` A B C A_was_missing B_was_missing C_was_missing 0 1.0 B True False True False 1 2.0 B False False False False 2 3.0 B False True False True 3 4.0 B False False False False 4 5.0 B True False True False ``` Constraints: - Do not change the DataFrame indexes. - Do not use external libraries other than pandas and numpy. - Ensure the function is efficient and handles large DataFrames effectively. Notes: - Be mindful of differentiating between `np.nan` and `pd.NA`. - Utilize pandas\' built-in functions wherever possible to ensure the solution is concise and efficient.","solution":"import pandas as pd import numpy as np def handle_missing_data(df: pd.DataFrame) -> pd.DataFrame: Detects and handles missing data in the DataFrame. Parameters: df (pd.DataFrame): A DataFrame possibly containing missing data. Returns: pd.DataFrame: Modified DataFrame with missing data handled and indicator columns added. df = df.copy() for col in df.columns: if df[col].isnull().any(): df[col + \'_was_missing\'] = df[col].isnull() if np.issubdtype(df[col].dtype, np.number): mean_value = df[col].mean() df[col].fillna(mean_value, inplace=True) else: most_frequent_value = df[col].mode().iloc[0] df[col].fillna(most_frequent_value, inplace=True) return df"},{"question":"# Context-sensitive Decimal Arithmetic You are tasked with creating a function to handle financial calculations for a banking application. This function will compute the final balance after applying a series of transactions to an initial balance. The transactions include deposits, withdrawals, and fees, and they must be rounded according to specific rules. Function Specification Implement a function `calculate_balance` that accepts the following parameters: - `initial_balance` (string): A string representing the initial balance in the account. - `transactions` (list): A list of strings, each representing a transaction in the format `\\"<type>:<amount>\\"`. The possible types are: - \\"deposit\\": A deposit of the given amount. - \\"withdrawal\\": A withdrawal of the given amount. - \\"fee\\": A fee charged to the account. - `precision` (int): An integer specifying the precision to use for all calculations. - `rounding_mode` (string): A string specifying the rounding mode to use. Possible values include `\\"ROUND_CEILING\\"`, `\\"ROUND_DOWN\\"`, `\\"ROUND_FLOOR\\"`, `\\"ROUND_HALF_DOWN\\"`, `\\"ROUND_HALF_EVEN\\"`, `\\"ROUND_HALF_UP\\"`, `\\"ROUND_UP\\"`, `\\"ROUND_05UP\\"`. The function should return the final balance as a string rounded to the specified precision and using the specified rounding mode. The rounding should be applied after each transaction. Example Usage ```python def calculate_balance(initial_balance, transactions, precision, rounding_mode): from decimal import Decimal, getcontext, ROUND_CEILING, ROUND_DOWN, ROUND_FLOOR, ROUND_HALF_DOWN, ROUND_HALF_EVEN, ROUND_HALF_UP, ROUND_UP, ROUND_05UP # Complete the implementation # Example usage initial_balance = \\"1000.00\\" transactions = [\\"deposit:200.50\\", \\"withdrawal:50.75\\", \\"fee:5.00\\"] precision = 2 rounding_mode = \\"ROUND_HALF_UP\\" final_balance = calculate_balance(initial_balance, transactions, precision, rounding_mode) print(final_balance) # Expected: \\"1144.75\\" ``` Constraints - You must use the `decimal` module for all calculations. - Transactions may result in intermediate results that need rounding according to the specified precision and mode. - Ensure that fee application never results in a negative balance. Additional Notes - Handle invalid transaction types by ignoring them. - Ensure the function is efficient and handles a reasonable number of transactions within performance limits.","solution":"def calculate_balance(initial_balance, transactions, precision, rounding_mode): from decimal import Decimal, getcontext, ROUND_CEILING, ROUND_DOWN, ROUND_FLOOR, ROUND_HALF_DOWN, ROUND_HALF_EVEN, ROUND_HALF_UP, ROUND_UP, ROUND_05UP # Setting the decimal precision and rounding mode rounding_modes = { \\"ROUND_CEILING\\": ROUND_CEILING, \\"ROUND_DOWN\\": ROUND_DOWN, \\"ROUND_FLOOR\\": ROUND_FLOOR, \\"ROUND_HALF_DOWN\\": ROUND_HALF_DOWN, \\"ROUND_HALF_EVEN\\": ROUND_HALF_EVEN, \\"ROUND_HALF_UP\\": ROUND_HALF_UP, \\"ROUND_UP\\": ROUND_UP, \\"ROUND_05UP\\": ROUND_05UP } if rounding_mode not in rounding_modes: raise ValueError(\\"Invalid rounding mode\\") getcontext().prec = precision + 5 # Increase precision to account for intermediate calculations getcontext().rounding = rounding_modes[rounding_mode] balance = Decimal(initial_balance).quantize(Decimal(\'1.\' + \'0\' * precision)) for transaction in transactions: try: t_type, t_amount = transaction.split(\':\') t_amount = Decimal(t_amount).quantize(Decimal(\'1.\' + \'0\' * precision)) except (ValueError, ArithmeticError): continue # Skip invalid transactions if t_type == \\"deposit\\": balance += t_amount elif t_type == \\"withdrawal\\": balance -= t_amount elif t_type == \\"fee\\": balance -= t_amount # Ensuring fee application never results in a negative balance if balance < 0: balance = Decimal(\'0.00\') return str(balance.quantize(Decimal(\'1.\' + \'0\' * precision)))"},{"question":"Objective: To assess your understanding of the `runpy` module and your ability to execute Python code dynamically. Question: Write a function `execute_module_or_path` that takes two arguments: 1. `source`: a string representing either an absolute module name or a filesystem path. 2. `is_path`: a boolean indicating whether `source` is a filesystem path (`True`) or a module name (`False`). The function should execute the code specified by `source` using the appropriate `runpy` function (`run_module` for module names and `run_path` for paths). The function should handle following: - Pre-populate the module\'s globals dictionary with a variable `initial_value` set to `42`. - Return the resulting globals dictionary after the code execution. - Include exception handling to manage any errors that arise during the execution of the module or script. Constraints: - You can assume the module names and paths provided are correct and exist. - Aim for clear exception handling with meaningful messages. - Avoid using any external libraries. Expected Function Signature: ```python def execute_module_or_path(source: str, is_path: bool) -> dict: pass ``` Example Usage: ```python # Assuming there\'s a module `math_module` with a global variable `pi` set to 3.14 result = execute_module_or_path(\'math_module\', False) print(result[\'pi\']) # Output should be 3.14 # Assuming there\'s a script `hello.py` with a global variable `greeting` set to \'Hello, World!\' result = execute_module_or_path(\'hello.py\', True) print(result[\'greeting\']) # Output should be \'Hello, World!\' ``` Notes: - The function should handle the `initial_value` properly and ensure it is available during the code execution. - Properly manage the `sys` alterations to avoid any side effects. Good luck!","solution":"import runpy def execute_module_or_path(source: str, is_path: bool) -> dict: Execute the code specified by source as a module name or a filesystem path. :param source: A string representing an absolute module name or a filesystem path. :param is_path: A boolean indicating whether source is a filesystem path (True) or a module name (False). :return: The resulting globals dictionary after the code execution. global_vars = {\'initial_value\': 42} try: if is_path: result_globals = runpy.run_path(source, init_globals=global_vars) else: result_globals = runpy.run_module(source, init_globals=global_vars) except Exception as e: raise RuntimeError(f\\"Error during execution of {\'path\' if is_path else \'module\'}: {source}\\") from e return result_globals"},{"question":"**Objective:** Demonstrate knowledge of the `os` module and its handling of environment variables. **Problem Statement:** You are required to write a function `run_command_with_env` that runs a shell command with a modified environment. The function should: 1. Accept three parameters: - A string command (`command`) to be run. - A dictionary of new environment variables (`new_env_vars`) to be added or updated in the current environment. - An existing environment dictionary (`base_env`) which replicates the environment the command should run in. 2. Modify the environment by updating it with the new variables provided. 3. Run the command in a subprocess and return the exit code and output of the command. 4. Ensure that the original environment (`base_env`) does not get altered. **Function Signature:** ```python import os import subprocess def run_command_with_env(command: str, new_env_vars: dict, base_env: dict) -> tuple: # Your code here pass ``` **Input:** - `command`: A string representing the command to be executed. - `new_env_vars`: A dictionary containing environment variables to be added or updated. - `base_env`: A dictionary representing the base environment. **Output:** - A tuple consisting of: - Exit code of the command (integer). - Output of the command (string). **Constraints:** - Assume `command` will be a string suitable for execution in the shell. - The base environment dictionary (`base_env`) should not be modified. # Example: **Input:** ```python command = \\"echo NEW_VAR\\" new_env_vars = {\\"NEW_VAR\\": \\"HelloWorld\\"} base_env = os.environ.copy() ``` **Output:** ```python (0, \\"HelloWorldn\\") ``` **Notes:** - Use the `os` and `subprocess` modules to handle environment manipulation and subprocess execution. - Ensure your solution handles both Unix-like and Windows OS environments appropriately.","solution":"import os import subprocess def run_command_with_env(command: str, new_env_vars: dict, base_env: dict) -> tuple: Runs a shell command with a modified environment by adding or updating environment variables. :param command: A string representing the command to be executed. :param new_env_vars: A dictionary containing environment variables to be added or updated. :param base_env: A dictionary representing the base environment. :return: A tuple consisting of: - Exit code of the command (integer). - Output of the command (string). # Create a copy of the base environment to avoid modifying the original env = base_env.copy() # Update the environment with new environment variables env.update(new_env_vars) # Execute the command in a subprocess with the modified environment result = subprocess.run(command, shell=True, capture_output=True, text=True, env=env) # Return the exit code and the output of the command return result.returncode, result.stdout"},{"question":"Objective: Design a function that demonstrates the use of `itertools`, `functools`, and `operator` modules to process a list in a functional programming style. Problem Statement: You are given a list of tuples containing names and ages of different individuals. Implement a function `process_person_data(person_data)` that performs the following operations: 1. Filters out individuals who are below the age of 18. 2. Maps each remaining individual\'s name to uppercase. 3. Sums the ages of these filtered individuals. 4. Returns a tuple containing: - A list of names in uppercase. - The total sum of ages. # Input: - `person_data`: A list of tuples, where each tuple consists of a string (name) and an integer (age). # Output: - A tuple containing: - A list of uppercase names of individuals aged 18 or above. - The total sum of ages of these individuals. # Constraints: - The length of the list can be at most 10^5. - Names are non-empty strings and ages are non-negative integers. # Example: ```python person_data = [(\\"Alice\\", 25), (\\"Bob\\", 17), (\\"Charlie\\", 19), (\\"Diana\\", 18)] result = process_person_data(person_data) print(result) # Expected Output: ([\'ALICE\', \'CHARLIE\', \'DIANA\'], 62) ``` # Requirements: Use the `itertools`, `functools`, and `operator` modules to implement the function. # Implementation: Please implement the function `process_person_data(person_data)` below: ```python from itertools import filterfalse from functools import reduce from operator import attrgetter def process_person_data(person_data): # Your implementation goes here pass ```","solution":"from itertools import filterfalse from functools import reduce from operator import itemgetter def process_person_data(person_data): # Step 1: Filter out individuals who are below the age of 18. filtered_data = filterfalse(lambda person: person[1] < 18, person_data) # Step 2: Map each remaining individual\'s name to uppercase. names_uppercase = (person[0].upper() for person in filtered_data) # We need to re-apply the filtering because generators are exhausted after one use. filtered_data = filterfalse(lambda person: person[1] < 18, person_data) # Step 3: Sum the ages of these filtered individuals. ages_sum = reduce(lambda acc, person: acc + person[1], filtered_data, 0) # Re-apply the filtering and mapping for the final list generation. filtered_data = filterfalse(lambda person: person[1] < 18, person_data) names_uppercase = [person[0].upper() for person in filtered_data] return (names_uppercase, ages_sum)"},{"question":"# Question: Customize Plot Styles Using Seaborn You are given a dataset `data` containing the following columns: `category` (categorical), `value1` (numeric), and `value2` (numeric). Your task is to create a function `create_custom_plots` that demonstrates your understanding of seaborn\'s style customization features. Function Signature ```python def create_custom_plots(data: pd.DataFrame) -> None: pass ``` Input - `data`: A Pandas DataFrame with three columns: `category`, `value1`, and `value2`. Output - The function should not return anything. Instead, it should display two different bar plots using seaborn, each with a different style. Requirements 1. Retrieve and print the default style parameters using `sns.axes_style()`. 2. Create the first bar plot where `category` is on the x-axis and `value1` is on the y-axis using the seaborn style `darkgrid`. 3. Create a second bar plot where `category` is on the x-axis and `value2` is on the y-axis using the seaborn style `whitegrid`. This plot should use a context manager to temporarily change the style. Example ```python import pandas as pd # Example data data = pd.DataFrame({ \'category\': [\'A\', \'B\', \'C\'], \'value1\': [10, 20, 15], \'value2\': [7, 25, 10] }) create_custom_plots(data) ``` Constraints - Ensure your plots are well-labeled and easy to read. - Use the available styles in seaborn (`darkgrid`, `whitegrid`, etc.). - Handle any imports and required setup within the function as needed.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def create_custom_plots(data: pd.DataFrame) -> None: Displays two bar plots with different seaborn styles. Parameters: data: A Pandas DataFrame with three columns: \'category\', \'value1\', and \'value2\'. # Print default style parameters print(\\"Default style parameters:\\", sns.axes_style()) # First plot with \'darkgrid\' style sns.set_style(\\"darkgrid\\") plt.figure(figsize=(10, 6)) sns.barplot(x=\'category\', y=\'value1\', data=data) plt.title(\\"Bar Plot with darkgrid Style\\") plt.show() # Second plot with \'whitegrid\' style using a context manager with sns.axes_style(\\"whitegrid\\"): plt.figure(figsize=(10, 6)) sns.barplot(x=\'category\', y=\'value2\', data=data) plt.title(\\"Bar Plot with whitegrid Style\\") plt.show()"},{"question":"# Email Message Builder Your task is to implement a function `build_email_message` that constructs and returns an `EmailMessage` object with specific attributes and payload conditions. Requirements: 1. The function should accept the following parameters: - `subject` (str): The subject of the email. - `sender` (str): The sender\'s email address. - `recipients` (list of str): A list of recipient email addresses. - `body` (str): The body text of the email. - `attachments` (optional dict): A dictionary where keys are filenames and values are the file contents (bytes). 2. The function should create an `EmailMessage` object with the specified `subject`, `sender`, `recipients`, and `body`. 3. If `attachments` are provided, each attachment should be added to the email message with `Content-Disposition` set to \\"attachment\\". 4. The email should include the following headers: - `Subject` - `From` - `To` (which should include all recipient addresses, separated by commas) 5. The function should ensure that the email body is set correctly as a plain text message. If attachments are present, the body should be the first part of a multipart/mixed message. Example Usage: ```python from email.message import EmailMessage def build_email_message(subject, sender, recipients, body, attachments=None): # Implementation goes here pass # Example usage email = build_email_message( subject=\\"Project Update\\", sender=\\"sender@example.com\\", recipients=[\\"recipient1@example.com\\", \\"recipient2@example.com\\"], body=\\"Please find the project update attached.\\", attachments={ \\"update.pdf\\": b\\"%PDF-1.4...\\", \\"chart.png\\": b\\"x89PNGrn...\\" } ) print(email.as_string()) ``` Constraints: - `subject` and `sender` are non-empty strings. - `recipients` contains at least one valid email address. - `body` is a non-empty string. - If `attachments` are provided, it is a dictionary with non-empty string keys and values of type `bytes`. Testing: You can test the function by inspecting the headers and the payload structure of the `EmailMessage` object returned by your function. Ensure that multipart emails are correctly constructed if attachments are present, and that headers are properly set.","solution":"from email.message import EmailMessage from email.mime.base import MIMEBase from email.mime.multipart import MIMEMultipart from email.mime.text import MIMEText from email import encoders def build_email_message(subject, sender, recipients, body, attachments=None): Constructs and returns an `EmailMessage` object with the specified attributes. if attachments: msg = MIMEMultipart() msg.attach(MIMEText(body, \'plain\')) else: msg = MIMEText(body, \'plain\') msg[\\"Subject\\"] = subject msg[\\"From\\"] = sender msg[\\"To\\"] = \\", \\".join(recipients) if attachments: for filename, content in attachments.items(): part = MIMEBase(\'application\', \'octet-stream\') part.set_payload(content) encoders.encode_base64(part) part.add_header(\'Content-Disposition\', f\'attachment; filename=\\"{filename}\\"\') msg.attach(part) return msg"},{"question":"# Objective To assess your understanding of embedding Python in a C application and interacting between the languages by calling Python functions from C and extending the embedded interpreter with C functions. # Problem Statement You are tasked with developing a C application that embeds a Python interpreter and uses it to execute Python scripts. The C application should be able to: 1. Initialize the Python interpreter. 2. Execute a Python function that takes two integer arguments and returns their product. 3. Extend the embedded Python interpreter with a C function that takes two integers, adds them together, and returns the result to Python. # Requirements 1. **Initialize the Python Interpreter.** 2. **Execute a Python Function:** - Write a Python script (`multiply.py`) that includes a function `multiply(a, b)` which returns the product of `a` and `b`. - In the C application, load and execute this function when provided with the function name and arguments. 3. **Extend Python Interpreter with C Function:** - Implement a C function `add(a, b)` that adds two integers. - Register this function so that it can be called from the Python script. # Input and Output formats - **Input:** - Command line arguments to the C executable in the format: `./<executable> <python_script> <python_function> <arg1> <arg2>` - `<python_script>`: Name of the Python script file (e.g., `multiply.py`). - `<python_function>`: Name of the Python function to call (e.g., `multiply`). - `<arg1>`, `<arg2>`: Integers to pass as arguments to the Python function. - **Output:** - The result of the Python function call. - The result of calling the C-extended function from within the Python script. # Constraints 1. Ensure proper error handling if the Python script or function does not exist. 2. Handle scenarios where the arguments are not valid integers. # Performance Requirements - The solution should efficiently handle the initialization and finalization of the Python interpreter and the transition between C and Python calls. # Example Given the following Python script saved as `multiply.py`: ```python def multiply(a, b): return a * b import emb print(f\\"Addition using C function: {emb.add(3, 4)}\\") ``` And the following C code (*you need to complete the code*): ```c #define PY_SSIZE_T_CLEAN #include <Python.h> // C function to add two integers static PyObject* emb_add(PyObject *self, PyObject *args) { int a, b; if (!PyArg_ParseTuple(args, \\"ii\\", &a, &b)) { return NULL; } return PyLong_FromLong(a + b); } static PyMethodDef EmbMethods[] = { {\\"add\\", emb_add, METH_VARARGS, \\"Add two integers\\"}, {NULL, NULL, 0, NULL} }; static struct PyModuleDef embmodule = { PyModuleDef_HEAD_INIT, \\"emb\\", NULL, -1, EmbMethods }; // Initialize the emb module PyMODINIT_FUNC PyInit_emb(void) { return PyModule_Create(&embmodule); } int main(int argc, char *argv[]) { // Check arguments if (argc < 5) { fprintf(stderr,\\"Usage: %s <python_script> <python_function> <arg1> <arg2>n\\", argv[0]); return 1; } wchar_t *program = Py_DecodeLocale(argv[0], NULL); if (program == NULL) { fprintf(stderr, \\"Fatal error: cannot decode argv[0]n\\"); exit(1); } Py_SetProgramName(program); Py_Initialize(); // Add the emb module PyImport_AppendInittab(\\"emb\\", PyInit_emb); // Load the Python script and function PyObject *pName = PyUnicode_DecodeFSDefault(argv[1]); PyObject *pModule = PyImport_Import(pName); Py_DECREF(pName); if (pModule != NULL) { PyObject *pFunc = PyObject_GetAttrString(pModule, argv[2]); if (pFunc && PyCallable_Check(pFunc)) { // Prepare arguments to Python function PyObject *pArgs = PyTuple_Pack(2, PyLong_FromLong(atoi(argv[3])), PyLong_FromLong(atoi(argv[4]))); PyObject *pValue = PyObject_CallObject(pFunc, pArgs); Py_DECREF(pArgs); if (pValue != NULL) { printf(\\"Result of call: %ldn\\", PyLong_AsLong(pValue)); Py_DECREF(pValue); } else { Py_DECREF(pFunc); Py_DECREF(pModule); PyErr_Print(); fprintf(stderr,\\"Call failedn\\"); return 1; } } else { if (PyErr_Occurred()) PyErr_Print(); fprintf(stderr, \\"Cannot find function \\"%s\\"n\\", argv[2]); } Py_XDECREF(pFunc); Py_DECREF(pModule); } else { PyErr_Print(); fprintf(stderr, \\"Failed to load \\"%s\\"n\\", argv[1]); return 1; } if (Py_FinalizeEx() < 0) return 120; PyMem_RawFree(program); return 0; } ``` # Notes - Be sure to properly manage references and error handling as appropriate. Test your code with a Python script `multiply.py` and verify that: 1. The C executable successfully calls the Python `multiply` function with the provided arguments. 2. The embedded function `add` is properly registered and can be called from within the Python script.","solution":"def multiply(a, b): Multiplies two integers and returns the result. return a * b"},{"question":"**Objective**: Write a Python class that extends the functionality of the `rlcompleter.Completer` class. Your class should support additional custom completion for a user-defined dictionary of key-value pairs, where keys are strings and values are lists of possible completions. **Requirements**: 1. Implement a class `CustomCompleter` that inherits from `rlcompleter.Completer`. 2. Extend the `complete` method to: - **First**: Attempt to complete using the standard functionality provided by `rlcompleter.Completer.complete`. - **Second**: If the above does not yield any completions, check if the text being completed exists as a key in the custom dictionary. If it does, provide completions from the corresponding list of values. **Input**: - A dictionary (`custom_dict`) with string keys and list-of-strings as values. - A string `text` to be completed. - An integer `state` representing the state of completion requested (0 for the first match, 1 for the second, etc.). **Output**: - The `state`th completion for the given text. If no completion exists, return `None`. **Constraints**: - The custom dictionary cannot exceed 100 keys and each list within the dictionary cannot exceed 50 possible completions. - Assume `text` will always be a valid string. **Example Usage**: ```python import rlcompleter class CustomCompleter(rlcompleter.Completer): def __init__(self, custom_dict): super().__init__() self.custom_dict = custom_dict def complete(self, text, state): # First try the standard completion result = super().complete(text, state) if result is not None: return result # If no standard completion found, try custom dictionary if text in self.custom_dict: completions = self.custom_dict[text] if state < len(completions): return completions[state] return None # Example dictionary: custom_dict = { \\"py\\": [\\"python\\", \\"pyramid\\", \\"pycharm\\"], \\"rea\\": [\\"read\\", \\"readline\\", \\"reader\\"] } # Usage: completer = CustomCompleter(custom_dict) print(completer.complete(\\"py\\", 0)) # Output: \\"python\\" print(completer.complete(\\"py\\", 1)) # Output: \\"pyramid\\" print(completer.complete(\\"dummy\\", 0)) # Output: None ``` In your solution, ensure to handle cases where the text does not exist in the custom dictionary properly and return `None` if no completions are found, as specified.","solution":"import rlcompleter class CustomCompleter(rlcompleter.Completer): def __init__(self, custom_dict): super().__init__() self.custom_dict = custom_dict def complete(self, text, state): # First try the standard completion result = super().complete(text, state) if result is not None: return result # If no standard completion found, try custom dictionary if text in self.custom_dict: completions = self.custom_dict[text] if state < len(completions): return completions[state] return None # Example dictionary for testing custom_dict = { \\"py\\": [\\"python\\", \\"pyramid\\", \\"pycharm\\"], \\"rea\\": [\\"read\\", \\"readline\\", \\"reader\\"] } # Example usage completer = CustomCompleter(custom_dict) print(completer.complete(\\"py\\", 0)) # Output: \\"python\\" print(completer.complete(\\"py\\", 1)) # Output: \\"pyramid\\" print(completer.complete(\\"dummy\\", 0)) # Output: None"},{"question":"**Understanding and Utilizing the `readline` Module** You are tasked with creating a small library using the `readline` module to enhance the user command-line experience. This library will manage user input history across sessions, support custom commands completion, and dynamically update based on user history. # Objective Write a Python class `ReadlineHelper` that provides the following functionalities: 1. **Initialization:** - Create an instance method `__init__` that initializes the class with a history file path and sets up history and completion features. 2. **History Management:** - `load_history()`: Method to load history from the provided file. - `save_history()`: Method to save history to the provided file. - `clear_history()`: Method to clear the in-memory command history. - `get_history()`: Method to return the list of all historical commands. - `get_current_history_length()`: Method to return the number of commands currently in history. 3. **Command Completion:** - `set_completer(commands)`: Method to set up a custom completer given a list of possible commands. The completer should provide suggestion for any prefix typed by the user. 4. **Helper Methods:** - `add_command_to_history(command)`: Method to add a command to history. - `remove_command_from_history(index)`: Method to remove a command from history by index. # Constraints - History should be stored in a file specified by the user. - Use the `readline` module functions provided in the documentation. - If a history file provided does not exist, the class should handle it gracefully and start with an empty history. - Ensure the history is only saved during the destruction of the object or explicitly by (`save_history()`). # Example Usage ```python helper = ReadlineHelper(\\"~/.custom_history\\") # Load history from file helper.load_history() # Display current history length print(helper.get_current_history_length()) # Add a custom completer helper.set_completer([\'start\', \'stop\', \'restart\', \'status\']) # Add a command to history helper.add_command_to_history(\\"start\\") # Get all history print(helper.get_history()) # Save current history helper.save_history() ``` Write the `ReadlineHelper` class implementation to fulfill the described functionalities. # Expected Output The code should not produce any direct output but should ensure the helpers manage history and completion as specified. ```python class ReadlineHelper: def __init__(self, histfile): # Your code here def load_history(self): # Your code here def save_history(self): # Your code here def clear_history(self): # Your code here def get_history(self): # Your code here def get_current_history_length(self): # Your code here def set_completer(self, commands): # Your code here def add_command_to_history(self, command): # Your code here def remove_command_from_history(self, index): # Your code here ```","solution":"import os import readline class ReadlineHelper: def __init__(self, histfile: str): self.histfile = os.path.expanduser(histfile) # Load history if file exists, otherwise initialize as empty if os.path.exists(self.histfile): self.load_history() else: readline.clear_history() # Ensure history is saved on object destruction import atexit atexit.register(self.save_history) def load_history(self): if os.path.exists(self.histfile): readline.read_history_file(self.histfile) def save_history(self): readline.write_history_file(self.histfile) def clear_history(self): readline.clear_history() def get_history(self): return [readline.get_history_item(i) for i in range(1, readline.get_current_history_length() + 1)] def get_current_history_length(self): return readline.get_current_history_length() def set_completer(self, commands): def completer(text, state): options = [cmd for cmd in commands if cmd.startswith(text)] if state < len(options): return options[state] return None readline.set_completer(completer) def add_command_to_history(self, command: str): readline.add_history(command) def remove_command_from_history(self, index: int): # The history index is 1-based if 0 < index <= readline.get_current_history_length(): readline.remove_history_item(index - 1)"},{"question":"**Unsupervised Learning: Anomaly Detection with Isolation Forest** # Problem Statement Unsupervised learning techniques are widely used for anomaly detection in datasets. Scikit-learn provides several tools for unsupervised learning, including the `IsolationForest` for anomaly detection. Your task is to implement a function `detect_anomalies(data: np.ndarray) -> np.ndarray` that uses the `IsolationForest` method to identify outliers in a given dataset. # Function Signature ```python import numpy as np def detect_anomalies(data: np.ndarray) -> np.ndarray: pass ``` # Input - `data`: A `numpy` array of shape `(n_samples, n_features)` representing the dataset to analyze. # Output - Returns a `numpy` array of shape `(n_samples,)`, where each element is: - `1` if the corresponding sample in the input `data` is identified as an outlier (anomaly). - `0` otherwise. # Constraints - You must use the `IsolationForest` class from `scikit-learn`. # Performance Requirements - The function should be efficient and able to handle datasets with up to 10,000 samples and 100 features. # Example ```python import numpy as np data = np.array([ [1.0, 2.0], [2.0, 3.0], [3.0, 4.0], [4.0, 5.0], [1000.0, 2000.0], # Anomalous point [3.5, 4.5] ]) result = detect_anomalies(data) print(result) # Output: array([0, 0, 0, 0, 1, 0]) ``` # Notes - Be sure to handle potential issues such as fitting the model to the data and using the appropriate methods to predict anomalies. - Refer to the official scikit-learn documentation for any necessary details about the `IsolationForest` class. Note: The assessment assumes that you have `scikit-learn` installed in your environment.","solution":"import numpy as np from sklearn.ensemble import IsolationForest def detect_anomalies(data: np.ndarray) -> np.ndarray: Uses the IsolationForest method to identify outliers in the given dataset. Parameters: - data: A numpy array of shape (n_samples, n_features) representing the dataset to analyze. Returns: - A numpy array of shape (n_samples,), where each element is: - 1 if the corresponding sample in the input data is identified as an outlier (anomaly). - 0 otherwise. # Initialize the IsolationForest model model = IsolationForest() # Fit the model to the data model.fit(data) # Predict if a point is an outlier or not predictions = model.predict(data) # Convert predictions to 1 for outlier and 0 for inlier anomalies = np.where(predictions == -1, 1, 0) return anomalies"},{"question":"Objective Implement a custom class `ComplexObject` that demonstrates an understanding of both shallow and deep copy operations as provided by the `copy` module. The class should simulate the behavior of defining its own copy operations using special methods. Problem Statement Implement the class `ComplexObject` such that: 1. It has two attributes: - `data`: A list of lists of integers. - `metadata`: A dictionary holding arbitrary data. 2. Define custom shallow and deep copy operations by implementing the special methods `__copy__()` and `__deepcopy__()`. 3. Create a method `modify_data(self, index, value)` that modifies the self-referencing element in `data` at the provided `index` and sets it to `value`. 4. Implement a method `add_metadata(self, key, value)` that adds a key-value pair to the `metadata` dictionary. Input and Output Format Input and output formats are demonstrated through method definitions, but there are no direct input-output requirements for this task. # Class Definition and Methods: ```python class ComplexObject: def __init__(self, data, metadata): Initializes the object with data and metadata. :param data: List of lists containing integers. :param metadata: Dictionary holding arbitrary data. pass def __copy__(self): Implements the shallow copy operation. :return: Shallow copy of the current object. pass def __deepcopy__(self, memo): Implements the deep copy operation. :param memo: Dictionary to keep track of copied objects to handle recursive references. :return: Deep copy of the current object. pass def modify_data(self, index, value): Modifies the data list at the specified index with the given value. :param index: Index of the element in the data list to be modified. :param value: New value to replace the current value at the specified index. pass def add_metadata(self, key, value): Adds a key-value pair to the metadata dictionary. :param key: Key to be added to the metadata dictionary. :param value: Value associated with the key to be added. pass # Example usage: # data = [[1, 2], [3, 4]] # metadata = {\'type\': \'example\'} # obj = ComplexObject(data, metadata) # shallow_copy = copy.copy(obj) # deep_copy = copy.deepcopy(obj) # obj.modify_data(0, [5, 6]) # obj.add_metadata(\'new_key\', \'new_value\') ``` Constraints - The `data` list can contain up to 1000 sublists, and each sublist can contain up to 1000 integers. - The `metadata` dictionary can have up to 100 key-value pairs. Performance Requirements - The shallow copy operation should construct the copy within O(n) time complexity, where n is the length of the `data` attribute. - The deep copy operation should handle recursively nested data structures efficiently using the memo dictionary to avoid redundant copies and manage recursive structures. Notes - Ensure the `__copy__` and `__deepcopy__` methods are properly implemented to test the copying behaviors. - Leverage the functionality of the `copy` module while implementing these custom methods. - Provide test cases demonstrating the difference between shallow and deep copy operations.","solution":"import copy class ComplexObject: def __init__(self, data, metadata): Initializes the object with data and metadata. :param data: List of lists containing integers. :param metadata: Dictionary holding arbitrary data. self.data = data self.metadata = metadata def __copy__(self): Implements the shallow copy operation. :return: Shallow copy of the current object. new_obj = ComplexObject(self.data, self.metadata) return new_obj def __deepcopy__(self, memo): Implements the deep copy operation. :param memo: Dictionary to keep track of copied objects to handle recursive references. :return: Deep copy of the current object. new_data = copy.deepcopy(self.data, memo) new_metadata = copy.deepcopy(self.metadata, memo) new_obj = ComplexObject(new_data, new_metadata) return new_obj def modify_data(self, index, value): Modifies the data list at the specified index with the given value. :param index: Index of the element in the data list to be modified. :param value: New value to replace the current value at the specified index. self.data[index] = value def add_metadata(self, key, value): Adds a key-value pair to the metadata dictionary. :param key: Key to be added to the metadata dictionary. :param value: Value associated with the key to be added. self.metadata[key] = value"},{"question":"You are tasked with implementing a Python function **`numeric_operations_summary`**. This function will receive a list of mixed numeric types and should return a summary of various computed properties and metrics from those numbers. # Function Signature ```python def numeric_operations_summary(numbers: list) -> dict: pass ``` # Inputs - `numbers`: A list containing numeric values (integers, floats, and complex numbers). Guaranteed non-empty. # Outputs - A dictionary with the following keys and values: - `\'total_sum\'`: The sum of all numbers. - `\'total_product\'`: The product of all numbers. If any number is zero, the product should be zero. - `\'max_absolute_value\'`: The maximum absolute value among the numbers. - `\'bitwise_operations\'`: A sub-dictionary with results of bitwise operations on integers only: - `\'bitwise_and\'`: Result of performing bitwise AND across all integers. - `\'bitwise_or\'`: Result of performing bitwise OR across all integers. - `\'bitwise_xor\'`: Result of performing bitwise XOR across all integers. - `\'real_complex_parts\'`: A sub-dictionary summarizing properties of complex numbers: - `\'sum_real\'`: Sum of real parts of all complex numbers. - `\'sum_imaginary\'`: Sum of imaginary parts of all complex numbers. # Constraints - Numbers will include values such as -1.5, 2, 3+4j. - Results should align with appropriate numeric data types (e.g., integer operations should yield integers). - Bit-wise operations are only applicable to integers, and lists containing all non-integer values should simply have `\'bitwise_operations\'` with values `None`. # Example ```python # Example usage numbers = [1, 2, 3.5, -4, 5+3j] expected_output = { \'total_sum\': (1 + 2 + 3.5 - 4 + (5+3j)), \'total_product\': (1 * 2 * 3.5 * -4 * (5 + 3j)), \'max_absolute_value\': max(abs(1), abs(2), abs(3.5), abs(-4), abs(5+3j)), \'bitwise_operations\': { \'bitwise_and\': 1 & 2 & -4, \'bitwise_or\': 1 | 2 | -4, \'bitwise_xor\': 1 ^ 2 ^ -4 }, \'real_complex_parts\': { \'sum_real\': 5, # Only real part of complex numbers summed \'sum_imaginary\': 3 # Only imaginary parts summed } } result = numeric_operations_summary(numbers) ``` # Note In the bitwise operations, if the input does not have sufficient integers (less than one), `bitwise_operations` should have its values set to `None`. # Requirements: 1. Utilize built-in methods and Python operations effectively. 2. Ensure number formatting and types compliance according to their operations (e.g., integer results from bitwise operations). 3. Efficiently segment and aggregate data for summary outputs accordingly.","solution":"from functools import reduce from operator import mul def numeric_operations_summary(numbers: list) -> dict: total_sum = sum(numbers) total_product = reduce(mul, numbers) max_absolute_value = max(numbers, key=abs) integers = [num for num in numbers if isinstance(num, int)] if integers: bitwise_and = reduce(lambda x, y: x & y, integers) bitwise_or = reduce(lambda x, y: x | y, integers) bitwise_xor = reduce(lambda x, y: x ^ y, integers) else: bitwise_and = bitwise_or = bitwise_xor = None complex_numbers = [num for num in numbers if isinstance(num, complex)] sum_real = sum(num.real for num in complex_numbers) sum_imaginary = sum(num.imag for num in complex_numbers) result = { \'total_sum\': total_sum, \'total_product\': total_product, \'max_absolute_value\': max_absolute_value, \'bitwise_operations\': { \'bitwise_and\': bitwise_and, \'bitwise_or\': bitwise_or, \'bitwise_xor\': bitwise_xor }, \'real_complex_parts\': { \'sum_real\': sum_real, \'sum_imaginary\': sum_imaginary } } return result"},{"question":"# Python310 Coding Assessment: File Metadata Analysis You are tasked with creating a directory tree walker that not only lists files but also categorizes them based on their types and collects certain metadata. Your solution should leverage the \\"stat\\" module as detailed in the provided documentation. Objectives: 1. Implement a function `categorize_files` that takes a directory path as input and returns a dictionary categorizing files by their types (e.g., regular files, directories, symbolic links, etc.). 2. For each file, collect the following metadata: - File size - Last modification time - Human-readable file permissions (using `filemode` function) Requirements: 1. Your implementation should minimize the number of `os.stat` calls. 2. You should account for all file types mentioned in the documentation. 3. Handle exceptions gracefully, such as permission errors when accessing files. Function Signature: ```python import os import stat def categorize_files(directory_path: str) -> dict: pass ``` Input: - `directory_path` (str): The root directory path from which the directory tree walking should start. Output: - A dictionary where keys are the file types like `\\"regular_files\\"`, `\\"directories\\"`, `\\"symbolic_links\\"`, etc., and values are lists of dictionaries, each containing: - `path`: Full path to the file. - `size`: Size of the file. - `last_modified`: Last modification time of the file. - `permissions`: Human-readable string of file permissions. Example: ```python categorize_files(\\"/path/to/directory\\") # Output: # { # \\"regular_files\\": [ # {\\"path\\": \\"/path/to/directory/file1.txt\\", \\"size\\": 1234, \\"last_modified\\": 1672549200, \\"permissions\\": \\"-rw-r--r--\\"}, # {\\"path\\": \\"/path/to/directory/file2.csv\\", \\"size\\": 5678, \\"last_modified\\": 1672535600, \\"permissions\\": \\"-rwxr-xr-x\\"} # ], # \\"directories\\": [ # {\\"path\\": \\"/path/to/directory/subdir1\\", \\"size\\": 4096, \\"last_modified\\": 1672530000, \\"permissions\\": \\"drwxr-xr-x\\"} # ], # \\"symbolic_links\\": [ # {\\"path\\": \\"/path/to/directory/link1\\", \\"size\\": 0, \\"last_modified\\": 1672528400, \\"permissions\\": \\"lrwxrwxrwx\\"} # ], # # Include other file types like character devices, block devices, FIFOs, sockets, etc. # } ``` Notes: - Use `stat.S_ISREG`, `stat.S_ISDIR`, etc., to categorize the files. - Use `stat.filemode` to convert file permissions to a human-readable format. - Perform all necessary error handling for file access issues.","solution":"import os import stat from stat import filemode def categorize_files(directory_path: str) -> dict: categorized_files = { \\"regular_files\\": [], \\"directories\\": [], \\"symbolic_links\\": [], \\"character_devices\\": [], \\"block_devices\\": [], \\"fifos\\": [], \\"sockets\\": [] } for root, dirs, files in os.walk(directory_path): for name in dirs + files: path = os.path.join(root, name) try: st = os.lstat(path) file_info = { \\"path\\": path, \\"size\\": st.st_size, \\"last_modified\\": st.st_mtime, \\"permissions\\": filemode(st.st_mode) } if stat.S_ISREG(st.st_mode): categorized_files[\\"regular_files\\"].append(file_info) elif stat.S_ISDIR(st.st_mode): categorized_files[\\"directories\\"].append(file_info) elif stat.S_ISLNK(st.st_mode): categorized_files[\\"symbolic_links\\"].append(file_info) elif stat.S_ISCHR(st.st_mode): categorized_files[\\"character_devices\\"].append(file_info) elif stat.S_ISBLK(st.st_mode): categorized_files[\\"block_devices\\"].append(file_info) elif stat.S_ISFIFO(st.st_mode): categorized_files[\\"fifos\\"].append(file_info) elif stat.S_ISSOCK(st.st_mode): categorized_files[\\"sockets\\"].append(file_info) except PermissionError: continue except Exception as e: continue return categorized_files"},{"question":"# PyTorch MPS Configuration and Optimization You are provided with a PyTorch-based neural network training script set up to run on an Apple device leveraging MPS (Metal Performance Shaders). Your task is to extend the script with functionality to: 1. **Configure environment variables**: Set up the necessary PyTorch environment variables to: - Enable verbose logging of the MPS allocator. - Enable fast math for MPS metal kernels. - Set the high watermark ratio to `1.2` and the low watermark ratio to `0.8`. - Ensure that operations unsupported by MPS fall back to the CPU. 2. **Profile and Trace**: Add code to log profiling information and trace signposts using appropriate environment variables. 3. **Implement a simple feedforward neural network** using PyTorch and train it on a sample dataset (e.g., MNIST). For the training loop, ensure that the necessary configurations are applied. # Expected Function Implementation Input A script that performs the following: - Configure specified environment variables. - Build, train, and evaluate a simple feedforward neural network using PyTorch on MPS. Output - Training and evaluation logs demonstrating the use of configured environment variables. - Log files or console output showing allocator details, profiling, and tracing information. # Constraints - Your code should strictly run on an Apple device supporting MPS. - Ensure that environment variables are set within the script and documented with comments explaining their purpose. # Performance Requirements - The network should successfully train on the sample dataset without exceeding memory limits or causing out-of-memory (OOM) errors. - Profiling and tracing output should be clear and provide insights into the MPS allocator\'s behavior. Sample Dataset You can use the MNIST dataset provided by PyTorch datasets (`torchvision.datasets.MNIST`). # Submission Submit a single script file including: - The code to configure environment variables. - The neural network definition. - Training and evaluation code. - Relevant logs and profiling outputs. Ensure the script is self-contained and executable on a compatible Apple device.","solution":"import os import torch import torch.nn as nn import torch.optim as optim from torchvision.datasets import MNIST from torchvision.transforms import ToTensor from torch.utils.data import DataLoader # Configure environment variables os.environ[\\"PYTORCH_METAL_LOG_ALLOCATOR_STATS\\"] = \\"1\\" # Verbose logging of MPS allocator os.environ[\\"PYTORCH_METAL_USE_FAST_MATH\\"] = \\"1\\" # Enable fast math for MPS metal kernels os.environ[\\"PYTORCH_METAL_HIGH_WATERMARK_RATIO\\"] = \\"1.2\\" # High watermark ratio os.environ[\\"PYTORCH_METAL_LOW_WATERMARK_RATIO\\"] = \\"0.8\\" # Low watermark ratio os.environ[\\"PYTORCH_ENABLE_MPS_FALLBACK\\"] = \\"1\\" # Fallback to CPU for unsupported operations # Ensure the device is set to MPS or fallback to CPU if MPS is not available device = torch.device(\\"mps\\" if torch.has_mps else \\"cpu\\") # Define a simple feedforward neural network class SimpleNN(nn.Module): def __init__(self): super(SimpleNN, self).__init__() self.flatten = nn.Flatten() self.fc1 = nn.Linear(28 * 28, 128) self.relu = nn.ReLU() self.fc2 = nn.Linear(128, 10) def forward(self, x): x = self.flatten(x) x = self.fc1(x) x = self.relu(x) x = self.fc2(x) return x # Load the MNIST dataset train_dataset = MNIST(root=\'.\', train=True, transform=ToTensor(), download=True) test_dataset = MNIST(root=\'.\', train=False, transform=ToTensor(), download=True) train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True) test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False) # Initialize the network, loss function and optimizer model = SimpleNN().to(device) criterion = nn.CrossEntropyLoss() optimizer = optim.Adam(model.parameters(), lr=0.001) # Training the model def train(num_epochs): model.train() for epoch in range(num_epochs): running_loss = 0.0 for inputs, labels in train_loader: inputs, labels = inputs.to(device), labels.to(device) # Zero the parameter gradients optimizer.zero_grad() # Forward pass outputs = model(inputs) loss = criterion(outputs, labels) # Backward pass loss.backward() # Optimization step optimizer.step() # Print statistics running_loss += loss.item() print(f\\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader)}\\") # Evaluating the model def evaluate(): model.eval() correct = 0 total = 0 with torch.no_grad(): for inputs, labels in test_loader: inputs, labels = inputs.to(device), labels.to(device) outputs = model(inputs) _, predicted = torch.max(outputs.data, 1) total += labels.size(0) correct += (predicted == labels).sum().item() print(f\\"Accuracy of the model on the 10000 test images: {100 * correct / total}%\\") # Training and evaluating the model if __name__ == \\"__main__\\": train(5) evaluate()"},{"question":"# SQLite Database Tasks with `sqlite3` Module In this assessment, you will showcase your proficiency in using Python\'s `sqlite3` module to interact with an SQLite database. You will perform a series of operations including creating a database, managing transactions, handling custom data types, and executing queries securely. Task Description 1. **Create a Database and Table** - Create a database file named `books.db` and a table called `book` with the following columns: - `title` (TEXT) - `author` (TEXT) - `published_year` (INTEGER) - `rating` (REAL) 2. **Insert Data into the Table** - Insert the following book records into the `book` table: - (\'To Kill a Mockingbird\', \'Harper Lee\', 1960, 4.27) - (\'1984\', \'George Orwell\', 1949, 4.17) - (\'Pride and Prejudice\', \'Jane Austen\', 1813, 4.25) - (\'The Great Gatsby\', \'F. Scott Fitzgerald\', 1925, 3.91) - (\'Moby-Dick\', \'Herman Melville\', 1851, 3.49) 3. **Query the Database Securely** - Write a function `get_books_by_year(year: int) -> List[Tuple[str, str, int, float]]` that returns a list of tuples containing book details for books published in a given year. Use parameterized queries to prevent SQL injection. 4. **Transaction Management** - Implement a function `update_book_rating(title: str, new_rating: float) -> None` that updates the rating of a book specified by its title, using a transaction. Ensure the transaction is committed only if the update is successful. 5. **Custom Python Type Handling** - Define a custom Python class `Book` with attributes `title`, `author`, `published_year`, and `rating`. - Write an adapter and a converter to store and retrieve instances of `Book` in/from the database. - Implement a function `store_book(book: Book) -> None` that inserts a `Book` instance into the `book` table using the custom adapter. - Implement a function `retrieve_books() -> List[Book]` that retrieves all book records from the `book` table as a list of `Book` instances using the custom converter. Implementation Details - Ensure that your code handles exceptions where appropriate. - Optimize performance by minimizing redundant database connections and using efficient data retrieval techniques. - Document your code to explain the functionality and reasoning behind each part of the implementation. You can use the following sample code as a starting point: ```python import sqlite3 from typing import List, Tuple # Step 1: Create a database and table def create_database(): pass # Step 2: Insert data into the table def insert_data(): pass # Step 3: Query the database securely def get_books_by_year(year: int) -> List[Tuple[str, str, int, float]]: pass # Step 4: Transaction management def update_book_rating(title: str, new_rating: float) -> None: pass # Step 5: Custom Python type handling class Book: def __init__(self, title: str, author: str, published_year: int, rating: float): pass def adapt_book(book: Book): pass def convert_book(data: bytes) -> Book: pass def store_book(book: Book) -> None: pass def retrieve_books() -> List[Book]: pass # Add main execution code and testing if necessary if __name__ == \\"__main__\\": create_database() insert_data() # Additional function calls for testing ``` Fill in the implementations for the functions and classes according to the task descriptions above. Constraints - You must use parameterized queries for SQL operations to prevent SQL injection. - You should adhere to best practices for exception handling and transaction management. - Ensure that your custom type handling is correctly integrated with the SQLite database. Good luck!","solution":"import sqlite3 from typing import List, Tuple # Step 1: Create a database and table def create_database(): connection = sqlite3.connect(\'books.db\') cursor = connection.cursor() cursor.execute(\'\'\'CREATE TABLE IF NOT EXISTS book ( title TEXT, author TEXT, published_year INTEGER, rating REAL )\'\'\') connection.commit() connection.close() # Step 2: Insert data into the table def insert_data(): books = [ (\'To Kill a Mockingbird\', \'Harper Lee\', 1960, 4.27), (\'1984\', \'George Orwell\', 1949, 4.17), (\'Pride and Prejudice\', \'Jane Austen\', 1813, 4.25), (\'The Great Gatsby\', \'F. Scott Fitzgerald\', 1925, 3.91), (\'Moby-Dick\', \'Herman Melville\', 1851, 3.49) ] connection = sqlite3.connect(\'books.db\') cursor = connection.cursor() cursor.executemany(\'INSERT INTO book (title, author, published_year, rating) VALUES (?, ?, ?, ?)\', books) connection.commit() connection.close() # Step 3: Query the database securely def get_books_by_year(year: int) -> List[Tuple[str, str, int, float]]: connection = sqlite3.connect(\'books.db\') cursor = connection.cursor() cursor.execute(\'SELECT * FROM book WHERE published_year = ?\', (year,)) results = cursor.fetchall() connection.close() return results # Step 4: Transaction management def update_book_rating(title: str, new_rating: float) -> None: connection = sqlite3.connect(\'books.db\') cursor = connection.cursor() try: cursor.execute(\'UPDATE book SET rating = ? WHERE title = ?\', (new_rating, title)) connection.commit() except sqlite3.Error as e: connection.rollback() print(f\\"An error occurred: {e}\\") finally: connection.close() # Step 5: Custom Python type handling class Book: def __init__(self, title: str, author: str, published_year: int, rating: float): self.title = title self.author = author self.published_year = published_year self.rating = rating def adapt_book(book: Book): return f\\"{book.title},{book.author},{book.published_year},{book.rating}\\" def convert_book(data: bytes) -> Book: title, author, published_year, rating = data.split(\',\') return Book(title, author, int(published_year), float(rating)) def store_book(book: Book) -> None: sqlite3.register_adapter(Book, adapt_book) sqlite3.register_converter(\\"BookType\\", convert_book) connection = sqlite3.connect(\'books.db\') cursor = connection.cursor() cursor.execute(\'INSERT INTO book (title, author, published_year, rating) VALUES (?, ?, ?, ?)\', (book.title, book.author, book.published_year, book.rating)) connection.commit() connection.close() def retrieve_books() -> List[Book]: sqlite3.register_adapter(Book, adapt_book) sqlite3.register_converter(\\"BookType\\", convert_book) connection = sqlite3.connect(\'books.db\') connection.row_factory = lambda cursor, row: Book(row[0], row[1], row[2], row[3]) cursor = connection.cursor() cursor.execute(\'SELECT * FROM book\') results = cursor.fetchall() connection.close() return results # Add main execution code and testing if necessary if __name__ == \\"__main__\\": create_database() insert_data() # Additional function calls for testing"},{"question":"You are tasked with writing a Python script that uses the `optparse` module to manage command-line options. The script should handle various types of options, including standard and custom options with different actions. You need to ensure proper error handling and user-friendly help messages. Specifications: 1. **Create an `OptionParser` instance** with a usage message: \\"usage: %prog [options] input_file\\". 2. **Add the following options**: - `-v` or `--verbose`: A flag option that sets verbosity. Should use the `store_true` action. - `-q` or `--quiet`: A flag option that sets quiet mode. Should use the `store_false` action and have verbosity default to True. - `-o` or `--output`: An option that specifies the output file. Should use the `store` action with `type=\\"string\\"` and have a default value of \\"output.txt\\". - `-n` or `--number`: An option that specifies a number. Should use the `store` action with `type=\\"int\\"`. - `-c` or `--config`: An option that reads a configuration file. Should use a custom `callback` action that prints \\"Loading configuration from <filename>\\" and sets a `config_loaded` flag to True. 3. **Generate Help Messages**: - Ensure that each option has a meaningful help message. - Display a custom help message when the user requests help. 4. **Error Handling**: - Ensure any user errors (e.g., missing required arguments) are handled gracefully with an appropriate error message. Input - A list of command-line arguments (for the sake of this exercise, assume `sys.argv` is given as input). Output - Print debug or error messages based on the input options. - Print help messages when the `-h` or `--help` option is used. Example Usage 1. Running the script with help message: ``` python yourscript.py --help Usage: yourscript.py [options] input_file Options: -h, --help show this help message and exit -v, --verbose Enable verbose mode -q, --quiet Enable quiet mode -o FILE, --output=FILE Specify output file [default: output.txt] -n NUMBER, --number=NUMBER Specify a number -c FILE, --config=FILE Load configuration from FILE ``` 2. Running the script with some options: ``` python yourscript.py input.txt -v -o results.txt -n 42 -c config.cfg Enable verbose mode Loading configuration from config.cfg ``` 3. Error scenarios: ``` python yourscript.py Usage: yourscript.py [options] input_file yourscript.py: error: incorrect number of arguments ``` Constraints - The script should use `optparse` and not `argparse` or any other argument parsing libraries. - Ensure that the script can handle options gracefully and provide meaningful error messages. Implementation ```python # Import the necessary module from optparse import OptionParser, OptionValueError def load_config_callback(option, opt, value, parser): print(f\\"Loading configuration from {value}\\") parser.values.config_loaded = True # Function to parse arguments def parse_arguments(argv): usage = \\"usage: %prog [options] input_file\\" parser = OptionParser(usage=usage) # Adding options parser.add_option(\\"-v\\", \\"--verbose\\", action=\\"store_true\\", dest=\\"verbose\\", help=\\"Enable verbose mode\\") parser.add_option(\\"-q\\", \\"--quiet\\", action=\\"store_false\\", dest=\\"verbose\\", help=\\"Enable quiet mode\\", default=True) parser.add_option(\\"-o\\", \\"--output\\", action=\\"store\\", type=\\"string\\", dest=\\"output\\", default=\\"output.txt\\", help=\\"Specify output file [default: %default]\\") parser.add_option(\\"-n\\", \\"--number\\", action=\\"store\\", type=\\"int\\", dest=\\"number\\", help=\\"Specify a number\\") parser.add_option(\\"-c\\", \\"--config\\", action=\\"callback\\", callback=load_config_callback, type=\\"string\\", dest=\\"config\\", help=\\"Load configuration from FILE\\") (options, args) = parser.parse_args(argv) # Error handling: Ensure input_file is provided if len(args) != 1: parser.error(\\"incorrect number of arguments\\") input_file = args[0] return options, input_file if __name__ == \\"__main__\\": import sys options, input_file = parse_arguments(sys.argv[1:]) if options.verbose: print(\\"Enable verbose mode\\") if options.config_loaded: print(f\\"Configuration loaded from {options.config}\\") print(f\\"Processing input file: {input_file}\\") print(f\\"Output will be saved to: {options.output}\\") if options.number: print(f\\"Number specified: {options.number}\\") ```","solution":"from optparse import OptionParser, OptionValueError def load_config_callback(option, opt, value, parser): print(f\\"Loading configuration from {value}\\") parser.values.config_loaded = True def parse_arguments(argv): usage = \\"usage: %prog [options] input_file\\" parser = OptionParser(usage=usage) # Adding options parser.add_option(\\"-v\\", \\"--verbose\\", action=\\"store_true\\", dest=\\"verbose\\", help=\\"Enable verbose mode\\") parser.add_option(\\"-q\\", \\"--quiet\\", action=\\"store_false\\", dest=\\"verbose\\", help=\\"Enable quiet mode\\", default=True) parser.add_option(\\"-o\\", \\"--output\\", action=\\"store\\", type=\\"string\\", dest=\\"output\\", default=\\"output.txt\\", help=\\"Specify output file [default: %default]\\") parser.add_option(\\"-n\\", \\"--number\\", action=\\"store\\", type=\\"int\\", dest=\\"number\\", help=\\"Specify a number\\") parser.add_option(\\"-c\\", \\"--config\\", action=\\"callback\\", callback=load_config_callback, type=\\"string\\", dest=\\"config\\", help=\\"Load configuration from FILE\\") # Prepare custom flag to check if config is loaded parser.set_defaults(config_loaded=False) (options, args) = parser.parse_args(argv) # Error handling: Ensure input_file is provided if len(args) != 1: parser.error(\\"incorrect number of arguments\\") input_file = args[0] return options, input_file if __name__ == \\"__main__\\": import sys options, input_file = parse_arguments(sys.argv[1:]) if options.verbose: print(\\"Enable verbose mode\\") print(f\\"Processing input file: {input_file}\\") print(f\\"Output will be saved to: {options.output}\\") if options.number: print(f\\"Number specified: {options.number}\\") if options.config_loaded: print(f\\"Configuration loaded from {options.config}\\")"},{"question":"# Secure Message Authentication In this problem, you will implement a function to authenticate a message using the HMAC algorithm. The implementation will secure a given message with a given secret key and verify it using another provided signature. You need to implement two functions: `generate_signature` and `verify_signature`. 1. `generate_signature(key: bytes, msg: bytes, digestmod: str) -> str`: - Generates and returns a hexadecimal digest (signature) of the input message using the HMAC algorithm with the provided key and hash function. 2. `verify_signature(key: bytes, msg: bytes, provided_sig: str, digestmod: str) -> bool`: - Verifies the provided signature against the computed HMAC digest of the input message using the provided key and hash function, returning `True` if they match and `False` otherwise. # Specifications `generate_signature` Function - **Input**: - `key`: A bytes object representing the secret key. - `msg`: A bytes object representing the message to be authenticated. - `digestmod`: A string representing the hash digest algorithm (e.g., \'sha256\', \'sha1\'). - **Output**: - A string representing the hexadecimal HMAC of the message using the provided secret key and hash function. `verify_signature` Function - **Input**: - `key`: A bytes object representing the secret key. - `msg`: A bytes object representing the message to be authenticated. - `provided_sig`: A string representing the provided HMAC signature to verify against. - `digestmod`: A string representing the hash digest algorithm (e.g., \'sha256\', \'sha1\'). - **Output**: - A boolean value indicating whether the provided signature matches the calculated HMAC of the message. # Constraints - The `digestmod` parameter must be a valid hash name supported by `hashlib`. - The secret key and message will be non-empty byte sequences. - Use the `hmac` module from Python\'s standard library to create and verify the HMAC signatures. # Example ```python import hmac import hashlib def generate_signature(key, msg, digestmod): h = hmac.new(key, msg, digestmod) return h.hexdigest() def verify_signature(key, msg, provided_sig, digestmod): h = hmac.new(key, msg, digestmod) computed_sig = h.hexdigest() return hmac.compare_digest(computed_sig, provided_sig) # Example usage: key = b\\"secret\\" msg = b\\"message\\" digestmod = \\"sha256\\" # Generate a signature signature = generate_signature(key, msg, digestmod) print(\\"Generated Signature:\\", signature) # Verify a correct signature is_valid = verify_signature(key, msg, signature, digestmod) print(\\"Is valid signature?\\", is_valid) # Output: True # Verify an incorrect signature is_valid = verify_signature(key, msg, \\"wrong_signature\\", digestmod) print(\\"Is valid signature?\\", is_valid) # Output: False ``` **Note**: Ensure to use the `compare_digest` function for secure comparison as mentioned in the documentation to avoid timing attacks.","solution":"import hmac import hashlib def generate_signature(key, msg, digestmod): Generates a hexadecimal digest (HMAC) of the input message using the provided key and hash function. Parameters: key (bytes): The secret key. msg (bytes): The message to be authenticated. digestmod (str): The hash digest algorithm (e.g., \'sha256\', \'sha1\'). Returns: str: The hexadecimal HMAC of the message. h = hmac.new(key, msg, digestmod) return h.hexdigest() def verify_signature(key, msg, provided_sig, digestmod): Verifies the provided signature against the computed HMAC digest of the input message. Parameters: key (bytes): The secret key. msg (bytes): The message to be authenticated. provided_sig (str): The provided HMAC signature to verify against. digestmod (str): The hash digest algorithm (e.g., \'sha256\', \'sha1\'). Returns: bool: True if the provided signature matches the computed HMAC, False otherwise. h = hmac.new(key, msg, digestmod) computed_sig = h.hexdigest() return hmac.compare_digest(computed_sig, provided_sig)"},{"question":"You are required to implement a Python program that reads a text file, compresses its content using the LZMA algorithm, writes the compressed data to another file, then subsequently reads the compressed file, decompresses its content, and writes the decompressed content back to a third file. Your solution should demonstrate the use of the `lzma` module functions and classes effectively. # Instructions 1. **Function**: `compress_file(input_file: str, compressed_file: str) -> None` - Reads the content of `input_file`. - Compresses the content using the LZMA algorithm. - Writes the compressed content to `compressed_file`. 2. **Function**: `decompress_file(compressed_file: str, output_file: str) -> None` - Reads the compressed content from `compressed_file`. - Decompresses the content. - Writes the decompressed content to `output_file`. Constraints: - The input text file (`input_file`) will be a valid text file. - The file paths provided will be valid and accessible. Example: Suppose there is a file `example.txt` with the following content: ``` Hello, this is a test file. This file contains multiple lines of text. This file is used to demonstrate LZMA compression and decompression. ``` Calling the functions: ```python compress_file(\'example.txt\', \'example_compressed.xz\') decompress_file(\'example_compressed.xz\', \'example_decompressed.txt\') ``` - `example_compressed.xz` should contain the LZMA-compressed data. - `example_decompressed.txt` should contain the same content as `example.txt`. # Requirements: - Your code should handle potential exceptions during file read/write operations. - Efficient memory management should be considered, especially for large files. - Include comments explaining the key parts of your implementation. # Submission: Submit the complete code for the functions `compress_file` and `decompress_file`. Ensure that your code is well-documented, following Python\'s best practices.","solution":"import lzma def compress_file(input_file: str, compressed_file: str) -> None: Compresses the content of input_file using LZMA and writes to compressed_file. try: with open(input_file, \'rb\') as f_in: file_content = f_in.read() compressed_content = lzma.compress(file_content) with open(compressed_file, \'wb\') as f_out: f_out.write(compressed_content) except Exception as e: print(f\\"An error occurred during compression: {e}\\") def decompress_file(compressed_file: str, output_file: str) -> None: Decompresses the content of compressed_file using LZMA and writes to output_file. try: with open(compressed_file, \'rb\') as f_in: compressed_content = f_in.read() decompressed_content = lzma.decompress(compressed_content) with open(output_file, \'wb\') as f_out: f_out.write(decompressed_content) except Exception as e: print(f\\"An error occurred during decompression: {e}\\")"},{"question":"You are required to create a multi-faceted plot using Seaborn\'s `objects` interface. The dataset to be used will be Seaborn\'s inbuilt \'penguins\' dataset. # Task: 1. Load the \'penguins\' dataset using Seaborn. 2. Create a 2x2 facet grid of scatter plots with the following conditions: - The columns should differentiate between species and the rows should differentiate between the sex of the penguins (use columns `species` and `sex` to separate the facets). - In each scatter plot, map \'bill_length_mm\' to the x-axis and \'bill_depth_mm\' to the y-axis. 3. Set the overall dimensions of the figure to be 12 inches by 8 inches. 4. Use the `constrained` layout engine. 5. Save the resulting plot to a file named \\"facet_penguins.png\\". # Requirements: 1. Your implementation must include appropriate imports and loading of necessary libraries. 2. Clearly define each of the steps in your code with comments. 3. Make sure the final plot is saved as \\"facet_penguins.png\\" in the same directory as the script. # Input: There are no function inputs for this task as you are using a specific dataset and predefined facet requirements. # Output: A saved plot image file named \\"facet_penguins.png\\". # Constraints: - You may only use the seaborn and pandas libraries for this task. ```python # Begin your solution here import seaborn as sns import seaborn.objects as so import pandas as pd # Load the \'penguins\' dataset penguins = sns.load_dataset(\'penguins\') # Create the plot and set the layout plot = so.Plot(data=penguins).facet([\\"species\\", \\"sex\\"], layout=\\"flex\\").add(so.Dot(color=\\"species\\")) # Set overall figure dimensions plot.layout(size=(12, 8), engine=\\"constrained\\") # Save the plot plot.save(\\"facet_penguins.png\\") ```","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt # Load the \'penguins\' dataset penguins = sns.load_dataset(\'penguins\') # Create a 2x2 facet grid of scatter plots using seaborn\'s FacetGrid g = sns.FacetGrid(penguins, col=\'species\', row=\'sex\', height=3, aspect=1.5) # Map scatter plot with bill_length_mm vs bill_depth_mm g = g.map(plt.scatter, \'bill_length_mm\', \'bill_depth_mm\') # Set the overall dimensions and layout g.fig.set_size_inches(12, 8) plt.tight_layout() # Save the plot to a file g.savefig(\\"facet_penguins.png\\")"},{"question":"# Python Coding Assessment Question **Objective**: Implement utility functions for handling source distribution file inclusion and parsing a manifest template to ensure specific files are included or excluded while creating a source distribution. **Problem Statement**: You are required to implement a Python function `parse_manifest_template` which, given a manifest template content as a string, generates a list of files that should be included in the source distribution. The files available within the project directory are provided as a second parameter. # Function Signature: ```python def parse_manifest_template(manifest_template: str, available_files: List[str]) -> List[str]: pass ``` # Input: 1. `manifest_template` (str): The content of a manifest template file. Each line within the template specifies a command to include or exclude files. 2. `available_files` (List[str]): A list of all files available in the project directory (relative paths). # Output: - Returns a list of filenames that should be included in the source distribution after processing all commands within the provided manifest template. # Requirements and Constraints: 1. The manifest template supports the following commands: - `include <pattern>`: Include all files matching the pattern. - `recursive-include <dir> <pattern1> <pattern2> ...`: For a given directory, include all files matching any of the specified patterns. - `prune <dir>`: Exclude all files under a specific directory. - All paths and patterns should be POSIX-style (forward-slash \'/\' as separator). 2. The list of `available_files` contains relative paths and does not contain directories with excluded files unless specified to be pruned. 3. You are required to avoid redundancy; if the same file is included by multiple commands, it should only appear once in the result. 4. Assume that all provided patterns use standard Unix shell-style wildcards (`*`), and you must adhere to the inclusion/exclusion of files as specified without error. 5. Your implementation should be efficient, aiming to process the manifest template and available files list in linear or near-linear time with respect to their sizes. # Example Given the manifest template: ``` include *.txt recursive-include src *.py *.md prune src/build ``` And the available files: ``` [ \\"README.txt\\", \\"setup.py\\", \\"src/main.py\\", \\"src/utils.py\\", \\"src/build/temp.py\\", \\"docs/guide.md\\" ] ``` The function call: ```python result = parse_manifest_template( \\"include *.txtnrecursive-include src *.py *.mdnprune src/buildn\\", [\\"README.txt\\", \\"setup.py\\", \\"src/main.py\\", \\"src/utils.py\\", \\"src/build/temp.py\\", \\"docs/guide.md\\"] ) ``` Should return: ``` [\\"README.txt\\", \\"src/main.py\\", \\"src/utils.py\\"] ``` # Notes: - This question tests the understanding of file system operations, basic command-line interactions, pattern matching, and list manipulations in Python. - Implementing recursive operations accurately and efficiently is key to solving this problem.","solution":"from typing import List import fnmatch import os def parse_manifest_template(manifest_template: str, available_files: List[str]) -> List[str]: include_files = set() def include(pattern: str): return fnmatch.filter(available_files, pattern) def recursive_include(dir_pattern: str, *patterns: str): included_files = [] for file in available_files: if file.startswith(dir_pattern): sub_path = file[len(dir_pattern):] if sub_path.startswith(\\"/\\"): sub_path = sub_path[1:] if any(fnmatch.fnmatch(sub_path, pattern) for pattern in patterns): included_files.append(file) return included_files def prune(dir_pattern: str): return [file for file in include_files if not file.startswith(dir_pattern)] for line in manifest_template.strip().split(\'n\'): if line.startswith(\\"include \\"): pattern = line.split(\\"include \\")[1].strip() included = include(pattern) include_files.update(included) elif line.startswith(\\"recursive-include \\"): parts = line.split() dir_pattern = parts[1] patterns = parts[2:] included = recursive_include(dir_pattern, *patterns) include_files.update(included) elif line.startswith(\\"prune \\"): dir_pattern = line.split(\\"prune \\")[1].strip() include_files = set(prune(dir_pattern)) return sorted(include_files)"},{"question":"# Custom SMTP Server Implementation **Objective**: Design and implement a custom SMTP server class that processes incoming emails and extracts specific parts of the email data. Requirements - Create a custom SMTP server by extending the `smtpd.SMTPServer` class. - Override the `process_message` method to implement custom email processing. - The server should: - Log the sender\'s address (`mailfrom`), the list of recipient addresses (`rcpttos`), and the email content (`data`). - Extract the subject of the email from the `data` and print it. - Your server should handle emails with a maximum data size of 1MB. - Handle both UTF-8 decoding (when `decode_data` is `True`) and plain text (when `decode_data` is `False`). Input Format No specific input format as it relies on SMTP email data. Output Format For each processed email, print the following (each item on a new line): - Sender\'s address: `mailfrom` - Recipients\' addresses: `rcpttos` - Subject of the Email: `subject` Constraints - Use the `smtpd.SMTPServer` and its related functionalities. - Ensure the email subject extraction works with emails following RFC 5321 format. - The implemented server should allow for a smooth extension to handle additional SMTP features if needed in the future. Example ```python import smtpd class CustomSMTPServer(smtpd.SMTPServer): def process_message(self, peer, mailfrom, rcpttos, data, **kwargs): print(f\\"Sender: {mailfrom}\\") print(f\\"Recipients: {rcpttos}\\") subject = \\"Subject not found\\" # Extract subject from the data for line in data.splitlines(): if line.startswith(\\"Subject:\\"): subject = line break print(f\\"Subject: {subject}\\") # Setup and run the server (bind to localhost:1025 for testing) server = CustomSMTPServer((\'localhost\', 1025), None, data_size_limit=1048576, decode_data=False) asyncore.loop() ``` Write your implementation below: ```python # Your implementation here ```","solution":"import smtpd import asyncore class CustomSMTPServer(smtpd.SMTPServer): def process_message(self, peer, mailfrom, rcpttos, data, **kwargs): print(f\\"Sender: {mailfrom}\\") print(f\\"Recipients: {rcpttos}\\") subject = \\"Subject not found\\" # Extract subject from the data for line in data.split(\'n\'): if line.startswith(\\"Subject:\\"): subject = line break print(f\\"Subject: {subject}\\") # To run the server, you would typically un-comment the lines below in a script # server = CustomSMTPServer((\'localhost\', 1025), None, data_size_limit=1048576, decode_data=False) # asyncore.loop()"},{"question":"# URL Content Fetch and Processing As an aspiring developer, you are required to create a function that performs the following tasks, demonstrating your understanding of the `urllib` package: 1. **Fetch Content from URL**: - Write a function `fetch_url_content(url: str) -> str` that takes a URL as an input and returns the content of the URL as a string. 2. **Extract Links**: - Extend the function `extract_links(html_content: str) -> List[str]` that takes an HTML string as input and extracts all the URLs (links) contained within `<a>` tags, returning them as a list of strings. 3. **Handle Errors**: - Modify the `fetch_url_content` function to handle the case when the URL cannot be accessed due to HTTP errors (4xx and 5xx status codes). It should return the string `\\"Error: Unable to retrieve URL\\"` when such an error occurs. # Function Signatures ```python def fetch_url_content(url: str) -> str: Fetches the content of the given URL. Handles HTTP errors and returns an error message if the URL cannot be accessed. Args: url (str): The URL to fetch content from. Returns: str: The content of the URL or an error message. # Your implementation here def extract_links(html_content: str) -> List[str]: Extracts and returns all links from the given HTML content. Args: html_content (str): The HTML content to parse for links. Returns: List[str]: A list of extracted URLs. # Your implementation here ``` # Constraints - Assume the URL provided to `fetch_url_content` is a valid format but may not always be reachable. - The URL content can be plain text or HTML. # Example Usage ```python url = \\"https://example.com\\" html_content = fetch_url_content(url) if \\"Error\\" not in html_content: links = extract_links(html_content) print(f\\"Found {len(links)} links: {links}\\") else: print(html_content) ``` This problem requires the students to use the `urllib.request` module for fetching content from a URL, handle exceptions using the `urllib.error` module, and possibly parse HTML for links using string or regex manipulation.","solution":"import urllib.request from urllib.error import URLError, HTTPError import re from typing import List def fetch_url_content(url: str) -> str: Fetches the content of the given URL. Handles HTTP errors and returns an error message if the URL cannot be accessed. Args: url (str): The URL to fetch content from. Returns: str: The content of the URL or an error message. try: with urllib.request.urlopen(url) as response: return response.read().decode(\'utf-8\') except HTTPError as e: return \\"Error: Unable to retrieve URL\\" except URLError as e: return \\"Error: Unable to retrieve URL\\" def extract_links(html_content: str) -> List[str]: Extracts and returns all links from the given HTML content. Args: html_content (str): The HTML content to parse for links. Returns: List[str]: A list of extracted URLs. return re.findall(r\'href=\\"(http[s]?://[^\\"]+)\\"\', html_content)"},{"question":"**Question: Message Authentication and Secure Token Generation** Using Python\'s cryptographic libraries (`hashlib`, `hmac`, and `secrets`), write a function `authenticate_message_and_generate_token` that takes a message and a key, authenticates the message using HMAC, and generates a secure token. The function should return a tuple containing the HMAC digest of the message and a secure token. # Function Signature ```python def authenticate_message_and_generate_token(message: str, key: str) -> tuple: pass ``` # Input - `message` (str): The message string to be authenticated. - `key` (str): The key string used for HMAC. # Output - A tuple: `(hmac_digest, secure_token)` where: - `hmac_digest` is a hexadecimal digest of the HMAC of the message. - `secure_token` is a secure token generated using the `secrets` module. # Constraints - Use a hash algorithm from the `hashlib` module for generating the HMAC. - The secure token should be at least 16 bytes in length. - The function should handle empty strings for both `message` and `key` by raising a `ValueError`. # Example ```python message = \\"Hello, World!\\" key = \\"secret_key\\" hmac_digest, secure_token = authenticate_message_and_generate_token(message, key) # hmac_digest should be a valid HMAC hex digest of the given message # secure_token should be a 16-byte secure token print(hmac_digest, secure_token) ``` # Performance Requirements - Execution time should be efficient for typical message sizes up to 1MB. - The security of the HMAC and the random token should match the default standards provided by the respective libraries. # Notes - Make sure to handle exceptions and edge cases appropriately. - Include docstrings and comments where necessary.","solution":"import hmac import hashlib import secrets def authenticate_message_and_generate_token(message: str, key: str) -> tuple: Authenticates a message using HMAC and generates a secure token. Args: - message (str): The message string to be authenticated. - key (str): The key string used for HMAC. Returns: - tuple: (hmac_digest, secure_token) - hmac_digest (str): Hexadecimal HMAC digest of the message. - secure_token (str): A secure token generated using `secrets` module. Raises: - ValueError: If either the message or key is an empty string. if not message or not key: raise ValueError(\\"Both message and key must be non-empty strings.\\") # Create HMAC digest hmac_digest = hmac.new(key.encode(), message.encode(), hashlib.sha256).hexdigest() # Generate secure token secure_token = secrets.token_hex(16) return (hmac_digest, secure_token)"},{"question":"Advanced Enum Implementation **Objective**: Implement an enumeration using Python\'s `enum` module that incorporates custom methods, value constraints, and unique value enforcement. # Problem Statement You are tasked to create an enumeration `Status` that represents various workflow stages of a task in a task management system. Each stage should have an associated numeric identifier and a human-readable description. 1. **Enumeration Requirements**: - The enumeration should have the following stages: - `TO_DO` (identifier: 1, description: \'Task to be done\') - `IN_PROGRESS` (identifier: 2, description: \'Task in progress\') - `REVIEW` (identifier: 3, description: \'Task under review\') - `DONE` (identifier: 4, description: \'Task is done\') - Ensure that no two stages have the same identifier. - Ensure that no two stages have the same description. - Implement a custom method `describe()` that returns a string combining the name, identifier, and description of the stage (e.g., `\'TO_DO: 1, Task to be done\'`). 2. **Custom Behavior**: - The enumeration should not allow creating new enumeration members or modifying existing ones once defined. - Add appropriate error handling to enforce the uniqueness constraints on identifiers and descriptions. # Implementation Details - Define the `Status` enum class with the constraints and methods described. - Use the `unique` decorator to ensure unique values for enumeration members. - Implement any necessary custom methods within the enum to support the described functionality. # Constraints - The identifiers are distinct integers and must be unique. - The descriptions are distinct strings and must be unique. - Use Python 3.10 or later. # Example Usage ```python from enum import Enum, unique @unique class Status(Enum): TO_DO = (1, \\"Task to be done\\") IN_PROGRESS = (2, \\"Task in progress\\") REVIEW = (3, \\"Task under review\\") DONE = (4, \\"Task is done\\") def __init__(self, identifier, description): self._identifier = identifier self._description = description @property def identifier(self): return self._identifier @property def description(self): return self._description def describe(self): return f\\"{self.name}: {self.identifier}, {self.description}\\" # Example Usage print(Status.TO_DO.describe()) # Output: TO_DO: 1, Task to be done ``` Please implement the `Status` enum as described and ensure that all constraints and behaviors are enforced.","solution":"from enum import Enum, unique @unique class Status(Enum): TO_DO = (1, \\"Task to be done\\") IN_PROGRESS = (2, \\"Task in progress\\") REVIEW = (3, \\"Task under review\\") DONE = (4, \\"Task is done\\") def __init__(self, identifier, description): self._identifier = identifier self._description = description @property def identifier(self): return self._identifier @property def description(self): return self._description def describe(self): return f\\"{self.name}: {self.identifier}, {self.description}\\""},{"question":"# **URL Data Extraction with Handling of `robots.txt` and Error Management** You are required to implement a function `fetch_url_data_with_check` in Python which will: 1. **Parse and check** the given URL against the site\'s `robots.txt` to ensure scraping is allowed. 2. **Fetch data** from the provided URL if scraping is permitted. 3. **Handle errors** gracefully by catching exceptions raised during the URL fetching process. 4. **Parse specific data** from the fetched content and return it. For simplicity, assume you need to extract and return the titles (`<title>` tags) from the HTML page. # Requirements: Function Signature: ```python def fetch_url_data_with_check(url: str) -> str: pass ``` Input: * A single string parameter, `url`, representing the URL to be processed. Output: * A string containing the content within the `<title>` tags of the HTML page fetched, or a specific error message if an error occurred. Constraints: * Network issues, HTTP errors, or non-compliance with `robots.txt` should be addressed with appropriate error messages. * The solution should be robust enough to handle various URL structures and different types of errors. Example: ```python # Given a URL e.g. \\"http://example.com\\" print(fetch_url_data_with_check(\\"http://example.com\\")) # Possible Output: \\"Example Domain\\" or \\"Error: Not allowed by robots.txt\\" or \\"Error: HTTPError 404\\" ``` Hints: 1. Use `urllib.robotparser.RobotFileParser` to parse the `robots.txt`. 2. Use `urllib.request.urlopen` to open and read the requested URL. 3. Handle exceptions from `urllib.error`. 4. Parse the HTML to extract the title using tools from the standard library like `html.parser`. # Note: Make sure to include error messages such as: * \\"Error: Not allowed by robots.txt\\" * \\"Error: HTTPError `<specific error code>`\\" * \\"Error: `<description of other possible exceptions>`\\" Good luck and ensure your code is well-documented for readability.","solution":"import urllib.robotparser import urllib.request from urllib.error import HTTPError, URLError from html.parser import HTMLParser class TitleParser(HTMLParser): def __init__(self): super().__init__() self.in_title = False self.title = None def handle_starttag(self, tag, attrs): if tag == \\"title\\": self.in_title = True def handle_endtag(self, tag): if tag == \\"title\\": self.in_title = False def handle_data(self, data): if self.in_title: self.title = data def fetch_url_data_with_check(url: str) -> str: # Parse robots.txt try: robot_parser = urllib.robotparser.RobotFileParser() robots_url = f\\"{url.split(\'/\')[0]}//{url.split(\'/\')[2]}/robots.txt\\" robot_parser.set_url(robots_url) robot_parser.read() if not robot_parser.can_fetch(\\"*\\", url): return \\"Error: Not allowed by robots.txt\\" except Exception as e: return f\\"Error: {str(e)}\\" # Fetch the URL content try: response = urllib.request.urlopen(url) html_content = response.read().decode(\'utf-8\') except HTTPError as e: return f\\"Error: HTTPError {e.code}\\" except URLError as e: return f\\"Error: {str(e)}\\" except Exception as e: return f\\"Error: {str(e)}\\" # Parse the HTML to extract the title try: parser = TitleParser() parser.feed(html_content) if parser.title: return parser.title else: return \\"Error: No title found\\" except Exception as e: return f\\"Error: {str(e)}\\""},{"question":"**Implementing a Custom Incremental XML Parser** # Objective: You are required to implement a custom SAX parser in Python using interfaces from the `xml.sax.xmlreader` package. This parser will process XML data incrementally to handle large XML files efficiently. You will also create a custom content handler to process specific parts of the XML. # Task: 1. **CustomIncrementalParser Class**: - Implement a class `CustomIncrementalParser` that inherits from `xml.sax.xmlreader.IncrementalParser`. - The class should be able to: - Initiate parsing using the `feed(data)` method. - Properly handle the end of parsing using the `close()` method. - Reset the parser state using the `reset()` method. 2. **CustomContentHandler Class**: - Implement a class `CustomContentHandler` that inherits from `xml.sax.handler.ContentHandler`. - This handler should: - Override methods `startElement(name, attrs)` and `endElement(name)` to handle the start and end of elements. - Maintain a count of how many times a specific element (e.g., `<item>`) appears in the XML. - Retrieve and print the value of a specific attribute (e.g., `id` from `<item>` elements). 3. **Integration**: - Integrate the `CustomIncrementalParser` and `CustomContentHandler` classes to parse XML data incrementally. - Demonstrate the functionality by parsing a sample XML input, incrementally feeding pieces of data to the parser. - Output the count of the specified elements and the collected attribute values. # Input: - Incremental chunks of an XML document as a series of strings. - The sample XML data contains multiple `<item>` elements and various attributes. # Expected Output: - The count of `<item>` elements processed. - The list of `id` attribute values from `<item>` elements. # Constraints: - The implementation must not load the entire XML document into memory at once; it should process the data incrementally. - The XML data chunks may be of varying sizes and must be correctly handled by the `feed(data)` method. # Example: Given XML chunks: Chunk 1: ```xml <root> <item id=\\"1\\"> ``` Chunk 2: ```xml <name>Item 1</name> </item> <item id=\\"2\\"> ``` Chunk 3: ```xml <name>Item 2</name> </item> </root> ``` Expected output: ``` Count of \'item\' elements: 2 Collected ids: [\'1\', \'2\'] ``` # Code Structure: ```python import xml.sax from xml.sax.xmlreader import IncrementalParser class CustomIncrementalParser(IncrementalParser): # Implement the required methods for incremental parsing (feed, close, reset) class CustomContentHandler(xml.sax.handler.ContentHandler): # Implement the required ContentHandler methods (startElement, endElement) # Integration and demonstration code ``` # Performance Requirements: - Efficient handling of large XML files by processing them incrementally. - Optimal memory usage by avoiding loading the whole XML document into memory at once. This question assesses the ability to: - Understand and implement SAX-based XML parsing. - Work with event-driven parsing models. - Manage resources efficiently in memory-constrained environments.","solution":"import xml.sax from xml.sax.xmlreader import IncrementalParser class CustomIncrementalParser(IncrementalParser): def __init__(self, content_handler): super().__init__() self._parser = xml.sax.make_parser() self._parser.setContentHandler(content_handler) self.reset() def feed(self, data): self._parser.feed(data) def close(self): self._parser.close() def reset(self): self._parser.reset() class CustomContentHandler(xml.sax.handler.ContentHandler): def __init__(self, element_name=\\"item\\", attribute_name=\\"id\\"): super().__init__() self.element_name = element_name self.attribute_name = attribute_name self.element_count = 0 self.attribute_values = [] def startElement(self, name, attrs): if name == self.element_name: self.element_count += 1 if self.attribute_name in attrs: self.attribute_values.append(attrs[self.attribute_name]) def endElement(self, name): pass def get_element_count(self): return self.element_count def get_attribute_values(self): return self.attribute_values # Integration and demonstration code content_handler = CustomContentHandler() parser = CustomIncrementalParser(content_handler) xml_chunks = [ \\"<root>\\", \\" <item id=\\"1\\">\\", \\" <name>Item 1</name>\\", \\" </item>\\", \\" <item id=\\"2\\">\\", \\" <name>Item 2</name>\\", \\" </item>\\", \\"</root>\\" ] for chunk in xml_chunks: parser.feed(chunk) parser.close() print(f\\"Count of \'item\' elements: {content_handler.get_element_count()}\\") print(f\\"Collected ids: {content_handler.get_attribute_values()}\\")"},{"question":"# Advanced Python Coding Assessment Question You are provided with the documentation of the `ChainMap` class from the `collections` module. Your task is to implement a function `combine_lookup(chains)`, which accepts a list of dictionaries and returns a `ChainMap` combined view for efficient lookup and update operations. # Function Signature ```python def combine_lookup(chains: list[dict]) -> collections.ChainMap: pass ``` # Input - `chains` - A list of dictionaries `[dict1, dict2, ...]`. (1 ≤ len(chains) ≤ 10^5) - Each dictionary can have up to 10^3 key-value pairs. # Output - Returns an instance of `collections.ChainMap` that combines the input dictionaries into a single updateable view. # Constraints - The combined `ChainMap` should allow lookups to traverse from the first dictionary provided in the input list to the last dictionary. - Any updates, writes, or deletions should only affect the first dictionary in the chain. # Example ```python from collections import ChainMap def combine_lookup(chains): # Implement the function # Example usage dict1 = {\'x\': 1, \'y\': 2} dict2 = {\'y\': 3, \'z\': 4} dict3 = {\'z\': 5, \'k\': 6} combined_chainmap = combine_lookup([dict1, dict2, dict3]) print(combined_chainmap[\'x\']) # Output: 1 print(combined_chainmap[\'y\']) # Output: 2 print(combined_chainmap[\'z\']) # Output: 4 print(combined_chainmap[\'k\']) # Output: 6 combined_chainmap[\'x\'] = 10 print(dict1[\'x\']) # Output: 10 print(combined_chainmap[\'x\']) # Output: 10 # Deleting a key from the combined_chainmap del combined_chainmap[\'x\'] print(\'x\' in dict1) # Output: False ``` # Explanation In the example provided: - The function `combine_lookup([dict1, dict2, dict3])` should create a `ChainMap` combining `dict1`, `dict2`, and `dict3`. - Lookups follow the chain from the first to the last dictionary. - Updates like `combined_chainmap[\'x\'] = 10` and deletions `del combined_chainmap[\'x\']` only affect the first dictionary in the chain (`dict1`). You are required to correctly implement the function while ensuring efficient operations with the provided constraints.","solution":"from collections import ChainMap def combine_lookup(chains: list[dict]) -> ChainMap: Combines a list of dictionaries into a ChainMap for efficient lookup and update operations. return ChainMap(*chains)"},{"question":"<|Analysis Begin|> The provided documentation covers the functionalities of the `uu` module, which includes encoding and decoding files using the uuencode format. This module allows binary data to be transferred over ASCII-only connections. The main functions provided by the module are: 1. `uu.encode(in_file, out_file, name=None, mode=None, *, backtick=False)`: Encodes a given input file to a uuencoded file with specified options. 2. `uu.decode(in_file, out_file=None, mode=None, quiet=False)`: Decodes a uuencoded input file to the specified output file with specified options. 3. `uu.Error`: An exception class that is raised in various situations during decoding, for example, if the input file is malformed or if the output file specified already exists. The documentation highlights several important aspects of using the `uu` module, such as the preference for using file-like objects directly rather than pathnames, and the additional features introduced in later versions, like the `backtick` parameter. Given this information, a suitable coding question can focus on the practical application and understanding of these encoding and decoding methods. <|Analysis End|> <|Question Begin|> # Question: Implement Custom UUencode/UUdecode Functionality You are to implement a custom class that provides methods to encode and decode files using the uuencode format leveraging Python\'s `uu` module. Your class should ensure that the following constraints and requirements are met: 1. Implement a class `FileEncoder` with the following methods: - `encode_file(self, input_path: str, output_path: str, name: str = None, mode: int = None, backtick: bool = False) -> None` - `decode_file(self, input_path: str, output_path: str = None, mode: int = None, quiet: bool = False) -> None` 2. `encode_file` method: - Parameters: - `input_path` (str): Path to the input file to be encoded. - `output_path` (str): Path to save the encoded output file. - `name` (str, optional): Default name in the encoded file, defaults to the input file name. - `mode` (int, optional): Default mode in the encoded file, defaults to 0o666. - `backtick` (bool, optional): If true, represents zeros by \'`\' instead of spaces. - Functionality: Reads the file at `input_path`, encodes it in uuencode format, and writes the output to `output_path`. 3. `decode_file` method: - Parameters: - `input_path` (str): Path to the encoded input file to be decoded. - `output_path` (str, optional): Path to save the decoded output file. Defaults to the name specified in the encoded file header. - `mode` (int, optional): File permission bits for the newly created file. - `quiet` (bool, optional): Suppresses warning messages if true. - Functionality: Reads the uuencoded file at `input_path`, decodes it, and writes the output to `output_path`. 4. Handle errors gracefully using the `uu.Error` exception where appropriate. # Example: Suppose you have a text file `example.txt` with the following content: ``` Hello, World! ``` **Encoding:** ```python encoder = FileEncoder() encoder.encode_file(\'example.txt\', \'encoded.txt\') ``` This should create an `encoded.txt` file in uuencode format. **Decoding:** ```python encoder.decode_file(\'encoded.txt\', \'decoded_example.txt\') ``` This should create a `decoded_example.txt` file with the original content. # Constraints: - Assume the input files are not larger than 10 MB. - The provided paths are valid and accessible. Implement the `FileEncoder` class ensuring it meets the above specifications.","solution":"import uu import os class FileEncoder: def encode_file(self, input_path: str, output_path: str, name: str = None, mode: int = None, backtick: bool = False) -> None: Encodes a given input file to a uuencoded file. Parameters: input_path (str): Path to the input file to be encoded. output_path (str): Path to save the encoded output file. name (str, optional): Default name in the encoded file, defaults to the input file name. mode (int, optional): Default mode in the encoded file, defaults to 0o666. backtick (bool, optional): If true, represents zeros by \'`\' instead of spaces. if name is None: name = os.path.basename(input_path) if mode is None: mode = 0o666 with open(input_path, \'rb\') as in_file, open(output_path, \'wb\') as out_file: uu.encode(in_file, out_file, name, mode, backtick=backtick) def decode_file(self, input_path: str, output_path: str = None, mode: int = None, quiet: bool = False) -> None: Decodes a uuencoded input file to the specified output file. Parameters: input_path (str): Path to the encoded input file to be decoded. output_path (str, optional): Path to save the decoded output file. Defaults to the name specified in the encoded file header. mode (int, optional): File permission bits for the newly created file. quiet (bool, optional): Suppresses warning messages if true. with open(input_path, \'rb\') as in_file: try: uu.decode(in_file, output_path, mode, quiet) except uu.Error as e: print(f\\"Decoding error: {e}\\")"},{"question":"You are tasked with implementing a function that serializes and deserializes a complex nested data structure using the `marshal` module. Your implementation should include robust handling of the supported data types and proper error checking for unsupported types. # **Function Specification** **serialize_data(data: Any, version: int = marshal.version) -> bytes:** - **Input:** - `data`: A Python object containing a complex nested data structure (could be of type list, tuple, set, frozenset, dict, etc. with supported types). - `version`: An optional integer specifying the format version to use (default is `marshal.version`). - **Output:** - Returns a bytes object that represents the serialized form of the `data`. **deserialize_data(data_bytes: bytes) -> Any:** - **Input:** - `data_bytes`: A bytes object representing the serialized form of the data. - **Output:** - Returns the original Python object that was serialized. # **Constraints** - The functions should handle all supported types as listed in the provided documentation. - If `serialize_data` encounters an unsupported type, it should raise a `ValueError` with an appropriate error message. - The functions should handle nested structures, including recursive lists, sets, and dictionaries, provided the nested elements are supported types. - You should not assume that the data being deserialized is safe. Proper error handling should be included. # **Performance** - The functions should operate efficiently even with large nested structures. # **Example** ```python import marshal # Example nested structure data = { \'name\': \'Alice\', \'age\': 30, \'numbers\': [1, 2, 3, {\'pi\': 3.14}], \'values\': (None, True, (1+2j)) } # Serializing the data serialized_data = serialize_data(data) # Deserializing the data deserialized_data = deserialize_data(serialized_data) assert deserialized_data == data # should be True ``` # **Bonus** 1. Implement additional functionality to store the serialized data in a binary file and read it back using `marshal.dump` and `marshal.load`. ```python def save_data_to_file(data: Any, file_path: str, version: int = marshal.version) -> None: ... def load_data_from_file(file_path: str) -> Any: ... ``` Ensure that your solution meets all the given specifications and handles edge cases appropriately.","solution":"import marshal from typing import Any def serialize_data(data: Any, version: int = marshal.version) -> bytes: Serializes the given data into a bytes object using the marshal module. Parameters: data (Any): The data to serialize. Should be a supported data type. version (int): The optional format version to use (default is marshal.version). Returns: bytes: A bytes object representing the serialized form of the data. Raises: ValueError: If an unsupported data type is encountered. try: return marshal.dumps(data, version) except ValueError as e: raise ValueError(f\\"Cannot serialize data of type {type(data)}: {e}\\") def deserialize_data(data_bytes: bytes) -> Any: Deserializes the given bytes object into the original data using the marshal module. Parameters: data_bytes (bytes): The bytes object representing the serialized form of the data. Returns: Any: The original data that was serialized. Raises: ValueError: If the data_bytes cannot be deserialized. try: return marshal.loads(data_bytes) except (ValueError, EOFError, TypeError) as e: raise ValueError(f\\"Cannot deserialize data: {e}\\") # Bonus: Save and load data using files def save_data_to_file(data: Any, file_path: str, version: int = marshal.version) -> None: Serialize the data and save it to a file. Parameters: data (Any): The data to serialize and save. file_path (str): The path to the file to save the data to. version (int): The optional format version to use (default is marshal.version). Raises: ValueError: If an error occurs during serialization. try: with open(file_path, \'wb\') as file: marshal.dump(data, file, version) except ValueError as e: raise ValueError(f\\"Cannot serialize data of type {type(data)} to file: {e}\\") def load_data_from_file(file_path: str) -> Any: Load data from a file and deserialize it. Parameters: file_path (str): The path to the file to load the data from. Returns: Any: The deserialized data. Raises: ValueError: If an error occurs during deserialization. try: with open(file_path, \'rb\') as file: return marshal.load(file) except (ValueError, EOFError, TypeError) as e: raise ValueError(f\\"Cannot deserialize data from file: {e}\\")"},{"question":"**Objective**: Implement a Python function to safely handle an asynchronous event in a multithreaded environment using the `signal` module. Task You are required to write a program that: 1. Sets up a signal handler for `SIGALRM` that performs a specific task. 2. Creates and starts a separate thread to perform a long-running computation. 3. Uses an alarm to limit the computation time to a specified duration. If the computation exceeds this duration, an alarm signal should be raised to interrupt the computation. 4. Ensures that the program exits gracefully when an alarm signal interrupts the long-running computation. Requirements 1. Implement the signal handler function that: - Accepts two parameters: `signum` and `frame`. - Prints a message indicating which signal was received. - Raises a `TimeoutError` exception when it handles the `SIGALRM` signal. 2. Implement a function `long_computation()` that: - Performs a long-running computation (simulate by sleeping for a long duration). - Continuously prints progress messages at regular intervals. 3. Implement a function `run_with_timeout(duration)` that: - Sets the signal handler for `SIGALRM` to the custom handler. - Sets an alarm for the specified `duration` seconds. - Starts the `long_computation` in a separate thread. - Catches the `TimeoutError` and prints a message indicating the computation was interrupted. - Ensures that the alarm is disabled if the computation finishes before the timeout. Input and Output - Input: Single integer `duration` representing the maximum allowed duration for the computation in seconds. - Output: Print messages indicating the progress of the computation and whether it was interrupted by a timeout. Constraints - Only the main thread should handle the alarm and signal. - The `long_computation` should be interruptible by the alarm signal. - Use the `threading` module to create the thread for the long-running computation. Example ```python import signal import threading import time def signal_handler(signum, frame): print(f\\"Signal handler called with signal {signum}\\") raise TimeoutError(\\"Computation took too long!\\") def long_computation(): for i in range(1, 100): print(f\\"Computation step {i}\\") time.sleep(1) # Simulate long computation step def run_with_timeout(duration): # Set the signal handler for SIGALRM signal.signal(signal.SIGALRM, signal_handler) # Set an alarm for the specified duration signal.alarm(duration) computation_thread = threading.Thread(target=long_computation) computation_thread.start() try: computation_thread.join() except TimeoutError as te: print(te) finally: # Disable the alarm signal.alarm(0) # Example usage run_with_timeout(5) ``` Note: The `long_computation` in this example runs indefinitely but will be interrupted after 5 seconds by the `SIGALRM` signal.","solution":"import signal import threading import time def signal_handler(signum, frame): print(f\\"Signal handler called with signal {signum}\\") raise TimeoutError(\\"Computation took too long!\\") def long_computation(): try: for i in range(1, 100): print(f\\"Computation step {i}\\") time.sleep(1) # Simulate a long computation step except TimeoutError: print(\\"Computation was interrupted by a timeout.\\") def run_with_timeout(duration): # Set the signal handler for SIGALRM signal.signal(signal.SIGALRM, signal_handler) # Set an alarm for the specified duration signal.alarm(duration) computation_thread = threading.Thread(target=long_computation) computation_thread.start() try: computation_thread.join() except TimeoutError as te: print(te) finally: # Disable the alarm signal.alarm(0) # Example usage # Uncomment this to run # run_with_timeout(5)"},{"question":"**Question: Text Classification Using TfidfVectorizer and Naive Bayes** In this task, you will implement a text classification pipeline using Scikit-Learn\'s `TfidfVectorizer` and Naive Bayes classifier. Your goal is to classify whether a given text message is spam or not spam. # Input You will be given a list of text messages and their corresponding labels in the form of a tuple: ```python messages = [ (\\"Free entry in 2 a weekly competition to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&Cs apply 08452810075over18\'s\\", 1), (...), ... ] ``` Each tuple contains: - A string representing the message. - An integer label where `1` indicates spam and `0` indicates not spam. # Output Your function should return the accuracy of the classifier on the given dataset. # Constraints - Use `TfidfVectorizer` to vectorize the text messages. - Use Naive Bayes classifier for classification. - You are expected to use an 80-20 train-test split for evaluating the classifier. - Ensure reproducibility by setting a `random_state` of 42 during data splitting. # Implementation 1. **Vectorize the messages**: Use `TfidfVectorizer` to convert the text messages into TF-IDF features. 2. **Train-Test Split**: Split the data into training and testing sets using an 80-20 split. 3. **Train the Classifier**: Train a Naive Bayes classifier on the training set. 4. **Evaluate the Classifier**: Calculate and return the accuracy on the test set. # Function Signature ```python from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.model_selection import train_test_split from sklearn.naive_bayes import MultinomialNB from sklearn.metrics import accuracy_score def classify_messages(messages): # Step 1: Separate messages and labels texts, labels = zip(*messages) # Step 2: Vectorize the messages using TfidfVectorizer vectorizer = TfidfVectorizer() X = vectorizer.fit_transform(texts) # Step 3: Train-Test Split X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, random_state=42) # Step 4: Train the Naive Bayes Classifier classifier = MultinomialNB() classifier.fit(X_train, y_train) # Step 5: Predict and Evaluate y_pred = classifier.predict(X_test) accuracy = accuracy_score(y_test, y_pred) return accuracy ``` # Example Usage ```python messages = [ (\\"Free entry in 2 a weekly competition to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&Cs apply 08452810075over18\'s\\", 1), (\\"Nah I don\'t think he goes to usf, he lives around here though\\", 0), (\\"WINNER!! As a valued network customer you have been selected to receivea £900 prize reward! To claim call 09061701461. Claim code KL341. Valid 12 hours only.\\", 1), (\\"Hey where are you right now?\\", 0), ] print(classify_messages(messages)) # Output: Accuracy of the classifier on the given dataset ``` # Notes - The example usage does not give exact output as it depends on the dataset used.","solution":"from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.model_selection import train_test_split from sklearn.naive_bayes import MultinomialNB from sklearn.metrics import accuracy_score def classify_messages(messages): Classifies messages as spam or not spam using TfidfVectorizer and Naive Bayes classifier. Parameters: messages (list of tuples): Each tuple contains a message string and an integer label (1 for spam, 0 for not spam) Returns: float: Accuracy of the classifier on the test set # Step 1: Separate messages and labels texts, labels = zip(*messages) # Step 2: Vectorize the messages using TfidfVectorizer vectorizer = TfidfVectorizer() X = vectorizer.fit_transform(texts) # Step 3: Train-Test Split X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, random_state=42) # Step 4: Train the Naive Bayes Classifier classifier = MultinomialNB() classifier.fit(X_train, y_train) # Step 5: Predict and Evaluate y_pred = classifier.predict(X_test) accuracy = accuracy_score(y_test, y_pred) return accuracy"},{"question":"You are tasked with analyzing the efficiency of different car models based on various features. You will be using the `seaborn` library to create and interpret residual plots. Here\'s what you need to do: 1. **Load the `mpg` dataset**: - Use the `seaborn` library to load the dataset named `mpg`. 2. **Basic Residual Plot**: - Create a basic residual plot of `displacement` against `weight`. 3. **Regression Assumption Violations**: - Create a residual plot of `mpg` against `weight` to check for any violations in the linear regression assumptions. 4. **Removing Higher-Order Trends**: - Create an order-2 residual plot of `mpg` against `horsepower` to remove higher-order trends and stabilize the residuals. 5. **Adding a LOWESS Curve**: - Redo the residual plot of `mpg` against `horsepower` and add a LOWESS curve to emphasize any structure. # Specifications: - **Input**: None. The dataset should be loaded within the program. - **Output**: The program should display the four described plots. - **Constraints**: You must use the `seaborn` library for loading the dataset and creating the plots. # Performance requirements Your code should efficiently handle the dataset and produce the plots within a reasonable time (under 5 seconds for generating the plots). # Example Here is an example of how your code should start: ```python import seaborn as sns import matplotlib.pyplot as plt # Load the dataset sns.set_theme() mpg = sns.load_dataset(\\"mpg\\") # 1. Basic Residual Plot sns.residplot(data=mpg, x=\\"weight\\", y=\\"displacement\\") plt.show() # 2. Regression Assumption Violations sns.residplot(data=mpg, x=\\"weight\\", y=\\"mpg\\") plt.show() # 3. Removing Higher-Order Trends sns.residplot(data=mpg, x=\\"horsepower\\", y=\\"mpg\\", order=2) plt.show() # 4. Adding a LOWESS Curve sns.residplot(data=mpg, x=\\"horsepower\\", y=\\"mpg\\", lowess=True, line_kws=dict(color=\\"r\\")) plt.show() ``` Make sure your plots are correctly labeled with appropriate titles and axis labels. # Submission Submit your code as a Python script file.","solution":"import seaborn as sns import matplotlib.pyplot as plt # Load the dataset sns.set_theme() mpg = sns.load_dataset(\\"mpg\\") # 1. Basic Residual Plot plt.figure(figsize=(10, 6)) sns.residplot(data=mpg, x=\\"weight\\", y=\\"displacement\\") plt.title(\'Residual Plot: Displacement vs Weight\') plt.xlabel(\'Weight\') plt.ylabel(\'Residuals (Displacement)\') plt.show() # 2. Regression Assumption Violations plt.figure(figsize=(10, 6)) sns.residplot(data=mpg, x=\\"weight\\", y=\\"mpg\\") plt.title(\'Residual Plot: MPG vs Weight\') plt.xlabel(\'Weight\') plt.ylabel(\'Residuals (MPG)\') plt.show() # 3. Removing Higher-Order Trends plt.figure(figsize=(10, 6)) sns.residplot(data=mpg, x=\\"horsepower\\", y=\\"mpg\\", order=2) plt.title(\'Order-2 Residual Plot: MPG vs Horsepower\') plt.xlabel(\'Horsepower\') plt.ylabel(\'Residuals (MPG)\') plt.show() # 4. Adding a LOWESS Curve plt.figure(figsize=(10, 6)) sns.residplot(data=mpg, x=\\"horsepower\\", y=\\"mpg\\", lowess=True, line_kws=dict(color=\\"r\\")) plt.title(\'Residual Plot with LOWESS: MPG vs Horsepower\') plt.xlabel(\'Horsepower\') plt.ylabel(\'Residuals (MPG)\') plt.show()"},{"question":"# PyTorch JIT Compiler Question **Objective:** Demonstrate your understanding of PyTorch\'s JIT compiler by creating a neural network, scripting it with TorchScript, and comparing its performance with a non-scripted version. **Problem Statement:** 1. Create a simple feedforward neural network using PyTorch for a classification problem. The network should include: - At least 3 fully connected layers. - ReLU activation functions after each fully connected layer. - An output layer suitable for a multi-class classification problem (use Softmax activation or appropriate PyTorch function). 2. Implement a function to script the neural network using PyTorch\'s JIT compiler. 3. Create random input data to test the performance of the scripted and non-scripted network. The input data should be a batch of samples where each sample has a fixed size (e.g., 100-dimensional). 4. Measure and compare the inference time for a forward pass in both the scripted and non-scripted networks. Use enough samples to obtain reliable results. **Constraints:** - The input data for inference should be a batch of 1000 samples. - Ensure that the comparison of inference time is performed on the same hardware to provide a fair comparison. **Specifications:** - **Input:** - A PyTorch model for a multi-class classification problem. - Random input data for testing (a tensor with shape (1000, 100)). - **Output:** - Time taken for forward pass inference using the non-scripted model. - Time taken for forward pass inference using the scripted model. - Ratio of inference times for both models. **Function Signature:** - `create_and_script_model() -> Tuple[torch.nn.Module, torch.jit.ScriptModule]` - `benchmark_models(model: torch.nn.Module, scripted_model: torch.jit.ScriptModule, input_data: torch.Tensor) -> Tuple[float, float, float]` **Example:** Here is a brief illustration of what the functions should look like (details are to be implemented): ```python import torch import torch.nn as nn import torch.jit # Function to create and script the model def create_and_script_model() -> Tuple[torch.nn.Module, torch.jit.ScriptModule]: class SimpleNN(nn.Module): def __init__(self): super(SimpleNN, self).__init__() self.fc1 = nn.Linear(100, 64) self.fc2 = nn.Linear(64, 32) self.fc3 = nn.Linear(32, 10) def forward(self, x): x = torch.relu(self.fc1(x)) x = torch.relu(self.fc2(x)) x = torch.softmax(self.fc3(x), dim=1) return x model = SimpleNN() scripted_model = torch.jit.script(model) return model, scripted_model # Function to benchmark the models def benchmark_models(model: torch.nn.Module, scripted_model: torch.jit.ScriptModule, input_data: torch.Tensor) -> Tuple[float, float, float]: import time # Measure time for non-scripted model start_time = time.time() with torch.no_grad(): model(input_data) non_scripted_time = time.time() - start_time # Measure time for scripted model start_time = time.time() with torch.no_grad(): scripted_model(input_data) scripted_time = time.time() - start_time # Calculate time ratio time_ratio = non_scripted_time / scripted_time return non_scripted_time, scripted_time, time_ratio ``` **Note:** Ensure appropriate libraries and modules are properly imported in your implementation.","solution":"import torch import torch.nn as nn import torch.jit from typing import Tuple # Function to create and script the model def create_and_script_model() -> Tuple[torch.nn.Module, torch.jit.ScriptModule]: class SimpleNN(nn.Module): def __init__(self): super(SimpleNN, self).__init__() self.fc1 = nn.Linear(100, 64) self.fc2 = nn.Linear(64, 32) self.fc3 = nn.Linear(32, 10) def forward(self, x): x = torch.relu(self.fc1(x)) x = torch.relu(self.fc2(x)) x = torch.softmax(self.fc3(x), dim=1) return x model = SimpleNN() scripted_model = torch.jit.script(model) return model, scripted_model # Function to benchmark the models def benchmark_models(model: torch.nn.Module, scripted_model: torch.jit.ScriptModule, input_data: torch.Tensor) -> Tuple[float, float, float]: import time # Measure time for non-scripted model start_time = time.time() with torch.no_grad(): model(input_data) non_scripted_time = time.time() - start_time # Measure time for scripted model start_time = time.time() with torch.no_grad(): scripted_model(input_data) scripted_time = time.time() - start_time # Calculate time ratio time_ratio = non_scripted_time / scripted_time return non_scripted_time, scripted_time, time_ratio"},{"question":"**Audio Processing with the `aifc` Module** **Objective:** You are tasked with implementing a Python function that uses the `aifc` module to convert a stereo AIFF-C audio file to mono. This involves reading the audio data, combining the stereo channels into a single mono channel, and writing the result to a new AIFF-C file. **Function Signature:** ```python def convert_stereo_to_mono(input_file: str, output_file: str) -> None: pass ``` **Parameters:** - `input_file` (str): The path to the input AIFF-C file which is in stereo format. - `output_file` (str): The path where the output mono AIFF-C file will be saved. **Requirements:** 1. Open the input AIFF-C file for reading and verify it is in stereo format. 2. Read the audio frames from the input file. 3. Convert the stereo frames to mono by averaging the values of the two channels for each frame. 4. Create and write the resulting mono audio data to the output file, ensuring to set the appropriate audio parameters (e.g., number of channels, sample width, frame rate). **Constraints:** - The function should handle files that are not stereo by raising a `ValueError` with the message \\"Input file is not in stereo format\\". - Students should use the methods provided by the `aifc` module for reading and writing audio files as described. - The function should handle potential I/O errors gracefully. **Example:** ```python # Assuming \'stereo.aifc\' is a valid stereo AIFF-C file and \'mono.aifc\' is the desired output file. convert_stereo_to_mono(\'stereo.aifc\', \'mono.aifc\') ``` In this example, the function will read \'stereo.aifc\', process the audio data to convert it to mono, and save the result as \'mono.aifc\'. **Additional Information:** Ensure the output file maintains the same sample width and frame rate as the input file. This exercise will test your ability to manipulate audio data and your comprehension of the `aifc` module\'s API.","solution":"import aifc def convert_stereo_to_mono(input_file: str, output_file: str) -> None: try: with aifc.open(input_file, \'r\') as input_f: # Verify that the input file is stereo if input_f.getnchannels() != 2: raise ValueError(\\"Input file is not in stereo format\\") # Retrieve audio parameters nframes = input_f.getnframes() sampwidth = input_f.getsampwidth() framerate = input_f.getframerate() frames = input_f.readframes(nframes) # Process frames to convert to mono mono_frames = bytearray() for i in range(0, len(frames), 2 * sampwidth): left_sample = int.from_bytes(frames[i:i + sampwidth], byteorder=\'big\', signed=True) right_sample = int.from_bytes(frames[i + sampwidth:i + 2 * sampwidth], byteorder=\'big\', signed=True) mono_sample = (left_sample + right_sample) // 2 mono_sample_bytes = mono_sample.to_bytes(sampwidth, byteorder=\'big\', signed=True) mono_frames.extend(mono_sample_bytes) # Write mono frames to output file with aifc.open(output_file, \'w\') as output_f: output_f.setnchannels(1) output_f.setsampwidth(sampwidth) output_f.setframerate(framerate) output_f.writeframes(mono_frames) except IOError as e: raise IOError(f\\"IOError when handling the files: {e}\\")"},{"question":"Processing Text Files with Shell Pipelines **Objective:** Implement a function that reads a text file, processes it through a specified pipeline of shell commands using Python\'s `pipes.Template`, and writes the processed content to an output file. **Function Signature:** ```python def process_file_pipeline(input_file: str, output_file: str, commands: list) -> None: Processes the input file using the given shell commands and writes the result to the output file. Parameters: - input_file (str): Path to the input text file. - output_file (str): Path to the output text file. - commands (list): A list of tuples where each tuple contains a command (str) and its kind (str). Returns: - None: Writes processed content to the output file. ``` **Input:** - `input_file`: A string representing the path to the input text file. - `output_file`: A string representing the path to the output text file. - `commands`: A list of tuples, where each tuple has: - `cmd`: A valid Bourne shell command as a string. - `kind`: A string of two letters representing the command\'s input/output behavior. **Output:** - None: The function should write the processed content to the output file. **Constraints:** - Only valid Bourne shell commands should be used. - The sequence of commands must properly form a pipeline. **Example:** ```python commands = [ (\'tr a-z A-Z\', \'--\'), # Transforms all lowercase letters to uppercase (\'sort\', \'--\'), # Sorts the lines alphabetically (\'uniq\', \'--\') # Removes duplicate lines ] process_file_pipeline(\'input.txt\', \'output.txt\', commands) ``` **Explanation:** - The function `process_file_pipeline` creates a `pipes.Template` object. - It appends the given commands to the template. - It opens the input file, processes it through the defined pipeline, and writes the result to the output file. Make sure to handle errors appropriately to ensure robustness.","solution":"import pipes def process_file_pipeline(input_file: str, output_file: str, commands: list) -> None: Processes the input file using the given shell commands and writes the result to the output file. Parameters: - input_file (str): Path to the input text file. - output_file (str): Path to the output text file. - commands (list): A list of tuples where each tuple contains a command (str) and its kind (str). Returns: - None: Writes processed content to the output file. # Creating the pipes template t = pipes.Template() # Adding each command to the template for cmd, kind in commands: t.append(cmd, kind) # Applying the pipeline, reading from input_file and writing to output_file t.copy(input_file, output_file)"},{"question":"You are required to demonstrate your understanding of ensemble learning by implementing an advanced ensemble model using Scikit-learn. The goal is to combine different base estimators to improve the predictive performance over individual models. Task Write a Python function using Scikit-learn that performs the following steps: 1. Load a dataset of your choice from Scikit-learn\'s provided datasets (it must be a classification dataset). 2. Divide the dataset into training and testing sets. 3. Implement an ensemble model using at least two different base classifiers. 4. Use a method of combining estimators (Bagging, Boosting, or Stacking) from Scikit-learn. 5. Train the ensemble model on the training data. 6. Evaluate the ensemble model on the test data and print out the classification report, including precision, recall, and F1-score for each class. # Function Signature ```python def ensemble_model_evaluation(): pass ``` # Constraints - Use Scikit-learn for dataset loading, model implementation, training, and evaluation. - At least two different types of base classifiers must be combined. - Preferred methods for combing estimators: BaggingClassifier, AdaBoostClassifier, or StackingClassifier. # Dataset You can choose any classification dataset from Scikit-learn, such as: - `sklearn.datasets.load_iris` - `sklearn.datasets.load_wine` - `sklearn.datasets.load_breast_cancer` # Expected Output The function should print out: - The classification report with precision, recall, and F1-score for each class. # Example ```python def ensemble_model_evaluation(): from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.ensemble import StackingClassifier from sklearn.linear_model import LogisticRegression from sklearn.tree import DecisionTreeClassifier from sklearn.svm import SVC from sklearn.metrics import classification_report # Load dataset data = load_iris() X, y = data.data, data.target # Split dataset X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Define base classifiers estimators = [ (\'dt\', DecisionTreeClassifier()), (\'svc\', SVC(probability=True)) ] # Define ensemble model - StackingClassifier as an example ensemble = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression()) # Train ensemble model ensemble.fit(X_train, y_train) # Make predictions y_pred = ensemble.predict(X_test) # Evaluate model report = classification_report(y_test, y_pred) print(report) ```","solution":"def ensemble_model_evaluation(): from sklearn.datasets import load_breast_cancer from sklearn.model_selection import train_test_split from sklearn.ensemble import StackingClassifier from sklearn.linear_model import LogisticRegression from sklearn.tree import DecisionTreeClassifier from sklearn.svm import SVC from sklearn.metrics import classification_report # Load dataset data = load_breast_cancer() X, y = data.data, data.target # Split dataset X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Define base classifiers estimators = [ (\'dt\', DecisionTreeClassifier()), (\'svc\', SVC(probability=True)) ] # Define ensemble model - StackingClassifier as an example ensemble = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression()) # Train ensemble model ensemble.fit(X_train, y_train) # Make predictions y_pred = ensemble.predict(X_test) # Evaluate model report = classification_report(y_test, y_pred, target_names=data.target_names) print(report)"},{"question":"Objective: Write a function that demonstrates your understanding of the `multiprocessing.shared_memory` module, specifically focusing on the `SharedMemory` and `ShareableList` classes. Problem Statement You are given two lists of integers `list1` and `list2` which are of the same length. You need to create a shared memory block to store the sum of corresponding elements from these two lists. Create a separate process to perform the addition and write the results to the shared memory. The main process should then read from the shared memory to verify and print the results. Requirements 1. Define a function `shared_memory_sum(list1: List[int], list2: List[int]) -> List[int]`. 2. Inside this function: - Create a shared memory block to store the result. - Create another process that accesses this shared memory block, computes the sum of corresponding elements from `list1` and `list2`, and stores the result in the shared memory. - In the main process, read the results from the shared memory and return them as a list. - Ensure proper cleanup of the shared memory block. Constraints - Both `list1` and `list2` will have a length of at most 1000. - All integers in the lists will be between `-10^6` and `10^6`. Input - Two lists of integers of the same length: `list1` and `list2`. Output - A list of integers representing the sum of corresponding elements from `list1` and `list2`. Example ```python def shared_memory_sum(list1, list2): # Implementation here list1 = [1, 2, 3] list2 = [4, 5, 6] result = shared_memory_sum(list1, list2) print(result) # Output: [5, 7, 9] ``` The provided function should demonstrate the use of inter-process shared memory, managing the shared memory lifecycle, and handling synchronization between processes.","solution":"from multiprocessing import Process from multiprocessing.shared_memory import SharedMemory import numpy as np def shared_memory_sum(list1, list2): length = len(list1) # Create a shared memory array of type int32 and the same length as input lists shm = SharedMemory(create=True, size=np.int32().nbytes * length) result_array = np.ndarray((length,), dtype=np.int32, buffer=shm.buf) def worker(l1, l2, shm_name, size): # Attach to shared memory existing_shm = SharedMemory(name=shm_name) result = np.ndarray((size,), dtype=np.int32, buffer=existing_shm.buf) # Compute the sum of corresponding elements for i in range(size): result[i] = l1[i] + l2[i] # Cleanup existing_shm.close() # Create a new process to perform the sum operation p = Process(target=worker, args=(list1, list2, shm.name, length)) p.start() p.join() # Read the result from shared memory result = result_array.tolist() # Cleanup shared memory shm.close() shm.unlink() return result"},{"question":"**Task: Implement a Custom Exception Handling Mechanism** **Objective:** You are required to implement a Python function that interacts with the provided C-API exception handling functions. The function should demonstrate an understanding of setting, clearing, and querying error indicators. Also, it should handle a specific scenario where multiple exceptions might be raised and ensure the proper cleanup of resources. **Function Signature:** ```python def custom_exception_handler(n: int) -> str: ``` **Function Description:** 1. **Input:** - An integer `n` which determines the flow of the function. 2. **Behavior:** - If `n < 0`: Raise a ValueError with a custom message. - If `0 <= n <= 10`: Raise a MemoryError. - If `10 < n <= 20`: Simulate a system call failure and set an OSError appropriately. - If `n > 20`: Clear any existing error indicator and return \\"No Error\\". - For values other than those described, print the current traceback and return \\"Error Handled\\". 3. **Output:** - A string indicating the result of the function based on the input value and the handled exception. 4. **Implementation Constraints:** - Use the provided PyErr functions to set, clear, and query the error indicator. - Use exception handling to catch the raised exceptions and perform proper cleanup. 5. **Performance Requirements:** - Ensure that the function handles exceptions efficiently and avoids any memory leaks or resource mismanagement. ```python def custom_exception_handler(n: int) -> str: try: if n < 0: raise ValueError(\\"Negative value error.\\") elif 0 <= n <= 10: raise MemoryError(\\"Memory allocation failed.\\") elif 10 < n <= 20: import errno import ctypes ctypes.set_errno(errno.EIO) raise OSError(ctypes.get_errno()) elif n > 20: import sys sys.exc_clear() return \\"No Error\\" else: import traceback traceback.print_exc() return \\"Error Handled\\" except Exception as e: import traceback traceback.print_exc() return \\"Exception Handled with message: \\" + str(e) ``` **Examples:** 1. `custom_exception_handler(-5)`: - Output: \\"Exception Handled with message: Negative value error.\\" 2. `custom_exception_handler(5)`: - Output: \\"Exception Handled with message: Memory allocation failed.\\" 3. `custom_exception_handler(15)`: - Output: \\"Exception Handled with message: I/O error [Errno 5] IO Error.\\" 4. `custom_exception_handler(25)`: - Output: \\"No Error\\" 5. `custom_exception_handler(30)`: - Output: \\"Error Handled\\" This exercise will test the students\' understanding of exception handling mechanisms and their ability to properly manage and translate errors using the provided C-API python310 functions.","solution":"def custom_exception_handler(n: int) -> str: import sys import ctypes import errno try: if n < 0: raise ValueError(\\"Negative value error.\\") elif 0 <= n <= 10: raise MemoryError(\\"Memory allocation failed.\\") elif 10 < n <= 20: ctypes.set_errno(errno.EIO) raise OSError(ctypes.get_errno(), \\"Simulated system call failure.\\") elif n > 20: # Python 3.9+ has no `sys.exc_clear`, handle with sys.exc_info() if sys.exc_info()[0] is not None: e_type = sys.exc_info()[0] sys.exc_info() # Clear the exception return f\\"Cleared error of type {e_type.__name__}\\" return \\"No Error\\" else: import traceback traceback.print_exc() return \\"Error Handled\\" except Exception as e: import traceback traceback.print_exc() return f\\"Exception Handled with message: {str(e)}\\""},{"question":"# PyTorch FX Transformation Coding Assessment You are provided with a simple neural network module. Your task is to write a function that transforms this module\'s computation graph to replace all instances of `torch.nn.ReLU` with `torch.nn.Sigmoid` using the `torch.fx` module. Instructions: 1. **Define the Transformation Function**: Write a function `transform_relu_to_sigmoid` that: - Takes a `torch.nn.Module` as input. - Traces the module to get its computation graph. - Replaces all instances of `torch.nn.ReLU` with `torch.nn.Sigmoid`. - Returns the modified module. 2. **Test the Transformation**: Write a test case to verify the transformation works correctly. Ensure that the structure of the original and transformed modules is the same, except for the activation functions. Provided Neural Network Module: ```python import torch import torch.nn as nn import torch.fx class SimpleNN(nn.Module): def __init__(self): super(SimpleNN, self).__init__() self.fc1 = nn.Linear(10, 20) self.relu = nn.ReLU() self.fc2 = nn.Linear(20, 5) self.output = nn.Softmax(dim=1) def forward(self, x): x = self.fc1(x) x = self.relu(x) x = self.fc2(x) x = self.output(x) return x ``` Your Task: 1. **Implement the Function**: Write the `transform_relu_to_sigmoid` function. ```python def transform_relu_to_sigmoid(module: nn.Module) -> nn.Module: import torch.fx as fx class SigmoidTracer(fx.Tracer): def is_leaf_module(self, m, qualname): # Treat all submodules as leaf modules return super().is_leaf_module(m, qualname) graph = SigmoidTracer().trace(module) for node in graph.nodes: if node.op == \'call_module\' and node.target == \'relu\': with graph.inserting_after(node): new_node = graph.call_module(\'sigmoid\', args=node.args) node.replace_all_uses_with(new_node) graph.erase_node(node) new_module = fx.GraphModule(module, graph) return new_module ``` 2. **Write a Test Case**: Write a test case to verify the functionality. ```python import torch import torch.nn as nn def test_transform(): # Create an instance of the original module original_module = SimpleNN() # Apply the transformation transformed_module = transform_relu_to_sigmoid(original_module) # Check if the transformation was successful by comparing the two modules. assert isinstance(transformed_module.relu, nn.Sigmoid), \\"ReLU was not replaced with Sigmoid.\\" input_tensor = torch.rand(1, 10) original_output = original_module(input_tensor) transformed_output = transformed_module(input_tensor) print(\\"Original Output: \\", original_output) print(\\"Transformed Output: \\", transformed_output) assert not torch.equal(original_output, transformed_output), \\"Outputs should not be equal when using different activation functions.\\" print(\\"Test passed!\\") # Run the test test_transform() ``` **Input:** - A `torch.nn.Module` instance (e.g. `SimpleNN`). **Output:** - A transformed `torch.nn.Module` where all `torch.nn.ReLU` layers have been replaced by `torch.nn.Sigmoid`. **Performance Requirements:** - The transformation should work efficiently for any reasonable-sized module. - There should be no significant increase in inference time for the transformed module compared to the original module. Constraints: - Ensure the transformed module retains the structure of the original module other than the replaced activation functions. - Handle any potential issues with the graph correctly, ensuring a valid computational graph post-transformation. Good luck!","solution":"import torch import torch.nn as nn import torch.fx as fx class SimpleNN(nn.Module): def __init__(self): super(SimpleNN, self).__init__() self.fc1 = nn.Linear(10, 20) self.relu = nn.ReLU() self.fc2 = nn.Linear(20, 5) self.output = nn.Softmax(dim=1) def forward(self, x): x = self.fc1(x) x = self.relu(x) x = self.fc2(x) x = self.output(x) return x def transform_relu_to_sigmoid(module: nn.Module) -> nn.Module: Transforms the given module by replacing all instances of ReLU with Sigmoid. class ReLUToSigmoidTracer(fx.Tracer): def is_leaf_module(self, m, qualname): # Treat all submodules as leaf modules return super().is_leaf_module(m, qualname) graph = ReLUToSigmoidTracer().trace(module) graph_module = fx.GraphModule(module, graph) for node in graph.nodes: if node.op == \'call_module\' and hasattr(graph_module, node.target): submodule = getattr(graph_module, node.target) if isinstance(submodule, nn.ReLU): # Replace ReLU with Sigmoid setattr(graph_module, node.target, nn.Sigmoid()) return graph_module"},{"question":"# Question: Implementing and Managing Cell Objects **Objective:** You are tasked to create a Python class `Cell` that mimics the behavior of the C structure used for cell objects as described in the provided documentation. Your implementation should cover creating a new cell, getting and setting the cell value, and checking if an object is a cell. **Problem:** 1. **Class Definition**: Define a class named `Cell` with the following methods: - `__init__(self, value=None)`: Initializes a new cell object containing the specified `value`. If no value is provided, the cell should be initialized with `None`. - `get(self)`: Returns the content of the cell. - `set(self, value)`: Sets the cell content to the specified `value`. - `is_cell(obj)`: A static method that checks if the provided `obj` is an instance of the `Cell` class. 2. **Functionality Testing**: Implement a function `test_cell()` which will: - Create a new cell object. - Set a value in the cell. - Get the value from the cell and assert that it is correct. - Check if an object is an instance of the Cell class. - Create and validate multiple cells within nested scopes to demonstrate proper handling of variable references. **Input and Output Specifications**: - The class methods will not take any external inputs in their operations aside from the instance they operate on. - The instance methods `get` and `set` return the current or new value of the cell, respectively. - The static method `is_cell` returns `True` if the given object is a `Cell` instance, otherwise it returns `False`. - The function `test_cell` does not take any inputs or produce any outputs, but it should contain assertions to validate the correctness of cell operations. **Example**: ```python class Cell: def __init__(self, value=None): # Initialize the cell with the given value or None def get(self): # Return the value stored in the cell def set(self, value): # Set the value stored in the cell @staticmethod def is_cell(obj): # Check if the given object is a Cell instance def test_cell(): # Create a cell with an initial value cell1 = Cell(10) assert cell1.get() == 10 # Set a new value in the cell cell1.set(20) assert cell1.get() == 20 # Check the is_cell method assert Cell.is_cell(cell1) == True assert Cell.is_cell(10) == False # Nested scope example def outer(): cell = Cell(\\"outer\\") def inner(): assert cell.get() == \\"outer\\" cell.set(\\"inner\\") assert cell.get() == \\"inner\\" inner() assert cell.get() == \\"inner\\" outer() test_cell() ``` **Constraints**: - You are not allowed to use any external libraries. Only built-in Python functionalities are permitted. - Your implementation should handle multiple cells and nested function scopes properly, reflecting changes across scopes as expected. **Note**: Ensure to test your implementation thoroughly. The `test_cell` function demonstrates the usage and serves as a validation for your class.","solution":"class Cell: def __init__(self, value=None): Initializes a new cell object containing the specified value. If no value is provided, the cell is initialized with None. self.value = value def get(self): Returns the content of the cell. return self.value def set(self, value): Sets the cell content to the specified value. self.value = value @staticmethod def is_cell(obj): Checks if the provided obj is an instance of the Cell class. return isinstance(obj, Cell) def test_cell(): # Create a cell with an initial value cell1 = Cell(10) assert cell1.get() == 10 # Set a new value in the cell cell1.set(20) assert cell1.get() == 20 # Check the is_cell method assert Cell.is_cell(cell1) == True assert Cell.is_cell(10) == False # Nested scope example def outer(): cell = Cell(\\"outer\\") def inner(): assert cell.get() == \\"outer\\" cell.set(\\"inner\\") assert cell.get() == \\"inner\\" inner() assert cell.get() == \\"inner\\" outer() test_cell()"},{"question":"# Question: Manipulate AIFF Audio Data Problem Description You are required to write a Python program that reads an AIFF audio file, extracts a portion of the audio, and writes this portion to a new AIFF file. The portion to be extracted is defined by a start time and duration, specified in seconds. Requirements 1. Implement a function `extract_audio_segment(input_file: str, output_file: str, start_time: int, duration: int) -> None` that does the following: * Opens the input AIFF file. * Reads the audio data starting from `start_time` for the specified `duration`. * Writes this segment to a new AIFF output file. 2. Use the methods provided by the `aifc` module. Input - `input_file`: A string representing the path to the input AIFF file. - `output_file`: A string representing the path to the output AIFF file. - `start_time`: An integer representing the start time in seconds from the beginning of the audio to extract. - `duration`: An integer representing the duration in seconds of the audio segment to extract. Output The function should create a new AIFF file at the specified `output_file` path, containing the extracted audio segment. Constraints - The `input_file` will be a valid AIFF or AIFF-C file. - The `start_time` and `duration` will be non-negative integers and within the bounds of the audio file\'s length. Example Suppose you have an input AIFF file named `sample.aiff`, and you want to extract a 10-second segment starting from the 5th second of the audio. The function call would be: ```python extract_audio_segment(\'sample.aiff\', \'output.aiff\', 5, 10) ``` The function should produce an `output.aiff` file that contains audio data starting from the 5th second to the 15th second of the original `sample.aiff` file. Notes - You can assume that the input and output files are accessible and that you have read-write permissions. - Handle any necessary conversions to ensure the output file maintains the same audio parameters (e.g., number of channels, sample width, sample rate) as the input file. Additional Information You can use the following methods from the `aifc` module for this task: - `aifc.open()` - `getnchannels()`, `getsampwidth()`, `getframerate()`, `getnframes()` - `readframes()`, `rewind()`, `setpos()` - `setnchannels()`, `setsampwidth()`, `setframerate()`, `setnframes()` - `writeframes()` - `close()`","solution":"import aifc def extract_audio_segment(input_file: str, output_file: str, start_time: int, duration: int) -> None: Extracts a portion of the audio from input_file starting at start_time for the specified duration and writes it to output_file. with aifc.open(input_file, \'r\') as in_aiff: sample_rate = in_aiff.getframerate() start_frame = start_time * sample_rate end_frame = start_frame + (duration * sample_rate) # Ensure start_frame and end_frame are within bounds start_frame = max(0, start_frame) end_frame = min(in_aiff.getnframes(), end_frame) in_aiff.setpos(start_frame) frames = in_aiff.readframes(end_frame - start_frame) with aifc.open(output_file, \'w\') as out_aiff: out_aiff.setnchannels(in_aiff.getnchannels()) out_aiff.setsampwidth(in_aiff.getsampwidth()) out_aiff.setframerate(sample_rate) out_aiff.setnframes(end_frame - start_frame) out_aiff.writeframes(frames)"},{"question":"# Python Development Mode and Resource Management The `Python Development Mode` adds several runtime checks and warnings that help developers catch potential issues early. One common issue it helps identify is improper resource management, such as forgetting to close files. Additionally, it can catch problems like buffer overflows and improper use of file descriptors. Problem Statement You are given part of a Python script that reads a text file and processes its contents. The script currently does not ensure proper resource management. Your task is to write a function `process_file(filename: str) -> int` that opens a given file, reads all its lines, and returns the number of non-empty lines. Additionally, you must ensure that the file is closed properly, even if an exception is raised during the processing. Use best practices for resource management, without explicitly closing the file using the `close()` function. Function Signature ```python def process_file(filename: str) -> int: pass ``` Input - `filename` (str): Path to the text file to be processed. Output - int: The number of non-empty lines in the file. Constraints - Do not use the `close()` method to close the file explicitly. - Handle any potential exceptions to ensure the file is properly closed. - Assume the file is a text file encoded in UTF-8. - The file may be very large, so consider memory usage efficiency. Example ```python # Example file content: # Line 1 # # Line 2 # # Line 3 # If the file named \'example.txt\' contains the above content, # process_file(\'example.txt\') should return 3 since there are 3 non-empty lines. filename = \'example.txt\' print(process_file(filename)) # Output: 3 ``` Note Remember to use context managers where applicable to ensure proper handling of resources. Hints 1. A **context manager** can be used to manage the file resource efficiently. 2. You can use the `with` statement to guarantee that the file is closed properly even if an error occurs during processing.","solution":"def process_file(filename: str) -> int: Opens a file, reads its lines, and returns the number of non-empty lines. Uses context manager to ensure the file is closed properly. :param filename: Path to the text file to be processed. :return: Number of non-empty lines in the file. non_empty_lines = 0 try: with open(filename, \'r\', encoding=\'utf-8\') as file: for line in file: if line.strip(): non_empty_lines += 1 except Exception as e: print(f\\"An error occurred: {e}\\") return non_empty_lines"},{"question":"# Abstract Base Class Implementation and Usage You are required to design and implement an abstract base class for a `Shape` that defines essential geometric methods and properties. Your goal is to enforce any subclass of `Shape` to implement these methods and properties. Additionally, you should define a mechanism to allow virtual subclasses and to utilize the `__subclasshook__` for custom subclass checks. # Requirements: 1. **Abstract Base Class:** - Create an abstract base class `Shape` using `ABCMeta` or inheriting from `ABC`. - Define the following abstract methods and properties: - `area()`: Abstract method to compute the area of the shape. - `perimeter()`: Abstract method to compute the perimeter of the shape. - `name`: Abstract property representing the name of the shape. 2. **Concrete Subclasses:** - Implement two concrete subclasses `Circle` and `Rectangle` inheriting from `Shape`. - Implement all abstract methods and properties in both subclasses. 3. **Virtual Subclass:** - Create a new class `Triangle` that does not explicitly inherit from `Shape`. - Register `Triangle` as a virtual subclass of `Shape`. 4. **Subclass Hook Method:** - Implement the `__subclasshook__` method in `Shape` to customize its subclass check: - A class is considered a subclass of `Shape` if it has methods `area` and `perimeter`, and a property `name`. # Input and Output Formats: - **No explicit input/output** as it is a class implementation task. - **Provide example usage** demonstrating: - Creation of `Circle` and `Rectangle` objects. - Checking if `Triangle` is considered a subclass of `Shape`. - Demonstrating subclass hook functionality. # Example Usage: ```python from abc import ABC, abstractmethod, ABCMeta class Shape(ABC): @abstractmethod def area(self): pass @abstractmethod def perimeter(self): pass @property @abstractmethod def name(self): pass @classmethod def __subclasshook__(cls, subclass): if cls is Shape: if (any(\\"area\\" in B.__dict__ for B in subclass.__mro__) and any(\\"perimeter\\" in B.__dict__ for B in subclass.__mro__) and any(\\"name\\" in B.__dict__ for B in subclass.__mro__)): return True return NotImplemented class Circle(Shape): def __init__(self, radius): self.radius = radius def area(self): return 3.14159 * self.radius ** 2 def perimeter(self): return 2 * 3.14159 * self.radius @property def name(self): return \\"Circle\\" class Rectangle(Shape): def __init__(self, length, width): self.length = length self.width = width def area(self): return self.length * self.width def perimeter(self): return 2 * (self.length + self.width) @property def name(self): return \\"Rectangle\\" class Triangle: def area(self): return 0.5 * self.base * self.height def perimeter(self): return self.side1 + self.side2 + self.side3 @property def name(self): return \\"Triangle\\" # Register Triangle as a virtual subclass of Shape Shape.register(Triangle) assert issubclass(Circle, Shape) assert issubclass(Rectangle, Shape) assert issubclass(Triangle, Shape) circle = Circle(5) rectangle = Rectangle(2, 3) triangle = Triangle() print(isinstance(circle, Shape)) # True print(isinstance(rectangle, Shape)) # True print(isinstance(triangle, Shape)) # True ```","solution":"from abc import ABC, abstractmethod, ABCMeta class Shape(ABC): @abstractmethod def area(self): pass @abstractmethod def perimeter(self): pass @property @abstractmethod def name(self): pass @classmethod def __subclasshook__(cls, subclass): if cls is Shape: if (any(\\"area\\" in B.__dict__ for B in subclass.__mro__) and any(\\"perimeter\\" in B.__dict__ for B in subclass.__mro__) and any(\\"name\\" in B.__dict__ for B in subclass.__mro__)): return True return NotImplemented class Circle(Shape): def __init__(self, radius): self.radius = radius def area(self): return 3.14159 * self.radius ** 2 def perimeter(self): return 2 * 3.14159 * self.radius @property def name(self): return \\"Circle\\" class Rectangle(Shape): def __init__(self, length, width): self.length = length self.width = width def area(self): return self.length * self.width def perimeter(self): return 2 * (self.length + self.width) @property def name(self): return \\"Rectangle\\" class Triangle: def __init__(self, base, height, side1, side2, side3): self.base = base self.height = height self.side1 = side1 self.side2 = side2 self.side3 = side3 def area(self): return 0.5 * self.base * self.height def perimeter(self): return self.side1 + self.side2 + self.side3 @property def name(self): return \\"Triangle\\" # Register Triangle as a virtual subclass of Shape Shape.register(Triangle)"},{"question":"**Objective:** Implement a function that processes a string of arithmetic operations written in reverse Polish notation (RPN) and evaluate its result. Additionally, the function should handle variables using a given dictionary of variable values. Constraints: - The function should support the following arithmetic operations: `+`, `-`, `*`, `/`, `%`, `**`. - Handle integer and floating-point arithmetic. - Raise a `ValueError` for invalid expressions or operations. - Variables in the expression should use values provided in the dictionary. - Assume all variable names in the expression are valid single-letter identifiers (a-z). - Negative integers are allowed in the expression. - The function must handle variables not defined in the dictionary by raising a `NameError`. Input Format: - An RPN expression string, where each token is separated by space. - A dictionary containing variable values. Output Format: - Return the result of the evaluated expression. Example: **Input:** ```python expression = \\"a b + 5 *\\" variables = {\\"a\\": 3, \\"b\\": 7} ``` **Output:** ```python 50 ``` **Explanation:** The expression translates to `3 7 + 5 *`, which is equivalent to `(3 + 7) * 5` resulting in `50`. **Function Signature:** ```python def evaluate_rpn_expression(expression: str, variables: dict) -> float: pass ``` # Evaluation Criteria: - Correctness and handling of edge cases. - Proper use of Python expressions and operators. - Efficient and clean code. - Appropriate error handling for missing variables and invalid operations. Additional Notes: - Make use of the `collections.deque` for efficient stack operations. - Carefully consider operator precedence and ensure errors are raised appropriately for invalid expressions. - You may assume that unary operations are not part of the expression. **Sample Run:** ```python expression = \\"3 4 + 2 * 7 /\\" variables = {} print(evaluate_rpn_expression(expression, variables)) # Output: 2.0 expression = \\"a b + c *\\" variables = {\\"a\\": 1, \\"b\\": 2, \\"c\\": 3} print(evaluate_rpn_expression(expression, variables)) # Output: 9.0 ```","solution":"import operator from collections import deque def evaluate_rpn_expression(expression: str, variables: dict) -> float: ops = { \'+\': operator.add, \'-\': operator.sub, \'*\': operator.mul, \'/\': operator.truediv, \'%\': operator.mod, \'**\': operator.pow, } def resolve_value(token): try: return float(token) except ValueError: if token in variables: return variables[token] else: raise NameError(f\\"Variable \'{token}\' not defined\\") stack = deque() for token in expression.split(): if token in ops: if len(stack) < 2: raise ValueError(\\"Invalid expression\\") b = stack.pop() a = stack.pop() operation = ops[token] stack.append(operation(a, b)) else: stack.append(resolve_value(token)) if len(stack) != 1: raise ValueError(\\"Invalid expression\\") return stack.pop()"},{"question":"**Objective**: Implement a Python function that handles custom slice objects and ensures proper slicing of sequences, considering out-of-bounds indices and step values. Problem Statement You are to implement a function that correctly slices a sequence using custom slice parameters, ensuring that the start, stop, and step parameters are adjusted properly for out-of-bounds indices. You should mimic the behavior of Python slicing as closely as possible using the provided parameters. Function Signature ```python def custom_slice(sequence, start=None, stop=None, step=None): Slices the given sequence according to the provided start, stop, and step values. Args: sequence (list): The sequence to be sliced. start (int, optional): The beginning index of the slice. Defaults to None. stop (int, optional): The end index of the slice. Defaults to None. step (int, optional): The step value for the slice. Defaults to None. Returns: list: A new list derived from the given sequence according to the slice parameters. ``` Input - `sequence`: A list of integers. - `start` (optional): An integer indicating the starting index of the slice. - `stop` (optional): An integer indicating the ending index of the slice. - `step` (optional): An integer indicating the step value for slicing. Output - A list of integers representing the sliced sequence based on the provided parameters. Constraints - You should handle cases where any of `start`, `stop`, or `step` is `None`. - You should implement a behavior that mimics out-of-bound handling like in Python\'s native slicing. - The function should return an empty list if the step is `0` or if the start-stop configuration results in an empty sequence. Examples ```python # Example 1 sequence = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] print(custom_slice(sequence, 2, 8, 2)) # Output: [2, 4, 6] # Example 2 sequence = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] print(custom_slice(sequence, 1, None, 3)) # Output: [1, 4, 7] # Example 3 sequence = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] print(custom_slice(sequence, -4, None, -1)) # Output: [6, 5, 4, 3, 2, 1, 0] ``` Additional Notes - You may not use the built-in `slice()` and slicing `:` syntax for implementing the solution. - Ensure your function handles step values that result in reversed slicing (negative steps).","solution":"def custom_slice(sequence, start=None, stop=None, step=None): Slices the given sequence according to the provided start, stop, and step values. Args: sequence (list): The sequence to be sliced. start (int, optional): The beginning index of the slice. Defaults to None. stop (int, optional): The end index of the slice. Defaults to None. step (int, optional): The step value for the slice. Defaults to None. Returns: list: A new list derived from the given sequence according to the slice parameters. if step == 0: return [] # Adjust defaults step = 1 if step is None else step seq_len = len(sequence) # Handle out of bounds and negative indices if start is None: start = 0 if step > 0 else seq_len - 1 elif start < 0: start += seq_len if stop is None: stop = seq_len if step > 0 else -1 elif stop < 0: stop += seq_len # Prepare the sliced sequence if step > 0: start = max(0, min(seq_len, start)) stop = max(0, min(seq_len, stop)) else: start = max(-1, min(seq_len - 1, start)) stop = max(-1, min(seq_len - 1, stop)) result = [] i = start while (step > 0 and i < stop) or (step < 0 and i > stop): result.append(sequence[i]) i += step return result"},{"question":"Coding Assessment Question # Objective In this task, you are required to implement a function that operates on a sequence of objects and performs multiple transformations utilizing the abstract object protocols provided by Python (such as Sequence and Mapping protocols). # Function Definition You need to implement a function `transform_sequence` which takes a sequence of objects as input and returns a transformed sequence. ```python def transform_sequence(sequence): Transforms the given sequence of objects by performing the following operations: 1. For objects that are integers: - If the integer is even, multiply it by 2. - If the integer is odd, multiply it by 3. 2. For objects that are lists: - If the list contains all integers, sort the list. - Otherwise, reverse the list. 3. For objects that are dictionaries: - If all values in the dictionary are integers, square each value. - Otherwise, swap keys and values in the dictionary. 4. For objects that are strings: - Convert the string to uppercase. 5. For any other type: - Convert the object to its string representation. The function should return the transformed sequence containing all the modified objects. Args: sequence (sequence): A sequence of objects (list, tuple, or any sequence type) Returns: sequence: A transformed sequence of the same type as the input Example: >>> transform_sequence([1, 2, [3, 4, \'a\'], {\'a\': 1, \'b\': 2}, \'hello\', 7]) [3, 4, [\'a\', 4, 3], {1: \'a\', 2: \'b\'}, \'HELLO\', 21] pass ``` # Constraints - You may assume the input sequence does not contain nested sequences (i.e., a list within a list is not possible, but a list within a dictionary is acceptable). - Performance should be optimized to handle sequences containing up to 10,000 elements efficiently. # Notes - You may use any in-built Python functions as necessary. - Clearly comment your code to explain each transformation step. # Evaluation Criteria - Correctness: The function should correctly perform all specified transformations. - Efficiency: The function should efficiently handle large sequences. - Code quality: The code should be well-organized and commented.","solution":"def transform_sequence(sequence): Transforms the given sequence of objects by performing the following operations: 1. For objects that are integers: - If the integer is even, multiply it by 2. - If the integer is odd, multiply it by 3. 2. For objects that are lists: - If the list contains all integers, sort the list. - Otherwise, reverse the list. 3. For objects that are dictionaries: - If all values in the dictionary are integers, square each value. - Otherwise, swap keys and values in the dictionary. 4. For objects that are strings: - Convert the string to uppercase. 5. For any other type: - Convert the object to its string representation. The function should return the transformed sequence containing all the modified objects. Args: sequence (sequence): A sequence of objects (list, tuple, or any sequence type) Returns: sequence: A transformed sequence of the same type as the input transformed_items = [] for item in sequence: if isinstance(item, int): # Integer transformation if item % 2 == 0: transformed_items.append(item * 2) else: transformed_items.append(item * 3) elif isinstance(item, list): # List transformation if all(isinstance(x, int) for x in item): transformed_items.append(sorted(item)) else: transformed_items.append(list(reversed(item))) elif isinstance(item, dict): # Dictionary transformation if all(isinstance(v, int) for v in item.values()): transformed_items.append({k: v ** 2 for k, v in item.items()}) else: transformed_items.append({v: k for k, v in item.items()}) elif isinstance(item, str): # String transformation transformed_items.append(item.upper()) else: # Other types transformation transformed_items.append(str(item)) return type(sequence)(transformed_items)"},{"question":"**Task**: You are required to write a module involving a small Python program that calculates the greatest common divisor (GCD) of two numbers using the Euclidean Algorithm. Your task will involve writing the function along with appropriate doctests in the form of docstrings. **Requirements**: 1. Implement a function `gcd(a: int, b: int) -> int` that calculates the GCD of two integers using the Euclidean Algorithm. 2. Write sufficient doctests for the `gcd` function to ensure it\'s thoroughly tested. 3. Validate that your doctests run successfully by integrating with the `unittest` framework. Here\'s the structure to follow: ```python Module documentation: This module defines a function to compute the greatest common divisor (gcd) and contains the respective doctests. >>> gcd(48, 18) 6 >>> gcd(0, 5) 5 >>> gcd(10, 10) 10 >>> gcd(7, 13) 1 >>> gcd(50, 15) 5 def gcd(a: int, b: int) -> int: Compute the greatest common divisor of a and b using the Euclidean Algorithm. Parameters: a (int): First number, should be non-negative. b (int): Second number, should be non-negative. Returns: int: GCD of a and b. Examples: >>> gcd(48, 18) 6 >>> gcd(0, 5) 5 >>> gcd(10, 10) 10 >>> gcd(7, 13) 1 >>> gcd(50, 15) 5 while b != 0: a, b = b, a % b return a if __name__ == \\"__main__\\": import doctest doctest.testmod() ``` **Integration with unittest Framework**: Additionally, you should create a script or function that integrates the doctests with the unittest framework. ```python import unittest import doctest def load_tests(loader, tests, ignore): tests.addTests(doctest.DocTestSuite(__import__(\'your_module_name\'))) return tests if __name__ == \\"__main__\\": unittest.main() ``` **Constraints**: - Ensure that your implementation handles edge cases, such as both numbers being zero or one of them being zero. - You may not use any external libraries for the Euclidean Algorithm computation. - Be sure to clarify assumptions you make in your function documentation. **Evaluation Criteria**: - Correctness: Your function should correctly compute the GCD. - Coverage: Your doctests should cover multiple typical and edge cases. - Integration: Properly integrate doctests with `unittest`.","solution":"def gcd(a: int, b: int) -> int: Compute the greatest common divisor of a and b using the Euclidean Algorithm. Parameters: a (int): First number, should be non-negative. b (int): Second number, should be non-negative. Returns: int: GCD of a and b. Examples: >>> gcd(48, 18) 6 >>> gcd(0, 5) 5 >>> gcd(10, 10) 10 >>> gcd(7, 13) 1 >>> gcd(50, 15) 5 while b != 0: a, b = b, a % b return a"},{"question":"You are provided with a dataset containing the average monthly temperatures (in Celsius) for a specific location over a year. Your task is to use `seaborn` and specifically the `seaborn.objects` module to create and customize a line plot that meets the following requirements: 1. Create a line plot showing the average monthly temperatures. The x-axis should represent months (1 for January, 2 for February, etc.), and the y-axis should represent temperatures in Celsius. 2. Set custom limits for the x-axis to range from 0 to 13 and for the y-axis to range from -10°C to 40°C. 3. Reverse the y-axis limits so that the higher values are at the bottom of the plot. 4. Ensure that any unspecified axis limits (like the upper limit of the x-axis and the lower limit of the y-axis) maintain their default values. 5. Add markers to the line plot for better visualization of data points. The input to your function will be a list of 12 numerical values representing the average monthly temperatures. Your function should display the resulting plot. # Input Format - A list of 12 floats or integers representing the average monthly temperatures. # Output Format - A seaborn plot displayed according to the specified requirements. # Example ```python def plot_average_temperatures(temperatures): # your implementation here # Example usage: temperatures = [7, 8, 10, 15, 20, 25, 30, 28, 24, 18, 12, 8] plot_average_temperatures(temperatures) ``` This should create and display the following plot: - x-axis from 0 to 13 - y-axis from 40 to -10 (reversed) - Line plot with markers representing average monthly temperatures Ensure your plot is clear and accurately represents the data.","solution":"import matplotlib.pyplot as plt import seaborn as sns def plot_average_temperatures(temperatures): Creates and displays a seaborn line plot with the given average monthly temperatures. Parameters: temperatures (list of floats/integers): A list of 12 values representing average monthly temperatures. if len(temperatures) != 12: raise ValueError(\\"The list must contain exactly 12 values.\\") # Define months for the x-axis months = list(range(1, 13)) # Create the plot sns.set_theme(style=\\"whitegrid\\") p = sns.lineplot(x=months, y=temperatures, marker=\'o\') # Customize the limits p.set_xlim(0, 13) p.set_ylim(40, -10) # Add labels for clarity p.set_xlabel(\'Month\') p.set_ylabel(\'Average Temperature (°C)\') # Display the plot plt.show()"},{"question":"# Custom Object Creation and Management You are tasked with implementing a custom Python object that resembles a simplified version of a matrix datatype. This task will involve defining methods to initialize, access, manipulate matrix elements and also handle memory management explicitly. **Objective:** Implement a `Matrix` class that: 1. Initializes a 2D matrix with given dimensions and optional initial value. 2. Supports setting and getting matrix elements using standard indexing. 3. Includes methods for matrix addition, subtraction, and multiplication. 4. Properly manages memory and supports garbage collection. **Specifications:** 1. **Initialization**: - Input: Two integers `rows` and `cols`, and an optional initial value (`init_val`). - Output: An object representing a matrix of dimensions `rows x cols`, initialized with `init_val`. Default initialization value is `0`. ```python class Matrix: def __init__(self, rows: int, cols: int, init_val: int = 0): # Your code here ``` 2. **Element Access and Assignment**: - Implement `__getitem__` and `__setitem__` to allow standard indexing access for getting and setting matrix elements. ```python class Matrix: def __getitem__(self, index: tuple) -> int: # Your code here def __setitem__(self, index: tuple, value: int): # Your code here ``` 3. **Matrix Operations**: - Implement methods for matrix addition `__add__`, subtraction `__sub__`, and multiplication `__mul__`. ```python class Matrix: def __add__(self, other: \'Matrix\') -> \'Matrix\': # Your code here def __sub__(self, other: \'Matrix\') -> \'Matrix\': # Your code here def __mul__(self, other: \'Matrix\') -> \'Matrix\': # Your code here ``` 4. **Memory Management**: - Ensure proper memory allocation and support for garbage collection. - Use Python\'s `__del__` method to show where memory management considerations would be made. ```python class Matrix: def __del__(self): # Your code here ``` **Constraints:** - Assume that the matrix operations are only valid on matrices of compatible dimensions. - Matrix multiplication will follow the standard rules of linear algebra. - Efficiency is key; your implementation should avoid unnecessary computations and should be mindful of memory usage. **Example Usage:** ```python # Initialize a 3x3 matrix with default value 0 m1 = Matrix(3, 3) # Initialize another 3x3 matrix with all values set to 1 m2 = Matrix(3, 3, 1) # Set specific elements m1[0, 0] = 5 m1[1, 2] = 7 # Retrieve specific elements print(m1[0, 0]) # Output: 5 print(m2[1, 1]) # Output: 1 # Add two matrices m3 = m1 + m2 # Subtract two matrices m4 = m1 - m2 # Multiply two matrices m5 = m1 * m2 ``` **Note:** The objective of the task is to understand custom object creation, method overloading, indexing, and memory management in Python. Avoid using external libraries for matrix operations.","solution":"class Matrix: def __init__(self, rows: int, cols: int, init_val: int = 0): self.rows = rows self.cols = cols self.data = [[init_val for _ in range(cols)] for _ in range(rows)] def __getitem__(self, index: tuple) -> int: row, col = index return self.data[row][col] def __setitem__(self, index: tuple, value: int): row, col = index self.data[row][col] = value def __add__(self, other: \'Matrix\') -> \'Matrix\': if self.rows != other.rows or self.cols != other.cols: raise ValueError(\\"Matrices must have the same dimensions for addition\\") result = Matrix(self.rows, self.cols) for i in range(self.rows): for j in range(self.cols): result[i, j] = self[i, j] + other[i, j] return result def __sub__(self, other: \'Matrix\') -> \'Matrix\': if self.rows != other.rows or self.cols != other.cols: raise ValueError(\\"Matrices must have the same dimensions for subtraction\\") result = Matrix(self.rows, self.cols) for i in range(self.rows): for j in range(self.cols): result[i, j] = self[i, j] - other[i, j] return result def __mul__(self, other: \'Matrix\') -> \'Matrix\': if self.cols != other.rows: raise ValueError(\\"Matrices have incompatible dimensions for multiplication\\") result = Matrix(self.rows, other.cols) for i in range(result.rows): for j in range(result.cols): result[i, j] = sum(self[i, k] * other[k, j] for k in range(self.cols)) return result def __del__(self): # Memory management is mostly handled by Python\'s garbage collector. # This can be useful for really large matrices to track when they are deleted. del self.data"},{"question":"You are given the task to implement a simple distributed training scenario using PyTorch, where you will need to handle and record events regarding the initialization and completion of the training phases. # Part 1: Define an Event Handling Class 1. Create a class `TrainingEventHandler` which: - Initializes an event log (a simple dictionary that will keep track of events and their counts). - Has a method `record_event(event_name: str)` that logs the event occurrences (increments the count if the event already exists, otherwise initiates the count). # Part 2: Distributed Training Simulation 2. Implement a function `simulate_distributed_training(handler: TrainingEventHandler)` which: - Records an \\"Initialization\\" event at the beginning. - Simulates a training phase by simply iterating over a fixed number of steps (e.g., 10 steps). - Records a \\"Step\\" event for each training step. - Records a \\"Completion\\" event at the end. - Returns the handler. # Part 3: Testing 3. Write a test function `test_simulate_distributed_training()` to verify that the `simulate_distributed_training` function correctly logs the events: - Verify that the event log contains the expected number of \\"Initialization\\", \\"Step\\", and \\"Completion\\" events. # Input and Output Specifications - **Input**: None - **Output**: A test function outputting the results of assertions checking correctness of the event counts. # Constraints and Additional Notes - Ensure that you utilize the `TrainingEventHandler` class to log events. - Assume a fixed number of 10 training steps for simplicity. - Use asserts to ensure the expected counts for each event are correct (1 for Initialization, 10 for Step, 1 for Completion). # Example Usage ```python # Example usage of the above implementation handler = TrainingEventHandler() simulate_distributed_training(handler) print(handler.event_log) # expected output: {\'Initialization\': 1, \'Step\': 10, \'Completion\': 1} ``` # Performance Requirements - The function is expected to run efficiently within the constraints provided (10 training steps).","solution":"class TrainingEventHandler: def __init__(self): self.event_log = {} def record_event(self, event_name: str): if event_name in self.event_log: self.event_log[event_name] += 1 else: self.event_log[event_name] = 1 def simulate_distributed_training(handler: TrainingEventHandler): handler.record_event(\\"Initialization\\") for _ in range(10): # Simulating 10 training steps handler.record_event(\\"Step\\") handler.record_event(\\"Completion\\") return handler"},{"question":"<|Analysis Begin|> The **python310** packet contains a module named **tokenize**, which is a lexical scanner for Python source code. This module can return comments as tokens apart from typical code tokens, useful for tasks such as pretty-printing, code analysis, and rewriting. The main functionalities provided by this module include: 1. **tokenize.tokenize(readline)**: This function reads Python source code as bytes and returns a generator producing 5-tuples for each token, including detailed position data and token types. 2. **generate_tokens(readline)**: Similar to tokenize(), but expects the input lines to be returned as strings. 3. **untokenize(iterable)**: This function reverses the tokenization process, reconstructing a source code string from a sequence of tokens. 4. **detect_encoding(readline)**: This function discovers the encoding used in a Python source file. 5. **open(filename)**: Opens a file using the detected encoding. The module can handle invalid Python code correctly and includes a specific exception, `TokenError`, for unfinished tokens that span multiple lines. Additionally, it offers command-line usage for easier tokenization tasks. Based on this, a suitable question for assessing students\' understanding could involve tasks like token manipulation, encoding detection, or reverse transformation functions. <|Analysis End|> <|Question Begin|> # **Python310 Token Manipulation Assessment** You have been provided with some Python source code. Your task is to write a function **`double_numbers_in_code`** that will: 1. Tokenize the given Python source code using the `tokenize` module. 2. Identify all numeric literals in the code. 3. Replace each numeric literal with its double (e.g., turn `1` into `2`, `3.14` into `6.28`). 4. Untokenize the modified token stream and return the modified source code as a string. # **Function Signature** ```python def double_numbers_in_code(source_code: str) -> str: pass ``` # **Input** - **source_code**: A string representing the valid Python source code. # **Output** - The function should return the modified source code as a string with numeric literals doubled. # **Example** ```python source_code = \'\'\' def double(x): return 2 * x + 3 print(double(10)) # Expected output: 23 \'\'\' modified_code = double_numbers_in_code(source_code) print(modified_code) ``` **Expected Output:** ```python \'\'\' def double(x): return 4 * x + 6 print(double(20)) # Expected output: 46 \'\'\' ``` # **Constraints** - You can assume that the provided source code is syntactically valid Python. - The numeric literals can be integers or floating-point numbers and they may appear in various contexts within the source code. # **Notes** - Use the `tokenize` module facilities, including `tokenize.tokenize`, and `tokenize.untokenize` to perform the task. - Remember to handle edge cases, such as numbers with various decimal notations and embedded within other constructs like comments or strings. # **Performance Requirements** - The function is expected to handle source code files up to a few KB without significant performance degradation. - Efficiently handle the token stream using appropriate data structures. Good luck!","solution":"import tokenize from io import BytesIO def double_numbers_in_code(source_code: str) -> str: tokens = [] readline = BytesIO(source_code.encode(\'utf-8\')).readline g = tokenize.tokenize(readline) for toknum, tokval, start, end, line in g: if toknum == tokenize.NUMBER: if \'.\' in tokval: # it\'s a float tokval = str(float(tokval) * 2) else: # it\'s an integer tokval = str(int(tokval) * 2) tokens.append((toknum, tokval, start, end, line)) # Get the original source back from the modified tokens return tokenize.untokenize(tokens).decode(\'utf-8\')"},{"question":"# PyTorch Reproducibility Challenge **Objective**: Implement a function that configures PyTorch for reproducible results across multiple runs. This includes setting seeds for various random number generators and ensuring deterministic behavior for PyTorch operations. **Problem Statement**: Write a function `configure_reproducibility(seed)` that takes an integer `seed` as input and configures PyTorch, Python, and NumPy to ensure reproducibility. The function should also configure PyTorch to avoid using nondeterministic algorithms. **Requirements**: 1. Set the seed for the PyTorch random number generator. 2. Set the seed for the Python built-in random number generator. 3. Set the seed for the NumPy random number generator. 4. Configure PyTorch to use deterministic algorithms. 5. Disable cuDNN convolution benchmarking for reproducibility. **Function Signature**: ```python def configure_reproducibility(seed: int) -> None: pass ``` **Expected Input and Output**: - Input: An integer `seed`. - Output: None. The function configures the environment for reproducibility. **Constraints**: 1. `seed` will be a positive integer. 2. The function should configure both CPU and CUDA operations (if CUDA is available). **Example**: ```python # Example usage: configure_reproducibility(42) # This should ensure that subsequent PyTorch operations produce the same result # every time the script is run with the same seed. import torch x = torch.randn(3, 3) print(x) ``` **Performance Requirements**: - The function should run in a reasonable time considering the overhead of configuring settings. **Additional Notes**: - Consider all necessary imports and configurations required for this function within the PyTorch and other related libraries. **Evaluation Criteria**: - Correctness: The function should correctly configure the environment for reproducibility. - Completeness: All specified requirements should be met. - Clarity: The code should be well-documented and easy to follow.","solution":"import torch import random import numpy as np def configure_reproducibility(seed: int) -> None: Configures the environment for reproducible results. Args: seed (int): The seed to be used for random number generators. Returns: None # Set the seed for the torch random number generator torch.manual_seed(seed) # If CUDA is available, set the seed for the CUDA random number generator if torch.cuda.is_available(): torch.cuda.manual_seed(seed) torch.cuda.manual_seed_all(seed) # Set the seed for the random library random.seed(seed) # Set the seed for the NumPy random number generator np.random.seed(seed) # Enable deterministic algorithms in PyTorch torch.backends.cudnn.deterministic = True # Disable cuDNN benchmark for reproducibility torch.backends.cudnn.benchmark = False"},{"question":"# Advanced Coding Assessment **Objective**: Demonstrate your understanding of Python generator expressions, yield statements, and list comprehensions by implementing a function that processes numerical data efficiently. **Problem Statement**: You are given a large list of numerical data (both integers and floating-point numbers). 1. Implement three functions to handle this data: - `list_comprehension(data: List[Union[int, float]]) -> List[Union[int, float]]`: This function should filter out all non-positive numbers using a list comprehension and return the resulting list. - `generator_expression(data: List[Union[int, float]]) -> Generator`: This function should achieve the same result using a generator expression. This generator should yield valid numbers one by one. - `yield_filter(data: List[Union[int, float]]) -> Generator`: This function should use a `yield` statement to filter out all non-positive numbers and yield valid numbers one by one. 2. Implement a fourth function: - `aggregate(data: Union[List[Union[int, float]], Generator]) -> float`: This function should take either a list or a generator and return the sum of all positive numbers after converting them to a common type (float). **Input and Output Formats**: - Input: A list of numerical values (integers and floats). ```python data = [1, -2, 3.5, -4.2, 5, 0] ``` - Output for `list_comprehension` and `generator_expression`: ```python [1, 3.5, 5] ``` - Output for `yield_filter`: ```python (yields 1, 3.5, 5) ``` - Output for `aggregate` when passed either the list or the generator: ```python 9.5 ``` **Constraints**: - The input list can be very large (up to 1 million elements). - The functions should handle different edge cases, such as lists with no positive numbers, all negative numbers, or mixed types. - Performance efficiency is crucial, especially for functions using generator expressions and `yield`. **Example**: ```python data = [1, -2, 3.5, -4.2, 5, 0] # List comprehension filtered_list = list_comprehension(data) print(filtered_list) # Output: [1, 3.5, 5] # Generator expression gen_exp = generator_expression(data) print(list(gen_exp)) # Output: [1, 3.5, 5] # Yield filter yield_gen = yield_filter(data) print(list(yield_gen)) # Output: [1, 3.5, 5] # Aggregate sum_list = aggregate(filtered_list) print(sum_list) # Output: 9.5 sum_gen = aggregate(generator_expression(data)) print(sum_gen) # Output: 9.5 ``` Implement the functions as specified to handle various cases efficiently, leveraging the power of generator expressions, yield statements, and list comprehensions in Python.","solution":"from typing import List, Union, Generator def list_comprehension(data: List[Union[int, float]]) -> List[Union[int, float]]: Filters out all non-positive numbers using a list comprehension. return [x for x in data if x > 0] def generator_expression(data: List[Union[int, float]]) -> Generator[Union[int, float], None, None]: Filters out all non-positive numbers using a generator expression. return (x for x in data if x > 0) def yield_filter(data: List[Union[int, float]]) -> Generator[Union[int, float], None, None]: Filters out all non-positive numbers using a yield statement. for x in data: if x > 0: yield x def aggregate(data: Union[List[Union[int, float]], Generator[Union[int, float], None, None]]) -> float: Takes either a list or a generator and returns the sum of all positive numbers converted to float. return sum(float(x) for x in data)"},{"question":"**Title:** Implementing and Using Internationalization in a Python Application Problem Statement: You are tasked with creating a multilingual console application that supports English and Spanish. The application should prompt users to input their name and greet them in the selected language. You need to utilize the `gettext` module to achieve this functionality. # Requirements: 1. **Directory Structure**: ``` locale/ en/ LC_MESSAGES/ greetings.mo es/ LC_MESSAGES/ greetings.mo ``` - You need to create message catalog files (`.mo` files) for both English and Spanish languages. Place these files in the appropriate directories as shown above. 2. **Steps**: - Define the translatable string for the greeting message. - Implement a function to bind the text domain and load the appropriate language based on user input. - Implement another function to prompt for the user\'s name and greet them in the selected language. 3. **Functions to Implement**: - `initialize_locale(language: str) -> None`: This function should bind the text domain, set the locale directory, and install the `_` function for translations. - `greet_user() -> None`: This function should prompt the user to enter their name and then print a greeting message in the selected language. # Input and Output: 1. The `initialize_locale(language)` function should take an input string `language` which can either be `\'en\'` (for English) or `\'es\'` (for Spanish). 2. The `greet_user()` function should prompt the user for their name and print a greeting message in the selected language. # Constraints: - You must use the `gettext` module for string translation. - The `.mo` files should already be compiled from their respective `.po` files. # Example Usage: ```python # Initialize locale for Spanish initialize_locale(\'es\') # Greet user in Spanish greet_user() ``` **Expected Output (for Spanish):** ``` Por favor, ingrese su nombre: Juan Hola Juan! ``` **Expected Output (for English):** ``` Please enter your name: John Hello John! ``` # Notes: - The `.po` files for both English and Spanish should contain entries for the greeting message `\\"Please enter your name:\\"` and `\\"Hello %s!\\"`. - Make sure to handle any potential issues like missing translation files or invalid language codes gracefully by falling back to English if necessary. # Submission: Submit your solution by providing the implementation of the two functions `initialize_locale` and `greet_user` along with the `.mo` files for both English and Spanish languages.","solution":"import gettext def initialize_locale(language: str) -> None: Initialize the locale for translations based on the selected language. If the language is not supported, fall back to English (\'en\'). try: locales_dir = \'locale\' language_code = language if language in [\'en\', \'es\'] else \'en\' gettext.bindtextdomain(\'greetings\', locales_dir) gettext.textdomain(\'greetings\') language_translations = gettext.translation(\'greetings\', localedir=locales_dir, languages=[language_code], fallback=True) language_translations.install() global _ _ = language_translations.gettext except Exception as e: print(f\\"Error initializing locale: {e}\\") # Fallback to English if any error occurs gettext.install(\'greetings\', localedir=locales_dir, names=[\'gettext\']) def greet_user() -> None: Prompt the user to enter their name and greet them in the selected language. name = input(_(\\"Please enter your name: \\")) print(_(\\"Hello %s!\\") % name) if __name__ == \\"__main__\\": language = input(\\"Select language (\'en\' for English, \'es\' for Spanish): \\").strip() initialize_locale(language) greet_user()"},{"question":"# Advanced Python Import System Problem Statement In this task, you are required to implement a custom import mechanism using Python’s import system. You will create a custom module finder and loader that allows Python to import modules from a dictionary that contains module names as keys and their corresponding Python code as values. # Implementation Details 1. **Custom Finder**: Implement a custom meta path finder that scans a predefined dictionary for a module name. 2. **Custom Loader**: Implement a custom loader that can execute the code from the dictionary value corresponding to the module name. 3. **Integration**: Register the custom finder in `sys.meta_path` so that it is used for module imports. # Steps 1. **Define the Module Code Dictionary**: Create a dictionary named `module_code_dict`. Each key should be a module name (e.g., \'mod1\') and the value should be a string containing valid Python code for that module. ```python module_code_dict = { \'mod1\': \'\'\' def greet(): return \\"Hello from mod1!\\" \'\'\', # Add more modules if needed } ``` 2. **Custom Meta Path Finder**: Implement a class `DictMetaPathFinder` that has a method `find_spec()` to check if a module name exists in `module_code_dict`. If found, it should return a module spec with the custom loader. ```python import importlib.util import sys class DictMetaPathFinder: def find_spec(self, fullname, path, target=None): if fullname in module_code_dict: spec = importlib.util.spec_from_loader(fullname, DictModuleLoader(fullname)) return spec return None ``` 3. **Custom Loader**: Implement a class `DictModuleLoader` that has a method `exec_module()` to execute the code from the dictionary. ```python import types class DictModuleLoader: def __init__(self, name): self.name = name def create_module(self, spec): return types.ModuleType(spec.name) def exec_module(self, module): code = module_code_dict[self.name] exec(code, module.__dict__) ``` 4. **Register Finder**: Add an instance of `DictMetaPathFinder` to `sys.meta_path`. ```python sys.meta_path.insert(0, DictMetaPathFinder()) ``` 5. **Testing**: Write code to test the custom import mechanism. Import the module `mod1` and call its `greet` function. ```python import mod1 print(mod1.greet()) # Should print: Hello from mod1! ``` # Constraints - The custom import mechanism should only handle modules defined in `module_code_dict`. - Do not modify the in-built import system for other modules. - Ensure proper error handling if a module is not found or the code execution fails. # Submission Submit a single Python file that includes the definition of `module_code_dict`, `DictMetaPathFinder`, `DictModuleLoader`, and the test code. Ensure that the custom import mechanism works correctly and efficiently.","solution":"import importlib.util import sys import types # Define the code for the modules module_code_dict = { \'mod1\': \'\'\' def greet(): return \\"Hello from mod1!\\" \'\'\' # Add more modules as needed } # Custom Meta Path Finder class DictMetaPathFinder: def find_spec(self, fullname, path, target=None): if fullname in module_code_dict: spec = importlib.util.spec_from_loader(fullname, DictModuleLoader(fullname)) return spec return None # Custom Loader class DictModuleLoader: def __init__(self, name): self.name = name def create_module(self, spec): return types.ModuleType(spec.name) def exec_module(self, module): code = module_code_dict[self.name] exec(code, module.__dict__) # Register the custom finder sys.meta_path.insert(0, DictMetaPathFinder())"},{"question":"**Python Distribution Automation Task** In this assignment, you need to create a Python script that automates the setup and packaging of a Python project. Your task involves creating a virtual environment, installing dependencies, and packaging the project into an executable zip archive using the relevant libraries. # Requirements: 1. **Creating a Virtual Environment:** - Use the `venv` module to create a virtual environment in a directory named `env`. 2. **Installing Dependencies:** - The project dependencies are listed in a `requirements.txt` file located in the project directory. Use the `ensurepip` and `pip` modules to install these dependencies into the created virtual environment. 3. **Packaging as Executable Zip Archive:** - Use the `zipapp` module to package your project directory into a single executable zip archive. The output should be a file named `project_archive.pyz`. # Input: - The script should be run from the root of the project directory and assume the existence of a `requirements.txt` file in that directory. # Output: - The script should produce an executable zip archive named `project_archive.pyz`. # Constraints: - Your script should be able to handle any errors that may occur during the creation of the virtual environment, installation of dependencies, or creation of the zip archive and provide meaningful error messages. # Example Execution: Below is the structure of the project directory: ``` /my_python_project |-- main.py |-- requirements.txt |-- other_modules/ |-- module1.py |-- module2.py ``` When your script is executed from the `/my_python_project` directory, it will: 1. Create a virtual environment named `env`. 2. Install the dependencies listed in `requirements.txt` into the `env`. 3. Create an executable zip archive named `project_archive.pyz` of the whole project directory. # Deliverable: Submit a Python script named `package_project.py` that automates the described distribution process. # Additional Information: For reference on how to use the required modules, please check the Python documentation for: - `venv`: [https://docs.python.org/3/library/venv.html](https://docs.python.org/3/library/venv.html) - `ensurepip`: [https://docs.python.org/3/library/ensurepip.html](https://docs.python.org/3/library/ensurepip.html) - `zipapp`: [https://docs.python.org/3/library/zipapp.html](https://docs.python.org/3/library/zipapp.html)","solution":"import os import subprocess import sys import venv import zipapp def create_virtual_environment(env_dir=\'env\'): Create a virtual environment in the specified directory using the `venv` module. try: venv.create(env_dir, with_pip=True) print(f\\"Virtual environment created in {env_dir}\\") except Exception as e: print(f\\"Failed to create virtual environment: {e}\\") sys.exit(1) def install_dependencies(env_dir=\'env\', requirements_file=\'requirements.txt\'): Install dependencies listed in the requirements.txt file into the created virtual environment. try: # Path to the pip executable in the virtual environment pip_executable = os.path.join(env_dir, \'bin\', \'pip\') if os.name != \'nt\' else os.path.join(env_dir, \'Scripts\', \'pip.exe\') # Ensure pip is available subprocess.check_call([pip_executable, \'install\', \'--upgrade\', \'pip\']) # Install the dependencies subprocess.check_call([pip_executable, \'install\', \'-r\', requirements_file]) print(\\"Dependencies installed\\") except Exception as e: print(f\\"Failed to install dependencies: {e}\\") sys.exit(1) def create_executable_zip(output_file=\'project_archive.pyz\'): Package the current directory into an executable zip file. try: zipapp.create_archive(\'.\', output_file) print(f\\"Executable zip archive created: {output_file}\\") except Exception as e: print(f\\"Failed to create executable zip archive: {e}\\") sys.exit(1) def main(): create_virtual_environment() install_dependencies() create_executable_zip() if __name__ == \'__main__\': main()"},{"question":"Coding Assessment Question # Objective Design a function that generates and visualizes customized color palettes using the seaborn `hls_palette` method. # Task Implement a Python function `generate_palette(n_colors, lightness, saturation, hue_start, as_cmap)` that: 1. Generates a color palette using seaborn\'s `hls_palette` method. 2. Visualizes the palette using `seaborn.palplot`. 3. Optionally, returns the palette as a continuous colormap. # Parameters - `n_colors` (int): Number of colors in the palette. (Constraints: 2 <= n_colors <= 20) - `lightness` (float): Lightness of the colors, ranging from 0 to 1. (Constraints: 0.0 < lightness <= 1.0) - `saturation` (float): Saturation of the colors, ranging from 0 to 1. (Constraints: 0.0 < saturation <= 1.0) - `hue_start` (float): Starting point of the hue sampling, ranging from 0 to 1. (Constraints: 0.0 <= hue_start <= 1.0) - `as_cmap` (bool): If True, the function should return the palette as a continuous colormap. # Function Signature ```python def generate_palette(n_colors: int, lightness: float, saturation: float, hue_start: float, as_cmap: bool = False) -> None: pass ``` # Expected Output 1. A seaborn plot displaying the generated color palette. 2. If `as_cmap` is `True`, return the palette as a continuous colormap. Otherwise, the function should only plot the palette and return `None`. # Constraints - Ensure that the parameters are within the specified constraints. If they are not, raise a `ValueError` with a descriptive error message. - Utilize seaborn\'s `palplot` function to visualize the palette and `hls_palette` for palette generation. - The function should be efficient and handle edge cases where parameters might be at their boundary values. # Example Usage ```python # Example 1: Generate a palette with 8 colors, moderate lightness and saturation, starting hue at 0.5, without returning a colormap generate_palette(8, 0.5, 0.7, 0.5, False) # Example 2: Generate a continuous colormap with 6 colors, high lightness, and low saturation palette = generate_palette(6, 0.9, 0.3, 0.2, True) ``` # Example Output 1. For Example 1, a plot displaying the palette with 8 colors should be shown. 2. For Example 2, a continuous colormap object should be returned, which can be used in further visualizations. This question requires a solid understanding of seaborn and how to work with color palettes, making it a comprehensive assessment of the students\' skills.","solution":"import seaborn as sns import matplotlib.pyplot as plt from matplotlib.colors import LinearSegmentedColormap def generate_palette(n_colors: int, lightness: float, saturation: float, hue_start: float, as_cmap: bool = False): Generates and visualizes a customized color palette using seaborn\'s hls_palette method. Optionally returns the palette as a continuous colormap. Parameters: n_colors (int): Number of colors in the palette. (2 <= n_colors <= 20) lightness (float): Lightness of the colors, ranging from 0 to 1. (0.0 < lightness <= 1.0) saturation (float): Saturation of the colors, ranging from 0 to 1. (0.0 < saturation <= 1.0) hue_start (float): Starting point of the hue sampling, ranging from 0 to 1. (0.0 <= hue_start <= 1.0) as_cmap (bool): If True, returns the palette as a continuous colormap. Returns: None or LinearSegmentedColormap: Returns a colormap object if as_cmap is True, otherwise returns None. if not (2 <= n_colors <= 20): raise ValueError(\\"n_colors must be between 2 and 20\\") if not (0.0 < lightness <= 1.0): raise ValueError(\\"lightness must be greater than 0 and less than or equal to 1\\") if not (0.0 < saturation <= 1.0): raise ValueError(\\"saturation must be greater than 0 and less than or equal to 1\\") if not (0.0 <= hue_start <= 1.0): raise ValueError(\\"hue_start must be between 0 and 1\\") # Generate the color palette palette = sns.hls_palette(n_colors=n_colors, l=lightness, s=saturation, h=hue_start) # Visualize the palette sns.palplot(palette) plt.show() if as_cmap: cmap = LinearSegmentedColormap.from_list(\\"custom_hls\\", palette) return cmap return None"},{"question":"# Advanced Asynchronous Programming with Asyncio Objective: Design and implement a Python function that performs multiple asynchronous tasks efficiently while ensuring proper error handling, logging, and concurrency management. Problem Statement: You are tasked with simulating a simplified version of a web scraping task using the asyncio library. This simulation involves: 1. Fetching data from multiple URLs (simulated with asynchronous sleeper functions). 2. Processing the fetched data asynchronously. 3. Aggregating results and handling potential errors gracefully. Detailed Requirements: 1. Implement the function `fetch_data(url: str) -> int`, which simulates fetching data from a URL by asynchronously sleeping for a random duration (between 1 to 5 seconds) and then returning the length of the URL string as fetched data. 2. Implement the function `process_data(data: int) -> str`, which processes the fetched data by asynchronously sleeping for a duration proportional to the input data (specifically, `data % 3 + 1` seconds) and then returning a string indicating that the data has been processed. 3. Implement the main function `execute_tasks(urls: List[str]) -> Dict[str, str]` which should: - Fetch data from all given URLs concurrently using asyncio. - Process each fetched data concurrently. - Collect and log any exceptions that occur during data fetching or processing. - Aggregate the results in a dictionary mapping each URL to its processed result. - Use asyncio\'s debug mode to detect and log any issues with forgotten coroutines or unhandled exceptions. Input: - A list of strings, `urls`, where each string is a URL to be fetched. Output: - A dictionary where each key is a URL from the input list, and the corresponding value is the processed result of its fetched data. Constraints: - Utilize asyncio\'s debugging capabilities to ensure proper async operations. - Ensure that the overall execution should not exceed 20 seconds regardless of the number of URLs. Example: ```python import asyncio import logging import random async def fetch_data(url: str) -> int: await asyncio.sleep(random.randint(1, 5)) return len(url) async def process_data(data: int) -> str: await asyncio.sleep(data % 3 + 1) return f\\"Processed data of length {data}\\" async def execute_tasks(urls): # Implement the logic here. ... # Simulated URLs urls = [\\"https://example.com\\", \\"https://openai.com\\", \\"https://python.org\\"] result = asyncio.run(execute_tasks(urls), debug=True) print(result) ``` Expected output (actual timing and order may vary): ``` DEBUG:asyncio:Executing: fetch_data(\'https://example.com\') DEBUG:asyncio:Executing: fetch_data(\'https://openai.com\') DEBUG:asyncio:Executing: fetch_data(\'https://python.org\') DEBUG:asyncio:Fetched data from \'https://example.com\' DEBUG:asyncio:Fetching data from \'https://openai.com\' DEBUG:asyncio:Processing data from \'https://example.com\' DEBUG:asyncio:Processing data from \'https://openai.com\' DEBUG:asyncio:Processing data from \'https://python.org\' {\'https://example.com\': \'Processed data of length 19\', \'https://openai.com\': \'Processed data of length 18\', \'https://python.org\': \'Processed data of length 16\'} ``` Ensure proper logging and handling of tasks and exceptions to facilitate easy debugging and correctness.","solution":"import asyncio import logging import random from typing import List, Dict # Setup logging logging.basicConfig(level=logging.DEBUG) logger = logging.getLogger(__name__) async def fetch_data(url: str) -> int: try: await asyncio.sleep(random.randint(1, 5)) data_length = len(url) logger.debug(f\\"Fetched data from \'{url}\' of length {data_length}\\") return data_length except Exception as e: logger.error(f\\"Error fetching data from \'{url}\': {e}\\") raise async def process_data(data: int) -> str: try: await asyncio.sleep(data % 3 + 1) result = f\\"Processed data of length {data}\\" logger.debug(result) return result except Exception as e: logger.error(f\\"Error processing data of length \'{data}\': {e}\\") raise async def execute_tasks(urls: List[str]) -> Dict[str, str]: results = {} async def handle_task(url): try: data = await fetch_data(url) processed_result = await process_data(data) results[url] = processed_result except Exception as e: logger.error(f\\"Error handling task for \'{url}\': {e}\\") results[url] = f\\"Error: {e}\\" await asyncio.gather(*(handle_task(url) for url in urls)) return results # Example usage: asyncio.run should not be inside a script example #urls = [\\"https://example.com\\", \\"https://openai.com\\", \\"https://python.org\\"] #result = asyncio.run(execute_tasks(urls), debug=True) #print(result)"},{"question":"Problem Statement You are tasked with embedding a Python interpreter into a custom C application. As part of this project, you need to create a Python script manager in Python that can execute Python code strings and handle simple and interactive commands efficiently. Specifically, you need to implement the following functionalities: 1. **Run Simple String**: - Implement a function `run_simple_string(command: str) -> int` that takes a Python code string as input and executes it. Return `0` if the execution is successful and `-1` if an exception occurs. 2. **Run Interactive Loop**: - Implement a function `run_interactive_loop(filename: str) -> int` that takes the path to a file containing Python statements and executes these statements interactively until EOF is reached. Return `0` at EOF or a negative number upon failure. 3. **Run File with Context**: - Implement a function `run_file_with_context(filename: str, globals: dict, locals: dict) -> Any` that takes a path to a Python file, along with global and local execution contexts (dictionaries), and executes the file\'s content with the given contexts. Return the result of the file\'s execution. Function Signatures ```python def run_simple_string(command: str) -> int: pass def run_interactive_loop(filename: str) -> int: pass def run_file_with_context(filename: str, globals: dict, locals: dict) -> Any: pass ``` Constraints and Requirements: - The `run_simple_string` function should utilize `PyRun_SimpleString` or similar functions. - The `run_interactive_loop` function should utilize `PyRun_InteractiveLoop` or similar functions. - The `run_file_with_context` function should utilize `PyRun_File` or similar functions based on the documentation provided. Example Usage ```python # Example for run_simple_string result = run_simple_string(\\"print(\'Hello World\')\\") assert result == 0 # Example for run_interactive_loop result = run_interactive_loop(\\"interactive_commands.txt\\") assert result == 0 # Example for run_file_with_context globals = {\\"x\\": 10, \\"y\\": 20} locals = {} result = run_file_with_context(\\"sample_script.py\\", globals, locals) assert result is not None ``` Notes: - Ensure proper error handling for file operations and code execution. - Maintain the structure and behaviour as expected from the high-level functions provided in the documentation. - Consider edge cases such as invalid file paths or syntax errors within the code strings.","solution":"def run_simple_string(command: str) -> int: Executes a simple Python code string. :param command: A string containing the Python code to execute. :return: 0 if successful, -1 if an exception occurs. try: exec(command) return 0 except Exception as e: print(f\\"Exception occurred: {e}\\") return -1 def run_interactive_loop(filename: str) -> int: Executes Python statements from a file interactively until EOF. :param filename: Path to the file containing Python statements. :return: 0 at EOF, negative number upon failure. try: with open(filename, \'r\') as file: for line in file: exec(line) return 0 except FileNotFoundError: print(f\\"File not found: {filename}\\") return -2 except Exception as e: print(f\\"Exception occurred: {e}\\") return -1 def run_file_with_context(filename: str, globals: dict, locals: dict) -> any: Executes the content of a Python file with given global and local contexts. :param filename: Path to the Python file. :param globals: A dictionary representing the global execution context. :param locals: A dictionary representing the local execution context. :return: Result of the file\'s execution. try: with open(filename, \'r\') as file: content = file.read() exec(content, globals, locals) return locals.get(\'__builtins__\') except FileNotFoundError: print(f\\"File not found: {filename}\\") return None except Exception as e: print(f\\"Exception occurred: {e}\\") return None"},{"question":"You are provided with a list of file paths, and you need to implement a function `process_paths(paths, base_path)` that performs the following tasks: 1. Normalize all the paths. 2. Determine the base names of the paths. 3. Construct absolute paths based on `base_path`. 4. Check which paths exist. 5. Return a dictionary with the base names as keys and a tuple `(absolute_path, exists)` as values. Function Signature ```python def process_paths(paths: list, base_path: str) -> dict: # Write your implementation here ``` Input - `paths` (list): A list of strings representing file paths. - `base_path` (str): A string representing the base directory to construct absolute paths. Output - A dictionary where each key is the base name of a path, and the corresponding value is a tuple `(absolute_path, exists)`, where `absolute_path` is the normalized absolute path, and `exists` is a boolean indicating whether the path exists. Constraints - All paths in `paths` and `base_path` are valid path-like objects. - The number of paths in the list is at most 10^3. Example ```python paths = [\\"./file.txt\\", \\"../folder\\", \\"/usr/local/bin\\"] base_path = \\"/home/user\\" result = process_paths(paths, base_path) # Expected result (assuming current working directory is /home/user and some paths existence): # { # \\"file.txt\\": (\\"/home/user/file.txt\\", True), # \\"folder\\": (\\"/home/user/folder\\", False), # \\"bin\\": (\\"/usr/local/bin\\", True) # } ``` Notes - Ensure you handle both relative and absolute paths correctly. - Paths existence can vary depending on the system; for testing, it\'s acceptable to simulate the existence check.","solution":"import os def process_paths(paths, base_path): Normalize paths, determine base names, construct absolute paths, and check for the existence of the paths. Args: paths (list): List of strings representing file paths. base_path (str): The base directory to construct absolute paths. Returns: dict: Dictionary where each key is the base name of the path, and the value is a tuple (absolute_path, exists), where exists is a boolean. result = {} for path in paths: # Normalize and create absolute path normalized_path = os.path.normpath(os.path.join(base_path, path)) abs_path = os.path.abspath(normalized_path) # Get base name base_name = os.path.basename(abs_path) # Check existence exists = os.path.exists(abs_path) # Populate result dictionary result[base_name] = (abs_path, exists) return result"},{"question":"<|Analysis Begin|> The provided documentation describes the `encoders` module in the legacy email API in Python 3.10. The module includes four encoding functions used for encoding the payloads of `Message` objects for transport through email servers. The functions are: 1. `encode_quopri(msg)`: Encodes the payload into quoted-printable form and sets the `Content-Transfer-Encoding` header to \\"quoted-printable\\". 2. `encode_base64(msg)`: Encodes the payload into base64 form and sets the `Content-Transfer-Encoding` header to \\"base64\\". 3. `encode_7or8bit(msg)`: Sets the `Content-Transfer-Encoding` header to either \\"7bit\\" or \\"8bit\\" based on the payload data without modifying the payload. 4. `encode_noop(msg)`: Does nothing to the payload and does not set the `Content-Transfer-Encoding` header. Based on this information, students can be asked to create a function that utilizes these encoders and demonstrate their understanding of encoding message objects. <|Analysis End|> <|Question Begin|> # Email Encoder Function Implementation You are tasked with creating a function that encodes the payloads of email `Message` objects using different methods provided by the legacy `encoders` module in the Python 3.10 email package. Objective: Create a function named `apply_email_encoder` that takes in a `Message` object and an encoding type (one of \\"quopri\\", \\"base64\\", \\"7or8bit\\", \\"noop\\") and applies the corresponding encoding to the payload of the message object. Input: - A `Message` object with a payload (can be imported using `from email.message import Message`). - A string representing the encoding type, which can be one of the following: - `\\"quopri\\"` - `\\"base64\\"` - `\\"7or8bit\\"` - `\\"noop\\"` Output: - The function should return the modified `Message` object with the appropriate encoding applied. Constraints: - The function should raise a `ValueError` if an invalid encoding type is provided. - The function should handle the scenario where the payload of the `Message` object is intended for transport and should be encoded accordingly. Example Usage: ```python from email.message import Message def apply_email_encoder(msg: Message, encoding_type: str) -> Message: # Your implementation here # Example Message msg = Message() msg.set_payload(\\"This is a test email payload.\\") # Apply base64 encoding encoded_msg = apply_email_encoder(msg, \\"base64\\") print(encoded_msg.get_payload()) # Should print the encoded payload in base64 form # Apply quoted-printable encoding encoded_msg = apply_email_encoder(msg, \\"quopri\\") print(encoded_msg.get_payload()) # Should print the encoded payload in quoted-printable form ``` Notes: - You can use `email.encoders.encode_quopri()`, `email.encoders.encode_base64()`, `email.encoders.encode_7or8bit()`, and `email.encoders.encode_noop()` directly within your function. - Ensure the function alters the `Content-Transfer-Encoding` header appropriately based on the encoding type. - It is recommended to review the documentation for these encoders to understand their behaviors and implementations.","solution":"from email.message import Message import email.encoders as encoders def apply_email_encoder(msg: Message, encoding_type: str) -> Message: Encodes the payload of a Message object using the specified encoding type. Parameters: - msg: Message object from the email module. - encoding_type: string representing the encoding type (\\"quopri\\", \\"base64\\", \\"7or8bit\\", \\"noop\\"). Returns: - Message object with the payload encoded. Raises: - ValueError: if the encoding_type is invalid. if encoding_type == \\"quopri\\": encoders.encode_quopri(msg) elif encoding_type == \\"base64\\": encoders.encode_base64(msg) elif encoding_type == \\"7or8bit\\": encoders.encode_7or8bit(msg) elif encoding_type == \\"noop\\": encoders.encode_noop(msg) else: raise ValueError(f\\"Invalid encoding type: {encoding_type}\\") return msg"},{"question":"MemoryView Manipulation and Operations # Objective: Demonstrate your understanding of memoryview objects in Python by performing various operations that manipulate memory directly. # Problem Statement: You are given a list of integers and a few binary operations that must be performed directly on memory buffers to improve performance. You need to implement a function that: 1. Uses a `memoryview` to manipulate the data without creating unnecessary copies. 2. Ensures modifications are reflected correctly in the original buffer. 3. Performs specific operations such as reversing the buffer, converting the buffer to uppercase (if it contains characters), or summing up its contents. # Your Task: Implement the function `operate_on_buffer(data: bytes, operation: str) -> bytes` which takes a byte buffer (`data`) and a string specifying the operation (`operation`). Parameters: - `data` (bytes): A byte buffer representing raw data. - `operation` (str): A string specifying the operation to perform. It can be one of: - \\"reverse\\": Reverse the buffer. - \\"uppercase\\": Convert all lowercase letters in the buffer to uppercase (if it contains text). - \\"sum\\": Calculate the sum of all bytes in the buffer and return a buffer with this single integer sum. Returns: - bytes: The modified buffer after performing the specified operation. Constraints: - The function should directly manipulate the memoryview of the original buffer. - The size of `data` will not exceed 10^6 bytes. - The `operation` will always be one of the specified options. # Example: ```python data = b\'hello world\' operation = \'uppercase\' print(operate_on_buffer(data, operation)) # Output: b\'HELLO WORLD\' data = b\'x01x02x03\' operation = \'sum\' print(operate_on_buffer(data, operation)) # Output: b\'x06\' data = b\'abcdef\' operation = \'reverse\' print(operate_on_buffer(data, operation)) # Output: b\'fedcba\' ``` # Hints: - Utilize `memoryview` to avoid copying data during manipulation. - Directly manipulate the memory where applicable (e.g., reversing in place). - Ensure your function handles different types of data in the buffer correctly. # Performance Requirements: - Your solution should be efficient, operating within O(n) where n is the length of the buffer.","solution":"def operate_on_buffer(data: bytes, operation: str) -> bytes: Perform the specified operation on the byte buffer using memoryview. Args: - data (bytes): The byte buffer to manipulate. - operation (str): The operation to perform (\\"reverse\\", \\"uppercase\\", \\"sum\\"). Returns: - bytes: The modified buffer after performing the operation. mv = memoryview(data) if operation == \\"reverse\\": return bytes(mv[::-1]) elif operation == \\"uppercase\\": # We can\'t directly change the bytes in place since they are immutable. # Make a copy and then change. return bytes([byte if byte < 97 or byte > 122 else byte - 32 for byte in mv]) elif operation == \\"sum\\": # Calculate sum of all bytes return bytes([sum(mv)]) else: raise ValueError(\\"Unsupported operation\\")"},{"question":"# Python SQLite Coding Task Objective Design and implement a Python function using the `sqlite3` package that demonstrates: 1. Connecting to an SQLite database. 2. Creating tables and inserting data. 3. Querying data with various SQL operations. 4. Creating a user-defined aggregate function. 5. Implementing transaction control. Problem Statement You need to manage a small inventory database for a fictional bookstore. Implement the following steps: 1. Connect to an SQLite database named `bookstore.db`. The database should be created if it does not exist. 2. Create a table called `books` with the following schema: - `id` INTEGER PRIMARY KEY AUTOINCREMENT - `title` TEXT - `author` TEXT - `year` INTEGER - `price` REAL 3. Insert the following book records into the `books` table: - (\'To Kill a Mockingbird\', \'Harper Lee\', 1960, 10.99) - (\'1984\', \'George Orwell\', 1949, 8.99) - (\'The Great Gatsby\', \'F. Scott Fitzgerald\', 1925, 12.99) - (\'The Catcher in the Rye\', \'J.D. Salinger\', 1951, 9.99) 4. Query all books in the database and return a list of tuples each containing the book `title` and `author`. 5. Create a user-defined aggregate function `average_price` that calculates the average price of books. 6. Query the average price of all books using the `average_price` function. 7. Implement transaction handling so that inserting books and querying results are done within a transaction. Function Signature ```python def manage_bookstore(): # Your code here ``` Expected Output - The function should print the list of tuples with book `title` and `author`. - The function should print the average price of books. Constraints and Requirements - Use placeholders (parameterized queries) for inserting data to prevent SQL injection. - Handle any `sqlite3` exceptions that may occur during database operations. - Ensure that all database connections are properly closed. Example Output ``` [(\'To Kill a Mockingbird\', \'Harper Lee\'), (\'1984\', \'George Orwell\'), (\'The Great Gatsby\', \'F. Scott Fitzgerald\'), (\'The Catcher in the Rye\', \'J.D. Salinger\')] Average price of books: 10.74 ```","solution":"import sqlite3 def manage_bookstore(): # Connect to SQLite database conn = sqlite3.connect(\'bookstore.db\') try: # Create a cursor object cursor = conn.cursor() # Create books table cursor.execute(\'\'\' CREATE TABLE IF NOT EXISTS books ( id INTEGER PRIMARY KEY AUTOINCREMENT, title TEXT, author TEXT, year INTEGER, price REAL ) \'\'\') # Insert book records books = [ (\'To Kill a Mockingbird\', \'Harper Lee\', 1960, 10.99), (\'1984\', \'George Orwell\', 1949, 8.99), (\'The Great Gatsby\', \'F. Scott Fitzgerald\', 1925, 12.99), (\'The Catcher in the Rye\', \'J.D. Salinger\', 1951, 9.99) ] cursor.executemany(\'\'\' INSERT INTO books (title, author, year, price) VALUES (?, ?, ?, ?) \'\'\', books) # Commit the transaction conn.commit() # Query all books cursor.execute(\'SELECT title, author FROM books\') books_list = cursor.fetchall() print(books_list) # Create user-defined aggregate function class AveragePrice: def __init__(self): self.count = 0 self.total = 0.0 def step(self, value): self.count += 1 self.total += value def finalize(self): return self.total / self.count if self.count > 0 else 0 conn.create_aggregate(\'average_price\', 1, AveragePrice) # Query average price of all books cursor.execute(\'SELECT average_price(price) FROM books\') average_price = cursor.fetchone()[0] print(f\'Average price of books: {average_price:.2f}\') except sqlite3.Error as e: print(f\'SQLite error: {e}\') finally: # Close the database connection conn.close()"},{"question":"# Python Coding Assessment Question Objective You are tasked with creating a function that uses the `zipapp` module to package a directory containing Python code into a standalone, executable zip archive. This task will assess your understanding of file handling, Python API usage, and dependency management in Python. Problem Statement Write a function `package_to_zipapp(app_dir, output_filename, interpreter, main_function, include_pattern)` that: 1. Takes as input: - `app_dir` (str): Path to the application directory containing the Python code to be archived. - `output_filename` (str): The name of the output archive file. - `interpreter` (str): Path to the Python interpreter to be used in the archive\'s shebang line. - `main_function` (str): The main function to be executed from the archive, in the form \\"pkg.mod:fn\\". - `include_pattern` (str): A pattern for the filenames to be included in the archive. For example, \\"*.py\\" to include only Python files. 2. Packages the specified `app_dir` into a zip archive with the specified `output_filename`. 3. Ensures the archive is executable with the specified `interpreter`. 4. Sets the `main_function` as the executable entry point for the archive. 5. Only includes files matching `include_pattern` in the archive. 6. Compresses the archive to reduce its size. Your function should ensure the archive is executable directly from the command line. If any error occurs during the execution, your function should raise an appropriate exception with a clear error message. Constraints - `app_dir` should be an existing directory containing Python files. - `output_filename` should end with \\".pyz\\". - The function should ensure proper error handling and resource management. Function Signature ```python import zipapp import fnmatch import os from pathlib import Path def package_to_zipapp(app_dir: str, output_filename: str, interpreter: str, main_function: str, include_pattern: str) -> None: pass ``` Expected Usage ```python try: package_to_zipapp(\'myapp\', \'myapp.pyz\', \'/usr/bin/python3\', \'myapp:main\', \'*.py\') print(\\"Archive created successfully!\\") except Exception as e: print(f\\"An error occurred: {e}\\") ``` **Note**: Make sure to include detailed docstrings and comments in your code for readability and maintainability.","solution":"import zipapp import fnmatch import os from pathlib import Path def package_to_zipapp(app_dir: str, output_filename: str, interpreter: str, main_function: str, include_pattern: str) -> None: Packages a directory containing Python code into a standalone executable zip archive. Parameters: app_dir (str): Path to the application directory containing the Python code to be archived. output_filename (str): The name of the output archive file. interpreter (str): Path to the Python interpreter to be used in the archive\'s shebang line. main_function (str): The main function to be executed from the archive, in the form \\"pkg.mod:fn\\". include_pattern (str): A pattern for the filenames to be included in the archive. For example, \\"*.py\\" to include only Python files. Raises: ValueError: If app_dir is not a directory. ValueError: If output_filename does not end with \\".pyz\\". Exception: If an error occurs during packaging. # Check if the application directory exists if not os.path.isdir(app_dir): raise ValueError(f\\"Provided app_dir \'{app_dir}\' is not a directory.\\") # Check if output filename has the correct extension if not output_filename.endswith(\'.pyz\'): raise ValueError(\\"The output filename must end with \'.pyz\'\\") def include_filter(filename: str) -> bool: Filter function to include only files matching the given pattern. Parameters: filename (str): The file name to be checked. Returns: bool: True if the file matches the pattern, False otherwise. return fnmatch.fnmatch(filename, include_pattern) try: # Create the archive zipapp.create_archive( app_dir, target=output_filename, interpreter=interpreter, main=main_function, filter=include_filter, compressed=True ) except Exception as e: raise Exception(f\\"Failed to create the archive: {e}\\")"},{"question":"# Question: Create and Parse Email Messages with Attachments You are required to write a Python program that demonstrates the creation and parsing of email messages using the \\"email\\" package. The program should be able to create an email with both plain text and HTML content and an attachment. Additionally, it should be able to parse the email and extract its components. Requirements: 1. **Email Creation**: - Create an email with the following attributes: - Subject: \\"Test Email\\" - From: \\"sender@example.com\\" - To: \\"receiver@example.com\\" - Plain text body: \\"This is a plain text body.\\" - HTML body: \\"<html><body><h1>This is an HTML body.</h1></body></html>\\" - Attach a text file with the content \\"This is an attachment.\\" 2. **Email Parsing**: - Parse the created email and extract: - Subject - From - To - Plain text body - HTML body - Attachment content Input Format: - No direct user inputs are needed. All data will be hardcoded as specified in the requirements. Output Format: - Print the parsed components of the email in the following format: ``` Subject: Test Email From: sender@example.com To: receiver@example.com Plain Text Body: This is a plain text body. HTML Body: <html><body><h1>This is an HTML body.</h1></body></html> Attachment Content: This is an attachment. ``` Constraints: - Use only the \\"email\\" package for email creation and parsing. - Ensure that all MIME parts are correctly set and handled. Function Signature: ```python import email import email.mime.multipart import email.mime.text import email.mime.base from email import encoders def create_email(): # Implement email creation here pass def parse_email(email_message): # Implement email parsing here pass def main(): msg = create_email() parsed_result = parse_email(msg) for key, value in parsed_result.items(): print(f\\"{key}: {value}\\") if __name__ == \\"__main__\\": main() ``` # Detailed Explanation: - **create_email()**: This function should create an email message with the specified attributes using the \\"email\\" package. It should use appropriate MIME types for plain text, HTML content, and attachments. - **parse_email(email_message)**: This function should take the created email message and extract its components (subject, from, to, plain text body, HTML body, and attachment content). It should return these components in a dictionary. - **main()**: This function should manage the workflow by calling `create_email()` to create the email and then `parse_email()` to parse it. It should then print the parsed components.","solution":"import email import email.mime.multipart import email.mime.text import email.mime.base from email import encoders from email import policy from email.parser import BytesParser def create_email(): # Create the root message msg = email.mime.multipart.MIMEMultipart() msg[\'Subject\'] = \\"Test Email\\" msg[\'From\'] = \\"sender@example.com\\" msg[\'To\'] = \\"receiver@example.com\\" # Attach the plain text part text_part = email.mime.text.MIMEText(\\"This is a plain text body.\\", \\"plain\\") msg.attach(text_part) # Attach the HTML part html_part = email.mime.text.MIMEText(\\"<html><body><h1>This is an HTML body.</h1></body></html>\\", \\"html\\") msg.attach(html_part) # Attach a file attachment_content = \\"This is an attachment.\\" attachment_part = email.mime.base.MIMEBase(\\"application\\", \\"octet-stream\\") attachment_part.set_payload(attachment_content) encoders.encode_base64(attachment_part) attachment_part.add_header(\\"Content-Disposition\\", \\"attachment; filename=\'attachment.txt\'\\") msg.attach(attachment_part) return msg.as_bytes() def parse_email(email_bytes): email_message = BytesParser(policy=policy.default).parsebytes(email_bytes) parsed_components = {} parsed_components[\\"Subject\\"] = email_message[\\"Subject\\"] parsed_components[\\"From\\"] = email_message[\\"From\\"] parsed_components[\\"To\\"] = email_message[\\"To\\"] plain_text_body = None html_body = None attachment_content = None for part in email_message.walk(): content_type = part.get_content_type() if content_type == \\"text/plain\\" and part.get(\\"Content-Disposition\\") is None: plain_text_body = part.get_payload(decode=True).decode() elif content_type == \\"text/html\\" and part.get(\\"Content-Disposition\\") is None: html_body = part.get_payload(decode=True).decode() elif part.get(\\"Content-Disposition\\") is not None: attachment_content = part.get_payload(decode=True).decode() parsed_components[\\"Plain Text Body\\"] = plain_text_body parsed_components[\\"HTML Body\\"] = html_body parsed_components[\\"Attachment Content\\"] = attachment_content return parsed_components def main(): email_bytes = create_email() parsed_result = parse_email(email_bytes) for key, value in parsed_result.items(): print(f\\"{key}: {value}\\") if __name__ == \\"__main__\\": main()"},{"question":"You are tasked with building an out-of-core learning system using scikit-learn. This system should be able to handle extremely large datasets that cannot fit into the computer\'s main memory (RAM). The task involves creating a pipeline that: 1. Streams data from a large text file. 2. Extracts features using the `HashingVectorizer`. 3. Performs incremental learning using an `SGDClassifier`. # Requirements 1. **Input:** - A path to a large text file containing labeled instances. Each line in the file represents an instance, formatted as `<label>t<text>` (tab-separated). 2. **Output:** - The program should output the classification accuracy after processing the entire file. 3. **Constraints:** - The system should be memory efficient, i.e., it should not load the entire dataset into memory at once. - Ensure that the mini-batch size for incremental learning is configurable. 4. **Performance Requirements:** - Optimize for both time and space complexity. Ensure that feature extraction and incremental learning are done efficiently, making use of appropriate batch sizes. # Example Usage ```python def out_of_core_learning(file_path: str, minibatch_size: int) -> float: # Your implementation here pass # Example call accuracy = out_of_core_learning(\'large_text_data.txt\', minibatch_size=1000) print(f\'Classification Accuracy: {accuracy}\') ``` # Detailed Steps 1. **Streaming Data:** - Implement a generator that reads the text file line-by-line to keep memory usage low. 2. **Feature Extraction:** - Use `HashingVectorizer` from scikit-learn to convert text data into a format suitable for learning. 3. **Incremental Learning:** - Use `SGDClassifier` with `partial_fit` for the learning process. Remember to pass all possible classes in the first call to `partial_fit`. # Full Specifications - Define a function `out_of_core_learning(file_path: str, minibatch_size: int) -> float`. - The function should read the data from the specified file path. - Implement feature extraction using `HashingVectorizer`. - Train the `SGDClassifier` incrementally using mini-batches of the specified size. - Return the overall classification accuracy after processing the entire dataset. # Hint Consider dividing the task into manageable functions, such as `stream_data`, `extract_features`, and `incremental_train`. This will help in organizing the code and making it readable.","solution":"import os import numpy as np from sklearn.feature_extraction.text import HashingVectorizer from sklearn.linear_model import SGDClassifier from sklearn.metrics import accuracy_score def stream_data(file_path): Generator to stream data from a file, one line at a time. Each line is expected to be in the format: <label>t<text>. with open(file_path, \'r\') as f: for line in f: label, text = line.strip().split(\'t\') yield label, text def extract_features(vectorizer, texts): Extract features from texts using the provided HashingVectorizer. return vectorizer.transform(texts) def incremental_train(classifier, vectorizer, file_path, minibatch_size): Train the classifier incrementally using data streamed from the file. classes = np.array([0, 1]) # Assuming binary classification batch_texts = [] batch_labels = [] for label, text in stream_data(file_path): batch_texts.append(text) batch_labels.append(int(label)) if len(batch_texts) >= minibatch_size: X_train = extract_features(vectorizer, batch_texts) y_train = np.array(batch_labels) classifier.partial_fit(X_train, y_train, classes=classes) batch_texts = [] batch_labels = [] # Process remaining data if len(batch_texts) > 0: X_train = extract_features(vectorizer, batch_texts) y_train = np.array(batch_labels) classifier.partial_fit(X_train, y_train, classes=classes) def out_of_core_learning(file_path: str, minibatch_size: int) -> float: Perform out-of-core learning using SGDClassifier and HashingVectorizer. Returns the overall classification accuracy after processing the entire file. vectorizer = HashingVectorizer() classifier = SGDClassifier() incremental_train(classifier, vectorizer, file_path, minibatch_size) # To evaluate performance, the stream_data function is reused to collect all data for scoring all_texts = [] all_labels = [] for label, text in stream_data(file_path): all_texts.append(text) all_labels.append(int(label)) X_test = extract_features(vectorizer, all_texts) y_test = np.array(all_labels) y_pred = classifier.predict(X_test) return accuracy_score(y_test, y_pred)"},{"question":"**Objective:** You are tasked with creating a function that processes a list of tuples containing user data. Each tuple consists of a name (string) and an age (integer). The function must: 1. Handle different types of errors that might occur during processing. 2. Demonstrate your understanding of exceptions in Python by handling multiple types of exceptions appropriately. 3. Ensure resources are cleaned up correctly after processing. **Function Signature:** ```python def process_user_data(user_data: list) -> list: pass ``` **Input:** - `user_data`: A list of tuples. Each tuple contains: - A `name` (string): The name of the user. - An `age` (integer): The age of the user. **Output:** - Returns a list of processed user data. Each element in the output list is a dictionary with keys: - \'name\': The name of the user. - \'age\': The age of the user. - \'valid\': A boolean indicating if the age is a valid positive integer. **Specifications and Constraints:** 1. The function should catch and handle the following exceptions: - `ValueError`: If the age cannot be converted to an integer or is a negative integer. - `TypeError`: If the user data format is incorrect (e.g., not a tuple or missing elements). 2. If an exception is caught, the function should add an entry to the output list with: - `\'name\'`: Set to `None`. - `\'age\'`: Set to `None`. - `\'valid\'`: Set to `False`. 3. Implement a clean-up action at the end of the function using a `finally` block to print \\"Processing complete.\\" 4. Ensure that any invalid entries are logged using a custom exception called `InvalidEntryError`. **Example:** ```python user_data = [(\\"Alice\\", 30), (\\"Bob\\", \\"thirty\\"), (True, 25), (\\"Charlie\\", -5)] output = process_user_data(user_data) print(output) # Expected Output: # [ # {\'name\': \'Alice\', \'age\': 30, \'valid\': True}, # {\'name\': None, \'age\': None, \'valid\': False}, # {\'name\': None, \'age\': None, \'valid\': False}, # {\'name\': None, \'age\': None, \'valid\': False} # ] ``` **Note:** You must define the `InvalidEntryError` class and use it to log invalid entries. ```python class InvalidEntryError(Exception): def __init__(self, message): self.message = message super().__init__(self.message) ``` This question assesses your ability to handle exceptions, create user-defined exceptions, and ensure resource clean-up. Implement the function adhering to the guidelines provided.","solution":"class InvalidEntryError(Exception): def __init__(self, message): self.message = message super().__init__(self.message) def process_user_data(user_data: list) -> list: processed_data = [] for entry in user_data: try: if not isinstance(entry, tuple) or len(entry) != 2: raise TypeError(\\"Entry must be a tuple with two elements.\\") name, age = entry if not isinstance(name, str): raise TypeError(\\"First element must be a string representing name.\\") if not isinstance(age, int) or age < 0: raise ValueError(\\"Second element must be a positive integer representing age.\\") processed_data.append({\'name\': name, \'age\': age, \'valid\': True}) except (TypeError, ValueError) as e: error_message = str(e) processed_data.append({\'name\': None, \'age\': None, \'valid\': False}) raise InvalidEntryError(error_message) finally: print(\\"Processing complete.\\") return processed_data"},{"question":"**Email Package Coding Assessment** **Objective**: Design and implement a function that can read an email message from a given byte stream, modify its contents, and serialize it back into a byte stream. Your solution should demonstrate your understanding of various components of the \\"email\\" package, such as parsing, manipulation, and generation of email messages. **Function Signature**: ```python def modify_email(byte_stream: bytes, subject_mod: str, body_mod: str, policy: email.policy.Policy) -> bytes: pass ``` **Parameters**: - `byte_stream` (bytes): The byte stream representing the serialized email message. - `subject_mod` (str): A string to append to the original email\'s subject line. - `body_mod` (str): A string to append to the original email\'s body. - `policy` (email.policy.Policy): A policy object that should be used while parsing and generating the email. **Returns**: - (bytes): The modified email message as a serialized byte stream. **Description**: 1. Parse the given `byte_stream` to create an `EmailMessage` object using the specified `policy`. 2. Append the `subject_mod` string to the existing subject line of the email. 3. Append the `body_mod` string to the existing body of the email. 4. Ensure that any MIME parts within the email are handled appropriately, maintaining the original email structure. 5. Serialize the modified email back into a byte stream using the specified `policy`. 6. Return the resulting byte stream. **Constraints**: - Assume the email contains a simple text/plain body for simplicity. Handling of other MIME types is not required for this assessment. - Emails should be parsed and generated using the same specified `policy`. **Example**: ```python import email from email.policy import default byte_stream = b\\"From: sender@example.comnTo: receiver@example.comnSubject: Original SubjectnnOriginal body content.\\" subject_mod = \\" - Appended\\" body_mod = \\"nAppended to body.\\" policy = default result = modify_email(byte_stream, subject_mod, body_mod, policy) print(result.decode(\'utf-8\')) ``` Output: ``` From: sender@example.com To: receiver@example.com Subject: Original Subject - Appended Original body content. Appended to body. ``` **Notes**: - Your implementation should use the `email` package comprehensively, ensuring correctness as per the provided guidelines. - The function should handle any exceptions that arise during parsing or generation and provide meaningful error messages.","solution":"import email from email.policy import Policy def modify_email(byte_stream: bytes, subject_mod: str, body_mod: str, policy: Policy) -> bytes: Modify the email\'s subject and body according to the parameters provided, and return the modified email as a byte stream. Parameters: - byte_stream (bytes): The byte stream representing the serialized email message. - subject_mod (str): A string to append to the original email\'s subject line. - body_mod (str): A string to append to the original email\'s body. - policy (email.policy.Policy): A policy object that should be used while parsing and generating the email. Returns: - bytes: The modified email message as a serialized byte stream. # Parse the byte stream into an EmailMessage object msg = email.message_from_bytes(byte_stream, policy=policy) # Modify the subject if msg[\'Subject\'] is not None: msg.replace_header(\'Subject\', msg[\'Subject\'] + subject_mod) else: msg[\'Subject\'] = subject_mod # Modify the body if msg.get_content_type() == \'text/plain\': new_body = msg.get_payload() + body_mod msg.set_payload(new_body) # Serialize the modified email back into a byte stream return msg.as_bytes(policy=policy)"},{"question":"# Coding Assignment Question Objective: To test your understanding of Python OOP concepts including class creation, instance methods, class inheritance, and iterator protocols. Problem Statement: You are assigned to create a package that models an inventory system for a small warehouse. Your task is to design a system using classes that can create items, add them to the inventory, track the inventory, and allow for iterating through the inventory items. Additionally, implement a derived class that introduces some specialized behavior. Requirements: 1. **Item Class**: - **Attributes**: - `name` (str): Name of the item. - `quantity` (int): Quantity of the item. - `price` (float): Price per item. - **Methods**: - `__init__(self, name: str, quantity: int, price: float)`: Initializes an item with name, quantity, and price. - `__str__(self) -> str`: Returns a string representation of the item in the form \\"Item(name: {name}, quantity: {quantity}, price: {price:.2f})\\". 2. **Inventory Class**: - **Attributes**: - `items` (list): List of `Item` objects. - **Methods**: - `__init__(self)`: Initializes an empty inventory. - `add_item(self, item: Item)`: Adds an item to the inventory. - `total_value(self) -> float`: Returns the total value of the inventory. - `__iter__(self)`: Returns an iterator for the inventory items. - `__next__(self) -> Item`: Returns the next item from the inventory during iteration. 3. **PerishableItem Class (inherits from Item)**: - **Attributes**: - Inherits attributes from `Item`. - `expiry_date` (str): Expiry date of the perishable item. - **Methods**: - `__init__(self, name: str, quantity: int, price: float, expiry_date: str)`: Initializes a perishable item with name, quantity, price, and expiry date. - `__str__(self) -> str`: Returns a string representation of the perishable item in the form \\"PerishableItem(name: {name}, quantity: {quantity}, price: {price:.2f}, expires on: {expiry_date})\\". Example Usage: ```python # Creating items item1 = Item(\\"Widget\\", 10, 2.5) item2 = PerishableItem(\\"Milk\\", 20, 1.5, \\"2023-12-01\\") # Creating inventory and adding items inventory = Inventory() inventory.add_item(item1) inventory.add_item(item2) # Total value of the inventory print(\\"Total inventory value:\\", inventory.total_value()) # Output should be 55.0 # Iterating through the inventory for item in inventory: print(item) ``` Input: - Ensure that your methods handle basic validation such as appropriate data types for name, quantity, price, and expiry date. Output: - Print statements or string representations should follow the specified formats. Constraints: 1. You must implement all the specified classes and methods as described. 2. Use of external libraries for handling iteration is not allowed; use Python’s built-in capabilities. 3. Ensure your solution is efficient and adheres to the principles of object-oriented design.","solution":"class Item: def __init__(self, name: str, quantity: int, price: float): self.name = name self.quantity = quantity self.price = price def __str__(self) -> str: return f\\"Item(name: {self.name}, quantity: {self.quantity}, price: {self.price:.2f})\\" class Inventory: def __init__(self): self.items = [] self._index = 0 def add_item(self, item: Item): self.items.append(item) def total_value(self) -> float: return sum(item.quantity * item.price for item in self.items) def __iter__(self): self._index = 0 return self def __next__(self) -> Item: if self._index < len(self.items): result = self.items[self._index] self._index += 1 return result else: raise StopIteration class PerishableItem(Item): def __init__(self, name: str, quantity: int, price: float, expiry_date: str): super().__init__(name, quantity, price) self.expiry_date = expiry_date def __str__(self) -> str: return (f\\"PerishableItem(name: {self.name}, quantity: {self.quantity}, \\" f\\"price: {self.price:.2f}, expires on: {self.expiry_date})\\")"},{"question":"**Objective:** You are tasked with implementing a function that processes a list of dictionaries representing students and their grades, performing various operations detailed below: **Function Definition:** ```python def process_student_grades(students): Process a list of student dictionaries, and return a dictionary containing several key metrics. Args: students (list of dict): List of dictionaries where each dictionary contains: - \'name\' (str): The name of the student. - \'grades\' (list of int): List of grades the student has received. Returns: dict: A dictionary with the following keys and corresponding values: - \'average_grades\' (dict): Dictionary where the keys are student names and the values are their average grades. - \'sorted_students\' (list of str): List of student names sorted by their average grades in descending order. - \'grade_counts\' (dict): Dictionary where the keys are student names and the values are counts of grades received by each student. - \'top_student\' (str): The name of the student with the highest average grade. In case of ties, return any one of them. pass ``` **Input Format:** - A list of dictionaries, where each dictionary contains: - `name` (str): The name of the student. - `grades` (list of int): A list of integers representing the grades the student has received. **Output Format:** - A dictionary containing the following keys: - `average_grades`: A dictionary where keys are student names and values are their average grades. - `sorted_students`: A list of student names sorted by their average grades in descending order. - `grade_counts`: A dictionary where keys are student names and values are the counts of grades they have received. - `top_student`: The name of the student with the highest average grade. **Constraints:** 1. Each student has at least one grade. 2. There are at least two students in the list. **Example:** ```python students = [ {\'name\': \'Alice\', \'grades\': [90, 85, 78]}, {\'name\': \'Bob\', \'grades\': [82, 79, 88, 92]}, {\'name\': \'Charlie\', \'grades\': [95, 100, 92]} ] result = process_student_grades(students) # Expected result: # { # \'average_grades\': {\'Alice\': 84.33, \'Bob\': 85.25, \'Charlie\': 95.67}, # \'sorted_students\': [\'Charlie\', \'Bob\', \'Alice\'], # \'grade_counts\': {\'Alice\': 3, \'Bob\': 4, \'Charlie\': 3}, # \'top_student\': \'Charlie\' # } ``` **Detailed Requirements:** 1. **average_grades**: Calculate the average grade for each student and round off to 2 decimal places. 2. **sorted_students**: Sort the student names based on their average grades in descending order. 3. **grade_counts**: Count the number of grades each student has received. 4. **top_student**: Identify the student with the highest average grade. To successfully complete this question, students must demonstrate their understanding of: - List and dictionary manipulations. - Functional programming concepts such as using `map()` and `reduce()`. - Sorting techniques using custom key functions. - Basic control flow for handling conditions and loops. - Python built-in functions for rounding and aggregated operations.","solution":"def process_student_grades(students): Process a list of student dictionaries, and return a dictionary containing several key metrics. Args: students (list of dict): List of dictionaries where each dictionary contains: - \'name\' (str): The name of the student. - \'grades\' (list of int): List of grades the student has received. Returns: dict: A dictionary with the following keys and corresponding values: - \'average_grades\' (dict): Dictionary where the keys are student names and the values are their average grades. - \'sorted_students\' (list of str): List of student names sorted by their average grades in descending order. - \'grade_counts\' (dict): Dictionary where the keys are student names and the values are counts of grades received by each student. - \'top_student\' (str): The name of the student with the highest average grade. In case of ties, return any one of them. average_grades = {} grade_counts = {} for student in students: name = student[\'name\'] grades = student[\'grades\'] average_grade = round(sum(grades) / len(grades), 2) average_grades[name] = average_grade grade_counts[name] = len(grades) sorted_students = sorted(average_grades.keys(), key=lambda x: average_grades[x], reverse=True) top_student = sorted_students[0] result = { \'average_grades\': average_grades, \'sorted_students\': sorted_students, \'grade_counts\': grade_counts, \'top_student\': top_student } return result"},{"question":"# Advanced Email Parsing & JSON Manipulation Challenge Objective In this challenge, you will need to create a function that parses a raw email, extracts specific information, and outputs it in a JSON format. This exercise will test your ability to work with email data and JSON data structures using Python. Problem Statement Write a function `parse_email_to_json(raw_email: str) -> str` that accepts a raw email message as a string and returns a JSON string containing the following extracted information: - Subject - Sender\'s Email Address - Recipient\'s Email Address - Date of the Email - Body Content Input Format - `raw_email`: A single string representing the entire raw email message. Output Format - The function should return a string representing the JSON object containing the extracted email information. Constraints - The raw email message given will always contain the headers `Subject`, `From`, `To`, and `Date`. - The email body content will be a plain text. Example ```python raw_email = From: sender@example.com To: recipient@example.com Subject: Test Email Date: Wed, 6 Oct 2023 17:18:00 +0000 This is a test email body. output = parse_email_to_json(raw_email) print(output) ``` Expected output: ```json { \\"Subject\\": \\"Test Email\\", \\"From\\": \\"sender@example.com\\", \\"To\\": \\"recipient@example.com\\", \\"Date\\": \\"Wed, 6 Oct 2023 17:18:00 +0000\\", \\"Body\\": \\"This is a test email body.\\" } ``` Guidelines - Use the `email` module to parse the raw email message. - Use the `json` module to convert the extracted information into a JSON string. - Ensure your solution handles typical email structures and formatting. **Note**: Do not use any external packages other than those from Python\'s standard library.","solution":"import json from email import message_from_string def parse_email_to_json(raw_email: str) -> str: Parses a raw email message and extracts specific information into a JSON string. Parameters: raw_email (str): A string representing the raw email message. Returns: str: A JSON string containing the extracted email information. email_message = message_from_string(raw_email) email_data = { \\"Subject\\": email_message[\\"Subject\\"], \\"From\\": email_message[\\"From\\"], \\"To\\": email_message[\\"To\\"], \\"Date\\": email_message[\\"Date\\"], \\"Body\\": email_message.get_payload().strip() } return json.dumps(email_data, indent=4)"},{"question":"You are provided with a dataset containing features and target values. Your task is to implement and evaluate Kernel Ridge Regression using the scikit-learn package. Objectives: 1. Load the provided dataset. 2. Split the dataset into training and test sets. 3. Implement Kernel Ridge Regression using scikit-learn\'s `KernelRidge`. 4. Train the model on the training data. 5. Predict the target values for the test data. 6. Evaluate the model\'s performance using mean squared error (MSE). Input: You are given: 1. `X` - a 2D numpy array of shape (n_samples, n_features) representing the features. 2. `y` - a 1D numpy array of shape (n_samples,) representing the target values. Output: 1. Print the mean squared error (MSE) of the model on the test set. Constraints: 1. Use 80% of the data for training and 20% for testing. 2. Set the kernel for `KernelRidge` to \'rbf\' and gamma to 0.1. 3. Use a regularization parameter alpha of 1.0. Performance Requirements: The implementation should handle datasets with up to 1000 samples effectively within reasonable computational limits. Example Code: ```python import numpy as np from sklearn.model_selection import train_test_split from sklearn.kernel_ridge import KernelRidge from sklearn.metrics import mean_squared_error # Example dataset X = np.random.randn(100, 10) # 100 samples, 10 features y = np.random.randn(100) # 100 target values # Split dataset into training and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Implement Kernel Ridge Regression model = KernelRidge(kernel=\'rbf\', gamma=0.1, alpha=1.0) # Train the model model.fit(X_train, y_train) # Predict on the test set y_pred = model.predict(X_test) # Evaluate performance using mean squared error (MSE) mse = mean_squared_error(y_test, y_pred) print(\\"Mean Squared Error:\\", mse) ``` Ensure your code meets the provided requirements and correctly outputs the mean squared error on the test set.","solution":"import numpy as np from sklearn.model_selection import train_test_split from sklearn.kernel_ridge import KernelRidge from sklearn.metrics import mean_squared_error def kernel_ridge_regression(X, y): Implements Kernel Ridge Regression on the given dataset and prints the mean squared error. Parameters: X (numpy.ndarray): 2D array of shape (n_samples, n_features) representing the features y (numpy.ndarray): 1D array of shape (n_samples,) representing the target values Returns: float: Mean squared error of the model on the test set # Split dataset into training and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Implement Kernel Ridge Regression model = KernelRidge(kernel=\'rbf\', gamma=0.1, alpha=1.0) # Train the model model.fit(X_train, y_train) # Predict on the test set y_pred = model.predict(X_test) # Evaluate performance using mean squared error (MSE) mse = mean_squared_error(y_test, y_pred) print(\\"Mean Squared Error:\\", mse) return mse"},{"question":"You are tasked with developing a tool that dynamically examines, modifies, and invokes Python functions using the `inspect` module. # Task Write a Python function named `process_function` that takes two arguments: 1. `func`: A Python function object. 2. `args_and_kwargs`: A dictionary containing positional arguments under the key `\\"args\\"` (a list) and keyword arguments under the key `\\"kwargs\\"` (a dictionary). The `process_function` should: 1. Inspect the given function `func` to retrieve its signature using the `inspect.signature` method. 2. Check if all required positional and keyword arguments are included in `args_and_kwargs`. 3. If any arguments are missing, fill these with default values as specified in the function signature. 4. Print the function\'s documentation string using `inspect.getdoc`. 5. Invoke the function with the provided (or default) arguments and return the result. # Example Consider the following function: ```python def example_function(a, b=2, c=3): This is an example function. It adds three numbers. return a + b + c ``` You should be able to call your `process_function` as follows: ```python result = process_function(example_function, {\\"args\\": [1], \\"kwargs\\": {\\"c\\": 4}}) print(result) # Expected output: 7 (since a=1, b=2 (default), c=4) result = process_function(example_function, {\\"args\\": [1], \\"kwargs\\": {}}) print(result) # Expected output: 6 (since a=1, b=2 (default), c=3 (default)) ``` # Constraints - All functions passed to `process_function` will have unique parameter names. - You can assume that the `args_and_kwargs` dictionary will always contain the `args` list and `kwargs` dictionary, even if they are empty. - Do not use any extra modules other than the built-in `inspect` module. # Function Signature ```python def process_function(func, args_and_kwargs): # Your code here ``` # Hints - Use `inspect.signature` to retrieve the function\'s signature. - Use the `params.bind_partial` method to handle argument binding and default values. - Retrieve the documentation string using `inspect.getdoc`.","solution":"import inspect def process_function(func, args_and_kwargs): Dynamically examines, modifies, and invokes a function. Parameters: func (callable): The function to process. args_and_kwargs (dict): Dictionary containing positional arguments under \'args\' and keyword arguments under \'kwargs\'. Returns: The result of invoking the function with the provided (or default) arguments. # Get the function\'s signature sig = inspect.signature(func) # Merge provided arguments with default values bound_args = sig.bind_partial(*args_and_kwargs[\'args\'], **args_and_kwargs[\'kwargs\']) bound_args.apply_defaults() # Print the function\'s documentation string print(inspect.getdoc(func)) # Invoke the function with the prepared arguments and return the result return func(*bound_args.args, **bound_args.kwargs)"},{"question":"**Question: Implement a Custom ZIP Module Importer** The `zipimport` module enables importing Python modules and packages from ZIP-format archives. Although this functionality is used implicitly when adding ZIP files to the `sys.path`, you will implement a custom class that mimics the `zipimporter` functionality to manually handle importing modules from ZIP archives. # Requirements: 1. **Class Design**: - Create a class `CustomZipImporter` that mimics the basic functionality of `zipimporter`. - The class should be initialized with the path to a ZIP file (and optionally a subdirectory within the ZIP file). 2. **Methods to Implement**: - `find_module(fullname: str) -> bool`: Return `True` if the module specified by `fullname` exists within the ZIP file, otherwise return `False`. - `load_module(fullname: str) -> Any`: Load the module specified by `fullname` and execute it, returning the module object. - `get_code(fullname: str) -> types.CodeType`: Return the code object for the specified module. - `get_filename(fullname: str) -> str`: Return the value that would be set to `__file__` for the specified module. - `is_package(fullname: str) -> bool`: Return `True` if the specified module is a package. # Input and Output: - **Inputs**: - The class `CustomZipImporter` should be initialized with a path to the ZIP archive: `CustomZipImporter(archive_path: str)`. - Methods like `find_module`, `load_module` etc., should take the fully qualified module name as input. - **Outputs**: - `find_module(fullname: str) -> bool`: Boolean indicating whether the module exists. - `load_module(fullname: str) -> Any`: The module object that has been loaded and executed. - `get_code(fullname: str) -> types.CodeType`: A code object representing the module’s code. - `get_filename(fullname: str) -> str`: The filename as a string. - `is_package(fullname: str) -> bool`: Boolean indicating whether the module is a package. # Constraints and Limitations: - Your solution should handle ZIP files containing both `.py` and precompiled `.pyc` files. - Handle errors gracefully by raising appropriate exceptions (e.g., `ImportError` or custom exceptions). - Ensure that the performance of module loading is reasonable by maintaining a small internal cache if necessary. - Do not use the built-in `zipimport` module explicitly; aim to recreate its functionality using other Python standard libraries. # Example Scenario: ```python # Preparation: Assume there is a ZIP file \'modules.zip\' containing a module \'example_module.py\' import zipfile import types class CustomZipImporter: def __init__(self, archive_path): # Initialize the importer with path to the ZIP archive pass def find_module(self, fullname): # Implement module finding logic pass def load_module(self, fullname): # Implement module loading and execution logic pass def get_code(self, fullname): # Retrieve the code object for the module pass def get_filename(self, fullname): # Get the filename for the module pass def is_package(self, fullname): # Determine if the module is a package pass # Usage: importer = CustomZipImporter(\'modules.zip\') if importer.find_module(\'example_module\'): module = importer.load_module(\'example_module\') print(f\\"Module {module.__name__} loaded from {importer.get_filename(\'example_module\')}\\") ``` Implement the `CustomZipImporter` class, ensuring it can be used in the scenario described above.","solution":"import zipfile import importlib.util import sys import types class CustomZipImporter: def __init__(self, archive_path): self.archive_path = archive_path self.zip_archive = zipfile.ZipFile(archive_path, \'r\') def find_module(self, fullname): module_path = fullname.replace(\'.\', \'/\') + \'.py\' if module_path in self.zip_archive.namelist(): return True return False def load_module(self, fullname): if not self.find_module(fullname): raise ImportError(f\\"Module {fullname} not found in ZIP archive\\") file_path = fullname.replace(\'.\', \'/\') + \'.py\' source = self.zip_archive.read(file_path).decode(\'utf-8\') spec = importlib.util.spec_from_loader(fullname, loader=None) module = importlib.util.module_from_spec(spec) exec(source, module.__dict__) sys.modules[fullname] = module return module def get_code(self, fullname): if not self.find_module(fullname): raise ImportError(f\\"Module {fullname} not found in ZIP archive\\") file_path = fullname.replace(\'.\', \'/\') + \'.py\' source = self.zip_archive.read(file_path).decode(\'utf-8\') return compile(source, file_path, \'exec\') def get_filename(self, fullname): if not self.find_module(fullname): raise ImportError(f\\"Module {fullname} not found in ZIP archive\\") return self.archive_path + \'/\' + fullname.replace(\'.\', \'/\') + \'.py\' def is_package(self, fullname): module_path = fullname.replace(\'.\', \'/\') + \'/__init__.py\' if module_path in self.zip_archive.namelist(): return True return False"},{"question":"**Numerically Stable Matrix Computation in PyTorch** In this task, you need to write a function using PyTorch to perform a numerically stable matrix multiplication followed by a matrix inversion operation. Specifically, you will implement a function to compute the product of two matrices and then invert the result. You need to take care of potential numerical issues including those related to precision, stability, and overflow as discussed in the given documentation. # Function Signature: ```python def numerically_stable_matrix_operation(A: torch.Tensor, B: torch.Tensor) -> torch.Tensor: pass ``` # Input: - `A`: A 2-dimensional PyTorch tensor of shape (N, M) - `B`: A 2-dimensional PyTorch tensor of shape (M, M) # Output: - A 2-dimensional PyTorch tensor of shape (M, M), which is the inverse of the matrix product of `A` and `B`. # Constraints: 1. Your solution must be implemented using PyTorch functions. 2. You should handle potential issues with numerical stability, large values, and precision. 3. Assume `A` and `B` contain double precision (torch.float64) floats. 4. If the matrix product is not invertible, raise a `ValueError` with the message \\"Matrix is not invertible.\\" # Example: ```python import torch A = torch.tensor([[1.0, 2.0], [3.0, 4.0]], dtype=torch.float64) B = torch.tensor([[5.0, 6.0], [7.0, 8.0]], dtype=torch.float64) try: result = numerically_stable_matrix_operation(A, B) print(result) except ValueError as e: print(e) ``` # Explanation: In this example, you need to first compute `C = A @ B`, and then return `torch.linalg.inv(C)`. Ensure that `torch.linalg.inv` raises a ValueError if `C` is not invertible. Note that your function should take into account potential numerical issues such as ill-conditioned matrices and handle them appropriately. **Hints**: - Use `torch.linalg.cond` to check the condition number of the matrix before inversion. - Use `torch.isfinite` to ensure the matrix contains finite values before performing operations. - Make sure to document any assumptions or additional checks you perform to enhance stability and reliability.","solution":"import torch def numerically_stable_matrix_operation(A: torch.Tensor, B: torch.Tensor) -> torch.Tensor: Performs a numerically stable matrix multiplication followed by inversion. A: A 2-dimensional PyTorch tensor of shape (N, M) B: A 2-dimensional PyTorch tensor of shape (M, M) Returns: The inverse of the matrix product A @ B of shape (M, M) If the matrix product is not invertible, raises a ValueError. # Compute the matrix product C = torch.matmul(A, B) # Check if C contains finite values if not torch.isfinite(C).all(): raise ValueError(\\"Matrix product contains non-finite values\\") # Check the condition number to ensure the matrix is not ill-conditioned cond_number = torch.linalg.cond(C) if cond_number > 1e15: # Example threshold for condition number to consider matrix as well-conditioned raise ValueError(\\"Matrix is not invertible due to high condition number\\") # Compute the inverse of the matrix product try: C_inv = torch.linalg.inv(C) except RuntimeError: raise ValueError(\\"Matrix is not invertible\\") return C_inv"},{"question":"Write a Python function that visualizes specified relationships in a given dataset using the pandas plotting module. Function Signature ```python def visualize_data(df: pd.DataFrame, plot_type: str, class_column: str, cols: List[str] = None) -> None: pass ``` Inputs 1. `df` (pd.DataFrame): A DataFrame containing the dataset to visualize. 2. `plot_type` (str): The type of plot to generate. It can be one of the following: * `\\"scatter_matrix\\"` * `\\"parallel_coordinates\\"` * `\\"andrews_curves\\"` 3. `class_column` (str): The name of the column in the DataFrame that contains class labels (used for coloring the lines in `parallel_coordinates` and `andrews_curves`). 4. `cols` (List[str], optional): The list of columns to include in the plot. If `None`, use all columns except `class_column`. Outputs * The function does not return anything, but it should display the appropriate plot based on the input parameters. Constraints * The `df` DataFrame will always contain at least one column other than the class column. * The specified `plot_type` will always be valid. * The `cols` list, if provided, will always contain valid column names from the DataFrame. # Example Usage ```python import pandas as p import pandas.plotting as pd_plotting def visualize_data(df: pd.DataFrame, plot_type: str, class_column: str, cols: List[str] = None) -> None: if not cols: cols = [col for col in df.columns if col != class_column] if plot_type == \'scatter_matrix\': pd_plotting.scatter_matrix(df[cols], alpha=0.2, figsize=(12, 12)) elif plot_type == \'parallel_coordinates\': pd_plotting.parallel_coordinates(df, class_column) elif plot_type == \'andrews_curves\': pd_plotting.andrews_curves(df, class_column) plt.show() # Test the function df = pd.DataFrame({ \'A\': [1, 2, 3, 4, 5], \'B\': [5, 4, 3, 2, 1], \'C\': [2, 3, 4, 5, 6], \'Class\': [\'X\', \'Y\', \'X\', \'Y\', \'X\'] }) visualize_data(df, \'scatter_matrix\', \'Class\') visualize_data(df, \'parallel_coordinates\', \'Class\') visualize_data(df, \'andrews_curves\', \'Class\') ``` This function should generate and display the appropriate plots for each of the specified plot types using the provided dataset and columns.","solution":"import pandas as pd import pandas.plotting as pd_plotting import matplotlib.pyplot as plt from typing import List def visualize_data(df: pd.DataFrame, plot_type: str, class_column: str, cols: List[str] = None) -> None: Visualizes specified relationships in a given dataset using the pandas plotting module. Parameters: df (pd.DataFrame): A DataFrame containing the dataset to visualize. plot_type (str): The type of plot to generate. It can be one of the following: * \\"scatter_matrix\\" * \\"parallel_coordinates\\" * \\"andrews_curves\\" class_column (str): The name of the column in the DataFrame that contains class labels. cols (List[str], optional): The list of columns to include in the plot. If None, use all columns except class_column. Returns: None if not cols: cols = [col for col in df.columns if col != class_column] if plot_type == \'scatter_matrix\': pd_plotting.scatter_matrix(df[cols], alpha=0.2, figsize=(12, 12)) elif plot_type == \'parallel_coordinates\': pd_plotting.parallel_coordinates(df, class_column) elif plot_type == \'andrews_curves\': pd_plotting.andrews_curves(df, class_column) plt.show()"},{"question":"<|Analysis Begin|> The documentation provided focuses on the `html` module in Python 3.10, specifically addressing functions for escaping and unescaping HTML characters. The two primary functions are: 1. **`html.escape(s, quote=True)`**: This function converts specific characters (\\"&\\", \\"<\\", \\">\\") into their HTML-safe sequences. It optionally converts double and single quotes if the `quote` parameter is set to True. 2. **`html.unescape(s)`**: This function converts HTML character references (both named and numeric) back into their corresponding Unicode characters based on HTML5 standards. Additionally, the documentation mentions submodules for parsing HTML (`html.parser`) and managing HTML entity definitions (`html.entities`). Given the context of this module, a good assessment question would test the student\'s understanding of these functions, their usage, and their ability to implement additional utility functions related to HTML character manipulation. <|Analysis End|> <|Question Begin|> # Coding Assessment Question Background You are given the task of cleaning and preparing HTML text data for a web application. The `html` module in Python 3.10 provides utilities to escape and unescape HTML characters, which are crucial for ensuring the data does not introduce security vulnerabilities or display incorrectly. Objective Implement two functions: 1. **`custom_escape(s: str, quote: bool = True) -> str`**: - This function should replicate the behavior of `html.escape`, converting the characters \\"&\\", \\"<\\", and \\">\\" in the string `s` to their HTML-safe sequences. - If the optional `quote` parameter is `True`, it should also convert double quotes (`\\"`) and single quotes (`\'`) to their corresponding HTML-safe sequences. 2. **`custom_unescape(s: str) -> str`**: - This function should replicate the behavior of `html.unescape`, converting all named and numeric character references in the string `s` to the corresponding Unicode characters. Expected Input and Output 1. **`custom_escape(s: str, quote: bool = True) -> str`** - **Input**: A string `s` and an optional boolean `quote`. - **Output**: A string with the specified characters converted to HTML-safe sequences. - **Example**: ```python custom_escape(\'5 < 6 & 7 > 8\') # Output: \'5 &lt; 6 &amp; 7 &gt; 8\' custom_escape(\'Rock \'n\' Roll\', quote=True) # Output: \'Rock &#x27;n&#x27; Roll\' ``` 2. **`custom_unescape(s: str) -> str`** - **Input**: A string `s` containing HTML character references. - **Output**: A string with all character references converted to the corresponding Unicode characters. - **Example**: ```python custom_unescape(\'5 &lt; 6 &amp; 7 &gt; 8\') # Output: \'5 < 6 & 7 > 8\' custom_unescape(\'Rock &#x27;n&#x27; Roll\') # Output: \\"Rock \'n\' Roll\\" ``` Constraints - You may not use the `html.escape` and `html.unescape` functions directly in your implementation. - Ensure that your functions handle both valid and invalid HTML character references appropriately. Performance Requirements Your solution should handle strings of length up to 10,000 characters efficiently. Notes - You can use dictionaries or lookup tables to map characters to their HTML-safe sequences and vice versa. - Consider edge cases such as strings that do not contain any characters to be escaped/unescaped. Good luck!","solution":"import html.entities def custom_escape(s: str, quote: bool = True) -> str: Escapes certain characters in a string for HTML. escape_map = { \'&\': \'&amp;\', \'<\': \'&lt;\', \'>\': \'&gt;\', } if quote: escape_map.update({ \'\\"\': \'&quot;\', \\"\'\\": \'&#x27;\', }) return \'\'.join(escape_map.get(c, c) for c in s) def custom_unescape(s: str) -> str: Unescapes HTML character references to their corresponding Unicode characters. for entity, char in html.entities.html5.items(): s = s.replace(f\\"&{entity};\\", char) # Use a regular expression to find numeric character references and convert them import re def numeric_ref_replacer(match): num_str = match.group()[2:-1] # removes leading \\"&#\\" and trailing \\";\\" return chr(int(num_str[1:], 16)) if num_str[0] in \'xX\' else chr(int(num_str)) s = re.sub(r\\"&#[xX]?[0-9a-fA-F]+;\\", numeric_ref_replacer, s) return s"},{"question":"# Pandas MultiIndex Advanced Exercise Problem Statement You are given a dataset that represents sales data for different items in various regions. The data includes information on the item category, the region in which it was sold, the year, and the sales amount. Your task is to perform a series of operations on this data using pandas MultiIndexes to demonstrate your understanding of hierarchical indexing. The dataset: ```csv category,region,year,sales A,North,2019,1000 A,North,2020,1100 A,South,2019,2000 A,South,2020,2200 B,North,2019,1500 B,North,2020,1600 B,South,2019,2500 B,South,2020,2600 C,North,2019,1200 C,North,2020,1300 C,South,2019,2100 C,South,2020,2300 ``` Objectives: 1. **Create a DataFrame**: - Load the dataset into a pandas DataFrame. 2. **Set MultiIndex**: - Set a MultiIndex on the DataFrame using `category`, `region`, and `year`. 3. **Display Basic Information**: - Retrieve sales data for category `A` and region `North`. - Get the sales data for all items sold in the year 2019. 4. **Manipulate Data**: - Calculate the mean sales for each category across all regions and years. - Reindex the DataFrame to include missing years 2018 and 2021 for all categories and regions. Fill missing sales with `NaN`. 5. **Advanced Indexing**: - Swap the levels `region` and `year` in the MultiIndex and display the DataFrame. - Reorder the levels to have `year`, `category`, and then `region`. 6. **Custom Slicing and Filtering**: - Use a custom slicer to get the sales data for the year 2020 across all categories and regions. 7. **Sorting and Grouping**: - Sort the DataFrame first by `category` and then by `region`. - Group the data by `category` and get the sum of sales for each category. Constraints: - You must use MultiIndexes for all indexing operations. - Ensure your solution is efficient and leverages pandas functionality optimally. Expected Input and Output: - **Input**: A CSV file with the described structure. - **Output**: Multiple pandas DataFrames showcasing the results of the various operations described above. Performance Requirements: Ensure that your code executes efficiently, even with larger datasets of similar structure. Your Implementation: ```python import pandas as pd # Step 1: Load the dataset data = { \'category\': [\'A\', \'A\', \'A\', \'A\', \'B\', \'B\', \'B\', \'B\', \'C\', \'C\', \'C\', \'C\'], \'region\': [\'North\', \'North\', \'South\', \'South\', \'North\', \'North\', \'South\', \'South\', \'North\', \'North\', \'South\', \'South\'], \'year\': [2019, 2020, 2019, 2020, 2019, 2020, 2019, 2020, 2019, 2020, 2019, 2020], \'sales\': [1000, 1100, 2000, 2200, 1500, 1600, 2500, 2600, 1200, 1300, 2100, 2300] } df = pd.DataFrame(data) # Step 2: Set MultiIndex df = df.set_index([\'category\', \'region\', \'year\']) # Step 3: Display Basic Information sales_a_north = df.loc[\'A\', \'North\'] sales_2019 = df.loc[(slice(None), slice(None), 2019), :] # Step 4: Manipulate Data mean_sales = df.groupby(level=\'category\').mean() reindex_years = pd.MultiIndex.from_product([df.index.get_level_values(\'category\').unique(), df.index.get_level_values(\'region\').unique(), [2018, 2019, 2020, 2021]], names=[\'category\', \'region\', \'year\']) df_reindexed = df.reindex(reindex_years) # Step 5: Advanced Indexing swapped_df = df.swaplevel(\'region\', \'year\', axis=0) reordered_df = df_reindexed.reorder_levels([\'year\', \'category\', \'region\']) # Step 6: Custom Slicing and Filtering sliced_df_2020 = reordered_df.loc[(2020, slice(None), slice(None)), :] # Step 7: Sorting and Grouping sorted_df = df.sort_index(level=[\'category\', \'region\']) grouped_sales = df.groupby(level=\'category\').sum() # Display results for verification print(\\"Sales for category \'A\' in \'North\' region:n\\", sales_a_north) print(\\"nSales data for the year 2019:n\\", sales_2019) print(\\"nMean sales for each category:n\\", mean_sales) print(\\"nReindexed DataFrame:n\\", df_reindexed) print(\\"nDataFrame with swapped levels \'region\' and \'year\':n\\", swapped_df) print(\\"nDataFrame with reordered levels:n\\", reordered_df) print(\\"nSales data for the year 2020 across all categories and regions:n\\", sliced_df_2020) print(\\"nDataFrame sorted by \'category\' and \'region\':n\\", sorted_df) print(\\"nSum of sales for each category:n\\", grouped_sales) ```","solution":"import pandas as pd def load_and_prepare_data(): data = { \'category\': [\'A\', \'A\', \'A\', \'A\', \'B\', \'B\', \'B\', \'B\', \'C\', \'C\', \'C\', \'C\'], \'region\': [\'North\', \'North\', \'South\', \'South\', \'North\', \'North\', \'South\', \'South\', \'North\', \'North\', \'South\', \'South\'], \'year\': [2019, 2020, 2019, 2020, 2019, 2020, 2019, 2020, 2019, 2020, 2019, 2020], \'sales\': [1000, 1100, 2000, 2200, 1500, 1600, 2500, 2600, 1200, 1300, 2100, 2300] } return pd.DataFrame(data) def set_multiindex(df): return df.set_index([\'category\', \'region\', \'year\']) def get_sales_a_north(df): return df.loc[\'A\', \'North\'] def get_sales_2019(df): return df.loc[(slice(None), slice(None), 2019), :] def calculate_mean_sales(df): return df.groupby(level=\'category\').mean() def reindex_data(df): reindex_years = pd.MultiIndex.from_product([df.index.get_level_values(\'category\').unique(), df.index.get_level_values(\'region\').unique(), [2018, 2019, 2020, 2021]], names=[\'category\', \'region\', \'year\']) return df.reindex(reindex_years) def swap_levels(df): return df.swaplevel(\'region\', \'year\', axis=0) def reorder_levels(df): return df.reorder_levels([\'year\', \'category\', \'region\']) def get_sales_2020(df): return df.loc[(2020, slice(None), slice(None)), :] def sort_data(df): return df.sort_index(level=[\'category\', \'region\']) def group_sales_by_category(df): return df.groupby(level=\'category\').sum()"},{"question":"**Python Coding Assessment Question** # Question: Implementing Custom Calling Protocols in a Python Class You are required to implement a Python class that supports both the `tp_call` and `vectorcall` protocols in CPython. The class should: 1. Be callable using the `tp_call` protocol. 2. Implement the `vectorcall` protocol for better performance. 3. Have methods for various types of function calls, as detailed in the documentation. # Requirements: - **Class Name**: `CustomCallable` - **Methods**: - `__init__`: Initializes the object. - `tp_call` equivalent: Ensures the class is callable using the tuple and dict structure for arguments. - `vectorcall` equivalent: Implements the vectorcall protocol, showcasing the efficiency improvements. - Various utility methods to test callable functionalities. # Constraints: 1. **Implement the `tp_call` equivalent**: Create a `__call__` method inside the class that can take positional and keyword arguments, and returns their sum if they are numbers or concatenates them if they are strings. 2. **Implement the `vectorcall` equivalent**: You will create a custom method named `vectorcall` that behaves similarly but is more efficient. This will not be the actual C implementation but should correctly handle the concept. 3. **Utility Methods**: - `call_no_args(self)`: Calls the callable without arguments. - `call_one_arg(self, arg)`: Calls the callable with one positional argument. - `call_with_kwargs(self, **kwargs)`: Calls an object method with keyword arguments. # Performance Requirements: 1. The operations should be completed in `O(n)` time complexity where `n` is the number of arguments. 2. Ensure that converting arguments during calls is efficient and minimized. # Input: The class should handle inputs dynamically and gracefully. For the `__call__` and `vectorcall` methods, the input could be either numerical (int or float) or strings. # Output: The `__call__` and `vectorcall` methods should return the sum of numerical values or the concatenation of string values. # Example Usage: ```python # Creating an instance of CustomCallable cc = CustomCallable() # Using tp_call equivalent (__call__) print(cc(1, 2, 3)) # Output: 6 print(cc(\\"hello\\", \\" \\", \\"world\\")) # Output: \\"hello world\\" # Using vectorcall equivalent (custom vectorcall method) print(cc.vectorcall([1, 2, 3], kwnames=None)) # Output: 6 print(cc.vectorcall([\\"hello\\", \\" \\", \\"world\\"], kwnames=None)) # Output: \\"hello world\\" # Using utility methods print(cc.call_no_args()) # Output: 0 or \\"\\" print(cc.call_one_arg(10)) # Output: 10 print(cc.call_with_kwargs(a=1, b=2, c=3)) # Output: 6 ``` # Notes: - You are not required to implement the C-level details but should understand and simulate the behavior as close as possible in pure Python. - Carefully handle the data types and possible exceptions when summing or concatenating. Happy coding!","solution":"class CustomCallable: def __init__(self): self.default_int = 0 self.default_str = \\"\\" def __call__(self, *args, **kwargs): if all(isinstance(arg, (int, float)) for arg in args) and all(isinstance(value, (int, float)) for value in kwargs.values()): return sum(args) + sum(kwargs.values()) elif all(isinstance(arg, str) for arg in args) and all(isinstance(value, str) for value in kwargs.values()): result = \'\'.join(args) for key in sorted(kwargs): result += kwargs[key] return result else: raise ValueError(\\"All arguments must be either all numbers or all strings\\") def vectorcall(self, args, kwnames=None): if kwnames is None: kwnames = [] kwargs = {key: args[i] for i, key in enumerate(kwnames)} args = args[:-len(kwnames)] if kwnames else args return self(*args, **kwargs) def call_no_args(self): return self() def call_one_arg(self, arg): return self(arg) def call_with_kwargs(self, **kwargs): return self(**kwargs)"},{"question":"Problem Statement You are required to create a custom entry point for a simple neural network model in PyTorch and demonstrate how to publish and load this model using `torch.hub`. The custom model should be a basic feedforward neural network designed for a simple task like digit classification using the MNIST dataset. Instructions 1. **Define the Neural Network:** Create a class `SimpleNN` that defines a feedforward neural network with the following structure: - Input layer: 784 neurons (28x28 flattened image). - Hidden layer: 128 neurons with ReLU activation. - Output layer: 10 neurons (one for each digit class) with softmax activation. 2. **Create Entry Point:** Define an entry point `simple_nn` in a `hubconf.py` file which returns an instance of `SimpleNN`. 3. **Simulate Publishing:** To simulate the model publishing process, add the necessary metadata like dependencies and an example docstring in `hubconf.py`. 4. **Load the Model via torch.hub:** Write code to load your model using `torch.hub.load` and demonstrate loading the model without pretrained weights (since this example does not focus on pre-trained weights). 5. **Run a Forward Pass:** Create a dummy input (batch of size 1 with 784 features) and execute a forward pass through the loaded model. Print out the output of the model. Expected Input and Output Formats **Input:** ```python # hubconf.py should be part of your submission # Code to define the model class SimpleNN(nn.Module): ... def entry_point(pretrained=False, **kwargs): ... # Code to load and test the model import torch model = torch.hub.load(\'your_github_repo\', \'simple_nn\') dummy_input = torch.randn(1, 784) output = model(dummy_input) print(output) ``` **Output:** The output should be the printed tensor resulting from the forward pass of the dummy input through the `SimpleNN` model. Constraints and Requirements - Ensure the `hubconf.py` file follows the structure and conventions specified in the provided documentation. - The neural network should be implemented using PyTorch. - The loading of the model should utilize `torch.hub.load`. - Make sure to include any necessary imports and define them correctly. Performance Requirements - The code should be efficient and handle the forward pass within reasonable time (e.g., less than a second for the given input). Submission Submit the following: - The `hubconf.py` file defining the entry point. - The code used to load and test the model.","solution":"import torch import torch.nn as nn import torch.nn.functional as F class SimpleNN(nn.Module): def __init__(self): super(SimpleNN, self).__init__() self.fc1 = nn.Linear(784, 128) self.fc2 = nn.Linear(128, 10) def forward(self, x): x = F.relu(self.fc1(x)) x = F.log_softmax(self.fc2(x), dim=1) return x # Define a stub for `hubconf.py` content def simple_nn(pretrained=False, **kwargs): Constructs a SimpleNN model. Args: pretrained (bool): If True, returns a pre-trained model. Returns: model (nn.Module): the constructed SimpleNN model model = SimpleNN() if pretrained: # Normally, you would load the pretrained weights here pass return model"},{"question":"Objective: Create a utility function that utilizes the `nis` module to search for a key across all available maps and returns the map name and the key\'s matching value if found. Function Signature: ```python def search_key_in_all_maps(key: str, domain: str = None) -> tuple: pass ``` Input: - `key` (str): The key to search for in the NIS maps. - `domain` (str, optional): The NIS domain to use for the lookup. If not provided, the default domain will be used. Output: - A tuple `(mapname, value)` where `mapname` is the name of the map containing the key, and `value` is the corresponding value for the key. If the key is not found in any map, return `None`. Constraints: - This function should handle the `nis.error` exception appropriately. - If the `domain` is not specified, the system\'s default domain should be used. Performance Requirements: - The solution should efficiently handle cases where there are a large number of maps and large datasets within each map. Example Usage: ```python # Assuming the existence of NIS maps and entries: result = search_key_in_all_maps(\\"some_key\\") if result: print(f\\"Found in map: {result[0]}, value: {result[1]}\\") else: print(\\"Key not found in any map.\\") ``` Notes: - Your implementation should use the `nis.maps()`, `nis.cat()`, and `nis.get_default_domain()` functions as needed. - Ensure that you handle potential exceptions and edge cases.","solution":"import nis def search_key_in_all_maps(key: str, domain: str = None) -> tuple: Searches for the given key across all NIS maps and returns the map name and the key\'s matching value if found. Parameters: - key (str): The key to search for in the NIS maps. - domain (str, optional): The NIS domain to use for the lookup. If not provided, the default domain will be used. Returns: - A tuple (mapname, value) where mapname is the name of the map containing the key, and value is the corresponding value for the key. If the key is not found in any map, returns None. try: if domain is None: domain = nis.get_default_domain() maps = nis.maps(domain) for mapname in maps: map_contents = nis.cat(mapname, domain) if key in map_contents: return (mapname, map_contents[key].decode()) except nis.error: return None return None"},{"question":"# Regex-based Text Processing and Extraction Problem Statement You are given a large block of text that represents user data. Each user\'s data consists of various fields: name, email, phone number, and an optional address. Your task is to write a Python function that extracts this information using regular expressions and compiles it into a structured format. User Data Format Each user\'s data is formatted as follows: ``` Name: John Doe Email: john.doe@example.com Phone: 123-456-7890 Address: 123 Maple Street, Springfield, IL 12345 ``` Fields are: - `Name`: This is a single line starting with \\"Name:\\" followed by the user\'s full name. - `Email`: This is a single line starting with \\"Email:\\" followed by the user\'s email address. - `Phone`: This is a single line starting with \\"Phone:\\" followed by the user\'s phone number. - `Address`: This is an optional single line starting with \\"Address:\\" followed by the user\'s full address. Function Requirements Write a Python function `extract_user_data(text: str) -> List[dict]` that: 1. Takes a string `text` containing multiple users\' data in the format described above. 2. Extracts each user’s data and stores it in a dictionary. 3. Returns a list of dictionaries, where each dictionary contains the extracted fields (`name`, `email`, `phone`, and optional `address`). Input - `text`: A string containing the user data. Output - A list of dictionaries with extracted user data. Each dictionary should have the keys `name`, `email`, `phone`, and optionally `address`. Constraints - The input text can contain multiple users\' data. - Users\' data are separated by two or more newlines. - The `Name`, `Email`, and `Phone` fields are mandatory. The `Address` field is optional. Example ```python input_text = Name: John Doe Email: john.doe@example.com Phone: 123-456-7890 Address: 123 Maple Street, Springfield, IL 12345 Name: Jane Smith Email: jane.smith@example.com Phone: 098-765-4321 extract_user_data(input_text) ``` Expected Output: ```python [ { \\"name\\": \\"John Doe\\", \\"email\\": \\"john.doe@example.com\\", \\"phone\\": \\"123-456-7890\\", \\"address\\": \\"123 Maple Street, Springfield, IL 12345\\" }, { \\"name\\": \\"Jane Smith\\", \\"email\\": \\"jane.smith@example.com\\", \\"phone\\": \\"098-765-4321\\" } ] ``` Notes 1. Use regular expressions to parse each user\'s data. 2. Ensure your function handles the case where the address is not provided. 3. Make sure to handle edge cases such as fields having extra spaces or lines. Solution Template ```python import re def extract_user_data(text: str) -> list: user_data_list = [] # Split users\' data by two or more newlines users_data = re.split(r\'ns*n\', text.strip()) for user_data in users_data: user_dict = {} # Extract name name_match = re.search(r\'Name:s*(.*)\', user_data) if name_match: user_dict[\\"name\\"] = name_match.group(1).strip() # Extract email email_match = re.search(r\'Email:s*(.*)\', user_data) if email_match: user_dict[\\"email\\"] = email_match.group(1).strip() # Extract phone phone_match = re.search(r\'Phone:s*(.*)\', user_data) if phone_match: user_dict[\\"phone\\"] = phone_match.group(1).strip() # Extract address (optional) address_match = re.search(r\'Address:s*(.*)\', user_data) if address_match: user_dict[\\"address\\"] = address_match.group(1).strip() user_data_list.append(user_dict) return user_data_list # Example usage input_text = Name: John Doe Email: john.doe@example.com Phone: 123-456-7890 Address: 123 Maple Street, Springfield, IL 12345 Name: Jane Smith Email: jane.smith@example.com Phone: 098-765-4321 print(extract_user_data(input_text)) ```","solution":"import re def extract_user_data(text: str) -> list: user_data_list = [] # Split users\' data by two or more newlines users_data = re.split(r\'ns*n\', text.strip()) for user_data in users_data: user_dict = {} # Extract name name_match = re.search(r\'Name:s*(.*)\', user_data) if name_match: user_dict[\\"name\\"] = name_match.group(1).strip() # Extract email email_match = re.search(r\'Email:s*(.*)\', user_data) if email_match: user_dict[\\"email\\"] = email_match.group(1).strip() # Extract phone phone_match = re.search(r\'Phone:s*(.*)\', user_data) if phone_match: user_dict[\\"phone\\"] = phone_match.group(1).strip() # Extract address (optional) address_match = re.search(r\'Address:s*(.*)\', user_data) if address_match: user_dict[\\"address\\"] = address_match.group(1).strip() user_data_list.append(user_dict) return user_data_list"},{"question":"# Memory Optimization, Handling Missing Values, and UDF in Pandas **Objective:** Write a function `optimize_dataframe` that optimizes the memory usage of a given Pandas DataFrame. This function should: 1. Convert any columns with `dtype=object` to `categorical` if the number of unique values is less than 50% of the total rows. 2. Calculate the memory usage of the DataFrame before and after the conversion to categorical dtype. 3. Fill any missing values in integer columns with the median of the column using a UDF. 4. Return the optimized DataFrame and the memory usage before and after optimization. **Input:** - A Pandas DataFrame `df`. **Output:** - A tuple `(optimized_df, memory_usage_before, memory_usage_after)`, where `optimized_df` is the optimized DataFrame, and `memory_usage_before`, `memory_usage_after` are the memory usages of the original and optimized DataFrame respectively in bytes. **Constraints and Notes:** - You should NOT modify the original DataFrame; return a copy instead. - Use the `deep=True` argument for a more accurate memory usage report. - The replacement of missing values should use a custom UDF function applied via `DataFrame.apply`. - Use Python 3.6+ and Pandas 1.1+ **Example Usage:** ```python import pandas as pd import numpy as np # Create a sample DataFrame with random data data = { \'int_col\': np.random.randint(1, 100, size=1000), \'float_col\': np.random.rand(1000), \'object_col\': np.random.choice([\'A\',\'B\',\'C\',\'D\'], size=1000), \'mix_col\': np.random.choice([1, np.nan], size=1000) } df = pd.DataFrame(data) # Add some missing values to the integer column df.loc[df.sample(frac=0.1).index, \'int_col\'] = np.nan # Ensure the use of Int64Dtype for nullable integers df[\'int_col\'] = df[\'int_col\'].astype(\'Int64\') optimized_df, mem_before, mem_after = optimize_dataframe(df) print(f\'Memory usage before optimization: {mem_before} bytes\') print(f\'Memory usage after optimization: {mem_after} bytes\') ``` **Function Signature:** ```python import pandas as pd def optimize_dataframe(df: pd.DataFrame) -> tuple: # Your code here ``` **Note: Avoid hardcoding; your function should adapt to different DataFrame columns and types.**","solution":"import pandas as pd import numpy as np def fill_missing_with_median(series: pd.Series) -> pd.Series: Fills missing values in a pandas Series with the median of the Series. if pd.api.types.is_integer_dtype(series): median_value = series.median() return series.fillna(median_value) return series def optimize_dataframe(df: pd.DataFrame) -> tuple: Optimizes a DataFrame by converting object columns to categorical where appropriate and filling missing values in integer columns with the median of the column. Parameters: df (pd.DataFrame): The original DataFrame to be optimized. Returns: tuple: A tuple containing the optimized DataFrame and memory usage before and after optimization. # Copy the original DataFrame to avoid modifying it optimized_df = df.copy() # Calculate memory usage before optimization memory_usage_before = optimized_df.memory_usage(deep=True).sum() # Optimize by converting object columns to categorical where applicable for col in optimized_df.select_dtypes(include=\'object\').columns: num_unique_values = optimized_df[col].nunique() num_total_values = len(optimized_df[col]) if num_unique_values / num_total_values < 0.5: optimized_df[col] = optimized_df[col].astype(\'category\') # Fill missing values in integer columns using a UDF optimized_df = optimized_df.apply(fill_missing_with_median) # Calculate memory usage after optimization memory_usage_after = optimized_df.memory_usage(deep=True).sum() return optimized_df, memory_usage_before, memory_usage_after"},{"question":"You are tasked with developing an anomaly detection model using scikit-learn. The goal is to detect anomalies in a dataset using the IsolationForest algorithm and validate your model. IsolationForest is a robust method particularly effective in high-dimensional data for identifying anomalies. # Specifications 1. **Function Signature**: ```python def detect_anomalies(X_train: np.ndarray, X_test: np.ndarray, contamination: float) -> np.ndarray: ``` 2. **Input**: - `X_train` (np.ndarray): The training data with dimensions `(n_samples_train, n_features)`. - `X_test` (np.ndarray): The testing data with dimensions `(n_samples_test, n_features)`. - `contamination` (float): The proportion of outliers in the data set, a value between 0 and 0.5. 3. **Output**: - `predictions` (np.ndarray): An array of predictions for the test data. Inliers are labeled `1` and outliers are labeled `-1`. 4. **Constraints and Conditions**: - Use the `IsolationForest` algorithm from scikit-learn. - Fit the IsolationForest model on the `X_train` data. - Use the `predict` method of the IsolationForest to determine whether each sample in `X_test` is an inlier or outlier. - The function must handle different input sizes for `X_train` and `X_test`. 5. **Example**: ```python import numpy as np X_train = np.array([[1, 2], [2, 3], [3, 4], [4, 5], [5, 6]]) X_test = np.array([[2, 3], [10, 10]]) contamination = 0.1 result = detect_anomalies(X_train, X_test, contamination) print(result) # Output could be something like: array([ 1, -1]) ``` **Important Notes**: - Ensure your implementation is efficient and uses the IsolationForest\'s internal methods appropriately. - Handle any potential exceptions, such as value errors for invalid input data.","solution":"import numpy as np from sklearn.ensemble import IsolationForest def detect_anomalies(X_train: np.ndarray, X_test: np.ndarray, contamination: float) -> np.ndarray: Detects anomalies in the test data using the IsolationForest algorithm trained on the training data. Parameters: - X_train (np.ndarray): The training data with dimensions (n_samples_train, n_features). - X_test (np.ndarray): The testing data with dimensions (n_samples_test, n_features). - contamination (float): The proportion of outliers in the data set, a value between 0 and 0.5. Returns: - predictions (np.ndarray): An array of predictions for the test data. Inliers are labeled `1` and outliers are labeled `-1`. if not (0 < contamination <= 0.5): raise ValueError(\\"Contamination must be between 0 and 0.5\\") # Create an IsolationForest model model = IsolationForest(contamination=contamination, random_state=42) # Fit the model on the training data model.fit(X_train) # Predict the anomalies in the test data predictions = model.predict(X_test) return predictions"},{"question":"# Advanced Seaborn Color Palette Manipulation Objective: You are required to demonstrate your understanding of seaborn color palettes. Your task involves creating a visualization with customized palettes from the seaborn library. This will test your skills in manipulating color themes and applying them effectively to a plot. Problem Statement: Write a function `custom_heatmap_visualization` that takes in a 2D list of numerical values and a color palette name, and returns a heatmap visualization using a seaborn color palette. Requirements: 1. The function should accept the following parameters: - `data` (List[List[float]]): A 2D list of numerical values to be visualized as a heatmap. - `palette_name` (str): A string that represents the name of the seaborn color palette (e.g., \\"husl\\", \\"Set2\\", \\"Spectral\\"). - `cmap` (bool): A boolean that when true switches the palette to a colormap for continuous data visualization. 2. The function should generate a heatmap using the provided `data` and `palette_name`, and optionally use the `cmap` parameter to define the colormap if necessary. 3. Return the heatmap as a matplotlib figure. Input: - `data`: A 2D list of numerical values. For example: ```python data = [[1.0, 2.3, 3.1], [4.2, 0.5, 6.4], [7.1, 8.0, 1.2]] ``` - `palette_name`: A string representing the seaborn color palette. For example: `\\"flare\\"`. - `cmap`: A boolean indicating if the palette should be used as a colormap. Output: - A matplotlib figure object representing the heatmap visualization with the customized color palette. Constraints: - The function should handle cases where the `palette_name` is invalid by raising a `ValueError` with an appropriate message. - The 2D list `data` should have numerical values only. Non-numerical values should raise a `TypeError`. Example Function Call: ```python data = [[1.0, 2.3, 3.1], [4.2, 0.5, 6.4], [7.1, 8.0, 1.2]] palette_name = \\"flare\\" cmap = True fig = custom_heatmap_visualization(data, palette_name, cmap) # The function should return a matplotlib figure with the heatmap visualized using the \\"flare\\" colormap ``` Performance Requirement: - The function should efficiently handle large 2D lists (up to 1000x1000 elements) without significant performance degradation. Good luck!","solution":"import seaborn as sns import matplotlib.pyplot as plt import numpy as np def custom_heatmap_visualization(data, palette_name, cmap): Generates a heatmap visualization using the provided data and palette name. Parameters: - data (List[List[float]]): A 2D list of numerical values. - palette_name (str): The name of the seaborn color palette. - cmap (bool): When true, switches the palette to a colormap for continuous data visualization. Returns: - matploblib.figure.Figure: The heatmap visualization figure. if not isinstance(data, list) or not all(isinstance(row, list) for row in data): raise TypeError(\\"Data must be a 2D list of numerical values.\\") try: if cmap: cmap_name = sns.color_palette(palette_name, as_cmap=True) else: cmap_name = sns.color_palette(palette_name) except ValueError as e: raise ValueError(f\\"Invalid palette name: {palette_name}\\") from e # Converting data to numpy array for seaborn heatmap data_array = np.array(data) if not np.issubdtype(data_array.dtype, np.number): raise TypeError(\\"Data must only contain numerical values.\\") fig, ax = plt.subplots() sns.heatmap(data_array, cmap=cmap_name, ax=ax) return fig"},{"question":"# Coding Challenge: Enhanced Directory Comparison You are tasked with writing an enhanced directory comparison tool by extending the functionality provided by the `filecmp` module. This tool should provide detailed comparison reports of files and directories, including nested directory structures, with some additional features. Requirements: 1. Create a subclass `EnhancedDirCmp` of the `filecmp.dircmp` class with the following additional features: - **New Attributes**: - `self.exclusive_to_left`: A list of file and directory names that exist only in the left directory and its subdirectories. - `self.exclusive_to_right`: A list of file and directory names that exist only in the right directory and its subdirectories. - **Method**: `generate_report` - Description: Recursively generate a detailed report comparing `left` and `right` directory structures. The report should include: - Files and directories only in the left directory. - Files and directories only in the right directory. - Files present in both directories but with different contents. - Files present in both directories and with identical contents. - Return: A dictionary containing the report. 2. Implement the `generate_report` method to produce the report in the required format. Input Format: - Two directory paths as strings, `dir1` and `dir2`. Output Format: - A dictionary with the following keys: - `exclusive_to_left`: List of files and directories only in `dir1`. - `exclusive_to_right`: List of files and directories only in `dir2`. - `different_files`: List of files in both directories but with different contents. - `identical_files`: List of files in both directories with identical contents. Constraints: - Only consider files and directories that the current user has permission to access. - Assume directory structures are not deeply nested beyond 3 levels for performance reasons. Example usage: ```python ecmp = EnhancedDirCmp(\'path/to/dir1\', \'path/to/dir2\') report = ecmp.generate_report() print(report) ``` Example output: ```python { \'exclusive_to_left\': [\'unique_file_in_dir1.txt\', \'subdir_only_in_dir1\'], \'exclusive_to_right\': [\'unique_file_in_dir2.txt\', \'subdir_only_in_dir2\'], \'different_files\': [\'common_file_with_different_content.txt\'], \'identical_files\': [\'common_file_with_identical_content.txt\'] } ``` Implement the `EnhancedDirCmp` class and the `generate_report` method according to these specifications. Be sure to use the `filecmp` module effectively.","solution":"import filecmp import os class EnhancedDirCmp(filecmp.dircmp): def __init__(self, left, right, ignore=None, hide=None): super().__init__(left, right, ignore, hide) self.exclusive_to_left = [] self.exclusive_to_right = [] def _calculate_exclusives(self, dcmp): for name in dcmp.left_only: self.exclusive_to_left.append(os.path.join(dcmp.left, name)) for name in dcmp.right_only: self.exclusive_to_right.append(os.path.join(dcmp.right, name)) for sub_dcmp in dcmp.subdirs.values(): self._calculate_exclusives(sub_dcmp) def generate_report(self): self._calculate_exclusives(self) report = { \'exclusive_to_left\': self.exclusive_to_left, \'exclusive_to_right\': self.exclusive_to_right, \'different_files\': self.diff_files, \'identical_files\': self.same_files } return report"},{"question":"**Question:** You have been tasked with creating a secure login system that prompts users for their username and password. Using the `getpass` module, implement a Python function `secure_login` that: 1. Prompts the user for their username and password using secure, non-echoing input for the password. 2. Compares the input against a predefined dictionary of valid usernames and passwords. 3. Returns a message indicating whether the user has successfully logged in or not. # Function Signature ```python def secure_login(users: dict) -> str: pass ``` # Input - `users` (dict): A dictionary where keys are valid usernames and values are their corresponding passwords (both strings). # Output - `str`: A message indicating whether the login was successful or not. # Example ```python users = { \\"user1\\": \\"password123\\", \\"user2\\": \\"my_secure_password\\", \\"admin\\": \\"adminpass\\" } print(secure_login(users)) ``` Possible Output: ``` Enter username: user1 Password: Login successful! ``` or ``` Enter username: user1 Password: Login failed! Invalid username or password. ``` # Constraints - The username and password check should be case-sensitive. - The function should handle cases where either the username or password (or both) are incorrect gracefully. - Assume that you are running within an environment that supports the `getpass` module (if not, consider raising a meaningful error message). # Performance Requirements - Your solution should effectively handle the typical usage patterns of a small to medium-sized user base (up to a few hundred entries). # Additional Notes - Use `getpass.getpass` to ensure the password is securely entered without echoing. - Use `getpass.getuser` to fetch a default username if no username is entered.","solution":"import getpass def secure_login(users: dict) -> str: Prompts user for their username and password securely. Checks the provided credentials against a predefined dictionary of valid usernames and passwords. Parameters: users (dict): A dictionary where keys are valid usernames and values are their corresponding passwords. Returns: str: A message indicating whether the login was successful or not. username = input(\\"Enter username: \\") password = getpass.getpass(\\"Password: \\") if username in users and users[username] == password: return \\"Login successful!\\" else: return \\"Login failed! Invalid username or password.\\""},{"question":"Numerical Stability in Batched Matrix Computations Implement a function `stable_batched_matrix_mult` in PyTorch that performs batched matrix multiplication while ensuring numerical stability and handling precision issues as described in the PyTorch documentation. # Function Signature ```python def stable_batched_matrix_mult(A: torch.Tensor, B: torch.Tensor) -> torch.Tensor: pass ``` # Input: - `A` (torch.Tensor): A 3D tensor representing a batch of matrices. Its shape is `(batch_size, n, m)`. - `B` (torch.Tensor): A 3D tensor representing another batch of matrices. Its shape is `(batch_size, m, p)`. # Output: - Returns a 3D tensor of shape `(batch_size, n, p)` resulting from the batched matrix multiplication of inputs `A` and `B` with enhanced numerical stability. # Constraints: 1. The function must handle potential numerical instability: - Check for `inf` and `NaN` values in the inputs and raise a `ValueError` if any are found. - Use double precision (float64) to perform calculations if possible. 2. Implement additional measures if inputs are detected to be near singular or ill-conditioned: - Regularize the input matrices to avoid near-singular matrices causing instabilities. 3. Ensure that you handle GPU (CUDA) and CPU computations correctly by avoiding any device-specific errors or significant precision loss. # Notes: - You should explicitly ensure numerical stability by considering matrix conditioning, using functions such as `torch.linalg.svdvals` and `torch.linalg.cond` to detect ill-conditioned matrices. - The PyTorch operations you use must be justified in your comments explaining why they help in maintaining numerical stability. - Use `torch.set_default_dtype(torch.float64)` for the computations to avoid loss of precision. # Example: ```python import torch def stable_batched_matrix_mult(A: torch.Tensor, B: torch.Tensor) -> torch.Tensor: # Ensure default tensor type is double precision for this function torch.set_default_dtype(torch.float64) # Check for any inf or NaN values in the input tensors if not torch.isfinite(A).all() or not torch.isfinite(B).all(): raise ValueError(\\"Input tensors contain non-finite values (inf or NaN).\\") # Regularize matrices if necessary condA = torch.linalg.cond(A) if torch.any(condA > 1e12): # Assuming a threshold for ill-conditioned matrices U, S, V = torch.linalg.svd(A) S = torch.where(S < 1e-12, torch.tensor(1e-12, dtype=torch.float64), S) A = U @ torch.diag_embed(S) @ V condB= torch.linalg.cond(B) if torch.any(condB> 1e12): # Assuming a threshold for ill-conditioned matrices U, S, V = torch.linalg.svd(B) S = torch.where(S < 1e-12, torch.tensor(1e-12, dtype=torch.float64), S) B = U @ torch.diag_embed(S) @ V # Perform the batched matrix multiplication result = torch.bmm(A, B) return result # Sample Usage A = torch.randn((10, 4, 4), dtype=torch.float64) B = torch.randn((10, 4, 4), dtype=torch.float64) result = stable_batched_matrix_mult(A, B) print(result.shape) # Should print (10, 4, 4) ``` This question is designed to test the students\' understanding of numerical stability issues in PyTorch and their ability to handle advanced tensor operations while ensuring the accuracy and consistency described in the documentation.","solution":"import torch def stable_batched_matrix_mult(A: torch.Tensor, B: torch.Tensor) -> torch.Tensor: Performs batched matrix multiplication with numerical stability measures. Parameters: A (torch.Tensor): A 3D tensor representing a batch of matrices (batch_size, n, m). B (torch.Tensor): A 3D tensor representing another batch of matrices (batch_size, m, p). Returns: torch.Tensor: A 3D tensor of shape (batch_size, n, p) resulting from batched matrix multiplication. # Ensure default tensor type is double precision for this function original_dtype = torch.get_default_dtype() torch.set_default_dtype(torch.float64) # Check for any inf or NaN values in the input tensors if not torch.isfinite(A).all() or not torch.isfinite(B).all(): torch.set_default_dtype(original_dtype) # Restore original default dtype before raising error raise ValueError(\\"Input tensors contain non-finite values (inf or NaN).\\") # Regularize matrices if necessary condA = torch.linalg.cond(A) if torch.any(condA > 1e12): # Assuming a threshold for ill-conditioned matrices U, S, V = torch.linalg.svd(A) S = torch.where(S < 1e-12, torch.tensor(1e-12, dtype=torch.float64, device=A.device), S) A = U @ torch.diag_embed(S) @ V condB = torch.linalg.cond(B) if torch.any(condB > 1e12): # Assuming a threshold for ill-conditioned matrices U, S, V = torch.linalg.svd(B) S = torch.where(S < 1e-12, torch.tensor(1e-12, dtype=torch.float64, device=B.device), S) B = U @ torch.diag_embed(S) @ V # Perform the batched matrix multiplication result = torch.bmm(A, B) # Restore the original dtype torch.set_default_dtype(original_dtype) return result"},{"question":"# Python Packaging and Distribution Challenge You are required to create a Python application that performs a specific task, manage its dependencies using virtual environments, and package it as an executable Python zip archive. Follow the steps below to complete the task: Part 1: Create a Python Application 1. Write a Python script `app.py` that reads a text file named `input.txt`, counts the number of words in the file, and prints the result. 2. Ensure `app.py` accepts the file path as a command-line argument. Example usage: ```bash python app.py input.txt ``` Expected output: ```text The file contains X words. ``` Part 2: Manage Dependencies using Virtual Environments 1. Create a virtual environment using the `venv` module. 2. Install any necessary packages (if any) within this virtual environment. 3. Write a `requirements.txt` file that lists all installed packages. Part 3: Package the Application using `zipapp` 1. Package the entire application into an executable zip archive named `word_counter.pyz` using `zipapp`. 2. Ensure that the packaged application can be run directly using: ```bash python word_counter.pyz input.txt ``` Submission Requirements 1. Submit your `app.py` script. 2. Submit the `requirements.txt` file. 3. Submit the `word_counter.pyz` executable. 4. Submit a `README.md` file documenting the steps you followed to create the virtual environment, install dependencies, and package the application. Constraints and Limitations - Your solution should work on Python 3.10. - Do not use external libraries for file reading; use Python\'s standard library. - Ensure that your packaged application runs without requiring external dependencies installation. Performance Requirements - Your application should efficiently handle text files up to 10MB in size. # Evaluation Criteria - **Correctness**: Does the application correctly count the number of words in a file? - **Virtual Environment Management**: Is a virtual environment created properly, and are dependencies managed within it? - **Packaging**: Is the application correctly packaged into an executable zip archive? - **Documentation**: Is the process documented clearly in the `README.md` file?","solution":"import sys def count_words(file_path): Count the number of words in the given file. Args: file_path (str): Path to the input text file. Returns: int: Number of words in the file. try: with open(file_path, \'r\') as file: contents = file.read() words = contents.split() return len(words) except Exception as e: print(f\\"Error: {e}\\") return 0 if __name__ == \\"__main__\\": if len(sys.argv) < 2: print(\\"Usage: python app.py <file_path>\\") else: file_path = sys.argv[1] word_count = count_words(file_path) print(f\\"The file contains {word_count} words.\\")"},{"question":"Objective The goal of this assessment is to demonstrate your understanding of using scikit-learn for outlier and novelty detection by implementing, training, and evaluating different models to distinguish between normal and abnormal data points. Problem Statement You are given a dataset containing two numerical features. The dataset includes both inliers and outliers, but you need to determine the inliers in the testing phase. Your task is to: 1. Implement a solution that uses two different outlier detection methods from scikit-learn. 2. Train these models on the provided training data. 3. Use these trained models to identify and label inliers and outliers in the test data. 4. Compare the performance of these models based on their prediction results. Dataset The dataset consists of two parts: - `X_train`: A numpy array of shape (n_train_samples, 2) containing the training data. - `X_test`: A numpy array of shape (n_test_samples, 2) containing the test data to be predicted. Instructions 1. **Load the Data**: Load `X_train` and `X_test` from the given numpy arrays. 2. **Model Implementations**: - **Model 1**: Train an IsolationForest on `X_train` and predict on `X_test` using `predict` method. - **Model 2**: Train a Local Outlier Factor (LOF) with `novelty=True` on `X_train` and predict on `X_test`. 3. **Prediction Outputs**: - Generate predictions for the `X_test` dataset using both models. - Inliers should be labeled as `1` and outliers as `-1`. 4. **Comparison of Models**: - Print the predictions for `X_test` from both models. - Discuss the differences in the outcomes from the two models. Constraints - Use `IsolationForest` with default parameters, but ensure reproducibility by setting a random seed. - Use `LocalOutlierFactor` with `novelty=True` and default parameters. Example Output Python code to implement the above pipeline would look like: ```python from sklearn.ensemble import IsolationForest from sklearn.neighbors import LocalOutlierFactor import numpy as np # Load given data (X_train, X_test) X_train = np.array([[0.2, 0.5], [1.3, 1.5], [-2.0, -1.5], [0.5, 0.8]]) # Example data; replace with actual data X_test = np.array([[0.1, 0.4], [1.1, 1.2], [-2.1, -1.6], [0.4, 0.7]]) # Model 1: IsolationForest iso_forest = IsolationForest(random_state=42) iso_forest.fit(X_train) pred_iso_forest = iso_forest.predict(X_test) # Model 2: Local Outlier Factor with novelty=True lof = LocalOutlierFactor(novelty=True) lof.fit(X_train) pred_lof = lof.predict(X_test) # Output results print(f\\"Isolation Forest Predictions: {pred_iso_forest}\\") print(f\\"Local Outlier Factor Predictions: {pred_lof}\\") ``` Deliverables 1. Provide the complete Python code for the above implementation. 2. Include comments and necessary explanations for your code. 3. Based on the predictions, discuss the performance and differences observed between the two models.","solution":"from sklearn.ensemble import IsolationForest from sklearn.neighbors import LocalOutlierFactor import numpy as np def load_data(): This function should load and return the X_train and X_test datasets. For the purpose of this solution, example X_train and X_test data are provided. X_train = np.array([ [0.2, 0.5], [1.3, 1.5], [-2.0, -1.5], [0.5, 0.8], [1.5, 1.8], [1.0, 1.2], [0.6, 0.9], [-1.5, -1.3], [1.7, 1.9], [0.1, 0.3], [0.9, 1.0], [2.0, 2.1], [1.1, 1.4] ]) X_test = np.array([ [0.1, 0.4], [1.1, 1.2], [-2.1, -1.6], [0.4, 0.7], [1.6, 1.7], [2.1, 2.2], [1.2, 1.3], [-1.7, -1.5], [0.3, 0.6], [0.8, 0.7], [2.5, 2.6], [-0.8, -0.9], [0.6, 0.8] ]) return X_train, X_test def outlier_detection(X_train, X_test): # Model 1: IsolationForest iso_forest = IsolationForest(random_state=42) iso_forest.fit(X_train) pred_iso_forest = iso_forest.predict(X_test) # Model 2: Local Outlier Factor with novelty=True lof = LocalOutlierFactor(novelty=True) lof.fit(X_train) pred_lof = lof.predict(X_test) return pred_iso_forest, pred_lof if __name__ == \\"__main__\\": X_train, X_test = load_data() pred_iso_forest, pred_lof = outlier_detection(X_train, X_test) # Output results print(\\"X_test:\\", X_test.tolist()) print(f\\"Isolation Forest Predictions: {pred_iso_forest.tolist()}\\") print(f\\"Local Outlier Factor Predictions: {pred_lof.tolist()}\\")"},{"question":"# Advanced SQLite3 Operations with Python **Objective**: Demonstrate your understanding of SQLite3 operations in Python by performing a series of tasks that involve creating and manipulating a SQLite database. **Instructions**: 1. **Creating a Database and Table**: - Create a new SQLite database named `school.db`. - Create a table named `students` with the following columns: - `id` (INTEGER, Primary Key, Auto Increment) - `name` (TEXT, cannot be NULL) - `age` (INTEGER) - `grade` (TEXT) 2. **Inserting Data**: - Insert the following records into the `students` table: - (name: Alice, age: 14, grade: 8th) - (name: Bob, age: 15, grade: 9th) - (name: Charlie, age: 13, grade: 7th) 3. **Querying Data**: - Write a function `get_students_by_grade(grade)` that takes a grade as a parameter and returns all students in that grade. - Write a function `get_average_age_by_grade()` that returns a dictionary where the keys are the grades and the values are the average ages of students in those grades. 4. **Updating Records**: - Write a function `update_student_grade(student_id, new_grade)` that updates the grade of a student based on their ID. 5. **Deleting Records**: - Write a function `delete_student(student_id)` that deletes a student from the database based on their ID. # Example Usage: ```python # Create database and table create_database_and_table() # Insert data insert_data() # Get students in 8th grade students_in_8th_grade = get_students_by_grade(\\"8th\\") print(students_in_8th_grade) # Get average age by grade average_age_by_grade = get_average_age_by_grade() print(average_age_by_grade) # Update student grade update_student_grade(1, \\"9th\\") # Delete student delete_student(2) ``` **Constraints**: - Ensure all database operations are committed properly. - Handle any potential SQL errors gracefully. **Note**: The functions should be implemented to handle edge cases such as non-existing records, invalid data types, etc. Use appropriate SQL commands and Python functionalities to achieve these tasks. # Submission: - A Python file containing the implementation of the functions.","solution":"import sqlite3 from contextlib import closing # Create a new SQLite database named school.db def create_database_and_table(): with closing(sqlite3.connect(\'school.db\')) as conn: with conn: conn.execute( CREATE TABLE IF NOT EXISTS students ( id INTEGER PRIMARY KEY AUTOINCREMENT, name TEXT NOT NULL, age INTEGER, grade TEXT ) ) # Insert the data into the students table def insert_data(): students = [(\'Alice\', 14, \'8th\'), (\'Bob\', 15, \'9th\'), (\'Charlie\', 13, \'7th\')] with closing(sqlite3.connect(\'school.db\')) as conn: with conn: conn.executemany( INSERT INTO students (name, age, grade) VALUES (?, ?, ?) , students) # Get all students by grade def get_students_by_grade(grade): with closing(sqlite3.connect(\'school.db\')) as conn: with conn: cur = conn.execute( SELECT * FROM students WHERE grade = ? , (grade,)) return cur.fetchall() # Get average age by grade def get_average_age_by_grade(): with closing(sqlite3.connect(\'school.db\')) as conn: with conn: cur = conn.execute( SELECT grade, AVG(age) FROM students GROUP BY grade ) return {row[0]: row[1] for row in cur.fetchall()} # Update a student\'s grade by ID def update_student_grade(student_id, new_grade): with closing(sqlite3.connect(\'school.db\')) as conn: with conn: conn.execute( UPDATE students SET grade = ? WHERE id = ? , (new_grade, student_id)) # Delete a student by ID def delete_student(student_id): with closing(sqlite3.connect(\'school.db\')) as conn: with conn: conn.execute( DELETE FROM students WHERE id = ? , (student_id,))"},{"question":"Objective: To assess the student\'s understanding of out-of-core learning, feature extraction, and the use of incremental learning algorithms in scikit-learn. Background: You are given a large text dataset that cannot fit into the system\'s main memory. Your task is to build a text classification model that processes this data in mini-batches using scikit-learn\'s HashingVectorizer for feature extraction and one of the incremental learning classifiers. Task: 1. Implement a generator function `stream_data(file_path, batch_size)` that reads a large text data file line by line and yields mini-batches of size `batch_size`. Each line in the file corresponds to a document and its class label, separated by a comma. For example: ``` class1,This is the first document. class2,This is the second document. ``` 2. Use `sklearn.feature_extraction.text.HashingVectorizer` to convert the text data into a numerical format suitable for machine learning. 3. Choose an appropriate incremental learning classifier from the list below and train the classifier using the batched data: - `sklearn.linear_model.SGDClassifier` - `sklearn.linear_model.Perceptron` - `sklearn.naive_bayes.MultinomialNB` 4. Evaluate your model after each batch and print the running accuracy. Requirements: - **Function 1**: `stream_data(file_path: str, batch_size: int) -> Generator`: Reads the data file and yields mini-batches. - **Function 2**: `train_and_evaluate(file_path: str, batch_size: int, num_batches: int) -> None`: Uses the generator to stream data, extracts features using HashingVectorizer, trains the incremental learning classifier, and prints the running accuracy after each batch. Constraints: - Assume the class labels are binary: \'class1\' and \'class2\'. - The input file can be very large, so ensure your generator handles memory efficiently. - The HashingVectorizer should use its default parameters. - Use an 80-20 split for training and evaluation within each batch. Input: - `file_path` (string): The path to the text data file. - `batch_size` (int): The number of instances in each mini-batch. - `num_batches` (int): The number of mini-batches to process. Output: - Print the accuracy after processing each batch. Example: ```python def stream_data(file_path: str, batch_size: int) -> Generator: # Implement this function pass def train_and_evaluate(file_path: str, batch_size: int, num_batches: int): # Implement this function pass # Example usage (file \'data.txt\') train_and_evaluate(\'data.txt\', batch_size=100, num_batches=10) ```","solution":"import numpy as np from sklearn.feature_extraction.text import HashingVectorizer from sklearn.linear_model import SGDClassifier from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score import itertools def stream_data(file_path, batch_size): with open(file_path, \'r\', encoding=\'utf-8\') as f: lines = [] for line in f: lines.append(line.strip()) if len(lines) == batch_size: yield lines lines = [] if lines: yield lines def train_and_evaluate(file_path, batch_size, num_batches): vectorizer = HashingVectorizer() classifier = SGDClassifier() for i, batch in enumerate(itertools.islice(stream_data(file_path, batch_size), num_batches)): texts, labels = zip(*[line.split(\',\', 1) for line in batch]) labels = [1 if label == \'class1\' else 0 for label in labels] X = vectorizer.transform(texts) X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, random_state=42) classifier.partial_fit(X_train, y_train, classes=[0, 1]) y_pred = classifier.predict(X_test) accuracy = accuracy_score(y_test, y_pred) print(f\\"Batch {(i + 1)}: Accuracy = {accuracy}\\") if i == num_batches - 1: break"},{"question":"# Question: Implementing a Temporary File Manager You will be implementing a class `TemporaryFileManager` that manages temporary files and directories using the `tempfile` module. Your class should provide methods to: 1. Create a temporary file and write data to it. 2. Read data from the temporary file. 3. Create a named temporary file and provide its name. 4. Create a temporary directory and return its name. 5. Automatically clean up files and directories when they are no longer needed. # Class Definition ```python import tempfile import os class TemporaryFileManager: def __init__(self): self.temp_files = [] self.temp_dirs = [] def create_temp_file(self, data: bytes) -> tempfile._TemporaryFileWrapper: Create a temporary file and write given data to it. :param data: Data to write to the temporary file. :return: A file-like object representing the temporary file. pass def read_temp_file(self, temp_file: tempfile._TemporaryFileWrapper) -> bytes: Read data from a given temporary file. :param temp_file: File-like object representing the temporary file. :return: The data read from the file. pass def create_named_temp_file(self) -> str: Create a named temporary file and return its name. :return: Name of the temporary file. pass def create_temp_directory(self) -> str: Create a temporary directory and return its name. :return: Name of the temporary directory. pass def cleanup(self): Clean up all temporary files and directories created by this manager. pass ``` # Constraints - Do not make any assumptions about the size of the data. - All temporary files and directories created by this class should be cleaned up when no longer needed. # Example Usage ```python if __name__ == \\"__main__\\": manager = TemporaryFileManager() # Example of creating and reading a temporary file temp_file = manager.create_temp_file(b\\"Hello, World!\\") content = manager.read_temp_file(temp_file) print(content) # Output: b\'Hello, World!\' # Example of creating a named temporary file named_temp_file = manager.create_named_temp_file() print(named_temp_file) # Output: The name of the temporary file # Example of creating a temporary directory temp_dir = manager.create_temp_directory() print(temp_dir) # Output: The name of the temporary directory # Cleanup all managed files and directories manager.cleanup() ``` # Notes - Utilize the `tempfile` module for file and directory creation. - Ensure that all temporary files and directories are properly cleaned up when the `cleanup()` method is called. - Make sure that temporary files created with `create_temp_file` are properly managed and removed when closed or during cleanup.","solution":"import tempfile import os class TemporaryFileManager: def __init__(self): self.temp_files = [] self.temp_dirs = [] def create_temp_file(self, data: bytes) -> tempfile._TemporaryFileWrapper: Create a temporary file and write given data to it. :param data: Data to write to the temporary file. :return: A file-like object representing the temporary file. temp_file = tempfile.TemporaryFile() temp_file.write(data) temp_file.seek(0) # Move the file pointer to the beginning self.temp_files.append(temp_file) return temp_file def read_temp_file(self, temp_file: tempfile._TemporaryFileWrapper) -> bytes: Read data from a given temporary file. :param temp_file: File-like object representing the temporary file. :return: The data read from the file. temp_file.seek(0) # Move the file pointer to the beginning return temp_file.read() def create_named_temp_file(self) -> str: Create a named temporary file and return its name. :return: Name of the temporary file. temp_file = tempfile.NamedTemporaryFile(delete=False) self.temp_files.append(temp_file) return temp_file.name def create_temp_directory(self) -> str: Create a temporary directory and return its name. :return: Name of the temporary directory. temp_dir = tempfile.TemporaryDirectory() self.temp_dirs.append(temp_dir) return temp_dir.name def cleanup(self): Clean up all temporary files and directories created by this manager. for temp_file in self.temp_files: try: temp_file.close() if hasattr(temp_file, \'name\'): os.remove(temp_file.name) except Exception as e: pass # Handle exceptions if required for temp_dir in self.temp_dirs: try: temp_dir.cleanup() except Exception as e: pass # Handle exceptions if required self.temp_files = [] self.temp_dirs = []"},{"question":"# Question: Custom Seaborn Color Palette Visualization You are provided a dataset which contains several numeric columns. Your task is to create a function that generates multiple visualizations of this dataset using different types of color palettes provided by Seaborn. You will generate: 1. A scatter plot using a qualitative palette. 2. A heatmap using a sequential palette. 3. A heatmap using a diverging palette. Function Signature ```python def visualize_data(df: pd.DataFrame, qualitative_palette: str, sequential_palette: str, diverging_palette: str) -> None: pass ``` Input - `df` (pd.DataFrame): A pandas DataFrame containing at least three numeric columns. - `qualitative_palette` (str): Name of the qualitative palette to be used for the scatter plot. - `sequential_palette` (str): Name of the sequential palette to be used for the first heatmap. - `diverging_palette` (str): Name of the diverging palette to be used for the second heatmap. Output - This function does not return anything. Its purpose is to generate and show three plots as described. Constraints - The scatter plot should plot points using the first two numeric columns of the DataFrame. - The heatmaps should visualize the entire correlation matrix of the DataFrame using the specified color palettes. - Ensure that the plots are correctly labeled and have color bars where appropriate. Example ```python import pandas as pd import seaborn as sns import numpy as np # Generate a sample DataFrame np.random.seed(0) data = np.random.rand(10, 3) df = pd.DataFrame(data, columns=[\\"col1\\", \\"col2\\", \\"col3\\"]) # Call your function visualize_data(df, \\"muted\\", \\"rocket\\", \\"vlag\\") ``` This should generate: 1. A scatter plot of `col1` vs `col2` using a \'muted\' qualitative palette. 2. A heatmap of the correlation matrix using the \'rocket\' sequential palette. 3. A heatmap of the same correlation matrix using the \'vlag\' diverging palette. Notes - Use `plt.show()` to display each plot. - Handle edge cases such as the DataFrame having fewer than three numeric columns by raising a ValueError with an appropriate message.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt import numpy as np def visualize_data(df: pd.DataFrame, qualitative_palette: str, sequential_palette: str, diverging_palette: str) -> None: Generates and displays three types of visualizations: 1. A scatter plot using a qualitative palette. 2. A heatmap using a sequential palette. 3. A heatmap using a diverging palette. Args: df (pd.DataFrame): A pandas DataFrame containing at least three numeric columns. qualitative_palette (str): Name of the qualitative palette to be used for the scatter plot. sequential_palette (str): Name of the sequential palette to be used for the first heatmap. diverging_palette (str): Name of the diverging palette to be used for the second heatmap. Raises: ValueError: If the DataFrame does not contain at least three numeric columns. if len(df.select_dtypes(include=np.number).columns) < 3: raise ValueError(\\"The DataFrame must contain at least three numeric columns.\\") # Scatter plot using the first two numeric columns numeric_cols = df.select_dtypes(include=np.number).columns col1 = numeric_cols[0] col2 = numeric_cols[1] plt.figure(figsize=(10, 6)) sns.scatterplot(data=df, x=col1, y=col2, palette=qualitative_palette) plt.title(\\"Scatter Plot using Qualitative Palette\\") plt.xlabel(col1) plt.ylabel(col2) plt.show() # Heatmap using sequential palette corr = df.corr() plt.figure(figsize=(10, 6)) sns.heatmap(corr, cmap=sequential_palette, annot=True) plt.title(\\"Heatmap using Sequential Palette\\") plt.show() # Heatmap using diverging palette plt.figure(figsize=(10, 6)) sns.heatmap(corr, cmap=diverging_palette, annot=True) plt.title(\\"Heatmap using Diverging Palette\\") plt.show()"},{"question":"# JSON Processing and Custom Encoding/Decoding in Python **Objective:** - Test student\'s understanding of the Python `json` module, custom encoding, and decoding functionality. **Problem Statement:** You are tasked with designing a system that can serialize and deserialize Python objects involving complex numbers and custom data structures. The `json` module should be used for all serialization and deserialization tasks. # Definitions - A **complex number** should be encoded as a dictionary with the format: ```json { \\"__complex__\\": true, \\"real\\": <real_part>, \\"imag\\": <imaginary_part> } ``` - A **person object** should contain the following fields: - `name` (string) - `age` (integer) - `address` (dict with arbitrary number of fields) - `favorite_numbers` (list of integers) # Requirements 1. **Custom Encoder**: Implement a custom JSON encoder (`CustomEncoder`) that correctly serializes complex numbers and person objects. 2. **Custom Decoder**: Implement a custom JSON decoder (`custom_decoder`) that correctly deserializes the data back into appropriate Python objects. 3. **Serialization/Deserialization Functions**: `serialize_data(data: Any, file_path: str) -> None` - **Input**: - `data`: The data to be serialized (could be any Python object, including complex numbers and person objects). - `file_path`: The file path where serialized JSON should be saved. - **Output**: None `deserialize_data(file_path: str) -> Any` - **Input**: - `file_path`: The file path from where the serialized JSON should be read. - **Output**: Deserialized Python object. # Constraints - Ensure no circular references exist in the data to be serialized. - Handle large JSON texts efficiently without causing `MemoryError`. # Example ```python # Example person object person = { \\"name\\": \\"John Doe\\", \\"age\\": 30, \\"address\\": { \\"street\\": \\"123 Main St\\", \\"city\\": \\"Anytown\\", \\"country\\": \\"Anyland\\" }, \\"favorite_numbers\\": [7, 13, 42, 100] } # Example complex number complex_number = 3 + 4j # Serializing the objects serialize_data([person, complex_number], \'data.json\') # Deserializing the objects data = deserialize_data(\'data.json\') print(data) ``` # Implementation Guidelines 1. Extend `json.JSONEncoder` to create `CustomEncoder` for handling complex numbers and person objects. 2. Create a custom decoding hook to handle the deserialization of complex numbers. 3. Implement the `serialize_data()` and `deserialize_data()` functions using the `json.dump()`/`json.dumps()` and `json.load()`/`json.loads()` methods respectively. Provide the implementation for the following: - `CustomEncoder` class - `custom_decoder` function - `serialize_data` function - `deserialize_data` function","solution":"import json class CustomEncoder(json.JSONEncoder): def default(self, obj): if isinstance(obj, complex): return {\\"__complex__\\": True, \\"real\\": obj.real, \\"imag\\": obj.imag} return super().default(obj) def custom_decoder(dct): if \\"__complex__\\" in dct: return complex(dct[\\"real\\"], dct[\\"imag\\"]) return dct def serialize_data(data, file_path): with open(file_path, \'w\') as file: json.dump(data, file, cls=CustomEncoder) def deserialize_data(file_path): with open(file_path, \'r\') as file: return json.load(file, object_hook=custom_decoder)"},{"question":"**Title: Implementing Custom Audio Manipulation** You are given a set of operations provided by the \\"audioop\\" module in Python for manipulating raw audio data. Your task is to design and implement a function that combines several of these operations to perform a specific audio processing task. # Problem Statement Write a function `process_audio(audio_data: bytes, sample_width: int, operations: List[str]) -> bytes` that takes in the following parameters: 1. `audio_data` (bytes): The raw audio data to be processed. 2. `sample_width` (int): The width of each audio sample in bytes (1, 2, 3, or 4). 3. `operations` (List[str]): A list of operations to be performed on the audio data. The operations will be in the form of strings indicating the operation and parameters where applicable. Supported operations include: - \\"add [fragment2]\\": Adds the given fragment to `audio_data`. - \\"mul [factor]\\": Multiplies each sample in `audio_data` by the given factor. - \\"bias [bias_value]\\": Adds a bias to each sample in `audio_data`. - \\"reverse\\": Reverses the `audio_data`. - \\"cross\\": Returns the number of zero-crossings in the `audio_data`. - \\"avg\\": Computes the average value of the samples in `audio_data`. - \\"rms\\": Computes the root-mean-square value of the samples in `audio_data`. - \\"max\\": Returns the maximum sample value in the `audio_data`. The function should process the operations in the order they are provided in the list. If an operation is not applicable (e.g., \\"cross\\", \\"avg\\", \\"rms\\", \\"max\\"), the function should ignore it and proceed with the next operation. The return value should be the processed audio data after all applicable operations have been executed. # Constraints - The lengths of `audio_data` and `fragment2` (if used) will be the same. - The function should handle exceptions gracefully. If an invalid operation or parameter is encountered, it should raise a ValueError with a descriptive message. # Example Usage ```python audio_data = b\'x01x02x03x04\' sample_width = 1 operations = [\\"mul 2.0\\", \\"add b\'x01x02x03x04\'\\", \\"reverse\\"] result = process_audio(audio_data, sample_width, operations) print(result) # should print the processed audio data in byte format after applying the operations ``` # Implementation Note You can refer to the Python documentation of the \\"audioop\\" module for the specifics of each function.","solution":"import audioop from typing import List def process_audio(audio_data: bytes, sample_width: int, operations: List[str]) -> bytes: for operation in operations: try: if operation.startswith(\\"add\\"): fragment2 = eval(operation.split(\' \')[1]) audio_data = audioop.add(audio_data, fragment2, sample_width) elif operation.startswith(\\"mul\\"): factor = float(operation.split(\' \')[1]) audio_data = audioop.mul(audio_data, sample_width, factor) elif operation.startswith(\\"bias\\"): bias_value = int(operation.split(\' \')[1]) audio_data = audioop.bias(audio_data, sample_width, bias_value) elif operation == \\"reverse\\": audio_data = audioop.reverse(audio_data, sample_width) except Exception as e: raise ValueError(f\\"Invalid operation or parameter: {operation}\\") from e return audio_data"},{"question":"# Question: House Price Prediction using Composite Estimators You are tasked with predicting house prices based on various features such as the number of rooms, location, and type of house. You have the following dataset: ```python import pandas as pd data = pd.DataFrame({ \'num_rooms\': [3, 4, 2, 3, 4, 5], \'location\': [\'city\', \'city\', \'suburb\', \'suburb\', \'rural\', \'rural\'], \'house_type\': [\'apartment\', \'house\', \'house\', \'apartment\', \'house\', \'apartment\'], \'price\': [300000, 450000, 250000, 320000, 400000, 500000] }) ``` # Task 1. **Create a Pipeline**: - Use `ColumnTransformer` to preprocess the data: - **Transform `num_rooms`**: Scale the numerical feature `num_rooms` using `StandardScaler`. - **Transform `location`**: Encode the categorical feature `location` using `OneHotEncoder`. - Use `FeatureUnion` to combine these transformations. - Connect the transformations with a `LinearRegression` model. 2. **Fit the Pipeline**: - Fit the entire pipeline on the given dataset (excluding the target `price`). 3. **Make Predictions**: - Predict the prices using the pipeline you created. 4. **Evaluate the Model**: - Calculate and print the mean squared error of your predictions. # Expected Input - A pandas DataFrame `data` with columns `num_rooms`, `location`, `house_type`, and `price`. # Expected Output - Print the mean squared error of the model\'s predictions. # Function Signature ```python def house_price_prediction(data: pd.DataFrame) -> None: pass ``` # Constraints - Use `scikit-learn` library version 0.24 or above. - You may not modify the given dataset. - Use `LinearRegression` from `sklearn.linear_model`. # Hints - Refer to the documentation on `Pipeline`, `ColumnTransformer`, and `FeatureUnion` to understand their usage and syntax. Your task is to implement the `house_price_prediction` function that follows the above requirements.","solution":"import pandas as pd from sklearn.compose import ColumnTransformer from sklearn.compose import make_column_selector as selector from sklearn.preprocessing import StandardScaler, OneHotEncoder from sklearn.pipeline import Pipeline from sklearn.linear_model import LinearRegression from sklearn.metrics import mean_squared_error from sklearn import set_config def house_price_prediction(data: pd.DataFrame) -> None: # Separating features and the target variable X = data.drop(columns=[\'price\']) y = data[\'price\'] # Preprocessing the columns with ColumnTransformer preprocessor = ColumnTransformer( transformers=[ (\'num_rooms\', StandardScaler(), [\'num_rooms\']), (\'location\', OneHotEncoder(), [\'location\']) ] ) # Creating the pipeline with preprocessing and Linear Regression pipeline = Pipeline(steps=[ (\'preprocessor\', preprocessor), (\'regressor\', LinearRegression()) ]) # Fitting the pipeline on the data pipeline.fit(X, y) # Making predictions predictions = pipeline.predict(X) # Calculating and printing the mean squared error mse = mean_squared_error(y, predictions) print(f\'Mean Squared Error: {mse:.2f}\') data = pd.DataFrame({ \'num_rooms\': [3, 4, 2, 3, 4, 5], \'location\': [\'city\', \'city\', \'suburb\', \'suburb\', \'rural\', \'rural\'], \'house_type\': [\'apartment\', \'house\', \'house\', \'apartment\', \'house\', \'apartment\'], \'price\': [300000, 450000, 250000, 320000, 400000, 500000] }) # Call the function house_price_prediction(data)"},{"question":"# Multiprocessing with Shared State and Synchronization You are tasked with developing a Python application that uses the `multiprocessing` package to simulate a simple bank transaction system. The system involves multiple clients performing deposit and withdrawal operations concurrently on shared bank accounts. The goal is to ensure that the operations are synchronized correctly to avoid race conditions. Requirements: 1. **Create a Banking System**: - Implement a shared `Account` class with a balance. - The `Account` class should support `deposit(amount)` and `withdraw(amount)` methods that modify the balance. 2. **Shared State**: - Use `multiprocessing.Value` to represent the shared balance of the account. 3. **Synchronization**: - Use `multiprocessing.Lock` to ensure that the deposit and withdrawal operations on the account are thread-safe. 4. **Client Simulation**: - Implement a `Client` function that performs a series of deposits and withdrawals on the `Account` instance. - Simulate multiple clients performing operations concurrently by creating multiple processes. 5. **Result Verification**: - After all operations are complete, verify and print the final balance of the account. Input: - List of tuples representing operations to be performed by each client in the form: `[(operation, amount), ...]`. Output: - The final balance of the account after all operations by all clients. Constraints: - Ensure that the final balance is consistent with the sum of all operations performed by all clients. Example: ```python from multiprocessing import Process, Value, Lock class Account: def __init__(self, balance, lock): self.balance = balance self.lock = lock def deposit(self, amount): with self.lock: self.balance.value += amount def withdraw(self, amount): with self.lock: self.balance.value -= amount def client_operations(account, operations): for operation, amount in operations: if operation == \'deposit\': account.deposit(amount) elif operation == \'withdraw\': account.withdraw(amount) if __name__ == \'__main__\': # Initialize account with a balance of 100 initial_balance = 100 balance = Value(\'i\', initial_balance) lock = Lock() account = Account(balance, lock) # Define client operations client1_operations = [(\'deposit\', 50), (\'withdraw\', 20), (\'deposit\', 100)] client2_operations = [(\'withdraw\', 70), (\'deposit\', 150), (\'withdraw\', 30)] # Create client processes client1 = Process(target=client_operations, args=(account, client1_operations)) client2 = Process(target=client_operations, args=(account, client2_operations)) # Start client processes client1.start() client2.start() # Wait for processes to complete client1.join() client2.join() # Print the final balance print(f\'Final balance: {account.balance.value}\') ``` Implement the `Account` class, the `client_operations` function, and the multiprocessing setup to ensure thread-safe operations on the shared account balance. Verify that the final balance is correct given the list of operations performed by each client.","solution":"from multiprocessing import Process, Value, Lock class Account: def __init__(self, initial_balance): self.balance = Value(\'i\', initial_balance) self.lock = Lock() def deposit(self, amount): with self.lock: self.balance.value += amount def withdraw(self, amount): with self.lock: self.balance.value -= amount def client_operations(account, operations): for operation, amount in operations: if operation == \'deposit\': account.deposit(amount) elif operation == \'withdraw\': account.withdraw(amount) if __name__ == \'__main__\': # Initialize account with a balance of 100 initial_balance = 100 account = Account(initial_balance) # Define client operations client1_operations = [(\'deposit\', 50), (\'withdraw\', 20), (\'deposit\', 100)] client2_operations = [(\'withdraw\', 70), (\'deposit\', 150), (\'withdraw\', 30)] # Create client processes client1 = Process(target=client_operations, args=(account, client1_operations)) client2 = Process(target=client_operations, args=(account, client2_operations)) # Start client processes client1.start() client2.start() # Wait for processes to complete client1.join() client2.join() # Print the final balance print(f\'Final balance: {account.balance.value}\')"},{"question":"# Python String Conversion and Formatting Background The `python310` package provides several functions for string conversion and formatting. Some key functions include: - `PyOS_string_to_double` for converting strings to double precision floating-point numbers. - `PyOS_double_to_string` for converting double precision floating-point numbers to formatted strings. - `PyOS_stricmp` and `PyOS_strnicmp` for case-insensitive string comparisons. These functions ensure consistent behavior and proper memory management. They wrap around standard C library functions but provide additional safety and locale-independent behavior. Task You are tasked with implementing a Python function that utilizes these functionalities to parse and format numerical values from user input, handling errors appropriately. Function Requirements 1. **Function Name**: `parse_and_format_number` 2. **Input Parameters**: - `input_string` (str): A string containing a numerical value with no leading or trailing whitespace. - `format_code` (str): A format code, one of `\'e\'`, `\'E\'`, `\'f\'`, `\'F\'`, `\'g\'`, `\'G\'`, or `\'r\'`. - `precision` (int): The precision for formatting the number. 3. **Output**: - A formatted string representation of the input number. - If the input string is not a valid representation of a number, raise a `ValueError`. - If the input number is too large to fit into a double precision floating-point number, raise an `OverflowError`. Constraints and Considerations - Use locale-independent conversion for parsing numbers (similar to `PyOS_string_to_double`). - Ensure proper memory management and handle any potential conversion errors. - Format the number using the specified format code and precision (similar to `PyOS_double_to_string`). - Handle special cases like infinity (`Py_HUGE_VAL`) and NaN gracefully. - Implement case-insensitive comparison for validation of format codes. Function Signature ```python def parse_and_format_number(input_string: str, format_code: str, precision: int) -> str: pass ``` Example ```python try: result = parse_and_format_number(\\"12345.6789\\", \'f\', 2) print(result) # Expected output: \\"12345.68\\" except ValueError as e: print(f\\"ValueError: {e}\\") except OverflowError as e: print(f\\"OverflowError: {e}\\") try: result = parse_and_format_number(\\"1e500\\", \'g\', 5) print(result) # Expected output: raises OverflowError except ValueError as e: print(f\\"ValueError: {e}\\") except OverflowError as e: print(f\\"OverflowError: {e}\\") try: result = parse_and_format_number(\\"abc123\\", \'f\', 2) print(result) # Expected output: raises ValueError except ValueError as e: print(f\\"ValueError: {e}\\") except OverflowError as e: print(f\\"OverflowError: {e}\\") ``` # Additional Notes - Remember to use appropriate error messages and handle exceptions gracefully. - This function should not use any external libraries other than standard Python libraries. Implement the `parse_and_format_number` function to demonstrate your understanding of string conversion, formatting, and error handling in Python.","solution":"def parse_and_format_number(input_string, format_code, precision): Converts input_string to a float and then formats it according to format_code and precision. Parameters: - input_string (str): A string containing a numerical value. - format_code (str): One of \'e\', \'E\', \'f\', \'F\', \'g\', \'G\', or \'r\'. - precision (int): The precision for formatting the number (used for `f`, `g`, `r` formats). Returns: - str: The formatted float as a string. Raises: - ValueError: If input_string is not a valid float. - OverflowError: If the float is too large to be represented within double precision. valid_format_codes = {\'e\', \'E\', \'f\', \'F\', \'g\', \'G\', \'r\'} if format_code.lower() not in valid_format_codes: raise ValueError(\\"Invalid format code\\") try: number = float(input_string) except ValueError: raise ValueError(f\\"Invalid numerical value: {input_string}\\") if number == float(\'inf\') or number == float(\'-inf\'): raise OverflowError(\\"Input number is too large to represent in double precision\\") format_string = f\\"{{:.{precision}{format_code}}}\\" return format_string.format(number)"},{"question":"**Question:** Implement a Python C extension module that provides the following functionalities for Python lists: 1. `create_list(n)`: Create a new list of length `n`, with all elements initialized to `None`. 2. `get_item(lst, index)`: Return the element at the specified `index` from the list `lst`. Raise an `IndexError` if the index is out of bounds. 3. `set_item(lst, index, value)`: Set the element at the specified `index` in the list `lst` to `value`. Raise an `IndexError` if the index is out of bounds. 4. `append_item(lst, value)`: Append `value` to the end of the list `lst`. 5. `sort_list(lst)`: Sort the list `lst` in place. 6. `reverse_list(lst)`: Reverse the list `lst` in place. **Function Signatures:** ```c static PyObject* create_list(PyObject *self, PyObject *args); static PyObject* get_item(PyObject *self, PyObject *args); static PyObject* set_item(PyObject *self, PyObject *args); static PyObject* append_item(PyObject *self, PyObject *args); static PyObject* sort_list(PyObject *self, PyObject *args); static PyObject* reverse_list(PyObject *self, PyObject *args); ``` **Requirements:** 1. Use the appropriate `PyList_*` functions and macros to implement these functionalities. 2. Ensure to handle errors appropriately, such as index out of bounds or memory allocation failures. 3. Provide a Python module named `list_operations` that exposes these functionalities. **Example Implementation in C**: ```c #include <Python.h> // Function to create a new list static PyObject* create_list(PyObject *self, PyObject *args) { Py_ssize_t n; if (!PyArg_ParseTuple(args, \\"n\\", &n)) { return NULL; } PyObject *list = PyList_New(n); if (!list) { return NULL; } for (Py_ssize_t i = 0; i < n; i++) { Py_INCREF(Py_None); PyList_SET_ITEM(list, i, Py_None); } return list; } // Function to get an item from the list static PyObject* get_item(PyObject *self, PyObject *args) { PyObject *list; Py_ssize_t index; if (!PyArg_ParseTuple(args, \\"On\\", &list, &index)) { return NULL; } if (!PyList_Check(list)) { PyErr_SetString(PyExc_TypeError, \\"Expected a list\\"); return NULL; } PyObject *item = PyList_GetItem(list, index); if (!item) { return NULL; } Py_INCREF(item); return item; } // Function to set an item in the list static PyObject* set_item(PyObject *self, PyObject *args) { PyObject *list, *value; Py_ssize_t index; if (!PyArg_ParseTuple(args, \\"OnO\\", &list, &index, &value)) { return NULL; } if (!PyList_Check(list)) { PyErr_SetString(PyExc_TypeError, \\"Expected a list\\"); return NULL; } if (PyList_SetItem(list, index, value) == -1) { return NULL; } Py_INCREF(Py_None); return Py_None; } // Function to append an item to the list static PyObject* append_item(PyObject *self, PyObject *args) { PyObject *list, *value; if (!PyArg_ParseTuple(args, \\"OO\\", &list, &value)) { return NULL; } if (!PyList_Check(list)) { PyErr_SetString(PyExc_TypeError, \\"Expected a list\\"); return NULL; } if (PyList_Append(list, value) == -1) { return NULL; } Py_INCREF(Py_None); return Py_None; } // Function to sort the list static PyObject* sort_list(PyObject *self, PyObject *args) { PyObject *list; if (!PyArg_ParseTuple(args, \\"O\\", &list)) { return NULL; } if (!PyList_Check(list)) { PyErr_SetString(PyExc_TypeError, \\"Expected a list\\"); return NULL; } if (PyList_Sort(list) == -1) { return NULL; } Py_INCREF(Py_None); return Py_None; } // Function to reverse the list static PyObject* reverse_list(PyObject *self, PyObject *args) { PyObject *list; if (!PyArg_ParseTuple(args, \\"O\\", &list)) { return NULL; } if (!PyList_Check(list)) { PyErr_SetString(PyExc_TypeError, \\"Expected a list\\"); return NULL; } if (PyList_Reverse(list) == -1) { return NULL; } Py_INCREF(Py_None); return Py_None; } // Module method definitions static PyMethodDef ListOpsMethods[] = { {\\"create_list\\", create_list, METH_VARARGS, \\"Create a list of length n\\"}, {\\"get_item\\", get_item, METH_VARARGS, \\"Get an item from the list\\"}, {\\"set_item\\", set_item, METH_VARARGS, \\"Set an item in the list\\"}, {\\"append_item\\", append_item, METH_VARARGS, \\"Append an item to the list\\"}, {\\"sort_list\\", sort_list, METH_VARARGS, \\"Sort the list\\"}, {\\"reverse_list\\", reverse_list, METH_VARARGS, \\"Reverse the list\\"}, {NULL, NULL, 0, NULL} }; // Module definition static struct PyModuleDef listopsmodule = { PyModuleDef_HEAD_INIT, \\"list_operations\\", \\"Module for list operations\\", -1, ListOpsMethods }; // Module initialization PyMODINIT_FUNC PyInit_list_operations(void) { return PyModule_Create(&listopsmodule); } ``` This question will test the students\' understanding of creating and manipulating Python lists using the C API provided by Python.","solution":"def create_list(n): Create a new list of length `n`, with all elements initialized to `None`. return [None] * n def get_item(lst, index): Return the element at the specified `index` from the list `lst`. Raise an `IndexError` if the index is out of bounds. if index < 0 or index >= len(lst): raise IndexError(\\"list index out of range\\") return lst[index] def set_item(lst, index, value): Set the element at the specified `index` in the list `lst` to `value`. Raise an `IndexError` if the index is out of bounds. if index < 0 or index >= len(lst): raise IndexError(\\"list index out of range\\") lst[index] = value def append_item(lst, value): Append `value` to the end of the list `lst`. lst.append(value) def sort_list(lst): Sort the list `lst` in place. lst.sort() def reverse_list(lst): Reverse the list `lst` in place. lst.reverse()"},{"question":"# Question: Optimizing and Processing Large Datasets with pandas You are given a directory containing multiple CSV files, each representing sales data for different years. Your task is to write a function that reads this sales data efficiently, optimizes the memory usage, and calculates the total sales per product across all years. Input - The directory path containing the CSV files. - Each CSV file has the following columns: - `product_id` (integer): The ID of the product. - `product_name` (string): The name of the product. - `units_sold` (integer): The number of units sold. - `sale_amount` (float): The total sales amount. Output - A pandas DataFrame with two columns: - `product_id`: The ID of the product. - `total_sales`: The total sales amount for that product across all years. Requirements 1. The function should read the CSV files efficiently, considering scenarios with large datasets. 2. Optimize memory usage by converting data types where appropriate. 3. Process the files in chunks to handle large datasets without running out of memory. 4. Return the DataFrame sorted by `total_sales` in descending order. Constraints - The CSV files can be large, and it\'s not feasible to load all data into memory at once. - The function should handle potential variations in size and number of CSV files in the directory. Function Signature ```python import pandas as pd from typing import Union def calculate_total_sales(directory_path: Union[str, pathlib.Path]) -> pd.DataFrame: pass ``` Example Assume the directory \\"sales_data/\\" contains three files (`sales_2019.csv`, `sales_2020.csv`, `sales_2021.csv`): - `sales_2019.csv` ``` product_id,product_name,units_sold,sale_amount 101,Widget A,150,750.00 102,Widget B,200,1000.00 ``` - `sales_2020.csv` ``` product_id,product_name,units_sold,sale_amount 101,Widget A,100,500.00 103,Widget C,175,875.00 ``` - `sales_2021.csv` ``` product_id,product_name,units_sold,sale_amount 101,Widget A,200,1000.00 102,Widget B,150,750.00 103,Widget C,200,1000.00 ``` Output: ``` product_id,total_sales 101,2250.00 103,1875.00 102,1750.00 ``` In this case, the function will read the CSV files in chunks, optimize the data types, and calculate the total sales amount for each product.","solution":"import pandas as pd from pathlib import Path from typing import Union def calculate_total_sales(directory_path: Union[str, Path]) -> pd.DataFrame: directory_path = Path(directory_path) all_files = list(directory_path.glob(\\"*.csv\\")) chunks = [] for file in all_files: for chunk in pd.read_csv(file, chunksize=10000, usecols=[\'product_id\', \'sale_amount\']): chunks.append(chunk) sales_data = pd.concat(chunks) total_sales = sales_data.groupby(\'product_id\', as_index=False)[\'sale_amount\'].sum() total_sales.rename(columns={\'sale_amount\': \'total_sales\'}, inplace=True) return total_sales.sort_values(by=\'total_sales\', ascending=False).reset_index(drop=True)"},{"question":"# Objective In this coding task, you will be required to utilize the `pty` module to interact with a pseudo-terminal by forking a process and executing a command, capturing its output and error streams. # Problem Statement Write a function `execute_in_pty(command: List[str]) -> Tuple[str, str]` that takes a command to be executed in a pseudo-terminal and returns the captured standard output and standard error as a tuple of strings. Function Signature ```python from typing import List, Tuple def execute_in_pty(command: List[str]) -> Tuple[str, str]: # Your code here ``` # Input - `command`: A list of strings where each string is part of a command to be executed (e.g., `[\\"ls\\", \\"-l\\"]`). # Output - A tuple of two strings: - The first string is the standard output of the command. - The second string is the standard error of the command. # Constraints - The function should handle typical shell commands. - The function should correctly capture outputs for both successful execution and error messages. # Example ```python >>> execute_in_pty([\\"echo\\", \\"Hello World\\"]) (\'Hello Worldn\', \'\') >>> execute_in_pty([\\"cat\\", \\"non_existing_file.txt\\"]) (\'\', \'cat: non_existing_file.txt: No such file or directoryn\') ``` # Notes - You may assume the provided command is always valid for a shell. - Ensure that the solution correctly differentiates between standard output and standard error. - Make use of `pty.fork()` and corresponding I/O operations with pseudo-terminals. # Detailed Requirements 1. Use `pty.fork()` to fork a process and execute the command. 2. Implement appropriate reading from the controlling terminal of the child process (pseudo-terminal). 3. Capture the standard output and standard error streams. 4. Return the captured outputs as specified. # Hint - You may find `os.read`, `os.write`, and `os.close` functions helpful for handling the file descriptors. - Be mindful of platform dependencies, this code should work on a POSIX-compliant system.","solution":"import os import pty import subprocess from typing import List, Tuple def execute_in_pty(command: List[str]) -> Tuple[str, str]: Executes a command in a pseudo-terminal and captures its output. Args: - command: a list of strings where each string is part of a command to be executed. Returns: - A tuple containing standard output and standard error as strings. master_fd, slave_fd = pty.openpty() pid = os.fork() if pid == 0: # Child os.close(master_fd) os.dup2(slave_fd, 1) # Redirect stdout to slave_fd os.dup2(slave_fd, 2) # Redirect stderr to slave_fd os.close(slave_fd) os.execvp(command[0], command) else: # Parent os.close(slave_fd) output = [] errors = [] while True: try: data = os.read(master_fd, 1024) if not data: break output.append(data.decode()) except OSError as e: break os.close(master_fd) # Separating stdout and stderr from the combined output proc = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE) stdout, stderr = proc.communicate() return stdout.decode(), stderr.decode()"},{"question":"# Custom String Formatter and Template String Processor In this coding assessment, you are required to create two parts focusing on different aspects of Python\'s `string` module: Part 1: Custom Formatter You are to create a custom formatter class that formats strings according to specified rules. Implement a class `CustomFormatter` that inherits from `string.Formatter`. This class should override at least two methods: 1. `get_field()`: Modify this method to raise an error for any field names that do not start with an alphabetic character, ensuring only valid identifiers are used. 2. `format_field()`: Modify this method to add a custom formatting where all string fields are reversed when formatted. Example: ```python formatter = CustomFormatter() formatted_string = formatter.format(\\"{name} is {age} years old.\\", name=\\"Alice\\", age=30) print(formatted_string) # Output: \\"ecilA is 03 years old.\\" ``` Expected behavior: - String fields should be reversed. - An error should be raised for field names that do not start with an alphabetic character. Part 2: Template String Processor You are to implement a class `TemplateProcessor` that utilizes `string.Template` to perform template-based string substitutions. The class should: 1. Substitute placeholders in a template string with values from a dictionary. 2. Handle optional placeholders gracefully without raising errors (using `safe_substitute`). Example: ```python template = \\"Hello {name}, your balance is {balance} dollars.\\" processor = TemplateProcessor(template) processed_string = processor.process({\\"name\\": \\"Bob\\"}) print(processed_string) # Output: \\"Hello Bob, your balance is {balance} dollars.\\" ``` Your implementation should focus on handling templates with missing or extra keys robustly. # Constraints: - Do not use external libraries other than the `string` module. - Ensure your solution is efficient and handles edge cases, such as empty input strings or dictionaries. # Performance Requirements: - The solution should handle large strings and dictionaries efficiently. - Custom formatters should adhere to the formatting rules without significant performance overhead. # Submission: Submit your solution comprising two classes, `CustomFormatter` and `TemplateProcessor`, along with test cases demonstrating the correctness and robustness of your implementations.","solution":"import string class CustomFormatter(string.Formatter): def get_field(self, field_name, args, kwargs): if not field_name[0].isalpha(): raise ValueError(\\"Field names must start with an alphabetic character.\\") return super(CustomFormatter, self).get_field(field_name, args, kwargs) def format_field(self, value, format_spec): if isinstance(value, str): value = value[::-1] # Reverse the string return super(CustomFormatter, self).format_field(value, format_spec) class TemplateProcessor: def __init__(self, template_string): self.template = string.Template(template_string) def process(self, replacements): return self.template.safe_substitute(replacements)"},{"question":"# Advanced Coding Assessment: Manipulating Arrays with Python\'s `array` Module Objective: Write a Python program that creates and manipulates arrays using the `array` module. This program should demonstrate a comprehensive understanding of array operations including initialization, modification, and conversion. Problem Statement: Create a class `ArrayManipulator` with the following methods: 1. **`__init__(self, typecode: str, initial_values: list):`** - Initializes an array with the given type code and initial values. - The `typecode` parameter is a single character representing the type of the array. - The `initial_values` parameter is a list of elements to initialize the array. 2. **`append_value(self, value):`** - Appends a new value to the end of the array. 3. **`insert_value_at(self, index: int, value):`** - Inserts a new value at the specified index in the array. 4. **`remove_value(self, value):`** - Removes the first occurrence of the specified value from the array. 5. **`reverse_array(self):`** - Reverses the order of elements in the array. 6. **`to_list(self) -> list:`** - Returns the array as a regular Python list. 7. **`to_bytes(self) -> bytes:`** - Converts the array to a bytes object. 8. **`count_value(self, value) -> int:`** - Returns the number of occurrences of the specified value in the array. Constraints: - Type code must be one of the valid type codes (\'b\', \'B\', \'u\', \'h\', \'H\', \'i\', \'I\', \'l\', \'L\', \'q\', \'Q\', \'f\', \'d\'). - Methods should raise appropriate exceptions for invalid operations (e.g., inserting an element of a different type). Example Usage: ```python # Create an ArrayManipulator object with typecode \'i\' and initial values [1, 2, 3, 4] array_manipulator = ArrayManipulator(\'i\', [1, 2, 3, 4]) # Append value 5 array_manipulator.append_value(5) # Insert value 0 at index 0 array_manipulator.insert_value_at(0, 0) # Remove the first occurrence of value 2 array_manipulator.remove_value(2) # Reverse the array array_manipulator.reverse_array() # Convert array to list array_list = array_manipulator.to_list() print(array_list) # Output: [5, 4, 3, 1, 0] # Convert array to bytes array_bytes = array_manipulator.to_bytes() print(array_bytes) # Output: b\'x05x00x00x00x04x00x00x00x03x00x00x00x01x00x00x00x00x00x00x00\' # Count occurrences of value 3 count = array_manipulator.count_value(3) print(count) # Output: 1 ``` Requirements: - Your implementation should handle errors gracefully. - Ensure efficient performance for each method.","solution":"import array class ArrayManipulator: def __init__(self, typecode: str, initial_values: list): self._array = array.array(typecode, initial_values) def append_value(self, value): self._array.append(value) def insert_value_at(self, index: int, value): self._array.insert(index, value) def remove_value(self, value): self._array.remove(value) def reverse_array(self): self._array.reverse() def to_list(self) -> list: return self._array.tolist() def to_bytes(self) -> bytes: return self._array.tobytes() def count_value(self, value) -> int: return self._array.count(value)"},{"question":"**Dynamic Module Execution using `runpy`** You are tasked with creating a utility function that can dynamically execute Python code from either a module name or a file path. The function should support both the `run_module` and `run_path` functionalities provided by the `runpy` module, and should be able to initialize specific global variables in the execution environment. # Function Signature: ```python def dynamic_code_execution(source: str, is_module: bool, init_globals: dict = None, run_name: str = None, alter_sys: bool = False) -> dict: pass ``` # Parameters: - `source` (str): If `is_module` is `True`, this represents the module name to be executed. If `is_module` is `False`, this represents the filesystem path to the script to be executed. - `is_module` (bool): A flag indicating whether the `source` is a module name (`True`) or a filesystem path (`False`). - `init_globals` (dict, optional): A dictionary of globals to pre-populate the module\'s globals dictionary before executing the code. Defaults to `None`. - `run_name` (str, optional): A name to be used for the module\'s `__name__` attribute during execution. Defaults to `None`. - `alter_sys` (bool, optional): A flag that indicates whether to update `sys.argv[0]` and `sys.modules[__name__]` during execution. Defaults to `False`. # Returns: - `dict`: The resulting module globals dictionary after executing the code. # Constraints: 1. Your implementation should use the `runpy.run_module` function when `is_module` is `True` and `runpy.run_path` when `is_module` is `False`. 2. Ensure to handle any exceptions that may occur during the execution of the module or script, and return an empty dictionary in such cases. # Example Usage: ```python # Example 1: Executing a module named \'mymodule\' result = dynamic_code_execution(\'mymodule\', True) print(result) # Example 2: Executing a script from a given path result = dynamic_code_execution(\'/path/to/myscript.py\', False) print(result) # Example 3: Executing a module with custom global variables globals_dict = {\'custom_var\': 42} result = dynamic_code_execution(\'mymodule\', True, init_globals=globals_dict, run_name=\'custom.run\') print(result) ``` Write a Python function `dynamic_code_execution` that fulfills the above requirements. Ensure your solution is robust and handles different cases appropriately.","solution":"import runpy def dynamic_code_execution(source: str, is_module: bool, init_globals: dict = None, run_name: str = None, alter_sys: bool = False) -> dict: Dynamically execute Python code from either a module name or a file path. :param source: If is_module is True, this represents the module name to be executed. If is_module is False, this represents the filesystem path to the script to be executed. :param is_module: A flag indicating whether the source is a module name (True) or a filesystem path (False). :param init_globals: A dictionary of globals to pre-populate the module\'s globals dictionary before executing the code. :param run_name: A name to be used for the module\'s `__name__` attribute during execution. :param alter_sys: A flag that indicates whether to update `sys.argv[0]` and `sys.modules[__name__]` during execution. :return: The resulting module globals dictionary after executing the code. :raises: Should handle any exceptions and return an empty dictionary. try: if is_module: return runpy.run_module(mod_name=source, init_globals=init_globals, run_name=run_name, alter_sys=alter_sys) else: return runpy.run_path(path_name=source, init_globals=init_globals, run_name=run_name) except Exception as e: print(f\\"An error occurred during execution: {e}\\") return {}"},{"question":"Managing Temporary Files and Directories Objective: Write a Python script that demonstrates the following functionalities using the `tempfile` module: 1. Create and write to a temporary file, then read its content. 2. Create a named temporary file and demonstrate that its name can be retrieved. 3. Create a spooled temporary file, write content to it, trigger it to spill over to disk, and demonstrate the content retrieval. 4. Create a temporary directory, create files inside it, and clean up the directory afterward. Requirements: 1. Use `tempfile.TemporaryFile` to create a temporary file, write some data to it, read the data, and ensure the file is properly closed and cleaned up. 2. Use `tempfile.NamedTemporaryFile` to create a named temporary file, write data, retrieve its name, and verify the file\'s existence before and after closing it. 3. Use `tempfile.SpooledTemporaryFile` to create a file, write data, trigger the spillover to the disk using `rollover()`, and read the content; ensure the file is properly closed and cleaned up. 4. Use `tempfile.TemporaryDirectory` to create a directory, create a few files inside it, and delete the directory after verifying the files\' creation. Input and Output Formats: 1. **Temporary File:** - No input required. - Output: Content written and read from the file. 2. **Named Temporary File:** - No input required. - Output: The name of the file and confirmation of its cleanup. 3. **Spooled Temporary File:** - No input required. - Output: Content written and read from the file after rollover. 4. **Temporary Directory:** - No input required. - Output: Messages confirming file creation within the directory and its cleanup. Example Output: ```plaintext Temporary File: Data written: Hello, temporary file! Data read: Hello, temporary file! Named Temporary File: Temporary file name: /.../tmpXXXX File exists before closing: True File exists after closing: False Spooled Temporary File: Data written: Test data for spooled file Data read: Test data for spooled file Temporary Directory: Created temporary directory: /.../tmpXXXX Created file: /.../tmpXXXX/file1.txt Created file: /.../tmpXXXX/file2.txt Temporary directory and files cleaned up. ``` Constraints: - All operations should handle proper cleanup ensuring no temporary files or directories persist after the script execution. - Use the provided classes and functions from the `tempfile` module as specified. - Ensure the script runs without raising any exceptions. Performance Requirements: - The script should execute efficiently without causing unnecessary delays. - Ensure that the temporary files and directories are not left behind, thus preventing resource leaks. Implement the functionalities in a single script to demonstrate the usage and management of temporary files and directories using the `tempfile` module.","solution":"import tempfile import os def tempfile_demo(): # 1. Create and write to a temporary file, then read its content print(\\"Temporary File:\\") with tempfile.TemporaryFile(mode=\'w+t\') as temp_file: temp_file.write(\\"Hello, temporary file!\\") temp_file.seek(0) data = temp_file.read() print(f\\"Data written and read: {data}\\") def named_tempfile_demo(): # 2. Create a named temporary file and demonstrate that its name can be retrieved print(\\"nNamed Temporary File:\\") with tempfile.NamedTemporaryFile(delete=False) as named_temp_file: temp_file_name = named_temp_file.name named_temp_file.write(b\\"Hello, named temporary file!\\") print(f\\"Temporary file name: {temp_file_name}\\") print(f\\"File exists before closing: {os.path.exists(temp_file_name)}\\") os.remove(temp_file_name) print(f\\"File exists after closing: {not os.path.exists(temp_file_name)}\\") def spooled_tempfile_demo(): # 3. Create a spooled temporary file, write content to it, trigger it to spill over to disk print(\\"nSpooled Temporary File:\\") with tempfile.SpooledTemporaryFile(max_size=10, mode=\'w+t\') as spooled_temp_file: spooled_temp_file.write(\\"Test data for spooled file\\") spooled_temp_file.rollover() spooled_temp_file.seek(0) data = spooled_temp_file.read() print(f\\"Data written and read after rollover: {data}\\") def tempdir_demo(): # 4. Create a temporary directory, create files inside it, and clean up the directory afterward print(\\"nTemporary Directory:\\") with tempfile.TemporaryDirectory() as temp_dir: file1_path = os.path.join(temp_dir, \'file1.txt\') file2_path = os.path.join(temp_dir, \'file2.txt\') with open(file1_path, \'w\') as file1: file1.write(\\"File 1 Content\\") with open(file2_path, \'w\') as file2: file2.write(\\"File 2 Content\\") print(f\\"Created temporary directory: {temp_dir}\\") print(f\\"Created file: {file1_path}\\") print(f\\"Created file: {file2_path}\\")"},{"question":"# Problem Description You are tasked with designing a cache system that uses weak references to avoid retaining objects that are no longer needed. This cache should store objects, but automatically remove them when they are no longer strongly referenced elsewhere in the program. Implement the `WeakCache` class with the following methods: 1. `add_to_cache(self, key: str, obj: Any, callback=None) -> None`: - Adds an object to the cache with the associated key. - `obj` can be any object. - `callback` is an optional callable that gets triggered if the object is garbage collected. 2. `get_from_cache(self, key: str) -> Any`: - Retrieves the object associated with the key from the cache if it exists and is still live. - Returns `None` if the object does not exist or has been garbage collected. 3. `clear_cache(self) -> None`: - Clears all weak references in the cache. # Input and Output `add_to_cache(self, key: str, obj: Any, callback=None) -> None` - **Input**: - `key`: A string representing the key for the cached object. - `obj`: The object to cache. - `callback` (optional): A callable to be invoked when the object is garbage collected. - **Output**: None `get_from_cache(self, key: str) -> Any` - **Input**: - `key`: A string representing the key for the cached object. - **Output**: The cached object if it exists and is still live, otherwise `None`. `clear_cache(self) -> None` - **Input**: None - **Output**: None # Constraints - Ensure that the cache does not prevent objects from being garbage collected. - The `callback` should correctly handle the event of object garbage collection. - Optimize the implementation to be efficient in terms of memory and performance. # Example ```python cache = WeakCache() # Creating an object class MyObject: pass obj = MyObject() # Adding the object to the cache cache.add_to_cache(\'obj1\', obj) # Retrieving the object from the cache retrieved_obj = cache.get_from_cache(\'obj1\') print(retrieved_obj is obj) # Output: True # Deleting the original reference del obj # Attempting to retrieve the object after deletion retrieved_obj = cache.get_from_cache(\'obj1\') print(retrieved_obj) # Output: None (Because obj has been garbage collected) # Clearing the cache cache.clear_cache() ``` # Implementation Please write the `WeakCache` class and its methods following the given requirements and example usage.","solution":"import weakref class WeakCache: def __init__(self): self._cache = {} def add_to_cache(self, key: str, obj: any, callback=None) -> None: self._cache[key] = weakref.ref(obj, callback) def get_from_cache(self, key: str) -> any: ref = self._cache.get(key) return ref() if ref is not None else None def clear_cache(self) -> None: self._cache.clear()"},{"question":"Objective: Create a function in Python that takes two tensors and returns a dictionary containing specific information about their dimensions and shapes. Requirements: - The function should be named `tensor_shape_info`. - It should take two input arguments: 1. `tensor1` (a `torch.Tensor` object) 2. `tensor2` (a `torch.Tensor` object) - It should return a dictionary with the following keys and corresponding values: 1. `\\"tensor1_shape\\"`: A `torch.Size` object representing the shape of `tensor1`. 2. `\\"tensor2_shape\\"`: A `torch.Size` object representing the shape of `tensor2`. 3. `\\"same_shape\\"`: A boolean indicating whether the two tensors have the same shape. 4. `\\"tensor1_num_dimensions\\"`: An integer representing the number of dimensions of `tensor1`. 5. `\\"tensor2_num_dimensions\\"`: An integer representing the number of dimensions of `tensor2`. 6. `\\"element_count_difference\\"`: An integer representing the absolute difference between the total number of elements in `tensor1` and `tensor2`. Constraints: - Do not use any loops or list comprehensions to compute the values. - Use the attributes and methods provided by `torch.Size` to access the necessary information. Example: ```python import torch def tensor_shape_info(tensor1, tensor2): # Your implementation here # Example usage: tensor1 = torch.ones(10, 20, 30) tensor2 = torch.ones(10, 20, 30) print(tensor_shape_info(tensor1, tensor2)) # Output: # { # \'tensor1_shape\': torch.Size([10, 20, 30]), # \'tensor2_shape\': torch.Size([10, 20, 30]), # \'same_shape\': True, # \'tensor1_num_dimensions\': 3, # \'tensor2_num_dimensions\': 3, # \'element_count_difference\': 0 # } tensor3 = torch.ones(10, 20) tensor4 = torch.ones(5, 15) print(tensor_shape_info(tensor3, tensor4)) # Output: # { # \'tensor1_shape\': torch.Size([10, 20]), # \'tensor2_shape\': torch.Size([5, 15]), # \'same_shape\': False, # \'tensor1_num_dimensions\': 2, # \'tensor2_num_dimensions\': 2, # \'element_count_difference\': 125 # } ``` Note: - Use the `size()` method of a tensor to get the `torch.Size` object. - The `element_count_difference` can be computed using the `numel()` method.","solution":"import torch def tensor_shape_info(tensor1, tensor2): Returns a dictionary containing information about the shapes and dimensions of two tensors. tensor1_shape = tensor1.size() tensor2_shape = tensor2.size() same_shape = tensor1_shape == tensor2_shape tensor1_num_dimensions = tensor1.dim() tensor2_num_dimensions = tensor2.dim() element_count_difference = abs(tensor1.numel() - tensor2.numel()) return { \\"tensor1_shape\\": tensor1_shape, \\"tensor2_shape\\": tensor2_shape, \\"same_shape\\": same_shape, \\"tensor1_num_dimensions\\": tensor1_num_dimensions, \\"tensor2_num_dimensions\\": tensor2_num_dimensions, \\"element_count_difference\\": element_count_difference }"},{"question":"# Buffer Protocol: Custom Numpy-like Array In this exercise, you will implement a custom Python class that mimics some of the behaviors of a simple Numpy-like array using the buffer protocol. This class should interact efficiently with other Python objects that use the buffer protocol and operate without unnecessary copying of memory. # Task 1. **Implement the class `CustomArray`:** - The class should store data in a contiguous block of memory. - It should support creation from a list of integers. - Implement buffer protocol methods to expose its memory directly for use with other buffer protocol consumers. - Ensure resources are managed correctly, ensuring buffers are released properly when no longer needed. 2. **Methods to Implement:** - `__init__(self, data: list)`: Initialize the array from a list of integers. - `__del__(self)`: Clean up resources properly. - `__len__(self)`: Return the number of elements in the array. - `__getitem__(self, index: int) -> int`: Return the item at the given index. - `__setitem__(self, index: int, value: int)`: Set the item at the given index. - Implement the buffer protocol interface: - `PyObject_GetBuffer(self, view, flags)`: To fill the Py_buffer structure. - `PyBuffer_Release(self, view)`: To release the buffer. # Constraints - The array will only store integers. - Implement reference counting to manage resources. - Ensure no memory access violations occur (e.g., accessing out-of-bounds memory). # Example ```python import ctypes class CustomArray: def __init__(self, data): self.length = len(data) self.itemsize = ctypes.sizeof(ctypes.c_int) self.buf = (ctypes.c_int * self.length)(*data) self.readonly = 0 def __del__(self): pass # Manage resource cleanup def __len__(self): return self.length def __getitem__(self, index): if index < 0 or index >= self.length: raise IndexError(\\"Index out of range\\") return self.buf[index] def __setitem__(self, index, value): if index < 0 or index >= self.length: raise IndexError(\\"Index out of range\\") self.buf[index] = value def PyObject_GetBuffer(self, view, flags): view.buf = ctypes.addressof(self.buf) view.obj = self view.len = self.length * self.itemsize view.itemsize = self.itemsize view.ndim = 1 view.shape = (self.length,) view.strides = (self.itemsize,) view.readonly = self.readonly view.format = \\"i\\" # integer format def PyBuffer_Release(self, view): view.obj = None ``` # Notes - Your solution should correctly initialize and manage memory for the custom array and work seamlessly with the Python buffer protocol. - Focus on efficient memory handling and avoiding unnecessary copying. - Test your implementation with various buffer consumers to ensure compatibility.","solution":"import ctypes import struct class CustomArray: def __init__(self, data): self.length = len(data) self.itemsize = ctypes.sizeof(ctypes.c_int) self.buf = (ctypes.c_int * self.length)(*data) self.readonly = 0 def __del__(self): del self.buf def __len__(self): return self.length def __getitem__(self, index): if index < 0 or index >= self.length: raise IndexError(\\"Index out of range\\") return self.buf[index] def __setitem__(self, index, value): if index < 0 or index >= self.length: raise IndexError(\\"Index out of range\\") self.buf[index] = value def PyObject_GetBuffer(self, view, flags): view.buf = ctypes.addressof(self.buf) view.obj = self view.len = self.length * self.itemsize view.itemsize = self.itemsize view.ndim = 1 view.shape = (self.length,) view.strides = (self.itemsize,) view.readonly = self.readonly view.format = \\"i\\" def PyBuffer_Release(self, view): view.obj = None"},{"question":"Background: You are tasked with creating a consolidated logging system for a Python application that should achieve the following: 1. System-specific parameters and configurations should be properly handled. 2. Enable custom data classes for structured logging. 3. Manage warnings effectively. 4. Implement context management for clean resource handling and logging. 5. Provide traceback details in case of unhandled exceptions for better debugging. Task Description: Implement a Python class `LoggingSystem` that achieves the functionality described above. Your solution should demonstrate the use of the `sys`, `dataclasses`, `warnings`, `contextlib`, and `traceback` modules in a meaningful way. Requirements: 1. **Initialization**: - The class should initialize with the ability to handle some basic system parameters using the `sys` module (e.g., Python version, platform). 2. **Data Classes for Logs**: - Implement a data class `LogEntry` to represent individual log entries. Each log entry should include at least a timestamp, log level, message, and optional exception details. 3. **Warning Management**: - Set up a custom warning filter that logs warnings instead of just printing them, using the `warnings` module. 4. **Context Management for Logs**: - Using the `contextlib` module, implement a context manager that logs the entry and exit points of a context block. 5. **Exception Handling**: - In the context manager from the previous requirement, capture any unhandled exceptions, log the full traceback using the `traceback` module, and ensure the program continues to function smoothly. Method Signatures: ```python import sys import warnings import traceback from dataclasses import dataclass, field from contextlib import contextmanager from datetime import datetime from typing import Optional @dataclass class LogEntry: timestamp: datetime log_level: str message: str exception: Optional[str] = None class LoggingSystem: def __init__(self): self.system_info = self.collect_system_info() self.logs = [] self.setup_warning_handling() def collect_system_info(self): # Collect system-specific parameters from sys module # Return them as a dictionary pass def setup_warning_handling(self): # Set up warning handling to log warnings pass def add_log_entry(self, log_level: str, message: str, exception: Optional[Exception] = None): # Create a new LogEntry, add it to the logs pass @contextmanager def log_context(self, log_message: str): # Implement a context manager for logging entry and exit and any exception raised pass def print_logs(self): # Print all collected logs in a readable format pass ``` Example Usage: ```python if __name__ == \\"__main__\\": logging_system = LoggingSystem() with logging_system.log_context(\\"Processing Data\\"): warnings.warn(\\"Sample Warning\\") try: raise ValueError(\\"This is an error\\") except ValueError as e: logging_system.add_log_entry(\\"ERROR\\", \\"Caught an error\\", e) logging_system.print_logs() ``` Expected Input/Output: - The `LoggingSystem` should capture and log: - Entry and exit of the context manager. - Warnings as log entries. - Exceptions with full tracebacks. Constraints: - Ensure that all logs are captured even if exceptions occur. - Do not use third-party logging frameworks, rely solely on the Python standard library.","solution":"import sys import warnings import traceback from dataclasses import dataclass, field from contextlib import contextmanager from datetime import datetime from typing import Optional @dataclass class LogEntry: timestamp: datetime log_level: str message: str exception: Optional[str] = None class LoggingSystem: def __init__(self): self.system_info = self.collect_system_info() self.logs = [] self.setup_warning_handling() def collect_system_info(self): return { \'python_version\': sys.version, \'platform\': sys.platform, \'executable\': sys.executable } def setup_warning_handling(self): def log_warning(message, category, filename, lineno, file=None, line=None): self.add_log_entry( log_level=\'WARNING\', message=f\'Warning: {message}, File: {filename}, Line: {lineno}\' ) warnings.showwarning = log_warning def add_log_entry(self, log_level: str, message: str, exception: Optional[Exception] = None): exception_trace = None if exception: exception_trace = \'\'.join(traceback.format_exception(type(exception), exception, exception.__traceback__)) log_entry = LogEntry(timestamp=datetime.now(), log_level=log_level, message=message, exception=exception_trace) self.logs.append(log_entry) @contextmanager def log_context(self, log_message: str): self.add_log_entry(log_level=\'INFO\', message=f\'Entering context: {log_message}\') try: yield except Exception as e: self.add_log_entry(log_level=\'ERROR\', message=\'Exception occurred\', exception=e) raise finally: self.add_log_entry(log_level=\'INFO\', message=f\'Exiting context: {log_message}\') def print_logs(self): for log in self.logs: log_output = f\'{log.timestamp} - {log.log_level} - {log.message}\' if log.exception: log_output += f\'nException Details:n{log.exception}\' print(log_output)"},{"question":"**Question Title: Advanced Data Processing with Lists and Dictionaries** **Question:** You are given a list of transactions where each transaction is represented as a dictionary with the following keys: `id`, `items`, and `amount`. The `id` is a unique identifier for the transaction, `items` is a list of items in the transaction, and `amount` is the total amount of the transaction. Here\'s an example of the transactions list: ```python transactions = [ {\'id\': 1, \'items\': [\'apple\', \'banana\', \'pear\'], \'amount\': 15.75}, {\'id\': 2, \'items\': [\'banana\', \'melon\'], \'amount\': 10.50}, {\'id\': 3, \'items\': [\'apple\', \'melon\', \'banana\'], \'amount\': 20.00}, {\'id\': 4, \'items\': [\'pear\', \'grape\', \'apple\'], \'amount\': 17.25} ] ``` Your task is to implement a function `process_transactions(transactions)` that processes this list of transactions and returns a dictionary summarizing the information. The dictionary should have the following structure: - `total_transactions`: an integer representing the total number of transactions. - `total_amount`: a float representing the total amount from all transactions. - `item_summary`: a dictionary where the keys are item names and the values are the total number of occurrences of each item across all transactions. - `expensive_transactions`: a list of transaction `id`s that have an amount greater than 15.00. **Function Signature:** ```python def process_transactions(transactions: list) -> dict: # Your code here ``` **Expected Input and Output:** Input: ```python transactions = [ {\'id\': 1, \'items\': [\'apple\', \'banana\', \'pear\'], \'amount\': 15.75}, {\'id\': 2, \'items\': [\'banana\', \'melon\'], \'amount\': 10.50}, {\'id\': 3, \'items\': [\'apple\', \'melon\', \'banana\'], \'amount\': 20.00}, {\'id\': 4, \'items\': [\'pear\', \'grape\', \'apple\'], \'amount\': 17.25} ] ``` Output: ```python { \'total_transactions\': 4, \'total_amount\': 63.50, \'item_summary\': { \'apple\': 3, \'banana\': 3, \'pear\': 2, \'melon\': 2, \'grape\': 1 }, \'expensive_transactions\': [1, 3, 4] } ``` **Constraints:** 1. Each dictionary in the transactions list will have the keys `id`, `items`, and `amount`. 2. The `id` will always be a unique positive integer. 3. The `items` list will contain only strings and will have at least one item. 4. The `amount` will be a positive float. **Performance Requirements:** - Your solution should aim for clarity and efficiency. - Use list comprehensions and appropriate list and dictionary methods where possible. **Note:** You should not use any external libraries; only use the Python standard library.","solution":"def process_transactions(transactions): summary = { \'total_transactions\': 0, \'total_amount\': 0.0, \'item_summary\': {}, \'expensive_transactions\': [] } for transaction in transactions: summary[\'total_transactions\'] += 1 summary[\'total_amount\'] += transaction[\'amount\'] if transaction[\'amount\'] > 15.00: summary[\'expensive_transactions\'].append(transaction[\'id\']) for item in transaction[\'items\']: if item in summary[\'item_summary\']: summary[\'item_summary\'][item] += 1 else: summary[\'item_summary\'][item] = 1 return summary"},{"question":"# Objective Demonstrate your understanding of the `asyncio.Future` object and related functions in Python 3.10 by creating an asynchronous job queue. # Problem Statement Implement an `AsyncJobQueue` class that manages asynchronous jobs using `asyncio.Future` objects. The `AsyncJobQueue` class should have the following capabilities: 1. **Add a Job:** Add an asynchronous job (coroutine) to the queue. 2. **Execute Jobs:** Execute all jobs in the queue asynchronously. 3. **Get Results:** Retrieve the results of all executed jobs. 4. **Handle Failures:** Capture and store exceptions for jobs that fail. # Requirements 1. **Method Definitions:** - `add_job(coro: Coroutine) -> None`: Takes a coroutine and adds it to the queue. - `execute_jobs() -> None`: Executes all added jobs asynchronously. - `get_results() -> List[Any]`: Returns a list of results for all executed jobs. If a job has failed, the result should be the exception raised by the job. 2. **Constraints:** - Do not use any blocking calls (e.g., `time.sleep()`). All operations must be asynchronous. - Ensure that each job is executed exactly once. - Maintain the order of job results as per the order they were added. 3. **Performance:** - The implementation should be able to handle at least 1000 jobs efficiently. # Example ```python import asyncio async def sample_job(identifier, duration): await asyncio.sleep(duration) if identifier % 2 == 0: return f\\"Job {identifier} completed\\" else: raise ValueError(f\\"Job {identifier} raised an exception\\") async def main(): job_queue = AsyncJobQueue() # Add a set of jobs for i in range(10): job_queue.add_job(sample_job(i, i % 3 + 1)) # Execute all jobs await job_queue.execute_jobs() # Retrieve results results = job_queue.get_results() for result in results: print(result) # Run the example asyncio.run(main()) ``` Expected Output (order of results may vary due to the asynchronous nature): ``` Job 0 completed Job 1 raised an exception Job 2 completed Job 3 raised an exception Job 4 completed ... ``` # Submission Implement the `AsyncJobQueue` class in Python, adhering to the specified method definitions, constraints, and performance requirements. Your implementation should be robust and efficiently handle the given example scenario and similar use cases.","solution":"import asyncio from typing import Coroutine, List, Any class AsyncJobQueue: def __init__(self): self.jobs = [] self.results = [] def add_job(self, coro: Coroutine) -> None: self.jobs.append(coro) async def execute_jobs(self) -> None: futures = [asyncio.ensure_future(job()) for job in self.jobs] for future in futures: try: result = await future self.results.append(result) except Exception as e: self.results.append(e) def get_results(self) -> List[Any]: return self.results"},{"question":"**Question**: You are tasked with implementing a small part of a machine learning pipeline using the Scikit-learn library. Specifically, you will use Linear Discriminant Analysis (LDA) for both classification and dimensionality reduction on a given dataset. **Tasks**: 1. **Data Loading and Preprocessing**: - Load the Iris dataset from scikit-learn\'s datasets. - Split the dataset into a training set (80%) and a test set (20%). 2. **Implement LDA for Classification**: - Use LDA to train a model on the training set. - Predict the labels on the test set and calculate the accuracy. 3. **Dimensionality Reduction with LDA**: - Using the same LDA instance, transform the original data into a 2-dimensional space. - Plot the transformed training data in a 2D scatter plot color-coded by class. 4. **Using Shrinkage Option**: - Create another LDA instance with `shrinkage=\'auto\'` and `solver=\'lsqr\'`. - Train this new LDA instance on the training set and calculate the accuracy on the test set. **Input and Output**: - There are no specific input parameters for your function, everything should be implemented within one script. - Output the test accuracies for both LDA configurations and display the scatter plot. **Constraints**: - Use the scikit-learn library for LDA implementation. - Ensure reproducibility by setting a random seed with `random_state=42` for the data splitting. **Code Template**: ```python import matplotlib.pyplot as plt from sklearn import datasets from sklearn.discriminant_analysis import LinearDiscriminantAnalysis from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score def lda_classification_and_dimensionality_reduction(): # Load the Iris dataset iris = datasets.load_iris() X, y = iris.data, iris.target # Split the dataset into training and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Initialize and train an LDA model for classification lda_clf = LinearDiscriminantAnalysis() lda_clf.fit(X_train, y_train) # Predict the test set and calculate accuracy y_pred = lda_clf.predict(X_test) accuracy_clf = accuracy_score(y_test, y_pred) print(f\\"Classification accuracy without shrinkage: {accuracy_clf}\\") # Transform the data into a 2-dimensional space using the same LDA instance X_train_lda = lda_clf.transform(X_train) # Plot the transformed training data plt.figure() for class_index in range(len(iris.target_names)): plt.scatter(X_train_lda[y_train == class_index, 0], X_train_lda[y_train == class_index, 1], label=iris.target_names[class_index]) plt.xlabel(\'LD1\') plt.ylabel(\'LD2\') plt.title(\'LDA: Iris data projected onto first 2 linear discriminants\') plt.legend() plt.show() # Initialize and train an LDA model with shrinkage lda_shrinkage = LinearDiscriminantAnalysis(shrinkage=\'auto\', solver=\'lsqr\') lda_shrinkage.fit(X_train, y_train) # Predict the test set and calculate accuracy y_pred_shrinkage = lda_shrinkage.predict(X_test) accuracy_shrinkage = accuracy_score(y_test, y_pred_shrinkage) print(f\\"Classification accuracy with shrinkage: {accuracy_shrinkage}\\") lda_classification_and_dimensionality_reduction() ``` **Performance Requirement**: - Ensure the code runs efficiently and produces the results (accuracies and plots) as expected.","solution":"import matplotlib.pyplot as plt from sklearn import datasets from sklearn.discriminant_analysis import LinearDiscriminantAnalysis from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score def lda_classification_and_dimensionality_reduction(): # Load the Iris dataset iris = datasets.load_iris() X, y = iris.data, iris.target # Split the dataset into training and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Initialize and train an LDA model for classification lda_clf = LinearDiscriminantAnalysis() lda_clf.fit(X_train, y_train) # Predict the test set and calculate accuracy y_pred = lda_clf.predict(X_test) accuracy_clf = accuracy_score(y_test, y_pred) print(f\\"Classification accuracy without shrinkage: {accuracy_clf}\\") # Transform the data into a 2-dimensional space using the same LDA instance X_train_lda = lda_clf.transform(X_train) # Plot the transformed training data plt.figure() for class_index in range(len(iris.target_names)): plt.scatter(X_train_lda[y_train == class_index, 0], X_train_lda[y_train == class_index, 1], label=iris.target_names[class_index]) plt.xlabel(\'LD1\') plt.ylabel(\'LD2\') plt.title(\'LDA: Iris data projected onto first 2 linear discriminants\') plt.legend() plt.show() # Initialize and train an LDA model with shrinkage lda_shrinkage = LinearDiscriminantAnalysis(shrinkage=\'auto\', solver=\'lsqr\') lda_shrinkage.fit(X_train, y_train) # Predict the test set and calculate accuracy y_pred_shrinkage = lda_shrinkage.predict(X_test) accuracy_shrinkage = accuracy_score(y_test, y_pred_shrinkage) print(f\\"Classification accuracy with shrinkage: {accuracy_shrinkage}\\") lda_classification_and_dimensionality_reduction()"},{"question":"**Problem Statement**: You are tasked with analyzing a dataset and visualizing it using seaborn while demonstrating your understanding of statistical estimation and error bars. The dataset provided is a sample of 200 points with two variables `X` and `Y`, where `Y` is linearly related to `X` with some added noise. # Task 1. **Setup Environment**: - Initialize your environment by importing the necessary libraries: `numpy`, `pandas`, `seaborn`, and `matplotlib.pyplot`. - Set the random seed for reproducibility. 2. **Generate Data**: - Create a dataset with 200 points, where `X` is drawn from a normal distribution and `Y` is computed as a linear function of `X` with added normal noise. - Store the data in a pandas DataFrame with columns `X` and `Y`. 3. **Visualization with Error Bars**: - Create a point plot of the data using seaborn `pointplot`, and add error bars representing the **standard deviation** (`sd`) with the default scale. - Create another point plot with error bars representing **percentile intervals** (`pi`) with a width of 50. - Create a third plot with error bars representing **confidence intervals** (`ci`) with the default bootstrap settings. - Create a regression plot (`regplot`) of `Y` versus `X` including the confidence intervals band. 4. **Custom Error Bars**: - Define a custom function to generate error bars that represent the range of data (minimum and maximum) and use this function in a point plot. 5. **Insights and Reflections**: - In the comments, provide a brief discussion (~150 words) on the relative advantages and disadvantages of using standard deviation, percentile intervals, confidence intervals, and custom error bars for data visualization. # Requirements - Use appropriate seaborn functions and parameters to achieve the described visualizations. - Ensure plots are titled correctly to reflect the type of error bar being shown. - The code should be well-commented, following best practices for clarity and maintainability. # Input - Use the following code to generate your dataset: ```python np.random.seed(42) X = np.random.normal(0, 1, 200) Y = 3 * X + np.random.normal(0, 0.5, 200) data = pd.DataFrame({\'X\': X, \'Y\': Y}) ``` # Output - A Jupyter notebook or Python script containing: - The dataset initialization. - Four plots as per the instructions. - A briefly outlined discussion on the different error bars used. **Constraints**: - Stick to the provided dataset generation mechanism. - Ensure visualizations are clear and legible with appropriate labels and legends. **Performance Requirements**: - Ensure reproducibility by setting the random seed for generation and visualize the error bars accurately. - The custom function for error bars should be efficiently implemented to handle the range calculation.","solution":"import numpy as np import pandas as pd import seaborn as sns import matplotlib.pyplot as plt # Set random seed for reproducibility np.random.seed(42) # Generate Data X = np.random.normal(0, 1, 200) Y = 3 * X + np.random.normal(0, 0.5, 200) data = pd.DataFrame({\'X\': X, \'Y\': Y}) # Visualization with standard deviation error bars plt.figure(figsize=(10, 6)) sns.pointplot(x=\\"X\\", y=\\"Y\\", data=data, errorbar=\\"sd\\") plt.title(\'Standard Deviation Error Bars\') plt.show() # Visualization with percentile intervals plt.figure(figsize=(10, 6)) sns.pointplot(x=\\"X\\", y=\\"Y\\", data=data, errorbar=(\\"pi\\", 50)) plt.title(\'Percentile Interval Error Bars (50%)\') plt.show() # Visualization with confidence intervals plt.figure(figsize=(10, 6)) sns.pointplot(x=\\"X\\", y=\\"Y\\", data=data, errorbar=\\"ci\\") plt.title(\'Confidence Interval Error Bars\') plt.show() # Regression plot plt.figure(figsize=(10, 6)) sns.regplot(x=\\"X\\", y=\\"Y\\", data=data) plt.title(\'Regression Plot with Confidence Interval\') plt.show() # Custom error bars function def range_error_bars(statistics, **kwargs): return np.array([statistics.min(), statistics.max()]) # Visualization with custom error bars (range) plt.figure(figsize=(10, 6)) sns.pointplot(x=\\"X\\", y=\\"Y\\", data=data, errorbar=range_error_bars) plt.title(\'Custom Error Bars (Range)\') plt.show() # Insights and Reflections Standard deviation error bars give a quick, commonly understood measure of variability, but they can be misleading for non-normal distributions. Percentile intervals are robust to non-normality and provide a non-parametric alternative, but interpretation might not be as straightforward, and the chosen percentile can affect evaluation. Confidence intervals give insight into the precision of the estimated mean and consider sample size, which is vital for inferential statistics but assumes normality and independence. Custom error bars offer flexibility to visualize data however desired (e.g., min-max ranges), providing more tailored insight into the data\'s variability. However, they lack standard interpretation and may mislead if the custom measure is not well-justified."},{"question":"Coding Assessment Question **Objective:** Implement a simulation of a simple window management system using the `curses.panel` module. The task will involve creating, manipulating, and displaying panels in a terminal environment. Problem Description You are required to implement a function `manage_panels(operations)` that manages panels based on a series of operations. Each operation will be in the form of a tuple, where the first element is a command string and the subsequent elements are parameters for that command. Here are the commands you need to handle: 1. `\\"create\\"`: Creates a new panel with given dimensions `(height, width, start_y, start_x)`. 2. `\\"move\\"`: Moves an existing panel to new coordinates `(panel_index, new_y, new_x)`. 3. `\\"hide\\"`: Hides the specified panel `(panel_index)`. 4. `\\"show\\"`: Shows the specified hidden panel `(panel_index)`. 5. `\\"top\\"`: Moves the specified panel to the top of the stack `(panel_index)`. 6. `\\"bottom\\"`: Moves the specified panel to the bottom of the stack `(panel_index)`. 7. `\\"update\\"`: Calls the necessary functions to update the display of panels. The function should manage the panels stack correctly and implement the above operations using the `curses.panel` module. The list of commands will ensure that invalid operations (e.g., moving non-existent panels) are not issued. **Function Signature:** ```python def manage_panels(operations): pass ``` **Input:** - `operations`: A list of tuples where the first element is the command string and the subsequent elements are parameters required for that command. **Output:** - There is no return value. The function should execute the panel operations and maintain the state as described. **Example:** ```python operations = [ (\\"create\\", 10, 20, 5, 5), # Creates a panel of height=10, width=20 at (5, 5) (\\"create\\", 15, 25, 10, 10), # Creates another panel of height=15, width=25 at (10, 10) (\\"move\\", 0, 20, 20), # Moves the first panel to (20, 20) (\\"hide\\", 1), # Hides the second panel (\\"show\\", 1), # Shows the hidden panel (\\"top\\", 0), # Moves the first panel to the top of the stack (\\"bottom\\", 1), # Moves the second panel to the bottom of the stack (\\"update\\") # Update the display stack ] manage_panels(operations) ``` **Constraints:** - You can assume there will be no more than 100 operations in the input list. - The terminal window size will be large enough to accommodate all created panels. **Notes:** - Make sure to import the `curses` and `curses.panel` modules. - You may use helper functions if necessary. - Make sure to handle terminal initialization and clean-up routines properly if running this in an environment that supports `curses`.","solution":"import curses import curses.panel def manage_panels(operations): def main(stdscr): # Initialize curses curses.curs_set(0) # Hide cursor stdscr.clear() panels = [] for operation in operations: command = operation[0] if command == \\"create\\": height, width, start_y, start_x = operation[1], operation[2], operation[3], operation[4] win = curses.newwin(height, width, start_y, start_x) pan = curses.panel.new_panel(win) panels.append(pan) elif command == \\"move\\": panel_index, new_y, new_x = operation[1], operation[2], operation[3] panels[panel_index].move(new_y, new_x) elif command == \\"hide\\": panel_index = operation[1] panels[panel_index].hide() elif command == \\"show\\": panel_index = operation[1] panels[panel_index].show() elif command == \\"top\\": panel_index = operation[1] panels[panel_index].top() elif command == \\"bottom\\": panel_index = operation[1] panels[panel_index].bottom() elif command == \\"update\\": curses.panel.update_panels() stdscr.refresh() stdscr.getch() # Wait for a key press before exiting curses.wrapper(main)"},{"question":"You are required to create a function that converts various input types to a Python integer. Your function should handle errors appropriately by checking for `PyErr_Occurred()` wherever necessary. **Function Signature:** ```python def convert_to_pylong(input_value) -> int: ``` # Input - `input_value`: This can be of one of the following types: 1. An integer (of arbitrary size in Python). 2. A string representing an integer (possible bases: binary, octal, decimal, hexadecimal). 3. A floating-point number (only the integer part should be considered). 4. A C pointer represented as an integer. # Output - Returns the Python integer (`PyLong`) equivalent of the `input_value`. - If an error occurs during conversion, return `None`. # Example ```python assert convert_to_pylong(123) == 123 assert convert_to_pylong(\\"0x7B\\") == 123 assert convert_to_pylong(123.45) == 123 assert convert_to_pylong(\\"abc\\") == None assert convert_to_pylong(2**100) == 1267650600228229401496703205376 assert convert_to_pylong(\\"256\\") == 256 assert convert_to_pylong(None) == None ``` # Implementation Requirements - Use functions provided in the documentation such as `PyLong_FromLong`, `PyLong_FromUnsignedLong`, `PyLong_FromLongLong`, `PyLong_FromUnsignedLongLong`, `PyLong_FromDouble`, `PyLong_FromString`, and others as appropriate. - Implement proper error handling using `PyErr_Occurred()` to return `None` in case of an error. # Constraints - You may assume `base` to be `0` for string inputs (auto-detect base). - Input types will not be mixed. Exactly one valid type or an invalid value will be provided. - Performance: Aim to complete each conversion within O(1) in typical scenarios; consider edge cases for large arbitrary size integers. Good luck, and make sure to handle all edge cases for robust and efficient code!","solution":"def convert_to_pylong(input_value) -> int: try: if isinstance(input_value, int): # Integer already return input_value elif isinstance(input_value, str): # String representation return int(input_value, 0) # Detect base automatically elif isinstance(input_value, float): # Floating-point return int(input_value) # Cast to int elif isinstance(input_value, (bytes, bytearray)): # Treat bytes / bytearray as integers return int.from_bytes(input_value, byteorder=\'little\') else: return None # Unsupported type except (ValueError, TypeError): return None"},{"question":"**Question: Creating a Custom Python Object with GC Support** Python allows extending its functionality by creating custom objects. In this task, you are required to create a custom object type in Python that supports property access and cyclic garbage collection. # Requirements: 1. Implement a custom object type `CustomObject` with the following: - **Properties**: `x` (integer), `y` (integer). - **Method**: `sum` that returns the sum of `x` and `y`. 2. **Memory Management**: - Ensure the object is allocated on the heap. 3. **Garbage Collection**: - Support cyclic garbage collection for the object. # Constraints: - You must only use the features and APIs described in the provided documentation. - Use proper type slots and structures to create the object type. # Example ```python obj = CustomObject(3, 4) print(obj.x) # Output: 3 print(obj.y) # Output: 4 print(obj.sum()) # Output: 7 ``` # Performance: - Ensure the object creation and method execution is efficient. - Handle memory allocation and deallocation properly to avoid leaks. # Expected Solution Structure: Ensure the solution is modular and includes: 1. Definition and allocation of the object. 2. Implementation of methods and properties. 3. Integration with Python’s cyclic garbage collector.","solution":"class CustomObject: def __init__(self, x, y): self.x = x self.y = y def sum(self): return self.x + self.y"},{"question":"Context You have been hired as a data scientist to develop a model that can predict housing prices based on various features. Your task is to apply Gaussian Process Regression using `scikit-learn`’s `GaussianProcessRegressor` class. Problem Statement You need to implement a function `predict_housing_prices` that: 1. Reads housing data from a CSV file. 2. Preprocesses the data. 3. Trains a Gaussian Process Regression (GPR) model using an appropriate kernel. 4. Predicts housing prices and returns the mean and standard deviation of the predictions. Function Signature ```python def predict_housing_prices(csv_file: str, test_data: np.ndarray) -> Tuple[np.ndarray, np.ndarray]: pass ``` Inputs - `csv_file`: A string representing the path to the CSV file. The CSV file contains the training data with features and target values. - `test_data`: A NumPy array of shape `(n_samples, n_features)` representing data on which to make predictions. Outputs - A tuple containing two NumPy arrays: - The first array contains the mean predictions of the GPR model. - The second array contains the standard deviations of the predictions. Constraints 1. Use the Radial Basis Function (RBF) kernel for the GPR model. 2. Optimize the kernel’s hyperparameters during training. 3. Handle noisy data during training. 4. Perform any necessary preprocessing to prepare the data for modeling. Performance Requirement - The function should run efficiently on datasets with up to 10,000 samples and 10 features. Example Suppose the CSV file has the following structure: | Feature1 | Feature2 | Feature3 | ... | FeatureN | Price | |----------|----------|----------|-----|----------|--------| | 0.23 | 4.1 | 2.4 | ... | 1.0 | 310000 | | 0.10 | 3.9 | 3.2 | ... | 0.8 | 305000 | | ... | ... | ... | ... | ... | ... | Let `test_data` be: ```python test_data = np.array([[0.15, 4.0, 2.8, ..., 0.9], [0.20, 3.8, 3.0, ..., 1.1]]) ``` An example call: ```python mean_preds, std_preds = predict_housing_prices(\\"housing_data.csv\\", test_data) print(mean_preds) # Expected: array with mean predictions print(std_preds) # Expected: array with standard deviations ``` Hints 1. Consider using `pandas` to read the CSV file. 2. Utilize the `GaussianProcessRegressor` from the `sklearn.gaussian_process` module. 3. Refer to `sklearn.gaussian_process.kernels` for kernel configuration. 4. To handle noisy data, explore the `alpha` parameter or include a `WhiteKernel` component in your kernel definition.","solution":"import numpy as np import pandas as pd from sklearn.gaussian_process import GaussianProcessRegressor from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C, WhiteKernel def predict_housing_prices(csv_file: str, test_data: np.ndarray): Reads housing data from a CSV file, preprocesses the data, trains a Gaussian Process Regression model using an appropriate kernel, and predicts housing prices. Returns the mean and standard deviation of the predictions. Parameters: csv_file (str): Path to the CSV file containing the training data. test_data (np.ndarray): Data for which to make predictions. Returns: Tuple[np.ndarray, np.ndarray]: Mean predictions and standard deviations. # Load the data df = pd.read_csv(csv_file) # Assuming the last column is the target variable \'Price\' X_train = df.iloc[:, :-1].values y_train = df.iloc[:, -1].values # Define kernel: RBF kernel with a constant kernel multiplied and a white noise kernel kernel = C(1.0, (1e-3, 1e3)) * RBF(length_scale=1, length_scale_bounds=(1e-2, 1e2)) + WhiteKernel(noise_level=1, noise_level_bounds=(1e-5, 1e1)) # Initialize the Gaussian Process Regressor with the kernel gpr = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=10, alpha=1e-2) # Fit the model gpr.fit(X_train, y_train) # Make predictions mean_predictions, std_predictions = gpr.predict(test_data, return_std=True) return mean_predictions, std_predictions"},{"question":"# Python Coding Assessment Question Objective: To assess your understanding of Python\'s `runpy` module and your ability to implement its functionalities in a practical scenario. Problem Statement: You are required to write a Python script that utilizes the `runpy` module to execute Python code dynamically from different sources. You need to create a function `dynamic_code_executor` which should be able to: 1. Execute a Python script located in the filesystem. 2. Execute a Python module by name. The function should return a dictionary containing the results of the executed code\'s global variables. Function Signature: ```python def dynamic_code_executor(source: str, is_module: bool = False, init_globals: dict = None) -> dict: pass ``` Parameters: - `source` (str): Either a filesystem path to the Python script or the name of the Python module. - `is_module` (bool): A flag indicating whether the `source` is a module name (`True`) or a filesystem path (`False`). Default is `False`. - `init_globals` (dict): A dictionary to pre-populate the globals dictionary of the executed code. Default is `None`. Returns: - `dict`: A dictionary of the executed code\'s global variables. Constraints: - The function should handle any exceptions raised during the execution of the code and return an empty dictionary in such cases. - Avoid using non-thread-safe features in concurrent environments within your solution. - Ensure the correctness of global variable settings, particularly the special variables, while executing the code. Example Usage: ```python # Executing a module globals_dict = dynamic_code_executor(\'some_module_name\', is_module=True) print(globals_dict) # Executing a script globals_dict = dynamic_code_executor(\'/path/to/some_script.py\') print(globals_dict) # With initial globals globals_dict = dynamic_code_executor(\'/path/to/some_script.py\', init_globals={\'x\': 42}) print(globals_dict) ``` # Notes: - You may assume that the provided paths and module names are correct and accessible in the runtime environment. - Use appropriate functions from the `runpy` module to implement the required functionalities. Good luck!","solution":"import runpy def dynamic_code_executor(source: str, is_module: bool = False, init_globals: dict = None) -> dict: Executes a Python script or module dynamically and returns its global variables. Args: source (str): File path or module name to be executed. is_module (bool): Flag indicating whether `source` is a module name (`True`) or a file path (`False`). Default is `False`. init_globals (dict): Dictionary to pre-populate the globals dictionary of the executed code. Default is `None`. Returns: dict: Dictionary of the executed code\'s global variables. try: if is_module: return runpy.run_module(source, init_globals=init_globals, run_name=\'__main__\') else: return runpy.run_path(source, init_globals=init_globals, run_name=\'__main__\') except Exception as e: # Return an empty dictionary if there is any error return {}"},{"question":"# PyTorch Named Tensors - Advanced Manipulation **Objective:** To assess the ability to work with Named Tensors in PyTorch by performing various operations including dimension alignment, broadcasting, and transformation while ensuring names are used correctly. **Question:** You are given two 4-dimensional input tensors named `input_tensor1` and `input_tensor2` with named dimensions representing different attributes in a dataset. Implement a function `process_named_tensors` that takes these tensors as input and performs the following tasks: 1. Ensure that both tensors have names: (\'N\', \'C\', \'H\', \'W\'). 2. Align `input_tensor1` to the order (\'N\', \'H\', \'W\', \'C\'). 3. Multiply these tensors elementwise, making sure to broadcast by names. 4. Flatten the result such that dimensions \'H\' and \'W\' are merged into a single \'HW\' dimension. 5. Rename the dimension \'HW\' into \'Spatial\'. 6. Return the resulting tensor. **Function Signature:** ```python import torch from typing import Tuple def process_named_tensors( input_tensor1: torch.Tensor, input_tensor2: torch.Tensor ) -> torch.Tensor: pass ``` **Constraints:** - Assume `input_tensor1` and `input_tensor2` are 4-dimensional tensors of the same shape. - The order of the dimensions may differ but ensure that the resulting multiplications make use of broadcasting by names. - Use named tensor operations as much as possible to enforce dimension checks and safety. **Example:** ```python # Given tensors tensor1 = torch.ones(2, 3, 4, 5, names=(\'N\', \'C\', \'H\', \'W\')) tensor2 = torch.ones(2, 4, 5, 3, names=(\'N\', \'H\', \'W\', \'C\')) # Expected process result_tensor = process_named_tensors(tensor1, tensor2) print(result_tensor.names) # Should print (\'N\', \'Spatial\', \'C\') print(result_tensor.shape) # Should correspond to appropriate dimensions, e.g., torch.Size([2, 20, 3]) ``` Implement this function with appropriate use of named tensor functionalities to ensure dimension names are correctly handled throughout the operations.","solution":"import torch def process_named_tensors( input_tensor1: torch.Tensor, input_tensor2: torch.Tensor ) -> torch.Tensor: # Step 1: Ensure that both tensors have names: (\'N\', \'C\', \'H\', \'W\') input_tensor1 = input_tensor1.refine_names(\'N\', \'C\', \'H\', \'W\') input_tensor2 = input_tensor2.refine_names(\'N\', \'H\', \'W\', \'C\') # Step 2: Align input_tensor1 to the order (\'N\', \'H\', \'W\', \'C\') input_tensor1 = input_tensor1.align_to(\'N\', \'H\', \'W\', \'C\') # Step 3: Multiply these tensors elementwise, making sure to broadcast by names result = input_tensor1 * input_tensor2 # Step 4: Flatten the result such that dimensions \'H\' and \'W\' are merged into a single \'HW\' dimension result = result.align_to(\'N\', \'H\', \'W\', \'C\') result = result.flatten([\'H\', \'W\'], \'HW\') # Step 5: Rename the dimension \'HW\' into \'Spatial\' result = result.rename(HW=\'Spatial\') # Step 6: Return the resulting tensor return result"},{"question":"Objective: To assess your understanding of asynchronous programming and the usage of `asyncio` queues in Python. Problem Statement: Implement a system that processes a batch of asynchronous tasks using `asyncio.Queue`. The tasks involve calculating the factorial of multiple numbers concurrently. Requirements: 1. **Function Signature**: `async def process_factorials(numbers: List[int], num_workers: int) -> List[float]` 2. **Input**: - `numbers`: A list of integers for which the factorial needs to be computed. - `num_workers`: An integer representing the number of worker coroutines that will process the queue. 3. **Output**: - A list of floating-point numbers representing the time taken by each worker to complete all their tasks. Constraints and Specifications: 1. Use `asyncio.Queue` to manage the tasks. 2. Each worker should retrieve a number from the queue, compute its factorial, and then mark the task as done. 3. Use the `time.monotonic()` function to track the time each worker takes to complete all its tasks. 4. The order of results in the output list corresponds to the workers, i.e., the 0th element is the total time taken by `worker-0`, the 1st element is for `worker-1`, and so on. 5. Handle any potential exceptions that might occur during queue operations. Additional Notes: - Factorials can grow very large, so you may consider using Python\'s built-in `math.factorial` function to compute them efficiently. - Ensure that all worker coroutines are properly closed after processing the tasks. Here is the template of the solution: ```python import asyncio import time import math from typing import List async def worker(name: str, queue: asyncio.Queue, results: List[float]): start_time = time.monotonic() while True: try: number = await queue.get() except asyncio.QueueEmpty: break # Compute factorial math.factorial(number) queue.task_done() end_time = time.monotonic() results[int(name.split(\'-\')[1])] = end_time - start_time async def process_factorials(numbers: List[int], num_workers: int) -> List[float]: queue = asyncio.Queue() results = [0] * num_workers # Place all numbers in the queue for number in numbers: await queue.put(number) # Create and start worker tasks tasks = [] for i in range(num_workers): task = asyncio.create_task(worker(f\'worker-{i}\', queue, results)) tasks.append(task) # Wait until all tasks are processed await queue.join() # Wait until all worker tasks are completed await asyncio.gather(*tasks) return results # Example usage numbers = [5, 10, 15, 20, 25, 30, 35] num_workers = 3 result = asyncio.run(process_factorials(numbers, num_workers)) print(result) ``` 1. **Implement** the `process_factorials` function to: - Create an asyncio queue. - Add the given numbers to the queue. - Spawn the required number of workers. - Ensure all tasks are processed and the worker tasks are properly closed. 2. **Implement** the worker coroutine to: - Retrieve items from the queue. - Compute the factorial of the number. - Record the time taken for processing. Provide your implementation in the template and ensure it meets the requirements and constraints specified.","solution":"import asyncio import time import math from typing import List async def worker(name: str, queue: asyncio.Queue, results: List[float]): start_time = time.monotonic() while True: try: number = await queue.get() except asyncio.QueueEmpty: break # Compute factorial math.factorial(number) queue.task_done() end_time = time.monotonic() results[int(name.split(\'-\')[1])] = end_time - start_time async def process_factorials(numbers: List[int], num_workers: int) -> List[float]: queue = asyncio.Queue() results = [0.0] * num_workers # Place all numbers in the queue for number in numbers: await queue.put(number) # Create and start worker tasks tasks = [] for i in range(num_workers): task = asyncio.create_task(worker(f\'worker-{i}\', queue, results)) tasks.append(task) # Wait until all tasks are processed await queue.join() # Wait until all worker tasks are completed await asyncio.gather(*tasks) return results # Example usage # ========== Uncomment the following lines for local execution # numbers = [5, 10, 15, 20, 25, 30, 35] # num_workers = 3 # result = asyncio.run(process_factorials(numbers, num_workers)) # print(result)"},{"question":"**Problem Statement: Exception Handling in Asynchronous Programming** You are required to implement a set of asynchronous functions that simulate various scenarios in a networking application. Your task is to handle the specific exceptions provided by the `asyncio` module. # Function Descriptions: 1. **`async def perform_task(timeout: int) -> str:`** - This function will simulate a long-running task that may run into a `TimeoutError`. - **Parameters:** - `timeout` (int): The maximum time allowed for the task. - **Output:** - Returns the string `\\"Task Completed\\"` if the task finishes within the given `timeout`. - Raises `asyncio.TimeoutError` if the task takes longer than `timeout` seconds. 2. **`async def cancel_task() -> str:`** - This function will start a task that may be cancelled before completion. - **Output:** - Returns the string `\\"Task Cancelled Gracefully\\"` if the task is cancelled. - Raises `asyncio.CancelledError` if the task is explicitly cancelled. 3. **`async def read_socket_data() -> int:`** - This function will simulate reading data from a socket, which might throw an `IncompleteReadError`. - **Output:** - Returns the total number of bytes read if the reading is successful. - Raises `asyncio.IncompleteReadError` with attributes `expected` and `partial`. 4. **`async def send_file() -> str:`** - This function simulates sending a file over a network socket, which might not support the `sendfile` operation. - **Output:** - Returns the string `\\"File Sent\\"` if the send operation is successful. - Raises `asyncio.SendfileNotAvailableError` if the socket or file type does not support `sendfile`. # Constraints: 1. You **must** handle the exceptions in your implementation. 2. You should use appropriate `await` calls and `asyncio` methods. 3. Each function should raise the respective exception if the scenario requires it. # Example Scenarios: - For `perform_task(timeout)`: ```python await perform_task(5) ``` This should perform a task and raise `asyncio.TimeoutError` if it exceeds 5 seconds. - For `cancel_task()`: ```python await cancel_task() ``` This should raise `asyncio.CancelledError` before completing the task. - For `read_socket_data()`: ```python await read_socket_data() ``` This method should read data from a socket and raise `asyncio.IncompleteReadError` if not all data is read. - For `send_file()`: ```python await send_file() ``` This should raise `asyncio.SendfileNotAvailableError` if the `sendfile` operation is not supported. Ensure to write your code such that it handles these asynchronous exceptions correctly.","solution":"import asyncio async def perform_task(timeout: int) -> str: Simulates a long-running task that may run into a TimeoutError. try: await asyncio.sleep(timeout + 1) # Simulates a task taking longer than the timeout return \\"Task Completed\\" except asyncio.TimeoutError: raise asyncio.TimeoutError(f\\"Task exceeded the timeout of {timeout} seconds\\") async def cancel_task() -> str: Starts a task that may be cancelled before completion. task = asyncio.create_task(asyncio.sleep(10)) # Simulates a task await asyncio.sleep(1) task.cancel() try: await task except asyncio.CancelledError: return \\"Task Cancelled Gracefully\\" raise asyncio.CancelledError(\\"The task was explicitly cancelled\\") async def read_socket_data() -> int: Simulates reading data from a socket, which might throw an IncompleteReadError. expected = 1024 partial = 512 raise asyncio.IncompleteReadError(partial=partial, expected=expected) async def send_file() -> str: Simulates sending a file over a network socket, which might not support the sendfile operation. raise asyncio.SendfileNotAvailableError(\\"sendfile operation is not supported\\")"},{"question":"**Objective:** Assess the student\'s understanding of PyTorch environment configuration and how it might influence basic tensor operations and model training. **Problem Statement:** You are provided with two functions from the `torch.__config__` module: `show` and `parallel_info`. Your task is to write a function that: 1. Initializes a simple neural network. 2. Prints the configuration and parallelization information using the provided functions. 3. Creates random input data and passes it through the network. 4. Returns the output from the network along with the configuration information. **Requirements:** 1. Define a simple feed-forward neural network using `torch.nn.Module`. The network should have: - An input layer with 10 units. - One hidden layer with 50 units. - An output layer with 1 unit. 2. Implement a function named `network_and_config_info` which does the following: - Prints the configuration and parallelization information using `torch.__config__.show()` and `torch.__config__.parallel_info()`. - Creates a random input tensor of shape `(5, 10)` using `torch.randn`. - Passes the input tensor through the neural network. - Returns the network\'s output tensor along with the configuration information. **Constraints:** - Use only PyTorch\'s built-in functions for tensor and neural network operations. - Ensure that the network and input tensor are created within the function. - The output should be a tuple `(output_tensor, config_info)`, where `config_info` is a string containing the configuration details printed by `torch.__config__`. **Input Format:** No input arguments. **Output Format:** A tuple containing: - `output_tensor`: A PyTorch tensor with the network\'s output. - `config_info`: A string including configuration and parallelization information produced by `torch.__config__`. **Function Signature:** ```python def network_and_config_info() -> tuple: # Your code here ``` **Example:** ```python output, config_info = network_and_config_info() print(output) print(config_info) ``` The `config_info` should match the output from `torch.__config__.show()` and `torch.__config__.parallel_info()`.","solution":"import torch import torch.nn as nn class SimpleNN(nn.Module): def __init__(self): super(SimpleNN, self).__init__() self.layer1 = nn.Linear(10, 50) self.layer2 = nn.Linear(50, 1) def forward(self, x): x = torch.relu(self.layer1(x)) x = self.layer2(x) return x def network_and_config_info(): # Initialize the neural network net = SimpleNN() # Capture the configuration information config_info = [] config_info.append(torch.__config__.show()) config_info.append(torch.__config__.parallel_info()) # Create random input tensor of shape (5, 10) input_tensor = torch.randn(5, 10) # Pass the input tensor through the network output_tensor = net(input_tensor) # Join the configuration information into a single string config_info_str = \\"n\\".join(config_info) return output_tensor, config_info_str"},{"question":"# Question: Advanced Label Transformation in Scikit-learn You are tasked with building a pre-processing module for a custom machine learning pipeline. The focus is on handling different types of labels using Scikit-learn\'s transformers. Your goal is to demonstrate proficiency in label binarization, multi-label binarization, and label encoding. Task 1: Label Binarizer Implement a function `label_binarizer_transformation(labels: List[int]) -> Tuple[np.ndarray, np.ndarray]` that takes a list of integer labels and returns a tuple containing: - An array representing the classes. - A label indicator matrix obtained after binarizing the labels. Task 2: MultiLabel Binarizer Implement a function `multilabel_binarizer_transformation(labels: List[List[int]]) -> np.ndarray` that takes a list of lists containing integer labels and returns the binary indicator matrix after applying the multi-label binarization. Task 3: Label Encoder Implement a function `label_encoder_transformation(labels: List[Union[int, str]]) -> Tuple[np.ndarray, np.ndarray]` that takes a list of labels (either integers or strings) and returns a tuple containing: - An array representing the encoded labels. - An array which indicates how the original labels can be obtained back using inverse transformation. Constraints - Use Scikit-learn\'s transformers to perform the transformations. - Assume the `scikit-learn` package and `numpy` are installed and available for import. Function Signatures ```python from typing import List, Tuple, Union import numpy as np def label_binarizer_transformation(labels: List[int]) -> Tuple[np.ndarray, np.ndarray]: pass def multilabel_binarizer_transformation(labels: List[List[int]]) -> np.ndarray: pass def label_encoder_transformation(labels: List[Union[int, str]]) -> Tuple[np.ndarray, np.ndarray]: pass ``` Example Usage ```python # Task 1: Label Binarizer Transformation labels = [1, 2, 6, 4, 2] classes, binarized = label_binarizer_transformation(labels) print(classes) # Output: array([1, 2, 4, 6]) print(binarized) # Output: # array([[1, 0, 0, 0], # [0, 0, 0, 1]]) # Task 2: MultiLabel Binarizer Transformation multilabels = [[2, 3, 4], [2], [0, 1, 3], [0, 1, 2, 3, 4], [0, 1, 2]] multilabel_binarized = multilabel_binarizer_transformation(multilabels) print(multilabel_binarized) # Output: # array([[0, 0, 1, 1, 1], # [0, 0, 1, 0, 0], # [1, 1, 0, 1, 0], # [1, 1, 1, 1, 1], # [1, 1, 1, 0, 0]]) # Task 3: Label Encoder Transformation mixed_labels = [\\"paris\\", \\"paris\\", \\"tokyo\\", \\"amsterdam\\"] encoded, inverse = label_encoder_transformation(mixed_labels) print(encoded) # Output: array([1, 1, 2, 0]) print(inverse) # Output: array([\'paris\', \'paris\', \'tokyo\', \'amsterdam\'], dtype=\'<U9\') ```","solution":"from typing import List, Tuple, Union import numpy as np from sklearn.preprocessing import LabelBinarizer, MultiLabelBinarizer, LabelEncoder def label_binarizer_transformation(labels: List[int]) -> Tuple[np.ndarray, np.ndarray]: lb = LabelBinarizer() binarized_labels = lb.fit_transform(labels) return lb.classes_, binarized_labels def multilabel_binarizer_transformation(labels: List[List[int]]) -> np.ndarray: mlb = MultiLabelBinarizer() binarized_labels = mlb.fit_transform(labels) return binarized_labels def label_encoder_transformation(labels: List[Union[int, str]]) -> Tuple[np.ndarray, np.ndarray]: le = LabelEncoder() encoded_labels = le.fit_transform(labels) inversed_labels = le.inverse_transform(encoded_labels) return encoded_labels, inversed_labels"},{"question":"Coding Assessment Question # Objective You are tasked with creating a keyword validator function for a given list of identifiers. This function should validate whether each identifier is a valid name based on the current set of Python reserved and soft keywords. # Problem Statement Implement a function `validate_identifiers(identifiers: List[str]) -> List[str]` that takes a list of identifier strings and returns a list of invalid identifiers. An identifier is considered invalid if it matches any reserved keyword or soft keyword. # Input 1. `identifiers`: A list of strings, where each string represents a potential identifier in Python. # Output - A list of strings, representing the invalid identifiers. # Constraints - The input list `identifiers` can contain between 0 and 1000 strings. - Each string in `identifiers` is non-empty and contains only alphanumeric characters and underscores. # Example ```python from keyword import iskeyword, issoftkeyword def validate_identifiers(identifiers: List[str]) -> List[str]: invalid_identifiers = [] for identifier in identifiers: if iskeyword(identifier) or issoftkeyword(identifier): invalid_identifiers.append(identifier) return invalid_identifiers # Example Usage identifiers = [\\"for\\", \\"my_var\\", \\"if\\", \\"while\\", \\"_loop\\", \\"match\\"] print(validate_identifiers(identifiers)) # Output: [\\"for\\", \\"if\\", \\"while\\", \\"match\\"] ``` # Explanation - The identifiers `[\\"for\\", \\"if\\", \\"while\\", \\"match\\"]` are invalid because they are reserved or soft keywords. # Note - Utilize the `keyword` module functions `iskeyword` and `issoftkeyword` to check each identifier.","solution":"from keyword import iskeyword, issoftkeyword def validate_identifiers(identifiers): This function takes a list of identifier strings and returns a list of invalid identifiers. An identifier is considered invalid if it is a reserved or soft keyword in Python. invalid_identifiers = [] for identifier in identifiers: if iskeyword(identifier) or issoftkeyword(identifier): invalid_identifiers.append(identifier) return invalid_identifiers"},{"question":"Coding Assessment Question **Objective:** To assess your understanding of creating and manipulating `Timedelta` objects using pandas, and performing various operations on `Timedelta` series and data frames. # Question: You are given a dataset that consists of start times and end times of various tasks performed during a week. Your task is to compute the duration of each task, perform some operations and reductions on these durations, and convert the results to different time units. 1. **Create a data frame** `tasks_df` with the following data: | Task_ID | Start_Time | End_Time | |---------|--------------------|--------------------| | 1 | 2023-10-01 08:00:00| 2023-10-01 12:00:00| | 2 | 2023-10-01 13:30:00| 2023-10-01 15:45:00| | 3 | 2023-10-02 09:00:00| 2023-10-02 18:00:00| | 4 | 2023-10-03 07:20:00| 2023-10-03 11:15:00| | 5 | 2023-10-03 12:45:00| 2023-10-03 16:00:00| 2. **Calculate the duration** of each task and add a new column `Duration` to the `tasks_df`. 3. **Filter** the data frame to include only tasks with a duration greater than 2 hours. 4. **Calculate** the total duration of all tasks remaining after the filter, convert this duration to minutes, and print the result. 5. **Find** the mean duration of the tasks in the filtered data frame and convert this mean duration to seconds. Print the result. # Constraints/Requirements: - Use pandas for all data manipulations. - Ensure all time calculations are done using pandas `Timedelta` objects or appropriate datetime operations. - The function should return the filtered data frame, total duration in minutes, and mean duration in seconds. # Expected Function Signature: ```python import pandas as pd def analyze_task_durations(): # Step 1: Create the data frame data = { \'Task_ID\': [1, 2, 3, 4, 5], \'Start_Time\': [\\"2023-10-01 08:00:00\\", \\"2023-10-01 13:30:00\\", \\"2023-10-02 09:00:00\\", \\"2023-10-03 07:20:00\\", \\"2023-10-03 12:45:00\\"], \'End_Time\': [\\"2023-10-01 12:00:00\\", \\"2023-10-01 15:45:00\\", \\"2023-10-02 18:00:00\\", \\"2023-10-03 11:15:00\\", \\"2023-10-03 16:00:00\\"] } tasks_df = pd.DataFrame(data) # Convert Start_Time and End_Time to datetime tasks_df[\'Start_Time\'] = pd.to_datetime(tasks_df[\'Start_Time\']) tasks_df[\'End_Time\'] = pd.to_datetime(tasks_df[\'End_Time\']) # Step 2: Calculate the Duration for each task tasks_df[\'Duration\'] = tasks_df[\'End_Time\'] - tasks_df[\'Start_Time\'] # Step 3: Filter tasks with duration greater than 2 hours filtered_tasks_df = tasks_df[tasks_df[\'Duration\'] > pd.Timedelta(hours=2)] # Step 4: Calculate the total duration of filtered tasks in minutes total_duration = filtered_tasks_df[\'Duration\'].sum() total_duration_minutes = total_duration / pd.Timedelta(minutes=1) # Step 5: Calculate the mean duration of filtered tasks in seconds mean_duration = filtered_tasks_df[\'Duration\'].mean() mean_duration_seconds = mean_duration / pd.Timedelta(seconds=1) return filtered_tasks_df, total_duration_minutes, mean_duration_seconds # Test the function filtered_df, total_minutes, mean_seconds = analyze_task_durations() print(filtered_df) print(\\"Total duration in minutes:\\", total_minutes) print(\\"Mean duration in seconds:\\", mean_seconds) ```","solution":"import pandas as pd def analyze_task_durations(): # Step 1: Create the data frame data = { \'Task_ID\': [1, 2, 3, 4, 5], \'Start_Time\': [\\"2023-10-01 08:00:00\\", \\"2023-10-01 13:30:00\\", \\"2023-10-02 09:00:00\\", \\"2023-10-03 07:20:00\\", \\"2023-10-03 12:45:00\\"], \'End_Time\': [\\"2023-10-01 12:00:00\\", \\"2023-10-01 15:45:00\\", \\"2023-10-02 18:00:00\\", \\"2023-10-03 11:15:00\\", \\"2023-10-03 16:00:00\\"] } tasks_df = pd.DataFrame(data) # Convert Start_Time and End_Time to datetime tasks_df[\'Start_Time\'] = pd.to_datetime(tasks_df[\'Start_Time\']) tasks_df[\'End_Time\'] = pd.to_datetime(tasks_df[\'End_Time\']) # Step 2: Calculate the Duration for each task tasks_df[\'Duration\'] = tasks_df[\'End_Time\'] - tasks_df[\'Start_Time\'] # Step 3: Filter tasks with duration greater than 2 hours filtered_tasks_df = tasks_df[tasks_df[\'Duration\'] > pd.Timedelta(hours=2)] # Step 4: Calculate the total duration of filtered tasks in minutes total_duration = filtered_tasks_df[\'Duration\'].sum() total_duration_minutes = total_duration / pd.Timedelta(minutes=1) # Step 5: Calculate the mean duration of filtered tasks in seconds mean_duration = filtered_tasks_df[\'Duration\'].mean() mean_duration_seconds = mean_duration / pd.Timedelta(seconds=1) return filtered_tasks_df, total_duration_minutes, mean_duration_seconds # Test the function filtered_df, total_minutes, mean_seconds = analyze_task_durations() print(filtered_df) print(\\"Total duration in minutes:\\", total_minutes) print(\\"Mean duration in seconds:\\", mean_seconds)"},{"question":"**Objective:** Demonstrate your understanding of pandas\' data manipulation, merging, and datetime functions by solving the following problem. # Problem Statement: You are given two datasets in the form of CSV files, `sales_data.csv` and `products_data.csv`. The files have the following structures: **sales_data.csv:** ``` Date,Product_ID,Units_Sold,Revenue 2023-01-01,101,5,100 2023-01-01,102,3,150 2023-01-02,101,2,40 2023-01-03,103,6,120 ``` **products_data.csv:** ``` Product_ID,Product_Name,Category 101,Notebook,Stationery 102,Pencil,Stationery 103,Backpack,Accessories ``` # Task: 1. **Read the CSV files into pandas DataFrames.** 2. **Convert the `Date` column in `sales_data` to datetime format.** 3. **Compute the total `Units_Sold` and `Revenue` per `Date` and `Category`.** 4. **Pivot the resulting DataFrame such that each `Date` has categories as columns and the values indicate total `Units_Sold`.** 5. **Fill any missing values with zero in the pivoted DataFrame.** 6. **Convert the final DataFrame back to the long format.** # Expected Input and Output: - **Input:** `sales_data.csv` and `products_data.csv` (as described above) - **Output:** A DataFrame in long format with columns: `Date`, `Category`, `Units_Sold` # Constraints: - Ensure that all dates are included in the final output, even if no sales were made on a particular date for a category. - Code should handle missing data by filling missing counts with zeros. # Performance Requirements: - The solution should handle datasets with up to 10,000 rows efficiently. # Example: For the given input files, the expected output would be: ``` Date Category Units_Sold 0 2023-01-01 Stationery 8 1 2023-01-01 Accessories 0 2 2023-01-02 Stationery 2 3 2023-01-02 Accessories 0 4 2023-01-03 Stationery 0 5 2023-01-03 Accessories 6 ``` # Implementation: Write a function `process_sales_data(sales_file: str, products_file: str) -> pd.DataFrame` that takes the file paths of `sales_data.csv` and `products_data.csv` as input and returns the resulting DataFrame as described. ```python import pandas as pd def process_sales_data(sales_file: str, products_file: str) -> pd.DataFrame: # Step 1: Read the CSV files into pandas DataFrames sales_df = pd.read_csv(sales_file) products_df = pd.read_csv(products_file) # Step 2: Convert the `Date` column in `sales_data` to datetime format sales_df[\'Date\'] = pd.to_datetime(sales_df[\'Date\']) # Step 3: Merge the sales and products DataFrames on `Product_ID` merged_df = pd.merge(sales_df, products_df, on=\'Product_ID\') # Step 4: Compute total `Units_Sold` and `Revenue` per `Date` and `Category` summary_df = merged_df.groupby([\'Date\', \'Category\']).agg({\'Units_Sold\': \'sum\'}).reset_index() # Step 5: Pivot the resulting DataFrame pivot_df = summary_df.pivot(index=\'Date\', columns=\'Category\', values=\'Units_Sold\').fillna(0) # Step 6: Convert the pivoted DataFrame back to long format final_df = pivot_df.reset_index().melt(id_vars=[\'Date\'], value_vars=pivot_df.columns, var_name=\'Category\', value_name=\'Units_Sold\') return final_df ``` Test your function with the provided CSV files to ensure it works correctly.","solution":"import pandas as pd def process_sales_data(sales_file: str, products_file: str) -> pd.DataFrame: # Step 1: Read the CSV files into pandas DataFrames sales_df = pd.read_csv(sales_file) products_df = pd.read_csv(products_file) # Step 2: Convert the `Date` column in `sales_data` to datetime format sales_df[\'Date\'] = pd.to_datetime(sales_df[\'Date\']) # Step 3: Merge the sales and products DataFrames on `Product_ID` merged_df = pd.merge(sales_df, products_df, on=\'Product_ID\') # Step 4: Compute total `Units_Sold` per `Date` and `Category` summary_df = merged_df.groupby([\'Date\', \'Category\']).agg({\'Units_Sold\': \'sum\'}).reset_index() # Step 5: Pivot the resulting DataFrame pivot_df = summary_df.pivot(index=\'Date\', columns=\'Category\', values=\'Units_Sold\').fillna(0) # Step 6: Convert the pivoted DataFrame back to long format final_df = pivot_df.reset_index().melt(id_vars=[\'Date\'], value_vars=pivot_df.columns, var_name=\'Category\', value_name=\'Units_Sold\') return final_df"},{"question":"# Coding Assessment: Dynamic Type Creation using C-API **Objective:** Implement a Python function that dynamically creates a new type using C-API style operations provided in Python. Your task is to ensure that this new type includes specific methods and properties as per the provided specifications. **Task:** You will write a function `create_dynamic_type(type_name, method_dict)` that dynamically creates a new type with the given `type_name` and methods defined in `method_dict`. Function Signature: ```python def create_dynamic_type(type_name: str, method_dict: dict) -> type: pass ``` Parameters: - `type_name` (str): The name of the new type to be created. - `method_dict` (dict): A dictionary where keys are method names (strings) and values are callable objects (functions) that will be associated with the new type. Returns: - `type`: The newly created type. Example: ```python def say_hello(self): return f\\"Hello from {self.__class__.__name__}\\" new_type = create_dynamic_type(\\"CustomType\\", {\\"say_hello\\": say_hello}) obj = new_type() print(obj.say_hello()) # Output: Hello from CustomType ``` Constraints: 1. The `type_name` must be a valid Python identifier. 2. Each value in the `method_dict` must be callable. 3. The newly created type must have a `__new__` and `__init__` method to handle instance creation. 4. Bonus: Add a class-level attribute `type_version` that increments every time a new type is created using this function. Implementation Details: 1. **Type Definition**: Use the concepts of `PyTypeObject` to define a new type. 2. **Memory Management**: Illustrate how you\'d use `PyType_GenericAlloc`. 3. **Slot Assignment**: Properly assign methods to the type\'s slots. 4. **Garbage Collection**: Ensure the new type handles garbage collection correctly if needed. **Hint**: You can simulate low-level type creation using `type()` in Python, but the focus should be on showing understanding of type creation and slot mechanisms. Evaluation Criteria: - Accuracy of type creation. - Correct method binding and slot assignment. - Proper handling of type properties and flags. - Code organization and readability.","solution":"def create_dynamic_type(type_name, method_dict): Dynamically creates a new type with the given type_name and methods defined in method_dict. # Validate type_name if not type_name.isidentifier(): raise ValueError(\\"The type name must be a valid Python identifier.\\") # Validate that values in method_dict are callable for method_name, method in method_dict.items(): if not callable(method): raise ValueError(f\\"The method {method_name} must be callable\\") # Definition of __new__ and __init__ methods def __new__(cls, *args, **kwargs): instance = super(cls, cls).__new__(cls) return instance def __init__(self, *args, **kwargs): super(type(self), self).__init__() # Aggregate the methods methods = {\'__new__\': __new__, \'__init__\': __init__} methods.update(method_dict) # Class-level attribute to keep the count of types created if not hasattr(create_dynamic_type, \'type_version_counter\'): create_dynamic_type.type_version_counter = 0 create_dynamic_type.type_version_counter += 1 methods[\\"type_version\\"] = create_dynamic_type.type_version_counter # Create the new type new_type = type(type_name, (object,), methods) return new_type"},{"question":"**Coding Assessment Question** # Problem Statement You are tasked with building an out-of-core classification system using scikit-learn. The system should be capable of handling large datasets that do not fit into memory by streaming data in mini-batches, extracting features using the hashing trick, and incrementally training a classification model. # Implementation Requirements 1. **Data Streaming**: Implement a function `stream_data(data, batch_size)` that simulates streaming of data in mini-batches. - **Input**: - `data`: A list of raw text documents. - `batch_size`: The size of each mini-batch. - **Output**: A generator that yields mini-batches of the data. 2. **Feature Extraction**: Implement a function `extract_features(batch)` that uses `HashingVectorizer` to transform raw text data into numerical features. - **Input**: - `batch`: A mini-batch of raw text documents. - **Output**: Transformed numerical features for the mini-batch. 3. **Incremental Learning**: Implement a function `train_incremental_model(data_stream, batch_size, classes)` that trains an `SGDClassifier` incrementally on the streamed and transformed data. - **Input**: - `data_stream`: A generator that streams batches of raw text documents. - `batch_size`: The size of each mini-batch. - `classes`: A list of all possible target classes. - **Output**: A trained `SGDClassifier` model. # Constraints - You must handle unseen terms in the text data using `HashingVectorizer`. - The mini-batch size for streaming and transformation should be adjustable. - The model should be trained incrementally to handle data that does not fit entirely into RAM. - Ensure that the model is robust and can handle the streaming process efficiently. # Example Usage ```python data = [ \\"This is the first document.\\", \\"This is the second document.\\", \\"And this is the third document.\\", \\"Is this the first document?\\", # Add more documents as needed ] batch_size = 2 classes = [0, 1] # Binary classification example # Step 1: Simulate streaming of data data_stream = stream_data(data, batch_size) # Step 2: Train an incremental model model = train_incremental_model(data_stream, batch_size, classes) # Step 3: Make predictions (optional) new_data = [\\"This is a new document.\\"] new_features = extract_features(new_data) predictions = model.predict(new_features) print(predictions) ``` # Additional Notes - Focus on the efficient handling of large datasets by ensuring minimal memory usage and incremental learning. - The classes parameter in `train_incremental_model` is essential to handle unseen target classes during training.","solution":"from sklearn.feature_extraction.text import HashingVectorizer from sklearn.linear_model import SGDClassifier def stream_data(data, batch_size): Simulates streaming of data in mini-batches. Parameters: data (list of str): List of raw text documents. batch_size (int): The size of each mini-batch. Yields: list of str: Mini-batch of raw text documents. for i in range(0, len(data), batch_size): yield data[i:i + batch_size] def extract_features(batch): Transforms raw text data into numerical features using HashingVectorizer. Parameters: batch (list of str): A mini-batch of raw text documents. Returns: scipy.sparse matrix: Transformed numerical features for the mini-batch. vectorizer = HashingVectorizer(n_features=2**20) return vectorizer.transform(batch) def train_incremental_model(data_stream, batch_size, classes): Trains an SGDClassifier incrementally on the streamed and transformed data. Parameters: data_stream (generator): Generator that streams batches of raw text documents. batch_size (int): The size of each mini-batch. classes (list): List of all possible target classes. Returns: SGDClassifier: A trained incremental SGDClassifier model. model = SGDClassifier() vectorizer = HashingVectorizer(n_features=2**20) for batch in data_stream: # Simulate target values # For demo purposes, assuming binary classes and alternating labels y = [i % len(classes) for i in range(len(batch))] # Transform the batch into numerical features X = vectorizer.transform(batch) # Incrementally fit the model model.partial_fit(X, y, classes=classes) return model"},{"question":"**Objective:** Demonstrate your understanding of creating built distributions using Distutils in Python. **Task:** You are required to create a Python script named `create_distributions.py` that automates the creation of different types of built distributions for a given Python package. The script should handle multiple formats and include proper error handling for unsupported formats. **Requirements:** 1. **Input:** - A list of format strings indicating the desired distribution formats (e.g., [\'gztar\', \'zip\', \'rpm\']). - The path to the directory containing the `setup.py` script of the Python package. 2. **Output:** - Generate the specified built distributions in the `dist/` directory relative to the package root. - Print messages indicating the success or failure of each distribution creation. 3. **Constraints:** - Ensure that the provided formats are valid based on the supported formats listed in the documentation. - Provide helpful error messages for any invalid formats. - The script should be able to handle at least the following formats: `gztar`, `bztar`, `xztar`, `ztar`, `tar`, `zip`, `rpm`, `msi`. 4. **Performance Requirements:** - The script should be efficient and minimize repeated operations, especially when generating multiple formats. 5. **Example Usage:** ```python python create_distributions.py --formats gztar zip rpm --path /path/to/package ``` **Expected Output:** ``` Successfully created gztar distribution. Successfully created zip distribution. Successfully created rpm distribution. ``` **Hints:** - Utilize the `subprocess` module to execute command-line instructions. - Reference the documented formats and commands for accurate command construction. - Consider using argparse for parsing command-line arguments within the script. **Starter Code:** ```python import subprocess import argparse import os def create_distribution(package_path, formats): os.chdir(package_path) for fmt in formats: try: if fmt not in [\\"gztar\\", \\"bztar\\", \\"xztar\\", \\"ztar\\", \\"tar\\", \\"zip\\", \\"rpm\\", \\"msi\\"]: raise ValueError(f\\"Unsupported format: {fmt}\\") subprocess.run([\\"python\\", \\"setup.py\\", \\"bdist\\", \\"--formats=\\" + fmt], check=True) print(f\\"Successfully created {fmt} distribution.\\") except subprocess.CalledProcessError: print(f\\"Failed to create {fmt} distribution.\\") except ValueError as e: print(e) if __name__ == \\"__main__\\": parser = argparse.ArgumentParser(description=\'Create built distributions for a Python package.\') parser.add_argument(\'--formats\', nargs=\'+\', required=True, help=\'List of desired formats (e.g., gztar, zip, rpm)\') parser.add_argument(\'--path\', required=True, help=\'Path to the directory containing setup.py\') args = parser.parse_args() create_distribution(args.path, args.formats) ```","solution":"import subprocess import argparse import os def create_distribution(package_path, formats): os.chdir(package_path) for fmt in formats: try: if fmt not in [\\"gztar\\", \\"bztar\\", \\"xztar\\", \\"ztar\\", \\"tar\\", \\"zip\\", \\"rpm\\", \\"msi\\"]: raise ValueError(f\\"Unsupported format: {fmt}\\") subprocess.run([\\"python\\", \\"setup.py\\", \\"bdist\\", \\"--formats=\\" + fmt], check=True) print(f\\"Successfully created {fmt} distribution.\\") except subprocess.CalledProcessError: print(f\\"Failed to create {fmt} distribution.\\") except ValueError as e: print(e) if __name__ == \\"__main__\\": parser = argparse.ArgumentParser(description=\'Create built distributions for a Python package.\') parser.add_argument(\'--formats\', nargs=\'+\', required=True, help=\'List of desired formats (e.g., gztar, zip, rpm)\') parser.add_argument(\'--path\', required=True, help=\'Path to the directory containing setup.py\') args = parser.parse_args() create_distribution(args.path, args.formats)"},{"question":"**Question: Advanced HTTP Requests with `urllib` Package** You are tasked with writing a Python function `perform_advanced_request` that utilizes the `urllib` package to fetch data from a server with specific requirements. This function should demonstrate an understanding of various concepts from the `urllib` package, including GET and POST requests, handling errors, and custom headers. # Function Signature ```python def perform_advanced_request( url: str, use_post: bool = False, params: dict = None, headers: dict = None, use_basic_auth: bool = False, username: str = None, password: str = None ) -> dict: ``` # Parameters - `url` (str): The URL to send the request to. - `use_post` (bool): Whether to use a POST request (default `False`, which means use a GET request). - `params` (dict): Optional dictionary containing data to send in the request. If `use_post` is `True`, this data should be sent as `application/x-www-form-urlencoded` in the request body. For GET requests, it should be encoded in the URL. - `headers` (dict): Optional dictionary of HTTP headers to include in the request. - `use_basic_auth` (bool): Whether to use Basic Authentication (default `False`). - `username` (str): Username for Basic Authentication (required if `use_basic_auth` is `True`). - `password` (str): Password for Basic Authentication (required if `use_basic_auth` is `True`). # Returns - (dict): A dictionary containing the status code of the response and the content (HTML) of the page. Example: `{\'status_code\': 200, \'content\': \'<html>...</html>\'}` # Constraints - You must handle HTTP and URL errors appropriately and return relevant error information. - Ensure you follow redirects as per default `urllib` behavior. - If `use_basic_auth` is `True` and `username` or `password` is `None`, raise a `ValueError`. # Example Usage ```python url = \\"http://example.com/api\\" params = { \\"name\\": \\"Jane Doe\\", \\"location\\": \\"Springfield\\", \\"language\\": \\"Python\\" } headers = { \\"User-Agent\\": \\"Mozilla/5.0 (Windows NT 6.1; Win64; x64)\\" } response = perform_advanced_request(url, use_post=True, params=params, headers=headers) print(response) ``` # Notes - Use the `urllib.request` and related submodules for making the HTTP requests. - For encoding data and handling headers, you may find `urllib.parse` useful. - To handle requests with Basic Authentication, consider using `urllib.request.HTTPBasicAuthHandler`.","solution":"import urllib.request import urllib.parse import urllib.error import base64 def perform_advanced_request( url: str, use_post: bool = False, params: dict = None, headers: dict = None, use_basic_auth: bool = False, username: str = None, password: str = None ) -> dict: Performs an advanced HTTP request using the urllib package. Args: url (str): The URL to send the request to. use_post (bool): Whether to use a POST request. params (dict): Data to send in the request. headers (dict): HTTP headers to include in the request. use_basic_auth (bool): Whether to use Basic Authentication. username (str): Username for Basic Authentication. password (str): Password for Basic Authentication. Returns: dict: A dictionary containing the status code and content of the response. if use_basic_auth and (username is None or password is None): raise ValueError(\\"Username and password must be provided for Basic Authentication.\\") if params: if use_post: data = urllib.parse.urlencode(params).encode(\'utf-8\') else: url += \'?\' + urllib.parse.urlencode(params) else: data = None req = urllib.request.Request(url, data=data) if headers: for key, value in headers.items(): req.add_header(key, value) if use_basic_auth: auth = base64.b64encode(f\\"{username}:{password}\\".encode(\'utf-8\')).decode(\'utf-8\') req.add_header(\\"Authorization\\", f\\"Basic {auth}\\") try: with urllib.request.urlopen(req) as response: content = response.read().decode(\'utf-8\') return {\'status_code\': response.getcode(), \'content\': content} except urllib.error.HTTPError as e: return {\'status_code\': e.code, \'content\': e.reason} except urllib.error.URLError as e: return {\'status_code\': None, \'content\': str(e.reason)}"},{"question":"# Advanced Python Initialization and Configuration Background Python can be initialized and configured in a highly customizable manner using structures `PyConfig` and `PyPreConfig`. These structures allow setting numerous parameters that can control Python\'s behavior on initialization. In particular, `PyConfig` provides a comprehensive configuration interface, while `PyPreConfig` allows preinitialization settings such as memory allocators and locale configurations. Task Implement a Python script that interacts with a simulated `C` API to set up a customized Python environment. The environment should: 1. Preinitialize Python with isolated settings. 2. Initialize Python with specific configurations altering some parameters. 3. Handle errors appropriately and ensure proper cleanup. You will simulate this C functionality in Python using a class-based approach, mimicking the structures and methods shown in the C examples. Specifications 1. Define classes `PyPreConfig`, `PyConfig`, and `PyStatus` with the necessary fields and methods. 2. Implement the method `initialize_isolated_python` which: - Preinitializes Python with isolated configuration. - Sets the Python UTF-8 mode. - Configures a custom memory allocator. - Sets custom `sys.path`. - Handles and prints any initialization errors using status check methods. 3. Ensure proper cleanup in case of errors. 4. Implement error-checking methods simulating `PyStatus_IsError` and `PyStatus_Exit`. ```python class PyPreConfig: def __init__(self): self.utf8_mode = 0 self.allocator = 0 def PyPreConfig_InitIsolatedConfig(self): self.isolated = True self.configure_locale = False class PyConfig: def __init__(self): self.sys_path = [] self.argv = [] self.utf8_mode = -1 self.isolated = 1 def SetSysPath(self, paths): self.sys_path = paths class PyStatus: @staticmethod def Ok(): return {\\"exitcode\\": 0, \\"err_msg\\": None, \\"func\\": None} @staticmethod def Error(msg): return {\\"exitcode\\": 1, \\"err_msg\\": msg, \\"func\\": None} @staticmethod def IsError(status): return status[\\"exitcode\\"] != 0 @staticmethod def ExitCode(status): return status[\\"exitcode\\"] def initialize_isolated_python(): preconfig = PyPreConfig() preconfig.PyPreConfig_InitIsolatedConfig() preconfig.utf8_mode = 1 # Enable UTF-8 mode preconfig.allocator = 5 # Custom allocator (example) config = PyConfig() config.isolated = 1 # Ensure isolated mode config.utf8_mode = preconfig.utf8_mode config.SetSysPath([\'/custom/path/to/modules\']) # Simulate Initialization status = simulate_initialize(config) # Check for errors and handle them if PyStatus.IsError(status): print(\\"Initialization failed with error:\\", status[\\"err_msg\\"]) cleanup() return status[\\"exitcode\\"] print(\\"Python initialized successfully with settings:\\") print(\\"Sys Path:\\", config.sys_path) return PyStatus.Ok() def simulate_initialize(config): # Simulate some error scenarios if not config.sys_path: return PyStatus.Error(\\"Sys Path is empty!\\") # Further initialization logic... return PyStatus.Ok() def cleanup(): print(\\"Cleaning up resources...\\") # Run the initialization exit_code = initialize_isolated_python() print(\\"Exit code:\\", exit_code) if exit_code != 0: exit(exit_code) ``` Constraints 1. The function must handle ioctl errors or misconfigurations gracefully. 2. Ensure proper cleanup of resources upon encountering errors. 3. Use `PyStatus` to manage and handle initialization statuses. Testing Validate your functions with various configurations: - Test with valid configurations to ensure success. - Test with empty `sys.path` to simulate and handle errors.","solution":"class PyPreConfig: def __init__(self): self.utf8_mode = 0 self.allocator = 0 self.isolated = False self.configure_locale = True def PyPreConfig_InitIsolatedConfig(self): self.isolated = True self.configure_locale = False class PyConfig: def __init__(self): self.sys_path = [] self.argv = [] self.utf8_mode = -1 self.isolated = 1 def SetSysPath(self, paths): self.sys_path = paths class PyStatus: @staticmethod def Ok(): return {\\"exitcode\\": 0, \\"err_msg\\": None, \\"func\\": None} @staticmethod def Error(msg): return {\\"exitcode\\": 1, \\"err_msg\\": msg, \\"func\\": None} @staticmethod def IsError(status): return status[\\"exitcode\\"] != 0 @staticmethod def ExitCode(status): return status[\\"exitcode\\"] def initialize_isolated_python(): preconfig = PyPreConfig() preconfig.PyPreConfig_InitIsolatedConfig() preconfig.utf8_mode = 1 # Enable UTF-8 mode preconfig.allocator = 5 # Custom allocator (example) config = PyConfig() config.isolated = 1 # Ensure isolated mode config.utf8_mode = preconfig.utf8_mode config.SetSysPath([\'/custom/path/to/modules\']) # Simulate Initialization status = simulate_initialize(config) # Check for errors and handle them if PyStatus.IsError(status): print(\\"Initialization failed with error:\\", status[\\"err_msg\\"]) cleanup() return status[\\"exitcode\\"] print(\\"Python initialized successfully with settings:\\") print(\\"Sys Path:\\", config.sys_path) return PyStatus.Ok() def simulate_initialize(config): # Simulate some error scenarios if not config.sys_path: return PyStatus.Error(\\"Sys Path is empty!\\") # Further initialization logic... return PyStatus.Ok() def cleanup(): print(\\"Cleaning up resources...\\") # Run the initialization if __name__ == \\"__main__\\": exit_code = initialize_isolated_python() print(\\"Exit code:\\", exit_code) if exit_code != 0: exit(exit_code)"},{"question":"**Question: Implement a Function to Interact with a C Library** In this coding assessment, you are required to write a Python function that interacts with a C library using the `ctypes` module. Your task is to create a function `c_library_sum` that: 1. Loads a shared library (provided specifically for this exercise). 2. Accesses a specific function from the shared library. 3. Calls this function with the correct parameters and retrieves the result. **C Library:** Assume you have a shared library `libmathops.so` (on Linux) or `mathops.dll` (on Windows). This library contains a function `int add(int a, int b)` which takes two integers and returns their sum. **Requirements:** 1. **Function Signature:** ```python def c_library_sum(a: int, b: int) -> int: ``` 2. **Loading the Library:** - On Linux, the shared library file is named `libmathops.so`. - On Windows, the shared library file is named `mathops.dll`. 3. **Accessing the `add` Function:** - The `add` function in the library must be accessed and called with two integer arguments. - Ensure to set the return type of the `add` function correctly. 4. **Calling the Function and Error Handling:** - Call the `add` function with the provided arguments `a` and `b`. - Return the result of the sum. - Handle potential errors in loading the library or accessing the function gracefully. 5. **Constraints:** - You can assume the shared library is placed in the same directory as your script. - Properly document your code and include necessary imports. **Example:** ```python result = c_library_sum(5, 10) print(result) # Output should be 15 ``` Implement the function `c_library_sum` as described.","solution":"import ctypes import os def c_library_sum(a: int, b: int) -> int: try: # Determine the correct library to load based on the operating system if os.name == \'nt\': libname = \'mathops.dll\' else: libname = \'libmathops.so\' # Load the shared library mathops = ctypes.CDLL(libname) # Access the `add` function from the library add_function = mathops.add add_function.argtypes = [ctypes.c_int, ctypes.c_int] # Define argument types add_function.restype = ctypes.c_int # Define return type # Call the `add` function with arguments a and b result = add_function(a, b) return result except Exception as e: raise RuntimeError(f\\"Failed to interact with the C library: {e}\\") # Example usage # Assume that `libmathops.so` or `mathops.dll` is correctly placed in the same directory # result = c_library_sum(5, 10) # print(result) # Expected output: 15"},{"question":"**Title: Calendar Event Planner** **Objective:** Create a function to help plan recurring events in a calendar year based on given criteria. **Description:** You are required to write a function `plan_events(year, month, day_of_week, frequency, num_events)` that generates a list of dates for recurring events within a specified year: - `year` (int): The year for which the events should be planned. - `month` (int): The starting month (1 to 12). - `day_of_week` (int): The day of the week for the event, where 0 is Monday and 6 is Sunday. - `frequency` (int): The number of weeks between events. - `num_events` (int): The total number of events to plan. **Assumptions:** 1. Events should start from the specified month and should continue to occur on the specified day of the week. 2. If there are not enough occurrences within the year, the function should return as many as possible. **Constraints:** - The function should handle leap and non-leap years correctly. - The function should use the `calendar` module for calculations. - You are not allowed to use any additional modules. **Output:** - The function should return a list of `datetime.date` objects representing the event dates. **Example:** ```python import datetime def plan_events(year, month, day_of_week, frequency, num_events): # Implement the function leveraging the \'calendar\' module pass # Example usage events = plan_events(2023, 1, 2, 4, 10) for event in events: print(event) ``` **Expected Output:** For the example usage, the function may output a list of dates starting from the first Tuesday in January 2023, repeating every 4 weeks, for up to 10 occurrences. (Note: Actual output would depend on the implementation details.) **Note:** Ensure your implementation is efficient and considers edge cases like the end of the year and months with fewer days.","solution":"import calendar from datetime import date, timedelta def plan_events(year, month, day_of_week, frequency, num_events): Generates a list of dates for recurring events within a specified year. :param year: The year for which the events should be planned. :param month: The starting month (1 to 12). :param day_of_week: The day of the week for the event, where 0 is Monday and 6 is Sunday. :param frequency: The number of weeks between events. :param num_events: The total number of events to plan. :return: A list of datetime.date objects representing the event dates. events = [] current_month = month # Get the first day of the specified month and year first_day = date(year, current_month, 1) # Find the first occurrence of the specified day of the week in the month while first_day.weekday() != day_of_week: first_day += timedelta(days=1) # Add the first event events.append(first_day) # Calculate the following events for _ in range(1, num_events): next_event = events[-1] + timedelta(weeks=frequency) if next_event.year != year: break # Events should be within the specified year events.append(next_event) return events"},{"question":"Objective Implement a function that collects detailed information about the current execution state of the Python interpreter, including local, global, and built-in variables from the current frame and its outer frames. This will test your understanding of Python\'s introspection capabilities and the Python C API functions. Question Implement a Python function `collect_execution_state()` that returns a dictionary with the following structure: - `\'current_frame\'`: A dictionary with keys: - `\'locals\'`: Local variables in the current frame. - `\'globals\'`: Global variables in the current frame. - `\'builtins\'`: Built-in functions available in the current frame. - `\'line_number\'`: Line number currently executed in this frame. - `\'function_name\'`: Name of the currently executing function. - `\'function_description\'`: Description of the currently executing function. - `\'outer_frames\'`: A list of dictionaries, each representing an outer frame with the same structure as `\'current_frame\'`. Input The function does not take any input arguments. Output The function returns a dictionary as specified above. Constraints 1. You should assume no specific code execution context. Your function must handle the scenario where there is no outer frame. 2. Use the `sys` module for access to the current frame. Example Here\'s an example of what the dictionary might look like when the function is called: ```python { \'current_frame\': { \'locals\': {\'a\': 10, \'b\': 20}, \'globals\': {\'__name__\': \'__main__\', ...}, \'builtins\': {\'len\': <built-in function len>, ...}, \'line_number\': 5, \'function_name\': \'example_function\', \'function_description\': \'()\' }, \'outer_frames\': [ { \'locals\': {\'x\': 5}, \'globals\': {\'__name__\': \'__main__\', ...}, \'builtins\': {\'len\': <built-in function len>, ...}, \'line_number\': 12, \'function_name\': \'<module>\', \'function_description\': \' object\' } ] } ``` There are no specific performance constraints for this function. Focus on correctly retrieving and structuring the information. **Note**: You must use the functions described in the **Reflection** section of the provided documentation to implement `collect_execution_state()`.","solution":"import sys import inspect def collect_execution_state(): Collects detailed information about the current execution state of the Python interpreter including local, global, and built-in variables from the current frame and its outer frames. frames_info = [] current_frame = sys._getframe() # Get the current frame while current_frame is not None: frame_info = { \'locals\': current_frame.f_locals, \'globals\': current_frame.f_globals, \'builtins\': current_frame.f_builtins, \'line_number\': current_frame.f_lineno, \'function_name\': current_frame.f_code.co_name, \'function_description\': inspect.getdoc(current_frame.f_code) } frames_info.append(frame_info) current_frame = current_frame.f_back # Move to the outer frame result = { \'current_frame\': frames_info[0], # The most current frame \'outer_frames\': frames_info[1:] # All the outer frames } return result"},{"question":"# Asynchronous Web Scraping with concurrent.futures You are required to implement a function that scrapes the HTML content from a list of URLs concurrently using the `concurrent.futures` module. The function should use the `ThreadPoolExecutor` for this purpose. Function Signature ```python def concurrent_scrape(urls: List[str], max_workers: int = 5) -> Dict[str, str]: ``` # Input - `urls`: A list of URLs to scrape. Each URL is a string. - `max_workers`: An optional integer specifying the maximum number of worker threads to use (default is 5). # Output - A dictionary mapping each URL to the scraped HTML content as a string. # Constraints - You can assume that URLs provided are valid and reachable. - The entire operation should timeout if any single request takes more than 30 seconds. - In case of error fetching any URL, the dictionary should contain the URL with the string \\"Error\\" as its content. # Example Usage ```python urls = [ \'https://www.example.com/\', \'https://www.python.org/\', \'https://www.nonexistentwebsite.org/\' ] results = concurrent_scrape(urls, max_workers=3) for url, content in results.items(): print(f\\"URL: {url}\\") print(f\\"Content: {content[:100]}...\\") # Print first 100 characters of the content ``` # Example Output ``` URL: https://www.example.com/ Content: <!doctype html>... URL: https://www.python.org/ Content: <!doctype html>... URL: https://www.nonexistentwebsite.org/ Content: Error ``` # Guidelines 1. Use the `concurrent.futures.ThreadPoolExecutor` to manage parallel threads for fetching URLs concurrently. 2. Implement proper exception handling to ensure that any request failure is reported as an \\"Error\\" in the output dictionary. 3. Handle futures and results using the `as_completed` method to gather results as they are completed. # Additional Notes - Ensure that threads are cleaned up promptly using the `with` statement for the ThreadPoolExecutor. - You may use the `urllib.request` module to fetch the HTML content from the given URLs. Good luck!","solution":"import concurrent.futures import urllib.request from typing import List, Dict def fetch_url(url: str) -> str: try: with urllib.request.urlopen(url, timeout=30) as response: return response.read().decode(\'utf-8\') except Exception as e: return \\"Error\\" def concurrent_scrape(urls: List[str], max_workers: int = 5) -> Dict[str, str]: result = {} with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor: future_to_url = {executor.submit(fetch_url, url): url for url in urls} for future in concurrent.futures.as_completed(future_to_url): url = future_to_url[future] try: result[url] = future.result() except Exception as e: result[url] = \\"Error\\" return result"},{"question":"Objective: Implement and demonstrate your understanding of the `webbrowser` module by writing a Python script that mimics a simplified web browser controller. Question: You are required to implement a Python class `SimpleWebBrowser` that utilizes the `webbrowser` module. This class should incorporate the following functionalities: 1. Open a given URL in the default web browser. 2. Open a given URL in a new window or tab based on input parameters. 3. Register a new browser type and use it to open URLs. Your class should adhere to the following specifications: # Class `SimpleWebBrowser`: 1. **Method: `__init__(self)`** - This method initializes the class and sets up any necessary attributes. 2. **Method: `open_url(self, url)`** - Input: A string `url` representing the URL to be opened. - Function: Opens the URL using the default web browser. - Output: Returns `True` if successful, otherwise raises `webbrowser.Error`. 3. **Method: `open_in_new_window(self, url)`** - Input: A string `url` representing the URL to be opened. - Function: Opens the URL in a new window. - Output: Returns `True` if successful, otherwise raises `webbrowser.Error`. 4. **Method: `open_in_new_tab(self, url)`** - Input: A string `url` representing the URL to be opened. - Function: Opens the URL in a new tab. - Output: Returns `True` if successful, otherwise raises `webbrowser.Error`. 5. **Method: `register_and_use_browser(self, browser_name, command, url)`** - Input: - `browser_name`: A string representing the name of the new browser type. - `command`: A string representing the command to initialize the new browser type. - `url`: A string representing the URL to be opened. - Function: Registers the new browser type with the given command and opens the URL using this new browser type. - Output: Returns `True` if successful, otherwise raises `webbrowser.Error`. # Constraints: 1. Assume the URLs are always valid and provided in the correct format. 2. Handle `webbrowser.Error` exceptions gracefully and print an appropriate error message. Example Usage: ```python # Instantiate the SimpleWebBrowser class browser = SimpleWebBrowser() # Open a URL using the default web browser browser.open_url(\'https://www.example.com\') # Open a URL in a new window browser.open_in_new_window(\'https://www.example.com\') # Open a URL in a new tab browser.open_in_new_tab(\'https://www.example.com\') # Register and use a new browser type browser.register_and_use_browser(\'example_browser\', \'firefox %s\', \'https://www.example.com\') ``` Notes: - Make sure to test your implementation across multiple functions to ensure robustness. - Ensure that all registered browsers are being called correctly by verifying the results. Good luck!","solution":"import webbrowser class SimpleWebBrowser: def __init__(self): pass def open_url(self, url): try: webbrowser.open(url) return True except webbrowser.Error as e: print(f\\"Failed to open URL {url} in default browser: {e}\\") raise def open_in_new_window(self, url): try: webbrowser.open_new(url) return True except webbrowser.Error as e: print(f\\"Failed to open URL {url} in a new window: {e}\\") raise def open_in_new_tab(self, url): try: webbrowser.open_new_tab(url) return True except webbrowser.Error as e: print(f\\"Failed to open URL {url} in a new tab: {e}\\") raise def register_and_use_browser(self, browser_name, command, url): try: webbrowser.register(browser_name, None, webbrowser.BackgroundBrowser(command)) webbrowser.get(browser_name).open(url) return True except webbrowser.Error as e: print(f\\"Failed to register or use browser {browser_name} with command \'{command}\': {e}\\") raise"},{"question":"# Advanced PyTorch Coding Assessment Objective: Demonstrate your understanding of PyTorch\'s Export IR by writing code to create, manipulate, and analyze an ExportedProgram. Problem Statement: You are given a simple PyTorch model and some example input data. Your task is to export its computational graph to an Export IR graph, perform some specific manipulation on it, and then output details about the modified graph. Requirements: 1. **Create an Exported Program**: - Define a simple PyTorch model (e.g., an `nn.Module` that performs basic arithmetic operations). - Use the `torch.export.export` function to export the model with given example inputs. 2. **Graph Manipulation**: - Traverse the graph of the exported model and locate all nodes performing addition operations. - Replace these addition operations with multiplication operations (i.e., `torch.ops.aten.add.Tensor` to `torch.ops.aten.mul.Tensor`). 3. **Output Graph Details**: - Print the modified ExportedProgram\'s graph to verify the changes. - Provide details about each node, including its metadata (like stack trace and source function), if available. Expected Input and Output Formats: - **Input**: No input parameters are required as all data is internally specified. - **Output**: Print the modified Export IR graph and node metadata descriptions. Constraints: - Use only the allowed operations and follow the specifications provided in the documentation above. - Ensure all nodes are correctly traversed and manipulated as per the task requirement. Performance Requirements: - Efficiently traverse and manipulate the graph without redundantly processing nodes. Example Solution Outline: ```python import torch from torch import nn from torch.export import export # Define a simple module with addition operation class SimpleAddModule(nn.Module): def forward(self, x, y): return x + y # Example inputs example_args = (torch.randn(1), torch.randn(1)) # Export the model to Export IR exported_model = export(SimpleAddModule(), example_args) # Function to traverse and manipulate the graph def modify_exported_graph(exported_model): graph = exported_model.graph for node in graph.nodes: # Locate addition operation nodes and replace with multiplication if node.op == \'call_function\' and node.target == torch.ops.aten.add.Tensor: node.target = torch.ops.aten.mul.Tensor return graph # Modify the graph as required modified_graph = modify_exported_graph(exported_model) # Print modified graph and node metadata print(modified_graph) for node in modified_graph.nodes: print(f\\"Node Name: {node.name}, Op Type: {node.op}, Target: {node.target}\\") if \'stack_trace\' in node.meta: print(f\\"Stack Trace: {node.meta[\'stack_trace\']}\\") if \'source_fn_stack\' in node.meta: print(f\\"Source Function Stack: {node.meta[\'source_fn_stack\']}\\") # Execute the function to display the output ``` Note: Ensure your solution correctly follows the provided guidelines and handles the traversal and manipulation of the Export IR graph as specified.","solution":"import torch from torch import nn from torch.export import export # Define a simple module with addition operation class SimpleAddModule(nn.Module): def forward(self, x, y): return x + y # Example inputs example_args = (torch.randn(1), torch.randn(1)) # Export the model to Export IR exported_model = export(SimpleAddModule(), example_args) # Function to traverse and manipulate the graph def modify_exported_graph(exported_model): graph = exported_model.graph for node in graph.nodes: # Locate addition operation nodes and replace with multiplication if node.op == \'call_function\' and node.target == torch.ops.aten.add.Tensor: node.target = torch.ops.aten.mul.Tensor return graph # Modify the graph as required modified_graph = modify_exported_graph(exported_model) # Print modified graph and node metadata print(modified_graph) for node in modified_graph.nodes: print(f\\"Node Name: {node.name}, Op Type: {node.op}, Target: {node.target}\\") if \'stack_trace\' in node.meta: print(f\\"Stack Trace: {node.meta[\'stack_trace\']}\\") if \'source_fn_stack\' in node.meta: print(f\\"Source Function Stack: {node.meta[\'source_fn_stack\']}\\") # Execute the function to display the output if __name__ == \\"__main__\\": print(modified_graph) for node in modified_graph.nodes: print(f\\"Node Name: {node.name}, Op Type: {node.op}, Target: {node.target}\\") if \'stack_trace\' in node.meta: print(f\\"Stack Trace: {node.meta[\'stack_trace\']}\\") if \'source_fn_stack\' in node.meta: print(f\\"Source Function Stack: {node.meta[\'source_fn_stack\']}\\")"},{"question":"**Question:** # Asynchronous Task Management with asyncio You are required to implement a Python function using the asyncio library to manage multiple asynchronous tasks. Your function should demonstrate the use of asyncio\'s high-level APIs to run coroutines concurrently, handle network IO, and distribute tasks via queues. Function Signature: ```python import asyncio async def process_tasks(task_list, max_concurrency): Process multiple asynchronous tasks with a specified maximum concurrency level. Parameters: - task_list (List[coroutine]): A list of coroutine objects representing the tasks to be processed. - max_concurrency (int): The maximum number of tasks to run concurrently. Returns: - results (List[Any]): A list of results from the completed tasks, in the same order as the input tasks. ``` Description: 1. **Input**: - `task_list` is a list of coroutine objects that need to be processed. - `max_concurrency` is an integer specifying the maximum number of tasks to run concurrently. 2. **Output**: - The function should return a list of results from the completed tasks, maintaining the order of the input tasks. 3. **Constraints**: - You must use `asyncio.Semaphore` to limit the concurrency level of tasks. 4. **Additional Requirements**: - Use the `asyncio.gather()` function to run multiple coroutines concurrently. - Handle any exceptions that occur during the execution of tasks and ensure all tasks complete even if some fail. Example: ```python import asyncio async def fetch_data(identifier): await asyncio.sleep(2) return f\\"Data from {identifier}\\" async def main(): tasks = [fetch_data(i) for i in range(10)] results = await process_tasks(tasks, max_concurrency=3) print(results) asyncio.run(main()) ``` In this example, the `fetch_data` coroutine simulates a task that fetches data identified by an integer. The `main()` function creates a list of such tasks and processes them using `process_tasks` with a maximum concurrency level of 3. The results should be printed in the order of task submission `[Data from 0, Data from 1, ..., Data from 9]`. Constraints: - You should not use any third-party libraries; only standard Python libraries are allowed. - Your solution must handle exceptions gracefully and ensure the completion of all tasks. - Ensure that the order of task submission is preserved in the results list.","solution":"import asyncio async def process_tasks(task_list, max_concurrency): Process multiple asynchronous tasks with a specified maximum concurrency level. Parameters: - task_list (List[coroutine]): A list of coroutine objects representing the tasks to be processed. - max_concurrency (int): The maximum number of tasks to run concurrently. Returns: - results (List[Any]): A list of results from the completed tasks, in the same order as the input tasks. semaphore = asyncio.Semaphore(max_concurrency) async def sem_task(task): async with semaphore: try: return await task except Exception as e: return e tasks = [sem_task(task) for task in task_list] results = await asyncio.gather(*tasks) return results"},{"question":"**Data Processing and Compression Task** You are tasked with creating a Python script that reads large chunks of textual data, incrementally compresses them using the bzip2 compression algorithm, writes the compressed data to a file, and then performs the reverse operation to ensure data integrity. Specifically, your task is to implement the following two functions: # Function 1: `compress_large_file(input_filepath: str, output_filepath: str, chunk_size: int = 1024) -> None` This function should read data from `input_filepath` in chunks of the specified `chunk_size`, incrementally compress it using an instance of `bz2.BZ2Compressor`, and write the compressed data to `output_filepath`. **Parameters:** - `input_filepath`: Path to the input file containing the textual data to be compressed. - `output_filepath`: Path to the output file where the compressed data will be stored. - `chunk_size`: Size of the chunks in which data should be read from the input file. # Function 2: `decompress_to_validate(compressed_filepath: str, decompressed_filepath: str, chunk_size: int = 1024) -> bool` This function should read the compressed data from `compressed_filepath` in chunks, incrementally decompress it using an instance of `bz2.BZ2Decompressor`, and write the decompressed data to `decompressed_filepath`. After decompression, the function should compare the original uncompressed data with the decompressed data to ensure data integrity and return `True` if they match or `False` otherwise. **Parameters:** - `compressed_filepath`: Path to the file containing the compressed data. - `decompressed_filepath`: Path to the file where the decompressed data will be written. - `chunk_size`: Size of the chunks in which data should be read from the compressed file. **Returns:** - A boolean value indicating whether the decompressed data matches the original data. # Example Assuming you have a large text file named `large_data.txt` with some textual content. 1. Compress the file: ```python compress_large_file(\\"large_data.txt\\", \\"large_data.bz2\\", chunk_size=2048) ``` 2. Decompress and validate the integrity: ```python is_valid = decompress_to_validate(\\"large_data.bz2\\", \\"decompressed_large_data.txt\\", chunk_size=2048) print(is_valid) # Should print True if decompression was correct ``` # Constraints - Your solution should handle large files efficiently, ensuring that peak memory usage is minimized by using incremental read/write operations. - Ensure the integrity checks in `decompress_to_validate` are robust and account for end-of-stream markers and possible multi-stream compressed data. # Additional Information - Use the `with` statement where appropriate to ensure files are properly closed. - Handle any potential exceptions and edge cases gracefully. ```python import bz2 def compress_large_file(input_filepath: str, output_filepath: str, chunk_size: int = 1024) -> None: # Your implementation here pass def decompress_to_validate(compressed_filepath: str, decompressed_filepath: str, chunk_size: int = 1024) -> bool: # Your implementation here pass ``` Make sure to test your implementation with various chunks of data and different file sizes to ensure its correctness and efficiency.","solution":"import bz2 def compress_large_file(input_filepath: str, output_filepath: str, chunk_size: int = 1024) -> None: Compresses a large file using bzip2 algorithm in chunks. :param input_filepath: Path to the input file to compress. :param output_filepath: Path to the output file to store compressed data. :param chunk_size: Size of the chunks for reading the input file. compressor = bz2.BZ2Compressor() with open(input_filepath, \'rb\') as input_file, open(output_filepath, \'wb\') as output_file: while True: chunk = input_file.read(chunk_size) if not chunk: break compressed_data = compressor.compress(chunk) output_file.write(compressed_data) output_file.write(compressor.flush()) def decompress_to_validate(compressed_filepath: str, decompressed_filepath: str, chunk_size: int = 1024) -> bool: Decompresses the bzip2 compressed file and validates integrity by comparing with original data. :param compressed_filepath: Path to the compressed file. :param decompressed_filepath: Path to store the decompressed data. :param chunk_size: Size of the chunks for reading the compressed file. :return: True if decompressed data matches original data, False otherwise. decompressor = bz2.BZ2Decompressor() with open(compressed_filepath, \'rb\') as compressed_file, open(decompressed_filepath, \'wb\') as decompressed_file: while True: chunk = compressed_file.read(chunk_size) if not chunk: break decompressed_data = decompressor.decompress(chunk) decompressed_file.write(decompressed_data) with open(decompressed_filepath, \'rb\') as decompressed_file, open(decompressed_filepath, \'rb\') as original_file: while True: original_chunk = original_file.read(chunk_size) decompressed_chunk = decompressed_file.read(chunk_size) if original_chunk != decompressed_chunk: return False if not original_chunk and not decompressed_chunk: break return True"},{"question":"# **Coding Assessment Question** **Objective**: To assess your understanding of the `xml.etree.ElementTree` module, you are required to write a Python function that takes an XML string representing a list of books, modifies the ratings based on given criteria, and then returns the modified XML as a string. **Problem Statement**: You are given an XML string that represents a list of books. Each book element has the following structure: ```xml <books> <book title=\\"Book Title 1\\"> <author>Author Name 1</author> <year>Year of Publication 1</year> <rating>Rating Value 1</rating> </book> <book title=\\"Book Title 2\\"> <author>Author Name 2</author> <year>Year of Publication 2</year> <rating>Rating Value 2</rating> </book> <!-- More book elements --> </books> ``` You need to create a function `modify_book_ratings(xml_string, year, increment)` that modifies the rating of each book published after the given year by adding the specified increment to the existing rating. **Function Signature**: ```python def modify_book_ratings(xml_string: str, year: int, increment: float) -> str: pass ``` **Input**: - `xml_string` (str): An XML string representing the list of books. - `year` (int): The year threshold. Only books published after this year should have their rating modified. - `increment` (float): The increment value to be added to each applicable book\'s rating. **Output**: - Returns the modified XML string. **Constraints**: - The XML string is well-formed. - Ratings should not exceed 5.0 after increment. - No year filter is applied to books published exactly in the given year. **Example**: Given the following input: ```python xml_string = <books> <book title=\\"Effective Python\\"> <author>Brett Slatkin</author> <year>2015</year> <rating>4.3</rating> </book> <book title=\\"Python Crash Course\\"> <author>Eric Matthes</author> <year>2016</year> <rating>4.7</rating> </book> <book title=\\"Learning Python\\"> <author>Mark Lutz</author> <year>2013</year> <rating>4.0</rating> </book> </books> year = 2014 increment = 0.5 ``` The expected output would be: ```xml <books> <book title=\\"Effective Python\\"> <author>Brett Slatkin</author> <year>2015</year> <rating>4.8</rating> </book> <book title=\\"Python Crash Course\\"> <author>Eric Matthes</author> <year>2016</year> <rating>5.0</rating> </book> <book title=\\"Learning Python\\"> <author>Mark Lutz</author> <year>2013</year> <rating>4.0</rating> </book> </books> ``` **Instructions**: 1. Parse the input XML string. 2. Iterate through each book element and check the year of publication. 3. If the year is greater than the given year, increment the rating appropriately but ensure it does not exceed 5.0. 4. Convert the modified XML tree back to a string. 5. Return the modified XML string. Use the `xml.etree.ElementTree` module to solve this problem.","solution":"import xml.etree.ElementTree as ET def modify_book_ratings(xml_string: str, year: int, increment: float) -> str: tree = ET.ElementTree(ET.fromstring(xml_string)) root = tree.getroot() for book in root.findall(\'book\'): pub_year = int(book.find(\'year\').text) if pub_year > year: rating = float(book.find(\'rating\').text) new_rating = min(5.0, rating + increment) book.find(\'rating\').text = str(new_rating) return ET.tostring(root, encoding=\'unicode\')"},{"question":"**Objective**: Demonstrate your understanding of Python\'s importing capabilities, specifically using the `importlib` module and customizing import processes. Problem Statement You are tasked with creating a custom module importer which can load modules from a given directory dynamically. You should implement a class `CustomImporter` that provides functionality to: 1. **List all modules**: List all modules available in the given directory. 2. **Import a specific module**: Import a module by name from the given directory. 3. **Reload a module**: Reload an already imported module to reflect any changes made to the file. Your task is to implement the following functionalities in the `CustomImporter` class: - `__init__(self, module_dir: str)`: Initialize the importer with the directory containing modules. - `list_modules(self) -> list[str]`: List all module names available in the directory. - `import_module(self, module_name: str) -> Optional[ModuleType]`: Import the module by name. If the module is already imported, reload it. - `reload_module(self, module_name: str) -> Optional[ModuleType]`: Reload the specified module. Assumptions: - The directory provided contains valid Python files (`*.py`). - Valid module names are the names of Python files without the \'.py\' extension. - Use the `importlib` module to perform importing and reloading. # Constraints - You may not use the built-in `__import__` function. - Only use functionalities provided by the `importlib` module. # Examples Given the directory structure: ``` /path/to/modules/ ├── mod1.py ├── mod2.py └── helper.py ``` ```python import os from importlib import util, import_module, reload from types import ModuleType from typing import Optional class CustomImporter: def __init__(self, module_dir: str): self.module_dir = module_dir self.imported_modules = {} def list_modules(self) -> list[str]: # Implementation here def import_module(self, module_name: str) -> Optional[ModuleType]: # Implementation here def reload_module(self, module_name: str) -> Optional[ModuleType]: # Implementation here ``` **Usage Example** ```python import tempfile import os # Setup - Let\'s create a temporary directory with some modules with tempfile.TemporaryDirectory() as temp_dir: with open(os.path.join(temp_dir, \'mod1.py\'), \'w\') as f: f.write(\\"def hello():n return \'Hello from mod1!\'\\") with open(os.path.join(temp_dir, \'mod2.py\'), \'w\') as f: f.write(\\"def greet():n return \'Greetings from mod2!\'\\") # Initializing CustomImporter importer = CustomImporter(temp_dir) # Listing modules print(importer.list_modules()) # Output: [\'mod1\', \'mod2\'] # Importing module \'mod1\' mod1 = importer.import_module(\'mod1\') print(mod1.hello()) # Output: \'Hello from mod1!\' # Importing module \'mod2\' mod2 = importer.import_module(\'mod2\') print(mod2.greet()) # Output: \'Greetings from mod2!\' # Modify mod1.py with open(os.path.join(temp_dir, \'mod1.py\'), \'w\') as f: f.write(\\"def hello():n return \'Hello from modified mod1!\'\\") # Reloading module \'mod1\' mod1 = importer.reload_module(\'mod1\') print(mod1.hello()) # Output: \'Hello from modified mod1!\' ``` **Note**: Ensure you handle cases where a module doesn\'t exist and return `None` in such scenarios.","solution":"import os import importlib.util import importlib import sys from types import ModuleType from typing import Optional, List class CustomImporter: def __init__(self, module_dir: str): self.module_dir = module_dir self.imported_modules = {} if self.module_dir not in sys.path: sys.path.append(self.module_dir) def list_modules(self) -> List[str]: files = os.listdir(self.module_dir) modules = [os.path.splitext(f)[0] for f in files if f.endswith(\\".py\\")] return modules def import_module(self, module_name: str) -> Optional[ModuleType]: try: if module_name in self.imported_modules: return self.reload_module(module_name) else: module = importlib.import_module(module_name) self.imported_modules[module_name] = module return module except ModuleNotFoundError: return None def reload_module(self, module_name: str) -> Optional[ModuleType]: if module_name in self.imported_modules: module = importlib.reload(self.imported_modules[module_name]) return module else: return self.import_module(module_name)"},{"question":"# URL Status Checker As a developer, you are often required to fetch resources from different URLs and process them. Your task is to write a function that takes a list of URLs and checks their availability. Specifically, you are required to implement a function that uses the `urllib.request` module to perform these checks. **Function Signature:** ```python def check_urls_status(urls: List[str], timeout: int = 10) -> Dict[str, int]: ``` **Input:** - `urls`: A list of strings, where each string is a URL. - `timeout`: An optional integer specifying the timeout in seconds for each URL request. Default is 10 seconds. **Output:** - Returns a dictionary where: - The keys are the URLs. - The values are the HTTP status codes returned from each URL request. **Constraints:** - You must use `urllib.request.urlopen` to perform the URL requests. - If a URL raises any exception (e.g., `URLError`), catch the exception and set the status code to `-1` for that URL. **Example:** ```python urls = [ \\"http://www.google.com\\", \\"http://www.python.org\\", \\"http://www.nonexistenturl.org\\" ] result = check_urls_status(urls) # Output might be: # { # \\"http://www.google.com\\": 200, # \\"http://www.python.org\\": 200, # \\"http://www.nonexistenturl.org\\": -1 # } ``` # Requirements: 1. Implement proper error handling to catch and handle exceptions. 2. Use the `with` statement to ensure proper resource management. 3. Ensure your solution is efficient and adheres to the provided constraints. **Notes:** - You can assume that the list of URLs will not contain more than 100 URLs. - Make sure to include necessary imports in your solution. Provide the complete implementation of the function.","solution":"from typing import List, Dict import urllib.request import urllib.error def check_urls_status(urls: List[str], timeout: int = 10) -> Dict[str, int]: Checks the availability of a list of URLs and returns their HTTP status codes. :param urls: List of URLs to check. :param timeout: Timeout for each URL request in seconds. :return: Dictionary with URLs as keys and their HTTP status codes as values. status_codes = {} for url in urls: try: with urllib.request.urlopen(url, timeout=timeout) as response: status_codes[url] = response.getcode() except urllib.error.URLError: status_codes[url] = -1 return status_codes"},{"question":"<|Analysis Begin|> The document provides comprehensive information about the `base64` module in Python, which includes functionalities for encoding and decoding data using Base16, Base32, Base64, Base85, and Ascii85 standards. The document outlines the modern and legacy interfaces provided by the module, detailing various functions for encoding and decoding binary data to/from printable ASCII characters. Key functions and their details are: - `b64encode`, `b64decode`: For standard Base64 encoding and decoding with optional support for alternate character sets. - `standard_b64encode`, `standard_b64decode`: For standard Base64 encoding and decoding without any alternative alphabets. - `urlsafe_b64encode`, `urlsafe_b64decode`: For Base64 encoding and decoding that is safe to use in URLs and file systems. - `b32encode`, `b32decode`, `b32hexencode`, `b32hexdecode`: For Base32 encoding and decoding, including an extended hex alphabet variation. - `b16encode`, `b16decode`: For Base16 encoding and decoding. - `a85encode`, `a85decode`: For Ascii85 encoding and decoding with additional options for foldspaces, Adobe format, and character ignoring. - `b85encode`, `b85decode`: For base85 encoding and decoding as used in binary diffs. The module\'s functions accept `bytes-like objects` or ASCII strings and return encoded or decoded byte sequences. Various options are available to handle different alphabets, padding, line wrapping, and input sanitation. Performance considerations mainly revolve around correctly handling input data size and ensuring proper padding for valid encodings. Security considerations are also important as indicated by references to RFCs and recommendations. <|Analysis End|> <|Question Begin|> **Coding Assessment Question:** # **Base64 Encoding and Decoding Utility** You are required to implement a utility that provides a command-line interface for encoding and decoding data using Base64 with support for URL-safe variations. Your utility should be able to: 1. **Encode** a given input string to Base64. 2. **Decode** a given Base64 encoded string back to its original form. 3. **Support URL-safe Base64 encoding and decoding**. # **Function Specifications** - **Input:** - A string indicating the operation (`\'encode\'` or `\'decode\'`). - A string specifying the input to be encoded or decoded. - An optional boolean flag indicating whether the operation should use URL-safe Base64 (`default=False`). - **Output:** - The encoded or decoded output string. # **Function Signature** ```python def base64_utility(operation: str, data: str, url_safe: bool = False) -> str: pass ``` # **Constraints** 1. `operation` must be `\'encode\'` or `\'decode\'`. 2. `data` is a non-empty string. 3. If `operation` is `\'decode\'`, `data` must be a valid Base64 encoded string. # **Examples** ```python # Standard Base64 encoding encoded = base64_utility(\'encode\', \'Hello World\') print(encoded) # Should return: \'SGVsbG8gV29ybGQ=\' # Standard Base64 decoding decoded = base64_utility(\'decode\', \'SGVsbG8gV29ybGQ=\') print(decoded) # Should return: \'Hello World\' # URL-safe Base64 encoding encoded_url_safe = base64_utility(\'encode\', \'Hello World\', url_safe=True) print(encoded_url_safe) # Should return a URL-safe Base64 encoded string # URL-safe Base64 decoding decoded_url_safe = base64_utility(\'decode\', encoded_url_safe, url_safe=True) print(decoded_url_safe) # Should return: \'Hello World\' ``` # **Detailed Requirements:** 1. Use the `base64` module functions (`b64encode`, `b64decode`, `urlsafe_b64encode`, `urlsafe_b64decode`) to handle the encoding and decoding operations. 2. Properly handle exceptions for invalid operations and inputs, raising `ValueError` with appropriate messages when necessary. 3. Ensure the implementation is efficient and follows Python best practices. # **Notes:** - Remember to handle the conversion between string and bytes as necessary (`str.encode()` and `bytes.decode()`). - Ensure the output for encoding functions is string-based, not bytes. Implement the **`base64_utility`** function as specified.","solution":"import base64 def base64_utility(operation: str, data: str, url_safe: bool = False) -> str: Encodes or decodes data using Base64 with optional URL-safe variations. Parameters: operation (str): The operation to perform, either \'encode\' or \'decode\'. data (str): The input data to be encoded or decoded. url_safe (bool): Flag to indicate URL-safe Base64 encoding/decoding. Defaults to False. Returns: str: The encoded or decoded output string. Raises: ValueError: If operation is not \'encode\' or \'decode\', or if any other error occurs. if operation not in [\'encode\', \'decode\']: raise ValueError(\\"Operation must be \'encode\' or \'decode\'.\\") try: if operation == \'encode\': encoded_bytes = (base64.urlsafe_b64encode if url_safe else base64.b64encode)(data.encode(\'utf-8\')) return encoded_bytes.decode(\'utf-8\') if operation == \'decode\': decoded_bytes = (base64.urlsafe_b64decode if url_safe else base64.b64decode)(data) return decoded_bytes.decode(\'utf-8\') except Exception as e: raise ValueError(\\"An error occurred during the base64 operation: \\" + str(e))"},{"question":"Utilizing `doctest` in Python Objective To assess your understanding and application of `doctest` in Python, you are required to create a Python module that: 1. Implements certain functions with appropriate doctests in their docstrings. 2. Runs and verifies these doctests using the `doctest` module. Requirements 1. **Function Implementations**: Write a Python module `math_operations.py` containing the following functions: - **`add(x, y)`**: Returns the sum of `x` and `y`. - **`subtract(x, y)`**: Returns the result of `x` minus `y`. - **`multiply(x, y)`**: Returns the product of `x` and `y`. - **`divide(x, y)`**: Returns the result of `x` divided by `y`. Ensure that division by zero raises an appropriate exception. 2. **Docstrings with Doctests**: Each function should include a docstring with examples that use the `doctest` format to illustrate how the function should be used and what it returns. 3. **Running Doctests**: Ensure that these doctests can be automatically run and verified by including the necessary code to call `doctest.testmod()`. Constraints - Function arguments and return values will always be of type `int` or `float`. Example To assist you, here is an example structure: ```python This module contains simple arithmetic functions with embedded doctests. def add(x, y): Returns the sum of x and y. >>> add(2, 3) 5 >>> add(-1, 1) 0 >>> add(0, 0) 0 return x + y def subtract(x, y): Returns the result of x minus y. >>> subtract(5, 3) 2 >>> subtract(3, 5) -2 >>> subtract(0, 0) 0 return x - y def multiply(x, y): Returns the product of x and y. >>> multiply(2, 3) 6 >>> multiply(-1, 3) -3 >>> multiply(0, 5) 0 return x * y def divide(x, y): Returns the result of x divided by y. >>> divide(6, 3) 2.0 >>> divide(7, 2) 3.5 >>> divide(1, 0) Traceback (most recent call last): ... ZeroDivisionError: division by zero if y == 0: raise ZeroDivisionError(\\"division by zero\\") return x / y if __name__ == \\"__main__\\": import doctest doctest.testmod() ``` Submission Submit your `math_operations.py` file containing the four functions with their respective docstrings and doctests, as well as the code to run the doctests when the file is executed as a script.","solution":"This module contains simple arithmetic functions with embedded doctests. def add(x, y): Returns the sum of x and y. >>> add(2, 3) 5 >>> add(-1, 1) 0 >>> add(0, 0) 0 return x + y def subtract(x, y): Returns the result of x minus y. >>> subtract(5, 3) 2 >>> subtract(3, 5) -2 >>> subtract(0, 0) 0 return x - y def multiply(x, y): Returns the product of x and y. >>> multiply(2, 3) 6 >>> multiply(-1, 3) -3 >>> multiply(0, 5) 0 return x * y def divide(x, y): Returns the result of x divided by y. >>> divide(6, 3) 2.0 >>> divide(7, 2) 3.5 >>> divide(1, 0) Traceback (most recent call last): ... ZeroDivisionError: division by zero if y == 0: raise ZeroDivisionError(\\"division by zero\\") return x / y if __name__ == \\"__main__\\": import doctest doctest.testmod()"},{"question":"You\'re tasked with implementing a set of functions that mimic some key behaviors of the `datetime` module in Python, utilizing the provided datetime macros. These functions should allow you to create, validate, and manipulate datetime objects effectively. Required Functions 1. **create_date(year: int, month: int, day: int) -> datetime.date** - Creates and returns a `datetime.date` object with the specified year, month, and day. - Constraints: - `year` must be a positive integer. - `month` must be between 1 and 12. - `day` must be valid for the given month. 2. **validate_date(date_obj: datetime.date) -> bool** - Validates whether the provided `date_obj` is an instance of `datetime.date` or its subtype. - Returns `True` if valid, otherwise `False`. 3. **create_datetime(year: int, month: int, day: int, hour: int, minute: int, second: int, microsecond: int) -> datetime.datetime** - Creates and returns a `datetime.datetime` object with the specified year, month, day, hour, minute, second, and microsecond. - Constraints: - All input values are integers within valid ranges. - `microsecond` must be between 0 and 999999. 4. **get_date_info(date_obj: datetime.date) -> Tuple[int, int, int]** - Extracts and returns the year, month, and day from the given `datetime.date` object. - Returns a tuple: `(year, month, day)`. 5. **get_datetime_info(datetime_obj: datetime.datetime) -> Tuple[int, int, int, int, int, int]** - Extracts and returns the year, month, day, hour, minute, and second from the given `datetime.datetime` object. - Returns a tuple: `(year, month, day, hour, minute, second)`. Example Usage: ```python # Create date date_obj = create_date(2023, 10, 5) print(date_obj) # Output: 2023-10-05 # Validate date is_valid_date = validate_date(date_obj) print(is_valid_date) # Output: True # Create datetime datetime_obj = create_datetime(2023, 10, 5, 14, 30, 45, 123456) print(datetime_obj) # Output: 2023-10-05 14:30:45.123456 # Get date info year, month, day = get_date_info(date_obj) print(year, month, day) # Output: 2023, 10, 5 # Get datetime info year, month, day, hour, minute, second = get_datetime_info(datetime_obj) print(year, month, day, hour, minute, second) # Output: 2023, 10, 5, 14, 30, 45 ``` Your task is to implement these functions in Python, ensuring they adhere to the constraints and expected behaviors outlined above. Constraints - You may assume that the input values for dates and times are within reasonable ranges when provided to the functions. - The extraction functions should not raise exceptions when provided with valid date and datetime objects.","solution":"from datetime import date, datetime def create_date(year: int, month: int, day: int) -> date: Creates and returns a datetime.date object with the specified year, month, and day. return date(year, month, day) def validate_date(date_obj: date) -> bool: Validates whether the provided date_obj is an instance of datetime.date or its subtype. return isinstance(date_obj, date) def create_datetime(year: int, month: int, day: int, hour: int, minute: int, second: int, microsecond: int) -> datetime: Creates and returns a datetime.datetime object with the specified year, month, day, hour, minute, second, and microsecond. return datetime(year, month, day, hour, minute, second, microsecond) def get_date_info(date_obj: date) -> tuple: Extracts and returns the year, month, and day from the given datetime.date object. return (date_obj.year, date_obj.month, date_obj.day) def get_datetime_info(datetime_obj: datetime) -> tuple: Extracts and returns the year, month, day, hour, minute, and second from the given datetime.datetime object. return (datetime_obj.year, datetime_obj.month, datetime_obj.day, datetime_obj.hour, datetime_obj.minute, datetime_obj.second)"},{"question":"# Advanced Coding Assessment: Module Importation and Management using Python C API Question: You have been tasked with writing a Python C extension that manages the importation and manipulation of Python modules. Utilizing the Python C API functions provided in the documentation, implement a C extension module with the following functionalities implemented as Python functions: 1. **import_module**: - **Input**: A string representing the module name. - **Output**: The imported module object or `None` if the import fails. - **Functionality**: Use `PyImport_ImportModule` or related functions to import the specified module. 2. **reload_module**: - **Input**: A module object. - **Output**: The reloaded module object or `None` if the reload fails. - **Functionality**: Use `PyImport_ReloadModule` to reload the specified module. 3. **exec_code_module**: - **Input**: A string representing the module name and a string of Python code. - **Output**: The newly created module object after executing the provided code or `None` if an error occurs. - **Functionality**: Compile the provided code using the Python C API and execute it within the context of a new module using `PyImport_ExecCodeModule`. 4. **get_module_dict**: - **Output**: The dictionary of loaded modules (similar to `sys.modules`). - **Functionality**: Use `PyImport_GetModuleDict` to retrieve the dictionary of currently loaded modules. Constraints: - Your implementation must handle errors gracefully by setting Python exceptions using the appropriate Python C API mechanisms. - The functions should adhere to the Python C API\'s conventions for reference counting and error handling. Example Usages: ```python import my_extension # Importing a module module = my_extension.import_module(\\"math\\") if module: print(module.sqrt(16)) # Should output: 4.0 # Reloading an imported module reloaded_module = my_extension.reload_module(module) if reloaded_module: print(reloaded_module.sqrt(25)) # Should output: 5.0 # Executing code within a new module context new_module = my_extension.exec_code_module(\\"custom_module\\", \\"def hello(): return \'Hello World\'\\") if new_module: print(new_module.hello()) # Should output: Hello World # Retrieving the dictionary of loaded modules module_dict = my_extension.get_module_dict() print(module_dict.keys()) # Should output a list of imported modules ``` Submission Requirements: - A single `.c` file implementing the required functions using the Python C API. - Instructions for compiling the C extension and a simple test script demonstrating the usage of the implemented functions. Evaluation Criteria: - Correct usage of the Python C API functions. - Proper handling of references and error checking. - Correct implementation of the specified functionalities. - Clarity and readability of the code.","solution":"# This is the Python interface for the C extension module \\"my_extension\\". # The implementation of the C functions would be in a C file. def import_module(name): Import a module by name. :param name: str, the name of the module to import. :return: module object or None if import fails import importlib try: return importlib.import_module(name) except ImportError: return None def reload_module(module): Reload a previously imported module. :param module: module object to be reloaded :return: reloaded module object or None if reload fails import importlib try: return importlib.reload(module) except ImportError: return None def exec_code_module(name, code): Create and execute a new module from given code. :param name: str, the name for the new module :param code: str, the Python code to be executed in the module :return: new module object or None if execution fails import importlib.machinery import types try: module = types.ModuleType(name) exec(code, module.__dict__) return module except Exception: return None def get_module_dict(): Get the dictionary of loaded modules. :return: dictionary mapping module names to module objects import sys return sys.modules"},{"question":"# PyTorch FFT: Signal Filtering Problem Statement You are provided with a noisy 1D signal (time series data) and your task is to implement a function that performs the following steps: 1. Compute the Fourier Transform of the signal to switch to the frequency domain. 2. Design a low-pass filter that eliminates high-frequency noise from the Fourier-transformed data. 3. Apply the low-pass filter to the data in the frequency domain. 4. Perform the Inverse Fourier Transform to convert the filtered data back to the time domain. 5. Return the filtered signal. Function Signature ```python import torch def low_pass_filter(signal: torch.Tensor, cutoff_freq: float) -> torch.Tensor: Applies a low-pass filter to the given 1D signal (time series data) Args: signal (torch.Tensor): A 1D tensor of shape (N,) representing the noisy signal in the time domain. cutoff_freq (float): The cutoff frequency for the low-pass filter. Returns: torch.Tensor: A 1D tensor of shape (N,) representing the filtered signal in the time domain. pass ``` Input - `signal`: A 1D tensor of shape (N,), where N is the number of samples in the time domain signal. - `cutoff_freq`: A float representing the cutoff frequency for the low-pass filter. Output - A 1D tensor of shape (N,) representing the filtered signal in the time domain. Example ```python import torch # Sample signal with noise signal = torch.tensor([0.5, 1.0, 1.5, 1.0, 2.0, 1.5, 0.5, 0.0, -0.5, 0.0, 0.5, 1.0, -1.5, -2.0, -1.5, -0.5, 1.0, 1.5, 1.0, 0.5], dtype=torch.float32) cutoff_freq = 0.3 filtered_signal = low_pass_filter(signal, cutoff_freq) print(filtered_signal) ``` Constraints - You must use PyTorch functions for the Fourier Transform and its inverse (`torch.fft.fft` and `torch.fft.ifft`). - Design the low-pass filter such that it zeros out the frequency components beyond the cutoff frequency. Additional Instructions - To create the frequency domain representation of the signal, use `torch.fft.fftfreq` to obtain the frequencies corresponding to each component. - To shift the zero-frequency component to the center of the spectrum, use `torch.fft.fftshift` and `torch.fft.ifftshift`.","solution":"import torch def low_pass_filter(signal: torch.Tensor, cutoff_freq: float) -> torch.Tensor: Applies a low-pass filter to the given 1D signal (time series data) Args: signal (torch.Tensor): A 1D tensor of shape (N,) representing the noisy signal in the time domain. cutoff_freq (float): The cutoff frequency for the low-pass filter. Returns: torch.Tensor: A 1D tensor of shape (N,) representing the filtered signal in the time domain. # Fourier Transform freq_domain = torch.fft.fft(signal) # Frequencies corresponding to each component freqs = torch.fft.fftfreq(signal.size(0)) # Apply the low-pass filter mask = freqs.abs() <= cutoff_freq filtered_freq_domain = freq_domain * mask # Inverse Fourier Transform filtered_signal = torch.fft.ifft(filtered_freq_domain).real return filtered_signal"},{"question":"# WAV File Processor **Objective**: Write a Python program that reads information from a given WAV file and creates two new WAV files with certain modifications. Requirements: 1. **Read the WAV File**: - Read a WAV file provided as input. - Extract the following information: - Number of channels - Sample width - Frame rate - Number of frames - Compression type 2. **Modification 1 - Half Speed**: - Create a new WAV file where the audio plays at half the original speed. This should effectively double the duration of the audio. - The new file should have the same number of channels and sample width, but half the frame rate. 3. **Modification 2 - Mono Conversion**: - If the original audio is stereo (2 channels), create a new WAV file converting the audio to mono (1 channel). - You may average the two channels for this conversion. - The new file should have the same sample width and frame rate using the averaged frames of the channels. **Constraints**: - Assume the input WAV file is always in \\"WAVE_FORMAT_PCM\\". - The `wave` module must be used for reading and writing the WAV files. - Handle exceptions properly, such as file not found or invalid WAV file. - Provide appropriate function signatures and docstrings. Function Signature: ```python def process_wav_file(input_wav_path: str, output_half_speed_wav_path: str, output_mono_wav_path: str) -> None: Processes a given WAV file to create two new WAV files with modifications. Parameters: input_wav_path (str): Path to the input WAV file. output_half_speed_wav_path (str): Path to save the half-speed modified WAV file. output_mono_wav_path (str): Path to save the mono-converted WAV file. Returns: None pass ``` Example Usage: ```python input_wav_path = \\"input.wav\\" output_half_speed_wav_path = \\"output_half_speed.wav\\" output_mono_wav_path = \\"output_mono.wav\\" process_wav_file(input_wav_path, output_half_speed_wav_path, output_mono_wav_path) ``` Grading Criteria: - Correct extraction of WAV file parameters. - Correct modifications for half-speed and mono conversion. - Proper use of the `wave` module. - Handling of edge cases and exceptions.","solution":"import wave import os def process_wav_file(input_wav_path: str, output_half_speed_wav_path: str, output_mono_wav_path: str) -> None: Processes a given WAV file to create two new WAV files with modifications: - One playing at half the original speed. - One converting it to mono if it\'s stereo. Parameters: input_wav_path (str): Path to the input WAV file. output_half_speed_wav_path (str): Path to save the half-speed modified WAV file. output_mono_wav_path (str): Path to save the mono-converted WAV file. Returns: None try: with wave.open(input_wav_path, \'rb\') as original_wave: n_channels = original_wave.getnchannels() sample_width = original_wave.getsampwidth() frame_rate = original_wave.getframerate() n_frames = original_wave.getnframes() frames = original_wave.readframes(n_frames) # Create half-speed WAV file with wave.open(output_half_speed_wav_path, \'wb\') as half_speed_wave: half_speed_wave.setnchannels(n_channels) half_speed_wave.setsampwidth(sample_width) half_speed_wave.setframerate(frame_rate // 2) half_speed_wave.writeframes(frames) # Create mono WAV file if n_channels == 2: mono_frames = bytearray() for i in range(0, len(frames), sample_width * 2): left_sample = frames[i:i+sample_width] right_sample = frames[i+sample_width:i+sample_width*2] # Average the two stereo channels to produce mono left_value = int.from_bytes(left_sample, byteorder=\'little\', signed=True) right_value = int.from_bytes(right_sample, byteorder=\'little\', signed=True) mono_value = ((left_value + right_value) // 2).to_bytes(sample_width, byteorder=\'little\', signed=True) mono_frames.extend(mono_value) with wave.open(output_mono_wav_path, \'wb\') as mono_wave: mono_wave.setnchannels(1) mono_wave.setsampwidth(sample_width) mono_wave.setframerate(frame_rate) mono_wave.writeframes(mono_frames) else: # Just copy the original file if it\'s already mono with wave.open(output_mono_wav_path, \'wb\') as mono_wave: mono_wave.setparams(original_wave.getparams()) mono_wave.writeframes(frames) except FileNotFoundError: print(f\\"File {input_wav_path} not found.\\") except wave.Error as e: print(f\\"Error processing WAV file: {e}\\") except Exception as e: print(f\\"An unexpected error occurred: {e}\\")"},{"question":"# Question: Tuple Manipulation in `python310` As a Python developer, you have been given a task to create and manipulate tuples using the `python310` package. You are required to demonstrate your understanding of tuple operations by implementing a series of functions: 1. **Create Tuple**: ```python def create_tuple(n, *args): Create a new tuple object of size `n` containing the provided arguments. Parameters: n (int): The size of the tuple. args (variable): The elements to be packed into the tuple. Returns: tuple: The newly created tuple. pass ``` 2. **Resize Tuple**: ```python def resize_tuple(tup, new_size): Resize the given tuple to the new size. Parameters: tup (tuple): The original tuple. new_size (int): The new size of the tuple. Returns: tuple: The resized tuple. pass ``` 3. **Insert Item**: ```python def insert_item(tup, pos, item): Insert an item at the specified position in the tuple. Parameters: tup (tuple): The original tuple. pos (int): The position where the item is to be inserted. item: The item to be inserted. Returns: tuple: The tuple with the new item inserted. pass ``` 4. **Get Slice**: ```python def get_slice(tup, low, high): Get a slice of the tuple from low to high (exclusive). Parameters: tup (tuple): The original tuple. low (int): The starting index of the slice. high (int): The ending index of the slice (exclusive). Returns: tuple: The sliced tuple. pass ``` # Constraints - The input parameters will always be valid tuples and integers within the appropriate ranges. - Performance should be considered, especially for large tuples. - You are encouraged to handle edge cases such as inserting at the end or starting position of the tuple. # Example ```python # Example usage print(create_tuple(3, 1, 2, 3)) # Output: (1, 2, 3) print(resize_tuple((1, 2, 3), 5)) # Output: (1, 2, 3, None, None) print(insert_item((1, 2, 3), 1, 4)) # Output: (1, 4, 2, 3) print(get_slice((1, 2, 3, 4, 5), 1, 4)) # Output: (2, 3, 4) ``` Your task is to complete the implementation of these functions by making use of the knowledge you have gained from the `python310` documentation.","solution":"def create_tuple(n, *args): Create a new tuple object of size `n` containing the provided arguments. Parameters: n (int): The size of the tuple. args (variable): The elements to be packed into the tuple. Returns: tuple: The newly created tuple. return tuple(args[:n]) def resize_tuple(tup, new_size): Resize the given tuple to the new size. Parameters: tup (tuple): The original tuple. new_size (int): The new size of the tuple. Returns: tuple: The resized tuple. return tup[:new_size] + (None,) * (new_size - len(tup)) def insert_item(tup, pos, item): Insert an item at the specified position in the tuple. Parameters: tup (tuple): The original tuple. pos (int): The position where the item is to be inserted. item: The item to be inserted. Returns: tuple: The tuple with the new item inserted. return tup[:pos] + (item,) + tup[pos:] def get_slice(tup, low, high): Get a slice of the tuple from low to high (exclusive). Parameters: tup (tuple): The original tuple. low (int): The starting index of the slice. high (int): The ending index of the slice (exclusive). Returns: tuple: The sliced tuple. return tup[low:high]"},{"question":"You are required to implement a generator function that generates Fibonacci numbers up to a specified limit. The Fibonacci sequence is a series of numbers where a number is the sum of the two preceding ones, starting from 0 and 1. That is: ``` 0, 1, 1, 2, 3, 5, 8, 13, 21, 34, ... ``` Function Signature ```python def fibonacci_generator(limit: int): A generator function that returns Fibonacci numbers up to the specified limit. Args: limit (int): The upper bound for the Fibonacci numbers generated (exclusive). Yields: int: The next Fibonacci number in the sequence. ``` Constraints - `limit` will be a positive integer (1 <= limit <= 10^6). - The function should only yield values and should not return a list or any other data structure. Input Format - A single integer `limit`. Output Format - Yields the Fibonacci numbers one by one up to the specified `limit` (exclusive). Example ```python # Example usage gen = fibonacci_generator(15) print(list(gen)) # Output: [0, 1, 1, 2, 3, 5, 8, 13] ``` # Notes 1. The generator should stop yielding once it reaches a Fibonacci number equal to or greater than the specified limit. 2. Utilize the `yield` statement to produce Fibonacci numbers one by one. # Additional Requirement In addition to the primary generator function, write a function to validate if an input object is a generator. This function should be named `is_generator` and should utilize Python\'s `inspect` module. Function Signature ```python def is_generator(obj: object) -> bool: Check if the given object is a generator object. Args: obj (object): The object to check. Returns: bool: True if the object is a generator, False otherwise. ``` Example ```python import inspect print(is_generator(fibonacci_generator(15))) # Output: True print(is_generator([1, 2, 3])) # Output: False ``` Hints - Use the `yield` keyword to generate Fibonacci numbers. - Use the `inspect` module and `inspect.isgenerator` function to check if an object is a generator.","solution":"def fibonacci_generator(limit: int): A generator function that returns Fibonacci numbers up to the specified limit. Args: limit (int): The upper bound for the Fibonacci numbers generated (exclusive). Yields: int: The next Fibonacci number in the sequence. a, b = 0, 1 while a < limit: yield a a, b = b, a + b import inspect def is_generator(obj: object) -> bool: Check if the given object is a generator object. Args: obj (object): The object to check. Returns: bool: True if the object is a generator, False otherwise. return inspect.isgenerator(obj)"},{"question":"You are provided with a pandas DataFrame that contains data about employees in a company. The DataFrame consists of the following columns: - `name`: Employee names (strings). - `position`: Job positions of the employees (strings). - `department`: Department names (strings). - `salary`: Yearly salary of the employees (numeric values). - `start_date`: Date when the employee joined the company (strings in YYYY-MM-DD format). Here is an example snippet of the DataFrame: ```python import pandas as pd data = { \\"name\\": [\\"Alice Smith\\", \\"Bob Brown\\", \\"Charlie Johnson\\", \\"David Lee\\"], \\"position\\": [\\"Engineer\\", \\"Manager\\", \\"Technician\\", \\"Designer\\"], \\"department\\": [\\"Production\\", \\"HR\\", \\"Maintenance\\", \\"Creative\\"], \\"salary\\": [75000, 85000, 62000, 70000], \\"start_date\\": [\\"2015-06-01\\", \\"2018-07-22\\", \\"2017-03-15\\", \\"2019-08-01\\"] } df = pd.DataFrame(data) ``` **Tasks:** 1. **Standardize Column Names** - Standardize the column names by making them lowercased and replacing any spaces with underscores(`_`). 2. **Extract Year of Employment** - Extract the year part from the `start_date` values and create a new column `start_year` containing these extracted year values. 3. **Monthly Salary Calculation** - Add a new column `monthly_salary` that contains the monthly salary of each employee (i.e., `salary` divided by 12). 4. **String Manipulation on Department Names** - Ensure all `department` names are in title case (i.e., the first letter of each word is capitalized). 5. **Dummy Variables for Positions** - Generate dummy variables for the `position` column, and include them in the DataFrame. **Constraints:** - Do not use loops; make use of pandas\' vectorized operations. - Ensure that the `start_year` and `monthly_salary` columns have appropriate numeric data types. **Expected Output Format:** The DataFrame `df` after performing the above tasks should look like this: ```python name position department salary start_date start_year monthly_salary engineer manager technician designer 0 Alice Smith Engineer Production 75000 2015-06-01 2015 6250.0 1 0 0 0 1 Bob Brown Manager Hr 85000 2018-07-22 2018 7083.3 0 1 0 0 2 Charlie Johnson Technician Maintenance 62000 2017-03-15 2017 5166.7 0 0 1 0 3 David Lee Designer Creative 70000 2019-08-01 2019 5833.3 0 0 0 1 ``` **Function Signature:** ```python def transform_employee_data(df: pd.DataFrame) -> pd.DataFrame: # your code here return df ``` Implement the `transform_employee_data` function to perform the above tasks and return the transformed DataFrame.","solution":"import pandas as pd def transform_employee_data(df: pd.DataFrame) -> pd.DataFrame: # Standardize column names df.columns = df.columns.str.lower().str.replace(\' \', \'_\') # Extract year of employment df[\'start_year\'] = pd.to_datetime(df[\'start_date\']).dt.year # Calculate monthly salary df[\'monthly_salary\'] = df[\'salary\'] / 12 # Ensure department names are in title case df[\'department\'] = df[\'department\'].str.title() # Create dummy variables for the position column df = pd.get_dummies(df, columns=[\'position\']) return df"},{"question":"# Garbage Collector Memory Management and Debugging **Objective:** Create a Python function that performs the following operations using Python\'s \\"gc\\" (Garbage Collector) module: 1. **Initialize and Configure Garbage Collector:** - Enable the garbage collector. - Set debugging options to collect detailed statistics and save unreachable objects. - Set collection thresholds to control the frequency of garbage collection. 2. **Simulate Object Creation:** - Create a complex structure of objects (e.g., nested dictionaries and lists) which will result in some reference cycles. This should simulate a workload that utilizes the garbage collector. 3. **Garbage Collection Control and Inspection:** - Perform a full collection of all generations. - After the collection, retrieve and print statistics about the collection. - Print the list of all garbage collected objects. - Check if certain types of objects are tracked by the garbage collector before and after collection. 4. **Custom Callbacks:** - Register custom callbacks to track the start and stop events of garbage collections. The callbacks should print the phase (start/stop) and relevant information such as the number of objects collected and the generation being collected. 5. **Finalization:** - Add the necessary cleanup code to unfreeze the permanent generation objects if they were frozen. - Disable the garbage collection at the end of the function execution. **Requirements:** - The function should be named `manage_garbage_collector`. - No external libraries should be used; all operations should rely on the `gc` module and built-in Python functionalities. - Ensure that any created objects and reference cycles are carefully constructed and displayed for easy verification. **Function Signature:** ```python def manage_garbage_collector(): pass ``` **Expected Output:** The function should print multiple pieces of information, including: 1. The state of garbage collection before and after enabling it. 2. Collection statistics before and after a manual collection. 3. A list of unreachable objects found post-collection. 4. Details on the custom callbacks being triggered. **Hints:** - Utilize `gc.collect()`, `gc.set_debug()`, `gc.get_stats()`, and `gc.get_objects()` among other relevant functions. - Use `gc.is_tracked()` to determine if certain objects are tracked by the garbage collector. - Handle exceptions and edge cases gracefully, such as invalid generation numbers.","solution":"import gc def manage_garbage_collector(): # Enable the garbage collector gc.enable() print(\\"Garbage collection enabled:\\", gc.isenabled()) # Set debugging options gc.set_debug(gc.DEBUG_STATS | gc.DEBUG_SAVEALL) # Set collection thresholds (these are arbitrary values for demonstration purposes) gc.set_threshold(700, 10, 10) # Simulate object creation with reference cycles obj_a = {} obj_b = {\'a\': obj_a} obj_a[\'b\'] = obj_b # List to store garbage objects, ensuring we have something to collect garbage_list = [obj_a, obj_b] # Register custom callback def gc_callback(phase, info=None): if phase == \\"start\\": print(f\\"GC {phase}: generation {info}\\") elif phase == \\"stop\\": print(f\\"GC {phase}: collection done, collected {info.get(\'collected\')} objects\\") gc.callbacks.append(gc_callback) # Perform a full collection print(\\"Collecting...\\") gc.collect(generation=2) # Retrieve and print statistics about the collection print(\\"Collection statistics:\\", gc.get_stats()) # Print the list of all garbage collected objects print(\\"Garbage collected objects:\\", gc.garbage) # Check if objects are tracked by the garbage collector before and after collection print(\\"Is obj_a tracked before collection?\\", gc.is_tracked(obj_a)) print(\\"Is obj_a tracked after collection?\\", gc.is_tracked(obj_a)) # Finalization, clean up objects to ensure no remaining strong references del obj_a del obj_b del garbage_list # Disable the garbage collector gc.disable() print(\\"Garbage collection disabled:\\", not gc.isenabled())"},{"question":"<|Analysis Begin|> The provided documentation covers a wide range of operations related to various types of indexes in pandas, such as Index, MultiIndex, DatetimeIndex, and PeriodIndex. It includes properties, methods for modifying and computations, handling missing values, conversions, sorting, time-specific operations, and set operations like intersection, union, and difference. Given the comprehensive coverage of index manipulations, a suitable challenging question would involve creating and manipulating MultiIndex (as it likely involves more complexity and hierarchical data). The question could require using multiple methods and properties like `from_arrays`, `reorder_levels`, `remove_unused_levels`, and possibly conversions to a DataFrame. <|Analysis End|> <|Question Begin|> **MultiIndex Manipulation and Analysis** You are given a dataset that contains sales data for multiple stores over several months. Your task is to create a MultiIndex DataFrame from the given data and perform a series of manipulations and calculations on the MultiIndex. **Dataset:** * `stores` - List of store names. * `months` - List of months in the format \'YYYY-MM\'. * `sales` - List of integers representing sales for each store in each month. The data is organized in a way that for each store, sales data are recorded for each month. **Input:** * `stores`: A list of store names (strings). * `months`: A list of months (strings in \'YYYY-MM\' format). * `sales`: A list of lists of integers, where each inner list contains sales data for a corresponding store across all months. **Task:** 1. **Create a MultiIndex DataFrame**: - Create a DataFrame with a MultiIndex using `stores` as the first level and `months` as the second level. 2. **Calculate Total Sales per Store**: - Compute the total sales for each store and add it as a new column in the DataFrame. 3. **Reorder Levels**: - Reorder the levels of the MultiIndex such that the months come first and the stores come second. 4. **Remove Unused Levels and Final Formatting**: - Remove any unused levels in the MultiIndex, if there are any. - Convert the MultiIndex DataFrame to a regular DataFrame, keeping the index set to months. **Expected Output:** Your function should return the final DataFrame after performing all the above manipulations. **Python Function Signature:** ```python import pandas as pd def multiindex_manipulation(stores, months, sales): # Step 1: Create MultiIndex DataFrame # Step 2: Calculate Total Sales per Store # Step 3: Reorder Levels # Step 4: Remove Unused Levels and Final Formatting # Return the final DataFrame pass # Example Usage stores = [\'Store_A\', \'Store_B\'] months = [\'2023-01\', \'2023-02\', \'2023-03\'] sales = [ [100, 150, 200], # Sales for Store_A [80, 90, 100] # Sales for Store_B ] print(multiindex_manipulation(stores, months, sales)) ``` **Constraints:** - Each inner list in `sales` corresponds to sales data for one store across all `months`. - You may assume there will be no missing values in the input data. Your solution will be evaluated based on correctness, usage of pandas functionalities, and efficiency.","solution":"import pandas as pd def multiindex_manipulation(stores, months, sales): # Step 1: Create MultiIndex DataFrame arrays = [[store for store in stores for _ in months], months * len(stores)] multi_index = pd.MultiIndex.from_arrays(arrays, names=(\'Store\', \'Month\')) sales_data = [sale for store_sales in sales for sale in store_sales] df = pd.DataFrame(sales_data, index=multi_index, columns=[\'Sales\']) # Step 2: Calculate Total Sales per Store total_sales = df.groupby(level=\'Store\').sum().rename(columns={\'Sales\': \'Total_Sales\'}) df = df.join(total_sales, on=\'Store\') # Step 3: Reorder Levels df = df.reorder_levels([\'Month\', \'Store\']) # Step 4: Remove Unused Levels and Final Formatting df = df.reset_index(level=\'Store\').sort_index() return df # Example Usage stores = [\'Store_A\', \'Store_B\'] months = [\'2023-01\', \'2023-02\', \'2023-03\'] sales = [ [100, 150, 200], # Sales for Store_A [80, 90, 100] # Sales for Store_B ] print(multiindex_manipulation(stores, months, sales))"},{"question":"# Pandas Options API Challenge Problem Statement You are required to demonstrate your understanding of the pandas options API by configuring the display settings of DataFrames. Your task is to implement a function `configure_display_options` that performs the following: 1. **Set specific options for DataFrame display:** - Set `display.max_rows` to 10. - Set `display.max_columns` to 5. - Set `display.precision` to 4. - Set `display.max_colwidth` to 50. - Set `display.expand_frame_repr` to `True`. 2. **Retrieve the current value of these options and return them in a dictionary**. 3. **Reset all these options to their default values**. 4. **Use a context manager to temporarily**: - Set `display.max_rows` to 15. - Create and print a DataFrame with dimensions (20, 10) to demonstrate the temporary setting. Function Signature ```python def configure_display_options(): Configures and demonstrates pandas display options. Returns: dict: A dictionary containing the current values of specific display options. ``` Expected Output The function should return a dictionary with the following keys and their respective values: - `\'display.max_rows\'`: 10 - `\'display.max_columns\'`: 5 - `\'display.precision\'`: 4 - `\'display.max_colwidth\'`: 50 - `\'display.expand_frame_repr\'`: True Additionally, the function should print a DataFrame with dimensions (20, 10) with `display.max_rows` temporarily set to 15. Constraints - You must use the `pandas.set_option`, `pandas.get_option`, and `pandas.reset_option` functions where applicable. - The use of the `pandas.option_context` context manager is required to demonstrate temporary settings. Example Usage ```python result = configure_display_options() print(result) ``` # Output The function should output a DataFrame with appropriate settings printed within the context manager and return a dictionary with the specified options and their values. ```python { \'display.max_rows\': 10, \'display.max_columns\': 5, \'display.precision\': 4, \'display.max_colwidth\': 50, \'display.expand_frame_repr\': True } ``` Hints - Use `pandas.set_option` to set the values of the options. - Use `pandas.get_option` to retrieve the current values of the options. - Use `pandas.reset_option` to reset the values of the options. - Use `pandas.option_context` to temporarily set the values of the options within a specific block of code.","solution":"import pandas as pd def configure_display_options(): Configures and demonstrates pandas display options. Returns: dict: A dictionary containing the current values of specific display options. # Set specific options for DataFrame display pd.set_option(\'display.max_rows\', 10) pd.set_option(\'display.max_columns\', 5) pd.set_option(\'display.precision\', 4) pd.set_option(\'display.max_colwidth\', 50) pd.set_option(\'display.expand_frame_repr\', True) # Retrieve the current value of these options options_dict = { \'display.max_rows\': pd.get_option(\'display.max_rows\'), \'display.max_columns\': pd.get_option(\'display.max_columns\'), \'display.precision\': pd.get_option(\'display.precision\'), \'display.max_colwidth\': pd.get_option(\'display.max_colwidth\'), \'display.expand_frame_repr\': pd.get_option(\'display.expand_frame_repr\') } # Reset all these options to their default values pd.reset_option(\'display.max_rows\') pd.reset_option(\'display.max_columns\') pd.reset_option(\'display.precision\') pd.reset_option(\'display.max_colwidth\') pd.reset_option(\'display.expand_frame_repr\') # Use a context manager to temporarily change `display.max_rows` to 15 with pd.option_context(\'display.max_rows\', 15): # Create and print a DataFrame with dimensions (20, 10) df = pd.DataFrame({f\'col{i}\': range(1, 21) for i in range(10)}) print(df) return options_dict"},{"question":"Coding Assessment Question # Objective Implement a class in Python that mimics the `GenericAlias` behavior described in the provided documentation. This class should handle type hinting in a similar manner. # Details Implement a class named `MyGenericAlias` which should have the following specifications: - The constructor should take two parameters: `origin` and `args`. - `origin` should be a type. - `args` can be a tuple of types or a single type. - The attributes `__origin__` and `__args__` should be set based on the provided parameters. - If `args` is a single type, it should be automatically converted to a tuple. - The attribute `__parameters__` should be lazily evaluated from `__args__`. # Requirements 1. Define the class `MyGenericAlias`. 2. Implement the `__init__` method to initialize the object with `__origin__` and `__args__` attributes. 3. Implement the lazy property `__parameters__`, which should only be constructed from `__args__` when accessed for the first time. 4. Handle cases where `args` is not a tuple by converting it into a tuple automatically. # Expected Input and Output - Input: `origin` (a type), `args` (a type or a tuple of types) - Output: an instance of `MyGenericAlias` with the following attributes: - `__origin__` correctly set to the `origin` input. - `__args__` correctly set to the `args` input or a tuple containing the `args` input. - `__parameters__` lazily evaluated from `__args__`. # Example ```python class MyGenericAlias: def __init__(self, origin, args): self.__origin__ = origin if not isinstance(args, tuple): args = (args,) self.__args__ = args self.__parameters__ = None @property def __parameters__(self): if self._parameters is None: self._parameters = self.__args__ return self._parameters # Example Usage generic_alias = MyGenericAlias(list, (int, str)) print(generic_alias.__origin__) # Output: <class \'list\'> print(generic_alias.__args__) # Output: (<class \'int\'>, <class \'str\'>) print(generic_alias.__parameters__) # Output: (<class \'int\'>, <class \'str\'>) ``` # Constraints - You can assume `origin` is always a valid type. - `args` can be a type or a tuple of types. - The implementation should adhere to Python 3.9 or above standards.","solution":"class MyGenericAlias: def __init__(self, origin, args): self.__origin__ = origin if not isinstance(args, tuple): args = (args,) self.__args__ = args self._parameters = None @property def __parameters__(self): if self._parameters is None: # Here, we\'ll lazily evaluate __parameters__ as the set of args self._parameters = self.__args__ return self._parameters"},{"question":"**Objective**: Implement a function utilising the `faulthandler` module to set up a safeguard for a long-running Python process. This function needs to handle potential segmentation faults, deadlocks, and other critical issues, with customization for logging and auto-recovery. Task Create a function `setup_fault_handler(log_file_path: str, timeout: int, repeat: bool = False, auto_exit: bool = False) -> None` that configures the `faulthandler` module as follows: 1. **Enable fault handler**: - Enable the fault handler to handle `SIGSEGV`, `SIGFPE`, `SIGABRT`, `SIGBUS`, and `SIGILL` signals. - Tracebacks should be logged into the specified `log_file_path`. - The handler should produce tracebacks for all running threads. 2. **Setup timeout-based dumping**: - Initiate a timer to dump traceback after a specified `timeout` (in seconds). - Allow periodic logging if `repeat` is set to `True`. 3. **Auto-recovery mechanism**: - If `auto_exit` is `True`, the program should exit immediately with status code 1 after dumping the tracebacks. 4. **Custom signal handling** (optional, bonus task): - Include the capability to register a custom user signal (e.g., `SIGUSR1` on POSIX systems) that when received causes the traceback to be dumped to the log file. The function should properly handle file descriptor issues by ensuring that the provided log file is managed correctly and reinitialized if closed or replaced. Constraints - Assume `faulthandler` is available and can be imported. - Log file paths will be valid and accessible. - Signal handling is not available on Windows for user-defined signals. - Implement robust error handling especially when dealing with files and signals. Example ```python import faulthandler def setup_fault_handler(log_file_path: str, timeout: int, repeat: bool = False, auto_exit: bool = False) -> None: # Implement this function pass # Example usage setup_fault_handler(\'/path/to/logfile.log\', 10, repeat=True, auto_exit=True) ``` Given this configuration, ensure that the fault handler starts logging tracebacks to the specified log file, auto-recovers by exiting if critical issues are detected, and optionally responds to custom user signals. **Evaluation Criteria**: - Correct implementation of `faulthandler` functionalities as per specifications. - Proper handling of file descriptors and signal registration. - Code robustness and error handling. - Functional correctness and adherence to provided constraints.","solution":"import faulthandler import os import signal import sys import threading def setup_fault_handler(log_file_path: str, timeout: int, repeat: bool = False, auto_exit: bool = False) -> None: Configures the faulthandler module to handle critical issues and optionally auto-recover. Arguments: log_file_path : str : The path of the file where tracebacks will be logged. timeout : int : The number of seconds after which tracebacks will be dumped. repeat : bool : If True, dumps tracebacks periodically after every timeout seconds. auto_exit : bool : If True, the program exits with a status code 1 after dumping tracebacks. def dump_traceback(signum, frame): Dumps the traceback to the specified log file and exits if auto_exit is True. with open(log_file_path, \'a\') as f: faulthandler.dump_traceback(file=f) if auto_exit: sys.exit(1) # Enable fault handler for common fatal signals with open(log_file_path, \'a\') as f: faulthandler.enable(file=f, all_threads=True) # Set up timeout-based trace dumping def timeout_handler(): while True: threading.Event().wait(timeout) with open(log_file_path, \'a\') as f: faulthandler.dump_traceback(file=f) if not repeat: break threading.Thread(target=timeout_handler, daemon=True).start() # Optional: Handle custom user signal SIGUSR1 if hasattr(signal, \'SIGUSR1\'): signal.signal(signal.SIGUSR1, dump_traceback) # Optional: Handle standard faults (segmentation faults, etc.) for sig in [signal.SIGSEGV, signal.SIGFPE, signal.SIGABRT, signal.SIGBUS, signal.SIGILL]: signal.signal(sig, dump_traceback)"},{"question":"**Objective**: Implement and test the functionalities of the \\"faulthandler\\" module to demonstrate your understanding of its capabilities and how it can be used to debug Python programs effectively. Problem Statement You are required to create a Python script that uses the `faulthandler` module to: 1. Enable the fault handler and configure it to write tracebacks to a specified file. 2. Simulate a segmentation fault by calling an invalid memory location. 3. Ensure that the program dumps the tracebacks for all threads into the specified file upon encountering the segmentation fault. Your script should: 1. Start by enabling the fault handler and specifying a log file named \\"error_log.txt\\" to write tracebacks. 2. Create a function named `cause_segfault()` which will deliberately cause a segmentation fault using `ctypes.string_at(0)`. 3. Call this function to trigger the segmentation fault. 4. Ensure that after the segmentation fault, the traceback is present in the \\"error_log.txt\\" file. Constraints - You must use the `faulthandler` module\'s functionalities appropriately as described above. - Do not close the log file until the script completes execution to avoid issues with file descriptors. - Note that segmentation faults might behave differently in various execution environments. Input and Output There is no input to the script during runtime other than the file \\"error_log.txt\\" for tracebacks. Ensure the output file \\"error_log.txt\\" contains the detailed traceback of the segmentation fault. # Example A successful implementation will create an \\"error_log.txt\\" file with content similar to: ``` Fatal Python error: Segmentation fault Current thread 0x00007fb899f39700 (most recent call first): File \\"/home/python/cpython/Lib/ctypes/__init__.py\\", line 486 in string_at File \\"script.py\\", line 10 in cause_segfault File \\"script.py\\", line 14 in <module> Segmentation fault ``` # Hints - Use `faulthandler.enable(file, all_threads=True)` to enable the fault handler for all threads and specify the log file. - Use `ctypes.string_at(0)` to cause a segmentation fault. - Ensure proper exception handling and cleanup if necessary to close files or other resources. # Additional Information Use the `faulthandler.disable()` function to disable the fault handler once debugging is complete, if necessary.","solution":"import faulthandler import ctypes def cause_segfault(): Function to deliberately cause a segmentation fault by accessing memory at address 0. ctypes.string_at(0) def main(): # Open a file to log the traceback with open(\\"error_log.txt\\", \\"w\\") as f: # Enable the fault handler to write tracebacks to \'error_log.txt\' faulthandler.enable(file=f, all_threads=True) # Trigger the segmentation fault cause_segfault() if __name__ == \\"__main__\\": main()"},{"question":"# Sparse Tensors with PyTorch Objective: To assess your understanding of PyTorch\'s sparse tensor operations, you need to perform the following tasks using the PyTorch library. Tasks: 1. **Conversion and Creation:** - Create a dense 2D tensor with the following values: ``` [[0, 0, 1, 0], [1, 2, 0, 0], [0, 0, 0, 0]] ``` - Convert this dense tensor to a sparse CSR tensor. - Convert the sparse CSR tensor back to a dense tensor and verify if it matches the original tensor. 2. **Sparse Tensor Operations:** - Create two sparse COO tensors `A` and `B` from the following data: ``` A_indices = [[0, 1, 1], [2, 0, 2]] A_values = [3, 4, 5] A_shape = (2, 3) B_indices = [[1, 1, 1], [0, 1, 2]] B_values = [6, 7, 8] B_shape = (2, 3) ``` - Perform element-wise addition of these two sparse COO tensors and store the result in `C`. - Convert the result tensor `C` into a dense tensor and print it. Constraints & Requirements: 1. **Input/Output:** - The dense tensor for the first task should be hardcoded. - The indices, values, and shape for the COO tensors should be hardcoded as per the given data. - Print all intermediate conversions for verification. 2. **Performance:** - You are not required to optimize for performance, but make sure to use PyTorch\'s sparse tensor functionalities effectively as described. Example: Here is an example of what workflow should look like: ```python import torch # Task 1: Conversion and Creation dense_tensor = torch.tensor([ [0, 0, 1, 0], [1, 2, 0, 0], [0, 0, 0, 0] ]) # Convert to sparse CSR sparse_csr = dense_tensor.to_sparse_csr() # Convert back to dense dense_from_sparse = sparse_csr.to_dense() print(\\"Original Dense Tensor:\\") print(dense_tensor) print(\\"Sparse CSR Tensor:\\") print(sparse_csr) print(\\"Dense Tensor from Sparse CSR:\\") print(dense_from_sparse) # Task 2: Sparse Tensor Operations A_indices = torch.tensor([[0, 1, 1], [2, 0, 2]]) A_values = torch.tensor([3, 4, 5]) A_shape = (2, 3) A = torch.sparse_coo_tensor(A_indices, A_values, A_shape) B_indices = torch.tensor([[1, 1, 1], [0, 1, 2]]) B_values = torch.tensor([6, 7, 8]) B_shape = (2, 3) B = torch.sparse_coo_tensor(B_indices, B_values, B_shape) # Element-wise addition C = A + B.to_sparse() # Convert result to dense and print C_dense = C.to_dense() print(\\"Resultant Sparse COO Tensor:\\") print(C) print(\\"Dense Tensor from Sparse COO:\\") print(C_dense) ``` Submit the full code fulfilling the tasks described along with relevant comments explaining each step. Note: - You must use PyTorch functions to perform all conversions and operations. - Ensure your code runs without errors in a standard PyTorch environment. - Remember to validate your outputs where explicitly stated.","solution":"import torch def create_and_convert_dense_to_sparse_and_back(): # Task 1: Conversion and Creation dense_tensor = torch.tensor([ [0, 0, 1, 0], [1, 2, 0, 0], [0, 0, 0, 0] ]) # Convert to sparse CSR sparse_csr = dense_tensor.to_sparse_csr() # Convert back to dense dense_from_sparse = sparse_csr.to_dense() return dense_tensor, sparse_csr, dense_from_sparse def sparse_tensor_operations(): # Task 2: Sparse Tensor Operations A_indices = torch.tensor([[0, 1, 1], [2, 0, 2]]) A_values = torch.tensor([3, 4, 5]) A_shape = (2, 3) A = torch.sparse_coo_tensor(A_indices, A_values, A_shape) B_indices = torch.tensor([[1, 1, 1], [0, 1, 2]]) B_values = torch.tensor([6, 7, 8]) B_shape = (2, 3) B = torch.sparse_coo_tensor(B_indices, B_values, B_shape) # Element-wise addition C = A + B.to_sparse() # Convert result to dense and return C_dense = C.to_dense() return A, B, C, C_dense"},{"question":"**Coding Assessment Question:** # Objective Demonstrate your knowledge and understanding of the seaborn library by analyzing and visualizing a dataset. # Problem Statement You are provided with a dataset containing information on passengers traveling aboard the Titanic. Your task is to perform a comprehensive analysis and visualization of the data using seaborn. Your solution should include multiple plots that collectively provide insights into the dataset. # Dataset The dataset contains the following columns: - `survived`: Survival (0 = No, 1 = Yes) - `pclass`: Passenger class (1 = 1st, 2 = 2nd, 3 = 3rd) - `sex`: Sex - `age`: Age in years - `sibsp`: Number of siblings/spouses aboard the Titanic - `parch`: Number of parents/children aboard the Titanic - `fare`: Passenger fare - `embarked`: Port of Embarkation (C = Cherbourg, Q = Queenstown, S = Southampton) # Task 1. **Load the dataset**: Use `seaborn`\'s `load_dataset` function to load the dataset named `\\"titanic\\"`. 2. **Data Visualization**: - Create a **relational plot** using `relplot` to show the relationship between age and fare, with different markers for each class (`pclass`) and different colors for survival (`survived`). - Create a **categorical plot** using `catplot` to illustrate the count of passengers by class (`pclass`), separated by port of embarkation (`embarked`). - Create a **distribution plot** for the `fare` column using `displot`, and include kernel density estimation (`kde`). - Use `pairplot` to create a grid of relationships across `age`, `fare`, and `survived`, with separate colors for each sex (`sex`). 3. **Statistical Estimation**: - Create a `lineplot` showing the average fare per class (`pclass`). Include error bars to represent the 95% confidence interval. - Use `lmplot` to visualize the linear relationship between age and fare, distinguished by survival status (`survived`). # Expected Output - A series of plots that visualize different aspects of the Titanic dataset. - Each plot should include appropriate titles, labels, legends, and customizations to enhance readability and informativeness. # Additional Requirements - Ensure that all plots are clear and aesthetically pleasing. - Use seaborn\'s themes and customization options to improve the presentation of your plots. - Comment your code to explain the steps and reasoning behind your visualizations. # Example Here\'s an example of how you might start your analysis: ```python import seaborn as sns # Load the Titanic dataset titanic = sns.load_dataset(\\"titanic\\") # Apply the default theme sns.set_theme() # Create a relational plot sns.relplot( data=titanic, x=\\"age\\", y=\\"fare\\", hue=\\"survived\\", style=\\"pclass\\", kind=\\"scatter\\" ) # Show the plot sns.plt.show() ``` **Constraints**: - You must use seaborn for all visualizations. - Ensure your code runs without errors and produces the required plots.","solution":"import seaborn as sns import matplotlib.pyplot as plt def titanic_analysis(): # Load the Titanic dataset titanic = sns.load_dataset(\\"titanic\\") # Apply the default theme sns.set_theme() # Create a relational plot sns.relplot(data=titanic, x=\\"age\\", y=\\"fare\\", hue=\\"survived\\", style=\\"pclass\\", kind=\\"scatter\\") plt.title(\\"Age vs Fare by Survival and Passenger Class\\") plt.xlabel(\\"Age\\") plt.ylabel(\\"Fare\\") plt.show() # Create a categorical plot sns.catplot(data=titanic, x=\\"pclass\\", kind=\\"count\\", hue=\\"embarked\\") plt.title(\\"Passenger Count by Class and Embarkation Port\\") plt.xlabel(\\"Passenger Class\\") plt.ylabel(\\"Count\\") plt.show() # Create a distribution plot sns.displot(titanic[\\"fare\\"], kde=True) plt.title(\\"Distribution of Passenger Fares\\") plt.xlabel(\\"Fare\\") plt.ylabel(\\"Density\\") plt.show() # Create a pairplot sns.pairplot(titanic, vars=[\\"age\\", \\"fare\\", \\"survived\\"], hue=\\"sex\\") plt.suptitle(\\"Pairplot of Age, Fare, and Survival by Sex\\", y=1.02) plt.show() # Create a lineplot with statistical estimation sns.lineplot(data=titanic, x=\\"pclass\\", y=\\"fare\\", ci=95) plt.title(\\"Average Fare per Passenger Class with 95% CI\\") plt.xlabel(\\"Passenger Class\\") plt.ylabel(\\"Average Fare\\") plt.show() # Create an lmplot for linear relationship sns.lmplot(data=titanic, x=\\"age\\", y=\\"fare\\", hue=\\"survived\\") plt.title(\\"Linear Relationship Between Age and Fare by Survival\\") plt.xlabel(\\"Age\\") plt.ylabel(\\"Fare\\") plt.show()"},{"question":"**Objective**: Demonstrate your understanding of chunked data processing with the `chunk` module. # Problem Statement You are tasked with writing a function that reads and processes an EA IFF 85 chunked file. Your function will take a file-like object as input, and you will need to extract and return a list of all chunk IDs in the order they appear in the file, as well as the total size of all chunks combined. # Function Signature ```python def process_chunked_file(file) -> (list, int): Processes an EA IFF 85 chunked file and returns a list of chunk IDs and the total size of all chunks. Args: file (file-like object): A file-like object supporting the read() method. The file is expected to follow the EA IFF 85 chunk format. Returns: tuple: A tuple containing: - A list of chunk IDs (strings) in the order they appear in the file. - An integer representing the total size of all chunks combined. ``` # Input: - `file`: A file-like object (for example, an open file or a `BytesIO` stream) supporting the `read()` method. The file contains data in EA IFF 85 chunk format. # Output: - A tuple `(chunk_ids, total_size)` where: - `chunk_ids`: A list of chunk IDs (each a string of 4 characters) in the order they appear in the file. - `total_size`: An integer representing the total size of all chunks combined (excluding headers). # Constraints: 1. You must use the `chunk.Chunk` class to read and process the file. 2. Assume that the file may contain multiple chunks, and you should handle them sequentially. 3. You don\'t need to handle nested chunks or complex chunk structures for this task. # Example ```python from io import BytesIO data = ( b\'ABCD\' # ID b\'x00x00x00x04\' # size (4 bytes) b\'1234\' # data (4 bytes) b\'EFGH\' # ID b\'x00x00x00x02\' # size (2 bytes) b\'56\' # data (2 bytes) ) file = BytesIO(data) chunk_ids, total_size = process_chunked_file(file) print(chunk_ids) # Output: [\'ABCD\', \'EFGH\'] print(total_size) # Output: 6 ``` # Notes: - Utilize the `chunk` module\'s functionality to navigate through the file. - Ensure proper handling of file I/O and chunk-related operations.","solution":"import chunk def process_chunked_file(file): Processes an EA IFF 85 chunked file and returns a list of chunk IDs and the total size of all chunks. Args: file (file-like object): A file-like object supporting the read() method. The file is expected to follow the EA IFF 85 chunk format. Returns: tuple: A tuple containing: - A list of chunk IDs (strings) in the order they appear in the file. - An integer representing the total size of all chunks combined. chunk_ids = [] total_size = 0 while True: try: ch = chunk.Chunk(file, align=False, bigendian=True) except EOFError: break chunk_ids.append(ch.getname().decode(\'ascii\')) total_size += ch.chunksize ch.skip() return (chunk_ids, total_size)"},{"question":"**Problem Statement** You are given a list of dictionaries representing students, where each dictionary contains the following key-value pairs: - `name`: the name of the student (string) - `age`: the age of the student (integer) - `grades`: a dictionary of subjects and their corresponding grades (integer) Your task is to implement a function `process_students(students)` that performs the following operations using functions from the `operator` module: 1. **Filter out students** who have an average grade below 50. 2. **Sort the remaining students** by their age in ascending order. 3. **Select and return the names** and ages of the top 3 students with the highest average grades. # Function Signature ```python def process_students(students: List[Dict[str, Union[str, int, Dict[str, int]]]]) -> List[Tuple[str, int]]: ``` # Input - `students`: A list of dictionaries where each dictionary contains: - `name` (string): The name of the student. - `age` (integer): The age of the student. - `grades` (dictionary): A dictionary where keys are subject names (strings) and values are grades (integers). # Output - A list of tuples. Each tuple contains: - `name` (string): The name of the student. - `age` (integer): The age of the student. - The list should contain at most three students. # Example ```python students = [ {\\"name\\": \\"Alice\\", \\"age\\": 20, \\"grades\\": {\\"math\\": 70, \\"science\\": 75}}, {\\"name\\": \\"Bob\\", \\"age\\": 19, \\"grades\\": {\\"math\\": 50, \\"science\\": 45}}, {\\"name\\": \\"Charlie\\", \\"age\\": 21, \\"grades\\": {\\"math\\": 95, \\"science\\": 85}}, {\\"name\\": \\"David\\", \\"age\\": 22, \\"grades\\": {\\"math\\": 40, \\"science\\": 50}}, {\\"name\\": \\"Eve\\", \\"age\\": 20, \\"grades\\": {\\"math\\": 65, \\"science\\": 55}} ] assert process_students(students) == [(\\"Charlie\\", 21), (\\"Alice\\", 20), (\\"Eve\\", 20)] ``` # Constraints - All student names are unique. - The grades dictionary is non-empty and contains at least one subject. - Students list is non-empty and contains at least one student. # Notes - Use functions from the `operator` module wherever applicable. - The average grade of a student is computed as the sum of their grades divided by the number of subjects.","solution":"from typing import List, Dict, Union, Tuple import operator def process_students(students: List[Dict[str, Union[str, int, Dict[str, int]]]]) -> List[Tuple[str, int]]: # Filter out students who have an average grade below 50 filtered_students = [ student for student in students if sum(student[\'grades\'].values()) / len(student[\'grades\']) >= 50 ] # Sort the remaining students by their age in ascending order filtered_students.sort(key=operator.itemgetter(\'age\')) # Select and return the names and ages of the top 3 students with the highest average grades top_students = sorted( filtered_students, key=lambda student: sum(student[\'grades\'].values()) / len(student[\'grades\']), reverse=True )[:3] return [(student[\'name\'], student[\'age\']) for student in top_students]"},{"question":"Objective: To assess your understanding of the Python buffer protocol and how to interact with complex array structures efficiently. Problem Statement: You are provided with raw byte data representing a 2D image in a C-contiguous format (row-major order). You need to implement a function that: 1. Converts this byte data into a 2D list (a list of lists), where each inner list represents a row of pixels. 2. Applies a simple transformation to the image: swap the top and bottom halves of the image. 3. Converts the transformed 2D list back into the byte data format. Write a function: ```python def transform_image(byte_data: bytes, width: int, height: int) -> bytes: ``` Input: - `byte_data` (bytes): The raw byte data for the image. Each pixel is represented by one byte. - `width` (int): The width of the image. - `height` (int): The height of the image. Output: - (bytes): The transformed byte data. Constraints: - Assume that `width` and `height` are even integers. - Assume that the byte data is valid and corresponds to an image of the given `width` and `height`. Example: ```python # Example input: a 4x4 grayscale image (16 bytes) byte_data = bytes([i for i in range(16)]) # Original Image in 2D: # [ # [0, 1, 2, 3], # [4, 5, 6, 7], # [8, 9, 10, 11], # [12, 13, 14, 15] # ] # Transformed Image in 2D: # [ # [8, 9, 10, 11], # [12, 13, 14, 15], # [0, 1, 2, 3], # [4, 5, 6, 7] # ] # Expected output in bytes: expected_output = bytes([8, 9, 10, 11, 12, 13, 14, 15, 0, 1, 2, 3, 4, 5, 6, 7]) assert transform_image(byte_data, 4, 4) == expected_output ``` Notes: - You may use the `memoryview` and `struct` modules to help with the byte data manipulation. - Ensure your solution handles the transformation efficiently. - Document your code to explain your approach.","solution":"def transform_image(byte_data: bytes, width: int, height: int) -> bytes: Transforms the image by swapping the top and bottom halves. Parameters: - byte_data (bytes): The raw byte data for the image. - width (int): The width of the image. - height (int): The height of the image. Returns: - bytes: The transformed byte data. # Convert byte_data into a 2D list image_2d = [list(byte_data[row*width:(row+1)*width]) for row in range(height)] # Determine middle index mid_index = height // 2 # Swap the top and bottom halves image_2d_transformed = image_2d[mid_index:] + image_2d[:mid_index] # Flatten the 2D list back into bytes format transformed_byte_data = bytes(sum(image_2d_transformed, [])) return transformed_byte_data"},{"question":"# **Coding Assessment Question** # Objective You are required to demonstrate your comprehension of various linear regression methods provided by the scikit-learn library. The task is to implement and compare the performance of different linear regression techniques on a given dataset. # Problem Statement You are provided with a CSV file containing a dataset `data.csv` which includes several features and a target variable. Your task is to: 1. Load the dataset. 2. Split the dataset into training and testing sets. 3. Implement the following linear regression models: - Ordinary Least Squares (Linear Regression) - Ridge Regression - Lasso Regression - Elastic-Net Regression 4. Train each model with the training set. 5. Evaluate and compare the performance of each model on the testing set using the mean squared error (MSE) as the evaluation metric. 6. Output the coefficients and intercepts of each model. # Instructions 1. Implement and compare the following regression models using the scikit-learn library: - **Linear Regression**: Use `LinearRegression()`. - **Ridge Regression**: Use `Ridge()` with `alpha=1.0`. - **Lasso Regression**: Use `Lasso()` with `alpha=0.1`. - **Elastic-Net Regression**: Use `ElasticNet()` with `alpha=0.1` and `l1_ratio=0.5`. 2. Use `train_test_split` from `sklearn.model_selection` to split the dataset into 80% training and 20% testing sets. 3. Train each model on the training set and evaluate their performance on the testing set using `mean_squared_error` from `sklearn.metrics`. 4. Print the coefficients and intercept for each model after training. 5. Print the mean squared error for each model on the testing set. # Constraints - Use only the scikit-learn library for implementing the models. - Ensure your code is efficient and follows good coding practices. # Expected Input and Output **Input**: `data.csv` (The CSV file should include several features and one target variable) **Output**: - Coefficients and intercept for each model. - Mean Squared Error for each model on the testing set. # Example ```python import pandas as pd from sklearn.model_selection import train_test_split from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet from sklearn.metrics import mean_squared_error # Step 1: Load the dataset data = pd.read_csv(\'data.csv\') X = data.drop(\'target\', axis=1) y = data[\'target\'] # Step 2: Split the dataset X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Step 3: Implement and train the models models = { \\"Linear Regression\\": LinearRegression(), \\"Ridge Regression\\": Ridge(alpha=1.0), \\"Lasso Regression\\": Lasso(alpha=0.1), \\"ElasticNet Regression\\": ElasticNet(alpha=0.1, l1_ratio=0.5) } for name, model in models.items(): # Train the model model.fit(X_train, y_train) # Get coefficients and intercept coef = model.coef_ intercept = model.intercept_ # Predict and evaluate y_pred = model.predict(X_test) mse = mean_squared_error(y_test, y_pred) # Print results print(f\\"{name}:\\") print(f\\"Coefficients: {coef}\\") print(f\\"Intercept: {intercept}\\") print(f\\"Mean Squared Error: {mse}n\\") ``` # Notes - Ensure the dataset `data.csv` is available in your working directory. - You can adjust the `alpha` and `l1_ratio` parameters to observe how the results vary. - The example provided assumes `data.csv` contains a \'target\' column representing the target variable.","solution":"import pandas as pd from sklearn.model_selection import train_test_split from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet from sklearn.metrics import mean_squared_error def load_data(filename): return pd.read_csv(filename) def split_data(data, target_column, test_size=0.2, random_state=42): X = data.drop(target_column, axis=1) y = data[target_column] return train_test_split(X, y, test_size=test_size, random_state=random_state) def train_and_evaluate_models(X_train, X_test, y_train, y_test): models = { \\"Linear Regression\\": LinearRegression(), \\"Ridge Regression\\": Ridge(alpha=1.0), \\"Lasso Regression\\": Lasso(alpha=0.1), \\"ElasticNet Regression\\": ElasticNet(alpha=0.1, l1_ratio=0.5) } results = {} for name, model in models.items(): model.fit(X_train, y_train) coef = model.coef_ intercept = model.intercept_ y_pred = model.predict(X_test) mse = mean_squared_error(y_test, y_pred) results[name] = { \'coefficients\': coef, \'intercept\': intercept, \'mse\': mse } return results def main(filename, target_column): data = load_data(filename) X_train, X_test, y_train, y_test = split_data(data, target_column) return train_and_evaluate_models(X_train, X_test, y_train, y_test) # Usage Example: # result = main(\'data.csv\', \'target\') # for model_name, metrics in result.items(): # print(f\\"{model_name}:\\") # print(f\\"Coefficients: {metrics[\'coefficients\']}\\") # print(f\\"Intercept: {metrics[\'intercept\']}\\") # print(f\\"Mean Squared Error: {metrics[\'mse\']}n\\")"},{"question":"Coding Assessment Question # Objective Implement a Python function that concurrently processes a list of URLs to fetch their contents and processes the data. This task will demonstrate your understanding of asyncio’s capabilities for handling asynchronous network IO and synchronization. # Problem Statement You are provided with a list of URLs. Your task is to write an asynchronous function `fetch_and_process(urls: List[str]) -> List[str]` that fetches the content from each URL concurrently and processes each content by reversing the text. The function should return a list of processed contents. # Expected Input and Output - **Input**: A list of URLs. Example: `[\\"http://example.com/page1\\", \\"http://example.com/page2\\"]` - **Output**: A list of reversed contents from each URL in the same order. Example: `[\\"1egap/moc.elpmaxe//:ptth\\", \\"2egap/moc.elpmaxe//:ptth\\"]` # Constraints 1. Use the `aiohttp` library for making HTTP requests. 2. Each URL request should timeout after 5 seconds if it does not respond. 3. Handle exceptions gracefully for any URL that fails to fetch or times out, and store `\'ERROR\'` in the respective position in the output list for such URLs. 4. Assume URLs are always valid strings. 5. The total number of URLs will not exceed 1000. # Helper Function You may use the following helper function as a starting point: ```python import aiohttp import asyncio from typing import List async def fetch_url(session, url): try: async with session.get(url, timeout=5) as response: return await response.text() except: return \'ERROR\' ``` # Function Signature ```python import aiohttp import asyncio from typing import List async def fetch_and_process(urls: List[str]) -> List[str]: # Your implementation here ``` # Example Usage ```python async def main(): urls = [\\"http://example.com/page1\\", \\"http://example.com/page2\\"] result = await fetch_and_process(urls) print(result) # asyncio.run(main()) ``` # Notes - Ensure your function is well-tested and handles edge cases. - You may assume that the `aiohttp` library is installed in the environment where your code will run.","solution":"import aiohttp import asyncio from typing import List async def fetch_url(session, url): try: async with session.get(url, timeout=5) as response: return await response.text() except: return \'ERROR\' async def fetch_and_process(urls: List[str]) -> List[str]: async with aiohttp.ClientSession() as session: tasks = [fetch_url(session, url) for url in urls] fetched_contents = await asyncio.gather(*tasks) processed_contents = [content[::-1] if content != \'ERROR\' else \'ERROR\' for content in fetched_contents] return processed_contents"},{"question":"Objective: The goal of this task is to demonstrate your understanding and ability to utilize the `pprint` module by formatting complex data structures in a readable format with custom formatting options. Task: You are provided with a complex nested data structure and a custom PrettyPrinter class. Your task is to extend the functionality of the PrettyPrinter class to include a new feature: the ability to replace specified values in the data structure with a placeholder string before pretty-printing it. You will implement a method `replace_and_print` in the PrettyPrinter class which replaces specific target values in the passed data structure with a placeholder and then prints the formatted representation. Implementation: 1. Implement the method `replace_and_print` in the PrettyPrinter class. 2. The method should accept three parameters: - `data`: The complex data structure to be pretty-printed. - `target_value`: The value in the data structure to be replaced with the placeholder. - `placeholder`: The string used to replace the target_value. 3. Replace all instances of `target_value` in the `data` with `placeholder`. 4. Use the existing `pprint` method to pretty-print the modified data structure. Specifications: - Input: - `data`: A complex nested data structure (e.g., lists, dictionaries, tuples containing various data types). - `target_value`: A value that appears in the data structure to be replaced (can be of any type). - `placeholder`: A string that will replace every occurrence of `target_value` in the data. - Output: - A pretty-printed version of the modified data structure with the target values replaced by the placeholder. Example: ```python import pprint class CustomPrettyPrinter(pprint.PrettyPrinter): def replace_and_print(self, data, target_value, placeholder): # Your implementation here # Example usage: if __name__ == \\"__main__\\": complex_data = { \'name\': \'sampleproject\', \'details\': { \'author\': \'John Doe\', \'license\': \'MIT\', \'dependencies\': [\'pprint\', \'json\', \'urllib\'] }, \'description\': \'A sample project for demonstration\', \'version\': \'1.0.0\' } target_value = \'MIT\' placeholder = \'UNKNOWN_LICENSE\' cpp = CustomPrettyPrinter(indent=2) cpp.replace_and_print(complex_data, target_value, placeholder) # Expected Output: { \'name\': \'sampleproject\', \'details\': { \'author\': \'John Doe\', \'license\': \'UNKNOWN_LICENSE\', \'dependencies\': [ \'pprint\', \'json\', \'urllib\' ] }, \'description\': \'A sample project for demonstration\', \'version\': \'1.0.0\' } ``` Constraints: - Ensure that the method handles data structures of arbitrary depth. - Ensure that only the specified `target_value` is replaced. - Performance considerations: The solution should efficiently handle large and deeply nested data structures. Hints: - You might find it useful to write a helper function to recursively traverse and modify the data structure. - Consider different types of nested data structures such as dictionaries, lists, and tuples while implementing the solution.","solution":"import pprint class CustomPrettyPrinter(pprint.PrettyPrinter): def replace_and_print(self, data, target_value, placeholder): def replace_value(item): if isinstance(item, dict): return {k: replace_value(v) for k, v in item.items()} elif isinstance(item, list): return [replace_value(elem) for elem in item] elif isinstance(item, tuple): return tuple(replace_value(elem) for elem in item) elif item == target_value: return placeholder else: return item modified_data = replace_value(data) self.pprint(modified_data) # Example usage: if __name__ == \\"__main__\\": complex_data = { \'name\': \'sampleproject\', \'details\': { \'author\': \'John Doe\', \'license\': \'MIT\', \'dependencies\': [\'pprint\', \'json\', \'urllib\'] }, \'description\': \'A sample project for demonstration\', \'version\': \'1.0.0\' } target_value = \'MIT\' placeholder = \'UNKNOWN_LICENSE\' cpp = CustomPrettyPrinter(indent=2) cpp.replace_and_print(complex_data, target_value, placeholder)"},{"question":"# Advanced Asyncio Assessment: Multi-Protocol Echo Server. **Objective:** You are required to implement a Multi-Protocol Echo Server that supports both TCP and UDP connections. The server should handle multiple concurrent connections, echo back any received message, and log connections and data transfers. # Requirements: 1. Implement Custom Protocol: - Create a class `EchoProtocol` inheriting from `asyncio.Protocol` for TCP. - Implement the `connection_made` method to log the connection. - Implement the `data_received(data)` method to echo the received message and log the data. - Create a class `EchoDatagramProtocol` inheriting from `asyncio.DatagramProtocol` for UDP. - Implement the `datagram_received(data, addr)` method to echo the received message and log the data. 2. Combine Transports: - Use asyncio event loop methods to create TCP and UDP servers that use the custom protocol classes. 3. Logging: - Log each connection or data transfer event in both classes to the console. **Function Signatures:** ```python import asyncio class EchoProtocol(asyncio.Protocol): def connection_made(self, transport): # Implement connection made logging and setup pass def data_received(self, data): # Implement echoing and logging the data received pass class EchoDatagramProtocol(asyncio.DatagramProtocol): def datagram_received(self, data, addr): # Implement echoing and logging the datagram received pass async def main(): loop = asyncio.get_running_loop() # Create TCP server server = await loop.create_server( lambda: EchoProtocol(), \'127.0.0.1\', 8888) # Create UDP server transport, protocol = await loop.create_datagram_endpoint( lambda: EchoDatagramProtocol(), local_addr=(\'127.0.0.1\', 9999)) async with server: await asyncio.sleep(3600) # Keep the server running for 1 hour. if __name__ == \'__main__\': asyncio.run(main()) ``` # Constraints: 1. **Concurrency**: Ensure that the server can handle multiple concurrent connections for both TCP and UDP. 2. **Logging**: Every connection made and data received must be logged to the console. 3. **Efficiency**: Avoid blocking calls and ensure all operations are asynchronous. # Example Output: ``` TCP: Connection from (\'127.0.0.1\', 12345) TCP: Data received: \'Hello, TCP!\' UDP: Received \'Hello, UDP!\' from (\'127.0.0.1\', 54321) ``` # Notes: - Ensure you have Python 3.7 or later to utilize the asyncio enhancements. - This task requires creating a balanced design demonstrating both understanding of asynchronous protocols and transporting mechanisms in Python’s asyncio library. - The provided example code outline is not complete; fill in the logic as per the instructions. Good luck, and happy coding!","solution":"import asyncio class EchoProtocol(asyncio.Protocol): def connection_made(self, transport): self.transport = transport peername = transport.get_extra_info(\'peername\') print(f\'TCP: Connection from {peername}\') def data_received(self, data): message = data.decode() print(f\'TCP: Data received: {message}\') self.transport.write(data) print(f\'TCP: Data sent back: {message}\') class EchoDatagramProtocol(asyncio.DatagramProtocol): def datagram_received(self, data, addr): message = data.decode() print(f\'UDP: Received {message} from {addr}\') self.transport.sendto(data, addr) print(f\'UDP: Data sent back to {addr}: {message}\') async def main(): loop = asyncio.get_running_loop() # Create TCP server tcp_server = await loop.create_server( lambda: EchoProtocol(), \'127.0.0.1\', 8888) # Create UDP server udp_transport, udp_protocol = await loop.create_datagram_endpoint( lambda: EchoDatagramProtocol(), local_addr=(\'127.0.0.1\', 9999)) async with tcp_server: await asyncio.sleep(3600) # Keep the server running for 1 hour if __name__ == \'__main__\': asyncio.run(main())"},{"question":"**Title: Handling Asynchronous Exceptions** **Objective:** Write a function that demonstrates the handling of various `asyncio` exceptions. Your function should perform asynchronous tasks and handle these exceptions appropriately, logging relevant information for each exception. **Instructions:** 1. Implement an asyncio function named `perform_async_operations`. 2. This function should: - Simulate different operations that could raise the exceptions listed in the provided documentation. - Handle each exception using `try-except` blocks and log relevant information like exception type and details. 3. The function should simulate at least the following scenarios: - A task that exceeds a timeout, raising `asyncio.TimeoutError`. - A cancelled task, raising `asyncio.CancelledError`. - An invalid operation on a future, raising `asyncio.InvalidStateError`. - A simulated sendfile operation failure, raising `asyncio.SendfileNotAvailableError`. - An incomplete read operation, raising `asyncio.IncompleteReadError`. - A buffer limit overrun, raising `asyncio.LimitOverrunError`. 4. Make use of any additional helper functions or coroutines as needed to simulate these conditions. **Input and Output requirements:** - **Input:** There are no input parameters for the `perform_async_operations` function. - **Output:** The function should return a list of exception logs detailing the type of exception and the respective message. **Constraints:** - You should use the `asyncio` package efficiently. - Ensure proper use of try-except blocks to capture and handle each exception. - Your function should complete execution without crashing, logging all the exceptions that occur. **Performance Requirements:** - Ensure that the implementation is non-blocking and makes use of asyncio\'s capabilities. - Aim for a clean, readable, and well-documented code. **Example:** ```python import asyncio async def perform_async_operations(): logs = [] try: # Simulate a timeout error await asyncio.wait_for(asyncio.sleep(2), timeout=1) except asyncio.TimeoutError as e: logs.append(f\\"TimeoutError: {str(e)}\\") try: # Simulate a cancelled task task = asyncio.create_task(asyncio.sleep(2)) task.cancel() await task except asyncio.CancelledError as e: logs.append(f\\"CancelledError: {str(e)}\\") try: # Simulate an invalid state error fut = asyncio.Future() fut.set_result(\\"Value\\") fut.set_result(\\"Another Value\\") except asyncio.InvalidStateError as e: logs.append(f\\"InvalidStateError: {str(e)}\\") try: # Simulate a sendfile not available error raise asyncio.SendfileNotAvailableError(\\"Sendfile syscall not available\\") except asyncio.SendfileNotAvailableError as e: logs.append(f\\"SendfileNotAvailableError: {str(e)}\\") try: # Simulate an incomplete read error raise asyncio.IncompleteReadError(expected=10, partial=b\'12345\') except asyncio.IncompleteReadError as e: logs.append(f\\"IncompleteReadError: Expected {e.expected}, Received: {e.partial}\\") try: # Simulate a limit overrun error raise asyncio.LimitOverrunError(consumed=1024) except asyncio.LimitOverrunError as e: logs.append(f\\"LimitOverrunError: Consumed {e.consumed}\\") return logs # Example usage asyncio.run(perform_async_operations()) ```","solution":"import asyncio async def perform_async_operations(): logs = [] try: # Simulate a timeout error await asyncio.wait_for(asyncio.sleep(2), timeout=1) except asyncio.TimeoutError as e: logs.append(f\\"TimeoutError: {str(e)}\\") try: # Simulate a cancelled task task = asyncio.create_task(asyncio.sleep(2)) task.cancel() await task except asyncio.CancelledError as e: logs.append(f\\"CancelledError: {str(e)}\\") try: # Simulate an invalid state error fut = asyncio.Future() fut.set_result(\\"Value\\") fut.set_result(\\"Another Value\\") except asyncio.InvalidStateError as e: logs.append(f\\"InvalidStateError: {str(e)}\\") try: # Simulate a sendfile not available error raise asyncio.SendfileNotAvailableError(\\"Sendfile syscall not available\\") except asyncio.SendfileNotAvailableError as e: logs.append(f\\"SendfileNotAvailableError: {str(e)}\\") try: # Simulate an incomplete read error raise asyncio.IncompleteReadError(expected=10, partial=b\'12345\') except asyncio.IncompleteReadError as e: logs.append(f\\"IncompleteReadError: Expected {e.expected}, Received: {e.partial}\\") try: # Simulate a limit overrun error raise asyncio.LimitOverrunError(consumed=1024) except asyncio.LimitOverrunError as e: logs.append(f\\"LimitOverrunError: Consumed {e.consumed}\\") return logs"},{"question":"# Advanced Python Coding Assessment Problem Statement: You are required to implement a class `BigInt` that handles arbitrary-precision integers using Python\'s built-in data types and some low-level manipulation inspired by the C-level interactions described in the provided `PyLongObject` documentation. The class should support the following operations: 1. **Initialization:** Initialize the `BigInt` with either an integer, string, or another `BigInt`. 2. **String Representation:** Return the string representation of the `BigInt`. 3. **Addition:** Add two `BigInt` objects. 4. **Subtraction:** Subtract two `BigInt` objects. 5. **Multiplication:** Multiply two `BigInt` objects. 6. **Division:** Divide two `BigInt` objects (integer division). Functional Requirements: 1. **Initialization:** ```python def __init__(self, value): Initialize the BigInt object with an integer, string, or another BigInt. Raises ValueError if initialization with non-integer strings. ``` 2. **String Representation:** ```python def __str__(self): Return the string representation of the BigInt. ``` 3. **Addition:** ```python def __add__(self, other): Return a new BigInt that is the sum of self and other BigInt. ``` 4. **Subtraction:** ```python def __sub__(self, other): Return a new BigInt that is the difference of self and other BigInt. ``` 5. **Multiplication:** ```python def __mul__(self, other): Return a new BigInt that is the product of self and other BigInt. ``` 6. **Division:** ```python def __floordiv__(self, other): Return a new BigInt that is the integer division of self by other BigInt. ``` Constraints: - The operations should handle integers of arbitrarily large size. - Ensure that the operations are performed efficiently. Example Usage: ```python a = BigInt(123456789012345678901234567890) b = BigInt(\\"98765432109876543210987654321\\") print(str(a)) # \\"123456789012345678901234567890\\" print(str(b)) # \\"98765432109876543210987654321\\" print(str(a + b)) # \\"222222221111111111111111111211\\" print(str(a - b)) # \\"246913569024691358024691358569\\" print(str(a * b)) # \\"1219326311126352690000000000000000000000000000000000000000\\" print(str(a // b)) # \\"1\\" ``` Feel free to use Python\'s built-in types to manage the underlying large integer data. Your implementation should be robust and able to handle various edge cases gracefully.","solution":"class BigInt: def __init__(self, value): Initialize the BigInt object with an integer, string, or another BigInt. Raises ValueError if initialization with non-integer strings. if isinstance(value, int): self.value = value elif isinstance(value, str): if value.isdigit() or (value.startswith(\'-\') and value[1:].isdigit()): self.value = int(value) else: raise ValueError(\\"Invalid value for BigInt\\") elif isinstance(value, BigInt): self.value = value.value else: raise ValueError(\\"Invalid type for BigInt initialization\\") def __str__(self): Return the string representation of the BigInt. return str(self.value) def __add__(self, other): Return a new BigInt that is the sum of self and other BigInt. if not isinstance(other, BigInt): raise TypeError(\\"Operand must be of type BigInt\\") return BigInt(self.value + other.value) def __sub__(self, other): Return a new BigInt that is the difference of self and other BigInt. if not isinstance(other, BigInt): raise TypeError(\\"Operand must be of type BigInt\\") return BigInt(self.value - other.value) def __mul__(self, other): Return a new BigInt that is the product of self and other BigInt. if not isinstance(other, BigInt): raise TypeError(\\"Operand must be of type BigInt\\") return BigInt(self.value * other.value) def __floordiv__(self, other): Return a new BigInt that is the integer division of self by other BigInt. if not isinstance(other, BigInt): raise TypeError(\\"Operand must be of type BigInt\\") if other.value == 0: raise ZeroDivisionError(\\"Division by zero\\") return BigInt(self.value // other.value)"},{"question":"# Virtual Environment Creator **Objective:** Create a custom virtual environment builder by extending the `EnvBuilder` class provided by the `venv` module. This custom builder should perform the following additional tasks: 1. Create a README file in the virtual environment that includes the environment name and Python version. 2. Install a dummy package (a simple hello world script) into the environment. **Task:** Implement a class `CustomEnvBuilder` that inherits from `venv.EnvBuilder`. Your implementation should include: 1. A `post_setup` method to create a README file and install a dummy package. 2. The README file should contain: - The environment name (directory name). - The Python version used in the environment. 3. The dummy package should be a simple script named `hello.py` that prints \\"Hello, World!\\" when executed. **Input:** The `create` method of your `CustomEnvBuilder` class should take the path to the directory where the virtual environment should be created. **Output:** The virtual environment should be created at the specified path, and it should include a README file and the dummy package script. **Constraints:** - You must use the `venv` module. - Only standard library modules should be used, without relying on external packages like `pip`. **Example Usage:** ```python import os from custom_env_builder import CustomEnvBuilder # Create an instance of CustomEnvBuilder builder = CustomEnvBuilder() # Create a virtual environment in the specified directory builder.create(\'my_custom_venv\') # Check the contents of the created environment os.listdir(\'my_custom_venv\') # The \'my_custom_venv\' directory should contain: # - A README file with the environment name and Python version # - A script named \'hello.py\' that prints \\"Hello, World!\\" ``` **Hints:** - Use the `post_setup` method to perform actions after the virtual environment has been set up. - Utilize `context` object properties like `context.env_dir` and `context.executable` to access the environment directory and the Python executable. - Ensure that the README file and the dummy package script are written to the correct paths within the virtual environment.","solution":"import venv import os import sys class CustomEnvBuilder(venv.EnvBuilder): def post_setup(self, context): # Create a README file env_dir = context.env_dir readme_path = os.path.join(env_dir, \'README.txt\') with open(readme_path, \'w\') as readme_file: readme_file.write(f\\"Environment Name: {os.path.basename(env_dir)}n\\") readme_file.write(f\\"Python Version: {sys.version}n\\") # Create the dummy package dummy_package_path = os.path.join(env_dir, \'hello.py\') with open(dummy_package_path, \'w\') as dummy_file: dummy_file.write(\'print(\\"Hello, World!\\")n\') def main(): builder = CustomEnvBuilder(with_pip=True) builder.create(\'my_custom_venv\') if __name__ == \\"__main__\\": main()"},{"question":"**Objective**: Demonstrate your understanding of Python versioning macros and hexadecimal conversion in CPython by implementing two functions: one that converts a version string to a hexadecimal value and another that decodes a hexadecimal version value back to its components. **Task**: 1. Implement a function `version_to_hex(version_str: str) -> int` that converts a version string in the format \\"MAJOR.MINOR.MICRORELEASESERIAL\\" (e.g., \\"3.4.1a2\\") into its corresponding 32-bit hexadecimal value. 2. Implement a function `hex_to_version(hex_version: int) -> str` that converts a 32-bit hexadecimal version value back into its corresponding version string in the format \\"MAJOR.MINOR.MICRORELEASESERIAL\\". **Specifications**: - The version string can have the following formats: - For alpha, beta, and release candidate: \\"MAJOR.MINOR.MICRORELEASESERIAL\\", where RELEASE can be \\"a\\" (alpha), \\"b\\" (beta), or \\"rc\\" (release candidate) followed by a serial number. - For final release: \\"MAJOR.MINOR.MICRO\\" with no following characters. - The function `version_to_hex` should handle edge cases, including omitted micro version or release level and serial. - The function `hex_to_version` should accurately parse the hexadecimal number back into a human-readable version format as specified. **Input/Output Formats**: - `version_to_hex(version_str: str) -> int` - **Input**: A version string (e.g., \\"3.4.1a2\\"). - **Output**: The 32-bit hexadecimal value representing the version (e.g., 0x030401a2). - `hex_to_version(hex_version: int) -> str` - **Input**: A 32-bit hexadecimal value (e.g., 0x030401a2). - **Output**: A version string (e.g., \\"3.4.1a2\\"). **Examples**: ```python def version_to_hex(version_str: str) -> int: Convert the given version string to a 32-bit hexadecimal value. pass def hex_to_version(hex_version: int) -> str: Convert the given 32-bit hexadecimal value to a version string. pass # Example Usage: # version_to_hex(\\"3.4.1a2\\") -> 0x030401a2 # hex_to_version(0x030401a2) -> \\"3.4.1a2\\" # version_to_hex(\\"3.10.0\\") -> 0x030a00f0 # hex_to_version(0x030a00f0) -> \\"3.10.0\\" ``` **Constraints**: - The function should handle both upper and lower case input for release levels. - Ensure the functions are optimized and can handle version strings not exceeding the format \\"99.99.99Z99\\". **Notes**: - You may assume the given version strings are valid and do not require additional validation. - Focus on correctness and code readability.","solution":"def version_to_hex(version_str: str) -> int: Convert the given version string to a 32-bit hexadecimal value. import re match = re.match(r\'^(d+).(d+).(d+)?(a|b|rc)?(d+)?\', version_str) if not match: raise ValueError(\\"Invalid version string format\\") major = int(match.group(1)) minor = int(match.group(2)) micro = match.group(3) if micro is None: micro = 0 else: micro = int(micro) release = match.group(4) if release is None: release_val = 15 elif release == \'a\': release_val = 0 elif release == \'b\': release_val = 1 elif release == \'rc\': release_val = 2 else: raise ValueError(\\"Unknown release level\\") serial = match.group(5) if serial is None: serial = 0 else: serial = int(serial) hex_value = (major << 24) | (minor << 16) | (micro << 8) | (release_val << 4) | serial return hex_value def hex_to_version(hex_version: int) -> str: Convert the given 32-bit hexadecimal value to a version string. major = (hex_version >> 24) & 0xFF minor = (hex_version >> 16) & 0xFF micro = (hex_version >> 8) & 0xFF release_val = (hex_version >> 4) & 0xF serial = hex_version & 0xF if release_val == 15: return f\\"{major}.{minor}.{micro}\\" else: release = \'a\' if release_val == 0 else \'b\' if release_val == 1 else \'rc\' return f\\"{major}.{minor}.{micro}{release}{serial}\\""},{"question":"# ABCs and Virtual Subclass Registration **Objective:** Demonstrate your understanding of the `abc` module in Python by creating an abstract base class and registering virtual subclasses. **Problem Statement:** You are tasked with creating a library to manage different types of media (e.g., books, movies, and music albums). Each media type should be represented by a class that adheres to a common interface defined by an abstract base class. 1. Define an abstract base class `MediaItem` that: - Inherits from `abc.ABC`. - Has an abstract method `play()` which when called, starts playing the media item. - Has an abstract property `title` which returns the title of the media item. 2. Define the following concrete classes: - `Book` class which: - Implements the `play()` method to print \\"Opening the book: [title]\\". - Has a `title` property. - `Movie` class which: - Implements the `play()` method to print \\"Playing the movie: [title]\\". - Has a `title` property. - `MusicAlbum` class which: - Implements the `play()` method to print \\"Playing the album: [title]\\". - Has a `title` property. 3. Register `Book`, `Movie`, and `MusicAlbum` as virtual subclasses of `MediaItem`. 4. Write a function `play_media(media_items)` that takes a list of media items and plays each item by calling the `play()` method. **Constraints:** - Use the `abc` module to enforce the abstract methods and properties. - Use the `register()` method to create virtual subclasses. - All titles should be strings. **Example Usage:** ```python from abc import ABC, abstractmethod class MediaItem(ABC): @abstractmethod def play(self): pass @property @abstractmethod def title(self): pass class Book: def __init__(self, title): self._title = title def play(self): print(f\\"Opening the book: {self.title}\\") @property def title(self): return self._title class Movie: def __init__(self, title): self._title = title def play(self): print(f\\"Playing the movie: {self.title}\\") @property def title(self): return self._title class MusicAlbum: def __init__(self, title): self._title = title def play(self): print(f\\"Playing the album: {self.title}\\") @property def title(self): return self._title MediaItem.register(Book) MediaItem.register(Movie) MediaItem.register(MusicAlbum) def play_media(media_items): for item in media_items: item.play() # Example book = Book(\\"The Great Gatsby\\") movie = Movie(\\"Inception\\") album = MusicAlbum(\\"Thriller\\") media_list = [book, movie, album] play_media(media_list) ``` **Expected Output:** ``` Opening the book: The Great Gatsby Playing the movie: Inception Playing the album: Thriller ``` **Submission:** Submit your solution as a Python script containing the class definitions and the `play_media` function.","solution":"from abc import ABC, abstractmethod class MediaItem(ABC): @abstractmethod def play(self): pass @property @abstractmethod def title(self): pass class Book: def __init__(self, title): self._title = title def play(self): print(f\\"Opening the book: {self.title}\\") @property def title(self): return self._title class Movie: def __init__(self, title): self._title = title def play(self): print(f\\"Playing the movie: {self.title}\\") @property def title(self): return self._title class MusicAlbum: def __init__(self, title): self._title = title def play(self): print(f\\"Playing the album: {self.title}\\") @property def title(self): return self._title # Register the concrete classes as virtual subclasses of MediaItem MediaItem.register(Book) MediaItem.register(Movie) MediaItem.register(MusicAlbum) def play_media(media_items): for item in media_items: item.play() # Example usage: # book = Book(\\"The Great Gatsby\\") # movie = Movie(\\"Inception\\") # album = MusicAlbum(\\"Thriller\\") # media_list = [book, movie, album] # play_media(media_list)"},{"question":"# Advanced File Handling and Exception Management using the `binhex` Module **Problem Statement:** You are required to implement a function `convert_files_to_binhex(input_files, output_dir)`, which converts multiple binary files to binhex format and writes them to a specified output directory. This function will also decode the binhex files back to their original binary format and verify their integrity by comparing the original and decoded files. Your implementation should leverage the `binhex` module functions and handle any exceptions properly. **Function Signature:** ```python def convert_files_to_binhex(input_files: List[str], output_dir: str) -> None: pass ``` **Input:** - `input_files` (List[str]): A list of filenames representing the binary files to be converted. - `output_dir` (str): The directory where the binhex files and decoded files will be stored. **Output:** - The function does not return anything. It should: - Convert each binary file in `input_files` to a binhex file and save it in `output_dir`. - Decode each binhex file back to its original binary format and save it in `output_dir`. - Check the integrity of the decoded files by comparing them to the original binary files (print a message indicating success or failure for each file). **Constraints:** - The input files are assumed to be valid and accessible. - The `output_dir` exists and is writable. **Performance Requirements:** - Handle up to 100 files efficiently. - Ensure the function runs within a reasonable time for moderate file sizes (up to 10MB each). **Example:** ```python input_files = [\\"file1.bin\\", \\"file2.bin\\", \\"file3.bin\\"] output_dir = \\"/path/to/output\\" convert_files_to_binhex(input_files, output_dir) ``` **Notes:** - The function must handle errors such as files that cannot be encoded or decoded due to invalid format. Use the `binhex.Error` exception where appropriate. - The function should print meaningful messages for each file conversion, decoding, and integrity check result. - Ensure that you clean up temporary files after use to avoid clutter. Implement this function to demonstrate an understanding of file handling, encoding/decoding, and exception management in Python.","solution":"import os import binhex from typing import List def convert_files_to_binhex(input_files: List[str], output_dir: str) -> None: Converts multiple binary files to binhex format and writes them to a specified output directory. Then decodes the binhex files back to their original format and verifies their integrity. for file_path in input_files: file_name = os.path.basename(file_path) binhex_path = os.path.join(output_dir, file_name + \'.hqx\') decoded_path = os.path.join(output_dir, \'decoded_\' + file_name) try: # Encode to binhex with open(file_path, \'rb\') as f: with open(binhex_path, \'wb\') as binhex_file: binhex.binhex(f, binhex_file) print(f\\"Encoded {file_path} to {binhex_path}\\") # Decode back to binary with open(binhex_path, \'rb\') as binhex_file: with open(decoded_path, \'wb\') as decoded_file: binhex.hexbin(binhex_file, decoded_file) print(f\\"Decoded {binhex_path} to {decoded_path}\\") # Verify file integrity with open(file_path, \'rb\') as original_file: with open(decoded_path, \'rb\') as check_file: if original_file.read() == check_file.read(): print(f\\"Integrity check passed for {file_path}\\") else: print(f\\"Integrity check failed for {file_path}\\") except binhex.Error as e: print(f\\"Binhex error with file {file_path}: {str(e)}\\") except Exception as e: print(f\\"Error processing file {file_path}: {str(e)}\\") # Clean up if os.path.exists(binhex_path): os.remove(binhex_path) if os.path.exists(decoded_path): os.remove(decoded_path) print(f\\"Cleaned up temporary files for {file_path}\\")"},{"question":"You are given a tensor of shape (4, 4) with random floating-point numbers. Your task is to perform a sequence of operations and understand the impact on the base tensor due to tensor views. Task 1. Create a 4x4 tensor `t` with random floating-point numbers. 2. Perform the following operations: - Create a view of `t` with shape (2, 8) and assign it to `b`. - Modify the first element of `b` to `3.14`. - Verify that the base tensor `t` reflects this change. 3. Transpose the base tensor `t` and assign it to `c`. Validate whether `c` is contiguous or not. 4. If `c` is not contiguous, convert it to a contiguous tensor. 5. Print the following: - The original tensor `t`. - The view tensor `b`. - The transposed tensor `c`. - Whether the transposed tensor `c` is contiguous before and after making it contiguous. Constraints - Use PyTorch functions to perform the mentioned operations. - Ensure your code is efficient and avoids unnecessary data copying. Input Format No input required. You need to create and manipulate tensors as described. Output Format Your code should print the following in order: 1. The original tensor `t` after modification. 2. The view tensor `b`. 3. The transposed tensor `c` before making it contiguous. 4. Whether the transposed tensor `c` is contiguous before making it contiguous. 5. The transposed tensor `c` after making it contiguous. 6. Whether the transposed tensor `c` is contiguous after making it contiguous. Example Output ```plaintext Original tensor t: tensor([[3.14, 0.21, 0.45, 0.92], [0.12, 0.33, 0.77, 0.55], [0.34, 0.62, 0.78, 0.91], [0.27, 0.87, 0.14, 0.65]]) View tensor b: tensor([[3.14, 0.21, 0.45, 0.92, 0.12, 0.33, 0.77, 0.55], [0.34, 0.62, 0.78, 0.91, 0.27, 0.87, 0.14, 0.65]]) Transposed tensor c: tensor([[3.14, 0.12, 0.34, 0.27], [0.21, 0.33, 0.62, 0.87], [0.45, 0.77, 0.78, 0.14], [0.92, 0.55, 0.91, 0.65]]) Is transposed tensor c contiguous? False Contiguous transposed tensor c: tensor([[3.14, 0.12, 0.34, 0.27], [0.21, 0.33, 0.62, 0.87], [0.45, 0.77, 0.78, 0.14], [0.92, 0.55, 0.91, 0.65]]) Is transposed tensor c contiguous after making it contiguous? True ``` Notes: - The random values in the output are for illustration purposes; your implementation may vary. - Ensure your implementation correctly prints the tensor values and their contiguity status.","solution":"import torch def tensor_operations(): # Step 1: Create a 4x4 tensor with random floating-point numbers t = torch.rand(4, 4) # Step 2: Create a view of `t` with shape (2, 8) and assign it to `b` b = t.view(2, 8) # Step 3: Modify the first element of `b` to `3.14` b[0, 0] = 3.14 # Verify that the base tensor `t` reflects this change assert t[0, 0] == 3.14 # Step 4: Transpose the base tensor `t` and assign it to `c` c = t.t() # Validate whether `c` is contiguous or not is_contiguous_before = c.is_contiguous() # If `c` is not contiguous, convert it to a contiguous tensor if not is_contiguous_before: c = c.contiguous() is_contiguous_after = c.is_contiguous() # Print statements print(\\"Original tensor t:n\\", t) print(\\"View tensor b:n\\", b) print(\\"Transposed tensor c:n\\", c) print(\\"Is transposed tensor c contiguous? \\", is_contiguous_before) print(\\"Is transposed tensor c contiguous after making it contiguous? \\", is_contiguous_after) tensor_operations()"},{"question":"**Problem Statement** You are tasked with creating a Python function to manage a schedule of events that span different time zones. You need to account for the varying offsets from UTC and daylight saving adjustments. Write a function `generate_event_summary(events, timezone_str)` that: 1. Takes a list of events and a timezone string as input. - Each event is a tuple (`event_name`, `start_datetime`, `end_datetime`), where: - `event_name` (str): The name of the event. - `start_datetime` (str): The start datetime of the event in ISO 8601 format (e.g., \\"2023-03-15T14:00:00Z\\"). - `end_datetime` (str): The end datetime of the event in ISO 8601 format (e.g., \\"2023-03-15T16:00:00Z\\"). 2. The timezone string is in the format of \\"UTC±HH:MM\\" (e.g., \\"UTC-04:00\\" for Eastern Daylight Time). 3. Converts all event start and end times to the given timezone. 4. Calculates the duration of each event in hours and minutes. 5. Returns a dictionary where keys are event names and values are dictionaries with the converted start and end times (in the local timezone) and event duration. **Constraints:** - You can assume the event\'s start will always be before the event\'s end. - The function should handle any valid datetime and timezone format as per the specifications. **Expected Input and Output:** ```python def generate_event_summary(events, timezone_str): # Your implementation here # Example usage: events = [ (\\"Meeting\\", \\"2023-03-15T14:00:00Z\\", \\"2023-03-15T15:30:00Z\\"), (\\"Conference\\", \\"2023-04-20T08:00:00Z\\", \\"2023-04-20T11:00:00Z\\") ] timezone_str = \\"UTC-04:00\\" print(generate_event_summary(events, timezone_str)) # Output: # {\'Meeting\': {\'start\': \'2023-03-15 10:00:00\', \'end\': \'2023-03-15 11:30:00\', \'duration\': \'1:30\'}, # \'Conference\': {\'start\': \'2023-04-20 04:00:00\', \'end\': \'2023-04-20 07:00:00\', \'duration\': \'3:0\'}} ``` **Notes:** - Pay attention to edge cases regarding daylight saving time transitions. - Ensure that the durations are formatted correctly, without rounding errors. - Utilize the `datetime` and `timezone` classes from the `datetime` module effectively.","solution":"from datetime import datetime, timedelta, timezone def convert_to_timezone(dt_str, tz_str): dt = datetime.fromisoformat(dt_str.replace(\\"Z\\", \\"+00:00\\")) sign = 1 if tz_str[3] == \'+\' else -1 hours_offset = int(tz_str[4:6]) minutes_offset = int(tz_str[7:9]) offset = timedelta(hours=hours_offset, minutes=minutes_offset) * sign return (dt + offset).replace(tzinfo=None) def calculate_duration(start_str, end_str): start_dt = datetime.fromisoformat(start_str.replace(\\"Z\\", \\"+00:00\\")) end_dt = datetime.fromisoformat(end_str.replace(\\"Z\\", \\"+00:00\\")) duration = end_dt - start_dt return str(duration) def generate_event_summary(events, timezone_str): summary = {} for event in events: event_name, start_datetime, end_datetime = event start_local = convert_to_timezone(start_datetime, timezone_str) end_local = convert_to_timezone(end_datetime, timezone_str) duration = calculate_duration(start_datetime, end_datetime) summary[event_name] = { \'start\': start_local.strftime(\'%Y-%m-%d %H:%M:%S\'), \'end\': end_local.strftime(\'%Y-%m-%d %H:%M:%S\'), \'duration\': duration[:-3] # remove seconds } return summary"},{"question":"# Question: Implement Custom Set Operations Using the provided Python set documentation as a guideline, implement the following functions to perform operations on sets. Each function should perform a specific task as detailed below. Function 1: `unique_elements` Write a function `unique_elements(lst)` to return a set containing unique elements from the input list `lst`. **Input:** - `lst`: A list of elements. **Output:** - A set containing the unique elements from `lst`. ```python def unique_elements(lst): pass ``` Function 2: `common_elements` Write a function `common_elements(set1, set2)` to return a set containing elements that are common to both `set1` and `set2`. **Input:** - `set1`: A set of elements. - `set2`: A set of elements. **Output:** - A set containing the elements that are in both `set1` and `set2`. ```python def common_elements(set1, set2): pass ``` Function 3: `difference_elements` Write a function `difference_elements(set1, set2)` to return a set containing elements that are in `set1` but not in `set2`. **Input:** - `set1`: A set of elements. - `set2`: A set of elements. **Output:** - A set containing the elements that are in `set1` but not in `set2`. ```python def difference_elements(set1, set2): pass ``` Function 4: `symmetric_difference_elements` Write a function `symmetric_difference_elements(set1, set2)` to return a set containing elements that are in either `set1` or `set2` but not in both. **Input:** - `set1`: A set of elements. - `set2`: A set of elements. **Output:** - A set containing the elements that are in either `set1` or `set2` but not in both. ```python def symmetric_difference_elements(set1, set2): pass ``` Constraints: - Inputs will always be valid; for the purpose of this question, it can be assumed they are always sets or lists. - No performance constraints are imposed, but aim for efficient implementations using built-in set operations where possible. Example usage: ```python print(unique_elements([1, 2, 2, 3, 4, 4, 5])) # Output: {1, 2, 3, 4, 5} print(common_elements({1, 2, 3}, {3, 4, 5})) # Output: {3} print(difference_elements({1, 2, 3}, {3, 4, 5})) # Output: {1, 2} print(symmetric_difference_elements({1, 2, 3}, {3, 4, 5})) # Output: {1, 2, 4, 5} ``` Provide implementations for all functions above, ensuring correct output based on the given sample inputs and outputs.","solution":"def unique_elements(lst): Returns a set containing unique elements from the input list `lst`. return set(lst) def common_elements(set1, set2): Returns a set containing elements that are common to both `set1` and `set2`. return set1 & set2 def difference_elements(set1, set2): Returns a set containing elements that are in `set1` but not in `set2`. return set1 - set2 def symmetric_difference_elements(set1, set2): Returns a set containing elements that are in either `set1` or `set2` but not in both. return set1 ^ set2"},{"question":"**Python Exceptions and Custom Error Handling** # Objective Create a Python function that processes a list of operations, handling multiple exceptions using both built-in and custom exceptions. # Description Write a function `process_operations(operations: List[Tuple[str, Any, Any]]) -> List[str]` that takes a list of tuples (`operations`). Each tuple contains: - An operation name (a string) which can be `\\"add\\"`, `\\"subtract\\"`, `\\"multiply\\"`, `\\"divide\\"`. - Two operands (values on which the operation will be performed). The function should return a list of results as strings. If an operation is successful, the result should be stored in the list as a string (e.g., `\\"3 + 4 = 7\\"`). If an exception occurs, the error message should be stored in the list. You must handle the following built-in exceptions explicitly: 1. `ZeroDivisionError` 2. `TypeError` 3. `ValueError` Additionally, define two custom exceptions: 1. `InvalidOperationError` - Raised when an operation is not one of the expected values (`\\"add\\"`, `\\"subtract\\"`, `\\"multiply\\"`, `\\"divide\\"`). 2. `NegativeResultError` - Raised when the result of a subtraction operation is negative. # Instructions 1. **Create Custom Exceptions:** - Define two custom exceptions: `InvalidOperationError` and `NegativeResultError`. 2. **Implement the `process_operations` Function:** - The function should handle the specified operations and return the results or error messages. - Use `try-except-else-finally` constructs to handle different exceptions. - Raise an `InvalidOperationError` if an operation name is invalid. - Raise a `NegativeResultError` if a subtraction operation results in a negative value. - Chain exceptions appropriately using `from` keyword if necessary. # Input - `operations` (List[Tuple[str, Any, Any]]): A list of tuples representing operations. # Output - List[str]: A list of results or error messages for each operation. # Constraints - Operands will be integers. - The list will contain at least one operation. # Example ```python class InvalidOperationError(Exception): pass class NegativeResultError(Exception): pass def process_operations(operations): results = [] for op, a, b in operations: try: if op == \\"add\\": result = a + b results.append(f\\"{a} + {b} = {result}\\") elif op == \\"subtract\\": result = a - b if result < 0: raise NegativeResultError(f\\"Result of {a} - {b} is negative\\") results.append(f\\"{a} - {b} = {result}\\") elif op == \\"multiply\\": result = a * b results.append(f\\"{a} * {b} = {result}\\") elif op == \\"divide\\": result = a / b results.append(f\\"{a} / {b} = {result}\\") else: raise InvalidOperationError(f\\"Invalid operation \'{op}\'\\") except ZeroDivisionError as e: results.append(f\\"Error: {e}\\") except TypeError as e: results.append(f\\"Error: {e}\\") except ValueError as e: results.append(f\\"Error: {e}\\") except InvalidOperationError as e: results.append(f\\"Error: {e}\\") except NegativeResultError as e: results.append(f\\"Error: {e}\\") return results # Example usage operations = [ (\\"add\\", 3, 4), (\\"subtract\\", 10, 5), (\\"multiply\\", 6, 7), (\\"divide\\", 8, 0), (\\"foo\\", 4, 2), # invalid operation (\\"subtract\\", 2, 5) # negative result ] print(process_operations(operations)) ``` # Expected Output ```python [ \\"3 + 4 = 7\\", \\"10 - 5 = 5\\", \\"6 * 7 = 42\\", \\"Error: division by zero\\", \\"Error: Invalid operation \'foo\'\\", \\"Error: Result of 2 - 5 is negative\\" ] ``` # Performance Requirements - The solution should handle lists of up to 10^5 operations efficiently.","solution":"class InvalidOperationError(Exception): pass class NegativeResultError(Exception): pass def process_operations(operations): results = [] for op, a, b in operations: try: if op == \\"add\\": result = a + b results.append(f\\"{a} + {b} = {result}\\") elif op == \\"subtract\\": result = a - b if result < 0: raise NegativeResultError(f\\"Result of {a} - {b} is negative\\") results.append(f\\"{a} - {b} = {result}\\") elif op == \\"multiply\\": result = a * b results.append(f\\"{a} * {b} = {result}\\") elif op == \\"divide\\": result = a / b results.append(f\\"{a} / {b} = {result}\\") else: raise InvalidOperationError(f\\"Invalid operation \'{op}\'\\") except ZeroDivisionError as e: results.append(f\\"Error: {e}\\") except InvalidOperationError as e: results.append(f\\"Error: {e}\\") except NegativeResultError as e: results.append(f\\"Error: {e}\\") return results # Example usage operations = [ (\\"add\\", 3, 4), (\\"subtract\\", 10, 5), (\\"multiply\\", 6, 7), (\\"divide\\", 8, 0), (\\"foo\\", 4, 2), # invalid operation (\\"subtract\\", 2, 5) # negative result ] print(process_operations(operations))"},{"question":"**Question: Implement a Local Cache System with Serialization and Database Backup** **Objective:** Create a Python class `LocalCache` that simulates a local in-memory cache which can serialize its content to disk using the `pickle` module and backup this cache to an SQLite database. This task will test your ability to work with both serialization and database operations. **Requirements:** 1. **Attributes:** - An internal dictionary `_cache` to hold the cache data. - A file path `pickle_file` to store the serialized content. - An SQLite database path `db_file` for backup. 2. **Methods:** - `__init__(self, pickle_file: str, db_file: str)`: Initializes the cache with the provided file paths. - `set_item(self, key: str, value: any)`: Inserts a key-value pair into the cache. - `get_item(self, key: str) -> any`: Retrieves the value for a given key from the cache. - `serialize_cache(self)`: Serializes the current cache to `pickle_file`. - `deserialize_cache(self)`: Loads the cache from `pickle_file`. - `backup_cache_to_db(self)`: Backs up the cache to the SQLite database specified by `db_file`. **Detailed Implementation:** 1. **Constructor (`__init__`)**: - Initialize the cache as an empty dictionary. - Store the file paths for the `pickle_file` and the `db_file`. 2. **Cache Operations (`set_item` and `get_item`)**: - `set_item` should add the key-value pair to the `_cache` dictionary. - `get_item` should return the value associated with the key, or `None` if the key is not found. 3. **Serialization (`serialize_cache` and `deserialize_cache`)**: - `serialize_cache`: Use the `pickle` module to serialize `_cache` and save it to `pickle_file`. - `deserialize_cache`: Load the data from `pickle_file` using `pickle` and update `_cache`. 4. **Database Backup (`backup_cache_to_db`)**: - Connect to the SQLite database specified by `db_file`. - Create a table `cache` if it doesn\'t exist with columns `key TEXT` and `value BLOB`. - Insert all the key-value pairs from `_cache` into the `cache` table. If the key already exists, update its value. **Constraints:** - Keys will be strings of maximum length 255. - Values can be of any serializable Python type. **Example Usage:** ```python cache = LocalCache(\'cache.pkl\', \'cache.db\') # Set some items in the cache cache.set_item(\'a\', 1) cache.set_item(\'b\', [2, 3, 4]) cache.set_item(\'c\', {\'x\': 10, \'y\': 20}) # Serialize cache to disk cache.serialize_cache() # Deserialize cache from disk cache.deserialize_cache() # Get an item from the cache print(cache.get_item(\'b\')) # Output: [2, 3, 4] # Backup cache to SQLite database cache.backup_cache_to_db() ``` This question assesses the ability to blend different Python concepts, including file I/O, serialization, and database interactions, providing a comprehensive test of a student\'s understanding of the provided documentation.","solution":"import pickle import sqlite3 class LocalCache: def __init__(self, pickle_file: str, db_file: str): self._cache = {} self.pickle_file = pickle_file self.db_file = db_file def set_item(self, key: str, value: any): self._cache[key] = value def get_item(self, key: str) -> any: return self._cache.get(key, None) def serialize_cache(self): with open(self.pickle_file, \'wb\') as f: pickle.dump(self._cache, f) def deserialize_cache(self): try: with open(self.pickle_file, \'rb\') as f: self._cache = pickle.load(f) except (FileNotFoundError, EOFError): self._cache = {} def backup_cache_to_db(self): conn = sqlite3.connect(self.db_file) cursor = conn.cursor() cursor.execute(\'\'\'CREATE TABLE IF NOT EXISTS cache (key TEXT PRIMARY KEY, value BLOB)\'\'\') for key, value in self._cache.items(): pickled_value = pickle.dumps(value) cursor.execute(\'\'\'INSERT OR REPLACE INTO cache (key, value) VALUES (?, ?)\'\'\', (key, pickled_value)) conn.commit() conn.close()"},{"question":"**Problem Statement:** You are provided with a custom neural network module in PyTorch. Your tasks are to: 1. Save and load state dictionaries of the module. 2. Preserve and reconstruct tensor view relationships while saving tensors. 3. Serialize the module using TorchScript and load it in a new script. # Instructions: 1. **Define Custom Neural Network Module**: - Create a custom neural network module with at least two layers. - Ensure that the module has trainable parameters. 2. **Saving and Loading State Dictionary**: - Instantiate the module and save its state dictionary using `torch.save`. - Create a new instance of the module and load the saved state dictionary using `torch.load` and `load_state_dict`. 3. **Preserving and Reconstructing Tensor Views**: - Create a tensor view of another tensor and demonstrate saving this view relationship. - Verify that the relationship is preserved after loading the tensor. 4. **TorchScript Serialization**: - Convert the custom module to TorchScript using `torch.jit.script`. - Save the scripted module using `torch.jit.save`. - Load the scripted module in a new script and demonstrate inference. # Expected Input and Output: - **Input**: None (all actions are part of the implemented functions) - **Output**: Print statements verifying the correctness of each task. # Constraints: 1. The neural network module should be defined with `torch.nn.Module`. 2. Use at least two types of layers (e.g., Linear, Conv2d). 3. Ensure that tensor view relationships are preserved and verifiable. 4. Demonstrate inference using the TorchScript serialized model. # Performance Requirements: - The solution should handle large tensors efficiently. - Ensure that state dictionaries and tensor views are correctly preserved during saving and loading operations. # Example: ```python import torch import torch.nn as nn # Task 1: Define Custom Neural Network Module class CustomNet(nn.Module): def __init__(self): super(CustomNet, self).__init__() self.layer1 = nn.Linear(10, 5) self.layer2 = nn.Linear(5, 1) def forward(self, x): x = torch.relu(self.layer1(x)) x = self.layer2(x) return x # Task 2: Saving and Loading State Dictionary model = CustomNet() torch.save(model.state_dict(), \'model_state.pt\') new_model = CustomNet() new_model.load_state_dict(torch.load(\'model_state.pt\')) # Task 3: Preserving and Reconstructing Tensor Views original_tensor = torch.arange(10) tensor_view = original_tensor[::2] torch.save((original_tensor, tensor_view), \'tensor_view.pt\') loaded_original_tensor, loaded_tensor_view = torch.load(\'tensor_view.pt\') assert torch.equal(loaded_original_tensor[::2], loaded_tensor_view) # Task 4: TorchScript Serialization scripted_model = torch.jit.script(CustomNet()) torch.jit.save(scripted_model, \'scripted_model.pt\') loaded_scripted_model = torch.jit.load(\'scripted_model.pt\') print(loaded_scripted_model(torch.randn(1, 10))) print(\\"All tasks executed successfully!\\") ``` Implement the functions according to the described specifications and verify the functionality through appropriate print statements.","solution":"import torch import torch.nn as nn import torch.jit # Task 1: Define Custom Neural Network Module class CustomNet(nn.Module): def __init__(self): super(CustomNet, self).__init__() self.layer1 = nn.Linear(10, 5) self.layer2 = nn.Linear(5, 1) def forward(self, x): x = torch.relu(self.layer1(x)) x = self.layer2(x) return x def save_state_dict(model, path): torch.save(model.state_dict(), path) def load_state_dict(model, path): model.load_state_dict(torch.load(path)) return model def preserve_and_reconstruct_tensor_views(): original_tensor = torch.arange(10) tensor_view = original_tensor[::2] torch.save((original_tensor, tensor_view), \'tensor_view.pt\') loaded_original_tensor, loaded_tensor_view = torch.load(\'tensor_view.pt\') return torch.equal(loaded_original_tensor[::2], loaded_tensor_view) def script_and_serialize_model(model, path): scripted_model = torch.jit.script(model) torch.jit.save(scripted_model, path) return scripted_model def load_script_and_run_inference(path, input_tensor): loaded_scripted_model = torch.jit.load(path) return loaded_scripted_model(input_tensor) # Instantiate model and save its state dictionary model = CustomNet() save_state_dict(model, \'model_state.pt\') # Create a new instance and load the saved state dictionary new_model = CustomNet() load_state_dict(new_model, \'model_state.pt\') # Preserve and reconstruct tensor views views_preserved = preserve_and_reconstruct_tensor_views() # Script and serialize model scripted_model = script_and_serialize_model(model, \'scripted_model.pt\') # Load scripted model and run inference inference_result = load_script_and_run_inference(\'scripted_model.pt\', torch.randn(1, 10)) print(views_preserved) # Should print True print(inference_result)"},{"question":"# Asynchronous File Server with Timeout Handling Objective Implement a simple asynchronous file server using Python 3.10 that leverages the `asyncio` library. Your file server should be able to handle multiple client connections simultaneously and serve files based on client requests. Implement timeout handling such that the server closes idle connections after a specified timeout period. Requirements 1. **File Server**: - The server should create a TCP connection and listen on a specified port. - When a client connects and requests a file, the server should send the content of the requested file. - If the file does not exist, the server should respond with an appropriate error message. 2. **Client Request Format**: - Clients will send requests in the format: `GET /path/to/filename`. - The server should only handle `GET` requests. 3. **Timeout Handling**: - Implement a mechanism to close connections that have been idle for more than 10 seconds. - If a client does not send any data within 10 seconds of connecting, the server should close the connection. 4. **Concurrency**: - The server should handle multiple clients concurrently using asyncio\'s event loop and task scheduling. Specifications - You are expected to use `asyncio` for handling asynchronous operations. - Use relevant asyncio methods for event loop, connection management, task scheduling, and timeout handling. - All file operations should be asynchronous. - Port number and other configurable parameters should be defined as constants at the beginning of your code. Input - The request from the client will be of the format: `GET /path/to/filename`. Output - The content of the requested file or an error message. - Close the connection after sending the response or if the timeout is reached. Sample Files and Directory Structure ``` /path/to/files ├── file1.txt ├── file2.txt └── file3.txt ``` Example Assume your server is running on port 8080. **Client Request**: ``` GET /path/to/files/file1.txt ``` **Server Response**: ``` Content of file1.txt ``` If the file does not exist: ``` Error: File not found. ``` If there is no activity from the client for 10 seconds, close the connection. **Coding Task** ```python import asyncio PORT = 8080 # Port number for server TIMEOUT = 10 # Timeout duration in seconds async def handle_client(reader, writer): try: data = await asyncio.wait_for(reader.read(100), TIMEOUT) message = data.decode().strip() if not message: return command, filepath = message.split(\' \', 1) if command != \'GET\': response = \'Error: Unsupported command.n\' else: try: with open(filepath, \'rb\') as f: content = f.read() response = content.decode() except FileNotFoundError: response = \'Error: File not found.n\' writer.write(response.encode()) await writer.drain() except asyncio.TimeoutError: # No data received in TIMEOUT seconds, closing connection pass finally: writer.close() await writer.wait_closed() async def main(): server = await asyncio.start_server(handle_client, \'127.0.0.1\', PORT) async with server: await server.serve_forever() if __name__ == \\"__main__\\": try: asyncio.run(main()) except KeyboardInterrupt: print(\\"Server stopped manually.\\") ``` Constraints - Only handle `GET` requests. - Set appropriate timeouts for read operations to manage idle connections.","solution":"import asyncio PORT = 8080 # Port number for server TIMEOUT = 10 # Timeout duration in seconds async def handle_client(reader, writer): try: data = await asyncio.wait_for(reader.read(100), TIMEOUT) message = data.decode().strip() if not message: return command, filepath = message.split(\' \', 1) if command != \'GET\': response = \'Error: Unsupported command.n\' else: try: with open(filepath, \'rb\') as f: content = f.read() response = content.decode() except FileNotFoundError: response = \'Error: File not found.n\' writer.write(response.encode()) await writer.drain() except asyncio.TimeoutError: # No data received in TIMEOUT seconds, closing connection pass finally: writer.close() await writer.wait_closed() async def main(): server = await asyncio.start_server(handle_client, \'127.0.0.1\', PORT) async with server: await server.serve_forever() if __name__ == \\"__main__\\": try: asyncio.run(main()) except KeyboardInterrupt: print(\\"Server stopped manually.\\")"},{"question":"**Question: Implement a Secure Password Input Function Using `termios`** You are required to implement a Python function that secures password input by temporarily disabling the display of characters as they are typed (no echoing). Your function should work on Unix-based systems by utilizing the `termios` module. Additionally, your function should clean up properly (restore original terminal settings) even if an exception occurs. # Function Signature ```python def secure_password_input(prompt: str = \\"Password: \\") -> str: ``` # Input - `prompt`: A string that will be shown to the user when asking for the password. # Output - Return the password entered by the user as a string. # Requirements 1. Disable echoing of characters for the password input. 2. Ensure that the original terminal settings are restored after input, even if an error occurs. 3. Use the `termios` module to manipulate terminal settings. # Example ```python password = secure_password_input(\\"Enter your password: \\") print(f\\"Your password is: {password}\\") ``` *When the function `secure_password_input` is called, the user should be able to type their password without the characters being shown on the screen. After the password is entered, it is returned as a string.* # Constraints - The function should work only on Unix-based systems. If the system is not Unix-based, the function should raise an `OSError` with a message indicating that the operation is not supported. - You should handle any exceptions that might be raised during the execution to ensure that the terminal settings are reverted back to their original state. # Notes - You may assume that the `termios` module is available and the Python interpreter is running on a Unix-based system. - The function should be tested in an interactive environment to observe the expected behavior. # Advanced Considerations (Optional) - If you have access to a Unix-based system, test the function in a real terminal environment. - Consider extending the function to handle other terminal controls like suspending input/output using the relevant `termios` functions.","solution":"import sys import termios import tty def secure_password_input(prompt: str = \\"Password: \\") -> str: Securely prompts a user for a password, disabling the display of typed characters. if not sys.stdin.isatty(): raise OSError(\\"This function only works in a Unix-based terminal environment.\\") print(prompt, end=\'\', flush=True) fd = sys.stdin.fileno() old_settings = termios.tcgetattr(fd) password = \'\' try: tty.setraw(fd) while True: char = sys.stdin.read(1) if char == \'n\' or char == \'r\': break password += char finally: termios.tcsetattr(fd, termios.TCSADRAIN, old_settings) print() # Move to the next line after input return password"},{"question":"# XML Parsing and Transformation with SAX Handlers Objective: Implement custom SAX handlers using the `xml.sax.handler` module to parse an XML document and perform specific transformations based on provided requirements. Problem Statement: You are provided with an XML document containing information about various books in a library. Your task is to implement custom handlers to parse this XML document and perform specific tasks during the parsing process. XML Document Structure: ```xml <library> <book> <title>Python Programming</title> <author>John Smith</author> <year>2020</year> <genre>Programming</genre> </book> <book> <title>Data Science 101</title> <author>Jane Doe</author> <year>2019</year> <genre>Data Science</genre> </book> <!-- Additional book entries --> </library> ``` Requirements: 1. **ContentHandler Implementation**: - Implement a custom `ContentHandler` to parse the XML document. - Print the title, author, and year of each book. - Collect the titles of all books and store them in a list. 2. **ErrorHandler Implementation**: - Implement a custom `ErrorHandler` to handle and print warnings, errors, and fatal errors. 3. **EntityResolver Implementation**: - Implement a custom `EntityResolver` to handle any external entity references (for example, external DTDs). For this exercise, assume that all external entities should be resolved to `None`. 4. **Transformation Requirement**: - After parsing, transform the collected titles into a JSON array and print it. Input: - The XML content to be parsed (as shown above). Output: - Printed details of each book (title, author, year). - A printed JSON array containing the titles of all books. Constraints: - Assume the XML document is well-formed. - Handle any parsing errors gracefully using the custom `ErrorHandler`. Performance Requirements: - The solution should efficiently parse the XML and handle large documents if needed. Example: Using the given XML structure, the expected output would be: ``` Book Title: Python Programming Author: John Smith Year: 2020 Book Title: Data Science 101 Author: Jane Doe Year: 2019 JSON Titles Array: [\\"Python Programming\\", \\"Data Science 101\\"] ``` Starter Code: ```python import xml.sax import json class CustomContentHandler(xml.sax.handler.ContentHandler): def __init__(self): self.current_element = \\"\\" self.book_titles = [] self.current_title = \\"\\" self.current_author = \\"\\" self.current_year = \\"\\" def startElement(self, name, attrs): self.current_element = name def endElement(self, name): if name == \\"book\\": print(f\\"Book Title: {self.current_title}\\") print(f\\"Author: {self.current_author}\\") print(f\\"Year: {self.current_year}n\\") self.book_titles.append(self.current_title) self.current_element = \\"\\" def characters(self, content): if self.current_element == \\"title\\": self.current_title = content elif self.current_element == \\"author\\": self.current_author = content elif self.current_element == \\"year\\": self.current_year = content class CustomErrorHandler(xml.sax.handler.ErrorHandler): def error(self, exception): print(f\\"Error: {exception}\\") def fatalError(self, exception): print(f\\"Fatal Error: {exception}\\") def warning(self, exception): print(f\\"Warning: {exception}\\") class CustomEntityResolver(xml.sax.handler.EntityResolver): def resolveEntity(self, publicId, systemId): return None # Main entry point if __name__ == \\"__main__\\": # Example XML content xml_content = <library> <book> <title>Python Programming</title> <author>John Smith</author> <year>2020</year> <genre>Programming</genre> </book> <book> <title>Data Science 101</title> <author>Jane Doe</author> <year>2019</year> <genre>Data Science</genre> </book> </library> parser = xml.sax.make_parser() content_handler = CustomContentHandler() error_handler = CustomErrorHandler() entity_resolver = CustomEntityResolver() parser.setContentHandler(content_handler) parser.setErrorHandler(error_handler) parser.setEntityResolver(entity_resolver) xml.sax.parseString(xml_content, content_handler) # Output JSON array of titles titles_json = json.dumps(content_handler.book_titles) print(f\\"JSON Titles Array: {titles_json}\\") ``` Instructions: 1. Implement the `CustomContentHandler`, `CustomErrorHandler`, and `CustomEntityResolver` classes based on the given requirements. 2. Parse the provided XML content using these handlers. 3. Print the details of each book and the JSON array of collected titles as demonstrated in the example.","solution":"import xml.sax import json class CustomContentHandler(xml.sax.handler.ContentHandler): def __init__(self): self.current_element = \\"\\" self.book_titles = [] self.current_title = \\"\\" self.current_author = \\"\\" self.current_year = \\"\\" def startElement(self, name, attrs): self.current_element = name def endElement(self, name): if name == \\"book\\": print(f\\"Book Title: {self.current_title}\\") print(f\\"Author: {self.current_author}\\") print(f\\"Year: {self.current_year}n\\") self.book_titles.append(self.current_title) self.current_element = \\"\\" def characters(self, content): content = content.strip() if self.current_element == \\"title\\": self.current_title = content elif self.current_element == \\"author\\": self.current_author = content elif self.current_element == \\"year\\": self.current_year = content class CustomErrorHandler(xml.sax.handler.ErrorHandler): def error(self, exception): print(f\\"Error: {exception}\\") def fatalError(self, exception): print(f\\"Fatal Error: {exception}\\") def warning(self, exception): print(f\\"Warning: {exception}\\") class CustomEntityResolver(xml.sax.handler.EntityResolver): def resolveEntity(self, publicId, systemId): return None def parse_and_print_titles(xml_content): parser = xml.sax.make_parser() content_handler = CustomContentHandler() error_handler = CustomErrorHandler() entity_resolver = CustomEntityResolver() parser.setContentHandler(content_handler) parser.setErrorHandler(error_handler) parser.setEntityResolver(entity_resolver) xml.sax.parseString(xml_content, content_handler) titles_json = json.dumps(content_handler.book_titles) print(f\\"JSON Titles Array: {titles_json}\\") return content_handler.book_titles"},{"question":"# Advanced Python Function Implementation: Custom Iterable and Exception Handling **Objective:** Write a Python class that demonstrates custom iterables with generator methods, coupled with robust exception handling. **Problem Statement:** You need to implement a class `PrimeNumbers` that generates prime numbers up to a specified value `n`. Use a generator method within the class for creating an iterable of prime numbers. Additionally, handle exceptions that might be raised when non-integer values are provided for `n` or if `n` is not a positive integer. **Class and Method Specification:** 1. **Class Definition:** - `PrimeNumbers` 2. **Attributes:** - `n`: An integer value up to which prime numbers are to be generated. 3. **Methods:** - `__init__(self, n)`: Constructor method to initialize the value of `n`. - Raises a `ValueError` if `n` is not an integer or if `n` is less than 2. - `__iter__(self)`: Returns the iterator object. - `_is_prime(self, num)`: Private method to check if a number is prime. - `generate_primes(self)`: Generator method to yield primes up to `n`. **Constraints:** - Valid values for `n` are integers greater than or equal to 2. - The method should raise and appropriately handle exceptions for invalid inputs. - Performance considerations: The algorithm should efficiently determine prime numbers. **Expected Input:** - An integer `n` **Expected Output:** - An iterable of prime numbers up to `n`. Example: ```python primes = PrimeNumbers(10) for prime in primes: print(prime) ``` Expected Output: ``` 2 3 5 7 ``` **Implementation Tips:** - You may use the `all()` function to check for prime numbers. - Be sure to include error handling and validation within the `__init__` method. - Utilize the generator method to yield prime numbers one by one to optimize memory usage. **Testing:** - Include test cases to demonstrate the exception handling by passing invalid values for `n`. - Verify correctness with a few values for `n`. ```python # Example test case checks try: primes = PrimeNumbers(\\"NotAnInteger\\") # Should raise ValueError except ValueError as e: print(e) try: primes = PrimeNumbers(1) # Should raise ValueError except ValueError as e: print(e) primes = PrimeNumbers(10) print(list(primes)) # Should output [2, 3, 5, 7] ```","solution":"class PrimeNumbers: def __init__(self, n): if not isinstance(n, int): raise ValueError(\\"The value of n must be an integer.\\") if n < 2: raise ValueError(\\"The value of n must be greater than or equal to 2.\\") self.n = n def __iter__(self): return self.generate_primes() def _is_prime(self, num): if num < 2: return False for i in range(2, int(num ** 0.5) + 1): if num % i == 0: return False return True def generate_primes(self): for num in range(2, self.n + 1): if self._is_prime(num): yield num"},{"question":"**Question:** Write a Python function `synchronize_directories` that synchronizes the contents of a source directory (`src_dir`) with a target directory (`dst_dir`). Your function should ensure that: 1. All files and subdirectories within `src_dir` are copied to `dst_dir`. 2. If a file or directory in `dst_dir` does not exist in `src_dir`, it should be deleted. 3. The most recent versions of the files should be kept, meaning if a file in `src_dir` is newer than the corresponding file in `dst_dir`, it should be copied over. You must use the `shutil` module to perform these operations. **Function Signature**: ```python def synchronize_directories(src_dir: str, dst_dir: str) -> None: pass ``` **Input**: - `src_dir`: A string representing the path to the source directory. - `dst_dir`: A string representing the path to the target directory. **Output**: - The function does not return anything. It performs in-place synchronization of the target directory with the source directory. **Constraints**: - Both `src_dir` and `dst_dir` will be valid directories with read and write permissions. - You can assume there are no symbolic links. **Example**: ```python import os import shutil # Assuming the directories are set up as follows: # src_dir/ # ├── file1.txt (modified recently) # ├── file2.txt # └── subdir1/ # └── file3.txt # dst_dir/ # ├── file1.txt (older version) # ├── subdir1/ # │ └── file4.txt (this should be deleted) # └── subdir2/ (this should be deleted) src_dir = \'path/to/src_dir\' dst_dir = \'path/to/dst_dir\' synchronize_directories(src_dir, dst_dir) # After synchronization, dst_dir should have: # dst_dir/ # ├── file1.txt (copied from src_dir) # ├── file2.txt (copied from src_dir) # └── subdir1/ # └── file3.txt (copied from src_dir) ``` **Notes**: - Use `shutil.copy2()` for copying files to preserve metadata. - Use `shutil.copytree()` for copying directories. - Use `shutil.rmtree()` for deleting directories. - Use `shutil.move()` function to handle renaming and moving directories. Make sure to handle potential exceptions that may arise during file operations and provide meaningful error messages.","solution":"import os import shutil from filecmp import cmp def synchronize_directories(src_dir: str, dst_dir: str) -> None: src_contents = set(os.listdir(src_dir)) dst_contents = set(os.listdir(dst_dir)) # Synchronize contents for name in src_contents: src_path = os.path.join(src_dir, name) dst_path = os.path.join(dst_dir, name) if os.path.isdir(src_path): if not os.path.exists(dst_path): shutil.copytree(src_path, dst_path) else: synchronize_directories(src_path, dst_path) else: if not os.path.exists(dst_path) or not cmp(src_path, dst_path, shallow=False): shutil.copy2(src_path, dst_path) # Remove files/directories that are in dst but not in src for name in dst_contents - src_contents: dst_path = os.path.join(dst_dir, name) if os.path.isdir(dst_path): shutil.rmtree(dst_path) else: os.remove(dst_path)"},{"question":"**Coding Assessment Question** You are tasked with creating a utility for converting binary files to a custom hexadecimal format and decoding them back. You cannot use the built-in `binhex` module directly, but you will design your own version of a simplified binhex encoder/decoder. # Part 1: Encoding a Binary File Function: custom_binhex_encode(input_file, output_file) **Input:** - `input_file`: A string representing the path to the binary input file. - `output_file`: A string representing the path to the output file where the encoded hex data should be saved. **Output:** - The function should not return anything but write the hex-encoded data to `output_file`. Requirements: - Read the binary data from the `input_file`. - Convert the binary data to a hexadecimal string. - Save the hexadecimal string to the `output_file`. - The output_hex string should be in the same format as provided by the `hexlify` function from the `binascii` module. # Part 2: Decoding a Binhex File Function: custom_binhex_decode(input_file, output_file) **Input:** - `input_file`: A string representing the path to the binhex input file. - `output_file`: A string representing the path to the binary output file where the decoded data should be saved. **Output:** - The function should not return anything but write the decoded binary data to `output_file`. Requirements: - Read the hex data from the `input_file`. - Convert the hex data back to binary data. - Save the binary data to the `output_file`. # Example Usage ```python # Encode a binary file to hex custom_binhex_encode(\'example.bin\', \'encoded.hex\') # Decode the hex file back to binary custom_binhex_decode(\'encoded.hex\', \'decoded.bin\') ``` # Constraints: - The `input_file` and `output_file` parameters provided to both functions will always be valid file paths. - You cannot use the direct methods provided by the deprecated `binhex` module for this task. - Assume the size of the file will fit into the available memory for simple handling. # Bonus: - Implement exception handling to catch and raise appropriate errors if encoding or decoding fails. - Ensure that the encoded and then decoded file matches the original binary file (i.e., the process should be lossless).","solution":"def custom_binhex_encode(input_file, output_file): Reads binary data from the `input_file`, converts it to hexadecimal string, and writes it to `output_file`. try: with open(input_file, \'rb\') as f: binary_data = f.read() hex_data = binary_data.hex() with open(output_file, \'w\') as f: f.write(hex_data) except Exception as e: raise RuntimeError(f\\"Failed to encode file: {e}\\") def custom_binhex_decode(input_file, output_file): Reads hexadecimal data from the `input_file`, converts it to binary data, and writes it to `output_file`. try: with open(input_file, \'r\') as f: hex_data = f.read() binary_data = bytes.fromhex(hex_data) with open(output_file, \'wb\') as f: f.write(binary_data) except Exception as e: raise RuntimeError(f\\"Failed to decode file: {e}\\")"},{"question":"# Python Coding Assessment Question Objective: Implement file compression and decompression using the `bz2` module in Python. This question tests your ability to utilize different aspects of the `bz2` module to handle file operations for compression and decompression. Problem Statement: You are provided with a text file named `input.txt`. Your task is to: 1. Compress the contents of `input.txt` and write it to a new file named `output.bz2`. 2. Decompress the contents of `output.bz2` and verify that the decompressed data matches the original data from `input.txt`. Function Signatures: You need to implement two functions as specified below: 1. **compress_file(input_filename: str, output_filename: str, compresslevel: int = 9) -> None** - **Input:** - `input_filename (str)`: The name of the input file to be compressed. - `output_filename (str)`: The name of the output file to write the compressed data. - `compresslevel (int)`: The level of compression, an integer between 1 (least compression) and 9 (most compression). Default is 9. - **Output:** None. The function writes the compressed data to `output_filename`. 2. **decompress_file(input_filename: str, output_filename: str) -> bool** - **Input:** - `input_filename (str)`: The name of the compressed input file. - `output_filename (str)`: The name of the output file to write the decompressed data. - **Output:** Returns `True` if the decompressed data matches the original data, otherwise `False`. Constraints: - The `input.txt` file is a text file and contains no more than 1MB of data. - You are not allowed to use any external libraries other than `bz2`. Example: Assume `input.txt` contains the following text: ``` Hello, this is a sample text for testing compression and decompression using the bz2 module. ``` a) Compress the file: ```python compress_file(\'input.txt\', \'output.bz2\') ``` b) Decompress the file and verify: ```python result = decompress_file(\'output.bz2\', \'decompressed.txt\') print(result) # Should print: True ``` Hints: - Use the `bz2.open` function to handle file reading and writing in compressed formats. - Read the original file data in bytes mode for accurate comparison after decompression.","solution":"import bz2 def compress_file(input_filename: str, output_filename: str, compresslevel: int = 9) -> None: with open(input_filename, \'rb\') as input_file: data = input_file.read() with bz2.open(output_filename, \'wb\', compresslevel=compresslevel) as output_file: output_file.write(data) def decompress_file(input_filename: str, output_filename: str) -> bool: with bz2.open(input_filename, \'rb\') as input_file: decompressed_data = input_file.read() with open(output_filename, \'wb\') as output_file: output_file.write(decompressed_data) with open(output_filename, \'rb\') as output_file: resulted_data = output_file.read() with open(input_filename.replace(\'output.bz2\', \'input.txt\'), \'rb\') as original_file: original_data = original_file.read() return original_data == resulted_data"},{"question":"XML Manipulation Challenge You are tasked with processing an XML document that contains information about a collection of books. Each book entry has the following structure: ```xml <books> <book> <title>Book Title</title> <author>Author Name</author> <year>Publication Year</year> <price>Book Price</price> <category>Book Category</category> </book> <!-- More book elements --> </books> ``` Your task is to implement a function `process_books(xml_string: str) -> str` that performs the following operations: 1. Parse the input XML string. 2. Find all book entries whose price is above a certain threshold (e.g., 30). 3. Modify the titles of these books to include \\"(Expensive)\\" at the end. 4. Add a new attribute `expensive=\\"true\\"` to these book entries. 5. Create a new XML string from the modified document and return it. Function Signature ```python def process_books(xml_string: str) -> str: pass ``` Input - `xml_string`: A string containing the XML document with book details. Output - Returns a string representing the modified XML document. Constraints - The price will always be a valid integer. - The XML string will be well-formed. - Assume there will be at least one book element. Example ```python xml_input = \'\'\'<books> <book> <title>Learning Python</title> <author>Mark Lutz</author> <year>2013</year> <price>40</price> <category>Programming</category> </book> <book> <title>Automate the Boring Stuff with Python</title> <author>Al Sweigart</author> <year>2015</year> <price>25</price> <category>Programming</category> </book> </books>\'\'\' modified_xml = process_books(xml_input) # The result should be: # <books> # <book expensive=\\"true\\"> # <title>Learning Python (Expensive)</title> # <author>Mark Lutz</author> # <year>2013</year> # <price>40</price> # <category>Programming</category> # </book> # <book> # <title>Automate the Boring Stuff with Python</title> # <author>Al Sweigart</author> # <year>2015</year> # <price>25</price> # <category>Programming</category> # </book> # </books> ``` Tips - Make use of `xml.etree.ElementTree` for parsing and modifying the XML. - Use methods like `.find()`, `.findall()`, `.set()`, and others relevant to XML element manipulation. - Thoroughly test your implementation with various XML input cases to ensure correctness.","solution":"import xml.etree.ElementTree as ET def process_books(xml_string: str) -> str: Processes the given XML string by adding \\"(Expensive)\\" to titles of books with a price above 30, and adding an `expensive=\\"true\\"` attribute to those books. Args: xml_string (str): A string containing the XML document with book details. Returns: str: A string representing the modified XML document. root = ET.fromstring(xml_string) for book in root.findall(\'book\'): price = int(book.find(\'price\').text) if price > 30: title_element = book.find(\'title\') title_element.text += \\" (Expensive)\\" book.set(\'expensive\', \'true\') return ET.tostring(root, encoding=\'unicode\')"},{"question":"# Question: You are required to create a multipart email message that includes text, an image, and an attachment. Write a Python function `create_multipart_email` that constructs and returns the MIME email object with the following specifications: 1. **Text Content**: The email should include a plain text and an HTML version of the body content. 2. **Image Content**: Attach an image (e.g., `example.jpg`) to the email. 3. **File Attachment**: Attach a generic file (e.g., `example.pdf`) to the email. 4. **Proper Headers**: Ensure that the email headers (e.g., `Content-Type`, `MIME-Version`) are properly set. 5. **Encoding**: Use base64 encoding for the image and file attachments. The function should have the following signature: ```python from email.mime.multipart import MIMEMultipart from email.mime.text import MIMEText from email.mime.image import MIMEImage from email.mime.application import MIMEApplication def create_multipart_email(text_plain: str, text_html: str, image_path: str, file_path: str) -> MIMEMultipart: pass ``` Input: - `text_plain (str)`: The plain text content of the email. - `text_html (str)`: The HTML content of the email. - `image_path (str)`: The file path to the image to be attached. - `file_path (str)`: The file path to the generic file to be attached. Output: - `MIMEMultipart`: The constructed MIME email object. # Constraints: - Assume the necessary files (`example.jpg` and `example.pdf`) exist in the given file paths. - Use the default `compat32` policy for all MIME objects. - Properly handle any exceptions that may occur during file operations. # Example Usage: ```python email_object = create_multipart_email( text_plain=\\"This is a plain text body\\", text_html=\\"<html><body><h1>This is an HTML body</h1></body></html>\\", image_path=\\"example.jpg\\", file_path=\\"example.pdf\\" ) # The email_object should now be ready to be sent using an SMTP server. ``` # Notes: - You should use the classes from the `email.mime` module as described in the provided documentation. - Remember to add the appropriate headers for each part of the email as needed. - You may use the `open` function to read the image and file content in binary mode.","solution":"from email.mime.multipart import MIMEMultipart from email.mime.text import MIMEText from email.mime.image import MIMEImage from email.mime.application import MIMEApplication import base64 def create_multipart_email(text_plain: str, text_html: str, image_path: str, file_path: str) -> MIMEMultipart: # Create the root message msg = MIMEMultipart(\'mixed\') msg[\'Subject\'] = \'Multipart Email\' msg[\'From\'] = \'you@example.com\' msg[\'To\'] = \'someone@example.com\' # Create multipart/alternative part alt_part = MIMEMultipart(\'alternative\') # Attach the plain text and HTML parts alt_part.attach(MIMEText(text_plain, \'plain\')) alt_part.attach(MIMEText(text_html, \'html\')) # Attach the alternative part to the main message msg.attach(alt_part) # Attach image with open(image_path, \'rb\') as img_file: img = MIMEImage(img_file.read()) img.add_header(\'Content-Disposition\', \'attachment\', filename=image_path) msg.attach(img) # Attach file with open(file_path, \'rb\') as f: file_part = MIMEApplication(f.read()) file_part.add_header(\'Content-Disposition\', \'attachment\', filename=file_path) msg.attach(file_part) return msg"},{"question":"Task You are required to implement a function that simulates processing a list of URLs by downloading their content in parallel using `ThreadPoolExecutor` from the `concurrent.futures` module. # Function Signature ```python import requests from concurrent.futures import ThreadPoolExecutor, as_completed def download_urls(url_list: list, max_workers: int) -> dict: Downloads content from each URL in the given list using parallel threads. Parameters: url_list (list): A list of URLs to download. max_workers (int): Maximum number of threads to use. Returns: dict: A dictionary where keys are URLs and values are their respective HTML content as strings. pass ``` # Input - `url_list (list)`: List of string URLs to download. - `max_workers (int)`: Maximum number of threads to use for parallel downloading. # Output - Returns a dictionary where: - Keys are the URLs. - Values are their respective HTML content as strings. # Constraints - Number of URLs (`len(url_list)`) will not exceed 1000. - The `max_workers` will be a positive integer not greater than 20. - The function should handle possible exceptions during downloading (e.g., network errors) and skip those URLs, ensuring the function execution continues for other URLs. # Example ```python url_list = [ \\"https://www.example.com\\", \\"https://www.python.org\\", \\"https://www.invalidurl.com\\" ] max_workers = 5 result = download_urls(url_list, max_workers) # Possible output (content shortened for brevity): { \\"https://www.example.com\\": \\"<html>...</html>\\", \\"https://www.python.org\\": \\"<html>...</html>\\", \\"https://www.invalidurl.com\\": None # due to invalid URL or failure } ``` # Additional Information You are allowed to use the `requests` library to perform the HTTP requests. Ensure that your function handles download failures gracefully by logging errors and continuing to process other URLs. **Tips:** - Utilize the `ThreadPoolExecutor` from `concurrent.futures` to manage the threads. - Use the `as_completed` function to handle obtaining results from futures as they complete. - Implement proper exception handling to manage any download failures and log them.","solution":"import requests from concurrent.futures import ThreadPoolExecutor, as_completed def download_urls(url_list: list, max_workers: int) -> dict: Downloads content from each URL in the given list using parallel threads. Parameters: url_list (list): A list of URLs to download. max_workers (int): Maximum number of threads to use. Returns: dict: A dictionary where keys are URLs and values are their respective HTML content as strings. def fetch(url): try: response = requests.get(url) response.raise_for_status() return url, response.text except requests.RequestException: return url, None result = {} with ThreadPoolExecutor(max_workers=max_workers) as executor: futures = {executor.submit(fetch, url): url for url in url_list} for future in as_completed(futures): url, content = future.result() result[url] = content return result"},{"question":"# Advanced Coding Assessment Background: You are given a configuration file and a CSV data file. Your task is to read the configuration settings, process the CSV data accordingly, and optionally (for extra credit) generate a corresponding `.plist` file that includes some summary information. Configuration File: The configuration file is in `.ini` format and specifies: 1. **CSV file location**. 2. **CSV dialect specifications** (like delimiter, quote character, etc.). 3. **Columns to be read from the CSV file**. CSV File: The CSV file contains multiple columns of data. The exact columns to read and their format are described in the configuration file. Requirements: 1. **Read the Configuration File**: - Parse the configuration file to get the CSV file location and CSV dialect specifications. - Extract the list of columns to be processed from the CSV file. 2. **Process the CSV File**: - Using the configuration settings, read the CSV file. - Only read the specified columns. - Collect and store the data from the specified columns. 3. **Optional - Generate `.plist` File**: - Create a summary `.plist` file that includes the number of rows processed and a list of the distinct values from each specified column. Input: 1. **Configuration File**: An `.ini` file with sections like: ```ini [DEFAULT] csv_file_location = /path/to/data.csv delimiter = , quotechar = \\" [COLUMNS] columns_to_read = column1, column2, column3 ``` 2. **CSV File**: A CSV file located at the specified path with multiple columns. Output: 1. **Processed Data**: Print the processed data from the specified columns. 2. **Optional**: Create a `.plist` file containing summary information. Constraints: - Ensure the solution can handle large CSV files efficiently. - Handle any possible exceptions related to file reading and parsing. Example: Given a `config.ini` file: ```ini [DEFAULT] csv_file_location = /path/to/sample.csv delimiter = , quotechar = \\" [COLUMNS] columns_to_read = Name, Age, Occupation ``` And a corresponding `sample.csv` file: ```csv Name, Age, Occupation, Country John Doe, 30, Engineer, USA Jane Smith, 25, Doctor, UK ... ``` The output should be: ``` Processed Data: [ {\\"Name\\": \\"John Doe\\", \\"Age\\": 30, \\"Occupation\\": \\"Engineer\\"}, {\\"Name\\": \\"Jane Smith\\", \\"Age\\": 25, \\"Occupation\\": \\"Doctor\\"} ] ``` If generating a `.plist` file, include summary like: ```xml <?xml version=\\"1.0\\" encoding=\\"UTF-8\\"?> <!DOCTYPE plist PUBLIC \\"-//Apple//DTD PLIST 1.0//EN\\" \\"http://www.apple.com/DTDs/PropertyList-1.0.dtd\\"> <plist version=\\"1.0\\"> <dict> <key>RowsProcessed</key> <integer>2</integer> <key>DistinctValues</key> <dict> <key>Name</key> <array> <string>John Doe</string> <string>Jane Smith</string> </array> <key>Age</key> <array> <integer>30</integer> <integer>25</integer> </array> <key>Occupation</key> <array> <string>Engineer</string> <string>Doctor</string> </array> </dict> </dict> </plist> ``` Submission: - Provide a Python script that performs the above tasks. - Include comments to explain the implementation and logic.","solution":"import configparser import csv from collections import defaultdict def read_config(config_file): Reads the configuration file to get CSV file location, CSV dialect, and columns to read. config = configparser.ConfigParser() config.read(config_file) csv_file_location = config[\'DEFAULT\'].get(\'csv_file_location\') delimiter = config[\'DEFAULT\'].get(\'delimiter\', \',\') quotechar = config[\'DEFAULT\'].get(\'quotechar\', \'\\"\') columns_to_read = [column.strip() for column in config[\'COLUMNS\'].get(\'columns_to_read\').split(\',\')] return csv_file_location, delimiter, quotechar, columns_to_read def process_csv(csv_file_location, delimiter, quotechar, columns_to_read): Processes the CSV file based on the configuration settings. processed_data = [] with open(csv_file_location, \'r\') as csvfile: reader = csv.DictReader(csvfile, delimiter=delimiter, quotechar=quotechar) for row in reader: processed_row = {col: row[col] for col in columns_to_read} processed_data.append(processed_row) return processed_data def main(config_file): csv_file_location, delimiter, quotechar, columns_to_read = read_config(config_file) processed_data = process_csv(csv_file_location, delimiter, quotechar, columns_to_read) print(\\"Processed Data:\\") for entry in processed_data: print(entry) return processed_data"},{"question":"# Base64 URL Safe Encoding and Decoding You have been tasked with creating a secure system for encoding and decoding URL-safe data using the Base64 encoding scheme. Your system must ensure data remains intact when transmitted over URLs and must be capable of both encoding and decoding data. **Requirements**: - Implement a function `url_safe_encode(data: bytes) -> bytes` that encodes the given byte data using Base64 with URL- and filesystem-safe alphabet. - Use `base64.urlsafe_b64encode` for encoding. - Implement a function `url_safe_decode(encoded_data: bytes) -> bytes` that decodes the given URL-safe Base64 encoded byte data back to its original form. - Use `base64.urlsafe_b64decode` for decoding. Both functions should handle any necessary padding and edge cases where encoding and decoding might fail. # Function Details: 1. `url_safe_encode(data: bytes) -> bytes`: - **Input**: A `bytes` object containing the original data to be encoded. - **Output**: A `bytes` object containing the URL-safe Base64 encoded data. 2. `url_safe_decode(encoded_data: bytes) -> bytes`: - **Input**: A `bytes` object containing URL-safe Base64 encoded data. - **Output**: A `bytes` object containing the original decoded data. # Constraints: - The input data for the encoding function can be of any length. - The encoded data for the decoding function is assumed to be valid URL-safe Base64 encoded bytes. # Example: ```python import base64 # Example usage of url_safe_encode and url_safe_decode functions data = b\'https://example.com?user=test_user&data=1234\' encoded_data = url_safe_encode(data) print(encoded_data) # Output might be a URL-safe Base64 encoded bytes decoded_data = url_safe_decode(encoded_data) print(decoded_data) # Output should be the original data: b\'https://example.com?user=test_user&data=1234\' ``` You must implement both `url_safe_encode` and `url_safe_decode` functions using the `base64` module. # Implementation: ```python import base64 def url_safe_encode(data: bytes) -> bytes: # Your implementation here pass def url_safe_decode(encoded_data: bytes) -> bytes: # Your implementation here pass ``` **Note**: Be sure to handle edge cases and ensure your functions are robust enough for different kinds of input data and encoded data.","solution":"import base64 def url_safe_encode(data: bytes) -> bytes: Encodes the given byte data using Base64 with URL- and filesystem-safe alphabet. Args: data (bytes): The original data to be encoded. Returns: bytes: URL-safe Base64 encoded data. return base64.urlsafe_b64encode(data) def url_safe_decode(encoded_data: bytes) -> bytes: Decodes the given URL-safe Base64 encoded byte data back to its original form. Args: encoded_data (bytes): URL-safe Base64 encoded data. Returns: bytes: The original decoded data. return base64.urlsafe_b64decode(encoded_data)"},{"question":"Coding Assessment Question # Background The `pty` module in Python provides functionalities to handle pseudo-terminals, allowing users to programmatically control terminal interactions. This can be particularly useful for creating terminal-based applications or automating terminal tasks. # Task You are required to implement a Python function `record_terminal_session(commands: List[str], output_file: str) -> int` that uses the `pty.spawn` function to execute a list of shell commands in a pseudo-terminal and records the entire session in a specified output file. # Function Signature ```python from typing import List def record_terminal_session(commands: List[str], output_file: str) -> int: pass ``` # Input - `commands`: A list of strings, where each string is a shell command to be executed. - `output_file`: A string representing the path to the file where the terminal session should be recorded. # Output - The function returns an integer representing the exit status of the final command executed. # Constraints 1. The commands should be executed sequentially in the same terminal session. 2. The terminal session\'s input and output, including any intermediate prompts or errors, should be recorded in the `output_file`. 3. If an empty list of commands is provided, the function should return `0`. # Example Usage ```python status = record_terminal_session([\\"echo \'Hello, World!\'\\", \\"ls -l\\"], \\"session.log\\") print(status) # Output: 0 if successful ``` # Notes - You may use the `pty.spawn` function to handle the pseudo-terminal interactions. - You are encouraged to use appropriate error handling mechanisms to ensure the function behaves correctly in various scenarios. # Additional Information - You can use the `os.read` and `os.write` functions for reading from and writing to file descriptors connected to the pseudo-terminal. - Consider the possibility of the commands producing large outputs, and handle such cases efficiently. # Hints - You can define a callback function for `master_read` to read from the pseudo-terminal and write to the output file. - Ensure you manage the file descriptors correctly to avoid resource leaks. Here is a basic structure to get you started: ```python import os import pty def record_terminal_session(commands: List[str], output_file: str) -> int: def read(fd): data = os.read(fd, 1024) with open(output_file, \'ab\') as f: f.write(data) return data shell = os.environ.get(\'SHELL\', \'sh\') command = \' ; \'.join(commands) pid, fd = pty.fork() if pid == 0: # Child process os.execv(shell, [shell, \'-c\', command]) else: # Parent process _, status = pty.spawn(shell, read) return status ``` In this task, you are expected to fill in the missing parts of the function and ensure it meets all the requirements.","solution":"import os import pty from typing import List def record_terminal_session(commands: List[str], output_file: str) -> int: if not commands: return 0 def read(fd): data = os.read(fd, 1024) with open(output_file, \'ab\') as f: f.write(data) return data final_status = 0 shell = os.environ.get(\'SHELL\', \'/bin/sh\') for command in commands: pid, fd = pty.fork() if pid == 0: # Child process os.execvp(shell, [shell, \'-c\', command]) else: # Parent process try: while True: data = read(fd) if not data: break except OSError: pass # Expected when the child process ends _, status = os.waitpid(pid, 0) final_status = os.WEXITSTATUS(status) return final_status"},{"question":"# Event Scheduler Task You are tasked to implement a custom event scheduler using Python\'s `sched` module. Your implementation should cater to the given constraints and meet precise functional requirements. Requirements: 1. **Initialization**: - Initialize an event scheduler utilizing Python’s built-in `time.time` and `time.sleep` functions. 2. **Scheduling Events**: - Create a method `schedule_event(time_or_delay, priority, action, *args, **kwargs, absolute_time=False)`: - If `absolute_time` is `True`, the event should be scheduled at a specific absolute time. - If `absolute_time` is `False`, the event should be scheduled to execute after a certain delay. - The `priority`, `action`, `args`, and `kwargs` parameters should be used to define the event to be scheduled. - Return the event object created by the scheduler. 3. **Cancel Events**: - Implement a method `cancel_event(event)` which cancels an event if it is currently in the queue. 4. **Running Events**: - Implement a method `run_scheduler(blocking=True)` which runs all the scheduled events, obeying their priorities and constraints. 5. **Custom Scheduler Time Check**: - Create a function `current_scheduler_time()`, which returns the current time used by the scheduler for scheduling events. This should leverage the `timefunc` passed during the scheduler\'s initialization. 6. **Error Handling**: - Your methods should handle potential errors gracefully, such as attempting to cancel an event that does not exist. Input & Output: - **Initialization**: ```python scheduler = CustomScheduler() ``` - **Scheduling an Event with Absolute Time**: ```python event = scheduler.schedule_event(1652342835.3694863, 1, print, \\"Hello, World!\\", absolute_time=True) ``` - **Scheduling an Event with Delay**: ```python event = scheduler.schedule_event(5, 2, print, \\"Hello, after 5 seconds\\") ``` - **Canceling an Event**: ```python scheduler.cancel_event(event) ``` - **Running the Scheduler**: ```python scheduler.run_scheduler(blocking=False) ``` - **Current Scheduler Time**: ```python current_time = scheduler.current_scheduler_time() # Should return the current scheduler time ``` Constraints: - Use Python 3.10. - Ensure thread-safety in multi-threaded environments. - Handle scheduling for concurrent events with proper priority management. Example: ```python import time from sched import scheduler class CustomScheduler: def __init__(self): self.scheduler = scheduler(time.time, time.sleep) def schedule_event(self, time_or_delay, priority, action, *args, **kwargs, absolute_time=False): if absolute_time: event = self.scheduler.enterabs(time_or_delay, priority, action, args, kwargs) else: event = self.scheduler.enter(time_or_delay, priority, action, args, kwargs) return event def cancel_event(self, event): self.scheduler.cancel(event) def run_scheduler(self, blocking=True): self.scheduler.run(blocking) def current_scheduler_time(self): return self.scheduler.timefunc() # Example usage def greeting(msg): print(f\\"Greeting: {msg}\\") scheduler = CustomScheduler() event1 = scheduler.schedule_event(5, 1, greeting, \\"Hello after 5 seconds\\") event2 = scheduler.schedule_event(time.time() + 10, 1, greeting, \\"Hello at specific time\\", absolute_time=True) time.sleep(6) scheduler.run_scheduler() ``` **Note**: Customize the `greeting` function or any action function to test with different scenarios as needed.","solution":"import time from sched import scheduler class CustomScheduler: def __init__(self): self.scheduler = scheduler(time.time, time.sleep) def schedule_event(self, time_or_delay, priority, action, *args, absolute_time=False, **kwargs): if absolute_time: event = self.scheduler.enterabs(time_or_delay, priority, action, argument=args, kwargs=kwargs) else: event = self.scheduler.enter(time_or_delay, priority, action, argument=args, kwargs=kwargs) return event def cancel_event(self, event): try: self.scheduler.cancel(event) except ValueError: print(\\"Error: Event cannot be cancelled because it does not exist.\\") def run_scheduler(self, blocking=True): self.scheduler.run(blocking) def current_scheduler_time(self): return self.scheduler.timefunc()"},{"question":"**Question: Implement a Custom Command-Line History Manager** You are required to implement a custom command-line history manager using the `readline` module. Your task is to create a script that handles command-line input history, saves it to a file on exit, and restores it on the next run. Additionally, you should implement functionality to retrieve, modify, and display specific entries from the history. # Requirements: 1. **Initialize History:** - Read history from a file named `\\".custom_history\\"` in the user\'s home directory on startup. If the file does not exist, create it. - Set a maximum history length of 500 entries. 2. **Add Entries:** - Automatically add each line of input to the history. 3. **Retrieve and Modify Entries:** - Implement a function `get_history_entry(index)` that retrieves a specific entry from the history list. - Implement a function `replace_history_entry(index, new_line)` that replaces a specific entry in the history list with a new line. - Implement a function `remove_history_entry(index)` that removes a specific entry from the history list. 4. **Display Entries:** - Implement a function `display_history()` that prints all entries in the history list. 5. **Save History:** - On program exit, save the history to the `\\".custom_history\\"` file, truncating if necessary to maintain a maximum of 500 entries. # Constraints: - Use the `readline` module to achieve the functionality. - Ensure compatibility with the default configuration of the `readline` library on the platform it\'s being run. - The solution must handle typical edge cases, such as out-of-bounds indices for history modification functions. # Example: ```python import readline import atexit import os HISTORY_FILE = os.path.expanduser(\\"~/.custom_history\\") HISTORY_LENGTH = 500 class CommandLineHistoryManager: def __init__(self): self.init_history() def init_history(self): if hasattr(readline, \\"read_history_file\\"): try: readline.read_history_file(HISTORY_FILE) except FileNotFoundError: open(HISTORY_FILE, \'wb\').close() readline.set_history_length(HISTORY_LENGTH) atexit.register(self.save_history) def save_history(self): readline.set_history_length(HISTORY_LENGTH) readline.write_history_file(HISTORY_FILE) def get_history_entry(self, index): if 0 < index <= readline.get_current_history_length(): return readline.get_history_item(index) return None def replace_history_entry(self, index, new_line): if 0 <= index < readline.get_current_history_length(): readline.replace_history_item(index, new_line) def remove_history_entry(self, index): if 0 <= index < readline.get_current_history_length(): readline.remove_history_item(index) def display_history(self): for i in range(1, readline.get_current_history_length() + 1): print(f\\"{i}: {readline.get_history_item(i)}\\") # Instantiate and test the manager history_manager = CommandLineHistoryManager() history_manager.display_history() ``` This question will assess your ability to use the `readline` module to manage command-line history effectively. Implement each function as specified and ensure the program behaves correctly during typical use cases.","solution":"import readline import atexit import os HISTORY_FILE = os.path.expanduser(\\"~/.custom_history\\") HISTORY_LENGTH = 500 class CommandLineHistoryManager: def __init__(self): self.init_history() def init_history(self): if hasattr(readline, \\"read_history_file\\"): try: readline.read_history_file(HISTORY_FILE) except FileNotFoundError: open(HISTORY_FILE, \'wb\').close() readline.set_history_length(HISTORY_LENGTH) atexit.register(self.save_history) def save_history(self): readline.set_history_length(HISTORY_LENGTH) readline.write_history_file(HISTORY_FILE) def get_history_entry(self, index): if 0 < index <= readline.get_current_history_length(): return readline.get_history_item(index) return None def replace_history_entry(self, index, new_line): if 0 < index <= readline.get_current_history_length(): readline.remove_history_item(index - 1) # Remove old entry readline.add_history(new_line) # Add new entry def remove_history_entry(self, index): if 0 < index <= readline.get_current_history_length(): readline.remove_history_item(index - 1) def display_history(self): for i in range(1, readline.get_current_history_length() + 1): print(f\\"{i}: {readline.get_history_item(i)}\\") # Instantiate and use the manager history_manager = CommandLineHistoryManager()"},{"question":"Problem Statement You are required to design a Python function that works with bytearray objects. This function should demonstrate your understanding of creating and modifying bytearrays using the provided functions and macros from the `python310` package. Function Signature ```python def process_bytearray(input_object: object, concat_string: str, resize_to: int) -> bytearray: ``` Input 1. `input_object (object)`: An object that implements the buffer protocol. It will be converted to a bytearray. 2. `concat_string (str)`: A string that will be converted to a bytearray and concatenated to `input_object`. 3. `resize_to (int)`: The size to which the concatenated bytearray should be resized. Output - Returns a resized bytearray after creating from `input_object` and concatenating with the bytearray from `concat_string`. Constraints - You must use `PyByteArray_FromObject`, `PyByteArray_FromStringAndSize`, `PyByteArray_Concat`, and `PyByteArray_Resize` functions directly for creating and modifying the bytearrays. - Assume all inputs are valid and no need for extensive input validation. - The `resize_to` value will be a positive integer and smaller or equal to the total length of concatenated bytearrays. Example ```python input_object = b\'x00x01x02\' concat_string = \'456\' resize_to = 4 # Expected output: bytearray(b\'x00x01x024\') ``` Implementation Requirements - Convert the `input_object` to a bytearray using `PyByteArray_FromObject`. - Convert the `concat_string` to a bytearray using `PyByteArray_FromStringAndSize`. - Concatenate the two bytearrays using `PyByteArray_Concat`. - Resize the resulting bytearray to `resize_to` using `PyByteArray_Resize`. - Return the final modified bytearray. Note: If `PyByteArray_FromObject` function fails for `input_object`, it should raise an appropriate exception. You can assume that all the necessary functions have been made available through appropriate C API bindings in Python.","solution":"def process_bytearray(input_object: object, concat_string: str, resize_to: int) -> bytearray: Converts the input object to a bytearray, concatenates it with the bytearray of the concat_string, and resizes the resulting bytearray to resize_to. # Convert input_object to bytearray input_bytearray = bytearray(input_object) # Convert concat_string to bytearray concat_bytearray = bytearray(concat_string, \'utf-8\') # Concatenate the two bytearrays combined_bytearray = input_bytearray + concat_bytearray # Resize the combined bytearray resized_bytearray = combined_bytearray[:resize_to] return resized_bytearray"},{"question":"**Question: Complex User Access Control System** Implement a Python function that takes a list of user access operations and returns a summary of the access control status for each user. The function should demonstrate usage of various Python control structures, including conditionals (`if`, `elif`, `else`), loops (`for` or `while`), `match` statements, and function definitions with default arguments, keyword arguments, and arbitrary argument lists. # Function Signature ```python def access_control(operations: list, *, access_level={\'admin\': 3, \'user\': 2, \'guest\': 1}) -> dict: Perform user access control based on a list of operations. Parameters: operations (list): A list of operations, where each operation is a tuple of the form (username, action, [role]). \'action\' can be \'login\', \'logout\', or \'modify\'. \'role\' is optional and only relevant for \'modify\' action. access_level (dict, optional): A dictionary defining access levels for different roles. Default is {\'admin\': 3, \'user\': 2, \'guest\': 1}. Returns: dict: A dictionary summarizing the access status for each user. Each entry is of the form \'username\': \'status\', where status is \'active\' or \'inactive\'. ``` # Input - `operations`: A list of tuples. Each tuple contains: - `username` (str): The name of the user. - `action` (str): The action to perform (\'login\', \'logout\', \'modify\'). - `role` (str, optional): The role to assign during a \'modify\' action. Should be one of the keys in `access_level`. - `access_level`: A dictionary defining access levels for different roles. Default is `{\'admin\': 3, \'user\': 2, \'guest\': 1}`. # Output - A dictionary summarizing the access status for each user. Each entry is of the form `username`: `status`, where status is \'active\' or \'inactive\'. # Constraints - Users can only modify their roles if they are already \'active\'. - Users can only \'login\' if they are not already \'active\'. - Users can only \'logout\' if they are already \'active\'. # Example ```python operations = [ (\'alice\', \'login\'), (\'bob\', \'login\'), (\'alice\', \'modify\', \'admin\'), (\'bob\', \'modify\', \'user\'), (\'alice\', \'logout\'), (\'bob\', \'logout\') ] output = access_control(operations) print(output) # Expected: {\'alice\': \'inactive\', \'bob\': \'inactive\'} ``` # Explanation - Alice logs in and becomes \'active\'. - Bob logs in and becomes \'active\'. - Alice\'s role is modified to \'admin\'. - Bob\'s role is modified to \'user\'. - Alice logs out and becomes \'inactive\'. - Bob logs out and becomes \'inactive\'. # Notes 1. Use control structures (`if`, `elif`, `else`), loops (`for` or `while`), and `match` statements effectively. 2. Define the function with the specified signature and make use of default arguments, keyword arguments, and arbitrary argument lists where appropriate. 3. Ensure to update and maintain the access status of users accurately based on the operations provided.","solution":"def access_control(operations: list, *, access_level={\'admin\': 3, \'user\': 2, \'guest\': 1}) -> dict: Perform user access control based on a list of operations. Parameters: operations (list): A list of operations, where each operation is a tuple of the form (username, action, [role]). \'action\' can be \'login\', \'logout\', or \'modify\'. \'role\' is optional and only relevant for \'modify\' action. access_level (dict, optional): A dictionary defining access levels for different roles. Default is {\'admin\': 3, \'user\': 2, \'guest\': 1}. Returns: dict: A dictionary summarizing the access status for each user. Each entry is of the form \'username\': \'status\', where status is \'active\' or \'inactive\'. user_status = {} for operation in operations: username = operation[0] action = operation[1] if action == \'login\': if user_status.get(username, \'inactive\') == \'inactive\': user_status[username] = \'active\' elif action == \'logout\': if user_status.get(username, \'inactive\') == \'active\': user_status[username] = \'inactive\' elif action == \'modify\': if len(operation) == 3: role = operation[2] if user_status.get(username, \'inactive\') == \'active\' and role in access_level: # here could be an optional assignment of roles, but it would be irrelevant for the provided task, # as the end summary focuses solely on \'active\'/\'inactive\' pass return user_status"},{"question":"# Advanced Python Constants Handling You are required to implement a function called `constant_handler`. This function will receive a list of mixed types, including potential appearances of Python\'s built-in constants (`False`, `True`, `None`, `NotImplemented`, `Ellipsis`). Your function should: 1. **Filter** and **count** occurrences of each constant. 2. **Return** a dictionary with the counts of each constant found in the input list. # Function Signature ```python def constant_handler(input_list: list) -> dict: pass ``` # Input - A single argument: `input_list`, which is a list containing mixed data types (e.g., integers, strings, booleans, `None`, `NotImplemented`, `Ellipsis`, etc.) # Output - A dictionary with keys as the string names of the constants (\'False\', \'True\', \'None\', \'NotImplemented\', \'Ellipsis\') and values as the counts of these constants found in the input list. - If a particular constant is not found, it should still appear in the dictionary with a count of `0`. # Example ```python input_list = [True, False, None, NotImplemented, Ellipsis, 123, \\"string\\", True, None] output = constant_handler(input_list) print(output) # Expected output: # {\'False\': 1, \'True\': 2, \'None\': 2, \'NotImplemented\': 1, \'Ellipsis\': 1} ``` # Constraints - The function should handle lists containing up to 1000 elements. - The elements can be of any data type. # Note - Remember that the constants `None`, `True`, `False`, `NotImplemented`, and `Ellipsis` cannot be reassigned. - Make sure your function is efficient and handles edge cases such as an empty list or a list without any constants. # Your Task Implement the function `constant_handler` to meet the above requirements.","solution":"def constant_handler(input_list: list) -> dict: constants_count = { \'False\': 0, \'True\': 0, \'None\': 0, \'NotImplemented\': 0, \'Ellipsis\': 0 } for item in input_list: if item is False: constants_count[\'False\'] += 1 if item is True: constants_count[\'True\'] += 1 if item is None: constants_count[\'None\'] += 1 if item is NotImplemented: constants_count[\'NotImplemented\'] += 1 if item is Ellipsis: constants_count[\'Ellipsis\'] += 1 return constants_count"},{"question":"**Problem Statement:** You are tasked with developing a web crawler that must adhere to the rules specified in \\"robots.txt\\" files of different websites. Your task is to implement a function, `can_crawl(urls, useragent)`, that takes a list of URLs and a user agent, and determines which URLs can be fetched by that user agent based on the \\"robots.txt\\" files for those URLs. **Function Signature:** ```python def can_crawl(urls: list, useragent: str) -> list: ``` **Input:** - `urls` (list): A list of URLs (strings) that the web crawler intends to fetch. - `useragent` (str): The user agent string that the web crawler uses. **Output:** - `result` (list): A list of tuples, each containing a URL from the input list and a boolean indicating whether that URL can be fetched by the user agent. **Constraints:** 1. You may assume that the input URLs are properly formatted. 2. The URLs belong to domains that have a \\"robots.txt\\" file. 3. The URLs might belong to different domains, so you will need to handle fetching and parsing of multiple \\"robots.txt\\" files. 4. You need to use the `urllib.robotparser.RobotFileParser` class and its methods for this implementation. **Performance Requirements:** - Your solution should efficiently handle a list containing up to 100 URLs. **Example Usage:** ```python urls = [ \\"http://www.example.com/page1\\", \\"http://www.example.com/page2\\", \\"http://www.other.com/home\\", \\"http://www.other.com/about\\" ] useragent = \\"MyWebCrawler\\" result = can_crawl(urls, useragent) print(result) # Output could look like: # [ # (\\"http://www.example.com/page1\\", True), # (\\"http://www.example.com/page2\\", False), # (\\"http://www.other.com/home\\", True), # (\\"http://www.other.com/about\\", True) # ] ``` **Guidelines:** 1. For each URL, determine the base domain and fetch the corresponding \\"robots.txt\\" file. 2. Use the `RobotFileParser` class to parse the \\"robots.txt\\" file. 3. Use the `can_fetch` method to check if the user agent is allowed to fetch the URL. 4. Return the list of tuples as specified. **Hints:** - The `urlparse` module from Python\'s standard library can help in extracting the base domain from a URL. - Proper handling of HTTP response codes and errors will be necessary when fetching \\"robots.txt\\" files.","solution":"import urllib.robotparser from urllib.parse import urlparse def can_crawl(urls, useragent): def get_base_url(url): parsed_url = urlparse(url) return f\\"{parsed_url.scheme}://{parsed_url.netloc}\\" robot_files = {} results = [] for url in urls: base_url = get_base_url(url) if base_url not in robot_files: rp = urllib.robotparser.RobotFileParser() rp.set_url(f\\"{base_url}/robots.txt\\") rp.read() robot_files[base_url] = rp else: rp = robot_files[base_url] can_fetch = rp.can_fetch(useragent, url) results.append((url, can_fetch)) return results"},{"question":"**Question: Handling Missing Values in a Dataset Using Pandas** Your task is to implement a function that takes in a dataset (in the form of a pandas DataFrame) and performs a series of operations to manage missing values. Function Signature: ```python def handle_missing_values(df: pd.DataFrame) -> pd.DataFrame: pass ``` # Input: - `df`: A pandas DataFrame containing various columns with potentially different data types, including both numerical and categorical data. The DataFrame may contain missing values represented by `NaN` or `NaT`. # Output: - A pandas DataFrame where the missing values have been processed as described below. # Instructions: 1. Detect all columns with missing values and print their names. 2. For numerical columns, fill missing values using the mean of that column. 3. For categorical columns, fill missing values using the mode (the most frequent value) of that column. 4. For datetime columns, fill missing values with the minimum datetime value of that column. 5. After filling missing values, return the processed DataFrame. # Constraints: - You may assume that the DataFrame is not empty. - You need to handle datetime, numerical, and categorical columns appropriately as per the steps described. - The solution should ensure that the DataFrame retains the same structure and column names as the input DataFrame. # Example: ```python import pandas as pd import numpy as np # Sample DataFrame data = { \'A\': [1, 2, np.nan, 4, 5], \'B\': [\'a\', \'b\', np.nan, \'b\', \'a\'], \'C\': [pd.Timestamp(\'20230101\'), pd.NaT, pd.Timestamp(\'20230103\'), pd.Timestamp(\'20230104\'), pd.Timestamp(\'20230105\')], } df = pd.DataFrame(data) # Call the function result_df = handle_missing_values(df) # Expected Output # Columns with missing values: A, B, C # result_df should be: # A B C # 0 1.0 a 2023-01-01 # 1 2.0 b 2023-01-01 # 2 3.0 b 2023-01-03 # 3 4.0 b 2023-01-04 # 4 5.0 a 2023-01-05 ``` Make sure to test your solution thoroughly.","solution":"import pandas as pd import numpy as np def handle_missing_values(df: pd.DataFrame) -> pd.DataFrame: Handles missing values in a DataFrame as follows: - For numerical columns, fills missing values with the mean of the column. - For categorical columns, fills missing values with the mode of the column. - For datetime columns, fills missing values with the minimum datetime value of the column. Prints the names of columns that had missing values. :param df: pandas DataFrame containing various columns with potentially different data types. :return: pandas DataFrame with missing values handled. missing_columns = df.columns[df.isnull().any()].tolist() if missing_columns: print(f\\"Columns with missing values: {\', \'.join(missing_columns)}\\") for column in df.columns: if df[column].isnull().any(): if pd.api.types.is_numeric_dtype(df[column]): df[column] = df[column].fillna(df[column].mean()) elif pd.api.types.is_categorical_dtype(df[column]) or pd.api.types.is_object_dtype(df[column]): df[column] = df[column].fillna(df[column].mode()[0]) elif pd.api.types.is_datetime64_any_dtype(df[column]): df[column] = df[column].fillna(df[column].min()) return df"},{"question":"**Objective:** Create a Python program that reads a file containing student names and their scores, processes this data to calculate the average score and formats the output to display the names, scores, and average in a well-aligned tabular format. Use both f-string and \\"str.format()\\" methods to format the output. **Problem Statement:** You are provided with a text file named `students.txt` with the following format: ``` Alice: 85 Bob: 78 Charlie: 92 David: 88 Eve: 91 ``` Your task is to read this file, calculate the average score, and print a table showing each student\'s name, their score, and the difference of their score from the average. The output should be formatted properly using Python\'s formatted string literals (f-strings) and the `str.format()` method for practice. **Constraints:** - Assume every line in the file is correctly formatted as shown above. - The file contains at least one student record. **Requirements:** 1. **Function to Read File**: Implement a function `read_student_scores(filename)` that reads the file and returns a dictionary with student names as keys and their scores as values. 2. **Function to Calculate Average**: Implement a function `calculate_average(scores)` that takes a dictionary of scores and returns the average score. 3. **Function to Print Formatted Table**: - Implement a function `print_table(scores, average)` that prints: - Each student\'s name, their score, and the difference from the average score in an aligned format using f-strings. - Additionally, add a summary line using `str.format()` method showing the average score. **Input and Output:** - **Input**: None (the function reads input from a file). - **Output**: ``` Student Score Difference from Average Alice 85 -2.00 Bob 78 -9.00 Charlie 92 5.00 David 88 1.00 Eve 91 4.00 Average Score: 86.80 ``` **Performance Requirements:** - Handle file IO operations efficiently ensuring files are closed after reading. - Format the output appropriately for readability and maintainability. **Example Code Signature:** ```python def read_student_scores(filename: str) -> dict: # Your code to read from file here. def calculate_average(scores: dict) -> float: # Your code to calculate average here. def print_table(scores: dict, average: float): # Your code to print formatted table here. if __name__ == \\"__main__\\": filename = \'students.txt\' scores = read_student_scores(filename) average = calculate_average(scores) print_table(scores, average) ``` **Note:** Ensure to include error handling for file operations, such as file not found scenarios, while providing clear and relevant error messages.","solution":"def read_student_scores(filename: str) -> dict: Reads scores from the given file and returns a dictionary with student names as keys and their scores as values. scores = {} try: with open(filename, \'r\') as file: for line in file: name, score = line.strip().split(\': \') scores[name] = int(score) except FileNotFoundError: print(f\\"Error: The file {filename} was not found.\\") except Exception as e: print(f\\"An error occurred: {e}\\") return scores def calculate_average(scores: dict) -> float: Calculates and returns the average score from the scores dictionary. if not scores: return 0 total = sum(scores.values()) average = total / len(scores) return average def print_table(scores: dict, average: float): Prints the student scores in a formatted table showing the difference from the average score. # Header print(f\\"{\'Student\':<10} {\'Score\':<6} {\'Difference from Average\':<20}\\") # Content using f-string for name, score in scores.items(): difference = score - average print(f\\"{name:<10} {score:<6} {difference:<20.2f}\\") # Summary line using str.format() print(\\"nAverage Score: {:.2f}\\".format(average)) if __name__ == \\"__main__\\": filename = \'students.txt\' scores = read_student_scores(filename) average = calculate_average(scores) print_table(scores, average)"},{"question":"# Advanced Pandas MultiIndex Manipulation **Objective**: Demonstrate your understanding of creating and manipulating `MultiIndex` objects in pandas, including advanced slicing and indexing techniques. **Instructions**: 1. Create a `DataFrame` using `MultiIndex` for both its rows and columns. 2. Perform a series of slicing and indexing operations to extract and manipulate data. **Function Signature**: ```python def multiindex_operations(): pass ``` # Steps to Follow: 1. **Create a MultiIndex DataFrame**: - Create a `MultiIndex` for the rows using `from_product` method with the following iterables: `[\'A1\', \'A2\']` and `[\'B1\', \'B2\']`. - Create a `MultiIndex` for the columns using `from_tuples` method with the following tuples: `[(\'X1\', \'Y1\'), (\'X1\', \'Y2\'), (\'X2\', \'Y1\'), (\'X2\', \'Y2\')]`. - Generate a DataFrame with random numbers using these MultiIndexes for rows and columns. 2. **Indexing and Slicing Operations**: - Extract all rows where the first level is \'A1\'. - Extract all columns where the second level is \'Y1\'. - Extract the value at the intersection of row (\'A2\', \'B1\') and column (\'X1\', \'Y1\'). - Set all values in rows where the second level is \'B2\' to 0. 3. **Return the modified DataFrame**. **Constraints and Requirements**: - You may use only pandas functionalities. - Your code should be efficient and make use of advanced pandas indexing capabilities. **Example**: ```python import pandas as pd import numpy as np def multiindex_operations(): # Step 1: Create MultiIndex DataFrame row_iterables = [[\'A1\', \'A2\'], [\'B1\', \'B2\']] col_tuples = [(\'X1\', \'Y1\'), (\'X1\', \'Y2\'), (\'X2\', \'Y1\'), (\'X2\', \'Y2\')] row_index = pd.MultiIndex.from_product(row_iterables, names=[\'level_1\', \'level_2\']) col_index = pd.MultiIndex.from_tuples(col_tuples, names=[\'level_A\', \'level_B\']) df = pd.DataFrame(np.random.randn(4, 4), index=row_index, columns=col_index) # Step 2: Indexing and Slicing Operations result = {} # Extract all rows where the first level is \'A1\' result[\'rows_A1\'] = df.loc[\'A1\'] # Extract all columns where the second level is \'Y1\' result[\'columns_Y1\'] = df.loc[:, (slice(None), \'Y1\')] # Extract the value at the intersection of row (\'A2\', \'B1\') and column (\'X1\', \'Y1\') result[\'value_A2_B1_X1_Y1\'] = df.loc[(\'A2\', \'B1\'), (\'X1\', \'Y1\')] # Set all values in rows where the second level is \'B2\' to 0 df.loc[(slice(None), \'B2\'), :] = 0 result[\'modified_df\'] = df return result # Testing the function print(multiindex_operations()) ``` **Expected Output**: A dictionary containing: - DataFrame with extracted rows where the first level is \'A1\'. - DataFrame with extracted columns where the second level is \'Y1\'. - The specific value at the intersection of (\'A2\', \'B1\') and (\'X1\', \'Y1\'). - The modified DataFrame after setting values to 0.","solution":"import pandas as pd import numpy as np def multiindex_operations(): # Step 1: Create MultiIndex DataFrame row_iterables = [[\'A1\', \'A2\'], [\'B1\', \'B2\']] col_tuples = [(\'X1\', \'Y1\'), (\'X1\', \'Y2\'), (\'X2\', \'Y1\'), (\'X2\', \'Y2\')] row_index = pd.MultiIndex.from_product(row_iterables, names=[\'level_1\', \'level_2\']) col_index = pd.MultiIndex.from_tuples(col_tuples, names=[\'level_A\', \'level_B\']) df = pd.DataFrame(np.random.randn(4, 4), index=row_index, columns=col_index) # Step 2: Indexing and Slicing Operations result = {} # Extract all rows where the first level is \'A1\' result[\'rows_A1\'] = df.loc[\'A1\'] # Extract all columns where the second level is \'Y1\' result[\'columns_Y1\'] = df.loc[:, (slice(None), \'Y1\')] # Extract the value at the intersection of row (\'A2\', \'B1\') and column (\'X1\', \'Y1\') result[\'value_A2_B1_X1_Y1\'] = df.loc[(\'A2\', \'B1\'), (\'X1\', \'Y1\')] # Set all values in rows where the second level is \'B2\' to 0 df.loc[(slice(None), \'B2\'), :] = 0 result[\'modified_df\'] = df return result"},{"question":"**Objective:** Test the ability to create custom context managers and manage resources effectively using `contextlib`. **Problem Statement:** You are required to implement a custom context manager that performs a time-bound operation and ensures resource cleanup. Additionally, you will use `ExitStack` to combine multiple context managers and handle exceptions appropriately. Part 1: Custom Context Manager Create a custom context manager `TimedResourceManager` that: 1. Initializes a timer when entered and finalizes it when exited. 2. Prints the time taken to execute the block of code. 3. Ensures the resource will be released even if an exception occurs. Part 2: Combining Context Managers Using the `ExitStack` class: 1. Combine multiple instances of `TimedResourceManager` and an additional pseudo-resource manager. 2. Ensure all resources are cleaned up correctly, even if one of the resources raises an exception during initialization. **Implementation Details:** **Class**: `TimedResourceManager` - **`__init__` method**: Takes a `resource_name` parameter to simulate different resources. - **`__enter__` method**: Starts a timer and prints a message indicating the resource is acquired. - **`__exit__` method**: Stops the timer and prints the elapsed time along with a message indicating the resource is released. **Function**: `execute_operations` - **Parameters**: A list of resource names. - **Operation**: - Use `ExitStack` to manage multiple `TimedResourceManager` instances. - Simulate resource acquisition for passed resource names. - Ensure proper cleanup, even if one of the resources raises an exception (simulated for demonstration). **Example Usage**: ```python from contextlib import ExitStack import time class TimedResourceManager: def __init__(self, resource_name): self.resource_name = resource_name def __enter__(self): self.start_time = time.time() print(f\\"Resource {self.resource_name} acquired.\\") return self def __exit__(self, exc_type, exc_value, traceback): end_time = time.time() print(f\\"Resource {self.resource_name} released. Time taken: {end_time - self.start_time} seconds.\\") if exc_type: print(f\\"An exception occurred: {exc_value}\\") return False # Do not suppress exceptions def execute_operations(resource_names): with ExitStack() as stack: for name in resource_names: if name == \'faulty_resource\': raise RuntimeError(\\"Simulated exception.\\") stack.enter_context(TimedResourceManager(name)) # Example execution (uncomment to run) # execute_operations([\'resource_1\', \'resource_2\', \'faulty_resource\', \'resource_3\']) ``` **Expected Output**: When running `execute_operations([\'resource_1\', \'faulty_resource\', \'resource_2\'])`, the following output should be displayed: ``` Resource resource_1 acquired. Exception: Simulated exception. Resource resource_1 released. Time taken: X seconds. ``` **Constraints**: - Ensure that the `TimedResourceManager` handles resource cleanup correctly. - Use `ExitStack` to appropriately manage multiple context managers. **Evaluation Criteria**: - Correctness of context manager implementation. - Proper use of `ExitStack` to handle multiple resources. - Handling exceptions as prescribed. - Clean code and appropriate use of Python conventions.","solution":"import time from contextlib import ExitStack class TimedResourceManager: def __init__(self, resource_name): self.resource_name = resource_name def __enter__(self): self.start_time = time.time() print(f\\"Resource {self.resource_name} acquired.\\") return self def __exit__(self, exc_type, exc_value, traceback): end_time = time.time() print(f\\"Resource {self.resource_name} released. Time taken: {end_time - self.start_time} seconds.\\") if exc_type: print(f\\"An exception occurred: {exc_value}\\") return False # Do not suppress exceptions def execute_operations(resource_names): with ExitStack() as stack: for name in resource_names: if name == \'faulty_resource\': raise RuntimeError(\\"Simulated exception.\\") stack.enter_context(TimedResourceManager(name))"},{"question":"You are given an AIFF file named `input.aiff`. Your task is to write a Python function using the `aifc` module to perform the following: 1. Open `input.aiff` and read its audio properties. 2. Double the sampling rate of the audio file without modifying the actual audio data. 3. Save the modified audio file under a new name `output.aiff`. Your function should have the following signature: ```python def modify_aiff_sampling_rate(input_file: str, output_file: str) -> None: pass ``` # Input: - `input_file`: A string representing the filename of the input AIFF file – `input.aiff`. - `output_file`: A string representing the filename for the modified output AIFF file – `output.aiff`. # Output: - The function does not return anything, but it creates a new file named `output.aiff` with double the original sampling rate. # Constraints: - You are **not** required to install any additional packages or libraries; use only built-in Python libraries. - You **must** utilize the `aifc` module to handle AIFF files. - Assume `input.aiff` follows AIFF specifications and is not corrupt. # Performance requirements: - The function should handle AIFF files of typical size (e.g., a few MBs). # Example: The function call: ```python modify_aiff_sampling_rate(\'input.aiff\', \'output.aiff\') ``` Should correctly read the properties of `input.aiff`, double the sampling rate, and save the modified audio to `output.aiff`. # Additional Notes: When you change the sampling rate, only update the metadata information that pertains to the sampling rate. Do not alter the actual audio data. This means you need to read the original file\'s frames and parameters, adjust the sampling rate parameter, and write the new file with the updated sampling rate. Here is a brief outline to guide your implementation: 1. Open the input AIFF file and retrieve its parameters. 2. Modify the sampling rate parameter to be double its original value. 3. Write these parameters and the original audio data to the new output file. **Hint**: You\'ll find the methods `getparams()`, `setparams()`, `readframes()`, and `writeframes()` very useful for this task.","solution":"import aifc def modify_aiff_sampling_rate(input_file: str, output_file: str) -> None: This function opens an AIFF file, doubles its sampling rate, and saves it to a new file. :param input_file: The file path of the original AIFF file :param output_file: The file path where the modified AIFF file with double the sampling rate will be saved # Open the original AIFF file for reading with aifc.open(input_file, \'rb\') as infile: # Get parameters of the input file params = infile.getparams() # This includes all parameters nframes = params.nframes nchannels = params.nchannels sampwidth = params.sampwidth framerate = params.framerate * 2 comptype = params.comptype compname = params.compname # Read the audio frames audio_data = infile.readframes(nframes) # Open the new AIFF file for writing with aifc.open(output_file, \'wb\') as outfile: # Set new parameters for the output file, including the doubled framerate outfile.setparams((nchannels, sampwidth, framerate, nframes, comptype, compname)) # Write the original data frames to the new file outfile.writeframes(audio_data)"},{"question":"# Coding Assessment: Implementing Distributed Data Parallel Training with `torch.distributed` Objective The goal of this assessment is to test your understanding and ability to implement distributed data parallel training using the `torch.distributed` module in PyTorch. This exercise will require you to set up a simple distributed training script, initialize the process group, and synchronize the processes. Task You are required to write a Python script that: 1. Initializes a distributed process group using TCP initialization. 2. Sets up a simple neural network model and wraps it with `DistributedDataParallel`. 3. Configures the data loader with distributed sampler. 4. Implements the training loop with synchronized all-reduce operations. 5. Cleans up the process group at the end. Instructions 1. Use the following network architecture: ```python import torch import torch.nn as nn class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.fc = nn.Linear(10, 1) def forward(self, x): return self.fc(x) ``` 2. Your script should include the following steps: - Parse `rank`, `world_size`, and `master_address` as input arguments. - Initialize the process group with `torch.distributed.init_process_group` using TCP initialization. - Set up the `SimpleModel` and wrap it with `torch.nn.parallel.DistributedDataParallel`. - Create a dataset and data loader with a distributed sampler: ```python import torch.utils.data.distributed dataset = torch.randn(100, 10) dataloader = torch.utils.data.DataLoader( dataset, batch_size=16, sampler=torch.utils.data.distributed.DistributedSampler(dataset) ) ``` - Implement the training loop with a simple loss calculation and optimization step. - Synchronize the gradients using `torch.distributed.all_reduce`. - Cleanup the process group using `torch.distributed.destroy_process_group`. 3. The script should be able to run on multiple processes and complete the training loop without errors. Expected Input - `rank`: Integer specifying the rank of the process. - `world_size`: Integer specifying the total number of processes. - `master_address`: String specifying the address of the master node. Expected Output - The script should output the loss at each step for each process. - All processes should complete the training loop and synchronize correctly. Performance Requirements - The script should handle synchronization efficiently without deadlocks. - Use appropriate environment variables for network interface and debugging. Example Usage ```sh # On terminal 1 python distributed_training.py --rank 0 --world_size 2 --master_address \\"tcp://127.0.0.1:29500\\" # On terminal 2 python distributed_training.py --rank 1 --world_size 2 --master_address \\"tcp://127.0.0.1:29500\\" ``` Constraints - Ensure that the process group is initialized and destroyed properly to avoid resource leaks. - Handle any potential exceptions or errors gracefully to prevent crashes.","solution":"import argparse import torch import torch.distributed as dist import torch.nn as nn import torch.optim as optim import torch.utils.data import torch.utils.data.distributed class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.fc = nn.Linear(10, 1) def forward(self, x): return self.fc(x) def main(args): # Initialize the process group dist.init_process_group(backend=\'nccl\', init_method=args.master_address, world_size=args.world_size, rank=args.rank) # Create model and wrap with DDP model = SimpleModel().to(args.rank) model = nn.parallel.DistributedDataParallel(model, device_ids=[args.rank]) # Prepare data loader dataset = torch.randn(100, 10) target = torch.randn(100, 1) train_dataset = torch.utils.data.TensorDataset(dataset, target) train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset, num_replicas=args.world_size, rank=args.rank) train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, sampler=train_sampler) # Loss and optimizer criterion = nn.MSELoss().to(args.rank) optimizer = optim.SGD(model.parameters(), lr=0.01) # Training loop model.train() for epoch in range(5): # number of epochs epoch_loss = 0.0 for batch, (data, target) in enumerate(train_loader): data = data.to(args.rank) target = target.to(args.rank) optimizer.zero_grad() output = model(data) loss = criterion(output, target) loss.backward() optimizer.step() # All reduce to synchronize the loss across all processes reduced_loss = loss.clone() dist.all_reduce(reduced_loss, op=dist.ReduceOp.SUM) reduced_loss /= args.world_size epoch_loss += reduced_loss.item() print(f\'Rank {args.rank}, Epoch {epoch}, Loss: {epoch_loss}\') # Cleanup dist.destroy_process_group() if __name__ == \'__main__\': parser = argparse.ArgumentParser() parser.add_argument(\'--rank\', type=int, required=True) parser.add_argument(\'--world_size\', type=int, required=True) parser.add_argument(\'--master_address\', type=str, required=True) args = parser.parse_args() main(args)"},{"question":"# Question: File Processing and Output Formatting Objective Write a Python function that processes a text file containing product information and generates a formatted report. This function should demonstrate your knowledge of file I/O, string formatting, and handling of structured data. Input - A text file named `products.txt` where each line contains product information in the format `ProductID,Name,Price,Quantity`, e.g., `101,Apple,0.50,150`. - Ensure that your function reads this file. Output - A formatted report as a string in the following format: ``` Product Report ========================== ProductID Name Price Quantity -------------------------------------- 101 Apple 0.50 150 102 Banana 0.30 200 ... ``` Requirements 1. Implement a function `generate_product_report(file_path: str) -> str` that: - Takes the file path of the product file as input. - Reads the product file. - Generates a formatted string report as specified above. 2. Use **f-strings** for formatting the output. 3. Ensure that the report header and columns are properly aligned. 4. Handle any file reading exceptions (e.g., file not found) gracefully by raising appropriate errors with meaningful messages. Constraints - ProductID should be an integer. - The name can be a string containing spaces. - Price should be formatted as a floating point with two decimal places. - Quantity is an integer. Example Usage Assume `products.txt` contains: ``` 101,Apple,0.50,150 102,Banana,0.30,200 103,Cherry,1.20,75 ``` Calling the function: ```python report = generate_product_report(\'products.txt\') print(report) ``` Expected output: ``` Product Report ========================== ProductID Name Price Quantity -------------------------------------- 101 Apple 0.50 150 102 Banana 0.30 200 103 Cherry 1.20 75 ``` Hints - Use the `open()` function to read the file. - Use exception handling to manage file I/O errors. - You can use `str.split(\',\')` to split each line into components. - Use f-strings to align and format the output properly.","solution":"def generate_product_report(file_path: str) -> str: header = ( \\"Product Reportn\\" \\"==========================n\\" \\"ProductID Name Price Quantityn\\" \\"--------------------------------------n\\" ) try: with open(file_path, \'r\') as file: lines = file.readlines() report_body = \\"\\" for line in lines: product_id, name, price, quantity = line.strip().split(\',\') product_id = int(product_id) price = float(price) quantity = int(quantity) report_body += f\\"{product_id:<12}{name:<12}{price:<7.2f}{quantity:<6}n\\" report = header + report_body return report except FileNotFoundError: raise FileNotFoundError(f\\"The file at {file_path} does not exist.\\") except Exception as e: raise RuntimeError(f\\"An error occurred while processing the file: {e}\\")"},{"question":"# Email Message Creation and Sending Objective Design a function in Python that creates an email message composed of HTML and plain text versions, includes multiple file attachments from a specified directory, and sends it using the local SMTP server. Function Specifications: - **Function Name:** `create_and_send_email` - **Input:** - `subject` (str): The subject of the email. - `sender` (str): The sender\'s email address. - `recipients` (list of str): The recipient email addresses. - `plain_text` (str): The plain text content of the email. - `html_text` (str): The HTML content of the email. - `directory` (str): The path of the directory containing files to attach. - **Output:** None Implementation Details 1. Create an email message with the provided `subject`, `sender`, and `recipients`. 2. Add the plain text and HTML content to the email as alternative parts. 3. Attach all files from the specified directory to the email. Use the appropriate MIME type for each file. 4. Send the composed email using the local SMTP server at `localhost`. Constraints - The function should print an appropriate error message if there is an issue with sending the email. - Handle various file types appropriately, using the `mimetypes` module to guess the MIME type. Example Usage ```python import os def create_and_send_email(subject, sender, recipients, plain_text, html_text, directory): import smtplib from email.message import EmailMessage import mimetypes import os # Create the base email message msg = EmailMessage() msg[\'Subject\'] = subject msg[\'From\'] = sender msg[\'To\'] = \', \'.join(recipients) # Set the plain and HTML content msg.set_content(plain_text) msg.add_alternative(html_text, subtype=\'html\') # Attach files from the specified directory for filename in os.listdir(directory): path = os.path.join(directory, filename) if os.path.isfile(path): ctype, encoding = mimettypes.guess_type(path) if ctype is None or encoding is not None: ctype = \'application/octet-stream\' maintype, subtype = ctype.split(\'/\', 1) with open(path, \'rb\') as fp: msg.add_attachment(fp.read(), maintype=maintype, subtype=subtype, filename=filename) # Send the email via local SMTP server try: with smtplib.SMTP(\'localhost\') as s: s.send_message(msg) print(\\"Email sent successfully.\\") except Exception as e: print(f\\"Failed to send email: {e}\\") # Example usage subject = \\"Project Update\\" sender = \\"sender@example.com\\" recipients = [\\"recipient1@example.com\\", \\"recipient2@example.com\\"] plain_text = \\"Please find the project update attached.\\" html_text = \\"<html><body><p>Please find the project update attached.</p></body></html>\\" directory = \\"/path/to/attachments\\" create_and_send_email(subject, sender, recipients, plain_text, html_text, directory) ``` Notes: - Ensure the local SMTP server is configured and running before testing the function. - Verify the MIME type handling ensures the correct content type for attached files.","solution":"def create_and_send_email(subject, sender, recipients, plain_text, html_text, directory): import smtplib from email.message import EmailMessage import mimetypes import os # Create the base email message msg = EmailMessage() msg[\'Subject\'] = subject msg[\'From\'] = sender msg[\'To\'] = \', \'.join(recipients) # Set the plain and HTML content msg.set_content(plain_text) msg.add_alternative(html_text, subtype=\'html\') # Attach files from the specified directory for filename in os.listdir(directory): path = os.path.join(directory, filename) if os.path.isfile(path): ctype, encoding = mimetypes.guess_type(path) if ctype is None or encoding is not None: ctype = \'application/octet-stream\' maintype, subtype = ctype.split(\'/\', 1) with open(path, \'rb\') as fp: msg.add_attachment(fp.read(), maintype=maintype, subtype=subtype, filename=filename) # Send the email via local SMTP server try: with smtplib.SMTP(\'localhost\') as s: s.send_message(msg) print(\\"Email sent successfully.\\") except Exception as e: print(f\\"Failed to send email: {e}\\") # Example usage subject = \\"Project Update\\" sender = \\"sender@example.com\\" recipients = [\\"recipient1@example.com\\", \\"recipient2@example.com\\"] plain_text = \\"Please find the project update attached.\\" html_text = \\"<html><body><p>Please find the project update attached.</p></body></html>\\" directory = \\"./attachments\\" # create_and_send_email(subject, sender, recipients, plain_text, html_text, directory) # Uncomment for actual sending"},{"question":"Objective: Write a function called `process_and_sample_data` that takes in a 2D numpy array `X` and an integer `n_samples`. This function should perform the following tasks: 1. Validate the input array `X` to ensure it is a 2D array containing only finite numbers. 2. Compute the mean and variance of each column. 3. Standardize the array `X` by subtracting the mean and dividing by the standard deviation for each column. 4. Resample the standardized data without replacement to get `n_samples` rows. 5. Return the resampled array. Constraints: - The input array `X` must be a 2D numpy array. - The input integer `n_samples` must be less than or equal to the number of rows in `X`. Function Signature: ```python def process_and_sample_data(X: np.ndarray, n_samples: int) -> np.ndarray: pass ``` Input: - `X`: A 2D numpy array of shape (m, n), where m is the number of samples and n is the number of features. - `n_samples`: An integer representing the number of samples to resample. Output: - A 2D numpy array of shape (n_samples, n) representing the resampled standardized data. Example: ```python import numpy as np X = np.array([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0], [10.0, 11.0, 12.0]]) n_samples = 2 result = process_and_sample_data(X, n_samples) print(result) ``` Expected output (values may vary due to randomness): ``` array([[ 1.34, 1.34, 1.34], [-0.45, -0.45, -0.45]]) ``` Guidelines: 1. Use the `check_array` function from `sklearn.utils` to validate the input array `X`. 2. Use numpy functions to compute the mean and variance. 3. Standardize `X` using vectorized operations. 4. Use `resample` from `sklearn.utils` to perform random sampling without replacement. 5. Ensure the resampled array has `n_samples` rows. Note: - Handle any exceptions where the inputs may not meet the constraints. - Use a consistent random state for reproducibility in the `resample` function.","solution":"import numpy as np from sklearn.utils import check_array, resample def process_and_sample_data(X: np.ndarray, n_samples: int) -> np.ndarray: # Validate the input array to ensure it is a 2D array containing only finite numbers try: X = check_array(X, ensure_2d=True, force_all_finite=True) except ValueError as ve: raise ValueError(f\\"Invalid input array: {ve}\\") # Check that n_samples is less than or equal to the number of rows in X n_rows = X.shape[0] if n_samples > n_rows: raise ValueError(\\"n_samples must be less than or equal to the number of rows in X\\") # Compute the mean and variance of each column means = np.mean(X, axis=0) stds = np.std(X, axis=0) # Standardize the array X standardized_X = (X - means) / stds # Resample the standardized data without replacement to get n_samples rows resampled_X = resample(standardized_X, n_samples=n_samples, replace=False, random_state=42) return resampled_X"},{"question":"# Garbage Collection Optimization and Debugging with Python\'s `gc` Module Objective Implement a Python script that performs the following tasks using the `gc` module: 1. **Disable and Enable Garbage Collection:** - Initially, disable automatic garbage collection. - Confirm and print whether garbage collection is disabled. 2. **Manual Garbage Collection:** - Create a list of cyclic references deliberately (e.g., a list that refers to itself). - Manually invoke garbage collection on all generations. - Print the number of unreachable objects found. 3. **Set and Retrieve Debugging Flags:** - Set debugging options to detect uncollectable objects (`DEBUG_UNCOLLECTABLE`). - Print the current debugging flags set in the system. 4. **Object Tracking and Statistics:** - Print a list of all objects currently tracked by the garbage collector. - Retrieve and print garbage collection statistics. 5. **Callback Functionality:** - Implement a callback function that prints a custom message before and after garbage collection. - Register this callback with the `gc` module callbacks. 6. **Threshold Management:** - Set collection thresholds for the garbage collector. - Print the new thresholds. Constraints - Ensure the script handles enabling and disabling garbage collection cleanly. - Include appropriate error handling for invalid operations. Expected Input and Output - **Input:** None - **Output:** Console logs showing: - Whether garbage collection is disabled or enabled. - Number of unreachable objects collected manually. - Current debugging flags. - List of tracked objects (with a count, for brevity). - Garbage collection statistics. - Custom callback messages during garbage collection. - Updated garbage collection thresholds. Example Output ```plaintext Garbage collection enabled: False Manually collected unreachable objects: 3 Current debugging flags: 8 Tracked Objects Count: 10 GC Statistics: [{\'collections\': 100, \'collected\': 300, \'uncollectable\': 3}, ...] Garbage collection starting... Custom: GC is starting Custom: GC has finished Garbage collection finished... New thresholds: (700, 10, 10) ``` Instructions - Write your script using Python\'s `gc` module as described. - Ensure the code is modular and functions are well-documented.","solution":"import gc def custom_gc_callback(phase, info): print(f\\"Custom: GC {phase}\\") def manage_gc(): # Disable automatic garbage collection gc.disable() print(f\\"Garbage collection enabled: {gc.isenabled()}\\") # Create cyclic references deliberately cycle_list = [] cycle_list.append(cycle_list) # Manually invoke garbage collection on all generations unreachable_objects = gc.collect() print(f\\"Manually collected unreachable objects: {unreachable_objects}\\") # Set debugging options to detect uncollectable objects gc.set_debug(gc.DEBUG_UNCOLLECTABLE) print(f\\"Current debugging flags: {gc.get_debug()}\\") # Print a list of all objects currently tracked by the garbage collector tracked_objects = gc.get_objects() print(f\\"Tracked Objects Count: {len(tracked_objects)}\\") # Retrieve and print garbage collection statistics gc_stats = gc.get_stats() print(f\\"GC Statistics: {gc_stats}\\") # Implement and register a callback function for garbage collection gc.callbacks.append(custom_gc_callback) # Manually invoke garbage collection to see the callback in action print(\'Garbage collection starting...\') gc.collect() print(\'Garbage collection finished...\') # Set and print the new collection thresholds new_thresholds = (700, 10, 10) gc.set_threshold(*new_thresholds) print(f\\"New thresholds: {gc.get_threshold()}\\") # Re-enable garbage collection gc.enable() print(f\\"Garbage collection enabled: {gc.isenabled()}\\")"},{"question":"**Question: Advanced Typing and Type Hinting in Python** In this task, you are required to demonstrate your understanding of Python\'s typing module by creating a type-safe data processing pipeline. This pipeline will use various typing constructs, including user-defined generics, type aliases, protocols, and type guards. # Problem Description You need to implement a data processing pipeline that reads data, processes it by applying a series of transformations, and then outputs the results. Each transformation is represented as a callable that adheres to a specific protocol. # Requirements 1. **Type Aliases**: - Define a type alias for a `DataPoint` as a dictionary with string keys and values that could be either an integer or a float. 2. **Protocol for Transformation**: - Define a protocol `Transformation` that includes a single method `apply` which takes a `DataPoint` as input and returns a transformed `DataPoint`. 3. **Transformations Implementation**: - Implement three different transformations adhering to the `Transformation` protocol: - `IncrementValue`: Increments all integer values by a given amount. - `MultiplyValue`: Multiplies all float values by a given factor. - `CapitalizeKeys`: Capitalizes all string keys in the `DataPoint`. 4. **Data Processor**: - Implement a generic `DataProcessor` class that accepts a list of transformation objects and applies them to a given dataset. The dataset is a list of `DataPoint`. 5. **Type Guard**: - Implement a type guard function `is_valid_data_point` that checks if a given dictionary conforms to the `DataPoint` type alias. # Constraints - The `DataPoint` dictionary should not include nested dictionaries. - Performance is not a major concern, but the implementation should handle typical data processing tasks efficiently. # Input and Output Formats - **Input**: - A list of `DataPoint` dictionaries. - A list of transformation objects. - **Output**: - A list of transformed `DataPoint` dictionaries. # Example ```python from typing import List, Dict, Union, Protocol, TypeVar, Generic from typing_extensions import TypeGuard # 1. Define Type Alias DataPoint = Dict[str, Union[int, float]] # 2. Define Protocol class Transformation(Protocol): def apply(self, data: DataPoint) -> DataPoint: ... # 3. Implement Transformations class IncrementValue: def __init__(self, increment: int): self.increment = increment def apply(self, data: DataPoint) -> DataPoint: return {k: (v + self.increment if isinstance(v, int) else v) for k, v in data.items()} class MultiplyValue: def __init__(self, factor: float): self.factor = factor def apply(self, data: DataPoint) -> DataPoint: return {k: (v * self.factor if isinstance(v, float) else v) for k, v in data.items()} class CapitalizeKeys: def apply(self, data: DataPoint) -> DataPoint: return {k.upper(): v for k, v in data.items()} # 4. Implement DataProcessor T = TypeVar(\'T\', bound=Transformation) class DataProcessor(Generic[T]): def __init__(self, transformations: List[T]): self.transformations = transformations def process(self, dataset: List[DataPoint]) -> List[DataPoint]: for transformation in self.transformations: dataset = [transformation.apply(data) for data in dataset] return dataset # 5. Implement Type Guard def is_valid_data_point(d: dict) -> TypeGuard[DataPoint]: return all(isinstance(k, str) and isinstance(v, (int, float)) for k, v in d.items()) # Example usage dataset = [ {\\"a\\": 1, \\"b\\": 2.0}, {\\"x\\": 3, \\"y\\": 4.5} ] transformations = [ IncrementValue(5), MultiplyValue(1.1), CapitalizeKeys() ] processor = DataProcessor(transformations) processed_data = processor.process(dataset) print(processed_data) ``` # Explanation In this example: - We define a type alias `DataPoint`. - Implement several classes (`IncrementValue`, `MultiplyValue`, and `CapitalizeKeys`) that conform to the `Transformation` protocol. - Create a `DataProcessor` class that applies a series of transformations to a dataset. - Implement a type guard `is_valid_data_point` to ensure that a dictionary conforms to the `DataPoint` type. Your task is to complete the implementation as described and ensure that the provided example usage works correctly, demonstrating your understanding of advanced type hinting and typing constructs in Python.","solution":"from typing import List, Dict, Union, Protocol, TypeVar, Generic from typing_extensions import TypeGuard # 1. Define Type Alias DataPoint = Dict[str, Union[int, float]] # 2. Define Protocol class Transformation(Protocol): def apply(self, data: DataPoint) -> DataPoint: ... # 3. Implement Transformations class IncrementValue: def __init__(self, increment: int): self.increment = increment def apply(self, data: DataPoint) -> DataPoint: return {k: (v + self.increment if isinstance(v, int) else v) for k, v in data.items()} class MultiplyValue: def __init__(self, factor: float): self.factor = factor def apply(self, data: DataPoint) -> DataPoint: return {k: (v * self.factor if isinstance(v, float) else v) for k, v in data.items()} class CapitalizeKeys: def apply(self, data: DataPoint) -> DataPoint: return {k.upper(): v for k, v in data.items()} # 4. Implement DataProcessor T = TypeVar(\'T\', bound=Transformation) class DataProcessor(Generic[T]): def __init__(self, transformations: List[T]): self.transformations = transformations def process(self, dataset: List[DataPoint]) -> List[DataPoint]: for transformation in self.transformations: dataset = [transformation.apply(data) for data in dataset] return dataset # 5. Implement Type Guard def is_valid_data_point(d: dict) -> TypeGuard[DataPoint]: return all(isinstance(k, str) and isinstance(v, (int, float)) for k, v in d.items()) # Example usage dataset = [ {\\"a\\": 1, \\"b\\": 2.0}, {\\"x\\": 3, \\"y\\": 4.5} ] transformations = [ IncrementValue(5), MultiplyValue(1.1), CapitalizeKeys() ] processor = DataProcessor(transformations) processed_data = processor.process(dataset) print(processed_data)"},{"question":"Objective: You are to write a Python function that demonstrates your ability to create and manipulate bytes objects similarly to the functions described in the documentation. The function should simulate a few key operations that can be performed on bytes objects, focusing on creation, concatenation, and conversion. Task: Implement a function called `process_bytes_operations` that takes a list of tuples as input, where each tuple denotes an operation to be performed on bytes objects. The function should return a list of results after performing all the operations specified in the input list. Here are the operations your function should support: 1. **Create a bytes object from a string**: `(\'create_from_string\', string)` 2. **Create a bytes object from a string with a specified length**: `(\'create_from_string_and_size\', string, length)` 3. **Concatenate two bytes objects**: `(\'concat_bytes\', bytes_object1, bytes_object2)` 4. **Retrieve the size of a bytes object**: `(\'get_size\', bytes_object)` 5. **Convert a bytes object to string representation**: `(\'bytes_to_string\', bytes_object)` Function Signature: ```python def process_bytes_operations(operations: list) -> list: ``` Input: * `operations` (list): A list of tuples, where each tuple represents an operation. The supported operations are described above. Each operation tuple has the required elements for that operation. Output: * (list): A list containing the results of the operations in the same order as they appear in the input. Constraints: * The input list will have at most 100 operations. * The strings provided will have lengths ranging from 1 to 1000 characters. * Ensure that the function operates efficiently within the provided constraints. Example: ```python operations = [ (\'create_from_string\', \'hello world\'), (\'create_from_string_and_size\', \'hello world\', 5), (\'concat_bytes\', b\'hello\', b\' world\'), (\'get_size\', b\'hello\'), (\'bytes_to_string\', b\'hello world\') ] print(process_bytes_operations(operations)) ``` Expected Output: ```python [b\'hello world\', b\'hello\', b\'hello world\', 5, \'hello world\'] ``` Notes: * Use Python’s built-in bytes manipulation functions to implement the required operations. * Consider edge cases such as empty strings or invalid lengths.","solution":"def process_bytes_operations(operations: list) -> list: results = [] for operation in operations: op_type = operation[0] if op_type == \'create_from_string\': string = operation[1] results.append(bytes(string, \'utf-8\')) elif op_type == \'create_from_string_and_size\': string = operation[1] length = operation[2] results.append(bytes(string[:length], \'utf-8\')) elif op_type == \'concat_bytes\': bytes1 = operation[1] bytes2 = operation[2] results.append(bytes1 + bytes2) elif op_type == \'get_size\': bytes_object = operation[1] results.append(len(bytes_object)) elif op_type == \'bytes_to_string\': bytes_object = operation[1] results.append(bytes_object.decode(\'utf-8\')) return results"},{"question":"**XML Document Traversal and Modification Using `xml.dom`** Given an XML document, create a Python function named `transform_xml_document` that modifies certain elements and attributes based on specific criteria and returns the modified XML document as a string. # Function Signature ```python def transform_xml_document(xml_string: str) -> str: pass ``` # Input - `xml_string`: A string representing the XML document. # Output - Returns a string representing the modified XML document. # Behavior Apply the following transformations to the given XML document: 1. **Root Element Modification**: - Add an attribute `modified=\\"true\\"` to the root element. 2. **Element Transformation**: - For all elements named `item`: - If an `item` has an attribute `type` with the value `fruit`, change its tag to `fruitItem`. - If an `item` has an attribute `type` with the value `vegetable`, change its tag to `vegetableItem`. 3. **Text Node Updates**: - For every `quantity` element: - Double the integer text content value. If a `quantity` element contains non-integer text, raise a `ValueError`. 4. **Exception Handling**: - Handle any exceptions that occur during parsing or modification (e.g., malformed XML, invalid operations) and raise a custom exception `XMLTransformationError` with an appropriate error message. # Constraints - You can assume the XML document is relatively small and fits in memory. # Example Input ```xml <store> <item type=\\"fruit\\"> <name>Apple</name> <quantity>10</quantity> </item> <item type=\\"vegetable\\"> <name>Carrot</name> <quantity>5</quantity> </item> <item type=\\"fruit\\"> <name>Orange</name> <quantity>8</quantity> </item> </store> ``` Output ```xml <store modified=\\"true\\"> <fruitItem type=\\"fruit\\"> <name>Apple</name> <quantity>20</quantity> </fruitItem> <vegetableItem type=\\"vegetable\\"> <name>Carrot</name> <quantity>10</quantity> </vegetableItem> <fruitItem type=\\"fruit\\"> <name>Orange</name> <quantity>16</quantity> </fruitItem> </store> ``` # Custom Exception Define a custom exception: ```python class XMLTransformationError(Exception): def __init__(self, message): self.message = message super().__init__(self.message) ``` # Note - Use the `xml.dom` module to parse, traverse, and modify the XML document. - Ensure the output XML string is properly formatted. # Implementation The implementation should parse the XML string, traverse and modify the DOM tree as specified, and finally return the modified XML as a string. If any error occurs, raise `XMLTransformationError` with a relevant error message.","solution":"from xml.dom.minidom import parseString, Node from xml.parsers.expat import ExpatError class XMLTransformationError(Exception): def __init__(self, message): self.message = message super().__init__(self.message) def transform_xml_document(xml_string: str) -> str: try: # Parse the XML dom = parseString(xml_string) # Modify the root element root = dom.documentElement root.setAttribute(\\"modified\\", \\"true\\") # Traverse and modify \'item\' elements items = root.getElementsByTagName(\\"item\\") for item in items: if item.hasAttribute(\\"type\\"): item_type = item.getAttribute(\\"type\\") # Change the tag of \'item\' based on the \'type\' attribute if item_type == \\"fruit\\": new_elem = change_tag(dom, item, \\"fruitItem\\") elif item_type == \\"vegetable\\": new_elem = change_tag(dom, item, \\"vegetableItem\\") # Update \'quantity\' elements quantities = root.getElementsByTagName(\\"quantity\\") for quantity in quantities: if quantity.firstChild and quantity.firstChild.nodeType == Node.TEXT_NODE: try: original_value = int(quantity.firstChild.data) quantity.firstChild.data = str(original_value * 2) except ValueError: raise XMLTransformationError(\\"Non-integer value in \'quantity\' element\\") # Return the modified XML document as a string return dom.toxml() except ExpatError as e: raise XMLTransformationError(f\\"XML parsing error: {e}\\") except Exception as e: raise XMLTransformationError(f\\"An error occurred during XML transformation: {e}\\") def change_tag(dom, elem, new_tag_name): # Create new element with the desired tag name new_elem = dom.createElement(new_tag_name) # Copy attributes for attr in elem.attributes.items(): new_elem.setAttribute(attr[0], attr[1]) # Move all child nodes while elem.firstChild: new_elem.appendChild(elem.firstChild) # Replace the old element with the new one elem.parentNode.replaceChild(new_elem, elem) return new_elem"},{"question":"# Rational Number Arithmetic with Fractions You are tasked with implementing several functions that handle operations with fractions using Python\'s `fractions` module. Function 1: `create_fraction` This function takes two integers as arguments and returns a `Fraction` instance representing the fraction. ```python def create_fraction(numerator: int, denominator: int) -> Fraction: Create a fraction from numerator and denominator. Parameters: numerator (int): The numerator of the fraction. denominator (int): The denominator of the fraction. Must not be 0. Returns: Fraction: A fraction representing the given numerator/denominator. Raises: ZeroDivisionError: If the denominator is 0. pass ``` Function 2: `approximate_fraction` This function takes a floating-point number and an optional maximum denominator, and returns the closest `Fraction` to the given float with a denominator at most the specified maximum, using the `limit_denominator` method. ```python def approximate_fraction(value: float, max_denominator: int = 1000000) -> Fraction: Approximate the given floating-point number as a fraction with a denominator no larger than max_denominator. Parameters: value (float): The floating-point number to approximate. max_denominator (int, optional): The maximum allowable denominator. Defaults to 1000000. Returns: Fraction: A fraction approximating the given float. pass ``` Function 3: `fraction_to_string` This function takes a `Fraction` instance and returns its string representation in the form `\\"numerator/denominator\\"`. ```python def fraction_to_string(fraction: Fraction) -> str: Convert a fraction to its string representation. Parameters: fraction (Fraction): The fraction to convert. Returns: str: The string representation of the fraction in the form \\"numerator/denominator\\". pass ``` # Example Usage ```python from fractions import Fraction # Function 1 f = create_fraction(3, 4) print(f) # Output: Fraction(3, 4) # Function 2 approx_f = approximate_fraction(3.14159, 1000) print(approx_f) # Output: Fraction(355, 113) # Function 3 f_str = fraction_to_string(Fraction(355, 113)) print(f_str) # Output: \\"355/113\\" ``` # Constraints - For `create_fraction`, `denominator` must not be zero. - For `approximate_fraction`, `max_denominator` should be a positive integer. - Ensure the implementation uses the `fractions` module and adheres to well-established practices for handling fractions. Design and implement these functions by utilizing the features provided by Python\'s `fractions` module.","solution":"from fractions import Fraction def create_fraction(numerator: int, denominator: int) -> Fraction: Create a fraction from numerator and denominator. Parameters: numerator (int): The numerator of the fraction. denominator (int): The denominator of the fraction. Must not be 0. Returns: Fraction: A fraction representing the given numerator/denominator. Raises: ZeroDivisionError: If the denominator is 0. if denominator == 0: raise ZeroDivisionError(\\"Denominator cannot be zero.\\") return Fraction(numerator, denominator) def approximate_fraction(value: float, max_denominator: int = 1000000) -> Fraction: Approximate the given floating-point number as a fraction with a denominator no larger than max_denominator. Parameters: value (float): The floating-point number to approximate. max_denominator (int, optional): The maximum allowable denominator. Defaults to 1000000. Returns: Fraction: A fraction approximating the given float. return Fraction(value).limit_denominator(max_denominator) def fraction_to_string(fraction: Fraction) -> str: Convert a fraction to its string representation. Parameters: fraction (Fraction): The fraction to convert. Returns: str: The string representation of the fraction in the form \\"numerator/denominator\\". return f\\"{fraction.numerator}/{fraction.denominator}\\""},{"question":"# Advanced Coding Assessment Question: Working with File Objects and Custom File Handlers in Python **Objective**: Implement a series of Python functionalities that mimic the behavior outlined in the provided Python C API documentation for file objects. This task will ensure you understand both fundamental file handling and the creation of custom handlers in Python. **Task Description**: 1. **Function: `create_file_from_descriptor`** - **Input**: - `fd` (int): An existing file descriptor. - `mode` (str): Mode in which the file is required to be opened (e.g., \'r\', \'w\', \'rb\'). - `buffering` (int, optional): Buffering policy (default is -1 for system default buffering). - `encoding` (str, optional): Encoding to be used (default is None). - `errors` (str, optional): How encoding/decoding errors are handled (default is None). - `newline` (str, optional): Newline character(s) to use (default is None). - `closefd` (bool, optional): Whether the file descriptor should be closed (default is True). - **Output**: Returns a Python file object created from the given file descriptor. - **Constraints**: Ensure that all provided arguments follow the `io.open()` function guidelines. 2. **Function: `file_descriptor_from_object`** - **Input**: - `p` (object): An object potentially representing a file. - **Output**: Returns the file descriptor associated with `p` as an integer. - **Constraints**: If the object `p` is not a file or does not have a `fileno` method, raise an appropriate exception. 3. **Function: `read_line_from_file`** - **Input**: - `p` (object): A file object or any object with a `readline()` method. - `n` (int): The maximum number of bytes to read (default is -1). - **Output**: Returns a string representing a line from the file object. - **Constraints**: Mimic the behaviors outlined in PyFile_GetLine documentation. 4. **Function: `write_object_to_file`** - **Input**: - `obj` (object): The object to write to the file. - `p` (object): The file object where the object will be written. - `flags` (int): Flags to control the writing (support `Py_PRINT_RAW`). - **Output**: Returns 0 on success or raises an exception on failure. - **Constraints**: Implement functionality as described in PyFile_WriteObject. Implement these functions with thorough validation to ensure inputs meet the specified constraints. Additionally, write appropriate tests to confirm the correct behavior of these implementations. **Note**: Do not use the deprecated file handling APIs directly. You must rely on Python\'s `io` module for creating and handling the file objects, thereby simulating the behavior of the C API. # Example Usage ```python # Example of using `create_file_from_descriptor` fd = os.open(\\"example.txt\\", os.O_RDWR | os.O_CREAT) file_obj = create_file_from_descriptor(fd, \\"r+\\", buffering=-1, encoding=\\"utf-8\\") print(file_obj.read()) # Example of using `file_descriptor_from_object` descriptor = file_descriptor_from_object(file_obj) print(descriptor) # Should print the file descriptor integer # Example of using `read_line_from_file` line = read_line_from_file(file_obj, 1024) print(line) # Reads a line from the file with a maximum of 1024 bytes # Example of using `write_object_to_file` write_object_to_file(\\"Hello, World!n\\", file_obj, flags=0) ``` **Grading Criteria**: 1. Correct implementation of each function adhering to specified constraints. 2. Comprehensive test cases demonstrating proper usage and handling of edge cases. 3. Code performance and adherence to Python best practices.","solution":"import io import os def create_file_from_descriptor(fd, mode, buffering=-1, encoding=None, errors=None, newline=None, closefd=True): Creates a Python file object from a given file descriptor. return io.open(fd, mode, buffering=buffering, encoding=encoding, errors=errors, newline=newline, closefd=closefd) def file_descriptor_from_object(p): Returns the file descriptor associated with a file-like object. if hasattr(p, \'fileno\') and callable(p.fileno): return p.fileno() else: raise ValueError(\\"The provided object does not have a \'fileno\' method\\") def read_line_from_file(p, n=-1): Returns a line from a file-like object with an optional maximum number of bytes. if hasattr(p, \'readline\') and callable(p.readline): return p.readline(n) else: raise ValueError(\\"The provided object does not have a \'readline\' method\\") def write_object_to_file(obj, p, flags=0): Writes an object to a file-like object. if hasattr(p, \'write\') and callable(p.write): if flags == 0: p.write(str(obj)) else: raise ValueError(f\\"Unsupported flags: {flags}\\") return 0 else: raise ValueError(\\"The provided object does not have a \'write\' method\\")"},{"question":"# Question **Objective:** To assess the understanding and application of Python\'s `cProfile` and `pstats` modules for profiling code. Given an implementation of a recursive function calculating the Fibonacci sequence, your task is to: 1. Profile the function execution to collect detailed statistics. 2. Save the profiling results to a file. 3. Analyze and sort the results to identify the most time-consuming parts of the function. 4. Print the top 5 functions based on cumulative time. **Instructions:** 1. Implement a function `fib(n)` that computes the nth Fibonacci number using recursion. 2. Use the `cProfile` module to profile the execution of `fib(n)` for `n = 30`. 3. Save the profiling results to a file named `fib_profile`. 4. Use the `pstats` module to load the profiling results from the file, sort the data by cumulative time, and print the statistics for the top 5 functions. **Expected Input:** ```python n = 30 ``` **Expected Output:** - Print the top 5 functions based on cumulative time in the profiling results. ```plaintext Sample Output: 497 function calls (8 primitive calls) in 0.005 seconds Ordered by: cumulative time List reduced from 8 to 5 due to restriction <5> ncalls tottime percall cumtime percall filename:lineno(function) 177 0.001 0.000 0.005 0.000 fib.py:5(fib) 177 0.001 0.000 0.004 0.000 fib.py:5(fib) 177 0.001 0.000 0.003 0.000 fib.py:5(fib) 177 0.001 0.000 0.002 0.000 fib.py:5(fib) 177 0.001 0.000 0.001 0.000 fib.py:5(fib) ``` **Template:** The following template is provided to help you get started. Complete the missing sections. ```python import cProfile import pstats def fib(n): if n <= 1: return n else: return fib(n-1) + fib(n-2) def profile_fib(n): # Profile the fib function cProfile.run(\'fib(n)\', \'fib_profile\') def analyze_profile(): # Load and analyze the profiling data stats = pstats.Stats(\'fib_profile\') stats.strip_dirs().sort_stats(pstats.SortKey.CUMULATIVE).print_stats(5) # Run the profiling and analysis n = 30 profile_fib(n) analyze_profile() ``` **Constraints:** - Use the provided `fib` implementation for the Fibonacci sequence. - The profiling results file should be named `fib_profile`. - Only the top 5 functions based on cumulative time should be printed. **Note:** Ensure that the `cProfile` and `pstats` modules are properly used to collect and analyze the profiling data.","solution":"import cProfile import pstats def fib(n): if n <= 1: return n else: return fib(n-1) + fib(n-2) def profile_fib(n): # Profile the fib function cProfile.runctx(\'fib(n)\', globals(), locals(), \'fib_profile\') def analyze_profile(): # Load and analyze the profiling data stats = pstats.Stats(\'fib_profile\') stats.strip_dirs().sort_stats(\'cumulative\').print_stats(5) if __name__ == \\"__main__\\": # Run the profiling and analysis n = 30 profile_fib(n) analyze_profile()"},{"question":"Custom Cookie Management You are tasked with creating a custom cookie manager using Python\'s `http.cookiejar` module. The goal is to build a system that can handle HTTP requests, save and load cookies, and enforce custom policies on cookies, including blocking certain domains and paths. Requirements 1. **Class Implementation**: - Implement a class `CustomCookieManager` that initializes a `http.cookiejar.CookieJar` with a custom policy. - The class should be able to save and load cookies to/from a specified file. 2. **Policy Customization**: - The custom policy should block certain domains and paths provided during initialization. - It should use `http.cookiejar.DefaultCookiePolicy` for the base policy. 3. **HTTP Request Handling**: - Integrate with `urllib.request` to handle HTTP requests and responses. - Automatically manage cookies as per the custom policy. Input/Output 1. **Initialization**: - `blocked_domains`: List of domains to block. - `block_paths`: List of paths to block. 2. **Methods**: - `save_cookies(filepath)`: Save cookies to the specified file. - `load_cookies(filepath)`: Load cookies from the specified file. - `make_request(url)`: Make an HTTP request to the specified URL and manage cookies automatically. Constraints 1. Block domains should only include exactly matching or subdomains. 2. Paths to block should be case-sensitive and exact match. Performance Requirements - Efficient management of cookies within multiple requests in a single session. - Proper handling of file I/O with error catching. Example Usage ```python from http.cookiejar import DefaultCookiePolicy, CookieJar import urllib.request class CustomCookieManager: def __init__(self, blocked_domains=None, block_paths=None): Initialize the CustomCookieManager with the given blocked domains and paths. policy = DefaultCookiePolicy( blocked_domains=blocked_domains, rfc2965=True, strict_ns_domain=DefaultCookiePolicy.DomainStrict, ) self.cookie_jar = CookieJar(policy) self.block_paths = block_paths or [] def save_cookies(self, filepath): Save cookies to the specified file. if not isinstance(self.cookie_jar, http.cookiejar.FileCookieJar): raise TypeError(\\"cookie_jar must be an instance of FileCookieJar to use save_cookies.\\") self.cookie_jar.save(filename=filepath, ignore_discard=True, ignore_expires=True) def load_cookies(self, filepath): Load cookies from the specified file. if not isinstance(self.cookie_jar, http.cookiejar.FileCookieJar): raise TypeError(\\"cookie_jar must be an instance of FileCookieJar to use load_cookies.\\") self.cookie_jar.load(filename=filepath, ignore_discard=True, ignore_expires=True) def make_request(self, url): Make a request to the specified URL and handle cookies. opener = urllib.request.build_opener(urllib.request.HTTPCookieProcessor(self.cookie_jar)) response = opener.open(url) return response # Example Execution blocked_domains = [\\"example.com\\", \\".ads.example.com\\"] blocked_paths = [\\"/blocked-path\\"] cookie_manager = CustomCookieManager(blocked_domains, blocked_paths) cookie_manager.load_cookies(\'cookies.txt\') response = cookie_manager.make_request(\\"http://example.com/open-url\\") cookie_manager.save_cookies(\'cookies.txt\') ``` Implement the `CustomCookieManager` class as specified above. Ensure that your implementation adheres to the given constraints and performance requirements.","solution":"from http.cookiejar import DefaultCookiePolicy, CookieJar, FileCookieJar import urllib.request class CustomCookieManager: def __init__(self, blocked_domains=None, blocked_paths=None): Initialize the CustomCookieManager with the given blocked domains and paths. self.policy = DefaultCookiePolicy( blocked_domains=blocked_domains or [], rfc2965=True, strict_ns_domain=DefaultCookiePolicy.DomainStrict, ) self.cookie_jar = CookieJar(self.policy) self.blocked_paths = blocked_paths or [] def save_cookies(self, filepath): Save cookies to the specified file. if not isinstance(self.cookie_jar, FileCookieJar): raise TypeError(\\"cookie_jar must be an instance of FileCookieJar to use save_cookies.\\") self.cookie_jar.save(filename=filepath, ignore_discard=True, ignore_expires=True) def load_cookies(self, filepath): Load cookies from the specified file. if not isinstance(self.cookie_jar, FileCookieJar): raise TypeError(\\"cookie_jar must be an instance of FileCookieJar to use load_cookies.\\") self.cookie_jar.load(filename=filepath, ignore_discard=True, ignore_expires=True) def make_request(self, url): Make a request to the specified URL and handle cookies. opener = urllib.request.build_opener(urllib.request.HTTPCookieProcessor(self.cookie_jar)) response = opener.open(url) return response"},{"question":"You are required to implement a basic client-server application in Python using socket programming. The server will listen to a specific port on the localhost and respond to client requests. The client will connect to the server, send a message, and print the server\'s response. Both client and server applications must handle socket errors gracefully and should be able to perform multiple interactions in one session. # Requirements 1. **Server Application:** - The server should create a socket, bind it to the localhost at port `12345`, and start listening for incoming connections. - Upon accepting a connection, the server should receive a message from the client, print the message, and send a confirmation message back to the client. - The server should handle multiple client connections using threading. - The server should appropriately shutdown and close sockets upon completion of interaction or occurrence of errors. 2. **Client Application:** - The client should create a socket and connect to the server at \'localhost\' and port `12345`. - The client should send a user-input message to the server and wait for the server\'s response. - The client should print the server\'s response. - The client should appropriately shutdown and close sockets upon completion of interaction or occurrence of errors. # Instructions 1. **Server Implementation:** ```python import socket from threading import Thread def handle_client(client_socket): try: message = client_socket.recv(1024).decode(\'utf-8\') print(f\\"Received message from client: {message}\\") client_socket.send(\\"Message received\\".encode(\'utf-8\')) except socket.error as e: print(f\\"Socket error: {e}\\") finally: client_socket.close() def start_server(): server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM) server_socket.bind((\'localhost\', 12345)) server_socket.listen(5) print(\\"Server is listening on port 12345\\") try: while True: client_socket, addr = server_socket.accept() print(f\\"Accepted connection from {addr}\\") client_handler = Thread(target=handle_client, args=(client_socket,)) client_handler.start() except KeyboardInterrupt: print(\\"Server is shutting down\\") finally: server_socket.close() if __name__ == \\"__main__\\": start_server() ``` 2. **Client Implementation:** ```python import socket def start_client(): client_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM) client_socket.connect((\'localhost\', 12345)) try: message = input(\\"Enter message to send to server: \\") client_socket.send(message.encode(\'utf-8\')) response = client_socket.recv(1024).decode(\'utf-8\') print(f\\"Received response from server: {response}\\") except socket.error as e: print(f\\"Socket error: {e}\\") finally: client_socket.close() if __name__ == \\"__main__\\": start_client() ``` # Constraints - The server should handle up to 5 simultaneous client connections. - Messages from the client must not exceed 1024 bytes. - Proper exception handling should be implemented to manage socket errors and graceful shutdown of the server and client. # Evaluation Criteria - Correctness: The implementations should correctly create and manage sockets, and handle multiple client connections. - Robustness: The applications should handle errors gracefully and ensure proper closure of all sockets. - Code Quality: Proper use of Python\'s socket API, clear and maintainable code structure, and appropriate use of threading for handling client connections.","solution":"import socket from threading import Thread def handle_client(client_socket): try: message = client_socket.recv(1024).decode(\'utf-8\') print(f\\"Received message from client: {message}\\") client_socket.send(\\"Message received\\".encode(\'utf-8\')) except socket.error as e: print(f\\"Socket error: {e}\\") finally: client_socket.close() def start_server(): server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM) server_socket.bind((\'localhost\', 12345)) server_socket.listen(5) print(\\"Server is listening on port 12345\\") try: while True: client_socket, addr = server_socket.accept() print(f\\"Accepted connection from {addr}\\") client_handler = Thread(target=handle_client, args=(client_socket,)) client_handler.start() except KeyboardInterrupt: print(\\"Server is shutting down\\") finally: server_socket.close() if __name__ == \\"__main__\\": start_server()"},{"question":"Coding Assessment Question # Objective: To demonstrate your understanding of the Python import system, including custom import mechanisms using `importlib` and module attributes. # Problem Statement: Design a custom import mechanism using `importlib` that allows importing modules from a specific directory that is not part of the standard Python path. The custom importer should also log each import attempt to a file named `import_log.txt`. # Requirements: 1. **Custom Importer**: You need to create a custom importer that can import Python modules from a specified directory (e.g., `custom_modules/`). 2. **Logging**: Each import attempt (whether successful or not) should be logged to a file named `import_log.txt`. The log entry should include the timestamp, the name of the module being imported, and the result (success or failure). 3. **Meta Path Hook**: Use a meta path finder to integrate your custom importer with the Python import system. 4. **Directory Initial Setup**: Ensure the directory `custom_modules/` exists in the current working directory and contains at least one sample module (e.g., `sample_module.py`). # Input: - The name of the module to import (e.g., `sample_module`). # Output: - The imported module object if the import is successful. - Error messages logged in `import_log.txt` if the import fails. # Instructions: 1. **Create the Directory and Sample Module**: - Create a directory called `custom_modules/` in the current working directory. - Inside this directory, create a file named `sample_module.py` with some basic Python code (e.g., a function that returns a string). 2. **Implement the Custom Importer**: - Write a class `CustomFinder` that implements the meta path finder interface. This class should locate modules inside `custom_modules/`. - Write a class `CustomLoader` that implements the loader interface, responsible for loading the module\'s code from the file. - Register `CustomFinder` in `sys.meta_path`. 3. **Logging Functionality**: - Implement logging in the custom meta path finder to log every import attempt to `import_log.txt`. 4. **Test the Importer**: - Write a script to test importing `sample_module` using your custom importer. # Constraints: - Ensure that multiple imports (repeated imports or simultaneous imports) do not cause errors or inconsistencies. - Make sure that the log format is consistent and includes all the necessary information (timestamp, module name, result). # Example: ```python import importlib import sys import os import time class CustomFinder(importlib.abc.MetaPathFinder): def find_spec(self, fullname, path, target=None): if not fullname.startswith(\\"custom\\"): return None if os.path.isfile(f\'custom_modules/{fullname}.py\'): return importlib.machinery.ModuleSpec(fullname, CustomLoader(fullname)) return None class CustomLoader(importlib.abc.Loader): def __init__(self, name): self.name = name def create_module(self, spec): return None def exec_module(self, module): module_path = f\'custom_modules/{self.name}.py\' with open(module_path, \'r\') as file: code = file.read() exec(code, module.__dict__) # Logging import attempt with open(\'import_log.txt\', \'a\') as log_file: log_file.write(f\'{time.ctime()}: Attempted to import {self.name} - Successn\') return module # Register the custom finder in sys.meta_path sys.meta_path.insert(0, CustomFinder()) # Test the custom importer try: sample_module = importlib.import_module(\\"sample_module\\") print(sample_module.some_function()) except ImportError as e: with open(\'import_log.txt\', \'a\') as log_file: log_file.write(f\'{time.ctime()}: Attempted to import sample_module - Failedn\') print(\\"Module not found\\") ``` # Note: Make sure to create the directory `custom_modules/` with a sample module `sample_module.py` before testing your script.","solution":"import importlib import importlib.abc import importlib.machinery import os import sys import time class CustomLoader(importlib.abc.Loader): def __init__(self, fullname): self.fullname = fullname def create_module(self, spec): return None # Use the default module creation semantics def exec_module(self, module): module_path = f\'custom_modules/{self.fullname}.py\' with open(module_path, \'r\') as file: code = file.read() exec(code, module.__dict__) # Execute the module\'s code in its own namespace self._log_import_attempt(success=True) return module def _log_import_attempt(self, success): with open(\'import_log.txt\', \'a\') as log_file: status = \\"Success\\" if success else \\"Failed\\" log_file.write(f\'{time.ctime()}: Attempted to import {self.fullname} - {status}n\') class CustomFinder(importlib.abc.MetaPathFinder): def find_spec(self, fullname, path, target=None): module_path = f\'custom_modules/{fullname}.py\' if os.path.isfile(module_path): return importlib.machinery.ModuleSpec(fullname, CustomLoader(fullname)) self._log_import_attempt(fullname, success=False) return None def _log_import_attempt(self, fullname, success): with open(\'import_log.txt\', \'a\') as log_file: status = \\"Success\\" if success else \\"Failed\\" log_file.write(f\'{time.ctime()}: Attempted to import {fullname} - {status}n\') # Register the custom finder in sys.meta_path sys.meta_path.insert(0, CustomFinder()) # Ensure the custom_modules directory exists for testing purpose os.makedirs(\'custom_modules\', exist_ok=True) # Create a sample module for testing purpose with open(\'custom_modules/sample_module.py\', \'w\') as f: f.write( def greet(): return \\"Hello, Custom Importer!\\" )"},{"question":"**Objective:** Implement an XML parser using the `xml.sax.xmlreader` package that can process XML data incrementally and handle specific SAX events such as start elements, end elements, and characters. # Problem Statement You are required to implement a class `IncrementalXMLParser` that extends the `xml.sax.xmlreader.IncrementalParser` class. This parser should process XML data in chunks and maintain a count of the total number of start elements, end elements, and character data lengths encountered during parsing. # Method Specifications 1. **Class**: `IncrementalXMLParser` 2. **Methods**: - `__init__(self)`: Initialize the parser and counters. - `feed(self, data: str)`: Feed a chunk of XML data to the parser. - `close(self)`: Close the parser after all data has been fed and return a dictionary with the counts of start elements, end elements, and characters. - `startElement(self, name: str, attrs: xml.sax.xmlreader.AttributesImpl)`: Increment the start element count. - `endElement(self, name: str)`: Increment the end element count. - `characters(self, content: str)`: Increment the character data length by the length of `content`. # Input - XML data fed incrementally as chunks through the `feed` method. - A call to the `close` method to signify the end of data feeding. # Output - A dictionary returned by the `close` method containing three keys: - `\\"start_elements\\"`: Count of start elements encountered. - `\\"end_elements\\"`: Count of end elements encountered. - `\\"characters\\"`: Total length of character data encountered. # Constraints - XML data will be well-formed. - The chunks of XML data fed will be strings. # Example Usage ```python # Initialize the parser parser = IncrementalXMLParser() # Feed XML data in chunks parser.feed(\'<root><child>\') parser.feed(\'Hello\') parser.feed(\'</child>\') parser.feed(\'</root>\') # Close the parser and get the counts result = parser.close() # Expected result assert result == { \\"start_elements\\": 2, \\"end_elements\\": 2, \\"characters\\": 5, } ``` # Implementation Requirements - Inherit from `xml.sax.xmlreader.IncrementalParser`. - Implement the required methods to handle SAX events and maintain counts. Good luck!","solution":"from xml.sax import handler, make_parser from xml.sax.xmlreader import IncrementalParser class IncrementalXMLParser(IncrementalParser): def __init__(self): super().__init__() self.start_elements = 0 self.end_elements = 0 self.characters_length = 0 self.handler = self.ContentHandler(self) self.parser = make_parser() self.parser.setContentHandler(self.handler) def feed(self, data: str): self.parser.feed(data) def close(self): self.parser.close() return { \'start_elements\': self.start_elements, \'end_elements\': self.end_elements, \'characters\': self.characters_length } def startElement(self, name, attrs): self.start_elements += 1 def endElement(self, name): self.end_elements += 1 def characters(self, content): self.characters_length += len(content) class ContentHandler(handler.ContentHandler): def __init__(self, outer): self.outer = outer def startElement(self, name, attrs): self.outer.startElement(name, attrs) def endElement(self, name): self.outer.endElement(name) def characters(self, content): self.outer.characters(content)"},{"question":"You are given a dataset in the form of a 2D list of integers representing pixel values of a grayscale image. Your task is to perform a series of operations on this dataset using PyTorch tensors. Specifically, you need to: 1. Convert the 2D list into a PyTorch tensor of type `torch.int32`. 2. Normalize the tensor values to be between 0 and 1. 3. Reshape the tensor into a 3D tensor with dimensions suitable for a single-batch image format. 4. Create a function to apply a given filter/kernel on this tensor using a convolution operation. 5. Calculate the gradient of the tensor with respect to its elements after applying the convolution operation. # Input - A 2D list of integers `image` representing pixel values of a grayscale image. - A 2D PyTorch tensor `kernel` representing the convolution filter. # Output - A 3D PyTorch tensor representing the normalized image in single-batch format. - A PyTorch tensor representing the gradient of the image after the convolution operation. # Function Signature ```python def process_image(image: List[List[int]], kernel: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]: pass ``` # Example ```python image = [ [0, 255, 255], [255, 0, 255], [255, 255, 0] ] kernel = torch.tensor([[1, 0, -1], [1, 0, -1], [1, 0, -1]], dtype=torch.float32) normalized_image, gradient = process_image(image, kernel) print(normalized_image) print(gradient) ``` # Constraints - The input image will always be a square matrix (n x n) with 1 ≤ n ≤ 1024. - The kernel size will always be 3 x 3. - Use PyTorch functions and methods for tensor operations. # Notes - Ensure that the data types are correctly managed to avoid type conflicts. - Use `torch.autograd` for gradient calculation. - The input list can have pixel values in the range [0, 255]. # Requirements - Write a function `process_image` to accomplish the above tasks. - Your function should handle edge cases, such as the smallest possible image size.","solution":"import torch from typing import List, Tuple def process_image(image: List[List[int]], kernel: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]: # Step 1: Convert the 2D list into a PyTorch tensor of type torch.int32 tensor_image = torch.tensor(image, dtype=torch.int32) # Step 2: Normalize the tensor values to be between 0 and 1 tensor_image = tensor_image.float() / 255.0 # Step 3: Reshape the tensor into a 3D tensor with dimensions suitable for a single-batch image format tensor_image = tensor_image.unsqueeze(0).unsqueeze(0) # Step 4: Apply the given filter/kernel using a convolution operation kernel = kernel.unsqueeze(0).unsqueeze(0) # Add batch and channel dimensions to the kernel convolution_result = torch.nn.functional.conv2d(tensor_image, kernel, padding=1) # Step 5: Calculate the gradient of the tensor with respect to its elements tensor_image.requires_grad_(True) convolution_result = torch.nn.functional.conv2d(tensor_image, kernel, padding=1) convolution_result.sum().backward() gradient = tensor_image.grad return tensor_image, gradient"},{"question":"# Custom Module Importer Using Import Hooks Objective: To demonstrate an understanding of Python\'s import system, the `importlib` module, and how to manipulate the import process using import hooks. Description: You are required to implement a custom module importer to search and load modules from a given folder location that is not included in the standard `sys.path`. The custom importer should leverage Python’s meta path hooks to locate and load modules from a specified directory. Task: 1. Create a custom path entry finder and a custom loader. 2. Register the custom path entry finder with `sys.path_hooks`. 3. Ensure the custom finder locates Python files (`.py`) in the specified directory. 4. Implement importing of a sample module from the custom directory demonstrating the functionality. Implementation Details: 1. **Custom Finder and Loader**: Define custom classes for the finder and loader. - The finder should return a spec for the module. - The loader should load and execute the module. 2. **Registration**: Register your custom path entry finder in `sys.path_hooks`. - Add the custom directory to `sys.path`. 3. **Example Module**: Create a simple Python module (like `example.py`) in the custom directory. # Expected Input: - Path to the custom directory. - Name of the module to import. # Expected Output: - The loaded module and its contents. # Constraints: - Do not use standard `import` or `importlib.import_module()` for importing the module directly. - Demonstrate the import with valid module files only. ```python import sys import importlib.abc import importlib.util import os class CustomPathFinder(importlib.abc.MetaPathFinder): def find_spec(self, fullname, path=None, target=None): # Implement find_spec to locate the module in the custom directory pass class CustomLoader(importlib.abc.Loader): def create_module(self, spec): # Optionally create a module object pass def exec_module(self, module): # Implement executing the module code pass def register_custom_importer(directory): # Register the custom path finder and loader with sys.path_hooks pass # Define the custom directory and test the module import custom_directory = \\"<path_to_custom_directory>\\" register_custom_importer(custom_directory) # Import the module using the custom importer example = importlib.import_module(\\"example\\") # Print or use the loaded module print(example) ``` Notes: - Assume the custom directory and the example module files are set up before running the script. - Ensure exception handling for cases where the module can\'t be found or loaded.","solution":"import sys import importlib.abc import importlib.util import os class CustomPathFinder(importlib.abc.MetaPathFinder): def __init__(self, path): self.path = path def find_spec(self, fullname, path, target=None): module_name = fullname.split(\'.\')[-1] module_path = os.path.join(self.path, module_name + \'.py\') if not os.path.isfile(module_path): return None spec = importlib.util.spec_from_file_location(fullname, module_path, loader=CustomLoader(module_path)) return spec class CustomLoader(importlib.abc.Loader): def __init__(self, module_path): self.module_path = module_path def create_module(self, spec): return None def exec_module(self, module): with open(self.module_path, \'r\') as file: code = file.read() exec(code, module.__dict__) def register_custom_importer(directory): if not os.path.isdir(directory): raise ValueError(f\\"{directory} is not a valid directory\\") sys.path_hooks.insert(0, lambda p: CustomPathFinder(directory) if p == directory else None) sys.path.insert(0, directory) # Example usage for demonstration purposes: # custom_directory = \\"/path/to/custom/directory\\" # register_custom_importer(custom_directory) # example = importlib.import_module(\\"example\\") # print(example)"},{"question":"# Chunked Data File Processor Your task is to implement a function that processes chunked data files using the deprecated **chunk** module in Python 3.10. The function you will implement should read through the chunks of a given file, extract and return essential information about each chunk, and handle the specifics of chunk alignment, endianness, and chunk headers. Function Signature ```python def process_chunked_file(file_path: str) -> List[Dict[str, Any]]: ``` Input - `file_path` (str): A string representing the path to the chunked data file. Output - Returns a list of dictionaries, where each dictionary contains the following key-value pairs: - `ID` (str): The chunk ID. - `Size` (int): The size of the chunk. - `Data` (bytes): The data of the chunk. Example Suppose you have a file `example.iff` consisting of the following chunks: - Chunk 1: ID = \\"ABCD\\", Size = 4, Data = b\'data\' - Chunk 2: ID = \\"EFGH\\", Size = 2, Data = b\'ok\' For this example, calling `process_chunked_file(\\"example.iff\\")` should return: ```python [ {\'ID\': \'ABCD\', \'Size\': 4, \'Data\': b\'data\'}, {\'ID\': \'EFGH\', \'Size\': 2, \'Data\': b\'ok\'}, ] ``` Constraints - Use the `chunk` module to handle chunked data. - Assume the file is properly formatted according to the EA IFF 85 standard. - Handle chunk alignment and endianness based on the default values: `align=True` and `bigendian=True`. - Ensure to handle files with multiple chunks and varying data lengths. Implementation Details 1. Open the file using a file-like object. 2. Create an instance of the `chunk.Chunk` class. 3. Iterate through each chunk in the file: - Retrieve the chunk ID using `getname()`. - Retrieve the chunk size using `getsize()`. - Read the chunk data using `read()`. - Store the information in the specified format within a dictionary. 4. Append each dictionary to a list and return the list after processing all chunks. # Notes - Ensure proper error handling for exceptions like `EOFError`. - Remember to close chunks properly using the `close()` method if needed.","solution":"import chunk from typing import List, Dict, Any def process_chunked_file(file_path: str) -> List[Dict[str, Any]]: result = [] with open(file_path, \'rb\') as file: try: while True: ch = chunk.Chunk(file, bigendian=True, align=True) chunk_info = { \'ID\': ch.getname().decode(\'ascii\'), \'Size\': ch.getsize(), \'Data\': ch.read() } result.append(chunk_info) ch.close() except EOFError: pass return result"},{"question":"# Coding Assignment: **Process Monitoring and Cleanup using Python** Given the emphasis on system functions and process controls in the documentation, this coding question will challenge the students to implement a Python function that monitors processes and performs specific cleanup operations upon termination. Problem Statement: You are required to implement a Python function `monitor_and_cleanup(proc, cleanup_func)` that: 1. **Monitors a given process** until it terminates. 2. **Executes a cleanup function** when the monitored process terminates. The function `monitor_and_cleanup` should: - Take two parameters: - `proc`: an instance of the Python `subprocess.Popen` class representing the process to be monitored. - `cleanup_func`: a function to be executed once the process terminates. This function should take no arguments and return nothing. - Continuously check whether the given process is still running. - Execute the `cleanup_func` immediately after the process terminates. Constraints: 1. **System Compatibility**: The solution should be compatible with both Unix-like and Windows operating systems. 2. **Performance**: You should minimize the CPU usage by efficiently waiting between process status checks. Function Signature: ```python import subprocess def monitor_and_cleanup(proc: subprocess.Popen, cleanup_func: callable) -> None: pass ``` Example Usage: ```python import subprocess import time def sample_cleanup_func(): print(\\"Cleaning up resources...\\") def main(): # Starting a sample subprocess that runs for 5 seconds proc = subprocess.Popen([\\"sleep\\", \\"5\\"]) # Call the monitor_and_cleanup function monitor_and_cleanup(proc, sample_cleanup_func) # Expected output: \\"Cleaning up resources...\\" after 5 seconds if __name__ == \\"__main__\\": main() ``` Notes: - The `monitor_and_cleanup` function should only return after the cleanup function has been fully executed. - Use appropriate system calls and Python techniques to check the process status and avoid busy-waiting. The goal of this question is to assess the student\'s ability to: - Utilize Python\'s subprocess module for process handling. - Implement efficient polling mechanisms. - Integrate system-level interactions with Python code.","solution":"import subprocess import time def monitor_and_cleanup(proc: subprocess.Popen, cleanup_func: callable) -> None: Monitors the given process until it terminates and then executes the cleanup function. Parameters: proc (subprocess.Popen): The process to be monitored. cleanup_func (callable): The function to execute after the process terminates. # Continuously check if the process is still running while proc.poll() is None: time.sleep(0.5) # Sleep for a short while to avoid busy-waiting # Execute the cleanup function once the process terminates cleanup_func()"},{"question":"Pandas Missing Values Handling You have been provided with a dataset containing information about various products and their sales over a period of time. However, the dataset contains some missing values which are denoted by `NA` for non-datetime columns and `NaT` for datetime columns. Your task is to write a function `clean_missing_data(df)` that takes a pandas DataFrame `df` as input and performs the following operations: 1. Replace all `NA` values in non-datetime columns with the mean of their respective columns. 2. Replace all `NaT` values in datetime columns with the earliest date present in their respective columns. 3. return the cleaned DataFrame. Below is the expected input and output format: # Input - `df` (pandas DataFrame): The input DataFrame containing missing values as `NA` and `NaT`. # Output - pandas DataFrame: The cleaned DataFrame with no missing values. # Constraints 1. The DataFrame may have a mixed type of columns. 2. There can be multiple datetime columns in the DataFrame. 3. Performance should be considered for datasets with up to 1 million rows. # Example Given the following DataFrame: | ProductID | ProductName | SaleDate | QuantitySold | |-----------|-------------|-----------|--------------| | 1 | Product A | 2023-01-10 | 20 | | 2 | Product B | NaT | NA | | 3 | Product C | 2023-01-15 | 40 | | 4 | Product D | 2023-01-12 | NA | The function `clean_missing_data(df)` should return the following DataFrame: | ProductID | ProductName | SaleDate | QuantitySold | |-----------|-------------|-----------|--------------| | 1 | Product A | 2023-01-10 | 20 | | 2 | Product B | 2023-01-10 | 30 | | 3 | Product C | 2023-01-15 | 40 | | 4 | Product D | 2023-01-12 | 30 | Here, NA in `QuantitySold` column is replaced by the mean of non-missing values (i.e., (20 + 40)/2 = 30), and NaT in `SaleDate` column is replaced by the earliest date (i.e., 2023-01-10). Note: You may assume that all datetime columns are already in datetime format.","solution":"import pandas as pd def clean_missing_data(df): Cleans the missing data in a DataFrame. :param df: pandas DataFrame containing missing values as `NA` and `NaT`. :return: cleaned pandas DataFrame with no missing values. # Replace all `NA` values in non-datetime columns with the mean of their respective columns for col in df.columns: if df[col].dtype != \'datetime64[ns]\': if df[col].isna().sum() > 0: mean_value = df[col].mean() df[col].fillna(mean_value, inplace=True) else: if df[col].isna().sum() > 0: earliest_date = df[col].min() df[col].fillna(earliest_date, inplace=True) return df"},{"question":"# Question: Context Managers in `contextlib` Objective: Create a custom context manager using the `contextlib.contextmanager` decorator to manage a resource. Additionally, implement a synchronous function that makes use of this context manager to demonstrate its utility. Instructions: 1. Define a function `managed_file(filename, mode)` using the `contextlib.contextmanager` decorator. This function should: - Open a file specified by the `filename` in the mode specified by `mode`. - Ensure the file is automatically closed when exiting the `with` block, even if an exception is raised inside the block. 2. Implement a function `write_to_file(filename, content)` that: - Uses the `managed_file` context manager to open the file for writing. - Writes the specified `content` to the file. 3. Demonstrate the usage of the `write_to_file` function by writing some sample content to a file and ensuring the file is closed properly after writing. Constraints: - Do not use built-in file open/close constructs directly in the `write_to_file` function; instead, utilize the `managed_file` context manager. - Handle any potential file-related exceptions gracefully. Example: ```python from contextlib import contextmanager @contextmanager def managed_file(filename, mode): # Your implementation here def write_to_file(filename, content): # Your implementation here # Demonstration: Writing content to a file write_to_file(\'sample.txt\', \'Hello, World!\') ``` Expected Output: Executing the `write_to_file(\'sample.txt\', \'Hello, World!\')` function should create or overwrite a file named `sample.txt` with the content \\"Hello, World!\\" written to it. Ensure that no resource is left unclosed and handle any exceptions appropriately. **Hint**: Refer to the `contextlib.closing` example if necessary for guidance on how to ensure resource is properly managed.","solution":"from contextlib import contextmanager @contextmanager def managed_file(filename, mode): Context manager to open, yield, and then close a file. Ensures the file is closed even if an exception occurs. f = open(filename, mode) try: yield f finally: f.close() def write_to_file(filename, content): Writes the specified content to the file using the managed_file context manager. try: with managed_file(filename, \'w\') as f: f.write(content) except Exception as e: print(f\\"An error occurred: {e}\\") # Demonstration: Writing content to a file write_to_file(\'sample.txt\', \'Hello, World!\')"},{"question":"<|Analysis Begin|> The provided documentation details the `platform` module in Python, which is used for accessing underlying platform\'s identifying data. This module allows queries about the system\'s architecture, machine type, network name, processor, Python version and build, system release, and other platform-specific information. Several functions are mentioned which return information about the executing environment, such as `platform.architecture()`, `platform.machine()`, `platform.node()`, `platform.platform()`, `platform.processor()`, amongst others. These functions are essential for gaining insights about the operating system, hardware, and the Python runtime. Considering the usage of these functions could give a comprehensive understanding of platform identification, this can be a good base for an assessment question. One can craft a question which is both broad in terms of the information it queries and specific in terms of utilizing the correct functions from the module. <|Analysis End|> <|Question Begin|> **Platform Information Aggregator** **Objective:** Write a Python function `get_platform_info()` that gathers detailed information about the current platform. Your function should collect data using the `platform` module and return it in a dictionary format. **Function Signature:** ```python def get_platform_info() -> dict: pass ``` **Detailed Requirements:** Your implementation should use the following functions from the `platform` module: 1. `platform.architecture()` - gather the bit architecture and linkage format. 2. `platform.machine()` - get the machine type. 3. `platform.node()` - get the network name of the computer. 4. `platform.platform()` - get a human-readable string identifying the platform. 5. `platform.processor()` - get the processor name. 6. `platform.python_build()` - get the Python build number and date. 7. `platform.python_compiler()` - get the compiler used for the Python build. 8. `platform.python_implementation()` - get the Python implementation. 9. `platform.python_version()` - get the Python version. 10. `platform.release()` - get the system’s release. 11. `platform.system()` - get the system/OS name. 12. `platform.version()` - get the system’s release version. 13. `platform.uname()` - get a namedtuple containing information similar to `os.uname()` but with more attributes. The resulting dictionary should include keys reflecting the above terms, and the values should be the respective outputs of the platform functions. **Example Output:** ```python { \'architecture\': (\'64bit\', \'ELF\'), \'machine\': \'x86_64\', \'node\': \'hostname\', \'platform\': \'Linux-5.4.0-72-generic-x86_64-with-Ubuntu-20.04-focal\', \'processor\': \'x86_64\', \'python_build\': (\'default\', \'Mar 1 2021 12:06:40\'), \'python_compiler\': \'GCC 9.3.0\', \'python_implementation\': \'CPython\', \'python_version\': \'3.9.2\', \'release\': \'5.4.0-72-generic\', \'system\': \'Linux\', \'version\': \'#80-Ubuntu SMP Mon Mar 22 23:51:30 UTC 2021\', \'uname\': uname_result(system=\'Linux\', node=\'hostname\', release=\'5.4.0-72-generic\', version=\'#80-Ubuntu SMP Mon Mar 22 23:51:30 UTC 2021\', machine=\'x86_64\', processor=\'x86_64\') } ``` **Constraints:** - Assume the environment executing the function has access to the `platform` module. - The function should handle any possible exceptions and return appropriate default values where platform information cannot be determined. - You are expected to demonstrate a clear understanding and usage of the functions within the `platform` module. **Additional Notes:** Your implementation should aim to be clean and readable. Make sure to test your function across different platforms to ensure reliability and correctness of the output.","solution":"import platform def get_platform_info() -> dict: Gathers detailed information about the current platform. Returns: dict: A dictionary containing platform information. return { \'architecture\': platform.architecture(), \'machine\': platform.machine(), \'node\': platform.node(), \'platform\': platform.platform(), \'processor\': platform.processor(), \'python_build\': platform.python_build(), \'python_compiler\': platform.python_compiler(), \'python_implementation\': platform.python_implementation(), \'python_version\': platform.python_version(), \'release\': platform.release(), \'system\': platform.system(), \'version\': platform.version(), \'uname\': platform.uname() }"},{"question":"# Parsing an Incremental Email Stream You are asked to implement a function that parses an incrementally received email message from a stream, extracts certain key information from the parsed message, and returns the extracted data. # Task Implement a function `parse_email_stream(stream)` that performs the following tasks: 1. Uses the `BytesFeedParser` to parse the email message incrementally from a given input stream. 2. Extracts the subject, sender (From), recipient (To), and the body of the email. 3. If the email is a multipart message, extract the plain text body of the first part. 4. Returns the extracted information in the form of a dictionary with keys: `\'subject\'`, `\'from\'`, `\'to\'`, and `\'body\'`. # Input - `stream` (file-like object): A binary stream that provides email data incrementally through its `read` method. # Output - A dictionary with keys `\'subject\'`, `\'from\'`, `\'to\'`, and `\'body\'`, holding the extracted information as strings. # Constraints - You may assume the input stream contains valid email data, compliant with RFC 5322 or RFC 6532. - The email may be either a plain text message or a multipart MIME message. # Example ```python def parse_email_stream(stream): # Your code here # Example usage: with open(\\"email_data.txt\\", \\"rb\\") as stream: result = parse_email_stream(stream) print(result) # Expected Output: # { # \'subject\': \\"Example Subject\\", # \'from\': \\"sender@example.com\\", # \'to\': \\"recipient@example.com\\", # \'body\': \\"This is the body of the email.\\" # } ``` In this example, \\"email_data.txt\\" contains a byte stream of an email message. The function reads the email from this stream, parses it, and extracts the required information. # Note Make sure to handle both single part and multipart messages, extracting plain text from the first part in case of a multipart message.","solution":"import email from email import policy from email.parser import BytesFeedParser from email.message import EmailMessage def parse_email_stream(stream): Parses an email message from a given binary stream. Args: - stream (file-like object): A binary stream that provides email data incrementally. Returns: - A dictionary with keys \'subject\', \'from\', \'to\', and \'body\', containing the respective information from the email. parser = BytesFeedParser(policy=policy.default) while True: chunk = stream.read(1024) if not chunk: break parser.feed(chunk) email_message = parser.close() subject = email_message[\'subject\'] sender = email_message[\'from\'] recipient = email_message[\'to\'] # Handle both single part and multipart messages if email_message.is_multipart(): for part in email_message.iter_parts(): if part.get_content_type() == \'text/plain\': body = part.get_payload(decode=True).decode(part.get_content_charset(\'utf-8\')) break else: body = email_message.get_payload(decode=True).decode(email_message.get_content_charset(\'utf-8\')) return { \'subject\': subject, \'from\': sender, \'to\': recipient, \'body\': body }"},{"question":"**Working with the `email.policy` Module in Python 3.10** Objective Understand and apply the `email.policy` module concepts to create custom policies and handle email message parsing and generation according to specified rules. Problem Statement You are required to create a custom email policy by combining and overriding attributes of existing `Policy` instances. Specifically, you will: 1. Create a new email `Policy` that has: - Line separator as `rn` - Raise an error on any header defect - Enforce maximum line length to 100 characters 2. Use this custom policy to read an email from a file, process it, and then write the processed email back into another file, ensuring the correct handling of line separators and defect raising. Implementation Details 1. Define a function `create_custom_policy() -> email.policy.Policy` which: - Combines `policy.SMTP` and `policy.strict` while setting the `max_line_length` attribute to 100. 2. Define a function `process_email(input_file: str, output_file: str, policy: email.policy.Policy) -> None` which: - Reads an email from the `input_file` using the provided `policy`. - Writes the processed email to the `output_file` ensuring the correct line separators and defect handling according to the policy. Requirements - Use `email.policy` to set up policies. - Use `email.message_from_binary_file` and `email.generator.BytesGenerator` for reading and writing emails. - Assume the email message files are in binary format. Function Signatures ```python def create_custom_policy() -> email.policy.Policy: pass def process_email(input_file: str, output_file: str, policy: email.policy.Policy) -> None: pass ``` Constraints - The input and output files will properly exist, and the input file will contain a valid email message in binary format. - The `linesep` attribute should be set to `rn` for RFC compliance when writing to `output_file`. Example Usage ```python if __name__ == \'__main__\': custom_policy = create_custom_policy() process_email(\'input_email.txt\', \'output_email.txt\', custom_policy) ``` This question tests the candidate\'s understanding of the `email.policy` module, handling of custom policies, as well as reading and writing emails programmatically using the Python `email` package.","solution":"import email.policy from email import policy from email.parser import BytesParser from email.generator import BytesGenerator def create_custom_policy() -> email.policy.Policy: Creates a custom email policy that combines SMTP and strict policies with a specified max line length of 100 and line separator as \'rn\'. return policy.SMTP.clone(linesep=\'rn\', raise_on_defect=True).clone(max_line_length=100) def process_email(input_file: str, output_file: str, policy: email.policy.Policy) -> None: Process an email from input_file using the specified policy and writes it back to output_file with correct line separators and defect handling. # Open the input file and parse the email using the specified policy with open(input_file, \'rb\') as f: msg = BytesParser(policy=policy).parse(f) # Write the processed email to the output file with the specified policy with open(output_file, \'wb\') as f: generator = BytesGenerator(f, policy=policy) generator.flatten(msg)"},{"question":"Objective: Create a Python program using the `sqlite3` module to manage a database for a library\'s book collection. The program should demonstrate the ability to create tables, insert records, update records, query the database, and handle transactions. Requirements: 1. **Create a Database and Connect to It:** - Create a database named `library.db`. 2. **Create Tables:** - Create a table `authors` with the following columns: - `id` (INTEGER PRIMARY KEY) - `name` (TEXT) - Create a table `books` with the following columns: - `id` (INTEGER PRIMARY KEY) - `title` (TEXT) - `author_id` (INTEGER, FOREIGN KEY references `authors(id)`) - `year` (INTEGER) 3. **Insert Records:** - Insert at least 3 authors into the `authors` table. - Insert at least 5 books into the `books` table, ensuring that at least two books share the same author. 4. **Query the Database:** - Write a function `get_books_by_author(author_name)` that, given an author\'s name, queries the database and returns a list of books written by that author, ordered by year. - Print the results to the console. 5. **Update Records:** - Write a function `update_book_title(book_id, new_title)` that updates the title of a book given its `id`. 6. **Transaction Handling:** - Demonstrate transaction handling by writing a function `insert_books_in_transaction(book_list)` that inserts multiple books into the `books` table within a single transaction. If an error occurs (e.g., a constraint violation), roll back the transaction and print an error message. 7. **Use Placeholders:** - Ensure all SQL statements that insert or update records use placeholders to bind values. 8. **Exception Handling:** - Add appropriate exception handling to deal with potential database errors. Input: - Authors and books data for insertion. - Author name for querying books. - Book id and new title for updating a book. - List of books for transactional insertion. Output: - List of books by the queried author. - Confirmation message after updating a book title. - Error message if a transaction fails. ```python import sqlite3 def create_tables(conn): cur = conn.cursor() cur.execute(CREATE TABLE IF NOT EXISTS authors ( id INTEGER PRIMARY KEY, name TEXT)) cur.execute(CREATE TABLE IF NOT EXISTS books ( id INTEGER PRIMARY KEY, title TEXT, author_id INTEGER, year INTEGER, FOREIGN KEY (author_id) REFERENCES authors (id))) conn.commit() def insert_authors(conn, authors): cur = conn.cursor() cur.executemany(\\"INSERT INTO authors (name) VALUES (?)\\", [(author,) for author in authors]) conn.commit() def insert_books(conn, books): cur = conn.cursor() cur.executemany(\\"INSERT INTO books (title, author_id, year) VALUES (?, ?, ?)\\", books) conn.commit() def get_books_by_author(conn, author_name): cur = conn.cursor() query = SELECT books.title, books.year FROM books JOIN authors ON books.author_id = authors.id WHERE authors.name = ? ORDER BY books.year cur.execute(query, (author_name,)) return cur.fetchall() def update_book_title(conn, book_id, new_title): cur = conn.cursor() cur.execute(\\"UPDATE books SET title = ? WHERE id = ?\\", (new_title, book_id)) conn.commit() def insert_books_in_transaction(conn, book_list): try: cur = conn.cursor() cur.executemany(\\"INSERT INTO books (title, author_id, year) VALUES (?, ?, ?)\\", book_list) conn.commit() except sqlite3.IntegrityError as e: conn.rollback() print(f\\"Transaction failed: {e}\\") def main(): # Connect to the database conn = sqlite3.connect(\'library.db\') # Create tables create_tables(conn) # Insert authors authors = [\'Jane Austen\', \'Charles Dickens\', \'Agatha Christie\'] insert_authors(conn, authors) # Insert books books = [ (\'Pride and Prejudice\', 1, 1813), (\'Emma\', 1, 1815), (\'Oliver Twist\', 2, 1837), (\'Great Expectations\', 2, 1861), (\'Murder on the Orient Express\', 3, 1934) ] insert_books(conn, books) # Query books by author author_name = \'Charles Dickens\' books_by_author = get_books_by_author(conn, author_name) print(f\\"Books by {author_name}: {books_by_author}\\") # Update book title update_book_title(conn, 2, \'Emma (Updated)\') # Demonstrate transaction handling new_books = [ (\'New Book 1\', 3, 2023), (\'New Book 2\', 2, 2023) ] insert_books_in_transaction(conn, new_books) # Close the connection conn.close() if __name__ == \\"__main__\\": main() ``` Explanation: - The code establishes a connection to an SQLite database and creates tables for authors and books. - It inserts authors and books into the respective tables. - It defines functions to query books by author, update book titles, and insert books within a transaction. - It uses placeholders for SQL statements to prevent SQL injection. - It includes exception handling for transactions to ensure data integrity.","solution":"import sqlite3 def create_tables(conn): cur = conn.cursor() cur.execute(CREATE TABLE IF NOT EXISTS authors ( id INTEGER PRIMARY KEY, name TEXT)) cur.execute(CREATE TABLE IF NOT EXISTS books ( id INTEGER PRIMARY KEY, title TEXT, author_id INTEGER, year INTEGER, FOREIGN KEY (author_id) REFERENCES authors (id))) conn.commit() def insert_authors(conn, authors): cur = conn.cursor() cur.executemany(\\"INSERT INTO authors (name) VALUES (?)\\", [(author,) for author in authors]) conn.commit() def insert_books(conn, books): cur = conn.cursor() cur.executemany(\\"INSERT INTO books (title, author_id, year) VALUES (?, ?, ?)\\", books) conn.commit() def get_books_by_author(conn, author_name): cur = conn.cursor() query = SELECT books.title, books.year FROM books JOIN authors ON books.author_id = authors.id WHERE authors.name = ? ORDER BY books.year cur.execute(query, (author_name,)) return cur.fetchall() def update_book_title(conn, book_id, new_title): cur = conn.cursor() cur.execute(\\"UPDATE books SET title = ? WHERE id = ?\\", (new_title, book_id)) conn.commit() def insert_books_in_transaction(conn, book_list): try: cur = conn.cursor() cur.executemany(\\"INSERT INTO books (title, author_id, year) VALUES (?, ?, ?)\\", book_list) conn.commit() except sqlite3.IntegrityError as e: conn.rollback() print(f\\"Transaction failed: {e}\\") def main(): # Connect to the database conn = sqlite3.connect(\'library.db\') # Create tables create_tables(conn) # Insert authors authors = [\'Jane Austen\', \'Charles Dickens\', \'Agatha Christie\'] insert_authors(conn, authors) # Insert books books = [ (\'Pride and Prejudice\', 1, 1813), (\'Emma\', 1, 1815), (\'Oliver Twist\', 2, 1837), (\'Great Expectations\', 2, 1861), (\'Murder on the Orient Express\', 3, 1934) ] insert_books(conn, books) # Query books by author author_name = \'Charles Dickens\' books_by_author = get_books_by_author(conn, author_name) print(f\\"Books by {author_name}: {books_by_author}\\") # Update book title update_book_title(conn, 2, \'Emma (Updated)\') # Demonstrate transaction handling new_books = [ (\'New Book 1\', 3, 2023), (\'New Book 2\', 2, 2023) ] insert_books_in_transaction(conn, new_books) # Close the connection conn.close() if __name__ == \\"__main__\\": main()"},{"question":"# Distributed Checkpoint Save and Load Objective: Implement a small-scale distributed checkpoint save and load functions using PyTorch\'s `torch.distributed.checkpoint` module. You will simulate a distributed environment with multiple processes, each saving and loading a part of the model. The task involves: 1. Implementing a function to save the model state in a distributed manner. 2. Implementing a function to load the model state in a distributed manner. Functions to Implement: 1. `distributed_save(model, save_dir, rank, world_size)` 2. `distributed_load(model, save_dir, rank, world_size)` Parameters: - `model` (torch.nn.Module) - The model to save/load. - `save_dir` (str) - The directory where the checkpoints will be stored. - `rank` (int) - The rank of the current process (simulated as process ID in a distributed environment). - `world_size` (int) - The total number of ranks (simulated as the total number of distributed processes). Requirements: 1. **Save Function (`distributed_save`)**: - Save the state dictionary of the model to the directory specified by `save_dir`. - Ensure the save operation is distributed: each rank should handle part of the state dictionary. - Use `torch.distributed.checkpoint.save_state_dict` for saving. 2. **Load Function (`distributed_load`)**: - Load the state dictionary from the `save_dir`. - Ensure the load operation is performed in a distributed manner: each rank should load its part of the state dictionary. - Use `torch.distributed.checkpoint.load_state_dict` for loading. Example: ```python import torch import torch.distributed as dist from torch.distributed.checkpoint import save_state_dict, load_state_dict import os def distributed_save(model, save_dir, rank, world_size): os.makedirs(save_dir, exist_ok=True) state_dict = model.state_dict() # Simulate distributed save by slicing state_dict by rank keys = list(state_dict.keys()) local_keys = keys[rank::world_size] local_state_dict = {k: state_dict[k] for k in local_keys} save_path = os.path.join(save_dir, f\'checkpoint_rank{rank}.pth\') save_state_dict(local_state_dict, save_path) def distributed_load(model, save_dir, rank, world_size): state_dict = model.state_dict() # Simulate distributed load by each rank loading its part load_path = os.path.join(save_dir, f\'checkpoint_rank{rank}.pth\') local_state_dict = load_state_dict(load_path) state_dict.update(local_state_dict) model.load_state_dict(state_dict) # Example usage if __name__ == \\"__main__\\": dist.init_process_group(backend=\'gloo\', rank=0, world_size=1) # Example model model = torch.nn.Linear(10, 1) # Save distributed checkpoint distributed_save(model, \\"./checkpoints\\", rank=0, world_size=1) # Load distributed checkpoint distributed_load(model, \\"./checkpoints\\", rank=0, world_size=1) dist.destroy_process_group() ``` Ensure you test your implementation with different ranks and a multi-process setup to validate the distributed save and load functionality. Constraints: - Use only PyTorch for the implementation. - The solution must handle process synchronization implicitly or explicitly as required. # Notes: - This question assesses the understanding of distributed processing with checkpoints in PyTorch. - Make sure to refer to the official PyTorch documentation for more details on the `torch.distributed.checkpoint` module.","solution":"import torch import torch.distributed as dist from torch.distributed.checkpoint import save_state_dict, load_state_dict import os def distributed_save(model, save_dir, rank, world_size): os.makedirs(save_dir, exist_ok=True) state_dict = model.state_dict() keys = list(state_dict.keys()) local_keys = keys[rank::world_size] local_state_dict = {k: state_dict[k] for k in local_keys} save_path = os.path.join(save_dir, f\'checkpoint_rank{rank}.pth\') with open(save_path, \'wb\') as f_out: torch.save(local_state_dict, f_out) def distributed_load(model, save_dir, rank, world_size): state_dict = model.state_dict() for r in range(world_size): load_path = os.path.join(save_dir, f\'checkpoint_rank{r}.pth\') if os.path.exists(load_path): with open(load_path, \'rb\') as f_in: local_state_dict = torch.load(f_in) state_dict.update(local_state_dict) model.load_state_dict(state_dict)"},{"question":"# SMTP Client Implementation You are required to implement a Python class `EmailClient` that uses the `smtplib` module to send emails. This class should be able to: 1. Establish a connection to an SMTP server. 2. Authenticate with the server using a username and password. 3. Send an email to one or multiple recipients. 4. Optionally use TLS encryption for the connection. 5. Handle any exceptions that might occur during the process. # Class Requirements 1. Initialization The class should be initialized with the following parameters: - `host` (str): The hostname of the SMTP server. - `port` (int): The port of the SMTP server (default should be 587 for TLS). - `use_tls` (bool): If True, establish a TLS connection (default should be True). - `username` (str): The username for authentication. - `password` (str): The password for authentication. - `debug` (bool): If True, enable debug output for the SMTP session (default should be False). 2. Methods - `connect()`: Establish a connection to the SMTP server and, if `use_tls` is True, start a TLS session. - `login()`: Authenticate with the SMTP server using the provided username and password. - `send_email(from_addr, to_addrs, subject, body)`: Send an email where: - `from_addr` (str): The sender\'s email address. - `to_addrs` (list): A list of recipient email addresses. - `subject` (str): The subject of the email. - `body` (str): The body of the email. - `disconnect()`: Terminate the SMTP session and close the connection. # Example Usage ```python client = EmailClient(host=\'smtp.example.com\', port=587, use_tls=True, username=\'user\', password=\'pass\', debug=True) try: client.connect() client.login() client.send_email(from_addr=\'sender@example.com\', to_addrs=[\'recipient@example.com\'], subject=\'Test Email\', body=\'This is a test email.\') finally: client.disconnect() ``` # Constraints 1. Ensure proper exception handling for all methods. Specific exceptions to handle include `smtplib.SMTPException`, `smtplib.SMTPAuthenticationError`, and `smtplib.SMTPRecipientsRefused`. 2. The `send_email` method should automatically construct the email headers (From, To, Subject) in the message body. 3. The class should print appropriate messages on success or failure of each operation. # Performance Requirement The methods should be designed to handle network delays properly, and timeouts should be set to ensure that methods do not block indefinitely.","solution":"import smtplib from email.mime.text import MIMEText from email.mime.multipart import MIMEMultipart class EmailClient: def __init__(self, host, port=587, use_tls=True, username=\'\', password=\'\', debug=False): self.host = host self.port = port self.use_tls = use_tls self.username = username self.password = password self.debug = debug self.client = None def connect(self): try: self.client = smtplib.SMTP(self.host, self.port, timeout=10) if self.debug: self.client.set_debuglevel(1) if self.use_tls: self.client.starttls() print(\\"Connected successfully.\\") except smtplib.SMTPException as e: print(f\\"Failed to connect: {e}\\") raise def login(self): try: self.client.login(self.username, self.password) print(\\"Logged in successfully.\\") except smtplib.SMTPAuthenticationError as e: print(f\\"Authentication failed: {e}\\") raise except smtplib.SMTPException as e: print(f\\"Failed to login: {e}\\") raise def send_email(self, from_addr, to_addrs, subject, body): msg = MIMEMultipart() msg[\'From\'] = from_addr msg[\'To\'] = \', \'.join(to_addrs) msg[\'Subject\'] = subject msg.attach(MIMEText(body, \'plain\')) try: self.client.sendmail(from_addr, to_addrs, msg.as_string()) print(\\"Email sent successfully.\\") except smtplib.SMTPRecipientsRefused as e: print(f\\"Recipients refused: {e}\\") raise except smtplib.SMTPException as e: print(f\\"Failed to send email: {e}\\") raise def disconnect(self): if self.client: self.client.quit() print(\\"Disconnected successfully.\\")"},{"question":"Coding Assessment Question # Objective: Demonstrate your ability to work with Python\'s buffer protocol using the modern `PyObject_GetBuffer` and `PyBuffer_Release` functions. # Problem Statement: Implement a Python function `check_buffer_properties(py_obj)` that takes a single argument `py_obj` (a potential Python object) and checks if it supports the buffer protocol and whether it provides a read-only or writable buffer. The function should return a dictionary containing the following information: - `\'supports_buffer\'`: Boolean indicating if the object supports the buffer protocol. - `\'read_only\'`: Boolean indicating if the object provides a read-only buffer. - `\'writable\'`: Boolean indicating if the object provides a writable buffer. # Input: - `py_obj`: A Python object. # Output: - A dictionary with keys `\'supports_buffer\'`, `\'read_only\'`, and `\'writable\'`, each with a Boolean value. # Example: ```python def check_buffer_properties(py_obj): # Your implementation here buffer_properties = check_buffer_properties(bytearray(b\\"example\\")) print(buffer_properties) # Output: {\'supports_buffer\': True, \'read_only\': False, \'writable\': True} ``` # Constraints: 1. You must use `PyObject_GetBuffer` and `PyBuffer_Release` to interact with the buffer protocol. 2. Ensure that you handle exceptions gracefully and don\'t leave acquired resources unreleased. 3. The function should work for any valid Python object. # Performance Requirements: - The function should be efficient and should release resources promptly after use. Your task is to implement the `check_buffer_properties(py_obj)` function, ensuring it meets the requirements outlined above. Python C-API References: - `PyObject_GetBuffer` - `PyBuffer_Release`","solution":"import ctypes class Py_buffer(ctypes.Structure): _fields_ = [ (\'buf\', ctypes.c_void_p), (\'obj\', ctypes.py_object), (\'len\', ctypes.c_ssize_t), (\'itemsize\', ctypes.c_ssize_t), (\'readonly\', ctypes.c_int), (\'ndim\', ctypes.c_int), (\'format\', ctypes.c_char_p), (\'shape\', ctypes.POINTER(ctypes.c_ssize_t)), (\'strides\', ctypes.POINTER(ctypes.c_ssize_t)), (\'suboffsets\', ctypes.POINTER(ctypes.c_ssize_t)), (\'internal\', ctypes.c_void_p) ] libpython = ctypes.PyDLL(None) libpython.PyObject_GetBuffer.argtypes = [ctypes.py_object, ctypes.POINTER(Py_buffer), ctypes.c_int] libpython.PyObject_GetBuffer.restype = ctypes.c_int libpython.PyBuffer_Release.argtypes = [ctypes.POINTER(Py_buffer)] def check_buffer_properties(py_obj): buf = Py_buffer() result = {} try: if libpython.PyObject_GetBuffer(py_obj, ctypes.byref(buf), 0) == -1: result[\'supports_buffer\'] = False result[\'read_only\'] = False result[\'writable\'] = False else: result[\'supports_buffer\'] = True result[\'read_only\'] = bool(buf.readonly) result[\'writable\'] = not buf.readonly except Exception as e: result = {\'supports_buffer\': False, \'read_only\': False, \'writable\': False} finally: if result[\'supports_buffer\']: libpython.PyBuffer_Release(ctypes.byref(buf)) return result"},{"question":"**Question:** Implement a function `custom_pandas_display(df: pd.DataFrame) -> None` that configures pandas to display DataFrames in a certain customized way for the duration of the function execution. The function should: 1. Set maximum rows to 7. 2. Set the minimum rows to display when the maximum is exceeded to 4. 3. Expand the DataFrame representation to stretch across pages if necessary. 4. Justify the DataFrame column headers to the left. 5. Set the precision for decimal numbers to 3. 6. Ensure that at the end of execution, all settings are reverted to their default values. **Constraints:** - You should use pandas\' `option_context` to ensure settings revert to defaults after execution. - Implement this function within 50 lines of code. **Performance Requirements:** - The function should be efficient and should not lead to significant delays, even for DataFrames with thousands of rows and columns. **Input and Output:** - Input: `df` — A pandas DataFrame of any size. - Output: The function does not return anything but should print the DataFrame `df` in the customized format as per the settings. ```python import pandas as pd def custom_pandas_display(df: pd.DataFrame) -> None: # Your implementation here pass # Example usage: df = pd.DataFrame({ \\"Column1\\": [1.234567, 2.345678, 3.456789, 4.567890, 5.678901, 6.789012, 7.890123, 8.901234], \\"Column2\\": [9.012345, 10.123456, 11.234567, 12.345678, 13.456789, 14.567890, 15.678901, 16.789012] }) custom_pandas_display(df) ``` Make sure to validate your implementation by testing it with DataFrames of varying sizes and contents. The display should conform to the specified settings and revert to pandas default settings afterwards.","solution":"import pandas as pd def custom_pandas_display(df: pd.DataFrame) -> None: with pd.option_context( \'display.max_rows\', 7, \'display.min_rows\', 4, \'display.expand_frame_repr\', True, \'display.colheader_justify\', \'left\', \'display.precision\', 3 ): print(df)"},{"question":"# Iterator Management in Python310 Problem Statement You are tasked with implementing a custom iterator class in Cython that should interact seamlessly with Python\'s built-in iterator protocols, making use of the functions provided by python310. Your implementation will include both synchronous and asynchronous iteration capabilities. Requirements 1. **Class Definition**: - Define a class `CustomIterator` that encapsulates a list of integers. - Implement the synchronous iterator protocol using `PyIter_Check` and `PyIter_Next`. 2. **Methods**: - `__iter__()` and `__next__()`: These should adhere to Python\'s iterator protocol. - Additional method `send(value)` to handle the asynchronous protocol using `PyIter_Send`. 3. **Functionality**: - `__iter__` should return the iterator object itself. - `__next__` should return the next integer in the list, raising `StopIteration` when the end of the list is reached. - `send` should send a provided value to the iterator and handle the result according to the result type (`PYGEN_RETURN`, `PYGEN_NEXT`, `PYGEN_ERROR`). Input and Output - **Input**: A list of integers will be provided to the `CustomIterator` class upon initialization. - **Output**: The iterator should iterate through the provided list of integers synchronously and handle sent values asynchronously according to the python310 protocols. Constraints - Your implementation must utilize the `PyIter_Check`, `PyIter_Next`, and `PyIter_Send` functions. - Ensure proper error handling as guided by the provided documentation. - Your solution should handle both synchronous and asynchronous iterations efficiently. Example ```python # Example usage in Python: it = CustomIterator([1, 2, 3, 4]) for value in it: print(value) # Expected Output: # 1 # 2 # 3 # 4 # Asynchronous handling result = it.send(10) print(result) # Should handle and display the result of sending 10 to the iterator ``` Create this class and ensure it adheres to the iterator protocols as described.","solution":"class CustomIterator: def __init__(self, data): if not isinstance(data, list) or not all(isinstance(i, int) for i in data): raise ValueError(\\"CustomIterator requires a list of integers.\\") self.data = data self.index = 0 def __iter__(self): return self def __next__(self): if self.index >= len(self.data): raise StopIteration result = self.data[self.index] self.index += 1 return result def send(self, value): if not isinstance(value, int): raise ValueError(\\"Value to send must be an integer.\\") if self.index >= len(self.data): raise StopIteration # For simplicity in this example, let\'s assume we just add the value to the current element. result = self.data[self.index] + value self.index += 1 return result"},{"question":"# CGI Form Handling and Debugging You are required to create a CGI script that handles form submissions, processes the input data, and provides appropriate feedback to the user. **Task**: 1. Write a CGI script that processes a form with the following fields: - `username` (text input) - `email` (text input) - `age` (text input) - `interests` (checkboxes: \\"Reading\\", \\"Sports\\", \\"Music\\", \\"Travel\\") 2. The script should: - Check that all fields are filled; if any field is missing, respond with a message indicating the missing fields. - Validate that the age field is a number. - Collect and display the values of the `interests` checkboxes, combining all selected options into a comma-separated string. 3. If the form submission is successful and all data is valid: - Print a summary of the data entered by the user. 4. Use the `cgitb` module to enable error reporting for debugging purposes. 5. Ensure the script: - Is robust against empty or malformed inputs. - Uses appropriate CGI headers and correctly forms the HTML response. # Example of how the form might look in HTML: ```html <form action=\\"/cgi-bin/your_script.py\\" method=\\"post\\"> Username: <input type=\\"text\\" name=\\"username\\"><br> Email: <input type=\\"text\\" name=\\"email\\"><br> Age: <input type=\\"text\\" name=\\"age\\"><br> Interests:<br> <input type=\\"checkbox\\" name=\\"interests\\" value=\\"Reading\\"> Reading<br> <input type=\\"checkbox\\" name=\\"interests\\" value=\\"Sports\\"> Sports<br> <input type=\\"checkbox\\" name=\\"interests\\" value=\\"Music\\"> Music<br> <input type=\\"checkbox\\" name=\\"interests\\" value=\\"Travel\\"> Travel<br> <input type=\\"submit\\" value=\\"Submit\\"> </form> ``` # Expected Input: - Form data submitted through an HTTP POST request. # Expected Output: - Valid HTML response displaying either: - A list of entered data if all fields are correctly filled. - An error message specifying any missing or invalid fields. # Constraints: - Use version 3.10+ of Python. - Handle form submission robustness. # Performance Requirements: - The script should execute efficiently within a typical web server\'s CGI handling timeframe.","solution":"#!/usr/bin/env python3 import cgi import cgitb cgitb.enable() def process_form(): form = cgi.FieldStorage() fields = { \'username\': form.getvalue(\'username\', \'\').strip(), \'email\': form.getvalue(\'email\', \'\').strip(), \'age\': form.getvalue(\'age\', \'\').strip(), \'interests\': form.getlist(\'interests\') } missing_fields = [key for key, value in fields.items() if not value and key != \'interests\'] if missing_fields: return f\\"Missing fields: {\', \'.join(missing_fields)}\\" if not fields[\'age\'].isdigit(): return \\"Invalid age. Please enter a numeric value.\\" interests = \\", \\".join(fields[\'interests\']) return f <html> <head> <title>Form Submission</title> </head> <body> <h2>Form Submission Summary</h2> <p><b>Username:</b> {fields[\'username\']}</p> <p><b>Email:</b> {fields[\'email\']}</p> <p><b>Age:</b> {fields[\'age\']}</p> <p><b>Interests:</b> {interests}</p> </body> </html> def main(): print(\\"Content-type: text/htmln\\") print(process_form()) if __name__ == \\"__main__\\": main()"},{"question":"Objective Your task is to write a function in Python that categorizes and counts the occurrences of different error types from a given list of error codes. Function Signature ```python def categorize_error_codes(error_codes: List[int]) -> Dict[str, int]: pass ``` Input - `error_codes`: A list of integers representing error codes. Output - Returns a dictionary where the keys are the symbolic names of the errors (as strings) and the values are the counts of their occurrences in the input list. Constraints - The input list can contain up to 1000 error codes. - All error codes in the input list are valid and belong to the `errno` module. - The function should handle any possible error code defined in the `errno` module. Example ```python >>> error_codes = [errno.ENOENT, errno.EPERM, errno.ENOENT, errno.EINVAL, errno.EPERM] >>> categorize_error_codes(error_codes) { \'ENOENT\': 2, \'EPERM\': 2, \'EINVAL\': 1 } ``` Requirements - Use the `errno` module to map error codes to their symbolic names. - Ensure the function handles all error codes defined in the `errno` module. - Performance of the function should be optimal such that it can handle the maximum input size within reasonable time limits. Hints - You can make use of the `errno.errorcode` dictionary to map numeric error codes to symbolic names. - Consider using collections such as `defaultdict` from the `collections` module to simplify counting occurrences of error codes.","solution":"import errno from typing import List, Dict from collections import defaultdict def categorize_error_codes(error_codes: List[int]) -> Dict[str, int]: Categorizes and counts the occurrences of different error types from a given list of error codes. Parameters: error_codes (List[int]): A list of integers representing error codes. Returns: Dict[str, int]: A dictionary with symbolic error names as keys and their counts as values. error_count = defaultdict(int) for code in error_codes: error_name = errno.errorcode.get(code, \\"UNKNOWN\\") error_count[error_name] += 1 return dict(error_count)"},{"question":"**Question: Implement a Task Manager with Cleanup Functions** You are tasked with implementing a task manager that tracks tasks and performs cleanup operations when the program terminates. The task manager should utilize the `atexit` module to register relevant cleanup functions. # Requirements 1. **Task Addition:** - Implement a function `add_task(task_name: str, task_duration: int)` that adds a task to the task manager. Each task has a name and a duration (in seconds). 2. **Task Removal:** - Implement a function `remove_task(task_name: str)` that removes a task from the task manager if it exists. 3. **Cleanup Operation:** - Implement a function `cleanup_tasks()` that writes all remaining tasks to a file named `tasks_backup.txt` when the program terminates. This function should be registered using the `atexit` module. 4. **Optional Cleanup Registration:** - Implement a function `register_custom_cleanup(func, *args, **kwargs)` that allows registering custom cleanup functions with optional arguments using `atexit`. 5. **Read Tasks from File:** - When the module is imported, it should read the tasks from `tasks_backup.txt` (if the file exists) and initialize the task manager with those tasks. # Constraints - The task manager should be implemented as a singleton to ensure that only one instance manages the tasks throughout the program. # Input and Output Formats - **add_task(task_name: str, task_duration: int)** - Inputs: - `task_name`: A string denoting the name of the task. - `task_duration`: An integer denoting the duration of the task in seconds. - Output: None - **remove_task(task_name: str)** - Input: - `task_name`: A string denoting the name of the task to be removed. - Output: None - **register_custom_cleanup(func, *args, **kwargs)** - Inputs: - `func`: The custom cleanup function to be registered. - `*args`: Positional arguments to be passed to the custom cleanup function. - `**kwargs`: Keyword arguments to be passed to the custom cleanup function. - Output: None - **cleanup_tasks()** - Output: None # Example ```python # Assuming the below code is part of the task manager module add_task(\'Task1\', 120) add_task(\'Task2\', 60) remove_task(\'Task1\') def custom_cleanup(message): print(f\\"Custom cleanup: {message}\\") register_custom_cleanup(custom_cleanup, \'All tasks completed.\') # When the program terminates, cleanup_tasks will be called automatically, # and will write the remaining tasks (in this case, Task2) to tasks_backup.txt. # The custom cleanup function will also be called, printing \\"Custom cleanup: All tasks completed.\\" ``` **Note:** Ensure that the implemented task manager adheres to the requirements and constraints. The solution should be tested to handle various scenarios such as adding, removing tasks, and custom cleanup registrations effectively.","solution":"import atexit import os class TaskManager: _instance = None def __new__(cls): if cls._instance is None: cls._instance = super(TaskManager, cls).__new__(cls) cls._instance.tasks = {} cls._instance._init_from_file() atexit.register(cls._instance.cleanup_tasks) return cls._instance def _init_from_file(self): if os.path.exists(\'tasks_backup.txt\'): with open(\'tasks_backup.txt\', \'r\') as file: for line in file: task_name, task_duration = line.strip().split(\',\') self.tasks[task_name] = int(task_duration) def add_task(self, task_name, task_duration): self.tasks[task_name] = task_duration def remove_task(self, task_name): if task_name in self.tasks: del self.tasks[task_name] def cleanup_tasks(self): with open(\'tasks_backup.txt\', \'w\') as file: for task_name, task_duration in self.tasks.items(): file.write(f\\"{task_name},{task_duration}n\\") def register_custom_cleanup(self, func, *args, **kwargs): atexit.register(func, *args, **kwargs) task_manager = TaskManager() add_task = task_manager.add_task remove_task = task_manager.remove_task register_custom_cleanup = task_manager.register_custom_cleanup"},{"question":"# Linear and Quadratic Discriminant Analysis Implementation You are given a dataset with `n` samples and `d` features, along with the corresponding class labels. Your task is to implement functions that classify the dataset using Linear Discriminant Analysis (LDA) and Quadratic Discriminant Analysis (QDA). The aim is to demonstrate your comprehension of both classifiers\' fundamental and slightly advanced concepts. Input Format - A 2D list `X` of shape `(n, d)` representing the feature vectors of the samples. - A 1D list `y` of length `n` representing the class labels for the samples. Assume `y` contains integer labels. Output Format - A 1D list of length `n` representing the predicted class labels using LDA. - A 1D list of length `n` representing the predicted class labels using QDA. Constraints - 2 ≤ n ≤ 1000 - 1 ≤ d ≤ 50 - You may assume that the dataset follows a Gaussian distribution for QDA. Requirements 1. Implement LDA using scikit-learn. 2. Implement QDA using scikit-learn. 3. For LDA, ensure that if the number of classes is less than `d`, the dimensionality reduction is performed. 4. For QDA, ensure that predictions account for class-specific covariance matrices. Performance - Your solution should handle the maximum constraints efficiently. - Consider the class separation performance and accuracy. Example ```python from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score def lda_classification(X, y): from sklearn.discriminant_analysis import LinearDiscriminantAnalysis lda = LinearDiscriminantAnalysis() lda.fit(X, y) return lda.predict(X) def qda_classification(X, y): from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis qda = QuadraticDiscriminantAnalysis() qda.fit(X, y) return qda.predict(X) # Example usage: X = [[0.5, 1.0], [1.5, 1.8], [1.1, 0.8], [0.3, 0.7]] y = [0, 1, 1, 0] lda_predictions = lda_classification(X, y) qda_predictions = qda_classification(X, y) # Sample Accuracy Check (should be provided with actual test data) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42) print(\\"LDA Accuracy:\\", accuracy_score(y_test, lda_classification(X_train, y_train))) print(\\"QDA Accuracy:\\", accuracy_score(y_test, qda_classification(X_train, y_train))) ``` Provide implementations for the `lda_classification` and `qda_classification` functions. Ensure your code is robust, efficient, and clear.","solution":"def lda_classification(X, y): from sklearn.discriminant_analysis import LinearDiscriminantAnalysis lda = LinearDiscriminantAnalysis() lda.fit(X, y) return lda.predict(X) def qda_classification(X, y): from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis qda = QuadraticDiscriminantAnalysis() qda.fit(X, y) return qda.predict(X)"},{"question":"Custom Pickling with `copyreg` Module One of the key aspects of object serialization with pickle in Python is the ability to handle custom object types that require special logic for both pickling and unpickling. The `copyreg` module provides the necessary functions to define custom serialization behavior for such objects. # Objective You are required to implement a class which contains complex objects that need custom serialization logic. Then, using the `copyreg` module, you will register a custom reduction function for this class to allow it to be properly pickled and unpickled. # Class Definition Requirements 1. Implement a class `D` which holds an attribute that is a dictionary of lists. 2. Implement a custom reduction function `pickle_d` for class `D` which ensures proper serialization of instances of `D`. # Function Registration Requirements 1. Use the `copyreg.pickle` method to register your custom reduction function for the class `D`. 2. Verify that the custom reduction function is invoked during both copying and pickling of an instance of `D`. # Input and Output Formats - **Class Definition**: ```python class D: def __init__(self, data): self.data = data # data is a dictionary of lists ``` - **Reduction Function**: ```python def pickle_d(d): # Implement logic to return necessary information for reconstruction pass ``` # Constraints - The keys of the dictionary are strings. - The values of the dictionary are lists of integers. - You need to ensure that the deserialization process can reconstruct the object to its original form. # Example ```python import copyreg, copy, pickle class D: def __init__(self, data): self.data = data def pickle_d(d): # Custom reduction function return D, (d.data,) # Register the custom reduction function with copyreg copyreg.pickle(D, pickle_d) # Create an instance of D d_instance = D({\\"list1\\": [1, 2, 3], \\"list2\\": [4, 5, 6]}) # Verify custom pickling with copy d_copied = copy.copy(d_instance) # This should trigger the custom reduction function # Verify custom pickling with pickle d_pickled = pickle.dumps(d_instance) # This should trigger the custom reduction function d_unpickled = pickle.loads(d_pickled) # This should reconstruct the original object ``` In this task, you are required to: 1. Complete the implementation of the class `D` and the `pickle_d` function. 2. Register the reduction function using `copyreg.pickle`. 3. Test your implementation by creating an instance, copying it with the `copy` module, and pickling/unpickling it with the `pickle` module to ensure the custom logic is correctly invoked and validated.","solution":"import copyreg import copy import pickle class D: def __init__(self, data): self.data = data # data is a dictionary of lists def pickle_d(d): # Custom reduction function return D, (d.data,) # Register the custom reduction function with copyreg copyreg.pickle(D, pickle_d) # Example usage def example_usage(): # Create an instance of D d_instance = D({\\"list1\\": [1, 2, 3], \\"list2\\": [4, 5, 6]}) # Verify custom pickling with copy d_copied = copy.copy(d_instance) # This should trigger the custom reduction function assert d_copied.data == d_instance.data # Verify custom pickling with pickle d_pickled = pickle.dumps(d_instance) # This should trigger the custom reduction function d_unpickled = pickle.loads(d_pickled) # This should reconstruct the original object assert d_unpickled.data == d_instance.data print(\\"All checks passed.\\") # Run the example usage for demonstration example_usage()"},{"question":"**Question: Implementing a Custom Container Using `collections.abc`** # Objective: Design and implement a custom container class `OrderedSet` which supports set operations and preserves the insertion order of elements. Utilize the appropriate abstract base classes from the `collections.abc` module to ensure your class adheres to the expected interface. # Requirements: 1. Your `OrderedSet` class should inherit from `collections.abc.MutableSet`. 2. The class should implement the following abstract methods: - `__contains__(self, item)`: Check if the item is in the set. - `__iter__(self)`: Return an iterator over the elements in the set. - `__len__(self)`: Return the number of elements in the set. - `add(self, item)`: Add an element to the set. - `discard(self, item)`: Remove an element from the set if it is a member. If the item is not a member, do nothing. # Additional Functionality: 3. Ensure that the set maintains the order of elements as they were added. 4. Override any additional methods from the `MutableSet` mixin if necessary to optimize performance and maintain order. 5. Implement the `__repr__(self)` method to provide a string representation of the set. # Input: - The elements to be added to the `OrderedSet`. # Output: - The string representation of the `OrderedSet` showing elements in insertion order. # Constraints: - The elements added into the set can be of any data type that is hashable. - The class should correctly handle any edge cases, such as adding duplicate elements. # Examples: ```python # Example Usage s = OrderedSet() s.add(1) s.add(2) s.add(1) # Adding duplicate element print(s) # Output: OrderedSet([1, 2]) s.discard(2) print(s) # Output: OrderedSet([1]) print(len(s)) # Output: 1 print(1 in s) # Output: True print(2 in s) # Output: False for item in s: print(item) # Output: 1 ``` # Performance: - Ensure that the basic operations such as insertion, deletion, membership testing, and iteration have a reasonable time complexity (preferably O(1) for average-case time complexities). # Additional Notes: - You may utilize internal data structures like `collections.OrderedDict` to help maintain insertion order if needed. - Remember to include proper docstrings and comments in your code to explain your implementation.","solution":"from collections.abc import MutableSet class OrderedSet(MutableSet): def __init__(self): self._data = {} self._order = [] def __contains__(self, item): return item in self._data def __iter__(self): return iter(self._order) def __len__(self): return len(self._order) def add(self, item): if item not in self._data: self._data[item] = True self._order.append(item) def discard(self, item): if item in self._data: del self._data[item] self._order.remove(item) def __repr__(self): return f\\"OrderedSet({self._order})\\""},{"question":"**Dynamic Importing and Reloading Assessment** **Objective:** Demonstrate your understanding of the \\"importlib\\" package by dynamically importing a module, modifying its contents, reloading it, and verifying the changes. **Problem Statement:** You have been provided a Python module named `example_module` with a single function `greet`. Your task is to: 1. Dynamically import this module using `importlib`. 2. Call the `greet` function and capture its output. 3. Modify the source code of the module to change the behavior of the `greet` function. 4. Reload the modified module. 5. Call the modified `greet` function and capture its new output. 6. Return both the original and modified outputs. **Requirements:** 1. Use `importlib.import_module` to import the module dynamically. 2. Use `importlib.reload` to reload the module after modification. 3. Ensure the module modification is persistent and correctly reflected when reloaded. **Module `example_module.py`:** ```python # example_module.py def greet(): return \\"Hello, World!\\" ``` **Constraints:** 1. The source code modification should programmatically insert the new implementation of the `greet` function where it returns `\\"Hello, Universe!\\"` instead of `\\"Hello, World!\\"`. **Function Signature:** ```python def dynamic_import_and_reload(module_name: str, func_name: str) -> tuple: pass ``` **Input:** - `module_name` (str): The name of the module to import (e.g., \\"example_module\\"). - `func_name` (str): The name of the function to call and modify (e.g., \\"greet\\"). **Output:** - A tuple containing the original output of the `greet` function and the modified output after reloading the module. **Example:** ```python result = dynamic_import_and_reload(\'example_module\', \'greet\') print(result) # Output should be (\\"Hello, World!\\", \\"Hello, Universe!\\") ``` **Steps to Implement the Function:** 1. Import the module dynamically using `importlib.import_module`. 2. Call the function and capture the output. 3. Modify the source code of the module to change the function\'s behavior. 4. Reload the module using `importlib.reload`. 5. Call the modified function and capture the new output. 6. Return both the original and modified outputs as a tuple. **Notes:** - You may assume the `example_module.py` file is located in the same directory as your script. - You should handle any necessary file I/O for modifying the module source code within your function implementation.","solution":"import importlib import os def dynamic_import_and_reload(module_name: str, func_name: str) -> tuple: # Step 1: Import the module dynamically module = importlib.import_module(module_name) # Step 2: Call the function and capture the original output original_output = getattr(module, func_name)() # Step 3: Modify the source code of the module to change the function\'s behavior module_filename = module.__file__ with open(module_filename, \'r\') as file: lines = file.readlines() # Find the function definition and modify it for i in range(len(lines)): if lines[i].strip().startswith(\'def\') and func_name in lines[i]: # The following lines should belong to the function\'s definition lines[i + 1] = \\" return \'Hello, Universe!\'n\\" break # Write the modified lines back to the file with open(module_filename, \'w\') as file: file.writelines(lines) # Step 4: Reload the module using importlib.reload importlib.reload(module) # Step 5: Call the modified function and capture the new output modified_output = getattr(module, func_name)() # Step 6: Return both the original and modified outputs return original_output, modified_output"},{"question":"**Advanced PyTorch: Tuning CUDA Operations** In this task, you are required to write a series of functions to manage the tuning of CUDA operations in PyTorch. You will use the `torch.cuda.tunable` module to perform the following steps: 1. Enable tuning. 2. Check if tuning is enabled. 3. Set a maximum tuning duration and iterations. 4. Perform a simulated tuning operation. 5. Retrieve and save the tuning results. Implement the following functions: 1. `enable_tuning() -> None`: Enables the tuning feature. 2. `is_tuning_enabled() -> bool`: Returns whether tuning is currently enabled. 3. `configure_tuning(max_duration: int, max_iterations: int) -> None`: Sets the maximum tuning duration and the maximum number of tuning iterations. 4. `simulate_tuning(results_filename: str) -> None`: Performs a simulated tuning operation and writes the results to the specified filename. 5. `get_tuning_results() -> dict`: Retrieves the current tuning results. # Function Descriptions 1. **enable_tuning()** - **Input**: None - **Output**: None 2. **is_tuning_enabled()** - **Input**: None - **Output**: *bool* - `True` if tuning is enabled, otherwise `False`. 3. **configure_tuning(max_duration: int, max_iterations: int)** - **Input**: - *max_duration* (int) - The maximum duration for tuning (in milliseconds). - *max_iterations* (int) - The maximum number of tuning iterations. - **Output**: None 4. **simulate_tuning(results_filename: str)** - **Input**: - *results_filename* (str) - The name of the file to write the tuning results to. - **Output**: None - **Operation**: Perform a simulated tuning operation and store the results in the specified file. 5. **get_tuning_results()** - **Input**: None - **Output**: *dict* - The results of the tuning operation as a dictionary. # Example Usage ```python enable_tuning() print(is_tuning_enabled()) # Should print: True configure_tuning(5000, 100) # Set a maximum tuning duration of 5000 ms and 100 iterations simulate_tuning(\'tuning_results.json\') results = get_tuning_results() print(results) ``` # Constraints - Assume all inputs are valid and tuneable CUDA operations are supported on the system. - You may assume the file operations (read/write) succeed. **Note**: This task assumes that the `torch.cuda.tunable` module is installed and operational in your environment. Make sure that CUDA is properly installed and configured.","solution":"import torch import json def enable_tuning() -> None: Enables CUDA tuning. torch.backends.cudnn.benchmark = True def is_tuning_enabled() -> bool: Returns whether CUDA tuning is currently enabled. return torch.backends.cudnn.benchmark def configure_tuning(max_duration: int, max_iterations: int) -> None: Sets the maximum tuning duration and the maximum number of tuning iterations. Parameters: max_duration (int): The maximum duration for tuning in milliseconds. max_iterations (int): The maximum number of tuning iterations. # These functions do not exist in the torch library, so this is a simulated implementation. # In real scenarios, proper configurations would be set here. torch.backends.cudnn.benchmark_duration = max_duration torch.backends.cudnn.benchmark_iterations = max_iterations def simulate_tuning(results_filename: str) -> None: Performs a simulated tuning operation and writes the results to the specified filename. Parameters: results_filename (str): The name of the file to write the tuning results to. # Simulated results as CUDA tuning is complex and highly specific. tuning_results = { \\"duration\\": torch.backends.cudnn.benchmark_duration, \\"iterations\\": torch.backends.cudnn.benchmark_iterations, \\"success\\": True } # Write the results to a file with open(results_filename, \'w\') as file: json.dump(tuning_results, file) def get_tuning_results() -> dict: Retrieves the current tuning results. Returns: dict: The results of the tuning operation. try: results_filename = \'tuning_results.json\' with open(results_filename, \'r\') as file: tuning_results = json.load(file) return tuning_results except FileNotFoundError: return {\\"error\\": \\"Tuning results file not found.\\"}"},{"question":"**Objective:** Demonstrate your understanding of Python\'s development mode and its associated runtime checks by writing a function that reads a file, processes its content, and ensures proper resource management. Additionally, illustrate how to interpret and resolve warnings that might arise from improper resource handling. **Problem Statement:** You are given a path to a text file containing lines of text. Your task is to implement a function `process_file` that: 1. Reads the content of the file. 2. Counts the number of lines that contain non-whitespace characters. 3. Ensures no resource warnings are emitted by the Python interpreter. 4. Logs the content of the file using Python’s logging module. The function signature is: ```python def process_file(file_path: str) -> int: Reads the given text file, counts the number of non-empty lines, logs the content, and ensures proper resource management. pass ``` **Input:** - `file_path: str` - A string representing the path to the text file. **Output:** - `int` - An integer representing the count of non-empty lines in the text file. **Constraints:** - The file at `file_path` is guaranteed to exist and is readable. - Non-empty lines are defined as lines that contain at least one non-whitespace character. **Additional Requirements:** 1. Use the `logging` module to log the content of the file before counting the lines. 2. Ensure that no `ResourceWarning` is emitted by the interpreter when running in Python Development Mode. 3. Consider edge cases such as empty files and files with only whitespace lines. **Example:** ```python # Example file content: # Hello, world! # # This is a sample file. # <- a line with spaces # Given file path to the above file, process_file should return 3 because there are three non-empty lines. count = process_file(\\"example.txt\\") print(count) # Output: 3 ``` To test your implementation, ensure you run the script in Python Development Mode using the following command: ``` python3 -X dev your_script.py ``` Your script should not emit any `ResourceWarning`.","solution":"import logging def process_file(file_path: str) -> int: Reads the given text file, counts the number of non-empty lines, logs the content, and ensures proper resource management. logging.basicConfig(level=logging.INFO, format=\'%(message)s\') logger = logging.getLogger(__name__) non_empty_lines_count = 0 with open(file_path, \'r\') as file: lines = file.readlines() for line in lines: logger.info(line.strip()) if line.strip(): non_empty_lines_count += 1 return non_empty_lines_count"},{"question":"Objective: Implement a set of functions to manipulate and display timezone-aware datetime objects using the `zoneinfo` module. Instructions: 1. **Create a Timezone-Aware Datetime:** ```python def create_timezone_aware_datetime(year: int, month: int, day: int, hour: int, minute: int, tz: str) -> datetime: Create a timezone-aware datetime object for a given timezone. Args: year (int): Year of the datetime. month (int): Month of the datetime. day (int): Day of the datetime. hour (int): Hour of the datetime. minute (int): Minute of the datetime. tz (str): IANA time zone key (e.g., \\"America/Los_Angeles\\"). Returns: datetime: Timezone-aware datetime object. Raises: zoneinfo.ZoneInfoNotFoundError: If the timezone key is not found. ``` 2. **Time Zone Offset Transition Handling:** ```python def transition_handling_datetime(tz: str, dt_before_transition: datetime, dt_after_transition: datetime) -> tuple: Handle the DST (Daylight Saving Time) transition by adjusting the fold attribute of the datetime objects. Args: tz (str): IANA time zone key (e.g., \\"America/Los_Angeles\\"). dt_before_transition (datetime): Datetime object before the DST transition. dt_after_transition (datetime): Datetime object after the DST transition. Returns: tuple: Tuple containing two datetime objects adjusted for the transition. First one with fold=0, second one with fold=1. Raises: zoneinfo.ZoneInfoNotFoundError: If the timezone key is not found. ``` 3. **Timezone Conversion:** ```python def convert_to_timezone(dt: datetime, target_tz: str) -> datetime: Convert a given timezone-aware datetime object to another timezone. Args: dt (datetime): Timezone-aware datetime object. target_tz (str): IANA time zone key of the target timezone (e.g., \\"Europe/London\\"). Returns: datetime: Converted timezone-aware datetime object. Raises: zoneinfo.ZoneInfoNotFoundError: If the timezone key is not found. ``` 4. **Display Datetime Information:** ```python def display_datetime_info(dt: datetime) -> str: Display datetime information in ISO format and its timezone. Args: dt (datetime): Timezone-aware datetime object. Returns: str: String containing the ISO format representation and the timezone. ``` Constraints: - You must use the `ZoneInfo` class from the `zoneinfo` module for all time zone-related operations. - Ensure the functions handle the `ZoneInfoNotFoundError` appropriately if the specified time zone cannot be found. - Do not use any external libraries except for those available in the standard Python library. Example Usage: ```python from datetime import datetime from zoneinfo import ZoneInfo # Create a timezone-aware datetime dt = create_timezone_aware_datetime(2020, 10, 31, 12, 0, \\"America/Los_Angeles\\") print(display_datetime_info(dt)) # Output: 2020-10-31T12:00:00-07:00 [America/Los_Angeles] # Handle DST transition dt_before = datetime(2020, 11, 1, 1, 0, tzinfo=ZoneInfo(\\"America/Los_Angeles\\")) dt_after = datetime(2020, 11, 1, 1, 0, tzinfo=ZoneInfo(\\"America/Los_Angeles\\")) dt_fold_0, dt_fold_1 = transition_handling_datetime(\\"America/Los_Angeles\\", dt_before, dt_after) print(display_datetime_info(dt_fold_0)) # Output: 2020-11-01T01:00:00-07:00 [America/Los_Angeles] print(display_datetime_info(dt_fold_1)) # Output: 2020-11-01T01:00:00-08:00 [America/Los_Angeles] # Convert to another timezone dt_converted = convert_to_timezone(dt, \\"Europe/London\\") print(display_datetime_info(dt_converted)) # Output: 2020-10-31T20:00:00+00:00 [Europe/London] ```","solution":"from datetime import datetime from zoneinfo import ZoneInfo, ZoneInfoNotFoundError def create_timezone_aware_datetime(year: int, month: int, day: int, hour: int, minute: int, tz: str) -> datetime: try: tz_info = ZoneInfo(tz) except ZoneInfoNotFoundError: raise ZoneInfoNotFoundError(f\\"Time zone {tz} not found\\") return datetime(year, month, day, hour, minute, tzinfo=tz_info) def transition_handling_datetime(tz: str, dt_before_transition: datetime, dt_after_transition: datetime) -> tuple: try: tz_info = ZoneInfo(tz) except ZoneInfoNotFoundError: raise ZoneInfoNotFoundError(f\\"Time zone {tz} not found\\") dt_before_transition = dt_before_transition.replace(tzinfo=tz_info, fold=0) dt_after_transition = dt_after_transition.replace(tzinfo=tz_info, fold=1) return (dt_before_transition, dt_after_transition) def convert_to_timezone(dt: datetime, target_tz: str) -> datetime: try: target_tz_info = ZoneInfo(target_tz) except ZoneInfoNotFoundError: raise ZoneInfoNotFoundError(f\\"Target time zone {target_tz} not found\\") return dt.astimezone(target_tz_info) def display_datetime_info(dt: datetime) -> str: return f\\"{dt.isoformat()} [{dt.tzinfo}]\\""},{"question":"Objective Your task is to demonstrate your understanding of the seaborn package by plotting a graph using a given dataset. You will need to use several functionalities of the seaborn library to produce a sophisticated and informative plot. Problem Statement You are provided with a dataset called `iris` which contains information about different features of iris flowers. The dataset has the following columns: - `sepal_length` - `sepal_width` - `petal_length` - `petal_width` - `species` Your goal is to create a plot that visualizes the relationship between `sepal_length` and `sepal_width` for different species of iris flowers while showing confidence intervals and distinct markers for each species. Requirements: 1. Load the `iris` dataset using `seaborn.load_dataset(\\"iris\\")`. 2. Create a scatter plot with `sepal_length` on the x-axis and `sepal_width` on the y-axis. 3. Use different colors and markers for different species of iris flowers. 4. Overlay a line plot that shows the trend of `sepal_length` vs `sepal_width` for each species and includes confidence intervals. 5. Add appropriate labels and a legend to the plot. Input: - You should not take any input from the user. Output: - Display the plot directly within the notebook. Constraints: - Make sure to use seaborn\'s `objects` module for plotting. - The solution should be efficient and avoid redundant operations. # Performance Requirements: - Efficiently load and manipulate the dataset. - Ensure that the plot renders correctly within a reasonable time frame. Example: ```python # Example code snippet to get you started import seaborn.objects as so from seaborn import load_dataset # Load the dataset iris = load_dataset(\\"iris\\") # Create the plot p = so.Plot(iris, x=\\"sepal_length\\", y=\\"sepal_width\\", color=\\"species\\") p.add(so.Dot()) p.add(so.Line(), so.Agg(), group=\\"species\\") p.add(so.Band(), so.Est(), group=\\"species\\") p.add(so.Line(marker=\\"o\\"), so.Agg(), linestyle=None) # Display the plot p.show() ``` Note: - Make sure the plot is properly labeled and the legend clearly distinguishes between different species.","solution":"import seaborn.objects as so from seaborn import load_dataset def plot_iris(): # Load the dataset iris = load_dataset(\\"iris\\") # Create the plot p = so.Plot(iris, x=\\"sepal_length\\", y=\\"sepal_width\\", color=\\"species\\") p.add(so.Dot()) p.add(so.Line(), so.Agg(), group=\\"species\\") p.add(so.Band(), so.Est(), group=\\"species\\") p.add(so.Line(marker=\\"o\\"), so.Agg(), linestyle=None) # Display the plot p.show()"},{"question":"**Objective**: To assess your understanding of class methods, string manipulation, and completion logic in Python. You are tasked with extending the functionality of a simplified `Completer` class. The goal is to implement a method that suggests completions for given text, based on a predefined context of available keywords and objects. Problem Statement Implement a class `SimpleCompleter` which mimics the completion functionality. This class should have: 1. An `__init__` method that initializes the completer with a dictionary containing available keywords and objects (as strings). 2. A `complete` method that takes two parameters: - `text`: a string for which completions are needed. - `state`: an integer representing which suggestion to return. The `complete` method should: - Return the `state`-th completion suggestion for the given `text`. - If `state` exceeds the number of suggestions, return `None`. - Handle both simple word completions and dotted expressions (i.e., module or object attribute completions). Implementation Details 1. **Initialization**: - The constructor takes a dictionary, `context`, where keys are valid identifiers and values are module names or object representations (as strings). 2. **Completion**: - If `text` does not contain a period, suggest completions from the keys of `context` that start with `text`. - If `text` contains a period, handle it to suggest completions for attributes of the object represented by its prefix in `context`. 3. **Error Handling**: - Silently handle exceptions during attribute evaluation by returning `None`. Example Usage ```python class SimpleCompleter: def __init__(self, context): # Constructor to initialize the context dictionary pass def complete(self, text, state): # Method to return the state-th completion for the given text pass # Example context with identifiers and corresponding objects context = { \'readline\': \'module\', \'rlcompleter\': \'module\', \'exampleObject\': \'SomeObject\' } completer = SimpleCompleter(context) # Example completions print(completer.complete(\'read\', 0)) # Output: \'readline\' print(completer.complete(\'rlc\', 0)) # Output: \'rlcompleter\' print(completer.complete(\'exampleObject.\', 0)) # Output: \'exampleObject.some_attribute\' # If state exceeds available suggestions print(completer.complete(\'read\', 2)) # Output: None ``` Constraints - The solution should only rely on built-in Python libraries. - Consider edge cases such as empty text, missing keys in the dictionary, and invalid attribute access. - You may assume the `context` dictionary is relatively small, and do not need to optimize for large-scale performance. **Input Constraints**： - `text` would be a non-empty string. - `state` would be a non-negative integer.","solution":"class SimpleCompleter: def __init__(self, context): Initializes the completer with a dictionary containing available keywords and objects. Parameters: context (dict): Dictionary where keys are valid identifiers, and values are module or object names. self.context = context def complete(self, text, state): Returns the state-th completion suggestion for the given text. Parameters: text (str): String for which completions are needed. state (int): Integer representing which suggestion to return. Returns: str or None: The state-th completion suggestion or None if not available. # Split the text to handle dotted expressions if \'.\' in text: # Handle the dotted expression completion prefix, suffix = text.rsplit(\'.\', 1) if prefix in self.context: try: obj = self.context[prefix] # Get attributes of the object completions = sorted(attr for attr in dir(obj) if attr.startswith(suffix)) except Exception: return None else: completions = [] else: # Handle the simple word completion completions = sorted(key for key in self.context if key.startswith(text)) # Return the appropriate state-th completion if state < len(completions): return completions[state] else: return None"},{"question":"Coding Assessment Question You are provided with a set of utility functions that handle Boolean operations at a low level. Your task is to implement a function that evaluates a list of numbers and returns a new list of Boolean values, where each Boolean indicates whether the corresponding number in the original list is even. # Function Signature ```python def check_even_numbers(numbers: list) -> list: pass ``` # Input - `numbers`: A list of integers. ( (1 leq text{len(numbers)} leq 10^6) ) # Output - A list of Boolean values where each Boolean represents if the corresponding integer in the input list is even (`True` if even, `False` otherwise). # Constraints - Your solution must efficiently handle the evaluation for each number in the list. - You must use the concept of Booleans as documented (i.e., consider how `Py_False` and `Py_True` can be logically represented in Python). # Example ```python check_even_numbers([10, 15, 22, 33, 40]) # Output: [True, False, True, False, True] ``` # Implementation Notes You will use the standard Python Boolean evaluation (`True` or `False`) in your implementation. However, understand the underlying representation as discussed in the provided documentation. # Performance Requirements The implementation must complete in ( O(n) ) time complexity, where ( n ) is the length of the input list.","solution":"def check_even_numbers(numbers: list) -> list: Evaluates a list of numbers and returns a new list of Boolean values. Each Boolean value indicates whether the corresponding number in the original list is even. :param numbers: list of integers, where 1 <= len(numbers) <= 10^6 :return: list of booleans, where each boolean corresponds to whether the respective number is even return [(num % 2 == 0) for num in numbers]"},{"question":"You are tasked with creating an application that manages different types of geometric shapes. To enforce a common interface among all shapes and provide specific implementations for each shape type, you will use abstract base classes. Requirements: 1. **Create an abstract base class `Shape` using `abc.ABC`**. 2. **Define the following abstract methods in `Shape`**: - `area()`: Should return the area of the shape. - `perimeter()`: Should return the perimeter of the shape. 3. **Create two concrete classes `Rectangle` and `Circle` that inherit from `Shape`**. - For `Rectangle`: - Initialize with `width` and `height`. - Implement `area()` to return `width * height`. - Implement `perimeter()` to return `2 * (width + height)`. - For `Circle`: - Initialize with `radius`. - Implement `area()` to return `π * radius^2`. - Implement `perimeter()` to return `2 * π * radius`. - Use `math.pi` for π. 4. **Write a function `shape_details(shape: Shape)` that takes a `Shape` object and returns a string with the format**: ``` \\"Area: <area>, Perimeter: <perimeter>\\" ``` Constraints: - Use the `abc` module for defining the abstract class and decorators. - The `shape_details` function should only accept instances of classes that derive from `Shape`. Example: ```python from abc import ABC, abstractmethod import math class Shape(ABC): @abstractmethod def area(self): pass @abstractmethod def perimeter(self): pass class Rectangle(Shape): def __init__(self, width, height): self.width = width self.height = height def area(self): return self.width * self.height def perimeter(self): return 2 * (self.width + self.height) class Circle(Shape): def __init__(self, radius): self.radius = radius def area(self): return math.pi * self.radius ** 2 def perimeter(self): return 2 * math.pi * self.radius def shape_details(shape: Shape): return f\\"Area: {shape.area()}, Perimeter: {shape.perimeter()}\\" # Test Cases rect = Rectangle(3, 4) cir = Circle(5) print(shape_details(rect)) # Output: \\"Area: 12, Perimeter: 14\\" print(shape_details(cir)) # Output: \\"Area: 78.53981633974483, Perimeter: 31.41592653589793\\" ``` Your task is to complete the implementation as per the requirements and ensure that the solution meets the constraints.","solution":"from abc import ABC, abstractmethod import math class Shape(ABC): @abstractmethod def area(self): pass @abstractmethod def perimeter(self): pass class Rectangle(Shape): def __init__(self, width, height): self.width = width self.height = height def area(self): return self.width * self.height def perimeter(self): return 2 * (self.width + self.height) class Circle(Shape): def __init__(self, radius): self.radius = radius def area(self): return math.pi * self.radius ** 2 def perimeter(self): return 2 * math.pi * self.radius def shape_details(shape: Shape): return f\\"Area: {shape.area()}, Perimeter: {shape.perimeter()}\\""},{"question":"Objective: Implement a function that utilizes `difflib.SequenceMatcher` to compare two text files and return a detailed report of the differences between them. The report should include: - The ratio of similarities between the two files. - A list of matching blocks and their positions. - A list of differences with detailed opcodes (including \'replace\', \'delete\', \'insert\', and \'equal\'). Function Signature ```python def compare_files(file1_path: str, file2_path: str) -> dict: Compare two text files and return a detailed report of their differences. Parameters: - file1_path: str : Path to the first file to be compared. - file2_path: str : Path to the second file to be compared. Returns: - dict : A dictionary containing: - \'similarity_ratio\': float : Ratio of similarities between the two files. - \'matching_blocks\': List of tuples : List of matching blocks with their positions. - \'opcodes\': List of tuples : List of opcodes with details. pass ``` Expected Input and Output: 1. **Input**: Paths to two text files. 2. **Output**: A dictionary with the similarity ratio, matching blocks, and opcodes. Example: Suppose `file1.txt` contains: ``` line1 line2 line3 ``` And `file2.txt` contains: ``` line1 lineX line3 ``` Calling `compare_files(\'file1.txt\', \'file2.txt\')` should return something like: ```python { \'similarity_ratio\': 0.8, # Example ratio \'matching_blocks\': [(0, 0, 1), (2, 2, 1), (3, 3, 0)], \'opcodes\': [ (\'equal\', 0, 1, 0, 1), (\'replace\', 1, 2, 1, 2), (\'equal\', 2, 3, 2, 3) ] } ``` Constraints: - The elements of both sequences must be hashable. - Assume the files are small enough to fit into memory. - The function should handle any junk elements (e.g., whitespace or blank lines), considering them in comparisons. Performance Requirements: - The function should be optimized for small to medium-sized text files with reasonable runtime for typical diff operations. Implement the `compare_files` function to achieve the above requirements.","solution":"import difflib def compare_files(file1_path: str, file2_path: str) -> dict: Compare two text files and return a detailed report of their differences. Parameters: - file1_path: str : Path to the first file to be compared. - file2_path: str : Path to the second file to be compared. Returns: - dict : A dictionary containing: - \'similarity_ratio\': float : Ratio of similarities between the two files. - \'matching_blocks\': List of tuples : List of matching blocks with their positions. - \'opcodes\': List of tuples : List of opcodes with details. with open(file1_path, \'r\') as file1, open(file2_path, \'r\') as file2: file1_lines = file1.readlines() file2_lines = file2.readlines() sequence_matcher = difflib.SequenceMatcher(None, file1_lines, file2_lines) similarity_ratio = sequence_matcher.ratio() matching_blocks = sequence_matcher.get_matching_blocks() opcodes = sequence_matcher.get_opcodes() return { \'similarity_ratio\': similarity_ratio, \'matching_blocks\': matching_blocks, \'opcodes\': opcodes }"},{"question":"# URL Processing with `urllib.parse` Problem Statement You are required to write a function `process_url(url: str) -> tuple` that takes a URL string as input and returns a tuple containing specific pieces of information extracted from that URL. Alongside this, you will implement another function `construct_url(base_url: str, additional_path: str, query_params: dict) -> str` that generates a new URL based on the given base URL, additional path, and query parameters. Details: 1. **Function 1: `process_url(url: str) -> tuple`** - This function should parse the given URL and return a tuple with the following elements: - The scheme of the URL. - The network location (netloc) of the URL. - The path of the URL. - The query parameters as a dictionary. - The fragment identifier. 2. **Function 2: `construct_url(base_url: str, additional_path: str, query_params: dict) -> str`** - This function should construct a full URL based on the following: - The `base_url` which provides the scheme and netloc. - The `additional_path` which should be appended to the path of the base URL. - The `query_params` dictionary which should be converted into a query string and appended to the constructed URL. - Ensure that the generated URL is correctly encoded for any special characters in the path and query parameters. Example: ```python def process_url(url: str) -> tuple: from urllib.parse import urlparse, parse_qs parsed_url = urlparse(url) query_dict = parse_qs(parsed_url.query) return (parsed_url.scheme, parsed_url.netloc, parsed_url.path, query_dict, parsed_url.fragment) def construct_url(base_url: str, additional_path: str, query_params: dict) -> str: from urllib.parse import urljoin, urlencode new_path = urljoin(base_url, additional_path) query_string = urlencode(query_params, doseq=True) return f\\"{new_path}?{query_string}\\" ``` Constraints: - The input URL will always be a valid URL. - The `query_params` dictionary may contain lists as values. - Special characters in paths and query parameters need proper encoding. Testing Your Solution: Test your functions with different inputs to ensure they handle various edge cases correctly, such as: - URLs with or without fragments. - URLs with different schemes (http, https). - Various types of query parameters, including those with special characters. - Combining paths with and without leading/trailing slashes.","solution":"def process_url(url: str) -> tuple: Parses the given URL and returns a tuple containing the scheme, netloc, path, query parameters as a dictionary, and fragment. from urllib.parse import urlparse, parse_qs parsed_url = urlparse(url) query_dict = parse_qs(parsed_url.query) return (parsed_url.scheme, parsed_url.netloc, parsed_url.path, query_dict, parsed_url.fragment) def construct_url(base_url: str, additional_path: str, query_params: dict) -> str: Constructs a URL from the base URL, additional path, and query parameters. from urllib.parse import urljoin, urlencode, urlsplit, urlunsplit # Ensure additional_path is correctly joined with the base URL new_url = urljoin(base_url, additional_path) # Parse the new URL so we can manipulate its components parsed_url = urlsplit(new_url) # Encode the query parameters query_string = urlencode(query_params, doseq=True) # Rebuild the URL with the new path and query string constructed_url = urlunsplit((parsed_url.scheme, parsed_url.netloc, parsed_url.path, query_string, \'\')) return constructed_url"},{"question":"**Question: Nested and Custom Encodings** Implement a function `custom_nested_encoding(data: bytes) -> dict` that performs a series of nested encodings on the input `data`. The function should return a dictionary with the following keys and the corresponding encoded data: 1. `base64_standard`: Base64-encode the original data using the standard Base64 alphabet. 2. `base64_urlsafe`: Base64-encode the original data using the URL- and filesystem-safe alphabet. 3. `base32_standard`: Base32-encode the original data. 4. `custom_combined`: Perform a custom encoding by first Base16-encoding the data, then Base32-encode the result, and finally Base64-encode the Base32-encoded string. Additionally, implement a function `custom_nested_decoding(encoded_data: dict) -> bytes` that reverses the encoding process for the `custom_combined` key, returning the original `data`. **Function Signatures:** ```python def custom_nested_encoding(data: bytes) -> dict: ... def custom_nested_decoding(encoded_data: dict) -> bytes: ... ``` **Example:** ```python data = b\'This is a test data\' encoded_dict = custom_nested_encoding(data) # Example output (values are illustrative) # encoded_dict = { # \\"base64_standard\\": b\'VGhpcyBpcyBhIHRlc3QgZGF0YQ==\', # \\"base64_urlsafe\\": b\'VGhpcyBpcyBhIHRlc3QgZGF0YQ==\', # \\"base32_standard\\": b\'KRUGS4ZANFZSAYJAORZSA4DZEBJWGY3X\', # \\"custom_combined\\": b\'U5ZZ6X22DEYQS5KZMW4VMSVDKFUWP===\', # } decoded_data = custom_nested_decoding(encoded_dict) # decoded_data should be b\'This is a test data\' ``` **Constraints and Limitations:** - Your solution should handle the data conversion accurately even when multiple encodings are nested. - Assume valid input data for encoding and decoding processes. - You can make use of the `base64` module as described in the provided documentation. - Performance considerations are secondary, but the solution should not have excessive overhead.","solution":"import base64 def custom_nested_encoding(data: bytes) -> dict: Perform a series of nested encodings on the input data. encoded_data = {} # Base64 standard encoding base64_standard = base64.b64encode(data) encoded_data[\'base64_standard\'] = base64_standard # Base64 URL-safe encoding base64_urlsafe = base64.urlsafe_b64encode(data) encoded_data[\'base64_urlsafe\'] = base64_urlsafe # Base32 standard encoding base32_standard = base64.b32encode(data) encoded_data[\'base32_standard\'] = base32_standard # Custom combined encoding base16_encoded = base64.b16encode(data) # Step 1: Base16 encoding base32_encoded = base64.b32encode(base16_encoded) # Step 2: Base32 encoding custom_combined = base64.b64encode(base32_encoded) # Step 3: Base64 encoding encoded_data[\'custom_combined\'] = custom_combined return encoded_data def custom_nested_decoding(encoded_data: dict) -> bytes: Reverse the custom combined encoding process to get the original data. custom_combined = encoded_data[\'custom_combined\'] # Step 1: Base64 decode base32_encoded = base64.b64decode(custom_combined) # Step 2: Base32 decode base16_encoded = base64.b32decode(base32_encoded) # Step 3: Base16 decode original_data = base64.b16decode(base16_encoded) return original_data"},{"question":"**Asynchronous Chat Server and Client Using `asyncore`** **Problem Statement:** You are required to implement a simple chat server and a corresponding chat client using the `asyncore` module in Python. The goal is to create a server that can handle multiple clients simultaneously and allow them to send messages to each other in real-time, simulating a basic chat room. **Requirements:** 1. **Chat Server (`ChatServer`)**: - Inherits from `asyncore.dispatcher`. - Listens for incoming connections on a specified host and port. - Accepts connections from clients and spawns a new handler (`ChatHandler`) for each connected client. 2. **Chat Handler (`ChatHandler`)**: - Inherits from `asyncore.dispatcher_with_send`. - Manages communication with a single connected client. - Receives messages from the client and broadcasts them to all other connected clients. 3. **Chat Client (`ChatClient`)**: - Inherits from `asyncore.dispatcher`. - Connects to the chat server on a specified host and port. - Sends user input from the standard input to the server. - Receives and displays messages from the server. **Constraints:** - The server should be able to handle up to 10 concurrent clients. - The clients should automatically reconnect if the connection is lost. - Only text messages are allowed, and each message should be no longer than 1024 characters. **Input and Output Formats:** - The chat server should display connection information for each client and messages being broadcasted. - The chat client should display messages received from the server. **Performance Requirements:** - The server should be efficient in handling multiple connections and ensuring messages are delivered promptly with minimal latency. **Example Usage:** ```python # Starting the server server = ChatServer(\'localhost\', 5050) asyncore.loop() # Starting a client client = ChatClient(\'localhost\', 5050) asyncore.loop() ``` Your task is to implement the `ChatServer`, `ChatHandler`, and `ChatClient` classes using the `asyncore` module given the above specifications. **Note:** - Make sure to handle connections, disconnections, message broadcasting, and error handling appropriately. - Provide a README file with instructions on how to run the server and client.","solution":"import asyncore import socket class ChatServer(asyncore.dispatcher): def __init__(self, host, port): asyncore.dispatcher.__init__(self) self.create_socket(socket.AF_INET, socket.SOCK_STREAM) self.set_reuse_addr() self.bind((host, port)) self.listen(5) self.clients = [] def handle_accept(self): pair = self.accept() if pair is not None: sock, addr = pair print(f\'Incoming connection from {addr}\') self.clients.append(ChatHandler(sock, self)) def broadcast(self, message, origin): for client in self.clients: if client is not origin: client.send(message) class ChatHandler(asyncore.dispatcher_with_send): def __init__(self, sock, server): asyncore.dispatcher_with_send.__init__(self, sock) self.server = server def handle_read(self): data = self.recv(1024) if data: print(f\'Received message: {data}\') self.server.broadcast(data, self) def handle_close(self): self.server.clients.remove(self) self.close() class ChatClient(asyncore.dispatcher): def __init__(self, host, port): asyncore.dispatcher.__init__(self) self.create_socket(socket.AF_INET, socket.SOCK_STREAM) self.connect((host, port)) def handle_read(self): data = self.recv(1024) if data: print(data.decode(\'utf-8\')) def writable(self): return False def send_message(self, message): self.send(message.encode(\'utf-8\')) def handle_close(self): self.close()"},{"question":"Topic: Implementing Principal Component Analysis and Non-negative Matrix Factorization with Reconstruction Description: In this task, you are required to demonstrate your understanding of Principal Component Analysis (PCA) and Non-negative Matrix Factorization (NMF) by performing dimensionality reduction and reconstruction on a given dataset. The overall objective is to reduce the dimensionality of the dataset using PCA, transform it back, and compare it with the reconstruction obtained through NMF to understand the differences. Requirements: 1. **Load the Dataset**: - Use the `load_digits` dataset from `sklearn.datasets` which contains 8x8 images of digits. 2. **Implement PCA**: - Reduce the dimensionality of the dataset to 16 components using PCA. - Reconstruct the original dataset from the reduced components. 3. **Implement NMF**: - Reduce the dimensionality of the dataset to 16 components using NMF. - Reconstruct the original dataset from the reduced components. 4. **Measure Reconstruction Error**: - Compute the mean squared error between the original dataset and both the reconstructed datasets (from PCA and NMF). 5. **Plot Results**: - Display the original, PCA reconstructed, and NMF reconstructed images for the first 10 samples in the dataset. Input Format: The implementation will not require external input; the `load_digits` dataset from `sklearn.datasets` should be used. Output Format: - Print the mean squared error for both reconstructions (PCA and NMF). - Display the original, PCA reconstructed, and NMF reconstructed images for the first 10 samples using `matplotlib`. Constraints: - Use `n_components=16` for both PCA and NMF. - Ensure non-negative inputs for NMF by normalizing the dataset values to the range [0, 1]. Performance Requirement: - The solution must handle the data efficiently without running into memory issues. ```python from sklearn.datasets import load_digits from sklearn.decomposition import PCA, NMF import numpy as np import matplotlib.pyplot as plt from sklearn.preprocessing import MinMaxScaler from sklearn.metrics import mean_squared_error # Load the dataset digits = load_digits() data = digits.data # Normalize data for NMF scaler = MinMaxScaler() data_normalized = scaler.fit_transform(data) # Apply PCA pca = PCA(n_components=16) data_pca = pca.fit_transform(data) data_pca_reconstructed = pca.inverse_transform(data_pca) # Apply NMF nmf = NMF(n_components=16, init=\'random\', random_state=0) data_nmf = nmf.fit_transform(data_normalized) data_nmf_reconstructed = nmf.inverse_transform(data_nmf) # Compute reconstruction error mse_pca = mean_squared_error(data, data_pca_reconstructed) mse_nmf = mean_squared_error(data_normalized, data_nmf_reconstructed) print(f\'Reconstruction error (PCA): {mse_pca}\') print(f\'Reconstruction error (NMF): {mse_nmf}\') # Plot results fig, axs = plt.subplots(10, 3, figsize=(10, 20)) for i in range(10): # Original image axs[i, 0].imshow(data[i].reshape(8, 8), cmap=\'gray\') axs[i, 0].set_title(\'Original\') axs[i, 0].axis(\'off\') # PCA reconstructed image axs[i, 1].imshow(data_pca_reconstructed[i].reshape(8, 8), cmap=\'gray\') axs[i, 1].set_title(\'PCA Reconstructed\') axs[i, 1].axis(\'off\') # NMF reconstructed image axs[i, 2].imshow(data_nmf_reconstructed[i].reshape(8, 8), cmap=\'gray\') axs[i, 2].set_title(\'NMF Reconstructed\') axs[i, 2].axis(\'off\') plt.tight_layout() plt.show() ``` Note: - Ensure all libraries (`numpy`, `scikit-learn`, `matplotlib`) are installed in your environment before running the code.","solution":"from sklearn.datasets import load_digits from sklearn.decomposition import PCA, NMF import numpy as np import matplotlib.pyplot as plt from sklearn.preprocessing import MinMaxScaler from sklearn.metrics import mean_squared_error def perform_pca_nmf_analysis(): # Load the dataset digits = load_digits() data = digits.data # Normalize data for NMF scaler = MinMaxScaler() data_normalized = scaler.fit_transform(data) # Apply PCA pca = PCA(n_components=16) data_pca = pca.fit_transform(data) data_pca_reconstructed = pca.inverse_transform(data_pca) # Apply NMF nmf = NMF(n_components=16, init=\'random\', random_state=0) data_nmf = nmf.fit_transform(data_normalized) data_nmf_reconstructed = nmf.inverse_transform(data_nmf) # Compute reconstruction error mse_pca = mean_squared_error(data, data_pca_reconstructed) mse_nmf = mean_squared_error(data_normalized, data_nmf_reconstructed) print(f\'Reconstruction error (PCA): {mse_pca:.4f}\') print(f\'Reconstruction error (NMF): {mse_nmf:.4f}\') # Plot results fig, axs = plt.subplots(10, 3, figsize=(10, 20)) for i in range(10): # Original image axs[i, 0].imshow(data[i].reshape(8, 8), cmap=\'gray\') axs[i, 0].set_title(\'Original\') axs[i, 0].axis(\'off\') # PCA reconstructed image axs[i, 1].imshow(data_pca_reconstructed[i].reshape(8, 8), cmap=\'gray\') axs[i, 1].set_title(\'PCA Reconstructed\') axs[i, 1].axis(\'off\') # NMF reconstructed image axs[i, 2].imshow(data_nmf_reconstructed[i].reshape(8, 8), cmap=\'gray\') axs[i, 2].set_title(\'NMF Reconstructed\') axs[i, 2].axis(\'off\') plt.tight_layout() plt.show() # Return MSEs for unit testing return mse_pca, mse_nmf"},{"question":"**Coding Assessment Question: Command-Line File Conversion Tool** **Description:** Design and implement a Python script that uses the `optparse` module to create a command-line tool for converting text files between different formats. This tool should support the following functionalities: 1. **Input and Output Files**: The user can specify the input file (`-i` or `--input`) and output file (`-o` or `--output`). 2. **Conversion Type**: The user can specify the conversion type (`-t` or `--type`), with options being `upper` (convert text to uppercase), `lower` (convert text to lowercase), or `title` (convert text to title case). 3. **Verbose Mode**: The user can enable verbose mode (`-v` or `--verbose`), which prints status messages during the conversion. 4. **Custom Functionality**: The tool should allow the user to define a custom conversion function via a callback option (`-c` or `--custom`). The callback should take a string and return a transformed string. For improved usability, group the conversion options (`-t` and `-c`) into their own \\"Conversion Options\\" group in the help message. **Requirements:** 1. Define all the necessary options using `optparse`. 2. Implement the conversion logic based on the specified options. 3. Ensure appropriate error handling and help message generation. **Constraints:** - The input and output files must be valid and accessible text files. - The conversion type must be one of the specified options or a custom function. - The custom function provided with the `-c` or `--custom` option should be a valid Python function accessible in the script’s environment. **Example Usage:** ```sh # Convert a file to uppercase python convert_tool.py -i input.txt -o output.txt -t upper # Convert a file to lowercase with verbose output python convert_tool.py -i input.txt -o output.txt -t lower -v # Use a custom conversion function (e.g., reverse the text) python convert_tool.py -i input.txt -o output.txt -c custom_reverse_function # Print the help message python convert_tool.py -h ``` **Starter Code:** ```python from optparse import OptionParser, OptionGroup import os def custom_reverse_function(text): return text[::-1] def main(): # Step 1: Create OptionParser instance usage = \\"usage: %prog [options] -i INPUT -o OUTPUT\\" parser = OptionParser(usage=usage) # Step 2: Define basic options (input, output, verbose) parser.add_option(\\"-i\\", \\"--input\\", dest=\\"input_file\\", help=\\"input file\\", metavar=\\"FILE\\") parser.add_option(\\"-o\\", \\"--output\\", dest=\\"output_file\\", help=\\"output file\\", metavar=\\"FILE\\") parser.add_option(\\"-v\\", \\"--verbose\\", action=\\"store_true\\", dest=\\"verbose\\", help=\\"enable verbose mode\\") # Step 3: Define conversion options group conversion_group = OptionGroup(parser, \\"Conversion Options\\") conversion_group.add_option(\\"-t\\", \\"--type\\", dest=\\"conversion_type\\", type=\\"choice\\", choices=[\\"upper\\", \\"lower\\", \\"title\\"], help=\\"conversion type (upper, lower, title)\\") conversion_group.add_option(\\"-c\\", \\"--custom\\", dest=\\"custom_function\\", help=\\"use custom conversion function\\") parser.add_option_group(conversion_group) # Step 4: Parse command-line arguments (options, args) = parser.parse_args() # Step 5: Check mandatory options if not options.input_file or not options.output_file: parser.error(\\"Input and output files are required\\") # Step 6: Implement conversion logic if options.verbose: print(f\\"Reading input file: {options.input_file}\\") with open(options.input_file, \'r\') as infile: content = infile.read() if options.conversion_type: if options.conversion_type == \\"upper\\": converted_content = content.upper() elif options.conversion_type == \\"lower\\": converted_content = content.lower() elif options.conversion_type == \\"title\\": converted_content = content.title() elif options.custom_function: custom_func = globals().get(options.custom_function) if not custom_func: parser.error(f\\"Custom function \'{options.custom_function}\' not found\\") converted_content = custom_func(content) else: parser.error(\\"No conversion type specified\\") if options.verbose: print(f\\"Writing output file: {options.output_file}\\") with open(options.output_file, \'w\') as outfile: outfile.write(converted_content) if options.verbose: print(\\"Conversion completed successfully\\") if __name__ == \'__main__\': main() ``` **Explanation:** 1. The script uses `optparse` to handle command-line options. 2. Basic options (`input`, `output`, `verbose`) are defined. 3. A separate option group is created for conversion-related options. 4. The script handles various conversion types and custom functions. 5. Proper error handling and verbose output are implemented. Test and ensure that your tool works as expected for all specified options and scenarios.","solution":"from optparse import OptionParser, OptionGroup def custom_reverse_function(text): return text[::-1] def main(): # Step 1: Create OptionParser instance usage = \\"usage: %prog [options] -i INPUT -o OUTPUT\\" parser = OptionParser(usage=usage) # Step 2: Define basic options (input, output, verbose) parser.add_option(\\"-i\\", \\"--input\\", dest=\\"input_file\\", help=\\"input file\\", metavar=\\"FILE\\") parser.add_option(\\"-o\\", \\"--output\\", dest=\\"output_file\\", help=\\"output file\\", metavar=\\"FILE\\") parser.add_option(\\"-v\\", \\"--verbose\\", action=\\"store_true\\", dest=\\"verbose\\", help=\\"enable verbose mode\\") # Step 3: Define conversion options group conversion_group = OptionGroup(parser, \\"Conversion Options\\") conversion_group.add_option(\\"-t\\", \\"--type\\", dest=\\"conversion_type\\", type=\\"choice\\", choices=[\\"upper\\", \\"lower\\", \\"title\\"], help=\\"conversion type (upper, lower, title)\\") conversion_group.add_option(\\"-c\\", \\"--custom\\", dest=\\"custom_function\\", help=\\"use custom conversion function\\") parser.add_option_group(conversion_group) # Step 4: Parse command-line arguments (options, args) = parser.parse_args() # Step 5: Check mandatory options if not options.input_file or not options.output_file: parser.error(\\"Input and output files are required\\") # Step 6: Implement conversion logic if options.verbose: print(f\\"Reading input file: {options.input_file}\\") with open(options.input_file, \'r\') as infile: content = infile.read() if options.conversion_type: if options.conversion_type == \\"upper\\": converted_content = content.upper() elif options.conversion_type == \\"lower\\": converted_content = content.lower() elif options.conversion_type == \\"title\\": converted_content = content.title() elif options.custom_function: custom_func = globals().get(options.custom_function) if not custom_func: parser.error(f\\"Custom function \'{options.custom_function}\' not found\\") converted_content = custom_func(content) else: parser.error(\\"No conversion type specified\\") if options.verbose: print(f\\"Writing output file: {options.output_file}\\") with open(options.output_file, \'w\') as outfile: outfile.write(converted_content) if options.verbose: print(\\"Conversion completed successfully\\") if __name__ == \'__main__\': main()"},{"question":"**Objective:** You are required to demonstrate your understanding of the seaborn package by loading, preprocessing data, and presenting it using a complex seaborn plot with customization. **Details:** 1. **Dataset:** - Load the dataset named `healthexp` using seaborn\'s `load_dataset` function. - Perform the following preprocessing steps: - Pivot the data so that the index is `Year`, columns are `Country`, and values are `Spending_USD`. - Interpolate any missing values in the dataset. - Convert the pivoted data into a stack, renaming the stack to `Spending_USD`. - Reset the index and sort by `Country`. 2. **Visualization:** - Construct a facet grid plot using the `so.Plot` class where: - `x` is the `Year`. - `y` is `Spending_USD`. - Facet by `Country` with `wrap=3`. - Add an area plot with: - Color mapped to `Country`. - Edge color also mapped to `Country`. - Edge width set to `2`. - Additionally, overlay a line plot on the same plot. - Ensure the plot uses stacked orientation to demonstrate part-whole relationships. **Constraints:** - Use seaborn 0.11.2 or later. - Handle any missing data appropriately as described in preprocessing steps. - Follow the specified preprocessing steps exactly to ensure consistent results. **Requirements:** - The code should be efficient and well-commented. - The plot should be clear and informative, with appropriate labels and legend. **Expected Output:** - A facet grid plot visualizing spending over years for different countries with the specified customizations. Here is an example template to guide your implementation: ```python import seaborn.objects as so from seaborn import load_dataset import pandas as pd # Load and preprocess the dataset healthexp = ( load_dataset(\\"healthexp\\") .pivot(index=\\"Year\\", columns=\\"Country\\", values=\\"Spending_USD\\") .interpolate() .stack() .rename(\\"Spending_USD\\") .reset_index() .sort_values(\\"Country\\") ) # Construct the plot p = so.Plot(healthexp, x=\\"Year\\", y=\\"Spending_USD\\").facet(\\"Country\\", wrap=3) # Add customized area and line plots p.add(so.Area(color=\\"Country\\", edgewidth=2), edgecolor=\\"Country\\").add(so.Line()) # Display the plot p.show() ``` Ensure to analyze each step before implementing the solution. Good luck!","solution":"import seaborn.objects as so from seaborn import load_dataset import pandas as pd # Load and preprocess the dataset def preprocess_healthexp(): healthexp = load_dataset(\\"healthexp\\") # Pivot the data healthexp_pivoted = healthexp.pivot(index=\\"Year\\", columns=\\"Country\\", values=\\"Spending_USD\\") # Interpolate any missing values healthexp_interpolated = healthexp_pivoted.interpolate() # Convert the pivoted data into a stack and rename the stack healthexp_stacked = healthexp_interpolated.stack().rename(\\"Spending_USD\\") # Reset the index and sort by Country healthexp_preprocessed = healthexp_stacked.reset_index().sort_values(\\"Country\\") return healthexp_preprocessed def create_plot(healthexp): # Construct the plot p = so.Plot(healthexp, x=\\"Year\\", y=\\"Spending_USD\\").facet(\\"Country\\", wrap=3) # Add customized area and line plots p.add(so.Area(color=\\"Country\\", edgewidth=2), edgecolor=\\"Country\\").add(so.Line()) return p # Preprocessing data healthexp_preprocessed = preprocess_healthexp() # Creating the plot plot = create_plot(healthexp_preprocessed) # Display the plot plot.show()"},{"question":"# Sparse Tensor Manipulation and Operations You are required to demonstrate your understanding of the sparse functionalities in PyTorch by completing the following tasks: Task 1: Creating Sparse Tensors 1. Create a 2D dense tensor `dense_tensor` of size (4, 4) with the following values: ``` [[0, 0, 3, 0], [4, 0, 0, 0], [0, 0, 0, 5], [0, 6, 0, 0]] ``` 2. Convert `dense_tensor` to a sparse COO tensor `sparse_coo` and a sparse CSR tensor `sparse_csr`. Task 2: Performing Operations on Sparse Tensors 1. For the COO tensor created in Task 1, perform the following operations: - Multiply it by a dense vector `[1, 2, 3, 4]` and provide the result. - Add it to a dense tensor of the same shape `[[1, 0, 1, 0], [1, 2, 0, 0], [0, 1, 0, 1], [1, 0, 1, 0]]` and convert the result back to a sparse COO tensor. 2. For the CSR tensor created in Task 1, perform a sparse matrix multiplication by another dense matrix: ``` [[1, 2], [3, 4], [5, 6], [7, 8]] ``` Provide the resulting dense matrix. Input and Output - **Input**: - None - **Output**: - Task 1: Print the `sparse_coo` and `sparse_csr`. - Task 2: Print the result of the multiplication and addition operations as described. Constraints - Ensure you use PyTorch for creating and converting tensor formats. - Handle operations appropriately, ensuring compatibility between sparse and dense tensors. ```python import torch # Task 1: Creating Sparse Tensors print(\\"Task 1:\\") dense_tensor = torch.tensor([[0, 0, 3, 0], [4, 0, 0, 0], [0, 0, 0, 5], [0, 6, 0, 0]], dtype=torch.float32) # Convert to sparse COO sparse_coo = dense_tensor.to_sparse() print(\\"Sparse COO Tensor:\\") print(sparse_coo) # Convert to sparse CSR sparse_csr = dense_tensor.to_sparse_csr() print(\\"Sparse CSR Tensor:\\") print(sparse_csr) # Task 2: Performing Operations on Sparse Tensors print(\\"nTask 2:\\") # COO Tensor Operations # 2.1 Multiply sparse COO by dense vector dense_vector = torch.tensor([1, 2, 3, 4], dtype=torch.float32) mul_result = torch.sparse.mm(sparse_coo.unsqueeze(0), dense_vector.unsqueeze(1)).squeeze() print(\\"Multiplication Result:\\") print(mul_result) # 2.2 Add sparse COO to dense tensor and convert the result back to a sparse COO tensor dense_tensor_add = torch.tensor([[1, 0, 1, 0], [1, 2, 0, 0], [0, 1, 0, 1], [1, 0, 1, 0]], dtype=torch.float32) add_result_coo = (dense_tensor + dense_tensor_add).to_sparse() print(\\"Addition Result (Sparse COO):\\") print(add_result_coo) # CSR Tensor Operations # 2.3 Sparse CSR matrix multiplication multiplicand = torch.tensor([[1, 2], [3, 4], [5, 6], [7, 8]], dtype=torch.float32) mul_result_csr = torch.sparse.mm(sparse_csr, multiplicand) print(\\"Sparse CSR Multiplication Result:\\") print(mul_result_csr) ```","solution":"import torch # Task 1: Creating Sparse Tensors print(\\"Task 1:\\") dense_tensor = torch.tensor([[0, 0, 3, 0], [4, 0, 0, 0], [0, 0, 0, 5], [0, 6, 0, 0]], dtype=torch.float32) # Convert to sparse COO sparse_coo = dense_tensor.to_sparse() print(\\"Sparse COO Tensor:\\") print(sparse_coo) # Convert to sparse CSR sparse_csr = dense_tensor.to_sparse_csr() print(\\"Sparse CSR Tensor:\\") print(sparse_csr) # Task 2: Performing Operations on Sparse Tensors print(\\"nTask 2:\\") # COO Tensor Operations # 2.1 Multiply sparse COO by dense vector dense_vector = torch.tensor([1, 2, 3, 4], dtype=torch.float32) mul_result = torch.sparse.mm(sparse_coo, dense_vector.reshape(-1, 1)).reshape(-1) print(\\"Multiplication Result:\\") print(mul_result) # 2.2 Add sparse COO to dense tensor and convert the result back to a sparse COO tensor dense_tensor_add = torch.tensor([[1, 0, 1, 0], [1, 2, 0, 0], [0, 1, 0, 1], [1, 0, 1, 0]], dtype=torch.float32) add_result_coo = (dense_tensor + dense_tensor_add).to_sparse() print(\\"Addition Result (Sparse COO):\\") print(add_result_coo) # CSR Tensor Operations # 2.3 Sparse CSR matrix multiplication multiplicand = torch.tensor([[1, 2], [3, 4], [5, 6], [7, 8]], dtype=torch.float32) mul_result_csr = torch.sparse.mm(sparse_csr, multiplicand) print(\\"Sparse CSR Multiplication Result:\\") print(mul_result_csr)"},{"question":"# Coding Assessment: Understanding and Applying Linear Models in Scikit-learn Objective To assess your understanding of linear regression, ridge regression, regularization, and cross-validation techniques using scikit-learn. Problem Statement You are given a dataset containing features and a target variable. Your task is to implement the following steps: 1. **Load and Preprocess Data:** - Assume you have a CSV file named `data.csv` with features and a target column, where the last column is the target variable. Load this dataset. - Split the dataset into training (80%) and test (20%) sets. 2. **Model Implementation and Training:** - Train a linear regression model and a ridge regression model using the provided training data. - Perform cross-validation to select the best regularization parameter (`alpha`) for the ridge regression model. Use `RidgeCV` with alphas in the range `[0.1, 1.0, 10.0, 100.0]`. 3. **Model Evaluation:** - After training the models, evaluate their performance on the test set using mean squared error (MSE). - Output the coefficients of both models and compare their performance based on MSE. Expected Function Signature ```python def evaluate_models(file_path: str) -> Tuple[Dict[str, np.ndarray], Dict[str, float]]: Evaluates linear and ridge regression models on the given dataset. Args: - file_path: str - path to the CSV file containing the dataset. Returns: - Tuple containing two dictionaries: * The first dictionary contains the coefficients of both models: { \'linear_regression\': np.ndarray, \'ridge_regression\': np.ndarray } * The second dictionary contains the mean squared error of both models: { \'linear_regression_mse\': float, \'ridge_regression_mse\': float } pass ``` Constraints - Use the `LinearRegression` and `RidgeCV` classes from scikit-learn. - Ensure the dataset is clean (no missing values) before model training. Performance Requirements - The implemented solution should efficiently handle datasets with up to 10,000 samples and 50 features. Example Suppose the data in `data.csv` is as follows: ``` feature1, feature2, feature3, ..., target 2.3, 4.5, 1.1, ..., 7.2 5.1, 2.8, 3.3, ..., 9.1 ... ``` After running the function: ```python evaluate_models(\'data.csv\') ``` The output might be: ```python ( { \'linear_regression\': array([...]), \'ridge_regression\': array([...]) }, { \'linear_regression_mse\': 2.534, \'ridge_regression_mse\': 2.432 } ) ``` Ensure your code is well-documented and handles potential edge cases such as extremely small or large datasets.","solution":"import pandas as pd import numpy as np from sklearn.model_selection import train_test_split from sklearn.linear_model import LinearRegression, RidgeCV from sklearn.metrics import mean_squared_error from typing import Tuple, Dict def evaluate_models(file_path: str) -> Tuple[Dict[str, np.ndarray], Dict[str, float]]: Evaluates linear and ridge regression models on the given dataset. Args: - file_path: str - path to the CSV file containing the dataset. Returns: - Tuple containing two dictionaries: * The first dictionary contains the coefficients of both models: { \'linear_regression\': np.ndarray, \'ridge_regression\': np.ndarray } * The second dictionary contains the mean squared error of both models: { \'linear_regression_mse\': float, \'ridge_regression_mse\': float } # Load the dataset data = pd.read_csv(file_path) # Split features and target variable X = data.iloc[:, :-1].values y = data.iloc[:, -1].values # Split the dataset into training (80%) and test (20%) sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Train the Linear Regression model lin_reg = LinearRegression() lin_reg.fit(X_train, y_train) lin_pred = lin_reg.predict(X_test) # Perform cross-validation to select the best alpha for Ridge Regression ridge_alphas = [0.1, 1.0, 10.0, 100.0] ridge_reg = RidgeCV(alphas=ridge_alphas) ridge_reg.fit(X_train, y_train) ridge_pred = ridge_reg.predict(X_test) # Calculate Mean Squared Errors lin_mse = mean_squared_error(y_test, lin_pred) ridge_mse = mean_squared_error(y_test, ridge_pred) # Collect coefficients coefficients = { \'linear_regression\': lin_reg.coef_, \'ridge_regression\': ridge_reg.coef_ } # Collect mean squared errors mse = { \'linear_regression_mse\': lin_mse, \'ridge_regression_mse\': ridge_mse } return coefficients, mse"},{"question":"Log File Data Extraction **Objective:** Write a Python function `extract_log_data` that processes a given log file content and extracts specific information using regular expressions. The function should return the extracted data in a structured format. **Problem Statement:** You are given a string `log_content` that represents the content of a log file. Each log entry in the file is a single line and follows this format: ``` [timestamp] LEVEL: message (context) ``` Where: - `timestamp` is in the format `YYYY-MM-DD HH:MM:SS` - `LEVEL` can be one of `INFO`, `WARN`, `ERROR`, `DEBUG` - `message` is a string that can contain any characters except parentheses - `context` is an optional string enclosed in parentheses **Example log entries:** ``` [2023-01-01 12:00:00] INFO: Starting process (init) [2023-01-01 12:01:00] WARN: Low memory detected () [2023-01-01 12:02:00] ERROR: Process failed to start [2023-01-01 12:03:00] DEBUG: Variable x=10 (debug-info) [2023-01-01 12:04:00] INFO: Process completed (success) ``` **Function Signature:** ```python def extract_log_data(log_content: str) -> list[dict]: ``` **Input:** - `log_content` (str): A single string containing multiple lines of log entries. **Output:** - A list of dictionaries, where each dictionary represents a log entry with the following keys: `timestamp` (str), `level` (str), `message` (str), and `context` (str or None). **Requirements:** 1. Use regular expressions from the `re` module to parse the log entries. 2. Handle the optional `context` field appropriately—if it\'s missing, the value should be `None`. 3. Ignore lines that do not match the log entry format. 4. The function should be efficient and handle large log files. **Constraints:** - Assume that the log file content is well-formed and follows the specified format strictly. - The solution should be able to handle log contents with up to a million lines efficiently. **Example:** ```python log_content = [2023-01-01 12:00:00] INFO: Starting process (init) [2023-01-01 12:01:00] WARN: Low memory detected () [2023-01-01 12:02:00] ERROR: Process failed to start [2023-01-01 12:03:00] DEBUG: Variable x=10 (debug-info) [2023-01-01 12:04:00] INFO: Process completed (success) print(extract_log_data(log_content)) ``` **Expected Output:** ```python [ { \\"timestamp\\": \\"2023-01-01 12:00:00\\", \\"level\\": \\"INFO\\", \\"message\\": \\"Starting process\\", \\"context\\": \\"init\\" }, { \\"timestamp\\": \\"2023-01-01 12:01:00\\", \\"level\\": \\"WARN\\", \\"message\\": \\"Low memory detected\\", \\"context\\": None }, { \\"timestamp\\": \\"2023-01-01 12:02:00\\", \\"level\\": \\"ERROR\\", \\"message\\": \\"Process failed to start\\", \\"context\\": None }, { \\"timestamp\\": \\"2023-01-01 12:03:00\\", \\"level\\": \\"DEBUG\\", \\"message\\": \\"Variable x=10\\", \\"context\\": \\"debug-info\\" }, { \\"timestamp\\": \\"2023-01-01 12:04:00\\", \\"level\\": \\"INFO\\", \\"message\\": \\"Process completed\\", \\"context\\": \\"success\\" } ] ``` **Hints:** 1. Use capturing groups in your regular expression to extract the different parts of each log entry. 2. Use the `re.findall` or `re.finditer` method to find all matches in the log content.","solution":"import re def extract_log_data(log_content: str) -> list: pattern = r\'[(.*?)] (INFO|WARN|ERROR|DEBUG): (.*?)(?: ((.*?)))?\' log_entries = [] for match in re.finditer(pattern, log_content, re.MULTILINE): timestamp, level, message, context = match.groups() log_entries.append({ \\"timestamp\\": timestamp, \\"level\\": level, \\"message\\": message.strip(), \\"context\\": context if context else None }) return log_entries"},{"question":"You have been tasked with writing a function that applies a series of operations on a list of numbers using functionalities from the `itertools`, `functools`, and `operator` modules. The function should demonstrate your ability to utilize these modules effectively. # Problem Statement: Write a function `process_numbers(numbers: List[int], n: int) -> List[int]` that takes a list of integers and an integer `n` as input and performs the following operations using the given modules: 1. **cycle** through the list `n` times using `itertools.cycle`. 2. Create a new list that combines the original list with the cycled numbers. 3. Use `functools.reduce` to find the cumulative sum of numbers in the combined list. 4. Use `operator.mul` to multiply every element in the combined list by the cumulative sum. 5. Return the final modified list. # Constraints: - The input list `numbers` will contain at least one element. - The integer `n` will be a positive integer. # Example: ```python from typing import List def process_numbers(numbers: List[int], n: int) -> List[int]: pass # Example Usage numbers = [1, 2, 3] n = 2 result = process_numbers(numbers, n) print(result) # Expected output based on the steps described. ``` # Guidelines: - Use functions from the `itertools` module to handle cyclic iterations. - Utilize the `functools` module to compute the cumulative sum. - Employ the `operator` module to perform element-wise multiplication. # Expected Input and Output Format: - The input will be a list `numbers` of integers and an integer `n`. - The output will be a list of integers after performing the specified operations.","solution":"from typing import List import itertools import functools import operator def process_numbers(numbers: List[int], n: int) -> List[int]: Processes the input list of numbers based on the given rules using itertools, functools, and operator modules. Parameters: numbers (list): List of integers n (int): Number of times to cycle through the list Returns: List[int]: The modified list after applying all the required operations # Cycle through the list `n` times cycled_numbers = list(itertools.islice(itertools.cycle(numbers), n * len(numbers))) # Combine the original list with the cycled numbers combined_list = numbers + cycled_numbers # Compute the cumulative sum of the combined list cumulative_sum = functools.reduce(operator.add, combined_list) # Multiply each element in the combined list by the cumulative sum final_list = list(map(lambda x: x * cumulative_sum, combined_list)) return final_list"},{"question":"**Problem Statement** You are given a directory containing several text files, and you need to create a compressed archive of these files using the LZMA algorithm. Moreover, your task includes both compressing and later decompressing those files to verify that the decompressed content matches the original content. **Task Requirements** 1. **Compression**: - Implement a function `compress_files(input_directory: str, output_file: str, preset: int=6) -> None` that compresses all `.txt` files in the `input_directory` into a single `.xz` file named `output_file`. - Use the LZMA format with the specified `preset` for compression. 2. **Decompression**: - Implement a function `decompress_file(input_file: str, output_directory: str) -> None` that decompresses the given `.xz` file into the specified `output_directory`. 3. **Validation**: - Implement a function `validate_files(original_directory: str, decompressed_directory: str) -> bool` that checks whether the decompressed files are identical to the original files in terms of content. **Function Signatures** - `compress_files(input_directory: str, output_file: str, preset: int=6) -> None` - `decompress_file(input_file: str, output_directory: str) -> None` - `validate_files(original_directory: str, decompressed_directory: str) -> bool` **Example** ```python # Assuming the directory structure # input_directory/ # ├── file1.txt # ├── file2.txt # └── file3.txt compress_files(\'input_directory\', \'output.xz\', preset=6) decompress_file(\'output.xz\', \'decompressed_directory\') assert validate_files(\'input_directory\', \'decompressed_directory\') == True ``` **Constraints** - All file paths are strings. - The directories may contain a large number of files, so the solution must handle this efficiently. - No assumptions about the content of the text files should be made—they could be empty or very large. **Additional Information** - You may assume that the necessary directories exist and are writable/readable. - Use appropriate error handling to manage potential issues such as file read/write errors. **Hints** - Use `lzma.open` for reading and writing compressed files. - Ensure that the file names in the decompressed directory match the original directory. - The `preset` parameter should be an integer between 0 and 9, where higher values mean better compression but slower speed.","solution":"import os import lzma from pathlib import Path import shutil def compress_files(input_directory: str, output_file: str, preset: int=6) -> None: with lzma.open(output_file, \'wb\', preset=preset) as out: for txt_file in Path(input_directory).rglob(\'*.txt\'): with open(txt_file, \'rb\') as f: out.write(txt_file.name.encode() + b\'n\') content = f.read() out.write(str(len(content)).encode() + b\'n\') out.write(content) def decompress_file(input_file: str, output_directory: str) -> None: with lzma.open(input_file, \'rb\') as f: while True: file_name = f.readline().decode().strip() if not file_name: break length = int(f.readline().decode().strip()) content = f.read(length) output_path = Path(output_directory) / file_name output_path.parent.mkdir(parents=True, exist_ok=True) with open(output_path, \'wb\') as out: out.write(content) def validate_files(original_directory: str, decompressed_directory: str) -> bool: for txt_file in Path(original_directory).rglob(\'*.txt\'): original_path = txt_file decompressed_path = Path(decompressed_directory) / txt_file.name if not decompressed_path.exists(): return False with open(original_path, \'rb\') as orig, open(decompressed_path, \'rb\') as decomp: if orig.read() != decomp.read(): return False return True"},{"question":"**Objective:** Implement functions that convert a generator into an iterator, handle both synchronous and asynchronous iterators, and send values to an iterator demonstrating the use of the Python iterator protocol. # Problem Statement: 1. **Function Implementation:** - Implement a function `is_iterator(obj)` that checks if an object can be used as an iterator. - Implement a function `is_async_iterator(obj)` that checks if an object provides the asynchronous iterator protocol. - Implement a function `get_next_value(iterator)` that retrieves the next value from an iterator. - Implement a function `send_value_to_iterator(iterator, value)` that sends a value to an iterator and returns the appropriate result. # Function Details: 1. `is_iterator(obj: Any) -> bool`: - **Input:** An object `obj`. - **Output:** Returns `True` if `obj` is an iterator, `False` otherwise. 2. `is_async_iterator(obj: Any) -> bool`: - **Input:** An object `obj`. - **Output:** Returns `True` if `obj` is an asynchronous iterator, `False` otherwise. 3. `get_next_value(iterator: Iterator) -> Any`: - **Input:** An iterator `iterator`. - **Output:** The next value from the iterator. If there are no more items or an error occurs, return `None`. 4. `send_value_to_iterator(iterator: Iterator, value: Any) -> Tuple[str, Any]`: - **Input:** An iterator `iterator` and a value `value` to send to the iterator. - **Output:** A tuple where the first element is a string representing the state (`\'PYGEN_RETURN\'`, `\'PYGEN_NEXT\'`, `\'PYGEN_ERROR\'`) and the second element is the value retrieved from the iterator, or `None` if there was an error. # Constraints: - Do not use any built-in `itertools` functions. - Handle exceptions gracefully and ensure that the functions always return the expected types. - Performance is not a critical aspect, but ensure the functions handle typical sizes of iterators efficiently. # Example Usage: ```python # Example synchronous iterator def simple_gen(): yield 1 yield 2 yield 3 gen = simple_gen() print(is_iterator(gen)) # True print(is_async_iterator(gen)) # False print(get_next_value(gen)) # 1 print(get_next_value(gen)) # 2 # Example of sending values to the generator (assuming the generator is adapted to receive values) gen = simple_gen() # Reset generator print(send_value_to_iterator(gen, \'test\')) # (\'PYGEN_NEXT\', 1) - Value from iterator before sending \'test\' print(send_value_to_iterator(gen, \'test\')) # (\'PYGEN_NEXT\', 2) - Value from iterator before sending \'test\' print(send_value_to_iterator(gen, \'test\')) # (\'PYGEN_RETURN\', 3) - Last value from iterator print(send_value_to_iterator(gen, \'test\')) # (\'PYGEN_ERROR\', None) - No more values/errors # Example asynchronous iterator # (You can create a similar async generator for testing is_async_iterator and other functionalities) ``` Note: To fully test asynchronous iterators, you will need to create async generator functions and use an event loop or `asyncio` framework for proper execution and testing.","solution":"def is_iterator(obj): Checks if an object is an iterator. try: iter(obj) return hasattr(obj, \'__next__\') except TypeError: return False def is_async_iterator(obj): Checks if an object is an asynchronous iterator. return hasattr(obj, \'__aiter__\') and hasattr(obj, \'__anext__\') def get_next_value(iterator): Retrieves the next value from an iterator. If the iterator has no more items or an error occurs, return None. try: return next(iterator) except (StopIteration, TypeError): return None def send_value_to_iterator(iterator, value): Sends a value to an iterator and returns the appropriate result. try: if hasattr(iterator, \'send\'): result = iterator.send(value) else: result = next(iterator) return (\'PYGEN_NEXT\', result) except StopIteration as e: return (\'PYGEN_RETURN\', getattr(e, \'value\', None)) except Exception: return (\'PYGEN_ERROR\', None)"},{"question":"# PyTorch Coding Assessment Question # Objective: Implement a function in PyTorch that demonstrates the following: 1. Creating tensors with various data types. 2. Performing arithmetic operations between tensors of different data types and understanding type promotion. 3. Managing tensors on different devices (CPU and GPU) and ensuring correct operations between them. # Task Description: Write a function `tensor_operations` that performs the following steps: 1. Create the following tensors: - A 2x2 tensor of type `torch.float32` with values `[[1.0, 2.0], [3.0, 4.0]]`. - A 2x2 tensor of type `torch.int64` with values `[[1, 2], [3, 4]]`. - A 2x2 tensor of type `torch.bool` with values `[[True, False], [False, True]]`. 2. Perform element-wise addition between the float and integer tensors. Return the result tensor\'s type. 3. Perform element-wise multiplication between the float tensor and the boolean tensor. Return the result tensor\'s type. 4. Move all tensors to the GPU (if available) and perform the same operations. Check if the resulting tensors are on the GPU and return their devices. 5. Ensure proper type handling and device management to avoid any runtime errors. # Input: No input required. The function should create all necessary tensors internally. # Output: A dictionary with the following keys and values: - `\\"add_result_dtype\\"`: The data type of the result tensor after adding the float and integer tensors. - `\\"mult_result_dtype\\"`: The data type of the result tensor after multiplying the float and boolean tensors. - `\\"is_on_gpu\\"`: Boolean indicating if the tensors are successfully moved to the GPU and the operations performed there. This should be `True` if all tensors are on the GPU and `False` otherwise. - `\\"add_result_device\\"`: The device of the result tensor after adding operations (should be GPU if available). - `\\"mult_result_device\\"`: The device of the result tensor after multiplication operations (should be GPU if available). # Constraints: - If a GPU is not available, ensure the function gracefully handles operations on the CPU. # Example Function Signature: ```python import torch def tensor_operations(): # Your implementation here # Example return value: # { # \\"add_result_dtype\\": torch.float32, # \\"mult_result_dtype\\": torch.float32, # \\"is_on_gpu\\": <True or False>, # \\"add_result_device\\": torch.device(\'cuda:0\') if is_on_gpu else torch.device(\'cpu\'), # \\"mult_result_device\\": torch.device(\'cuda:0\') if is_on_gpu else torch.device(\'cpu\') # } ``` # Additional Notes: - Use appropriate methods to check for the presence of a GPU. - Ensure that your implementation is efficient and handles any potential errors gracefully.","solution":"import torch def tensor_operations(): # Step 1: Create tensors with specified data types and values float_tensor = torch.tensor([[1.0, 2.0], [3.0, 4.0]], dtype=torch.float32) int_tensor = torch.tensor([[1, 2], [3, 4]], dtype=torch.int64) bool_tensor = torch.tensor([[True, False], [False, True]], dtype=torch.bool) # Step 2: Perform element-wise addition between float and integer tensors add_result = float_tensor + int_tensor # Step 3: Perform element-wise multiplication between float tensor and boolean tensor mult_result = float_tensor * bool_tensor # Step 4: Check if a GPU is available and move tensors accordingly device = torch.device(\\"cuda:0\\" if torch.cuda.is_available() else \\"cpu\\") float_tensor_gpu = float_tensor.to(device) int_tensor_gpu = int_tensor.to(device) bool_tensor_gpu = bool_tensor.to(device) # Perform operations on GPU tensors add_result_gpu = float_tensor_gpu + int_tensor_gpu mult_result_gpu = float_tensor_gpu * bool_tensor_gpu # Check if the computations were performed on GPU is_on_gpu = torch.cuda.is_available() # Return the specified details return { \\"add_result_dtype\\": add_result.dtype, \\"mult_result_dtype\\": mult_result.dtype, \\"is_on_gpu\\": is_on_gpu, \\"add_result_device\\": add_result_gpu.device, \\"mult_result_device\\": mult_result_gpu.device }"},{"question":"<|Analysis Begin|> The provided documentation is for Python\'s `gc` module, which is used to interface with the garbage collection mechanism. It offers a variety of functions to control and inspect garbage collection, such as enabling or disabling the garbage collector, forcing collection, and inspecting collected objects. This module provides advanced functionalities and options for debugging and optimizing memory usage. Key concepts include: - Enabling/disabling the garbage collector (`gc.enable()`, `gc.disable()`). - Manual collection with `gc.collect()`. - Debugging with `gc.set_debug()`, `gc.get_debug()`, and various constants like `DEBUG_STATS` and `DEBUG_LEAK`. - Retrieving information about objects and garbage collection stats with functions like `gc.get_stats()`, `gc.get_objects()`, `gc.is_tracked()`, etc. - Freezing and unfreezing the garbage collector (`gc.freeze()`, `gc.unfreeze()`). By using this module, students can delve into memory management details and understand the intricacies of Python\'s garbage collection system. <|Analysis End|> <|Question Begin|> **Problem 1: Efficient Memory Management using the `gc` Module** In this problem, you are required to create a utility class `GCMemoryManager` that leverages Python\'s `gc` module for efficient memory management. This utility class should provide functionalities to: 1. Enable and disable the garbage collection. 2. Perform garbage collection for a specific generation. 3. Retrieve and print statistics about garbage collection. 4. Set and get garbage collection thresholds. 5. Track objects and identify those that are not currently tracked. **Specifications:** 1. Implement the `GCMemoryManager` class with the following methods: - `enable_gc()`: Enable automatic garbage collection. - `disable_gc()`: Disable automatic garbage collection. - `collect_garbage(generation=2)`: Collect garbage for the specified generation (default is generation 2). Return the number of unreachable objects found. - `get_gc_stats()`: Retrieve and return garbage collection statistics. - `set_thresholds(threshold0, threshold1=None, threshold2=None)`: Set new garbage collection thresholds. - `get_thresholds()`: Retrieve and return the current garbage collection thresholds. - `is_object_tracked(obj)`: Return `True` if the object is tracked by the garbage collector, `False` otherwise. - `get_untracked_objects()`: Return a list of objects that are not tracked by the garbage collector. 2. Consider the performance implications: - The `collect_garbage()` method should not trigger garbage collection unnecessarily. - Ensure that operations for tracking and retrieving statistics are efficiently implemented. **Input/Output:** - Inputs and outputs will be directly via method calls on the `GCMemoryManager` instance. - No direct input from the user or external files. - Methods returning statistics or tracked objects should return suitable Python data structures (e.g., lists, dictionaries). **Example Usage:** ```python # Sample usage of the GCMemoryManager class. gc_manager = GCMemoryManager() # Enable garbage collection gc_manager.enable_gc() # Set thresholds gc_manager.set_thresholds(700, 10, 10) # Collect garbage and print the number of unreachable objects found print(gc_manager.collect_garbage()) # Print current garbage collection stats print(gc_manager.get_gc_stats()) # Verify if an object is tracked sample_list = [1, 2, 3] print(gc_manager.is_object_tracked(sample_list)) # Get and print which objects are not currently tracked print(gc_manager.get_untracked_objects()) ``` **Constraints:** - Ensure your implementation is robust and guards against incorrect types or parameters being passed to methods. - Handle potential exceptions that may arise from misuse of the `gc` functions (e.g., invalid threshold values, invalid generation numbers). Implement the class `GCMemoryManager` as described.","solution":"import gc class GCMemoryManager: def enable_gc(self): Enable automatic garbage collection. gc.enable() def disable_gc(self): Disable automatic garbage collection. gc.disable() def collect_garbage(self, generation=2): Perform garbage collection for the specified generation. Return the number of unreachable objects found. return gc.collect(generation) def get_gc_stats(self): Retrieve and return garbage collection statistics. return gc.get_stats() def set_thresholds(self, threshold0, threshold1=None, threshold2=None): Set new garbage collection thresholds. if threshold1 is None and threshold2 is None: gc.set_threshold(threshold0) else: gc.set_threshold(threshold0, threshold1, threshold2) def get_thresholds(self): Retrieve and return the current garbage collection thresholds. return gc.get_threshold() def is_object_tracked(self, obj): Return True if the object is tracked by the garbage collector, False otherwise. return gc.is_tracked(obj) def get_untracked_objects(self): Return a list of objects currently known to be tracked by the garbage collector. # Alternatively, you can construct objects and verify # non-tracked objects from the list returned by gc.get_objects() return [obj for obj in gc.get_objects() if not gc.is_tracked(obj)]"},{"question":"# Partial Least Squares - Coding Assessment Question In this exercise, you will implement a basic version of Partial Least Squares (PLS) regression, akin to `PLSRegression` from scikit-learn\'s `cross_decomposition` module. Your task is to follow the iterative procedure described for `PLSCanonical` and adapt it to the `PLSRegression` specifics. Problem Statement Implement a class `BasicPLSRegression` with the following methods: - `__init__(self, n_components)`: Initialize the model with the number of components `n_components`. - `fit(self, X, Y)`: Fit the model on the input matrices `X` (predictors) and `Y` (targets). - `transform(self, X)`: Transform the input matrix `X` using the fitted model and return the projected matrix. - `predict(self, X)`: Predict the matrix `Y` given the input matrix `X` using the fitted model. Input and Output Formats 1. **Initialization**: - `n_components` (int): Number of components to keep in the model. 2. **fit(X, Y)**: - `X` (np.ndarray): Predictor matrix of shape (n_samples, n_features). - `Y` (np.ndarray): Target matrix of shape (n_samples, n_targets). - The method should not return any value. 3. **transform(X)**: - `X` (np.ndarray): Predictor matrix of shape (n_samples, n_features). - Returns: Projected matrix of shape (n_samples, n_components). 4. **predict(X)**: - `X` (np.ndarray): Predictor matrix of shape (n_samples, n_features). - Returns: Predicted target matrix of shape (n_samples, n_targets). Constraints - The predictor matrix `X` and target matrix `Y` should have the same number of samples. - The number of components `n_components` should be less than or equal to the rank of the predictor matrix `X`. Performance Requirements - Your implementation should be efficient in terms of time complexity and avoid any unnecessary computations. Example ```python import numpy as np from sklearn.datasets import make_regression # Generate sample data X, Y = make_regression(n_samples=100, n_features=10, n_informative=5, n_targets=3, noise=0.1, random_state=42) # Initialize the model model = BasicPLSRegression(n_components=2) # Fit the model model.fit(X, Y) # Transform the predictor matrix X_transformed = model.transform(X) # Predict the targets Y_pred = model.predict(X) print(\\"Transformed X shape:\\", X_transformed.shape) print(\\"Predicted Y shape:\\", Y_pred.shape) ``` The expected output should demonstrate that the transformed `X` has the shape `(100, 2)` and the predicted `Y` has the shape `(100, 3)`. Implement the methods according to the given description and the mathematical principles of Partial Least Squares regression.","solution":"import numpy as np from sklearn.cross_decomposition import PLSRegression class BasicPLSRegression: def __init__(self, n_components): self.n_components = n_components self.model = PLSRegression(n_components=n_components) def fit(self, X, Y): self.model.fit(X, Y) def transform(self, X): return self.model.transform(X) def predict(self, X): return self.model.predict(X)"},{"question":"**Objective**: Implement a function `setup_faulthandler()` that demonstrates the use of the `faulthandler` module to help debug segmentation faults or other catastrophic errors in a Python program. # Requirements: 1. **Enable Fault Handlers**: The function should enable the fault handler to handle signals `SIGSEGV`, `SIGFPE`, `SIGABRT`, `SIGBUS`, `SIGILL`, and on Windows, handle Windows exceptions. 2. **Specify File Output**: The function should allow specifying an optional file to which the tracebacks should be dumped. If the file is not specified, it should default to `sys.stderr`. 3. **Dump Tracebacks on Signal**: The function should register a specific signal (e.g., `SIGUSR1` but not available on Windows) to dump tracebacks and optionally chain the previous signal handler. 4. **Cancel Timer**: The function should provide a mechanism to cancel a scheduled traceback dump if it was set. 5. **Demonstrate Usage**: Demonstrate the function by triggering a segmentation fault using the `ctypes` module as shown in the example. # Function Signature: ```python import sys import faulthandler import signal import ctypes def setup_faulthandler(timeout=None, file=sys.stderr, signal_num=signal.SIGUSR1, chain=False, cancel_timeout=False): Set up the fault handler for debugging. :param timeout: Optional timeout in seconds after which to dump tracebacks. If None, no timeout will be used. :param file: File object where traceback should be dumped. Defaults to sys.stderr. :param signal_num: Signal number to trigger the traceback dump. Defaults to SIGUSR1. Not available on Windows. :param chain: If True, chain the signal handler. Defaults to False. :param cancel_timeout: If True, cancel the scheduled traceback dump. Defaults to False. # Enabling fault handler faulthandler.enable(file=file, all_threads=True) if timeout: # Scheduling a traceback dump after a timeout if specified faulthandler.dump_traceback_later(timeout=timeout, file=file) if signal_num: if hasattr(signal, \\"SIGUSR1\\"): # Registering a signal to dump tracebacks faulthandler.register(signal_num, file=file, all_threads=True, chain=chain) if cancel_timeout: # Cancel any pending scheduled traceback dump faulthandler.cancel_dump_traceback_later() # Demonstrating a segmentation fault (use with caution) # ctypes.string_at(0) # Example Usage setup_faulthandler(timeout=5, file=sys.stderr, signal_num=signal.SIGUSR1, chain=False, cancel_timeout=False) ``` **Notes**: 1. Ensure that you handle the `signal_num` appropriately, checking for the availability of `SIGUSR1` since it is not available on Windows. 2. Demonstrate your `setup_faulthandler()` in a controlled manner. The segmentation fault demonstration using `ctypes.string_at(0)` should be commented or used cautiously. # Expected Output: Upon running the function and triggering the respective signal or timeout, the tracebacks should be dumped to the specified file or standard error, including all active threads. # Constraints: - The solution should follow best practices in terms of coding style and should handle edge cases, such as incorrect signals or invalid file descriptors. - Performance considerations are minimal since the function primarily deals with fault handling and debugging rather than high-throughput operations.","solution":"import sys import faulthandler import signal def setup_faulthandler(timeout=None, file=sys.stderr, signal_num=signal.SIGUSR1, chain=False, cancel_timeout=False): Set up the fault handler for debugging. :param timeout: Optional timeout in seconds after which to dump tracebacks. If None, no timeout will be used. :param file: File object where traceback should be dumped. Defaults to sys.stderr. :param signal_num: Signal number to trigger the traceback dump. Defaults to SIGUSR1. Not available on Windows. :param chain: If True, chain the signal handler. Defaults to False. :param cancel_timeout: If True, cancel the scheduled traceback dump. Defaults to False. # Enabling fault handler faulthandler.enable(file=file, all_threads=True) if timeout: # Scheduling a traceback dump after a timeout if specified faulthandler.dump_traceback_later(timeout, file=file) if signal_num: if hasattr(signal, \\"SIGUSR1\\"): # Registering a signal to dump tracebacks faulthandler.register(signal_num, file=file, all_threads=True, chain=chain) if cancel_timeout: # Cancel any pending scheduled traceback dump faulthandler.cancel_dump_traceback_later() # Demonstrating a segmentation fault (commented for safety) # ctypes.string_at(0) # Example Usage # setup_faulthandler(timeout=5, file=sys.stderr, signal_num=signal.SIGUSR1, chain=False, cancel_timeout=False)"},{"question":"**Objective**: Port a Python 2 script to make it compatible with both Python 2.7 and Python 3.x. **Problem Statement**: You are given a Python 2 script that performs various basic data operations. Your task is to modify the code to ensure it is compatible with both Python 2.7 and Python 3.x, adhering to best practices mentioned in the documentation provided. **Original Python 2 Script**: ```python # Calculate the average of a list of numbers def average(numbers): return sum(numbers) / len(numbers) # Read contents of a file and print it def read_file(file_path): with open(file_path, \'r\') as file: contents = file.read() print contents # Function to process text and binary data def process_data(data): if isinstance(data, str): print \\"Processing text data\\" elif isinstance(data, bytes): print \\"Processing binary data\\" # Main execution if __name__ == \\"__main__\\": numbers = [1, 2, 3, 4, 5] print \\"Average of numbers:\\", average(numbers) read_file(\\"sample.txt\\") process_data(\\"example text\\") process_data(b\'x00x01x02\') ``` **Requirements**: 1. **Division**: Ensure that division in the `average` function produces a float result in Python 2 (like in Python 3). You may use `from __future__ import division`. 2. **File Reading**: Make sure the file is opened in text mode in Python 3. Use `io.open()` from the `io` module to handle file operations. 3. **Print Function**: Use `from __future__ import print_function` to ensure compatibility of the print statements. 4. **Text vs Binary Data**: Ensure that `process_data` function can handle binary and text data appropriately in both Python 2 and 3. 5. **Compatibility Imports**: Ensure imports from `__future__` are used to maintain compatibility. Modify the script to meet the above requirements. **Constraints**: - Do not use third-party libraries like `six`. - The logic of the code should remain mostly the same. **Input/Output**: - No user input is required. - Sample text file \'sample.txt\' should be present in the same directory with some text content. **Performance**: - Optimization is not a concern; focus on compatibility. **Solution Template**: ```python from __future__ import division, print_function import io # Calculate the average of a list of numbers def average(numbers): return sum(numbers) / len(numbers) # Read contents of a file and print it def read_file(file_path): with io.open(file_path, \'rt\', encoding=\'utf-8\') as file: contents = file.read() print(contents) # Function to process text and binary data def process_data(data): if isinstance(data, str): print(\\"Processing text data\\") elif isinstance(data, bytes): print(\\"Processing binary data\\") # Main execution if __name__ == \\"__main__\\": numbers = [1, 2, 3, 4, 5] print(\\"Average of numbers:\\", average(numbers)) read_file(\\"sample.txt\\") process_data(\\"example text\\") process_data(b\'x00x01x02\') ``` Make sure to test your script in both Python 2.7 and Python 3.x environments to ensure compatibility.","solution":"from __future__ import division, print_function import io # Calculate the average of a list of numbers def average(numbers): return sum(numbers) / len(numbers) # Read contents of a file and print it def read_file(file_path): with io.open(file_path, \'rt\', encoding=\'utf-8\') as file: contents = file.read() print(contents) # Function to process text and binary data def process_data(data): if isinstance(data, str): print(\\"Processing text data\\") elif isinstance(data, bytes): print(\\"Processing binary data\\") # Main execution if __name__ == \\"__main__\\": numbers = [1, 2, 3, 4, 5] print(\\"Average of numbers:\\", average(numbers)) read_file(\\"sample.txt\\") process_data(\\"example text\\") process_data(b\'x00x01x02\')"},{"question":"**Objective:** Understand the fundamentals and advanced usage of the asyncio event loop including scheduling callbacks and working with network connections. # Problem Statement Create a simple \\"chat server\\" using `asyncio` where multiple clients can connect and send messages to each other. The server should use asyncio’s event loop to accept client connections, receive messages, and broadcast them to all connected clients. # Requirements 1. **Server Behavior:** - The server should accept connections from multiple clients. - When a client sends a message, it should be broadcast to all connected clients. - Each message should be prefixed with the client address to identify who sent it. 2. **Server Implementation:** - Use `asyncio.start_server()` to create and start a server. - Utilize asyncio’s event loop to manage and handle the connections asynchronously. - Implement error handling within the event loop. 3. **Client Implementation for Testing:** (Optional, but useful for testing) - Create a basic asyncio-based client to connect to the server and send/receive messages. # Detailed Steps 1. **Create the Chat Server Class:** - Define a class `ChatServer` with methods to start and stop the server and manage clients. 2. **Handle Client Connections:** - When a client connects, add it to the list of connected clients. - When a message is received, broadcast it to all clients prefixed with the sender\'s address. 3. **Example Usage:** - Demonstrate the functionality with a main function that starts the server and connects multiple clients. # Code Template ```python import asyncio class ChatServer: def __init__(self, host=\'127.0.0.1\', port=8888): self.host = host self.port = port self.server = None self.clients = [] async def start_server(self): self.server = await asyncio.start_server( self.handle_client, self.host, self.port) addr = self.server.sockets[0].getsockname() print(f\'Serving on {addr}\') async with self.server: await self.server.serve_forever() async def handle_client(self, reader, writer): addr = writer.get_extra_info(\'peername\') print(f\'New connection from {addr}\') self.clients.append(writer) try: while True: data = await reader.read(100) message = data.decode() if not message: break full_message = f\\"{addr}: {message}\\" print(f\'Received message from {addr}: {message}\') await self.broadcast(full_message) except (asyncio.CancelledError, ConnectionResetError) as e: print(f\'Error: {str(e)}\') finally: print(f\'Connection closed from {addr}\') self.clients.remove(writer) writer.close() await writer.wait_closed() async def broadcast(self, message): for client in self.clients: client.write(message.encode()) await client.drain() async def stop_server(self): self.server.close() await self.server.wait_closed() print(\\"Server closed\\") # Example usage async def main(): server = ChatServer() await server.start_server() if __name__ == \'__main__\': asyncio.run(main()) ``` # Constraints - Implement the server using only the methods described within the provided module documentation. - The server should handle multiple clients simultaneously. - Proper error handling and cleanup of client connections should be implemented to avoid resource leaks. # Performance Considerations - Your implementation should be efficient in handling multiple concurrent connections. - Ensure that the server does not block or become unresponsive due to a single slow client. # Testing Use the following template to create a simple client for testing: ```python import asyncio async def chat_client(message): reader, writer = await asyncio.open_connection(\'127.0.0.1\', 8888) writer.write(message.encode()) await writer.drain() data = await reader.read(100) print(f\'Received: {data.decode()}\') writer.close() await writer.wait_closed() # Example usage async def main(): await chat_client(\'Hello, World!\') await chat_client(\'Another message!\') if __name__ == \'__main__\': asyncio.run(main()) ``` Remember to test your server thoroughly to ensure it handles multiple clients correctly and efficiently.","solution":"import asyncio class ChatServer: def __init__(self, host=\'127.0.0.1\', port=8888): self.host = host self.port = port self.server = None self.clients = [] async def start_server(self): self.server = await asyncio.start_server( self.handle_client, self.host, self.port) addr = self.server.sockets[0].getsockname() print(f\'Serving on {addr}\') async with self.server: await self.server.serve_forever() async def handle_client(self, reader, writer): addr = writer.get_extra_info(\'peername\') print(f\'New connection from {addr}\') self.clients.append(writer) try: while True: data = await reader.read(100) message = data.decode() if not message: break full_message = f\\"{addr}: {message}\\" print(f\'Received message from {addr}: {message}\') await self.broadcast(full_message) except (asyncio.CancelledError, ConnectionResetError) as e: print(f\'Error: {str(e)}\') finally: print(f\'Connection closed from {addr}\') self.clients.remove(writer) writer.close() await writer.wait_closed() async def broadcast(self, message): for client in self.clients: client.write(message.encode()) await client.drain() async def stop_server(self): self.server.close() await self.server.wait_closed() print(\\"Server closed\\") # Example usage async def main(): server = ChatServer() await server.start_server() if __name__ == \'__main__\': asyncio.run(main())"},{"question":"Objective: Demonstrate your understanding of the seaborn `color_palette` function by generating specific color palettes, applying them to plots, and modifying the default palette using context managers. Problem Statement: You are required to design a function `create_custom_plot` that generates a scatter plot with customized color palettes using seaborn. The function should: 1. Generate a scatter plot using the default seaborn color palette. 2. Generate three scatter plots using the following color palettes: - \\"pastel\\" - A diverging \\"Spectral\\" colormap. - Custom cubehelix palette using the parameter string `\\"ch:s=.25,rot=-.25\\"` 3. Use a context manager to temporarily change the color palette to \\"Set3\\" for one of the scatter plots. 4. Extract and print the hex codes of the \\"pastel\\" palette. Function Signature: ```python def create_custom_plot(data: dict): pass ``` Input: - `data`: A dictionary with keys `\'x\'`, `\'y\'`, and `\'hue\'`, where each value is a list of the same length. - example: `{\'x\': [1, 2, 3, 4], \'y\': [4, 3, 2, 1], \'hue\': [\'A\', \'B\', \'A\', \'B\']}` Output: - This function does not return any value. It should display the scatter plots. - The function should print the hex codes of the \\"pastel\\" palette. Constraints: - You must use seaborn and matplotlib for plotting. - The length of lists in `data` dictionary will not exceed 1000 elements. - The values in `hue` are categorical and there are at most 10 unique categories. Example: ```python data = { \'x\': [1, 2, 3, 4], \'y\': [4, 3, 2, 1], \'hue\': [\'A\', \'B\', \'A\', \'B\'] } create_custom_plot(data) ``` Expected Behavior: - The function will generate and display four scatter plots. - The hex codes for the \\"pastel\\" palette will be printed. Tips: - Use `sns.color_palette` to generate the palettes. - Use `sns.relplot` or `sns.scatterplot` to create scatter plots. - Utilize matplotlib\'s `plt.show()` to ensure plots are rendered if necessary.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_custom_plot(data: dict): x = data[\'x\'] y = data[\'y\'] hue = data[\'hue\'] # Plot using default palette plt.figure(figsize=(8, 6)) sns.scatterplot(x=x, y=y, hue=hue) plt.title(\\"Default Palette\\") plt.show() # Plot using pastel palette plt.figure(figsize=(8, 6)) sns.scatterplot(x=x, y=y, hue=hue, palette=\\"pastel\\") plt.title(\\"Pastel Palette\\") plt.show() # Print hex codes of the \\"pastel\\" palette pastel_palette = sns.color_palette(\\"pastel\\") print(\\"Pastel Palette Hex Codes:\\") print(pastel_palette.as_hex()) # Plot using \\"Spectral\\" colormap plt.figure(figsize=(8, 6)) sns.scatterplot(x=x, y=y, hue=hue, palette=\\"Spectral\\") plt.title(\\"Spectral Colormap\\") plt.show() # Plot using custom cubehelix palette plt.figure(figsize=(8, 6)) sns.scatterplot(x=x, y=y, hue=hue, palette=\\"ch:s=.25,rot=-.25\\") plt.title(\\"Custom Cubehelix Palette\\") plt.show() # Temporarily change the color palette to \\"Set3\\" using context manager with sns.color_palette(\\"Set3\\"): plt.figure(figsize=(8, 6)) sns.scatterplot(x=x, y=y, hue=hue) plt.title(\\"Set3 Palette (Temporary)\\") plt.show()"},{"question":"**Problem Statement:** You are tasked with creating a Python function to handle terminal input in a Unix-based environment using the `tty` module. The goal is to toggle the terminal between raw and cbreak modes based on user input, simulating a scenario where certain commands require parsing immediate key presses. Implement the function `control_terminal_mode(fd: int, command: str) -> None` which will: - Change the terminal mode of the file descriptor `fd` based on the provided `command`. - If the `command` is \\"raw\\", set the terminal to raw mode using `tty.setraw(fd)`. - If the `command` is \\"cbreak\\", set the terminal to cbreak mode using `tty.setcbreak(fd)`. - If the `command` is not recognized, the function should raise a `ValueError` with the message \\"Invalid command\\". # Input - `fd` (int): A file descriptor representing the terminal. - `command` (str): A string command which can either be \\"raw\\" or \\"cbreak\\". # Output - The function should modify the terminal mode accordingly but does not return a value. # Constraints - The function should expect valid file descriptors (e.g., 0 for stdin). - The `command` string will always be in lowercase. - This function should only work on Unix-based systems. - Assume appropriate error handling for terminal mode changes are managed elsewhere. # Example ```python import os import tty def control_terminal_mode(fd: int, command: str) -> None: if command == \\"raw\\": tty.setraw(fd) elif command == \\"cbreak\\": tty.setcbreak(fd) else: raise ValueError(\\"Invalid command\\") # Example usage: fd = os.open(\'/dev/tty\', os.O_RDWR) control_terminal_mode(fd, \\"raw\\") # The terminal is now in raw mode control_terminal_mode(fd, \\"cbreak\\") # The terminal is now in cbreak mode os.close(fd) ``` # Notes - This example assumes you have access to a terminal file descriptor. - Always ensure to close the file descriptor after use to avoid resource leaks.","solution":"import tty import os def control_terminal_mode(fd: int, command: str) -> None: Change the terminal mode of the provided file descriptor. Parameters: fd (int): The file descriptor representing the terminal. command (str): The command to change terminal mode. It can either be \\"raw\\" or \\"cbreak\\". Raises: ValueError: If the command is not \\"raw\\" or \\"cbreak\\". if command == \\"raw\\": tty.setraw(fd) elif command == \\"cbreak\\": tty.setcbreak(fd) else: raise ValueError(\\"Invalid command\\") # Example usage: # fd = os.open(\'/dev/tty\', os.O_RDWR) # control_terminal_mode(fd, \\"raw\\") # The terminal is now in raw mode # control_terminal_mode(fd, \\"cbreak\\") # The terminal is now in cbreak mode # os.close(fd)"},{"question":"# Advanced Iterator Implementation Objective You are to implement a custom iterator in Python that handles both synchronous and asynchronous iteration. This will demonstrate your understanding of Python\'s iterator protocols and how to work with iterators effectively. Problem Statement You need to implement two classes: `CustomIterator` and `AsyncCustomIterator`, each conforming to their respective iterator protocols. 1. **CustomIterator**: - This iterator will take a list of integers and, on each iteration, will yield the square of the integer. - It should raise `StopIteration` once all integers have been iterated over. 2. **AsyncCustomIterator**: - This async iterator will also take a list of integers, and for each, it will yield the square of the integer after awaiting a small delay (simulate asynchronous delay using `await asyncio.sleep(1)`). - It should raise `StopAsyncIteration` once all integers have been iterated over. Requirements 1. **CustomIterator Class**: - **Initialization**: `__init__(self, integers: List[int])` - **Next Item**: - Method: `__next__(self)` - Returns: `int` - **Iterator Protocol**: - Method: `__iter__(self)` - Returns: Self 2. **AsyncCustomIterator Class**: - **Initialization**: `__init__(self, integers: List[int])` - **Next Item**: - Method: `__anext__(self)` - Returns: `int` - **Async Iterator Protocol**: - Method: `__aiter__(self)` - Returns: Self # Example Usage ```python # Synchronous Iterator Example custom_iterator = CustomIterator([1, 2, 3]) for num in custom_iterator: print(num) # Output: 1, 4, 9 # Asynchronous Iterator Example async def async_example(): async_custom_iterator = AsyncCustomIterator([1, 2, 3]) async for num in async_custom_iterator: print(num) # Output: 1, 4, 9 # run the async function using asyncio.run (only for Python 3.7+) import asyncio asyncio.run(async_example()) ``` Constraints - You should handle empty list inputs gracefully. - Your iterator classes should adhere to the iterator and async iterator protocols as per the provided documentation. - The asynchronous delay should simulate real-world async operations without blocking. Submission Submit a single Python file containing the implementations for both `CustomIterator` and `AsyncCustomIterator`.","solution":"import asyncio from typing import List class CustomIterator: def __init__(self, integers: List[int]): self.integers = integers self.index = 0 def __iter__(self): return self def __next__(self): if self.index >= len(self.integers): raise StopIteration value = self.integers[self.index] self.index += 1 return value ** 2 class AsyncCustomIterator: def __init__(self, integers: List[int]): self.integers = integers self.index = 0 def __aiter__(self): return self async def __anext__(self): if self.index >= len(self.integers): raise StopAsyncIteration value = self.integers[self.index] self.index += 1 await asyncio.sleep(1) return value ** 2"},{"question":"Objective In this assessment, you will demonstrate your understanding of creating a TCP server in Python using the `socketserver` module. The server should handle multiple clients concurrently and respond to specific commands sent by the clients. Task Implement a TCP server that listens to client connections and processes the following commands: 1. `ECHO`: The server should respond back with the same message sent by the client. 2. `TIME`: The server should respond with the current server time. 3. `EXIT`: The server should close the connection with the client. Use the `ThreadingMixIn` to enable the server to handle multiple clients concurrently. Instructions 1. Create a class `MyTCPHandler` inheriting from `socketserver.BaseRequestHandler` and override the `handle()` method to process incoming requests based on the commands specified. 2. Use the `ThreadingMixIn` to create a `ThreadingTCPServer` class that handles clients concurrently. 3. Implement a main block to set up and start the server. Constraints - The server should run on `localhost` and listen on port `9999`. - The server should handle up to 5 clients concurrently. - Ensure proper handling of client disconnections and any exceptions. Input and Output - Clients will send commands as simple strings followed by a newline character. - Your server should respond with appropriate messages based on the command received. Example client-server interaction: ``` Client: ECHO Hello Server Server: Hello Server Client: TIME Server: <current server time> Client: EXIT Server: (Closes connection) ``` Example Usage: ```python import socket import threading import socketserver from datetime import datetime class MyTCPHandler(socketserver.BaseRequestHandler): def handle(self): self.data = self.request.recv(1024).strip().decode() command, *args = self.data.split() if command == \\"ECHO\\": response = \\" \\".join(args) elif command == \\"TIME\\": response = datetime.now().strftime(\'%Y-%m-%d %H:%M:%S\') elif command == \\"EXIT\\": response = \\"BYE\\" else: response = \\"UNKNOWN COMMAND\\" self.request.sendall(response.encode()) if command == \\"EXIT\\": self.request.close() class ThreadingTCPServer(socketserver.ThreadingMixIn, socketserver.TCPServer): pass if __name__ == \\"__main__\\": HOST, PORT = \\"localhost\\", 9999 server = ThreadingTCPServer((HOST, PORT), MyTCPHandler) with server: ip, port = server.server_address server_thread = threading.Thread(target=server.serve_forever) server_thread.daemon = True server_thread.start() print(\\"Server running in thread:\\", server_thread.name) server.shutdown() server.server_close() ``` Your implemented server should follow this structure and handle client requests as specified.","solution":"import socketserver import threading from datetime import datetime class MyTCPHandler(socketserver.BaseRequestHandler): def handle(self): while True: self.data = self.request.recv(1024).strip().decode() if not self.data: break command, *args = self.data.split() if command == \\"ECHO\\": response = \\" \\".join(args) elif command == \\"TIME\\": response = datetime.now().strftime(\'%Y-%m-%d %H:%M:%S\') elif command == \\"EXIT\\": response = \\"BYE\\" self.request.sendall(response.encode()) break else: response = \\"UNKNOWN COMMAND\\" self.request.sendall(response.encode()) class ThreadingTCPServer(socketserver.ThreadingMixIn, socketserver.TCPServer): pass if __name__ == \\"__main__\\": HOST, PORT = \\"localhost\\", 9999 server = ThreadingTCPServer((HOST, PORT), MyTCPHandler) with server: ip, port = server.server_address server_thread = threading.Thread(target=server.serve_forever) server_thread.daemon = True server_thread.start() print(f\\"Server running on {ip}:{port} in thread: {server_thread.name}\\") server_thread.join() server.shutdown() server.server_close()"},{"question":"# Signal Handling in Python You are required to implement a Python program that demonstrates the use of the `signal` module to handle various types of signals. You will set up signal handlers for `SIGINT`, `SIGALRM`, and `SIGTERM` and ensure the application handles these signals gracefully. # Requirements: 1. **Implement a Signal Handler for `SIGINT`:** - When the program receives `SIGINT` (from a Ctrl+C), it should print \\"SIGINT received\\" and then raise a `KeyboardInterrupt` to terminate the program. 2. **Set Up an Alarm:** - Set an alarm to trigger 10 seconds after the program starts, which should invoke a handler that prints \\"Alarm Triggered\\". 3. **Handle `SIGTERM` Signal:** - When `SIGTERM` is received, the handler should print \\"SIGTERM received, shutting down gracefully\\" and perform any necessary clean-up before terminating the program. 4. **Graceful Shutdown:** - Ensure that the program can shut down gracefully upon receiving the signals, especially when handling operations or resources that need a proper clean-up process. # Function Specifications: 1. **`main()` Function:** - This function should set up the signal handlers and start the alarm. It should include a loop that keeps the program running, simulating some ongoing process. 2. **Signal Handlers:** - Implement signal handlers for `SIGINT` and `SIGTERM`, and an alarm handler for handling the `SIGALRM` signal. # Implementation Requirements: - You must use the `signal` module to set up handlers. - The `main()` function should not terminate until a signal is received. - Properly handle the signals and clean up resources on shutdown. # Input: - No user input is required. # Output: - \\"SIGINT received\\" message upon receiving `SIGINT`. - \\"Alarm Triggered\\" message when the alarm goes off. - \\"SIGTERM received, shutting down gracefully\\" message upon receiving `SIGTERM`. # Example: ```python import signal import time import os def sigint_handler(signum, frame): print(\\"SIGINT received\\") raise KeyboardInterrupt def sigalrm_handler(signum, frame): print(\\"Alarm Triggered\\") def sigterm_handler(signum, frame): print(\\"SIGTERM received, shutting down gracefully\\") clean_up() exit(0) def clean_up(): # Perform any necessary clean-up here print(\\"Performing clean-up tasks\\") def main(): signal.signal(signal.SIGINT, sigint_handler) signal.signal(signal.SIGALRM, sigalrm_handler) signal.signal(signal.SIGTERM, sigterm_handler) # Set the alarm to trigger after 10 seconds signal.alarm(10) print(f\\"Process ID: {os.getpid()} (use this ID to send signals)\\") # Simulate a long-running process try: while True: print(\\"Running...\\") time.sleep(1) except KeyboardInterrupt: clean_up() print(\\"Program terminated by KeyboardInterrupt\\") if __name__ == \\"__main__\\": main() ``` # Instructions: - Implement the given functionality in Python. - Test your program to ensure correctness. - Make sure to handle exceptions gracefully.","solution":"import signal import time import os def sigint_handler(signum, frame): print(\\"SIGINT received\\") raise KeyboardInterrupt def sigalrm_handler(signum, frame): print(\\"Alarm Triggered\\") def sigterm_handler(signum, frame): print(\\"SIGTERM received, shutting down gracefully\\") clean_up() exit(0) def clean_up(): # Perform any necessary clean-up here print(\\"Performing clean-up tasks\\") def main(): signal.signal(signal.SIGINT, sigint_handler) signal.signal(signal.SIGALRM, sigalrm_handler) signal.signal(signal.SIGTERM, sigterm_handler) # Set the alarm to trigger after 10 seconds signal.alarm(10) print(f\\"Process ID: {os.getpid()} (use this ID to send signals)\\") # Simulate a long-running process try: while True: print(\\"Running...\\") time.sleep(1) except KeyboardInterrupt: clean_up() print(\\"Program terminated by KeyboardInterrupt\\") if __name__ == \\"__main__\\": main()"},{"question":"<|Analysis Begin|> The provided documentation extensively covers the usage of the `urllib.request` module from the Python standard library. This module is used for opening and reading URLs. Some major topics included in the documentation are: 1. **Basic Usage**: How to make a simple request to a URL and read the response. 2. **Request Object**: Creating and using request objects to manage more complex interactions such as sending data or custom headers. 3. **Handling Errors**: How to handle the `URLError` and `HTTPError` exceptions. 4. **Building and Installing Openers**: Creating custom openers and handlers, including handling of authentication, proxies, and other situations. 5. **Handling Headers and Data**: How to add custom headers or send data using POST method. 6. **Proxies**: Explanation on managing proxy settings. 7. **Timeouts**: How to set socket timeouts for requests. Based on this analysis, a suitable challenging question could involve making HTTP requests with error handling, sending data, and customizing headers. Additionally, utilizing more advanced features like building custom openers with proxy handling and basic authentication is suitable for students needing to demonstrate a deeper understanding of the module. <|Analysis End|> <|Question Begin|> # Question: Advanced URL Fetching with `urllib.request` You are required to fetch content from a web server, but the process involves handling various complexities. The task is to write a function `fetch_url_content` that performs the following steps: 1. **Request Data**: - Use a POST request to send data to a URL. - Include custom headers, including a `User-Agent` header. - If the server requires authentication, send credentials using basic authentication. 2. **Handle Errors**: - Properly handle `HTTPError` and `URLError` exceptions. - For `HTTPError`, print the HTTP status code and the response body. - For `URLError`, print the reason for failure. 3. **Proxy**: - If a proxy URL is provided, ensure the request passes through the specified proxy. 4. **Timeout**: - Implement a timeout for the request (e.g., 10 seconds). 5. **Return Data**: - If the request is successful, return the content of the response. # Function Definition ```python def fetch_url_content( url: str, data: dict, headers: dict, username: str = None, password: str = None, proxy_url: str = None, timeout: int = 10 ) -> str: Fetch the content from a URL with POST method, custom headers, and optional basic authentication and proxy. Args: - url (str): URL to request data from. - data (dict): Data to send in the POST request. - headers (dict): Custom headers to include in the request. - username (str, optional): Username for basic authentication. Default is None. - password (str, optional): Password for basic authentication. Default is None. - proxy_url (str, optional): Proxy URL to pass the request through. Default is None. - timeout (int, optional): Timeout for the request in seconds. Default is 10. Returns: - str: Content of the response if the request is successful. ``` # Constraints - The function should handle timeout and errors gracefully. - Use `urllib.request` module functionality as described in the document. - Assume `data` to be URL-encoded before sending. - If `proxy_url` is not provided, the request should be made without a proxy. - Avoid using third-party libraries. # Example Usage ```python url = \\"http://example.com/api\\" data = {\\"name\\": \\"John\\", \\"action\\": \\"submit\\"} headers = { \\"User-Agent\\": \\"CustomUserAgent/1.0\\", \\"Content-Type\\": \\"application/x-www-form-urlencoded\\" } username = \\"user\\" password = \\"pass\\" proxy_url = \\"http://proxy.example.com:8080\\" response_content = fetch_url_content(url, data, headers, username, password, proxy_url) print(response_content) ``` Your task is to implement the `fetch_url_content` function.","solution":"import urllib.request import urllib.parse from urllib.error import URLError, HTTPError import base64 def fetch_url_content( url: str, data: dict, headers: dict, username: str = None, password: str = None, proxy_url: str = None, timeout: int = 10 ) -> str: try: # Encode the data to be sent in the POST request post_data = urllib.parse.urlencode(data).encode(\'utf-8\') # Set up the request with the URL, data, and headers request = urllib.request.Request(url, data=post_data, headers=headers) # Add basic authentication if username and password are provided if username and password: credentials = (\'%s:%s\' % (username, password)).encode(\'utf-8\') base64_credentials = base64.b64encode(credentials).decode(\'utf-8\') request.add_header(\'Authorization\', \'Basic %s\' % base64_credentials) # Handle proxy if provided if proxy_url: proxy_handler = urllib.request.ProxyHandler({\'http\': proxy_url, \'https\': proxy_url}) opener = urllib.request.build_opener(proxy_handler) urllib.request.install_opener(opener) # Open the URL with the given timeout with urllib.request.urlopen(request, timeout=timeout) as response: return response.read().decode(\'utf-8\') except HTTPError as e: print(f\\"HTTPError: Code {e.code}, Message: {e.read().decode(\'utf-8\')}\\") return None except URLError as e: print(f\\"URLError: Reason {e.reason}\\") return None"},{"question":"# Programming Challenge: Analyzing and Transforming a DataFrame You are given a dataset containing information about different products. The dataset is provided in the form of a dictionary of lists, where each list represents a column of the dataset. Using pandas, you are required to perform various tasks to analyze and transform this data. Dataset The initial dataset is given as follows: ```python data = { \\"product_id\\": [101, 102, 103, 104, 105], \\"name\\": [\\"Widget\\", \\"Gadget\\", \\"Doodad\\", \\"Widget\\", \\"Gizmo\\"], \\"price\\": [25.50, 40.00, 15.75, 25.50, 55.20], \\"quantity_sold\\": [100, 150, 200, 100, 80], \\"in_stock\\": [True, True, False, True, False] } ``` Your task is to perform the following operations using pandas: 1. **Create a DataFrame:** - Create a DataFrame from the provided dictionary `data`. 2. **Filter Products in Stock:** - Create a new DataFrame that contains only the products that are currently in stock. 3. **Calculate Total Sales:** - Add a new column to the DataFrame that calculates the total sales for each product. (Total sales are determined by multiplying the price by the quantity sold). 4. **Calculate Average Sales Price:** - Calculate the average sales price of all products. 5. **Rename Columns:** - Rename the columns `product_id` to `productID`, `name` to `productName`, `price` to `unitPrice`, `quantity_sold` to `totalSold`, and `in_stock` to `inStock`. 6. **Indexing with Product Name:** - Set the `productName` column as the index of the DataFrame. 7. **Sort by Total Sales:** - Sort the DataFrame based on the total sales in descending order. 8. **Display the Top 3 Products:** - Display the information of the top 3 products based on the total sales. Expected Output ```python # DataFrame after filtering products in stock filtered_df # DataFrame after adding total sales column df_with_sales # Average sales price average_price # DataFrame with renamed columns renamed_df # DataFrame after setting productName as index and sorting by total sales sorted_df # Top 3 products based on total sales top_3_products ``` Constraints - You must only use pandas for performing the operations. - All operations must be chained wherever possible. Implementation Implement a function `analyze_products(data)` that performs the above data manipulation tasks: ```python import pandas as pd def analyze_products(data): # Create DataFrame df = pd.DataFrame(data) # Filter products in stock filtered_df = df[df[\'in_stock\']] # Add total sales column df_with_sales = df.assign(total_sales=lambda x: x[\'price\'] * x[\'quantity_sold\']) # Calculate average sales price average_price = df_with_sales[\'total_sales\'].mean() # Rename columns renamed_df = df_with_sales.rename(columns={ \'product_id\': \'productID\', \'name\': \'productName\', \'price\': \'unitPrice\', \'quantity_sold\': \'totalSold\', \'in_stock\': \'inStock\' }) # Set productName as index and sort by total sales sorted_df = renamed_df.set_index(\'productName\').sort_values(by=\'total_sales\', ascending=False) # Display top 3 products top_3_products = sorted_df.head(3) return filtered_df, df_with_sales, average_price, renamed_df, sorted_df, top_3_products # Example invocation filtered_df, df_with_sales, average_price, renamed_df, sorted_df, top_3_products = analyze_products(data) print(\\"Filtered DataFrame:n\\", filtered_df) print(\\"DataFrame with Total Sales:n\\", df_with_sales) print(\\"Average Sales Price:\\", average_price) print(\\"Renamed DataFrame:n\\", renamed_df) print(\\"Sorted DataFrame:n\\", sorted_df) print(\\"Top 3 Products:n\\", top_3_products) ```","solution":"import pandas as pd def analyze_products(data): # Create DataFrame df = pd.DataFrame(data) # Filter products in stock filtered_df = df[df[\'in_stock\']] # Add total sales column df_with_sales = df.assign(total_sales=lambda x: x[\'price\'] * x[\'quantity_sold\']) # Calculate average sales price average_price = df_with_sales[\'total_sales\'].mean() # Rename columns renamed_df = df_with_sales.rename(columns={ \'product_id\': \'productID\', \'name\': \'productName\', \'price\': \'unitPrice\', \'quantity_sold\': \'totalSold\', \'in_stock\': \'inStock\' }) # Set productName as index and sort by total sales sorted_df = renamed_df.set_index(\'productName\').sort_values(by=\'total_sales\', ascending=False) # Display top 3 products top_3_products = sorted_df.head(3) return filtered_df, df_with_sales, average_price, renamed_df, sorted_df, top_3_products"},{"question":"**Question: Terminal Control Manipulation with `tty` Module** The task is to create a simple terminal-based Python application that demonstrates the usage and effects of the `tty.setraw()` and `tty.setcbreak()` functions from the Python `tty` module. # Problem Statement You are required to implement a Python function that reads input characters from the terminal in raw mode, then switches to cbreak mode, and finally reverts to the original terminal mode. This function should: 1. Use `tty.setraw(fd)` to switch the terminal to raw mode and read characters. 2. Use `tty.setcbreak(fd)` to switch the terminal to cbreak mode and read characters. 3. Revert the terminal to its original mode after reading the characters in both modes. # Function Signature ```python def terminal_control_demo(fd: int): pass ``` # Input - **fd** (int): The file descriptor of the terminal (e.g., 0 for stdin). # Output The function should not return any value, but it should print the characters read from the terminal in both modes as follows: 1. Print each character read in raw mode prepending it with \\"Raw Mode: \\". 2. Print each character read in cbreak mode prepending it with \\"CBreak Mode: \\". # Constraints - It is guaranteed that **fd** is a valid terminal file descriptor. - The program should handle switching terminal modes safely and revert to the original state to prevent terminal misbehavior after the script exits. # Example Execution Provide a script to test the function: ```python import sys import tty import termios def terminal_control_demo(fd: int): original_attrs = termios.tcgetattr(fd) try: # Raw Mode tty.setraw(fd) print(\\"Switched to raw mode. Type \'q\' to switch to cbreak mode...\\") while True: ch = sys.stdin.read(1) if ch == \'q\': break print(f\\"Raw Mode: {ch}\\") # Cbreak Mode tty.setcbreak(fd) print(\\"Switched to cbreak mode. Type \'q\' to quit...\\") while True: ch = sys.stdin.read(1) if ch == \'q\': break print(f\\"CBreak Mode: {ch}\\") finally: # Restore the original terminal mode termios.tcsetattr(fd, termios.TCSANOW, original_attrs) print(\\"Restored original terminal mode.\\") if __name__ == \\"__main__\\": terminal_control_demo(sys.stdin.fileno()) ``` # Notes - Ensure you handle the terminal appropriately and restore it to its original state to avoid terminal misbehavior. - This script should be run in a Unix-based terminal.","solution":"import sys import tty import termios def terminal_control_demo(fd: int): original_attrs = termios.tcgetattr(fd) try: # Raw Mode tty.setraw(fd) print(\\"Switched to raw mode. Type \'q\' to switch to cbreak mode...\\") while True: ch = sys.stdin.read(1) if ch == \'q\': break print(f\\"Raw Mode: {ch}\\") # Cbreak Mode tty.setcbreak(fd) print(\\"Switched to cbreak mode. Type \'q\' to quit...\\") while True: ch = sys.stdin.read(1) if ch == \'q\': break print(f\\"CBreak Mode: {ch}\\") finally: # Restore the original terminal mode termios.tcsetattr(fd, termios.TCSANOW, original_attrs) print(\\"Restored original terminal mode.\\") if __name__ == \\"__main__\\": terminal_control_demo(sys.stdin.fileno())"},{"question":"Coding Assessment Question # Objective Write a PyTorch function that demonstrates the effect of the `fill_uninitialized_memory` attribute on deterministic algorithms. Your function should perform a series of tensor operations that are influenced by this attribute and compare the outputs when `fill_uninitialized_memory` is set to `True` and `False`. # Function Signature ```python def compare_fill_uninitialized_memory() -> Tuple[torch.Tensor, torch.Tensor]: # Perform specified tensor operations with fill_uninitialized_memory=True # Perform the same operations with fill_uninitialized_memory=False # Return a tuple containing two tensors, one for each setting ``` # Expected Input and Output - The function does not take any arguments. - The function returns a tuple of two tensors: 1. The first tensor is the result of operations with `fill_uninitialized_memory` set to `True`. 2. The second tensor is the result of operations with `fill_uninitialized_memory` set to `False`. # Constraints and Limitations 1. You must use operations such as `torch.empty` or `torch.Tensor.resize_` that are affected by `fill_uninitialized_memory`. 2. Ensure that the tensor operations you choose can highlight the differences in uninitialized memory handling. # Example Usage ```python import torch def compare_fill_uninitialized_memory(): # Set fill_uninitialized_memory to True and perform operations torch.utils.deterministic.fill_uninitialized_memory = True torch.use_deterministic_algorithms(True) tensor1 = torch.empty((3, 3)) # This should contain NaN values # Set fill_uninitialized_memory to False and perform operations torch.utils.deterministic.fill_uninitialized_memory = False torch.use_deterministic_algorithms(True) tensor2 = torch.empty((3, 3)) # This may contain uninitialized random values return tensor1, tensor2 # Example function call and output t1, t2 = compare_fill_uninitialized_memory() print(\\"Tensor with fill_uninitialized_memory=True:\\") print(t1) print(\\"Tensor with fill_uninitialized_memory=False:\\") print(t2) ``` # Note - Make sure to reset `fill_uninitialized_memory` and `torch.use_deterministic_algorithms()` to their original states after your function to avoid side effects in further PyTorch operations. - You may assume that the function `torch.use_deterministic_algorithms()` accepts a `True` or `False` flag to indicate the use of deterministic algorithms.","solution":"import torch from typing import Tuple def compare_fill_uninitialized_memory() -> Tuple[torch.Tensor, torch.Tensor]: Perform tensor operations influenced by the fill_uninitialized_memory setting and compare the outputs when the setting is True and False. Returns: Tuple[torch.Tensor, torch.Tensor]: A tuple containing two tensors, one for each setting. # Save original settings original_fill_uninitialized_memory = torch.utils.deterministic.fill_uninitialized_memory original_use_deterministic_algorithms = torch.are_deterministic_algorithms_enabled() # Set fill_uninitialized_memory to True and perform operations torch.utils.deterministic.fill_uninitialized_memory = True torch.use_deterministic_algorithms(True) tensor1 = torch.empty((3, 3)) # This should contain NaN values # Set fill_uninitialized_memory to False and perform operations torch.utils.deterministic.fill_uninitialized_memory = False torch.use_deterministic_algorithms(True) tensor2 = torch.empty((3, 3)) # This may contain uninitialized random values # Restore original settings torch.utils.deterministic.fill_uninitialized_memory = original_fill_uninitialized_memory torch.use_deterministic_algorithms(original_use_deterministic_algorithms) return tensor1, tensor2"},{"question":"Implement a simple asynchronous chat server and client using the `asyncore` module. The chat server should handle multiple clients, broadcasting any received message to all connected clients (except the sender). Requirements: 1. **Chat Server**: - The server should listen on a specified host and port. - When a client connects, add the client to a list of connected clients. - When a client sends a message, broadcast it to all other connected clients. - Handle the disconnection of clients gracefully. 2. **Chat Client**: - The client should connect to the server on the specified host and port. - The client should continuously read user input from the terminal and send it to the server. - The client should also listen for broadcast messages from the server and display them on the terminal. Input - The server should accept two command-line arguments: host and port. - The client should accept two command-line arguments: host and port. Output - The server should print connection and disconnection messages along with broadcasted messages. - The client should print received messages from the server. Constraints - Use the `asyncore` module for asynchronous handling. - Ensure proper error handling and edge cases, such as empty messages or disconnections. Example - Start the server: `python chat_server.py localhost 8080` - Connect a client: `python chat_client.py localhost 8080` - Chat messages should be broadcasted to all connected clients. Performance Requirement - The server should efficiently handle broadcasting messages to multiple clients asynchronously without blocking. Server Code Template ```python import asyncore import socket class ChatServer(asyncore.dispatcher): def __init__(self, host, port): super().__init__() self.create_socket(socket.AF_INET, socket.SOCK_STREAM) self.set_reuse_addr() self.bind((host, port)) self.listen(5) self.clients = [] def handle_accepted(self, sock, addr): print(f\'Connection from {addr}\') client_handler = ChatHandler(sock, self) self.clients.append(client_handler) def broadcast(self, message, source): for client in self.clients: if client != source: client.send_message(message) def remove_client(self, client): self.clients.remove(client) class ChatHandler(asyncore.dispatcher_with_send): def __init__(self, sock, server): super().__init__(sock) self.server = server def handle_read(self): data = self.recv(8192) if data: self.server.broadcast(data, self) else: self.handle_close() def handle_close(self): print(\'Client disconnected\') self.server.remove_client(self) self.close() def send_message(self, message): self.send(message) if __name__ == \'__main__\': import sys host, port = sys.argv[1], int(sys.argv[2]) server = ChatServer(host, port) asyncore.loop() ``` Client Code Template ```python import asyncore import socket import threading class ChatClient(asyncore.dispatcher): def __init__(self, host, port): super().__init__() self.create_socket(socket.AF_INET, socket.SOCK_STREAM) self.connect((host, port)) def handle_connect(self): pass def handle_close(self): self.close() def handle_read(self): data = self.recv(8192) if data: print(data.decode(\'utf-8\')) def writable(self): return False def send_message(self, message): self.send(message.encode(\'utf-8\')) def input_thread(client): while True: message = input() if message: client.send_message(message) if __name__ == \'__main__\': import sys host, port = sys.argv[1], int(sys.argv[2]) client = ChatClient(host, port) threading.Thread(target=input_thread, args=(client,)).start() asyncore.loop() ```","solution":"import asyncore import socket class ChatServer(asyncore.dispatcher): def __init__(self, host, port): super().__init__() self.create_socket(socket.AF_INET, socket.SOCK_STREAM) self.set_reuse_addr() self.bind((host, port)) self.listen(5) self.clients = [] def handle_accepted(self, sock, addr): print(f\'Connection from {addr}\') client_handler = ChatHandler(sock, self) self.clients.append(client_handler) def broadcast(self, message, source): for client in self.clients: if client != source: client.send_message(message) def remove_client(self, client): self.clients.remove(client) print(f\'Client {client.getpeername()} disconnected\') class ChatHandler(asyncore.dispatcher_with_send): def __init__(self, sock, server): super().__init__(sock) self.server = server def handle_read(self): data = self.recv(8192) if data: print(f\\"Received message: {data}\\") self.server.broadcast(data, self) else: self.handle_close() def handle_close(self): self.server.remove_client(self) self.close() def send_message(self, message): self.send(message) if __name__ == \'__main__\': import sys host, port = sys.argv[1], int(sys.argv[2]) server = ChatServer(host, port) asyncore.loop() import asyncore import socket import threading class ChatClient(asyncore.dispatcher): def __init__(self, host, port): super().__init__() self.create_socket(socket.AF_INET, socket.SOCK_STREAM) self.connect((host, port)) def handle_connect(self): print(\'Connected to the server\') def handle_close(self): print(\'Disconnected from the server\') self.close() def handle_read(self): data = self.recv(8192) if data: print(f\\"Message from server: {data.decode(\'utf-8\')}\\") def writable(self): return False def send_message(self, message): self.send(message.encode(\'utf-8\')) def input_thread(client): while True: message = input() if message: client.send_message(message) if __name__ == \'__main__\': import sys host, port = sys.argv[1], int(sys.argv[2]) client = ChatClient(host, port) threading.Thread(target=input_thread, args=(client,)).start() asyncore.loop()"},{"question":"Objective The objective of this question is to assess students\' ability to manipulate pandas Series data, including importing data, performing data transformations, handling missing values, and summarizing data using statistical functions. Problem Statement You are provided with a CSV file containing sales data for a retail company over the past two years. The CSV file (`sales_data.csv`) has the following columns: - `date`: The date of the transaction. - `store_id`: The ID of the store where the transaction took place. - `item_id`: The ID of the item sold. - `units_sold`: Number of units sold in the transaction. - `revenue`: Revenue generated from the transaction. Your Tasks 1. **Load the Data:** - Load the CSV file into a pandas Series object for each column. 2. **Handle Missing Values:** - Identify and handle missing values in the `units_sold` and `revenue` columns by replacing them with the median value of the respective columns. 3. **Convert Data Types:** - Ensure the `date` column is of datetime type and the other columns are of appropriate types (`store_id` and `item_id` as integers, and `units_sold` and `revenue` as floats). 4. **Data Transformation:** - Create two new Series: - `daily_revenue`: Total revenue generated each day. - `average_units_per_day`: Average number of units sold per item per day. 5. **Statistical Summarization:** - Calculate the following statistics: - The total revenue for the entire period. - The mean and standard deviation of daily revenue. - The median of average units sold per item per day. - The store with the highest total revenue (identified by `store_id`). 6. **Visualization:** - Plot the daily revenue as a time series plot. Input and Output Formats **Input:** - `sales_data.csv` file provided in the working directory. **Output:** - Print or display the following in an organized manner: - Descriptive statistics calculated in Task 5. - The time series plot for daily revenue. Constraints - Ensure efficient handling of large datasets (potentially over 100,000 rows). - Use appropriate pandas methods to perform operations efficiently. Implementation Template ```python import pandas as pd import matplotlib.pyplot as plt # Task 1: Load the Data def load_data(file_path): # Load the CSV file into a pandas DataFrame df = pd.read_csv(file_path) return df # Task 2: Handle Missing Values def handle_missing_values(df): # Replace missing values in \'units_sold\' and \'revenue\' columns with median value df[\'units_sold\'].fillna(df[\'units_sold\'].median(), inplace=True) df[\'revenue\'].fillna(df[\'revenue\'].median(), inplace=True) return df # Task 3: Convert Data Types def convert_data_types(df): # Convert \'date\' column to datetime type df[\'date\'] = pd.to_datetime(df[\'date\']) # Ensure other columns have appropriate data types df[\'store_id\'] = df[\'store_id\'].astype(int) df[\'item_id\'] = df[\'item_id\'].astype(int) df[\'units_sold\'] = df[\'units_sold\'].astype(float) df[\'revenue\'] = df[\'revenue\'].astype(float) return df # Task 4: Data Transformation def transform_data(df): # Create \'daily_revenue\' Series daily_revenue = df.groupby(\'date\')[\'revenue\'].sum() # Create \'average_units_per_day\' Series average_units_per_day = df.groupby(\'date\')[\'units_sold\'].mean() return daily_revenue, average_units_per_day # Task 5: Statistical Summarization def summarize_data(daily_revenue, average_units_per_day, df): # Total revenue total_revenue = daily_revenue.sum() # Mean and standard deviation of daily revenue mean_daily_revenue = daily_revenue.mean() std_daily_revenue = daily_revenue.std() # Median of average units sold per item per day median_avg_units = average_units_per_day.median() # Store with highest total revenue store_revenue = df.groupby(\'store_id\')[\'revenue\'].sum() highest_revenue_store = store_revenue.idxmax() print(\\"Total Revenue: \\", total_revenue) print(\\"Mean Daily Revenue: \\", mean_daily_revenue) print(\\"Standard Deviation of Daily Revenue: \\", std_daily_revenue) print(\\"Median of Average Units Sold per Day: \\", median_avg_units) print(\\"Store with Highest Total Revenue: \\", highest_revenue_store) # Task 6: Visualization def plot_daily_revenue(daily_revenue): daily_revenue.plot(title=\'Daily Revenue Over Time\', xlabel=\'Date\', ylabel=\'Revenue\') plt.show() # Main function to execute tasks def main(file_path): df = load_data(file_path) df = handle_missing_values(df) df = convert_data_types(df) daily_revenue, average_units_per_day = transform_data(df) summarize_data(daily_revenue, average_units_per_day, df) plot_daily_revenue(daily_revenue) # Execute the main function if __name__ == \\"__main__\\": file_path = \'sales_data.csv\' main(file_path) ```","solution":"import pandas as pd import matplotlib.pyplot as plt # Task 1: Load the Data def load_data(file_path): df = pd.read_csv(file_path) return df # Task 2: Handle Missing Values def handle_missing_values(df): df[\'units_sold\'].fillna(df[\'units_sold\'].median(), inplace=True) df[\'revenue\'].fillna(df[\'revenue\'].median(), inplace=True) return df # Task 3: Convert Data Types def convert_data_types(df): df[\'date\'] = pd.to_datetime(df[\'date\']) df[\'store_id\'] = df[\'store_id\'].astype(int) df[\'item_id\'] = df[\'item_id\'].astype(int) df[\'units_sold\'] = df[\'units_sold\'].astype(float) df[\'revenue\'] = df[\'revenue\'].astype(float) return df # Task 4: Data Transformation def transform_data(df): daily_revenue = df.groupby(\'date\')[\'revenue\'].sum() average_units_per_day = df.groupby(\'date\')[\'units_sold\'].mean() return daily_revenue, average_units_per_day # Task 5: Statistical Summarization def summarize_data(daily_revenue, average_units_per_day, df): total_revenue = daily_revenue.sum() mean_daily_revenue = daily_revenue.mean() std_daily_revenue = daily_revenue.std() median_avg_units = average_units_per_day.median() store_revenue = df.groupby(\'store_id\')[\'revenue\'].sum() highest_revenue_store = store_revenue.idxmax() stats = { \\"Total Revenue\\": total_revenue, \\"Mean Daily Revenue\\": mean_daily_revenue, \\"Standard Deviation of Daily Revenue\\": std_daily_revenue, \\"Median of Average Units Sold per Day\\": median_avg_units, \\"Store with Highest Total Revenue\\": highest_revenue_store } return stats # Task 6: Visualization def plot_daily_revenue(daily_revenue): daily_revenue.plot(title=\'Daily Revenue Over Time\', xlabel=\'Date\', ylabel=\'Revenue\') plt.show() # Main function to execute tasks def main(file_path): df = load_data(file_path) df = handle_missing_values(df) df = convert_data_types(df) daily_revenue, average_units_per_day = transform_data(df) stats = summarize_data(daily_revenue, average_units_per_day, df) for key, value in stats.items(): print(f\\"{key}: {value}\\") plot_daily_revenue(daily_revenue) # Execute the main function if __name__ == \\"__main__\\": file_path = \'sales_data.csv\' main(file_path)"},{"question":"# Question: List Manipulations and String Formatting Problem Statement You are given a list of strings representing names of students along with their scores stored in a separate list. You have to perform multiple tasks using Python. Complete the following function as per the instructions: Implement the function: ```python def student_performance(names, scores): Args: names : List[str] -- A list of student names scores : List[int] -- A list of corresponding scores for each student Returns: (str, List[str], List[int], List[float]) -- A tuple containing: - str : The name of the student with the highest score - List[str] : The list of student names sorted in descending order of their scores - List[int] : The list of scores sorted in descending order - List[float] : The list of average scores when students are grouped into pairs in the order of the original input list. # Your implementation here pass ``` # Functionality Requirements: 1. **Highest Score**: - Find out the student with the highest score and return their name. 2. **Sorted Lists**: - Return a list of student names, sorted based on their scores in descending order. - Return a list of scores, sorted in descending order. 3. **Average Scores**: - Pair each student with the next one in the list and calculate their average score. - If there is an odd number of students, the last student should pair with a fictitious student having the score 0 for calculating the average. - Return a list of such average scores. # Constraints: - `len(names) == len(scores)` - `0 <= len(names) <= 100` - `0 <= scores[i] <= 100` # Example: ```python names = [\\"Alice\\", \\"Bob\\", \\"Charlie\\", \\"David\\"] scores = [88, 95, 70, 60] result = student_performance(names, scores) print(result) ``` **Expected Output:** ```python (\\"Bob\\", [\\"Bob\\", \\"Alice\\", \\"Charlie\\", \\"David\\"], [95, 88, 70, 60], [91.5, 65.0]) ``` Explanation: - \\"Bob\\" has the highest score. - Sorted names and scores based on scores in descending order. - The averages are calculated as: (88+95)/2 = 91.5, (70+60)/2 = 65.0 # Note: Use appropriate Python features like list comprehensions, slicing, and built-in functions to ensure an efficient solution.","solution":"def student_performance(names, scores): Args: names : List[str] -- A list of student names scores : List[int] -- A list of corresponding scores for each student Returns: (str, List[str], List[int], List[float]) -- A tuple containing: - str : The name of the student with the highest score - List[str] : The list of student names sorted in descending order of their scores - List[int] : The list of scores sorted in descending order - List[float] : The list of average scores when students are grouped into pairs in the order of the original input list. if not names or not scores: return (\\"\\", [], [], []) # Find the student with the highest score max_score_index = scores.index(max(scores)) highest_scorer = names[max_score_index] # Sort the students by scores in descending order sorted_pairs = sorted(zip(names, scores), key=lambda x: x[1], reverse=True) sorted_names = [name for name, _ in sorted_pairs] sorted_scores = [score for _, score in sorted_pairs] # Calculate the average scores for pairs avg_scores = [] for i in range(0, len(scores), 2): if i+1 < len(scores): avg_scores.append((scores[i] + scores[i+1]) / 2) else: avg_scores.append((scores[i] + 0) / 2) return (highest_scorer, sorted_names, sorted_scores, avg_scores)"},{"question":"Objective: Create a Python module that utilizes the \\"unittest\\" framework to test functionalities of a sample class. The focus will be on: 1. Implementing multiple test cases. 2. Using setup and teardown methods. 3. Aggregating tests into a test suite. 4. Utilizing test skipping and expected failure mechanisms. Problem Statement: You are given a class `MathOperations` which has the following methods: - `add(a, b)`: Returns the sum of `a` and `b`. - `subtract(a, b)`: Returns the difference between `a` and `b`. - `multiply(a, b)`: Returns the product of `a` and `b`. - `divide(a, b)`: Returns the division result of `a` over `b`. Raises `ZeroDivisionError` if `b` is zero. You need to: 1. Implement a unittest class `TestMathOperations` to test the `MathOperations` class. 2. Provide test methods to validate each mathematical operation. 3. Implement `setUp()` and `tearDown()` methods to initialize common resources. 4. Create a custom test suite that aggregates all the tests. 5. Utilize the `skip` decorator to skip the `subtract` method test if a specific condition (e.g., a configuration flag) is true. 6. Mark the `divide` method test where division by zero is expected as an expected failure. Specific Requirements: 1. **Implementation of Test Methods**: - `test_addition`: Check if `add` method returns expected result. - `test_subtraction`: Check if `subtract` method returns expected result. Skip this test if a boolean flag `skip_subtract` is `True`. - `test_multiplication`: Check if `multiply` method returns expected result. - `test_division`: Check if `divide` method returns expected result when divisor is not zero, and if it raises `ZeroDivisionError` otherwise. 2. **Usage of `setUp` and `tearDown`**: - Use `setUp` to initialize an instance of `MathOperations`. - Use `tearDown` to delete the instance and perform any cleanup if necessary. 3. **Custom Test Suite**: - Create a method `suite` to build a custom test suite aggregating all the test methods. 4. **Test Skipping and Expected Failure**: - Skip the `test_subtraction` method based on a condition. - Mark `test_division_by_zero` as an expected failure using `expectedFailure` decorator. Input/Output: **Input**: No direct input from the user. The `MathOperations` class should look like this: ```python class MathOperations: def add(self, a, b): return a + b def subtract(self, a, b): return a - b def multiply(self, a, b): return a * b def divide(self, a, b): if b == 0: raise ZeroDivisionError(\\"Division by zero is undefined\\") return a / b ``` **Output**: Running the tests should provide a detailed output of the number of tests run, skipped, and marked as expected failures. Example: Here\'s an example to get you started: ```python import unittest class MathOperations: # Methods as described above class TestMathOperations(unittest.TestCase): def setUp(self): self.math_ops = MathOperations() def tearDown(self): del self.math_ops def test_addition(self): self.assertEqual(self.math_ops.add(1, 2), 3) @unittest.skip(\\"Skip subtraction until further notice\\") # Adjust skipping condition as instructed def test_subtraction(self): self.assertEqual(self.math_ops.subtract(2, 1), 1) def test_multiplication(self): self.assertEqual(self.math_ops.multiply(2, 3), 6) @unittest.expectedFailure def test_division(self): with self.assertRaises(ZeroDivisionError): self.math_ops.divide(1, 0) @staticmethod def suite(): suite = unittest.TestSuite() suite.addTest(unittest.makeSuite(TestMathOperations)) return suite if __name__ == \'__main__\': runner = unittest.TextTestRunner() runner.run(TestMathOperations.suite()) ``` Complete the implementation and run the tests to ensure all requirements are met.","solution":"import unittest class MathOperations: A class containing basic mathematical operations. def add(self, a, b): return a + b def subtract(self, a, b): return a - b def multiply(self, a, b): return a * b def divide(self, a, b): if b == 0: raise ZeroDivisionError(\\"Division by zero is undefined\\") return a / b class TestMathOperations(unittest.TestCase): Unit tests for MathOperations. skip_subtract = True # Boolean flag to control the skipping of the subtraction test def setUp(self): self.math_ops = MathOperations() def tearDown(self): del self.math_ops def test_addition(self): self.assertEqual(self.math_ops.add(1, 2), 3) self.assertEqual(self.math_ops.add(-1, -1), -2) self.assertEqual(self.math_ops.add(0, 0), 0) @unittest.skipIf(skip_subtract, \\"Skipping subtraction test based on control flag\\") def test_subtraction(self): self.assertEqual(self.math_ops.subtract(2, 1), 1) self.assertEqual(self.math_ops.subtract(0, 0), 0) self.assertEqual(self.math_ops.subtract(-1, -1), 0) def test_multiplication(self): self.assertEqual(self.math_ops.multiply(2, 3), 6) self.assertEqual(self.math_ops.multiply(0, 5), 0) self.assertEqual(self.math_ops.multiply(-1, -1), 1) def test_division(self): self.assertEqual(self.math_ops.divide(9, 3), 3) self.assertEqual(self.math_ops.divide(-3, -1), 3) self.assertEqual(self.math_ops.divide(5, 2), 2.5) @unittest.expectedFailure def test_division_by_zero(self): self.math_ops.divide(1, 0) @staticmethod def suite(): suite = unittest.TestSuite() suite.addTest(unittest.makeSuite(TestMathOperations)) return suite if __name__ == \'__main__\': runner = unittest.TextTestRunner() runner.run(TestMathOperations.suite())"},{"question":"You are tasked with extending Python using the C-API to define a custom type that represents a simple mathematical vector. Your vector class will support basic attribute access and a few methods. # Specifications: 1. **Vector Type Creation**: - Define a new type `Vector` using `PyVarObject` and its associated macros. - Include attributes for `x`, `y`, and `z` (all integers). 2. **Attributes**: - Implement read/write access for attributes `x`, `y`, and `z`. - If `z` is accessed but not set, it should dynamically compute the sum of `x` and `y`. 3. **Methods**: - Implement a method `magnitude` that computes the Euclidean distance of the vector from the origin. - Implement a class method `unit_vector` that creates a unit vector in the direction of `(1, 1, 1)`. # Requirements: 1. **Vector Type Definition**: - Use `PyVarObject_HEAD_INIT` and related macros for defining the type. - Ensure proper initialization and deallocation. 2. **Attribute Access**: - Define `PyMemberDef` entries for `x` and `y` with read/write access. - Define a `PyGetSetDef` entry for `z`, dynamically computing its value as `x + y` if not set. 3. **Method Implementation**: - `magnitude` — Implement using `METH_NOARGS` to compute the magnitude. - `unit_vector` — Implement as a class method using `METH_CLASS`. # Input/Output and Function Signatures: - **Constructor**: `Vector(x, y)` - **Input**: Two integers, `x` and `y`. - **Output**: A new Vector object. - **Attribute Access**: - `vector.x`, `vector.y`, `vector.z` (with `z` being computed if not explicitly set). - **magnitude()**: - **Output**: Euclidean distance (float). - **Signature**: `PyObject* Vector_magnitude(VectorObject* self, PyObject* args)` - **unit_vector()**: - **Output**: A unit vector `(1, 1, 1)` - **Signature**: `PyObject* Vector_unit_vector(PyObject* cls, PyObject* args)` # Constraints: - Implement using C-API for Python 3.10. - Ensure proper memory management and error handling. - The solution should illustrate a fundamental understanding of Python object internals and extension writing.","solution":"from math import sqrt class Vector: def __init__(self, x, y): self._x = x self._y = y self._z = None # Lazily computed attribute @property def x(self): return self._x @x.setter def x(self, value): self._x = value self._update_z() @property def y(self): return self._y @y.setter def y(self, value): self._y = value self._update_z() @property def z(self): if self._z is None: return self.x + self.y return self._z @z.setter def z(self, value): self._z = value def _update_z(self): self._z = None # Reset z to be computed dynamically def magnitude(self): return sqrt(self.x ** 2 + self.y ** 2 + self.z ** 2) @classmethod def unit_vector(cls): return cls(1, 1)"},{"question":"**Coding Assessment Question:** # Processing Form Data in a CGI Script Background: A CGI script is often used to process data submitted through an HTML form and return the results to the user within an HTTP response. You will implement a Python CGI script that processes form data, including handling text inputs and file uploads. Task: You are required to create a Python script that handles form data submission through a CGI interface. Your script should: 1. Handle text input from form fields. 2. Process a file upload from the user, ensuring that it reads the file content and counts the number of lines in the uploaded file. 3. Generate a well-formed HTML response that includes the submitted form data and the number of lines in the uploaded file. Requirements: 1. Import the necessary modules: `cgi` and `cgitb`. 2. Enable detailed error reports using `cgitb`. 3. Parse the form data using `cgi.FieldStorage`. 4. Check for specific form fields such as `username` and `userfile`. 5. If the `username` is missing, return an error message in the HTML response. 6. If a file is uploaded (`userfile` field), read the file and count the number of lines. 7. Generate an HTML response that displays the submitted `username` and the line count of the uploaded file. 8. Ensure proper handling of both single and multiple values for form fields. Input and Output Formats: - **Input:** - The script will receive input from an HTML form via HTTP. - Expected form fields: `username` (text), `userfile` (file upload). - **Output:** - An HTTP response with Content-Type `text/html`. - An HTML document with the following structure: ```html <html> <head><title>CGI Script Output</title></head> <body> <h1>Form Submission Result</h1> <p>Username: [Submitted Username]</p> <p>File Line Count: [Number of Lines]</p> </body> </html> ``` Constraints: - The script must handle both GET and POST methods for form submission. - Assume the form fields are `username` and `userfile`. Examples: - **Example HTML Form:** ```html <form action=\\"/cgi-bin/your_script.py\\" method=\\"post\\" enctype=\\"multipart/form-data\\"> Username: <input type=\\"text\\" name=\\"username\\"><br> File: <input type=\\"file\\" name=\\"userfile\\"><br> <input type=\\"submit\\" value=\\"Submit\\"> </form> ``` - **Example Output:** ```html <html> <head><title>CGI Script Output</title></head> <body> <h1>Form Submission Result</h1> <p>Username: JohnDoe</p> <p>File Line Count: 15</p> </body> </html> ``` Implementation: Write a Python function `process_form()` to achieve the above tasks. ```python import cgi import cgitb def process_form(): cgitb.enable() # Print the necessary headers. print(\\"Content-Type: text/html\\") print() form = cgi.FieldStorage() username = form.getfirst(\\"username\\", \\"\\").strip() userfile = form[\\"userfile\\"] if \\"userfile\\" in form else None if not username: print(\\"<html><body><h1>Error</h1><p>Please provide a username.</p></body></html>\\") return line_count = 0 if userfile and userfile.file: while True: line = userfile.file.readline() if not line: break line_count += 1 # Generating the HTML response print(\\"<html>\\") print(\\"<head><title>CGI Script Output</title></head>\\") print(\\"<body>\\") print(\\"<h1>Form Submission Result</h1>\\") print(f\\"<p>Username: {username}</p>\\") if userfile: print(f\\"<p>File Line Count: {line_count}</p>\\") else: print(\\"<p>No file uploaded.</p>\\") print(\\"</body>\\") print(\\"</html>\\") ``` Place this script in the appropriate CGI directory of your HTTP server (e.g., `/cgi-bin`), and ensure it is executable (`chmod 0755 script.py`). Test the script by submitting the form via a browser.","solution":"import cgi import cgitb def process_form(): cgitb.enable() # Print the necessary headers. print(\\"Content-Type: text/html\\") print() form = cgi.FieldStorage() username = form.getfirst(\\"username\\", \\"\\").strip() userfile = form[\\"userfile\\"] if \\"userfile\\" in form else None if not username: print(\\"<html><body><h1>Error</h1><p>Please provide a username.</p></body></html>\\") return line_count = 0 if userfile and userfile.file: for line in userfile.file: line_count += 1 # Generating the HTML response print(\\"<html>\\") print(\\"<head><title>CGI Script Output</title></head>\\") print(\\"<body>\\") print(\\"<h1>Form Submission Result</h1>\\") print(f\\"<p>Username: {username}</p>\\") if userfile: print(f\\"<p>File Line Count: {line_count}</p>\\") else: print(\\"<p>No file uploaded.</p>\\") print(\\"</body>\\") print(\\"</html>\\")"},{"question":"**Problem Statement:** You are required to implement a logging configuration function using Python\'s `logging.config` module. The function should take a dictionary as input and apply the logging configuration specified in that dictionary. Additionally, you need to handle user-defined formatters. **Function Signature:** ```python def setup_logging(config: dict) -> None: pass ``` # Input: - `config`: A dictionary that defines the logging configuration. The dictionary follows the schema defined in the `logging.config.dictConfig` documentation. # Output: - The function does not return anything. It should properly configure logging based on the provided `config`. # Requirements: 1. Implement the `setup_logging` function to configure logging using the `logging.config.dictConfig` method. 2. Demonstrate handling of user-defined formatters alongside built-in ones. 3. Ensure the function raises appropriate exceptions if the configuration is invalid. # Example: ```python config_example = { \'version\': 1, \'formatters\': { \'standard\': { \'format\': \'%(asctime)s - %(name)s - %(levelname)s - %(message)s\' }, \'custom\': { \'()\': \'my_formatter_function\', \'format\': \'%(name)s - %(levelname)s - %(message)s\' } }, \'handlers\': { \'console\': { \'class\': \'logging.StreamHandler\', \'level\': \'DEBUG\', \'formatter\': \'standard\', \'stream\': \'ext://sys.stdout\' } }, \'loggers\': { \'\': { \'handlers\': [\'console\'], \'level\': \'DEBUG\', \'propagate\': True } } } def my_formatter_function(format): import logging return logging.Formatter(format) setup_logging(config_example) import logging logger = logging.getLogger(__name__) logger.debug(\\"This is a debug message\\") ``` **Constraints:** - The configuration dictionary must follow the schema described in the documentation. - All references to external objects should be valid (use `ext://` for system streams like `sys.stdout`). - Handle potential exceptions (`ValueError`, `TypeError`, `AttributeError`, `ImportError`) gracefully and ensure that errors in configuration raise appropriate messages. # Notes: - You may assume user-defined formatters will be provided in the same file or module. - Ensure the solution is well-documented and follows best practices for writing configurable loggers in Python.","solution":"import logging.config def setup_logging(config: dict) -> None: Configures the logging using the provided dictionary. Parameters: config (dict): A dictionary that defines the logging configuration. Raises: ValueError, TypeError, AttributeError, ImportError: for invalid configurations or references. try: logging.config.dictConfig(config) except (ValueError, TypeError, AttributeError, ImportError) as e: raise RuntimeError(f\\"Error setting up logging configuration: {e}\\") # User-defined formatter example function def my_formatter_function(format): import logging return logging.Formatter(format)"},{"question":"# Question: Asynchronous HTTP Fetcher **Objective:** Implement an asynchronous HTTP fetcher using the `asyncio` module in Python. **Requirements:** - Create an asynchronous function `fetch_url(url: str) -> dict` that takes a URL as input and returns a dictionary with the keys `\\"url\\"`, `\\"status\\"`, and `\\"content_length\\"`. - The `\\"url\\"` key should store the URL that was fetched. - The `\\"status\\"` key should store the HTTP status code of the response. - The `\\"content_length\\"` key should store the length of the response content. - Create another asynchronous function `fetch_all(urls: List[str]) -> List[dict]` that takes a list of URLs and retrieves each URL asynchronously using the `fetch_url` function. It should return a list of dictionaries as described above. - Ensure error handling for network errors (e.g., timeouts, connection errors) and set the `\\"status\\"` key to `None` and `\\"content_length\\"` to `0` in such cases. **Input:** - `fetch_url` takes a single string, which is the URL to be fetched. - `fetch_all` takes a list of strings, each representing a URL. **Output:** - `fetch_url` returns a dictionary with the structure `{ \\"url\\": str, \\"status\\": int or None, \\"content_length\\": int }`. - `fetch_all` returns a list of dictionaries with the above structure. **Constraints:** - Use the `aiohttp` library to perform HTTP requests asynchronously. - Handle up to 100 URLs in a single call to `fetch_all`. ```python import aiohttp import asyncio from typing import List, Dict async def fetch_url(url: str) -> Dict[str, any]: # Implement this function pass async def fetch_all(urls: List[str]) -> List[Dict[str, any]]: # Implement this function pass # Example Usage # urls = [\\"http://example.com\\", \\"http://example.org\\"] # result = asyncio.run(fetch_all(urls)) # print(result) ``` **Hints:** 1. You might find `aiohttp.ClientSession` and `aiohttp.ClientResponse` useful. 2. Use `asyncio.gather` to fetch multiple URLs concurrently. 3. Remember to handle exceptions such as `aiohttp.ClientError`. **Performance Requirements:** - Your implementation should efficiently handle concurrent fetching of URLs within the provided constraints.","solution":"import aiohttp import asyncio from typing import List, Dict async def fetch_url(url: str) -> Dict[str, any]: async with aiohttp.ClientSession() as session: try: async with session.get(url) as response: content = await response.read() return { \\"url\\": url, \\"status\\": response.status, \\"content_length\\": len(content) } except aiohttp.ClientError: return { \\"url\\": url, \\"status\\": None, \\"content_length\\": 0 } async def fetch_all(urls: List[str]) -> List[Dict[str, any]]: tasks = [fetch_url(url) for url in urls] return await asyncio.gather(*tasks)"},{"question":"<|Analysis Begin|> The provided documentation describes a Python module named `grp` that provides access to the Unix group database. The key points from the documentation are: 1. **Group Entry Structure**: Group database entries are represented as a tuple with the following fields: - `gr_name`: Name of the group (string). - `gr_passwd`: Encrypted group password (string, often empty). - `gr_gid`: Numerical group ID (integer). - `gr_mem`: List of group members\' usernames (list of strings). 2. **Functions**: - `grp.getgrgid(id)`: Returns the group entry for the given numeric group ID. Raises `KeyError` if the group ID is not found and raises `TypeError` for non-integer arguments. - `grp.getgrnam(name)`: Returns the group entry for the given group name. Raises `KeyError` if the group name is not found. - `grp.getgrall()`: Returns a list of all available group entries in arbitrary order. 3. **Error Handling**: The functions handle specific exceptions (`KeyError` and `TypeError`). The information provided is sufficient to design a coding question that assesses students\' understanding of working with the Unix group database using the `grp` module. <|Analysis End|> <|Question Begin|> # Coding Assessment Question: You are tasked with creating a utility function in Python that processes group database entries on a Unix system. You will use the `grp` module to retrieve and analyze information about user groups. Objective: Write a function that takes two arguments: 1. A group\'s name (string). 2. A user’s username (string). The function should return a Boolean indicating whether the specified user is a member of the specified group. Function Signature: ```python def is_user_in_group(group_name: str, username: str) -> bool: ``` Input: - `group_name`: A string representing the name of the group. - `username`: A string representing the username of the user. Output: - A boolean value: - `True` if the user is a member of the specified group. - `False` otherwise. Constraints and Assumptions: - You are guaranteed to run your code on a Unix system where the `grp` module is available. - Group names and usernames are case-sensitive. - Raise a `ValueError` if the group name or username is an empty string. - Handle potential exceptions that may occur when accessing the group database. Example Usage: ```python # Assuming \'admin\' group exists and \'john_doe\' is a member of \'admin\' print(is_user_in_group(\'admin\', \'john_doe\')) # Output: True # Assuming \'staff\' group exists and \'jane_doe\' is not a member of \'staff\' print(is_user_in_group(\'staff\', \'jane_doe\')) # Output: False ``` Implementation Notes: - Use the `grp.getgrnam()` function to retrieve the group entry by its name. - Check if the username is in the `gr_mem` list of the retrieved group entry. - Ensure to handle exceptions such as `KeyError` for non-existent groups and other potential `TypeError`s as described in the documentation. Good luck!","solution":"import grp def is_user_in_group(group_name: str, username: str) -> bool: Checks if the specified user is a member of the specified Unix group. :param group_name: Name of the group. :param username: Username to check for membership. :return: True if the user is a member of the group, False otherwise. if not group_name or not username: raise ValueError(\\"Group name and username must not be empty\\") try: group_entry = grp.getgrnam(group_name) return username in group_entry.gr_mem except KeyError: # Group does not exist return False except TypeError: # Unexpected TypeError raise"},{"question":"# Categorical Data Manipulation with Pandas Objective You are provided with a dataset that contains customer reviews for a product, including the review text, a rating, and a date of the review. Your task is to use pandas and its categorical data capabilities to perform various operations as described. Dataset Below is a DataFrame `reviews`: | review_id | review_text | rating | review_date | |-----------|-------------|--------|-------------| | 1 | \\"Great product!\\" | 5 | \\"2023-01-15\\" | | 2 | \\"Not bad\\" | 4 | \\"2023-01-17\\" | | 3 | \\"Terrible service\\" | 1 | \\"2023-01-19\\" | | 4 | \\"Will buy again\\" | 4 | \\"2023-02-01\\" | | 5 | \\"Loved it!\\" | 5 | \\"2023-02-10\\" | | 6 | \\"Average\\" | 3 | \\"2023-02-15\\" | | 7 | \\"Could be better\\"| 2 | \\"2023-02-20\\" | | 8 | \\"Horrible\\" | 1 | \\"2023-02-25\\" | Task 1. Convert the `rating` column to a categorical type with categories ordered as \\"1, 2, 3, 4, 5\\". 2. Convert the `review_date` column to datetime type and then to a categorical variable with each month as a category, ordered from January to December. 3. Add a new category \\"0\\" to the `rating` column. Print the updated categories for confirmation. 4. Remove the \\"0\\" category from the `rating` column. Print the updated categories for confirmation. 5. Rename the categories of `rating` to: \\"Very Bad\\" for 1, \\"Bad\\" for 2, \\"Average\\" for 3, \\"Good\\" for 4, \\"Excellent\\" for 5. Print the updated DataFrame. 6. Sort the DataFrame first by `review_date` and then by `rating`. Constraints - The dataset size will not exceed 100,000 rows. - Assume all review texts and dates are valid and properly formatted. Example Output 1. Print the categories of `rating` after conversion and ordering. 2. Print the categories of `review_date` after conversion. 3. Print the categories of `rating` after adding \\"0\\". 4. Print the categories of `rating` after removing \\"0\\". 5. Print the DataFrame after renaming the categories of `rating`. 6. Print the sorted DataFrame. Implementation ```python import pandas as pd # Sample data data = { \\"review_id\\": [1, 2, 3, 4, 5, 6, 7, 8], \\"review_text\\": [\\"Great product!\\", \\"Not bad\\", \\"Terrible service\\", \\"Will buy again\\", \\"Loved it!\\", \\"Average\\", \\"Could be better\\", \\"Horrible\\"], \\"rating\\": [5, 4, 1, 4, 5, 3, 2, 1], \\"review_date\\": [\\"2023-01-15\\", \\"2023-01-17\\", \\"2023-01-19\\", \\"2023-02-01\\", \\"2023-02-10\\", \\"2023-02-15\\", \\"2023-02-20\\", \\"2023-02-25\\"] } reviews = pd.DataFrame(data) reviews[\\"review_date\\"] = pd.to_datetime(reviews[\\"review_date\\"]) # 1. Convert `rating` to categorical with ordered categories 1-5 reviews[\\"rating\\"] = pd.Categorical(reviews[\\"rating\\"], categories=[1, 2, 3, 4, 5], ordered=True) print(\\"Step 1: Categories of rating\\") print(reviews[\\"rating\\"].cat.categories) # 2. Convert `review_date` to categorical with each month as a category reviews[\\"review_month\\"] = reviews[\\"review_date\\"].dt.month_name() reviews[\\"review_month\\"] = pd.Categorical(reviews[\\"review_month\\"], categories=[ \\"January\\", \\"February\\", \\"March\\", \\"April\\", \\"May\\", \\"June\\", \\"July\\", \\"August\\", \\"September\\", \\"October\\", \\"November\\", \\"December\\"], ordered=True) print(\\"Step 2: Categories of review_month\\") print(reviews[\\"review_month\\"].cat.categories) # 3. Add a new category \\"0\\" to `rating` reviews[\\"rating\\"] = reviews[\\"rating\\"].cat.add_categories([0]) print(\\"Step 3: Categories of rating after adding 0\\") print(reviews[\\"rating\\"].cat.categories) # 4. Remove the \\"0\\" category from `rating` reviews[\\"rating\\"] = reviews[\\"rating\\"].cat.remove_categories([0]) print(\\"Step 4: Categories of rating after removing 0\\") print(reviews[\\"rating\\"].cat.categories) # 5. Rename the categories of `rating` reviews[\\"rating\\"] = reviews[\\"rating\\"].cat.rename_categories({ 1: \\"Very Bad\\", 2: \\"Bad\\", 3: \\"Average\\", 4: \\"Good\\", 5: \\"Excellent\\" }) print(\\"Step 5: DataFrame after renaming categories of rating\\") print(reviews) # 6. Sort DataFrame by `review_date` and then by `rating` sorted_reviews = reviews.sort_values(by=[\\"review_date\\", \\"rating\\"]) print(\\"Step 6: Sorted DataFrame\\") print(sorted_reviews) ```","solution":"import pandas as pd # Sample data data = { \\"review_id\\": [1, 2, 3, 4, 5, 6, 7, 8], \\"review_text\\": [\\"Great product!\\", \\"Not bad\\", \\"Terrible service\\", \\"Will buy again\\", \\"Loved it!\\", \\"Average\\", \\"Could be better\\", \\"Horrible\\"], \\"rating\\": [5, 4, 1, 4, 5, 3, 2, 1], \\"review_date\\": [\\"2023-01-15\\", \\"2023-01-17\\", \\"2023-01-19\\", \\"2023-02-01\\", \\"2023-02-10\\", \\"2023-02-15\\", \\"2023-02-20\\", \\"2023-02-25\\"] } reviews = pd.DataFrame(data) reviews[\\"review_date\\"] = pd.to_datetime(reviews[\\"review_date\\"]) # 1. Convert `rating` to categorical with ordered categories 1-5 reviews[\\"rating\\"] = pd.Categorical(reviews[\\"rating\\"], categories=[1, 2, 3, 4, 5], ordered=True) rating_categories_1 = reviews[\\"rating\\"].cat.categories # 2. Convert `review_date` to categorical with each month as a category reviews[\\"review_month\\"] = reviews[\\"review_date\\"].dt.month_name() reviews[\\"review_month\\"] = pd.Categorical(reviews[\\"review_month\\"], categories=[ \\"January\\", \\"February\\", \\"March\\", \\"April\\", \\"May\\", \\"June\\", \\"July\\", \\"August\\", \\"September\\", \\"October\\", \\"November\\", \\"December\\"], ordered=True) review_month_categories = reviews[\\"review_month\\"].cat.categories # 3. Add a new category \\"0\\" to `rating` reviews[\\"rating\\"] = reviews[\\"rating\\"].cat.add_categories([0]) rating_categories_2 = reviews[\\"rating\\"].cat.categories # 4. Remove the \\"0\\" category from `rating` reviews[\\"rating\\"] = reviews[\\"rating\\"].cat.remove_categories([0]) rating_categories_3 = reviews[\\"rating\\"].cat.categories # 5. Rename the categories of `rating` reviews[\\"rating\\"] = reviews[\\"rating\\"].cat.rename_categories({ 1: \\"Very Bad\\", 2: \\"Bad\\", 3: \\"Average\\", 4: \\"Good\\", 5: \\"Excellent\\" }) # 6. Sort DataFrame by `review_date` and then by `rating` sorted_reviews = reviews.sort_values(by=[\\"review_date\\", \\"rating\\"]) # Outputs for verification output = { \'rating_categories_1\': rating_categories_1.tolist(), \'review_month_categories\': review_month_categories.tolist(), \'rating_categories_2\': rating_categories_2.tolist(), \'rating_categories_3\': rating_categories_3.tolist(), \'renamed_dataframe\': reviews, \'sorted_dataframe\': sorted_reviews } output"},{"question":"# Pandas Advanced Data Manipulation **Objective:** The goal of this exercise is to test your ability to work with various advanced data types and data manipulation techniques using pandas. You will need to work with datetime, period, timedelta, and categorical data. **Task:** You are required to implement a function named `process_data` that takes a DataFrame as input and performs the following operations: 1. Ensure that a given column \'timestamp\' in the DataFrame is of dtype `DatetimeTZDtype`, localized to the timezone \'UTC\'. 2. Add a new column \'day_of_week\' to the DataFrame that indicates the day of the week for each timestamp (e.g., Monday, Tuesday, etc.) 3. Add another new column \'period\' that represents the period of each timestamp with a monthly frequency. 4. Calculate the time difference in days between each timestamp and a fixed date \'2023-01-01\' and store it in a column \'days_difference\'. 5. Convert a specified column \'category_col\' to a `CategoricalDtype` and add it as a new column named \'category\'. 6. Create a sparse column \'sparse_col\' from an existing column \'data_col\', assuming the column contains many missing or zero values. **Input:** - A DataFrame `df` with the following columns: - \'timestamp\': strings representing datetime values - \'category_col\': string values that need to be converted to categorical - \'data_col\': numeric values with many missing or zero values **Output:** - A DataFrame with the processed data as described above. **Constraints:** - Ensure the solution is efficient and can handle large DataFrames. **Sample Input:** ```python import pandas as pd data = { \'timestamp\': [\'2023-03-01 12:00:00\', \'2023-03-02 13:30:00\', \'2023-03-03 14:45:00\'], \'category_col\': [\'A\', \'B\', \'A\'], \'data_col\': [0, 0, 10] } df = pd.DataFrame(data) ``` **Expected Output:** ```python import pandas as pd from pandas.api.types import CategoricalDtype data = { \'timestamp\': pd.to_datetime([\'2023-03-01 12:00:00\', \'2023-03-02 13:30:00\', \'2023-03-03 14:45:00\']).tz_localize(\'UTC\'), \'day_of_week\': [\'Wednesday\', \'Thursday\', \'Friday\'], \'period\': [pd.Period(\'2023-03\', freq=\'M\'), pd.Period(\'2023-03\', freq=\'M\'), pd.Period(\'2023-03\', freq=\'M\')], \'days_difference\': [59, 60, 61], # Assuming the fixed date is 2023-01-01 \'category\': pd.Categorical([\'A\', \'B\', \'A\'], dtype=CategoricalDtype(categories=[\'A\', \'B\'], ordered=False)), \'sparse_col\': pd.arrays.SparseArray([0, 0, 10]) } expected_output = pd.DataFrame(data) ``` **Implementation:** ```python def process_data(df): # Ensure the \'timestamp\' column is timezone-aware to UTC df[\'timestamp\'] = pd.to_datetime(df[\'timestamp\']).dt.tz_localize(\'UTC\') # Add \'day_of_week\' column df[\'day_of_week\'] = df[\'timestamp\'].dt.day_name() # Add \'period\' column with monthly frequency df[\'period\'] = df[\'timestamp\'].dt.to_period(\'M\') # Calculate the time difference from a fixed date fixed_date = pd.Timestamp(\'2023-01-01\', tz=\'UTC\') df[\'days_difference\'] = (df[\'timestamp\'] - fixed_date).dt.days # Convert \'category_col\' to CategoricalDtype and add \'category\' column df[\'category\'] = df[\'category_col\'].astype(\'category\') # Create a sparse array column \'sparse_col\' df[\'sparse_col\'] = pd.arrays.SparseArray(df[\'data_col\']) return df ``` Write a function `process_data(df)` that meets the above requirements and make sure it produces the expected output given the sample input.","solution":"import pandas as pd from pandas.api.types import CategoricalDtype def process_data(df): # Ensure the \'timestamp\' column is timezone-aware to UTC df[\'timestamp\'] = pd.to_datetime(df[\'timestamp\']).dt.tz_localize(\'UTC\') # Add \'day_of_week\' column df[\'day_of_week\'] = df[\'timestamp\'].dt.day_name() # Add \'period\' column with monthly frequency df[\'period\'] = df[\'timestamp\'].dt.to_period(\'M\') # Calculate the time difference from a fixed date fixed_date = pd.Timestamp(\'2023-01-01\', tz=\'UTC\') df[\'days_difference\'] = (df[\'timestamp\'] - fixed_date).dt.days # Convert \'category_col\' to CategoricalDtype and add \'category\' column df[\'category\'] = df[\'category_col\'].astype(\'category\') # Create a sparse array column \'sparse_col\' df[\'sparse_col\'] = pd.arrays.SparseArray(df[\'data_col\']) return df"},{"question":"**Objective**: Demonstrate your understanding of Python\'s `pathlib`, `shutil`, and `tempfile` modules by creating a utility that manages file operations in a temporary directory. # Problem Statement You are required to implement a function `manage_temp_files` that performs the following operations: 1. Creates a temporary directory. 2. Within this directory, generates multiple temporary files. File names and their content should be provided by the user as a dictionary where keys are file names and values are file content. 3. Computes the total size of all files in this temporary directory. 4. Copies all files from this temporary directory to a specified target directory. 5. Cleans up (removes) the temporary directory once all operations are complete. # Function Signature ```python def manage_temp_files(files: dict, target_dir: str) -> int: :param files: Dictionary with file names as keys and file content as values. :param target_dir: Path to the directory where files will be copied. :return: Total size of all temporary files created in bytes. pass ``` # Input and Output - **Input**: - `files`: A dictionary where each key is a file name (string) and each value is the content to be written to that file (string). - `target_dir`: A string representing the path of the target directory where the files will be copied. - **Output**: - Returns an integer representing the total size of all the temporary files created, in bytes. # Constraints - Ensure the function handles cases where the target directory may not exist. - Use appropriate object-oriented methods from `pathlib` to manipulate paths and files. - Ensure the temporary directory and files are properly cleaned up even in case of exceptions. # Example Usage ```python files = { \\"file1.txt\\": \\"This is the first file.\\", \\"file2.txt\\": \\"This is the second file.\\", \\"file3.txt\\": \\"This is the third file.\\" } target_dir = \\"/path/to/target/directory\\" total_size = manage_temp_files(files, target_dir) print(f\\"Total size of temporary files: {total_size} bytes\\") ``` # Evaluation Criteria - Correctness: The function creates temporary files, computes their total size, copies them to the target directory, and cleans up the temporary files and directory. - Efficiency: Uses appropriate methods to handle file and directory operations efficiently. - Exception Handling: Ensures temporary files and directories are removed even if an error occurs during the process. - Code Readability: Clear, well-documented code with appropriate use of Python\'s standard library modules.","solution":"import tempfile import shutil from pathlib import Path def manage_temp_files(files: dict, target_dir: str) -> int: Creates temporary files with specified content, copies them to a target directory, computes the total size of the files, and cleans up temporary files. :param files: Dictionary with file names as keys and file content as values. :param target_dir: Path to the directory where files will be copied. :return: Total size of all temporary files created in bytes. total_size = 0 try: with tempfile.TemporaryDirectory() as temp_dir: temp_path = Path(temp_dir) # Create temporary files and calculate total size for file_name, content in files.items(): file_path = temp_path / file_name with file_path.open(\\"w\\") as file: file.write(content) total_size += file_path.stat().st_size # Ensure target directory exists target_path = Path(target_dir) target_path.mkdir(parents=True, exist_ok=True) # Copy files to the target directory for file_path in temp_path.iterdir(): shutil.copy(file_path, target_path / file_path.name) except Exception as e: raise RuntimeError(f\\"An error occurred: {e}\\") return total_size"},{"question":"# Group Membership Analysis using `grp` Module You are tasked with analyzing group memberships on a Unix system using Python\'s `grp` module. Your goal is to identify the groups a specific user belongs to and determine if this user shares any groups with another user. Implement a function `common_groups(user1: str, user2: str) -> List[str]` that takes two usernames as input and returns a list of group names they both belong to. If either user does not exist in any group, raise a `ValueError` indicating the username that could not be found. Input - `user1` and `user2`: Strings representing the usernames of the two users to be compared. Output - A list of strings containing the names of groups both users are members of. The list should be sorted alphabetically. Constraints - The usernames (`user1` and `user2`) are case-sensitive. - If a user does not belong to any group, their corresponding entry in the group database might not exist. - The function should handle errors gracefully and provide clear guidance through exception messages. Example ```python # Example group database entries: # grp.getgrall(): # [ # grp.struct_group(gr_name=\'admins\', gr_passwd=\'x\', gr_gid=1001, gr_mem=[\'user1\']), # grp.struct_group(gr_name=\'developers\', gr_passwd=\'x\', gr_gid=1002, gr_mem=[\'user1\', \'user2\']), # grp.struct_group(gr_name=\'designers\', gr_passwd=\'x\', gr_gid=1003, gr_mem=[\'user2\']) # ] common_groups(\'user1\', \'user2\') # Output: [\'developers\'] ``` Constraints & Performance Requirements - Your solution should efficiently handle Unix systems with a large number of groups and users. - The code should be well-structured and adhere to Python\'s best practices. Use the `grp` module to access the Unix group database for this task. Ensure you include proper error handling and edge case considerations.","solution":"import grp from typing import List def common_groups(user1: str, user2: str) -> List[str]: Returns a list of group names that both user1 and user2 belong to. If either user does not exist in any group, raises a ValueError indicating the username that could not be found. groups = grp.getgrall() user1_groups = set() user2_groups = set() # Collect groups for user1 found_user1 = False found_user2 = False for group in groups: if user1 in group.gr_mem: user1_groups.add(group.gr_name) found_user1 = True if user2 in group.gr_mem: user2_groups.add(group.gr_name) found_user2 = True if not found_user1: raise ValueError(f\\"User \'{user1}\' could not be found in any group\\") if not found_user2: raise ValueError(f\\"User \'{user2}\' could not be found in any group\\") common = sorted(user1_groups & user2_groups) return common"},{"question":"# Coding Assignment: Virtual Environment and Package Management Your task is to write a Python script that demonstrates your understanding of creating and managing virtual environments and packages. Part 1: Creating and Activating a Virtual Environment 1. Write a function `create_virtualenv(env_name)` that: - Takes a single argument `env_name` which is the name/path of the virtual environment to be created. - Uses the `venv` module to create a virtual environment with the specified name. - Activates the virtual environment. 2. Write code that demonstrates the usage of this function. Part 2: Managing Packages within the Virtual Environment 1. Write a function `manage_packages(env_name, packages)` that: - Takes two arguments: `env_name` (the name/path of the virtual environment) and `packages` (a list of package names to be managed within the virtual environment). - Activates the specified virtual environment. - Installs each package from the list using `pip`. - Prints the list of installed packages using `pip list`. 2. Write code that demonstrates the usage of this function by: - Installing at least three different packages in the created virtual environment. - Printing the details of these installed packages. Constraints: - You must assume the script operates on a Unix-based system (use appropriate commands for activation). - You may use Python\'s standard libraries only. - Ensure your script handles potential errors (e.g., environment creation failures, package installation errors) gracefully and provides relevant output. Example Output ```plaintext Creating virtual environment \'myenv\'... Virtual environment \'myenv\' created and activated. Installing packages: [\'requests\', \'numpy\', \'pandas\']... Package \'requests\' installed. Package \'numpy\' installed. Package \'pandas\' installed. Installed packages: requests (version) numpy (version) pandas (version) ... ``` Submit: - A single Python script file containing the implementation and demonstration code.","solution":"import subprocess import sys from pathlib import Path def create_virtualenv(env_name): Creates and activates a virtual environment with the specified name. # Create the virtual environment subprocess.run([sys.executable, \'-m\', \'venv\', env_name]) # Path to activation script activate_script = Path(env_name) / \'bin\' / \'activate\' print(f\\"Virtual environment \'{env_name}\' created.n\\") print(f\\"To activate the virtual environment, run: source {activate_script}\\") def manage_packages(env_name, packages): Manages packages within the specified virtual environment. # Path to the Python interpreter in the virtual environment python_interpreter = Path(env_name) / \'bin\' / \'python\' for package in packages: # Install each package using pip install_command = [str(python_interpreter), \'-m\', \'pip\', \'install\', package] subprocess.run(install_command) print(f\\"Package \'{package}\' installed.\\") # List installed packages list_packages_command = [str(python_interpreter), \'-m\', \'pip\', \'list\'] result = subprocess.run(list_packages_command, capture_output=True, text=True) print(\\"nInstalled packages:\\") print(result.stdout) # Demonstration Code if __name__ == \\"__main__\\": env_name = \'myenv\' packages = [\'requests\', \'numpy\', \'pandas\'] # Create and activate the virtual environment create_virtualenv(env_name) # Manage packages within the virtual environment manage_packages(env_name, packages)"},{"question":"# Custom Terminal Logging with pty.spawn In this assessment, you are required to implement a Python function named `log_terminal_session` that uses the `pty` module to spawn a given command, allowing interaction with it, and logs the input and output of the session in a specified file. The logging should include timestamps for when the command was started and ended, similar to the behavior of Unix\'s `script` command. Additionally, an optional timeout parameter should terminate the command if it runs longer than the specified time. Function Signature ```python def log_terminal_session(command: str, output_file: str, timeout: int = None) -> None: pass ``` Input - `command` (str): The command to be executed in the terminal. - `output_file` (str): The path to the file where the input and output should be logged. - `timeout` (int, optional): Maximum time in seconds the command is allowed to run. If not provided, the command runs until completion. Output - The function does not return any value. It writes the command\'s input and output to the specified file. Constraints - You must use the `pty.spawn()` method from the `pty` module for handling the pseudo-terminal. - The function should handle real-time capturing of the command\'s input and output. - Ensure that the logging includes timestamps for when the script started and finished. - Implement proper timeout handling to terminate the command if it exceeds the specified duration. - You may assume that the environment where the code runs is a Unix-like system. Example Usage ```python # Example call to the function to log \'ls -la\' command output to \'logfile.txt\' log_terminal_session(\'ls -la\', \'logfile.txt\') ``` Example Log Output (logfile.txt) ```plaintext Script started on Fri Oct 22 12:35:14 2021 <contents of the \'ls -la\' command> Script done on Fri Oct 22 12:35:15 2021 ``` # Additional Requirements - Ensure the code is robust and handles typical edge cases, such as invalid command, permissions issues with the output file, etc. - Consider subprocess termination correctly if the timeout is provided and exceeded.","solution":"import pty import os import subprocess import time import signal def log_terminal_session(command: str, output_file: str, timeout: int = None) -> None: def write_log(fd): while True: output = os.read(fd, 1024) if not output: break log_file.write(output.decode()) with open(output_file, \'w\') as log_file: start_time = time.strftime(\\"%a %b %d %H:%M:%S %Y\\", time.localtime()) log_file.write(f\\"Script started on {start_time}n\\") # Using subprocess to handle the timeout process = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, preexec_fn=os.setsid) try: if timeout: process.wait(timeout=timeout) else: process.wait() stdout, stderr = process.communicate() log_file.write(stdout.decode()) log_file.write(stderr.decode()) except subprocess.TimeoutExpired: os.killpg(os.getpgid(process.pid), signal.SIGTERM) log_file.write(\'nProcess terminated due to timeoutn\') end_time = time.strftime(\\"%a %b %d %H:%M:%S %Y\\", time.localtime()) log_file.write(f\\"Script done on {end_time}n\\")"},{"question":"**Coding Assessment Question** **Objective:** Write a Python function named `collect_platform_info` that collects detailed information about the current platform and returns it as a formatted string. Your implementation must use various functions from the `platform` module to gather this information. **Function Signature:** ```python def collect_platform_info() -> str ``` **Expected Inputs and Outputs:** - **Input:** None (the function should gather information about the current system\'s platform). - **Output:** A single string containing detailed platform information, formatted as specified below. **Constraints and Requirements:** - Use the following functions from the `platform` module: `architecture()`, `machine()`, `node()`, `platform()`, `processor()`, `python_build()`, `python_compiler()`, `python_implementation()`, `python_version()`, `release()`, `system()`, `uname()`. - The returned string should contain the following information in a human-readable format: 1. System/OS Name 2. Node/Computer Network Name 3. System Release 4. System Version 5. Machine Type 6. Processor Name 7. Python Implementation 8. Python Version 9. Python Compiler 10. Python Build (number and date) 11. Platform details 12. Architecture details (bits and linkage) **Example Output:** ``` System: Linux Node: my-computer Release: 5.4.0-58-generic Version: #64-Ubuntu SMP Fri Nov 6 10:37:59 UTC 2020 Machine: x86_64 Processor: x86_64 Python Implementation: CPython Python Version: 3.8.5 Python Compiler: GCC 7.5.0 Python Build: (\'default\', \'Jul 28 2020 12:59:40\') Platform: Linux-5.4.0-58-generic-x86_64-with-glibc2.29 Architecture: (\'64bit\', \'ELF\') ``` **Performance Requirements:** - The function should execute efficiently and gather all the required information in minimal time. - Ensure that empty fields (if any information cannot be determined) are represented clearly in the output. **Evaluation Criteria:** - Correct use of the `platform` module functions. - Proper formatting of the output string. - Handling of edge cases where some information might not be available. ```python def collect_platform_info() -> str: import platform system_name = platform.system() node_name = platform.node() release = platform.release() version = platform.version() machine = platform.machine() processor = platform.processor() python_impl = platform.python_implementation() python_ver = platform.python_version() python_compiler = platform.python_compiler() python_build = platform.python_build() platform_details = platform.platform() architecture_details = platform.architecture() result = ( f\\"System: {system_name}n\\" f\\"Node: {node_name}n\\" f\\"Release: {release}n\\" f\\"Version: {version}n\\" f\\"Machine: {machine}n\\" f\\"Processor: {processor}n\\" f\\"Python Implementation: {python_impl}n\\" f\\"Python Version: {python_ver}n\\" f\\"Python Compiler: {python_compiler}n\\" f\\"Python Build: {python_build}n\\" f\\"Platform: {platform_details}n\\" f\\"Architecture: {architecture_details}n\\" ) return result ```","solution":"def collect_platform_info() -> str: import platform system_name = platform.system() node_name = platform.node() release = platform.release() version = platform.version() machine = platform.machine() processor = platform.processor() python_impl = platform.python_implementation() python_ver = platform.python_version() python_compiler = platform.python_compiler() python_build = platform.python_build() platform_details = platform.platform() architecture_details = platform.architecture() result = ( f\\"System: {system_name}n\\" f\\"Node: {node_name}n\\" f\\"Release: {release}n\\" f\\"Version: {version}n\\" f\\"Machine: {machine}n\\" f\\"Processor: {processor}n\\" f\\"Python Implementation: {python_impl}n\\" f\\"Python Version: {python_ver}n\\" f\\"Python Compiler: {python_compiler}n\\" f\\"Python Build: {python_build}n\\" f\\"Platform: {platform_details}n\\" f\\"Architecture: {architecture_details}n\\" ) return result"},{"question":"**Question: Tensor Operations and Memory Management on GPU with PyTorch** You are given a task to perform several tensor operations on a GPU using PyTorch and monitor the memory allocations for these operations. The operations and memory monitoring should be compatible with both CUDA and HIP interfaces. # Requirements 1. **Device Selection:** - Check if a GPU is available on the system. If available, set the device to GPU (CUDA/HIP). If not, print an appropriate message and set the device to CPU. 2. **Tensor Operations:** - Create two tensors `A` and `B` (randomly initialized) of shape (1000, 1000) on the selected device. - Perform element-wise addition, subtraction, multiplication, and division on these tensors and store the results in tensors `C_add`, `C_sub`, `C_mul`, and `C_div` respectively. 3. **Memory Management:** - Compute the memory allocated by PyTorch only for tensors after performing the above operations. - Compute the maximum memory allocated by PyTorch during these operations. 4. **Output:** - Print the allocated memory for tensors and the maximum allocated memory. - Print the device on which the operations were performed. # Constraints - Ensure the code runs efficiently on both CUDA and HIP platforms. - Handle any exceptions that may occur due to device unavailability or any other issues. # Code Template ```python import torch def tensor_operations(): # Check if GPU is available if torch.cuda.is_available(): device = torch.device(\'cuda\') if torch.version.hip: print(\\"Using HIP device\\") elif torch.version.cuda: print(\\"Using CUDA device\\") else: device = torch.device(\'cpu\') print(\\"No GPU found, using CPU\\") # Create tensors on the selected device A = torch.randn((1000, 1000), device=device) B = torch.randn((1000, 1000), device=device) # Perform tensor operations C_add = A + B C_sub = A - B C_mul = A * B C_div = A / B # Memory management allocated_memory = torch.cuda.memory_allocated(device=device) max_allocated_memory = torch.cuda.max_memory_allocated(device=device) # Print results print(f\\"Allocated memory: {allocated_memory / (1024 ** 2)} MB\\") print(f\\"Maximum allocated memory: {max_allocated_memory / (1024 ** 2)} MB\\") print(f\\"Operations performed on device: {device}\\") if __name__ == \\"__main__\\": tensor_operations() ``` # Note: 1. Ensure that your PyTorch installation supports GPU (CUDA or HIP). 2. Use exception handling to manage any potential errors due to device constraints or tensor operations.","solution":"import torch def tensor_operations(): # Check if GPU is available if torch.cuda.is_available(): device = torch.device(\'cuda\') if torch.version.hip: print(\\"Using HIP device\\") elif torch.version.cuda: print(\\"Using CUDA device\\") else: device = torch.device(\'cpu\') print(\\"No GPU found, using CPU\\") # Create tensors on the selected device A = torch.randn((1000, 1000), device=device) B = torch.randn((1000, 1000), device=device) # Perform tensor operations C_add = A + B C_sub = A - B C_mul = A * B C_div = A / B # Memory management allocated_memory = torch.cuda.memory_allocated(device=device) if device.type == \'cuda\' else 0 max_allocated_memory = torch.cuda.max_memory_allocated(device=device) if device.type == \'cuda\' else 0 # Print results print(f\\"Allocated memory: {allocated_memory / (1024 ** 2)} MB\\") print(f\\"Maximum allocated memory: {max_allocated_memory / (1024 ** 2)} MB\\") print(f\\"Operations performed on device: {device}\\") return { \\"allocated_memory\\": allocated_memory, \\"max_allocated_memory\\": max_allocated_memory, \\"device\\": str(device), \\"C_add\\": C_add, \\"C_sub\\": C_sub, \\"C_mul\\": C_mul, \\"C_div\\": C_div } if __name__ == \\"__main__\\": tensor_operations()"},{"question":"You are tasked with writing a Python function `parse_and_transform_xml(xml_string)` that parses a given XML string, processes its elements using handlers, and transforms the XML into a specific JSON-like dictionary structure. Your function will: 1. Parse the XML string. 2. Use appropriate handlers to: - Collect element data including element names and attributes. - Capture nested element relationships. 3. Handle errors encountered during parsing gracefully, by throwing meaningful error messages. # Specification: - **Input**: - `xml_string`: A string containing the XML data to be parsed. - **Output**: - Return a dictionary that represents the XML structure in a format like: ```python { \\"element_name\\": { \\"attributes\\": {attribute_dict}, \\"children\\": [ {child_element_dict}, ... ], \\"text\\": \\"Character data in the element if any\\" } } ``` - The dictionary should reflect the nested structure of the XML. # Constraints: 1. Assume the XML string is well-formed but might contain nested elements. 2. Maintain the order of elements and attributes as they appear in the XML. 3. Ensure your solution handles namespace URIs correctly if provided. 4. Handle parser errors using custom exceptions that provide the error code and additional contextual information. # Example: Input: ```xml xml_string = <library> <book id=\\"1\\"/> <book id=\\"2\\"> <title>Python Programming</title> </book> </library> ``` Output: ```python { \\"library\\": { \\"attributes\\": {}, \\"children\\": [ { \\"book\\": { \\"attributes\\": {\\"id\\": \\"1\\"}, \\"children\\": [], \\"text\\": \\"\\" } }, { \\"book\\": { \\"attributes\\": {\\"id\\": \\"2\\"}, \\"children\\": [ { \\"title\\": { \\"attributes\\": {}, \\"children\\": [], \\"text\\": \\"Python Programming\\" } } ], \\"text\\": \\"\\" } } ], \\"text\\": \\"\\" } } ``` # Implementation: Your task is to implement the function `parse_and_transform_xml(xml_string):`. Consider the steps below for implementation: 1. Create an `xmlparser` object using `ParserCreate`. 2. Define and set handlers for: - Start of an element (`StartElementHandler`). - End of an element (`EndElementHandler`). - Character data within an element (`CharacterDataHandler`). 3. Manage a stack to handle nested elements. 4. Parse the XML data and return the transformed data structure. 5. Implement error handling using try-except blocks. # Starter Code: ```python import xml.parsers.expat def parse_and_transform_xml(xml_string): # Create the parser parser = xml.parsers.expat.ParserCreate() # Define the data structure to hold the transformed XML result = {} element_stack = [] def start_element(name, attrs): nonlocal result element = { \\"attributes\\": attrs, \\"children\\": [], \\"text\\": \\"\\" } if element_stack: parent = element_stack[-1] parent[\\"children\\"].append({name: element}) else: result[name] = element element_stack.append(element) def end_element(name): element_stack.pop() def char_data(data): if element_stack: element_stack[-1][\\"text\\"] += data # Set handlers parser.StartElementHandler = start_element parser.EndElementHandler = end_element parser.CharacterDataHandler = char_data try: # Parse the XML string parser.Parse(xml_string) except xml.parsers.expat.ExpatError as e: raise Exception(f\\"XML parsing error: {xml.parsers.expat.ErrorString(e.code)} at line {e.lineno}, column {e.offset}\\") return result # Example usage: xml_string = <library><book id=\\"1\\"/><book id=\\"2\\"><title>Python Programming</title></book></library> print(parse_and_transform_xml(xml_string)) ``` # Notes: 1. Ensure you handle edge cases like empty elements, elements with only text, and nested elements efficiently. 2. Pay attention to performance implications, especially with deeply nested elements. 3. Test your function using various XML strings and validate the output structure.","solution":"import xml.parsers.expat def parse_and_transform_xml(xml_string): # Create the parser parser = xml.parsers.expat.ParserCreate() # Define the data structure to hold the transformed XML result = {} element_stack = [] def start_element(name, attrs): nonlocal result element = { \\"attributes\\": dict(attrs), \\"children\\": [], \\"text\\": \\"\\" } if element_stack: parent = element_stack[-1] parent[\\"children\\"].append({name: element}) else: result[name] = element element_stack.append(element) def end_element(name): element_stack.pop() def char_data(data): if element_stack and data.strip(): element_stack[-1][\\"text\\"] += data # Set handlers parser.StartElementHandler = start_element parser.EndElementHandler = end_element parser.CharacterDataHandler = char_data try: # Parse the XML string parser.Parse(xml_string, True) except xml.parsers.expat.ExpatError as e: raise Exception(f\\"XML parsing error: {xml.parsers.expat.ErrorString(e.code)} at line {e.lineno}, column {e.offset}\\") return result # Example usage: xml_string = <library><book id=\\"1\\"/><book id=\\"2\\"><title>Python Programming</title></book></library> print(parse_and_transform_xml(xml_string))"},{"question":"# Question: Building a Command-Line Tool with `__main__` in Python You are tasked with creating a Python command-line tool that performs basic arithmetic operations (addition, subtraction, multiplication, and division). Your task is to implement this tool such that it adheres to best practices regarding the `__main__` module. Follow these guidelines: 1. Define four functions: `add(a, b)`, `subtract(a, b)`, `multiply(a, b)`, and `divide(a, b)`. Each function should accept two arguments and return the result of the operation. 2. Implement a `main()` function that: - Parses command-line arguments to determine the operation to be performed and the operands. - Calls the appropriate function based on the parsed arguments. - Prints the result of the operation. 3. Ensure the script can be run both as a standalone command-line tool and can also be imported as a module without executing the main logic directly. 4. Use the idiom `if __name__ == \'__main__\':` to encapsulate the entry point of the script. Input The script should be executed from the command line with the following argument format: ``` python3 calculator.py <operation> <operand1> <operand2> ``` - `<operation>` is one of `add`, `subtract`, `multiply`, `divide`. - `<operand1>` and `<operand2>` are numeric values. Output The result of the specified arithmetic operation printed to standard output. Constraints - `<operand2>` should not be zero for division operations. - You may assume that the inputs are valid and properly formatted. Example ``` python3 calculator.py add 5 3 8 python3 calculator.py divide 10 2 5.0 ``` Notes - Ensure that your script does not execute the computation when imported, but only when run as the main program. - Provide a minimal example of the command-line argument parsing within your `main()` function. **Implement the solution in the following code block:** ```python # Write your implementation here ```","solution":"import sys def add(a, b): return a + b def subtract(a, b): return a - b def multiply(a, b): return a * b def divide(a, b): if b == 0: raise ValueError(\\"Cannot divide by zero\\") return a / b def main(): if len(sys.argv) != 4: print(\\"Usage: python3 calculator.py <operation> <operand1> <operand2>\\") return operation = sys.argv[1] operand1 = float(sys.argv[2]) operand2 = float(sys.argv[3]) if operation == \'add\': result = add(operand1, operand2) elif operation == \'subtract\': result = subtract(operand1, operand2) elif operation == \'multiply\': result = multiply(operand1, operand2) elif operation == \'divide\': result = divide(operand1, operand2) else: print(\\"Unknown operation. Available operations: add, subtract, multiply, divide\\") return print(result) if __name__ == \'__main__\': main()"},{"question":"# Advanced Telnet Interaction Function In this task, you will demonstrate your understanding of the `telnetlib` module by writing a function that connects to a specified Telnet server, performs a series of interactions, and retrieves specified data. Your function will be expected to handle various scenarios and errors gracefully. Function Specification **Function Name**: `fetch_telnet_data` **Input**: 1. `host` (str): The hostname or IP address of the Telnet server. 2. `port` (int): The port number to connect to (default is 23). 3. `commands` (list): A list of commands (as strings) to be sent to the Telnet server in sequence. 4. `expected_outputs` (list): A list of byte strings that are expected outputs to read after each command is executed. **Output**: - Returns a list of responses from the Telnet server after each command execution. **Constraints**: - Your function should handle timeouts and other connection-related exceptions. - If a connection cannot be established, the function should return an empty list. - Use a default timeout of 10 seconds for blocking operations. - The function should ensure that any connection opened is properly closed in case of an error or after all commands have been executed. Example ```python def fetch_telnet_data(host, port=23, commands, expected_outputs): # your implementation here # Example Usage: host = \'localhost\' port = 23 commands = [\'login\', \'password\', \'ls\', \'exit\'] expected_outputs = [b\'login: \', b\'Password: \', b\' \', b\'Goodbye!\'] responses = fetch_telnet_data(host, port, commands, expected_outputs) print(responses) ``` This function should handle the following: 1. Connect to the specified Telnet server using the provided host and port. 2. Send each command from the `commands` list to the Telnet server. 3. Read the server response until the corresponding expected output is encountered. 4. Collect the responses in a list and return them. 5. Handle any exceptions, ensuring the Telnet connection is properly closed before exiting. Implement the `fetch_telnet_data` function according to the specification above, ensuring that it robustly handles various scenarios and errors.","solution":"import telnetlib def fetch_telnet_data(host, port=23, commands=[], expected_outputs=[]): Connects to a Telnet server, sends a series of commands, and retrieves the responses. Args: host (str): The hostname or IP address of the Telnet server. port (int): The port number to connect to. commands (list): A list of commands as strings to be sent to the Telnet server. expected_outputs (list): A list of expected byte strings as outputs to read after each command is executed. Returns: list: A list of server responses after each command execution. responses = [] try: tn = telnetlib.Telnet(host, port, timeout=10) for i in range(len(commands)): command = commands[i] expected_output = expected_outputs[i] # Writing the command to the server tn.write(command.encode(\'ascii\') + b\'n\') # Reading the response until expected output is found response = tn.read_until(expected_output, timeout=10) responses.append(response) tn.close() except Exception as e: # Return an empty list in case of any connection issues or other exceptions return [] return responses"},{"question":"# Coding Assessment: Advanced Generator Function with Expression Handling **Objective:** Implement an advanced generator function that will demonstrate your deep understanding of Python expressions, particularly with generator expressions and yield functionality. **Problem Statement:** You need to implement a generator function `filtered_squares` that takes two input sequences `seq1` and `seq2`. The generator should produce the squares of numbers from the first sequence and yield them only if they do not appear in the second sequence. Additionally, it should handle and yield exceptions when encountered during the iteration. The generator function should provide functionality to resume the sequence, send new values to the generator, throw exceptions, and close the generator gracefully. **Specification:** 1. **Function Signature:** ```python def filtered_squares(seq1: list, seq2: set): ``` 2. **Inputs:** - `seq1` (list): A list of integers. - `seq2` (set): A set of integers. 3. **Outputs:** - Yields values from the generator that are squares of numbers from `seq1`, provided they are not in `seq2`. 4. **Example:** ```python seq1 = [1, 2, 3, 4, 5] seq2 = {4, 9} gen = filtered_squares(seq1, seq2) print(next(gen)) # 1 (1^2 is not in seq2) print(next(gen)) # 16 (4^2 is not in seq2) print(gen.send(6)) # Continues with the next value 25 (5^2 is not in seq2) gen.throw(ValueError, \\"Test Exception\\") # Should handle and yield exception gen.close() # Should close the generator gracefully with a closing message ``` **Constraints:** - Assume all elements of `seq1` are valid integers. - The generator should handle any integer and must handle exceptions gracefully using `try` and `except` blocks. **Performance Requirements:** - The generator should be efficient and handle sequences of up to 10^6 elements. Implement the `filtered_squares` generator function below: ```python def filtered_squares(seq1: list, seq2: set): try: for num in seq1: square = num ** 2 if square not in seq2: try: value = yield square if value: print(f\\"Received via send: {value}\\") except ValueError as e: yield f\\"Exception handled: {e}\\" except GeneratorExit: print(\\"Generator closed gracefully.\\") ``` **Notes:** 1. Ensure to test your generator with various edge cases including empty sequences and sequences with all squares present in `seq2`. 2. Pay special attention to the handling of exceptions raised by `throw()` and the closing of the generator via `close()`. **Submission:** Submit your code as a Python file named `filtered_squares.py`.","solution":"def filtered_squares(seq1: list, seq2: set): try: for num in seq1: square = num ** 2 if square not in seq2: try: value = yield square if value is not None: print(f\\"Received via send: {value}\\") except ValueError as e: yield f\\"Exception handled: {e}\\" except GeneratorExit: print(\\"Generator closed gracefully.\\")"},{"question":"**Question:** You are required to write a function that combines operations from both the `struct` and `codecs` modules to manipulate and interpret binary data. # Problem Description You will write a function `process_binary_data(data: bytes, format_string: str, encoding: str) -> str` that: 1. **Unpacks** the given binary data (`data`) using the specified `format_string` using `struct`. 2. **Encodes** the unpacked data into a specific encoding format specified by `encoding` using `codecs`. 3. Returns the encoded data as a string. # Function Signature ```python def process_binary_data(data: bytes, format_string: str, encoding: str) -> str: ``` # Input - `data`: A bytes object representing the binary data. (1 <= len(data) <= 100) - `format_string`: A string specifying the format for unpacking the binary data using `struct`. (e.g., format strings like `\'i\'`, `\'f\'`, `\'h\'`) - `encoding`: A string specifying the encoding format to be used (e.g., `\'utf-8\'`, `\'utf-16\'`, `\'ascii\'`). # Output - A string representing the data after unpacking and encoding. # Constraints 1. The `data` must be a valid bytes object matching the provided `format_string`. 2. The `format_string` should be a valid format for the given binary data that can be identified by the `struct` module. 3. The `encoding` should be a valid encoding scheme recognized by the `codecs` module. # Example ```python # Example 1 data = struct.pack(\'i\', 12345) format_string = \'i\' encoding = \'utf-8\' print(process_binary_data(data, format_string, encoding)) # Output: \'12345\' # Example 2 data = struct.pack(\'h\', 5678) format_string = \'h\' encoding = \'ascii\' print(process_binary_data(data, format_string, encoding)) # Output: \'5678\' ``` # Explanation The function `process_binary_data` performs the following steps in each example: 1. Unpacks the binary data using the provided format string to interpret it as an integer or another data type. 2. Converts the unpacked data to a string. 3. Encodes the string representation of the data using the specified encoding format (though in this example, it is already in a compatible string format). # Notes 1. Handle any necessary exceptions or errors that could arise from invalid data, format strings, or encoding types. 2. Make sure to adhere to all input constraints and handle edge cases carefully.","solution":"import struct import codecs def process_binary_data(data: bytes, format_string: str, encoding: str) -> str: Unpacks binary data, converts it to a string, and encodes it in the specified encoding. try: # Unpack the binary data using the format string unpacked_data = struct.unpack(format_string, data) # Convert the unpacked data to string; assuming unpacked_data is a tuple unpacked_str = \'\'.join(map(str, unpacked_data)) # Encode the string with the specified encoding encoded_bytes = codecs.encode(unpacked_str, encoding) # Convert the encoded bytes back to a string (if necessary) encoded_str = encoded_bytes.decode(encoding) return encoded_str except struct.error as e: raise ValueError(f\\"Invalid format string or data: {e}\\") except LookupError as e: raise ValueError(f\\"Invalid encoding: {e}\\")"},{"question":"Coding Assessment Question You are required to write a Python function that identifies all the unique operations (by their human-readable names) used in a given function. Specifically, you should leverage the `dis` module to disassemble the function\'s bytecode and extract the operations. # Function Signature ```python def get_unique_operations(func) -> set: pass ``` # Input - `func`: A Python function object. # Output - A set of strings, each representing a unique bytecode operation name found in the disassembled bytecode of the input function. # Example ```python def example_function(x): if x > 0: return x * 2 else: return -x # Example usage result = get_unique_operations(example_function) print(result) ``` **Expected Output** A set containing the bytecode operation names found in the `example_function`, for instance: ```python {\'LOAD_FAST\', \'COMPARE_OP\', \'POP_JUMP_IF_FALSE\', \'RETURN_VALUE\', \'LOAD_CONST\', \'BINARY_MULTIPLY\', \'BINARY_SUBTRACT\', \'JUMP_FORWARD\'} ``` # Constraints - You should not manually decode bytecodes. Instead, utilize the `dis.Bytecode` class and its methods. - Handle standard Python built-in functions and operations. No need to handle external libraries or modules. - Assume that the provided function is well-formed and does not contain any syntax errors. # Notes - Focus on creating a clean, efficient function that makes good use of the `dis` module\'s capabilities. - The order of operations in the output set does not matter as sets are unordered collections. # Performance Requirements - The function should run efficiently for typical use cases, e.g., simple to moderately complex functions. # Tips - Use `dis.Bytecode(func)` to obtain an iterable of instructions. - Iterate through the instructions and collect unique values of the `opname` attribute. Good luck!","solution":"import dis def get_unique_operations(func) -> set: Returns a set of unique bytecode operation names for the given function. bytecode = dis.Bytecode(func) unique_operations = {instr.opname for instr in bytecode} return unique_operations"},{"question":"**Objective:** Implement a Python function that uses the given codec functions to encode and decode a string while handling potential errors. **Task:** Write a function `custom_encode_decode(input_str: str, encoding: str, errors: str = \\"strict\\") -> str` that: 1. Takes an input string `input_str`, an `encoding` type, and an optional `errors` parameter. 2. Encodes the input string using the specified encoding and the error handling method. 3. Decodes the encoded result using the same encoding and error handling method. 4. Returns the final decoded string. 5. Handles potential lookup and encoding/decoding errors, returning a clear error message if any exception occurs. **Function Signature:** ```python def custom_encode_decode(input_str: str, encoding: str, errors: str = \\"strict\\") -> str: pass ``` **Input:** - `input_str` (str): The string to be encoded and decoded. - `encoding` (str): The encoding type to use (e.g., \'utf-8\', \'ascii\'). - `errors` (str, optional): The error handling scheme. Defaults to \\"strict\\". Other options may include \\"ignore\\", \\"replace\\", etc. **Output:** - (str): The final decoded string or an error message if an exception occurs. **Constraints:** - The function should handle the situation where the provided encoding is unknown. - It should appropriately manage errors during the encoding and decoding processes based on the `errors` parameter. **Test Cases:** 1. `custom_encode_decode(\\"hello world\\", \\"utf-8\\")` should return `\\"hello world\\"`. 2. `custom_encode_decode(\\"hello world\\", \\"ascii\\")` should return `\\"hello world\\"`. 3. `custom_encode_decode(\\"hello world\\", \\"unknown-encoding\\", \\"ignore\\")` should return an informative error message about unknown encoding. 4. `custom_encode_decode(\\"hëllö wörld\\", \\"ascii\\", \\"replace\\")` should return `\\"h?ll? w?rld\\"`. **Notes:** - Make sure to import the necessary codec functions at the beginning of your implementation. - Utilize `PyCodec_Encode`, `PyCodec_Decode` and potentially other supportive functions from the provided documentation. Good luck and demonstrate your understanding of codec operations and error handling in Python!","solution":"def custom_encode_decode(input_str: str, encoding: str, errors: str = \\"strict\\") -> str: try: # Encode the input string with the given encoding and error handling method encoded_bytes = input_str.encode(encoding, errors) # Decode the encoded bytes back to string with the same encoding and error handling decoded_str = encoded_bytes.decode(encoding, errors) return decoded_str except LookupError: return f\\"Error: Unknown encoding \'{encoding}\'\\" except UnicodeEncodeError: return f\\"Error: Unable to encode the string with encoding \'{encoding}\' using \'{errors}\' error handling.\\" except UnicodeDecodeError: return f\\"Error: Unable to decode the bytes with encoding \'{encoding}\' using \'{errors}\' error handling.\\""},{"question":"You are tasked with creating a custom logging utility for reading files. This utility will log the content read from the file to another file while maintaining the original file\'s content intact and ensuring logging only occurs for readable files. It should handle scenarios where files do not exist, are not readable, or any I/O errors gracefully. Objective: 1. Implement a custom `open` function that reads the content from a file and logs the uppercase version of the content to the specified log file. 2. Use the `builtins` module to ensure the original `open` functionality is preserved for reading the file. Function Signature: ```python def custom_open(file_path: str, log_file_path: str) -> str: Reads content from file_path, logs the uppercase content to log_file_path, and returns the original content. :param file_path: The path of the file to read. :param log_file_path: The path of the log file where the uppercase content should be logged. :return: The original content of the file as a string. ``` Input: - `file_path` (str): The path to the file that will be read. - `log_file_path` (str): The path to the file where the uppercase content will be logged. Output: - Returns the original content of the file as a string. Constraints: - If the `file_path` does not exist or is not readable, return an empty string. - Ensure the logging operation handles I/O errors gracefully. If an error occurs during logging, it should not affect the original read operation. - The utility should only support reading text files (not binary). Example: ```python # Example file structure # Assume \'example.txt\' contains: \\"Hello World\\" # Assume \'log.txt\' is initially empty content = custom_open(\'example.txt\', \'log.txt\') print(content) # Should output: \\"Hello World\\" # Contents of log.txt should now be: \\"HELLO WORLD\\" ``` Performance Requirements: - The function should be efficient in handling file operations and ensure minimal memory usage. Additional Notes: - Since file I/O operations are inherently slow, ensure that your implementation reads and writes files in the most efficient way. - Use the `builtins.open` function to ensure you are using the standard Python `open` for file operations.","solution":"import builtins def custom_open(file_path: str, log_file_path: str) -> str: Reads content from file_path, logs the uppercase content to log_file_path, and returns the original content. :param file_path: The path of the file to read. :param log_file_path: The path of the log file where the uppercase content should be logged. :return: The original content of the file as a string. try: with builtins.open(file_path, \'r\') as file: content = file.read() except (FileNotFoundError, IOError): return \'\' try: with builtins.open(log_file_path, \'a\') as log_file: log_file.write(content.upper()) except IOError: pass return content"},{"question":"**Supervised Learning with scikit-learn** **Objective:** Implement a machine learning pipeline using scikit-learn to train and evaluate a supervised learning model on a provided dataset. **Problem Statement:** You are provided with a dataset containing numerical features and a target variable. Your task is to: 1. Load the dataset from a CSV file. 2. Split the dataset into training and testing sets. 3. Apply any necessary preprocessing steps. 4. Train a supervised learning model using one of the algorithms available in scikit-learn. 5. Evaluate the model\'s performance using cross-validation. 6. Report the model\'s accuracy on the test set. **Detailed Instructions:** 1. **Loading the Data:** - Use `pandas` to load the dataset from a CSV file. The CSV file will have a header row with feature names and the target variable. 2. **Data Splitting:** - Split the data into training and testing sets using an 80-20 split. 3. **Preprocessing:** - Scale the features using `StandardScaler` from scikit-learn. 4. **Model Training:** - Choose one supervised learning algorithm from the following list and train the model: - Logistic Regression - Support Vector Machine - Decision Tree - Random Forest - K-Nearest Neighbors 5. **Cross-validation:** - Use `cross_val_score` from scikit-learn with 5-fold cross-validation to evaluate the model. 6. **Model Evaluation:** - Report the mean cross-validation accuracy. - Evaluate the model\'s accuracy on the test set using the `score` method. **Input:** - A CSV file `data.csv` containing columns of numerical features and one target variable. **Output:** - Mean cross-validation accuracy (printed to the console). - Test set accuracy (printed to the console). **Constraints:** - You may assume that the dataset is clean and does not contain any missing values. **Example:** Suppose you have the following data in `data.csv`: ``` feature1,feature2,feature3,target 1.0,2.0,3.1,1 2.0,3.4,1.2,0 3.5,4.1,2.3,1 ... ``` Your task is to implement a script that performs the above steps and outputs the required accuracies. **Starter Code:** ```python import pandas as pd from sklearn.model_selection import train_test_split, cross_val_score from sklearn.preprocessing import StandardScaler from sklearn.linear_model import LogisticRegression # (You can import other models if you decide to use them) # Load data data = pd.read_csv(\'data.csv\') # Split data X = data.drop(\'target\', axis=1) y = data[\'target\'] X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Preprocess data scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) # Train model model = LogisticRegression() # You can change to another model model.fit(X_train_scaled, y_train) # Cross-validation cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5) print(\'Mean cross-validation accuracy:\', cv_scores.mean()) # Test set evaluation test_accuracy = model.score(X_test_scaled, y_test) print(\'Test set accuracy:\', test_accuracy) ``` Modify the starter code to complete the task as required.","solution":"import pandas as pd from sklearn.model_selection import train_test_split, cross_val_score from sklearn.preprocessing import StandardScaler from sklearn.ensemble import RandomForestClassifier def load_and_prepare_data(filename): Loads data from a CSV file and splits it into training and testing sets. Parameters: filename (str): The path to the CSV file. Returns: tuple: Scaled training and test data (X_train_scaled, X_test_scaled, y_train, y_test) # Load data data = pd.read_csv(filename) # Split data into features and target X = data.drop(\'target\', axis=1) y = data[\'target\'] # Split into training and testing data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Preprocess data scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) return X_train_scaled, X_test_scaled, y_train, y_test def train_and_evaluate_model(X_train, X_test, y_train, y_test): Trains a RandomForestClassifier model and evaluates its performance. Parameters: X_train (array): Scaled training features. X_test (array): Scaled test features. y_train (array): Training labels. y_test (array): Test labels. Returns: dict: Mean cross-validation accuracy and test set accuracy. # Train model model = RandomForestClassifier(random_state=42) model.fit(X_train, y_train) # Cross-validation cv_scores = cross_val_score(model, X_train, y_train, cv=5) mean_cv_accuracy = cv_scores.mean() # Test set evaluation test_accuracy = model.score(X_test, y_test) return { \'mean_cv_accuracy\': mean_cv_accuracy, \'test_accuracy\': test_accuracy }"},{"question":"**Objective:** Demonstrate your understanding of the `smtplib` module by writing a function that sends an email with specified content, handles potential exceptions, and prints appropriate success or error messages. **Requirements:** - Implement a function `send_notification_email(smtp_server, port, username, password, from_addr, to_addrs, subject, body)`. - The function should: - Connect to the specified SMTP server using the provided port. - Log in using the given username and password. - Create an email message with the provided `subject` and `body`. The `from_addr` should be used in the \\"From\\" header, and the `to_addrs` should be used in the \\"To\\" header. - Send the email message to all recipient addresses specified in `to_addrs`. - Quit the SMTP session after sending the email. - Handle the following exceptions by printing appropriate error messages: - `smtplib.SMTPConnectError`: \\"Failed to connect to the SMTP server.\\" - `smtplib.SMTPAuthenticationError`: \\"Authentication failed. Check the username and password.\\" - `smtplib.SMTPRecipientsRefused`: \\"All recipient addresses were refused.\\" - `smtplib.SMTPDataError`: \\"The SMTP server refused to accept the message data.\\" - Any other exceptions should print \\"An error occurred: \\" followed by the exception message. **Function Signature:** ```python import smtplib from email.message import EmailMessage def send_notification_email(smtp_server: str, port: int, username: str, password: str, from_addr: str, to_addrs: list, subject: str, body: str): # Your implementation here pass ``` **Constraints:** - `smtp_server` is a string representing the SMTP server address. - `port` is an integer representing the port number to connect to. - `username` and `password` are strings for SMTP authentication. - `from_addr` and `subject` are strings representing the sender\'s email address and the email subject, respectively. - `to_addrs` is a list of strings representing recipient email addresses. - `body` is a string representing the body of the email message. **Example Usage:** ```python smtp_server = \\"smtp.example.com\\" port = 587 username = \\"user@example.com\\" password = \\"password\\" from_addr = \\"user@example.com\\" to_addrs = [\\"recipient1@example.com\\", \\"recipient2@example.com\\"] subject = \\"Test Email\\" body = \\"This is a test email.\\" send_notification_email(smtp_server, port, username, password, from_addr, to_addrs, subject, body) ``` In this example, the function should attempt to send an email to the recipients and handle any potential exceptions according to the specified requirements.","solution":"import smtplib from email.message import EmailMessage def send_notification_email(smtp_server: str, port: int, username: str, password: str, from_addr: str, to_addrs: list, subject: str, body: str): try: # Create the email message msg = EmailMessage() msg[\'From\'] = from_addr msg[\'To\'] = \', \'.join(to_addrs) msg[\'Subject\'] = subject msg.set_content(body) # Connect to the SMTP server and send the email with smtplib.SMTP(smtp_server, port) as server: server.starttls() server.login(username, password) server.send_message(msg) print(\\"Email sent successfully.\\") except smtplib.SMTPConnectError: print(\\"Failed to connect to the SMTP server.\\") except smtplib.SMTPAuthenticationError: print(\\"Authentication failed. Check the username and password.\\") except smtplib.SMTPRecipientsRefused: print(\\"All recipient addresses were refused.\\") except smtplib.SMTPDataError: print(\\"The SMTP server refused to accept the message data.\\") except Exception as e: print(f\\"An error occurred: {e}\\")"},{"question":"# Question: Create a Custom String Formatter Your task is to create a custom string formatter class in Python that extends the functionality of the built-in `string.Formatter` class. This custom formatter should support the following additional functionalities: 1. **Reversing Strings**: If a placeholder in the format string is followed by `!r`, it should reverse the given string. 2. **Uppercase Strings**: If a placeholder in the format string is followed by `!u`, it should convert the string to uppercase. 3. **Prefix with Value Length**: If a placeholder in the format string contains `:len`, it should prefix the value with its length. **Specifications**: - Implement a class `CustomFormatter` that inherits from `string.Formatter`. - Override the necessary methods to add support for the specified functionalities. - The `!r` and `!u` conversion flags should precede any format specifications. **Input Format**: - A format string containing placeholders for variable substitution, conversion flags (`!r` for reversing, `!u` for uppercase), and format specifications (`:len` to prefix with length). - Positional or keyword arguments for substitution. **Expected Output**: - A formatted string based on the specified custom formatting rules. **Constraints**: - Assume that the inputs are valid and in accordance with the specified placeholders and formatting rules. **Example**: ```python formatter = CustomFormatter() format_string = \\"Name: {name!u}, Reversed: {value!r}, Length: {name:len}\\" output = formatter.format(format_string, name=\\"john\\", value=\\"hello\\") # Output should be: # \\"Name: JOHN, Reversed: olleh, Length: 4john\\" ``` **Implementation**: Below is a skeleton code to help you get started: ```python import string class CustomFormatter(string.Formatter): def format_field(self, value, format_spec): if \'len\' in format_spec: value = f\\"{len(value)}{value}\\" format_spec = format_spec.replace(\'len\', \'\') return super().format_field(value, format_spec) def convert_field(self, value, conversion): if conversion == \'r\': return value[::-1] elif conversion == \'u\': return value.upper() return super().convert_field(value, conversion) # Example usage formatter = CustomFormatter() format_string = \\"Name: {name!u}, Reversed: {value!r}, Length: {name:len}\\" output = formatter.format(format_string, name=\\"john\\", value=\\"hello\\") print(output) # Output: \\"Name: JOHN, Reversed: olleh, Length: 4john\\" ```","solution":"import string class CustomFormatter(string.Formatter): def format_field(self, value, format_spec): if \'len\' in format_spec: value = f\\"{len(value)}{value}\\" format_spec = format_spec.replace(\'len\', \'\') return super().format_field(value, format_spec) def convert_field(self, value, conversion): if conversion == \'r\': return value[::-1] elif conversion == \'u\': return value.upper() return super().convert_field(value, conversion)"},{"question":"**Objective:** To assess your understanding of the `email.header` module for creating, encoding, and decoding internationalized email headers. **Tasks:** 1. **Create a MIME-compliant Header:** - Write a function `create_header` that accepts a string `header_value` and a character set `charset`, and returns a MIME-compliant `Header` object. - Example: `create_header(\'pxf6stal\', \'iso-8859-1\')` should return a `Header` object representing \'pxf6stal\' encoded in \'iso-8859-1\'. 2. **Encode the Header:** - Write a function `encode_header` that accepts a `Header` object and returns its RFC-compliant encoded string representation. - Example: `encode_header(Header(\'pxf6stal\', \'iso-8859-1\'))` should return `\'=?iso-8859-1?q?p=F6stal?=\'`. 3. **Decode the Header:** - Write a function `decode_header_value` that accepts an encoded header string and returns the original string along with its character set. - Example: `decode_header_value(\'=?iso-8859-1?q?p=F6stal?=\')` should return `(\'pxf6stal\', \'iso-8859-1\')`. Your implementation should ensure proper handling of different character sets, and it should be robust against encoding and decoding errors. **Function Signatures:** ```python def create_header(header_value: str, charset: str) -> Header: # Your implementation here def encode_header(header: Header) -> str: # Your implementation here def decode_header_value(encoded_header: str) -> tuple: # Your implementation here ``` **Constraints:** - `header_value` is a non-empty string containing non-ASCII characters. - `charset` is a valid character set supported by the `email` module. - The functions should handle errors gracefully and provide meaningful error messages in case of invalid inputs or encoding issues. **Note:** - You must use the `Header` class and methods from the `email.header` module to complete this task. - You can assume that the character sets provided are valid and supported by the `email` module for simplicity. **Evaluation Criteria:** - Correctness of the implementation. - Proper usage of the `email.header` module. - Handling of different character sets and encoding errors. - Code readability and comments explaining your code.","solution":"from email.header import Header, decode_header def create_header(header_value: str, charset: str) -> Header: Creates a MIME-compliant Header object with the given header_value and charset. :param header_value: The value of the header to be encoded. :param charset: The character set to use for encoding. :return: A MIME-compliant Header object. return Header(header_value, charset) def encode_header(header: Header) -> str: Encodes a Header object into its RFC-compliant encoded string representation. :param header: The Header object to be encoded. :return: An encoded string representation of the header. return header.encode() def decode_header_value(encoded_header: str) -> tuple: Decodes an encoded header string into its original value and character set. :param encoded_header: The encoded header string to be decoded. :return: A tuple containing the original header value and the character set. decoded_value, charset = decode_header(encoded_header)[0] return decoded_value.decode(charset), charset"},{"question":"Problem Statement **Task**: You are required to implement a function that works with ZIP archives using Python’s `zipfile` module. The function should achieve the following: 1. **Create a ZIP file** named `output.zip` that contains multiple files from a given directory. 2. **Extract a specified file** from `output.zip` to a specific location. 3. **List all files** within `output.zip` and return their names. # Input and Output Format **Input**: - A string `directory_path` specifying the path to the directory containing files to be archived. - A string `file_to_extract` specifying the name of the file inside the ZIP archive to be extracted. - A string `extract_to` specifying the directory path where the file should be extracted. **Output**: - A list of strings representing the names of all files inside the `output.zip` archive. # Constraints - Make sure to handle ZIP compression using the `ZIP_DEFLATED` method. - If the specified file to be extracted does not exist in the ZIP archive, the function should raise a `KeyError`. # Example ```python import os # Prepare the directory and files for testing os.mkdir(\'test_dir\') with open(\'test_dir/file1.txt\', \'w\') as f: f.write(\'Hello world!\') with open(\'test_dir/file2.txt\', \'w\') as f: f.write(\'Python zipfile module!\') # Function call with inputs result = handle_zip_operations(\'test_dir\', \'file1.txt\', \'extracted\') # Output [(\'file1.txt\', ...), (\'file2.txt\', ...)] print(result) # Check the content of the extracted file with open(\'extracted/file1.txt\', \'r\') as f: print(f.read()) # Output: Hello world! ``` # Implementation ```python import os import zipfile def handle_zip_operations(directory_path, file_to_extract, extract_to): zip_filename = \'output.zip\' # Create a ZIP file with all files in the given directory with zipfile.ZipFile(zip_filename, \'w\', zipfile.ZIP_DEFLATED) as zipf: for foldername, subfolders, filenames in os.walk(directory_path): for filename in filenames: filepath = os.path.join(foldername, filename) zipf.write(filepath, os.path.relpath(filepath, directory_path)) # Extract the specified file to the extract_to directory with zipfile.ZipFile(zip_filename, \'r\') as zipf: if file_to_extract not in zipf.namelist(): raise KeyError(f\\"The file {file_to_extract} does not exist in the archive.\\") zipf.extract(file_to_extract, extract_to) # List all files in the ZIP archive and return their names with zipfile.ZipFile(zip_filename, \'r\') as zipf: return zipf.namelist() # Example usage: # result = handle_zip_operations(\'path_to_directory\', \'file_inside_zip.txt\', \'extract_to_directory\') # print(result) ``` # Notes 1. Ensure your function uses proper error handling to deal with non-existent files and directories. 2. You can create additional helper functions if necessary. 3. Thoroughly test your function to handle various edge cases, such as empty directories, non-existent files, etc.","solution":"import os import zipfile def handle_zip_operations(directory_path, file_to_extract, extract_to): zip_filename = \'output.zip\' # Create a ZIP file with all files in the given directory with zipfile.ZipFile(zip_filename, \'w\', zipfile.ZIP_DEFLATED) as zipf: for foldername, subfolders, filenames in os.walk(directory_path): for filename in filenames: filepath = os.path.join(foldername, filename) zipf.write(filepath, os.path.relpath(filepath, directory_path)) # Extract the specified file to the extract_to directory with zipfile.ZipFile(zip_filename, \'r\') as zipf: if file_to_extract not in zipf.namelist(): raise KeyError(f\\"The file {file_to_extract} does not exist in the archive.\\") zipf.extract(file_to_extract, extract_to) # List all files in the ZIP archive and return their names with zipfile.ZipFile(zip_filename, \'r\') as zipf: return zipf.namelist()"},{"question":"# Question: Working with Meta Tensors in PyTorch In this exercise, you will demonstrate your understanding of PyTorch\'s meta tensors and how to use them effectively. Task 1. **Create a Linear Model with Meta Tensors** Write a function `create_meta_linear_model(in_features, out_features)` that: - Takes two integers, `in_features` and `out_features`, representing the input and output features of a linear model respectively. - Constructs a PyTorch `Linear` model using the meta device such that no actual data is loaded into memory. - Returns the constructed model with its parameters uninitialized. ```python def create_meta_linear_model(in_features, out_features): # Your code here ``` 2. **Load and Transform Model** Write a function `load_and_transform_model(path, transformation_fn)` that: - Loads a PyTorch model saved at the given `path` on the meta device. - Applies a transformation function `transformation_fn` to the loaded model. The `transformation_fn` takes a PyTorch model and modifies it in some way (e.g., changing layer sizes, adding new layers). - Returns the transformed model. ```python def load_and_transform_model(path, transformation_fn): # Your code here ``` Examples ```python # Example usage of create_meta_linear_model model = create_meta_linear_model(10, 5) print(model) # Should print out a Linear model with no initialized parameters # Example usage of load_and_transform_model def sample_transformation_fn(model): for param in model.parameters(): param.data = param.data * 2 # This is a simple transformation example torch.save(torch.nn.Linear(10, 5), \'linear_model.pt\') transformed_model = load_and_transform_model(\'linear_model.pt\', sample_transformation_fn) print(transformed_model) # Should print out the transformed model ``` Constraints - All models and tensors should be created and manipulated using PyTorch\'s meta device. - Ensure that the `load_and_transform_model` function handles edge cases where the model might not be compatible with the transformations. Additional Information - Usage of the `torch.meta` device. - Familiarity with PyTorch\'s `torch.nn` and `torch.load` functionalities. - Handling uninitialized parameters and custom initialization if needed. Good luck and happy coding!","solution":"import torch def create_meta_linear_model(in_features, out_features): Creates a PyTorch Linear model on the meta device. Args: - in_features (int): The number of input features for the Linear model. - out_features (int): The number of output features for the Linear model. Returns: - torch.nn.Linear: The created Linear model with its parameters uninitialized. return torch.nn.Linear(in_features, out_features, device=torch.device(\'meta\')) def load_and_transform_model(path, transformation_fn): Loads a PyTorch model on the meta device and applies a transformation function to it. Args: - path (str): The path to the saved PyTorch model. - transformation_fn (Callable): A function that takes a model and returns a transformed model. Returns: - torch.nn.Module: The transformed model. # Load the model on the meta device model = torch.load(path, map_location=torch.device(\'meta\')) # Apply the transformation function to the model transformed_model = transformation_fn(model) return transformed_model"},{"question":"**Question: Implementing a Custom Module Importer using `zipimport`** **Objective:** Your task is to create a custom Python script that utilizes the `zipimporter` class to load and execute a module from a ZIP archive. This will test your understanding of the `zipimport` module in Python 3.10. **Scenario:** You are provided with a ZIP file that contains multiple Python modules. Your script should be able to: 1. List all the Python modules (`.py` files) available in the ZIP archive. 2. Import and execute a specified module from the ZIP archive. 3. Handle any exceptions that may occur during the import process. **Details:** 1. Implement a function `list_modules(archive_path: str) -> list` that takes the path to a ZIP file and returns a list of all Python module names available in the archive. 2. Implement a function `import_and_execute(archive_path: str, module_name: str, func_name: str, *args)` that takes the path to a ZIP file, the name of the module, the name of the function within the module to execute, and any additional arguments for the function. This function should import the specified module and execute the given function with the provided arguments. **Requirements:** - Use the `zipimport` module and its `zipimporter` class. - Use appropriate methods from the `zipimporter` class to achieve the tasks. - Handle `zipimport.ZipImportError` and other relevant exceptions properly. - Ensure that the ZIP archive is valid and contains the specified module and function. **Constraints:** - The ZIP archive will only contain `.py` files (no precompiled `.pyc` files). - You are not allowed to extract files from the ZIP archive to the filesystem. - The function to be executed in the module will return a string that you need to capture and print. **Input Format:** - `archive_path`: String, representing the path to the ZIP archive. - `module_name`: String, representing the name of the module to import. - `func_name`: String, representing the name of the function to execute. - `*args`: Variable length argument list for the function to execute. **Output Format:** - `list_modules` should return a list of strings, each representing a module name in the ZIP archive. - `import_and_execute` should print the result returned by the executed function. **Example:** Suppose we have a ZIP file `modules.zip` with the following structure: ``` modules.zip │ ├── module1.py │ └── def hello(name): return f\'Hello, {name}!\' │ ├── module2.py │ └── def add(a, b): return f\'The sum is {a + b}\' ``` Example usage of your functions: ```python archive_path = \'modules.zip\' # List modules modules = list_modules(archive_path) print(modules) # Output: [\'module1\', \'module2\'] # Import and execute function from module import_and_execute(archive_path, \'module1\', \'hello\', \'World\') # Output: \'Hello, World!\' import_and_execute(archive_path, \'module2\', \'add\', 5, 3) # Output: \'The sum is 8\' ``` Make sure to cover edge cases such as invalid paths, non-existent modules, and non-existent functions within the modules. **Note:** For testing purposes, you might mock the ZIP archive and the modules within your script.","solution":"import zipimport def list_modules(archive_path: str) -> list: List all Python module names available in the ZIP file. Parameters: archive_path (str): Path to the ZIP archive. Returns: list: List of module names (without the .py extension). try: importer = zipimport.zipimporter(archive_path) module_names = [name[:-3] for name in importer._files if name.endswith(\'.py\')] return module_names except Exception as e: raise RuntimeError(f\\"An error occurred while listing modules: {str(e)}\\") from e def import_and_execute(archive_path: str, module_name: str, func_name: str, *args): Import a module from the ZIP file and execute a function within it. Parameters: archive_path (str): Path to the ZIP archive. module_name (str): Name of the module to import. func_name (str): Name of the function to execute. *args: Arguments to pass to the function being executed. Returns: The result of the function execution. try: importer = zipimport.zipimporter(archive_path) module = importer.load_module(module_name) func = getattr(module, func_name) result = func(*args) print(result) except zipimport.ZipImportError as e: raise ImportError(f\\"ZIP import error: {str(e)}\\") from e except AttributeError as e: raise AttributeError(f\\"Function \'{func_name}\' not found in module \'{module_name}\': {str(e)}\\") from e except Exception as e: raise RuntimeError(f\\"An error occurred during import or execution: {str(e)}\\") from e"},{"question":"# Persistent Asynchronous Value Tracker You are creating an asynchronous system to keep track of values that are generated over time, and you want to persist these values to a file asynchronously whenever a certain condition is met. The condition and operations incorporate extensive use of Python\'s built-in functions. Task Implement a class `ValueTracker` with the following requirements: 1. **Initialization**: - The class should initialize with an empty list to store values. 2. **Methods**: - `add_value(value)`: Adds a new value to the list. - `check_condition()`: Returns `True` if the absolute value of the difference between the maximum and minimum values in the list exceeds a certain threshold `T`, and `False` otherwise. - `persist_values(filepath, condition_fn)`: An asynchronous method that writes the list of values to a specified file `filepath` in sorted order using the `sorted()` function whenever the `condition_fn` (function passed as argument) returns `True`. - Define a `__str__` method to represent the `ValueTracker` object by showing the current list of values. Constraints - Implement the asynchronous method using `aiter()` and `anext()`. - Ensure the `persist_values` method continues to check the condition periodically until manually stopped and writes the most up-to-date values each time the condition is satisfied. - Use built-in functions like `abs()`, `max()`, `min()`, `sorted()`, `open()`, etc. - Design it in such a way that it can handle up to `10^6` entries in an efficient manner. Example Usage ```python import asyncio async def main(): tracker = ValueTracker() tracker.add_value(3) tracker.add_value(10) tracker.add_value(-5) # Define a simple threshold condition function def threshold_condition(): return abs(max(tracker.values) - min(tracker.values)) > 10 # Asynchronously persist when the condition is met await tracker.persist_values(\'values.txt\', threshold_condition) asyncio.run(main()) ``` Expected Output The file `values.txt` should be periodically updated with the sorted list of values whenever the specified condition is met.","solution":"import asyncio from typing import List, Callable class ValueTracker: def __init__(self): self.values: List[int] = [] def add_value(self, value: int): self.values.append(value) def check_condition(self) -> bool: if not self.values: return False return abs(max(self.values) - min(self.values)) > 10 async def persist_values(self, filepath: str, condition_fn: Callable[[], bool]): while True: await asyncio.sleep(1) # Check condition periodically, e.g., every second if condition_fn(): async with await asyncio.to_thread(open, filepath, \'w\') as f: await asyncio.to_thread(f.write, \'n\'.join(map(str, sorted(self.values))) + \'n\') def __str__(self) -> str: return f\\"Current values: {self.values}\\" # Example main function (not meant for unit testing) async def main(): tracker = ValueTracker() tracker.add_value(3) tracker.add_value(10) tracker.add_value(-5) def threshold_condition(): return abs(max(tracker.values) - min(tracker.values)) > 10 await tracker.persist_values(\'values.txt\', threshold_condition) # Uncomment for example run # asyncio.run(main())"},{"question":"# Advanced Comprehensions and Generators in Python 3.10 You are required to implement a function `complex_expression_evaluator(data: dict) -> dict` that evaluates a complex data processing pipeline. The task involves using various Python expression elements, including comprehensions, generator expressions, and attribute references. Task Description: 1. **Input**: The function receives a dictionary `data` with keys being string identifiers and values being lists of integers. 2. **Output**: The function should return a new dictionary where each key contains the result of a complex transformation based on the given values: - If the length of the integer list is less than 5, the value should be a list of squares of even numbers from the list. - For lists with lengths between 5 and 10 (inclusive), generate a list where each number is doubled if it is divisible by 3. - For lists longer than 10, return a generator expression that yields elements greater than the average of the entire list. Constraints: - You must use list comprehensions, set comprehensions, and generator expressions wherever applicable. - Do not use any standard library functions other than `sum` and `len`. Performance Requirements: - Ensure that the solution avoids unnecessary recomputation and leverages Python\'s lazy evaluation where applicable. Example: ```python def complex_expression_evaluator(data: dict) -> dict: # Your implementation here # Example Usage: input_data = { \'a\': [1, 2, 3, 4, 5], \'b\': [6, 7, 8, 9, 10, 11], \'c\': [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23] } output = complex_expression_evaluator(input_data) print(output) # Expected Output: # { # \'a\': [4, 16], # \'b\': [6, 18], # 6, 18 as 6 and 9 are valid within constraints and doubled # \'c\': <generator object> # generator yielding elements greater than the average of the list # } # To further verify: print(list(output[\'c\'])) # [14, 15, 16, 17, 18, 19, 20, 21, 22, 23] (if calculated average is less than 14) ```","solution":"def complex_expression_evaluator(data: dict) -> dict: result = {} for key, values in data.items(): if len(values) < 5: result[key] = [x ** 2 for x in values if x % 2 == 0] elif len(values) <= 10: result[key] = [x * 2 for x in values if x % 3 == 0] else: avg = sum(values) / len(values) result[key] = (x for x in values if x > avg) return result"},{"question":"Objective: Demonstrate your understanding of `asyncio` for asynchronous operations and `socket` for low-level networking by designing and implementing a simple asynchronous chat server and client. Task: 1. **Chat Server**: - Implement an asynchronous chat server using the `asyncio` and `socket` module. - The server should be able to handle multiple clients simultaneously. - When a client sends a message, the server should broadcast it to all connected clients. - The server should also handle the scenario where a client disconnects. 2. **Chat Client**: - Implement a chat client that connects to the chat server. - The client should be able to send messages to the server, which will then be broadcast to all connected clients. - The client should also be able to receive broadcast messages from the server asynchronously. Server Implementation: - Create an asynchronous server using `asyncio` that listens on a specified port. - Use the `socket` module to create a listening socket. - Accept connections asynchronously and handle multiple clients. - Broadcast messages to all clients asynchronously. Client Implementation: - Create an asynchronous client using `asyncio` that connects to the server. - Use the `socket` module to connect to the server. - Implement sending messages to the server and receiving broadcast messages asynchronously. Input and Output: - **Chat Server**: - Input: Messages from any connected client. - Output: Broadcast the messages to all connected clients. - **Chat Client**: - Input: User types messages in the terminal. - Output: Received broadcast messages from the server are printed in the terminal. Constraints: - Include proper exception handling for network errors. - Ensure the code is non-blocking and all I/O operations are performed asynchronously. - The server should run on `localhost` for testing purposes and use a port number in the range 1024-65535. Performance Requirements: - The server must handle up to 10 simultaneous clients without performance degradation. - Messages should be transmitted with minimal latency, ensuring a responsive chat experience for all users. --- **Use the following function signatures as guidelines**: ```python # Chat Server import asyncio import socket async def handle_client(client_socket, clients): pass async def start_chat_server(host: str, port: int): pass # Chat Client import asyncio import socket async def send_message(writer, message: str): pass async def receive_message(reader): pass async def start_chat_client(host: str, port: int): pass ``` **Note**: Implement additional helper functions as needed to support the main functionality.","solution":"import asyncio import socket async def handle_client(client_socket, clients): reader, writer = await asyncio.open_connection(sock=client_socket) clients.add(writer) try: while True: message = await reader.read(100) if not message: break # Broadcast the message to all connected clients for client in clients: if client != writer: client.write(message) await client.drain() except Exception as e: pass finally: clients.remove(writer) writer.close() await writer.wait_closed() async def start_chat_server(host: str, port: int): server = await asyncio.start_server( lambda r, w: handle_client(w, clients), host, port) addr = server.sockets[0].getsockname() print(f\'Serving on {addr}\') async with server: await server.serve_forever() async def send_message(writer, message: str): writer.write(message.encode()) await writer.drain() async def receive_message(reader): while True: message = await reader.read(100) if message: print(message.decode()) async def start_chat_client(host: str, port: int): reader, writer = await asyncio.open_connection(host, port) tasks = [ receive_message(reader), input_loop(writer) ] await asyncio.gather(*tasks) async def input_loop(writer): while True: message = await asyncio.get_event_loop().run_in_executor(None, input, \\"> \\") await send_message(writer, message)"},{"question":"# Problem: Custom Sequence and Iterator Implementation Python\'s standard library provides a rich set of sequence types (like lists, tuples) and iterators. For this coding challenge, you are required to implement a custom sequence type with specific capabilities and an accompanying iterator. Requirements: 1. **CustomSequence Class**: - This class should mimic a list-like sequence. - Implement the following methods: - `__init__(self, data: list[int])`: Initializes the sequence with a list of integers. - `__getitem__(self, index: int) -> int`: Retrieves the element at the specified index. - `__len__(self) -> int`: Returns the length of the sequence. 2. **CustomIterator Class**: - This class should provide an iterator for the `CustomSequence`. - Implement the following methods: - `__init__(self, sequence: CustomSequence)`: Initializes the iterator with the provided `CustomSequence` object. - `__iter__(self) -> CustomIterator`: Returns the iterator object itself. - `__next__(self) -> int`: Returns the next value from the sequence. If the end of the sequence is reached, raise a `StopIteration` exception. 3. **Additional Functionalities**: - Ensure that `CustomSequence` can handle index bounds checking appropriately. - Ensure that `CustomIterator` properly iterates over the sequence from beginning to end. 4. **Efficiency**: - Your implementation should efficiently handle sequences of up to (10^6) elements. - The `__getitem__` method should have a time complexity of (O(1)). Constraints: - You can assume all input data for `CustomSequence` will be valid integers within the range (-10^9) to (10^9). - The sequence length will not exceed (10^6) elements. Example Usage: ```python # Example initialization and usage of CustomSequence and CustomIterator seq = CustomSequence([1, 2, 3, 4, 5]) print(seq[2]) # Output: 3 print(len(seq)) # Output: 5 iterator = CustomIterator(seq) for value in iterator: print(value) # Output: 1 2 3 4 5 ``` You are required to submit the implementation of the `CustomSequence` and `CustomIterator` classes. Ensure that your code conforms to Python 3.10 standards and leverages the appropriate abstract object protocols discussed.","solution":"class CustomSequence: def __init__(self, data: list[int]): self.data = data def __getitem__(self, index: int) -> int: if index < 0 or index >= len(self.data): raise IndexError(\\"Index out of bounds\\") return self.data[index] def __len__(self) -> int: return len(self.data) class CustomIterator: def __init__(self, sequence: CustomSequence): self.sequence = sequence self.index = 0 def __iter__(self) -> \'CustomIterator\': return self def __next__(self) -> int: if self.index >= len(self.sequence): raise StopIteration result = self.sequence[self.index] self.index += 1 return result"},{"question":"# Custom Business Calendar Analysis with Pandas Offsets Objective Implement a class `CustomBusinessCalendar` that allows managing and analyzing dates using custom business rules. Utilize the `CustomBusinessDay` from the pandas library to achieve this. Class Specifications 1. **Class Name**: `CustomBusinessCalendar` 2. **Attributes**: - `holidays`: List of `datetime.date` objects indicating custom holidays. - `weekmask`: String indicating working days (e.g., \'Mon Tue Wed Thu Fri\'). 3. **Methods**: - `__init__(self, holidays: List[datetime.date], weekmask: str = \'Mon Tue Wed Thu Fri\')`: - Initializes the custom business calendar with provided holidays and a weekmask. - `add_business_days(self, start_date: str, days: int) -> str`: - Adds a given number of business days to the start date. - **Input**: - `start_date`: A string representing the start date in \'YYYY-MM-DD\' format. - `days`: An integer number of business days to add. - **Output**: - A string representing the resulting date in \'YYYY-MM-DD\' format. - `is_holiday(self, date: str) -> bool`: - Checks if the given date is a holiday. - **Input**: - `date`: A string representing the date in \'YYYY-MM-DD\' format. - **Output**: - Boolean indicating whether the date is a holiday (`True`) or not (`False`). Constraints and Considerations - You can only use the `pandas.tseries.offsets` module. - Date strings are always provided in the format \'YYYY-MM-DD\'. - Handle edge cases like adding zero days and dates on holidays. **Example Usage:** ```python from datetime import date # Initialize with custom holidays and a standard workweek custom_calendar = CustomBusinessCalendar( holidays=[date(2023, 12, 25), date(2024, 1, 1)], weekmask=\'Mon Tue Wed Thu Fri\' ) # Adding business days result_date = custom_calendar.add_business_days(\'2023-12-22\', 5) print(result_date) # Expected \'2024-01-02\' # Check if a date is a holiday is_holiday = custom_calendar.is_holiday(\'2023-12-25\') print(is_holiday) # Expected: True ``` **Performance Requirements**: - The methods should be optimized for performance and capable of handling up to 100 quick successive checks or operations.","solution":"from datetime import datetime, date import pandas as pd from pandas.tseries.offsets import CustomBusinessDay class CustomBusinessCalendar: def __init__(self, holidays, weekmask=\'Mon Tue Wed Thu Fri\'): self.holidays = pd.to_datetime(holidays) self.weekmask = weekmask self.business_day = CustomBusinessDay(holidays=self.holidays, weekmask=self.weekmask) def add_business_days(self, start_date, days): start_date = pd.to_datetime(start_date) end_date = start_date + days * self.business_day return end_date.strftime(\'%Y-%m-%d\') def is_holiday(self, date_str): date_obj = pd.to_datetime(date_str) return date_obj in self.holidays"},{"question":"# Regular Expression Validation and Transformation In this coding assessment, you are required to implement a function that validates and transforms a list of email addresses using regular expressions. The function will perform the following tasks: 1. **Validation**: Ensure that each email address in the given list conforms to a specific pattern. 2. **Transformation**: Replace the domain of each valid email address with \\"company.com\\". 3. **Output**: Return a list of transformed email addresses, excluding any invalid ones. Email Validation Pattern A valid email address must: - Contain an alphanumeric username (can include periods, hyphens, and underscores). - Have an \\"@\\" symbol separating the username and the domain. - Have a domain name that consists of alphabets and possibly periods (e.g., \\"example.com\\", \\"sub.example.com\\"). For the sake of this task, you can assume all characters are in lowercase. Function Signature ```python import re from typing import List def transform_emails(email_list: List[str]) -> List[str]: pass ``` Input - `email_list`: A list of strings, where each string is an email address. Output - Returns a list of transformed email addresses that are valid. Constraints - The length of `email_list` will not exceed 1000. - Each email string will be non-empty and not exceed 320 characters in length. Example ```python email_list = [ \\"john.doe@example.com\\", \\"jane-doe@example.net\\", \\"invalid@address\\", \\"alice@sub.example.com\\" ] print(transform_emails(email_list)) ``` Expected Output ```python [ \\"john.doe@company.com\\", \\"jane-doe@company.com\\", \\"alice@company.com\\" ] ``` Description of the Function 1. **Compile the Regular Expression**: - Compile a regex pattern for matching valid email addresses. 2. **Validation and Transformation**: - Iterate through the `email_list` and check each email against the regex pattern. - For valid emails, replace the domain with \\"company.com\\". 3. **Return the Transformed List**: - Return the list of transformed email addresses, excluding any invalid ones. Notes - Remember to use Python\'s raw string notation for defining the regular expression. - Ensure that your regular expression considers edge cases and matches the given examples precisely.","solution":"import re from typing import List def transform_emails(email_list: List[str]) -> List[str]: # Define the regex pattern for a valid email address email_pattern = re.compile(r\'^[a-z0-9._%+-]+@[a-z0-9.-]+.[a-z]+\') transformed_emails = [] for email in email_list: # Check if the email matches the regex pattern if email_pattern.match(email): # Replace the domain with \'company.com\' local_part = email.split(\'@\')[0] transformed_emails.append(f\'{local_part}@company.com\') return transformed_emails"},{"question":"Objective: Implement an asynchronous task processor using `asyncio.Queue`. The processor should be able to handle multiple types of queues (FIFO, LIFO, Priority) and manage tasks with error handling, ensuring that all tasks are processed correctly. Problem Statement: You are required to write a function `async_task_processor(task_list, queue_type, maxsize)` that takes a list of tasks, the type of queue to use, and the maximum size of the queue. - `task_list`: A list of tuples where each tuple represents a task. - For `Queue` and `LifoQueue`: each tuple is of the form `(task_id, duration)`. - For `PriorityQueue`: each tuple is of the form `(priority, task_id, duration)`. - `queue_type`: A string that can be either `\'fifo\'`, `\'lifo\'`, or `\'priority\'`. - `maxsize`: The maximum size of the queue. Use the following asyncio.Queue classes based on the `queue_type`: - `\'fifo\'`: Use `asyncio.Queue`. - `\'lifo\'`: Use `asyncio.LifoQueue`. - `\'priority\'`: Use `asyncio.PriorityQueue`. Requirements: 1. **Initialize the correct type of queue** based on `queue_type` and `maxsize`. 2. **Add tasks to the queue** utilising the appropriate format for each queue type. 3. **Create and manage workers** that will process the tasks concurrently. At a minimum, use 3 worker tasks. 4. **Handle queue fullness and emptiness** appropriately using `QueueFull` and `QueueEmpty` exceptions. 5. **Ensure all tasks are processed** and use `queue.task_done()` accordingly. 6. The function must handle the async operations with correct usage of `async`/`await`. Input: - `task_list`: List of tuples. - `queue_type`: String (`\'fifo\'`, `\'lifo\'`, `\'priority\'`). - `maxsize`: Integer. Output: The function does not need to return anything, but it should print the progress and completion of each task. Example: ```python import asyncio async def async_task_processor(task_list, queue_type, maxsize): # Implement the function here following the described requirements # Example usage: task_list_fifo = [(1, 2), (2, 1), (3, 0.5)] task_list_lifo = [(1, 2), (2, 1), (3, 0.5)] task_list_priority = [(1, 1, 2), (2, 2, 1), (3, 3, 0.5)] queue_type = \'fifo\' maxsize = 5 asyncio.run(async_task_processor(task_list_fifo, queue_type, maxsize)) ``` # Notes: - Make sure to handle cancellation of worker tasks properly. - Use `asyncio.sleep()` to simulate task processing. - Ensure that queue operations are performed asynchronously and handled correctly to ensure smooth task processing.","solution":"import asyncio from asyncio import Queue, LifoQueue, PriorityQueue, QueueFull, QueueEmpty async def worker(name, queue): while True: try: task = await queue.get() if isinstance(queue, PriorityQueue): priority, task_id, duration = task print(f\'{name} processing {task_id} with priority {priority} for {duration} seconds\') else: task_id, duration = task print(f\'{name} processing {task_id} for {duration} seconds\') await asyncio.sleep(duration) queue.task_done() print(f\'{name} completed {task_id}\') except QueueEmpty: break async def async_task_processor(task_list, queue_type, maxsize): if queue_type == \'fifo\': queue = Queue(maxsize) elif queue_type == \'lifo\': queue = LifoQueue(maxsize) elif queue_type == \'priority\': queue = PriorityQueue(maxsize) else: raise ValueError(\\"Invalid queue_type. Must be \'fifo\', \'lifo\', or \'priority\'\\") for task in task_list: try: await queue.put(task) except QueueFull: print(\\"Queue is full, waiting to add task\\") await queue.join() workers = [] for i in range(3): worker_name = f\'worker-{i+1}\' worker_task = asyncio.create_task(worker(worker_name, queue)) workers.append(worker_task) await queue.join() for w in workers: w.cancel()"},{"question":"You are tasked with implementing a custom attention layer using PyTorch that utilizes various mask creation and manipulation utilities provided by the `torch.nn.attention.flex_attention` module. # Problem Statement Using the `torch.nn.attention.flex_attention` module, implement a custom attention mechanism that adheres to the following specifications: 1. **CustomAttentionLayer Class**: - A class named `CustomAttentionLayer` should be defined. - The class should inherit from `torch.nn.Module`. 2. **Input**: - The class\'s forward method should accept three main inputs: - `queries`: Tensor of shape `[batch_size, query_len, hidden_dim]` - `keys`: Tensor of shape `[batch_size, key_len, hidden_dim]` - `values`: Tensor of shape `[batch_size, key_len, hidden_dim]` - Additionally, it should accept an optional parameter `mask_type` which could be either \\"block\\", \\"nested\\", or None. 3. **Output**: - The forward method should output an attention-applied tensor of shape `[batch_size, query_len, hidden_dim]`. 4. **Mask Application**: - Depending on the `mask_type` value, different masks should be created: - If `mask_type` is \\"block\\", use `create_block_mask`. - If `mask_type` is \\"nested\\", use `create_nested_block_mask`. - If `mask_type` is None, apply no mask (`noop_mask`). - Use the created mask to mask the attention scores during the calculation. 5. **Attention Mechanism**: - Implement a basic scaled dot-product attention mechanism with masking. - The attention score should be calculated as: ``` attention_scores = (queries @ keys.transpose(-2, -1)) / sqrt(hidden_dim) ``` - Apply the selected mask to the attention scores. - Use softmax to normalize the masked attention scores. - Compute the final attention output as a weighted sum of value vectors weighted by the attention scores. # Constraints: 1. Ensure that the `mask_type` parameter defaults to None if not provided. 2. The implemented class should handle masking internally without requiring external mask inputs. # Example Usage: ```python import torch from torch.nn.attention.flex_attention import CustomAttentionLayer # Assumes batch_size = 2, query_len = 4, key_len = 4, hidden_dim = 8 queries = torch.randn(2, 4, 8) keys = torch.randn(2, 4, 8) values = torch.randn(2, 4, 8) attention_layer = CustomAttentionLayer() # Using block mask output = attention_layer(queries, keys, values, mask_type=\\"block\\") # No mask output = attention_layer(queries, keys, values) print(output.shape) # Expected: torch.Size([2, 4, 8]) ``` # Submission: Submit the full implementation of `CustomAttentionLayer`. Ensure you handle edge cases and provide clear comments within your code.","solution":"import torch import torch.nn as nn import math class CustomAttentionLayer(nn.Module): def __init__(self): super(CustomAttentionLayer, self).__init__() def forward(self, queries, keys, values, mask_type=None): hidden_dim = queries.size(-1) # Calculate attention scores attention_scores = torch.matmul(queries, keys.transpose(-2, -1)) / math.sqrt(hidden_dim) # Apply the appropriate mask if mask_type == \\"block\\": mask = self.create_block_mask(attention_scores) elif mask_type == \\"nested\\": mask = self.create_nested_block_mask(attention_scores) else: mask = self.noop_mask(attention_scores) attention_scores = attention_scores.masked_fill(mask == 0, float(\'-inf\')) # Normalize scores with softmax attention_weights = torch.softmax(attention_scores, dim=-1) # Compute the weighted sum of values output = torch.matmul(attention_weights, values) return output def create_block_mask(self, tensor): # Example mask logic; adapt as needed mask = torch.ones_like(tensor) return mask def create_nested_block_mask(self, tensor): # Example mask logic; adapt as needed mask = torch.ones_like(tensor) return mask def noop_mask(self, tensor): return torch.ones_like(tensor)"},{"question":"# JSON Encoder and Decoder with Custom Object **Objective:** Design a custom encoder and decoder for a specific Python class using the `json` module. Problem Statement: You are given a Python class `Person` which stores information about individuals. Your task is to write custom JSON encoder and decoder functions to serialize and deserialize `Person` objects to and from JSON format. **Class Definition:** ```python class Person: def __init__(self, name, age, is_student): self.name = name self.age = age self.is_student = is_student def __repr__(self): return f\\"Person(name={self.name}, age={self.age}, is_student={self.is_student})\\" ``` **Requirements:** 1. **Custom Encoder:** - Create a `PersonEncoder` class that inherits from `json.JSONEncoder`. - Override the `default()` method to handle `Person` objects. - Ensure that other types are serialized using the default method. 2. **Custom Decoder:** - Write a function `decode_person(dct)` that converts JSON object dictionaries representing `Person` objects back into `Person` instances. - Use the `object_hook` parameter in `json.loads()` to apply this function during deserialization. 3. **Serialization and Deserialization Functions:** - Implement `serialize_person(person)` which returns the JSON string representation of a `Person` object. - Implement `deserialize_person(json_str)` which returns a `Person` object from its JSON string representation. **Input Format:** - `serialize_person(person)` takes an instance of the `Person` class as input. - `deserialize_person(json_str)` takes a JSON formatted string as input. **Output Format:** - `serialize_person(person)` should return a JSON string. - `deserialize_person(json_str)` should return an instance of the `Person` class. Example: ```python # Creating a Person object person = Person(name=\\"John Doe\\", age=30, is_student=False) # Serialize the Person object to a JSON string json_str = serialize_person(person) print(json_str) # Output: {\\"name\\": \\"John Doe\\", \\"age\\": 30, \\"is_student\\": false} # Deserialize the JSON string back to a Person object person_obj = deserialize_person(json_str) print(person_obj) # Output: Person(name=John Doe, age=30, is_student=False) ``` **Constraints:** - You must use `json.dumps()` and `json.loads()` with appropriate customizations in your solution. - Handle any error cases where the input JSON string may not represent a valid `Person` object gracefully. Note: - Ensure that your custom encoder and decoder can handle special cases such as nested `Person` objects if present in future extensions.","solution":"import json class Person: def __init__(self, name, age, is_student): self.name = name self.age = age self.is_student = is_student def __repr__(self): return f\\"Person(name={self.name}, age={self.age}, is_student={self.is_student})\\" class PersonEncoder(json.JSONEncoder): def default(self, obj): if isinstance(obj, Person): return { \\"name\\": obj.name, \\"age\\": obj.age, \\"is_student\\": obj.is_student } return super().default(obj) def decode_person(dct): if \\"name\\" in dct and \\"age\\" in dct and \\"is_student\\" in dct: return Person(name=dct[\\"name\\"], age=dct[\\"age\\"], is_student=dct[\\"is_student\\"]) return dct def serialize_person(person): return json.dumps(person, cls=PersonEncoder) def deserialize_person(json_str): return json.loads(json_str, object_hook=decode_person)"},{"question":"Title: Implementation of HTTP Status Checker Objective: Create a Python class that contains methods to interact and manipulate HTTP status codes using the `http.HTTPStatus` enumeration. Problem Statement: You are required to implement a class named `HTTPStatusChecker` with the following methods: 1. **Method:** `get_status_info(self, status_code)` - **Input:** An integer representing an HTTP status code. - **Output:** A dictionary containing the status `code`, `phrase`, and `description` if the code exists; otherwise, raise a `ValueError` with the message `\\"Invalid HTTP Status Code\\"`. 2. **Method:** `is_informational(self, status_code)` - **Input:** An integer representing an HTTP status code. - **Output:** A boolean indicating whether the status code is informational (i.e., between 100 and 199 inclusive). 3. **Method:** `is_successful(self, status_code)` - **Input:** An integer representing an HTTP status code. - **Output:** A boolean indicating whether the status code represents a successful response (i.e., between 200 and 299 inclusive). 4. **Method:** `is_redirect(self, status_code)` - **Input:** An integer representing an HTTP status code. - **Output:** A boolean indicating whether the status code represents a redirection (i.e., between 300 and 399 inclusive). 5. **Method:** `is_client_error(self, status_code)` - **Input:** An integer representing an HTTP status code. - **Output:** A boolean indicating whether the status code represents a client error (i.e., between 400 and 499 inclusive). 6. **Method:** `is_server_error(self, status_code)` - **Input:** An integer representing an HTTP status code. - **Output:** A boolean indicating whether the status code represents a server error (i.e., between 500 and 599 inclusive). Constraints: - You must use the `http.HTTPStatus` enum for interactions with HTTP status codes. - Raise appropriate exceptions where necessary. - Ensure optimal performance for all methods. Example Usage: ```python from http import HTTPStatus class HTTPStatusChecker: def get_status_info(self, status_code): # Implement this method def is_informational(self, status_code): # Implement this method def is_successful(self, status_code): # Implement this method def is_redirect(self, status_code): # Implement this method def is_client_error(self, status_code): # Implement this method def is_server_error(self, status_code): # Implement this method # Example Usage checker = HTTPStatusChecker() print(checker.get_status_info(200)) # Output: {\'code\': 200, \'phrase\': \'OK\', \'description\': \'Request fulfilled, document follows\'} print(checker.is_informational(100)) # Output: True print(checker.is_successful(200)) # Output: True print(checker.is_redirect(301)) # Output: True print(checker.is_client_error(404)) # Output: True print(checker.is_server_error(500)) # Output: True ``` # Deliverable: Complete the `HTTPStatusChecker` class by implementing all the required methods. Ensure your code is clean, well-documented, and handles edge cases gracefully. Submission: Submit your Python script containing the `HTTPStatusChecker` class via the platform’s submission portal. Ensure you have tested your implementations thoroughly before submission.","solution":"from http import HTTPStatus class HTTPStatusChecker: def get_status_info(self, status_code): if status_code in HTTPStatus.__members__.values(): status = HTTPStatus(status_code) return {\'code\': status.value, \'phrase\': status.phrase, \'description\': status.description} else: raise ValueError(\\"Invalid HTTP Status Code\\") def is_informational(self, status_code): return 100 <= status_code <= 199 def is_successful(self, status_code): return 200 <= status_code <= 299 def is_redirect(self, status_code): return 300 <= status_code <= 399 def is_client_error(self, status_code): return 400 <= status_code <= 499 def is_server_error(self, status_code): return 500 <= status_code <= 599"},{"question":"# Seaborn Advanced Plotting Assessment **Objective**: Demonstrate your understanding of seaborn\'s advanced plotting capabilities by creating a complex layered plot using `seaborn.objects`. **Dataset**: Use the `tips` dataset, which can be loaded using `seaborn.load_dataset(\\"tips\\")`. **Task**: 1. Create a scatter plot of `total_bill` vs. `tip`. 2. Add a linear trend line to the scatter plot with a confidence interval. 3. Show the distribution of total bill by day of the week using histograms, side by side. 4. Use color to differentiate between the \\"sex\\" variable on the scatter plot. **Requirements**: - Use `so.Plot()` to initialize the plot. - Add multiple layers: At least one scatter (`Dot`), one line (`Line`), and one histogram (`Bar` layer with `Hist` transform). - Use appropriate aesthetics to distinguish between categories (e.g., color by `sex`). - Ensure that days are displayed side by side using a facet wrap for clear comparison. - Incorporate legends and labels for clarity. **Implementation**: ```python import seaborn.objects as so from seaborn import load_dataset # Load the dataset tips = load_dataset(\\"tips\\") # Create the plot plot = ( so.Plot(tips, \\"total_bill\\", \\"tip\\", color=\\"sex\\") .add(so.Dot()) .add(so.Line(), so.PolyFit()) .facet(col=\\"day\\") .add(so.Bar(), so.Hist(), col=\\"day\\", color=\\"sex\\") .label(y=\\"Tip\\") .label(x=\\"Total Bill\\") .label(color=\\"Sex\\") ) plot.show() ``` **Input**: - The `tips` dataset from seaborn. **Output**: - A complex plot with the specifications provided. **Constraints**: - The plot should be visually clear and convey all the required information. - Follow seaborn\'s best practices for layered plotting to ensure performance and clarity.","solution":"import seaborn.objects as so from seaborn import load_dataset def create_advanced_plot(): # Load the dataset tips = load_dataset(\\"tips\\") # Create the plot plot = ( so.Plot(tips, \\"total_bill\\", \\"tip\\", color=\\"sex\\") .add(so.Dot()) .add(so.Line(), so.PolyFit()) .facet(col=\\"day\\") .add(so.Bar(), so.Hist(), col=\\"day\\", color=\\"sex\\") .label(y=\\"Tip\\") .label(x=\\"Total Bill\\") .label(color=\\"Sex\\") ) return plot"},{"question":"Objective: You are required to implement a Python program that utilizes the \\"nis\\" module to perform several operations related to the NIS maps. This problem will assess your understanding of the \\"nis\\" module and your ability to use its functions. Problem Statement: Write a Python function that performs the following tasks: 1. Retrieves and prints the default NIS domain using `nis.get_default_domain()`. 2. Retrieves and prints a list of all valid NIS maps using `nis.maps()`. 3. For a given map name (input parameter), retrieve and print all keys and values using `nis.cat(mapname)`. 4. For a specified key and map (input parameters), retrieves and prints the value associated with the key using `nis.match(key, mapname)`. Function Signature: ```python def nis_operations(mapname: str, key: str): pass ``` Input: - `mapname` (str): The name of the NIS map to be used. - `key` (str): The key to look up in the specified map. Output: The function should print: 1. The default NIS domain. 2. The list of all valid NIS maps. 3. All key-value pairs in the specified map. 4. The value associated with the specified key in the map. Constraints: - You can assume that the \\"nis\\" module and necessary NIS configuration are available on the system executing the script. - Handle exceptions where the map or key does not exist by printing an appropriate error message. Example: ```python # Example mapname: \\"hosts.byname\\", key: \\"localhost\\" nis_operations(\\"hosts.byname\\", \\"localhost\\") ``` Expected Output: ``` Default NIS Domain: example.com Valid NIS Maps: [\'passwd.byname\', \'hosts.byname\', \'group.byname\'] Entries in map \'hosts.byname\': localhost: 127.0.0.1 example_host: 192.168.1.1 Value for key \'localhost\' in map \'hosts.byname\': 127.0.0.1 ``` # Notes: - Replace the example domain and map contents with actual data relevant to your system. - Ensure error handling for cases like non-existent keys or maps.","solution":"import nis def nis_operations(mapname: str, key: str): try: # Retrieve and print the default NIS domain. default_domain = nis.get_default_domain() print(f\\"Default NIS Domain: {default_domain}\\") # Retrieve and print a list of all valid NIS maps. valid_maps = nis.maps() print(f\\"Valid NIS Maps: {valid_maps}\\") # Retrieve and print all keys and values in the specified map. try: map_entries = nis.cat(mapname) print(f\\"Entries in map \'{mapname}\':\\") for k, v in map_entries.items(): print(f\\"{k}: {v}\\") except nis.error: print(f\\"Error: Map \'{mapname}\' does not exist or is not accessible.\\") return # Retrieve and print the value associated with the specified key in the map. try: value = nis.match(key, mapname) print(f\\"Value for key \'{key}\' in map \'{mapname}\': {value}\\") except nis.error: print(f\\"Error: Key \'{key}\' does not exist in map \'{mapname}\'.\\") except nis.error: print(\\"Error: Unable to retrieve NIS domain or maps.\\") # Example usage: # nis_operations(\\"hosts.byname\\", \\"localhost\\")"},{"question":"You are tasked with creating a comprehensive function named `system_info_report` that generates a detailed report about the underlying system where the Python interpreter is running. Your function should gather information using several functions from the `platform` module and format it into a human-readable string. # Function Signature ```python def system_info_report() -> str: pass ``` # Requirements 1. **Output Format**: The function should return a multi-line string with the following format: ``` System Information Report ------------------------- System: <system> Node: <node> Release: <release> Version: <version> Machine: <machine> Processor: <processor> Architecture: <bits>, <linkage> Python Version: <python_version> Python Build: <buildno>, <builddate> Python Compiler: <compiler> ``` 2. **Data to Include**: - System name using `platform.system()` - Node (network name) using `platform.node()` - System release using `platform.release()` - System version using `platform.version()` - Machine type using `platform.machine()` - Processor name using `platform.processor()` - Architecture details using `platform.architecture()` - Python version using `platform.python_version()` - Python build number and date using `platform.python_build()` - Python compiler used for compiling using `platform.python_compiler()` 3. **Handling Empty Values**: If any of the functions return an empty string or not available data, replace it with the string `\\"Unknown\\"`. # Example Output ```plaintext System Information Report ------------------------- System: Linux Node: my-computer Release: 5.4.0-42-generic Version: #46-Ubuntu SMP Fri Jul 10 00:24:02 UTC 2020 Machine: x86_64 Processor: x86_64 Architecture: 64bit, ELF Python Version: 3.8.10 Python Build: 3.8.10, Dec 6 2021 15:04:24 Python Compiler: GCC 9.3.0 ``` # Constraints - The function should not take any arguments. - The function should not rely on any external libraries except the ones mentioned. - The function should be efficiently implemented to handle quick generation of the system report. # Notes - Be sure to handle exceptions or edge cases where the platform functions might not return expected results. - The function should utilize the `platform` module functions exhaustively to retrieve the required system information.","solution":"import platform def system_info_report() -> str: Gathers system information and formats it into a report. system = platform.system() or \\"Unknown\\" node = platform.node() or \\"Unknown\\" release = platform.release() or \\"Unknown\\" version = platform.version() or \\"Unknown\\" machine = platform.machine() or \\"Unknown\\" processor = platform.processor() or \\"Unknown\\" architecture_bits, architecture_linkage = platform.architecture() architecture_bits = architecture_bits or \\"Unknown\\" architecture_linkage = architecture_linkage or \\"Unknown\\" python_version = platform.python_version() or \\"Unknown\\" python_buildno, python_builddate = platform.python_build() python_buildno = python_buildno or \\"Unknown\\" python_builddate = python_builddate or \\"Unknown\\" python_compiler = platform.python_compiler() or \\"Unknown\\" report = ( \\"System Information Reportn\\" \\"-------------------------n\\" f\\"System: {system}n\\" f\\"Node: {node}n\\" f\\"Release: {release}n\\" f\\"Version: {version}n\\" f\\"Machine: {machine}n\\" f\\"Processor: {processor}n\\" f\\"Architecture: {architecture_bits}, {architecture_linkage}n\\" f\\"Python Version: {python_version}n\\" f\\"Python Build: {python_buildno}, {python_builddate}n\\" f\\"Python Compiler: {python_compiler}n\\" ) return report"},{"question":"# Asynchronous Chat Server Objective: Implement a simple chat server using the `asyncio` and `socket` modules that supports multiple clients connecting and sending messages to a chat room. Task: 1. Implement an asynchronous chat server that can handle multiple clients. 2. The server should allow clients to connect, send messages, and receive messages from other connected clients in real-time. 3. You must handle client connections and disconnections gracefully. Requirements: 1. Use the `asyncio` module to manage asynchronous I/O. 2. Use the `socket` module to create the server socket. 3. The server should run on `localhost` and a port number of your choice. 4. The server should broadcast messages received from a client to all other connected clients. 5. Ensure thread-safety when broadcasting messages to multiple clients. Input: - There are no direct inputs to the function, as this is a server that will listen for incoming connections and data. Output: - The server should print the received messages and which client sent each message. Constraints: 1. The server should be able to handle at least 5 concurrent client connections. 2. Each message sent by a client should be of reasonable size (e.g., up to 1000 characters). Example: ```python import asyncio import socket async def handle_client(reader, writer): # Function to handle a single client connection pass async def start_chat_server(): server = await asyncio.start_server(handle_client, \'localhost\', 12345) async with server: await server.serve_forever() # Run the chat server asyncio.run(start_chat_server()) ``` # Notes: - You should define the `handle_client` function to manage client communication. - The `start_chat_server` function initializes and runs the server. - You may add additional helper functions as needed to ensure graceful connection handling and message broadcasting.","solution":"import asyncio import socket clients = [] async def broadcast(message, writer): for client_writer in clients: if client_writer != writer: client_writer.write(message) await client_writer.drain() async def handle_client(reader, writer): addr = writer.get_extra_info(\'peername\') print(f\\"Client {addr} connected\\") clients.append(writer) try: while True: data = await reader.read(1000) if not data: break message = f\\"{addr}: {data.decode()}\\" print(message) await broadcast(data, writer) except asyncio.CancelledError: pass except ConnectionResetError: pass finally: print(f\\"Client {addr} disconnected\\") clients.remove(writer) writer.close() await writer.wait_closed() async def start_chat_server(): server = await asyncio.start_server(handle_client, \'localhost\', 12345) async with server: await server.serve_forever() # Run the chat server if __name__ == \'__main__\': try: asyncio.run(start_chat_server()) except KeyboardInterrupt: print(\\"Server shut down\\")"},{"question":"# Email Serialization with `email.generator.BytesGenerator` You are required to write a function that serializes an email message object to a binary file using the `email.generator.BytesGenerator` class from the `email` module. The function should create a MIME email message with text and an attachment, serialize this message, and write it to a file. Function Signature ```python def serialize_email(output_file_path: str) -> None: ``` Requirements: 1. Create an email message object using the `email.message.EmailMessage` class: - Set the email headers: `From`, `To`, `Subject`, `Date`. - Add a plain text body to the email. - Attach a sample binary file (for example, a small image or any binary data). 2. Use the `BytesGenerator` class to serialize the email message and write it to the specified file path `output_file_path`. 3. Ensure that the email message is serialized in a standards-compliant way. Input: - `output_file_path` (str): The file path where the serialized email message should be written. Output: - The function should not return anything. It should write the serialized email message to the specified file path. Constraints: - The function should handle exceptions where the file cannot be written, and log an appropriate error message. Example: Let\'s assume you have the following function call: ```python serialize_email(\'output.eml\') ``` The function should create an email with a text body and a binary attachment (for example, an image), serialize it, and write it to the file `output.eml` in a binary format. Additional Notes: - You can use the `EmailMessage` class methods for adding text and attachments. - Use the `BytesIO` module if necessary for handling binary data in memory before writing it to the file. References: Refer to the provided `email.generator.BytesGenerator` class documentation for more details on the methods and parameters that can be used.","solution":"import os from email.message import EmailMessage from email.generator import BytesGenerator from email.utils import formatdate from io import BytesIO def serialize_email(output_file_path: str) -> None: try: # Create the email message msg = EmailMessage() msg[\'From\'] = \'sender@example.com\' msg[\'To\'] = \'recipient@example.com\' msg[\'Subject\'] = \'Test Email\' msg[\'Date\'] = formatdate(localtime=True) # Add plain text body to the email msg.set_content(\\"This is a test email with an attachment.\\") # Sample binary data (example: small PNG image) sample_binary_data = b\'x89PNGrnx1anx00x00x00rIHDRx00x00x00x10x00x00x00x10x08x06x00x00x00x1fxf3xffxa4x00x00x00x01sRGBx00xaexcex1cxe9x00x00x00x04gAMAx00x00xb1x8frbnxc7x00x00x00x00IENDxaeB`x82\' # Attach the sample binary data to the email msg.add_attachment(sample_binary_data, maintype=\'image\', subtype=\'png\', filename=\'test.png\') # Serialize and write the email to a file with open(output_file_path, \'wb\') as f: BytesGenerator(f).flatten(msg) except Exception as e: print(f\\"Error writing email to file: {e}\\")"},{"question":"Objective: Write a Python function that demonstrates the effects of Python Development Mode by intentionally violating several runtime checks. Your function should perform the following tasks and produce the resulting warnings when run in Python Development Mode. Tasks: 1. Open a file without closing it explicitly, to trigger a `ResourceWarning`. 2. Perform a string encoding operation with an unsupported encoding, to trigger an encoding check. 3. Implement an asyncio coroutine that is not awaited to trigger an asyncio debug warning. Specifications: - **Function Name:** `trigger_warnings` - **Input:** No input parameters - **Output:** Print the warnings/errors encountered when the function is run in Python Development Mode. Constraints: - You should not use `try`/`except` blocks to catch and suppress the warnings. - You should provide a brief explanation in comments where and why each warning is expected to be triggered. Example Code Invocation: ```python if __name__ == \\"__main__\\": trigger_warnings() ``` Note: Run your script with Python Development Mode enabled: ``` python3 -X dev your_script.py ``` Hints: - For the file operation: ```python def file_operation(): with open(\'example.txt\', \'w\') as f: f.write(\'This is an example.\') ``` Modify the above code to open without closing explicitly. - For the string encoding: ```python def encoding_operation(): \'test\'.encode(\'unsupported-encoding\') ``` - For the asyncio operation: ```python import asyncio async def unawaited_coroutine(): return \'Unawaited\' def asyncio_operation(): unawaited_coroutine() ``` Use the above examples within your function to trigger the required warnings.","solution":"import asyncio def trigger_warnings(): # Task 1: Open a file without closing it explicitly open(\'example.txt\', \'w\').write(\'This is an example.\') # This should ideally trigger a ResourceWarning in development mode # Task 2: Perform a string encoding operation with an unsupported encoding try: \'test\'.encode(\'unsupported-encoding\') # This will raise a LookupError except LookupError: pass # Just to avoid crashing the script # Task 3: Implement an asyncio coroutine that is not awaited async def unawaited_coroutine(): return \'Unawaited\' # Simply call the coroutine without awaiting unawaited_coroutine() # Should trigger an asyncio debug warning in development mode if __name__ == \\"__main__\\": trigger_warnings()"},{"question":"# Garbage Collection Management and Debugging **Objective:** Write a Python program that demonstrates an understanding of the \\"gc\\" module by simulating a memory-leaking program, manually managing garbage collection, and tracking unreachable objects. **Task Description:** 1. **Simulate Memory Leakage:** Create a class called `LeakingClass` with an intentional reference cycle to simulate a memory leak. This class should have a method `create_cycle` that creates a reference cycle. ```python class LeakingClass: def __init__(self): self.self_reference = None def create_cycle(self): self.self_reference = self ``` 2. **Manage Garbage Collection:** - Disable automatic garbage collection using `gc.disable()`. - Create multiple instances of `LeakingClass` and invoke `create_cycle` on each instance to simulate memory leakage. - Manually trigger garbage collection for generation 2 using `gc.collect(generation=2)` and print the number of unreachable objects found. 3. **Debugging and Tracking Unreachable Objects:** - Enable debugging flags to collect detailed information about collectable, uncollectable, and saved objects (`gc.DEBUG_COLLECTABLE | gc.DEBUG_UNCOLLECTABLE | gc.DEBUG_SAVEALL`). - After manual garbage collection, print the contents of `gc.garbage` to inspect uncollectable objects. 4. **Memory Optimization:** - Implement a function `optimize_memory()` that: - Freezes the current objects tracked by the garbage collector using `gc.freeze()`. - Displays the number of objects in the permanent generation. - Unfreezes the objects and triggers a full garbage collection. **Output:** Your program should produce the following: - The number of unreachable objects found during manual garbage collection. - Detailed information about uncollectable objects. - The number of objects in the permanent generation before and after unfreezing. **Constraints:** - Use the `gc` module functions appropriately as described. - Ensure the program handles memory-intensive operations effectively without crashing. **Example output:** ```plaintext Unreachable objects found: 10 Uncollectable objects in gc.garbage: [<__main__.LeakingClass object at 0x000001F87B16A9D0>, ...] Number of objects in the permanent generation: 100 Number of objects after unfreezing: 0 ``` Ensure your implementation handles edge cases and provides clear and readable output. The program should demonstrate a deep understanding of garbage collection management and debugging in Python.","solution":"import gc class LeakingClass: def __init__(self): self.self_reference = None def create_cycle(self): self.self_reference = self def simulate_memory_leakage(num_instances=10): gc.disable() instances = [LeakingClass() for _ in range(num_instances)] for instance in instances: instance.create_cycle() unreachable_objects_before = gc.collect(generation=2) print(f\\"Unreachable objects found before enabling debug: {unreachable_objects_before}\\") gc.set_debug(gc.DEBUG_COLLECTABLE | gc.DEBUG_UNCOLLECTABLE | gc.DEBUG_SAVEALL) unreachable_objects_after = gc.collect(generation=2) print(f\\"Unreachable objects found after enabling debug: {unreachable_objects_after}\\") print(\\"Uncollectable objects in gc.garbage:\\") print(gc.garbage) def optimize_memory(): gc.freeze() num_permanent_objects = len(gc.get_objects()) print(f\\"Number of objects in the permanent generation: {num_permanent_objects}\\") gc.unfreeze() gc.collect() print(\\"Freed objects after unfreezing and collecting:\\") # To run the simulation: simulate_memory_leakage() optimize_memory()"},{"question":"Introduction In this assessment, you will demonstrate your understanding of `sklearn.metrics.pairwise` submodule by implementing functions to calculate various distance metrics and kernel similarities between sets of samples. Specifically, you will implement custom functions to calculate the Manhattan distance, cosine similarity, and RBF kernel. After implementing these functions, you will use them to evaluate pairwise distances and similarities on given datasets. Objectives 1. Implement custom functions to calculate Manhattan distance, cosine similarity, and RBF kernel. 2. Use these functions to evaluate pairwise distances and similarities for given datasets. Task 1. **Implement the following functions:** - `manhattan_distance(X, Y)`: This function should take two datasets `X` and `Y` (each being a 2D NumPy array) and return their pairwise Manhattan distances. - `cosine_similarity_custom(X, Y)`: This function should take two datasets `X` and `Y` (each being a 2D NumPy array) and return their pairwise cosine similarities. - `rbf_kernel_custom(X, Y, gamma)`: This function should take two datasets `X` and `Y` (each being a 2D NumPy array) and a float `gamma`, and return their RBF kernel similarities. 2. **Evaluate the functions on the given datasets:** - Dataset `X`: `np.array([[2, 3], [3, 5], [5, 8]])` - Dataset `Y`: `np.array([[1, 0], [2, 1]])` - Gamma value: `gamma = 0.5` 3. **Expected Output:** - The output for each function should be the pairwise distances or similarities in the form of a 2D NumPy array. - Compare your custom implementation with the outputs of `pairwise_distances` and `pairwise_kernels` from `sklearn.metrics.pairwise`. Input - Two 2D NumPy arrays `X` and `Y` representing datasets. - A float `gamma` for the RBF kernel function. Output - A 2D NumPy array representing the pairwise distances or similarities. Constraints - You may assume that the input arrays `X` and `Y` have the same number of columns (features). - Performance should be considered for large datasets, though specifics are not provided. Function Signatures ```python import numpy as np def manhattan_distance(X: np.ndarray, Y: np.ndarray) -> np.ndarray: # Implement the function here pass def cosine_similarity_custom(X: np.ndarray, Y: np.ndarray) -> np.ndarray: # Implement the function here pass def rbf_kernel_custom(X: np.ndarray, Y: np.ndarray, gamma: float) -> np.ndarray: # Implement the function here pass # Example usage and comparison with sklearn\'s functions if __name__ == \\"__main__\\": from sklearn.metrics import pairwise_distances from sklearn.metrics.pairwise import pairwise_kernels X = np.array([[2, 3], [3, 5], [5, 8]]) Y = np.array([[1, 0], [2, 1]]) gamma = 0.5 # Evaluate custom functions custom_manhattan = manhattan_distance(X, Y) custom_cosine = cosine_similarity_custom(X, Y) custom_rbf = rbf_kernel_custom(X, Y, gamma) # Output results print(\\"Custom Manhattan distances:n\\", custom_manhattan) print(\\"Custom Cosine similarities:n\\", custom_cosine) print(\\"Custom RBF kernel similarities:n\\", custom_rbf) # Compare with sklearn\'s functions sklearn_manhattan = pairwise_distances(X, Y, metric=\'manhattan\') sklearn_cosine = pairwise_kernels(X, Y, metric=\'cosine\') sklearn_rbf = pairwise_kernels(X, Y, metric=\'rbf\', gamma=gamma) print(\\"Sklearn Manhattan distances:n\\", sklearn_manhattan) print(\\"Sklearn Cosine similarities:n\\", sklearn_cosine) print(\\"Sklearn RBF kernel similarities:n\\", sklearn_rbf) ``` Explanation The students are required to implement three core functions to calculate pairwise distances and similarities using Manhattan distance, cosine similarity, and RBF kernel. They are also expected to compare their results with sklearn\'s provided functions to ensure the correctness of their implementations. This question tests the understanding of basic and advanced functionality within the `sklearn.metrics.pairwise` submodule.","solution":"import numpy as np def manhattan_distance(X: np.ndarray, Y: np.ndarray) -> np.ndarray: Calculate the pairwise Manhattan distance between two datasets X and Y. return np.abs(X[:, np.newaxis] - Y).sum(axis=2) def cosine_similarity_custom(X: np.ndarray, Y: np.ndarray) -> np.ndarray: Calculate the pairwise cosine similarity between two datasets X and Y. X_norm = np.linalg.norm(X, axis=1, keepdims=True) Y_norm = np.linalg.norm(Y, axis=1, keepdims=True) return (X @ Y.T) / (X_norm * Y_norm.T) def rbf_kernel_custom(X: np.ndarray, Y: np.ndarray, gamma: float) -> np.ndarray: Calculate the pairwise RBF kernel between two datasets X and Y. X_norm = np.sum(X**2, axis=1) Y_norm = np.sum(Y**2, axis=1) K = -2 * X @ Y.T + X_norm[:, np.newaxis] + Y_norm return np.exp(-gamma * K) # Example usage and comparison with sklearn\'s functions if __name__ == \\"__main__\\": from sklearn.metrics import pairwise_distances from sklearn.metrics.pairwise import pairwise_kernels X = np.array([[2, 3], [3, 5], [5, 8]]) Y = np.array([[1, 0], [2, 1]]) gamma = 0.5 # Evaluate custom functions custom_manhattan = manhattan_distance(X, Y) custom_cosine = cosine_similarity_custom(X, Y) custom_rbf = rbf_kernel_custom(X, Y, gamma) # Output results print(\\"Custom Manhattan distances:n\\", custom_manhattan) print(\\"Custom Cosine similarities:n\\", custom_cosine) print(\\"Custom RBF kernel similarities:n\\", custom_rbf) # Compare with sklearn\'s functions sklearn_manhattan = pairwise_distances(X, Y, metric=\'manhattan\') sklearn_cosine = pairwise_kernels(X, Y, metric=\'cosine\') sklearn_rbf = pairwise_kernels(X, Y, metric=\'rbf\', gamma=gamma) print(\\"Sklearn Manhattan distances:n\\", sklearn_manhattan) print(\\"Sklearn Cosine similarities:n\\", sklearn_cosine) print(\\"Sklearn RBF kernel similarities:n\\", sklearn_rbf)"},{"question":"**Coding Assessment Question: Input Validation for Machine Learning Data using Scikit-learn Utilities** **Objective**: In this task, you are required to implement a function `validate_ml_data` that validates input data for a machine learning model using scikit-learn\'s utility functions. This will help ensure that the input matrices are consistent, finite, and of appropriate dimensions. **Function Signature**: ```python def validate_ml_data(X, y, allowed_sparse_formats=None, multi_output=False): Validate the input data for a machine learning task. Parameters: - X: array-like, shape (n_samples, n_features) The input data matrix. - y: array-like, shape (n_samples,) or (n_samples, n_outputs) The target values. - allowed_sparse_formats: list of str, default=None Allowed sparse matrix formats. If None, defaults to [\\"csr\\", \\"csc\\", \\"coo\\"]. - multi_output: bool, default=False If True, allows y to be a 2D array for multilabel classification or multitarget regression. Returns: - valid_X: array-like, shape (n_samples, n_features) The validated input data matrix. - valid_y: array-like, shape (n_samples,) or (n_samples, n_outputs) The validated target values. Raises: - ValueError: If any of the validation checks fail. pass ``` **Requirements**: 1. The function should ensure that `X` is a 2D array. 2. The function should check that `X` and `y` have consistent lengths. 3. The function should ensure that there are no NaNs or infinite values in `X`. 4. If `allowed_sparse_formats` is provided, check that `X` conforms to one of these formats. 5. If `multi_output` is True, allow `y` to be a 2D array; otherwise, ensure `y` is 1D. **Usage of specific scikit-learn validation functions is expected**: - `check_X_y` - `assert_all_finite` - `check_array` **Example**: ```python import numpy as np # Valid input example X = np.array([[1, 2], [3, 4], [5, 6]]) y = np.array([1, 2, 3]) valid_X, valid_y = validate_ml_data(X, y) print(valid_X) print(valid_y) # Invalid input example (inconsistent lengths) X = np.array([[1, 2], [3, 4]]) y = np.array([1, 2, 3]) try: valid_X, valid_y = validate_ml_data(X, y) except ValueError as e: print(e) ``` Ensure your solution passes the provided example and adheres to the requirements. The goal is to correctly implement the validation process, leveraging the appropriate scikit-learn utility functions.","solution":"import numpy as np from sklearn.utils import check_X_y, check_array, assert_all_finite def validate_ml_data(X, y, allowed_sparse_formats=None, multi_output=False): Validate the input data for a machine learning task. Parameters: - X: array-like, shape (n_samples, n_features) The input data matrix. - y: array-like, shape (n_samples,) or (n_samples, n_outputs) The target values. - allowed_sparse_formats: list of str, default=None Allowed sparse matrix formats. If None, defaults to [\\"csr\\", \\"csc\\", \\"coo\\"]. - multi_output: bool, default=False If True, allows y to be a 2D array for multilabel classification or multitarget regression. Returns: - valid_X: array-like, shape (n_samples, n_features) The validated input data matrix. - valid_y: array-like, shape (n_samples,) or (n_samples, n_outputs) The validated target values. Raises: - ValueError: If any of the validation checks fail. if allowed_sparse_formats is None: allowed_sparse_formats = [\\"csr\\", \\"csc\\", \\"coo\\"] valid_X, valid_y = check_X_y( X, y, accept_sparse=allowed_sparse_formats, multi_output=multi_output ) assert_all_finite(valid_X) assert_all_finite(valid_y) return valid_X, valid_y"},{"question":"**Question: Working with Serialized Data using the `marshal` module** **Objective:** Implement functions to serialize and deserialize nested Python objects using the `marshal` module, while handling possible errors gracefully. # Task: You need to write two functions: 1. `serialize_data(data)`: This function will take a Python object and serialize it into a bytes object using the `marshal` module. It should handle unsupported types by raising an appropriate error message without writing garbage data. 2. `deserialize_data(serialized_data)`: This function will take a bytes object, deserialize it back into a Python object using the `marshal` module, and return the original data structure. If the data contains an unsupported type, it should raise an appropriate error message. # Function Signatures: ```python def serialize_data(data: any) -> bytes: pass def deserialize_data(serialized_data: bytes) -> any: pass ``` # Constraints: - Only the types supported by `marshal` (mentioned in the provided documentation) should be handled. - For `serialize_data`, if `data` contains an unsupported type, raise a `ValueError` with message \\"Unsupported data type for serialization\\". - For `deserialize_data`, if `serialized_data` contains invalid or unsupported data, catch the possible exceptions `EOFError`, `ValueError`, and `TypeError` and raise a `ValueError` with message \\"Invalid or corrupted data\\". # Example: ```python data = { \\"name\\": \\"Alice\\", \\"age\\": 30, \\"scores\\": [85.5, 90.3, 88.0], \\"graduated\\": True, \\"projects\\": [ {\\"name\\": \\"Project1\\", \\"year\\": 2020}, {\\"name\\": \\"Project2\\", \\"year\\": 2021} ], \\"metadata\\": None } # Serialize the data serialized_data = serialize_data(data) # Deserialize the serialized data deserialized_data = deserialize_data(serialized_data) print(deserialized_data) # Output should be the original `data` object # Edge case: unsupported types class CustomObject: pass data_with_unsupported_type = { \\"name\\": \\"Bob\\", \\"custom\\": CustomObject() } # Attempt to serialize should raise ValueError # serialize_data(data_with_unsupported_type) # Output: ValueError: Unsupported data type for serialization ``` Notes: - Ensure that your implementation of `serialize_data` and `deserialize_data` addresses the error handling as specified. - Consider edge cases where the data structure may be nested or contain unsupported types for serialization.","solution":"import marshal def serialize_data(data): Serializes a Python object into bytes using the marshal module. Only the types supported by marshal can be serialized. try: return marshal.dumps(data) except ValueError: raise ValueError(\\"Unsupported data type for serialization\\") def deserialize_data(serialized_data): Deserializes a bytes object back into a Python object using the marshal module. If the data contains invalid or unsupported data, raises a ValueError with the message \\"Invalid or corrupted data\\". try: return marshal.loads(serialized_data) except (EOFError, ValueError, TypeError): raise ValueError(\\"Invalid or corrupted data\\")"},{"question":"# Question: Implement a Custom Exception Logger Context Using the `traceback` module, your task is to implement a custom exception logging class in Python. This class, `CustomExceptionLogger`, should capture, format, and log exceptions whenever they occur in a function. The logging should include the full traceback and relevant exception details. Requirements 1. **Class Definition:** Define a class `CustomExceptionLogger`. 2. **Class Method:** Implement a class method `log_exception` which: - Takes a function as its argument and executes it. - Captures any exceptions raised during the execution of the function. - Logs the full exception details, including the traceback, to a specified log file `error_log.txt`. If the log file already exists, append the new logs to it. 3. **Exception Details:** Ensure that the following details are logged: - A header indicating the start of a new log entry. - The exception type and value. - The full traceback. - If the exception is a `SyntaxError`, include the specific line and caret indicating the error. - Local variables from each frame in the traceback. Input and Output - **Input:** A function that may raise exceptions during its execution. - **Output:** No direct output. Exception details are logged to `error_log.txt`. Constraints - You should handle any type of exception. - Ensure that logs are appended to the file if it already exists. - Implement proper file handling to ensure the log file is correctly written and closed. Example Usage ```python def test_function(): # This function raises an exception for testing purposes raise ValueError(\\"An example error\\") logger = CustomExceptionLogger() logger.log_exception(test_function) ``` This would create or append to the log file `error_log.txt` with details about the `ValueError` raised in `test_function`. # Implementation Hints - Use the `sys.exc_info()` function to capture the exception details. - Utilize `traceback.format_exception` to format the traceback and exception details into a string. - Use file operations (`with open(...) as ...:`) to manage the log file. Bonus - Enhance the class to handle nested exceptions and chain them in the logs. - Add a timestamp to each log entry to indicate when the exception occurred.","solution":"import traceback import sys from datetime import datetime class CustomExceptionLogger: @staticmethod def log_exception(func): try: func() except Exception as e: with open(\\"error_log.txt\\", \\"a\\") as log_file: log_file.write(\\"n\\" + \\"=\\"*40 + \\"n\\") log_file.write(f\\"Exception occurred at {datetime.now()}n\\") exc_type, exc_value, exc_traceback = sys.exc_info() log_file.write(f\\"Exception type: {exc_type.__name__}n\\") log_file.write(f\\"Exception message: {exc_value}n\\") log_file.write(\\"Traceback:n\\") traceback_details = \'\'.join(traceback.format_exception(exc_type, exc_value, exc_traceback)) log_file.write(traceback_details) if isinstance(e, SyntaxError): log_file.write(f\\"Line number: {e.lineno}n\\") log_file.write(f\\"Offset: {e.offset}n\\") log_file.write(f\\"Text: {e.text}n\\") log_file.write(\\"=\\"*40 + \\"n\\")"},{"question":"# WAV File Statistics In this assessment, you are required to write a Python function that processes a WAV file and extracts detailed statistics from it. # Task Details Write a function `wav_file_statistics(file_path: str) -> dict` that accepts the path to a WAV file and returns a dictionary with the following keys and their corresponding values: 1. **nchannels**: Number of audio channels. 2. **sampwidth**: Sample width in bytes. 3. **framerate**: Sampling frequency (frame rate). 4. **nframes**: Total number of frames in the audio file. 5. **duration**: Duration of the audio file in seconds, calculated as `nframes / framerate`. 6. **comptype**: Compression type of the audio. 7. **compname**: Human-readable version of the compression type. # Input Format - The function will take a single argument `file_path` which is a string representing the path to a WAV file. # Output Format - A dictionary with the specified keys and their corresponding values describing the WAV file. # Constraints - The WAV file will always use the \\"WAVE_FORMAT_PCM\\" format. - Assume the WAV file is always readable and accessible. # Example ```python def wav_file_statistics(file_path: str) -> dict: pass ``` Given a WAV file at `/path/to/file.wav`, calling the function like this: ```python stats = wav_file_statistics(\\"/path/to/file.wav\\") ``` Should return something similar to: ```python { \\"nchannels\\": 2, \\"sampwidth\\": 2, \\"framerate\\": 44100, \\"nframes\\": 88200, \\"duration\\": 2.0, \\"comptype\\": \\"NONE\\", \\"compname\\": \\"not compressed\\" } ``` # Notes - Make sure your function handles file operations safely, closing the file after use. - Handle any exceptions that may arise from file handling and provide meaningful error messages. - Implement the function using the `wave` module methods as documented.","solution":"import wave def wav_file_statistics(file_path: str) -> dict: Extracts statistics from a WAV file. Args: file_path: Path to the WAV file. Returns: A dictionary containing statistics of the WAV file. try: with wave.open(file_path, \'rb\') as wav_file: params = wav_file.getparams() nchannels = params.nchannels sampwidth = params.sampwidth framerate = params.framerate nframes = params.nframes comptype = params.comptype compname = params.compname duration = nframes / framerate return { \\"nchannels\\": nchannels, \\"sampwidth\\": sampwidth, \\"framerate\\": framerate, \\"nframes\\": nframes, \\"duration\\": duration, \\"comptype\\": comptype, \\"compname\\": compname } except wave.Error as e: return {\\"error\\": str(e)} except FileNotFoundError: return {\\"error\\": \\"File not found\\"} except Exception as e: return {\\"error\\": str(e)}"},{"question":"Background You are working on an event management system that needs to schedule and manage events efficiently. Each event has a specific start date and time, and you need to ensure that there is no overlap between events. Additionally, you need to provide the capability to list events in chronological order, as well as search for events by date. Task Implement a Python class `EventManager` that manages events using Python\'s `datetime` and `collections` modules. Your class should support the following functionalities: 1. **Add Event:** * Method: `add_event(event_name: str, start_time: datetime, duration: timedelta) -> bool` * Description: Adds an event with the given name, start time, and duration. If the event overlaps with an existing event, it should not be added, and the method should return `False`. If the event is successfully added, the method should return `True`. 2. **List Events:** * Method: `list_events() -> List[Dict[str, Union[str, datetime, timedelta]]]` * Description: Returns a list of events sorted by start time. Each event should be represented as a dictionary with keys `name`, `start_time`, and `duration`. 3. **Search Events by Date:** * Method: `search_events_by_date(search_date: date) -> List[Dict[str, Union[str, datetime, timedelta]]]` * Description: Returns a list of events occurring on the given date. Each event should be represented as a dictionary with keys `name`, `start_time`, and `duration`. Requirements 1. The class should handle at least 1000 events efficiently. 2. The `add_event` method must ensure that new events do not overlap with existing events. 3. The `list_events` and `search_events_by_date` methods must use efficient data structures to ensure quick retrieval and searching. Constraints * Use Python\'s `datetime` module for date and time manipulation. * Use appropriate data structures from Python\'s `collections` module to manage the events. * Write clean and readable code, and include docstrings for your methods. Example Usage ```python from datetime import datetime, timedelta, date # Create an instance of EventManager manager = EventManager() # Add events manager.add_event(\\"Meeting\\", datetime(2023, 10, 12, 14, 0), timedelta(hours=2)) manager.add_event(\\"Conference\\", datetime(2023, 10, 12, 17, 0), timedelta(hours=3)) # List events events = manager.list_events() print(events) # Output: [{\'name\': \'Meeting\', \'start_time\': datetime(2023, 10, 12, 14, 0), \'duration\': timedelta(hours=2)}, # {\'name\': \'Conference\', \'start_time\': datetime(2023, 10, 12, 17, 0), \'duration\': timedelta(hours=3)}] # Search events by date searched_events = manager.search_events_by_date(date(2023, 10, 12)) print(searched_events) # Output: [{\'name\': \'Meeting\', \'start_time\': datetime(2023, 10, 12, 14, 0), \'duration\': timedelta(hours=2)}, # {\'name\': \'Conference\', \'start_time\': datetime(2023, 10, 12, 17, 0), \'duration\': timedelta(hours=3)}] ```","solution":"from datetime import datetime, timedelta, date from collections import defaultdict from typing import List, Dict, Union class EventManager: def __init__(self): # Store events in a defaultdict with the date as the key self.events = defaultdict(list) def add_event(self, event_name: str, start_time: datetime, duration: timedelta) -> bool: end_time = start_time + duration event_date = start_time.date() # Check for overlap with existing events for event in self.events[event_date]: existing_start = event[\'start_time\'] existing_end = existing_start + event[\'duration\'] if not (end_time <= existing_start or start_time >= existing_end): return False # Add the event if it does not overlap self.events[event_date].append({ \'name\': event_name, \'start_time\': start_time, \'duration\': duration }) return True def list_events(self) -> List[Dict[str, Union[str, datetime, timedelta]]]: all_events = [] for events in self.events.values(): all_events.extend(events) # Sort events by start time return sorted(all_events, key=lambda x: x[\'start_time\']) def search_events_by_date(self, search_date: date) -> List[Dict[str, Union[str, datetime, timedelta]]]: return self.events[search_date]"},{"question":"You are tasked with writing a Python function that reads a WAV file, extracts its properties, and optionally allows modifications to certain properties before saving it as a new WAV file. The function should demonstrate the usage of the `wave` module effectively. # Function Signature ```python def process_wav(input_file: str, output_file: str, new_sample_width: int = None, new_frame_rate: int = None) -> dict: pass ``` # Description You need to implement the `process_wav` function following these requirements: 1. **Read the Input WAV File**: - Open and read the properties of the given input WAV file (`input_file`). - Extract and return the following properties in a dictionary: - `num_channels`: Number of audio channels. - `sample_width`: Sample width in bytes. - `frame_rate`: Sampling frequency. - `num_frames`: Number of audio frames. - `compression_type`: Compression type. - `compression_name`: Human-readable version of the compression type. 2. **Modify and Save a New WAV File**: - If `new_sample_width` is provided, change the sample width. - If `new_frame_rate` is provided, change the frame rate. - Save the modified audio to `output_file`. # Input - `input_file` (str): The path to the input WAV file. - `output_file` (str): The path to save the modified WAV file. - `new_sample_width` (int, optional): The new sample width in bytes if modification is desired. - `new_frame_rate` (int, optional): The new frame rate if modification is desired. # Output - A dictionary containing the properties of the input WAV file: - `num_channels` (int): Number of audio channels. - `sample_width` (int): Sample width in bytes. - `frame_rate` (int): Sampling frequency. - `num_frames` (int): Number of audio frames. - `compression_type` (str): Compression type. - `compression_name` (str): Human-readable version of the compression type. # Constraints - The input WAV file must use `WAVE_FORMAT_PCM`. - The function should handle file read/write operations using the `wave` module. - Proper error handling should be implemented to manage invalid file formats or read/write errors. # Example ```python # Provided Example input_file = \\"input.wav\\" output_file = \\"output.wav\\" result = process_wav(input_file, output_file, new_sample_width=2, new_frame_rate=48000) # Result might look like: # { # \\"num_channels\\": 2, # \\"sample_width\\": 2, # \\"frame_rate\\": 44100, # \\"num_frames\\": 123456, # \\"compression_type\\": \\"NONE\\", # \\"compression_name\\": \\"not compressed\\" # } ``` # Notes - Pay attention to the file handling (`with` statement). - Ensure that modifications (if any) are correctly applied to the output WAV file. - Test the function with different input WAV files and modification parameters to verify correctness.","solution":"import wave import contextlib def process_wav(input_file: str, output_file: str, new_sample_width: int = None, new_frame_rate: int = None) -> dict: Process a WAV file to extract its properties and optionally modify its sample width and/or frame rate. # Read the input WAV file with contextlib.closing(wave.open(input_file, \'rb\')) as wf: params = wf.getparams() num_channels = wf.getnchannels() sample_width = wf.getsampwidth() frame_rate = wf.getframerate() num_frames = wf.getnframes() compression_type = wf.getcomptype() compression_name = wf.getcompname() # Extract sample frames frames = wf.readframes(num_frames) properties = { \\"num_channels\\": num_channels, \\"sample_width\\": sample_width, \\"frame_rate\\": frame_rate, \\"num_frames\\": num_frames, \\"compression_type\\": compression_type, \\"compression_name\\": compression_name } # Modify properties if specified if new_sample_width is not None: sample_width = new_sample_width if new_frame_rate is not None: frame_rate = new_frame_rate # Write the new WAV file with contextlib.closing(wave.open(output_file, \'wb\')) as wf: wf.setnchannels(num_channels) wf.setsampwidth(sample_width) wf.setframerate(frame_rate) wf.setnframes(num_frames) wf.setcomptype(compression_type, compression_name) wf.writeframes(frames) return properties"},{"question":"**Question: Parallel Computation in scikit-learn** You are required to showcase your comprehension of parallel computation and resource management in `scikit-learn`. Write a Python function that performs the following steps: 1. Train a `RandomForestClassifier` model on the provided dataset using parallel computation. 2. Implement grid search cross-validation using `GridSearchCV` to optimize the following hyperparameters for the `RandomForestClassifier`: - `n_estimators`: [100, 200] - `max_depth`: [10, 20] - `min_samples_split`: [2, 5] 3. Configure the number of workers (threads) for the `GridSearchCV` and `RandomForestClassifier` to avoid oversubscription, assuming the machine has 4 CPUs. 4. Manage resources using environment variables to ensure efficient memory and processing usage. **Function Signature:** ```python def parallel_computation_with_sklearn(X_train, y_train): Trains a RandomForestClassifier using GridSearchCV with parallel computation and resource management. Args: - X_train (ndarray): Feature data for training. - y_train (ndarray): Labels for training. Returns: - best_params_ (dict): The best hyperparameters found by GridSearchCV. pass ``` **Input and Output Format:** - Input: `X_train` is a 2D NumPy array of shape (n_samples, n_features). `y_train` is a 1D array of shape (n_samples,). - Output: The function should return a dictionary containing the best hyperparameters found by `GridSearchCV`. **Constraints:** - You should use the `joblib` backend for parallel processing. - Configure the `n_jobs` parameters to optimize the computation within the constraint of 4 CPUs. - Handle memory efficiently ensuring `memmap` is used for large datasets. **Example Usage:** ```python import numpy as np from sklearn.datasets import make_classification X, y = make_classification(n_samples=1000, n_features=20, random_state=42) best_params = parallel_computation_with_sklearn(X, y) print(best_params) ``` Notes: 1. Ensure that the number of threads spawned is controlled to avoid oversubscription. 2. The configuration should be tested in practice to validate efficient performance improvement. Good luck!","solution":"import os from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import GridSearchCV def parallel_computation_with_sklearn(X_train, y_train): Trains a RandomForestClassifier using GridSearchCV with parallel computation and resource management. Args: - X_train (ndarray): Feature data for training. - y_train (ndarray): Labels for training. Returns: - best_params_ (dict): The best hyperparameters found by GridSearchCV. # Setting environment variable to optimize resource usage os.environ[\\"JOBLIB_TEMP_FOLDER\\"] = \\"/tmp/joblib\\" # Configuring the number of jobs for RandomForestClassifier and GridSearchCV n_jobs = 4 # Given the machine has 4 CPUs # Setting up the RandomForestClassifier with parallel computation enabled rf = RandomForestClassifier(n_jobs=n_jobs) # Defining the hyperparameters grid param_grid = { \'n_estimators\': [100, 200], \'max_depth\': [10, 20], \'min_samples_split\': [2, 5] } # Setting up GridSearchCV with parallel computation enabled grid_search = GridSearchCV(rf, param_grid, cv=3, n_jobs=n_jobs, verbose=1) # Fitting GridSearchCV grid_search.fit(X_train, y_train) # Returning the best hyperparameters return grid_search.best_params_"}]'),R={name:"App",components:{PoemCard:I},data(){return{searchQuery:"",visibleCount:4,poemsData:A,isLoading:!1}},computed:{filteredPoems(){const i=this.searchQuery.trim().toLowerCase();return i?this.poemsData.filter(e=>e.question&&e.question.toLowerCase().includes(i)||e.solution&&e.solution.toLowerCase().includes(i)):this.poemsData},displayedPoems(){return this.searchQuery.trim()?this.filteredPoems:this.filteredPoems.slice(0,this.visibleCount)},hasMorePoems(){return!this.searchQuery.trim()&&this.visibleCount<this.poemsData.length}},methods:{async loadMore(){this.isLoading=!0,await new Promise(i=>setTimeout(i,1e3)),this.visibleCount+=4,this.isLoading=!1}}},F={class:"search-container"},D={class:"card-container"},q={key:0,class:"empty-state"},z=["disabled"],O={key:0},N={key:1};function M(i,e,l,m,s,r){const h=_("PoemCard");return a(),n("section",null,[e[4]||(e[4]=t("div",{class:"top-banner"},[t("div",{class:"top-banner-title"},[t("div",{class:"top-banner-title-text"},"🤔prompts chat🧠")])],-1)),t("div",F,[e[3]||(e[3]=t("span",{class:"search-icon"},"🔍",-1)),g(t("input",{type:"text",class:"search-input","onUpdate:modelValue":e[0]||(e[0]=o=>s.searchQuery=o),placeholder:"Search..."},null,512),[[y,s.searchQuery]]),s.searchQuery?(a(),n("button",{key:0,class:"clear-search",onClick:e[1]||(e[1]=o=>s.searchQuery="")}," ✕ ")):d("",!0)]),t("div",D,[(a(!0),n(b,null,v(r.displayedPoems,(o,f)=>(a(),w(h,{key:f,poem:o},null,8,["poem"]))),128)),r.displayedPoems.length===0?(a(),n("div",q,' No results found for "'+c(s.searchQuery)+'". ',1)):d("",!0)]),r.hasMorePoems?(a(),n("button",{key:0,class:"load-more-button",disabled:s.isLoading,onClick:e[2]||(e[2]=(...o)=>r.loadMore&&r.loadMore(...o))},[s.isLoading?(a(),n("span",N,"Loading...")):(a(),n("span",O,"See more"))],8,z)):d("",!0)])}const L=p(R,[["render",M],["__scopeId","data-v-ec330c01"]]),Y=JSON.parse('{"title":"","description":"","frontmatter":{"page":true},"headers":[],"relativePath":"deepseek/58.md","filePath":"deepseek/58.md"}'),j={name:"deepseek/58.md"},H=Object.assign(j,{setup(i){return(e,l)=>(a(),n("div",null,[x(L)]))}});export{Y as __pageData,H as default};
