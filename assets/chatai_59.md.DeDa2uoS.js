import{_ as p,o as a,c as s,a as t,m as u,t as c,C as _,M as g,U as y,f as d,F as b,p as v,e as w,q as x}from"./chunks/framework.B1z0IdBH.js";const k={name:"PoemCard",props:{poem:{type:Object,required:!0}}},T={class:"poem-container"},C={class:"review"},S={class:"review-title"},P={class:"review-content"};function E(i,e,l,m,n,o){return a(),s("div",T,[t("div",C,[t("div",S,[e[0]||(e[0]=t("span",{class:"icon"},"question:",-1)),u(c(l.poem.question),1)]),e[2]||(e[2]=t("div",{class:"divider"},null,-1)),t("p",P,[e[1]||(e[1]=t("span",{class:"icon"},"answer:",-1)),u(c(l.poem.solution),1)])])])}const I=p(k,[["render",E],["__scopeId","data-v-ad35afcc"]]),D=JSON.parse('[{"question":"# Command Line Parser for a Mock File Processing Script Introduction You are to implement a command line parsing function for a mock file processing script using the `getopt` module. The script should accept various options and arguments as described below and handle them accordingly. Specifications 1. **Function Signature:** ```python def parse_arguments(argv: list) -> dict: ``` 2. **Input:** - `argv`: A list of command line arguments (emulating `sys.argv[1:]`), each element being a string. 3. **Output:** - A dictionary with parsed options and arguments. The dictionary should have the following structure: ```python { \\"help\\": bool, \\"verbose\\": bool, \\"output\\": str or None, \\"files\\": list of str } ``` 4. **Options:** - `-h` or `--help`: Display a help message and exit. If this option is found, populate the dictionary with `help: True` and terminate parsing. - `-v` or `--verbose`: Set verbosity. If this option is found, set `verbose: True`. - `-o <file>` or `--output=<file>`: Specify output file. Set `output: <file>` where `<file>` is the provided file name. - Remaining arguments are considered as files and should populate the `files` list in the output dictionary. 5. **Constraints:** - If an unknown option or a missing required argument is encountered, raise a `ValueError` with the message from the `getopt.GetoptError` exception. - Ensure the `output` is `None` if the output file is not specified. 6. **Example:** Input: ```python argv = [\\"-v\\", \\"--output\\", \\"result.txt\\", \\"file1\\", \\"file2\\"] ``` Output: ```python { \\"help\\": False, \\"verbose\\": True, \\"output\\": \\"result.txt\\", \\"files\\": [\\"file1\\", \\"file2\\"] } ``` Input: ```python argv = [\\"-h\\"] ``` Output: ```python { \\"help\\": True, \\"verbose\\": False, \\"output\\": None, \\"files\\": [] } ``` Implementation Notes: - Use the `getopt.getopt` function to parse the `argv`. - Handle and propagate `getopt.GetoptError` exceptions appropriately. Starter Code: ```python import getopt def parse_arguments(argv: list) -> dict: options = { \\"help\\": False, \\"verbose\\": False, \\"output\\": None, \\"files\\": [] } try: opts, files = getopt.getopt(argv, \\"hvo:\\", [\\"help\\", \\"verbose\\", \\"output=\\"]) for opt, arg in opts: if opt in (\\"-h\\", \\"--help\\"): options[\\"help\\"] = True return options elif opt in (\\"-v\\", \\"--verbose\\"): options[\\"verbose\\"] = True elif opt in (\\"-o\\", \\"--output\\"): options[\\"output\\"] = arg options[\\"files\\"] = files except getopt.GetoptError as err: raise ValueError(str(err)) return options ``` Implement the function in the starter code template and test it against the provided examples.","solution":"import getopt def parse_arguments(argv: list) -> dict: options = { \\"help\\": False, \\"verbose\\": False, \\"output\\": None, \\"files\\": [] } try: opts, files = getopt.getopt(argv, \\"hvo:\\", [\\"help\\", \\"verbose\\", \\"output=\\"]) for opt, arg in opts: if opt in (\\"-h\\", \\"--help\\"): options[\\"help\\"] = True return options elif opt in (\\"-v\\", \\"--verbose\\"): options[\\"verbose\\"] = True elif opt in (\\"-o\\", \\"--output\\"): options[\\"output\\"] = arg options[\\"files\\"] = files except getopt.GetoptError as err: raise ValueError(str(err)) return options"},{"question":"# Task: Memory Usage Analysis using `tracemalloc` You are provided with a Python script that has potential memory inefficiencies. Your task is to write a function, `analyze_memory_usage`, to analyze the memory usage of this script using the `tracemalloc` module and identify memory leaks or inefficient memory usage. Below is the given script: ```python def inefficient_function(): large_list = [] for i in range(100000): large_list.append(i) return sum(large_list) def another_inefficient_function(): small_list = [i for i in range(1000)] return sum(small_list) ``` Function to Implement ```python def analyze_memory_usage(): # Your code goes here ``` The `analyze_memory_usage` function should: 1. **Start the memory tracing** with a limit of 25 frames. 2. **Run** the provided script to ensure memory allocation takes place. 3. **Take two snapshots**: `snapshot1` before running any functions and `snapshot2` after running both `inefficient_function` and `another_inefficient_function`. 4. **Compare** the two snapshots and find the top 5 sources of memory allocations. 5. **Filter out** any allocations not from the script file. 6. **Print** the results in a human-readable format. Example Output: ```text Top 5 allocations: 1. script.py:3: 1000 KiB large_list.append(i) 2. script.py:9: 200 KiB small_list = [i for i in range(1000)] ... ``` Notes: - Use `tracemalloc` and ensure proper start and stop calls. - Consider using filtering to precisely narrow down the memory allocation to specific lines in the script. - Ensure the output is sorted by the size of the allocations. - Validate that your function works correctly with other scripts that you might create for testing purposes. Good luck, and make sure your solution is efficient and clear!","solution":"import tracemalloc def analyze_memory_usage(script): # Start tracing memory allocations tracemalloc.start(25) snapshot1 = tracemalloc.take_snapshot() # Run the script exec(script) snapshot2 = tracemalloc.take_snapshot() # Stop tracing to free memory tracemalloc.stop() # Get the difference between both snapshots top_stats = snapshot2.compare_to(snapshot1, \'lineno\') print(\\"Top 5 memory allocations:\\") for stat in top_stats[:5]: print(f\\"{stat}\\") # Example usage with the provided script script = def inefficient_function(): large_list = [] for i in range(100000): large_list.append(i) return sum(large_list) def another_inefficient_function(): small_list = [i for i in range(1000)] return sum(small_list) inefficient_function() another_inefficient_function() # Run the memory analysis analyze_memory_usage(script)"},{"question":"# Advanced PyTorch Coding Assessment Objective Your task is to implement a function using PyTorch that demonstrates your understanding of tensor operations and memory management. Problem Statement You need to: 1. Create a large PyTorch tensor. 2. Check the memory stats using the `torch.mtia.memory.memory_stats` function. 3. Perform multiple in-place operations on this tensor. 4. Check the memory stats again after performing the operations. 5. Return the memory stats before and after the operations for comparison. Specifications - You will implement a function `analyze_memory_stats` which takes the size of the tensor as an input parameter. - The input parameter `tensor_size` is a tuple representing the shape of the tensor (e.g., `(1000, 1000)`). Constraints - You must use in-place tensor operations to modify the tensor. - Ensure efficient memory usage throughout the operations. Input - `tensor_size` (tuple): A tuple of integers representing the shape of the tensor to be created. Output - A tuple containing two dictionaries: - The first dictionary contains memory stats before the in-place operations. - The second dictionary contains memory stats after the in-place operations. Example ```python import torch from torch.mtia.memory import memory_stats def analyze_memory_stats(tensor_size): # Create a large tensor with the given size tensor = torch.rand(tensor_size) # Get initial memory stats initial_stats = memory_stats() # Perform in-place operations on the tensor tensor.mul_(2) tensor.add_(5) tensor.sub_(3) # Get memory stats after operations final_stats = memory_stats() return (initial_stats, final_stats) # Example usage tensor_size = (1000, 1000) initial_stats, final_stats = analyze_memory_stats(tensor_size) print(\\"Initial Stats:\\", initial_stats) print(\\"Final Stats:\\", final_stats) ``` Notes - Ensure that `torch.mtia.memory.memory_stats` is properly imported and used. - The example provided should be tested and adapted to fit the actual `memory_stats` function behavior and output format.","solution":"import torch def analyze_memory_stats(tensor_size): Analyze memory stats before and after performing in-place operations on a tensor. Parameters: tensor_size (tuple): The shape of the tensor to create. Returns: tuple: A tuple containing two dictionaries - memory stats before and after the operations. # Create a large tensor with the given size tensor = torch.rand(tensor_size) # Get initial memory stats initial_allocated = torch.cuda.memory_allocated() initial_reserved = torch.cuda.memory_reserved() # Perform in-place operations on the tensor tensor.mul_(2) tensor.add_(5) tensor.sub_(3) # Get memory stats after operations final_allocated = torch.cuda.memory_allocated() final_reserved = torch.cuda.memory_reserved() initial_stats = {\\"memory_allocated\\": initial_allocated, \\"memory_reserved\\": initial_reserved} final_stats = {\\"memory_allocated\\": final_allocated, \\"memory_reserved\\": final_reserved} return (initial_stats, final_stats)"},{"question":"# Python Typing and Generics Objective Implement a small library that leverages Python\'s \\"typing\\" module. This library will handle registering users and validating their data. The purpose of this exercise is to assess your understanding of type hints, generics, and user-defined types. Instructions 1. Define a **User** class that includes the following attributes: - `username: str` - `email: str` - `age: int` - `is_active: bool` 2. Create a **UserRegistry** class utilizing generics and type hints. This class should: - Store users in a list. - Provide a method `add_user` to add a new user to the registry. - Provide a method `get_user_by_username` to retrieve a user based on their username. - Ensure that all data being added to the registry is of the correct type. 3. Implement a **validate_user_data** function outside the UserRegistry class that: - Takes a dictionary as input. - Validates that the dictionary contains valid data for a `User`. - Returns a boolean indicating the data\'s validity. Example ```python from typing import List, Generic, TypeVar, Dict # Define your type hints and classes here T = TypeVar(\'T\', bound=\'User\') class User: def __init__(self, username: str, email: str, age: int, is_active: bool): self.username = username self.email = email self.age = age self.is_active = is_active class UserRegistry(Generic[T]): def __init__(self): self.users: List[T] = [] def add_user(self, user: T) -> None: self.users.append(user) def get_user_by_username(self, username: str) -> T: for user in self.users: if user.username == username: return user raise ValueError(\\"User not found\\") def validate_user_data(data: Dict[str, any]) -> bool: required_keys = {\\"username\\", \\"email\\", \\"age\\", \\"is_active\\"} if not all(key in data for key in required_keys): return False if not isinstance(data[\\"username\\"], str): return False if not isinstance(data[\\"email\\"], str): return False if not isinstance(data[\\"age\\"], int): return False if not isinstance(data[\\"is_active\\"], bool): return False return True # Example usage registry = UserRegistry[User]() user_data = {\\"username\\": \\"john_doe\\", \\"email\\": \\"john@example.com\\", \\"age\\": 30, \\"is_active\\": True} if validate_user_data(user_data): user = User(**user_data) registry.add_user(user) retrieved_user = registry.get_user_by_username(\\"john_doe\\") print(retrieved_user.username, retrieved_user.email, retrieved_user.age, retrieved_user.is_active) else: print(\\"Invalid user data\\") ``` Constraints - Ensure the `UserRegistry` class enforces type hints and user generics appropriately. - The `validate_user_data` function should strictly check for the correct types specified. - Methods in the `UserRegistry` should handle cases where the user is not found gracefully. Notes - Code readability and proper use of type annotations are crucial. - Your implementation should aim to make adding and retrieving users intuitive and error-free.","solution":"from typing import List, Generic, TypeVar, Dict # Define your type hints and classes here T = TypeVar(\'T\', bound=\'User\') class User: def __init__(self, username: str, email: str, age: int, is_active: bool): self.username = username self.email = email self.age = age self.is_active = is_active class UserRegistry(Generic[T]): def __init__(self): self.users: List[T] = [] def add_user(self, user: T) -> None: self.users.append(user) def get_user_by_username(self, username: str) -> T: for user in self.users: if user.username == username: return user raise ValueError(\\"User not found\\") def validate_user_data(data: Dict[str, any]) -> bool: required_keys = {\\"username\\", \\"email\\", \\"age\\", \\"is_active\\"} if not all(key in data for key in required_keys): return False if not isinstance(data[\\"username\\"], str): return False if not isinstance(data[\\"email\\"], str): return False if not isinstance(data[\\"age\\"], int): return False if not isinstance(data[\\"is_active\\"], bool): return False return True"},{"question":"Background A telecommunications company holds a critical dataset containing details about their customer base and their subscription details. Your task is to analyze this dataset to gain insights on customer behavior and the company\'s performance. Dataset Description The dataset is provided as a `pandas.DataFrame` with the following columns: - `customer_id`: Unique identifier for each customer (integer). - `subscription_date`: Date the customer started subscribing (string in `YYYY-MM-DD` format). - `churn_date`: Date the customer canceled their subscription (string in `YYYY-MM-DD` format or `NaN` if still active). - `subscription_type`: Type of subscription (`Basic`, `Standard`, `Premium`). - `monthly_fee`: Monthly fee for the subscription (float). - `name`: Name of the customer (string). - `age`: Age of the customer (integer). - `state`: State where the customer resides (string). Task Write a Python function using pandas to perform the following operations: 1. **Data Cleaning**: - Convert `subscription_date` and `churn_date` to datetime objects. - Fill any missing `churn_date` with the current date to indicate the subscription is still active. 2. **Analysis**: - **1**: Calculate the total monthly revenue from active subscriptions. - **2**: Calculate the churn rate (percentage of customers who have canceled their subscription). - **3**: Determine the distribution of subscription types among active customers. - **4**: Find out the state with the highest number of customers. Constraints - Assume the dataset fits in memory. - Assume that there are no duplicate `customer_id`s. Function Signature ```python import pandas as pd from typing import Dict, Any def analyze_customer_data(df: pd.DataFrame) -> Dict[str, Any]: # Your implementation here pass ``` Example ```python data = { \'customer_id\': [1, 2, 3, 4, 5], \'subscription_date\': [\'2023-01-10\', \'2022-05-15\', \'2021-12-01\', \'2023-03-20\', \'2023-01-11\'], \'churn_date\': [\'2023-07-10\', \'NaN\', \'2022-12-15\', \'2023-08-20\', \'NaN\'], \'subscription_type\': [\'Basic\', \'Standard\', \'Premium\', \'Premium\', \'Basic\'], \'monthly_fee\': [10.0, 15.0, 25.0, 25.0, 10.0], \'name\': [\'Alice\', \'Bob\', \'Charlie\', \'David\', \'Eva\'], \'age\': [28, 34, 45, 23, 36], \'state\': [\'CA\', \'NV\', \'CA\', \'NV\', \'CA\'] } df = pd.DataFrame(data) result = analyze_customer_data(df) print(result) ``` Expected Output (example): ```python { \'total_monthly_revenue\': 85.0, # Example value; it depends on the current date \'churn_rate\': 40.0, # Example value calculated based on churned customers \'subscription_distribution\': { # Example values \'Basic\': 2, \'Standard\': 1, \'Premium\': 2 }, \'state_with_max_customers\': \'CA\' # Example value } ``` **Note:** Ensure you test your function with various date inputs to confirm it handles different scenarios correctly.","solution":"import pandas as pd from typing import Dict, Any from datetime import datetime def analyze_customer_data(df: pd.DataFrame) -> Dict[str, Any]: # Convert subscription_date and churn_date to datetime objects df[\'subscription_date\'] = pd.to_datetime(df[\'subscription_date\'], format=\'%Y-%m-%d\') df[\'churn_date\'] = pd.to_datetime(df[\'churn_date\'], format=\'%Y-%m-%d\', errors=\'coerce\') # Fill missing churn_date with the current date current_date = pd.to_datetime(datetime.now().date()) df[\'churn_date\'] = df[\'churn_date\'].fillna(current_date) # Calculate the total monthly revenue from active subscriptions active_customers = df[df[\'churn_date\'] == current_date] total_monthly_revenue = active_customers[\'monthly_fee\'].sum() # Calculate the churn rate churned_customers = df[df[\'churn_date\'] != current_date].shape[0] total_customers = df.shape[0] churn_rate = (churned_customers / total_customers) * 100 # Determine the distribution of subscription types among active customers subscription_distribution = active_customers[\'subscription_type\'].value_counts().to_dict() # Find out the state with the highest number of customers state_with_max_customers = df[\'state\'].value_counts().idxmax() # Consolidate the results into a dictionary result = { \'total_monthly_revenue\': total_monthly_revenue, \'churn_rate\': churn_rate, \'subscription_distribution\': subscription_distribution, \'state_with_max_customers\': state_with_max_customers } return result"},{"question":"Objective Implement a custom sequence class that inherits from `collections.abc.Sequence`. Your implementation should define all required abstract methods and should be able to work with the mixin methods provided by the `Sequence` ABC. Task 1. Create a class `CustomSequence` that inherits from `collections.abc.Sequence`. 2. Implement the required abstract methods: `__getitem__` and `__len__`. 3. Add an additional method `append` to allow adding elements to the sequence. Specifications - `__getitem__(self, index)` should return the element at the specified index. - `__len__(self)` should return the number of elements in the sequence. - `append(self, value)` should add the given value to the sequence. Input and Output - The `CustomSequence` class should be able to handle various data types including integers, strings, and other objects. - The sequence should support typical sequence operations such as indexing and iteration. Constraints - Do not use built-in Python sequence types like lists or tuples as the internal storage of your `CustomSequence` class. - You may use any other data structures such as linked lists or arrays to manage the internal data. - Your implementation should perform efficiently with a linear time complexity for the `__getitem__` and `__len__` methods. Example ```python from collections.abc import Sequence class CustomSequence(Sequence): def __init__(self): # Initialize your internal data structure self.data = [] def __getitem__(self, index): # Implement method to get item by index return self.data[index] def __len__(self): # Implement method to get size of sequence return len(self.data) def append(self, value): # Implement method to add elements to sequence self.data.append(value) # Example usage seq = CustomSequence() seq.append(1) seq.append(2) seq.append(3) print(seq[0]) # Output: 1 print(len(seq)) # Output: 3 print(list(seq)) # Output: [1, 2, 3] ``` In this question: - You must define the `CustomSequence` class as described. - Include error handling for invalid indices. - Ensure your class works as expected with Python\'s built-in functions like `len()` and `list()`.","solution":"from collections.abc import Sequence class CustomSequence(Sequence): def __init__(self): self.data = [] def __getitem__(self, index): if index < 0 or index >= len(self.data): raise IndexError(\\"Index out of range\\") return self.data[index] def __len__(self): return len(self.data) def append(self, value): self.data.append(value)"},{"question":"**Lexical Analysis of Bash-like Commands** The \\"shlex\\" module in Python can be used to create lexical analyzers for simple shell-like syntaxes. Your task is to implement a function that takes a multiline string representing a sequence of commands and returns a list of tokens for each command after performing lexical analysis. # Function Signature ```python def analyze_shell_commands(commands: str, posix: bool = True) -> list: pass ``` # Input - `commands` (str): A multiline string where each line represents a shell command. - `posix` (bool): Optional flag indicating whether to use POSIX mode (default is True). # Output - `list`: A list of lists, where each inner list contains tokens for the corresponding command. # Constraints - The multiline string can contain any valid shell command syntax. - Handle quoted strings, escape sequences, and comments as specified in the `shlex` module documentation. - Assume the maximum length of a single command line will not exceed 1000 characters. # Example ```python commands = echo \\"Hello, World!\\" # This is a comment ls -la /home/user grep -r \\"search term\\" /directory result = analyze_shell_commands(commands) # Expected Output: # [ # [\'echo\', \'Hello, World!\'], # [\'ls\', \'-la\', \'/home/user\'], # [\'grep\', \'-r\', \'search term\', \'/directory\'] # ] ``` # Notes - Make sure to strip out any comments indicated by `#`. - Preserve literal values inside quotes and treat escape characters according to the `shlex` documentation. - Each line from the input string should be treated as a separate command. # Implementation Tips - Utilize the `shlex.split` method for tokenizing each line. - Consider using a loop to process each command line individually and collect the tokens in a list.","solution":"import shlex def analyze_shell_commands(commands: str, posix: bool = True) -> list: Tokenizes a multiline string of shell commands into a list of lists of tokens for each command. :param commands: A multiline string where each line represents a shell command. :param posix: Flag indicating whether to use POSIX mode (default is True). :return: A list of lists, where each inner list contains tokens for the corresponding command. tokenized_commands = [] for command in commands.strip().splitlines(): lexer = shlex.shlex(command, posix=posix) lexer.whitespace_split = True lexer.commenters = \'#\' tokenized_commands.append(list(lexer)) return tokenized_commands"},{"question":"# Question: DataFrame Memory and Mutation Challenge You are given a dataset representing a company\'s employee records. The dataset includes the following columns: - `EmployeeID` (int) - Unique identifier for each employee. - `Name` (object) - Full name of the employee. - `Department` (object) - The department in which the employee works. - `Salary` (float) - The salary of the employee. - `JoinDate` (datetime64) - The date the employee joined the company. - `YearsInCompany` (int) - Number of years the employee has worked in the company. You are tasked with two specific objectives: 1. **Optimize DataFrame Memory Usage:** Implement a function `optimize_memory_usage` that will optimize the memory usage of the given DataFrame by: - Converting object types to categories wherever appropriate. - Reporting the memory usage before and after optimization using `deep` memory introspection. 2. **Handle Missing Values and Perform Safe Mutations:** Implement a function `handle_missing_and_mutate` that performs the following tasks: - Handles any missing values in the `YearsInCompany` column by filling them with the mean value of that column. - Safely applies a user-defined function that updates the `Salary` of each employee based on their department and years in the company. Ensure that the original DataFrame is not mutated during this operation. Here are the detailed requirements: Function: `optimize_memory_usage(df: pd.DataFrame) -> pd.DataFrame` **Input:** - `df` (pd.DataFrame): The input DataFrame containing employee records. **Output:** - A tuple containing: - The original memory usage of the DataFrame as a string (e.g., \\"Original Memory Usage: 123.45 KB\\"). - The optimized DataFrame. - The optimized memory usage of the DataFrame as a string (e.g., \\"Optimized Memory Usage: 101.23 KB\\"). **Constraints:** - Use `memory_usage=\'deep\'` for accurate memory measurement. Function: `handle_missing_and_mutate(df: pd.DataFrame, update_salary_func: Callable[[pd.Series], pd.Series]) -> pd.DataFrame` **Input:** - `df` (pd.DataFrame): The input DataFrame containing employee records. - `update_salary_func` (Callable[[pd.Series], pd.Series]): A user-defined function to update the `Salary` of each employee based on the department and years in the company. **Output:** - A new DataFrame with the following transformations applied: - Missing values in `YearsInCompany` column filled with the mean of this column. - `Salary` updated using the `update_salary_func` provided. **Constraints:** - Ensure that the original DataFrame (`df`) is not mutated. # Example Usage ```python import pandas as pd # Sample DataFrame data = { \\"EmployeeID\\": [1, 2, 3, 4, 5], \\"Name\\": [\\"Alice\\", \\"Bob\\", \\"Charlie\\", \\"David\\", \\"Eva\\"], \\"Department\\": [\\"HR\\", \\"IT\\", \\"IT\\", \\"Finance\\", \\"HR\\"], \\"Salary\\": [50000.0, 60000.0, 70000.0, 80000.0, 90000.0], \\"JoinDate\\": pd.to_datetime([\\"2020-01-01\\", \\"2019-01-01\\", \\"2018-01-01\\", \\"2017-01-01\\", \\"2016-01-01\\"]), \\"YearsInCompany\\": [2, 3, 4, pd.NA, 6] } df = pd.DataFrame(data) # Function to update salary def update_salary(row): if row[\\"Department\\"] == \\"IT\\": row[\\"Salary\\"] *= 1.1 elif row[\\"Department\\"] == \\"HR\\": row[\\"Salary\\"] *= 1.05 return row # Usage of optimize_memory_usage function original_memory, optimized_df, optimized_memory = optimize_memory_usage(df) print(original_memory) print(optimized_df) print(optimized_memory) # Usage of handle_missing_and_mutate function new_df = handle_missing_and_mutate(df, update_salary) print(new_df) ``` Write the implementation of the functions `optimize_memory_usage` and `handle_missing_and_mutate` to accomplish the tasks outlined above.","solution":"import pandas as pd import numpy as np def optimize_memory_usage(df: pd.DataFrame) -> tuple: Optimizes the memory usage of the given DataFrame by converting object types to categories wherever appropriate and reports the memory usage before and after optimization. original_memory_usage = df.memory_usage(deep=True).sum() / 1024 original_memory_usage_str = f\\"Original Memory Usage: {original_memory_usage:.2f} KB\\" df_optimized = df.copy() for col in df_optimized.select_dtypes(include=[\'object\']).columns: df_optimized[col] = df_optimized[col].astype(\'category\') optimized_memory_usage = df_optimized.memory_usage(deep=True).sum() / 1024 optimized_memory_usage_str = f\\"Optimized Memory Usage: {optimized_memory_usage:.2f} KB\\" return original_memory_usage_str, df_optimized, optimized_memory_usage_str def handle_missing_and_mutate(df: pd.DataFrame, update_salary_func: callable) -> pd.DataFrame: Handles any missing values in the YearsInCompany column by filling them with the mean value, and safely applies a user-defined function to update the Salary of each employee. Ensures that the original DataFrame is not mutated. df_new = df.copy() mean_years = df_new[\'YearsInCompany\'].astype(float).mean() df_new[\'YearsInCompany\'].fillna(mean_years, inplace=True) updated_df = df_new.apply(update_salary_func, axis=1) return updated_df"},{"question":"**Advanced Seaborn Plotting Exercise** **Objective:** Demonstrate your understanding of Seaborn\'s advanced plotting techniques using `seaborn.objects` by customizing plot limits, markers, and data transformations. **Task:** 1. Create a dataset that consists of 20 data points where `x` values are from 1 to 20 and `y` values are the square of `x`. 2. Using `seaborn.objects`: - Plot the dataset using a line plot with circular markers. - Customize the plot limits as follows: - Set the x-axis limits from 0 to 25. - Set the y-axis limits from 0 to 500. - Reverse the y-axis. - Keep the default upper limit of the x-axis while setting the lower limit to 5. **Requirements:** - Your solution must define a function `create_custom_plot()` that when called, generates the described plot. - The function should not return any value but should display the plot. - Make sure to use the `seaborn.objects` module for plotting. **Example Function Signature:** ```python import seaborn.objects as so def create_custom_plot(): # Your implementation goes here pass ``` **Evaluation Criteria:** - Correctness: The plot must meet all specified customization requirements. - Usage of seaborn.objects: The solution must utilize seaborn\'s object-oriented interface. - Code Readability: The code should be well-organized and documented. **Hints:** - You may find `so.Plot` and `p.limit()` essential for setting up the plot and modifying limits. - Reversing the axis requires swapping the `min` and `max` values. - Use `None` for maintaining default values where required.","solution":"import seaborn.objects as so import pandas as pd def create_custom_plot(): # Create the dataset data = {\'x\': list(range(1, 21)), \'y\': [x**2 for x in range(1, 21)]} df = pd.DataFrame(data) # Create the plot using seaborn objects p = (so.Plot(df, x=\'x\', y=\'y\') .add(so.Line(marker=\'o\')) .limit(x=(0, 25), y=(500, 0))) # Reverse the y-axis by swapping min and max # Display the plot p.show()"},{"question":"# KDE Plot Advanced Visualization Challenge You are given two datasets: `iris` and `tips` from the seaborn library. Your task is to perform the following steps using seaborn\'s KDE plot functionality to demonstrate your understanding of fundamental and advanced concepts of this package. # Requirements: 1. **Load the Datasets**: Load the `iris` and `tips` datasets from seaborn. 2. **Univariate KDE Plot with Customization**: - Plot the distribution of the `total_bill` variable from the `tips` dataset. - Adjust the smoothing parameter to `bw_adjust=0.5`. - Use a log scale for the x-axis. - Customize the plot with a fill, use the `crest` palette, add alpha transparency of 0.7, and remove the line outline. 3. **Bivariate KDE Plot with Fill and Custom Levels**: - Plot a bivariate KDE plot for the `waiting` and `duration` variables from the `geyser` dataset. - Use the `mako` cmap for coloring. - Fill the contours and set the number of levels to 30. 4. **Conditional Distribution with Hue**: - Plot the distribution of `total_bill` in the `tips` dataset using the `time` variable for conditioning with hue. - Normalize this distribution using the `fill` option. 5. **Aggregation and Weighted KDE Plot**: - Aggregate the `tips` dataset by size, calculating the mean `total_bill` and count of observations. - Create a weighted KDE plot for the aggregated `total_bill`, using the count of observations as weights. # Input Format: - No input is required from the user. The datasets will be loaded internally. # Output Format: - Display the plots as described in the requirement. # Constraints: - You must use seaborn\'s `kdeplot` function. - Ensure your solution is efficient and avoids unnecessary recomputation. Here is the expected function signature: ```python def kde_plot_challenge(): import seaborn as sns import matplotlib.pyplot as plt sns.set_theme() # 1. Load the datasets tips = sns.load_dataset(\\"tips\\") iris = sns.load_dataset(\\"iris\\") geyser = sns.load_dataset(\\"geyser\\") # 2. Univariate KDE Plot with Customization plt.figure(figsize=(10, 6)) sns.kdeplot(data=tips, x=\\"total_bill\\", bw_adjust=0.5, log_scale=True, fill=True, palette=\\"crest\\", alpha=0.7, linewidth=0) plt.title(\\"Customized Univariate KDE Plot of Total Bill\\") plt.show() # 3. Bivariate KDE Plot with Fill and Custom Levels plt.figure(figsize=(10, 6)) sns.kdeplot(data=geyser, x=\\"waiting\\", y=\\"duration\\", cmap=\\"mako\\", fill=True, levels=30) plt.title(\\"Bivariate KDE Plot of Waiting and Duration\\") plt.show() # 4. Conditional Distribution with Hue plt.figure(figsize=(10, 6)) sns.kdeplot(data=tips, x=\\"total_bill\\", hue=\\"time\\", multiple=\\"fill\\") plt.title(\\"Conditional KDE Plot of Total Bill by Time\\") plt.show() # 5. Aggregation and Weighted KDE Plot tips_agg = tips.groupby(\\"size\\").agg(total_bill=(\\"total_bill\\", \\"mean\\"), n=(\\"total_bill\\", \\"count\\")).reset_index() plt.figure(figsize=(10, 6)) sns.kdeplot(data=tips_agg, x=\\"total_bill\\", weights=\\"n\\") plt.title(\\"Weighted KDE Plot of Aggregated Total Bill\\") plt.show() ``` # Notes: - Ensure you have the seaborn and matplotlib libraries installed. - Verify your output visually against the requirements.","solution":"import seaborn as sns import matplotlib.pyplot as plt def kde_plot_challenge(): sns.set_theme() # 1. Load the datasets tips = sns.load_dataset(\\"tips\\") iris = sns.load_dataset(\\"iris\\") geyser = sns.load_dataset(\\"geyser\\") # 2. Univariate KDE Plot with Customization plt.figure(figsize=(10, 6)) sns.kdeplot(data=tips, x=\\"total_bill\\", bw_adjust=0.5, log_scale=True, fill=True, palette=\\"crest\\", alpha=0.7, linewidth=0) plt.title(\\"Customized Univariate KDE Plot of Total Bill\\") plt.show() # 3. Bivariate KDE Plot with Fill and Custom Levels plt.figure(figsize=(10, 6)) sns.kdeplot(data=geyser, x=\\"waiting\\", y=\\"duration\\", cmap=\\"mako\\", fill=True, levels=30) plt.title(\\"Bivariate KDE Plot of Waiting and Duration\\") plt.show() # 4. Conditional Distribution with Hue plt.figure(figsize=(10, 6)) sns.kdeplot(data=tips, x=\\"total_bill\\", hue=\\"time\\", multiple=\\"fill\\") plt.title(\\"Conditional KDE Plot of Total Bill by Time\\") plt.show() # 5. Aggregation and Weighted KDE Plot tips_agg = tips.groupby(\\"size\\").agg(total_bill=(\\"total_bill\\", \\"mean\\"), n=(\\"total_bill\\", \\"count\\")).reset_index() plt.figure(figsize=(10, 6)) sns.kdeplot(data=tips_agg, x=\\"total_bill\\", weights=\\"n\\") plt.title(\\"Weighted KDE Plot of Aggregated Total Bill\\") plt.show()"},{"question":"**Objective**: Implement a secure message storage system using Python\'s cryptographic services. **Problem Description**: You are tasked with creating a Python class called `SecureMessageStorage` for securely storing and retrieving messages. The class should use hashlib for hashing, hmac for message authentication, and secrets for generating random tokens. **Functionality Requirements**: 1. **Initialization**: - The class should initialize with a secret key used for HMAC and generating tokens. 2. **Store Message**: - Method: `store_message(user_id: str, message: str) -> str` - Encrypt and store a given user message with the following procedure: - Generate a random token using the secrets module. - Create a hash of the message using the BLAKE2 algorithm. - Create an HMAC of the hashed message using the secret key. - Store the message securely (in memory) associated with the HMAC and the user ID. - Return the generated token. 3. **Retrieve Message**: - Method: `retrieve_message(user_id: str, token: str) -> str` - Retrieve and return a stored message using the token and user ID. - Verify the HMAC to ensure the integrity of the message. 4. **Delete Message**: - Method: `delete_message(user_id: str, token: str) -> bool` - Delete a stored message using the token and user ID. - Return `True` if the message was found and deleted, otherwise return `False`. **Constraints**: - The system should not store plaintext messages. - The hash algorithm used must be BLAKE2. - Ensure that HMAC validation is performed correctly. **Input/Output Formats**: ```python class SecureMessageStorage: def __init__(self, secret_key: bytes): # Initialize with the given secret key def store_message(self, user_id: str, message: str) -> str: Stores a message securely and returns a token. Parameters: user_id (str): The identifier of the user. message (str): The message to be stored. Returns: str: A token that can be used to retrieve the message. pass def retrieve_message(self, user_id: str, token: str) -> str: Retrieves a stored message using the user_id and token. Parameters: user_id (str): The identifier of the user. token (str): The token returned during storing. Returns: str: The original message if found, otherwise an empty string. pass def delete_message(self, user_id: str, token: str) -> bool: Deletes a stored message using the user_id and token. Parameters: user_id (str): The identifier of the user. token (str): The token returned during storing. Returns: bool: True if the message was found and deleted, otherwise False. pass ``` **Example Usage**: ```python # Initialize the storage system with a secret key secret_key = b\'super_secret_key\' storage = SecureMessageStorage(secret_key) # Store a message token = storage.store_message(\'user123\', \'My secret message\') print(token) # Outputs the generated token # Retrieve the message retrieved_message = storage.retrieve_message(\'user123\', token) print(retrieved_message) # Outputs \'My secret message\' # Delete the message status = storage.delete_message(\'user123\', token) print(status) # Outputs True # Try to retrieve the deleted message retrieved_message = storage.retrieve_message(\'user123\', token) print(retrieved_message) # Outputs an empty string ``` **Performance Requirements**: - The solution should handle at least 10,000 messages efficiently in terms of both time and memory usage. **Notes**: - Pay careful attention to secure handling of cryptographic keys and tokens. - Consider edge cases such as non-existent user IDs or tokens, and ensure your methods handle them gracefully.","solution":"import hashlib import hmac import secrets class SecureMessageStorage: def __init__(self, secret_key: bytes): self.secret_key = secret_key self.storage = {} def __generate_token(self): return secrets.token_hex(16) def __hash_message(self, message): hash_blake2 = hashlib.blake2b() hash_blake2.update(message.encode()) return hash_blake2.digest() def __create_hmac(self, message_hash): return hmac.new(self.secret_key, message_hash, hashlib.blake2b).hexdigest() def store_message(self, user_id: str, message: str) -> str: token = self.__generate_token() message_hash = self.__hash_message(message) message_hmac = self.__create_hmac(message_hash) # Store the message with user_id and token as keys if user_id not in self.storage: self.storage[user_id] = {} self.storage[user_id][token] = { \\"hash\\": message_hash, \\"hmac\\": message_hmac, \\"message\\": message } return token def retrieve_message(self, user_id: str, token: str) -> str: if user_id in self.storage and token in self.storage[user_id]: stored_data = self.storage[user_id][token] stored_hmac = self.__create_hmac(stored_data[\\"hash\\"]) if hmac.compare_digest(stored_hmac, stored_data[\\"hmac\\"]): return stored_data[\\"message\\"] return \\"\\" def delete_message(self, user_id: str, token: str) -> bool: if user_id in self.storage and token in self.storage[user_id]: del self.storage[user_id][token] # Cleanup user_id if no more messages exist if not self.storage[user_id]: del self.storage[user_id] return True return False"},{"question":"Objective Design a function involving basic to intermediate computation, document it with examples, and use the doctest module to ensure its correctness. # Problem Statement Write a Python function that calculates the nth Fibonacci number using an iterative approach. The Fibonacci sequence is defined as: - F(0) = 0 - F(1) = 1 - F(n) = F(n-1) + F(n-2) for n > 1 Document this function with doctests demonstrating its use. The docstring should include: - Basic functionality with fixed inputs. - Error handling for invalid inputs (e.g., negative integers). # Requirements: 1. **Function:** `fibonacci(n)` - **Input:** `n` (an integer representing the position in the Fibonacci sequence, where n >= 0) - **Output:** an integer representing the nth Fibonacci number. - **Constraints:** - The function should handle large values but doesn\'t need to optimize for extremely large values (i.e., handling up to `n = 100` efficiently is sufficient). - **Error Handling:** - Raise a `ValueError` if `n` is negative with the message \\"n must be a non-negative integer.\\" 2. **Docstring:** - Include a brief description of the function. - Examples using **doctests** that: - Calculate specific Fibonacci numbers. - Handle invalid input by raising appropriate errors. 3. **Verification:** - Use the doctest module to verify the examples in the docstring. # Example Usage: ```python def fibonacci(n): Calculate the nth Fibonacci number iteratively. Examples: >>> fibonacci(0) 0 >>> fibonacci(1) 1 >>> fibonacci(10) 55 >>> fibonacci(20) 6765 >>> fibonacci(-1) Traceback (most recent call last): ... ValueError: n must be a non-negative integer >>> fibonacci(30.5) Traceback (most recent call last): ... ValueError: n must be an integer :param n: Integer, the position in the Fibonacci sequence (n >= 0) :return: Integer, the nth Fibonacci number :raises ValueError: if n is negative or not an integer if not isinstance(n, int): raise ValueError(\\"n must be an integer\\") if n < 0: raise ValueError(\\"n must be a non-negative integer\\") a, b = 0, 1 for _ in range(n): a, b = b, a + b return a if __name__ == \\"__main__\\": import doctest doctest.testmod() ``` Ensure your function passes all the described doctests by running the module.","solution":"def fibonacci(n): Calculate the nth Fibonacci number iteratively. Examples: >>> fibonacci(0) 0 >>> fibonacci(1) 1 >>> fibonacci(10) 55 >>> fibonacci(20) 6765 >>> fibonacci(-1) Traceback (most recent call last): ... ValueError: n must be a non-negative integer >>> fibonacci(30.5) Traceback (most recent call last): ... ValueError: n must be an integer :param n: Integer, the position in the Fibonacci sequence (n >= 0) :return: Integer, the nth Fibonacci number :raises ValueError: if n is negative or not an integer if not isinstance(n, int): raise ValueError(\\"n must be an integer\\") if n < 0: raise ValueError(\\"n must be a non-negative integer\\") a, b = 0, 1 for _ in range(n): a, b = b, a + b return a if __name__ == \\"__main__\\": import doctest doctest.testmod()"},{"question":"**Question: Manipulating Audio Files with `aifc`** # Objective: Write a Python function using the `aifc` module to read an AIFF or AIFF-C audio file, modify the audio data by doubling the sample values, and write the modified audio data to a new AIFF-C file. # Function Signature: ```python def modify_audio(input_file: str, output_file: str) -> None: pass ``` # Input: - `input_file`: A string representing the path to the input AIFF or AIFF-C file. - `output_file`: A string representing the path to the output AIFF-C file where the modified audio data will be saved. # Output: - The function should write the modified audio data to the specified output file. It does not return any value. # Requirements: 1. **Reading the Input File**: - Open the input file in read mode using `aifc.open()`. - Retrieve the number of channels, sample width, frame rate, and number of frames using appropriate methods. - Read all the audio frames. 2. **Modifying the Audio Data**: - Convert the audio frames to a NumPy array (you can assume NumPy is available). - Double the value of each sample in the audio data. 3. **Writing to the Output File**: - Open the output file in write mode using `aifc.open()`. - Set the appropriate audio file parameters (number of channels, sample width, frame rate) for the output file. - Write the modified frames to the output file. - Close both the input and output files properly. # Constraints: - Assume the input file exists and is a valid AIFF or AIFF-C file. - The function should handle files up to 100MB in size efficiently. # Example Usage: ```python modify_audio(\'input.aiff\', \'output.aifc\') ``` In this example, the function reads audio data from `input.aiff`, doubles the values of each sample, and writes the modified audio data to `output.aifc`.","solution":"import aifc import numpy as np def modify_audio(input_file: str, output_file: str) -> None: This function reads an AIFF or AIFF-C audio file, modifies the audio data by doubling the sample values, and writes the modified audio data to a new AIFF-C file. # Open the input file with aifc.open(input_file, \'r\') as in_file: # Retrieve audio parameters num_channels = in_file.getnchannels() sample_width = in_file.getsampwidth() frame_rate = in_file.getframerate() num_frames = in_file.getnframes() # Read all frames audio_frames = in_file.readframes(num_frames) # Convert audio frames to numpy array for modification audio_data = np.frombuffer(audio_frames, dtype=np.int16) # Modify the audio data by doubling each sample value modified_audio_data = audio_data * 2 # Ensure we handle any potential clipping by capping the values to int16 range max_int16 = np.iinfo(np.int16).max min_int16 = np.iinfo(np.int16).min modified_audio_data = np.clip(modified_audio_data, min_int16, max_int16).astype(np.int16) # Convert the modified audio data back to bytes modified_audio_frames = modified_audio_data.tobytes() # Write the modified audio data to the output file with aifc.open(output_file, \'w\') as out_file: # Set the output file parameters out_file.setnchannels(num_channels) out_file.setsampwidth(sample_width) out_file.setframerate(frame_rate) # Write the modified frames to the output file out_file.writeframes(modified_audio_frames)"},{"question":"You are provided with the task of managing custom module search paths for a Python application. The goal is to ensure that your application can dynamically adjust its `sys.path` based on user-specific and site-specific paths. Objectives: 1. Implement a function `get_effective_sys_path()` that computes and returns the effective `sys.path` after considering both site-specific and user-specific paths. 2. Implement a function `add_custom_paths(paths: List[str])` that adds a list of custom paths to the current `sys.path`, ensuring no duplicates and processing any relevant `.pth` files in the directories. Detailed Requirements: 1. **Function 1: `get_effective_sys_path`** - **Input**: None. - **Output**: List of strings representing the effective `sys.path`. - **Behavior**: - The function should combine global site-packages directories (using `site.getsitepackages()`), user-specific site-packages directory (using `site.getusersitepackages()`), and any directories specified in `.pth` files in these directories. - Ensure that no path is duplicated in the result. - Skip non-existing paths. - The order of paths should follow the order they are typically added by the `site` module. ```python def get_effective_sys_path() -> List[str]: pass ``` 2. **Function 2: `add_custom_paths`** - **Input**: A list of strings where each string is a path to be added to `sys.path`. - **Output**: None. - **Behavior**: - Each path in the input list should be added to `sys.path` if not already present. - If a directory contains a `.pth` file, process it to add additional paths as specified in the file. - Ensure to skip non-existing paths and avoid processing a path more than once. ```python def add_custom_paths(paths: List[str]): pass ``` Constraints: - Assume Python 3.10 environment. - Paths should be valid directories. - `.pth` files may contain comments, blank lines, directories, and executable import statements. - Minimize the number of operations to add and check paths for efficiency. Example usage: ```python # Given .pth files in relevant directories: # /example/p1/ -> has p1.pth file with content: \\"/additional/path1\\" # /example/p2/ -> does not have any .pth file add_custom_paths([\'/example/p1\', \'/example/p2\']) effective_paths = get_effective_sys_path() print(effective_paths) # Output might be: [\'/example/p1\', \'/additional/path1\', \'/example/p2\', ... other site paths ...] ``` Design the implementation for both functions considering the provided documentation and ensuring you handle the parsing and processing of `.pth` files as specified.","solution":"import sys import os import site def get_effective_sys_path(): Computes and returns the effective sys.path after considering both site-specific and user-specific paths. # Get the site-specific packages directories site_paths = site.getsitepackages() # Get the user-specific site-packages directory user_site_path = site.getusersitepackages() # Combine paths and ensure no duplicates in order effective_paths = list(dict.fromkeys(sys.path + site_paths + [user_site_path])) # Ensure no non-existing paths are included effective_paths = [path for path in effective_paths if os.path.exists(path)] return effective_paths def add_custom_paths(paths): Adds a list of custom paths to sys.path, ensuring no duplicates and processing any relevant .pth files in the directories. new_paths = [] for path in paths: if path in sys.path: continue if not os.path.exists(path): continue if os.path.isdir(path): pth_files = [os.path.join(path, file) for file in os.listdir(path) if file.endswith(\'.pth\')] for pth_file in pth_files: with open(pth_file, \'r\') as f: for line in f: line = line.strip() if line and not line.startswith(\'#\'): if os.path.exists(line): new_paths.append(line) new_paths.append(path) # Append distinct new paths to sys.path for new_path in new_paths: if new_path not in sys.path: sys.path.append(new_path)"},{"question":"**Coding Assessment Question** **Objective:** Using the `tempfile` module, write a Python function to handle temporary directories and files, demonstrating the ability to create, manage, and clean up temporary resources efficiently. **Question:** Implement a function `process_temp_files(data_list: List[str]) -> List[str]` that performs the following: 1. Creates a temporary directory. 2. Inside this directory, for each string in `data_list`, create a temporary file and write the string to the file. 3. Read the contents back from each of these temporary files, convert all text to uppercase, and store the contents in a list. 4. Return the list of uppercase strings. 5. Ensure that all temporary files and the temporary directory are cleaned up automatically after their use. **Function Signature:** ```python from typing import List def process_temp_files(data_list: List[str]) -> List[str]: pass ``` **Input:** - `data_list`: A list of strings. (1 ≤ len(data_list) ≤ 100) **Output:** - Return a list of strings where each string is the uppercase content read from the corresponding temporary file. **Constraints:** - Ensure that temporary files and the temporary directory are properly cleaned up after use. - Use context managers to handle file operations for safety and simplicity. **Example:** ```python data_list = [\\"hello\\", \\"world\\", \\"tempfile\\", \\"module\\"] result = process_temp_files(data_list) print(result) # Output: [\'HELLO\', \'WORLD\', \'TEMPFILE\', \'MODULE\'] ``` **Note:** Make effective use of the `tempfile` module\'s features, such as `TemporaryDirectory` and `TemporaryFile`.","solution":"from typing import List import tempfile import os def process_temp_files(data_list: List[str]) -> List[str]: result = [] with tempfile.TemporaryDirectory() as temp_dir: temp_files = [] for i, data in enumerate(data_list): temp_file_path = os.path.join(temp_dir, f\\"tempfile_{i}.txt\\") with open(temp_file_path, \'w\') as temp_file: temp_file.write(data) temp_files.append(temp_file_path) for temp_file_path in temp_files: with open(temp_file_path, \'r\') as temp_file: content = temp_file.read() result.append(content.upper()) return result"},{"question":"# Kernel Approximation using RBFSampler and Nystroem Methods You are tasked with implementing a small pipeline to compare the efficiency and accuracy of two kernel approximation methods: `RBFSampler` and `Nystroem`, on a classification task using the Support Vector Machine (SVM) algorithm. Requirements: 1. **Input and Output Formats:** - **Input:** None. Use generated datasets within the code. - **Output:** Dictionary containing the accuracy and execution time for each method. 2. **Constraints:** - Use datasets with a minimum of 1000 samples and 20 features. - Use the RBF kernel for both methods with `gamma=0.1`. 3. **Performance Requirements:** - Measure and compare the execution time of the fit and transform operations for both methods. - Evaluate and compare the classification accuracy of the linear SVM model trained on data transformed by these methods. # Instructions: 1. **Generate a synthetic dataset** with 1000 samples and 20 features using `make_classification` from `sklearn.datasets`. 2. **Implement two kernel approximation methods:** - `RBFSampler` with 100 components. - `Nystroem` with 100 components. 3. **Transform the dataset** using both methods and measure the transformation time. 4. **Train a linear SVM model** on the transformed datasets. 5. **Evaluate the model accuracy** on the training data and collect the results. 6. **Compare and print the results**: Create a function `compare_kernel_approximations()` that returns a dictionary with the following structure: ```python { \\"RBFSampler\\": {\\"accuracy\\": <accuracy>, \\"time\\": <transformation_time>}, \\"Nystroem\\": {\\"accuracy\\": <accuracy>, \\"time\\": <transformation_time>} } ``` # Example ```python from sklearn.kernel_approximation import RBFSampler, Nystroem from sklearn.linear_model import SGDClassifier from sklearn.datasets import make_classification from sklearn.metrics import accuracy_score import time def compare_kernel_approximations(): # Generate dataset X, y = make_classification(n_samples=1000, n_features=20, random_state=42) results = {} # RBFSampler method rbf_sampler = RBFSampler(gamma=0.1, n_components=100, random_state=42) start_time = time.time() X_features_rbf = rbf_sampler.fit_transform(X) transformation_time_rbf = time.time() - start_time clf_rbf = SGDClassifier(max_iter=1000, random_state=42) clf_rbf.fit(X_features_rbf, y) accuracy_rbf = clf_rbf.score(X_features_rbf, y) results[\\"RBFSampler\\"] = {\\"accuracy\\": accuracy_rbf, \\"time\\": transformation_time_rbf} # Nystroem method nystroem = Nystroem(kernel=\'rbf\', gamma=0.1, n_components=100, random_state=42) start_time = time.time() X_features_nys = nystroem.fit_transform(X) transformation_time_nys = time.time() - start_time clf_nys = SGDClassifier(max_iter=1000, random_state=42) clf_nys.fit(X_features_nys, y) accuracy_nys = clf_nys.score(X_features_nys, y) results[\\"Nystroem\\"] = {\\"accuracy\\": accuracy_nys, \\"time\\": transformation_time_nys} return results # Check the results print(compare_kernel_approximations()) ```","solution":"from sklearn.kernel_approximation import RBFSampler, Nystroem from sklearn.linear_model import SGDClassifier from sklearn.datasets import make_classification from sklearn.metrics import accuracy_score import time def compare_kernel_approximations(): # Generate dataset X, y = make_classification(n_samples=1000, n_features=20, random_state=42) results = {} # RBFSampler method rbf_sampler = RBFSampler(gamma=0.1, n_components=100, random_state=42) start_time = time.time() X_features_rbf = rbf_sampler.fit_transform(X) transformation_time_rbf = time.time() - start_time clf_rbf = SGDClassifier(max_iter=1000, random_state=42) clf_rbf.fit(X_features_rbf, y) accuracy_rbf = clf_rbf.score(X_features_rbf, y) results[\\"RBFSampler\\"] = {\\"accuracy\\": accuracy_rbf, \\"time\\": transformation_time_rbf} # Nystroem method nystroem = Nystroem(kernel=\'rbf\', gamma=0.1, n_components=100, random_state=42) start_time = time.time() X_features_nys = nystroem.fit_transform(X) transformation_time_nys = time.time() - start_time clf_nys = SGDClassifier(max_iter=1000, random_state=42) clf_nys.fit(X_features_nys, y) accuracy_nys = clf_nys.score(X_features_nys, y) results[\\"Nystroem\\"] = {\\"accuracy\\": accuracy_nys, \\"time\\": transformation_time_nys} return results"},{"question":"# Advanced Python Programming Assignment You are tasked with enhancing a command-line interface (CLI) Python application by incorporating features from the `readline` module. Your solution should demonstrate the use of various functionalities of the `readline` module to improve the experience of the CLI application. Please follow the requirements below: Requirements 1. **Readline Initialization:** - Configure the readline module to enable tab completion and vi keybindings if the GNU readline library is being used. - If using the libedit library, enable the same features via appropriate initialization in ~/.editrc. 2. **History Management:** - Implement the functionality to load command history from a file named `.cli_history` located in the user’s home directory at the start of the program. - Save the updated command history to `.cli_history` before the program exits. - Limit the history length to 500 commands. Any commands beyond this limit should be truncated. 3. **Command Completion:** - Implement a custom completer function that suggests possible commands starting with the typed prefix when the Tab key is pressed. - The available commands are `help`, `exit`, `load`, `save`, and `list`. 4. **Additional Features:** - Add a brief help message to the CLI that lists available commands when the user types `help`. - Implement basic command handling for the following commands: - `exit`: Exits the application. - `load <filename>`: Displays a message indicating the filename to be loaded. - `save <filename>`: Displays a message indicating the filename to be saved. - `list`: Lists a predefined set of items. Implementation Details - Your solution should be implemented in a function named `run_cli()`. - The `run_cli()` function does not take any parameters and does not return any values. - Use the `readline` module for handling command history and completion. - Ensure that the command history and completion functions are correctly set up. Example Usage ```python >>> run_cli() CLI Application. Type \'help\' for a list of commands. > help Available commands: help, exit, load, save, list > load data.txt Loading data from \'data.txt\' > save data.txt Saving data to \'data.txt\' > list - Item 1 - Item 2 - Item 3 > exit Exiting ... ``` Submission Submit your Python script with the `run_cli()` function implemented as specified. Ensure your code is well-documented and follows best practices.","solution":"import readline import os import atexit # Define available commands for the custom completer COMMANDS = [\'help\', \'exit\', \'load\', \'save\', \'list\'] def completer(text, state): options = [cmd for cmd in COMMANDS if cmd.startswith(text)] if state < len(options): return options[state] else: return None def load_history(): histfile = os.path.join(os.path.expanduser(\\"~\\"), \\".cli_history\\") try: readline.read_history_file(histfile) except FileNotFoundError: pass readline.set_history_length(500) def save_history(): histfile = os.path.join(os.path.expanduser(\\"~\\"), \\".cli_history\\") readline.write_history_file(histfile) def run_cli(): readline.parse_and_bind(\'tab: complete\') readline.parse_and_bind(\'set editing-mode vi\') readline.set_completer(completer) load_history() atexit.register(save_history) print(\\"CLI Application. Type \'help\' for a list of commands.\\") while True: try: input_command = input(\'> \') if input_command == \'help\': print(\\"Available commands: \\" + \\", \\".join(COMMANDS)) elif input_command == \'exit\': print(\\"Exiting ...\\") break elif input_command.startswith(\'load \'): filename = input_command.split(\' \', 1)[1] print(f\\"Loading data from \'{filename}\'\\") elif input_command.startswith(\'save \'): filename = input_command.split(\' \', 1)[1] print(f\\"Saving data to \'{filename}\'\\") elif input_command == \'list\': print(\\"- Item 1n- Item 2n- Item 3\\") else: print(f\\"Unknown command: {input_command}\\") except EOFError: print() break if __name__ == \\"__main__\\": run_cli()"},{"question":"# PyTorch Custom Dataset and DataLoader In this exercise, you are required to create a custom dataset using the `torch.utils.data.Dataset` class and then utilize the `torch.utils.data.DataLoader` to load the data in batches, employing both single-process and multi-process loading strategies. Task 1: Custom Dataset 1. Create a custom dataset `RandomDataset` that: - Inherits from `torch.utils.data.Dataset`. - Generates `N` random samples, each sample is a tuple containing: - An array of 10 random float numbers as features. - An integer label which is `1` if the sum of features is greater than 0, otherwise `0`. - Implements the `__len__` method to return the length of the dataset. - Implements the `__getitem__` method to fetch a sample at a given index. ```python import torch from torch.utils.data import Dataset class RandomDataset(Dataset): def __init__(self, N): self.N = N self.data = [(torch.randn(10), 1 if torch.randn(10).sum() > 0 else 0) for _ in range(N)] def __len__(self): return self.N def __getitem__(self, idx): return self.data[idx] ``` Task 2: DataLoader with Custom Sampler 2. Create a custom sampler `EvenSampler` that: - Ensures that only samples with an even index are accessed. ```python from torch.utils.data import Sampler import numpy as np class EvenSampler(Sampler): def __init__(self, data_source): self.data_source = data_source def __iter__(self): return iter(np.arange(0, len(self.data_source), 2)) def __len__(self): return len(range(0, len(self.data_source), 2)) ``` Task 3: DataLoader Usage 3. Using the `RandomDataset` and `EvenSampler`: - Create a DataLoader with batch size 5. - Return and print batches in single-process mode (`num_workers=0`). - Return and print batches in multi-process mode (`num_workers=4`). ```python from torch.utils.data import DataLoader # Create dataset dataset = RandomDataset(100) # Create DataLoader with single-process mode data_loader_single = DataLoader(dataset, batch_size=5, sampler=EvenSampler(dataset), num_workers=0) print(\\"Single-process Data Loading\\") for batch in data_loader_single: print(batch) # Create DataLoader with multi-process mode data_loader_multi = DataLoader(dataset, batch_size=5, sampler=EvenSampler(dataset), num_workers=4) print(\\"Multi-process Data Loading\\") for batch in data_loader_multi: print(batch) ``` # Final Expected Output - You should see batched samples printed from both single-process and multi-process modes, illustrating that the data loader is correctly fetching and batching the data using the custom dataset and sampler. **Input Constraints**: - Assume `N = 100`. **Performance Requirements**: - The dataset needs to be efficiently iterable in both single-process and multi-process modes. **Testing**: - Verify that both single-process and multi-process outputs are generated correctly.","solution":"import torch from torch.utils.data import Dataset, DataLoader, Sampler import numpy as np class RandomDataset(Dataset): def __init__(self, N): self.N = N self.data = [(torch.randn(10), 1 if torch.randn(10).sum() > 0 else 0) for _ in range(N)] def __len__(self): return self.N def __getitem__(self, idx): return self.data[idx] class EvenSampler(Sampler): def __init__(self, data_source): self.data_source = data_source def __iter__(self): return iter(np.arange(0, len(self.data_source), 2)) def __len__(self): return len(range(0, len(self.data_source), 2)) # Create dataset dataset = RandomDataset(100) # Create DataLoader with single-process mode data_loader_single = DataLoader(dataset, batch_size=5, sampler=EvenSampler(dataset), num_workers=0) print(\\"Single-process Data Loading\\") for batch in data_loader_single: print(batch) # Create DataLoader with multi-process mode data_loader_multi = DataLoader(dataset, batch_size=5, sampler=EvenSampler(dataset), num_workers=4) print(\\"Multi-process Data Loading\\") for batch in data_loader_multi: print(batch)"},{"question":"Objective In this exercise, you will demonstrate your understanding of the `atexit` module by implementing a simple task tracker that logs tasks you have added and ensures the task list is saved to a file upon program termination. Task Description Create a Python class `TaskTracker` that supports adding tasks and automatically saves the list of tasks to a file when the program terminates. You must make use of the `atexit` module to handle the automatic saving of tasks. The `TaskTracker` class should have the following methods: 1. `__init__(self, filename)`: Initializes the task tracker with a given filename where tasks will be saved and read from. 2. `add_task(self, task)`: Adds a task to the task list. 3. `list_tasks(self)`: Returns the list of tasks currently stored. 4. `remove_task(self, task)`: Removes a task from the task list. If the task doesn’t exist, raise a `ValueError`. Upon program termination, the current list of tasks should be saved to the specified file. The tasks should be saved in plain text format, with one task per line. Example Usage ```python tracker = TaskTracker(\'tasks.txt\') tracker.add_task(\'Finish homework\') tracker.add_task(\'Read a book\') print(tracker.list_tasks()) # Expected Output: [\'Finish homework\', \'Read a book\'] tracker.remove_task(\'Finish homework\') print(tracker.list_tasks()) # Expected Output: [\'Read a book\'] # Upon exiting the program, tasks should be saved to \'tasks.txt\'. ``` Constraints - The task descriptions are non-empty strings. - Handle file I/O exceptions gracefully. - Use the `atexit` module to register the save operation. Notes - You can use the `atexit` module documentation for reference. - Make sure that the `tasks.txt` file is properly updated upon each program termination. - Ensure your solution works correctly under normal interpreter termination. Extra Challenge (Optional) - Extend the functionality to allow tasks to be marked as done and persist this state in the file. Submit your implementation of the `TaskTracker` class.","solution":"import atexit import os class TaskTracker: def __init__(self, filename): self.filename = filename self.tasks = [] self.load_tasks() atexit.register(self.save_tasks) def load_tasks(self): if os.path.exists(self.filename): with open(self.filename, \'r\') as f: self.tasks = [task.strip() for task in f.readlines()] def add_task(self, task): if task: self.tasks.append(task) def list_tasks(self): return self.tasks def remove_task(self, task): if task in self.tasks: self.tasks.remove(task) else: raise ValueError(\'Task not found\') def save_tasks(self): with open(self.filename, \'w\') as f: for task in self.tasks: f.write(task + \'n\')"},{"question":"# Coding Challenge: Handling Missing Data with Scikit-Learn Objective You are tasked with handling missing data in a dataset and building a machine learning pipeline that integrates imputation techniques with a classification model. Dataset Suppose you are given the following dataset with missing values (NaNs): ```python import numpy as np import pandas as pd data = { \'feature1\': [1, 2, np.nan, 4, 5], \'feature2\': [np.nan, 2, 3, np.nan, 5], \'feature3\': [\'a\', np.nan, \'a\', \'b\', \'b\'], \'target\': [0, 1, 0, 1, 0] } df = pd.DataFrame(data) ``` Task 1. **Part 1: Simple Imputation** - Impute the missing values in `feature1` and `feature2` using the mean of the respective columns. - Impute the missing values in `feature3` with the most frequent value of the column. 2. **Part 2: Create a Pipeline** - Create a machine learning pipeline that: - Imputes missing values as described in Part 1. - Adds binary indicator columns indicating the presence of missing values. - Uses a Decision Tree Classifier to classify the target column. 3. **Part 3: Training and Evaluation** - Split the data into training and test sets (70% training, 30% test). - Train the pipeline on the training data. - Evaluate the accuracy of the classifier on the test data. Implementation Details 1. **Input Format**: The function should accept no inputs. The dataset is predefined within the function. 2. **Output Format**: The function should print the accuracy of the classifier on the test data. Constraints - Use the `sklearn.impute` module for the imputation steps. - Ensure the pipeline handles missing value indicators properly and fits the classifier. Example Your implementation could be structured similarly to the following: ```python def handle_missing_data_and_classify(): from sklearn.impute import SimpleImputer, MissingIndicator from sklearn.pipeline import FeatureUnion, Pipeline from sklearn.compose import ColumnTransformer from sklearn.tree import DecisionTreeClassifier from sklearn.model_selection import train_test_split from sklearn.preprocessing import OneHotEncoder import numpy as np import pandas as pd # Dataset data = { \'feature1\': [1, 2, np.nan, 4, 5], \'feature2\': [np.nan, 2, 3, np.nan, 5], \'feature3\': [\'a\', np.nan, \'a\', \'b\', \'b\'], \'target\': [0, 1, 0, 1, 0] } df = pd.DataFrame(data) X = df[[\'feature1\', \'feature2\', \'feature3\']] y = df[\'target\'] numeric_features = [\'feature1\', \'feature2\'] categorical_features = [\'feature3\'] # Preprocessing for numerical data numeric_transformer = Pipeline(steps=[ (\'imputer\', SimpleImputer(strategy=\'mean\')), (\'indicator\', MissingIndicator()) ]) # Preprocessing for categorical data categorical_transformer = Pipeline(steps=[ (\'imputer\', SimpleImputer(strategy=\'most_frequent\')), (\'onehot\', OneHotEncoder(handle_unknown=\'ignore\')) ]) # Combine preprocessing steps preprocessor = ColumnTransformer( transformers=[ (\'num\', numeric_transformer, numeric_features), (\'cat\', categorical_transformer, categorical_features) ]) # Append classifier to preprocessing pipeline clf = Pipeline(steps=[ (\'preprocessor\', preprocessor), (\'classifier\', DecisionTreeClassifier()) ]) # Split data into train and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Train the pipeline clf.fit(X_train, y_train) # Evaluate the pipeline accuracy = clf.score(X_test, y_test) print(f\\"Accuracy: {accuracy * 100:.2f}%\\") # Example usage handle_missing_data_and_classify() ```","solution":"def handle_missing_data_and_classify(): from sklearn.impute import SimpleImputer, MissingIndicator from sklearn.pipeline import FeatureUnion, Pipeline from sklearn.compose import ColumnTransformer from sklearn.tree import DecisionTreeClassifier from sklearn.model_selection import train_test_split from sklearn.preprocessing import OneHotEncoder import numpy as np import pandas as pd # Dataset data = { \'feature1\': [1, 2, np.nan, 4, 5], \'feature2\': [np.nan, 2, 3, np.nan, 5], \'feature3\': [\'a\', np.nan, \'a\', \'b\', \'b\'], \'target\': [0, 1, 0, 1, 0] } df = pd.DataFrame(data) X = df[[\'feature1\', \'feature2\', \'feature3\']] y = df[\'target\'] numeric_features = [\'feature1\', \'feature2\'] categorical_features = [\'feature3\'] # Preprocessing for numerical data numeric_transformer = Pipeline(steps=[ (\'imputer\', SimpleImputer(strategy=\'mean\')), (\'indicator\', MissingIndicator(error_on_new=False)) ]) # Preprocessing for categorical data categorical_transformer = Pipeline(steps=[ (\'imputer\', SimpleImputer(strategy=\'most_frequent\')), (\'onehot\', OneHotEncoder(handle_unknown=\'ignore\')) ]) # Combine preprocessing steps preprocessor = ColumnTransformer( transformers=[ (\'num\', numeric_transformer, numeric_features), (\'cat\', categorical_transformer, categorical_features) ]) # Append classifier to preprocessing pipeline clf = Pipeline(steps=[ (\'preprocessor\', preprocessor), (\'classifier\', DecisionTreeClassifier(random_state=42)) ]) # Split data into train and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Train the pipeline clf.fit(X_train, y_train) # Evaluate the pipeline accuracy = clf.score(X_test, y_test) print(f\\"Accuracy: {accuracy * 100:.2f}%\\") # Example usage handle_missing_data_and_classify()"},{"question":"# Dynamic Python Code Interpreter **Objective**: Your task is to implement a function that receives Python code and executes it based on the type of input provided. The function should distinguish whether it is a complete program, an interactive input, or an expression, execute it, and return the appropriate output or error message. **Function Signature**: ```python def dynamic_python_interpreter(code: str, input_type: str) -> str: pass ``` **Parameters**: - `code` (str): The Python code to be executed. This could be a complete program, an interactive input, or an expression. - `input_type` (str): A string indicating the type of input. It can be `program`, `interactive`, or `expression`. **Expected Outputs**: - If `input_type` is `program`, the function should execute the complete Python program and return the output or any errors. - If `input_type` is `interactive`, the function should simulate the interactive execution of each line and return the combined output or any errors. - If `input_type` is `expression`, the function should evaluate the expression and return the result. **Constraints**: - Use built-in functions such as `exec()` and `eval()` for execution. - Handle multi-line inputs correctly, and maintain state for interactive inputs as a real interpreter would. - Ensure that any errors during execution are captured and returned as strings. **Examples**: 1. **Complete Program**: ```python code = def greet(name): return f\'Hello, {name}!\' print(greet(\'Alice\')) input_type = \'program\' print(dynamic_python_interpreter(code, input_type)) # Output: \'Hello, Alice!n\' ``` 2. **Interactive Input**: ```python code = a = 5 b = 10 c = a + b c input_type = \'interactive\' print(dynamic_python_interpreter(code, input_type)) # Output: \'15n\' ``` 3. **Expression**: ```python code = \\"3 + 4 * 2\\" input_type = \'expression\' print(dynamic_python_interpreter(code, input_type)) # Output: \'11\' ``` **Note**: The function should return all outputs as a single string, with each output or error message separated by a newline if there are multiple outputs/errors. **Advanced Challenge**: For additional complexity, you can implement state maintenance between multiple calls for interactive inputs, mimicking a persistent environment like a real interactive interpreter.","solution":"def dynamic_python_interpreter(code: str, input_type: str) -> str: import sys import traceback from io import StringIO def execute_program(code_to_run): buffer = StringIO() sys.stdout = buffer sys.stderr = buffer try: exec(code_to_run) except Exception as e: buffer.write(traceback.format_exc()) finally: sys.stdout = sys.__stdout__ sys.stderr = sys.__stderr__ return buffer.getvalue() def execute_interactive(code_to_run): buffer = StringIO() sys.stdout = buffer sys.stderr = buffer namespace = {} try: for line in code_to_run.splitlines(): exec(line, namespace) except Exception as e: buffer.write(traceback.format_exc()) finally: sys.stdout = sys.__stdout__ sys.stderr = sys.__stderr__ return buffer.getvalue() def eval_expression(code_to_run): try: result = eval(code_to_run) return str(result) except Exception as e: return traceback.format_exc() if input_type == \\"program\\": return execute_program(code) elif input_type == \\"interactive\\": return execute_interactive(code) elif input_type == \\"expression\\": return eval_expression(code) else: return \\"Invalid input type\\""},{"question":"**Python to C Conversion using Argument Clinic** # Context: You are working on enhancing the CPython codebase and need to utilize Argument Clinic to streamline the argument parsing for a couple of built-in functions. You will follow the Argument Clinic patterns to define input parameters, convert them appropriately using Argument Clinic\'s provided converters, and generate the corresponding C function implementation skeletons. # Task: Use Argument Clinic to convert a Python function to a C implementation that parses the arguments and handles different parameter types, default values, and return types. # Function Description: Convert the following Python function into a C implementation using Argument Clinic. The function called `math_operations.multiply_and_add` accepts three parameters: `x`, `y`, and `z`. The function multiplies `x` and `y` and adds `z` to the result. Python function signature: ```python def multiply_and_add(x: int, y: int, z: int = 0) -> int: Multiply x and y, then add z to the result. return x * y + z ``` # Requirements: 1. **Module and Function Definition**: Define the module and function using Argument Clinic input blocks. 2. **Parameter Converters**: Use the appropriate converters to handle the integer parameters, including a default value for `z`. 3. **Return Converter**: Implement the return type conversion so that the function returns an integer. 4. **Prototype and Implementation**: Generate the prototype and implementation code stubs. 5. **Docstring**: Include the proper docstring within the Argument Clinic input block. # Constraints: - All parameters are integers, and `z` has a default value of `0`. - Ensure that the return type is an integer. - Follow the Argument Clinic syntax and use both the `/` and `*` to distinguish positional-only and optional parameters, appropriately if needed. # Output: Provide the complete Argument Clinic block and the generated C implementation stubs with placeholder function bodies. # Example Completion: ```c /*[clinic input] module math_operations math_operations.multiply_and_add x: int y: int z: int = 0 / Multiply x and y, then add z to the result. [clinic start generated code]*/ static int math_operations_multiply_and_add_impl(int x, int y, int z) /*[clinic end generated code: output=xyz input=abc123]*/ { // Placeholder function body, actual implementation will go here. return x * y + z; } /* The following will be added to the relevant portion of the C file\'s method table */ #define MATH_OPERATIONS_MULTIPLY_AND_ADD_METHODDEF {\\"multiply_and_add\\", (PyCFunction)math_operations_multiply_and_add, METH_VARARGS, math_operations_multiply_and_add__doc__} /* Including the clinic generated code */ #include \\"clinic/math_operations.c.h\\" ```","solution":"def multiply_and_add(x: int, y: int, z: int = 0) -> int: Multiply x and y, then add z to the result. return x * y + z"},{"question":"**Question:** You are given a dataset `mpg` from the seaborn library, which contains data about car mileage, horsepower, weight, and origin among other attributes. Your task is to create a complex visualization using seaborn\'s `objects` interface that demonstrates your understanding of the package. **Instructions:** 1. Load the `mpg` dataset using `seaborn.load_dataset(\\"mpg\\")`. 2. Initialize a plot with `horsepower` on the x-axis and `mpg` (miles per gallon) on the y-axis. 3. Add a dot plot to this base plot: - Color the dots based on their `origin`. - Use different markers for different `origin` values. You should use at least three distinct markers. 4. Add a jitter to the dots to reflect local density better. 5. Set the transparency of the dots to 50%. 6. Scale the fill color of the dots by the `weight` of the cars using a binary color scale. 7. Ensure the plot is properly labeled with axis titles. **Expected Input and Output:** - **Input:** There are no direct inputs to this function. Your task is to write a script to generate the described plot. - **Output:** The function should output a plot as described above. You can display the plot inline in a Jupyter notebook environment. **Constraints:** - You should use the seaborn library for all plotting tasks. - Ensure to adhere to the attributes mentioned (color, markers, jitter, transparency, scaling). **Example Code Structure:** ```python import seaborn.objects as so from seaborn import load_dataset def create_complex_plot(): # Load dataset mpg = load_dataset(\\"mpg\\") # Initialize the plot plot = so.Plot(mpg, \\"horsepower\\", \\"mpg\\") # Configure and add the dot plot with all required properties plot.add(so.Dots(fillalpha=0.5), marker=\\"origin\\", color=\\"origin\\", fillcolor=\\"weight\\") .scale(marker=[\\"o\\", \\"x\\", \\"s\\"], fillcolor=\\"binary\\") .add(so.Jitter(0.25)) # Display the plot plot.show() # Calling the function to create and display the plot create_complex_plot() ```","solution":"import seaborn.objects as so from seaborn import load_dataset def create_complex_plot(): # Load dataset mpg = load_dataset(\\"mpg\\") # Initialize the plot plot = so.Plot(mpg, x=\\"horsepower\\", y=\\"mpg\\") # Configure and add the dot plot with all required properties plot.add(so.Dots(), jitter=0.25, alpha=0.5) .scale(marker={\\"USA\\": \\"o\\", \\"Japan\\": \\"x\\", \\"Europe\\": \\"s\\"}, color=\\"origin\\", fillcolor=\\"weight\\", fillcolor_scale=\\"binary\\") # Display the plot plot.label(x=\\"Horsepower\\", y=\\"Miles Per Gallon (mpg)\\", color=\\"Origin\\", fillcolor=\\"Weight\\") plot.show() # Calling the function to create and display the plot create_complex_plot()"},{"question":"Objective Create a custom interactive console that adds history logging and command suggestions, demonstrating your understanding of subclassing and extending the functionality of the `InteractiveConsole` class. Problem Statement You are required to implement a custom interactive console class `CustomInteractiveConsole` by extending the `InteractiveConsole` class from the `code` module. This custom console should have the following additional features: 1. **Command History Logging**: Save all commands entered during the session to a history list. 2. **Command Suggestion**: If the user enters an incomplete command, suggest completing the command based on the history. Specifications 1. **Class Definition**: - **Class Name**: `CustomInteractiveConsole` - **Superclass**: `InteractiveConsole` 2. **Attributes**: - **history**: A list to store all commands entered in the console. 3. **Methods**: - **init history attribute** - Override the `push` method to: - Log each command to the `history`. - Use `history` to provide command suggestions if the command is incomplete. - Define a method `suggest_completion` which: - Takes the current incomplete command as input. - Returns the longest prefix match from the `history` list as a suggestion. 4. **Input and Output**: - The class should interact with the user in a manner similar to the default Python interactive console, but with logging and suggestion features. - For simulation and testing, you can have a method to print session history. Constraints 1. Avoid using external libraries; implement all features using Python standard libraries only. 2. Ensure that the console can handle basic syntax errors gracefully using existing `InteractiveConsole` methods. Example Usage ```python console = CustomInteractiveConsole() console.interact(\\"Welcome to Custom Interactive Console\\") ``` During the session: - Entered command: `x = 10` - Entered command: `print(x` The console should: 1. Log the commands `[x = 10]` to the `history`. 2. Suggest: `print(x)` if the user presses tab or requests a suggestion. Notes - You may assume that the interactive session will not include multi-line commands for simplicity. - Test your implementation using a series of predefined commands to ensure logging and suggestion functionality.","solution":"import code class CustomInteractiveConsole(code.InteractiveConsole): def __init__(self, locals=None, filename=\\"<console>\\"): super(CustomInteractiveConsole, self).__init__(locals, filename) self.history = [] def push(self, line): self.history.append(line) suggestions = self.suggest_completion(line) if suggestions: print(f\\"Suggestion: {suggestions}\\") return super(CustomInteractiveConsole, self).push(line) def suggest_completion(self, line): for past_command in self.history: if past_command.startswith(line): return past_command return None"},{"question":"# PyTorch Environment Variable Configuration for MPS You are tasked with building a debugging and profiling tool to monitor and optimize the memory allocation behavior of PyTorch running on the Metal Performance Shaders (MPS) backend. Your tool should help in identifying and tuning memory management by configuring several PyTorch environment variables related to MPS. Task 1. Implement a function `configure_pytorch_mps(debug: bool, high_watermark: float, low_watermark: float) -> None` that configures the following environment variables: - `PYTORCH_DEBUG_MPS_ALLOCATOR` to 1 if `debug` is True, otherwise to 0. - `PYTORCH_MPS_HIGH_WATERMARK_RATIO` to the given `high_watermark` value. - `PYTORCH_MPS_LOW_WATERMARK_RATIO` to the given `low_watermark` value. 2. Implement a function `profile_mps(debug: bool) -> None` which performs the following: - Sets `PYTORCH_MPS_LOG_PROFILE_INFO` and `PYTORCH_MPS_TRACE_SIGNPOSTS` to suitable values for enabling verbose profiling if `debug` is True. - Executes a sample PyTorch operation, e.g., a tensor multiplication, and logs the MPS profiling information. Input and Output Formats - The `configure_pytorch_mps` function takes three parameters: - `debug` (bool): Whether to enable debug logging for MPS allocator. - `high_watermark` (float): The high watermark ratio for MPS allocator. - `low_watermark` (float): The low watermark ratio for MPS allocator. - The `profile_mps` function takes one parameter: - `debug` (bool): Whether to enable detailed profiling and trace signposts for MPS. Constraints and Limitations - Ensure the `high_watermark` is set within (0.0, 2.0] range. If it is out of this range, raise a `ValueError`. - Ensure the `low_watermark` is set within (0.0, high_watermark] range. If it is out of this range, raise a `ValueError`. - You may assume the required environment variable values and tensor operations will fit within typical runtime memory limits for your testing purposes. Example Usage ```python # Example usage of the configure_pytorch_mps function try: configure_pytorch_mps(debug=True, high_watermark=1.5, low_watermark=1.2) except ValueError as e: print(f\\"Configuration Error: {e}\\") # Example usage of the profile_mps function profile_mps(debug=True) ``` Implementing these functions will demonstrate your understanding of configuring and using environment variables in PyTorch for optimizing performance on MPS.","solution":"import os import torch def configure_pytorch_mps(debug: bool, high_watermark: float, low_watermark: float) -> None: if not (0.0 < high_watermark <= 2.0): raise ValueError(\\"high_watermark must be within (0.0, 2.0]\\") if not (0.0 < low_watermark <= high_watermark): raise ValueError(\\"low_watermark must be within (0.0, high_watermark]\\") os.environ[\'PYTORCH_DEBUG_MPS_ALLOCATOR\'] = \'1\' if debug else \'0\' os.environ[\'PYTORCH_MPS_HIGH_WATERMARK_RATIO\'] = str(high_watermark) os.environ[\'PYTORCH_MPS_LOW_WATERMARK_RATIO\'] = str(low_watermark) def profile_mps(debug: bool) -> None: os.environ[\'PYTORCH_MPS_LOG_PROFILE_INFO\'] = \'1\' if debug else \'0\' os.environ[\'PYTORCH_MPS_TRACE_SIGNPOSTS\'] = \'1\' if debug else \'0\' # Execute a sample tensor operation using MPS device = \'mps\' if torch.backends.mps.is_available() else \'cpu\' x = torch.ones((1000, 1000), device=device) y = torch.ones((1000, 1000), device=device) z = torch.matmul(x, y) # perform a tensor multiplication print(z)"},{"question":"In this exercise, you will demonstrate your understanding of the `zlib` module by implementing a Python class `ZlibUtility` which includes methods for compressing and decompressing data using zlib. Additionally, you will implement methods for calculating checksums using Adler-32 and CRC32. Your task is to ensure that the data can be compressed and decompressed accurately, and checksums can be calculated correctly for verification. # Class Definition: `ZlibUtility`: - **Methods**: 1. `compress_data(data: bytes, level: int = -1) -> bytes`: - Compresses the input `data` using the specified compression `level`. - Returns the compressed byte data. 2. `decompress_data(data: bytes, wbits: int = zlib.MAX_WBITS) -> bytes`: - Decompresses the input `data` using the specified `wbits`. - Returns the decompressed byte data. 3. `calculate_adler32(data: bytes, start_value: int = 1) -> int`: - Computes an Adler-32 checksum of the input `data`. - Returns the checksum as an unsigned integer. 4. `calculate_crc32(data: bytes, start_value: int = 0) -> int`: - Computes a CRC32 checksum of the input `data`. - Returns the checksum as an unsigned integer. # Input and Output: 1. **compress_data**: - **Input**: `data` - bytes object to be compressed, `level` - compression level (default is -1). - **Output**: Compressed byte data. 2. **decompress_data**: - **Input**: `data` - bytes object to be decompressed, `wbits` - size of the history buffer (default is `zlib.MAX_WBITS`). - **Output**: Decompressed byte data. 3. **calculate_adler32**: - **Input**: `data` - bytes object for checksum calculation, `start_value` - starting value of the checksum (default is 1). - **Output**: Adler-32 checksum as an unsigned integer. 4. **calculate_crc32**: - **Input**: `data` - bytes object for checksum calculation, `start_value` - starting value of the checksum (default is 0). - **Output**: CRC32 checksum as an unsigned integer. # Constraints: - The `data` input for compression and decompression methods should not exceed 10MB. # Example: ```python # Example usage: data = b\\"Example data to be compressed.\\" # Create an instance of ZlibUtility zlib_utility = ZlibUtility() compressed_data = zlib_utility.compress_data(data) print(compressed_data) # Outputs compressed data decompressed_data = zlib_utility.decompress_data(compressed_data) print(decompressed_data) # Outputs b\'Example data to be compressed.\' adler32_checksum = zlib_utility.calculate_adler32(data) print(adler32_checksum) # Outputs the Adler-32 checksum crc32_checksum = zlib_utility.calculate_crc32(data) print(crc32_checksum) # Outputs the CRC32 checksum ``` Ensure that your implementation handles the possible exceptions and edge cases appropriately based on the zlib module\'s behavior as described in the documentation.","solution":"import zlib class ZlibUtility: @staticmethod def compress_data(data: bytes, level: int = -1) -> bytes: Compresses the input data using the specified compression level. Returns the compressed byte data. return zlib.compress(data, level) @staticmethod def decompress_data(data: bytes, wbits: int = zlib.MAX_WBITS) -> bytes: Decompresses the input data using the specified wbits. Returns the decompressed byte data. return zlib.decompress(data, wbits) @staticmethod def calculate_adler32(data: bytes, start_value: int = 1) -> int: Computes an Adler-32 checksum of the input data. Returns the checksum as an unsigned integer. return zlib.adler32(data, start_value) @staticmethod def calculate_crc32(data: bytes, start_value: int = 0) -> int: Computes a CRC32 checksum of the input data. Returns the checksum as an unsigned integer. return zlib.crc32(data, start_value)"},{"question":"**Question: Custom Pickler and Unpickler Implementation** **Background**: `pickle` is a Python module used for serializing and deserializing Python object structures. This process allows you to save a Python object hierarchy into a byte stream and later reconstruct it. However, `pickle` can face challenges when dealing with complex and custom class instances. **Objective**: Write a custom `Pickler` and `Unpickler` along with custom methods in a class to manage its pickling process. **Problem Statement**: You are required to implement a class `ComplexObject` and provide custom `Pickler` and `Unpickler` for it. 1. Implement a class `ComplexObject` with the following attributes: - `name` (str) - `value` (int) - `data` (dict) 2. Override the `__getstate__` and `__setstate__` methods of `ComplexObject` to control what gets pickled and how the object is reconstituted during unpickling. 3. Implement a custom `Pickler` named `ComplexObjectPickler` that handles the pickling process of `ComplexObject`. Ensure it properly uses the `persistent_id` method to handle an attribute `data`. 4. Implement a custom `Unpickler` named `ComplexObjectUnpickler` that handles the unpickling process of `ComplexObject`. 5. Provide an example that demonstrates: - Creating an instance of `ComplexObject`. - Pickling the instance using `ComplexObjectPickler`. - Unpickling the instance using `ComplexObjectUnpickler`. - Ensuring the state is maintained for the `data` attribute. **Constraints**: - Ensure the `data` attribute can only contain dictionaries with string keys and integer values. - Any invalid data structure should raise an appropriate error during pickling. **Expected Input and Output**: Example usage: ```python # Create an instance of ComplexObject obj = ComplexObject(\\"example\\", 42, {\\"key1\\": 1, \\"key2\\": 2}) # Create a bytes buffer to hold the pickle data buffer = io.BytesIO() # Pickle the object using ComplexObjectPickler ComplexObjectPickler(buffer).dump(obj) # Move the buffer\'s cursor to the beginning buffer.seek(0) # Unpickle the object using ComplexObjectUnpickler unpickled_obj = ComplexObjectUnpickler(buffer).load() # Validate the state assert obj.name == unpickled_obj.name assert obj.value == unpickled_obj.value assert obj.data == unpickled_obj.data ``` **Your Tasks**: 1. Implement the `ComplexObject` class with custom `__getstate__` and `__setstate__` methods. 2. Implement the `ComplexObjectPickler` class with custom `persistent_id` handling. 3. Implement the `ComplexObjectUnpickler` class with custom `persistent_load` handling. 4. Provide an example demonstrating the full process with validation. Remember, attention to detail is key when dealing with serialization and deserialization processes. Ensure all aspects of the object\'s state are correctly managed.","solution":"import pickle class ComplexObject: def __init__(self, name, value, data): # Validate that data is a dictionary with string keys and integer values if not isinstance(data, dict) or not all(isinstance(k, str) and isinstance(v, int) for k, v in data.items()): raise ValueError(\\"data must be a dictionary with string keys and integer values\\") self.name = name self.value = value self.data = data def __getstate__(self): # Return state as a dictionary state = self.__dict__.copy() return state def __setstate__(self, state): # Restore instance attributes self.__dict__.update(state) class ComplexObjectPickler(pickle.Pickler): def persistent_id(self, obj): # Custom handling for the \'data\' attribute if isinstance(obj, ComplexObject): return (\'ComplexObject\', obj.__getstate__()) return None class ComplexObjectUnpickler(pickle.Unpickler): def persistent_load(self, pid): # Custom handling for loading \'data\' type_tag, state = pid if type_tag == \'ComplexObject\': obj = ComplexObject.__new__(ComplexObject) obj.__setstate__(state) return obj raise pickle.UnpicklingError(\\"unsupported persistent object\\") # Example usage import io # Create an instance of ComplexObject obj = ComplexObject(\\"example\\", 42, {\\"key1\\": 1, \\"key2\\": 2}) # Create a bytes buffer to hold the pickle data buffer = io.BytesIO() # Pickle the object using ComplexObjectPickler ComplexObjectPickler(buffer).dump(obj) # Move the buffer\'s cursor to the beginning buffer.seek(0) # Unpickle the object using ComplexObjectUnpickler unpickled_obj = ComplexObjectUnpickler(buffer).load() # Validate the state assert obj.name == unpickled_obj.name assert obj.value == unpickled_obj.value assert obj.data == unpickled_obj.data"},{"question":"# Question: Implementing an OptionParser for a Command-Line Tool You are tasked with creating a command-line tool called `data_processor` using the `optparse` module. This tool processes data files and provides users with various options to configure its behavior. You must implement an option parser that supports the following functionality: 1. **File Input**: - Option: `-i` or `--input` - Specifies the input file to process. This option should be mandatory. 2. **Output File**: - Option: `-o` or `--output` - Specifies the file to save the output. If not provided, the output should be saved to `output.txt`. 3. **Verbose Mode**: - Option: `-v` or `--verbose` - Enables verbose mode, printing detailed information during processing. 4. **Quiet Mode**: - Option: `-q` or `--quiet` - Enables quiet mode, suppressing all output except for critical errors. 5. **Processing Mode**: - Option: `-m` or `--mode` - Specifies the processing mode: `fast`, `normal`, or `thorough`. It should default to `normal`. 6. **Version Information**: - Option: `--version` - Prints the version of the tool and exits. # Implementation Details: - Define an `OptionParser` instance and add the options described above. - Ensure that the `--input` option is mandatory. - Handle conflicting options: If both `--verbose` and `--quiet` are specified, the tool should exit with an appropriate error message. - Provide clear, user-friendly help messages for each option. - Implement any necessary error handling for invalid or missing options. - Create a main processing function that uses the parsed options to simulate data processing. # Example Usage: ```bash # Run with verbose mode and specify input and output files python data_processor.py -i data.txt -o result.txt -v # Run with default settings and print the version python data_processor.py --version # Run in quiet mode with fast processing mode python data_processor.py --input=data.txt --mode=fast --quiet ``` # Function Signature ```python def main(): pass if __name__ == \\"__main__\\": main() ``` You need to implement the `main` function, which will handle command-line arguments using the `optparse` module and perform corresponding actions.","solution":"import sys from optparse import OptionParser def configure_parser(): parser = OptionParser(usage=\\"usage: %prog [options]\\", version=\\"%prog 1.0\\") parser.add_option(\\"-i\\", \\"--input\\", dest=\\"input\\", help=\\"Specify the input file\\", metavar=\\"FILE\\") parser.add_option(\\"-o\\", \\"--output\\", dest=\\"output\\", default=\\"output.txt\\", help=\\"Specify the output file [default: %default]\\", metavar=\\"FILE\\") parser.add_option(\\"-v\\", \\"--verbose\\", action=\\"store_true\\", dest=\\"verbose\\", default=False, help=\\"Enable verbose mode\\") parser.add_option(\\"-q\\", \\"--quiet\\", action=\\"store_true\\", dest=\\"quiet\\", default=False, help=\\"Enable quiet mode\\") parser.add_option(\\"-m\\", \\"--mode\\", dest=\\"mode\\", default=\\"normal\\", help=\\"Specify the processing mode: fast, normal, or thorough [default: %default]\\", metavar=\\"MODE\\") return parser def main(): parser = configure_parser() (options, args) = parser.parse_args() if not options.input: parser.error(\\"Input file not specified. Use -i or --input to specify the input file.\\") if options.verbose and options.quiet: parser.error(\\"Options --verbose and --quiet are mutually exclusive.\\") process_data(options) def process_data(options): if options.verbose: print(f\\"Processing {options.input} with mode {options.mode}...\\") print(f\\"Output will be saved to {options.output}\\") # Simulate data processing if options.mode == \\"fast\\": if not options.quiet: print(\\"Processing in fast mode...\\") elif options.mode == \\"normal\\": if not options.quiet: print(\\"Processing in normal mode...\\") elif options.mode == \\"thorough\\": if not options.quiet: print(\\"Processing in thorough mode...\\") else: sys.stderr.write(\\"Invalid processing mode specified.n\\") sys.exit(1) if not options.quiet: print(f\\"Data processing complete. Output saved to {options.output}\\") if __name__ == \\"__main__\\": main()"},{"question":"Objective: To assess your understanding of scikit-learn’s datasets and your ability to preprocess data, implement machine learning algorithms, and evaluate model performance. Question: You are provided with a toy dataset called the Iris dataset, which is included in scikit-learn’s default datasets. Your task is to perform the following: 1. Load the Iris dataset from `sklearn.datasets`. 2. Preprocess the dataset to prepare it for a machine learning algorithm. This includes: - Splitting the dataset into features (`X`) and target (`y`). - Standardizing the features to have mean 0 and variance 1. - Splitting the dataset into training and testing sets (80% training and 20% testing). 3. Implement a K-Nearest Neighbors (KNN) classifier. Train the model on the training set. 4. Evaluate the model on the testing set and print out the accuracy. 5. Print out a small classification report (precision, recall, and F1-score) for each class. Input: None. The dataset is to be loaded using `sklearn.datasets.load_iris()`. Output: - Print the accuracy of the KNN classifier on the testing set. - Print a classification report (precision, recall, and F1-score) for each class. Constraints: - The KNN classifier should use default parameters. - Standardize the features using `StandardScaler` from `sklearn.preprocessing`. - Use `train_test_split` from `sklearn.model_selection` to split the dataset. Performance Requirements: - The entire process should complete in a reasonable time frame, as the Iris dataset is small. Example Output: ``` Accuracy: 0.95 Classification Report: precision recall f1-score support 0 0.93 1.00 0.97 10 1 0.97 0.94 0.95 16 2 0.97 0.93 0.95 14 accuracy 0.95 40 macro avg 0.96 0.96 0.96 40 weighted avg 0.95 0.95 0.95 40 ``` Hints: - Refer to the scikit-learn documentation for usage examples of `train_test_split`, `StandardScaler`, and KNN implementation. - Make use of `classification_report` from `sklearn.metrics` to generate the classification report.","solution":"from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.neighbors import KNeighborsClassifier from sklearn.metrics import accuracy_score, classification_report def iris_knn_classification(): # Load the Iris dataset iris = load_iris() X, y = iris.data, iris.target # Standardize the features scaler = StandardScaler() X = scaler.fit_transform(X) # Split the dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Implement and train the KNN classifier knn = KNeighborsClassifier() knn.fit(X_train, y_train) # Evaluate the model y_pred = knn.predict(X_test) accuracy = accuracy_score(y_test, y_pred) report = classification_report(y_test, y_pred, target_names=iris.target_names) # Print the accuracy and classification report print(f\\"Accuracy: {accuracy:.2f}\\") print(\\"Classification Report:\\") print(report) return accuracy, report"},{"question":"# PyTorch TorchScript Coding Assessment Objective Demonstrate your comprehension of PyTorch\'s TorchScript by scripting a custom neural network module and leveraging TorchScript\'s built-in functionalities. Problem Statement You are required to implement a custom neural network module in PyTorch, then script it using TorchScript. Your task will involve: 1. Creating a simple Feed-Forward Neural Network (FFNN) module. 2. Scripting the module using TorchScript. 3. Demonstrating the scripted model\'s functionality. Instructions 1. **Define the Neural Network:** - Create a custom class `SimpleFFNN` which inherits from `torch.nn.Module`. - The network should consist of: - One input layer. - Two hidden layers with ReLU activation. - One output layer. 2. **Script the Module:** - Script the neural network module using TorchScript\'s `torch.jit.script`. 3. **Demonstrate Functionality:** - Create an instance of your `SimpleFFNN` model. - Run a forward pass with a random tensor input of shape `(1, 10)` through both the non-scripted and scripted models. - Verify that both models produce the same output. Input and Output Requirements - **Input:** - You don\'t need to take any user input for this task. Your class and method implementations should be self-contained. - **Output:** - Print the outputs from both the non-scripted and scripted versions of the model for comparison. Constraints - Use only PyTorch\'s built-in functions. - Ensure your model is compatible with TorchScript scripting. Example ```python import torch import torch.nn as nn # 1. Define the Neural Network class SimpleFFNN(nn.Module): def __init__(self): super(SimpleFFNN, self).__init__() self.layer1 = nn.Linear(10, 50) self.layer2 = nn.Linear(50, 50) self.layer3 = nn.Linear(50, 1) def forward(self, x): x = torch.relu(self.layer1(x)) x = torch.relu(self.layer2(x)) x = self.layer3(x) return x # 2. Script the Module model = SimpleFFNN() scripted_model = torch.jit.script(model) # 3. Demonstrate Functionality input_tensor = torch.randn(1, 10) output_non_scripted = model(input_tensor) output_scripted = scripted_model(input_tensor) print(f\'Output from non-scripted model: {output_non_scripted}\') print(f\'Output from scripted model: {output_scripted}\') assert torch.allclose(output_non_scripted, output_scripted), \\"Mismatch between scripted and non-scripted model outputs\\" ```","solution":"import torch import torch.nn as nn # 1. Define the Neural Network class SimpleFFNN(nn.Module): def __init__(self): super(SimpleFFNN, self).__init__() self.layer1 = nn.Linear(10, 50) self.layer2 = nn.Linear(50, 50) self.layer3 = nn.Linear(50, 1) def forward(self, x): x = torch.relu(self.layer1(x)) x = torch.relu(self.layer2(x)) x = self.layer3(x) return x # 2. Script the Module model = SimpleFFNN() scripted_model = torch.jit.script(model) # 3. Demonstrate Functionality input_tensor = torch.randn(1, 10) output_non_scripted = model(input_tensor) output_scripted = scripted_model(input_tensor) print(f\'Output from non-scripted model: {output_non_scripted}\') print(f\'Output from scripted model: {output_scripted}\') assert torch.allclose(output_non_scripted, output_scripted), \\"Mismatch between scripted and non-scripted model outputs\\""},{"question":"**Objective:** Implement a probability calibration process for a given classifier on a synthetic dataset and plot the calibration curve to visually inspect the calibration. **Problem Statement:** You are provided with a synthetic dataset and a classifier. Your task is to: 1. Train the classifier. 2. Calibrate the classifier to improve the predicted probabilities. 3. Plot the calibration curve to visualize the performance of the calibrated classifier compared to the uncalibrated one. 4. Implement functions to calculate and plot the Brier score loss before and after calibration. **Dataset:** Generate a synthetic binary classification dataset using `make_classification` from `sklearn.datasets`. **Classifier:** Use `RandomForestClassifier` from `sklearn.ensemble`. **Calibration Method:** Use `CalibratedClassifierCV` with both \'sigmoid\' and \'isotonic\' methods. **Requirements:** 1. Implement a function `calibrate_and_evaluate` with the following input and output: ```python def calibrate_and_evaluate(X, y): # X: numpy array of shape (n_samples, n_features), Input features # y: numpy array of shape (n_samples,), Target labels # return: None pass ``` 2. Inside `calibrate_and_evaluate`: - Split data into training and test sets. - Train a `RandomForestClassifier` on the training set. - Calibrate the trained classifier using `CalibratedClassifierCV` with both \'sigmoid\' and \'isotonic\' methods. - Plot and compare the calibration curves of both the uncalibrated and calibrated classifiers. - Calculate and print the Brier score loss for both the uncalibrated and calibrated classifiers on the test set. **Performance Constraints:** - Ensure that the calibration methods properly handle the dataset without any runtime errors. - The calibration should improve or at least make the probability estimates more reasonable based on the visual inspection from the calibration curve. **Example Usage:** ```python from sklearn.datasets import make_classification X, y = make_classification(n_samples=1000, n_features=20, random_state=42) calibrate_and_evaluate(X, y) ``` **Evaluation:** Your implementation will be evaluated based on: - Correct splitting of data. - Proper training and calibration of classifiers. - Accuracy of the calibration as demonstrated by the Brier score loss. - Correct plotting of calibration curves. **Additional Notes:** - You may use any relevant functions and modules from `sklearn` and `matplotlib` for plotting. - Include comments in your code to explain each step.","solution":"import numpy as np import matplotlib.pyplot as plt from sklearn.datasets import make_classification from sklearn.ensemble import RandomForestClassifier from sklearn.calibration import CalibratedClassifierCV, calibration_curve from sklearn.metrics import brier_score_loss from sklearn.model_selection import train_test_split def calibrate_and_evaluate(X, y): Calibrates a RandomForestClassifier and evaluates it using Brier score and calibration curves. Parameters: X (numpy.ndarray): Input features of shape (n_samples, n_features) y (numpy.ndarray): Target labels of shape (n_samples,) Returns: None # Split the data into training and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Train a RandomForestClassifier clf = RandomForestClassifier(n_estimators=100, random_state=42) clf.fit(X_train, y_train) # Uncalibrated predictions prob_pos_clf = clf.predict_proba(X_test)[:, 1] # Calibrate the classifier using \'sigmoid\' clf_sigmoid = CalibratedClassifierCV(clf, method=\'sigmoid\', cv=\'prefit\') clf_sigmoid.fit(X_train, y_train) prob_pos_sigmoid = clf_sigmoid.predict_proba(X_test)[:, 1] # Calibrate the classifier using \'isotonic\' clf_isotonic = CalibratedClassifierCV(clf, method=\'isotonic\', cv=\'prefit\') clf_isotonic.fit(X_train, y_train) prob_pos_isotonic = clf_isotonic.predict_proba(X_test)[:, 1] # Compute Brier scores brier_unclb = brier_score_loss(y_test, prob_pos_clf) brier_sigmoid = brier_score_loss(y_test, prob_pos_sigmoid) brier_isotonic = brier_score_loss(y_test, prob_pos_isotonic) print(\\"Brier scores:\\") print(f\\"Uncalibrated classifier: {brier_unclb:.3f}\\") print(f\\"Sigmoid calibrated classifier: {brier_sigmoid:.3f}\\") print(f\\"Isotonic calibrated classifier: {brier_isotonic:.3f}\\") # Calculate calibration curves prob_true_clf, prob_pred_clf = calibration_curve(y_test, prob_pos_clf, n_bins=10) prob_true_sigmoid, prob_pred_sigmoid = calibration_curve(y_test, prob_pos_sigmoid, n_bins=10) prob_true_isotonic, prob_pred_isotonic = calibration_curve(y_test, prob_pos_isotonic, n_bins=10) # Plot calibration curves plt.figure(figsize=(10, 10)) plt.plot(prob_pred_clf, prob_true_clf, \'s-\', label=\'Uncalibrated\') plt.plot(prob_pred_sigmoid, prob_true_sigmoid, \'s-\', label=\'Sigmoid-calibrated\') plt.plot(prob_pred_isotonic, prob_true_isotonic, \'s-\', label=\'Isotonic-calibrated\') plt.plot([0, 1], [0, 1], \'k:\', label=\'Perfectly calibrated\') plt.xlabel(\'Mean predicted probability\') plt.ylabel(\'Fraction of positives\') plt.title(\'Calibration plots (reliability curves)\') plt.legend() plt.show()"},{"question":"# Pandas Coding Assessment: Testing DataFrame Transformations Objective Your task is to implement a function that will transform a given DataFrame by normalizing all its numeric columns. You will also need to write tests to verify the correctness of this function using pandas\' testing utilities. Function Specification **Function Name:** `normalize_df` **Parameters:** - `df`: pandas DataFrame. The input DataFrame containing various columns of numeric data. **Returns:** - A pandas DataFrame where each numeric column is normalized such that its values range between 0 and 1. Non-numeric columns should remain unchanged. **Constraints:** - You may only use pandas and numpy libraries. - Ensure that the function preserves the original order of columns. - If all values in a numeric column are the same, the normalized values should be set to 0 for that column, as normalization won\'t be possible in the usual sense. Testing Specification In addition to the `normalize_df` function, write unit tests to verify its correctness. Use the pandas testing utilities to compare DataFrames. **Function Name:** `test_normalize_df` **Tests to Include:** 1. Test the function with a DataFrame containing multiple numeric columns. 2. Test the function with a DataFrame containing a mix of numeric and non-numeric columns. 3. Test the function with a DataFrame where all values in at least one column are the same. 4. Test the function with an empty DataFrame. **Example Usage:** ```python import pandas as pd import numpy as np from pandas import testing def normalize_df(df): df_numeric = df.select_dtypes(include=[np.number]) df_normalized = (df_numeric - df_numeric.min()) / (df_numeric.max() - df_numeric.min()) df_normalized.fillna(0, inplace=True) df_non_numeric = df.select_dtypes(exclude=[np.number]) return pd.concat([df_normalized, df_non_numeric], axis=1)[df.columns] def test_normalize_df(): df = pd.DataFrame({ \'A\': [1, 2, 3, 4, 5], \'B\': [10, 20, 30, 40, 50], \'C\': [\'x\', \'y\', \'z\', \'w\', \'v\'] }) expected_df = pd.DataFrame({ \'A\': [0.0, 0.25, 0.5, 0.75, 1.0], \'B\': [0.0, 0.25, 0.5, 0.75, 1.0], \'C\': [\'x\', \'y\', \'z\', \'w\', \'v\'] }) testing.assert_frame_equal(normalize_df(df), expected_df) # Add more tests as described in the Testing Specification section test_normalize_df() ``` **Note:** You don\'t need to implement the tests beyond the provided example. However, ensure that you cover all scenarios outlined in the Testing Specification.","solution":"import pandas as pd import numpy as np def normalize_df(df): df_numeric = df.select_dtypes(include=[np.number]) df_normalized = (df_numeric - df_numeric.min()) / (df_numeric.max() - df_numeric.min()) df_normalized.fillna(0, inplace=True) df_non_numeric = df.select_dtypes(exclude=[np.number]) return pd.concat([df_normalized, df_non_numeric], axis=1)[df.columns]"},{"question":"Objective Your task is to demonstrate your understanding of Seaborn\'s capabilities by customizing visuals and scaling elements of plots in a meaningful way. This question will assess your ability to apply different themes, styles, and scaling context to various types of plots. Question You are given a dataset representing the average annual temperatures (in Celsius) of five major cities over a span of 20 years. Write a Python function using Seaborn and Matplotlib that: 1. Plots a line chart for each city\'s temperature data using the `sinplot()` function. 2. Customizes the appearance of the plots by applying different Seaborn styles and themes. 3. Scales the plot elements for two different contexts - \'notebook\' and \'poster\'. Your function should: - Use at least three different Seaborn styles for the plots in a single figure. - Temporarily set these styles using the `with` statement. - Apply the \'notebook\' context with font scaling and a specific line width for one set of plots. - Apply the \'poster\' context with different scaling for another set of plots. - Remove the top and right spines for the plots. Expected Input and Output Formats ```python def temperature_plots(temperature_data: dict): temperature_data: A dictionary where keys are city names and values are lists of average annual temperatures (length 20). pass # Example usage: temperature_data = { \\"CityA\\": [15.6, 16.1, 14.8, ...], # 20 temperature values \\"CityB\\": [12.4, 13.0, 12.8, ...], # 20 temperature values \\"CityC\\": [23.5, 22.9, 23.1, ...], # 20 temperature values \\"CityD\\": [18.6, 19.2, 18.8, ...], # 20 temperature values \\"CityE\\": [10.0, 10.5, 10.3, ...] # 20 temperature values } temperature_plots(temperature_data) ``` The function should not return anything. However, it should display the customized plots as specified. Constraints and Notes - Ensure that the X-axis represents the years (0 to 19). - The Y-axis should represent temperature values. - Use colors and markers to differentiate between the cities. - Performance: The function should efficiently handle the plotting and customization tasks. Example Your function should produce a figure similar to the following outline: - Top-left quadrant: Line plots with `darkgrid` style. - Top-right quadrant: Line plots with `white` style. - Bottom-left quadrant: Line plots with `ticks` style. - Bottom-right quadrant: Line plots in \'poster\' context with increased font scaling and line width.","solution":"import matplotlib.pyplot as plt import seaborn as sns def temperature_plots(temperature_data: dict): Plots line charts for average annual temperatures of cities with different styles and contexts. temperature_data: A dictionary where keys are city names and values are lists of average annual temperatures (length 20). # Generate the x-axis values representing the years years = list(range(20)) # Define the styles to be used styles = [\'darkgrid\', \'white\', \'ticks\'] # Create a 2x2 grid for subplots fig, axs = plt.subplots(2, 2, figsize=(14, 10)) # Plot using three different styles for i, style in enumerate(styles): with sns.axes_style(style): row, col = divmod(i, 2) ax = axs[row, col] for city, temps in temperature_data.items(): sns.lineplot(x=years, y=temps, label=city, ax=ax) ax.set_title(f\'Seaborn style: {style}\') sns.despine(ax=ax, top=True, right=True) # Remove top and right spines # Plot using the \'poster\' context with sns.plotting_context(\'poster\', font_scale=1.2): ax = axs[1, 1] for city, temps in temperature_data.items(): sns.lineplot(x=years, y=temps, label=city, ax=ax) ax.set_title(\'Seaborn context: poster with scaling\') sns.despine(ax=ax, top=True, right=True) # Remove top and right spines plt.tight_layout() plt.show()"},{"question":"**Turtle Graphics Challenge** # Objective In this challenge, you are required to create an intricate design using the turtle graphics module in Python. Your task is to implement a function `draw_design` that uses various turtle methods to produce a predefined pattern. # Specifications 1. **Function Name:** `draw_design` 2. **Input:** None 3. **Output:** None 4. **Constraints:** - You must utilize at least the following turtle methods: `forward()`, `right()`, `left()`, `penup()`, `pendown()`, and `color()`. - Your design should include at least three different geometrical shapes (e.g., square, triangle, circle). - The design must be colorful, using at least three different colors. - The design should be drawn on a single screen with a white background. # Expected Functionality Your `draw_design` function should do the following: 1. Set up the turtle screen with a white background. 2. Use turtle methods to draw at least three geometrical shapes. 3. Ensure the shapes are drawn in different colors. 4. The turtle should use penup and pendown actions appropriately to move between drawing shapes without leaving unintended lines. 5. The turtle should return to the home position (0, 0) at the end of the function. # Example Here is an example to get you started: ```python from turtle import Screen, Turtle def draw_design(): screen = Screen() screen.bgcolor(\'white\') turtle = Turtle() # Drawing a yellow square turtle.color(\'yellow\') for _ in range(4): turtle.forward(100) turtle.right(90) turtle.penup() turtle.goto(-150, 0) turtle.pendown() # Drawing a red triangle turtle.color(\'red\') for _ in range(3): turtle.forward(100) turtle.left(120) turtle.penup() turtle.goto(150, 0) turtle.pendown() # Drawing a blue circle turtle.color(\'blue\') turtle.circle(50) # Return to home position turtle.penup() turtle.goto(0, 0) screen.mainloop() # Call the function to see the result draw_design() ``` In the above example: - A yellow square is drawn at the center. - A red triangle is drawn to the left of the square. - A blue circle is drawn to the right of the square. - The turtle returns to the home position (0, 0) after drawing. Ensure your design is more complex and includes three different shapes drawn in different colors. Good luck!","solution":"from turtle import Screen, Turtle def draw_design(): screen = Screen() screen.bgcolor(\'white\') turtle = Turtle() # Drawing a green square turtle.color(\'green\') for _ in range(4): turtle.forward(100) turtle.right(90) turtle.penup() turtle.goto(-150, 0) turtle.pendown() # Drawing an orange triangle turtle.color(\'orange\') for _ in range(3): turtle.forward(100) turtle.left(120) turtle.penup() turtle.goto(150, 0) turtle.pendown() # Drawing a purple circle turtle.color(\'purple\') turtle.circle(50) # Drawing a cyan pentagon turtle.penup() turtle.goto(0, -150) turtle.pendown() turtle.color(\'cyan\') for _ in range(5): turtle.forward(100) turtle.left(72) # Drawing a yellow hexagon turtle.penup() turtle.goto(-150, -150) turtle.pendown() turtle.color(\'yellow\') for _ in range(6): turtle.forward(80) turtle.left(60) # Return to home position turtle.penup() turtle.goto(0, 0) screen.mainloop()"},{"question":"# Functional Programming in Python Problem Statement You are tasked with implementing a Python function that processes a stream of data using various functional programming constructs including iterators, generators, and functionalities from the itertools module. The function to implement is `rolling_window_average(data, window_size)` that will take in: 1. An iterable `data` representing a stream of numerical data. 2. An integer `window_size` which determines the size of the rolling window over which we will compute the average. The function will return a generator that yields the rolling window averages. The rolling window average is calculated by taking the average of the latest `window_size` elements in the stream. If the number of elements provided is less than `window_size` at any given time, the average should be computed over the existing elements. Example For example, for the input `data = [10, 20, 30, 40, 50]` and `window_size = 3`, the output should be an iterator that yields the values `[10.0, 15.0, 20.0, 30.0, 40.0]`. **Explanation:** - The first element average is `10 / 1 = 10.0` - The second element average is `(10 + 20) / 2 = 15.0` - The third element average is `(10 + 20 + 30) / 3 = 20.0` - The fourth element average is `(20 + 30 + 40) / 3 = 30.0` - The fifth element average is `(30 + 40 + 50) / 3 = 40.0` Constraints - The input `data` will be an iterable containing numerical values. - The `window_size` will be a positive integer. Performance Your solution should handle large inputs efficiently and utilize Python\'s functional programming constructs where possible. Function Signature ```python def rolling_window_average(data, window_size): pass ``` Implementation Notes 1. You should utilize iterators to handle large sequences efficiently. 2. Use the `itertools` module where appropriate (e.g., use `itertools.islice` to handle slices of the iterator). 3. Use generator functions to yield results one at a time instead of returning a list. 4. Avoid using explicit loops or list comprehensions for the main logic, focus on functional programming constructs.","solution":"from itertools import islice def rolling_window_average(data, window_size): Returns a generator that yields rolling window averages of given size for the input data stream. :param data: An iterable representing a stream of numerical data. :param window_size: An integer representing the size of the rolling window. :return: A generator yielding the rolling window averages. it = iter(data) current_elements = [] for i in range(window_size): try: current_elements.append(next(it)) except StopIteration: break yield sum(current_elements) / len(current_elements) for value in it: current_elements = current_elements[1:] + [value] yield sum(current_elements) / window_size"},{"question":"# Pandas Data Manipulation and Analysis Problem Statement You are given a dataset of sales records stored in a pandas DataFrame. Each row in the DataFrame represents a single transaction and includes the following columns: - `transaction_id`: A unique identifier for the transaction. - `product_id`: A unique identifier for the product. - `quantity`: The quantity of the product sold in the transaction. - `price_per_unit`: The price per unit of the product. - `transaction_date`: The date when the transaction occurred. - `customer_id`: A unique identifier for the customer. - `salesperson_id`: A unique identifier for the salesperson handling the transaction. Your goal is to perform the following tasks: 1. **Calculate Total Sales**: - Calculate the total sales amount for each transaction and store it in a new column called `total_sales` (total_sales = quantity * price_per_unit). 2. **Salesperson Performance**: - Group the data by `salesperson_id` and calculate the total sales made by each salesperson. - Identify the top 3 salespersons with the highest total sales. 3. **Monthly Sales Trends**: - Extract the month and year from the `transaction_date` column and store it in a new column called `month_year` in the format \'YYYY-MM\'. - Group the data by `month_year` and calculate the total sales for each month. 4. **Missing Data Handling**: - If there are any missing values in the `quantity`, `price_per_unit`, or `transaction_date` columns, fill them with the median value of their respective columns. 5. **Product Popularity**: - Determine the top 5 products with the highest total sales amount. Input Format A pandas DataFrame `df` with the columns specified above. Output Format A dictionary containing: - `total_sales_df`: The updated DataFrame with the `total_sales` column. - `top_salespersons`: A DataFrame containing the top 3 salespersons with their total sales. - `monthly_sales`: A DataFrame containing the total sales for each month. - `top_products`: A DataFrame containing the top 5 products with the highest total sales amount. Function Signature ```python def analyze_sales_data(df: pd.DataFrame) -> dict: # Implementation goes here ``` Constraints - Your solution should handle large datasets efficiently. - You may assume that the `transaction_date` column is in a standard date format (e.g., \'YYYY-MM-DD\').","solution":"import pandas as pd def analyze_sales_data(df: pd.DataFrame) -> dict: # Task 1: Calculate Total Sales df[\'total_sales\'] = df[\'quantity\'] * df[\'price_per_unit\'] # Task 2: Salesperson Performance salesperson_performance = df.groupby(\'salesperson_id\')[\'total_sales\'].sum().reset_index() top_salespersons = salesperson_performance.nlargest(3, \'total_sales\') # Task 3: Monthly Sales Trends df[\'transaction_date\'] = pd.to_datetime(df[\'transaction_date\']) df[\'month_year\'] = df[\'transaction_date\'].dt.to_period(\'M\').astype(str) monthly_sales = df.groupby(\'month_year\')[\'total_sales\'].sum().reset_index() # Task 4: Missing Data Handling for column in [\'quantity\', \'price_per_unit\', \'transaction_date\']: if df[column].isnull().any(): median_value = df[column].median() df[column].fillna(median_value, inplace=True) # Task 5: Product Popularity product_sales = df.groupby(\'product_id\')[\'total_sales\'].sum().reset_index() top_products = product_sales.nlargest(5, \'total_sales\') # Return results in a dictionary return { \'total_sales_df\': df, \'top_salespersons\': top_salespersons, \'monthly_sales\': monthly_sales, \'top_products\': top_products }"},{"question":"# Question **Objective**: To assess your understanding of Python\'s `io` module, you are required to implement a custom class for handling in-memory binary streams. This class should mimic the behavior of Python\'s `BytesIO` class and provide some additional functionalities. # Task Create a custom class named `CustomBytesIO` that implements an in-memory binary stream. Your class should inherit from `io.BufferedIOBase` and provide the following methods: 1. **Constructor**: ```python def __init__(self, initial_bytes: bytes= b\'\'): ``` Initializes the stream with optional initial binary data. 2. **read(size=-1)**: Reads up to `size` bytes from the stream. If `size` is `-1`, reads until EOF. 3. **write(b)**: Writes the given bytes-like object to the stream and returns the number of bytes written. 4. **seek(offset, whence=io.SEEK_SET)**: Changes the stream position to the given byte `offset`. `whence` can be `io.SEEK_SET` (default), `io.SEEK_CUR`, or `io.SEEK_END`. 5. **tell()**: Returns the current stream position. 6. **getvalue()**: Returns `bytes` containing the entire contents of the buffer. # Constraints 1. The class should handle edge cases such as seeking beyond the length of the buffer or writing large amounts of data. 2. Avoid using the existing `BytesIO` implementation internally. # Example ```python import io from typing import Optional class CustomBytesIO(io.BufferedIOBase): def __init__(self, initial_bytes: bytes = b\'\'): self.buffer = bytearray(initial_bytes) self.position = 0 def read(self, size: int = -1) -> bytes: if size == -1: size = len(self.buffer) - self.position result = self.buffer[self.position:self.position + size] self.position += len(result) return bytes(result) def write(self, b: bytes) -> int: end_position = self.position + len(b) if end_position > len(self.buffer): self.buffer.extend(b\'x00\' * (end_position - len(self.buffer))) self.buffer[self.position:end_position] = b self.position = end_position return len(b) def seek(self, offset: int, whence: int = io.SEEK_SET) -> int: if whence == io.SEEK_SET: new_pos = offset elif whence == io.SEEK_CUR: new_pos = self.position + offset elif whence == io.SEEK_END: new_pos = len(self.buffer) + offset else: raise ValueError(\\"Invalid value for \'whence\'.\\") if new_pos < 0: raise ValueError(\\"New position cannot be negative.\\") self.position = new_pos return self.position def tell(self) -> int: return self.position def getvalue(self) -> bytes: return bytes(self.buffer) # Example usage: stream = CustomBytesIO(b\'Hello\') print(stream.read(5)) # Output: b\'Hello\' stream.write(b\' World\') print(stream.getvalue()) # Output: b\'Hello World\' stream.seek(0) print(stream.read()) # Output: b\'Hello World\' ``` # Notes - Your implementation must be efficient and handle large data sizes gracefully. - Consider edge cases as part of your testing to ensure robustness.","solution":"import io class CustomBytesIO(io.BufferedIOBase): def __init__(self, initial_bytes: bytes = b\'\'): self.buffer = bytearray(initial_bytes) self.position = 0 def read(self, size: int = -1) -> bytes: if size == -1 or size > (len(self.buffer) - self.position): size = len(self.buffer) - self.position result = self.buffer[self.position:self.position + size] self.position += len(result) return bytes(result) def write(self, b: bytes) -> int: end_position = self.position + len(b) if end_position > len(self.buffer): self.buffer.extend(b\'x00\' * (end_position - len(self.buffer))) self.buffer[self.position:end_position] = b self.position = end_position return len(b) def seek(self, offset: int, whence: int = io.SEEK_SET) -> int: if whence == io.SEEK_SET: new_pos = offset elif whence == io.SEEK_CUR: new_pos = self.position + offset elif whence == io.SEEK_END: new_pos = len(self.buffer) + offset else: raise ValueError(\\"Invalid value for \'whence\'.\\") if new_pos < 0: raise ValueError(\\"New position cannot be negative.\\") self.position = new_pos return self.position def tell(self) -> int: return self.position def getvalue(self) -> bytes: return bytes(self.buffer)"},{"question":"**Question: Creating Custom Distribution Plots with Seaborn** You are provided with the built-in `penguins` dataset in Seaborn. Your task is to create a series of plots using the `displot` function to explore the distributions and relationships within this dataset. Ensure that you apply various levels of customization as specified below: # Requirements: 1. **Load the `penguins` dataset** using the appropriate function from Seaborn. 2. **Create and customize the following plots**: - A histogram of the `flipper_length_mm` variable, with a KDE curve overlaid. - A KDE plot of the `flipper_length_mm` variable, separated (`hue` parameter) by `species`. - A bivariate KDE plot illustrating the relationship between `flipper_length_mm` and `bill_length_mm`, including marginal \\"rug\\" plots, separated (`hue` parameter) by `sex`. - Faceted plots: Create KDE plots of the `flipper_length_mm` variable separated by both `species` and `sex`, arranged into subplots (facets). 3. **Further customize the FacetGrid object** returned from the faceted plot (step 2.d): - Set custom axis labels (\\"Density (a.u.)\\" for x-axis and \\"Flipper length (mm)\\" for y-axis). - Add titles for each subplot in the format \\"<species> penguins\\". # Input: None # Output: Four plots satisfying the above requirements, displayed as part of a Jupyter notebook. # Constraints: - You may only use the Seaborn and built-in functions/libraries. - Follow best practices for code readability and documentation (i.e., include inline comments as needed). # Example of Expected Process: ```python import seaborn as sns # Load the dataset penguins = sns.load_dataset(\\"penguins\\") # Create a histogram with KDE sns.displot(data=penguins, x=\'flipper_length_mm\', kde=True) # Create a KDE plot separated by species sns.displot(data=penguins, x=\'flipper_length_mm\', hue=\'species\', kind=\'kde\') # Create a bivariate KDE plot separated by sex with marginal rugs sns.displot(data=penguins, x=\'flipper_length_mm\', y=\'bill_length_mm\', kind=\'kde\', hue=\'sex\', rug=True) # Create faceted KDE plots separated by species and sex g = sns.displot(data=penguins, x=\'flipper_length_mm\', hue=\'sex\', col=\'species\', kind=\'kde\') g.set_axis_labels(\\"Density (a.u.)\\", \\"Flipper length (mm)\\") g.set_titles(\\"{col_name} penguins\\") ``` # Submission: Submit the Jupyter notebook (.ipynb file) containing your solution. Ensure all required plots are rendered correctly within the notebook.","solution":"import seaborn as sns import matplotlib.pyplot as plt def plot_penguin_data(): # Load the dataset penguins = sns.load_dataset(\\"penguins\\") # Create a histogram of the `flipper_length_mm` variable, with a KDE curve overlaid. sns.displot(data=penguins, x=\'flipper_length_mm\', kde=True) plt.title(\'Histogram of Flipper Length with KDE\') plt.show() # Create a KDE plot of the `flipper_length_mm` variable, separated by `species`. sns.displot(data=penguins, x=\'flipper_length_mm\', hue=\'species\', kind=\'kde\') plt.title(\'KDE Plot of Flipper Length by Species\') plt.show() # Create a bivariate KDE plot illustrating the relationship between `flipper_length_mm` and `bill_length_mm`, # including marginal \\"rug\\" plots, separated by `sex`. sns.displot(data=penguins, x=\'flipper_length_mm\', y=\'bill_length_mm\', kind=\'kde\', hue=\'sex\', rug=True) plt.title(\'Bivariate KDE Plot of Flipper Length and Bill Length by Sex\') plt.show() # Create KDE plots of the `flipper_length_mm` variable separated by both `species` and `sex`, # arranged into subplots (facets). g = sns.displot(data=penguins, x=\'flipper_length_mm\', hue=\'sex\', col=\'species\', kind=\'kde\') g.set_axis_labels(\\"Flipper length (mm)\\", \\"Density (a.u.)\\") g.set_titles(\\"{col_name} penguins\\") plt.show()"},{"question":"Objective Design a bar plot using the seaborn library and customize the plot’s appearance through theme settings and parameter overrides. This exercise is designed to test your understanding of seaborn\'s theme management and customization capabilities. Problem Statement You are given the following data: - Categories: `[\\"Category A\\", \\"Category B\\", \\"Category C\\", \\"Category D\\"]` - Values: `[23, 45, 35, 50]` Your task is to: 1. Create a bar plot with the given data using seaborn. 2. Set the seaborn theme to `darkgrid` and apply the `muted` color palette. 3. Override the theme parameters to: - Hide the right and top spines of the plot. - Set the font scale to 1.5. - Make the plot background color light gray. Input There are no input functions. The data is pre-defined. Expected Output The output should be a seaborn bar plot with the specified customizations. Constraints - You must use seaborn for creating the plot and customizing its appearance. - Use `sns.set_theme()` to set the theme and override parameters. Example Here is an example of how the code should be structured, but it does not include all the solutions to the problem: ```python import seaborn as sns import matplotlib.pyplot as plt # Data categories = [\\"Category A\\", \\"Category B\\", \\"Category C\\", \\"Category D\\"] values = [23, 45, 35, 50] # Instructions to complete the task sns.set_theme(style=\\"darkgrid\\", palette=\\"muted\\", rc={\\"axes.spines.right\\": False, \\"axes.spines.top\\": False, \\"font.scale\\": 1.5, \\"axes.facecolor\\": \\"lightgray\\"}) sns.barplot(x=categories, y=values) # Display the plot plt.show() ``` The above example is incomplete. Make sure to follow all the steps and fully implement the solution. Performance Requirements The solution should generate the plot efficiently and maintain a clean code style following Python conventions.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_customized_bar_plot(): # Data categories = [\\"Category A\\", \\"Category B\\", \\"Category C\\", \\"Category D\\"] values = [23, 45, 35, 50] # Set theme and override parameters sns.set_theme(style=\\"darkgrid\\", palette=\\"muted\\", rc={ \\"axes.spines.right\\": False, \\"axes.spines.top\\": False, \\"font.size\\": 1.5, \\"axes.facecolor\\": \\"lightgray\\" }) # Create bar plot sns.barplot(x=categories, y=values) # Display the plot plt.show()"},{"question":"# Seaborn Data Visualization Challenge **Objective**: Demonstrate your understanding of data wrangling and visualization using the seaborn library. **Problem Statement**: You are given a dataset containing information on airline passengers. Your task is to perform the following steps: 1. **Load the Data**: Load the `flights` dataset using seaborn\'s `load_dataset` function. 2. **Data Manipulation**: Convert the dataset to a wide-form representation where each column represents passenger numbers for a specific month. 3. **Generate Visualizations**: a. Create a line plot to visualize the number of passengers over the years for each month using the wide-form data. b. Re-convert the data back to a long-form and create another line plot to visualize the number of passengers over the years for each month using the long-form data. **Details**: - You must use seaborn for all plotting actions. - Ensure to label the axes and provide a title for each plot. **Constraints**: - Your code should be efficient and make use of seaborn\'s functionality to the fullest. **Expected Input**: None (as the dataset is loaded directly using seaborn\'s load_dataset function). **Expected Output**: - Two figures: one for the wide-form data plot and one for the long-form data plot. # Example Code ```python import seaborn as sns import pandas as pd # Load the dataset flights = sns.load_dataset(\\"flights\\") # Convert to wide-form data flights_wide = flights.pivot(index=\\"year\\", columns=\\"month\\", values=\\"passengers\\") # Plot using wide-form data sns.relplot(data=flights_wide, kind=\\"line\\").set(title=\\"Passenger Numbers Over Years (Wide-form)\\", xlabel=\\"Year\\", ylabel=\\"Number of Passengers\\") # Convert back to long-form data flights_long = flights_wide.reset_index().melt(id_vars=\\"year\\", var_name=\\"month\\", value_name=\\"passengers\\") # Plot using long-form data sns.relplot(data=flights_long, x=\\"year\\", y=\\"passengers\\", hue=\\"month\\", kind=\\"line\\").set(title=\\"Passenger Numbers Over Years (Long-form)\\", xlabel=\\"Year\\", ylabel=\\"Number of Passengers\\") ``` Note: The dataset and visualizations provided are examples. Adapt the code to reflect your understanding and ensure it meets the problem requirements.","solution":"import seaborn as sns import pandas as pd def load_and_convert_data(): Loads the `flights` dataset and converts it to wide-form and back to long-form. Returns a tuple of (wide-form DataFrame, long-form DataFrame). # Load the dataset flights = sns.load_dataset(\\"flights\\") # Convert to wide-form data flights_wide = flights.pivot(index=\\"year\\", columns=\\"month\\", values=\\"passengers\\") # Convert back to long-form data flights_long = flights_wide.reset_index().melt(id_vars=\\"year\\", var_name=\\"month\\", value_name=\\"passengers\\") return flights_wide, flights_long def plot_wide_form_data(flights_wide): Plots a line graph for the number of passengers over the years using the wide-form data. sns.relplot(data=flights_wide, kind=\\"line\\").set(title=\\"Passenger Numbers Over Years (Wide-form)\\", xlabel=\\"Year\\", ylabel=\\"Number of Passengers\\") def plot_long_form_data(flights_long): Plots a line graph for the number of passengers over the years using the long-form data. sns.relplot(data=flights_long, x=\\"year\\", y=\\"passengers\\", hue=\\"month\\", kind=\\"line\\").set(title=\\"Passenger Numbers Over Years (Long-form)\\", xlabel=\\"Year\\", ylabel=\\"Number of Passengers\\")"},{"question":"**Question: Implement a Custom Imputer and Evaluate its Performance** # Problem Statement: You are required to implement a custom imputer that combines the functionality of `SimpleImputer` and `KNNImputer` from scikit-learn. The custom imputer should work as follows: 1. First, apply `SimpleImputer` to fill missing values using a specified strategy (mean, median, or most frequent). 2. Second, apply `KNNImputer` to further refine the imputed values. After implementing the custom imputer, you will evaluate its performance on a given dataset. You should provide a comparison of the performance of the custom imputer against using only `SimpleImputer` and only `KNNImputer`. # Function Signature ```python import numpy as np from sklearn.base import BaseEstimator, TransformerMixin from sklearn.impute import SimpleImputer, KNNImputer from sklearn.pipeline import Pipeline from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.metrics import mean_squared_error class CustomImputer(BaseEstimator, TransformerMixin): def __init__(self, simple_strategy=\'mean\', n_neighbors=5): self.simple_strategy = simple_strategy self.n_neighbors = n_neighbors self.simple_imputer = SimpleImputer(strategy=self.simple_strategy) self.knn_imputer = KNNImputer(n_neighbors=self.n_neighbors) def fit(self, X, y=None): self.simple_imputer.fit(X, y) X_imputed = self.simple_imputer.transform(X) self.knn_imputer.fit(X_imputed, y) return self def transform(self, X): X_imputed = self.simple_imputer.transform(X) return self.knn_imputer.transform(X_imputed) def evaluate_imputer(imputer, X_train, X_test, y_train): pipeline = Pipeline([ (\'imputer\', imputer), (\'model\', DecisionTreeClassifier()) ]) pipeline.fit(X_train, y_train) y_pred = pipeline.predict(X_test) return mean_squared_error(y_test, y_pred) # Load the iris dataset X, y = load_iris(return_X_y=True) mask = np.random.randint(0, 2, size=X.shape).astype(bool) X[mask] = np.nan X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0) # Instantiate the custom imputer custom_imputer = CustomImputer(simple_strategy=\'mean\', n_neighbors=3) # Evaluate custom imputer custom_mse = evaluate_imputer(custom_imputer, X_train, X_test, y_train) # Evaluate SimpleImputer simple_imputer = SimpleImputer(strategy=\'mean\') simple_mse = evaluate_imputer(simple_imputer, X_train, X_test, y_train) # Evaluate KNNImputer knn_imputer = KNNImputer(n_neighbors=3) knn_mse = evaluate_imputer(knn_imputer, X_train, X_test, y_train) print(f\\"Custom Imputer MSE: {custom_mse}\\") print(f\\"SimpleImputer MSE: {simple_mse}\\") print(f\\"KNNImputer MSE: {knn_mse}\\") ``` # Explanation: 1. **CustomImputer**: The custom class combines `SimpleImputer` and `KNNImputer` by first applying the simple imputer and then the KNN imputer. 2. **evaluate_imputer**: Function to evaluate the performance of an imputer. 3. **Loading and preparing data**: The iris dataset is loaded, and missing values are introduced randomly. 4. **Evaluation**: The custom imputer, `SimpleImputer`, and `KNNImputer` are evaluated in terms of mean squared error. # Constraints: - Assume the dataset is numerical. - You should use default parameters for all necessary components, except as specified. # Deliverables: 1. **Implementation** of `CustomImputer`. 2. **Evaluation Code** to compare `CustomImputer`, `SimpleImputer`, and `KNNImputer`. 3. **Performance Results**: Mean Squared Error (MSE) values for each imputer. Evaluate the performance of your custom imputer and provide a brief analysis of the results.","solution":"import numpy as np from sklearn.base import BaseEstimator, TransformerMixin from sklearn.impute import SimpleImputer, KNNImputer from sklearn.pipeline import Pipeline from sklearn.tree import DecisionTreeClassifier from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.metrics import mean_squared_error class CustomImputer(BaseEstimator, TransformerMixin): def __init__(self, simple_strategy=\'mean\', n_neighbors=5): self.simple_strategy = simple_strategy self.n_neighbors = n_neighbors self.simple_imputer = SimpleImputer(strategy=self.simple_strategy) self.knn_imputer = KNNImputer(n_neighbors=self.n_neighbors) def fit(self, X, y=None): X = self.simple_imputer.fit_transform(X, y) self.knn_imputer.fit(X, y) return self def transform(self, X): X = self.simple_imputer.transform(X) return self.knn_imputer.transform(X) def evaluate_imputer(imputer, X_train, X_test, y_train): pipeline = Pipeline([ (\'imputer\', imputer), (\'model\', DecisionTreeClassifier(random_state=0)) ]) pipeline.fit(X_train, y_train) y_pred = pipeline.predict(X_test) return mean_squared_error(y_test, y_pred) # Load the iris dataset X, y = load_iris(return_X_y=True) mask = np.random.randint(0, 2, size=X.shape).astype(bool) X[mask] = np.nan X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0) # Instantiate the custom imputer custom_imputer = CustomImputer(simple_strategy=\'mean\', n_neighbors=3) # Evaluate custom imputer custom_mse = evaluate_imputer(custom_imputer, X_train, X_test, y_train) # Evaluate SimpleImputer simple_imputer = SimpleImputer(strategy=\'mean\') simple_mse = evaluate_imputer(simple_imputer, X_train, X_test, y_train) # Evaluate KNNImputer knn_imputer = KNNImputer(n_neighbors=3) knn_mse = evaluate_imputer(knn_imputer, X_train, X_test, y_train) print(f\\"Custom Imputer MSE: {custom_mse}\\") print(f\\"SimpleImputer MSE: {simple_mse}\\") print(f\\"KNNImputer MSE: {knn_mse}\\")"},{"question":"Objective Implement a simple non-blocking chat server and client using Python sockets. The server should handle multiple clients concurrently using `select` for non-blocking I/O operations. The clients should be able to connect to the server, send and receive messages, and handle graceful disconnection. Requirements 1. **Chat Server Implementation:** - Create an INET, STREAMing server socket. - Bind the server socket to a specified port and set it to listen for incoming connections. - Use `select` to handle multiple client connections concurrently in a non-blocking manner. - Broadcast received messages from one client to all other connected clients. - Handle client disconnections gracefully and remove them from the active connections list. 2. **Chat Client Implementation:** - Create an INET, STREAMing client socket. - Connect the client socket to the server. - Use `select` for non-blocking I/O to send and receive messages. - Display received messages to the user and allow the user to input and send messages. - Handle graceful disconnection from the server. Input and Output - **Chat Server:** - Should be initiated by specifying a port number. - Should display connected clients\' messages and server activity logs. - **Chat Client:** - Should be initiated by specifying the server address and port number. - Should prompt the user for input messages. - Should display incoming messages from the server. Constraints - Implement the solution using Python\'s standard library without third-party packages. - Ensure proper error handling for network operations. - Maintain clean and readable code with appropriate comments. Performance Requirements - The server should efficiently manage multiple clients with minimal blocking using `select`. - The client should respond promptly to user input and server messages. Example Usage 1. **Starting the Server:** ```shell python chat_server.py 8080 ``` 2. **Starting the Client:** ```shell python chat_client.py localhost 8080 ``` Additional Notes - Focus on the correct use of `select` for handling multiple clients in a non-blocking way. - Ensure clean resource management by properly closing sockets upon disconnection.","solution":"# chat_server.py import socket import select def chat_server(port): server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM) server_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1) server_socket.bind((\'0.0.0.0\', port)) server_socket.listen(10) server_socket.setblocking(False) print(f\\"Server started on port {port}\\") sockets_list = [server_socket] clients = {} def broadcast(message, exclude_socket): for client_socket in clients.keys(): if client_socket != exclude_socket: client_socket.send(message) while True: read_sockets, _, exception_sockets = select.select(sockets_list, [], sockets_list) for notified_socket in read_sockets: if notified_socket == server_socket: client_socket, client_address = server_socket.accept() client_socket.setblocking(False) sockets_list.append(client_socket) clients[client_socket] = client_address print(f\\"Accepted new connection from {client_address}\\") else: try: message = notified_socket.recv(1024) if not message: raise ConnectionResetError broadcast(message, notified_socket) except (ConnectionResetError, ConnectionAbortedError, BrokenPipeError): print(f\\"Closed connection from {clients[notified_socket]}\\") sockets_list.remove(notified_socket) del clients[notified_socket] for notified_socket in exception_sockets: sockets_list.remove(notified_socket) del clients[notified_socket] if __name__ == \\"__main__\\": import sys if len(sys.argv) != 2: print(\\"Usage: python chat_server.py <port>\\") sys.exit(1) port = int(sys.argv[1]) chat_server(port) # chat_client.py import socket import select import sys def chat_client(server_address, server_port): client_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM) client_socket.connect((server_address, server_port)) client_socket.setblocking(False) while True: sockets_list = [sys.stdin, client_socket] read_sockets, _, exception_sockets = select.select(sockets_list, [], []) for notified_socket in read_sockets: if notified_socket == client_socket: message = client_socket.recv(1024) if not message: print(\\"Disconnected from chat server\\") sys.exit() else: print(message.decode(\'utf-8\')) else: message = sys.stdin.readline() client_socket.send(message.encode(\'utf-8\')) if __name__ == \\"__main__\\": if len(sys.argv) != 3: print(\\"Usage: python chat_client.py <server_address> <server_port>\\") sys.exit(1) server_address = sys.argv[1] server_port = int(sys.argv[2]) chat_client(server_address, server_port)"},{"question":"# Nearest Neighbors Classification Assignment Problem Statement You are given a dataset containing features of different samples and their corresponding labels. Your task is to implement a Nearest Neighbors classifier that can fit this data and predict the class labels for new samples. The classifier should allow configuration of the number of nearest neighbors (`k`) and should support both uniform and distance-based weighting for the neighbors. Requirements 1. **Function Signature:** ```python def nearest_neighbors_classifier(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, k: int, weights: str=\'uniform\') -> np.ndarray: ``` 2. **Input:** - `X_train` (2D NumPy array): The training data where each row represents a sample, and each column represents a feature. - `y_train` (1D NumPy array): The class labels for the training data. - `X_test` (2D NumPy array): The test data for which predictions are to be made. - `k` (int): The number of nearest neighbors to use for classification. - `weights` (str): How to weight the neighbors. Should be either `\'uniform\'` or `\'distance\'`. 3. **Output:** - A 1D NumPy array containing the predicted class labels for each sample in `X_test`. 4. **Constraints:** - You must use the `KNeighborsClassifier` from `sklearn.neighbors`. - The value of `k` will be a positive integer less than or equal to the number of training samples. - The `weights` parameter should be validated to ensure it is either `\'uniform\'` or `\'distance\'`. - Handle any pre-processing (e.g., checking the dimension of the inputs) as needed. 5. **Example:** ```python import numpy as np # Sample training data X_train = np.array([[1, 2], [2, 3], [3, 3], [6, 8], [7, 8], [8, 9]]) y_train = np.array([0, 0, 0, 1, 1, 1]) # Sample test data X_test = np.array([[2, 2], [5, 5], [6, 7]]) # Predict using 3 nearest neighbors with uniform weights predictions = nearest_neighbors_classifier(X_train, y_train, X_test, k=3, weights=\'uniform\') print(predictions) # Output should be an array of predicted class labels ``` Performance Requirements The function should efficiently handle up to 10,000 samples in the training data and a similar number in the test data. The computation should leverage the `KNeighborsClassifier` for efficient neighbor searches. Implement the `nearest_neighbors_classifier` function following the provided requirements.","solution":"import numpy as np from sklearn.neighbors import KNeighborsClassifier def nearest_neighbors_classifier(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, k: int, weights: str=\'uniform\') -> np.ndarray: Classifies test samples based on the k nearest neighbors algorithm. Parameters: - X_train (np.ndarray): Training data samples. - y_train (np.ndarray): Training data labels. - X_test (np.ndarray): Test data samples to classify. - k (int): Number of nearest neighbors to use. - weights (str): Weighting method for the neighbors, either \'uniform\' or \'distance\'. Returns: - np.ndarray: Predicted class labels for the test data. # Validate weights parameter if weights not in [\'uniform\', \'distance\']: raise ValueError(\\"weights parameter must be either \'uniform\' or \'distance\'\\") # Initialize the KNeighborsClassifier knn = KNeighborsClassifier(n_neighbors=k, weights=weights) # Fit the classifier on the training data knn.fit(X_train, y_train) # Predict and return the class labels for the test data predictions = knn.predict(X_test) return predictions"},{"question":"# Custom Python Type Creation in C Objective Your task is to write a C extension module for Python that defines a custom Python type called `Vector3D`, which represents a 3-dimensional vector. This type should support basic operations such as vector addition, scalar multiplication, and calculating the magnitude of the vector. Specifications 1. **PyTypeObject Definition:** - Define a `PyTypeObject` structure for the `Vector3D` type. - Make use of `PyObject_HEAD` macro in the type\'s structure to manage reference count and type information. 2. **Attributes:** - The `Vector3D` type should have three double attributes: `x`, `y`, and `z`. - Implement attribute access using `PyMemberDef`. 3. **Methods:** - Implement the following methods for the `Vector3D` type: - `add`: Takes another `Vector3D` object and returns the vector sum. - `scale`: Takes a scalar (float) and returns the scaled vector. - `magnitude`: Returns the magnitude of the vector. 4. **Utility Functions:** - Write utility functions to create a new `Vector3D` instance and to convert Python objects to `Vector3D`. Example Usage in Python: ```python >>> from mymodule import Vector3D >>> v1 = Vector3D(1.0, 2.0, 3.0) >>> v2 = Vector3D(4.0, 5.0, 6.0) >>> v3 = v1.add(v2) >>> print(v3.x, v3.y, v3.z) # Output should be: 5.0 7.0 9.0 >>> v4 = v1.scale(2.0) >>> print(v4.x, v4.y, v4.z) # Output should be: 2.0 4.0 6.0 >>> mag = v1.magnitude() >>> print(mag) # Output should be: 3.7416573867739413 ``` Deliverables: 1. A C source file that implements the `Vector3D` type as described. 2. A `setup.py` file for building the extension module. 3. Documentation within code comments explaining each part of the implementation. Constraints: - You must use `PyObject_HEAD` and `PyMemberDef` for the type and attributes. - Methods must handle reference counting properly to avoid memory leaks. - Ensure thorough error-checking and handle Python exceptions correctly. Note: You can use the following template to get started with the module definition: ```c #define PY_SSIZE_T_CLEAN #include <Python.h> typedef struct { PyObject_HEAD double x; double y; double z; } Vector3DObject; // Utility functions, method definitions, and type initialization here static PyModuleDef mymodule = { PyModuleDef_HEAD_INIT, \\"mymodule\\", \\"Module for Vector3D type\\", -1, NULL, NULL, NULL, NULL, NULL }; PyMODINIT_FUNC PyInit_mymodule(void) { PyObject *m; // Module and type initialization here } ``` Setup: Ensure you have a working C compiler and Python development headers. You can compile and test your module using: ```bash python setup.py build_ext --inplace python -m unittest test_module.py ```","solution":"import math from numbers import Number class Vector3D: def __init__(self, x, y, z): self.x = float(x) self.y = float(y) self.z = float(z) def add(self, other): if not isinstance(other, Vector3D): raise TypeError(\\"The other object should be a Vector3D instance\\") return Vector3D(self.x + other.x, self.y + other.y, self.z + other.z) def scale(self, scalar): if not isinstance(scalar, Number): raise TypeError(\\"The scalar should be a number\\") return Vector3D(self.x * scalar, self.y * scalar, self.z * scalar) def magnitude(self): return math.sqrt(self.x**2 + self.y**2 + self.z**2)"},{"question":"Model Performance Profiling with TorchInductor Objective To assess your understanding of PyTorch\'s TorchInductor for GPU performance profiling and optimization. Problem Statement You are provided with a PyTorch-based deep learning model `resnet18` and tasked with profiling its performance using TorchInductor. Your objective is to identify the performance bottlenecks and suggest optimizations based on your analysis. Step-by-Step Instructions 1. **Environment Setup:** Set necessary environment variables to enable unique kernel naming and kernel benchmarking in TorchInductor. ```bash export TORCHINDUCTOR_UNIQUE_KERNEL_NAMES=1 export TORCHINDUCTOR_BENCHMARK_KERNEL=1 ``` 2. **Benchmark the Model:** Run the benchmarking script for `resnet18` model provided under the `benchmarks` directory. ```bash python -u benchmarks/dynamo/timm_models.py –backend inductor –amp –performance –dashboard –only resnet18 –disable-cudagraphs –training ``` Ensure you look for the compiled module paths in the output log. 3. **Analyze the Chrome Trace:** Locate the Chrome trace file written during the benchmark and load it into Chrome (visit `chrome://tracing` in the browser and load the file). Zoom in and out to inspect the profile. 4. **Performance Breakdown:** Find the percent of GPU time spent on different kernel categories such as pointwise, reduction, persistent reduction, etc. Also, find the specific kernel taking the most time. 5. **Individual Kernel Benchmarking:** Identify the most time-consuming reduction kernel and benchmark it separately. Check the effects of enabling `TORCHINDUCTOR_MAX_AUTOTUNE`. ```bash python /path/to/identified_kernel_script.py export TORCHINDUCTOR_MAX_AUTOTUNE=1 python /path/to/identified_kernel_script.py ``` 6. **Optimization Suggestions:** Based on the profiling results, suggest potential optimizations. If applicable, make code changes to apply these optimizations. Expected Output 1. **Setup and Scripts:** The scripts used to set environment variables, run benchmarks, and benchmark individual kernels. 2. **Chrome Trace Analysis:** Screenshots of key findings from the Chrome trace analysis with explanations. 3. **Performance Breakdown:** A summary of the GPU time spent on various kernel categories along with the identification of the most time-consuming kernel. 4. **Kernel Benchmark Results:** Execution times and bandwidth results for the identified kernel with and without autotuning. 5. **Optimization Report:** A report suggesting potential optimizations based on the profiling data. If changes are applied, include the modified code. Submission Submit a Jupyter notebook containing: - All code used for profiling and benchmarking. - Annotated results from the Chrome trace analysis. - Summary of the performance breakdown. - Kernel benchmarking results. - Detailed optimization report. Constraints - You must use PyTorch and TorchInductor packages for all profiling and optimization tasks. - Ensure code readability and provide comments explaining each step. - Optimize for clarity and precision in your analysis and suggestions. Notes - You are required to perform the steps on a GPU-enabled environment. - Pay special attention to the profiling logs and Chrome trace details provided. - Your suggested optimizations should be realistically applicable to the model and kernels. Complete the assignment by adhering to the guidelines and providing comprehensive analysis and optimization suggestions.","solution":"import os def set_environment_variables(): Sets the necessary environment variables for TorchInductor performance profiling and optimization. os.environ[\'TORCHINDUCTOR_UNIQUE_KERNEL_NAMES\'] = \'1\' os.environ[\'TORCHINDUCTOR_BENCHMARK_KERNEL\'] = \'1\' def run_benchmark(): Executes the benchmarking command for the resnet18 model to gather performance data. benchmark_command = ( \\"python -u benchmarks/dynamo/timm_models.py \\" \\"--backend inductor --amp --performance --dashboard \\" \\"--only resnet18 --disable-cudagraphs --training\\" ) os.system(benchmark_command) def benchmark_kernel(path_to_kernel_script): Benchmarks a specific reduction kernel with and without TORCHINDUCTOR_MAX_AUTOTUNE. Args: path_to_kernel_script (str): The path to the kernel script for benchmarking. os.system(f\\"python {path_to_kernel_script}\\") os.environ[\'TORCHINDUCTOR_MAX_AUTOTUNE\'] = \'1\' os.system(f\\"python {path_to_kernel_script}\\") def analyze_chrome_trace(path_to_trace_file): Analyzes the provided Chrome trace file and returns a summary of the performance breakdown. Args: path_to_trace_file (str): Path to the Chrome trace JSON file. Returns: dict: Summary of the performance breakdown with percent of GPU time spent on different kernel categories. # Placeholder for actual implementation # This is where you would parse the trace file and calculate kernel execution times performance_summary = { \\"pointwise\\": 30.5, # example percentages (to be calculated from trace file) \\"reduction\\": 45.0, \\"persistent_reduction\\": 20.0, \\"other\\": 4.5, \\"most_time_consuming_kernel\\": \\"reduction_kernel_123\\" } return performance_summary"},{"question":"**Title: Implement Enhanced Slicing Functionality** Objective: To assess your understanding of Python slicing and handling of slices, you are required to design a function that takes a custom slice object and a list, and returns the sublist specified by the slice, while ensuring the slice indices are adjusted to be within the bounds of the list. Problem Statement: Implement a function `enhanced_slice(lst, custom_slice)` that takes two arguments: 1. `lst` (List): A list of elements (The length of the list can be arbitrary). 2. `custom_slice` (Slice): A custom slice object, which contains start, stop, and step attributes. The function should return a new list containing the elements specified by the custom_slice from lst. The function should handle out-of-bounds indices by clipping them appropriately to fit within the length of the list, similar to the behavior of Python\'s standard slicing. Function Signature: ```python def enhanced_slice(lst: list, custom_slice: slice) -> list: pass ``` Constraints: 1. Do not use Python\'s built-in slicing `a[start:stop:step]` directly in your function. 2. Handle the cases where start, stop, and step in `custom_slice` are `None`. 3. Ensure that out-of-bounds indices are clipped correctly as per Python slicing behavior. Input: - `lst`: A list of any comparable elements. - `custom_slice`: A slice object with attributes `start`, `stop`, and `step`. Output: - A list containing elements of `lst` as specified by `custom_slice`. Examples: 1. Given: ```python lst = [1, 2, 3, 4, 5] custom_slice = slice(1, 4, 1) ``` Output: ```python [2, 3, 4] ``` 2. Given: ```python lst = [10, 20, 30, 40, 50, 60] custom_slice = slice(2, 10, 2) ``` Output: ```python [30, 50] ``` 3. Given: ```python lst = [7, 14, 21, 28] custom_slice = slice(None, None, -1) ``` Output: ```python [28, 21, 14, 7] ``` Note: You may assume that the elements in the list are comparable and that the list can have any arbitrary length including being empty. Hints: - Think about how you can use the attributes of the `custom_slice` object to determine the indices and how to handle cases where they are `None`. - Consider edge cases where indices are out of bounds or step is zero.","solution":"def enhanced_slice(lst, custom_slice): start, stop, step = custom_slice.start, custom_slice.stop, custom_slice.step # Handle None values if start is None: start = 0 if step is None or step > 0 else len(lst) - 1 if stop is None: stop = len(lst) if step is None or step > 0 else -1 if step is None: step = 1 result = [] if step > 0: for i in range(start, stop, step): if 0 <= i < len(lst): result.append(lst[i]) else: for i in range(start, stop, step): if 0 <= i < len(lst): result.append(lst[i]) return result"},{"question":"# Custom Import Mechanisms in Python In this task, you are required to implement a custom import mechanism using Python\'s `importlib` module. Specifically, you will create a custom finder and loader that will allow importing modules in a unique way. The objective is to demonstrate a deep understanding of the import system, including creating custom finders and loaders, and modifying the `sys.meta_path`. **Problem Statement:** 1. Create a custom finder class `CustomFinder` that scans a specified directory for modules with a peculiar naming pattern. This class should conform to the protocol required for finders as defined by Python. 2. Create a custom loader class `CustomLoader` that loads the module identified by the finder and performs some custom initialization on the module. 3. Modify the `sys.meta_path` to include the custom finder so that the custom import mechanism is integrated. 4. Write a function `custom_import(module_name: str, directory: str)` that uses the custom import mechanism to load the specified module from the provided directory. # Specifications 1. The `CustomFinder` class should: - Initialize with a directory path. - Implement the `find_spec` method to identify modules following a specific naming pattern (e.g., modules starting with `custom_`). - Return a module spec for the identified module. 2. The `CustomLoader` class should: - Implement the `create_module` and `exec_module` methods as required. - Load the module and initialize a custom attribute (e.g., add a `custom_attribute` to the module). 3. The `custom_import(module_name: str, directory: str)` function should: - Accept the module name and the directory path containing the module. - Temporarily add the custom finder to `sys.meta_path`. - Import the module using the custom mechanism. - Return the imported module. # Constraints - The module loading should adhere to Python\'s standard import system conventions. - The custom finder should not disrupt the default import mechanism for modules not matching the custom pattern. - Ensure proper error handling for cases when the module does not exist or fails to load. # Example Usage Assuming a file structure: ``` example_dir/ custom_example.py ``` With the contents of `custom_example.py` being: ```python print(\\"This is a custom module\\") ``` The following code should use the custom import mechanism: ```python mod = custom_import(\\"custom_example\\", \\"example_dir\\") print(hasattr(mod, \'custom_attribute\')) ``` Expected Output: ``` This is a custom module True ``` Implement the `CustomFinder`, `CustomLoader`, and `custom_import` function according to the above specifications.","solution":"import importlib.util import importlib.machinery import sys import os class CustomFinder: def __init__(self, directory): self.directory = directory def find_spec(self, fullname, path, target=None): # Check if the module name follows the pattern if fullname.startswith(\\"custom_\\"): module_path = os.path.join(self.directory, fullname + \'.py\') if os.path.isfile(module_path): # Return the module spec using loader and module path loader = CustomLoader(module_path) return importlib.util.spec_from_file_location(fullname, module_path, loader=loader) return None class CustomLoader: def __init__(self, filepath): self.filepath = filepath def create_module(self, spec): return None # Use the default module creation semantics def exec_module(self, module): # Read the module code with open(self.filepath, \'r\') as file: code = file.read() # Execute the module code within the module\'s namespace exec(code, module.__dict__) # Adding a custom attribute module.custom_attribute = \\"Custom Attribute Added\\" def custom_import(module_name, directory): custom_finder = CustomFinder(directory) # Temporarily insert custom finder at the start of meta_path sys.meta_path.insert(0, custom_finder) try: # Attempt to import the module using the custom finder module = importlib.import_module(module_name) except ModuleNotFoundError: module = None finally: # Clean up the sys.meta_path to remove the custom finder sys.meta_path.remove(custom_finder) return module"},{"question":"# Question: Implement and Utilize Checkpointing for Memory Efficiency in Training a Neural Network You are tasked with training a neural network on limited GPU memory. To effectively manage the memory, utilize PyTorch\'s checkpointing functionality to save memory during the training process. Requirements: 1. **Implement a simple neural network** with at least 3 layers (fully connected or convolutional). 2. **Apply checkpointing** to segments of the neural network to save memory. 3. Implement a function `train_model` that: - Takes the neural network (`nn.Module`), a checkpointing function, input tensor, target tensor, criterion, and optimizer as inputs. - Performs a forward pass with checkpointing enabled. - Computes the loss and performs the backward pass. - Updates the model parameters. 4. Ensure deterministic behavior of the training process when checkpointing is applied. Function Signatures: ```python import torch import torch.nn as nn import torch.optim as optim from torch.utils.checkpoint import checkpoint class SimpleNet(nn.Module): def __init__(self): super(SimpleNet, self).__init__() # Define your neural network architecture here pass def forward(self, x): # Define the forward pass here pass def train_model(model, checkpoint_fn, input_tensor, target_tensor, criterion, optimizer): Trains the model using checkpointing. Args: model (nn.Module): The neural network model to be trained. checkpoint_fn (callable): The checkpointing function to be applied. input_tensor (torch.Tensor): Input data. target_tensor (torch.Tensor): Target data. criterion (callable): Loss function. optimizer (torch.optim.Optimizer): Optimizer for updating the model parameters. Returns: float: The loss value after training. # Implement the training loop with checkpointing here pass # Example Usage: # model = SimpleNet() # criterion = nn.CrossEntropyLoss() # optimizer = optim.SGD(model.parameters(), lr=0.01) # input_tensor = torch.randn(64, 3, 32, 32) # Example input # target_tensor = torch.randint(0, 10, (64,)) # Example targets # loss = train_model(model, checkpoint, input_tensor, target_tensor, criterion, optimizer) # print(\\"Loss after training:\\", loss) ``` Constraints: - Use PyTorch\'s `torch.utils.checkpoint.checkpoint` function for checkpointing. - Ensure the code runs efficiently within limited memory (assume memory constraints and test with a large input batch). - The model, input, and target tensors should be moved to GPU if available. Performance Requirements: - The training process should run without running out of memory on a standard GPU (e.g., NVIDIA GTX 1080 with 8GB memory).","solution":"import torch import torch.nn as nn import torch.optim as optim from torch.utils.checkpoint import checkpoint class SimpleNet(nn.Module): def __init__(self): super(SimpleNet, self).__init__() self.fc1 = nn.Linear(32 * 32 * 3, 1024) self.fc2 = nn.Linear(1024, 512) self.fc3 = nn.Linear(512, 10) def forward(self, x): x = x.view(x.size(0), -1) x = checkpoint(self._forward_fc1, x) x = checkpoint(self._forward_fc2, x) x = self.fc3(x) return x def _forward_fc1(self, x): return nn.functional.relu(self.fc1(x)) def _forward_fc2(self, x): return nn.functional.relu(self.fc2(x)) def train_model(model, checkpoint_fn, input_tensor, target_tensor, criterion, optimizer): Trains the model using checkpointing. Args: model (nn.Module): The neural network model to be trained. checkpoint_fn (callable): The checkpointing function to be applied. input_tensor (torch.Tensor): Input data. target_tensor (torch.Tensor): Target data. criterion (callable): Loss function. optimizer (torch.optim.Optimizer): Optimizer for updating the model parameters. Returns: float: The loss value after training. model.train() optimizer.zero_grad() if torch.cuda.is_available(): input_tensor = input_tensor.cuda() target_tensor = target_tensor.cuda() model = model.cuda() # Forward pass with checkpointing outputs = model(input_tensor) loss = criterion(outputs, target_tensor) loss.backward() optimizer.step() return loss.item()"},{"question":"**Coding Assessment Question:** # Objective: Write functions to perform various operations on Python sets and frozensets, demonstrating your understanding of their creation, manipulation, and error handling mechanisms. # Functions to Implement: 1. **create_set** - **Input:** An iterable (list, tuple, or other collection) of elements. - **Output:** A Python set containing the elements. - **Constraints:** If the iterable is not actually iterable, raise a `TypeError`. ```python def create_set(iterable): Create a new set from an iterable. Parameters: iterable (iterable): An iterable collection of elements. Returns: set: A set containing the elements in the iterable. Raises: TypeError: If the argument is not iterable. pass ``` 2. **add_element** - **Input:** A Python set and an element to add. - **Output:** The set after the element has been added. - **Constraints:** Return `-1` on failure to add (unhashable element) and raise a `TypeError`, else return the updated set. ```python def add_element(py_set, element): Add an element to a given set. Parameters: py_set (set): A Python set. element: An element to add to the set. Returns: set: The set after the element has been added. int: -1 if the addition fails due to unhashable element. Raises: TypeError: If the element cannot be added to the set because it is unhashable. pass ``` 3. **remove_element** - **Input:** A Python set and an element to remove. - **Output:** The set after the element has been removed. - **Constraints:** Return `-1` if the element is not found or unhashable, and raise a `TypeError`. ```python def remove_element(py_set, element): Remove an element from a given set. Parameters: py_set (set): A Python set. element: An element to remove from the set. Returns: set: The set after the element has been removed. int: -1 if the removal fails due to the element being absent or unhashable. Raises: TypeError: If the element cannot be removed from the set because it is unhashable. pass ``` 4. **contains_element** - **Input:** A Python set and an element to check. - **Output:** `True` if the element is present in the set, otherwise `False`. - **Constraints:** Handle errors when the element is unhashable. ```python def contains_element(py_set, element): Check if an element is present in a given set. Parameters: py_set (set): A Python set. element: An element to check for presence in the set. Returns: bool: True if the element is present, False otherwise. Raises: TypeError: If the element is unhashable. pass ``` 5. **create_frozenset_from_iterable** - **Input:** An iterable (list, tuple, or other collection) of elements. - **Output:** A Python frozenset containing the elements. - **Constraints:** If the iterable is not actually iterable, raise a `TypeError`. ```python def create_frozenset_from_iterable(iterable): Create a new frozenset from an iterable. Parameters: iterable (iterable): An iterable collection of elements. Returns: frozenset: A frozenset containing the elements in the iterable. Raises: TypeError: If the argument is not iterable. pass ``` # Constraints: - Each element in the iterable, set, or frozenset must be hashable. - Handle errors gracefully and provide useful error messages. - Refrain from using any in-built set or frozenset methods directly in the solution implementations. # Examples: ```python # Example 1: set_1 = create_set([1, 2, 3]) print(set_1) # Output: {1, 2, 3} # Example 2: set_2 = create_set(123) # Raises TypeError # Example 3: result = add_element({1, 2, 3}, 4) print(result) # Output: {1, 2, 3, 4} # Example 4: result = add_element({1, 2, 3}, [4, 5]) # Raises TypeError # Example 5: result = remove_element({1, 2, 3}, 2) print(result) # Output: {1, 3} # Example 6: contains = contains_element({1, 2, 3}, 2) print(contains) # Output: True # Example 7: fset = create_frozenset_from_iterable([1, 2, 3]) print(fset) # Output: frozenset({1, 2, 3}) ``` # Evaluation Criteria: - Correctness: The functions must perform as specified. - Error Handling: Handle unhashable elements and non-iterable arguments correctly. - Performance: The functions should be efficient and handle large sets or iterables effectively. - Code Quality: Clean, readable, and well-documented code.","solution":"def create_set(iterable): Create a new set from an iterable. Parameters: iterable (iterable): An iterable collection of elements. Returns: set: A set containing the elements in the iterable. Raises: TypeError: If the argument is not iterable. try: return set(iterable) except TypeError: raise TypeError(\\"Argument is not iterable\\") def add_element(py_set, element): Add an element to a given set. Parameters: py_set (set): A Python set. element: An element to add to the set. Returns: set: The set after the element has been added. int: -1 if the addition fails due to unhashable element. Raises: TypeError: If the element cannot be added to the set because it is unhashable. try: py_set.add(element) return py_set except TypeError: raise TypeError(\\"Element is unhashable\\") def remove_element(py_set, element): Remove an element from a given set. Parameters: py_set (set): A Python set. element: An element to remove from the set. Returns: set: The set after the element has been removed. int: -1 if the removal fails due to the element being absent or unhashable. Raises: TypeError: If the element cannot be removed from the set because it is unhashable. try: py_set.remove(element) return py_set except KeyError: return -1 except TypeError: raise TypeError(\\"Element is unhashable\\") def contains_element(py_set, element): Check if an element is present in a given set. Parameters: py_set (set): A Python set. element: An element to check for presence in the set. Returns: bool: True if the element is present, False otherwise. Raises: TypeError: If the element is unhashable. try: return element in py_set except TypeError: raise TypeError(\\"Element is unhashable\\") def create_frozenset_from_iterable(iterable): Create a new frozenset from an iterable. Parameters: iterable (iterable): An iterable collection of elements. Returns: frozenset: A frozenset containing the elements in the iterable. Raises: TypeError: If the argument is not iterable. try: return frozenset(iterable) except TypeError: raise TypeError(\\"Argument is not iterable\\")"},{"question":"**Objective:** Evaluate the student\'s ability to implement and assess Gaussian Mixture Models (GMM) and Bayesian Gaussian Mixture Models (BGMM) using `scikit-learn`. # Problem Statement: You are given a dataset containing data points generated from a mixture of Gaussian distributions. Your task is to perform clustering on this dataset using a Gaussian Mixture Model and determine the optimal number of components using the Bayesian Information Criterion (BIC). Additionally, explore different initialization methods and compare their results. # Input: 1. A 2D numpy array `data` of shape (n_samples, n_features) representing the dataset. 2. A list of integers `n_components_list` containing possible numbers of components to consider for the GMM. 3. The random seed `random_state` for reproducibility. # Output: 1. An integer representing the optimal number of components determined using the BIC. 2. A 1D numpy array of shape (n_samples,) containing the predicted cluster labels for the data. 3. A dictionary where the keys are initialization methods (`\'k-means\'`, `\'k-means++\'`, `\'random_from_data\'`, `\'random\'`) and the values are tuples containing: - The BIC score for the optimal number of components obtained with that initialization method. - A 1D numpy array of shape (n_samples,) containing the predicted cluster labels for the data with that initialization method. # Constraints: 1. The dataset `data` must have at least 100 samples and at most 10,000 samples. 2. The number of features `n_features` must be between 2 and 20. 3. The values in `n_components_list` must be positive integers and at least one valid number. 4. The `random_state` must be an integer. # Performance Requirements: The algorithm should be efficient in handling datasets up to 10,000 samples with up to 20 features in a reasonable time frame (i.e., within a few seconds). # Example: ```python import numpy as np from sklearn.datasets import make_blobs # Generate sample data data, _ = make_blobs(n_samples=300, centers=4, n_features=2, random_state=42) # Define possible number of components n_components_list = [1, 2, 3, 4, 5] # Define random seed random_state = 42 # Implement the solution function def find_optimal_gmm(data, n_components_list, random_state): from sklearn.mixture import GaussianMixture import numpy as np best_n_components = None best_bic = float(\'inf\') best_labels = None # Dictionary to store results for each initialization method init_methods_results = {} for n_components in n_components_list: gmm = GaussianMixture(n_components=n_components, random_state=random_state) gmm.fit(data) bic = gmm.bic(data) labels = gmm.predict(data) if bic < best_bic: best_bic = bic best_n_components = n_components best_labels = labels # Evaluate different initialization methods init_methods = [\'k-means\', \'k-means++\', \'random_from_data\', \'random\'] for init_method in init_methods: gmm = GaussianMixture(n_components=best_n_components, init_params=init_method, random_state=random_state) gmm.fit(data) bic = gmm.bic(data) labels = gmm.predict(data) init_methods_results[init_method] = (bic, labels) return best_n_components, best_labels, init_methods_results # Test the function print(find_optimal_gmm(data, n_components_list, random_state)) ``` # Submission: Please submit a Python function `find_optimal_gmm` meeting the requirements outlined above.","solution":"def find_optimal_gmm(data, n_components_list, random_state): from sklearn.mixture import GaussianMixture best_n_components = None best_bic = float(\'inf\') best_labels = None # Dictionary to store results for each initialization method init_methods_results = {} # Evaluate the model for different number of components for n_components in n_components_list: gmm = GaussianMixture(n_components=n_components, random_state=random_state) gmm.fit(data) bic = gmm.bic(data) labels = gmm.predict(data) if bic < best_bic: best_bic = bic best_n_components = n_components best_labels = labels # Evaluate different initialization methods init_methods = [\'kmeans\', \'random\'] for init_method in init_methods: gmm = GaussianMixture(n_components=best_n_components, init_params=init_method, random_state=random_state) gmm.fit(data) bic = gmm.bic(data) labels = gmm.predict(data) init_methods_results[init_method] = (bic, labels) return best_n_components, best_labels, init_methods_results"},{"question":"# PyTorch JIT Compilation and Optimization Objective In this exercise, you are required to demonstrate your understanding of PyTorch\'s JIT compilation process. You will convert a PyTorch model into its JIT compiled version and perform inference using both the original and the compiled models. You will compare the performance of both versions of the model. Task 1. Implement a simple multilayer perceptron (MLP) model in PyTorch. 2. Convert the MLP model into its JIT compiled version using the appropriate utility from `torch.utils.jit`. 3. Perform inference using both the original and the JIT compiled models. 4. Compare the execution time of both versions. Instructions 1. Implement an MLP class in PyTorch. 2. Create an instance of the model and trace it using `torch.jit.trace` or script it using `torch.jit.script`. 3. Save the traced or scripted model to a file. 4. Load the saved model from the file. 5. Define a random input tensor for inference. 6. Measure the time taken to perform inference on the original model and the JIT compiled model. 7. Print the inference time comparison. Input Format - You do not need to handle any specific input format as the function will work internally. Output Format - Print the time taken for inference using the original model and the time taken for inference using the JIT compiled model. Constraints - You should use the PyTorch package. - Ensure that all necessary modules are imported. - Use random input for fair comparison. Example of an MLP class: ```python import torch import torch.nn as nn import time class MLP(nn.Module): def __init__(self, input_size, hidden_size, output_size): super(MLP, self).__init__() self.fc1 = nn.Linear(input_size, hidden_size) self.relu = nn.ReLU() self.fc2 = nn.Linear(hidden_size, output_size) def forward(self, x): out = self.fc1(x) out = self.relu(out) out = self.fc2(out) return out # Example workflow (detailed steps to be implemented by the student) model = MLP(input_size=5, hidden_size=10, output_size=2) input_tensor = torch.randn(1, 5) # Trace or Script the model # jit_model = torch.jit.trace(model, input_tensor) # or torch.jit.script(model) # Save and Load the model # torch.jit.save(jit_model, \'mlp_jit.pt\') # loaded_model = torch.jit.load(\'mlp_jit.pt\') # Measure inference time for original model # Measure inference time for JIT compiled model ``` Your task is to fill in the missing portions of the code and complete the implementation.","solution":"import torch import torch.nn as nn import time class MLP(nn.Module): def __init__(self, input_size, hidden_size, output_size): super(MLP, self).__init__() self.fc1 = nn.Linear(input_size, hidden_size) self.relu = nn.ReLU() self.fc2 = nn.Linear(hidden_size, output_size) def forward(self, x): out = self.fc1(x) out = self.relu(out) out = self.fc2(out) return out def measure_inference_time(model, input_tensor, num_repeats=1000): start_time = time.time() with torch.no_grad(): for _ in range(num_repeats): _ = model(input_tensor) end_time = time.time() return (end_time - start_time) / num_repeats # Create the model instance input_size = 5 hidden_size = 10 output_size = 2 model = MLP(input_size, hidden_size, output_size) # Create a random input tensor input_tensor = torch.randn(1, input_size) # Trace the model jit_model = torch.jit.trace(model, input_tensor) # Save and load the traced model jit_model_file = \'mlp_jit.pt\' torch.jit.save(jit_model, jit_model_file) loaded_jit_model = torch.jit.load(jit_model_file) # Measure inference time for the original model original_model_time = measure_inference_time(model, input_tensor) print(f\\"Original model inference time: {original_model_time:.6f} seconds\\") # Measure inference time for the JIT compiled model jit_model_time = measure_inference_time(loaded_jit_model, input_tensor) print(f\\"JIT compiled model inference time: {jit_model_time:.6f} seconds\\")"},{"question":"**Question: Implementing and Utilizing Custom Generic Types in Python** Python 3.9 introduced **GenericAlias**, which is part of the type hinting feature. For this assessment, your task is to create a custom class that utilizes GenericAlias to allow type hinting and demonstrate its usage with a few examples. # Task 1. Implement a custom generic class `MyContainer` that uses `GenericAlias`. 2. Your `MyContainer` class should support the following functionalities: - Initialization with a variable number of arguments. - Getting items from the container using an index. - Setting items in the container using an index. - Representing the container as a string. 3. Use the `Py_GenericAlias` function to add type hinting support to `MyContainer`. 4. Demonstrate usage of your generic class `MyContainer` with examples showing different types (integers, strings, custom objects). # Specifications 1. The class `MyContainer` must support the following methods: - `__init__(self, *args)`: Initializes the container with given arguments. - `__getitem__(self, index)`: Returns the item at the specified index. - `__setitem__(self, index, value)`: Sets the item at the specified index. - `__repr__(self)`: Returns a string representation of the container. 2. Use `GenericAlias` to create type hints for this class. # Example Below is an example to guide you on what the final implementation might look like: ```python class MyContainer: def __init__(self, *args): self._data = list(args) def __getitem__(self, index): return self._data[index] def __setitem__(self, index, value): self._data[index] = value def __repr__(self): return str(self._data) # Usage examples: int_container = MyContainer[int](1, 2, 3) print(int_container) # Output: [1, 2, 3] print(int_container[1]) # Output: 2 int_container[1] = 5 print(int_container) # Output: [1, 5, 3] str_container = MyContainer[str](\\"a\\", \\"b\\", \\"c\\") print(str_container) # Output: [\'a\', \'b\', \'c\'] print(str_container[2]) # Output: \'c\' str_container[2] = \\"d\\" print(str_container) # Output: [\'a\', \'b\', \'d\'] ``` # Constraints 1. Python version should be 3.9 or higher. 2. Ensure that type hinting works correctly with different data types. # Additional Notes - Utilize the `Py_GenericAlias` function carefully to set type hints for the `MyContainer` class. - Ensure that the `MyContainer` class behaves like a standard list but supports type hinting using `GenericAlias`. # Submission Submit your implementation of the `MyContainer` class along with the usage examples demonstrating its functionality with different data types.","solution":"from typing import Generic, TypeVar, List T = TypeVar(\'T\') class MyContainer(Generic[T]): def __init__(self, *args: T) -> None: self._data: List[T] = list(args) def __getitem__(self, index: int) -> T: return self._data[index] def __setitem__(self, index: int, value: T) -> None: self._data[index] = value def __repr__(self) -> str: return repr(self._data)"},{"question":"Create a Python module that includes a function and a class. Document this module using docstrings that contain interactive examples. Use the doctest module to verify the correctness of these examples. Function Specification 1. **Function Name:** `reverse_string` - **Input:** A single string `s`. - **Output:** The string reversed. Class Specification **Class Name:** `SimpleCalculator` This class should have methods to add, subtract, multiply, and divide two numbers. Document these methods with examples. # Requirements 1. **Implementation:** - Define the function `reverse_string` and ensure it reverses the input string. - Define the class `SimpleCalculator` with the following methods: - `add(a, b)` - `subtract(a, b)` - `multiply(a, b)` - `divide(a, b)` 2. **Documentation:** - Provide docstrings for the function and each method in the class, containing interactive Python examples demonstrating their usage. - Ensure the examples include possible error scenarios, such as division by zero. 3. **Verification:** - Use the doctest module to verify that all examples in your docstrings are correct. - Include a sample script that demonstrates how to run the doctest module to validate your implementations. # Testing 1. **Test the Function:** ```python if __name__ == \\"__main__\\": import doctest doctest.testmod() ``` 2. **Expected Usage Examples (docstrings):** - **reverse_string** ```python def reverse_string(s): Reverses the given string. >>> reverse_string(\'hello\') \'olleh\' >>> reverse_string(\'Python\') \'nohtyP\' return s[::-1] ``` - **SimpleCalculator** ```python class SimpleCalculator: A simple calculator class to perform basic arithmetic operations. def add(self, a, b): Returns the sum of a and b. >>> calc = SimpleCalculator() >>> calc.add(2, 3) 5 return a + b def subtract(self, a, b): Returns the subtraction result of b from a. >>> calc = SimpleCalculator() >>> calc.subtract(5, 3) 2 return a - b def multiply(self, a, b): Returns the multiplication result of a and b. >>> calc = SimpleCalculator() >>> calc.multiply(2, 3) 6 return a * b def divide(self, a, b): Returns the division result of a by b. >>> calc = SimpleCalculator() >>> calc.divide(10, 2) 5.0 >>> calc.divide(5, 0) Traceback (most recent call last): ... ZeroDivisionError: division by zero if b == 0: raise ZeroDivisionError(\\"division by zero\\") return a / b ``` Make sure both your function and class are thoroughly tested using doctest as described.","solution":"def reverse_string(s): Reverses the given string. >>> reverse_string(\'hello\') \'olleh\' >>> reverse_string(\'Python\') \'nohtyP\' return s[::-1] class SimpleCalculator: A simple calculator class to perform basic arithmetic operations. >>> calc = SimpleCalculator() >>> calc.add(2, 3) 5 >>> calc.subtract(5, 3) 2 >>> calc.multiply(2, 3) 6 >>> calc.divide(10, 2) 5.0 >>> calc.divide(5, 0) Traceback (most recent call last): ... ZeroDivisionError: division by zero def add(self, a, b): Returns the sum of a and b. >>> calc = SimpleCalculator() >>> calc.add(2, 3) 5 return a + b def subtract(self, a, b): Returns the subtraction result of b from a. >>> calc = SimpleCalculator() >>> calc.subtract(5, 3) 2 return a - b def multiply(self, a, b): Returns the multiplication result of a and b. >>> calc = SimpleCalculator() >>> calc.multiply(2, 3) 6 return a * b def divide(self, a, b): Returns the division result of a by b. >>> calc = SimpleCalculator() >>> calc.divide(10, 2) 5.0 >>> calc.divide(5, 0) Traceback (most recent call last): ... ZeroDivisionError: division by zero if b == 0: raise ZeroDivisionError(\\"division by zero\\") return a / b"},{"question":"# Resource Management and Usage Monitoring You are required to manage and monitor system resources using Python\'s `resource` module. This will involve limiting resource usage by processes and retrieving detailed resource consumption data. Problem Statement 1. **Resource Limiting**: Write a function `limit_cpu_time(seconds)` that limits the CPU time for the current process to `seconds` seconds. If the limit setting fails, catch the exception and print an error message. 2. **Resource Usage**: Write a function `get_process_usage(detail=False)` that retrieves the resource usage statistics for the current process. If `detail` is set to `True`, return a detailed dictionary with the following keys: - `user_time`: Time in user mode. - `sys_time`: Time in system mode. - `max_resident_set`: Maximum resident set size. - `shared_memory`: Shared memory size. - `unshared_memory`: Unshared memory size. - `unshared_stack`: Unshared stack size. - `page_faults`: Total page faults (sum of major and minor). If `detail` is `False`, return a summary tuple with only three values: - Total time spent by the process (user time + system time). - Number of page faults. - Voluntary context switches. 3. **Process Monitoring**: Write a function `monitor_cpu_limited_task(seconds, workload_function)` that: - First, calls `limit_cpu_time(seconds)` to restrict the CPU time of the workload function. - Executes the workload function provided as an argument. - Retrieves and prints the detailed resource usage using `get_process_usage(True)`. Input and Output Formats 1. `limit_cpu_time(seconds: int) -> None` - Input: `seconds` (int) - Maximum CPU time in seconds. - Output: No return value. Prints an error message if exception occurs. 2. `get_process_usage(detail: bool) -> Union[dict, tuple]` - Input: `detail` (bool) - Flag indicating if detailed usage information is required. - Output: A dictionary with detailed resource usage information or a summary tuple. 3. `monitor_cpu_limited_task(seconds: int, workload_function: Callable) -> None` - Input: - `seconds` (int) - Maximum CPU time in seconds. - `workload_function` (Callable) - A function representing the workload to execute. - Output: No return value. Executes the workload function and prints detailed resource usage statistics. Example ```python import resource def example_workload(): for i in range(10**7): pass # Example usage: monitor_cpu_limited_task(1, example_workload) ``` # Constraints - Handle `OSError` specifically for the cases where resource limits cannot be set. - Use constants and functions provided by the `resource` module. - Ensure cross-compatibility and handle platform-specific limitations where certain resources might not be available. # Performance Requirements Ensure that your implementation handles the setup and retrieval of resource limits efficiently without causing significant overhead or performance degradation.","solution":"import resource def limit_cpu_time(seconds): Limits the CPU time for the current process to `seconds` seconds. try: resource.setrlimit(resource.RLIMIT_CPU, (seconds, seconds)) except resource.error as e: print(f\\"Error setting CPU limit: {str(e)}\\") def get_process_usage(detail=False): Retrieves the resource usage statistics for the current process. If `detail` is set to `True`, returns a detailed dictionary. If `detail` is `False`, returns a summary tuple. usage = resource.getrusage(resource.RUSAGE_SELF) if detail: return { \'user_time\': usage.ru_utime, \'sys_time\': usage.ru_stime, \'max_resident_set\': usage.ru_maxrss, \'shared_memory\': usage.ru_ixrss, \'unshared_memory\': usage.ru_idrss, \'unshared_stack\': usage.ru_isrss, \'page_faults\': usage.ru_majflt + usage.ru_minflt, } else: return ( usage.ru_utime + usage.ru_stime, usage.ru_majflt + usage.ru_minflt, usage.ru_nvcsw ) def monitor_cpu_limited_task(seconds, workload_function): Limits the CPU time of the workload function, executes it, and retrieves the detailed resource usage. limit_cpu_time(seconds) workload_function() usage_details = get_process_usage(True) print(usage_details)"},{"question":"# String Manipulation and Data Cleaning with pandas You are given a dataset containing several columns of text data. Your task is to clean and transform this data into a more structured format using pandas. Specifically, you should demonstrate your understanding of the `StringDtype`, string methods, and regular expressions provided by pandas. Dataset You have the following DataFrame `df`: ```python import pandas as pd data = { \'names\': [\' Alice\', \'Bob \', \' Carol\', \'David\', \'Eve \'], \'emails\': [\' alice@example.com\', \' bob@example.com \', \'carol@example.com \', \' david@example.com\', \'eve@example.com \'], \'status\': [\'Active \', \'Active\', \'Inactive \', \'Inactive\', \'Active \'] } df = pd.DataFrame(data) ``` Requirements 1. **Trim Whitespace**: Remove leading and trailing whitespaces from all string columns. 2. **Lowercase Emails**: Convert all email addresses to lowercase. 3. **Extract Username and Domain**: Split the email addresses into two new columns: `username` and `domain`. 4. **Normalize Status**: Create a new column `status_normalized` where \'Active\' is represented as 1 and \'Inactive\' is represented as 0. 5. **Remove Prefix**: Any email addresses starting with \' \' (space) should have the prefix removed using `str.removeprefix`. Output The transformed DataFrame should look like this: ```python names emails status username domain status_normalized 0 Alice alice@example.com Active alice example.com 1 1 Bob bob@example.com Active bob example.com 1 2 Carol carol@example.com Inactive carol example.com 0 3 David david@example.com Inactive david example.com 0 4 Eve eve@example.com Active eve example.com 1 ``` Constraints - Use the `StringDtype` for string columns where applicable. - Ensure your solution is efficient and handles missing values gracefully (though the sample data provided does not contain any missing values). # Implementation Create a function `clean_dataframe(df: pd.DataFrame) -> pd.DataFrame` that takes the input DataFrame `df` and returns the transformed DataFrame as per the requirements listed above. **Function Signature**: ```python def clean_dataframe(df: pd.DataFrame) -> pd.DataFrame: pass ``` Example Usage ```python import pandas as pd data = { \'names\': [\' Alice\', \'Bob \', \' Carol\', \'David\', \'Eve \'], \'emails\': [\' alice@example.com\', \' bob@example.com \', \'carol@example.com \', \' david@example.com\', \'eve@example.com \'], \'status\': [\'Active \', \'Active\', \'Inactive \', \'Inactive\', \'Active \'] } df = pd.DataFrame(data) cleaned_df = clean_dataframe(df) print(cleaned_df) ``` Your solution should meet all the requirements and produce the desired output DataFrame when running the above example.","solution":"import pandas as pd def clean_dataframe(df: pd.DataFrame) -> pd.DataFrame: # Set StringDtype for all columns to ensure string operations are efficient df = df.astype({ \'names\': pd.StringDtype(), \'emails\': pd.StringDtype(), \'status\': pd.StringDtype() }) # Trim leading and trailing whitespace from all columns df[\'names\'] = df[\'names\'].str.strip() df[\'emails\'] = df[\'emails\'].str.strip() df[\'status\'] = df[\'status\'].str.strip() # Convert emails to lowercase df[\'emails\'] = df[\'emails\'].str.lower() # Remove prefix space if present df[\'emails\'] = df[\'emails\'].str.removeprefix(\' \') # Split emails into \'username\' and \'domain\' df[[\'username\', \'domain\']] = df[\'emails\'].str.extract(r\'([^@]+)@(.+)\') # Normalize status column df[\'status_normalized\'] = df[\'status\'].apply(lambda x: 1 if x.lower() == \'active\' else 0) return df"},{"question":"**Title: Advanced Data Visualization with Seaborn and Jitter** **Objective:** Assess the ability to use seaborn\'s `objects` API to create and customize data visualizations, with a focus on applying and adjusting jitter. **Problem Statement:** You are provided with the built-in `penguins` dataset from seaborn, which contains measurements for penguin species: - `species`: Species of the penguin (Adelie, Chinstrap, Gentoo). - `body_mass_g`: Body mass in grams. - `flipper_length_mm`: Flipper length in millimeters. Your task is to create a Python function named `create_custom_plot` that performs the following steps: 1. Loads the `penguins` dataset. 2. Creates a scatter plot with body mass and flipper length as the axes, adds jitter to the points using both `x` and `y` parameters, and customizes the jitter width. 3. Saves the plot as an image file named `penguins_plot.png`. **Function Signature:** ```python def create_custom_plot(): pass ``` **Requirements:** 1. Load the `penguins` dataset using `seaborn.load_dataset(\\"penguins\\")`. 2. Use `so.Plot` to create a scatter plot with: - `body_mass_g` on the x-axis. - `flipper_length_mm` on the y-axis. 3. Apply jitter to the points with: - `x` jitter of 100 units. - `y` jitter of 5 units. 4. Save the resulting plot as `penguins_plot.png`. **Input:** - None **Output:** - Saves an image file named `penguins_plot.png` containing the jittered scatter plot. **Constraints and Considerations:** - Ensure that the plot is saved successfully and can be viewed as an image file. - Utilize the seaborn `objects` API as demonstrated in the examples. - Handle any missing data in the dataset appropriately by either ignoring it or using an appropriate method to fill it. **Example Usage:** ```python create_custom_plot() # This will save the plot as \'penguins_plot.png\' ``` Implement the `create_custom_plot` function to fulfill the requirements specified above.","solution":"import seaborn as sns import seaborn.objects as so import matplotlib.pyplot as plt def create_custom_plot(): # Load the penguins dataset penguins = sns.load_dataset(\\"penguins\\") # Create a scatter plot with jitter on both axes plot = ( so.Plot(penguins, x=\'body_mass_g\', y=\'flipper_length_mm\') .add(so.Dot(), so.Jitter(x=100, y=5)) ) # Save the plot as an image file plot.save(\\"penguins_plot.png\\")"},{"question":"**Objective**: Implement a custom tensor operation and convert this operation into a TorchScript function to be optimizable. # Problem Statement You are required to create a function that: 1. Takes two 1D tensors `a` and `b` of the same length. 2. Computes a new tensor `c` where each element `c_i` is calculated as follows: - If the corresponding elements of `a` and `b` are equal, `c_i` should be `a_i * b_i`. - Otherwise, `c_i` should be `a_i + b_i`. Then, script this function using TorchScript to optimize and serialize it. # Function Signature ```python import torch from typing import List def compute_tensor(a: torch.Tensor, b: torch.Tensor) -> torch.Tensor: # Implementation here # TorchScript version scripted_compute_tensor = torch.jit.script(compute_tensor) ``` # Input - `a` (torch.Tensor): A 1D tensor of integers. - `b` (torch.Tensor): A 1D tensor of integers of the same length as `a`. # Output - Returns a 1D tensor `c`, computed based on the above logic. # Constraints - The length of `a` and `b` is at most `1000`. - Elements of `a` and `b` are integers in the range `[-100, 100]`. # Example ```python a = torch.tensor([1, 2, 3, 4]) b = torch.tensor([1, 2, 0, 4]) output = compute_tensor(a, b) # output => tensor([1, 4, 3, 16]) # TorchScript conversion scripted_output = scripted_compute_tensor(a, b) # scripted_output => tensor([1, 4, 3, 16]) ``` # Instructions 1. Implement the `compute_tensor` function. 2. Convert this function to a TorchScript function using `torch.jit.script`. 3. Ensure the outputs from the original and scripted functions are the same. **Note**: Your implementation should be efficient and leverage PyTorch operations to handle tensor computations.","solution":"import torch from typing import List def compute_tensor(a: torch.Tensor, b: torch.Tensor) -> torch.Tensor: Computes a new tensor where each element c_i is: - a_i * b_i if a_i == b_i - a_i + b_i otherwise c = torch.where(a == b, a * b, a + b) return c # TorchScript version scripted_compute_tensor = torch.jit.script(compute_tensor)"},{"question":"**Multithreaded Task Scheduler Using Queues** # Objective Implement a multithreaded task scheduler that uses different types of queues (`Queue`, `LifoQueue`, and `PriorityQueue`) to manage tasks. Each task will have a unique identifier and a processing time that simulates work. # Problem Statement Design and implement a `TaskScheduler` class with the following requirements: 1. **Initialization**: - The `TaskScheduler` should be initialized with a queue type (`FIFO`, `LIFO`, or `Priority`). - Based on the queue type, appropriate queue (`Queue`, `LifoQueue`, or `PriorityQueue`) should be initialized. 2. **Methods**: - `add_task(task_id: int, processing_time: float, priority: int = 0)`: Adds a task to the queue. - For `PriorityQueue`, the task should be a tuple `(priority, (task_id, processing_time))`. - `start_workers(num_workers: int)`: Starts the specified number of worker threads that consume tasks from the queue. - `join()`: Ensures that main thread waits until all tasks have been processed. 3. **Task Processing**: - Each worker thread should retrieve tasks from the queue and simulate task execution by sleeping for `processing_time` seconds. - After processing a task, the worker should call `task_done` on the queue. # Input & Output Format - The `TaskScheduler` class must handle three types of queue initialization and process tasks accordingly. # Constraints - You may assume there are no more than 1000 tasks. - `processing_time` is a float between 0.1 to 1.0 seconds. - For simplicity, all operations are blocking and no timeout features are considered. # Example ```python import time import queue import threading class TaskScheduler: def __init__(self, queue_type: str): if queue_type == \'FIFO\': self.queue = queue.Queue() elif queue_type == \'LIFO\': self.queue = queue.LifoQueue() elif queue_type == \'Priority\': self.queue = queue.PriorityQueue() else: raise ValueError(\'Invalid queue type\') def add_task(self, task_id: int, processing_time: float, priority: int = 0): if isinstance(self.queue, queue.PriorityQueue): self.queue.put((priority, (task_id, processing_time))) else: self.queue.put((task_id, processing_time)) def start_workers(self, num_workers: int): def worker(): while True: task = self.queue.get() if isinstance(self.queue, queue.PriorityQueue): _, (task_id, processing_time) = task else: task_id, processing_time = task print(f\'Starting task {task_id}\') time.sleep(processing_time) print(f\'Completed task {task_id}\') self.queue.task_done() for _ in range(num_workers): threading.Thread(target=worker, daemon=True).start() def join(self): self.queue.join() # Example usage scheduler = TaskScheduler(\'FIFO\') scheduler.add_task(1, 0.5) scheduler.add_task(2, 0.3) scheduler.start_workers(2) scheduler.join() ``` In this example: - A `TaskScheduler` object is created with `FIFO` queue type. - Two tasks are added with IDs 1 and 2. - Two worker threads are started to process tasks concurrently. - The main thread waits until all tasks are completed.","solution":"import time import queue import threading class TaskScheduler: def __init__(self, queue_type: str): if queue_type == \'FIFO\': self.queue = queue.Queue() elif queue_type == \'LIFO\': self.queue = queue.LifoQueue() elif queue_type == \'Priority\': self.queue = queue.PriorityQueue() else: raise ValueError(\'Invalid queue type\') def add_task(self, task_id: int, processing_time: float, priority: int = 0): if isinstance(self.queue, queue.PriorityQueue): self.queue.put((priority, (task_id, processing_time))) else: self.queue.put((task_id, processing_time)) def start_workers(self, num_workers: int): def worker(): while True: task = self.queue.get() if isinstance(self.queue, queue.PriorityQueue): _, (task_id, processing_time) = task else: task_id, processing_time = task print(f\'Starting task {task_id}\') time.sleep(processing_time) print(f\'Completed task {task_id}\') self.queue.task_done() for _ in range(num_workers): threading.Thread(target=worker, daemon=True).start() def join(self): self.queue.join()"},{"question":"**Objective:** The objective of this question is to assess your ability to use scikit-learn to load, preprocess, and visualize a real-world dataset, as well as to apply a basic machine learning algorithm to it. # Problem Statement: You are required to fetch the \'Olivetti faces\' dataset using scikit-learn\'s dataset-loading utilities. Once loaded, you need to preprocess the data and then apply a machine learning algorithm to classify the images. 1. **Load the Dataset:** Write a function `load_olivetti_faces()` that fetches the \'Olivetti faces\' dataset using `fetch_olivetti_faces` from `sklearn.datasets`. 2. **Preprocess the Data:** Split the dataset into training and testing sets. Use `train_test_split` from `sklearn.model_selection` to split the images and their corresponding target labels in an 80-20 ratio. 3. **Train a Classifier:** - Train a k-Nearest Neighbors (k-NN) classifier on the training set. Use `KNeighborsClassifier` from `sklearn.neighbors`. - Use a value of `k=5` for the number of neighbors. 4. **Evaluate the Model:** - Evaluate the accuracy of the classifier on the testing set. - Plot a sample of the test images along with their predicted labels and the true labels. # Function Signature ```python import matplotlib.pyplot as plt from sklearn.datasets import fetch_olivetti_faces from sklearn.model_selection import train_test_split from sklearn.neighbors import KNeighborsClassifier from sklearn.metrics import accuracy_score def load_olivetti_faces(): Function to fetch and load the Olivetti faces dataset. Returns: (dict): sklearn.utils.Bunch object containing the dataset. pass def preprocess_data(data): Function to split the data into training and testing sets. Parameters: data (dict): The fetched Olivetti faces dataset. Returns: X_train, X_test, y_train, y_test: Split data into training and testing sets. pass def train_knn_classifier(X_train, y_train, k=5): Function to train a k-NN classifier. Parameters: X_train (ndarray): Training feature data. y_train (ndarray): Training labels. k (int): Number of neighbors to use. Returns: (KNeighborsClassifier): Trained k-NN classifier. pass def evaluate_and_plot(X_test, y_test, model): Function to evaluate the model on the testing set and to plot some test images with their predicted and actual labels. Parameters: X_test (ndarray): Testing feature data. y_test (ndarray): Testing labels. model (KNeighborsClassifier): Trained k-NN classifier. pass ``` # Constraints and Requirements: 1. You must use the `fetch_olivetti_faces()` function to load the dataset. 2. Use 80-20 train-test split for the data. 3. Use k-NN classifier with `k=5`. 4. Evaluate the model using accuracy score and display it. 5. Plot at least 5 test images with their predicted and actual labels. # Example After implementing the functions, the final part of your script should look like this: ```python data = load_olivetti_faces() X_train, X_test, y_train, y_test = preprocess_data(data) knn_model = train_knn_classifier(X_train, y_train) evaluate_and_plot(X_test, y_test, knn_model) ``` This will load the dataset, split it, train a k-NN classifier, and evaluate and plot the results. # Notes - Ensure you handle the data correctly and efficiently. - Use appropriate labels and titles in your plot for clarity. - Your code will be evaluated based on correctness, efficiency, and clarity of visualization.","solution":"import matplotlib.pyplot as plt from sklearn.datasets import fetch_olivetti_faces from sklearn.model_selection import train_test_split from sklearn.neighbors import KNeighborsClassifier from sklearn.metrics import accuracy_score def load_olivetti_faces(): Function to fetch and load the Olivetti faces dataset. Returns: dict: sklearn.utils.Bunch object containing the dataset. data = fetch_olivetti_faces() return data def preprocess_data(data): Function to split the data into training and testing sets. Parameters: data (dict): The fetched Olivetti faces dataset. Returns: tuple: (X_train, X_test, y_train, y_test), split data into training and testing sets. X_train, X_test, y_train, y_test = train_test_split(data.images.reshape((data.images.shape[0], -1)), data.target, test_size=0.2, random_state=0) return X_train, X_test, y_train, y_test def train_knn_classifier(X_train, y_train, k=5): Function to train a k-NN classifier. Parameters: X_train (ndarray): Training feature data. y_train (ndarray): Training labels. k (int): Number of neighbors to use. Returns: KNeighborsClassifier: Trained k-NN classifier. knn = KNeighborsClassifier(n_neighbors=k) knn.fit(X_train, y_train) return knn def evaluate_and_plot(X_test, y_test, model): Function to evaluate the model on the testing set and to plot some test images with their predicted and actual labels. Parameters: X_test (ndarray): Testing feature data. y_test (ndarray): Testing labels. model (KNeighborsClassifier): Trained k-NN classifier. # Predict the labels y_pred = model.predict(X_test) # Calculate and print the accuracy accuracy = accuracy_score(y_test, y_pred) print(f\\"Accuracy: {accuracy * 100:.2f}%\\") # Plot 5 test images with their predicted and actual labels plt.figure(figsize=(10, 5)) for i in range(5): plt.subplot(1, 5, i + 1) plt.imshow(X_test[i].reshape((64, 64)), cmap=\'gray\') plt.title(f\\"Pred: {y_pred[i]}nTrue: {y_test[i]}\\") plt.axis(\'off\') plt.show()"},{"question":"Problem Statement You are tasked with creating a Python function that compresses a sequence of text data, computes the checksum of the original and compressed data, and then decompresses the data to ensure it matches the original. This task will test your ability to use the `zlib` module effectively. # Function Signature ```python def compress_decompress_checksum(data_list: list[str], compress_level: int = -1) -> dict: ``` # Input - `data_list`: A list of strings, where each string represents a chunk of text data to be compressed and decompressed. - `compress_level`: (optional) An integer from 0 to 9 or -1 controlling the level of compression. Default is -1 (Z_DEFAULT_COMPRESSION). # Output - A dictionary with the following keys and values: - `\'original_checksums\'`: A list of unsigned 32-bit integers, each representing the CRC32 checksum of a corresponding string in the input `data_list`. - `\'compressed_checksums\'`: A list of unsigned 32-bit integers, each representing the CRC32 checksum of the compressed data corresponding to each string in the input `data_list`. - `\'is_match\'`: A boolean indicating whether the decompressed data matches the original input data. # Requirements 1. The function should use `zlib.compress` and `zlib.decompress` for compression and decompression. 2. The function should use `zlib.crc32` to compute checksums for both original and compressed data. 3. The function should check whether decompressed data matches the original data. # Example ```python data = [\\"Hello, World!\\", \\"The quick brown fox jumps over the lazy dog\\"] result = compress_decompress_checksum(data, compress_level=6) # Expected output: # { # \'original_checksums\': [b\'Hello, World!\', b\'The quick brown fox jumps over the lazy dog\'], # \'compressed_checksums\': [CRC32 value of compressed \\"Hello, World!\\", CRC32 value of compressed \\"The quick brown fox jumps over the lazy dog\\"], # \'is_match\': True # } ``` # Constraints - The strings in `data_list` will only contain printable ASCII characters. - Each string in `data_list` can have a length up to 10,000 characters. - The function should handle memory efficiently when dealing with large strings. # Notes - For calculating checksums, refer to the `zlib.crc32` function. - Ensure proper handling of any zlib specific exceptions.","solution":"import zlib def compress_decompress_checksum(data_list: list[str], compress_level: int = -1) -> dict: original_checksums = [] compressed_checksums = [] decompressed_data_list = [] for data in data_list: # Calculate checksum for original data original_checksum = zlib.crc32(data.encode(\'utf-8\')) & 0xffffffff original_checksums.append(original_checksum) # Compress the data compressed_data = zlib.compress(data.encode(\'utf-8\'), compress_level) # Calculate checksum for compressed data compressed_checksum = zlib.crc32(compressed_data) & 0xffffffff compressed_checksums.append(compressed_checksum) # Decompress the data decompressed_data = zlib.decompress(compressed_data).decode(\'utf-8\') decompressed_data_list.append(decompressed_data) # Check if decompressed data matches original data is_match = data_list == decompressed_data_list return { \'original_checksums\': original_checksums, \'compressed_checksums\': compressed_checksums, \'is_match\': is_match }"},{"question":"**Question: Implementing an Asynchronous Task Scheduler** You are requested to implement an asynchronous task scheduler using Python\'s asyncio module. The task scheduler should be able to perform the following: 1. **Run multiple asynchronous tasks concurrently.** 2. **Distribute tasks among worker coroutines using different types of queues (FIFO, Priority, LIFO).** 3. **Implement timeout functionality to cancel long-running tasks.** 4. **Gracefully handle task cancellations.** 5. **Use synchronization primitives to ensure thread-safe operations.** # Requirements: 1. **Function Definitions:** - `schedule_tasks(task_coros: List[Coroutine], queue_type: str, worker_count: int, timeout: int) -> List[Any]` 2. **Parameters:** - `task_coros`: List of coroutine functions to be run as tasks. - `queue_type`: Type of queue to use for distributing tasks among workers. Possible values: `\\"FIFO\\"`, `\\"Priority\\"`, `\\"LIFO\\"`. - `worker_count`: Number of worker coroutines to process tasks. - `timeout`: Maximum time in seconds to allow for each task before timing out. 3. **Expected Output:** - A list of results from the completed tasks. The order of results should correspond to the order of input tasks. - If a task times out or gets canceled, the corresponding result should be `None`. 4. **Constraints:** - Each task should be a coroutine function. - The scheduler should use the appropriate asyncio.Queue based on the `queue_type`. - Implement appropriate exception handling for timeouts and cancellations. 5. **Example Usage:** ```python import asyncio from typing import List, Coroutine, Any async def sample_task(duration: int, result: str) -> str: await asyncio.sleep(duration) return result async def main(): tasks = [sample_task(3, \\"Task1\\"), sample_task(2, \\"Task2\\"), sample_task(1, \\"Task3\\")] results = await schedule_tasks(tasks, \\"FIFO\\", 2, 4) print(results) # Expected: [\'Task1\', \'Task2\', \'Task3\'] # Run main() asyncio.run(main()) ``` 6. **Implementation Details:** - The function should leverage asyncio\'s `create_task`, `wait_for`, and appropriate queue (`Queue`, `PriorityQueue`, or `LifoQueue`). - Process tasks using worker coroutines. - Ensure the function is robust and handles exceptions gracefully. # Notes: - You may assume that for a `PriorityQueue`, each task coroutine\'s duration indicates its priority (lower duration means higher priority). - Use appropriate synchronization primitives if required to handle shared resources. - Make sure to test the function with different types of queues and varying timeout values.","solution":"import asyncio from asyncio import Queue, PriorityQueue, LifoQueue from typing import List, Coroutine, Any class Task: def __init__(self, priority, coro): self.priority = priority self.coro = coro def __lt__(self, other): return self.priority < other.priority async def worker(queue, results, timeout): while True: task = await queue.get() if task is None: break try: result = await asyncio.wait_for(task.coro, timeout=timeout) results.append(result) except asyncio.TimeoutError: results.append(None) except asyncio.CancelledError: results.append(None) finally: queue.task_done() async def schedule_tasks(task_coros: List[Coroutine], queue_type: str, worker_count: int, timeout: int) -> List[Any]: if queue_type == \\"Priority\\": queue = PriorityQueue() tasks = [Task(coro.get_timeout(), coro) for coro in task_coros] for task in tasks: await queue.put(task) elif queue_type == \\"LIFO\\": queue = LifoQueue() for coro in task_coros: await queue.put(Task(0, coro)) elif queue_type == \\"FIFO\\": queue = Queue() for coro in task_coros: await queue.put(Task(0, coro)) results = [] workers = [asyncio.create_task(worker(queue, results, timeout)) for _ in range(worker_count)] await queue.join() for _ in workers: await queue.put(None) await asyncio.gather(*workers) return results"},{"question":"Objective You are provided with a dataset of geographical points representing sightings of a species in a certain region. Your task is to perform detailed density estimation on this dataset using scikit-learn\'s `KernelDensity` class. Instructions 1. **Data Loading:** - You are provided with a CSV file `species_sightings.csv` that contains two columns: `latitude` and `longitude`. 2. **Function Implementation:** Implement the following three functions: a. `load_data(file_path: str) -> np.ndarray`: - This function should load the species sightings data from the CSV file and return it as a 2D NumPy array. b. `perform_kde(data: np.ndarray, kernel: str, bandwidth: float) -> KernelDensity`: - This function should create and fit a `KernelDensity` estimator using the provided data, kernel, and bandwidth. - Inputs: - `data`: 2D NumPy array of shape (n_samples, 2) where each row represents a geographical point. - `kernel`: A string representing the kernel to be used (e.g., \'gaussian\', \'tophat\', \'epanechnikov\', \'exponential\', \'linear\', \'cosine\'). - `bandwidth`: A float representing the bandwidth parameter for KDE. - Output: - A fitted `KernelDensity` estimator. c. `evaluate_log_density(estimator: KernelDensity, points: np.ndarray) -> np.ndarray`: - This function should return the log density estimates for given points using the provided KernelDensity estimator. - Inputs: - `estimator`: A fitted `KernelDensity` estimator. - `points`: A 2D NumPy array of shape (n_samples, 2) with the points for which the density is to be estimated. - Output: - A NumPy array of log density estimates for the given points. 3. **Bonus Task: Bandwidth Optimization** - Implement an additional function `optimize_bandwidth(data: np.ndarray, kernel: str, bandwidths: list) -> float` to identify the optimal bandwidth from a given list of candidate bandwidths. Use cross-validation or a similar approach to determine the best bandwidth. - Inputs: - `data`: 2D NumPy array of shape (n_samples, 2). - `kernel`: A string indicating the kernel to be used. - `bandwidths`: A list of floats representing the candidates for the bandwidth parameter. - Output: - The optimal bandwidth value from the provided list. Constraints - The CSV file `species_sightings.csv` can be assumed to be well-formed and error-free. - Ensure the implemented functions handle edge cases gracefully. Example Usage ```python # Example usage of the functions data = load_data(\'species_sightings.csv\') estimator = perform_kde(data, kernel=\'gaussian\', bandwidth=0.5) points = np.array([[34.5, -120.7], [35.1, -120.3]]) log_density = evaluate_log_density(estimator, points) print(log_density) # For the Bonus Task optimal_bandwidth = optimize_bandwidth(data, \'gaussian\', [0.1, 0.5, 1.0, 2.0]) print(f\\"Optimal Bandwidth: {optimal_bandwidth}\\") ``` # Note Make sure to import necessary libraries such as `numpy`, `pandas`, and `sklearn.utils`.","solution":"import pandas as pd import numpy as np from sklearn.neighbors import KernelDensity from sklearn.model_selection import GridSearchCV def load_data(file_path: str) -> np.ndarray: Load the species sightings data from a CSV file. Parameters: - file_path (str): The path to the CSV file. Returns: - data (np.ndarray): A 2D NumPy array with columns [latitude, longitude]. df = pd.read_csv(file_path) return df[[\'latitude\', \'longitude\']].to_numpy() def perform_kde(data: np.ndarray, kernel: str, bandwidth: float) -> KernelDensity: Create and fit a KernelDensity estimator using the provided data, kernel, and bandwidth. Parameters: - data (np.ndarray): A 2D NumPy array with shape (n_samples, 2). - kernel (str): The kernel to use for KDE (\'gaussian\', \'tophat\', etc.). - bandwidth (float): The bandwidth parameter for KDE. Returns: - kde (KernelDensity): A fitted KernelDensity estimator. kde = KernelDensity(kernel=kernel, bandwidth=bandwidth) kde.fit(data) return kde def evaluate_log_density(estimator: KernelDensity, points: np.ndarray) -> np.ndarray: Return the log density estimates for given points using the provided KernelDensity estimator. Parameters: - estimator (KernelDensity): A fitted KernelDensity estimator. - points (np.ndarray): A 2D NumPy array with the points to evaluate. Returns: - log_density (np.ndarray): A NumPy array of log density estimates for the given points. log_density = estimator.score_samples(points) return log_density def optimize_bandwidth(data: np.ndarray, kernel: str, bandwidths: list) -> float: Identify the optimal bandwidth from a given list of candidate bandwidths using cross-validation. Parameters: - data (np.ndarray): A 2D NumPy array with shape (n_samples, 2). - kernel (str): The kernel to use for KDE (\'gaussian\', \'tophat\', etc.). - bandwidths (list): A list of candidate bandwidths. Returns: - best_bandwidth (float): The optimal bandwidth value from the provided list. params = {\'bandwidth\': bandwidths} grid_search = GridSearchCV(KernelDensity(kernel=kernel), params, cv=5) grid_search.fit(data) best_bandwidth = grid_search.best_estimator_.bandwidth return best_bandwidth"},{"question":"Objective Write a PyTorch training script that is compliant with `torchrun` using the torch elastic features for distributed training. Your script should initialize a distributed process group, perform training with checkpointing, and be able to recover from worker failures. # Requirements 1. **Initialization and Argument Parsing**: - Parse command-line arguments for backend type and checkpoint path. - Load checkpoint if available and initialize the training state. 2. **Distributed Training Setup**: - Initialize the distributed process group using the backend specified in the arguments. 3. **Training Loop**: - Implement a training loop that iterates through epochs and batches in a dataset. - Train the model on each batch. - Save checkpoints at the end of each epoch. 4. **Checkpointing**: - Implement `save_checkpoint(path, state)` and `load_checkpoint(path)` functions. - Ensure the training can resume from the last saved checkpoint in case of a failure. # Input and Output - **Input**: - Arguments for backend type (`str`) and checkpoint path (`str`). - The dataset and model should be pre-defined. - **Output**: - The script should print progress messages (e.g., current epoch, batch number, etc.). - Save and load checkpoints from the specified path. # Constraints - Your script should be able to handle failures and resume training from the last checkpoint. - Assume a predefined dataset and a simple model for training. - You do not need to implement complex model architectures or data preprocessing; focus on the distributed and checkpointing functionality. Example Assume the following dataset and model for simplicity: ```python import torch import torch.distributed as dist import torch.nn as nn import torch.optim as optim from torch.utils.data import DataLoader, TensorDataset # Dummy dataset data = torch.randn(1000, 10) targets = torch.randn(1000, 1) dataset = TensorDataset(data, targets) dataloader = DataLoader(dataset, batch_size=32) # Simple model class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.linear = nn.Linear(10, 1) def forward(self, x): return self.linear(x) model = SimpleModel() ``` Your Task Complete the main function with the required functionalities: ```python import os import sys import torch import torch.distributed as dist import torch.optim as optim from torch.nn.parallel import DistributedDataParallel as DDP def save_checkpoint(path, state): # Implement saving state to path pass def load_checkpoint(path): # Implement loading state from path pass def main(): args = parse_args(sys.argv[1:]) state = load_checkpoint(args.checkpoint_path) initialize(state) dist.init_process_group(backend=args.backend) # Wrap model with DDP model = SimpleModel().to(state.device) model = DDP(model) optimizer = optim.SGD(model.parameters(), lr=0.01) for epoch in range(state.epoch, state.total_num_epochs): for batch in dataloader: optimizer.zero_grad() outputs = model(batch[0].to(state.device)) loss = loss_fn(outputs, batch[1].to(state.device)) loss.backward() optimizer.step() state.epoch += 1 save_checkpoint(args.checkpoint_path, state) if __name__ == \\"__main__\\": main() ``` Ensure the training can be conducted on a distributed setup and can recover from failures by loading the checkpoints.","solution":"import argparse import os import sys import torch import torch.distributed as dist import torch.optim as optim from torch.nn.parallel import DistributedDataParallel as DDP from torch.utils.data import DataLoader, TensorDataset import torch.nn as nn # Define the dummy dataset and model data = torch.randn(1000, 10) targets = torch.randn(1000, 1) dataset = TensorDataset(data, targets) dataloader = DataLoader(dataset, batch_size=32) class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.linear = nn.Linear(10, 1) def forward(self, x): return self.linear(x) def save_checkpoint(path, state): torch.save(state, path) def load_checkpoint(path): if os.path.exists(path): return torch.load(path) return None def parse_args(args): parser = argparse.ArgumentParser(description=\\"PyTorch Distributed Training Example\\") parser.add_argument(\\"--backend\\", type=str, default=\\"nccl\\") parser.add_argument(\\"--checkpoint-path\\", type=str, default=\\"checkpoint.pth\\") return parser.parse_args(args) def main(): args = parse_args(sys.argv[1:]) state = load_checkpoint(args.checkpoint_path) if state is None: state = { \'epoch\': 0, \'total_num_epochs\': 10, \'model_state_dict\': None, \'optimizer_state_dict\': None, } dist.init_process_group(backend=args.backend) # Model and optimizer setup model = SimpleModel().to(torch.device(\\"cuda\\" if torch.cuda.is_available() else \\"cpu\\")) if state[\'model_state_dict\']: model.load_state_dict(state[\'model_state_dict\']) model = DDP(model) optimizer = optim.SGD(model.parameters(), lr=0.01) if state[\'optimizer_state_dict\']: optimizer.load_state_dict(state[\'optimizer_state_dict\']) loss_fn = nn.MSELoss() device = torch.device(\\"cuda\\" if torch.cuda.is_available() else \\"cpu\\") for epoch in range(state[\'epoch\'], state[\'total_num_epochs\']): for batch in dataloader: optimizer.zero_grad() inputs, targets = batch inputs, targets = inputs.to(device), targets.to(device) outputs = model(inputs) loss = loss_fn(outputs, targets) loss.backward() optimizer.step() state[\'epoch\'] += 1 state[\'model_state_dict\'] = model.state_dict() state[\'optimizer_state_dict\'] = optimizer.state_dict() save_checkpoint(args.checkpoint_path, state) print(f\\"Epoch {state[\'epoch\']} completed\\") if __name__ == \\"__main__\\": main()"},{"question":"# Dataclass-based Hierarchical Inventory System You are tasked with implementing a hierarchical inventory system for an online retail store using Python\'s `dataclasses` module. The system should allow the creation of different types of items and manage their stock levels. The system should also provide functionalities to calculate the total stock value and update stock levels across different categories. # Requirements 1. **Base Item Class** - Create a dataclass named `BaseItem` with the following attributes: - `name`: a `str` representing the item name. - `unit_price`: a `float` representing the price per unit. - `quantity_on_hand`: an `int` representing the number of units available. Default is `0`. - Implement a method `total_cost` that returns the total cost of the item (i.e., `unit_price * quantity_on_hand`). 2. **Category Class** - Create a dataclass named `Category` that represents an inventory category. It should include: - `category_name`: a `str` representing the category name. - `items`: a `list` of `BaseItem` objects. - Implement a method `total_category_cost` that calculates and returns the total cost of all items in the category. 3. **Warehouse Class** - Create a dataclass named `Warehouse` that manages multiple categories. It should include: - `warehouse_name`: a `str` representing the warehouse name. - `categories`: a `list` of `Category` objects representing different categories in the warehouse. - Implement a method `total_warehouse_cost` that calculates and returns the total cost of all categories in the warehouse. - Implement a method `update_stock` that takes an `item_name`, `category_name`, and `new_quantity`, and updates the quantity of the specified item in the specified category. # Constraints - Use `dataclass` decorators and its features to create the classes. - Leverage `field` and `default_factory` where appropriate, especially for mutable default parameters. - Ensure the implementation is efficient for large inventories. # Example Usage ```python from dataclasses import dataclass, field from typing import List # Define your BaseItem, Category, and Warehouse classes here # Example code to demonstrate usage if __name__ == \\"__main__\\": # Creating items item1 = BaseItem(name=\\"Laptop\\", unit_price=1500.00, quantity_on_hand=5) item2 = BaseItem(name=\\"Smartphone\\", unit_price=800.00, quantity_on_hand=10) # Creating a category electronics = Category(category_name=\\"Electronics\\", items=[item1, item2]) # Creating a warehouse warehouse = Warehouse(warehouse_name=\\"Main Warehouse\\", categories=[electronics]) # Calculating total costs print(warehouse.total_warehouse_cost()) # Should output 15000 (for laptops) + 8000 (for smartphones) # Updating stock warehouse.update_stock(item_name=\\"Laptop\\", category_name=\\"Electronics\\", new_quantity=7) print(warehouse.total_warehouse_cost()) # Should reflect the updated stock levels ``` # Input and Output Specifications - Your solution should define the three classes (`BaseItem`, `Category`, and `Warehouse`) with the specified methods. - The `total_cost`, `total_category_cost`, and `total_warehouse_cost` methods should calculate the respective total costs. - The `update_stock` method should correctly update the respective item\'s `quantity_on_hand`. # Performance Requirements - The solution should handle updating and calculating costs efficiently, especially considering potentially large inventories in real-world scenarios.","solution":"from dataclasses import dataclass, field from typing import List @dataclass class BaseItem: name: str unit_price: float quantity_on_hand: int = 0 def total_cost(self) -> float: return self.unit_price * self.quantity_on_hand @dataclass class Category: category_name: str items: List[BaseItem] = field(default_factory=list) def total_category_cost(self) -> float: return sum(item.total_cost() for item in self.items) @dataclass class Warehouse: warehouse_name: str categories: List[Category] = field(default_factory=list) def total_warehouse_cost(self) -> float: return sum(category.total_category_cost() for category in self.categories) def update_stock(self, item_name: str, category_name: str, new_quantity: int): for category in self.categories: if category.category_name == category_name: for item in category.items: if item.name == item_name: item.quantity_on_hand = new_quantity return"},{"question":"Objective: Design a multi-client network echo server using the `select` module in Python. The server should be able to handle multiple clients concurrently, echoing back any messages it receives from them. Problem Statement: Implement a function `multi_client_echo_server(host: str, port: int) -> None` that launches an echo server listening on the specified hostname and port. The server should use the `select` module to handle multiple clients concurrently. Requirements: 1. The server should leverage the `select.select()` function to manage multiple clients. 2. The server should echo back any message it receives from a client. 3. The server should handle clients connecting, sending messages, and disconnecting gracefully. 4. Ensure that the server can handle the `OSError` and close connections properly when a client disconnects. 5. Optimize the server to handle sudden influxes of client connections efficiently. Input: - `host` (str): The hostname on which the server should listen. - `port` (int): The port number on which the server should listen. Example: ```python multi_client_echo_server(\'localhost\', 9000) ``` The server will start listening on `localhost` port `9000`. When a client connects and sends a message, the server echoes the message back to the client. Constraints: - Use only Python\'s built-in libraries. No third-party modules allowed. - The server should handle at least 50 concurrent client connections. - The server should be robust against client terminations and network errors. Performance: - The server should be responsive and handle multiple clients efficiently without significant delays. Notes: - You may want to use Python\'s `socket` library to create and manage sockets. - Consider edge cases like a client sending an empty message or disconnecting abruptly.","solution":"import select import socket def multi_client_echo_server(host: str, port: int) -> None: Launches an echo server listening on the specified hostname and port, handling multiple clients concurrently. server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM) server_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1) server_socket.bind((host, port)) server_socket.listen(5) print(f\'Server started on {host}:{port}\') sockets_list = [server_socket] clients = {} try: while True: read_sockets, _, exception_sockets = select.select(sockets_list, [], sockets_list) for notified_socket in read_sockets: if notified_socket == server_socket: client_socket, client_address = server_socket.accept() print(f\'Accepted new connection from {client_address[0]}:{client_address[1]}\') sockets_list.append(client_socket) clients[client_socket] = client_address else: message = notified_socket.recv(1024) if message: print(f\'Received message from {clients[notified_socket]}: {message.decode(\\"utf-8\\")}\') notified_socket.send(message) else: print(f\'Closed connection from {clients[notified_socket]}\') sockets_list.remove(notified_socket) del clients[notified_socket] for notified_socket in exception_sockets: sockets_list.remove(notified_socket) del clients[notified_socket] except KeyboardInterrupt: print(\'Server shutting down...\') finally: server_socket.close()"},{"question":"**Objective**: Assess the understanding of Python\'s C API in handling attributes, comparison, and other operations on Python objects. **Problem Statement**: You are required to implement a Python C extension module that provides the following functionalities: 1. **Attribute Management**: - A function `has_attr` that checks whether a given Python object has a specified attribute. - A function `get_attr` that retrieves the value of a specified attribute from a given Python object. - A function `set_attr` that sets the value of a specified attribute on a given Python object. - A function `del_attr` that deletes a specified attribute from a given Python object. 2. **Comparison Operations**: - A function `compare_objects` that compares two Python objects using a specified operator (e.g., `<`, `<=`, `==`, `!=`, `>`, `>=`). 3. **Iteration and Containment**: - A function `get_iterator` that returns an iterator for a given Python object. - A function `get_item` that returns an item at a given key/index from a given Python object. - A function `set_item` that sets the value at a given key/index on a given Python object. - A function `del_item` that deletes the item at a given key/index from a given Python object. - A function `is_instance` that checks if an object is an instance of a given class. 4. **Representation and Printing**: - A function `print_object` that prints a Python object using the equivalent of the `print` statement. **Function Prototypes**: ```c // Attribute Management PyObject *has_attr(PyObject *self, PyObject *args); PyObject *get_attr(PyObject *self, PyObject *args); PyObject *set_attr(PyObject *self, PyObject *args); PyObject *del_attr(PyObject *self, PyObject *args); // Comparison Operations PyObject *compare_objects(PyObject *self, PyObject *args); // Iteration and Containment PyObject *get_iterator(PyObject *self, PyObject *args); PyObject *get_item(PyObject *self, PyObject *args); PyObject *set_item(PyObject *self, PyObject *args); PyObject *del_item(PyObject *self, PyObject *args); PyObject *is_instance(PyObject *self, PyObject *args); // Representation and Printing PyObject *print_object(PyObject *self, PyObject *args); ``` **Input**: - The functions will take appropriate arguments as per Python C API conventions. **Output**: - The functions should return appropriate Python objects or handle errors as per the documentation. **Constraints**: 1. You must handle errors appropriately and raise Python exceptions where necessary. 2. Ensure type compatibility and manage references as required by the Python C API. **Performance Requirements**: - Efficient memory management and appropriate error handling are paramount. - The functions should ideally operate with a complexity that is linear to the size/nature of the input to reflect efficient API usage. Evaluate the implementation using sample Python scripts that utilize the extension to ensure all functionalities are working as expected. **Example**: ```python import your_extension_module as yext obj = {\'key\': \'value\'} # Attribute management assert yext.has_attr(obj, \'key\') == True assert yext.get_attr(obj, \'key\') == \'value\' yext.set_attr(obj, \'new_key\', \'new_value\') assert obj[\'new_key\'] == \'new_value\' yext.del_attr(obj, \'new_key\') assert \'new_key\' not in obj # Comparison assert yext.compare_objects(1, 2, \'<\') == True assert yext.compare_objects(2, 2, \'==\') == True assert yext.compare_objects(2, 1, \'>\') == True # Iteration and containment iterator = yext.get_iterator(obj) assert next(iterator) == \'key\' assert yext.get_item(obj, \'key\') == \'value\' yext.set_item(obj, \'key\', \'new_value\') assert obj[\'key\'] == \'new_value\' yext.del_item(obj, \'key\') assert \'key\' not in obj assert yext.is_instance(obj, dict) == True # Representation and printing yext.print_object(obj) # Should print \'{}\' ``` **Note**: You should also provide detailed documentation and annotations within your code to ensure clarity and maintainability.","solution":"def has_attr(obj, attr): Check whether a given Python object has a specified attribute. Args: obj : any The Python object. attr : str The name of the attribute. Returns: bool True if the attribute exists, else False. return hasattr(obj, attr) def get_attr(obj, attr): Retrieve the value of a specified attribute from a given Python object. Args: obj : any The Python object. attr : str The name of the attribute. Returns: any The value of the specified attribute. Raises: AttributeError: If the attribute does not exist. return getattr(obj, attr) def set_attr(obj, attr, value): Set the value of a specified attribute on a given Python object. Args: obj : any The Python object. attr : str The name of the attribute. value : any The value to set. setattr(obj, attr, value) def del_attr(obj, attr): Delete a specified attribute from a given Python object. Args: obj : any The Python object. attr : str The name of the attribute. Raises: AttributeError: If the attribute does not exist. delattr(obj, attr) def compare_objects(obj1, obj2, operator): Compare two Python objects using a specified operator. Args: obj1 : any The first Python object. obj2 : any The second Python object. operator : str The comparison operator. One of \'<\', \'<=\', \'==\', \'!=\', \'>\', \'>=\'. Returns: bool The result of the comparison. Raises: ValueError: If the operator is not one of the allowed values. if operator == \'<\': return obj1 < obj2 elif operator == \'<=\': return obj1 <= obj2 elif operator == \'==\': return obj1 == obj2 elif operator == \'!=\': return obj1 != obj2 elif operator == \'>\': return obj1 > obj2 elif operator == \'>=\': return obj1 >= obj2 else: raise ValueError(\\"Invalid comparison operator\\") def get_iterator(obj): Return an iterator for a given Python object. Args: obj : any The Python object. Returns: iterator An iterator for the object. Raises: TypeError: If the object is not iterable. return iter(obj) def get_item(obj, key): Return an item at a given key/index from a given Python object. Args: obj : any The Python object. key : any The key/index of the item to retrieve. Returns: any The item at the specified key/index. Raises: KeyError: If the key does not exist in the object. return obj[key] def set_item(obj, key, value): Set the value at a given key/index on a given Python object. Args: obj : any The Python object. key : any The key/index to set. value : any The value to set. obj[key] = value def del_item(obj, key): Delete the item at a given key/index from a given Python object. Args: obj : any The Python object. key : any The key/index of the item to delete. Raises: KeyError: If the key does not exist in the object. del obj[key] def is_instance(obj, cls): Check if an object is an instance of a given class. Args: obj : any The Python object. cls : type The class/type to check against. Returns: bool True if the object is an instance of the class, else False. return isinstance(obj, cls) def print_object(obj): Print a Python object using the equivalent of the print statement. Args: obj : any The Python object to print. print(obj)"},{"question":"Using the provided documentation, demonstrate your understanding of Python lists and related structures by solving the following problem: Problem: You are tasked with writing a function that takes a list of integers and returns a dictionary containing information about the list. The dictionary should have the following key-value pairs: - `\'original\'`: The original list as provided. - `\'sorted\'`: The list sorted in ascending order. - `\'unique\'`: A list of unique elements, sorted in ascending order. - `\'squares\'`: A list of the squares of each element in the list. - `\'evens\'`: A set of all even numbers in the list. - `\'odds\'`: A set of all odd numbers in the list. - `\'sum\'`: The sum of all elements in the list. - `\'average\'`: The average of the elements in the list. - `\'count\'`: A dictionary where the keys are the unique elements and the values are their respective counts in the original list. Your function should be defined as: ```python def analyze_list(lst: list) -> dict: # Your implementation here ``` Input: - A list of integers `lst` (1 ≤ len(lst) ≤ 1000, -1000 ≤ lst[i] ≤ 1000). Output: - A dictionary as described above containing the various transformations and analyses of the list. Example: ```python input_list = [1, 2, 2, 3, 4, 5, 5, 5, 6, 7, 8, 9, 10] output_dict = analyze_list(input_list) print(output_dict) ``` Expected Output: ```python { \'original\': [1, 2, 2, 3, 4, 5, 5, 5, 6, 7, 8, 9, 10], \'sorted\': [1, 2, 2, 3, 4, 5, 5, 5, 6, 7, 8, 9, 10], \'unique\': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], \'squares\': [1, 4, 4, 9, 16, 25, 25, 25, 36, 49, 64, 81, 100], \'evens\': {2, 4, 6, 8, 10}, \'odds\': {1, 3, 5, 7, 9}, \'sum\': 67, \'average\': 5.153846153846154, \'count\': {1: 1, 2: 2, 3: 1, 4: 1, 5: 3, 6: 1, 7: 1, 8: 1, 9: 1, 10: 1} } ``` Constraints and Notes: - You should make use of the list methods like `append()`, `extend()`, `remove()`, `pop()`, `clear()`, `index()`, `count()`, `sort()`, `reverse()`, and `copy()` where appropriate. - Emphasize using list comprehensions where feasible to create new lists. - Make sure to handle special cases such as empty lists or lists with one unique element. - Efficiency matters, but the primary focus is on correctness and demonstrating a comprehensive understanding of Python lists and related structures.","solution":"def analyze_list(lst: list) -> dict: # Convert the list to a set now to avoid multiple conversions later unique_set = set(lst) # Collecting all information about the list result = { \'original\': lst.copy(), \'sorted\': sorted(lst), \'unique\': sorted(unique_set), \'squares\': [x ** 2 for x in lst], \'evens\': {x for x in lst if x % 2 == 0}, \'odds\': {x for x in lst if x % 2 != 0}, \'sum\': sum(lst), \'average\': sum(lst) / len(lst) if lst else 0, \'count\': {x: lst.count(x) for x in unique_set} } return result"},{"question":"**Question: Shared Memory Data Exchange** In this task, you are required to create a Python script that demonstrates the use of shared memory for data exchange between multiple processes. The scenario involves two processes where one process (Writer) fills the shared memory with an array of integers, and the second process (Reader) reads and modifies this array. You need to implement both processes, ensuring proper synchronization and resource management. Also, implement a simple shared memory manager to ensure cleanup. # Requirements: 1. **Input Format**: - The script should not take any input from the user. - Define the array of integers within the Writer process. 2. **Output Format**: - Display the array as read by the Reader process, both before and after modification. 3. **Constraints**: - Use only the `multiprocessing.shared_memory` and `multiprocessing` modules. - The shared memory array should be properly cleaned up at the end. - Implement appropriate error handling. 4. **Performance Requirements**: - Ensure the solution is efficient and does not result in memory leaks or deadlocks. # Implementation Steps: 1. **Writer Process**: - Create a `SharedMemory` block and fill it with an array of integers. 2. **Reader Process**: - Attach to the existing `SharedMemory` block, read the array, display it, modify a few elements, and display the modified array. 3. Ensure proper use of `close()` and `unlink()` methods for resource management. 4. Implement a simple shared memory manager to encapsulate shared memory creation and destruction. # Example: ```python from multiprocessing import Process, shared_memory import numpy as np def writer(): # Create a shared memory block and fill it with an integer array array = np.array([10, 20, 30, 40, 50], dtype=np.int32) shm = shared_memory.SharedMemory(create=True, size=array.nbytes) buffer = np.ndarray(array.shape, dtype=array.dtype, buffer=shm.buf) buffer[:] = array[:] print(\\"Writer wrote array:\\", buffer[:]) shm.close() def reader(shm_name): # Attach to the existing shared memory block and read the array shm = shared_memory.SharedMemory(name=shm_name) buffer = np.ndarray((5,), dtype=np.int32, buffer=shm.buf) print(\\"Reader read array before modification:\\", buffer[:]) # Modify some elements buffer[0] = 100 buffer[1] = 200 print(\\"Reader modified array:\\", buffer[:]) shm.close() if __name__ == \\"__main__\\": # Create and start the writer process writer_process = Process(target=writer) writer_process.start() writer_process.join() # Now read the shared memory block name from the writer process shm = shared_memory.SharedMemory(name=writer_process.name) reader_process = Process(target=reader, args=(shm.name,)) reader_process.start() reader_process.join() shm.unlink() # Ensure proper cleanup ``` # Notes: - Ensure that the `writer` process creates the shared memory and writes data to it. - The `reader` process should be able to attach to the same shared memory, read, and modify data. - Properly manage resources by closing and unlinking shared memory when no longer needed.","solution":"from multiprocessing import Process, shared_memory import numpy as np def writer(): # Create a shared memory block and fill it with an integer array array = np.array([10, 20, 30, 40, 50], dtype=np.int32) shm = shared_memory.SharedMemory(create=True, size=array.nbytes) buffer = np.ndarray(array.shape, dtype=array.dtype, buffer=shm.buf) buffer[:] = array[:] print(\\"Writer wrote array:\\", buffer[:]) # Pass the shared memory name for the reader to use with open(\\"shm_name.txt\\", \\"w\\") as f: f.write(shm.name) shm.close() def reader(): # Read the shared memory name from file with open(\\"shm_name.txt\\", \\"r\\") as f: shm_name = f.read().strip() # Attach to the existing shared memory block and read the array shm = shared_memory.SharedMemory(name=shm_name) buffer = np.ndarray((5,), dtype=np.int32, buffer=shm.buf) print(\\"Reader read array before modification:\\", buffer[:]) # Modify some elements buffer[0] = 100 buffer[1] = 200 print(\\"Reader modified array:\\", buffer[:]) shm.close() shm.unlink() # Clean up shared memory if __name__ == \\"__main__\\": writer_process = Process(target=writer) writer_process.start() writer_process.join() reader_process = Process(target=reader) reader_process.start() reader_process.join()"},{"question":"**Question**: Implement a function `apply_window_and_compute_fft` using the PyTorch library. This function will apply a specified window function to a given signal and compute the Fast Fourier Transform (FFT) of the windowed signal. # Function Signature ```python import torch import torch.signal as sig def apply_window_and_compute_fft(signal: torch.Tensor, window_type: str, window_params: dict) -> torch.Tensor: pass ``` # Input - `signal` (torch.Tensor): A 1D tensor representing the signal data. - `window_type` (str): A string representing the type of window to apply (e.g., \'hann\', \'hamming\', etc.). - `window_params` (dict): A dictionary of parameters required for the specified window function. # Output - (torch.Tensor): A 1D tensor representing the FFT of the windowed signal. # Constraints - The `signal` tensor will have a length of at least 10 and at most 10000. - The `window_type` must be one of the window functions available in the `torch.signal.windows` module mentioned in the documentation. If an invalid window type is specified, your function should raise a `ValueError`. # Instructions 1. Create the specified window using the given parameters. 2. Apply the window to the input signal. 3. Compute and return the FFT of the windowed signal using PyTorch\'s `torch.fft.fft` function. # Example ```python signal = torch.tensor([0.5, 0.7, 1.2, 1.3, 0.6, 0.3, 0.2, 0.0, 0.1, 0.4], dtype=torch.float32) window_type = \'hann\' window_params = {} fft_result = apply_window_and_compute_fft(signal, window_type, window_params) print(fft_result) ``` Your implementation should correctly handle the different window types and their parameters and compute the FFT of the windowed signal.","solution":"import torch import torch.signal.windows as windows def apply_window_and_compute_fft(signal: torch.Tensor, window_type: str, window_params: dict) -> torch.Tensor: Apply a specified window function to a given signal and compute the FFT of the windowed signal. Parameters: - signal (torch.Tensor): A 1D tensor representing the signal data. - window_type (str): A string representing the type of window to apply (e.g., \'hann\', \'hamming\', etc.). - window_params (dict): A dictionary of parameters required for the specified window function. Returns: - torch.Tensor: A 1D tensor representing the FFT of the windowed signal. # Get the window function from the windows module try: window_func = getattr(windows, window_type) except AttributeError: raise ValueError(f\\"Invalid window type: {window_type}\\") # Create the window using the specified parameters window = window_func(signal.size(0), **window_params) # Apply the window to the signal windowed_signal = signal * window # Compute the FFT of the windowed signal fft_result = torch.fft.fft(windowed_signal) return fft_result"},{"question":"Objective: You need to write Python code that demonstrates your understanding of seaborn\'s advanced plotting capabilities, including how to create and customize bar plots and histograms. Task: Using the seaborn `diamonds` dataset, write a function named `create_custom_plot` that: 1. Plots a histogram of the `price` variable on a logarithmic scale. 2. Adds a stacked bar plot that differentiates the diamonds by their `cut`. 3. Adjusts the transparency of the bars based on the `clarity` variable. 4. Customizes the bar borders to be unfilled with a specific edge color and width. Function Signature: ```python def create_custom_plot() -> None: pass ``` Requirements: 1. **Input:** No input required for the function. 2. **Output:** The function should display the customized plot. 3. **Constraints:** - Use seaborn `objects` API. - Make sure the plot differentiates `cut` using stacked bars. - Adjust transparency with the `clarity` variable. - Customize the histogram\'s bar borders as unfilled, with edge color `\\"C0\\"` and edge width `1.5`. Example: Calling `create_custom_plot()` should perform the steps mentioned and display the customized plot.","solution":"import seaborn as sns import matplotlib.pyplot as plt import numpy as np def create_custom_plot() -> None: # Load the diamonds dataset diamonds = sns.load_dataset(\'diamonds\') diamonds[\'log_price\'] = np.log(diamonds[\'price\']) # Initialize the seaborn objects API p = sns.JointGrid(data=diamonds, x=\'log_price\', y=\'carat\', ratio=4) # Plot the histogram on a logarithmic scale p.plot_marginals(sns.histplot, element=\'step\', linewidth=1.5, edgecolor=\\"C0\\") # Define a column for transparency based on \'clarity\' diamonds[\'alpha\'] = diamonds[\'clarity\'].apply(lambda x: 0.6 if x == \'IF\' else 0.4) # Stacked bar plot with differentiation by \'cut\' and bar transparency based on \'clarity\' sns.histplot( data=diamonds, x=\'log_price\', hue=\'cut\', multiple=\'stack\', alpha=0.5, edgecolor=\\"C0\\", linewidth=1.5, ) plt.xlabel(\'Log(Price)\') plt.ylabel(\'Count\') plt.title(\'Histogram of Log(Price) Differentiated by Cut\') plt.show() # Example use create_custom_plot()"},{"question":"**Problem Statement:** You are given a legacy system that uses the deprecated \\"imghdr\\" module in Python to determine the type of an image. While the module works well for the supported formats, there is a need to extend its functionality to recognize additional custom image types based on specific byte sequences in their headers. Your task is to write a function `custom_imghdr_extend()` that extends the functionality of the `imghdr` module to recognize the following custom formats: 1. `cust1` - Identified by the byte sequence `b\'CUSTOM1\'` at the start of the file. 2. `cust2` - Identified by the byte sequence `b\'CUSTOM2\'` at the start of the file. # Requirements: 1. The function should add custom test functions to the `imghdr.tests` list. 2. The custom test functions should detect the `cust1` and `cust2` formats as described. 3. You should use the `imghdr.what()` function to verify the implementation. # Instructions: 1. Define the function `custom_imghdr_extend()`. 2. Inside this function, define two separate functions for testing the `cust1` and `cust2` image types. 3. Append these test functions to `imghdr.tests`. 4. Demonstrate the functionality by using `imghdr.what()` to detect the type of given sample files following the custom formats. # Function Signature: ```python def custom_imghdr_extend(): # Your code here ``` # Example: ```python # Sample implementation extension import imghdr def custom_imghdr_extend(): def test_cust1(h, f): if h[:7] == b\'CUSTOM1\': return \'cust1\' def test_cust2(h, f): if h[:7] == b\'CUSTOM2\': return \'cust2\' imghdr.tests.append(test_cust1) imghdr.tests.append(test_cust2) # Calling the function to extend imghdr functionality custom_imghdr_extend() # Create test files with the custom headers with open(\'test_cust1.img\', \'wb\') as f: f.write(b\'CUSTOM1\' + b\'x00\' * 100) with open(\'test_cust2.img\', \'wb\') as f: f.write(b\'CUSTOM2\' + b\'x00\' * 100) print(imghdr.what(\'test_cust1.img\')) # Output should be \'cust1\' print(imghdr.what(\'test_cust2.img\')) # Output should be \'cust2\' ``` # Constraints: - You are only allowed to modify the `imghdr.tests` list from within the `custom_imghdr_extend()` function. - The custom test functions should properly return `None` if the image format does not match.","solution":"import imghdr def custom_imghdr_extend(): Extends the functionality of the imghdr module to recognize custom image formats. def test_cust1(h, f): if h[:7] == b\'CUSTOM1\': return \'cust1\' def test_cust2(h, f): if h[:7] == b\'CUSTOM2\': return \'cust2\' imghdr.tests.append(test_cust1) imghdr.tests.append(test_cust2) # Calling the function to extend imghdr functionality custom_imghdr_extend()"},{"question":"**Objective:** Demonstrate your understanding of the `torch.fft` module in PyTorch by computing the Fast Fourier Transform (FFT) of a given 2-dimensional signal, apply frequency shifting, and then compute the inverse FFT to reconstruct the original signal. The code should also handle real and complex inputs effectively. **Problem Statement:** Write a function `process_fft` that takes a 2D input tensor representing a signal and performs the following steps: 1. Compute the 2-dimensional discrete Fourier Transform (DFT) using `torch.fft.fft2`. 2. Shift the zero-frequency component to the center of the spectrum using `torch.fft.fftshift`. 3. Apply an arbitrary filtering process (e.g., zero out all frequencies beyond a certain threshold). 4. Shift the zero-frequency component back to the original position using `torch.fft.ifftshift`. 5. Compute the inverse 2-dimensional DFT using `torch.fft.ifft2` to reconstruct the original signal. 6. Return the reconstructed signal and the filtered frequency representation as output. **Function Signature:** ```python import torch def process_fft(signal: torch.Tensor, threshold: float) -> (torch.Tensor, torch.Tensor): Parameters: signal (torch.Tensor): A 2D input tensor representing the original signal. It can be either real or complex. threshold (float): A threshold value to filter out high frequencies. Frequencies with magnitudes greater than this threshold should be zeroed out. Returns: reconstructed_signal (torch.Tensor): The reconstructed 2D signal tensor after applying the inverse FFT. filtered_fft (torch.Tensor): The filtered frequency representation (2D tensor) after thresholding. # Step 1: Compute the 2D FFT fft_2d = torch.fft.fft2(signal) # Step 2: Shift zero-frequency component to the center shifted_fft = torch.fft.fftshift(fft_2d) # Step 3: Apply filtering process magnitude = torch.abs(shifted_fft) mask = magnitude > threshold filtered_fft = shifted_fft.clone() filtered_fft[mask] = 0 # Step 4: Shift zero-frequency component back to the original position unshifted_fft = torch.fft.ifftshift(filtered_fft) # Step 5: Compute the inverse 2D FFT to reconstruct the original signal reconstructed_signal = torch.fft.ifft2(unshifted_fft).real return reconstructed_signal, filtered_fft ``` **Constraints:** - The input signal tensor can be of any size, but it must be a 2D tensor. - The threshold value will always be a positive float value. - Ensure the function handles both real and complex input signals appropriately. - Use PyTorch functions without relying on external libraries for FFT operations. **Example:** ```python import torch # Example input signal = torch.rand((128, 128)) # Process FFT with a threshold value threshold = 10.0 reconstructed_signal, filtered_fft = process_fft(signal, threshold) print(\\"Original Signal:\\") print(signal) print(\\"Reconstructed Signal:\\") print(reconstructed_signal) print(\\"Filtered FFT:\\") print(filtered_fft) ``` **Explanation:** This task will test the students\' ability to work with the FFT and IFFT functions in PyTorch, apply frequency-domain operations, and correctly transform signals back and forth between the time domain and frequency domain.","solution":"import torch def process_fft(signal: torch.Tensor, threshold: float) -> (torch.Tensor, torch.Tensor): Parameters: signal (torch.Tensor): A 2D input tensor representing the original signal. It can be either real or complex. threshold (float): A threshold value to filter out high frequencies. Frequencies with magnitudes greater than this threshold should be zeroed out. Returns: reconstructed_signal (torch.Tensor): The reconstructed 2D signal tensor after applying the inverse FFT. filtered_fft (torch.Tensor): The filtered frequency representation (2D tensor) after thresholding. # Step 1: Compute the 2D FFT fft_2d = torch.fft.fft2(signal) # Step 2: Shift zero-frequency component to the center shifted_fft = torch.fft.fftshift(fft_2d) # Step 3: Apply filtering process magnitude = torch.abs(shifted_fft) mask = magnitude > threshold filtered_fft = shifted_fft.clone() filtered_fft[mask] = 0 # Step 4: Shift zero-frequency component back to the original position unshifted_fft = torch.fft.ifftshift(filtered_fft) # Step 5: Compute the inverse 2D FFT to reconstruct the original signal reconstructed_signal = torch.fft.ifft2(unshifted_fft).real return reconstructed_signal, filtered_fft"},{"question":"**Objective**: Implement a function to simulate a server that processes client requests using the `asyncio` event loop in Python. # Problem Statement You are tasked with creating a server that can handle multiple client requests asynchronously using the `asyncio` package. Your server should: 1. Accept client connections. 2. Receive data from clients. 3. Respond back to the clients. 4. Run for a specified duration and then shut down gracefully. # Requirements 1. Implement the function `async_server(host: str, port: int, runtime: int) -> None` which will: - Start a server listening on the specified `host` and `port`. - Accept and handle connections from clients asynchronously. - Each client connection should send a list of numbers. The server should respond with the sum of the numbers. - Run the server for `runtime` seconds before shutting down. 2. Use `asyncio` methods and classes such as `asyncio.start_server`, `loop.create_task`, and handle the shutdown gracefully. 3. You must NOT use any other external libraries. # Expected Input and Output - **Input**: - `host` (str): The host address on which the server listens. - `port` (int): The port number on which the server listens. - `runtime` (int): The time in seconds the server should run before shutting down. - **Output**: - No direct output, but the server should print logs indicating client connections, received messages, and summation results. # Example ```python import asyncio async def async_server(host: str, port: int, runtime: int) -> None: # Placeholder for your implementation pass # Example usage: # asyncio.run(async_server(\'127.0.0.1\', 8888, 10)) ``` # Constraints - Ensure the server handles multiple clients concurrently. - Use appropriate exception handling to manage client disconnections and errors. - Implement the function within a single script, and ensure it runs using `asyncio.run()`. # Performance Requirements - The server must handle multiple concurrent connections efficiently using the asyncio event loop. - The shutdown must be graceful, ensuring all tasks complete before the loop is closed. # Hints 1. Consider defining an asynchronous function to handle client connections. 2. Utilize `asyncio.start_server` to initialize the server. 3. Use `loop.call_later` or similar methods for scheduling the shutdown.","solution":"import asyncio async def handle_client(reader, writer): try: data = await reader.read(100) message = data.decode().strip() numbers = list(map(int, message.split(\',\'))) result = sum(numbers) response = str(result) writer.write(response.encode()) await writer.drain() print(f\\"Received {message} from client, responding with {response}\\") except Exception as e: print(f\\"Error handling client: {e}\\") finally: writer.close() await writer.wait_closed() async def async_server(host: str, port: int, runtime: int) -> None: server = await asyncio.start_server(handle_client, host, port) async with server: await asyncio.sleep(runtime) print(\\"Shutting down server.\\") server.close() await server.wait_closed() # Example usage: # asyncio.run(async_server(\'127.0.0.1\', 8888, 10))"},{"question":"Objective You are required to demonstrate your understanding of `torch.xpu` by implementing a class that manages XPU computation, handles memory management, and utilizes streams for efficient execution. You will also need to incorporate random number generation for initialization tasks. Task Implement a class `XPUManager` with the following specifications: 1. **Initialization**: The class should initialize the XPU, set a specific device based on the device index, and ensure streams are configured correctly. 2. **Memory Management Methods**: - `report_memory()`: Returns a dictionary with information about currently allocated and reserved memory. - `reset_memory_stats()`: Resets the memory statistics to their initial state. - `clear_cache()`: Clears the XPU cache. 3. **Random Number Generation Methods**: - `set_seed(seed: int)`: Sets the manual seed for random number generation. - `get_rng_states()`: Returns the current state of the random number generators. 4. **Stream Management Methods**: - `create_stream()`: Create and return a new stream. - `synchronize()`: Synchronize the default stream and any active streams. 5. **Computation Method**: - `matrix_multiply(a: torch.Tensor, b: torch.Tensor) -> torch.Tensor`: Performs matrix multiplication using the XPU and returns the result. Constraints 1. The device index provided during initialization must be between `0` and `device_count - 1`. 2. The tensors used in `matrix_multiply` method will always be 2D tensors of compatible shapes for matrix multiplication. Example Usage ```python import torch class XPUManager: def __init__(self, device_index: int): # Initialize XPU and set device def report_memory(self) -> dict: # Report allocated and reserved memory def reset_memory_stats(self): # Reset memory statistics def clear_cache(self): # Clear XPU cache def set_seed(self, seed: int): # Set seed for RNG def get_rng_states(self) -> dict: # Return RNG states def create_stream(self) -> torch.xpu.Stream: # Create a new stream def synchronize(self): # Synchronize streams def matrix_multiply(self, a: torch.Tensor, b: torch.Tensor) -> torch.Tensor: # Perform matrix multiplication on XPU and return result # Example of class usage xpu_manager = XPUManager(device_index=0) xpu_manager.set_seed(42) result = xpu_manager.matrix_multiply(torch.randn(100, 200).xpu(), torch.randn(200, 300).xpu()) print(result) print(xpu_manager.report_memory()) xpu_manager.synchronize() xpu_manager.clear_cache() xpu_manager.reset_memory_stats() ``` Note: If any method requires additional helper functions or classes, you can define them within the class as well.","solution":"import torch class XPUManager: def __init__(self, device_index: int): if not torch.xpu.is_available(): raise ValueError(\\"XPU is not available on this system.\\") device_count = torch.xpu.device_count() if device_index < 0 or device_index >= device_count: raise ValueError(f\\"Device index must be between 0 and {device_count - 1}\\") self.device_index = device_index self.device = torch.device(f\'xpu:{device_index}\') self.streams = [] def report_memory(self) -> dict: return { \'allocated_memory\': torch.xpu.memory_allocated(self.device), \'reserved_memory\': torch.xpu.memory_reserved(self.device) } def reset_memory_stats(self): torch.xpu.reset_accumulated_memory_stats(self.device) def clear_cache(self): torch.xpu.empty_cache() def set_seed(self, seed: int): torch.manual_seed(seed) if torch.xpu.is_available(): torch.xpu.manual_seed(seed) def get_rng_states(self) -> dict: return { \'rng_state\': torch.get_rng_state(), \'xpu_rng_state\': torch.xpu.get_rng_state(self.device) if torch.xpu.is_available() else None } def create_stream(self) -> torch.xpu.Stream: stream = torch.xpu.Stream(self.device) self.streams.append(stream) return stream def synchronize(self): # Synchronize default stream torch.xpu.synchronize(self.device) # Synchronize all created streams for stream in self.streams: stream.synchronize() def matrix_multiply(self, a: torch.Tensor, b: torch.Tensor) -> torch.Tensor: # Ensure tensors are moved to the correct device if not a.is_xpu: a = a.to(self.device) if not b.is_xpu: b = b.to(self.device) return torch.matmul(a, b)"},{"question":"Objective Implement a function to process and interpret a set of binary data using both `struct` and `codecs` modules. The function should read a binary file, decode its content with a specific encoding, and extract structured data. This will assess your ability to work with binary data, encoding, and struct interpretation in Python. Task You are provided with a binary file that contains encoded textual data and structured binary data. Your task is to: 1. Read the binary file. 2. Decode the textual part of the data using UTF-8 encoding. 3. Extract specific structured data (an integer, a float, and a string) using the `struct` module. Function Signature ```python def process_binary_data(file_path: str) -> dict: pass ``` Input - `file_path` (str): The path to the binary file. Output - A dictionary with the following keys: - `decoded_text` (str): The decoded UTF-8 text. - `integer_value` (int): The extracted integer. - `float_value` (float): The extracted float. - `string_value` (str): The extracted string. Constraints - The binary file is structured as follows: - The first part is encoded textual data in UTF-8 (terminated with a null byte `x00`). - The second part is structured data packed in binary format (an integer, a float, and a length-prefixed string). Example Structure of Binary File ``` <UTF-8 Encoded Text>x00<Integer><Float><Length of String (1 byte)><String> ``` Example Suppose the binary file content is (hex representation): ``` 48656c6c6f20576f726c64003fa999999a3fc999999a0848656c6c6f ``` This corresponds to: - Text: \\"Hello World\\" + null byte - Integer: 0x3fa999999a (16909060) - Float: 1.05 - String: \\"Hello\\" The function should return: ```python { \\"decoded_text\\": \\"Hello World\\", \\"integer_value\\": 16909060, \\"float_value\\": 1.05, \\"string_value\\": \\"Hello\\" } ``` Notes - Use the `codecs` module for decoding the UTF-8 text. - Use the `struct` module to unpack the binary data. Implement the function `process_binary_data` to complete this task.","solution":"import struct def process_binary_data(file_path: str) -> dict: with open(file_path, \'rb\') as file: # Read the entire content of the file data = file.read() # Find the null terminator for the UTF-8 text text_end_index = data.index(b\'x00\') # Extract and decode the UTF-8 text decoded_text = data[:text_end_index].decode(\'utf-8\') # Remaining data after the null terminator remaining_data = data[text_end_index + 1:] # Unpack the integer and float integer_value, float_value = struct.unpack(\'if\', remaining_data[:8]) # Extract the length of the string string_length = remaining_data[8] # Extract the string using the length string_value = remaining_data[9:9 + string_length].decode(\'utf-8\') return { \'decoded_text\': decoded_text, \'integer_value\': integer_value, \'float_value\': float_value, \'string_value\': string_value }"},{"question":"**Asynchronous File Reader with Exception Handling** **Objective**: Write an asynchronous function that reads data from a file, processes it in chunks, and handles possible exceptions appropriately. # Requirements: 1. **Function Name**: `read_file_with_handling` 2. **Input**: - `file_path` (str): The path to the file to be read. - `chunk_size` (int): Number of bytes to read at a time. - `timeout` (float): Timeout for each read operation in seconds. 3. **Output**: A string containing all the data read from the file. 4. **Exception Handling**: - Handle `asyncio.TimeoutError`: When a read operation times out. - Handle `asyncio.CancelledError`: When the task is cancelled. - Handle `asyncio.IncompleteReadError`: When the read operation did not complete fully. - Handle `asyncio.LimitOverrunError`: When the buffer size limit is reached while looking for a separator. - Handle `asyncio.InvalidStateError`: When setting a result value for a Future object that already has a result value set. - Handle `asyncio.SendfileNotAvailableError`: When the `sendfile` syscall is not available for the given socket or file type. 5. **Constraints**: - Use `asyncio` for asynchronous operations. - Properly re-raise `CancelledError` after any custom handling. - Aggregate data read from the file and return it as a single string. - Ensure that resources are properly cleaned up (e.g., file handles are closed) in case of exceptions. # Example Usage ```python import asyncio async def main(): file_path = \'example.txt\' chunk_size = 1024 timeout = 2.0 try: file_content = await read_file_with_handling(file_path, chunk_size, timeout) print(file_content) except Exception as e: print(f\\"An error occurred: {e}\\") asyncio.run(main()) ``` # Function Signature ```python async def read_file_with_handling(file_path: str, chunk_size: int, timeout: float) -> str: # Implement the function here pass ``` # Notes - Ensure you test your function thoroughly with different files and scenarios, including non-existent files, large files, and scenarios that may trigger the various exceptions. - Pay attention to the order and method of exception handling to ensure that your asynchronous operations are efficient and robust.","solution":"import asyncio from asyncio import TimeoutError, CancelledError, IncompleteReadError, LimitOverrunError, InvalidStateError, SendfileNotAvailableError async def read_file_with_handling(file_path: str, chunk_size: int, timeout: float) -> str: data = \\"\\" try: with open(file_path, \'rb\') as file: while True: try: chunk = await asyncio.wait_for(file.read(chunk_size), timeout) if not chunk: break data += chunk.decode(\'utf-8\') except TimeoutError: print(\\"Read operation timed out.\\") except IncompleteReadError: print(\\"Read operation did not complete fully.\\") except LimitOverrunError: print(\\"Buffer size limit reached while looking for a separator.\\") except InvalidStateError: print(\\"Setting a result value for a Future object that already has a result value set.\\") except SendfileNotAvailableError: print(\\"Sendfile syscall is not available for the given socket or file type.\\") except FileNotFoundError: print(f\\"File {file_path} not found.\\") except IOError as e: print(f\\"IO error: {e}\\") except Exception as e: print(f\\"An unexpected error occurred: {e}\\") return data"},{"question":"# Implementing Character Encoding in Emails with `email.charset` **Context**: You are using Python for sending automated emails and need to ensure proper encoding of email headers and bodies according to the character sets specified. Python\'s `email.charset` module can help with this, particularly the `Charset` class. **Objective**: Your task is to implement the `header_encode` and `body_encode` methods of a `Charset` class to handle proper encoding for email headers and bodies, using quoted-printable or base64 encoding as required. **Details**: Implement the functions: - `header_encode(self, string: str) -> str` - `body_encode(self, string: str) -> str` # Input: - `string`: A string representing the text to be encoded. # Output: - Encoded string using the appropriate encoding format for email headers and bodies. # Constraints: 1. `header_encode` must use the `header_encoding` attribute to determine if quoted-printable (`\'QP\'`) or base64 (`\'BASE64\'`) encoding is required. 2. `body_encode` must use the `body_encoding` attribute to determine if quoted-printable (`\'QP\'`) or base64 (`\'BASE64\'`) encoding is required. # Assumptions: - The `Charset` class will contain attributes `header_encoding` and `body_encoding`, which can have values `\'QP\'` for quoted-printable, `\'BASE64\'` for base64 encoding, or `None` if no encoding is required. These attributes will be already set when the methods are called. # Example: ```python from email.charset import Charset import base64 import quopri class Charset: def __init__(self, header_encoding, body_encoding): self.header_encoding = header_encoding self.body_encoding = body_encoding def header_encode(self, string: str) -> str: # Implement header encoding logic based on self.header_encoding def body_encode(self, string: str) -> str: # Implement body encoding logic based on self.body_encoding # Example usage: cs = Charset(header_encoding=\'QP\', body_encoding=\'BASE64\') print(cs.header_encode(\'Sample Header\')) # Encodes using quoted-printable print(cs.body_encode(\'Sample Body\')) # Encodes using base64 ``` Use the `quopri` library for quoted-printable encoding and the `base64` library for base64 encoding in your implementation. **BONUS**: Create additional methods such as `get_body_encoding()` and `get_output_charset()` to further demonstrate your understanding of the `Charset` class and its functionalities.","solution":"from email.charset import Charset as EmailCharset import base64 import quopri class Charset: def __init__(self, header_encoding, body_encoding): self.header_encoding = header_encoding self.body_encoding = body_encoding def header_encode(self, string: str) -> str: if self.header_encoding == \'QP\': return quopri.encodestring(string.encode(\'utf-8\'), quotetabs=True).decode(\'utf-8\') elif self.header_encoding == \'BASE64\': return base64.b64encode(string.encode(\'utf-8\')).decode(\'utf-8\') else: return string def body_encode(self, string: str) -> str: if self.body_encoding == \'QP\': return quopri.encodestring(string.encode(\'utf-8\'), quotetabs=True).decode(\'utf-8\') elif self.body_encoding == \'BASE64\': return base64.b64encode(string.encode(\'utf-8\')).decode(\'utf-8\') else: return string"},{"question":"You are given a dataset about passengers on the Titanic. Using seaborn, you need to create a series of visualizations to analyze the dataset from multiple perspectives, focusing on categorical data. Specifically, you should: 1. Load the Titanic dataset using seaborn. 2. Create a `catplot` displaying the survival rate (`survived`) of passengers across different classes (`pclass`) and split by gender (`sex`). Use a `pointplot` for this visualization. 3. Generate a `boxplot` showing the distribution of passenger fares (`fare`) across different embarkation towns (`embarked`), split by passenger class (`pclass`). 4. Construct a `violinplot` to compare the age distribution of male and female passengers, further split by the class of the ticket. 5. Create a multi-faceted plot illustrating the count of survivors (`survived`) across different age groups. Use `FacetGrid` to create an additional facet based on the embarkation town (`embarked`). # Constraints - Assume the dataset has columns: `survived`, `pclass`, `sex`, `fare`, `embarked`, and `age`. - Make sure to handle cases with missing data appropriately. - Add meaningful titles and labels to the plots for better readability. # Example of Expected Output The solution must produce the following plots: 1. A point plot showing the survival rate of passengers across different classes and genders. 2. A box plot displaying passenger fares across embarkation towns, divided by class. 3. A violin plot showing the distribution of ages, comparing males and females across different classes. 4. A multi-faceted bar plot counting survivors across age groups, further broken down by embarkation town. # Performance Requirements - Use appropriate seaborn functions and parameters to create clear and informative visualizations. - Ensure the code is efficient and handles the dataset without unnecessary complexity. **Input:** - `titanic` dataset (automatically loaded from seaborn) **Output:** - Display the four specified seaborn plots with the described configurations ```python import seaborn as sns import matplotlib.pyplot as plt # Load the Titanic dataset titanic = sns.load_dataset(\\"titanic\\") # 1. Point plot of survival rate by class and gender sns.catplot(data=titanic, x=\\"pclass\\", y=\\"survived\\", hue=\\"sex\\", kind=\\"point\\") plt.title(\\"Survival Rate by Class and Gender\\") # 2. Box plot of fares by embarkation town and class sns.catplot(data=titanic, x=\\"embarked\\", y=\\"fare\\", hue=\\"pclass\\", kind=\\"box\\") plt.title(\\"Distribution of Fares by Embarkation Town and Class\\") # 3. Violin plot of age distribution by gender and class sns.catplot(data=titanic, x=\\"class\\", y=\\"age\\", hue=\\"sex\\", kind=\\"violin\\", split=True) plt.title(\\"Age Distribution by Gender and Class\\") # 4. Faceted bar plot of survivors by age groups and embarkation town age_bins = [0, 12, 18, 35, 60, 80] titanic[\'age_group\'] = pd.cut(titanic[\'age\'], bins=age_bins, labels=[\\"0-12\\", \\"12-18\\", \\"18-35\\", \\"35-60\\", \\"60-80\\"]) g = sns.catplot(data=titanic.dropna(subset=[\\"age_group\\"]), x=\\"age_group\\", hue=\\"survived\\", kind=\\"count\\", col=\\"embarked\\") g.set_titles(\\"Survivors by Age Group and Embarkation Town\\") plt.show() ```","solution":"import seaborn as sns import matplotlib.pyplot as plt import pandas as pd # Load the Titanic dataset titanic = sns.load_dataset(\\"titanic\\") # Handle missing data by dropping rows with missing \'age\', \'embarked\', \'fare\' or \'survived\' values titanic.dropna(subset=[\\"age\\", \\"embarked\\", \\"fare\\", \\"survived\\"], inplace=True) # 1. Point plot of survival rate by class and gender sns.catplot(data=titanic, x=\\"pclass\\", y=\\"survived\\", hue=\\"sex\\", kind=\\"point\\") plt.title(\\"Survival Rate by Class and Gender\\") plt.xlabel(\\"Passenger Class\\") plt.ylabel(\\"Survival Rate\\") plt.show() # 2. Box plot of fares by embarkation town and class sns.catplot(data=titanic, x=\\"embarked\\", y=\\"fare\\", hue=\\"pclass\\", kind=\\"box\\") plt.title(\\"Distribution of Fares by Embarkation Town and Class\\") plt.xlabel(\\"Embarkation Town\\") plt.ylabel(\\"Fare\\") plt.show() # 3. Violin plot of age distribution by gender and class sns.catplot(data=titanic, x=\\"pclass\\", y=\\"age\\", hue=\\"sex\\", kind=\\"violin\\", split=True) plt.title(\\"Age Distribution by Gender and Class\\") plt.xlabel(\\"Passenger Class\\") plt.ylabel(\\"Age\\") plt.show() # 4. Faceted bar plot of survivors by age groups and embarkation town age_bins = [0, 12, 18, 35, 60, 80] titanic[\'age_group\'] = pd.cut(titanic[\'age\'], bins=age_bins, labels=[\\"0-12\\", \\"12-18\\", \\"18-35\\", \\"35-60\\", \\"60-80\\"]) g = sns.catplot(data=titanic.dropna(subset=[\\"age_group\\"]), x=\\"age_group\\", hue=\\"survived\\", kind=\\"count\\", col=\\"embarked\\") g.set_titles(\\"Survivors by Age Group and Embarkation Town\\") plt.xlabel(\\"Age Group\\") plt.ylabel(\\"Count\\") plt.show()"},{"question":"Title: Parallel Matrix Multiplication using `concurrent.futures` **Objective**: Demonstrate comprehension of asynchronous execution, thread management, and exception handling using the `concurrent.futures` module in Python. **Problem Statement**: Parallelize the computation of matrix multiplication using `ThreadPoolExecutor` from the `concurrent.futures` module. Given two matrices `A` and `B`, your task is to implement a function `parallel_matrix_multiply` that computes their product matrix `C` in parallel. Use threads efficiently to compute each element of the resulting matrix, handle any potential exceptions gracefully, and ensure proper resource management. **Function Signature**: ```python def parallel_matrix_multiply(A: List[List[int]], B: List[List[int]]) -> List[List[int]]: pass ``` **Input Format**: - `A`: A list of `N` lists, each containing `M` integers representing a 2D matrix. - `B`: A list of `M` lists, each containing `P` integers representing a 2D matrix. **Output Format**: - `C`: A list of `N` lists, each containing `P` integers representing the product matrix of `A` and `B`. **Constraints**: - Matrices `A` and `B` follow the rules of matrix multiplication (`A`\'s column count is equal to `B`\'s row count). - Matrix dimensions and element values should be manageable within typical computational limits (e.g., 1 ≤ N, M, P ≤ 500 and elements < 1000). - Implement exception handling to catch and log any errors during the computation. **Example**: ```python A = [ [1, 2, 3], [4, 5, 6] ] B = [ [7, 8], [9, 10], [11, 12] ] result = parallel_matrix_multiply(A, B) print(result) ``` Expected output: ``` [ [58, 64], [139, 154] ] ``` **Additional Requirements**: 1. Use the `ThreadPoolExecutor` for parallel computation. 2. Manage resources using a `with` statement. 3. Handle exceptions using the `result()` method. 4. Optimize performance by choosing an appropriate number of worker threads. **Performance Requirements**: - Aim to achieve significant speedup over a simple single-threaded implementation by maximizing parallelism. - Avoid common pitfalls like deadlocks or excessive thread creation. Happy coding!","solution":"from concurrent.futures import ThreadPoolExecutor from typing import List def compute_element(A: List[List[int]], B: List[List[int]], row: int, col: int) -> int: # Helper function to compute the value of a single element in the result matrix (row, col) return sum(A[row][k] * B[k][col] for k in range(len(B))) def parallel_matrix_multiply(A: List[List[int]], B: List[List[int]]) -> List[List[int]]: Multiplies two matrices A and B using parallel computation. # Determine the dimensions of the result matrix C num_rows_A = len(A) num_cols_B = len(B[0]) # Initialize the result matrix with zeros C = [[0 for _ in range(num_cols_B)] for _ in range(num_rows_A)] with ThreadPoolExecutor() as executor: # Submit tasks to the executor to compute each element of the result matrix in parallel futures = {} for i in range(num_rows_A): for j in range(num_cols_B): # Submit a task to the executor to compute C[i][j] future = executor.submit(compute_element, A, B, i, j) futures[future] = (i, j) for future in futures: try: # Assign the computed value to the appropriate position in the result matrix result = future.result() i, j = futures[future] C[i][j] = result except Exception as e: # Handle exceptions gracefully print(f\\"An error occurred while computing C[{i}][{j}]: {e}\\") return C"},{"question":"**Title: Implement and Train a Neural Network on the MPS Device Using PyTorch** **Objective:** Demonstrate your understanding of PyTorch\'s MPS backend by implementing a simple neural network, initializing it and the input tensors on the MPS device, and training it on a sample dataset. This question will assess your ability to leverage the MPS device for high-performance GPU computations. **Problem Statement:** You are required to: 1. Implement a simple feedforward neural network using PyTorch. 2. Initialize the network and input tensors on the MPS device. 3. Train the network on a sample dataset using MPS device for a given number of epochs. **Instructions:** 1. **Network Implementation:** Implement a feedforward neural network with the following specifications: - Input layer with 784 units (assuming each input is a 28x28 flattened image). - One hidden layer with 128 units and ReLU activation. - Output layer with 10 units and softmax activation (for a 10-class classification problem). 2. **Data Preparation:** - Create a sample dataset using random values to simulate 1000 training samples: - Features: 1000 x 784 matrix of random floats. - Labels: 1000 dimension tensor containing random integer labels in the range [0, 9]. - Ensure that both features and labels are initialized on the MPS device. 3. **Training Procedure:** - Use CrossEntropyLoss as the loss function. - Use Adam optimizer with a learning rate of 0.001. - Train the network for 5 epochs, printing the loss value after each epoch. **Expected Functions and Classes:** - **Network Class:** ```python class SimpleNeuralNet(nn.Module): def __init__(self): super(SimpleNeuralNet, self).__init__() self.layer1 = nn.Linear(784, 128) self.relu = nn.ReLU() self.layer2 = nn.Linear(128, 10) def forward(self, x): x = self.layer1(x) x = self.relu(x) x = self.layer2(x) return x ``` - **Training Function:** ```python def train_on_mps_device(): # 1. Check for MPS availability and initialize device if not torch.backends.mps.is_available(): raise EnvironmentError(\\"MPS device not available.\\") mps_device = torch.device(\\"mps\\") # 2. Initialize model, tensors and optimizer model = SimpleNeuralNet().to(mps_device) features = torch.randn(1000, 784, device=mps_device) labels = torch.randint(0, 10, (1000,), device=mps_device) criterion = nn.CrossEntropyLoss() optimizer = torch.optim.Adam(model.parameters(), lr=0.001) # 3. Training loop for epoch in range(5): optimizer.zero_grad() outputs = model(features) loss = criterion(outputs, labels) loss.backward() optimizer.step() print(f\'Epoch {epoch + 1}, Loss: {loss.item()}\') ``` **Constraints:** 1. The solution must be implemented using PyTorch. 2. Ensure the code leverages the MPS device for computation. 3. Do not modify the structure of the provided `SimpleNeuralNet` class. 4. Assume that macOS with 12.3+ and an MPS-enabled device is available. **Performance Requirements:** Ensure that training on the simulated dataset completes without errors on the MPS device, and that the loss decreases over the epochs. **Note:** You do not need to achieve a specific accuracy since the dataset is synthetic.","solution":"import torch import torch.nn as nn import torch.optim as optim class SimpleNeuralNet(nn.Module): def __init__(self): super(SimpleNeuralNet, self).__init__() self.layer1 = nn.Linear(784, 128) self.relu = nn.ReLU() self.layer2 = nn.Linear(128, 10) def forward(self, x): x = self.layer1(x) x = self.relu(x) x = self.layer2(x) return x def train_on_mps_device(): # 1. Check for MPS availability and initialize device if not torch.backends.mps.is_available(): raise EnvironmentError(\\"MPS device not available.\\") mps_device = torch.device(\\"mps\\") # 2. Initialize model, tensors and optimizer model = SimpleNeuralNet().to(mps_device) features = torch.randn(1000, 784, device=mps_device) labels = torch.randint(0, 10, (1000,), device=mps_device) criterion = nn.CrossEntropyLoss() optimizer = optim.Adam(model.parameters(), lr=0.001) # 3. Training loop for epoch in range(5): optimizer.zero_grad() outputs = model(features) loss = criterion(outputs, labels) loss.backward() optimizer.step() print(f\'Epoch {epoch + 1}, Loss: {loss.item()}\')"},{"question":"# Custom Interactive Console Implementation Objective: Create a custom interactive console class that extends Python\'s standard interactive console capabilities. This console should allow users to define and use custom commands in addition to running regular Python code. Requirements: 1. **Class Definition:** - Define a class `CustomConsole` that inherits from `code.InteractiveConsole`. 2. **Custom Command Handling:** - Implement a method to register custom commands. Each command should be a function that can be invoked with optional arguments. - Commands should be invoked by entering a special prefix (e.g., `!command_name`), followed by any arguments. 3. **Command Execution:** - Override the method that processes input lines to check for custom commands and execute them if the input matches the special prefix. - If the input does not match a custom command, execute it as regular Python code. 4. **Interactive Experience:** - Ensure that the console continues to run interactively, allowing the user to enter multiple commands and code snippets. Implementation Details: - **Input Format:** Custom commands prefixed with `!` and optional arguments separated by spaces. Regular Python code as usual. - **Output Format:** Print the result of custom commands or Python code execution. Handle any exceptions and display error messages appropriately. Example Usage: The custom console should behave as follows: ```python from myconsole import CustomConsole def hello(name): print(f\\"Hello, {name}!\\") def add(a, b): print(int(a) + int(b)) console = CustomConsole() console.register_command(\'hello\', hello) console.register_command(\'add\', add) console.interact() # Console interaction: # > !hello Alice # Hello, Alice! # > !add 2 3 # 5 # > print(\\"This is regular Python code!\\") # This is regular Python code! ``` Use the following method signatures as a guide: ```python import code class CustomConsole(code.InteractiveConsole): def __init__(self, locals=None): super().__init__(locals) self.commands = {} def register_command(self, name, func): Register a custom command with the console. self.commands[name] = func def runsource(self, source, filename=\\"<input>\\", symbol=\\"single\\"): Override to process custom commands. # Check if the source matches a custom command pattern. if source.startswith(\'!\'): parts = source[1:].strip().split() command = parts[0] args = parts[1:] if command in self.commands: try: self.commands[command](*args) except Exception as e: print(f\\"Error executing command \'{command}\': {e}\\") return False return super().runsource(source, filename, symbol) ``` Constraints: - Do not use any non-standard libraries; rely only on the standard library modules. - Ensure your solution is efficient and handles typical interactive interpreter usage seamlessly. **Note:** Create appropriate unit tests to validate the functionality of your custom console.","solution":"import code class CustomConsole(code.InteractiveConsole): def __init__(self, locals=None): super().__init__(locals) self.commands = {} def register_command(self, name, func): Register a custom command with the console. self.commands[name] = func def runsource(self, source, filename=\\"<input>\\", symbol=\\"single\\"): Override to process custom commands. # Check if the source matches a custom command pattern. if source.startswith(\'!\'): parts = source[1:].strip().split() command = parts[0] args = parts[1:] if command in self.commands: try: self.commands[command](*args) except Exception as e: print(f\\"Error executing command \'{command}\': {e}\\") return False return super().runsource(source, filename, symbol)"},{"question":"**Coding Assessment Question** # Objective Demonstrate your understanding of the `seaborn.objects` interface by visualizing restaurant tips data with various transformations. # Problem Statement You are provided with a dataset `tips` containing information about tips received in a restaurant. Your task is to create visualizations using Seaborn\'s `so.Plot` and various transformations to showcase your ability to manage overlapping plots and combine multiple transformations. # Dataset The `tips` dataset, already available in Seaborn, contains the following columns: - `total_bill`: Total bill amount (in USD). - `tip`: Tip amount (in USD). - `sex`: Gender of the bill payer. - `smoker`: Indicates if the bill payer is a smoker. - `day`: Day of the week. - `time`: Time of day (Lunch or Dinner). - `size`: Size of the party. # Tasks 1. **Load the Dataset**: Load the `tips` dataset using Seaborn\'s `load_dataset` function. 2. **Day-wise Count Plot**: Create a bar plot to show the count of tips received on different days, differentiated by the time of day (`Lunch` or `Dinner`). Use dodge transformation to handle overlapping bars and fill in empty spaces if any. 3. **Total Bill with Gaps**: Create a bar plot showing the sum of `total_bill` for each day, differentiated by the sex (`Male` or `Female`). Ensure there is a gap of 0.2 between bars. 4. **Combined Semantic Variables**: Visualize the count of tips received, using dodged dot plot, differentiated by smoker status (`yes` or `no`) combined with time of day (`Lunch` or `Dinner`). Use appropriate transformations to avoid overlapping. 5. **Combine Dodge and Jitter**: Create a dot plot with dodge transformation, where dots are jittered to add randomness to their positions. # Constraints - Use the `seaborn.objects` interface throughout. - Ensure plots are rendered correctly and visually distinct. - Comment your code appropriately to explain each step. # Expected Input and Output - **Input**: No direct input; the dataset is loaded using Seaborn\'s `load_dataset`. - **Output**: Four different plots as described in the tasks with appropriate transformations and customizations. # Example ```python import seaborn.objects as so from seaborn import load_dataset # Task 1: Load the dataset tips = load_dataset(\\"tips\\").astype({\\"time\\": str}) # Task 2: Day-wise Count Plot differentiated by time of day with dodge and empty filled p1 = so.Plot(tips, \\"day\\", color=\\"time\\").add(so.Bar(), so.Count(), so.Dodge(empty=\\"fill\\")) p1.show() # Task 3: Total Bill with Gaps differentiated by gender p2 = so.Plot(tips, \\"day\\", \\"total_bill\\", color=\\"sex\\").add(so.Bar(), so.Agg(\\"sum\\"), so.Dodge(gap=0.2)) p2.show() # Task 4: Combined semantic variables with dodged dot plot p3 = so.Plot(tips, \\"day\\", color=\\"smoker\\").add(so.Dot(), so.Dodge(), fill=\\"time\\") p3.show() # Task 5: Combine Dodge with Jitter p4 = so.Plot(tips, \\"day\\", color=\\"sex\\").add(so.Dot(), so.Dodge(), so.Jitter()) p4.show() ``` Ensure your plots adhere to the tasks and constraints mentioned for a successful assessment.","solution":"import seaborn.objects as so from seaborn import load_dataset # Task 1: Load the dataset tips = load_dataset(\\"tips\\").astype({\\"time\\": str}) # Task 2: Day-wise Count Plot differentiated by time of day with dodge and empty filled def plot_day_wise_count(tips): Create a bar plot to show the count of tips received on different days, differentiated by the time of day (Lunch or Dinner), using dodge transformation. p1 = so.Plot(tips, \\"day\\", color=\\"time\\").add(so.Bar(), so.Count(), so.Dodge(empty=\\"fill\\")) return p1 # Task 3: Total Bill with Gaps differentiated by gender def plot_total_bill_with_gaps(tips): Create a bar plot showing the sum of total_bill for each day, differentiated by sex (Male or Female). Ensure there is a gap of 0.2 between bars. p2 = so.Plot(tips, \\"day\\", \\"total_bill\\", color=\\"sex\\").add(so.Bar(), so.Agg(\\"sum\\"), so.Dodge(gap=0.2)) return p2 # Task 4: Combined semantic variables with dodged dot plot def plot_combined_semantics(tips): Visualize the count of tips received, using dodged dot plot, differentiated by smoker status (yes or no) combined with time of day (Lunch or Dinner). p3 = so.Plot(tips, \\"day\\", color=\\"smoker\\").add(so.Dot(), so.Dodge(), fill=\\"time\\") return p3 # Task 5: Combine Dodge with Jitter def plot_dodge_jitter(tips): Create a dot plot with dodge transformation, where dots are jittered to add randomness to their positions. p4 = so.Plot(tips, \\"day\\", color=\\"sex\\").add(so.Dot(), so.Dodge(), so.Jitter()) return p4"},{"question":"# CGI Script for Handling User Form Inputs and File Uploads You are tasked with writing a Python CGI script using the `cgi` module to handle a simple web form submission. The form allows users to enter their name, address, and upload a text file. Your script should process the submitted form data, validate the inputs, and generate an appropriate HTML response. Additionally, any errors encountered should be logged and displayed in the browser using the `cgitb` module. Requirements 1. **Form Data Handling:** - Validate that both \'name\' and \'addr\' fields are non-empty. - If the fields are empty, return an error message in HTML format. 2. **File Upload Handling:** - Ensure the uploaded file is a text file (MIME type `text/plain`). - Read the contents of the file and count the number of lines. - Include a message in the HTML response displaying the line count. 3. **Logging and Error Handling:** - Use the `cgitb` module to handle and log errors for debugging purposes. - In case of errors, the error report should be displayed in the browser. 4. **Generate HTML Response:** - If all fields are valid and the file is processed correctly, generate a response displaying the user\'s name, address, and the line count of the uploaded file. - If there are any validation errors, display the appropriate error messages. Input Format: The web form submission will include: - `name`: A non-empty string representing the user\'s name. - `addr`: A non-empty string representing the user\'s address. - `userfile`: An uploaded text file. Output Format: Your script should output an HTML response as described above to the browser. Constraints: - Assume the script is placed in the server\'s CGI directory and invoked by the server. - The file upload size will not exceed 1MB. - Only text files will be uploaded. Example: Suppose a user submits the following data: - Name: John Doe - Address: 123 Elm St - Uploaded file contents: ``` Line 1 Line 2 Line 3 ``` The HTML response should be: ```html Content-Type: text/html <html> <head><title>Form Response</title></head> <body> <h1>Form Submission Results</h1> <p>Name: John Doe</p> <p>Address: 123 Elm St</p> <p>Uploaded file contains 3 lines.</p> </body> </html> ``` In case of an error (e.g., missing form fields), the response should include an error message within the HTML body, such as: ```html Content-Type: text/html <html> <head><title>Error</title></head> <body> <h1>Error</h1> <p>Please fill in the name and addr fields.</p> </body> </html> ``` Code Template Here is a template to start with. Implement the missing parts as per the requirements above: ```python #!/usr/bin/env python3 import cgi import cgitb cgitb.enable() # Enable detailed error messages def generate_response(name, addr, line_count=None, error_msg=None): print(\\"Content-Type: text/htmln\\") print(\\"<html>\\") print(\\"<head><title>Form Response</title></head>\\") print(\\"<body>\\") if error_msg: print(\\"<h1>Error</h1>\\") print(f\\"<p>{error_msg}</p>\\") else: print(\\"<h1>Form Submission Results</h1>\\") print(f\\"<p>Name: {name}</p>\\") print(f\\"<p>Address: {addr}</p>\\") if line_count is not None: print(f\\"<p>Uploaded file contains {line_count} lines.</p>\\") print(\\"</body>\\") print(\\"</html>\\") def main(): form = cgi.FieldStorage() name = form.getfirst(\\"name\\", \\"\\").strip() addr = form.getfirst(\\"addr\\", \\"\\").strip() fileitem = form[\\"userfile\\"] if not name or not addr: generate_response(name, addr, error_msg=\\"Please fill in the name and addr fields.\\") return if fileitem.file: mime_type = fileitem.type if mime_type == \\"text/plain\\": linecount = 0 while True: line = fileitem.file.readline() if not line: break linecount += 1 generate_response(name, addr, line_count=linecount) else: generate_response(name, addr, error_msg=\\"Uploaded file is not a text file.\\") else: generate_response(name, addr, error_msg=\\"File upload failed.\\") if __name__ == \\"__main__\\": main() ``` Complete the script by implementing the missing parts and handling all cases as described. Test it thoroughly before deployment.","solution":"#!/usr/bin/env python3 import cgi import cgitb cgitb.enable() # Enable detailed error messages def generate_response(name, addr, line_count=None, error_msg=None): print(\\"Content-Type: text/htmln\\") print(\\"<html>\\") print(\\"<head><title>Form Response</title></head>\\") print(\\"<body>\\") if error_msg: print(\\"<h1>Error</h1>\\") print(f\\"<p>{error_msg}</p>\\") else: print(\\"<h1>Form Submission Results</h1>\\") print(f\\"<p>Name: {name}</p>\\") print(f\\"<p>Address: {addr}</p>\\") if line_count is not None: print(f\\"<p>Uploaded file contains {line_count} lines.</p>\\") print(\\"</body>\\") print(\\"</html>\\") def main(): form = cgi.FieldStorage() name = form.getfirst(\\"name\\", \\"\\").strip() addr = form.getfirst(\\"addr\\", \\"\\").strip() fileitem = form[\\"userfile\\"] if not name or not addr: generate_response(name, addr, error_msg=\\"Please fill in the name and addr fields.\\") return if fileitem.file: mime_type = fileitem.type if mime_type == \\"text/plain\\": linecount = 0 while True: line = fileitem.file.readline() if not line: break linecount += 1 generate_response(name, addr, line_count=linecount) else: generate_response(name, addr, error_msg=\\"Uploaded file is not a text file.\\") else: generate_response(name, addr, error_msg=\\"File upload failed.\\") if __name__ == \\"__main__\\": main()"},{"question":"Advanced Python Coding Assessment # Objective Implement a Python function that utilizes the `concurrent.futures` module to concurrently download multiple web pages and process the content. # Problem Statement You are required to download content from a list of URLs using the `concurrent.futures` module for parallel execution. Once the content is downloaded, you must extract and count the frequency of a specific word in the combined content of all the web pages. # Input and Output Input - A list of strings where each string is a URL. - A string representing the word to be counted. Output - An integer representing the total count of the word across all the web pages. # Function Signature ```python import requests from concurrent.futures import ThreadPoolExecutor def count_word_in_urls(urls: list[str], word: str) -> int: pass ``` # Constraints - The number of URLs will not exceed 100. - The word to be counted will be a non-empty string containing only alphanumeric characters. - You should handle potential exceptions that may occur during the requests operations (e.g., network issues, invalid URLs). # Performance Requirements - Use `ThreadPoolExecutor` for the parallel execution of downloading tasks. - Optimize the solution to handle the maximum constraint efficiently. # Example ```python urls = [\\"http://example.com/page1\\", \\"http://example.com/page2\\"] word = \\"example\\" # Sample expected output result = count_word_in_urls(urls, word) print(result) # Output will depend on the actual content of the web pages ``` # Notes - Use the `requests` library for HTTP requests. - Ensure proper handling of possible exceptions such as connection errors or invalid URLs. - Ensure the word count is case-insensitive.","solution":"import requests from concurrent.futures import ThreadPoolExecutor def count_word_in_urls(urls, word): def fetch_url_content(url): try: response = requests.get(url) response.raise_for_status() return response.text except requests.RequestException: return \\"\\" word = word.lower() word_count = 0 with ThreadPoolExecutor(max_workers=10) as executor: contents = list(executor.map(fetch_url_content, urls)) combined_content = \\" \\".join(contents).lower() word_count = combined_content.split().count(word) return word_count"},{"question":"Objective: You are required to implement functions that take advantage of the `Fraction` class from the `fractions` module in Python. These functions should demonstrate your understanding of rational number arithmetic, conversion from different types, and fraction manipulation. Task: Implement the following three functions: 1. **add_fractions**: - **Input**: Two strings representing fractions in the form of `\\"[sign] numerator [\'/\' denominator]\\"` or any string accepted by `Fraction` constructor. - **Output**: A string representing the simplified fraction resulting from the sum of the two fractions. - **Example**: ```python add_fractions(\\"1/2\\", \\"3/4\\") -> \\"5/4\\" add_fractions(\\"-1/3\\", \\"1/6\\") -> \\"-1/6\\" ``` 2. **fraction_to_decimal**: - **Input**: A string representing a fraction. - **Output**: A float representing the decimal value of the given fraction. - **Example**: ```python fraction_to_decimal(\\"1/2\\") -> 0.5 fraction_to_decimal(\\"3/4\\") -> 0.75 ``` 3. **limit_fraction_denominator**: - **Input**: A float and an integer representing the maximum denominator. - **Output**: A string representing the fraction closest to the float with the given maximum denominator. - **Example**: ```python limit_fraction_denominator(3.141592653589793, 1000) -> \\"355/113\\" limit_fraction_denominator(0.3333333, 10) -> \\"1/3\\" ``` Constraints: - For `add_fractions`, the input strings will always represent valid fractions. - For `fraction_to_decimal`, the input string will always represent a valid fraction. - For `limit_fraction_denominator`, the float will always be a valid number and the maximum denominator will be a positive integer. Additional Information: - Use the `fractions.Fraction` class for all operations. - Ensure your functions handle both positive and negative fractions correctly. - Simplify fractions where appropriate. Notes: - Do not use any external libraries other than `fractions` and `decimal`. - The output fractions should be in their lowest terms for `add_fractions` and `limit_fraction_denominator`. Example Code: ```python from fractions import Fraction def add_fractions(frac1: str, frac2: str) -> str: # TODO: Implement the function pass def fraction_to_decimal(frac: str) -> float: # TODO: Implement the function pass def limit_fraction_denominator(value: float, max_denominator: int) -> str: # TODO: Implement the function pass ``` Implement these functions and test them to ensure they return the correct outputs as expected.","solution":"from fractions import Fraction def add_fractions(frac1: str, frac2: str) -> str: Adds two fractions and returns the result as a simplified fraction string. f1 = Fraction(frac1) f2 = Fraction(frac2) result = f1 + f2 return str(result) def fraction_to_decimal(frac: str) -> float: Converts a fraction string to its decimal (float) value. return float(Fraction(frac)) def limit_fraction_denominator(value: float, max_denominator: int) -> str: Converts a float to a fraction with a denominator not exceeding the given maximum. fraction = Fraction(value).limit_denominator(max_denominator) return str(fraction)"},{"question":"**Question: Implementing a Data Serialization and Deserialization Framework Using `xdrlib`** # Objective Design a Python script that uses the `xdrlib` module to serialize a mixed dataset into an XDR representation and then deserialize it back to its original form. Demonstrate the ability to handle various data types including integers, floats, strings, byte streams, and lists. # Problem Statement You are given a mixed data structure containing various data types: ```python sample_data = { \'name\': \'Alice\', # String \'age\': 30, # Integer \'height\': 1.65, # Float \'is_student\': False, # Boolean \'courses\': [ # List of Strings \'Mathematics\', \'Computer Science\', \'Physics\' ], \'profile_pic\': b\'x89PNGrnx1an\', # Byte stream } ``` You need to perform the following tasks: 1. Pack (serialize) this data structure into an XDR representation using the `xdrlib.Packer` class. 2. Unpack (deserialize) the XDR data back into the original data structure using the `xdrlib.Unpacker` class. 3. Implement error handling for both the packing and unpacking processes to ensure all edge cases are managed. # Instructions 1. Create a Python class `MixedDataSerializer` containing two methods: - `serialize(self, data: dict) -> bytes` - `deserialize(self, xdr_data: bytes) -> dict` 2. Implement the `serialize` method to convert the mixed data structure into XDR format. 3. Implement the `deserialize` method to convert XDR data back into the original Python data structure. 4. Ensure that the deserialized data matches the original `sample_data`. # Expected Input and Output Method: `serialize` - **Input**: Dictionary containing mixed data types. - **Output**: XDR-encoded byte string. Method: `deserialize` - **Input**: XDR-encoded byte string. - **Output**: Original dictionary with mixed data types. # Example ```python serializer = MixedDataSerializer() # Original data sample_data = { \'name\': \'Alice\', \'age\': 30, \'height\': 1.65, \'is_student\': False, \'courses\': [\'Mathematics\', \'Computer Science\', \'Physics\'], \'profile_pic\': b\'x89PNGrnx1an\', } # Serialize the data xdr_data = serializer.serialize(sample_data) print(xdr_data) # Should print the XDR-encoded byte string # Deserialize the data back to original format deserialized_data = serializer.deserialize(xdr_data) print(deserialized_data) # Should print the original sample_data dict assert deserialized_data == sample_data # This should be True ``` # Constraints and Considerations 1. Utilize appropriate `xdrlib` methods for packing and unpacking each data type. 2. Implement comprehensive error handling using `xdrlib.Error` and `xdrlib.ConversionError`. 3. Ensure that the serialized data adheres to XDR alignment and padding requirements. # Evaluation Criteria - Correct and efficient implementation of serialization and deserialization. - Handling of all specified data types and possible edge cases. - Proper error handling and informative error messages. - Code readability and adherence to Python coding standards.","solution":"import xdrlib class MixedDataSerializer: def serialize(self, data: dict) -> bytes: packer = xdrlib.Packer() try: packer.pack_string(data[\'name\'].encode(\'utf-8\')) packer.pack_int(data[\'age\']) packer.pack_double(data[\'height\']) packer.pack_bool(data[\'is_student\']) packer.pack_int(len(data[\'courses\'])) for course in data[\'courses\']: packer.pack_string(course.encode(\'utf-8\')) packer.pack_bytes(data[\'profile_pic\']) except (xdrlib.Error, TypeError) as e: raise ValueError(\\"Error packing data\\") from e return packer.get_buffer() def deserialize(self, xdr_data: bytes) -> dict: unpacker = xdrlib.Unpacker(xdr_data) try: name = unpacker.unpack_string().decode(\'utf-8\') age = unpacker.unpack_int() height = unpacker.unpack_double() is_student = unpacker.unpack_bool() num_courses = unpacker.unpack_int() courses = [] for _ in range(num_courses): courses.append(unpacker.unpack_string().decode(\'utf-8\')) profile_pic = unpacker.unpack_bytes() except (xdrlib.Error, TypeError) as e: raise ValueError(\\"Error unpacking data\\") from e return { \'name\': name, \'age\': age, \'height\': height, \'is_student\': is_student, \'courses\': courses, \'profile_pic\': profile_pic } # Example usage: # serializer = MixedDataSerializer() # sample_data = { # \'name\': \'Alice\', # \'age\': 30, # \'height\': 1.65, # \'is_student\': False, # \'courses\': [\'Mathematics\', \'Computer Science\', \'Physics\'], # \'profile_pic\': b\'x89PNGrnx1an\', # } # xdr_data = serializer.serialize(sample_data) # deserialized_data = serializer.deserialize(xdr_data) # assert deserialized_data == sample_data"},{"question":"Objective: To assess the student\'s ability to implement custom sorting functions using Python\'s `list.sort()`, `sorted()`, and key functions, demonstrating understanding of complex sorting techniques. Question: You are given a list of dictionaries, where each dictionary represents a book with the following attributes: `title` (string), `author` (string), `year` (integer), and `rating` (float). Your task is to implement a function that sorts this list of books based on multiple criteria. Implement the function `sort_books(books: List[Dict[str, Union[str, int, float]]], criteria: List[Tuple[str, bool]] -> List[Dict[str, Union[str, int, float]]])` that sorts the books based on a list of criteria. Each criterion is a tuple where the first element is the attribute to sort by (`\'title\'`, `\'author\'`, `\'year\'`, or `\'rating\'`), and the second element is a boolean indicating whether the sort should be in descending order (True for descending, False for ascending). Expected Input and Output: - **Input:** * `books`: A list of dictionaries, where each dictionary has keys `\'title\'`, `\'author\'`, `\'year\'`, and `\'rating\'`. * `criteria`: A list of tuples, where each tuple contains a string (one of `\'title\'`, `\'author\'`, `\'year\'`, or `\'rating\'`) and a boolean. - **Output:** * A sorted list of dictionaries according to the specified criteria. Constraints: - You may assume there will be no ties for simplicity. - The function should handle multiple levels of sorting as per the criteria provided. Example: ```python books = [ {\\"title\\": \\"Book A\\", \\"author\\": \\"Author X\\", \\"year\\": 2001, \\"rating\\": 4.5}, {\\"title\\": \\"Book B\\", \\"author\\": \\"Author Y\\", \\"year\\": 1999, \\"rating\\": 4.7}, {\\"title\\": \\"Book C\\", \\"author\\": \\"Author X\\", \\"year\\": 2000, \\"rating\\": 4.6}, {\\"title\\": \\"Book D\\", \\"author\\": \\"Author Z\\", \\"year\\": 2003, \\"rating\\": 4.8} ] criteria = [(\'year\', False), (\'rating\', True)] sorted_books = sort_books(books, criteria) # Output should be: # [ # {\\"title\\": \\"Book B\\", \\"author\\": \\"Author Y\\", \\"year\\": 1999, \\"rating\\": 4.7}, # {\\"title\\": \\"Book C\\", \\"author\\": \\"Author X\\", \\"year\\": 2000, \\"rating\\": 4.6}, # {\\"title\\": \\"Book A\\", \\"author\\": \\"Author X\\", \\"year\\": 2001, \\"rating\\": 4.5}, # {\\"title\\": \\"Book D\\", \\"author\\": \\"Author Z\\", \\"year\\": 2003, \\"rating\\": 4.8} # ] ``` **Note:** You may find the `operator.itemgetter` function useful for this task. Additionally, consider the stability of the sorts when implementing your solution. Good luck!","solution":"from typing import List, Dict, Union, Tuple def sort_books(books: List[Dict[str, Union[str, int, float]]], criteria: List[Tuple[str, bool]]) -> List[Dict[str, Union[str, int, float]]]: Sorts a list of books based on multiple criteria. Args: books: List[Dict[str, Union[str, int, float]]] criteria: List[Tuple[str, bool]] - a list of tuples where each tuple contains a string representing the attribute to sort by and a boolean indicating whether the sort should be in descending order. Returns: List[Dict[str, Union[str, int, float]]] - The sorted list of books. # Apply sorting criteria in reverse order for stable sort for key, descending in reversed(criteria): books.sort(key=lambda book: book[key], reverse=descending) return books"},{"question":"**Objective:** Create a PyTorch function that optimizes a tensor operation by leveraging available backends and their configurations. **Problem Statement:** Implement a function `optimize_matrix_multiplication` in PyTorch that performs matrix multiplication between two given tensors, and optimizes this operation by leveraging both `torch.backends.cuda` and `torch.backends.cudnn` configurations, if available. The function should: 1. Check if CUDA is available and built. 2. Enable TensorFloat-32 tensor cores for matrix multiplications (if available) by setting the appropriate backend attribute. 3. Enable cuDNN and set it to use deterministic algorithms. 4. Perform matrix multiplication between two tensors using the optimized settings. 5. Return the resulting tensor. **Function Signature:** ```python def optimize_matrix_multiplication(tensor_a: torch.Tensor, tensor_b: torch.Tensor) -> torch.Tensor: pass ``` **Input:** - `tensor_a` (torch.Tensor): A 2D tensor with dimensions (m, n). - `tensor_b` (torch.Tensor): A 2D tensor with dimensions (n, k). **Output:** - A 2D tensor with dimensions (m, k) resulting from the matrix multiplication of `tensor_a` and `tensor_b`. **Constraints:** - Assume `tensor_a` and `tensor_b` are always 2D tensors with compatible dimensions for matrix multiplication. - If CUDA is not available, the function should fall back to default PyTorch CPU operations. **Performance Requirements:** - The solution should leverage the available backends for optimal performance. - The function must be robust, checking feature availability before configuring backends. **Example:** ```python import torch a = torch.randn(100, 200) b = torch.randn(200, 300) result = optimize_matrix_multiplication(a, b) print(result.shape) # should print torch.Size([100, 300]) ``` **Hints:** - Use `torch.cuda.is_available()` to check CUDA availability. - Configure `torch.backends.cuda.matmul.allow_tf32` as needed. - Enable `torch.backends.cudnn.benchmark` to set performant settings. - Use tensor operations like `torch.matmul` for matrix multiplication.","solution":"import torch def optimize_matrix_multiplication(tensor_a: torch.Tensor, tensor_b: torch.Tensor) -> torch.Tensor: # Check if CUDA is available if torch.cuda.is_available(): # Enable TensorFloat-32 tensor cores for matrix multiplications torch.backends.cuda.matmul.allow_tf32 = True # Enable cuDNN and set it to use deterministic algorithms torch.backends.cudnn.benchmark = True torch.backends.cudnn.deterministic = True # Perform matrix multiplication using the optimized settings result = torch.matmul(tensor_a, tensor_b) return result"},{"question":"You are given a dataset of images of various fruits. Each image needs to be classified into two properties: 1. Type of fruit (`apple`, `pear`, `orange`). 2. Color of the fruit (`green`, `red`, `yellow`, `orange`). This is a typical example of a multiclass-multioutput classification problem. Your task is to implement a Python function named `classify_fruits` that takes two arguments: `X` (features) and `Y` (labels). The function should: 1. Use scikit-learn to create a `MultiOutputClassifier` with `RandomForestClassifier` as the base estimator. 2. Fit the model with the provided training data. 3. Predict the labels for the training data. 4. Return the predicted labels. **Input:** - `X`: A 2D numpy array or pandas DataFrame of shape `(n_samples, n_features)` representing the feature set. - `Y`: A 2D numpy array or pandas DataFrame of shape `(n_samples, 2)` representing the labels. The first column corresponds to the fruit type, and the second column corresponds to the fruit color. **Output:** - A 2D numpy array of shape `(n_samples, 2)` representing the predicted labels for each sample. **Constraints:** - Use `RandomForestClassifier` as the base estimator. - Ensure reproducibility by setting the `random_state` parameter to `42` in the `RandomForestClassifier`. Here is the function signature: ```python import numpy as np from sklearn.ensemble import RandomForestClassifier from sklearn.multioutput import MultiOutputClassifier def classify_fruits(X: np.ndarray, Y: np.ndarray) -> np.ndarray: # Create and fit the model multi_target_forest = MultiOutputClassifier(RandomForestClassifier(random_state=42)) multi_target_forest.fit(X, Y) # Predict the labels predicted_labels = multi_target_forest.predict(X) return predicted_labels ``` # Example ```python from sklearn.datasets import make_classification import numpy as np # Simulate a dataset for demonstration purposes X, y1 = make_classification(n_samples=10, n_features=5, n_classes=3, random_state=1) y2 = np.random.randint(0, 4, size=10) Y = np.vstack((y1, y2)).T # Call the function predicted_labels = classify_fruits(X, Y) print(predicted_labels) ``` **Performance requirements:** - The implemented solution should be able to handle moderate-sized datasets efficiently. # Notes: - Ensure that your solution is well-documented and includes comments to explain the key steps.","solution":"import numpy as np from sklearn.ensemble import RandomForestClassifier from sklearn.multioutput import MultiOutputClassifier def classify_fruits(X: np.ndarray, Y: np.ndarray) -> np.ndarray: Classify the fruits by type and color using a MultiOutputClassifier with RandomForestClassifier. Parameters: X (np.ndarray): Feature set of shape (n_samples, n_features). Y (np.ndarray): Labels of shape (n_samples, 2) where the first column is the fruit type and the second column is the fruit color. Returns: np.ndarray: Predicted labels of shape (n_samples, 2). # Create and fit the model multi_target_forest = MultiOutputClassifier(RandomForestClassifier(random_state=42)) multi_target_forest.fit(X, Y) # Predict the labels predicted_labels = multi_target_forest.predict(X) return predicted_labels"},{"question":"Coding Assessment Question # Objective This task assesses your understanding of the PyTorch module `torch.distributed.elastic.multiprocessing` and your ability to implement distributed processes using this package. # Problem Statement You are required to implement a function that distributes a computation task across multiple worker processes using PyTorch\'s `torch.distributed.elastic.multiprocessing`. Specifically, you will distribute a task that computes the squares of a given list of numbers. # Function Signature ```python def distributed_square_computation(numbers: List[int], num_workers: int) -> List[int]: Distributes the computation of squaring elements in the `numbers` list across `num_workers` processes. Args: numbers (List[int]): A list of integers to be squared. num_workers (int): The number of worker processes to use for the computation. Returns: List[int]: A list of squared numbers in the same order as the input. pass ``` # Input - `numbers`: A list of integers `numbers` where ( 1 leq text{len(numbers)} leq 10^5 ) and ( 0 leq text{numbers[i]} leq 10^4 ). - `num_workers`: An integer indicating the number of worker processes, where ( 1 leq text{num_workers} leq 32 ). # Output - A list of integers where each element is the square of the corresponding input number, maintaining the same order. # Requirements 1. Use `torch.distributed.elastic.multiprocessing.start_processes` to distribute the task among `num_workers` worker processes. 2. Each worker process should handle a chunk of the list and compute the squares of the numbers in that chunk. 3. Gather the results from all worker processes and combine them to form the final output list in the correct order. # Constraints - Assume the function will be executed in an environment where PyTorch and its dependencies are properly installed. - You should handle edge cases such as an empty list or a list with one element. # Example ```python numbers = [1, 2, 3, 4, 5] num_workers = 2 # Worker 1 will process [1, 2, 3] # Worker 2 will process [4, 5] distributed_square_computation(numbers, num_workers) # Output: [1, 4, 9, 16, 25] ``` # Additional Notes - Do not use the built-in `map` function or similar utilities that might simplify the task. - Ensure your implementation efficiently handles synchronization between worker processes if necessary.","solution":"from typing import List import torch.multiprocessing as mp def worker(worker_id, data_chunk, result_queue): Worker function to compute the squares of a given chunk of data. squared_chunk = [x ** 2 for x in data_chunk] result_queue.put((worker_id, squared_chunk)) def distributed_square_computation(numbers: List[int], num_workers: int) -> List[int]: Distributes the computation of squaring elements in the `numbers` list across `num_workers` processes. Args: numbers (List[int]): A list of integers to be squared. num_workers (int): The number of worker processes to use for the computation. Returns: List[int]: A list of squared numbers in the same order as the input. if not numbers: return [] # Split the data into chunks for each worker chunk_size = (len(numbers) + num_workers - 1) // num_workers chunks = [numbers[i * chunk_size:(i + 1) * chunk_size] for i in range(num_workers)] # Create a queue to collect results result_queue = mp.Queue() # Start worker processes processes = [] for worker_id in range(num_workers): p = mp.Process(target=worker, args=(worker_id, chunks[worker_id], result_queue)) processes.append(p) p.start() # Collect results from workers results = [None] * num_workers for _ in range(num_workers): worker_id, squared_chunk = result_queue.get() results[worker_id] = squared_chunk # Join all processes for p in processes: p.join() # Combine the result chunks into a single list squared_numbers = [] for chunk in results: if chunk is not None: squared_numbers.extend(chunk) return squared_numbers"},{"question":"**Coding Assessment Question:** # Objective: You will use the Seaborn library to generate and customize clustermap visualizations from a provided dataset. The following tasks will assess your understanding by having you load, manipulate, and visualize data using `sns.clustermap`. # Instructions: 1. **Load Dataset:** - Load the provided dataset into a pandas DataFrame. For this assessment, assume the dataset is the \\"Tips\\" dataset available in Seaborn. 2. **Data Preprocessing:** - Separate the dataset into numerical features and categorical features. 3. **Basic Clustermap:** - Generate a basic clustermap of the numerical features. 4. **Customize Clustermap:** - Create a more elaborate clustermap: - Change the size of the figure to 10 inches by 8 inches. - Use the \\"coolwarm\\" colormap and adjust the limits of the color range between 0 and 50. - Add colored labels for observations based on a categorical feature (e.g., \'sex\') using a custom color palette you define. 5. **Optimize Clustering:** - Adjust the clustering algorithm to use the \'euclidean\' metric and the \'average\' method. - Standardize the data within columns before generating the clustermap. # Example Input: ```python import seaborn as sns # Load the \\"Tips\\" dataset tips = sns.load_dataset(\\"tips\\") ``` # Expected Implementation: Your solution should involve multiple steps and make use of the `sns.clustermap()` function as demonstrated in the example below: ```python import seaborn as sns import pandas as pd # Load dataset tips = sns.load_dataset(\\"tips\\") # Separate numerical and categorical features numerical_features = tips.select_dtypes(include=[\'float64\', \'int64\']) categorical_features = tips.select_dtypes(include=[\'category\', \'object\']) # Basic clustermap sns.clustermap(numerical_features) # Define a custom color palette for labeling based on \'sex\' lut = {\'Male\': \'blue\', \'Female\': \'pink\'} row_colors = tips[\'sex\'].map(lut) # Advanced clustermap with customizations sns.clustermap( numerical_features, figsize=(10, 8), cmap=\\"coolwarm\\", vmin=0, vmax=50, row_colors=row_colors, metric=\'euclidean\', method=\'average\', standard_scale=1 ) ``` # Guidelines: - Ensure your code runs without errors and produces the required visualizations. - Use appropriate methods and parameters from the Seaborn library to achieve the desired customizations. - Validate your color palettes and clustering parameters to avoid distortions and ensure meaningful visualizations. # Constraints: - Do not hard-code the data or color mappings directly; use data-driven approaches where possible to make the solution more adaptable. # Submission: Submit your code in a Jupyter notebook with appropriate comments explaining each step and its purpose. Ensure the visualizations are clear and accurately reflect the customization requirements.","solution":"import seaborn as sns import pandas as pd def generate_clustermap(): # Load dataset tips = sns.load_dataset(\\"tips\\") # Separate numerical and categorical features numerical_features = tips.select_dtypes(include=[\'float64\', \'int64\']) categorical_features = tips.select_dtypes(include=[\'category\', \'object\']) # Basic clustermap sns.clustermap(numerical_features) # Define a custom color palette for labeling based on \'sex\' lut = {\'Male\': \'blue\', \'Female\': \'pink\'} row_colors = tips[\'sex\'].map(lut) # Advanced clustermap with customizations sns.clustermap( numerical_features, figsize=(10, 8), cmap=\\"coolwarm\\", vmin=0, vmax=50, row_colors=row_colors, metric=\'euclidean\', method=\'average\', standard_scale=1 ) return tips, numerical_features, categorical_features"},{"question":"**Objective**: Write a Python program to compare two sequences of text. The goal is to identify the differences between the two sequences and output these differences in a human-readable format. **Task**: Create a function `compare_sequences(seq1, seq2)` that: 1. Takes two arguments, `seq1` and `seq2`, which are lists of strings, each representing a sequence of text lines. 2. Uses the `difflib.SequenceMatcher` class to find the differences between the two sequences. 3. Outputs a detailed report of the differences, using the `difflib.Differ` class. 4. Returns the similarity ratio between the two sequences, using the `SequenceMatcher`\'s `ratio` method. # Input - `seq1`: A list of strings representing the first sequence. Each string ends with a newline character. - `seq2`: A list of strings representing the second sequence. Each string ends with a newline character. # Output - A detailed human-readable report of the differences between the two sequences, printed to the console. - A float value representing the similarity ratio between the two sequences, returned by the function. # Constraints - The sequences can contain any printable characters. - The sequences will not exceed 1000 lines each. # Function Signature ```python def compare_sequences(seq1: list[str], seq2: list[str]) -> float: # Write your code here ``` # Example ```python seq1 = [ \\" 1. Beautiful is better than ugly.n\\", \\" 2. Explicit is better than implicit.n\\", \\" 3. Simple is better than complex.n\\", \\" 4. Complex is better than complicated.n\\" ] seq2 = [ \\" 1. Beautiful is better than ugly.n\\", \\" 3. Simple is better than complex.n\\", \\" 4. Complicated is better than complex.n\\", \\" 5. Flat is better than nested.n\\" ] ratio = compare_sequences(seq1, seq2) print(f\\"Similarity ratio: {ratio:.3f}\\") ``` **Expected Output**: ``` 1. Beautiful is better than ugly. - 2. Explicit is better than implicit. - 3. Simple is better than complex. + 3. Simple is better than complex. ? ++ - 4. Complex is better than complicated. ? ^ ---- ^ + 4. Complicated is better than complex. ? ++++ ^ ^ + 5. Flat is better than nested. Similarity ratio: 0.667 ``` Implement the function `compare_sequences` to meet the above specifications.","solution":"from difflib import Differ, SequenceMatcher def compare_sequences(seq1, seq2): Compares the differences between two sequences of text lines and outputs human-readable differences while returning the similarity ratio between the two sequences. differ = Differ() diff = list(differ.compare(seq1, seq2)) for line in diff: print(line) matcher = SequenceMatcher(None, seq1, seq2) similarity_ratio = matcher.ratio() return similarity_ratio"},{"question":"# PyTorch Coding Assessment: Multidimensional FFT and Frequency Analysis You are given a multidimensional time-domain signal, and your task is to implement a function that computes its frequency-domain representation using Fourier transforms. Additionally, you will manipulate the frequency data to visualize significant frequency components prominently. Function Specification **Function Name**: `analyze_signal` **Inputs**: 1. `signal` (torch.Tensor): A real-valued multidimensional tensor representing the time-domain signal. 2. `shift` (bool): A boolean flag indicating whether to shift the zero-frequency component to the center using `fftshift`. **Outputs**: 1. `frequency_domain` (torch.Tensor): The frequency-domain representation of the input signal after applying the Fourier transform. 2. `frequencies` (torch.Tensor): The sample frequencies corresponding to the Fourier transformed signal. Constraints - The input tensor `signal` can have dimensions up to 3 (i.e., 1D, 2D, 3D signals). - The function should handle real-valued signals and use the appropriate FFT function for such inputs. - You must use `torch.fft.rfftn` for computing the Fourier transform of real-valued inputs. - Use `torch.fft.fftfreq` to compute the sample frequencies for each dimension. - If the `shift` flag is True, apply `torch.fft.fftshift` to both the frequency-domain representation and the sample frequencies. Example ```python import torch # Sample 2D signal signal = torch.tensor([ [1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0] ]) # Analyze the signal frequency_domain, frequencies = analyze_signal(signal, shift=True) print(\\"Frequency Domain Representation:n\\", frequency_domain) print(\\"Frequencies:n\\", frequencies) ``` Implementation of the function will demonstrate understanding of multidimensional FFT operations, the usage of helper functions to manipulate frequency data, and managing real-valued inputs efficiently. **Note** - The function should be efficient in handling inputs of various dimensions and sizes. - Proper handling and transformation of frequencies are crucial.","solution":"import torch def analyze_signal(signal, shift=False): Computes the frequency-domain representation of a real-valued multidimensional time-domain signal using FFT and optionally applies fftshift. Parameters: signal (torch.Tensor): A real-valued multidimensional tensor representing the time-domain signal. shift (bool): A boolean flag indicating whether to shift the zero-frequency component to the center. Returns: (torch.Tensor, torch.Tensor): Tuple containing the frequency-domain representation and the corresponding frequencies. # Compute the FFT of the signal frequency_domain = torch.fft.rfftn(signal) # Generate sample frequencies freqs = [torch.fft.fftfreq(n) for n in signal.shape] # If shift is True, apply fftshift to both frequency_domain and freqs if shift: frequency_domain = torch.fft.fftshift(frequency_domain) freqs = [torch.fft.fftshift(f) for f in freqs] return frequency_domain, freqs"},{"question":"Objective: This task assesses your ability to profile and optimize Python code using the provided debugging and profiling tools (`cProfile` for profiling and `tracemalloc` for memory allocation tracing). Problem Statement: You are given a Python script that performs various computations. Your task is to analyze and optimize this script using `cProfile` for profiling and `tracemalloc` for memory tracking. The goal is to identify bottlenecks and excessive memory usage points, then optimize the code to improve its performance and memory efficiency. Python Script to Analyze: ```python import time def compute_squares(n): result = [] for i in range(n): result.append(i**2) return result def compute_cubes(n): result = [] for i in range(n): result.append(i**3) return result def main(): start = time.time() squares = compute_squares(10**5) end = time.time() print(f\\"Computed squares in {end - start} seconds\\") start = time.time() cubes = compute_cubes(10**5) end = time.time() print(f\\"Computed cubes in {end - start} seconds\\") if __name__ == \\"__main__\\": main() ``` Requirements: 1. Profile the provided script using `cProfile` and identify the functions that take the most time to execute. 2. Trace the memory allocations of the script using `tracemalloc` and identify where most memory is used. 3. Refactor the script to optimize performance and reduce memory usage based on your profiling and tracing analysis. Ensure that the output of the optimized script remains the same. Input Format: There is no direct input for the script. The script\'s behavior is determined by its implementation. Output Format: The output should display the execution time before and after optimization for both computations (squares and cubes). Additionally, provide a brief explanation of the identified bottlenecks and optimizations applied. Constraints: - The script should handle computations for up to `10^5` elements efficiently. - Ensure the refactored code maintains accurate and consistent results. Performance Requirements: - The optimized script should show a noticeable improvement in execution time and/or memory usage as compared to the initial implementation. - Utilize the `cProfile` and `tracemalloc` modules effectively to guide your optimizations. Example Output: The output should include: 1. Original profiling and memory allocation details (text or summarized). 2. Description of the optimization steps. 3. Final profiling and memory allocation details (text or summarized). 4. Execution time comparison before and after optimization. Submission: Submit: 1. The original profiling and memory tracing results. 2. The optimized script. 3. The profiling and memory tracing results after optimization. 4. A brief report on the identified issues and optimizations applied.","solution":"import time # Optimize compute_squares and compute_cubes using list comprehensions def compute_squares(n): return [i**2 for i in range(n)] def compute_cubes(n): return [i**3 for i in range(n)] def main(): start = time.time() squares = compute_squares(10**5) end = time.time() print(f\\"Computed squares in {end - start} seconds\\") start = time.time() cubes = compute_cubes(10**5) end = time.time() print(f\\"Computed cubes in {end - start} seconds\\") if __name__ == \\"__main__\\": main()"},{"question":"**Question: Color Transformation Pipeline** You are tasked with writing a function that accepts a list of RGB color values and applies a series of color transformations. Specifically, you will convert the RGB values to the HSV color space, then from HSV to HLS, and finally from HLS back to RGB. The transformed RGB values will be returned as a list of tuples. # Function Signature ```python def color_transformation_pipeline(rgb_colors: List[Tuple[float, float, float]]) -> List[Tuple[float, float, float]]: pass ``` # Input - `rgb_colors`: A list of tuples, where each tuple contains three floating point values representing an RGB color. Each value in the tuple is between 0 and 1. - Example: `[(0.2, 0.4, 0.4), (0.1, 0.5, 0.9)]` # Output - Returns a list of tuples, where each tuple contains three floating point values representing the transformed RGB color after the series of conversions. - Example: `[(0.2, 0.4, 0.4), (0.1, 0.5, 0.9)]` # Requirements 1. You must use the `colorsys` module to perform the color conversions. 2. The function should iterate through each color in the input list, apply the transformations, and collect the transformed RGB values. 3. Ensure that the resulting RGB values remain within the valid range [0, 1]. 4. You are allowed to assume that the input list is non-empty and that all RGB values are valid floats in the range [0, 1]. # Example Given the input: ```python rgb_colors = [(0.2, 0.4, 0.4)] ``` The function will process the single color as follows: 1. Convert `(0.2, 0.4, 0.4)` from RGB to HSV. 2. Convert the resulting HSV value to HLS. 3. Convert the resulting HLS value back to RGB. The expected output for this example should convincingly show no significant change in the color due to the round-trip conversions: ```python output = [(0.2, 0.4, 0.4)] ``` # Additional Information You can assume that the transformation process is precise and the output will be close to the input values. Minor floating-point differences are expected due to conversion precision. Implement the function `color_transformation_pipeline` to satisfy the above requirements.","solution":"from typing import List, Tuple import colorsys def color_transformation_pipeline(rgb_colors: List[Tuple[float, float, float]]) -> List[Tuple[float, float, float]]: transformed_colors = [] for r, g, b in rgb_colors: # Convert RGB to HSV h, s, v = colorsys.rgb_to_hsv(r, g, b) # Convert HSV to HLS l = (2 - s) * v / 2 if l != 0: if l == 1: s = 0 elif l < 0.5: s = s * v / (2 * l) else: s = s * v / (2 - 2 * l) # Convert HLS back to RGB r, g, b = colorsys.hls_to_rgb(h, l, s) # Collect the transformed RGB colors transformed_colors.append((r, g, b)) return transformed_colors"},{"question":"**Question: Device and Memory Management in PyTorch** In this task, you are required to write code that demonstrates your understanding of device and memory management using the `torch.xpu` module in PyTorch. # Task 1. Write a function `initialize_device(device_id: int) -> None` that initializes the specified device by `device_id`. - If the device is not available or initialization fails, the function should raise a `RuntimeError`. 2. Write a function `memory_info(device_id: int) -> dict` that returns a dictionary containing the following memory statistics for the specified device: - `allocated` - the total memory allocated currently on the device. - `reserved` - the total memory reserved currently on the device. - `max_allocated` - the maximum memory allocated on the device. - `max_reserved` - the maximum memory reserved on the device. # Constraints - Assume there are at least two devices available. - Ensure adequate exception handling and informative error messages. # Input / Output Your functions should adhere to the following input/output protocols: - `initialize_device(device_id: int) -> None`: Initializes the given device. - **Input**: A device ID (integer). - **Output**: None. If initialization fails, raise a `RuntimeError`. - `memory_info(device_id: int) -> dict`: Returns memory statistics for the given device. - **Input**: A device ID (integer). - **Output**: A dictionary containing `allocated`, `reserved`, `max_allocated`, and `max_reserved` memory metrics (in bytes). # Example ```python def initialize_device(device_id: int) -> None: # Your implementation here pass def memory_info(device_id: int) -> dict: # Your implementation here pass # Example usage try: initialize_device(0) mem_info = memory_info(0) print(mem_info) except RuntimeError as e: print(f\\"Initialization failed with error: {e}\\") ``` # Notes - Utilize functions from `torch.xpu` library such as `set_device`, `memory_allocated`, `memory_reserved`, etc. - Ensure that the solution is efficient and handles different scenarios and edge cases appropriately.","solution":"import torch def initialize_device(device_id: int) -> None: Initializes the specified device by device_id. Raises a RuntimeError if the device is not available or initialization fails. if not torch.xpu.is_available(): raise RuntimeError(\\"torch.xpu is not available\\") if device_id < 0 or device_id >= torch.xpu.device_count(): raise RuntimeError(f\\"Device ID {device_id} is not valid\\") # Set the specified device torch.xpu.set_device(device_id) if torch.xpu.current_device() != device_id: raise RuntimeError(f\\"Failed to set the device to {device_id}\\") def memory_info(device_id: int) -> dict: Returns a dictionary containing the memory statistics for the specified device. if device_id < 0 or device_id >= torch.xpu.device_count(): raise RuntimeError(f\\"Device ID {device_id} is not valid\\") return { \\"allocated\\": torch.xpu.memory_allocated(device_id), \\"reserved\\": torch.xpu.memory_reserved(device_id), \\"max_allocated\\": torch.xpu.max_memory_allocated(device_id), \\"max_reserved\\": torch.xpu.max_memory_reserved(device_id) }"},{"question":"You are given a list of tasks, where each task is represented as a tuple `(priority, task_description)`. Your goal is to implement a task scheduler that can efficiently handle adding new tasks, removing tasks, and retrieving tasks based on their priority. You will use a heap-based priority queue for this purpose. # Task Implement a class `TaskScheduler` with the following methods: 1. **`__init__(self)`**: Initializes an empty priority queue. 2. **`add_task(self, priority: int, task: str) -> None`**: Adds a new task with the given priority to the scheduler. 3. **`remove_task(self, task: str) -> None`**: Removes the specified task from the scheduler. If the task is not found, raise a `KeyError`. 4. **`pop_task(self) -> str`**: Removes and returns the task with the highest priority (smallest priority number). If the scheduler is empty, raise a `KeyError`. 5. **`peek_task(self) -> str`**: Returns the task with the highest priority without removing it. If the scheduler is empty, raise a `KeyError`. # Example ```python ts = TaskScheduler() ts.add_task(2, \\"write code\\") ts.add_task(1, \\"write spec\\") ts.add_task(3, \\"create tests\\") print(ts.pop_task()) # Output: \\"write spec\\" print(ts.peek_task()) # Output: \\"write code\\" ts.remove_task(\\"write code\\") print(ts.pop_task()) # Output: \\"create tests\\" ``` # Constraints - You can assume that all task descriptions are unique. - Priority is an integer where a smaller number indicates a higher priority. - All methods must operate within O(log n) time complexity where n is the number of tasks in the scheduler (except initialization and peeking). Implement the `TaskScheduler` class in Python using the `heapq` module.","solution":"import heapq class TaskScheduler: def __init__(self): self.heap = [] self.task_map = {} def add_task(self, priority: int, task: str) -> None: heapq.heappush(self.heap, (priority, task)) self.task_map[task] = priority def remove_task(self, task: str) -> None: if task not in self.task_map: raise KeyError(f\\"Task \'{task}\' not found\\") priority = self.task_map.pop(task) self.heap = [(p, t) for p, t in self.heap if t != task] heapq.heapify(self.heap) def pop_task(self) -> str: if not self.heap: raise KeyError(\\"No tasks available\\") priority, task = heapq.heappop(self.heap) del self.task_map[task] return task def peek_task(self) -> str: if not self.heap: raise KeyError(\\"No tasks available\\") priority, task = self.heap[0] return task"},{"question":"# Question: Implementing LDA and QDA for Classification You are given a dataset for a multiclass classification problem. Your task is to implement classification using both Linear Discriminant Analysis (LDA) and Quadratic Discriminant Analysis (QDA) from the scikit-learn library. You will need to: 1. **Fit and Evaluate Classifiers**: - Train LDA and QDA classifiers on the provided training data. - Evaluate the performance of both classifiers on the provided testing data using accuracy as the metric. 2. **Dimensionality Reduction with LDA**: - Perform dimensionality reduction using LDA to project the data into a lower-dimensional space (specified with `n_components`). - Train a Logistic Regression classifier on the reduced-dimensionality data and evaluate its performance. 3. **Using Shrinkage in LDA**: - Train an LDA classifier using shrinkage for covariance estimation and evaluate its performance. 4. **Selecting Different Solvers for LDA**: - Train and evaluate the LDA classifier using different solvers (`svd`, `lsqr`, and `eigen`). Compare their performance. # Input: - `X_train`: 2D array of shape (n_samples_train, n_features) containing the training data. - `y_train`: 1D array of shape (n_samples_train,) containing the training labels. - `X_test`: 2D array of shape (n_samples_test, n_features) containing the testing data. - `y_test`: 1D array of shape (n_samples_test,) containing the testing labels. - `n_components`: Integer specifying the number of components for dimensionality reduction. # Output: - `results`: A dictionary containing: - `lda_accuracy`: Accuracy of LDA classifier on test data. - `qda_accuracy`: Accuracy of QDA classifier on test data. - `lda_logreg_accuracy`: Accuracy of Logistic Regression on LDA-reduced data. - `lda_shrinkage_accuracy`: Accuracy of LDA classifier with shrinkage on test data. - `lda_solver_accuracies`: A dictionary with keys as solvers (`svd`, `lsqr`, `eigen`) and values as their respective accuracies on test data. # Constraints: - Use scikit-learn\'s LDA and QDA implementations. - Use `accuracy_score` from scikit-learn to evaluate performance. # Example: ```python from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis from sklearn.linear_model import LogisticRegression from sklearn.metrics import accuracy_score def lda_qda_classification(X_train, y_train, X_test, y_test, n_components): results = {} # Train and evaluate LDA lda = LinearDiscriminantAnalysis() lda.fit(X_train, y_train) y_pred_lda = lda.predict(X_test) results[\'lda_accuracy\'] = accuracy_score(y_test, y_pred_lda) # Train and evaluate QDA qda = QuadraticDiscriminantAnalysis() qda.fit(X_train, y_train) y_pred_qda = qda.predict(X_test) results[\'qda_accuracy\'] = accuracy_score(y_test, y_pred_qda) # Dimensionality reduction with LDA and Logistic Regression lda_reduction = LinearDiscriminantAnalysis(n_components=n_components) X_train_reduced = lda_reduction.fit_transform(X_train, y_train) X_test_reduced = lda_reduction.transform(X_test) logreg = LogisticRegression() logreg.fit(X_train_reduced, y_train) y_pred_logreg = logreg.predict(X_test_reduced) results[\'lda_logreg_accuracy\'] = accuracy_score(y_test, y_pred_logreg) # LDA with shrinkage lda_shrinkage = LinearDiscriminantAnalysis(shrinkage=\'auto\', solver=\'lsqr\') lda_shrinkage.fit(X_train, y_train) y_pred_shrinkage = lda_shrinkage.predict(X_test) results[\'lda_shrinkage_accuracy\'] = accuracy_score(y_test, y_pred_shrinkage) # LDA with different solvers solvers = [\'svd\', \'lsqr\', \'eigen\'] results[\'lda_solver_accuracies\'] = {} for solver in solvers: lda_solver = LinearDiscriminantAnalysis(solver=solver) lda_solver.fit(X_train, y_train) y_pred_solver = lda_solver.predict(X_test) results[\'lda_solver_accuracies\'][solver] = accuracy_score(y_test, y_pred_solver) return results ``` Provide the necessary imports and example function call below. ```python # Example usage: # X_train, y_train, X_test, y_test = load_data() # User-defined function to load data # n_components = 2 # Example number for dimensionality reduction # result = lda_qda_classification(X_train, y_train, X_test, y_test, n_components) # print(result) ```","solution":"from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis from sklearn.linear_model import LogisticRegression from sklearn.metrics import accuracy_score def lda_qda_classification(X_train, y_train, X_test, y_test, n_components): results = {} # Train and evaluate LDA lda = LinearDiscriminantAnalysis() lda.fit(X_train, y_train) y_pred_lda = lda.predict(X_test) results[\'lda_accuracy\'] = accuracy_score(y_test, y_pred_lda) # Train and evaluate QDA qda = QuadraticDiscriminantAnalysis() qda.fit(X_train, y_train) y_pred_qda = qda.predict(X_test) results[\'qda_accuracy\'] = accuracy_score(y_test, y_pred_qda) # Dimensionality reduction with LDA and Logistic Regression lda_reduction = LinearDiscriminantAnalysis(n_components=n_components) X_train_reduced = lda_reduction.fit_transform(X_train, y_train) X_test_reduced = lda_reduction.transform(X_test) logreg = LogisticRegression() logreg.fit(X_train_reduced, y_train) y_pred_logreg = logreg.predict(X_test_reduced) results[\'lda_logreg_accuracy\'] = accuracy_score(y_test, y_pred_logreg) # LDA with shrinkage lda_shrinkage = LinearDiscriminantAnalysis(shrinkage=\'auto\', solver=\'lsqr\') lda_shrinkage.fit(X_train, y_train) y_pred_shrinkage = lda_shrinkage.predict(X_test) results[\'lda_shrinkage_accuracy\'] = accuracy_score(y_test, y_pred_shrinkage) # LDA with different solvers solvers = [\'svd\', \'lsqr\', \'eigen\'] results[\'lda_solver_accuracies\'] = {} for solver in solvers: lda_solver = LinearDiscriminantAnalysis(solver=solver) lda_solver.fit(X_train, y_train) y_pred_solver = lda_solver.predict(X_test) results[\'lda_solver_accuracies\'][solver] = accuracy_score(y_test, y_pred_solver) return results"},{"question":"# Machine Learning Assessment: Model Evaluation using scikit-learn Problem Statement You are provided with several toy datasets from the scikit-learn library. Your task is to: 1. Load the `wine` dataset. 2. Preprocess the dataset using standard techniques (e.g., handle missing values if any, normalize features). 3. Split the dataset into training and testing sets. 4. Train several classification models (Logistic Regression, Decision Tree, SVM) on the training data. 5. Evaluate the performance of each model on the test set using accuracy, precision, recall, and F1 score. 6. Select the best-performing model based on the F1 score and perform cross-validation to confirm the robustness of the model. Instructions 1. **Loading Dataset:** - Use the `load_wine` function from the `sklearn.datasets` module to load the dataset. 2. **Preprocessing:** - Handle any missing values appropriately if they exist. - Normalize the features (e.g., using `StandardScaler` from `sklearn.preprocessing`). 3. **Data Splitting:** - Split the dataset into 70% training and 30% testing sets using `train_test_split` from `sklearn.model_selection`. 4. **Model Training:** - Train the following classification models on the training set: - Logistic Regression (`LogisticRegression` from `sklearn.linear_model`) - Decision Tree (`DecisionTreeClassifier` from `sklearn.tree`) - Support Vector Machine (`SVC` from `sklearn.svm`) 5. **Model Evaluation:** - Evaluate each model on the test set using the following metrics: - Accuracy - Precision - Recall - F1 Score - Use `classification_report` from `sklearn.metrics` to obtain these metrics. 6. **Model Selection and Cross-Validation:** - Identify the best-performing model based on the F1 score. - Perform cross-validation (5 folds) using `cross_val_score` from `sklearn.model_selection` to ensure the selected model\'s robustness. Expected Input and Output Formats - **Input:** - The function should not require any input. - **Output:** - The function should print the evaluation metrics for each model. - The function should return the name of the best-performing model based on the F1 score. Constraints - You may assume that the `load_wine` function does not encounter any errors during loading. - Time complexity should be considered to be within acceptable performance bounds for toy datasets. Sample Code ```python from sklearn.datasets import load_wine from sklearn.model_selection import train_test_split, cross_val_score from sklearn.preprocessing import StandardScaler from sklearn.linear_model import LogisticRegression from sklearn.tree import DecisionTreeClassifier from sklearn.svm import SVC from sklearn.metrics import classification_report def evaluate_wine_classifiers(): # Load dataset data = load_wine() X, y = data.data, data.target # Preprocess data scaler = StandardScaler() X = scaler.fit_transform(X) # Split data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Initialize models models = { \\"Logistic Regression\\": LogisticRegression(), \\"Decision Tree\\": DecisionTreeClassifier(), \\"SVM\\": SVC() } best_model = None best_f1_score = 0 # Evaluate models for name, model in models.items(): model.fit(X_train, y_train) y_pred = model.predict(X_test) report = classification_report(y_test, y_pred, output_dict=True) f1_score = report[\'weighted avg\'][\'f1-score\'] print(f\\"Model: {name}\\") print(classification_report(y_test, y_pred)) if f1_score > best_f1_score: best_f1_score = f1_score best_model = model best_model_name = [key for key, value in models.items() if value == best_model][0] # Perform cross-validation cv_scores = cross_val_score(best_model, X, y, cv=5, scoring=\'f1_weighted\') print(f\\"Best model based on F1 score: {best_model_name}\\") print(\\"Cross-validation F1 scores:\\", cv_scores) print(\\"Mean cross-validation F1 score:\\", cv_scores.mean()) return best_model_name # Example execution best_model = evaluate_wine_classifiers() print(\\"Best Model:\\", best_model) ```","solution":"from sklearn.datasets import load_wine from sklearn.model_selection import train_test_split, cross_val_score from sklearn.preprocessing import StandardScaler from sklearn.linear_model import LogisticRegression from sklearn.tree import DecisionTreeClassifier from sklearn.svm import SVC from sklearn.metrics import classification_report def evaluate_wine_classifiers(): # Load dataset data = load_wine() X, y = data.data, data.target # Preprocess data scaler = StandardScaler() X = scaler.fit_transform(X) # Split data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Initialize models models = { \\"Logistic Regression\\": LogisticRegression(), \\"Decision Tree\\": DecisionTreeClassifier(), \\"SVM\\": SVC() } best_model = None best_f1_score = 0 # Evaluate models for name, model in models.items(): model.fit(X_train, y_train) y_pred = model.predict(X_test) report = classification_report(y_test, y_pred, output_dict=True) f1_score = report[\'weighted avg\'][\'f1-score\'] print(f\\"Model: {name}\\") print(classification_report(y_test, y_pred)) if f1_score > best_f1_score: best_f1_score = f1_score best_model = model best_model_name = [key for key, value in models.items() if value == best_model][0] # Perform cross-validation cv_scores = cross_val_score(best_model, X, y, cv=5, scoring=\'f1_weighted\') print(f\\"Best model based on F1 score: {best_model_name}\\") print(\\"Cross-validation F1 scores:\\", cv_scores) print(\\"Mean cross-validation F1 score:\\", cv_scores.mean()) return best_model_name # Example execution # best_model = evaluate_wine_classifiers() # print(\\"Best Model:\\", best_model)"},{"question":"**Objective:** Implement a function that uses the `glob` module to find specific types of files in a directory and its subdirectories. The function will also escape any special characters in the filenames found. Problem Statement You are tasked with implementing a function `find_and_escape_files` that takes the following parameters: - `pattern` (str): The pattern used to match file names. - `root_dir` (str): The root directory from which to start the search. - `recursive` (bool): Whether to search recursively into subdirectories. The function should return a list of file paths that match the given pattern. Each file path in the output should be the escaped version of the actual file path. Function Signature ```python def find_and_escape_files(pattern: str, root_dir: str, recursive: bool) -> list: pass ``` Example Usage: Consider the following directory structure: ``` /root ├── file1.txt ├── file2.txt ├── file[3].txt └── subdir ├── file4.txt └── file5[?].txt ``` **Input:** ```python pattern = \\"*.txt\\" root_dir = \\"/root\\" recursive = True ``` **Output:** ```python [ \'/root/file1.txt\', \'/root/file2.txt\', \'/root/file[3].txt\', \'/root/subdir/file4.txt\', \'/root/subdir/file5[?].txt\' ] ``` Constraints: 1. The function should use the `glob` module for matching paths. 2. The returned paths should include an escaped version using `glob.escape`. 3. If `recursive` is set to `True`, the function should search all subdirectories; otherwise, just the current directory. 4. Assume that file names and directory structures do not exceed memory limits. Notes: - You may assume that the `root_dir` provided is a valid directory path. - Handle file and directory names that contain special characters (e.g., `*`, `?`, `[`). Guidance: 1. Use `glob.glob` or `glob.iglob` for fetching file paths. 2. Utilize `glob.escape` to escape special characters in the file paths.","solution":"import glob import os def find_and_escape_files(pattern: str, root_dir: str, recursive: bool) -> list: Find files matching the pattern in the given directory and escape their paths. Args: - pattern (str): The pattern to match file names. - root_dir (str): The root directory to start the search. - recursive (bool): Whether to search recursively into subdirectories. Returns: - list: A list of escaped file paths matching the pattern. search_pattern = os.path.join(root_dir, \'**\', pattern) if recursive else os.path.join(root_dir, pattern) matched_files = glob.glob(search_pattern, recursive=recursive) escaped_files = [glob.escape(file_path) for file_path in matched_files] return escaped_files"},{"question":"You are tasked with developing a neural network to classify flowers in the Iris dataset using scikit-learn\'s `MLPClassifier`. The Iris dataset contains three classes of flowers (Setosa, Versicolour, and Virginica), and four features (sepal length, sepal width, petal length, and petal width). Your goal is to implement a function `train_and_evaluate_mlp` that trains an MLPClassifier on this dataset and evaluates its performance. # Requirements 1. **Function**: `train_and_evaluate_mlp()` 2. **Input**: None 3. **Output**: Tuple of two elements: - Trained `MLPClassifier` model. - Accuracy score of the model on the test set. # Constraints - Use `StandardScaler` for feature scaling. - Use `train_test_split` to split the dataset into training (80%) and testing (20%) sets. - The MLP should have: - One hidden layer with 10 neurons. - Use the `relu` activation function. - Set `max_iter` to 500. - Use the `adam` solver. - Set `random_state` to 42 for reproducibility. # Evaluation Criteria - The function must properly initialize and train an MLPClassifier. - The model should apply feature scaling appropriately. - The function must return the trained model and its accuracy score on the test dataset. # Example In the function `train_and_evaluate_mlp`, you should be able to use the following code to load the Iris dataset: ```python from sklearn.datasets import load_iris data = load_iris() X = data.data y = data.target ``` # Solution Template ```python def train_and_evaluate_mlp(): from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.neural_network import MLPClassifier from sklearn.metrics import accuracy_score # Load the dataset data = load_iris() X, y = data.data, data.target # Split the dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Scale the features scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) # Initialize and train the MLPClassifier mlp = MLPClassifier(hidden_layer_sizes=(10,), activation=\'relu\', solver=\'adam\', max_iter=500, random_state=42) mlp.fit(X_train, y_train) # Predict on the test set and calculate the accuracy y_pred = mlp.predict(X_test) accuracy = accuracy_score(y_test, y_pred) # Return the model and accuracy score return mlp, accuracy # Example usage: # model, acc = train_and_evaluate_mlp() # print(f\\"Trained MLP model: {model}\\") # print(f\\"Accuracy on test set: {acc}\\") ``` Ensure your solution meets the specified requirements and adheres to the constraints for full credit.","solution":"def train_and_evaluate_mlp(): from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.neural_network import MLPClassifier from sklearn.metrics import accuracy_score # Load the dataset data = load_iris() X, y = data.data, data.target # Split the dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Scale the features scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) # Initialize and train the MLPClassifier mlp = MLPClassifier(hidden_layer_sizes=(10,), activation=\'relu\', solver=\'adam\', max_iter=500, random_state=42) mlp.fit(X_train, y_train) # Predict on the test set and calculate the accuracy y_pred = mlp.predict(X_test) accuracy = accuracy_score(y_test, y_pred) # Return the model and accuracy score return mlp, accuracy"},{"question":"Objective: To assess students\' understanding of internationalization using Python\'s `gettext` module, including translation handling and dynamically changing languages. Problem Statement: You are tasked with creating a multilingual greeting application. The application should support English and Spanish, and it should allow the user to switch between these languages dynamically. Implement a Python class `GreetingApp` that handles the following functionalities: 1. **Initialization**: Load the appropriate gettext translations. 2. **Set Language**: A method to change the current language of the application. 3. **Greet**: A method to output a greeting message in the current language. Requirements: 1. The `GreetingApp` class should be initialized with a default language (English). 2. Implement a method `set_language(self, lang_code)` where `lang_code` is either `\'en\'` for English or `\'es\'` for Spanish, to switch the language dynamically. 3. Implement a method `greet(self, name)` that prints `Hello, <name>!` in the appropriate language. In Spanish, this should be `¡Hola, <name>!`. Input and Output Formats: - **Input**: The language code `lang_code` (a string), and `name` (a string). - **Output**: A greeting message printed to standard output. Constraints: - Assume valid inputs are always provided. - Performance considerations are negligible for this problem. Example Usage: ```python app = GreetingApp() # Default language is English app.greet(\\"Alice\\") # Output: Hello, Alice! app.set_language(\'es\') app.greet(\\"Bob\\") # Output: ¡Hola, Bob! app.set_language(\'en\') app.greet(\\"Carlos\\") # Output: Hello, Carlos! ``` Notes: - You may simulate the message catalog using dictionaries for this exercise due to the limitations of the environment. - Focus on the usage of `gettext` for the translations. Your Task: Implement the `GreetingApp` class with the specified methods. ```python import gettext class GreetingApp: def __init__(self): self.language = \'en\' self.translations = { \'en\': { \'greet\': \'Hello, {}!\', }, \'es\': { \'greet\': \'¡Hola, {}!\', } } def set_language(self, lang_code): if lang_code in self.translations: self.language = lang_code def greet(self, name): print(self.translations[self.language][\'greet\'].format(name)) ``` Test your implementation to ensure correctness.","solution":"class GreetingApp: def __init__(self): self.language = \'en\' self.translations = { \'en\': { \'greet\': \'Hello, {}!\', }, \'es\': { \'greet\': \'¡Hola, {}!\', } } def set_language(self, lang_code): if lang_code in self.translations: self.language = lang_code def greet(self, name): print(self.translations[self.language][\'greet\'].format(name))"},{"question":"# Python Coding Assessment: Custom Byte-Compilation Function The `compileall` module in Python provides various utility functions to compile Python source files into bytecode. This task will assess your understanding of Python file handling, recursion, and multiprocessing by asking you to implement a simplified version of a `compile_dir` function, using some of the functionalities provided in the `compileall` module. Task Write a function `custom_compile_dir(directory, recursive=True, force=False, optimization_level=0, workers=1, excluded_patterns=None)` that will recursively compile all `.py` files in the given directory into `.pyc` files. Function Signature ```python def custom_compile_dir(directory: str, recursive: bool=True, force: bool=False, optimization_level: int=0, workers: int=1, excluded_patterns: list=None) -> bool: pass ``` Parameters - `directory (str)`: The directory to compile. - `recursive (bool)`: If `True`, compile files in subdirectories recursively; otherwise, compile only files in the specified directory. - `force (bool)`: If `True`, forcibly recompile all files even if they were already up-to-date. - `optimization_level (int)`: The optimization level to use for the compilation (similar to `compileall` module). - `workers (int)`: The number of worker processes to use for parallel compilation. If `1`, no parallelism is used. - `excluded_patterns (list)`: A list of regex patterns as strings. If a file path matches any of these patterns, it will be excluded from compilation. Return - `bool`: Return `True` if all files compiled successfully, and `False` otherwise. Constraints 1. Handle file compilation using Python’s built-in `py_compile` module. 2. Respect the constraints and options provided in the parameters (e.g., recursion depth, force recompilation, parallelism). Example Usage ```python result = custom_compile_dir(\'some_dir\', recursive=True, force=True, optimization_level=2, workers=4, excluded_patterns=[r\'[/]test_\', r\'[/].svn\']) print(result) # Should print True if all compilations succeeded, False otherwise ``` Additional Information - Use `os.listdir`, `os.path.isdir`, and similar modules for directory traversal. - Use `re` module for handling regex matches to exclude files. - Use `multiprocessing.Pool` for managing parallel worker processes if `workers > 1`. You have been provided with comprehensive documentation on the `compileall` module that should help you in understanding how the provided functionalities work. Use that information, and Python’s built-in modules to implement the required functionality.","solution":"import os import re import py_compile from multiprocessing import Pool def _compile_file(file, force, optimization_level): try: py_compile.compile(file, cfile=None, dfile=None, doraise=not force, optimize=optimization_level) return True except py_compile.PyCompileError: return False def _should_exclude(file, excluded_patterns): if excluded_patterns: for pattern in excluded_patterns: if re.search(pattern, file): return True return False def custom_compile_dir(directory, recursive=True, force=False, optimization_level=0, workers=1, excluded_patterns=None): all_files = [] for root, dirs, files in os.walk(directory): for file in files: if file.endswith(\\".py\\"): file_path = os.path.join(root, file) if not _should_exclude(file_path, excluded_patterns): all_files.append(file_path) if not recursive: break if workers > 1: with Pool(workers) as pool: results = pool.starmap(_compile_file, [(file, force, optimization_level) for file in all_files]) else: results = [_compile_file(file, force, optimization_level) for file in all_files] return all(results)"},{"question":"Problem Statement: You are required to create a PyTorch Linear model, perform certain operations on the meta device and then convert it back to a CPU tensor with appropriate initialization. The task involves the following steps: 1. **Model Definition**: - Define a custom Linear model class that inherits from `torch.nn.Module`. - The model should have two linear layers: `Linear(10, 20)` and `Linear(20, 30)`. 2. **Meta Device Operations**: - Initialize an instance of your Linear model and move it onto the meta device. - Create an input tensor of shape `(5, 10)` and move it to the meta device. 3. **Transformation**: - Perform the forward pass of your model using the meta input tensor to obtain a meta output tensor. 4. **Transfer to CPU**: - Transfer both the model and the input tensor from the meta device to the CPU, ensuring proper initialization of parameters. - Reinitialize the parameters of your model using random values. 5. **Validation**: - Perform the forward pass of the reinitialized model using the reinitialized input tensor. - Return the output tensor. Constraints: - You must use PyTorch functions and classes as described in the documentation. - The initial tensors and the model must be correctly moved to the meta device, and likewise correctly brought back to the CPU with data initialization. - Use random initialization for reinitializing the parameters of the model when transferring to the CPU. Function Signature: ```python import torch import torch.nn as nn class CustomLinearModel(nn.Module): def __init__(self): super(CustomLinearModel, self).__init__() # Define Linear layers def forward(self, x): # Define forward pass pass def process_meta_to_cpu(): Initialize and process the CustomLinearModel on the meta device, then transfer the model and the input tensor to the CPU with random initialization. Returns: torch.Tensor: the output tensor obtained from the forward pass of the model on CPU. # Implement the function as described pass ``` # Example: ```python if __name__ == \\"__main__\\": output = process_meta_to_cpu() print(output) ``` This question assesses the student\'s ability to handle tensor operations on the meta device, including creating, manipulating, and transferring tensors and models correctly within PyTorch\'s framework.","solution":"import torch import torch.nn as nn class CustomLinearModel(nn.Module): def __init__(self): super(CustomLinearModel, self).__init__() self.layer1 = nn.Linear(10, 20) self.layer2 = nn.Linear(20, 30) def forward(self, x): x = self.layer1(x) x = self.layer2(x) return x def process_meta_to_cpu(): Initialize and process the CustomLinearModel on the meta device, then transfer the model and the input tensor to the CPU with random initialization. Returns: torch.Tensor: the output tensor obtained from the forward pass of the model on CPU. # Initialize model on meta device model = CustomLinearModel().to(\'meta\') # Create input tensor on meta device input_meta = torch.rand(5, 10, device=\'meta\') # Perform forward pass on meta device to get a meta output tensor output_meta = model(input_meta) # Transfer model to CPU and reinitialize parameters model_cpu = CustomLinearModel().to(\'cpu\') # Transfer input tensor to CPU input_cpu = torch.rand(5, 10, device=\'cpu\') # Perform forward pass on CPU with reinitialized model and input output_cpu = model_cpu(input_cpu) return output_cpu"},{"question":"Objective: Implement a custom serializable class using Python\'s `pickle` module that demonstrates your understanding of advanced pickling and unpickling techniques, including handling persistent IDs for external objects. Question: You are required to design a class `CustomClass` that can be pickled and unpickled using custom persistent IDs. Additionally, implement another class `CustomPickler` that handles the custom pickling process, and a class `CustomUnpickler` to handle the unpickling process. Here are the requirements: 1. **CustomClass**: - This class should have the attributes `name` (string) and `data` (list of integers). - When an instance of `CustomClass` is pickled, it should be referenced by a persistent ID. 2. **CustomPickler**: - This class should subclass `pickle.Pickler`. - Implement the `persistent_id` method to generate a persistent ID for instances of `CustomClass` using their `name` attribute. 3. **CustomUnpickler**: - This class should subclass `pickle.Unpickler`. - Implement the `persistent_load` method to reconstruct instances of `CustomClass` from their persistent IDs. 4. **Main Function**: - Write a main function that demonstrates the pickling and unpickling process. - Create instances of `CustomClass`, pickle them using `CustomPickler`, and unpickle them using `CustomUnpickler`. - Demonstrate that the reconstructed objects retain their original data. Constraints: - Follow the provided class structures and methods. Additional helper methods or functions are allowed. - Ensure the persistent IDs are unique and meaningful. Expected Input and Output: - **Input**: Creation of instances of `CustomClass`. - **Output**: Unpickled instances retaining their data. ```python # Your code starts here import pickle class CustomClass: def __init__(self, name, data): self.name = name self.data = data def __repr__(self): return f\\"CustomClass(name={self.name}, data={self.data})\\" class CustomPickler(pickle.Pickler): def persistent_id(self, obj): if isinstance(obj, CustomClass): return f\\"CustomClass-{obj.name}\\" return None class CustomUnpickler(pickle.Unpickler): def persistent_load(self, pid): if pid.startswith(\\"CustomClass-\\"): name = pid[len(\\"CustomClass-\\"):] # Reconstruct the CustomClass object with the given name # For simplicity, assume an empty list for data. return CustomClass(name, []) raise pickle.UnpicklingError(\\"Unknown persistent ID\\") def main(): # Create instances of CustomClass instances = [ CustomClass(\\"instance1\\", [1, 2, 3]), CustomClass(\\"instance2\\", [4, 5, 6]) ] # Pickle the instances using CustomPickler with open(\'custom_data.pickle\', \'wb\') as f: pickler = CustomPickler(f) pickler.dump(instances) # Unpickle the instances using CustomUnpickler with open(\'custom_data.pickle\', \'rb\') as f: unpickler = CustomUnpickler(f) unpickled_instances = unpickler.load() # Display the unpickled instances for instance in unpickled_instances: print(instance) if __name__ == \\"__main__\\": main() # Your code ends here ``` Explanation: 1. **CustomClass**: Defines a simple class with `name` and `data` attributes. 2. **CustomPickler**: Subclasses `pickle.Pickler` and defines `persistent_id` for instances of `CustomClass`. 3. **CustomUnpickler**: Subclasses `pickle.Unpickler` and defines `persistent_load` to reconstruct `CustomClass` instances. 4. **main()**: Demonstrates the pickling and unpickling process, ensuring that unpickled data matches the original data, using `CustomPickler` and `CustomUnpickler`. Additional Notes: - Ensure the main function runs without errors and demonstrates successful pickling and unpickling. - Enhance your solution with error handling, especially for unknown persistent IDs, to make it robust.","solution":"import pickle class CustomClass: def __init__(self, name, data): self.name = name self.data = data def __repr__(self): return f\\"CustomClass(name={self.name}, data={self.data})\\" class CustomPickler(pickle.Pickler): def persistent_id(self, obj): if isinstance(obj, CustomClass): return f\\"CustomClass-{obj.name}\\" return None class CustomUnpickler(pickle.Unpickler): def persistent_load(self, pid): if pid.startswith(\\"CustomClass-\\"): name = pid[len(\\"CustomClass-\\"):] # In real cases, look it up or re-create it - here we use static data for simplicity if name == \\"instance1\\": return CustomClass(name, [1, 2, 3]) elif name == \\"instance2\\": return CustomClass(name, [4, 5, 6]) raise pickle.UnpicklingError(\\"Unknown persistent ID\\") def main(): # Create instances of CustomClass instances = [ CustomClass(\\"instance1\\", [1, 2, 3]), CustomClass(\\"instance2\\", [4, 5, 6]) ] # Pickle the instances using CustomPickler with open(\'custom_data.pickle\', \'wb\') as f: pickler = CustomPickler(f) pickler.dump(instances) # Unpickle the instances using CustomUnpickler with open(\'custom_data.pickle\', \'rb\') as f: unpickler = CustomUnpickler(f) unpickled_instances = unpickler.load() # Display the unpickled instances for instance in unpickled_instances: print(instance) if __name__ == \\"__main__\\": main()"},{"question":"**Objective**: Implement a pair of functions that demonstrate the compression and decompression of data using Python\'s `zlib` library, along with checksum verification to ensure data integrity. Problem Statement Implement the following functions: 1. `compress_data(data: bytes, level: int = -1) -> bytes`: - This function should take in a bytes object `data` and an optional integer `level` controlling the level of compression. - Utilize the `zlib.compress()` function to compress the data. - Return the compressed data as a bytes object. 2. `decompress_data(data: bytes) -> bytes`: - This function should take in a bytes object `data` which represents the compressed data. - Utilize the `zlib.decompress()` function to decompress the data. - Return the decompressed data as a bytes object. 3. `checksum_verification(original_data: bytes, compressed_data: bytes) -> bool`: - This function should take in two bytes objects: `original_data` (uncompressed) and `compressed_data` (compressed and then decompressed). - Compute the Adler-32 checksum of `original_data` using `zlib.adler32()`. - Decompress the `compressed_data`, then compute the Adler-32 checksum of the decompressed data. - Return `True` if both checksums match, indicating data integrity; otherwise return `False`. Constraints - The `level` parameter in `compress_data` should be an integer between `-1` and `9`. - Assume that the compressed data is always correctly formatted and no error handling for decompression is required. Example ```python original = b\'This is a test string for compression and decompression.\' compressed = compress_data(original, level=6) decompressed = decompress_data(compressed) assert decompressed == original # The original and decompressed data should be the same is_intact = checksum_verification(original, decompressed) assert is_intact == True # Checksum should verify the integrity of the decompressed data ``` Performance Considerations - Ensure that the functions handle large data efficiently. - Avoid unnecessary memory copies or allocations where possible.","solution":"import zlib def compress_data(data: bytes, level: int = -1) -> bytes: Compress the input bytes data using zlib with the specified compression level. :param data: Input data in bytes to be compressed. :param level: Compression level, with -1 being the default. :return: Compressed data in bytes. return zlib.compress(data, level) def decompress_data(data: bytes) -> bytes: Decompress the input bytes data using zlib. :param data: Input compressed data in bytes to be decompressed. :return: Decompressed data in bytes. return zlib.decompress(data) def checksum_verification(original_data: bytes, compressed_data: bytes) -> bool: Verify the integrity of the original data by comparing checksums. :param original_data: The original data in bytes. :param compressed_data: The compressed data that has been decompressed. :return: True if the checksums match, False otherwise. original_checksum = zlib.adler32(original_data) decompressed_data = decompress_data(compressed_data) decompressed_checksum = zlib.adler32(decompressed_data) return original_checksum == decompressed_checksum"},{"question":"**Complex Event-Driven Application using Asyncio** # Problem Statement You are required to design and implement a simplified event-driven application using Python\'s `asyncio` module. This application simulates a basic server-client model where multiple clients send requests to the server concurrently, and the server processes each request with a defined latency. # Requirements 1. **Server**: - The server should handle multiple client connections concurrently. - Start the server using the `asyncio.start_server` method. - Each client request should be processed by a function `handle_client_request` that introduces a simulated processing delay using `await asyncio.sleep(delay)`. - Use a `Queue` to manage incoming client requests and ensure they are processed in a FIFO order. 2. **Client**: - Simulate client connections to the server. - Each client sends a request to the server and waits for a response. - The client should terminate after receiving the response. 3. **Synchronization**: - Use appropriate asyncio synchronization primitives or constructs (`Lock`, `Semaphore`, etc.) to handle critical sections if required. 4. **Exception Handling**: - Handle any potential asyncio-specific exceptions gracefully. - Ensure that resources are released properly upon client disconnection or any unexpected errors. # Function Signatures ```python import asyncio from typing import Tuple async def handle_client_request(reader: asyncio.StreamReader, writer: asyncio.StreamWriter, delay: int): Handles client request by introducing a simulated processing delay. Args: reader (asyncio.StreamReader): The reader stream for the client. writer (asyncio.StreamWriter): The writer stream for the client. delay (int): Processing delay in seconds for simulating latency. pass async def start_server(host: str, port: int, delay: int): Starts the server and handles incoming client requests using asyncio. Args: host (str): The host address to bind the server. port (int): The port number to bind the server. delay (int): Processing delay in seconds for simulating latency. pass async def simulate_client_request(host: str, port: int, message: str) -> str: Simulates a client requesting the server. Args: host (str): The server\'s host address to connect. port (int): The server\'s port number to connect. message (str): The message to send to the server. Returns: str: The server\'s response message. pass ``` # Input and Output 1. **Server Input**: Host, Port, Processing Delay 2. **Client Input**: Host, Port, Message 3. **Client Output**: Response message from the server. # Constraints - Host is a valid IP address. - Port is an integer between 1024 and 65535. - Delay is a non-negative integer representing seconds. - Message is a non-empty string. # Example ```python # Start server await start_server(\'localhost\', 8888, 2) # Simulate client requests response = await simulate_client_request(\'localhost\', 8888, \'Hello, Server!\') print(response) ``` # Notes: - You are required to implement all the necessary functionality. - Make sure to include adequate error handling, logging, and resource cleanup. - Ensure your code is efficient and does not lead to deadlocks or resource starvation.","solution":"import asyncio from asyncio import StreamReader, StreamWriter from typing import Tuple async def handle_client_request(reader: StreamReader, writer: StreamWriter, delay: int): Handles client request by introducing a simulated processing delay. Args: reader (StreamReader): The reader stream for the client. writer (StreamWriter): The writer stream for the client. delay (int): Processing delay in seconds for simulating latency. data = await reader.read(100) message = data.decode() addr = writer.get_extra_info(\'peername\') print(f\\"Received {message} from {addr}\\") print(f\\"Waiting for {delay} seconds\\") await asyncio.sleep(delay) print(\\"Sending: \\" + message) writer.write(data) await writer.drain() writer.close() await writer.wait_closed() async def start_server(host: str, port: int, delay: int): Starts the server and handles incoming client requests using asyncio. Args: host (str): The host address to bind the server. port (int): The port number to bind the server. delay (int): Processing delay in seconds for simulating latency. server = await asyncio.start_server( lambda r, w: handle_client_request(r, w, delay), host, port) addr = server.sockets[0].getsockname() print(f\'Serving on {addr}\') async with server: await server.serve_forever() async def simulate_client_request(host: str, port: int, message: str) -> str: Simulates a client requesting the server. Args: host (str): The server\'s host address to connect. port (int): The server\'s port number to connect. message (str): The message to send to the server. Returns: str: The server\'s response message. reader, writer = await asyncio.open_connection(host, port) print(f\'Send: {message}\') writer.write(message.encode()) await writer.drain() data = await reader.read(100) print(f\'Received: {data.decode()}\') writer.close() await writer.wait_closed() return data.decode()"},{"question":"**Question:** Implement and Compare Different Covariance Estimation Techniques **Objective:** This task will assess your understanding of various covariance estimation techniques using the `sklearn.covariance` package. You will be required to implement, fit, and compare these estimators on a synthetic dataset. **Input:** - A dataset `X` with shape (n_samples, n_features). - Assume `X` has already been generated and standardized. **Output:** - Covariance matrices for each of the methods specified. - A brief interpretation of the results. **Constraints and Requirements:** 1. You must use Scikit-learn\'s implementations as described in the provided documentation. 2. Compare at least the following covariance estimation techniques: - Empirical Covariance - Shrunk Covariance - Ledoit-Wolf shrinkage - Oracle Approximating Shrinkage (OAS) - Minimum Covariance Determinant 3. Plot the covariance matrices using a visualization library (e.g., Matplotlib) 4. Discuss the differences in the covariance matrices produced by each method. 5. Highlight any particular strengths or weaknesses observed for each method on the provided dataset. **Example Code Template:** ```python import numpy as np import matplotlib.pyplot as plt from sklearn.covariance import (EmpiricalCovariance, ShrunkCovariance, LedoitWolf, OAS, MinCovDet) # Assume X is your standardized dataset # X = ... # Define and fit each model emp_cov = EmpiricalCovariance().fit(X) shrunk_cov = ShrunkCovariance(shrinkage=0.1).fit(X) lw_cov = LedoitWolf().fit(X) oas_cov = OAS().fit(X) mcd_cov = MinCovDet().fit(X) # Extract covariance matrices cov_matrices = { \\"Empirical Covariance\\": emp_cov.covariance_, \\"Shrunk Covariance\\": shrunk_cov.covariance_, \\"Ledoit-Wolf Covariance\\": lw_cov.covariance_, \\"OAS Covariance\\": oas_cov.covariance_, \\"Minimum Covariance Determinant\\": mcd_cov.covariance_ } # Plot each covariance matrix fig, axes = plt.subplots(1, len(cov_matrices), figsize=(20, 5)) for ax, (title, cov_matrix) in zip(axes, cov_matrices.items()): cax = ax.matshow(cov_matrix, cmap=\'viridis\') ax.set_title(title) fig.colorbar(cax, ax=ax) plt.show() # Interpretation of results # Discuss differences in covariance matrices, strengths, and weaknesses. ``` **Your Task:** 1. Implement the complete code to compute and visualize all the specified covariance matrices. 2. Write a brief interpretation (3-4 paragraphs) discussing the differences in the covariance matrices, and the strengths and weaknesses of each method when applied to the given dataset. **Submission:** Submit the code and the interpretation as a single Jupyter Notebook file.","solution":"import numpy as np import matplotlib.pyplot as plt from sklearn.covariance import (EmpiricalCovariance, ShrunkCovariance, LedoitWolf, OAS, MinCovDet) # Generate synthetic dataset np.random.seed(42) X = np.random.randn(100, 5) # 100 samples, 5 features # Define and fit each model emp_cov = EmpiricalCovariance().fit(X) shrunk_cov = ShrunkCovariance(shrinkage=0.1).fit(X) lw_cov = LedoitWolf().fit(X) oas_cov = OAS().fit(X) mcd_cov = MinCovDet().fit(X) # Extract covariance matrices cov_matrices = { \\"Empirical Covariance\\": emp_cov.covariance_, \\"Shrunk Covariance\\": shrunk_cov.covariance_, \\"Ledoit-Wolf Covariance\\": lw_cov.covariance_, \\"OAS Covariance\\": oas_cov.covariance_, \\"Minimum Covariance Determinant\\": mcd_cov.covariance_ } # Plot each covariance matrix fig, axes = plt.subplots(1, len(cov_matrices), figsize=(20, 5)) for ax, (title, cov_matrix) in zip(axes, cov_matrices.items()): cax = ax.matshow(cov_matrix, cmap=\'viridis\') ax.set_title(title) fig.colorbar(cax, ax=ax) plt.show() # Interpretation of results Empirical Covariance: This is the simplest estimator that computes the sample covariance matrix directly from the data. It is unbiased but can be very sensitive to outliers and may not perform well when the number of samples is small relative to the number of features. Shrunk Covariance: The shrunk covariance matrix introduces a regularization parameter that makes the estimation more robust to noise and more stable. The amount of shrinkage controls the bias-variance tradeoff, potentially making the estimator more reliable in situations where there are few samples. Ledoit-Wolf Covariance: This method automatically determines an optimal shrinkage value, balancing complexity and stability. It generally performs well for high-dimensional data and provides a more stable covariance estimator than the empirical method without the need to manually tune parameters. OAS Covariance: Similar to Ledoit-Wolf, OAS aims to find an optimal balance between bias and variance but can be more effective when there are fewer samples relative to the number of features. It might perform better than Ledoit-Wolf when the data distribution deviates from assumptions made by Ledoit-Wolf. Minimum Covariance Determinant: This estimator is robust to outliers and aims to find a subset of the data that minimizes the determinant of the covariance matrix. It is particularly useful in the presence of outliers, but it can be computationally intensive to work with larger datasets."},{"question":"Implementing a Simple Asynchronous Chat Server Objective Your task is to implement a simple asynchronous chat server using the `asynchat` module. The server should be able to handle multiple clients, receive messages from clients, and broadcast messages to all connected clients. Requirements 1. You must create a subclass of `asynchat.async_chat` named `ChatHandler`. 2. Implement the `collect_incoming_data` method to buffer incoming data from clients. 3. Implement the `found_terminator` method to handle complete messages from clients. A message should be considered complete when a newline character (`n`) is encountered. 4. Create a method to broadcast messages to all connected clients. 5. Implement a basic server class that listens for incoming connections and spawns a new `ChatHandler` object for each connection. Detailed Steps 1. **Class Definition and Initialization:** - Define the `ChatHandler` class that inherits from `asynchat.async_chat`. - Initialize the class with a socket, client address, and a reference to a list of connected clients. - Set an appropriate terminator (e.g., newline character `n`). 2. **Data Collection:** - Implement the `collect_incoming_data` method to collect data in a buffer. 3. **Message Handling:** - Implement the `found_terminator` method to process complete messages. This method should: - Retrieve the complete message from the buffer. - Broadcast the message to all connected clients. 4. **Broadcast Method:** - Create a method `broadcast_message` in the `ChatHandler` class that sends a given message to all clients in the list. 5. **Server Implementation:** - Implement a basic server class `ChatServer` using `asyncore.dispatcher` that: - Listens on a specified port. - Accepts incoming connections. - Creates a new `ChatHandler` instance for each incoming connection and adds it to the list of connected clients. Input and Output Formats - **Input:** Multiple clients connecting to the server and sending messages. - **Output:** Messages received from one client should be broadcast to all connected clients. Constraints - Use the `asynchat` and `asyncore` modules provided by Python. - Handle multiple clients asynchronously. - Properly manage data buffers and termination conditions. Example (Partial Code Skeleton) ```python import asyncore import asynchat import socket class ChatHandler(asynchat.async_chat): def __init__(self, sock, clients): super().__init__(sock) self.clients = clients self.clients.append(self) self.set_terminator(b\'n\') self.ibuffer = [] def collect_incoming_data(self, data): self.ibuffer.append(data) def found_terminator(self): message = b\'\'.join(self.ibuffer).decode(\'utf-8\').strip() self.ibuffer = [] self.broadcast_message(message) def broadcast_message(self, message): for client in self.clients: if client != self: client.push((message + \'n\').encode(\'utf-8\')) class ChatServer(asyncore.dispatcher): def __init__(self, host, port): super().__init__() self.create_socket(socket.AF_INET, socket.SOCK_STREAM) self.bind((host, port)) self.listen(5) self.clients = [] def handle_accept(self): sock, addr = self.accept() ChatHandler(sock, self.clients) if __name__ == \\"__main__\\": server = ChatServer(\'localhost\', 12345) asyncore.loop() ``` Notes - You may need to handle certain edge cases, such as clients disconnecting or sending incomplete messages. - The provided example is a partial implementation. You must complete and test it to ensure it meets the requirements.","solution":"import asyncore import asynchat import socket class ChatHandler(asynchat.async_chat): def __init__(self, sock, clients): super().__init__(sock) self.clients = clients self.clients.append(self) self.set_terminator(b\'n\') self.ibuffer = [] def collect_incoming_data(self, data): self.ibuffer.append(data) def found_terminator(self): message = b\'\'.join(self.ibuffer).decode(\'utf-8\').strip() self.ibuffer = [] self.broadcast_message(message) def broadcast_message(self, message): for client in self.clients: if client != self: client.push((message + \'n\').encode(\'utf-8\')) class ChatServer(asyncore.dispatcher): def __init__(self, host, port): super().__init__() self.create_socket(socket.AF_INET, socket.SOCK_STREAM) self.set_reuse_addr() self.bind((host, port)) self.listen(5) self.clients = [] def handle_accept(self): sock, addr = self.accept() ChatHandler(sock, self.clients) if __name__ == \\"__main__\\": server = ChatServer(\'localhost\', 12345) asyncore.loop()"},{"question":"**Question: Implementing a Mock of Python Cell Objects** In Python, cell objects are a crucial part of closures, allowing variables to be referenced by multiple scopes. Your task is to implement a mock version of these cell objects in pure Python. This will help you understand the underlying mechanism of how closures and cell objects work together. You need to implement the following: 1. A `Cell` class that mimics the behavior of cell objects in Python. 2. Functions to interact with instances of the `Cell` class: - `cell_check(ob)`: Check if `ob` is an instance of the `Cell` class. - `cell_new(value)`: Create a new `Cell` object containing `value`. - `cell_get(cell)`: Return the contents of the `cell`. - `cell_set(cell, value)`: Set the contents of the `cell` to `value`. # Requirements: - The `Cell` class should encapsulate a single value and provide methods for setting and getting this value. - The functions should perform the required checks as described in the documentation. - The `cell_set` function should overwrite the existing value and ensure proper reference handling (for example, if working with an object, make sure to update references properly). # Example Usage: ```python # Creating a new cell object c = cell_new(10) print(cell_get(c)) # Output: 10 # Setting the value of the cell object cell_set(c, 20) print(cell_get(c)) # Output: 20 # Checking if an object is a cell print(cell_check(c)) # Output: True print(cell_check(5)) # Output: False ``` # Constraints: - Do not use any import statements other than those necessary for type hinting and annotations. - Ensure your code is well-documented and follows Python\'s PEP 8 guidelines. - The `Cell` class should handle any type of value, including `None`. Implementing these functions will require a good understanding of object-oriented programming principles and how Python manages references. Focus on encapsulation and ensuring that your class and functions closely mimic the behavior of python310\'s cell objects.","solution":"class Cell: Mock implementation of Python cell objects. def __init__(self, value): self._value = value def get(self): Returns the value contained in the cell. return self._value def set(self, value): Sets a new value to the cell. self._value = value def cell_check(ob): Check if ob is an instance of the Cell class. return isinstance(ob, Cell) def cell_new(value): Create a new Cell object containing value. return Cell(value) def cell_get(cell): Return the contents of the cell. if not cell_check(cell): raise TypeError(\\"Expected a Cell object\\") return cell.get() def cell_set(cell, value): Set the contents of the cell to value. if not cell_check(cell): raise TypeError(\\"Expected a Cell object\\") cell.set(value)"},{"question":"# Custom Importer Implementation **Objective:** Your task is to implement a custom importer in Python that utilizes the `importlib` module to perform lazy imports of modules. This means that the module should only be imported when it is first accessed. **Description:** 1. Implement a class `LazyImporter` that overrides the default import mechanism to provide the following functionalities: - Lazy importing of specified modules. - Cache the imported modules so that subsequent access doesn\'t trigger a re-import. - Ability to manually load all specified lazy imports immediately if required. 2. The `LazyImporter` should take a list of module names to be lazily imported. 3. The `LazyImporter` should have the following methods: - `__init__(self, modules: List[str])`: Initialize with a list of module names to be lazily imported. - `load_all(self)`: Import all specified modules immediately. - `get_module(self, module_name: str)`: Retrieve the module with the given name, importing it if necessary. **Input/Output:** - Input: A list of module names to be lazily imported. - Output: The class `LazyImporter` that provides the lazy importing mechanism as described. **Constraints:** 1. You should use the `importlib` module to handle the imports. 2. The implementation should handle the case where a module might not exist, raising an appropriate ImportError. 3. The caching mechanism should ensure that modules are only imported once, even if accessed multiple times. **Example Usage:** ```python # Example usage of LazyImporter lazy_importer = LazyImporter([\'math\', \'os\']) # Accessing a lazy imported module math_module = lazy_importer.get_module(\'math\') print(math_module.sqrt(16)) # Should print 4.0 # Attempting to access a non-lazy module should raise an ImportError try: unknown_module = lazy_importer.get_module(\'unknown\') except ImportError as e: print(e) # Expected output: \\"No module named \'unknown\'\\" # Manually load all lazy imports lazy_importer.load_all() # Accessing previously lazy imported module again should use cached version os_module = lazy_importer.get_module(\'os\') print(os_module.name) # Should print the OS name ``` **Note:** Ensure your implementation is efficient and avoids unnecessary imports and re-imports.","solution":"import importlib class LazyImporter: def __init__(self, modules): self.modules = modules self.imported_modules = {} def load_all(self): for module_name in self.modules: if module_name not in self.imported_modules: self.imported_modules[module_name] = importlib.import_module(module_name) def get_module(self, module_name): if module_name in self.imported_modules: return self.imported_modules[module_name] elif module_name in self.modules: module = importlib.import_module(module_name) self.imported_modules[module_name] = module return module else: raise ImportError(f\\"No module named \'{module_name}\'\\")"},{"question":"# Pandas Coding Assessment You are provided with a DataFrame containing various types of data. Here is an example setup: ```python import pandas as pd import numpy as np dtypes = [\\"int64\\", \\"float64\\", \\"datetime64[ns]\\", \\"timedelta64[ns]\\", \\"complex128\\", \\"object\\", \\"bool\\"] n = 5000 data = {t: np.random.randint(100, size=n).astype(t) for t in dtypes} df = pd.DataFrame(data) df[\\"categorical\\"] = df[\\"object\\"].astype(\\"category\\") ``` Task 1: Memory Usage Calculation Write a function `calculate_memory_usage` that takes the above DataFrame `df` and returns a dictionary with the following keys: - `\\"total_memory_usage_bytes\\"`: The total memory usage of the DataFrame in bytes. - `\\"column_memory_usage_bytes\\"`: A dictionary where the keys are column names and the values are the memory usage in bytes for each column. - `\\"index_memory_usage_bytes\\"`: The memory usage of the DataFrame\'s index in bytes. **Function Signature:** ```python def calculate_memory_usage(df: pd.DataFrame) -> dict: pass ``` **Example Output:** ```python calculate_memory_usage(df) # Output: # { # \\"total_memory_usage_bytes\\": 520000, # \\"column_memory_usage_bytes\\": { # \\"int64\\": 40000, # \\"float64\\": 40000, # \\"datetime64[ns]\\": 40000, # ... # }, # \\"index_memory_usage_bytes\\": 40000 #} ``` Task 2: Boolean Operations on DataFrame You need to check if any column in the DataFrame `df` contains any `True` values. Write a function `check_any_true` that takes the DataFrame `df` as input and returns a boolean indicating if there is at least one `True` value in any of the DataFrame\'s columns. **Function Signature:** ```python def check_any_true(df: pd.DataFrame) -> bool: pass ``` **Example Output:** ```python check_any_true(df) # Output: True or False depending on the random values generated in the `bool` column. ``` Constraints: - You are not allowed to use loops to iterate through the DataFrame for these calculations. - Use the functions and methods provided by pandas to achieve these tasks efficiently. Hints: - For memory usage, look into `DataFrame.memory_usage` method and its parameters. - For checking boolean values, consider `DataFrame.any` method.","solution":"import pandas as pd import numpy as np def calculate_memory_usage(df: pd.DataFrame) -> dict: Calculate the memory usage of the DataFrame including its columns and index. total_memory_usage_bytes = df.memory_usage(deep=True).sum() column_memory_usage_bytes = df.memory_usage(deep=True, index=False).to_dict() index_memory_usage_bytes = df.index.memory_usage() return { \\"total_memory_usage_bytes\\": total_memory_usage_bytes, \\"column_memory_usage_bytes\\": column_memory_usage_bytes, \\"index_memory_usage_bytes\\": index_memory_usage_bytes } def check_any_true(df: pd.DataFrame) -> bool: Check if any column in the DataFrame contains any True values. return df.any().any() # Example DataFrame Creation for Testing dtypes = [\\"int64\\", \\"float64\\", \\"datetime64[ns]\\", \\"timedelta64[ns]\\", \\"complex128\\", \\"object\\", \\"bool\\"] n = 5000 data = {t: np.random.randint(100, size=n).astype(t) for t in dtypes} df = pd.DataFrame(data) df[\\"categorical\\"] = df[\\"object\\"].astype(\\"category\\")"},{"question":"# Question: **Visualizing Categorical and Numerical Data from the Titanic Dataset with Seaborn** You are provided with the Titanic dataset. Your task is to use the Seaborn library to create informative visualizations that explore the relationships between different variables in the dataset. # Instructions: 1. **Load the Titanic Dataset:** ```python import seaborn as sns titanic = sns.load_dataset(\\"titanic\\") ``` 2. **Task 1: Count Plot of \'class\' Variable** - Create a count plot showing the number of passengers in each class (\'class\'). 3. **Task 2: Survival Rate by Class** - Modify the count plot to show the survival rate for each class using the \'hue\' parameter (i.e., creating a grouped count plot by the \'survived\' variable). 4. **Task 3: Normalize Counts to Show Percentages** - Further modify the plot from Task 2 to represent counts as percentages rather than raw numbers. 5. **Task 4: Customize Plot Aesthetics** - Change the theme to \'darkgrid\'. - Add appropriate titles and axis labels to each of the plots to make them more informative. # Implementation Details: - Use the provided code snippets as the base and expand upon them to complete each task. - Ensure the plots are clearly labeled and customized as specified. - Write your code in a function named `visualize_titanic_data`. # Expected Function Signature: ```python def visualize_titanic_data(): # Task 1: Count Plot of \'class\' Variable # Task 2: Survival Rate by Class # Task 3: Normalize Counts to Show Percentages # Task 4: Customize Plot Aesthetics pass ``` # Constraints: - Ensure your code runs without errors and generates the plots as described. - You may use additional seaborn/matplotlib functionalities for customization, but the core tasks should be implemented using the instructions provided above. # Example Output: The function should generate and display the following plots: 1. A count plot of passengers by class. 2. A grouped count plot showing survival rates by class. 3. A normalized count plot showing survival rates by class in percentages. 4. Customized versions of the above plots with the \'darkgrid\' theme and proper titles/labels.","solution":"import seaborn as sns import matplotlib.pyplot as plt def visualize_titanic_data(): # Load the Titanic dataset titanic = sns.load_dataset(\\"titanic\\") # Task 1: Count Plot of \'class\' Variable plt.figure(figsize=(10, 6)) sns.countplot(data=titanic, x=\'class\') plt.title(\'Number of Passengers by Class\') plt.xlabel(\'Class\') plt.ylabel(\'Count\') plt.show() # Task 2: Survival Rate by Class plt.figure(figsize=(10, 6)) sns.countplot(data=titanic, x=\'class\', hue=\'survived\') plt.title(\'Survival Rate by Class\') plt.xlabel(\'Class\') plt.ylabel(\'Count\') plt.legend(title=\'Survived\', labels=[\'No\', \'Yes\']) plt.show() # Task 3: Normalize Counts to Show Percentages class_survived = titanic.groupby([\'class\', \'survived\']).size().unstack().fillna(0) class_survived = class_survived.div(class_survived.sum(axis=1), axis=0) * 100 class_survived = class_survived.stack().reset_index(name=\'percentage\') plt.figure(figsize=(10, 6)) sns.barplot(data=class_survived, x=\'class\', y=\'percentage\', hue=\'survived\') plt.title(\'Survival Rate by Class (in %)\') plt.xlabel(\'Class\') plt.ylabel(\'Percentage\') plt.legend(title=\'Survived\', labels=[\'No\', \'Yes\']) plt.show() # Task 4: Customize Plot Aesthetics sns.set_theme(style=\'darkgrid\') plt.figure(figsize=(10, 6)) sns.countplot(data=titanic, x=\'class\') plt.title(\'Number of Passengers by Class (Darkgrid Theme)\') plt.xlabel(\'Class\') plt.ylabel(\'Count\') plt.show() plt.figure(figsize=(10, 6)) sns.countplot(data=titanic, x=\'class\', hue=\'survived\') plt.title(\'Survival Rate by Class (Darkgrid Theme)\') plt.xlabel(\'Class\') plt.ylabel(\'Count\') plt.legend(title=\'Survived\', labels=[\'No\', \'Yes\']) plt.show() plt.figure(figsize=(10, 6)) sns.barplot(data=class_survived, x=\'class\', y=\'percentage\', hue=\'survived\') plt.title(\'Survival Rate by Class (in %) (Darkgrid Theme)\') plt.xlabel(\'Class\') plt.ylabel(\'Percentage\') plt.legend(title=\'Survived\', labels=[\'No\', \'Yes\']) plt.show() # To execute the function and visualize the plots visualize_titanic_data()"},{"question":"**Title: Meeting Scheduler** # Objective: Create a Python program that schedules multiple meetings and identifies conflicts using advanced data types and functionalities from Python\'s `datetime` and `collections` modules. # Problem Statement: Design and implement a function `schedule_meetings(meeting_times: List[Tuple[str, str]]) -> Dict[str, List[str]]` that takes a list of tuples. Each tuple contains two strings representing the start and end times of a meeting in the format `YYYY-MM-DDTHH:MM`. The function should schedule these meetings and identify any conflicts. Here\'s a step-by-step breakdown of what your function needs to do: 1. **Input**: * A list of tuples `meeting_times`, where each tuple `(start, end)` represents the start and end times of a meeting. Example input: ```python meeting_times = [ (\\"2023-10-19T09:00\\", \\"2023-10-19T10:30\\"), (\\"2023-10-19T10:00\\", \\"2023-10-19T11:00\\"), (\\"2023-10-19T11:00\\", \\"2023-10-19T12:00\\") ] ``` 2. **Output**: * A dictionary with two keys: `\\"scheduled\\"` and `\\"conflicts\\"`. Each key points to a list of strings, with meetings formatted as `\\"<start> to <end>\\"`. Example output for the above input: ```python { \\"scheduled\\": [ \\"2023-10-19T09:00 to 2023-10-19T10:30\\", \\"2023-10-19T11:00 to 2023-10-19T12:00\\" ], \\"conflicts\\": [ \\"2023-10-19T10:00 to 2023-10-19T11:00\\" ] } ``` 3. **Constraints**: * Assume that all times are in the same time zone and have no \'daylight saving\' changes. * Meetings that overlap any other scheduled meetings are considered conflicts. If a meeting starts exactly when another meeting ends, it is not considered a conflict. # Performance Requirements: * Your solution should be efficient and able to handle up to 1000 meetings within a reasonable time frame. # Detailed Steps: 1. Parse the input strings into `datetime` objects. 2. Utilize a data structure from the `collections` module (e.g., `defaultdict` or `OrderedDict`) to manage the orders of meetings. 3. Sort the meetings by their start times. 4. Iterate through the sorted list and use a mechanism, perhaps leveraging the `heapq` module, to detect and manage conflicts. 5. Format the output as specified above. # Example Usage: ```python from typing import List, Tuple, Dict from datetime import datetime from collections import defaultdict def schedule_meetings(meeting_times: List[Tuple[str, str]]) -> Dict[str, List[str]]: # Your code here # Example meeting_times = [ (\\"2023-10-19T09:00\\", \\"2023-10-19T10:30\\"), (\\"2023-10-19T10:00\\", \\"2023-10-19T11:00\\"), (\\"2023-10-19T11:00\\", \\"2023-10-19T12:00\\") ] print(schedule_meetings(meeting_times)) ``` # Note: Make sure to thoroughly test your implementation with edge cases, such as meetings that start and end at the same time, completely overlapping meetings, and fully disjoint meetings.","solution":"from datetime import datetime from typing import List, Tuple, Dict def schedule_meetings(meeting_times: List[Tuple[str, str]]) -> Dict[str, List[str]]: # Parse the input strings into datetime objects and store them with their original string format meetings = [(datetime.fromisoformat(start), datetime.fromisoformat(end), f\\"{start} to {end}\\") for start, end in meeting_times] # Sort the meetings by their start times meetings.sort(key=lambda x: x[0]) scheduled = [] conflicts = [] end_of_last_meeting = None for start, end, original in meetings: if end_of_last_meeting is None or start >= end_of_last_meeting: scheduled.append(original) end_of_last_meeting = end else: conflicts.append(original) return { \\"scheduled\\": scheduled, \\"conflicts\\": conflicts }"},{"question":"**Coding Assessment Question** # Objective In this assessment, you will demonstrate your ability to use the Seaborn library to create and customize strip plots. You will also generate a faceted plot to compare categories across subplots. # Problem Statement You are given a dataset containing information about tips received by waitstaff at a restaurant. Using this dataset, you need to generate and customize several plots. Task 1: Basic Strip Plot Create a basic strip plot showing the distribution of total bill amounts (`total_bill`). Task 2: Categorical Comparison Create a strip plot to compare the total bill amounts for different days (`day`). Task 3: Orientation Generate a strip plot with the orientation swapped, showing days on the x-axis and total bill amounts on the y-axis. Task 4: Hue and Palette Customization Create a strip plot to compare the total bill amounts for different days (`day`), color-coded by the sex of the customer (`sex`). Use the `deep` palette. Task 5: Faceted Plot Generate a faceted plot using `sns.catplot` to compare the total bill amounts across different days (`day`), while differentiating between lunch and dinner times (`time`). Facet the plot by the sex of the customer (`sex`). # Input and Output * **Input:** The `tips` dataframe from the Seaborn library. * **Output:** Display the generated plots using Matplotlib. # Additional Constraints * Your solution should be efficient and utilize built-in functions of the Seaborn library. * Ensure the plots are clearly labeled and aesthetically pleasing. * Use appropriate parameters to customize the appearance of the plots. # Example Code Here is a template to help you get started: ```python import seaborn as sns import matplotlib.pyplot as plt # Load the dataset tips = sns.load_dataset(\\"tips\\") # Task 1: Basic Strip Plot sns.stripplot(data=tips, x=\\"total_bill\\") plt.show() # Task 2: Categorical Comparison sns.stripplot(data=tips, x=\\"total_bill\\", y=\\"day\\") plt.show() # Task 3: Orientation sns.stripplot(data=tips, x=\\"day\\", y=\\"total_bill\\") plt.show() # Task 4: Hue and Palette Customization sns.stripplot(data=tips, x=\\"total_bill\\", y=\\"day\\", hue=\\"sex\\", palette=\\"deep\\") plt.show() # Task 5: Faceted Plot sns.catplot(data=tips, x=\\"day\\", y=\\"total_bill\\", hue=\\"time\\", col=\\"sex\\", aspect=0.5) plt.show() ``` Use the example code to understand the structure of your solution, but do not copy it directly. Instead, customize and improve the code to meet the problem requirements.","solution":"import seaborn as sns import matplotlib.pyplot as plt # Load the dataset tips = sns.load_dataset(\\"tips\\") def basic_strip_plot(): Creates and displays a basic strip plot showing the distribution of total bill amounts. plt.figure() sns.stripplot(data=tips, x=\\"total_bill\\") plt.title(\'Distribution of Total Bill Amounts\') plt.xlabel(\'Total Bill ()\') plt.show() def categorical_comparison(): Creates and displays a strip plot comparing total bill amounts for different days. plt.figure() sns.stripplot(data=tips, x=\\"total_bill\\", y=\\"day\\") plt.title(\'Total Bill Amounts by Day\') plt.xlabel(\'Total Bill ()\') plt.ylabel(\'Day\') plt.show() def orientation_swap(): Creates and displays a strip plot with orientation swapped, showing days on the x-axis and total bill amounts on the y-axis. plt.figure() sns.stripplot(data=tips, x=\\"day\\", y=\\"total_bill\\") plt.title(\'Total Bill Amounts by Day (Swapped Orientation)\') plt.xlabel(\'Day\') plt.ylabel(\'Total Bill ()\') plt.show() def hue_palette_customization(): Creates and displays a strip plot to compare total bill amounts for different days, color-coded by the sex of the customer with the deep palette. plt.figure() sns.stripplot(data=tips, x=\\"total_bill\\", y=\\"day\\", hue=\\"sex\\", palette=\\"deep\\") plt.title(\'Total Bill Amounts by Day and Customer Sex\') plt.xlabel(\'Total Bill ()\') plt.ylabel(\'Day\') plt.legend(title=\'Sex\') plt.show() def faceted_plot(): Creates and displays a faceted plot comparing total bill amounts across different days, differentiating between lunch and dinner times, and faceted by the sex of the customer. g = sns.catplot(data=tips, x=\\"day\\", y=\\"total_bill\\", hue=\\"time\\", col=\\"sex\\", aspect=0.7, kind=\\"strip\\") g.set_titles(\\"{col_name}\\") g.set_axis_labels(\\"Day\\", \\"Total Bill ()\\") g.add_legend(title=\\"Time\\") plt.subplots_adjust(top=0.85) g.fig.suptitle(\'Total Bill Amounts by Day, Time, and Customer Sex\') plt.show()"},{"question":"Objective Design a class that represents a custom sequence type that strictly adheres to the interface of a sequence as defined by `collections.abc.Sequence`. Problem Statement Implement a custom class `CustomSequence` that behaves like a typical sequence in Python. Your class should inherit from `collections.abc.Sequence` and implement all the required methods. Additionally, it should have the ability to store and retrieve elements. Your class should support the following operations: 1. Indexing to get elements. 2. Checking for the presence of an element using `in` keyword. 3. Iterating over elements using a `for` loop. 4. Getting the length of the sequence using `len()`. 5. Getting the count of an element. 6. Getting the index of the first occurrence of an element. Requirements - Complete the implementation of the `CustomSequence` class. - Directly inherit from `collections.abc.Sequence`. - Store elements internally in a list. - The constructor takes an optional iterable. Implementation ```python import collections.abc class CustomSequence(collections.abc.Sequence): def __init__(self, iterable=None): if iterable is None: self._elements = [] else: self._elements = list(iterable) def __len__(self): Return the length of the sequence. return len(self._elements) def __getitem__(self, index): Return the element at the given index. return self._elements[index] def __contains__(self, value): Return True if the sequence contains the given value. return value in self._elements def __iter__(self): Return an iterator over the sequence. return iter(self._elements) def count(self, value): Return the number of occurrences of the value. return self._elements.count(value) def index(self, value): Return the index of the first occurrence of the value. return self._elements.index(value) # Example Usage: seq = CustomSequence([1, 2, 3, 3, 4]) print(len(seq)) # 5 print(seq[2]) # 3 print(3 in seq) # True for elem in seq: print(elem) # 1 2 3 3 4 print(seq.count(3)) # 2 print(seq.index(3)) # 2 ``` # Input - An iterable to initialize the sequence (optional). # Output - Operations performed on the `CustomSequence` class should behave as expected for a sequence. # Constraints - The sequence should only contain elements that are hashable and comparable. # Notes - Ensure that your implementation strictly adheres to the behavior of a sequence as defined in `collections.abc`.","solution":"import collections.abc class CustomSequence(collections.abc.Sequence): def __init__(self, iterable=None): if iterable is None: self._elements = [] else: self._elements = list(iterable) def __len__(self): Return the length of the sequence. return len(self._elements) def __getitem__(self, index): Return the element at the given index. return self._elements[index] def __contains__(self, value): Return True if the sequence contains the given value. return value in self._elements def __iter__(self): Return an iterator over the sequence. return iter(self._elements) def count(self, value): Return the number of occurrences of the value. return self._elements.count(value) def index(self, value): Return the index of the first occurrence of the value. return self._elements.index(value)"},{"question":"Objective Your task is to implement a function and use Python\'s \\"pdb\\" module to debug the function, capturing specific details about its execution. Problem Statement You are required to create a function `find_largest(numbers)` that takes a list of integers as an input and returns the largest integer in the list. Once you have implemented the function, you will use the \\"pdb\\" module to debug it, capturing the step-by-step execution and inspecting the stack frames. Function Signature ```python def find_largest(numbers: list) -> int: pass ``` Input - A list of integers `numbers`, where `1 <= len(numbers) <= 10^5` and each integer `-10^9 <= numbers[i] <= 10^9`. Output - An integer representing the largest number in the list. Example ```python print(find_largest([3, 1, 4, 1, 5, 9, 2, 6, 5, 3, 5])) # Output: 9 print(find_largest([-10, -20, -30, -40, -50])) # Output: -10 ``` Detailed Steps 1. Implement the function `find_largest`. 2. Use the \\"pdb\\" module to trace the execution of the function. Specifically: - Set a breakpoint at the start of the function. - Step through each line of code within the function. - Print the value of the list and the current largest number during each step. - Print the stack trace each time you hit the breakpoint. 3. Document the steps and outputs captured during the debugging process. Constraints - Use the \\"pdb\\" module for debugging. - Ensure that the function handles edge cases, such as lists with all negative numbers or very large/small values correctly. Example Usage with PDB ```python import pdb def find_largest(numbers: list) -> int: pdb.set_trace() # Start the debugger largest = numbers[0] for number in numbers: if number > largest: largest = number return largest numbers = [3, 1, 4, 1, 5, 9, 2, 6, 5, 3, 5] find_largest(numbers) ``` Additional Information - Use the following \\"pdb\\" commands during your debugging session: - `b` (break): Set a breakpoint. - `s` (step): Step through the code line by line. - `p` (print): Print the value of an expression. - `w` (where): Print the stack trace. - `c` (continue): Continue execution until the next breakpoint. By the end of this task, you should be able to demonstrate how to effectively use the \\"pdb\\" debugger to trace code execution, inspect variables, and understand the flow of the program.","solution":"def find_largest(numbers: list) -> int: Finds the largest integer in the list. largest = numbers[0] for number in numbers: if number > largest: largest = number return largest"},{"question":"# Question: Optimizing Prediction Performance with Scikit-learn Background: In this task, you will be working with synthetic data to evaluate and optimize the prediction performance of different scikit-learn models. You will be required to measure prediction latency and throughput and apply optimization techniques discussed in the given documentation. The task will test your understanding of feature extraction, sparse data representation, and model complexity. Dataset: Generate a synthetic dataset with a specified number of samples and features. The dataset should allow you to test both dense and sparse data representations. Instructions: 1. **Data Preparation**: - Generate a synthetic dataset with `n_samples = 10,000` and varying `n_features = [50, 100, 500, 1000]`. - Create two versions of the dataset: - Dense: Where most of the feature values are non-zero. - Sparse: Where the sparsity ratio is greater than 90%. 2. **Model Training**: - Train at least three different models (`SGDClassifier`, `NuSVC`, and `RandomForestClassifier`) on the dataset (use the same dataset for all models). 3. **Performance Measurement**: - Measure prediction latency and throughput for each model. - Evaluate the influence of the number of features on prediction latency. - Compare the performance of dense and sparse data representations. 4. **Optimization**: - Configure scikit-learn to reduce validation overhead. - Use bulk prediction mode and compare its performance with atomic mode. - Apply model compression techniques for linear models to improve latency. Requirements: - Write your code in Python using the scikit-learn library. - Implement functions for each of the tasks: data preparation, model training, performance measurement, and optimization. - Summarize and display your results in a tabular format with appropriate labels. Notes: - Assume that the synthetic dataset is binary-class. - Use appropriate performance metrics and visualization techniques to present your findings. - Ensure your code is well-documented and follows best practices for readability and maintainability. Expected Functions: 1. `generate_dataset(n_samples, n_features, sparsity_ratio)`: Generates and returns both dense and sparse versions of a synthetic dataset. 2. `train_model(model, X_train, y_train)`: Trains the given model on the provided dataset. 3. `measure_performance(model, X_test, y_test)`: Measures and returns the prediction latency and throughput of the given model. 4. `optimize_model()`: Applies optimization techniques and measures the impact on performance. Example Output: - A summary table comparing the prediction latency and throughput of each model for different datasets (dense vs sparse) and feature counts. - Visualizations showing the effect of feature count on prediction latency. - Performance comparison between bulk and atomic prediction modes. Good luck with your implementation!","solution":"import numpy as np from scipy import sparse from sklearn.datasets import make_classification from sklearn.linear_model import SGDClassifier from sklearn.svm import NuSVC from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score import time def generate_dataset(n_samples, n_features, sparsity_ratio): Generates dense and sparse synthetic datasets. Arguments: n_samples -- int n_features -- int sparsity_ratio -- float Returns: (X_dense, X_sparse, y) -- tuple of datasets X, y = make_classification(n_samples=n_samples, n_features=n_features, n_informative=n_features//2) # Create dense dataset X_dense = X # Create sparse dataset X_sparse = sparse.random(n_samples, n_features, density=1 - sparsity_ratio, format=\'csr\') X_sparse.data = np.random.randn(X_sparse.nnz) return X_dense, X_sparse, y def train_model(model, X_train, y_train): Trains the given model. Arguments: model -- scikit-learn model X_train -- training data y_train -- training labels Returns: model -- trained model model.fit(X_train, y_train) return model def measure_performance(model, X_test, y_test): Measures the prediction latency and throughput. Arguments: model -- trained scikit-learn model X_test -- test data y_test -- test labels Returns: metrics -- dict containing latency, throughput and accuracy start = time.time() y_pred = model.predict(X_test) end = time.time() latency = (end - start) / len(y_test) throughput = len(y_test) / (end - start) accuracy = accuracy_score(y_test, y_pred) return {\'latency\': latency, \'throughput\': throughput, \'accuracy\': accuracy} def optimize_model(model): Applies optimization techniques to a linear model. Arguments: model -- scikit-learn model Returns: optimized_model -- optimized model # Example: Optimize the model by converting to a dense format (if applicable) if isinstance(model, SGDClassifier): model.dense = True return model"},{"question":"**Question: Implementing a Custom ABC with Dynamic Subclass Registration** In this task, you will create an abstract base class that manages its own registry of subclasses. When specific criteria are met, it should dynamically register a class as a virtual subclass, allowing it to be recognized as part of the abstract base class without directly inheriting it. # Requirements: 1. **Define an Abstract Base Class `CustomIterable`**: - Use the `abc.ABC` class for simplicity. - Include an abstract method `__iter__()` which must be implemented by all concrete subclasses. - Implement a class method `__subclasshook__()` that checks if any subclass has implemented the `__iter__()` method directly in the subclass. 2. **Dynamic Subclass Registration**: - Implement a class method `register_if_valid(cls, subclass)` in `CustomIterable`. - This method should check if `subclass` has an attribute `validate` that returns `True`. - If the check passes, `subclass` should be registered as a virtual subclass of `CustomIterable`. # Input and Output: - **Input**: A class definition. - **Output**: `None`. The class method `register_if_valid` will modify the class behavior without returning any value. # Constraints: - Do not use explicit inheritance from `CustomIterable`. - Ensure the subclasses can be correctly identified using `isinstance` or `issubclass` after dynamic registration. # Example: ```python from abc import ABC, abstractmethod class CustomIterable(ABC): @abstractmethod def __iter__(self): while False: yield None @classmethod def __subclasshook__(cls, subclass): if cls is CustomIterable: if any(\\"__iter__\\" in B.__dict__ for B in subclass.__mro__): return True return NotImplemented @classmethod def register_if_valid(cls, subclass): if getattr(subclass, \'validate\', False): cls.register(subclass) # Example Subclass class ValidClass: def __iter__(self): yield 1 validate = True CustomIterable.register_if_valid(ValidClass) # Test to ensure dynamic registration print(issubclass(ValidClass, CustomIterable)) # Expected Output: True print(isinstance(ValidClass(), CustomIterable)) # Expected Output: True # Invalid Subclass (should not be registered) class InvalidClass: validate = False CustomIterable.register_if_valid(InvalidClass) print(issubclass(InvalidClass, CustomIterable)) # Expected Output: False print(isinstance(InvalidClass(), CustomIterable)) # Expected Output: False ``` # Notes: - You must ensure that `ValidClass` is dynamically registered as a subclass of `CustomIterable` based on the `validate` attribute. - `InvalidClass` should not be registered as it does not meet the criteria.","solution":"from abc import ABC, abstractmethod class CustomIterable(ABC): @abstractmethod def __iter__(self): while False: yield None @classmethod def __subclasshook__(cls, subclass): if cls is CustomIterable: if any(\\"__iter__\\" in B.__dict__ for B in subclass.__mro__): return True return NotImplemented @classmethod def register_if_valid(cls, subclass): if getattr(subclass, \'validate\', False): cls.register(subclass) # Example Subclass class ValidClass: def __iter__(self): yield 1 validate = True CustomIterable.register_if_valid(ValidClass) # Invalid Subclass class InvalidClass: validate = False CustomIterable.register_if_valid(InvalidClass)"},{"question":"# Problem: Log File Compressor and Decompressor You are required to implement two functions to handle compression and decompression of log files. The first function `compress_log_file` should compress the entirety of a given log file and store it as a gzip compressed file. The second function `decompress_log_file` should read a given gzip compressed log file, decompress its content, and return it as a string. Function Definitions: 1. **compress_log_file(input_file: str, output_file: str, compresslevel: int = 9) -> None** - **Input:** - `input_file`: The path to the log file that needs to be compressed. - `output_file`: The path where the compressed file should be stored. - `compresslevel`: Integer from 0 to 9 that controls the level of compression; `1` is the fastest and produces the least compression, and `9` is the slowest and produces the most compression (default is `9`). - **Output:** - This function does not return anything. It writes the compressed content to the specified `output_file`. - **Constraints:** - The `input_file` must exist and be readable. - The `output_file` should be writable. 2. **decompress_log_file(input_file: str) -> str** - **Input:** - `input_file`: The path to the gzip compressed log file that needs to be decompressed. - **Output:** - Returns the decompressed content as a string. - **Constraints:** - The `input_file` must exist and be a valid gzip compressed file. Example: ```python # Example - Compress a log file compress_log_file(\'example.log\', \'example.log.gz\', compresslevel=5) # Example - Decompress the compressed log file log_content = decompress_log_file(\'example.log.gz\') print(log_content) ``` Additional Notes: - Use the capabilities of the `gzip` module to implement these functions. - Handle any potential exceptions that might arise, especially while reading and writing files. - Ensure the integrity of the data by checking if the file was successfully compressed and decompressed. Hints: - Refer to the `gzip` module documentation for using `gzip.open`, `gzip.compress`, and `gzip.decompress` methods. - Utilize `with` statements for handling file operations to ensure proper resource management.","solution":"import gzip def compress_log_file(input_file: str, output_file: str, compresslevel: int = 9) -> None: Compresses the content of the input_file and writes it to the output_file using gzip compression. Args: - input_file: The path to the log file that needs to be compressed. - output_file: The path where the compressed file should be stored. - compresslevel: Integer from 0 to 9 that controls the level of compression (default is 9). try: with open(input_file, \'rb\') as fin: with gzip.open(output_file, \'wb\', compresslevel=compresslevel) as fout: fout.writelines(fin) except Exception as e: raise IOError(f\\"An error occurred during compression: {e}\\") def decompress_log_file(input_file: str) -> str: Decompresses the content of the gzip compressed input_file and returns it as a string. Args: - input_file: The path to the gzip compressed log file that needs to be decompressed. Returns: - The decompressed content as a string. try: with gzip.open(input_file, \'rb\') as fin: return fin.read().decode(\'utf-8\') except Exception as e: raise IOError(f\\"An error occurred during decompression: {e}\\")"},{"question":"**Objective:** You are required to write a Python script to profile the performance of another given Python function. Your task is to use the `cProfile` and `pstats` modules to profile the provided function, save the profiling results to a file, and then analyze and manipulate the results to extract specific insights. **Provided Function:** ```python def sample_calculation(): result = 0 for i in range(1, 10000): result += i ** 2 for i in range(1, 5000): result -= i ** 2 return result ``` **Tasks:** 1. **Profile the Function**: Use the `cProfile` module to profile the `sample_calculation` function. Save the profiling results to a file named `profile_results.prof`. 2. **Load and Analyze the Profiling Results**: Use the `pstats` module to load the profiling results from `profile_results.prof`. Perform the following analyses: - Print the first 10 lines of the statistics ordered by the cumulative time. - Print all profiling statistics related to file `sample_calculator.py`. - Identify and print the functions that called the most time-consuming function. **Expected Output:** Your script should generate output that provides insights into the performance of the `sample_calculation` function, highlighting which parts of the code are the most time-consuming. **Constraints:** - Use `cProfile` and `pstats` modules as discussed. - Your profiling report should be thorough and well-structured. - The first ten lines of the cumulative time-ordered statistics should include details about the number of calls and time spent. # Submission: Submit a single Python script file (`profile_script.py`) that profiles the given function and produces the required analyses. **Example Output:** ```plaintext Top 10 lines ordered by cumulative time: ncalls tottime percall cumtime percall filename:lineno(function) 1 0.000 0.000 20.005 20.005 <ipython-input-2-cb8e3c7a3ec6>:1(sample_calculation) ... Profiling statistics related to file `sample_calculator.py`: ... Functions calling the most time-consuming function: ... ``` Note: Replace the ellipses (`...`) with the actual profiling data generated by your script.","solution":"import cProfile import pstats def sample_calculation(): result = 0 for i in range(1, 10000): result += i ** 2 for i in range(1, 5000): result -= i ** 2 return result def profile_function(function_to_profile): # Profile the function profiler = cProfile.Profile() profiler.enable() function_to_profile() profiler.disable() # Save profiling results to a file profiler.dump_stats(\'profile_results.prof\') # Analyze the profiling results stats = pstats.Stats(\'profile_results.prof\') # Print the first 10 lines ordered by cumulative time print(\\"Top 10 lines ordered by cumulative time:\\") stats.sort_stats(\'cumtime\').print_stats(10) # Print all profiling statistics related to the file containing the function print(\\"nProfiling statistics related to the file containing the function:\\") stats.print_stats(\'__main__\') # Identify and print the functions that called the most time-consuming function print(\\"nFunctions that called the most time-consuming function:\\") stats.print_callers(10) # Example of how to profile the sample_calculation function if __name__ == \'__main__\': profile_function(sample_calculation)"},{"question":"You are given a dataset that contains information about sales transactions for different products across various regions and time periods. Your task is to use pandas to implement a series of functions to reshape and analyze this dataset. Dataset Description Assume the dataset `sales_data` is in a CSV file with the following columns: - `region`: The region code where the sale took place (e.g., \'North\', \'South\'). - `product`: The product category of the sale (e.g., \'Electronics\', \'Furniture\'). - `date`: The date of the sale. - `units_sold`: The number of units sold in the transaction. - `revenue`: The revenue generated by the transaction. Tasks 1. **Pivot Table Analysis**: Implement a function `pivot_sales_data` that pivots the `sales_data` to compute the total `revenue` for each region and product, summarized by month. ```python def pivot_sales_data(file_path: str) -> pd.DataFrame: Reads the sales data from the given file path, pivots the data to show total revenue for each region and product, summarized by month. Args: - file_path: str - Path to the CSV file containing sales data. Returns: - pd.DataFrame: Pivot table showing revenue by region and product, summarized by month. pass ``` 2. **Stacking and Unstacking**: Implement a function `stack_sales_data` that stacks the pivoted data from the previous task and returns it in a format suitable for analysis. ```python def stack_sales_data(pivoted_df: pd.DataFrame) -> pd.DataFrame: Stacks the pivoted sales data. Args: - pivoted_df: pd.DataFrame - Pivot table DataFrame from the previous function. Returns: - pd.DataFrame: Stacked DataFrame. pass ``` 3. **Melting Data**: Implement a function `melt_sales_data` that melts the original `sales_data` to unpivot all but the `date`, `region`, and `product` columns into identifier variables. ```python def melt_sales_data(file_path: str) -> pd.DataFrame: Reads the sales data from the given file path and melts it into a long format. Args: - file_path: str - Path to the CSV file containing sales data. Returns: - pd.DataFrame: Melted DataFrame with `date`, `region`, and `product` as identifier variables. pass ``` 4. **Cross-tabulation**: Implement a function `crosstab_sales_data` that creates a crosstab to show the frequency of transactions across different products and regions. ```python def crosstab_sales_data(file_path: str) -> pd.DataFrame: Reads the sales data from the given file path and computes a crosstab to display the frequency of transactions across products and regions. Args: - file_path: str - Path to the CSV file containing sales data. Returns: - pd.DataFrame: Crosstab DataFrame showing frequency of transactions. pass ``` Expected Input and Output - The functions should read the dataset from a CSV file located at the given `file_path`. - The CSV file is expected to have headers corresponding to the column names specified in the dataset description. Constraints - Ensure that the functions handle missing data gracefully. - Provide clear and concise code with appropriate comments. - Optimize performance for operations on large datasets, if applicable. Example ```python # Example usage: pivoted_df = pivot_sales_data(\'path/to/sales_data.csv\') stacked_df = stack_sales_data(pivoted_df) melted_df = melt_sales_data(\'path/to/sales_data.csv\') crosstab_df = crosstab_sales_data(\'path/to/sales_data.csv\') ``` Fill in the function definitions to achieve the described transformations and analyses.","solution":"import pandas as pd def pivot_sales_data(file_path: str) -> pd.DataFrame: Reads the sales data from the given file path, pivots the data to show total revenue for each region and product, summarized by month. Args: - file_path: str - Path to the CSV file containing sales data. Returns: - pd.DataFrame: Pivot table showing revenue by region and product, summarized by month. sales_data = pd.read_csv(file_path) sales_data[\'date\'] = pd.to_datetime(sales_data[\'date\']) sales_data[\'month\'] = sales_data[\'date\'].dt.to_period(\'M\') pivot_table = sales_data.pivot_table( values=\'revenue\', index=\'month\', columns=[\'region\', \'product\'], aggfunc=\'sum\', fill_value=0 ) return pivot_table def stack_sales_data(pivoted_df: pd.DataFrame) -> pd.DataFrame: Stacks the pivoted sales data. Args: - pivoted_df: pd.DataFrame - Pivot table DataFrame from the previous function. Returns: - pd.DataFrame: Stacked DataFrame. stacked_df = pivoted_df.stack(level=[0, 1]) return stacked_df def melt_sales_data(file_path: str) -> pd.DataFrame: Reads the sales data from the given file path and melts it into a long format. Args: - file_path: str - Path to the CSV file containing sales data. Returns: - pd.DataFrame: Melted DataFrame with `date`, `region`, and `product` as identifier variables. sales_data = pd.read_csv(file_path) melted_df = pd.melt( sales_data, id_vars=[\'date\', \'region\', \'product\'], value_vars=[\'units_sold\', \'revenue\'], var_name=\'metric\', value_name=\'value\' ) return melted_df def crosstab_sales_data(file_path: str) -> pd.DataFrame: Reads the sales data from the given file path and computes a crosstab to display the frequency of transactions across products and regions. Args: - file_path: str - Path to the CSV file containing sales data. Returns: - pd.DataFrame: Crosstab DataFrame showing frequency of transactions. sales_data = pd.read_csv(file_path) crosstab_df = pd.crosstab(index=sales_data[\'product\'], columns=sales_data[\'region\']) return crosstab_df"},{"question":"Objective: Design a Python program using the `turtle` module that draws a pattern based on user input, demonstrating your understanding of turtle graphics, event handling, and screen control. Task: Write a Python function `draw_pattern()` that performs the following operations: 1. Initializes a turtle graphics screen and a turtle object. 2. Prompts the user to input the number of sides for a regular polygon (minimum 3 sides). 3. Draws the regular polygon using the turtle. 4. Asks the user if they want to fill the polygon with color. If yes, prompt for the color and fill the polygon. 5. Ensures the turtle\'s pen color, pen size, and drawing speed can be customized by the user before drawing begins. 6. Adds an event listener to close the turtle graphics window when the user clicks on it. Input Format: - Number of sides (integer, minimum value 3). - Pen color (string, valid color name). - Pen size (integer, positive number). - Drawing speed (integer from 1 [slowest] to 10 [fastest]). - Fill option (string: \\"yes\\" or \\"no\\"). - Fill color (string, valid color name, applicable if fill option is \\"yes\\"). Output Format: - A turtle graphics window displaying the regular polygon as per the user’s specifications. Constraints: - The number of sides must be an integer equal or greater than 3. - The pen size must be a positive integer. - The drawing speed must be an integer between 1 and 10. - Color names must be valid as per the turtle module. Performance Requirements: - The program should be able to render and update the graphics in a reasonably responsive manner. Example Usage: ```python def draw_pattern(): import turtle # Initialization screen = turtle.Screen() pen = turtle.Turtle() # User inputs sides = int(input(\\"Enter the number of sides for the polygon (minimum 3): \\")) while sides < 3: sides = int(input(\\"Please enter a valid number of sides (minimum 3): \\")) pen_color = input(\\"Enter the pen color: \\") pen.size = int(input(\\"Enter the pen size: \\")) speed = int(input(\\"Enter the drawing speed (1-10): \\")) pen.color(pen_color) pen.pensize(pen.size) pen.speed(speed) fill = input(\\"Do you want to fill the polygon with color? (yes/no): \\").lower() if fill == \'yes\': fill_color = input(\\"Enter the fill color: \\") pen.begin_fill() pen.fillcolor(fill_color) # Drawing the polygon for _ in range(sides): pen.forward(100) pen.left(360 / sides) if fill == \'yes\': pen.end_fill() # Event handler to close the window on click def on_close(x, y): turtle.bye() screen.onclick(on_close) screen.mainloop() ``` Notes: - Ensure to handle invalid inputs gracefully by prompting the user again. - Utilize turtle\'s event handling to close the window effectively.","solution":"def draw_pattern(): import turtle # Initialization screen = turtle.Screen() pen = turtle.Turtle() # User inputs sides = int(input(\\"Enter the number of sides for the polygon (minimum 3): \\")) while sides < 3: sides = int(input(\\"Please enter a valid number of sides (minimum 3): \\")) pen_color = input(\\"Enter the pen color: \\") pen_size = int(input(\\"Enter the pen size (positive number): \\")) speed = int(input(\\"Enter the drawing speed (1-10): \\")) pen.color(pen_color) pen.pensize(pen_size) pen.speed(speed) fill = input(\\"Do you want to fill the polygon with color? (yes/no): \\").lower() if fill == \'yes\': fill_color = input(\\"Enter the fill color: \\") pen.begin_fill() pen.fillcolor(fill_color) # Drawing the polygon for _ in range(sides): pen.forward(100) pen.left(360 / sides) if fill == \'yes\': pen.end_fill() # Event handler to close the window on click def on_close(x, y): turtle.bye() screen.onclick(on_close) screen.mainloop()"},{"question":"# Question: You are given a 3-dimensional tensor with random values. Write a function `tensor_info()` that takes a tensor as input and returns a dictionary containing: 1. The size of each dimension of the tensor. 2. The total number of elements in the tensor. 3. A boolean indicating whether the tensor is a square matrix at each of its last two dimensions. Function Signature: ```python def tensor_info(t: torch.Tensor) -> dict: pass ``` Input: - `t` (torch.Tensor): A 3-dimensional tensor of shape `(a, b, c)`. Output: - A dictionary with the following keys: - `\'size\'`: A tuple containing the size of each dimension of the tensor. - `\'total_elements\'`: An integer representing the total number of elements in the tensor. - `\'is_square\'`: A boolean indicating whether the tensor is a square matrix at each of its last two dimensions. Example: ```python import torch t = torch.rand(10, 20, 30) result = tensor_info(t) print(result) # Expected output (sizes will vary based on input tensor): # { # \'size\': (10, 20, 30), # \'total_elements\': 6000, # \'is_square\': False # } ``` Constraints: - The input tensor will always be 3-dimensional. - Performance should handle tensors up to a reasonable size (e.g., dimensions up to 1000).","solution":"import torch def tensor_info(t: torch.Tensor) -> dict: Returns information about the tensor including its size, total number of elements, and whether it is a square matrix at each of its last two dimensions. Args: t (torch.Tensor): A 3-dimensional tensor. Returns: dict: A dictionary with keys \'size\', \'total_elements\', and \'is_square\'. size = t.size() total_elements = t.numel() is_square = size[1] == size[2] return { \'size\': size, \'total_elements\': total_elements, \'is_square\': is_square }"},{"question":"# Advanced Coding Assessment - Base64 Module Question: You are tasked with implementing a helper tool that leverages the `base64` module to encode and decode strings for different encodings. This tool must support multiple functionalities with both simple and complex encoding/decoding tasks. Follow the requirements below: 1. Implement the function `multi_base_encode(data: bytes, encoding: str, altchars: bytes = None, pad: bool = False) -> bytes` which accepts: - `data`: a `bytes` object to be encoded. - `encoding`: a string indicating the encoding type. It can be one of `\\"base64\\"`, `\\"base64url\\"`, `\\"base32\\"`, `\\"base16\\"`, `\\"ascii85\\"`, or `\\"base85\\"`. - `altchars`: an optional `bytes` object of length 2 used for specifying an alternative base64 alphabet for `base64` and `base64url` encodings. If not provided, default values are used. - `pad`: an optional boolean applicable for `base85` encoding to specify if padding should be applied. The default is `False`. The function should return the encoded data as `bytes`. 2. Implement the function `multi_base_decode(data: bytes, encoding: str, altchars: bytes = None, validate: bool = False, ignorechars: bytes = b\' tnrx0b\') -> bytes` which accepts: - `data`: a `bytes` object representing the data to be decoded. - `encoding`: a string indicating the encoding type. It can be one of `\\"base64\\"`, `\\"base64url\\"`, `\\"base32\\"`, `\\"base16\\"`, `\\"ascii85\\"`, or `\\"base85\\"`. - `altchars`: an optional `bytes` object of length 2 used for specifying the alternative base64 alphabet for `base64` and `base64url` encodings. If not provided, default values are used. - `validate`: an optional boolean to enable strict validation for base64 data. The default is `False`. - `ignorechars`: an optional `bytes` object of ASCII characters to be ignored during `ascii85` decoding. The default includes spaces and control characters. The function should return the decoded data as `bytes`. Constraints: 1. You must handle edge cases, such as invalid encoding types or incorrectly padded inputs, by raising appropriate exceptions with informative error messages. 2. Your implementation should efficiently handle input sizes up to a few MBs without performance bottlenecks. 3. `altchars` should only be considered for relevant encodings (`base64`, `base64url`). Example Usage: ```python data = b\\"Hello, World!\\" # Base64 encoding encoded_base64 = multi_base_encode(data, encoding=\\"base64\\") print(encoded_base64) # Output should be base64 encoded bytes # Base64 decoding decoded_base64 = multi_base_decode(encoded_base64, encoding=\\"base64\\") print(decoded_base64) # Output should be b\\"Hello, World!\\" # Base64url encoding encoded_base64url = multi_base_encode(data, encoding=\\"base64url\\") print(encoded_base64url) # Output should be base64url encoded bytes # Base64url decoding decoded_base64url = multi_base_decode(encoded_base64url, encoding=\\"base64url\\") print(decoded_base64url) # Output should be b\\"Hello, World!\\" ``` Implement these two functions in Python using the standard `base64` module functionalities as specified.","solution":"import base64 def multi_base_encode(data: bytes, encoding: str, altchars: bytes = None, pad: bool = False) -> bytes: if encoding == \\"base64\\": if altchars: return base64.b64encode(data, altchars) return base64.b64encode(data) elif encoding == \\"base64url\\": if altchars: return base64.urlsafe_b64encode(data).translate(bytes.maketrans(b\'+/\', altchars)) return base64.urlsafe_b64encode(data) elif encoding == \\"base32\\": return base64.b32encode(data) elif encoding == \\"base16\\": return base64.b16encode(data) elif encoding == \\"ascii85\\": return base64.a85encode(data) elif encoding == \\"base85\\": return base64.b85encode(data, pad = pad) else: raise ValueError(f\\"Unsupported encoding type: {encoding}\\") def multi_base_decode(data: bytes, encoding: str, altchars: bytes = None, validate: bool = False, ignorechars: bytes = b\' tnrx0b\') -> bytes: if encoding == \\"base64\\": if altchars: return base64.b64decode(data, altchars = altchars, validate = validate) return base64.b64decode(data, validate = validate) elif encoding == \\"base64url\\": if altchars: return base64.urlsafe_b64decode(data.translate(bytes.maketrans(altchars, b\'+/\'))) return base64.urlsafe_b64decode(data) elif encoding == \\"base32\\": return base64.b32decode(data) elif encoding == \\"base16\\": return base64.b16decode(data) elif encoding == \\"ascii85\\": return base64.a85decode(data, ignorechars = ignorechars) elif encoding == \\"base85\\": return base64.b85decode(data) else: raise ValueError(f\\"Unsupported encoding type: {encoding}\\")"},{"question":"# Advanced Python Sorting Challenge Your task is to implement a function that sorts a list of dictionaries representing tasks. Each task dictionary has the following keys: - `name`: A string representing the task name. - `priority`: An integer where a lower number indicates higher priority. - `duration`: An integer indicating the duration of the task in minutes. - `deadline`: A string representing the deadline in the format \\"YYYY-MM-DD\\". You need to sort the tasks based on the following criteria: 1. Primary: Sort by `deadline` in ascending order. 2. Secondary: For tasks with the same deadline, sort by `priority` in ascending order. 3. Tertiary: For tasks with the same priority and deadline, sort by `duration` in descending order. Implement the function `sort_tasks(tasks)` to perform this sorting. Input - `tasks`: A list of dictionaries, where each dictionary represents a task with the keys `name`, `priority`, `duration`, and `deadline`. Output - A list of dictionaries sorted based on the criteria mentioned. Constraints - The `tasks` list will contain at most 1000 task dictionaries. - The `deadline` string is guaranteed to be in the correct \\"YYYY-MM-DD\\" format. - The `priority` and `duration` values will be non-negative integers. Performance Requirement - The sorting operation should be efficient for the given constraints. Example ```python tasks = [ {\\"name\\": \\"Task A\\", \\"priority\\": 1, \\"duration\\": 60, \\"deadline\\": \\"2023-11-20\\"}, {\\"name\\": \\"Task B\\", \\"priority\\": 2, \\"duration\\": 30, \\"deadline\\": \\"2023-11-20\\"}, {\\"name\\": \\"Task C\\", \\"priority\\": 1, \\"duration\\": 45, \\"deadline\\": \\"2023-11-19\\"}, {\\"name\\": \\"Task D\\", \\"priority\\": 3, \\"duration\\": 90, \\"deadline\\": \\"2023-11-18\\"} ] sorted_tasks = sort_tasks(tasks) # Output: # [ # {\\"name\\": \\"Task D\\", \\"priority\\": 3, \\"duration\\": 90, \\"deadline\\": \\"2023-11-18\\"}, # {\\"name\\": \\"Task C\\", \\"priority\\": 1, \\"duration\\": 45, \\"deadline\\": \\"2023-11-19\\"}, # {\\"name\\": \\"Task A\\", \\"priority\\": 1, \\"duration\\": 60, \\"deadline\\": \\"2023-11-20\\"}, # {\\"name\\": \\"Task B\\", \\"priority\\": 2, \\"duration\\": 30, \\"deadline\\": \\"2023-11-20\\"} # ] ``` Hint Use the `sorted()` function with a `key` parameter that can handle the multiple sorting criteria using the `operator` module or a custom lambda function.","solution":"from datetime import datetime def sort_tasks(tasks): Sorts a list of task dictionaries based on the specified criteria. Criteria: 1. Primary: Sort by `deadline` in ascending order. 2. Secondary: For tasks with the same deadline, sort by `priority` in ascending order. 3. Tertiary: For tasks with the same priority and deadline, sort by `duration` in descending order. :param tasks: List of task dictionaries :return: Sorted list of task dictionaries return sorted(tasks, key=lambda x: (datetime.strptime(x[\'deadline\'], \\"%Y-%m-%d\\"), x[\'priority\'], -x[\'duration\']))"},{"question":"# Bytes Manipulation and Information Extraction in Python In this task, you are required to create and manipulate bytes objects using provided functions and extract specific information from them. Task 1: Create Bytes Objects 1. Implement a function `create_bytes_from_string(s: str) -> bytes` that takes a string `s` and returns a bytes object. 2. Implement a function `create_bytes_from_string_and_size(s: str, size: int) -> bytes` that takes a string `s` and an integer `size` and returns a bytes object with a copy of the string truncated or padded to the specified size. Task 2: Perform Type Checks 1. Implement a function `is_exact_bytes(o: object) -> bool` that takes an object `o` and returns `True` if the object is strictly a bytes object, `False` otherwise. 2. Implement a function `is_bytes(o: object) -> bool` that takes an object `o` and returns `True` if the object is a bytes object or an instance of a subtype of the bytes type, `False` otherwise. Task 3: Concatenate and Resize Bytes Objects 1. Implement a function `concatenate_bytes(b1: bytes, b2: bytes) -> bytes` that takes two bytes objects `b1` and `b2` and returns a new bytes object concatenated from them. 2. Implement a function `resize_bytes(b: bytes, new_size: int) -> bytes` that takes a bytes object `b` and resizes it to `new_size`. If resizing is not possible, the function should raise a `ValueError`. Task 4: Extract Information from Bytes Objects 1. Implement a function `get_bytes_size(b: bytes) -> int` that takes a bytes object `b` and returns its size. 2. Implement a function `get_bytes_contents(b: bytes) -> str` that takes a bytes object `b` and returns its content as a string. # Example Usage ```python # Task 1 print(create_bytes_from_string(\\"hello\\")) # Expected output: b\'hello\' print(create_bytes_from_string_and_size(\\"hello\\", 8)) # Expected output: b\'hellox00x00x00\' # Task 2 print(is_exact_bytes(b\'hello\')) # Expected output: True print(is_bytes(b\'hello\')) # Expected output: True print(is_exact_bytes(bytearray(b\'hello\'))) # Expected output: False # Task 3 print(concatenate_bytes(b\'hello\', b\'world\')) # Expected output: b\'helloworld\' print(resize_bytes(b\'hello\', 10)) # Expected output: b\'hellox00x00x00x00x00\' # Task 4 print(get_bytes_size(b\'hello\')) # Expected output: 5 print(get_bytes_contents(b\'hello\')) # Expected output: \'hello\' ``` # Constraints - The input strings for Task 1 functions will not exceed 100 characters. - The new size for the resizing function in Task 3 will not be larger than twice the original size of the bytes object. # Notes - If a function is supposed to create a bytes object and the input is invalid (e.g., a non-string for creating bytes from a string), it should raise a `TypeError`. - The resizing function should only be used to resize newly created bytes objects.","solution":"def create_bytes_from_string(s: str) -> bytes: Create a bytes object from a string. if not isinstance(s, str): raise TypeError(\\"Input should be a string\\") return s.encode() def create_bytes_from_string_and_size(s: str, size: int) -> bytes: Create a bytes object of a specified size from a string, truncating or padding as necessary. if not isinstance(s, str) or not isinstance(size, int): raise TypeError(\\"Input should be a string and an integer\\") byte_str = s.encode() if len(byte_str) > size: return byte_str[:size] else: return byte_str.ljust(size, b\'x00\') def is_exact_bytes(o: object) -> bool: Check if an object is exactly a bytes object. return isinstance(o, bytes) def is_bytes(o: object) -> bool: Check if an object is a bytes object or an instance of a subtype of the bytes type. return isinstance(o, (bytes,)) def concatenate_bytes(b1: bytes, b2: bytes) -> bytes: Concatenate two bytes objects. return b1 + b2 def resize_bytes(b: bytes, new_size: int) -> bytes: Resize a bytes object to a new size by truncating or padding it. if not isinstance(b, bytes) or not isinstance(new_size, int): raise TypeError(\\"Input should be a bytes object and an integer\\") if len(b) > new_size: return b[:new_size] else: return b.ljust(new_size, b\'x00\') def get_bytes_size(b: bytes) -> int: Get the size of a bytes object. return len(b) def get_bytes_contents(b: bytes) -> str: Get the contents of a bytes object as a string. return b.decode()"},{"question":"# Python `gzip` Module Coding Assessment **Objective**: Your task is to implement a function utilizing the python `gzip` module to perform file compression and decompression. This exercise aims to test your understanding of file I/O, compression, and error handling. **Function Specifications**: 1. **Function Name**: `compress_and_decompress_file` 2. **Input**: - A string `input_file_path` representing the path to the input file (uncompressed text file). - A string `output_file_path` representing the path to the output file (compressed file). - A string `decompressed_file_path` representing the path to the decompressed file (to store the decompressed output). - An integer `compression_level` indicating the level of compression (must be between 0 and 9). 3. **Output**: - None (the function should write to the given file paths). 4. **Behavior**: - The function should compress the contents of `input_file_path` and write the result to `output_file_path` using the specified `compression_level`. - The function should then decompress the contents of `output_file_path` and write the result to `decompressed_file_path`. - Ensure to handle any exceptions that may arise due to invalid gzip files or read/write errors, and log appropriate error messages. 5. **Constraints**: - Do not use any external libraries other than `gzip` and `io`. **Example Usage**: ```python def compress_and_decompress_file(input_file_path, output_file_path, decompressed_file_path, compression_level): import gzip import shutil # Validate compression level if not (0 <= compression_level <= 9): raise ValueError(\\"Compression level must be between 0 and 9.\\") try: # Compress the input file with open(input_file_path, \'rb\') as f_in: with gzip.open(output_file_path, \'wb\', compresslevel=compression_level) as f_out: shutil.copyfileobj(f_in, f_out) # Decompress the output file with gzip.open(output_file_path, \'rb\') as f_in: with open(decompressed_file_path, \'wb\') as f_out: shutil.copyfileobj(f_in, f_out) print(f\\"File {input_file_path} compressed to {output_file_path} and decompressed to {decompressed_file_path}.\\") except (gzip.BadGzipFile, OSError) as e: print(f\\"An error occurred: {e}\\") # Example call compress_and_decompress_file(\'example.txt\', \'example.txt.gz\', \'example_decompressed.txt\', 5) ``` In this function: - The input file `example.txt` is compressed to `example.txt.gz`. - The file `example.txt.gz` is then decompressed to `example_decompressed.txt`. - The compression level is validated and set according to the given parameter. **Requirements**: - Use the `gzip` module functionalities effectively. - Include error handling for potential issues with file operations. **Assessment Criteria**: - Correct implementation of compression and decompression. - Proper handling and validation of input parameters. - Effective error handling and logging.","solution":"def compress_and_decompress_file(input_file_path, output_file_path, decompressed_file_path, compression_level): import gzip import shutil # Validate compression level if not (0 <= compression_level <= 9): raise ValueError(\\"Compression level must be between 0 and 9.\\") try: # Compress the input file with open(input_file_path, \'rb\') as f_in: with gzip.open(output_file_path, \'wb\', compresslevel=compression_level) as f_out: shutil.copyfileobj(f_in, f_out) # Decompress the output file with gzip.open(output_file_path, \'rb\') as f_in: with open(decompressed_file_path, \'wb\') as f_out: shutil.copyfileobj(f_in, f_out) print(f\\"File {input_file_path} compressed to {output_file_path} and decompressed to {decompressed_file_path}.\\") except (gzip.BadGzipFile, OSError) as e: print(f\\"An error occurred: {e}\\")"},{"question":"# Question: Build a Command-Line Tool using `argparse` You are tasked with creating a command-line tool that processes a list of files and performs various operations on them, such as counting lines, words, or characters. The tool should be able to handle different options and arguments and provide appropriate help messages. Here are the detailed requirements: Requirements: 1. **Create an ArgumentParser**: - The program should have a description: \\"Tool for processing text files.\\" 2. **Add Positional Argument**: - `files`: A list of filenames to be processed. This should be a required positional argument that can accept one or more values. 3. **Add Optional Arguments**: - `--lines` or `-l`: If this flag is set, the program should count the number of lines in each file. - `--words` or `-w`: If this flag is set, the program should count the number of words in each file. - `--chars` or `-c`: If this flag is set, the program should count the number of characters in each file. 4. **Mutual Exclusion**: - Ensure that only one of `--lines`, `--words`, or `--chars` can be set at a time using mutually exclusive groups. 5. **Default Behavior**: - If none of the optional flags are set, the program should print the filename(s) without performing any counting. 6. **Error Handling**: - Handle errors gracefully if the specified files do not exist or cannot be read. 7. **Help Messages**: - Generate user-friendly help messages that describe how to use the tool and provide examples. Implementation: 1. **Define a function `process_file(file, option)`**: - This function should take a filename and an option (`\\"lines\\"`, `\\"words\\"`, or `\\"chars\\"`) and return the count based on the option provided. 2. **Define a function `main()`**: - Use `argparse` to handle command-line parsing and process the files based on the provided arguments and options. Input: ```bash python tool.py [OPTIONS] FILE [FILE ...] ``` Output: - Counts for lines, words, or characters as specified, or filenames if no counting option is selected. Examples: 1. Count lines in files: ```bash python tool.py --lines file1.txt file2.txt ``` Sample Output: ``` file1.txt: 10 lines file2.txt: 20 lines ``` 2. Count words in files: ```bash python tool.py --words file1.txt ``` Sample Output: ``` file1.txt: 100 words ``` 3. Invalid usage (multiple options): ```bash python tool.py --words --lines file1.txt ``` Sample Output: ``` usage: tool.py [-h] [--lines | --words | --chars] FILE [FILE ...] tool.py: error: argument --words: not allowed with argument --lines ``` Your Implementation Here: ```python import argparse import os def process_file(file, option): try: with open(file, \'r\') as f: if option == \'lines\': return file, len(f.readlines()), \'lines\' elif option == \'words\': return file, len(f.read().split()), \'words\' elif option == \'chars\': return file, len(f.read()), \'chars\' except Exception as e: return file, e, \'error\' def main(): parser = argparse.ArgumentParser(description=\'Tool for processing text files.\') parser.add_argument(\'files\', metavar=\'FILE\', nargs=\'+\', help=\'Files to be processed\') group = parser.add_mutually_exclusive_group() group.add_argument(\'--lines\', \'-l\', action=\'store_true\', help=\'Count lines\') group.add_argument(\'--words\', \'-w\', action=\'store_true\', help=\'Count words\') group.add_argument(\'--chars\', \'-c\', action=\'store_true\', help=\'Count characters\') args = parser.parse_args() option = None if args.lines: option = \'lines\' elif args.words: option = \'words\' elif args.chars: option = \'chars\' for file in args.files: result = process_file(file, option) if result[2] == \'error\': print(f\\"Could not process {result[0]}: {result[1]}\\") else: print(f\\"{result[0]}: {result[1]} {result[2]}\\") if __name__ == \'__main__\': main() ```","solution":"import argparse import os def process_file(file, option): try: with open(file, \'r\') as f: if option == \'lines\': return file, len(f.readlines()), \'lines\' elif option == \'words\': return file, len(f.read().split()), \'words\' elif option == \'chars\': return file, len(f.read()), \'chars\' except Exception as e: return file, str(e), \'error\' def main(): parser = argparse.ArgumentParser(description=\'Tool for processing text files.\') parser.add_argument(\'files\', metavar=\'FILE\', nargs=\'+\', help=\'Files to be processed\') group = parser.add_mutually_exclusive_group() group.add_argument(\'--lines\', \'-l\', action=\'store_true\', help=\'Count lines\') group.add_argument(\'--words\', \'-w\', action=\'store_true\', help=\'Count words\') group.add_argument(\'--chars\', \'-c\', action=\'store_true\', help=\'Count characters\') args = parser.parse_args() option = None if args.lines: option = \'lines\' elif args.words: option = \'words\' elif args.chars: option = \'chars\' for file in args.files: result = process_file(file, option) if result[2] == \'error\': print(f\\"Could not process {result[0]}: {result[1]}\\") else: print(f\\"{result[0]}: {result[1]} {result[2]}\\") if __name__ == \'__main__\': main()"},{"question":"Problem Statement: You need to design a custom PyTorch operator named `custom_relu` that performs a ReLU (Rectified Linear Unit) activation function on a given tensor but with an additional constraint: it scales the output by a factor that can be set dynamically during the execution. Your task is to: 1. Define a custom operator named `custom_relu` using PyTorch\'s `torch.library` module. 2. Implement a kernel for this custom operator that: - Takes an input tensor and a scaling factor. - Applies the ReLU function on the input tensor. - Scales the result by the given factor. 3. Register the kernel so that it can handle autograd (i.e., can compute gradients). 4. Write a test script that: - Verifies the forward pass of `custom_relu`. - Performs gradient checks using `torch.autograd.gradcheck`. Implementation Details: 1. **Function Signature**: ```python def custom_relu(input_tensor: torch.Tensor, scale_factor: float) -> torch.Tensor: # Implementation here ``` 2. **Input**: - `input_tensor (torch.Tensor)`: A tensor of any size containing the input data. - `scale_factor (float)`: A scaling factor to multiply with the ReLU output. 3. **Output**: - `output_tensor (torch.Tensor)`: A tensor containing the scaled ReLU output. 4. **Constraints**: - Implement the custom operator using `torch.library.custom_op` and other relevant APIs. - Ensure that the custom operator supports autograd for computing gradients. 5. **Performance Requirements**: - Efficiently handle tensors of any size. - Ensure the implementation is robust and handles edge cases (e.g., tensors with negative and positive values). Example: ```python import torch # Example input tensor input_tensor = torch.tensor([-1.0, 2.0, -3.0, 4.0], requires_grad=True) scale_factor = 2.0 # Expected output tensor after custom ReLU and scaling expected_output = torch.tensor([0.0, 4.0, 0.0, 8.0]) # Define and register custom_relu operator # (Insert your implementation based on the task description above) output_tensor = custom_relu(input_tensor, scale_factor) # Check the output assert torch.allclose(output_tensor, expected_output), \\"custom_relu forward pass failed!\\" # Gradient check torch.autograd.gradcheck(custom_relu, (input_tensor, scale_factor)) ``` In the above example, the `custom_relu` should correctly apply the ReLU function followed by scaling and should pass the gradient check for autograd support.","solution":"import torch from torch import nn from torch.autograd import Function from torch.autograd import gradcheck class CustomReLU(Function): @staticmethod def forward(ctx, input_tensor, scale_factor): ctx.save_for_backward(input_tensor) ctx.scale_factor = scale_factor relu_output = torch.relu(input_tensor) return relu_output * scale_factor @staticmethod def backward(ctx, grad_output): input_tensor, = ctx.saved_tensors scale_factor = ctx.scale_factor grad_input = grad_output.clone() grad_input[input_tensor < 0] = 0 return grad_input * scale_factor, None def custom_relu(input_tensor, scale_factor): return CustomReLU.apply(input_tensor, scale_factor)"},{"question":"# Objective: Implement a function to generate a comprehensive dictionary containing details about the current Python runtime environment and underlying operating system. Use the `platform` module to obtain these details. # Description: Write a function `get_environment_info()` that returns a dictionary containing the following keys and their respective values: - `\'architecture\'`: Tuple containing the bit architecture and linkage format of the current Python interpreter executable. - `\'machine\'`: The machine type. - `\'node\'`: The computer’s network name. - `\'platform\'`: A single string identifying the underlying platform. - `\'processor\'`: The (real) processor name. - `\'python_build\'`: Tuple containing the Python build number and date. - `\'python_compiler\'`: The compiler used for compiling Python. - `\'python_implementation\'`: The Python implementation. - `\'python_version\'`: The Python version in \'major.minor.patchlevel\' format. - `\'release\'`: The system’s release version. - `\'system\'`: The system/OS name. - `\'uname\'`: Named tuple containing system, node, release, version, machine, and processor. Make sure each key in the dictionary has a meaningful value. If any function returns an empty string or any value cannot be determined, use a default placeholder `\'<unknown>\'`. # Requirements: - Import necessary functions from the `platform` module. - Handle exceptions where applicable, ensuring that all keys in the dictionary are populated with appropriate values. # Constraints: - Assume the environment where the function runs has the necessary permissions to query system-level information. # Example: ```python def get_environment_info(): # implement the function here ``` Running `get_environment_info()` might yield results similar to: ```python { \'architecture\': (\'64bit\', \'ELF\'), \'machine\': \'x86_64\', \'node\': \'hostname\', \'platform\': \'Linux-5.11.0-27-generic-x86_64-with-glibc2.29\', \'processor\': \'x86_64\', \'python_build\': (\'default\', \'Jul 24 2020 14:43:09\'), \'python_compiler\': \'GCC 9.3.0\', \'python_implementation\': \'CPython\', \'python_version\': \'3.8.5\', \'release\': \'5.11.0-27-generic\', \'system\': \'Linux\', \'uname\': uname_result(system=\'Linux\', node=\'hostname\', release=\'5.11.0-27-generic\', version=\'#29~20.04.1-Ubuntu SMP Fri Jul 9 22:49:44 UTC 2021\', machine=\'x86_64\', processor=\'x86_64\') } ```","solution":"import platform def get_environment_info(): Returns a dictionary containing details about the current Python runtime environment and underlying operating system using the platform module. try: uname_info = platform.uname() except Exception: uname_info = (\'<unknown>\', \'<unknown>\', \'<unknown>\', \'<unknown>\', \'<unknown>\', \'<unknown>\') return { \'architecture\': platform.architecture() if platform.architecture() else (\'<unknown>\', \'<unknown>\'), \'machine\': platform.machine() or \'<unknown>\', \'node\': platform.node() or \'<unknown>\', \'platform\': platform.platform() or \'<unknown>\', \'processor\': platform.processor() or \'<unknown>\', \'python_build\': platform.python_build() if platform.python_build() else (\'<unknown>\', \'<unknown>\'), \'python_compiler\': platform.python_compiler() or \'<unknown>\', \'python_implementation\': platform.python_implementation() or \'<unknown>\', \'python_version\': platform.python_version() or \'<unknown>\', \'release\': platform.release() or \'<unknown>\', \'system\': platform.system() or \'<unknown>\', \'uname\': uname_info }"},{"question":"# Advanced Python Coding Assessment **Title: Implementing Robust File Handling with Python Development Mode** **Objective:** Assess your understanding of Python Development Mode and your ability to write code that follows best practices in resource management, particularly in file handling. **Question:** Python Development Mode introduces several runtime checks and enhanced warnings to assist in identifying issues in your code related to resource management, among other things. One common area where such issues often arise is in file handling. **Task:** 1. Write a Python function `count_lines_in_file(filepath: str) -> int` that takes the path to a text file and returns the count of lines in that file. 2. Ensure that your implementation: - Properly handles resource management by explicitly closing the file. - Avoids common pitfalls like forgetting to close the file or manually managing file descriptors. **Constraints:** - The file specified by `filepath` is guaranteed to exist and be readable. - Use Python Development Mode to verify your implementation during development. - You should not use any external libraries. **Performance Requirements:** - The function should efficiently handle files with up to 100,000 lines. **Example Usage:** ```python def count_lines_in_file(filepath: str) -> int: # Your implementation here # Assuming \'test.txt\' contains 10 lines of text. print(count_lines_in_file(\'test.txt\')) # Output: 10 ``` **Additional Notes:** 1. Run your script with `python3 -X dev script.py` during development to ensure that there are no `ResourceWarning` or other relevant warnings/errors. 2. Provide comments in your code explaining how it mitigates common file handling issues. **Deliverables:** - A Python script implementing the `count_lines_in_file` function. - Verification of the function\'s correct behavior using Python Development Mode as part of the script\'s execution.","solution":"def count_lines_in_file(filepath: str) -> int: Count the number of lines in the given file. Parameters: filepath (str): Path to the text file. Returns: int: Number of lines in the file. line_count = 0 # Open the file using \'with\' to ensure it is properly closed after reading with open(filepath, \'r\') as file: for line in file: line_count += 1 return line_count"},{"question":"# Python Module Simulation In this coding assessment, you will create a Python class that simulates some of the Python C API functionality for managing Python modules. You need to write a class `PyModule` that will have methods to create, manipulate, and retrieve module data similar to the C API methods described in the provided documentation. Class Requirements 1. **Class Name**: `PyModule` 2. **Initialization**: - The constructor should take a single argument `name` which sets the module\'s `__name__` attribute. The other module attributes `__doc__`, `__package__`, and `__loader__` should be initialized to `None`. 3. **Methods**: - `add_object(self, name: str, value: object) -> None`: Adds an object to the module\'s namespace. - `get_dict(self) -> dict`: Returns the module\'s dictionary. - `get_name(self) -> str`: Returns the module\'s name. - `add_functions(self, functions: list) -> None`: Adds functions to the module\'s namespace. Each function should be a tuple containing the function name as a string and the function itself (a callable). - `add_int_constant(self, name: str, value: int) -> None`: Adds an integer constant to the module. - `add_string_constant(self, name: str, value: str) -> None`: Adds a string constant to the module. # Example Usage ```python def example_function(): return \\"Hello, World!\\" module = PyModule(name=\\"example\\") # Add an object module.add_object(\\"example_obj\\", example_function) # Add functions module.add_functions([(\\"example_func\\", example_function)]) # Add constants module.add_int_constant(\\"ANSWER\\", 42) module.add_string_constant(\\"GREETING\\", \\"Hello\\") print(module.get_name()) # Output: example print(module.get_dict()) # Output: # { # \\"__name__\\": \\"example\\", # \\"__doc__\\": None, # \\"__package__\\": None, # \\"__loader__\\": None, # \\"example_obj\\": <function example_function at 0x...>, # \\"example_func\\": <function example_function at 0x...>, # \\"ANSWER\\": 42, # \\"GREETING\\": \\"Hello\\" # } ``` # Constraints - The module\'s name should always be a string. - The namespace dictionary should be implemented privately within the class and should not be directly modified outside of the class methods. - Ensure that the module name, integers, and string constants adhere to Python’s naming and type conventions. Submission Submit a single Python file containing the `PyModule` class implementation along with any supporting code to demonstrate its usage (as shown in the example usage).","solution":"class PyModule: def __init__(self, name: str): self.__name__ = name self.__doc__ = None self.__package__ = None self.__loader__ = None self.__dict = { \\"__name__\\": self.__name__, \\"__doc__\\": self.__doc__, \\"__package__\\": self.__package__, \\"__loader__\\": self.__loader__, } def add_object(self, name: str, value: object) -> None: self.__dict[name] = value def get_dict(self) -> dict: return self.__dict def get_name(self) -> str: return self.__name__ def add_functions(self, functions: list) -> None: for func_name, func in functions: if callable(func): self.__dict[func_name] = func else: raise ValueError(f\\"{func_name} is not callable\\") def add_int_constant(self, name: str, value: int) -> None: if isinstance(value, int): self.__dict[name] = value else: raise ValueError(f\\"{name} is not an integer\\") def add_string_constant(self, name: str, value: str) -> None: if isinstance(value, str): self.__dict[name] = value else: raise ValueError(f\\"{name} is not a string\\")"},{"question":"# Advanced Coding Assessment Question: Implementing Custom Object Types in Python **Objective:** Your task is to create a custom object type in Python that mimics some of the functionality described for C extension modules in the provided documentation. You will define a new type of object that supports basic arithmetic operations and attribute access. **Requirements:** 1. **Class Definition:** - Define a class `CustomNumber` which represents a numeric value. - The class should store a single integer or floating-point number as its value. 2. **Initialization:** - The class should be initialized with a single numeric value. 3. **Attribute Access:** - Implement a method `set_value` to update the stored value. - Implement a method `get_value` to retrieve the stored value. 4. **Arithmetic Operations:** - Support addition (`+`), subtraction (`-`), multiplication (`*`), and division (`/`) with other `CustomNumber` instances as well as with regular numbers. - Ensure these operations return new `CustomNumber` instances. 5. **String Representation:** - Implement a `__str__` method to provide a human-readable representation of the object. 6. **Error Handling:** - Raise appropriate exceptions if invalid operations are performed (e.g., division by zero). # Example Usage: ```python num1 = CustomNumber(10) num2 = CustomNumber(5) # Accessing attributes print(num1.get_value()) # Output: 10 # Setting a new value num1.set_value(20) # Arithmetic operations num3 = num1 + num2 print(num3.get_value()) # Output: 25 num4 = num1 / 2 print(num4.get_value()) # Output: 10.0 # String representation print(str(num4)) # Output: CustomNumber(value=10.0) ``` # Constraints: - You may assume that the input values for arithmetic operations will be valid numbers (integers or floats). - Focus on correctness and organization of your code. # Performance: - The operations should perform efficiently for basic arithmetic operations and attribute accesses. - Consider edge cases such as division by zero. Implement these requirements by defining the `CustomNumber` class with the functionalities described.","solution":"class CustomNumber: def __init__(self, value): if not isinstance(value, (int, float)): raise ValueError(\\"Value must be an integer or float\\") self.value = value def set_value(self, value): if not isinstance(value, (int, float)): raise ValueError(\\"Value must be an integer or float\\") self.value = value def get_value(self): return self.value def __add__(self, other): if isinstance(other, CustomNumber): return CustomNumber(self.value + other.value) elif isinstance(other, (int, float)): return CustomNumber(self.value + other) else: raise TypeError(\\"Unsupported operand type(s) for +\\") def __sub__(self, other): if isinstance(other, CustomNumber): return CustomNumber(self.value - other.value) elif isinstance(other, (int, float)): return CustomNumber(self.value - other) else: raise TypeError(\\"Unsupported operand type(s) for -\\") def __mul__(self, other): if isinstance(other, CustomNumber): return CustomNumber(self.value * other.value) elif isinstance(other, (int, float)): return CustomNumber(self.value * other) else: raise TypeError(\\"Unsupported operand type(s) for *\\") def __truediv__(self, other): if isinstance(other, CustomNumber): if other.value == 0: raise ZeroDivisionError(\\"division by zero\\") return CustomNumber(self.value / other.value) elif isinstance(other, (int, float)): if other == 0: raise ZeroDivisionError(\\"division by zero\\") return CustomNumber(self.value / other) else: raise TypeError(\\"Unsupported operand type(s) for /\\") def __str__(self): return f\\"CustomNumber(value={self.value})\\""},{"question":"# Python Coding Assessment **Objective:** Demonstrate your understanding of handling specific exceptions provided by the `asyncio` library in Python. You are required to write a function that performs an asynchronous read operation and handles different exceptions as specified. Task 1. Implement a function `async def robust_async_read(reader, n_bytes: int)` that attempts to read `n_bytes` of data from an asynchronous stream reader. 2. The function should handle the following exceptions: - `asyncio.TimeoutError` - `asyncio.CancelledError` - `asyncio.IncompleteReadError` - `asyncio.LimitOverrunError` 3. Depending on the exception raised, the function should return a specific message or raise another exception: - If a `asyncio.TimeoutError` occurs, return the string `\\"TimeoutError: Operation exceeded deadline.\\"` - If a `asyncio.CancelledError` occurs, return the string `\\"CancelledError: Operation was cancelled.\\"` - If a `asyncio.IncompleteReadError` occurs, return the string `\\"IncompleteReadError: {expected} bytes expected, {partial} bytes read.\\"` where `expected` and `partial` are replaced by the respective values from the exception. - If a `asyncio.LimitOverrunError` occurs, raise an `asyncio.LimitOverrunError` with the message `\\"Buffer size limit reached with {consumed} bytes consumed.\\"` Function Signature ```python import asyncio async def robust_async_read(reader: asyncio.StreamReader, n_bytes: int) -> str: pass ``` Input - `reader`: an instance of `asyncio.StreamReader`. - `n_bytes`: an integer representing the number of bytes to read. Output - A string message corresponding to the specific error handled or a raised custom exception if `asyncio.LimitOverrunError` occurs. Constraints - Assume `reader` is a valid `asyncio.StreamReader` object for the asynchronous stream. - `n_bytes` is a positive integer. Example ```python import asyncio async def example_usage(): reader = asyncio.StreamReader() # Assume this is correctly initialized and filled with data # Example read usage: try: result = await robust_async_read(reader, 1024) print(result) except asyncio.LimitOverrunError as e: print(e) # To run the example you need to start an asyncio event loop. ``` Notes - You are not required to create the `asyncio.StreamReader` object or simulate asynchronous stream data. The focus is purely on implementing the `robust_async_read` function and handling exceptions. - Ensure you use proper exception handling to correctly capture and process the mentioned exceptions.","solution":"import asyncio async def robust_async_read(reader: asyncio.StreamReader, n_bytes: int) -> str: try: return await reader.read(n_bytes) except asyncio.TimeoutError: return \\"TimeoutError: Operation exceeded deadline.\\" except asyncio.CancelledError: return \\"CancelledError: Operation was cancelled.\\" except asyncio.IncompleteReadError as e: return f\\"IncompleteReadError: {e.expected} bytes expected, {e.partial} bytes read.\\" except asyncio.LimitOverrunError as e: raise asyncio.LimitOverrunError(f\\"Buffer size limit reached with {e.consumed} bytes consumed.\\", e.consumed)"},{"question":"# Question: Text Classification with Multinomial Naive Bayes You are given a dataset containing text documents and their corresponding labels. Your task is to implement a text classification model using the `MultinomialNB` classifier from the scikit-learn library. You need to perform the following steps: 1. **Data Preprocessing**: - Convert the text documents into numerical feature vectors using a technique such as TF-IDF (Term Frequency-Inverse Document Frequency). 2. **Model Training and Evaluation**: - Split the dataset into training and testing sets. - Train a `MultinomialNB` classifier on the training set. - Evaluate the classifier on the testing set and report the classification accuracy. 3. **Hyperparameter Tuning**: - Experiment with different values of the smoothing parameter `alpha` in the `MultinomialNB` classifier. - Select and report the best value of `alpha` based on the classification accuracy on the testing set. # Input and Output Formats Input - A list of text documents. - A list of corresponding labels for the text documents. - An optional argument to specify the portion of data to be used as the testing set (default: 0.2, meaning 20% of the data). Output - The classification accuracy of the model on the testing set. - The best value of the smoothing parameter `alpha`. # Constraints - You should use the `MultinomialNB` classifier from the `sklearn.naive_bayes` module. - You should use the `TfidfVectorizer` from the `sklearn.feature_extraction.text` module for text preprocessing. - You must follow the constraints on training and testing data splits as specified by the input argument. # Example ```python from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.model_selection import train_test_split from sklearn.naive_bayes import MultinomialNB from sklearn.metrics import accuracy_score def text_classification(documents, labels, test_size=0.2): # Step 1: Data Preprocessing vectorizer = TfidfVectorizer() X = vectorizer.fit_transform(documents) # Step 2: Model Training and Evaluation X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=test_size, random_state=42) # Step 3: Hyperparameter Tuning best_alpha = None best_accuracy = 0 for alpha in [0.1, 0.5, 1.0, 1.5, 2.0]: clf = MultinomialNB(alpha=alpha) clf.fit(X_train, y_train) y_pred = clf.predict(X_test) accuracy = accuracy_score(y_test, y_pred) if accuracy > best_accuracy: best_accuracy = accuracy best_alpha = alpha return best_accuracy, best_alpha # Example Usage documents = [\\"This is a positive text\\", \\"This is a negative text\\", \\"...\\"] # List of text documents labels = [1, 0, ...] # Corresponding labels accuracy, best_alpha = text_classification(documents, labels) print(f\\"Best Accuracy: {accuracy}\\") print(f\\"Best alpha: {best_alpha}\\") ``` Complete the implementation of the `text_classification` function as described above.","solution":"from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.model_selection import train_test_split from sklearn.naive_bayes import MultinomialNB from sklearn.metrics import accuracy_score def text_classification(documents, labels, test_size=0.2): Perform text classification using Multinomial Naive Bayes with TF-IDF features. Args: - documents (list of str): List of text documents. - labels (list of int): List of corresponding labels for the text documents. - test_size (float): Portion of data to be used as the testing set. Default is 0.2. Returns: - best_accuracy (float): Best classification accuracy on the testing set. - best_alpha (float): Best value of the smoothing parameter alpha. # Step 1: Data Preprocessing vectorizer = TfidfVectorizer() X = vectorizer.fit_transform(documents) # Step 2: Model Training and Evaluation X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=test_size, random_state=42) # Step 3: Hyperparameter Tuning best_alpha = None best_accuracy = 0 for alpha in [0.1, 0.5, 1.0, 1.5, 2.0]: clf = MultinomialNB(alpha=alpha) clf.fit(X_train, y_train) y_pred = clf.predict(X_test) accuracy = accuracy_score(y_test, y_pred) if accuracy > best_accuracy: best_accuracy = accuracy best_alpha = alpha return best_accuracy, best_alpha"},{"question":"**Problem Statement:** You are required to define a custom PyTorch `Function` to implement an element-wise custom activation function called `CustomReLU` which works similarly to the ReLU function but with a cubic transformation applied to the inputs. Specifically, the function behaves as: [ CustomReLU(x) = begin{cases} 0 & text{if } x leq 0 x^3 & text{if } x > 0 end{cases} ] In addition, you need to verify the gradients computed by your custom function using `torch.autograd.gradcheck`. **Instructions:** 1. Define a class `CustomReLU` that inherits from `torch.autograd.Function`. 2. Implement the `forward` and `backward` static methods in the `CustomReLU` class. - In the `forward` method, apply the cubic transformation to the positive elements of the input tensor while keeping the negative elements as zero. - In the `backward` method, compute the gradient of the loss with respect to the inputs. 3. Create a test case to ensure the forward pass is working correctly. 4. Verify the implementation of your custom function by using `torch.autograd.gradcheck` to ensure the gradients are correct. **Requirements:** - The input tensor will be a 1-D tensor with `requires_grad=True`. - Implement proper gradient calculation for the custom activation function in the `backward` method. **Input:** - A 1-D input tensor `x` with gradient computation enabled. **Output:** - The output tensor after applying the custom ReLU activation function, along with successful gradient verification using `torch.autograd.gradcheck`. **Example:** ```python import torch from torch.autograd import Function class CustomReLU(Function): @staticmethod def forward(ctx, input): ctx.save_for_backward(input) output = input.clone() output[input <= 0] = 0 output[input > 0] = input[input > 0] ** 3 return output @staticmethod def backward(ctx, grad_output): input, = ctx.saved_tensors grad_input = grad_output.clone() grad_input[input <= 0] = 0 grad_input[input > 0] = 3 * input[input > 0] ** 2 return grad_input # Test the forward method x = torch.tensor([-1.0, 0.0, 2.0, -3.0, 4.0], requires_grad=True) custom_relu = CustomReLU.apply y = custom_relu(x) print(y) # Output should be tensor([ 0., 0., 8., 0., 64.], grad_fn=<CustomReLUBackward>) # Gradient checking input_grad_check = torch.randn(5, dtype=torch.double, requires_grad=True) gradcheck_result = torch.autograd.gradcheck(custom_relu, (input_grad_check,)) print(gradcheck_result) # Output should be True, indicating gradients are correct ``` Note: You should make sure your implementation passes the gradient check.","solution":"import torch from torch.autograd import Function class CustomReLU(Function): @staticmethod def forward(ctx, input): ctx.save_for_backward(input) output = input.clone() output[input <= 0] = 0 output[input > 0] = input[input > 0] ** 3 return output @staticmethod def backward(ctx, grad_output): input, = ctx.saved_tensors grad_input = grad_output.clone() grad_input[input <= 0] = 0 grad_input[input > 0] = grad_output[input > 0] * 3 * input[input > 0] ** 2 return grad_input # Custom activation function custom_relu = CustomReLU.apply"},{"question":"Objective: To assess your understanding of handling environment variables using the `os` module in Python, which provides a superset of the `posix` interface. Problem Statement: Write a Python function `update_environment` that: - Takes a dictionary of environment variable names and values as input. - Updates the current environment with these values using the `os` module. - After updating, executes a simple shell command (e.g., `printenv`) using `os.system()` to print all environment variables to verify the update. Function Signature: ```python def update_environment(env_vars: dict) -> int: pass ``` Input: - `env_vars` (dict): A dictionary where keys are environment variable names (strings) and values are the corresponding environment variable values (strings). Output: - (int): The function should return the exit status of the `os.system()` command. A return value of `0` typically means success in Unix-like systems. Constraints: 1. It is guaranteed that `env_vars` will only contain strings as keys and values. 2. Assume this function will be executed in a Unix-like environment. Example Usage: ```python env_variables = {\'TEST_VAR\': \'123\', \'HOME\': \'/custom/home\'} assert update_environment(env_variables) == 0 ``` This function should: - Update the environment such that `os.environ` reflects the changes in `env_variables`. - Use `os.system(\\"printenv\\")` to print all the environment variables ensuring the updates are applied. You should handle potential edge cases, such as when updating environment variables fails. Note: - Do not use the `posix` module directly as it may lead to discrepancies as discussed in the description.","solution":"import os def update_environment(env_vars: dict) -> int: Updates the environment with the provided environment variables and executes `printenv` command. Parameters: env_vars (dict): Dictionary of environment variable names and values. Returns: int: Exit status of the `printenv` command. for key, value in env_vars.items(): os.environ[key] = value return os.system(\\"printenv\\")"},{"question":"# Question: Implementing and Evaluating SVMs for Multi-class Classification You are provided with a multi-class dataset and are required to implement a Support Vector Machine (SVM) classifier using `sklearn.svm.SVC`. Your task is to preprocess the data, train an SVM classifier, tune its hyperparameters, and evaluate its performance. Additionally, you should visualize the decision boundaries for a better understanding of how the classifier separates different classes. Dataset The dataset `iris` can be loaded using `sklearn.datasets.load_iris`. Requirements: 1. **Data Loading and Preprocessing**: - Load the `iris` dataset. - Split the data into training and testing sets using an 80-20 split. - Standardize the features by removing the mean and scaling to unit variance. 2. **Model Training and Hyperparameter Tuning**: - Train an `SVC` model on the training data with an RBF kernel. - Tune the hyperparameters `C` and `gamma` using grid search with 5-fold cross-validation. Use the following parameter grid: ```python param_grid = { \'C\': [0.1, 1, 10, 100], \'gamma\': [1, 0.1, 0.01, 0.001] } ``` 3. **Model Evaluation**: - Evaluate the best model on the test set and report the accuracy. - Print the classification report for precision, recall, and F1-score per class. 4. **Visualization**: - Visualize the decision boundaries of the trained SVM classifier using a 2D plot (use only the first two features of the dataset for visualization purposes). Implementation Details: - **Input**: None - **Output**: Print the evaluation results and plot the decision boundaries. Constraints: - Use `StandardScaler` from `sklearn.preprocessing` for standardization. - Use `GridSearchCV` from `sklearn.model_selection` for hyperparameter tuning. - Use `classification_report` from `sklearn.metrics` to print the evaluation report. - Use `matplotlib.pyplot` for plotting. Example: ```python import numpy as np import matplotlib.pyplot as plt from sklearn import datasets from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.preprocessing import StandardScaler from sklearn.svm import SVC from sklearn.metrics import classification_report # Load dataset iris = datasets.load_iris() X = iris.data[:, :2] # Use only the first two features for visualization y = iris.target # Split the data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Standardize the features scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) # Train and tune SVM param_grid = {\'C\': [0.1, 1, 10, 100], \'gamma\': [1, 0.1, 0.01, 0.001]} svc = SVC(kernel=\'rbf\') grid_search = GridSearchCV(svc, param_grid, cv=5) grid_search.fit(X_train, y_train) # Evaluate the model best_svc = grid_search.best_estimator_ y_pred = best_svc.predict(X_test) print(classification_report(y_test, y_pred)) # Plot decision boundaries def plot_decision_boundaries(X, y, model, title=\\"Decision Boundaries\\"): h = .02 x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1 y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1 xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h)) Z = model.predict(np.c_[xx.ravel(), yy.ravel()]) Z = Z.reshape(xx.shape) plt.contourf(xx, yy, Z, alpha=0.8) plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors=\'k\', marker=\'o\', s=20) plt.title(title) plt.xlabel(\'Feature 1\') plt.ylabel(\'Feature 2\') plt.show() plot_decision_boundaries(X_train, y_train, best_svc, title=\\"SVM Decision Boundaries (Training Set)\\") plot_decision_boundaries(X_test, y_test, best_svc, title=\\"SVM Decision Boundaries (Test Set)\\") ``` Submission: Submit a single Python script file named `svm_classification.py` containing the complete implementation as described above.","solution":"import numpy as np import matplotlib.pyplot as plt from sklearn import datasets from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.preprocessing import StandardScaler from sklearn.svm import SVC from sklearn.metrics import classification_report # Load dataset iris = datasets.load_iris() X = iris.data[:, :2] # Use only the first two features for visualization y = iris.target # Split the data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Standardize the features scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) # Train and tune SVM param_grid = {\'C\': [0.1, 1, 10, 100], \'gamma\': [1, 0.1, 0.01, 0.001]} svc = SVC(kernel=\'rbf\') grid_search = GridSearchCV(svc, param_grid, cv=5) grid_search.fit(X_train, y_train) # Evaluate the model best_svc = grid_search.best_estimator_ y_pred = best_svc.predict(X_test) print(classification_report(y_test, y_pred)) # Plot decision boundaries def plot_decision_boundaries(X, y, model, title=\\"Decision Boundaries\\"): h = .02 x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1 y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1 xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h)) Z = model.predict(np.c_[xx.ravel(), yy.ravel()]) Z = Z.reshape(xx.shape) plt.contourf(xx, yy, Z, alpha=0.8) plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors=\'k\', marker=\'o\', s=20) plt.title(title) plt.xlabel(\'Feature 1\') plt.ylabel(\'Feature 2\') plt.show() plot_decision_boundaries(X_train, y_train, best_svc, title=\\"SVM Decision Boundaries (Training Set)\\") plot_decision_boundaries(X_test, y_test, best_svc, title=\\"SVM Decision Boundaries (Test Set)\\")"},{"question":"# **Color Transformation and Image Manipulation using `colorsys`** **Problem Statement** You are tasked with manipulating the colors of an image using the `colorsys` module. Specifically, you will transform the image from one color space to another and perform a specific manipulation in the target color space before converting it back to RGB. **Function Signatures** ```python def transform_image(image: List[List[Tuple[float, float, float]]]) -> List[List[Tuple[float, float, float]]]: Transforms the image by converting each pixel\'s RGB values to HSV, performs a manipulation to increase the saturation by a fixed amount, and converts it back to RGB. Parameters: image (List[List[Tuple[float, float, float]]]): A 2D list representing an image, where each tuple contains RGB values (r, g, b) as floating-point values in [0, 1]. Returns: List[List[Tuple[float, float, float]]]: A 2D list representing the transformed image with manipulated RGB values. pass ``` **Input Format** - `image`: A 2D list where `image[i][j]` represents the RGB values of the pixel at position `(i, j)` as a tuple `(r, g, b)` with each component in the range [0, 1]. **Output Format** - A 2D list where each element is an RGB tuple representing the transformed image with manipulated color values. **Task Requirements** 1. Convert each pixel\'s RGB values to HSV. 2. Increase the saturation value `S` of each pixel by 20%, ensuring it remains in the range [0, 1]. 3. Convert the modified HSV values back to RGB. 4. Return the transformed image. **Example** ```python image = [ [(0.2, 0.4, 0.4), (0.6, 0.8, 0.9)], [(0.1, 0.7, 0.8), (0.3, 0.5, 0.2)] ] transformed_image = transform_image(image) print(transformed_image) # Output should be a 2D list of RGB values with increased saturation. ``` **Constraints** 1. The input image will have at most 100x100 pixels. 2. Each pixel\'s RGB values are within the valid range of [0, 1]. **Notes** - Use the functions provided by the `colorsys` module for color space conversions. - Ensure that the increased saturation value is clamped within the range [0, 1]. Good luck!","solution":"import colorsys from typing import List, Tuple def transform_image(image: List[List[Tuple[float, float, float]]]) -> List[List[Tuple[float, float, float]]]: Transforms the image by converting each pixel\'s RGB values to HSV, performs a manipulation to increase the saturation by a fixed amount, and converts it back to RGB. Parameters: image (List[List[Tuple[float, float, float]]]): A 2D list representing an image, where each tuple contains RGB values (r, g, b) as floating-point values in [0, 1]. Returns: List[List[Tuple[float, float, float]]]: A 2D list representing the transformed image with manipulated RGB values. transformed_image = [] for row in image: transformed_row = [] for pixel in row: r, g, b = pixel h, s, v = colorsys.rgb_to_hsv(r, g, b) s = min(s * 1.2, 1.0) # Increase saturation by 20% and clamp to [0, 1] r, g, b = colorsys.hsv_to_rgb(h, s, v) transformed_row.append((r, g, b)) transformed_image.append(transformed_row) return transformed_image"},{"question":"# PyTorch JIT Compilation and Model Optimization In this assessment, you will demonstrate your understanding of PyTorch\'s Just-In-Time (JIT) compilation to script and optimize a neural network model. **Task: Implement a Function to Script and Verify Model** You need to implement a function `script_and_optimize_model` that takes a PyTorch neural network model and a sample input, scripts (compiles) the model using `torch.jit.script`, and verifies that the scripted model produces the same outputs as the original model. **Function Signature:** ```python def script_and_optimize_model(model: torch.nn.Module, sample_input: torch.Tensor) -> bool: Scripts and optimizes the given PyTorch model using torch.jit.script and verifies that the output of the scripted model matches the output of the original model. Args: model (torch.nn.Module): The PyTorch model to be scripted and optimized. sample_input (torch.Tensor): A sample input tensor to test the models. Returns: bool: True if the outputs of the original and scripted models are the same, False otherwise. ``` **Input:** - `model`: An instance of `torch.nn.Module`, representing a PyTorch model. - `sample_input`: An instance of `torch.Tensor`, representing a sample input to test the models. **Output:** - Returns a boolean value: - `True` if the output of the scripted model matches the output of the original model. - `False` otherwise. **Constraints:** - You may assume the model and sample_input are both valid. - You should compare the outputs using an appropriate method or tolerance level to account for numerical precision differences. **Example:** ```python import torch import torch.nn as nn class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.linear = nn.Linear(10, 1) def forward(self, x): return self.linear(x) # Instantiate the model and create a sample input model = SimpleModel() sample_input = torch.randn(5, 10) # Call the function is_optimized_correctly = script_and_optimize_model(model, sample_input) print(is_optimized_correctly) # Should print True if everything is implemented correctly ``` **Notes:** - You may use `torch.jit.script` for scripting the model. - Ensure to compare the output tensors using `torch.allclose` or a similar method to handle potential numerical precision differences. - Handle any potential issues that may arise due to the JIT compilation process.","solution":"import torch def script_and_optimize_model(model: torch.nn.Module, sample_input: torch.Tensor) -> bool: Scripts and optimizes the given PyTorch model using torch.jit.script and verifies that the output of the scripted model matches the output of the original model. Args: model (torch.nn.Module): The PyTorch model to be scripted and optimized. sample_input (torch.Tensor): A sample input tensor to test the models. Returns: bool: True if the outputs of the original and scripted models are the same, False otherwise. # Set the model to evaluation mode to ensure consistency in outputs model.eval() # Generate the original output with torch.no_grad(): original_output = model(sample_input) # Script the model using torch.jit.script scripted_model = torch.jit.script(model) # Generate the scripted model output with torch.no_grad(): scripted_output = scripted_model(sample_input) # Compare the original and scripted model outputs return torch.allclose(original_output, scripted_output, rtol=1e-05, atol=1e-08)"},{"question":"# Coding Assessment: Advanced Usage of Python Enums Objective To assess your understanding of Python\'s `enum` module, you are required to implement an enumeration that includes some advanced usage like custom values, method definitions, automatic values, and ensuring uniqueness. Task 1. **Define an enumeration class `VehicleType`** that represents different types of vehicles. Use the `Enum` class from the enum module. Each vehicle type should have a custom string value. The vehicle types are `CAR`, `TRUCK`, `MOTORCYCLE`, and `BICYCLE` with values \\"Car\\", \\"Truck\\", \\"Motorcycle\\", and \\"Bicycle\\" respectively. 2. **Define an enumeration class `Status`** to describe the status of a vehicle. The status types should use the `auto` feature to automatically generate values. The status types are `AVAILABLE`, `RENTED`, `IN_SERVICE`, and `UNAVAILABLE`. 3. **Ensure unique values** in the `Status` enumeration using the appropriate decorator. 4. **Define a method within the `VehicleType` enumeration** to return a description message for each vehicle type in the format: \\"The vehicle type is {vehicle_name}\\". 5. **Define an enumeration class `RentalRates`** with unique rental rates for each vehicle type as per the following rates: - `CAR`: 50 - `TRUCK`: 80 - `MOTORCYCLE`: 35 - `BICYCLE`: 15 6. **Implement a function `calculate_rental_cost(vehicle_type, status, days)`**: - **Inputs**: - `vehicle_type` (an instance of `VehicleType`): The type of vehicle to be rented. - `status` (an instance of `Status`): The current status of the vehicle. - `days` (int): The number of days the vehicle is rented. - **Output**: - If the status of the vehicle is not `AVAILABLE`, raise a `ValueError` with the message \\"Vehicle is not available for rent\\". - If the status is `AVAILABLE`, compute the rental cost by multiplying the number of days with the rental rate of the given vehicle type. Return the rental cost. Constraints - Ensure that the function calculates the cost accurately and raises errors appropriately. - You can assume valid inputs for `vehicle_type` and `days`. Example Usage ```python vehicle_type = VehicleType.CAR status = Status.AVAILABLE days = 5 try: cost = calculate_rental_cost(vehicle_type, status, days) print(f\'The cost of renting the {vehicle_type.value} for {days} days is: {cost}\') except ValueError as e: print(e) ``` Notes - Use the `unique` decorator from the `enum` module to ensure unique status values. - Implement the necessary imports and handle any potential exceptions as described. - Make sure to follow Python conventions and best practices.","solution":"from enum import Enum, auto, unique class VehicleType(Enum): CAR = \\"Car\\" TRUCK = \\"Truck\\" MOTORCYCLE = \\"Motorcycle\\" BICYCLE = \\"Bicycle\\" def description(self): return f\\"The vehicle type is {self.value}\\" @unique class Status(Enum): AVAILABLE = auto() RENTED = auto() IN_SERVICE = auto() UNAVAILABLE = auto() @unique class RentalRates(Enum): CAR = 50 TRUCK = 80 MOTORCYCLE = 35 BICYCLE = 15 def calculate_rental_cost(vehicle_type, status, days): if status != Status.AVAILABLE: raise ValueError(\\"Vehicle is not available for rent\\") rate = RentalRates[vehicle_type.name].value return rate * days"},{"question":"# Task: Implement a Custom Autograd Function and Module in PyTorch Problem Statement You are required to implement a custom autograd function and a neural network module that uses this function. The custom operation you will implement is the \\"Scaled Tanh\\" operation that scales the hyperbolic tangent of the inputs. Specifically, you need to: 1. Implement a custom autograd function `ScaledTanhFunction` that computes: - Forward pass: ( text{output} = alpha cdot tanh(text{input}) ) - Backward pass: Compute the gradient of the forward pass with respect to the input. 2. Implement an `nn.Module` called `ScaledTanh` that integrates the `ScaledTanhFunction`. Detailed Requirements 1. **Custom Autograd Function** - Create a class `ScaledTanhFunction` that inherits from `torch.autograd.Function`. - Implement the `forward` method that computes the scaled tanh. - Implement the `setup_context` method to save any tensors required for the backward pass. - Implement the `backward` method that computes the gradients with respect to the input. 2. **Custom Module** - Create a class `ScaledTanh` that inherits from `torch.nn.Module`. - Initialize the module with a scaling parameter `alpha`. - Implement the `forward` method using `ScaledTanhFunction`. Implementation Details 1. **Custom Function (`ScaledTanhFunction`):** - `forward(ctx, input, alpha)`: Computes the forward pass and returns the scaled tanh. - `setup_context(ctx, inputs, output)`: Saves the input and alpha for the backward pass. - `backward(ctx, grad_output)`: Computes the gradient of the output with respect to the input. 2. **Custom Module (`ScaledTanh`):** - `__init__(self, alpha)`: Initializes the module with the scaling parameter `alpha`. - `forward(self, input)`: Applies the `ScaledTanhFunction` to the input. Example ```python import torch import torch.nn as nn class ScaledTanhFunction(torch.autograd.Function): @staticmethod def forward(ctx, input, alpha): ctx.save_for_backward(input) ctx.alpha = alpha return alpha * input.tanh() @staticmethod def setup_context(ctx, inputs, output): input, alpha = inputs ctx.save_for_backward(input) ctx.alpha = alpha @staticmethod def backward(ctx, grad_output): input, = ctx.saved_tensors grad_input = grad_alpha = None if ctx.needs_input_grad[0]: grad_input = grad_output * (ctx.alpha * (1 - input.tanh() ** 2)) return grad_input, None # No gradient for alpha class ScaledTanh(nn.Module): def __init__(self, alpha): super(ScaledTanh, self).__init__() self.alpha = alpha def forward(self, input): return ScaledTanhFunction.apply(input, self.alpha) # Quick test input = torch.tensor([0.1, 0.2, 0.3], requires_grad=True) alpha = 2.0 scaled_tanh = ScaledTanh(alpha) output = scaled_tanh(input) output.sum().backward() print(\\"Input: \\", input) print(\\"Output: \\", output) print(\\"Gradients: \\", input.grad) ``` Constraints - The operation must support batched input tensors. - Ensure that the backward method is efficient and does not modify input tensors in-place. - Use the `gradcheck` function to verify the gradients if needed. Good luck!","solution":"import torch import torch.nn as nn class ScaledTanhFunction(torch.autograd.Function): @staticmethod def forward(ctx, input, alpha): ctx.save_for_backward(input) ctx.alpha = alpha return alpha * input.tanh() @staticmethod def backward(ctx, grad_output): input, = ctx.saved_tensors alpha = ctx.alpha grad_input = grad_output * alpha * (1 - input.tanh() ** 2) return grad_input, None class ScaledTanh(nn.Module): def __init__(self, alpha): super(ScaledTanh, self).__init__() self.alpha = alpha def forward(self, input): return ScaledTanhFunction.apply(input, self.alpha)"},{"question":"**Question: Comprehensive Scatter Plot and Relational Visualization using Seaborn** You are provided with the \\"tips\\" dataset, which contains information about the tip amount in relation to the total bill, as well as other details such as the day of the week, time of day, and size of the party. Your task is to create a comprehensive scatter plot and a set of relational visualizations using seaborn to analyze the data. # Requirements: 1. **Scatter Plot with Multiple Semantic Mappings:** - Create a scatter plot showing the relationship between `total_bill` and `tip`. - Use `hue` to differentiate between the days of the week (`day` column). - Use `style` to differentiate between the time of day (`time` column). - Use `size` to represent the size of the party (`size` column). - Control the size range with the minimum size being 20 and maximum size being 200. 2. **Relational Visualization with Facets:** - Use `relplot` to create a scatter plot of `total_bill` vs. `tip`. - The plot should be divided into separate columns for each time of day (`time`). - Differentiate the points by `hue` based on the day of the week (`day`). - Include a unique marker style for each day of the week. # Input: - The input data will be provided by loading the seaborn \'tips\' dataset. # Output: - Two plots should be generated: 1. A comprehensive scatter plot with multiple semantic mappings. 2. A multi-faceted relational plot. # Constraints: - Use seaborn for all visualizations. - Ensure all legends are clearly visible for interpretation. # Example Code Structure: ```python import seaborn as sns import matplotlib.pyplot as plt # Load the dataset tips = sns.load_dataset(\\"tips\\") # Scatter plot with multiple semantic mappings plt.figure(figsize=(10, 6)) sns.scatterplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"day\\", style=\\"time\\", size=\\"size\\", sizes=(20, 200)) plt.title(\\"Scatter Plot: Total Bill vs Tip with Multiple Semantic Mappings\\") plt.show() # Relational visualization with facets sns.relplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", col=\\"time\\", hue=\\"day\\", style=\\"day\\", kind=\\"scatter\\") plt.suptitle(\\"Relational Plot: Total Bill vs Tip Faceted by Time with Day Differentiation\\", y=1.02) plt.show() ``` Your implementation should follow the structure provided and produce the required visualizations as described.","solution":"import seaborn as sns import matplotlib.pyplot as plt # Load the dataset tips = sns.load_dataset(\\"tips\\") def create_scatter_plot(): Creates a scatter plot showing the relationship between total_bill and tip, with hue differentiating days of the week, style differentiating time of day, and size representing the size of the party. The size range is controlled between 20 and 200. plt.figure(figsize=(10, 6)) sns.scatterplot( data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"day\\", style=\\"time\\", size=\\"size\\", sizes=(20, 200) ) plt.title(\\"Scatter Plot: Total Bill vs Tip with Multiple Semantic Mappings\\") plt.show() def create_relational_plot(): Creates a relational plot showing the relationship between total_bill and tip, with separate columns for each time of day, and hue differentiating the days of the week. Each day also has a unique marker style. sns.relplot( data=tips, x=\\"total_bill\\", y=\\"tip\\", col=\\"time\\", hue=\\"day\\", style=\\"day\\", kind=\\"scatter\\" ) plt.suptitle(\\"Relational Plot: Total Bill vs Tip Faceted by Time with Day Differentiation\\", y=1.02) plt.show()"},{"question":"# FTP Client Automation Challenge In this challenge, you are required to implement a function that automates certain FTP client operations using Python\'s `ftplib` module. Your function should be capable of connecting to an FTP server, navigating directories, downloading files, and returning certain contents of the downloaded file. Task Write a function `ftp_download_and_parse` that: 1. Connects to an FTP server using the provided hostname, username, and password. 2. Navigates to a specified directory. 3. Downloads a specified file from the directory. 4. Parses the file to find and return lines that match a given `search_text`. Function Signature ```python def ftp_download_and_parse(hostname: str, username: str, password: str, directory: str, filename: str, search_text: str) -> list: pass ``` Input - `hostname` (str): The hostname of the FTP server. - `username` (str): The username for logging into the FTP server. - `password` (str): The password for logging into the FTP server. - `directory` (str): The directory to navigate to on the FTP server. - `filename` (str): The name of the file to download. - `search_text` (str): The text to search for within the downloaded file. Output - (list): A list of lines from the downloaded file that contain the `search_text`. Constraints - You can assume the FTP server is reachable and the provided credentials are accurate. - Handle any exceptions gracefully, ensuring the connection is properly closed in case of errors. Example ```python # Example Usage: result = ftp_download_and_parse( hostname=\'ftp.us.debian.org\', username=\'anonymous\', password=\'anonymous@\', directory=\'debian\', filename=\'README\', search_text=\'debian\' ) print(result) # Expected output: List of lines containing \'debian\' ``` Notes - Make sure to close the connection properly after the operations. - You may use the example provided in the documentation as a reference for the steps. - Pay attention to possible exceptions and handle them appropriately, ensuring resources are cleaned up. Evaluation Criteria: - Correctness: The function should perform all specified operations accurately. - Robustness: The function should handle various edge cases and exceptions gracefully. - Code Quality: The code should be clean, well-structured, and properly commented.","solution":"import ftplib import io def ftp_download_and_parse(hostname: str, username: str, password: str, directory: str, filename: str, search_text: str) -> list: Connects to an FTP server, navigates to a directory, downloads a file, and returns lines that contain a specified search text. lines_with_search_text = [] try: # Connect to the FTP server ftp = ftplib.FTP(hostname) ftp.login(user=username, passwd=password) # Change to the specified directory ftp.cwd(directory) # Download the specified file with io.BytesIO() as file_stream: ftp.retrbinary(f\'RETR {filename}\', file_stream.write) file_stream.seek(0) file_content = file_stream.read().decode(\'utf-8\') # Parse the file and find lines containing the search text for line in file_content.splitlines(): if search_text in line: lines_with_search_text.append(line) # Close the FTP connection ftp.quit() except ftplib.all_errors as e: print(f\\"FTP error: {e}\\") if \'ftp\' in locals(): ftp.quit() return lines_with_search_text"},{"question":"# Email Policy Customization and Handling Objective: You are required to demonstrate your understanding and ability to utilize the `email.policy` module to create custom behaviors for parsing and serializing email messages. Problem Statement: You are given a text file containing an email message. Your task is to: 1. Read the email message from the file. 2. Define a custom policy that: - Limits the maximum length of headers to `50` characters. - Uses `\\"rn\\"` as the line separator. - Ensures that any defects encountered during parsing raise exceptions. 3. Parse the email message using this custom policy. 4. Serialize the email message back to a byte array using another policy that: - Refolds any long headers. - Retain the original folding for headers shorter than `50` characters. Input: - An email message in a text file (`email_message.txt`). Output: - A byte array containing the serialized email message. Constraints: - You must handle and raise any defects encountered during parsing. - Ensure that line lengths in the serialized output comply with the RFC standards as configured in the custom policies. Instructions: 1. Define and use custom policies as described. 2. Implement the functionality to read, parse, and serialize the email using the specified policies. 3. Ensure that the byte array output is correctly formatted according to your policies. Sample Code Structure: ```python from email import message_from_binary_file, policy from email.generator import BytesGenerator def main(): # Step 1: Read the email message from a file with open(\'email_message.txt\', \'rb\') as f: email_message_bytes = f.read() # Step 2: Define a custom policy for parsing custom_policy_parsing = policy.EmailPolicy( max_line_length=50, linesep=\'rn\', raise_on_defect=True ) # Step 3: Parse the email message using the custom policy email_message = message_from_binary_file(email_message_bytes, policy=custom_policy_parsing) # Step 4: Define another custom policy for serialization custom_policy_serializing = custom_policy_parsing.clone( refold_source=\'long\' ) # Step 5: Serialize the email message back to a byte array output = BytesIO() generator = BytesGenerator(output, policy=custom_policy_serializing) generator.flatten(email_message) # Return the byte array return output.getvalue() if __name__ == \\"__main__\\": result = main() print(result) ``` This problem will test your understanding of the `email.policy` module, especially how to customize policies for parsing and serializing email messages, and how to handle defects according to the policy specifications.","solution":"from email import message_from_binary_file, policy from email.generator import BytesGenerator from io import BytesIO def read_parse_serialize_email(file_path): Function to read an email message from a file, parse it with a custom policy, and then serialize it back to a byte array with another custom policy. # Step 1: Read the email message from a file with open(file_path, \'rb\') as f: email_message_bytes = f.read() # Step 2: Define a custom policy for parsing custom_policy_parsing = policy.EmailPolicy( max_line_length=50, linesep=\'rn\', raise_on_defect=True ) # Step 3: Parse the email message using the custom policy email_message = message_from_binary_file(BytesIO(email_message_bytes), policy=custom_policy_parsing) # Step 4: Define another custom policy for serialization custom_policy_serializing = custom_policy_parsing.clone( refold_source=\'long\' ) # Step 5: Serialize the email message back to a byte array output = BytesIO() generator = BytesGenerator(output, policy=custom_policy_serializing) generator.flatten(email_message) # Return the byte array return output.getvalue()"},{"question":"Coding Assessment Question # Objective: Demonstrate comprehension of Python\'s `datetime` library, specifically the `datetime`, `date`, `time`, `timedelta`, `tzinfo`, and `timezone` classes, including their methods and attributes. # Problem Statement: You have been assigned to create a program that calculates the upcoming birthday given a person\'s birth date. The program should also handle time zone differences and calculate the exact age of the person when their next birthday occurs. # Requirements: 1. Implement a function `upcoming_birthday(birth_date: str, current_date: str, time_zone: str) -> dict` that calculates the upcoming birthday. 2. Handle any time zone differences appropriately. 3. Calculate the exact age (in years, months, and days) the person will be on their next birthday. 4. Use `datetime` objects to parse dates and calculate the differences. # Input: - `birth_date` (str): A string representing the birth date in ISO format (`YYYY-MM-DD`). - `current_date` (str): A string representing the current date in ISO format (`YYYY-MM-DD`). - `time_zone` (str): A string representing the time zone (e.g., `UTC+02:00`, `UTC-05:00`). # Output: - A dictionary with the following keys: - `next_birthday` (datetime): The exact timestamp of the next birthday. - `age` (dict): A dictionary with keys `years`, `months`, `days` representing the age on the next birthday. # Constraints: - The inputs will be valid ISO format dates and valid time zone strings. # Performance Requirements: - The solution should handle dates across MINYEAR to MAXYEAR. # Example: ```python from typing import Dict from datetime import datetime, timedelta, timezone def upcoming_birthday(birth_date: str, current_date: str, time_zone: str) -> Dict: # Your implementation here # Parse birth_date and current_date birth_date_dt = datetime.fromisoformat(birth_date) current_date_dt = datetime.fromisoformat(current_date) # Parse time_zone offset_hours, offset_minutes = map(int, time_zone[3:].split(\\":\\")) if time_zone[3] == \'+\': tz_offset = timedelta(hours=offset_hours, minutes=offset_minutes) else: tz_offset = -timedelta(hours=offset_hours, minutes=offset_minutes) tz = timezone(tz_offset) # Calculate the upcoming birthday next_birthday_year = current_date_dt.year if current_date_dt.date() > birth_date_dt.replace(year=current_date_dt.year).date(): next_birthday_year += 1 next_birthday = birth_date_dt.replace(year=next_birthday_year, tzinfo=tz) # Calculate the exact age on the next birthday years = next_birthday_year - birth_date_dt.year months = next_birthday.month - birth_date_dt.month if months < 0: months += 12 years -= 1 days = next_birthday.day - birth_date_dt.day if days < 0: prev_month = next_birthday - timedelta(days=next_birthday.day) days += (next_birthday - prev_month).days age = {\'years\': years, \'months\': months, \'days\': days} return { \'next_birthday\': next_birthday.isoformat(), \'age\': age } # Example usage birth_date = \\"1990-07-15\\" current_date = \\"2023-10-01\\" time_zone = \\"UTC+02:00\\" print(upcoming_birthday(birth_date, current_date, time_zone)) ``` # Notes: 1. Ensure your implementation handles leap years correctly. 2. Consider edge cases such as birth dates on February 29. 3. Handle both positive and negative UTC offsets for time zones.","solution":"from datetime import datetime, timedelta, timezone from typing import Dict def upcoming_birthday(birth_date: str, current_date: str, time_zone: str) -> Dict: # Parse birth_date and current_date birth_date_dt = datetime.fromisoformat(birth_date) current_date_dt = datetime.fromisoformat(current_date) # Parse time_zone sign = 1 if time_zone[3] == \'+\' else -1 offset_hours, offset_minutes = map(int, time_zone[4:].split(\\":\\")) tz_offset = timedelta(hours=offset_hours, minutes=offset_minutes) tz = timezone(sign * tz_offset) # Calculate the upcoming birthday next_birthday_year = current_date_dt.year if (current_date_dt.month, current_date_dt.day) >= (birth_date_dt.month, birth_date_dt.day): next_birthday_year += 1 try: next_birthday = birth_date_dt.replace(year=next_birthday_year, tzinfo=tz) except ValueError: # Handle February 29 for non-leap years next_birthday = birth_date_dt.replace(year=next_birthday_year, month=3, day=1, tzinfo=tz) # Calculate the exact age on the next birthday age_years = next_birthday_year - birth_date_dt.year age_months = next_birthday.month - birth_date_dt.month age_days = next_birthday.day - birth_date_dt.day if age_days < 0: previous_month = (next_birthday - timedelta(days=next_birthday.day)).month age_days += (next_birthday - next_birthday.replace(day=1) + timedelta(days=1)).days age_months -= 1 if age_months < 0: age_months += 12 age_years -= 1 age = { \'years\': age_years, \'months\': age_months, \'days\': age_days, } return { \'next_birthday\': next_birthday.isoformat(), \'age\': age }"},{"question":"**Problem Statement: Manage and Query Historical Data Using Python\'s Specialized Data Types** You are tasked with implementing a small library to manage and query historical weather data for different cities. The library should support the following functionalities: 1. **Adding Weather Data**: Add daily weather data for a city. Weather data for a day includes the date (as a `datetime.date` object), the high temperature, and the low temperature for that day. 2. **Calculating Average Temperatures**: Given a city and a time period (start date and end date, both as `datetime.date` objects), calculate the average high temperature and average low temperature over the specified period. 3. **Handling Missing Data**: If weather data for any days in the specified period is missing, it should assume a temperature of 0 for both high and low temperatures for those days. 4. **Enumerating Extreme Temperature Days**: Enumerate all days for a city that had temperatures (either high or low) outside the typical range (-10 to 50 degrees Celsius). Return these days with the corresponding temperatures. 5. **Handling Time Zones**: Optionally handle different time zones for cities using Python\'s `zoneinfo` module to ensure dates are properly managed in the context of required time zones. # Constraints: - Dates provided will be within a reasonable range suitable for Python\'s datetime module. - Temperature values are integers in degrees Celsius. # Input and Output Specifications: - `add_weather_data(city: str, date: datetime.date, high_temp: int, low_temp: int) -> None`: Adds weather data for a city. - `calculate_average_temperatures(city: str, start_date: datetime.date, end_date: datetime.date) -> Tuple[float, float]`: Returns the average high and low temperatures for a city over the specified period. - `enumerate_extreme_temperature_days(city: str) -> Dict[datetime.date, Tuple[int, int]]`: Returns a dictionary with dates as keys and a tuple of high and low temperatures for days with extreme temperatures. # Example Usage: ```python import datetime from typing import Tuple, Dict def add_weather_data(city: str, date: datetime.date, high_temp: int, low_temp: int) -> None: # Your implementation here def calculate_average_temperatures(city: str, start_date: datetime.date, end_date: datetime.date) -> Tuple[float, float]: # Your implementation here def enumerate_extreme_temperature_days(city: str) -> Dict[datetime.date, Tuple[int, int]]: # Your implementation here # Example usage add_weather_data(\'Amsterdam\', datetime.date(2021, 4, 1), 12, 5) add_weather_data(\'Amsterdam\', datetime.date(2021, 4, 2), 14, 7) add_weather_data(\'Amsterdam\', datetime.date(2021, 4, 3), 16, 9) average_temps = calculate_average_temperatures(\'Amsterdam\', datetime.date(2021, 4, 1), datetime.date(2021, 4, 3)) extreme_days = enumerate_extreme_temperature_days(\'Amsterdam\') print(average_temps) # Should output (14.0, 7.0) print(extreme_days) # If no extreme days, should output an empty dictionary ``` # Note: The implementation should effectively utilize Python\'s specialized data types covered in the documentation including `datetime`, containers from `collections`, and optionally `zoneinfo` for time zone management.","solution":"import datetime from typing import Tuple, Dict from collections import defaultdict weather_data = defaultdict(dict) def add_weather_data(city: str, date: datetime.date, high_temp: int, low_temp: int) -> None: Adds weather data for a city for a specific date. weather_data[city][date] = (high_temp, low_temp) def calculate_average_temperatures(city: str, start_date: datetime.date, end_date: datetime.date) -> Tuple[float, float]: Returns the average high and low temperatures for a city over the specified period. total_high = 0 total_low = 0 count = 0 current_date = start_date while current_date <= end_date: if current_date in weather_data[city]: high_temp, low_temp = weather_data[city][current_date] else: high_temp, low_temp = 0, 0 total_high += high_temp total_low += low_temp count += 1 current_date += datetime.timedelta(days=1) avg_high = total_high / count if count else 0 avg_low = total_low / count if count else 0 return avg_high, avg_low def enumerate_extreme_temperature_days(city: str) -> Dict[datetime.date, Tuple[int, int]]: Returns a dictionary with dates as keys and a tuple of high and low temperatures for days with extreme temperatures. extreme_days = {} for date, temps in weather_data[city].items(): high_temp, low_temp = temps if high_temp < -10 or high_temp > 50 or low_temp < -10 or low_temp > 50: extreme_days[date] = temps return extreme_days"},{"question":"You are tasked with developing a simple CGI script using the Python `cgi` module. This script will process a form submission and present the user with a customized HTML response. The form contains two input fields: \\"username\\" and \\"email\\". Requirements: 1. Create an HTML form that submits the data to your CGI script. 2. Implement the CGI script to: - Safely handle the form submission. - Validate that both \\"username\\" and \\"email\\" fields are provided. - If either field is missing, return an error message in HTML format. - If both fields are present, return a greeting message displaying the provided \\"username\\" and \\"email\\". Function Implementation: - **Input:** Form submission via HTTP POST request with fields \\"username\\" and \\"email\\". - **Output:** Custom HTML response. Constraints: - The CGI script must be written in Python using the `cgi` module. - Handle form data safely and securely. - Ensure proper error handling and debugging using the `cgitb` module. Performance Requirements: - Handle form submissions efficiently. - Avoid loading large files or unnecessary data into memory. Example: The HTML form should look something like this: ```html <form method=\\"POST\\" action=\\"/cgi-bin/your_script.py\\"> Username: <input type=\\"text\\" name=\\"username\\"><br> Email: <input type=\\"email\\" name=\\"email\\"><br> <input type=\\"submit\\" value=\\"Submit\\"> </form> ``` The CGI script should handle the submission, and if both fields are provided, respond with: ```html <!DOCTYPE html> <html> <head><title>Response</title></head> <body> <h1>Thank you for submitting the form!</h1> <p>Username: [provided username]</p> <p>Email: [provided email]</p> </body> </html> ``` If either field is missing, the response should be: ```html <!DOCTYPE html> <html> <head><title>Error</title></head> <body> <h1>Error</h1> <p>Please fill in both the username and email fields.</p> </body> </html> ``` Hints: - Use the `cgi.FieldStorage` class to handle form data. - Use `cgitb.enable()` for debugging during development. Implement the complete working CGI script below.","solution":"#!/usr/bin/env python3 import cgi import cgitb cgitb.enable() # Enable debugging def main(): # Create an instance of FieldStorage form = cgi.FieldStorage() # Get data from fields username = form.getvalue(\'username\') email = form.getvalue(\'email\') # Set the content type response to HTML print(\\"Content-type:text/htmlrnrn\\") print(\\"<html>\\") print(\\"<head>\\") print(\\"<title>Response</title>\\") print(\\"</head>\\") print(\\"<body>\\") if not username or not email: print(\\"<h1>Error</h1>\\") print(\\"<p>Please fill in both the username and email fields.</p>\\") else: print(\\"<h1>Thank you for submitting the form!</h1>\\") print(f\\"<p>Username: {username}</p>\\") print(f\\"<p>Email: {email}</p>\\") print(\\"</body>\\") print(\\"</html>\\") if __name__ == \\"__main__\\": main()"},{"question":"You have been given a list of tasks, where each task is represented as a tuple `(task_name, priority)`. A task queue should be developed to manage the scheduling of these tasks based on their priority using a min-heap. You are required to implement the following functionalities: 1. `add_task(task_queue, task)`: Function to add a new task to the queue. If the task already exists, update its priority. This should maintain the heap invariant. 2. `remove_task(task_queue, task_name)`: Function to remove an existing task from the queue. 3. `pop_task(task_queue)`: Function to pop and return the task with the highest priority (smallest priority number). Should raise an exception if the queue is empty. **Input/Output Format:** - The task queue will be initialized as an empty list. - Each task is represented as a tuple `(task_name, priority)` where `priority` is an integer. - The `add_task` function takes the task queue and a task tuple as inputs and updates the queue. - The `remove_task` function takes the task queue and a task name as inputs and updates the queue. - The `pop_task` function takes the task queue as input and returns the task with the highest priority. **Constraints:** - The number of tasks will not exceed 10^4. - The priority is an integer in the range `[-10^6, 10^6]`. - Task names are unique strings with a maximum length of 100 characters. **Example:** 1. Initialize the task queue: `task_queue = []` 2. Add a task: `add_task(task_queue, (\'Task1\', 3))` 3. Add another task: `add_task(task_queue, (\'Task2\', 1))` 4. Add a task with an updated priority: `add_task(task_queue, (\'Task1\', 2))` 5. Pop the highest priority task: `pop_task(task_queue) # Returns (\'Task2\', 1)` 6. Remove a task: `remove_task(task_queue, \'Task1\')` **Your Solution:** ```python import heapq from typing import List, Tuple, Any def add_task(task_queue: List[Tuple[int, int, str]], task: Tuple[str, int]) -> None: # Implement add_task by pushing to the heap pass def remove_task(task_queue: List[Tuple[int, int, str]], task_name: str) -> None: # Implement remove_task by marking the task as removed pass def pop_task(task_queue: List[Tuple[int, int, str]]) -> Tuple[Any, Any]: # Implement pop_task to return the task with the highest priority pass # Example operations task_queue = [] add_task(task_queue, (\'Task1\', 3)) add_task(task_queue, (\'Task2\', 1)) add_task(task_queue, (\'Task1\', 2)) print(pop_task(task_queue)) # (\'Task2\', 1) remove_task(task_queue, \'Task1\') ``` Your code should efficiently manage the task queue using the `heapq` module, handling updates, deletions, and retrievals of tasks based on their priorities.","solution":"import heapq from typing import List, Tuple, Any class TaskQueue: def __init__(self): self.task_queue = [] self.entry_finder = {} self.REMOVED = \'<removed-task>\' self.counter = 0 def add_task(self, task: Tuple[str, int]) -> None: \'Add a new task or update the priority of an existing task\' task_name, priority = task if task_name in self.entry_finder: self.remove_task(task_name) entry = [priority, self.counter, task_name] self.entry_finder[task_name] = entry heapq.heappush(self.task_queue, entry) self.counter += 1 def remove_task(self, task_name: str) -> None: \'Mark an existing task as REMOVED. Raise KeyError if not found.\' entry = self.entry_finder.pop(task_name) entry[-1] = self.REMOVED def pop_task(self) -> Tuple[str, int]: \'Remove and return the lowest priority task. Raise KeyError if empty.\' while self.task_queue: priority, count, task_name = heapq.heappop(self.task_queue) if task_name != self.REMOVED: del self.entry_finder[task_name] return (task_name, priority) raise KeyError(\'pop from an empty priority queue\') # Example operation task_manager = TaskQueue() task_manager.add_task((\'Task1\', 3)) task_manager.add_task((\'Task2\', 1)) task_manager.add_task((\'Task1\', 2)) print(task_manager.pop_task()) # (\'Task2\', 1) task_manager.remove_task(\'Task1\')"},{"question":"Objective: Create a function that validates and processes a list of HTTP response statuses. Using the `http.HTTPStatus` enum, your function should categorize each status code into its respective category (1xx, 2xx, 3xx, 4xx, 5xx) and provide a summary of the counts. Function Signature: ```python def categorize_http_statuses(status_codes: list[int]) -> dict: ``` Input: - `status_codes` (list of int): A list of HTTP status codes (e.g., `[200, 404, 500, 301]`). Output: - `result` (dict): A dictionary with keys as categories (e.g., \'Informational\', \'Success\', \'Redirection\', \'Client Error\', \'Server Error\') and values as the respective counts of status codes in that category. Constraints: - The status codes provided in the input list are guaranteed to be valid HTTP status codes as per the `http.HTTPStatus` enum. - Each status code should be categorized based on the following prefixes: - \'1xx\' for Informational - \'2xx\' for Success - \'3xx\' for Redirection - \'4xx\' for Client Error - \'5xx\' for Server Error Example: ```python from http import HTTPStatus def categorize_http_statuses(status_codes): result = { \'Informational\': 0, \'Success\': 0, \'Redirection\': 0, \'Client Error\': 0, \'Server Error\': 0 } for code in status_codes: status = HTTPStatus(code) if 100 <= status.value < 200: result[\'Informational\'] += 1 elif 200 <= status.value < 300: result[\'Success\'] += 1 elif 300 <= status.value < 400: result[\'Redirection\'] += 1 elif 400 <= status.value < 500: result[\'Client Error\'] += 1 elif 500 <= status.value < 600: result[\'Server Error\'] += 1 return result # Example usage status_codes = [200, 404, 500, 301, 102, 204] print(categorize_http_statuses(status_codes)) # Expected output: # { # \'Informational\': 1, # \'Success\': 2, # \'Redirection\': 1, # \'Client Error\': 1, # \'Server Error\': 1 # } ``` This question evaluates a student’s ability to correctly use enums, handle iteration and conditionals, and produce well-structured outputs in Python.","solution":"from http import HTTPStatus def categorize_http_statuses(status_codes): result = { \'Informational\': 0, \'Success\': 0, \'Redirection\': 0, \'Client Error\': 0, \'Server Error\': 0 } for code in status_codes: status = HTTPStatus(code) if 100 <= status.value < 200: result[\'Informational\'] += 1 elif 200 <= status.value < 300: result[\'Success\'] += 1 elif 300 <= status.value < 400: result[\'Redirection\'] += 1 elif 400 <= status.value < 500: result[\'Client Error\'] += 1 elif 500 <= status.value < 600: result[\'Server Error\'] += 1 return result"},{"question":"**Objective:** To assess your understanding of seaborn\'s `ecdfplot` function by creating various ECDF plots and customizing them. **Problem Statement:** You are given a dataset of penguins\' measurements. Use the seaborn library to create and customize ECDF plots based on the following tasks: 1. **Load the dataset:** Load the `penguins` dataset using seaborn\'s `load_dataset` function. 2. **Basic ECDF Plot:** Create a basic ECDF plot for the `flipper_length_mm` variable along the x-axis. Save this plot as `basic_ecdf.png`. 3. **Flipped ECDF Plot:** Flip the plot by assigning the `flipper_length_mm` variable to the y-axis. Save this plot as `flipped_ecdf.png`. 4. **Multiple Histograms (Wide-form dataset):** Create ECDF plots for each numeric column (bill length and bill depth) without assigning `x` or `y`. This should treat the dataset as wide-form and draw histograms for each numeric column. Save this plot as `wide_form_ecdf.png`. 5. **Hue Mapping (Long-form dataset):** Create an ECDF plot for the `bill_length_mm` variable with `species` as the hue. Save this plot as `hue_ecdf.png`. 6. **Custom Statistic:** Create an ECDF plot for the `bill_length_mm` variable with `species` as the hue and the statistic displayed as absolute counts. Save this plot as `count_ecdf.png`. 7. **Complementary ECDF:** Create a complementary ECDF plot (1 - CDF) for the `bill_length_mm` variable with `species` as the hue. Save this plot as `complementary_ecdf.png`. **Expected Input and Output Formats:** The function `create_ecdf_plots()` should take no input parameters and produce six output files: `basic_ecdf.png`, `flipped_ecdf.png`, `wide_form_ecdf.png`, `hue_ecdf.png`, `count_ecdf.png`, and `complementary_ecdf.png`. ```python def create_ecdf_plots(): # Your implementation here ``` **Constraints:** - Ensure all plots are properly labeled with titles and axes labels. - Use seaborn\'s default themes and styles for consistency. - Each task plot must be saved correctly with the specified file names. **Performance Requirements:** - The function should run efficiently, even though performance is not strictly critical for this task. **Important:** - Do not use any libraries or functions outside of seaborn and the standard Python library. - Ensure the plots are saved in the directory where the function is executed.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_ecdf_plots(): # Load the penguins dataset penguins = sns.load_dataset(\\"penguins\\") # Basic ECDF Plot plt.figure() sns.ecdfplot(data=penguins, x=\\"flipper_length_mm\\") plt.title(\'ECDF of Flipper Length\') plt.xlabel(\'Flipper Length (mm)\') plt.ylabel(\'ECDF\') plt.savefig(\\"basic_ecdf.png\\") # Flipped ECDF Plot plt.figure() sns.ecdfplot(data=penguins, y=\\"flipper_length_mm\\") plt.title(\'Flipped ECDF of Flipper Length\') plt.xlabel(\'ECDF\') plt.ylabel(\'Flipper Length (mm)\') plt.savefig(\\"flipped_ecdf.png\\") # Multiple ECDF Plots (Wide-form dataset) plt.figure() sns.ecdfplot(data=penguins) plt.title(\'ECDF for all Numeric Columns\') plt.xlabel(\'Value\') plt.ylabel(\'ECDF\') plt.savefig(\\"wide_form_ecdf.png\\") # ECDF Plot with Hue Mapping plt.figure() sns.ecdfplot(data=penguins, x=\\"bill_length_mm\\", hue=\\"species\\") plt.title(\'ECDF of Bill Length by Species\') plt.xlabel(\'Bill Length (mm)\') plt.ylabel(\'ECDF\') plt.savefig(\\"hue_ecdf.png\\") # Custom Statistic ECDF Plot plt.figure() sns.ecdfplot(data=penguins, x=\\"bill_length_mm\\", hue=\\"species\\", stat=\\"count\\") plt.title(\'Count ECDF of Bill Length by Species\') plt.xlabel(\'Bill Length (mm)\') plt.ylabel(\'Count\') plt.savefig(\\"count_ecdf.png\\") # Complementary ECDF Plot plt.figure() sns.ecdfplot(data=penguins, x=\\"bill_length_mm\\", hue=\\"species\\", complementary=True) plt.title(\'Complementary ECDF of Bill Length by Species\') plt.xlabel(\'Bill Length (mm)\') plt.ylabel(\'1 - ECDF\') plt.savefig(\\"complementary_ecdf.png\\")"},{"question":"# Custom Asynchronous TCP Server In this task, you need to use the `asyncio` module to build an asynchronous TCP server that handles multiple connections and echoes received data back to the clients. Requirements: 1. **Create Event Loop**: - Obtain an event loop using `asyncio.get_running_loop()`. 2. **Create Server**: - Implement a TCP server that listens on a user-specified host and port. - The server should accept incoming connections and create a new `EchoProtocol` instance for each connection. 3. **Protocol Handling**: - Define an `EchoProtocol` class that inherits from `asyncio.Protocol`. - Implement the `connection_made`, `data_received`, and `connection_lost` callbacks: - `connection_made`: Print a message when a connection is made. - `data_received`: Echo the received data back to the client. - `connection_lost`: Print a message when the connection is closed. 4. **Graceful Shutdown**: - Implement a signal handler to gracefully shut down the server on receiving an interrupt signal (e.g., SIGINT). - The server should stop accepting new connections, complete handling current connections, and close the event loop cleanly. Input and Output: - Input: host (str), port (int) - Output: Server operations and debug logs provided via print statements. # Example Usage: ```python import asyncio import signal class EchoProtocol(asyncio.Protocol): def connection_made(self, transport): self.transport = transport print(f\\"Connection made: {transport.get_extra_info(\'peername\')}\\") def data_received(self, data): message = data.decode() print(f\\"Data received: {message}\\") self.transport.write(data) def connection_lost(self, exc): print(\\"Connection lost\\") async def main(host, port): loop = asyncio.get_running_loop() server = await loop.create_server( lambda: EchoProtocol(), host, port ) def shutdown(): print(\\"Shutting down server\\") server.close() loop.stop() for s in (signal.SIGINT, signal.SIGTERM): loop.add_signal_handler(s, shutdown) print(f\\"Serving on {host}:{port}\\") try: await server.serve_forever() except asyncio.CancelledError: pass if __name__ == \\"__main__\\": host = \'127.0.0.1\' port = 8888 asyncio.run(main(host, port)) ``` # Submission Instructions: Submit a `.py` file containing the server implementation. Ensure that the program includes sufficient comments for clarity. **Constraints**: - Implement all necessary parts using the `asyncio` module. - Ensure that the server can handle multiple concurrent connections. - Provide debug messages using print statements at key stages of execution. Happy Coding!","solution":"import asyncio import signal class EchoProtocol(asyncio.Protocol): def connection_made(self, transport): self.transport = transport peername = transport.get_extra_info(\'peername\') print(f\\"Connection made: {peername}\\") def data_received(self, data): message = data.decode() print(f\\"Data received: {message}\\") self.transport.write(data) def connection_lost(self, exc): print(\\"Connection lost\\") async def main(host, port): loop = asyncio.get_running_loop() server = await loop.create_server( lambda: EchoProtocol(), host, port ) def shutdown(): print(\\"Shutting down server\\") server.close() for s in (signal.SIGINT, signal.SIGTERM): loop.add_signal_handler(s, shutdown) print(f\\"Serving on {host}:{port}\\") try: await server.serve_forever() except asyncio.CancelledError: pass if __name__ == \\"__main__\\": host = \'127.0.0.1\' port = 8888 asyncio.run(main(host, port))"},{"question":"# HTTP Client Simulation Objective Implement a Python class called `SimpleHttpClient` that uses the `http.client` module to perform HTTP operations like GET, POST, and handling different HTTP statuses. This class should encapsulate the functionality for making HTTP requests and processing responses. Class Definition ```python class SimpleHttpClient: def __init__(self, host, port=None, use_ssl=False, timeout=10): Initialize the HTTP(s) connection. :param host: The host to connect to. :param port: The port to connect to, default is 80 for HTTP and 443 for HTTPS. :param use_ssl: Boolean flag to indicate if SSL should be used. :param timeout: Connection timeout in seconds. pass def get(self, url, headers=None): Perform a GET request. :param url: The URL path for the GET request. :param headers: Dictionary of headers to include in the request. :return: Tuple of (status_code, reason, response_body) pass def post(self, url, body, headers=None): Perform a POST request. :param url: The URL path for the POST request. :param body: The body of the POST request. :param headers: Dictionary of headers to include in the request. :return: Tuple of (status_code, reason, response_body) pass def close(self): Close the connection. pass ``` Requirements 1. The `SimpleHttpClient` class should switch between HTTP and HTTPS based on the `use_ssl` parameter while initializing the connection. 2. The `get` method should perform a GET request using the connection object and return a tuple containing the status code, reason phrase, and response body as a string. 3. The `post` method should perform a POST request using the connection object and return a tuple containing the status code, reason phrase, and response body as a string. 4. Handle different HTTP status codes gracefully and return appropriate messages or handle errors as necessary. 5. Ensure the connection is appropriately closed after the operations using the `close` method. Example Usage ```python client = SimpleHttpClient(host=\\"www.example.com\\", use_ssl=True) status, reason, body = client.get(\\"/index.html\\") print(status, reason, body) client.close() ``` Constraints 1. Use the `http.client` module provided in the documentation. 2. Do not use external libraries such as `requests` for HTTP operations. 3. Ensure the code is well-documented and handles exceptions appropriately. This will test the student\'s ability to: - Initialize and manage HTTP(S) connections. - Use the `http.client` methods for making GET and POST requests. - Handle HTTP responses and possible exceptions. - Adhere to object-oriented principles in Python. Good luck!","solution":"import http.client import ssl class SimpleHttpClient: def __init__(self, host, port=None, use_ssl=False, timeout=10): Initialize the HTTP(s) connection. :param host: The host to connect to. :param port: The port to connect to, default is 80 for HTTP and 443 for HTTPS. :param use_ssl: Boolean flag to indicate if SSL should be used. :param timeout: Connection timeout in seconds. self.host = host self.port = port if port else (443 if use_ssl else 80) self.use_ssl = use_ssl self.timeout = timeout self.connection = self._create_connection() def _create_connection(self): if self.use_ssl: context = ssl.create_default_context() return http.client.HTTPSConnection(self.host, self.port, timeout=self.timeout, context=context) else: return http.client.HTTPConnection(self.host, self.port, timeout=self.timeout) def get(self, url, headers=None): Perform a GET request. :param url: The URL path for the GET request. :param headers: Dictionary of headers to include in the request. :return: Tuple of (status_code, reason, response_body) headers = headers or {} self.connection.request(\\"GET\\", url, headers=headers) response = self.connection.getresponse() return (response.status, response.reason, response.read().decode()) def post(self, url, body, headers=None): Perform a POST request. :param url: The URL path for the POST request. :param body: The body of the POST request. :param headers: Dictionary of headers to include in the request. :return: Tuple of (status_code, reason, response_body) headers = headers or {} self.connection.request(\\"POST\\", url, body=body, headers=headers) response = self.connection.getresponse() return (response.status, response.reason, response.read().decode()) def close(self): Close the connection. self.connection.close()"},{"question":"# Advanced Coding Assessment: Customizing Colormaps with Seaborn Objective: To evaluate your understanding of seaborn’s `cubehelix_palette` function, you will create custom colormaps to visualize data in different scenarios. This will demonstrate your ability to customize palettes and apply them effectively in visualizations. Task: Implement Python code that performs the following steps: 1. Load the \'iris\' dataset from seaborn. 2. Create three different scatter plot visualizations using seaborn where each uses a custom `cubehelix_palette`. The scatter plots should use the \'sepal_length\' on the x-axis and \'sepal_width\' on the y-axis. 3. Each plot should meet the following customization criteria for the `cubehelix_palette`: - **First Plot**: - Use a discrete palette with 10 colors. - Set the starting point of the helix to 3. - Rotate the helix by 0.5. - **Second Plot**: - Generate a continuous colormap. - Increase the saturation (hue) to 0.8. - Apply a non-linearity (gamma) of 0.7 to the luminance ramp. - **Third Plot**: - Reverse the direction of the luminance ramp. - Use a starting luminance of 0.4 and an ending luminance of 0.8. - Rotate the helix negatively by -0.3. Requirements: - Your code should include comments explaining each major step. - The generated plots should be displayed using matplotlib\'s `plt.show()` function. - Input: The function should not take any input parameters. - Output: The function should display three scatter plots meeting the above customization criteria. Constraints: - You should use seaborn version 0.11.1 or later. - You should not use any other dataset or seaborn function not mentioned above. Example Output: The output should be three scatter plots visualized one after another, each adhering to the specified customization criteria. ```python import seaborn as sns import matplotlib.pyplot as plt def visualize_custom_palettes(): # Step 1: Load Iris dataset iris = sns.load_dataset(\\"iris\\") # First Plot - Discrete palette with 10 colors plt.figure(figsize=(6, 4)) palette1 = sns.cubehelix_palette(10, start=3, rot=0.5) sns.scatterplot(x=\'sepal_length\', y=\'sepal_width\', palette=palette1, data=iris) plt.title(\'Discrete Palette with 10 Colors\') plt.show() # Second Plot - Continuous colormap with high saturation and non-linearity plt.figure(figsize=(6, 4)) palette2 = sns.cubehelix_palette(as_cmap=True, hue=0.8, gamma=0.7) sns.scatterplot(x=\'sepal_length\', y=\'sepal_width\', palette=palette2, data=iris) plt.title(\'Continuous Colormap with High Saturation\') plt.show() # Third Plot - Reversed luminance ramp with custom luminance and negative rotation plt.figure(figsize=(6, 4)) palette3 = sns.cubehelix_palette(reverse=True, dark=0.4, light=0.8, rot=-0.3) sns.scatterplot(x=\'sepal_length\', y=\'sepal_width\', palette=palette3, data=iris) plt.title(\'Reversed Luminance Ramp\') plt.show() # Call the function to generate plots visualize_custom_palettes() ``` Test your implementation to ensure that it meets the requirements and constraints mentioned above.","solution":"import seaborn as sns import matplotlib.pyplot as plt def visualize_custom_palettes(): Visualizes the Iris dataset using three scatter plots with different custom cubehelix palettes. # Load the Iris dataset iris = sns.load_dataset(\\"iris\\") # First Plot - Discrete palette with 10 colors, start=3, rot=0.5 plt.figure(figsize=(6, 4)) palette1 = sns.cubehelix_palette(10, start=3, rot=0.5) sns.scatterplot(x=\'sepal_length\', y=\'sepal_width\', hue=\'species\', palette=palette1, data=iris) plt.title(\'Discrete Palette with 10 Colors\') plt.show() # Second Plot - Continuous colormap with high saturation (hue=0.8) and non-linearity (gamma=0.7) plt.figure(figsize=(6, 4)) palette2 = sns.cubehelix_palette(as_cmap=True, hue=0.8, gamma=0.7) sns.scatterplot(x=\'sepal_length\', y=\'sepal_width\', hue=\'sepal_length\', palette=palette2, data=iris) plt.title(\'Continuous Colormap with High Saturation\') plt.show() # Third Plot - Reversed luminance ramp with custom luminance and negative rotation plt.figure(figsize=(6, 4)) palette3 = sns.cubehelix_palette(reverse=True, dark=0.4, light=0.8, rot=-0.3) sns.scatterplot(x=\'sepal_length\', y=\'sepal_width\', hue=\'species\', palette=palette3, data=iris) plt.title(\'Reversed Luminance Ramp\') plt.show() # Call the function to generate plots visualize_custom_palettes()"},{"question":"# Question: CPU Stream and Device Management with PyTorch Problem Statement PyTorch provides functionalities to manage CPU devices and computational streams, which are typically used to control asynchronous operations and device contexts. This question will test your understanding of these advanced concepts in a CPU context. You are required to implement a function `parallel_matrix_multiplication` that concurrently performs matrix multiplication using multiple CPU streams. The function should take: 1. An integer `num_streams` representing the number of streams (parallel computations) to use. 2. Two 2D lists `matrix_a` and `matrix_b` representing the matrices to be multiplied. The function should: 1. Divide the computation across streams, splitting the matrices appropriately. 2. Utilize PyTorch\'s stream and device management to perform matrix multiplications in parallel. 3. Synchronize the operations at the end to ensure the multiplication results are properly combined into a final output matrix. Input - `num_streams` (int): The number of parallel streams to use. (1 <= num_streams <= 8) - `matrix_a` (List[List[float]]): The first matrix to be multiplied. Dimensions are m x n. - `matrix_b` (List[List[float]]): The second matrix to be multiplied. Dimensions are n x p. Output - Returns a matrix (List[List[float]]) representing the result of the matrix multiplication with dimensions m x p. Constraints - You may assume all matrices are well-formed and dimensions are compatible for multiplication. - Performance is a concern for large matrices, so the use of streams should provide a noticeable speedup. Example ```python result = parallel_matrix_multiplication(2, [[1, 2], [3, 4]], [[5, 6], [7, 8]]) print(result) # Output should be [[19, 22], [43, 50]] ``` Hints - Utilize `torch.cpu.stream` to create streams. - Use `await` and `synchronize` to manage parallel computations. - Ensure the final output combines all partial results correctly. Function Signature ```python from typing import List def parallel_matrix_multiplication(num_streams: int, matrix_a: List[List[float]], matrix_b: List[List[float]]) -> List[List[float]]: pass ```","solution":"import torch from typing import List def parallel_matrix_multiplication(num_streams: int, matrix_a: List[List[float]], matrix_b: List[List[float]]) -> List[List[float]]: # Convert lists to torch tensors matrix_a = torch.tensor(matrix_a, dtype=torch.float32) matrix_b = torch.tensor(matrix_b, dtype=torch.float32) # Get dimensions m, n = matrix_a.shape _, p = matrix_b.shape # Determine the chunk size chunk_size = m // num_streams # Create streams streams = [torch.cuda.Stream() for _ in range(num_streams)] # Split the matrix_a by rows to perform parallel computation results = [] chunks = [(i * chunk_size, (i + 1) * chunk_size if i != num_streams - 1 else m) for i in range(num_streams)] for i, (start, end) in enumerate(chunks): stream = streams[i] with torch.cuda.stream(stream): result = torch.matmul(matrix_a[start:end], matrix_b).to(torch.device(\\"cpu\\")) results.append(result) # Synchronize all streams for stream in streams: stream.synchronize() # Concatenate results to form the final output matrix final_result = torch.cat(results).cpu().numpy().tolist() return final_result"},{"question":"**Question 1: Data Visualization using Seaborn** In this challenge, you are required to use the seaborn library to create and customize data visualizations based on a given dataset. Follow the steps and complete the functions as described below: 1. Load the `penguins` dataset using seaborn\'s `load_dataset` function. 2. Create a histogram plot showing the distribution of `body_mass_g` for different species of penguins using `sns.displot`. 3. Add a Kernel Density Estimate (KDE) curve to the histogram created in step 2. 4. Create a bivariate plot showing the relationship between `bill_length_mm` and `bill_depth_mm` for different species, using `hue` mapping to distinguish species. 5. Customize a FacetGrid to show two ECDF plots of `body_mass_g` separated by `sex` for each species. Ensure the plots have appropriate titles and labels. **Function Signatures:** ```python import seaborn as sns def load_penguins_data(): Load the penguins dataset using seaborn. Returns: DataFrame: The loaded penguins dataset. pass def plot_body_mass_histogram(penguins): Create a histogram plot showing the distribution of body_mass_g for different species. Args: penguins (DataFrame): The penguins dataset. pass def add_kde_to_histogram(penguins): Add a KDE curve to the histogram created in `plot_body_mass_histogram`. Args: penguins (DataFrame): The penguins dataset. pass def plot_bivariate_bill_relationship(penguins): Create a bivariate plot showing the relationship between `bill_length_mm` and `bill_depth_mm` for different species. Args: penguins (DataFrame): The penguins dataset. pass def customize_ecdf_facet_grid(penguins): Create a FacetGrid to show ECDF plots of `body_mass_g` separated by `sex` for each species. Customize the grid with titles and labels. Args: penguins (DataFrame): The penguins dataset. pass ``` **Constraints:** - Your solution should use the seaborn library to create and customize the plots. - Make sure to follow best practices for data visualization to ensure clarity and readability. - You must use the provided function signatures. **Example:** ```python # Example function usage: penguins = load_penguins_data() plot_body_mass_histogram(penguins) add_kde_to_histogram(penguins) plot_bivariate_bill_relationship(penguins) customize_ecdf_facet_grid(penguins) ```","solution":"import seaborn as sns import matplotlib.pyplot as plt def load_penguins_data(): Load the penguins dataset using seaborn. Returns: DataFrame: The loaded penguins dataset. penguins = sns.load_dataset(\'penguins\') return penguins def plot_body_mass_histogram(penguins): Create a histogram plot showing the distribution of body_mass_g for different species. Args: penguins (DataFrame): The penguins dataset. sns.displot(penguins, x=\\"body_mass_g\\", hue=\\"species\\", kind=\\"hist\\", multiple=\\"stack\\") plt.title(\\"Distribution of Body Mass by Species\\") plt.xlabel(\\"Body Mass (g)\\") plt.ylabel(\\"Frequency\\") plt.show() def add_kde_to_histogram(penguins): Add a KDE curve to the histogram created in `plot_body_mass_histogram`. Args: penguins (DataFrame): The penguins dataset. sns.displot(penguins, x=\\"body_mass_g\\", hue=\\"species\\", kind=\\"kde\\", fill=True) plt.title(\\"Distribution of Body Mass with KDE by Species\\") plt.xlabel(\\"Body Mass (g)\\") plt.ylabel(\\"Density\\") plt.show() def plot_bivariate_bill_relationship(penguins): Create a bivariate plot showing the relationship between `bill_length_mm` and `bill_depth_mm` for different species. Args: penguins (DataFrame): The penguins dataset. sns.scatterplot(data=penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", hue=\\"species\\", style=\\"species\\") plt.title(\\"Bill Length vs. Bill Depth by Species\\") plt.xlabel(\\"Bill Length (mm)\\") plt.ylabel(\\"Bill Depth (mm)\\") plt.legend(title=\\"Species\\") plt.show() def customize_ecdf_facet_grid(penguins): Create a FacetGrid to show ECDF plots of `body_mass_g` separated by `sex` for each species. Customize the grid with titles and labels. Args: penguins (DataFrame): The penguins dataset. g = sns.FacetGrid(penguins, row=\\"species\\", col=\\"sex\\", margin_titles=True) g.map_dataframe(sns.ecdfplot, x=\\"body_mass_g\\") g.set_axis_labels(\\"Body Mass (g)\\", \\"ECDF\\") g.set_titles(col_template=\\"{col_name} Penguins\\", row_template=\\"{row_name}\\") plt.suptitle(\\"ECDF of Body Mass by Species and Sex\\", y=1.02) plt.show()"},{"question":"# Pandas Series Manipulation and Analysis You are given a dataset in the form of a CSV file containing sales data for a retail company. Each row in the dataset represents a transaction and contains the following columns: - `Date` (string): The date of the transaction in the format `YYYY-MM-DD`. - `Product` (string): The name of the product sold. - `Quantity` (integer): The number of units sold. - `Price` (float): The price per unit in USD. Your task is to write a Python function using pandas that performs the following steps: 1. **Read the CSV File**: Load the data into a pandas DataFrame. 2. **Data Cleaning**: Handle any missing values by filling them with appropriate defaults. 3. **Data Transformation**: - Convert the `Date` column to a datetime object and set it as the index of the DataFrame. - Calculate a new column `Total_Sales` which is the product of `Quantity` and `Price`. 4. **Summary Statistics**: - Compute the total sales for each product and return the product with the highest total sales. - Calculate the monthly total sales and return the month with the lowest sales. 5. **Data Output**: Export the cleaned and transformed DataFrame to a new CSV file. # Implementation Details - **Input**: The function should take the file path of the input CSV file as an argument. - **Output**: The function should return a tuple with: - The name of the product with the highest total sales (string). - The month with the lowest total sales (as a datetime object). - The file path of the output CSV file (string). # Constraints - Assume the CSV file is well-formed but may contain missing values. - There will be at least one transaction for each month in the data. # Performance Requirements - Solutions should aim for readability and efficient use of pandas operations. # Example ```python import pandas as pd def analyze_sales_data(input_csv): # Step 1: Read the CSV file df = pd.read_csv(input_csv) # Step 2: Data Cleaning df.fillna({\'Quantity\': 0, \'Price\': 0.0}, inplace=True) # Step 3: Data Transformation df[\'Date\'] = pd.to_datetime(df[\'Date\']) df.set_index(\'Date\', inplace=True) df[\'Total_Sales\'] = df[\'Quantity\'] * df[\'Price\'] # Step 4: Summary Statistics product_total_sales = df.groupby(\'Product\')[\'Total_Sales\'].sum() highest_selling_product = product_total_sales.idxmax() monthly_sales = df[\'Total_Sales\'].resample(\'M\').sum() lowest_sales_month = monthly_sales.idxmin() # Step 5: Export Data output_csv = \'cleaned_sales_data.csv\' df.to_csv(output_csv) return highest_selling_product, lowest_sales_month, output_csv ``` Test the function with appropriate test cases ensuring it handles missing values, different date formats, and computes the required statistics accurately.","solution":"import pandas as pd def analyze_sales_data(input_csv): # Step 1: Read the CSV file df = pd.read_csv(input_csv) # Step 2: Data Cleaning df.fillna({\'Quantity\': 0, \'Price\': 0.0}, inplace=True) # Step 3: Data Transformation df[\'Date\'] = pd.to_datetime(df[\'Date\']) df.set_index(\'Date\', inplace=True) df[\'Total_Sales\'] = df[\'Quantity\'] * df[\'Price\'] # Step 4: Summary Statistics product_total_sales = df.groupby(\'Product\')[\'Total_Sales\'].sum() highest_selling_product = product_total_sales.idxmax() monthly_sales = df[\'Total_Sales\'].resample(\'M\').sum() lowest_sales_month = monthly_sales.idxmin() # Step 5: Export Data output_csv = \'cleaned_sales_data.csv\' df.to_csv(output_csv) return highest_selling_product, lowest_sales_month, output_csv"},{"question":"**Objective:** Create a function that formats a given report using the string formatting and template features provided by the `string` module. **Task:** Implement a function `generate_report(data, template)` that takes two arguments: 1. `data`: A dictionary containing key-value pairs where keys are placeholders in a template, and values are the corresponding data to substitute. 2. `template`: A string that uses the `Template` class syntax from the `string` module to define the report format. Your function should: 1. Validate that all required placeholders in the template are present in the data dictionary. 2. Perform the substitution using the `Template` class. 3. Handle any missing placeholders gracefully by leaving them as-is in the template. **Input Format:** - `data`: A dictionary with string keys and values. - `template`: A string using `` placeholders compatible with `string.Template`. **Output Format:** - A formatted string after substitution. **Constraints:** - The `data` dictionary will only contain ASCII characters in the keys and values. - The `template` string will only use the `placeholder` syntax for substitutions. - Performance should be efficient for typical input sizes (e.g., templates and data dictionaries under 1000 elements). # Example ```python def generate_report(data, template): from string import Template tmpl = Template(template) try: return tmpl.substitute(data) except KeyError as e: return f\\"Error: Missing placeholder - {e.args[0]}\\" # Example usage: data = { \'name\': \'Alice\', \'age\': \'30\', \'job\': \'Engineer\' } template = \\"Name: namenAge: agenJob: jobnLocation: location\\" print(generate_report(data, template)) # Output: # Error: Missing placeholder - location ``` **Notes:** - The function should use `Template` from the `string` module for substitution. - If any placeholder in the template is not found in the `data`, the function should print an error message indicating the missing placeholder. # Challenge: Extend your solution to handle nested replacement fields within the `template`. Perform substitutions dynamically so that placeholders inside format specifiers are resolved correctly.","solution":"def generate_report(data, template): from string import Template tmpl = Template(template) try: result = tmpl.substitute(data) except KeyError as e: return f\\"Error: Missing placeholder - {e.args[0]}\\" return result"},{"question":"**Objective**: This exercise aims to test your understanding of PyTorch tensors and their attributes, specifically `torch.dtype`, `torch.device`, and `torch.layout`. You are required to write a function that dynamically creates tensors of different configurations and performs operations based on their types and devices. **Task**: Implement a function `tensor_manipulation` that takes the following input parameters: - `shape` (tuple): The shape of the tensor to be created. - `dtype` (str): The data type of the tensor (e.g., \'float32\', \'int64\', \'bool\'). - `device` (str): The device where the tensor should be allocated (e.g., \'cpu\', \'cuda\'). - `operation` (str): The operation to be performed on the tensor. Possible values are `\'add\'`, `\'multiply\'`, representing element-wise addition and multiplication by 2 respectively. The function should return the resulting tensor after the specified operation is performed. **Constraints**: 1. The tensor\'s device should be properly set to either \'cpu\' or \'cuda\'. 2. The tensor\'s dtype should match the provided data type. 3. If the `operation` is `\'add\'`, add 1 to each element. 4. If the `operation` is `\'multiply\'`, multiply each element by 2. 5. Ensure the tensor\'s allocation is memory efficient based on the given `shape`. **Function Definition**: ```python def tensor_manipulation(shape: tuple, dtype: str, device: str, operation: str) -> torch.Tensor: # Your implementation here pass ``` **Example**: ```python # Example 1: result_tensor = tensor_manipulation((2, 3), \'float32\', \'cpu\', \'add\') print(result_tensor) # Should output a tensor of shape (2, 3) with dtype float32 on CPU where each element is 1. # Example 2: result_tensor = tensor_manipulation((3, 3), \'int64\', \'cuda\', \'multiply\') print(result_tensor) # Should output a tensor of shape (3, 3) with dtype int64 on GPU where each element is 0 * 2 = 0. # Example 3: result_tensor = tensor_manipulation((1, 5), \'bool\', \'cpu\', \'multiply\') print(result_tensor) # Should output a tensor of shape (1, 5) with dtype bool on CPU where each element remains the same (True or False). ``` **Notes**: - Use `torch.tensor` for tensor creation and `to` method for device placement. - You should use PyTorch\'s type promotion rules only if necessary, as described in the original documentation. - Your solution should handle exceptions such as invalid device or dtype gracefully. Good luck and happy coding!","solution":"import torch def tensor_manipulation(shape: tuple, dtype: str, device: str, operation: str) -> torch.Tensor: # Map string dtype to torch dtype dtype_mapping = { \'float32\': torch.float32, \'int64\': torch.int64, \'bool\': torch.bool, } if dtype not in dtype_mapping or device not in [\'cpu\', \'cuda\']: raise ValueError(\\"Invalid dtype or device\\") tensor_dtype = dtype_mapping[dtype] tensor_device = torch.device(device) # Create an initial tensor filled with zeros tensor = torch.zeros(shape, dtype=tensor_dtype, device=tensor_device) # Apply the specified operation if operation == \'add\': if dtype == \'bool\': raise ValueError(\\"Addition operation not supported for boolean tensor.\\") result_tensor = tensor + 1 elif operation == \'multiply\': result_tensor = tensor * 2 else: raise ValueError(\\"Invalid operation\\") return result_tensor"},{"question":"# Distributed Checkpointing in PyTorch **Objective:** Implement a coding solution to save and load a distributed model\'s state using PyTorch\'s Distributed Checkpoint (DCP) functionality. This task will assess your understanding of distributed model parallelism, state dictionary management, and asynchronous operations in PyTorch. **Problem Statement:** 1. Implement a function `save_distributed_checkpoint` that: - Saves the state of a given distributed model using DCP. - Uses asynchronous saving to optimize performance. - Ensures consistency in the state dictionary keys irrespective of the model\'s parallelism setup. 2. Implement a function `load_distributed_checkpoint` that: - Loads the state of a distributed model from the saved checkpoint. - Ensures the state dictionary matches the original model\'s state dictionary format before parallelism was applied. **Function Signatures:** ```python import torch import torch.distributed.checkpoint as dcp def save_distributed_checkpoint(model: torch.nn.Module, optimizer: torch.optim.Optimizer, checkpoint_dir: str) -> None: Save a distributed model\'s state using DCP with asynchronous saving. Args: - model (torch.nn.Module): The distributed model to be saved. - optimizer (torch.optim.Optimizer): The optimizer associated with the model. - checkpoint_dir (str): Directory to store the checkpoint files. Returns: None pass def load_distributed_checkpoint(model: torch.nn.Module, optimizer: torch.optim.Optimizer, checkpoint_dir: str) -> None: Load a distributed model\'s state from a DCP checkpoint. Args: - model (torch.nn.Module): The distributed model to load the state into. - optimizer (torch.optim.Optimizer): The optimizer associated with the model. - checkpoint_dir (str): Directory where the checkpoint files are stored. Returns: None pass ``` **Requirements:** 1. **Saving Function Requirements** (`save_distributed_checkpoint`): - Use `dcp.async_save` to save the model and optimizer state asynchronously. - Utilize the `get_model_state_dict` and `get_optimizer_state_dict` functions to ensure consistency in model and optimizer state dictionaries. - Ensure that the checkpoint is stored in the specified directory. 2. **Loading Function Requirements** (`load_distributed_checkpoint`): - Use `dcp.load` to load the model and optimizer states from the checkpoint. - Ensure the loaded state dictionary keys match those of the original model before parallelism. **Constraints:** - Assume the distributed environment is already set up with the required number of ranks. - The checkpoint directory path exists and is writable. - The model and optimizer are already initialized and in a distributed training setup. **Example Usage:** ```python model = ... # Your distributed model optimizer = ... # Your optimizer checkpoint_dir = \'/path/to/checkpoint_dir\' # Save the distributed model state save_distributed_checkpoint(model, optimizer, checkpoint_dir) # Load the distributed model state load_distributed_checkpoint(model, optimizer, checkpoint_dir) ```","solution":"import torch import torch.distributed.checkpoint as dcp import os def get_model_state_dict(model: torch.nn.Module): Returns the state dict of the model ensuring it always gets the state dict of the base model if wrapped in DataParallel or DistributedDataParallel if isinstance(model, (torch.nn.DataParallel, torch.nn.parallel.DistributedDataParallel)): return model.module.state_dict() return model.state_dict() def get_optimizer_state_dict(optimizer: torch.optim.Optimizer): Returns the optimizer state dict. return optimizer.state_dict() async def save_distributed_checkpoint(model: torch.nn.Module, optimizer: torch.optim.Optimizer, checkpoint_dir: str) -> None: Save a distributed model\'s state using DCP with asynchronous saving. Args: - model (torch.nn.Module): The distributed model to be saved. - optimizer (torch.optim.Optimizer): The optimizer associated with the model. - checkpoint_dir (str): Directory to store the checkpoint files. Returns: None os.makedirs(checkpoint_dir, exist_ok=True) model_state_dict = get_model_state_dict(model) optimizer_state_dict = get_optimizer_state_dict(optimizer) save_tasks = dcp.async_save({ \'model_state_dict\': model_state_dict, \'optimizer_state_dict\': optimizer_state_dict, }, checkpoint_dir) await save_tasks def load_distributed_checkpoint(model: torch.nn.Module, optimizer: torch.optim.Optimizer, checkpoint_dir: str) -> None: Load a distributed model\'s state from a DCP checkpoint. Args: - model (torch.nn.Module): The distributed model to load the state into. - optimizer (torch.optim.Optimizer): The optimizer associated with the model. - checkpoint_dir (str): Directory where the checkpoint files are stored. Returns: None checkpoint = dcp.load(checkpoint_dir) model.load_state_dict(checkpoint[\'model_state_dict\']) optimizer.load_state_dict(checkpoint[\'optimizer_state_dict\'])"},{"question":"**Coding Challenge: Pure Functions and Transformations with PyTorch** **Objective:** Demonstrate your understanding of transformations in `torch.func` by implementing a pure function and applying various `torch.func` transformations while complying with described constraints. **Problem Statement:** You need to implement a function that computes the sum of squares of a tensor and then apply the following transformations to the function using `torch.func`: 1. Compute the gradient of the function. 2. Vectorize the function using `vmap`. 3. Handle inplace operations within the function. **Function Implementation:** 1. **sum_of_squares(x: torch.Tensor) -> torch.Tensor:** - This function should compute the sum of the squares of all elements in the tensor `x`. 2. **gradient_of_sum_of_squares(x: torch.Tensor) -> torch.Tensor:** - Using `torch.func.grad`, calculate the gradient of `sum_of_squares`. 3. **vectorized_sum_of_squares(x: torch.Tensor) -> torch.Tensor:** - Use `torch.func.vmap` to vectorize `sum_of_squares`. 4. **modified_sum_of_squares(x: torch.Tensor) -> torch.Tensor:** - Ensure `sum_of_squares` handles inplace operations correctly (e.g., using `torch.zeros` instead of `Tensor.new_zeros`). **Constraints:** - Inputs will be non-empty tensors of up to dimension 2 with random values. - All outputs must retain proper gradients and vectorized behavior. - Avoid side effects and ensure proper handling of inplace operations. **Example:** ```python import torch from torch.func import grad, vmap def sum_of_squares(x: torch.Tensor) -> torch.Tensor: # Your implementation here that calculates sum of squares pass def gradient_of_sum_of_squares(x: torch.Tensor) -> torch.Tensor: # Your implementation here using torch.func.grad pass def vectorized_sum_of_squares(x: torch.Tensor) -> torch.Tensor: # Your implementation here using torch.func.vmap pass def modified_sum_of_squares(x: torch.Tensor) -> torch.Tensor: # Ensure sum_of_squares handles inplace operations correctly pass # Example usage x = torch.tensor([1.0, 2.0, 3.0]) grad_x = gradient_of_sum_of_squares(x) print(\\"Gradient Computation:\\", grad_x) x_batched = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]]) vec_output = vectorized_sum_of_squares(x_batched) print(\\"Vectorized Compilation:\\", vec_output) x_zeros = torch.tensor([1.0, 2.0, 3.0]) mod_output = modified_sum_of_squares(x_zeros) print(\\"Modified Sum of Squares with Correct Inplace Handling:\\", mod_output) ``` **Evaluation Criteria:** - Correct implementation of the sum_of_squares function. - Proper utilization of `torch.func.grad` for gradient computation. - Effective vectorization using `torch.func.vmap`. - Proper handling and adjustments for inplace operations. - Code clarity, efficiency, and adherance to pure function principles.","solution":"import torch from torch.func import grad, vmap def sum_of_squares(x: torch.Tensor) -> torch.Tensor: Computes the sum of squares of all elements in the tensor `x`. return torch.sum(x ** 2) def gradient_of_sum_of_squares(x: torch.Tensor) -> torch.Tensor: Calculates the gradient of the sum_of_squares function. return grad(sum_of_squares)(x) def vectorized_sum_of_squares(x: torch.Tensor) -> torch.Tensor: Vectorizes the sum_of_squares function. return vmap(sum_of_squares)(x) def modified_sum_of_squares(x: torch.Tensor) -> torch.Tensor: Handles inplace operations correctly within sum_of_squares. return torch.sum(torch.square(torch.clone(x))) # Example usage (These calls are not part of solution code, just for illustration): # x = torch.tensor([1.0, 2.0, 3.0]) # grad_x = gradient_of_sum_of_squares(x) # print(\\"Gradient Computation:\\", grad_x) # x_batched = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]]) # vec_output = vectorized_sum_of_squares(x_batched) # print(\\"Vectorized Compilation:\\", vec_output) # x_zeros = torch.tensor([1.0, 2.0, 3.0]) # mod_output = modified_sum_of_squares(x_zeros) # print(\\"Modified Sum of Squares with Correct Inplace Handling:\\", mod_output)"},{"question":"# Question: You have been provided with a dataset that contains information about iris flowers. Your task is to implement a Gaussian Naive Bayes classifier to classify the iris flowers into three categories based on their features. Dataset Description: - The dataset contains 150 samples and 4 features: SepalLength, SepalWidth, PetalLength, PetalWidth. - There are three classes: 0, 1, and 2 representing three different iris species. Requirements: 1. **Load the dataset**: Use the `load_iris` function from `sklearn.datasets`. 2. **Split the dataset**: Split the dataset into a training set (70%) and a testing set (30%) using `train_test_split`. 3. **Implement Gaussian Naive Bayes Classification**: - Train the classifier using the training set. - Predict the classes of the test set. 4. **Evaluation**: - Print the number of mislabeled points out of the total number of points. - Print the accuracy of the classifier. # Constraints: - Use the GaussianNB class from `sklearn.naive_bayes`. - Ensure the splits are reproducible by setting a random seed. # Input: - No input needed, as you will use the preloaded dataset from sklearn. # Output: - Print the following: - Number of mislabeled points in the test set. - Accuracy of the classifier. Example: ```python # Expected Output (example): # Number of mislabeled points out of a total 45 points: 3 # Accuracy: 0.9333 ``` # Implementation: ```python from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.naive_bayes import GaussianNB from sklearn.metrics import accuracy_score # Load dataset iris = load_iris() X, y = iris.data, iris.target # Split dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Implement Gaussian Naive Bayes gnb = GaussianNB() gnb.fit(X_train, y_train) y_pred = gnb.predict(X_test) # Evaluate the classifier num_mislabeled = (y_test != y_pred).sum() accuracy = accuracy_score(y_test, y_pred) print(f\\"Number of mislabeled points out of a total {X_test.shape[0]} points: {num_mislabeled}\\") print(f\\"Accuracy: {accuracy:.4f}\\") ``` **Additional Challenge**: **Implement from Scratch** For those looking for an additional challenge, try implementing the Gaussian Naive Bayes classifier without using `sklearn`\'s `GaussianNB`. You can refer to the mathematical formulation provided in the documentation for this purpose.","solution":"from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.naive_bayes import GaussianNB from sklearn.metrics import accuracy_score # Load dataset iris = load_iris() X, y = iris.data, iris.target # Split dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Implement Gaussian Naive Bayes gnb = GaussianNB() gnb.fit(X_train, y_train) y_pred = gnb.predict(X_test) # Evaluate the classifier num_mislabeled = (y_test != y_pred).sum() accuracy = accuracy_score(y_test, y_pred) print(f\\"Number of mislabeled points out of a total {X_test.shape[0]} points: {num_mislabeled}\\") print(f\\"Accuracy: {accuracy:.4f}\\")"},{"question":"You are tasked with evaluating a dataset comprised of various metrics stored within a text file. Each row in the data file represents a different metric, containing metric name and a series of space-separated numerical values. For example: ``` temperature 20 21 19 23 22 humidity 55 53 58 60 57 wind_speed 5 7 3 2 4 ``` Your goal is to display a summary of this data, consisting of the maximum, minimum, and average values for each metric. Furthermore, you need to ensure that the data file can be read and handled robustly, managing potential errors gracefully and avoiding crashes. # Requirements 1. Load the file content into memory and process each metric. 2. Calculate the maximum, minimum, and average for each metric. 3. Display the results in a formatted output. 4. Your implementation should utilize built-in Python functions as much as possible (e.g., `open`, `max`, `min`, `sum`, `len`, `map`, `filter`). # Function Signature ```python def summarize_metrics(filename: str) -> str: Read the metrics from the file and return the summary of maximum, minimum, and average values for each metric. Parameters: filename (str): The name of the file containing the metrics. Returns: str: A formatted string representation of the summary. ``` # Example Given `metrics.txt` containing: ``` temperature 20 21 19 23 22 humidity 55 53 58 60 57 wind_speed 5 7 3 2 4 ``` Calling `summarize_metrics(\'metrics.txt\')` should return: ``` temperature: max=23, min=19, avg=21.0 humidity: max=60, min=53, avg=56.6 wind_speed: max=7, min=2, avg=4.2 ``` # Constraints 1. The input file will contain at least one metric. 2. Each metric row in the file will have a unique name followed by one or more integer or floating point values. 3. Handle file reading exceptions (e.g., file not found) gracefully and provide a meaningful error message. 4. Ensure the calculations handle integer and float inputs correctly. # Notes - Utilize Python\'s built-in functions efficiently. - Consider edge cases such as an empty file or rows with insufficient data.","solution":"def summarize_metrics(filename: str) -> str: try: with open(filename, \'r\') as file: lines = file.readlines() if not lines: return \\"File is empty.\\" summary = [] for line in lines: parts = line.strip().split() if len(parts) < 2: continue metric_name = parts[0] try: values = list(map(float, parts[1:])) except ValueError: continue if values: max_value = max(values) min_value = min(values) avg_value = sum(values) / len(values) summary.append(f\\"{metric_name}: max={max_value}, min={min_value}, avg={avg_value:.1f}\\") if not summary: return \\"No valid data to summarize.\\" return \'n\'.join(summary) except FileNotFoundError: return f\\"File {filename} not found.\\""},{"question":"**Question**: Write a Python function that accepts a URL and a preference for how the URL should be opened (in the same window, a new window, or a new tab). The function should use the `webbrowser` module to perform the desired action and handle any exceptions that may occur. **Function Signature**: ```python def browse_url(url: str, open_mode: str) -> bool: pass ``` **Input**: - `url` (str): The URL to be opened. - `open_mode` (str): Specifies how the URL should be opened. It can take one of three values: - `\\"same\\"`: This should open the URL in the same browser window if possible. - `\\"new_window\\"`: This should open the URL in a new browser window if possible. - `\\"new_tab\\"`: This should open the URL in a new browser tab if possible. **Output**: - The function returns a boolean value: - `True` if the URL was successfully opened. - `False` if an exception occurred or the URL could not be opened. **Constraints**: - The function should handle inappropriate values for `open_mode` by raising a `ValueError` with the message `\\"Invalid open_mode. Use \'same\', \'new_window\' or \'new_tab\'\\"`. - Use exception handling to catch and handle any errors that occur while trying to open the URL. **Examples**: ```python browse_url(\\"https://www.google.com\\", \\"new_tab\\") # Expected output: True, opens in new tab browse_url(\\"https://www.google.com\\", \\"new_window\\") # Expected output: True, opens in new window browse_url(\\"https://www.google.com\\", \\"same\\") # Expected output: True, opens in the same window browse_url(\\"https://www.google.com\\", \\"invalid_mode\\") # Expected output: raises ValueError ```","solution":"import webbrowser def browse_url(url: str, open_mode: str) -> bool: Opens a URL in a web browser according to the specified mode. Parameters: url (str): The URL to be opened. open_mode (str): Specifies how the URL should be opened. It can take one of three values: \\"same\\", \\"new_window\\", \\"new_tab\\". Returns: bool: True if the URL was successfully opened, False if an exception occurred. try: if open_mode == \'same\': webbrowser.open(url, new=0) elif open_mode == \'new_window\': webbrowser.open(url, new=1) elif open_mode == \'new_tab\': webbrowser.open(url, new=2) else: raise ValueError(\\"Invalid open_mode. Use \'same\', \'new_window\' or \'new_tab\'\\") return True except Exception as e: return False"},{"question":"Advanced Audio Processing with `ossaudiodev` # Objective: Implement a function that records audio from a microphone, processes the recorded audio by increasing its volume, and plays back the modified audio using the `ossaudiodev` module in Python. # Problem Statement: Write a function `record_and_playback(duration: int, volume_increase: int)` that records audio from a default microphone for a specified duration, increases the volume of the recorded audio by the given percentage, and then plays back the modified audio. # Function Signature: ```python def record_and_playback(duration: int, volume_increase: int) -> None: ``` # Input: - `duration` (int): The recording duration in seconds. Must be a positive integer. - `volume_increase` (int): The percentage by which to increase the volume of the recorded audio. Must be a non-negative integer. # Requirements: 1. Open an audio device (`/dev/dsp`) for reading and another for writing using the `ossaudiodev.open()` function. 2. Set the audio format to 16-bit signed little-endian PCM, 1 channel (mono), and a sample rate of 44100 Hz using `setparameters()` method. 3. Record audio from the microphone for the specified duration. 4. Increase the volume of the recorded audio data by the given percentage. Ensure that you handle potential overflow issues (values exceeding the range [-32768, 32767] for 16-bit samples). 5. Play back the modified audio data through the audio playback device. 6. Ensure proper resource handling using context management (e.g., using `with` statements). # Output: - The function should not return any value. It should simply play back the modified audio after recording. # Constraints: - You must handle exceptions and ensure that devices are properly closed in case of errors. - Assume the audio device and mixer device are present and accessible at `\\"/dev/dsp\\"` and `\\"/dev/mixer\\"`, respectively. # Example: ```python # Record for 5 seconds and increase volume by 20% record_and_playback(5, 20) ``` # Notes: - Make sure to consult the `ossaudiodev` documentation provided above for method details and correct usage. - Ensure that file-like methods and ioctl calls are used appropriately for setting parameters and handling audio data.","solution":"import ossaudiodev import array def record_and_playback(duration: int, volume_increase: int) -> None: Records audio from the microphone for the given duration, increases the volume, and plays it back. Parameters: - duration (int): Duration in seconds to record audio. - volume_increase (int): Volume increase percentage. if duration <= 0: raise ValueError(\\"Duration must be a positive integer\\") if volume_increase < 0: raise ValueError(\\"Volume increase must be a non-negative integer\\") # Audio parameters format = ossaudiodev.AFMT_S16_LE channels = 1 rate = 44100 try: # Open the audio device for recording (read mode) with ossaudiodev.open(\\"r\\") as dsp_in: dsp_in.setparameters(format, channels, rate) # Calculate the number of frames to read frames = duration * rate # Read the audio data data = dsp_in.read(frames * 2) # 2 bytes per frame (16-bit audio) # Convert audio data to an array of short integers audio_array = array.array(\'h\', data) # Increase volume for i in range(len(audio_array)): audio_array[i] = int(audio_array[i] * (1 + volume_increase / 100.0)) # Make sure samples stay within 16-bit PCM range if audio_array[i] > 32767: audio_array[i] = 32767 elif audio_array[i] < -32768: audio_array[i] = -32768 modified_data = audio_array.tobytes() # Open the audio device for playback (write mode) with ossaudiodev.open(\\"w\\") as dsp_out: dsp_out.setparameters(format, channels, rate) # Play back the modified audio data dsp_out.write(modified_data) except Exception as e: print(f\\"An error occurred: {e}\\")"},{"question":"# Python Coding Assessment Question Objective: Implement a Python class that demonstrates the use of: - Class and instance variables. - Inheritance. - Private variables (name mangling). - Iterator or generator for iteration. # Problem Statement: You are required to create a class `LibraryItem` and its child class `Book` to manage a simple library system. The parent class should use private variables to encapsulate details and provide appropriate methods for data access and modification. Additionally, create an iterator within the `Book` class to iterate over multiple book instances. Specifications: 1. **Class `LibraryItem`:** - **Private Attributes:** - `_title` (string): Title of the library item. - `_author` (string): Author of the library item. - `_year` (integer): Publication year of the library item. - **Public Methods:** - `__init__(self, title, author, year)`: Initializes the title, author, and year. - `get_title(self)`: Returns the title of the item. - `get_author(self)`: Returns the author of the item. - `get_year(self)`: Returns the publication year of the item. - `set_title(self, title)`: Sets the title of the item. - `set_author(self, author)`: Sets the author of the item. - `set_year(self, year)`: Sets the publication year of the item. 2. **Class `Book` (inherits from `LibraryItem`):** - **Additional Attribute:** - `ISBN` (string): ISBN code of the book. - **Override Methods:** - `__init__(self, title, author, year, ISBN)`: Initializes the base class and sets ISBN. - **Public Methods:** - `get_ISBN(self)`: Returns the ISBN of the book. - `set_ISBN(self, ISBN)`: Sets the ISBN of the book. - Implement an iterator to iterate over a list of book instances. 3. **Iterator:** - Define an iterator or generator within the `Book` class to iterate over the list of book instances. Input and Output Format: - You do not need to handle input and output. The aim is to define the class structure as described. - Implement the class methods which are specified. Constraints: - All attributes should use appropriate data types. - Ensure proper encapsulation and data access using private variables. - Make sure to handle the iteration over book instances correctly. Example: Create an example that demonstrates the following: - Create a list of `Book` instances. - Iterate over the list using the defined iterator and print the details of each book. ```python class LibraryItem: # Implement this class class Book(LibraryItem): # Implement this class, including the iterator # Example usage: books = [ Book(\\"The Great Gatsby\\", \\"F. Scott Fitzgerald\\", 1925, \\"9780743273565\\"), Book(\\"1984\\", \\"George Orwell\\", 1949, \\"9780451524935\\"), Book(\\"Moby Dick\\", \\"Herman Melville\\", 1851, \\"9781503280786\\") ] for book in books: print(book.get_title(), book.get_author(), book.get_year(), book.get_ISBN()) ``` Your task is to implement the `LibraryItem` and `Book` classes as described above, ensuring that all specified methods and behaviors are correctly implemented.","solution":"class LibraryItem: def __init__(self, title, author, year): self.__title = title self.__author = author self.__year = year def get_title(self): return self.__title def get_author(self): return self.__author def get_year(self): return self.__year def set_title(self, title): self.__title = title def set_author(self, author): self.__author = author def set_year(self, year): self.__year = year class Book(LibraryItem): def __init__(self, title, author, year, ISBN): super().__init__(title, author, year) self.ISBN = ISBN self._book_list = [] def get_ISBN(self): return self.ISBN def set_ISBN(self, ISBN): self.ISBN = ISBN def __iter__(self): return iter(self._book_list) def add_book(self, book): self._book_list.append(book)"},{"question":"Objective: Use the seaborn library to analyze relationships between variables in a dataset and visualize these relationships using various types of regression plots. Customize the plots to enhance readability and presentation. Task: 1. **Dataset Loading**: - Load the \\"tips\\" dataset using seaborn. 2. **Linear Regression Plot**: - Plot the relationship between \\"total_bill\\" and \\"tip\\" using a simple linear regression. 3. **Polynomial Regression Plot**: - Plot the relationship between \\"total_bill\\" and \\"tip\\", but this time fit a second-order polynomial regression to capture any nonlinear trends. 4. **Log-linear Regression Plot**: - Plot the relationship between \\"total_bill\\" and \\"tip\\", applying a logarithmic transformation to the \\"total_bill\\" axis. 5. **Locally-weighted Scatterplot Smoothing (LOWESS)**: - Plot the relationship between \\"size\\" and \\"tip\\" using a locally-weighted scatterplot smoothing. 6. **Custom Appearance**: - Create a plot for the relationship between \\"total_bill\\" and \\"tip\\", using robust regression. Customize the plot to have: - Red \\"x\\" markers for data points. - A blue regression line. - Disable the confidence interval. Requirements: - Use seaborn functions for plotting. - Customize plot appearance using parameters such as `color`, `marker`, `line_kws`, and `ci`. - Each plot should have appropriate labels for the x and y axes and an appropriate title. Constraints: - Ensure plots are clearly labeled and legible. - The dataset should be loaded within the code. Input: None (data loaded internally within the function). Output: Visualization plots displaying various regression analyses on the \\"tips\\" dataset. ```python import seaborn as sns import matplotlib.pyplot as plt def analyze_and_plot_regressions(): # Load dataset tips = sns.load_dataset(\\"tips\\") # Linear regression plot plt.figure(figsize=(10, 6)) sns.regplot(data=tips, x=\\"total_bill\\", y=\\"tip\\") plt.xlabel(\\"Total Bill\\") plt.ylabel(\\"Tip\\") plt.title(\\"Linear Regression: Total Bill vs Tip\\") plt.show() # Polynomial regression plot plt.figure(figsize=(10, 6)) sns.regplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", order=2) plt.xlabel(\\"Total Bill\\") plt.ylabel(\\"Tip\\") plt.title(\\"Polynomial Regression (Order 2): Total Bill vs Tip\\") plt.show() # Log-linear regression plot plt.figure(figsize=(10, 6)) sns.regplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", logx=True) plt.xlabel(\\"Total Bill (Log Scale)\\") plt.ylabel(\\"Tip\\") plt.title(\\"Log-Linear Regression: Total Bill vs Tip\\") plt.show() # Locally-weighted scatterplot smoothing plt.figure(figsize=(10, 6)) sns.regplot(data=tips, x=\\"size\\", y=\\"tip\\", lowess=True) plt.xlabel(\\"Size\\") plt.ylabel(\\"Tip\\") plt.title(\\"LOWESS Smoothing: Size vs Tip\\") plt.show() # Custom appearance with robust regression plt.figure(figsize=(10, 6)) sns.regplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", robust=True, marker=\\"x\\", line_kws={\\"color\\": \\"blue\\"}) plt.xlabel(\\"Total Bill\\") plt.ylabel(\\"Tip\\") plt.title(\\"Robust Regression with Custom Appearance: Total Bill vs Tip\\") plt.show() # Uncomment the following line to call the function and produce the plots # analyze_and_plot_regressions() ``` Students should implement the function `analyze_and_plot_regressions()` as shown above, generating plots that provide insight into the relationships between different variables in the \\"tips\\" dataset using various regression methods and customizations.","solution":"import seaborn as sns import matplotlib.pyplot as plt def analyze_and_plot_regressions(): # Load dataset tips = sns.load_dataset(\\"tips\\") # Linear regression plot plt.figure(figsize=(10, 6)) sns.regplot(data=tips, x=\\"total_bill\\", y=\\"tip\\") plt.xlabel(\\"Total Bill\\") plt.ylabel(\\"Tip\\") plt.title(\\"Linear Regression: Total Bill vs Tip\\") plt.show() # Polynomial regression plot plt.figure(figsize=(10, 6)) sns.regplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", order=2) plt.xlabel(\\"Total Bill\\") plt.ylabel(\\"Tip\\") plt.title(\\"Polynomial Regression (Order 2): Total Bill vs Tip\\") plt.show() # Log-linear regression plot plt.figure(figsize=(10, 6)) sns.regplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", logx=True) plt.xlabel(\\"Total Bill (Log Scale)\\") plt.ylabel(\\"Tip\\") plt.title(\\"Log-Linear Regression: Total Bill vs Tip\\") plt.show() # Locally-weighted scatterplot smoothing plt.figure(figsize=(10, 6)) sns.regplot(data=tips, x=\\"size\\", y=\\"tip\\", lowess=True) plt.xlabel(\\"Size\\") plt.ylabel(\\"Tip\\") plt.title(\\"LOWESS Smoothing: Size vs Tip\\") plt.show() # Custom appearance with robust regression plt.figure(figsize=(10, 6)) sns.regplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", robust=True, marker=\\"x\\", color=\'red\', line_kws={\\"color\\": \\"blue\\"}, ci=None) plt.xlabel(\\"Total Bill\\") plt.ylabel(\\"Tip\\") plt.title(\\"Robust Regression with Custom Appearance: Total Bill vs Tip\\") plt.show()"},{"question":"**Title:** XML Document Manipulation Using `xml.dom` **Objective:** - To assess the ability to construct and manipulate XML documents using the `xml.dom` module in Python. **Problem Statement:** You are required to write a function `create_xml_document` that constructs an XML document representing a book catalog based on the provided specification. The function should also perform some basic manipulations like adding new books and listing all book titles in the catalog. **Function Signature:** ```python def create_xml_document(): pass ``` # Details: 1. **Creating the XML Document:** - Create an XML document with a root element `<catalog>`. - Add a few book elements to the catalog. Each book element should have the following child elements: - `<title>`: The title of the book. - `<author>`: The author of the book. - `<genre>`: The genre of the book. - `<price>`: The price of the book. - `<publish_date>`: The publish date of the book. 2. **Manipulating the XML Document:** - Add a function `add_book` within `create_xml_document` that takes parameters corresponding to book details (title, author, genre, price, and publish date) and adds a new book element to the catalog. - Add a function `list_books` within `create_xml_document` that returns a list of all book titles in the catalog. 3. **Constraints:** - Your implementation should appropriately handle `xml.dom` exceptions if they arise. - Assume each book element has unique titles. # Example: ```python def create_xml_document(): # Your implementation goes here. # Test the `add_book` functionality: add_book(\\"Sample Book Title\\", \\"Author Name\\", \\"Fiction\\", \\"29.99\\", \\"2023-01-01\\") # List all books and verify: book_titles = list_books() print(book_titles) # Expected output to include \\"Sample Book Title\\". # Example of using the above function create_xml_document() ``` **Note:** - This is an open-ended problem designed to test your understanding of constructing and manipulating XML documents using the `xml.dom` module. Think about data structures, appropriate element creation, and management within the XML document. The task should be submitted with appropriate test cases demonstrating the functionality of `add_book` and `list_books`.","solution":"from xml.dom.minidom import Document def create_xml_document(): # Create a new XML Document doc = Document() # Create the root element \'catalog\' catalog = doc.createElement(\\"catalog\\") doc.appendChild(catalog) def add_book(title, author, genre, price, publish_date): # Create a new book element book = doc.createElement(\\"book\\") # Create and append title element title_element = doc.createElement(\\"title\\") title_text = doc.createTextNode(title) title_element.appendChild(title_text) book.appendChild(title_element) # Create and append author element author_element = doc.createElement(\\"author\\") author_text = doc.createTextNode(author) author_element.appendChild(author_text) book.appendChild(author_element) # Create and append genre element genre_element = doc.createElement(\\"genre\\") genre_text = doc.createTextNode(genre) genre_element.appendChild(genre_text) book.appendChild(genre_element) # Create and append price element price_element = doc.createElement(\\"price\\") price_text = doc.createTextNode(price) price_element.appendChild(price_text) book.appendChild(price_element) # Create and append publish_date element publish_date_element = doc.createElement(\\"publish_date\\") publish_date_text = doc.createTextNode(publish_date) publish_date_element.appendChild(publish_date_text) book.appendChild(publish_date_element) # Append the book to the catalog catalog.appendChild(book) def list_books(): # Retrieve all book titles in the catalog book_titles = [] books = catalog.getElementsByTagName(\\"book\\") for book in books: title_elem = book.getElementsByTagName(\\"title\\")[0] book_titles.append(title_elem.firstChild.data) return book_titles # Adding these functions to be accessible from outside of create_xml_document function. create_xml_document.add_book = add_book create_xml_document.list_books = list_books return doc # Example usage: # doc = create_xml_document() # create_xml_document.add_book(\\"Sample Book Title\\", \\"Author Name\\", \\"Fiction\\", \\"29.99\\", \\"2023-01-01\\") # create_xml_document.list_books()"},{"question":"# Telnet Client Error Handling and Logging Objective Create a robust Telnet client that connects to a server, handles common errors gracefully, logs the interaction, and provides basic automation functionality. Task Write a Python script using the `telnetlib` module that: 1. Connects to a Telnet server using the provided host and port. 2. Reads until a specific login prompt is encountered. 3. Sends a username and waits for a password prompt. 4. Sends a password and waits for a command prompt. 5. Sends a command and captures the output until the end of the session. 6. Implements error handling for common network issues (e.g., timeout, connection dropped). 7. Logs all interactions (sent and received data) to a file. 8. Demonstrates the use of context management to automatically close the connection. Specifications 1. **Input**: - `host` (string): The hostname or IP address of the Telnet server. - `port` (int): The port number of the Telnet server. - `username` (string): The username for login. - `password` (string): The password for login. - `command` (string): The command to be executed after login. - `logfile` (string): Path to the log file where interactions will be recorded. 2. **Output**: - Log file recording all sent and received data. 3. **Constraints**: - Use the `telnetlib` module for the Telnet connection. - Handle `EOFError`, `OSError`, and timeout exceptions. - Ensure the connection is closed even if an error occurs. 4. **Performance**: - Handle network delays gracefully using appropriate timeout settings. Example ```python import telnetlib import logging def telnet_client(host, port, username, password, command, logfile): # Configure logging logging.basicConfig(filename=logfile, level=logging.DEBUG, format=\'%(asctime)s %(message)s\') try: # Establish connection with telnetlib.Telnet(host, port, timeout=10) as tn: logging.info(f\'Connected to {host}:{port}\') # Read until login prompt tn.read_until(b\'login: \') tn.write(username.encode(\'ascii\') + b\'n\') logging.info(f\'Sent username: {username}\') # Wait for password prompt and send password tn.read_until(b\'Password: \') tn.write(password.encode(\'ascii\') + b\'n\') logging.info(\'Sent password\') # Wait for the command prompt and send command tn.read_until(b\' \') tn.write(command.encode(\'ascii\') + b\'n\') logging.info(f\'Sent command: {command}\') # Read the output of the command output = tn.read_all().decode(\'ascii\') logging.info(\'Received output\') print(output) except (EOFError, OSError, TimeoutError) as e: logging.error(f\'Error encountered: {e}\') print(f\'Error: {e}\') # Example usage: telnet_client(\'localhost\', 23, \'user\', \'password\', \'ls\', \'telnet_log.txt\') ``` Notes - The above script sets up a Telnet session, handles possible errors, logs the interaction, and ensures the connection is properly closed using a context manager. - Ensure the script captures and logs all interaction with the Telnet server to allow for debugging and audit trails.","solution":"import telnetlib import logging def telnet_client(host, port, username, password, command, logfile): # Configure logging logging.basicConfig(filename=logfile, level=logging.DEBUG, format=\'%(asctime)s - %(message)s\') try: # Establish connection with telnetlib.Telnet(host, port, timeout=10) as tn: logging.info(f\'Connected to {host}:{port}\') # Read until login prompt tn.read_until(b\'login: \') tn.write(username.encode(\'ascii\') + b\'n\') logging.info(f\'Sent username: {username}\') # Wait for password prompt and send password tn.read_until(b\'Password: \') tn.write(password.encode(\'ascii\') + b\'n\') logging.info(\'Sent password\') # Wait for the command prompt and send command tn.read_until(b\' \') tn.write(command.encode(\'ascii\') + b\'n\') logging.info(f\'Sent command: {command}\') # Read the output of the command output = tn.read_all().decode(\'ascii\') logging.info(f\'Received output: {output}\') return output except (EOFError, OSError, TimeoutError) as e: logging.error(f\'Error encountered: {e}\') return str(e) # Example usage: # output = telnet_client(\'localhost\', 23, \'user\', \'password\', \'ls\', \'telnet_log.txt\') # print(output)"},{"question":"# Sparse Data Handling with Pandas In this exercise, you will work with pandas\' sparse data structures. You are given a large dataset containing numerous missing (NaN) or repetitive values, and your task is to process this data efficiently using the sparse data structures provided by pandas. Task: 1. **Data Preparation**: - Create a DataFrame of shape (10000, 10) filled with random values using `numpy.random.randn`. - Introduce `NaN` values randomly to have approximately 90% sparsity in the DataFrame. - Convert this DataFrame to a sparse DataFrame using `pd.SparseDtype`. 2. **Memory Usage Analysis**: - Calculate and print the memory usage of both the dense and sparse DataFrames. 3. **Sparse Calculations**: - Apply the numpy absolute value function (`np.abs()`) to the sparse DataFrame. - Convert the resulting DataFrame back to its dense form and print the result. 4. **Conversion with scipy.sparse**: - Convert the sparse DataFrame to a scipy sparse matrix. - Convert the scipy sparse matrix back to a pandas sparse DataFrame and print the first 5 rows. Expected Input and Output Formats: - **Input**: You don\'t need to provide any specific input from the user. You will generate the data within the program. - **Output**: - Print the memory usage of the dense and sparse DataFrames. - Print the dense DataFrame obtained after applying the numpy absolute value function. - Print the first 5 rows of the pandas sparse DataFrame converted from a scipy sparse matrix. Constraints: - Use `numpy` and `pandas` libraries for data generation and manipulation. - Ensure that the sparsity in the DataFrame is approximately 90%. Example: ```python import numpy as np import pandas as pd from scipy.sparse import csr_matrix # Step 1: Data Preparation np.random.seed(0) df = pd.DataFrame(np.random.randn(10000, 10)) mask = np.random.choice([1, 0], df.shape, p=[.9, .1]) df = df.where(mask == 0) # Convert to sparse DataFrame sparse_df = df.astype(pd.SparseDtype(\\"float\\", np.nan)) # Step 2: Memory Usage Analysis dense_memory = df.memory_usage().sum() / 1e3 sparse_memory = sparse_df.memory_usage().sum() / 1e3 print(f\\"Dense Memory Usage: {dense_memory:.2f} KB\\") print(f\\"Sparse Memory Usage: {sparse_memory:.2f} KB\\") # Step 3: Sparse Calculations abs_sparse_df = np.abs(sparse_df) abs_dense_df = abs_sparse_df.sparse.to_dense() print(abs_dense_df) # Step 4: Conversion with scipy.sparse sp_matrix = csr_matrix(sparse_df.to_numpy()) sdf_from_sp = pd.DataFrame.sparse.from_spmatrix(sp_matrix) print(sdf_from_sp.head()) ```","solution":"import numpy as np import pandas as pd from scipy.sparse import csr_matrix def handle_sparse_data(): # Step 1: Data Preparation np.random.seed(0) df = pd.DataFrame(np.random.randn(10000, 10)) mask = np.random.choice([1, 0], df.shape, p=[.9, .1]) df = df.where(mask == 0) # Convert to sparse DataFrame sparse_df = df.astype(pd.SparseDtype(\\"float\\", np.nan)) # Step 2: Memory Usage Analysis dense_memory = df.memory_usage().sum() / 1e3 sparse_memory = sparse_df.memory_usage().sum() / 1e3 print(f\\"Dense Memory Usage: {dense_memory:.2f} KB\\") print(f\\"Sparse Memory Usage: {sparse_memory:.2f} KB\\") # Step 3: Sparse Calculations abs_sparse_df = np.abs(sparse_df) abs_dense_df = abs_sparse_df.sparse.to_dense() print(abs_dense_df) # Step 4: Conversion with scipy.sparse sp_matrix = csr_matrix(sparse_df.to_numpy()) sdf_from_sp = pd.DataFrame.sparse.from_spmatrix(sp_matrix) print(sdf_from_sp.head()) return dense_memory, sparse_memory, abs_dense_df, sdf_from_sp.head()"},{"question":"# Configuration File Extractor You have been given several INI configuration files with a variety of sections and options. Your task involves creating a script that: 1. Reads multiple configuration files. 2. Extracts specific data based on given criteria. 3. Provides fallback values for missing options. 4. Handles different data types and performs interpolations. 5. Customizes parser behavior for certain use cases. Input Specifications: 1. **config_files**: A list of file paths to configuration files (strings). 2. **sections_options**: A dictionary where: - Keys are section names for which you have to extract data. - Values are lists of tuples where each tuple contains an option name and its fallback value. Output Specifications: Return a dictionary with the following structure: - Keys are the section names. - Values are dictionaries where keys are the option names and values are the corresponding fetched option values. If an option is not present and no fallback value is provided, it should be `None`. Constraints: - Sections and options are not case-sensitive. - If an option exists in multiple files, the value from the last file read should take precedence. - Handle errors gracefully and ensure that missing sections or options do not break the execution. Performance Requirements: - Efficiently read and process multiple configuration files. - Appropriate use of fallback and interpolation features. Example: Consider `config1.ini`: ```ini [DEFAULT] ServerAliveInterval = 45 [server] Port = 50022 ForwardX11 = no ``` And `config2.ini`: ```ini [server] Port = 4040 Compression = yes ``` And calling the function with: ```python config_files = [\'config1.ini\', \'config2.ini\'] sections_options = { \'server\': [(\'Port\', \'8080\'), (\'Compression\', \'no\'), (\'ForwardX11\', \'yes\')], } ``` The function should return: ```python { \'server\': { \'Port\': \'4040\', \'Compression\': \'yes\', \'ForwardX11\': \'no\' } } ``` Function Signature: ```python def extract_config_data(config_files: list, sections_options: dict) -> dict: pass ``` Note: - Use `configparser.ConfigParser` to implement this task. - Properly handle the interpolation, fallback values, and different data types as necessary.","solution":"import configparser def extract_config_data(config_files, sections_options): config = configparser.ConfigParser() config.optionxform = str # Make option names case sensitive # Read the config files for file in config_files: config.read(file) # Extract data based on given criteria result = {} for section, options in sections_options.items(): section_data = {} for option, fallback in options: if config.has_option(section, option): section_data[option] = config.get(section, option) else: section_data[option] = fallback result[section] = section_data return result"},{"question":"Objective Create a Python function that utilizes PyTorch\'s FX to generate a computational graph for a simple neural network model, modify the graph by adding an additional operation, and then convert the modified graph back to a PyTorch model. Task 1. **Define a Simple Neural Network Model**: - Create a simple feed-forward neural network model (`SimpleNN`) using `torch.nn.Module`. - The network should consist of two linear layers with ReLU activation in between. 2. **Generate FX Graph**: - Use `torch.fx.symbolic_trace` to convert the `SimpleNN` model into an FX computational graph. 3. **Modify the FX Graph**: - Add a new operation that scales the output of the second linear layer by a constant factor (let\'s say 2.0). This requires inserting a new `call_function` node. 4. **Convert Modified Graph Back to Model**: - Convert the modified FX graph back to a PyTorch model. 5. **Test the Modified Model**: - Define an input tensor and pass it through both the original and modified models. Ensure the output of the modified model is correctly scaled by the factor added. Specifications - The `SimpleNN` class should be defined as described. - You must use the FX module to create and manipulate the computational graph. - Verify the modified model functionality by comparing outputs. Sample Code Structure ```python import torch from torch import nn from torch.fx import symbolic_trace class SimpleNN(nn.Module): def __init__(self, input_size, hidden_size, output_size): super(SimpleNN, self).__init__() self.fc1 = nn.Linear(input_size, hidden_size) self.fc2 = nn.Linear(hidden_size, output_size) self.relu = nn.ReLU() def forward(self, x): x = self.fc1(x) x = self.relu(x) x = self.fc2(x) return x def modify_fx_graph(graph_module): # Code to modify the FX graph goes here pass def main(): # Create an instance of the SimpleNN model input_size, hidden_size, output_size = 10, 20, 1 model = SimpleNN(input_size, hidden_size, output_size) # Generate the initial FX graph graph_module = symbolic_trace(model) print(f\\"Original graph:n{graph_module}\\") # Modify the FX graph by adding a scaling operation modified_graph_module = modify_fx_graph(graph_module) print(f\\"Modified graph:n{modified_graph_module}\\") # Compare the output of the original and modified models x = torch.randn(1, input_size) original_output = graph_module(x) modified_output = modified_graph_module(x) print(f\\"Original Output: {original_output}\\") print(f\\"Modified Output: {modified_output}\\") if __name__ == \\"__main__\\": main() ``` What You Need to Implement 1. Complete the `SimpleNN` class definition. 2. Implement the `modify_fx_graph` function to add the scaling operation to the FX graph. 3. Ensure the modified graph can be converted back to a PyTorch model and tested. Constraints - Use only the libraries `torch` and `torch.fx`. - Ensure the modifications to the graph are functional and do not break the model execution. Expected Outcome The successful implementation should demonstrate: - The ability to create and modify an FX graph. - The correct implementation of adding an operation to the graph. - The original and modified models producing expected outputs with the modified model\'s output scaled correctly.","solution":"import torch from torch import nn from torch.fx import symbolic_trace, GraphModule class SimpleNN(nn.Module): def __init__(self, input_size, hidden_size, output_size): super(SimpleNN, self).__init__() self.fc1 = nn.Linear(input_size, hidden_size) self.fc2 = nn.Linear(hidden_size, output_size) self.relu = nn.ReLU() def forward(self, x): x = self.fc1(x) x = self.relu(x) x = self.fc2(x) return x def modify_fx_graph(graph_module): graph = graph_module.graph for node in graph.nodes: if node.op == \'output\': with graph.inserting_before(node): new_node = graph.create_node(\'call_function\', torch.mul, (node.args[0], 2.0), {}) node.replace_input_with(node.args[0], new_node) graph_module.recompile() return graph_module # Example main function if you want to test it manually (not part of final solution) def main(): input_size, hidden_size, output_size = 10, 20, 1 model = SimpleNN(input_size, hidden_size, output_size) # Generate initial FX graph graph_module = symbolic_trace(model) # Modify the FX graph modified_graph_module = modify_fx_graph(graph_module) # Compare original and modified model outputs x = torch.randn(1, input_size) with torch.no_grad(): original_output = model(x) modified_output = modified_graph_module(x) print(f\\"Original Output: {original_output}\\") print(f\\"Modified Output: {modified_output}\\") if __name__ == \\"__main__\\": main()"},{"question":"# Multi-threaded Temperature Logger As part of a climate monitoring system, you need to implement a temperature logger that records temperature readings from different sensors concurrently. Each sensor runs in its own thread and writes temperature data to a shared log file. To ensure that the log file is not corrupted due to concurrent writes, you need to use a lock to synchronize access to the file. Task Implement a multi-threaded temperature logging system using the `_thread` module. Your implementation should meet the following requirements: 1. Define a function `log_temperature(sensor_id, log_file, lock)`: - `sensor_id`: An integer representing the unique sensor ID. - `log_file`: The file object for the log file. - `lock`: A lock object for synchronizing write access to the log file. - This function should simulate a sensor reading by getting the current temperature (you can use a random temperature for simulation). - Write the sensor ID and temperature reading to the log file in the format: `Sensor {sensor_id}: {temperature}n`. - Ensure that the write operation is thread-safe using the provided lock. 2. Define a main function `start_logging(sensor_count, log_filename)`: - `sensor_count`: The number of sensor threads to start. - `log_filename`: The name of the log file to which temperature readings will be written. - This function should: - Create a lock object using `_thread.allocate_lock()`. - Open the log file in append mode. - Start `sensor_count` threads, each executing the `log_temperature` function with unique sensor IDs. - Wait for all threads to complete their execution. You need to handle any exceptions that may occur and ensure the log file is properly closed after all threads have finished writing. Constraints - Assume `sensor_count` will be a positive integer less than or equal to 50. - The log file should be opened and closed correctly to ensure data integrity and resource management. - You can simulate the temperature reading with a random float value between -30.0 and 50.0. Example Usage ```python import random import _thread import time def log_temperature(sensor_id, log_file, lock): # Implement the function as described def start_logging(sensor_count, log_filename): # Implement the function as described # Example usage start_logging(5, \'temperature_log.txt\') ``` In this example, five sensor threads will concurrently write temperature readings to `temperature_log.txt`.","solution":"import random import _thread import time def log_temperature(sensor_id, log_file, lock): Simulate a sensor reading and log it to the log file in a thread-safe manner. temperature = round(random.uniform(-30.0, 50.0), 2) with lock: log_file.write(f\\"Sensor {sensor_id}: {temperature}n\\") def start_logging(sensor_count, log_filename): Start multiple sensor threads to log temperatures concurrently. lock = _thread.allocate_lock() def start_sensor_thread(sensor_id): with open(log_filename, \'a\') as log_file: log_temperature(sensor_id, log_file, lock) threads = [] for sensor_id in range(1, sensor_count + 1): thread = _thread.start_new_thread(start_sensor_thread, (sensor_id,)) threads.append(thread) # Give threads time to finish their execution time.sleep(1)"},{"question":"# Question: XML Document Summarizer You are required to implement a function that takes an XML document as a string and returns a summary of its structure and content using the `xml.parsers.expat` module. Specifically, your function should summarize elements, attributes, and text content. Function Signature ```python def xml_document_summarizer(xml_content: str) -> dict: pass ``` Input - `xml_content` (str): A string representing the XML document to be parsed. Output - Returns a dictionary with the following structure: ```python { \\"elements\\": List[str], # List of element names in the order they appear \\"attributes\\": List[Tuple[str, Dict[str, str]]], # List of tuples: each containing element name and its attributes \\"text_content\\": List[str], # List of text content in the order they appear } ``` Example ```python xml_content = <root> <child1 name=\\"Jon\\">Some text here</child1> <child2 name=\\"Doe\\">Other text</child2> </root> summary = xml_document_summarizer(xml_content) print(summary) ``` Expected Output: ```python { \\"elements\\": [\\"root\\", \\"child1\\", \\"child2\\"], \\"attributes\\": [(\\"child1\\", {\\"name\\": \\"Jon\\"}), (\\"child2\\", {\\"name\\": \\"Doe\\"})], \\"text_content\\": [\\"Some text here\\", \\"Other text\\"] } ``` Constraints - The XML content can have nested elements. - The attribute values and text content should be collected and returned in the order they appear in the document. - Ignore any comments or processing instructions. - Handle empty elements gracefully. Instructions 1. Use `xml.parsers.expat.ParserCreate()` to create an XML parser object. 2. Define handler functions for: - Start of an element (`StartElementHandler`): to capture element names and attributes. - End of an element (`EndElementHandler`): to capture element closing. - Character data (`CharacterDataHandler`): to capture text content. 3. Set these handler functions on the XML parser object. 4. Parse the XML content and construct the summary dictionary as detailed. 5. Implement exception handling for parsing errors and return an appropriate message or empty dictionary if the input is not valid XML. Use this problem to demonstrate your ability to work with XML parsing, event-driven programming with handlers, and data collection and structuring using the `xml.parsers.expat` module.","solution":"import xml.parsers.expat def xml_document_summarizer(xml_content: str) -> dict: summary = { \\"elements\\": [], \\"attributes\\": [], \\"text_content\\": [] } def start_element(name, attrs): summary[\\"elements\\"].append(name) if attrs: summary[\\"attributes\\"].append((name, dict(attrs))) def end_element(name): pass def char_data(data): if data.strip(): summary[\\"text_content\\"].append(data.strip()) parser = xml.parsers.expat.ParserCreate() parser.StartElementHandler = start_element parser.EndElementHandler = end_element parser.CharacterDataHandler = char_data try: parser.Parse(xml_content) except xml.parsers.expat.ExpatError: return {\\"error\\": \\"Invalid XML\\"} return summary"},{"question":"**Python Version Decoder** Python uses a set of macros to represent its version information, encoded in a hexadecimal integer. Your task is to write a Python function that decodes this integer and returns the detailed version information. # Function Signature ```python def decode_python_version(hex_version: int) -> str: pass ``` # Input - `hex_version` (int): A hexadecimal integer representing the Python version. Constraints: `0x00000000 <= hex_version <= 0xFFFFFFFF`. # Output - `version_str` (str): A string representing the decoded Python version information in the format `\\"<major>.<minor>.<micro><level><serial>\\"`. # Constraints - The level should be represented as follows: - \'a\' for alpha (hex value 0xA) - \'b\' for beta (hex value 0xB) - \'rc\' for release candidate (hex value 0xC) - \'\' (an empty string) for final (hex value 0xF) # Examples Example 1 ```python input_hex = 0x030401a2 output_str = \\"3.4.1a2\\" assert decode_python_version(input_hex) == output_str ``` Example 2 ```python input_hex = 0x030a00f0 output_str = \\"3.10.0\\" assert decode_python_version(input_hex) == output_str ``` # Additional Information The hex_version integer encodes the version information as follows: - The most significant byte (bits 1-8) corresponds to the major version. - The second byte (bits 9-16) corresponds to the minor version. - The third byte (bits 17-24) corresponds to the micro version. - The fourth byte has two parts: - The first 4 bits (bits 25-28) correspond to the release level. - The last 4 bits (bits 29-32) correspond to the release serial. Use this information to decode the version accurately. # Notes - The final function should handle any valid hexadecimal version code and return the correct string representation. - Ensure the function is efficient and handles edge cases gracefully.","solution":"def decode_python_version(hex_version: int) -> str: Decodes a hexadecimal integer representing the Python version and returns the version information as a string. major = (hex_version >> 24) & 0xFF minor = (hex_version >> 16) & 0xFF micro = (hex_version >> 8) & 0xFF level_num = (hex_version >> 4) & 0xF serial = hex_version & 0xF level_dict = {0xA: \'a\', 0xB: \'b\', 0xC: \'rc\', 0xF: \'\'} level = level_dict.get(level_num, \'\') if level: version_str = f\\"{major}.{minor}.{micro}{level}{serial}\\" else: version_str = f\\"{major}.{minor}.{micro}\\" return version_str"},{"question":"Introduction You are required to write a function that processes path configuration files (`.pth` files) for a Python installation. Your task is to create a function that reads a given `.pth` file and updates the Python module search path (`sys.path`). This function will ensure that only valid and unique paths are added to `sys.path`. Requirements 1. The function should read a specified `.pth` file. 2. Ignore blank lines and lines starting with `#`. 3. Any line starting with `import` should be executed. 4. Non-existing paths should be ignored. 5. No path should be added to `sys.path` more than once. Function Signature ```python import sys def process_pth_file(pth_file_path: str) -> None: Reads the specified .pth file and updates sys.path according to its contents. :param pth_file_path: The path to the .pth file to be processed. pass ``` Input - `pth_file_path`: A string representing the file path to a `.pth` file (e.g., `\\"/usr/local/lib/python3.10/site-packages/foo.pth\\"`). Constraints - The `.pth` file can be assumed to be properly formatted. - The file paths specified within the `.pth` file can be either relative or absolute. Example Consider a `.pth` file with the following content: ``` # This is a comment ./relative_path /usr/local/lib/python3.10/site-packages/existing_path /non/existing/path import os # Another comment ``` Given this file, the function should: 1. Add `./relative_path` and `/usr/local/lib/python3.10/site-packages/existing_path` to `sys.path`, ensuring these directories exist. 2. Execute `import os`. Notes - You can use the `os.path.exists` method to check if a path exists. - This function should directly modify `sys.path`. Sample Test Case ```python import os import sys def test_process_pth_file(): # Setup: create a dummy .pth file pth_content = # This is a comment ./relative_path /usr/local/lib/python3.10/site-packages/existing_path /non/existing/path import os # This import statement should be executed # Another comment pth_file_path = \\"test.pth\\" with open(pth_file_path, \\"w\\") as f: f.write(pth_content) # Create the directories for the test os.makedirs(\\"./relative_path\\", exist_ok=True) os.makedirs(\\"/usr/local/lib/python3.10/site-packages/existing_path\\", exist_ok=True) # Call the function to process the .pth file process_pth_file(pth_file_path) # Check the result assert \\"./relative_path\\" in sys.path assert \\"/usr/local/lib/python3.10/site-packages/existing_path\\" in sys.path assert \\"/non/existing/path\\" not in sys.path # Cleanup os.remove(pth_file_path) os.rmdir(\\"./relative_path\\") os.rmdir(\\"/usr/local/lib/python3.10/site-packages/existing_path\\") # Run the sample test case test_process_pth_file() ```","solution":"import os import sys def process_pth_file(pth_file_path: str) -> None: Reads the specified .pth file and updates sys.path according to its contents. :param pth_file_path: The path to the .pth file to be processed. if not os.path.isfile(pth_file_path): raise FileNotFoundError(f\\"The file {pth_file_path} does not exist.\\") with open(pth_file_path, \'r\') as f: for line in f: stripped_line = line.strip() if not stripped_line or stripped_line.startswith(\'#\'): continue elif stripped_line.startswith(\'import\'): exec(stripped_line) elif os.path.exists(stripped_line): if stripped_line not in sys.path: sys.path.append(stripped_line)"},{"question":"# PyTorch Coding Assessment Objective: To test your understanding of PyTorch tensor attributes, device management, and memory layout, you will implement a function that performs multiple tasks using these concepts. Problem Description: Write a function `transform_and_operate` that accepts the following arguments: - `tensor1` (torch.Tensor): The first input tensor. - `tensor2` (torch.Tensor): The second input tensor. - `operation` (str): A string specifying the arithmetic operation to perform. It must be one of \'add\', \'subtract\', \'multiply\', \'divide\'. - `target_dtype` (torch.dtype): The target data type to which both input tensors should be cast before performing the operation. - `device` (torch.device): The device on which the resultant tensor should be allocated. - `memory_format` (torch.memory_format): The memory format to ensure for the resultant tensor. The function should perform the following steps: 1. Convert both `tensor1` and `tensor2` to the specified `target_dtype`. 2. Move both tensors to the specified device. 3. Perform the specified arithmetic operation (either \'add\', \'subtract\', \'multiply\', or \'divide\') on the tensors. 4. Ensure the resultant tensor follows the specified `memory_format`. 5. Return the resultant tensor. Constraints: - Raise a `ValueError` if the `operation` argument is not one of the specified operations. - Raise a `TypeError` if the `target_dtype` is not a valid `torch.dtype`. - Ensure that no integral output tensor gets a floating point result, and no boolean output tensor gets a non-boolean result. Performance Requirements: - The solution should handle tensors with large dimensions efficiently, ensuring proper memory and device handling. Function Signature: ```python import torch def transform_and_operate( tensor1: torch.Tensor, tensor2: torch.Tensor, operation: str, target_dtype: torch.dtype, device: torch.device, memory_format: torch.memory_format = torch.contiguous_format ) -> torch.Tensor: pass ``` Example: ```python import torch # Example usage of `transform_and_operate` function. tensor1 = torch.tensor([1.0, 2.0, 3.0]) tensor2 = torch.tensor([4.0, 5.0, 6.0]) operation = \'add\' target_dtype = torch.float32 device = torch.device(\'cuda:0\') memory_format = torch.channels_last result_tensor = transform_and_operate(tensor1, tensor2, operation, target_dtype, device, memory_format) print(result_tensor) print(result_tensor.dtype) print(result_tensor.device) print(result_tensor.is_contiguous(memory_format=torch.channels_last)) ``` In this example, `result_tensor` should be the sum of `tensor1` and `tensor2`, cast to `torch.float32`, allocated on CUDA device `0`, and confirmed to use channels-last memory format.","solution":"import torch def transform_and_operate( tensor1: torch.Tensor, tensor2: torch.Tensor, operation: str, target_dtype: torch.dtype, device: torch.device, memory_format: torch.memory_format = torch.contiguous_format ) -> torch.Tensor: # Check if operation is valid if operation not in [\'add\', \'subtract\', \'multiply\', \'divide\']: raise ValueError(f\\"Invalid operation \'{operation}\'. Must be one of \'add\', \'subtract\', \'multiply\', \'divide\'.\\") # Check if target_dtype is valid if not isinstance(target_dtype, torch.dtype): raise TypeError(f\\"Invalid target_dtype \'{target_dtype}\'. Must be a valid torch.dtype.\\") # Convert tensors to target_dtype and move to the specified device tensor1 = tensor1.to(dtype=target_dtype, device=device) tensor2 = tensor2.to(dtype=target_dtype, device=device) # Perform the specified operation if operation == \'add\': result = tensor1 + tensor2 elif operation == \'subtract\': result = tensor1 - tensor2 elif operation == \'multiply\': result = tensor1 * tensor2 elif operation == \'divide\': result = tensor1 / tensor2 # Ensure the resultant tensor follows the specified memory format result = result.to(memory_format=memory_format) return result"},{"question":"# Customizing String Formatting with Subclassing Formatter You are to implement a custom string formatter by subclassing the `string.Formatter` class. Your task is to add functionality to format numbers to include a suffix that represents large numbers more succinctly. For instance: - `1000` should be formatted as `\\"1K\\"` - `1500000` should be formatted as `\\"1.5M\\"` Your subclass should: 1. Override the `format_field` method to append the appropriate suffix (`K`, `M`, `B` for thousand, million, and billion respectively). 2. Properly handle floating-point precision up to one decimal place if required. You are required to implement a class `CustomFormatter` that subclasses `string.Formatter` and overrides the `format_field` method to achieve this. Your implementation should support formatting both integers and floats with the custom suffix. # Input The input will not be read from stdin. Instead, you need to write code that demonstrates the functionality of your class within the script. # Output Print formatted representations of the following numbers: - `1200` - `534000` - `2300000` - `7800000000` - `1500.56` - `967000.42` - `3200000000.99` # Sample Code Structure ```python import string class CustomFormatter(string.Formatter): def format_field(self, value, format_spec): # Custom implementation goes here # Call the base class method to handle default formatting return super().format_field(value, format_spec) # Test cases tests = [ 1200, 534000, 2300000, 7800000000, 1500.56, 967000.42, 3200000000.99 ] formatter = CustomFormatter() for test in tests: print(formatter.format(\\"{:custom_format}\\", test)) ``` **Constraints**: - The implementation should handle formatting for values less than `1000` without adding any suffix. - Assume the values will be within the range of `0` to `1e12`. - Floating-point numbers should maintain one decimal place if they are subject to formatting. **Requirements**: - You should use subclassing and overriding methods as described. - Properly comment your code to explain the logic.","solution":"import string class CustomFormatter(string.Formatter): def format_field(self, value, format_spec): # Handle custom formatting for large numbers with suffixes K, M, B if format_spec == \'custom_format\': if isinstance(value, (int, float)): if value >= 1e9: return f\\"{value / 1e9:.1f}B\\" elif value >= 1e6: return f\\"{value / 1e6:.1f}M\\" elif value >= 1e3: return f\\"{value / 1e3:.1f}K\\" else: return f\\"{value:.0f}\\" # Default formatting if \'custom_format\' is not specified return super().format_field(value, format_spec) # Example usage tests = [ 1200, 534000, 2300000, 7800000000, 1500.56, 967000.42, 3200000000.99 ] formatter = CustomFormatter() for test in tests: print(formatter.format(\\"{:custom_format}\\", test))"},{"question":"Objective: To assess your understanding of file path handling and error management in Python. Problem Statement: You are required to implement a function that takes a list of file paths and returns a list of their file system representations. If any path in the input list does not meet the conditions described in the documentation when converting to a file system representation, the function should raise a `TypeError`. Function Signature: ```python def convert_paths_to_fs_representation(paths: list) -> list: pass ``` Input: - `paths` (list): A list of file paths. Each element can be a `str`, `bytes`, or an object implementing the `os.PathLike` interface. Output: - Returns a list of the file system representations of the paths. Each element in the output list will be either a `str` or `bytes`. Constraints: - If an object in the `paths` list is not a `str`, `bytes`, or does not implement the `os.PathLike` interface properly (i.e., returns neither `str` nor `bytes` from its `__fspath__()` method), the function should raise a `TypeError`. Example: ```python paths = [\'/home/user/file.txt\', b\'/home/user/file2.txt\', YourPathLikeObject(\'/home/user/dir\')] print(convert_paths_to_fs_representation(paths)) ``` Expected Output (assuming `YourPathLikeObject` is implemented correctly): ```python [\'/home/user/file.txt\', b\'/home/user/file2.txt\', \'/home/user/dir\'] ``` Notes: 1. Implement functionality using the `os.PathLike` interface to ensure compatibility with various path-like objects. 2. You may assume that any custom path-like objects implement the `__fspath__()` method correctly if their use results in a `str` or `bytes` being returned. 3. Pay attention to handling different types and raising `TypeError` where appropriate as described in the documentation. Write a solution that meets the above requirements and passes all necessary test cases.","solution":"import os def convert_paths_to_fs_representation(paths): Converts a list of file paths to their file system representations. Args: paths (list): A list of file paths. Each element can be a str, bytes, or an object implementing the os.PathLike interface. Returns: list: A list of the file system representations of the paths. Raises: TypeError: If any path in the input list is not a str, bytes, or does not implement the os.PathLike interface. result = [] for path in paths: if isinstance(path, (str, bytes)): result.append(path) elif hasattr(path, \'__fspath__\'): fs_path = path.__fspath__() if isinstance(fs_path, (str, bytes)): result.append(fs_path) else: raise TypeError(f\\"__fspath__() should return str or bytes, not {type(fs_path)}\\") else: raise TypeError(f\\"Path should be of type str, bytes, or os.PathLike, not {type(path)}\\") return result"},{"question":"You are tasked with creating a utility that can import and execute Python modules from a given ZIP archive. Your utility should be able to handle both `.py` and `.pyc` files. Specifically, you will need to implement a function to find, load, and execute a module from a specified ZIP archive. # Requirements: 1. Implement a function `find_and_exec_module_from_zip` that takes two parameters: - `zip_path`: A string representing the path to the ZIP archive. - `module_name`: A string representing the fully qualified name of the module to be executed. 2. Your function should: - Add the given ZIP path to `sys.path`. - Utilize the `zipimport` module to find and load the specified module from the ZIP file. - Execute the loaded module and return the module object. 3. Ensure that your function raises an appropriate exception if the ZIP file is invalid or if the module cannot be found or loaded. # Constraints: - The given ZIP file will contain the required module and no other files except `.py` and `.pyc` files. - You should handle exceptions gracefully and raise a `zipimport.ZipImportError` with a suitable message if something goes wrong. # Example: ```python def find_and_exec_module_from_zip(zip_path: str, module_name: str): # Your implementation here pass # Example usage: # Assuming \'example.zip\' contains a Python module named \'sample_module\' module = find_and_exec_module_from_zip(\'example.zip\', \'sample_module\') print(module) ``` Expected Output: If the module `sample_module` exists in the provided `example.zip` archive, it gets imported and executed, and the module object is returned. In case of any error (like an invalid ZIP file or non-existent module), an appropriate `zipimport.ZipImportError` should be raised.","solution":"import sys import zipimport def find_and_exec_module_from_zip(zip_path: str, module_name: str): Finds, loads, and executes a module from a given ZIP archive. Parameters: - zip_path: Path to the ZIP archive. - module_name: Fully qualified name of the module to be executed. Returns: - The module object. Raises: - zipimport.ZipImportError: If the module cannot be loaded or the ZIP file is invalid. try: # Add ZIP path to sys.path sys.path.append(zip_path) # Create a zipimporter object for the given ZIP file importer = zipimport.zipimporter(zip_path) # Load the specified module module = importer.load_module(module_name) # Execute the module (this happens upon loading in Python) return module except Exception as e: raise zipimport.ZipImportError(f\\"Error loading module \'{module_name}\' from \'{zip_path}\': {e}\\")"},{"question":"**Background**: You are tasked with performing an exploratory data analysis on the Titanic dataset to understand the distribution and relationships of various features. You will use seaborn to create visualizations that help reveal these insights. The Titanic dataset contains the following relevant columns: - `survived`: Whether the passenger survived (0 = No, 1 = Yes). - `pclass`: Passenger class (1 = 1st, 2 = 2nd, 3 = 3rd). - `sex`: Sex of the passenger (male or female). - `age`: Age of the passenger. - `sibsp`: Number of siblings/spouses aboard the Titanic. - `parch`: Number of parents/children aboard the Titanic. - `fare`: Passenger fare. - `embarked`: Port of embarkation (C = Cherbourg, Q = Queenstown, S = Southampton). **Task**: 1. **Count of passengers by class**: Create a count plot showing the number of passengers in each class. Use the `sns.countplot` function. 2. **Survival count by class**: Create a count plot showing the number of survivors and non-survivors for each passenger class. 3. **Percentage of survival by class and sex**: Modify the count plot to display the percentage of survival within each class, broken down by sex. Normalize the count plot to show percentages instead of raw counts. **Details**: - You should use seaborn for all visualizations. - Ensure each plot includes appropriate titles and labels for axes. - Your final plot (task 3) should show clear distinctions between male and female passengers within each class, with percentages of survivors and non-survivors. **Inputs and Outputs**: - There are no specific inputs; the function should use the default Titanic dataset provided by seaborn. - The outputs should be plots displayed using seaborn\'s `sns.countplot` function with appropriate modifications for each task. **Constraints**: - Assume seaborn and other relevant libraries (e.g., pandas, matplotlib) are pre-installed and available for import. - Ensure the plots are visually clear and well-labeled. **Code Implementation**: ```python import seaborn as sns import matplotlib.pyplot as plt # Setting the theme sns.set_theme(style=\\"whitegrid\\") # Loading the Titanic dataset titanic = sns.load_dataset(\\"titanic\\") def plot_titanic_data(): # Task 1: Count of passengers by class plt.figure(figsize=(10, 6)) sns.countplot(data=titanic, x=\\"class\\") plt.title(\'Number of Passengers by Class\') plt.xlabel(\'Passenger Class\') plt.ylabel(\'Count\') plt.show() # Task 2: Survival count by class plt.figure(figsize=(10, 6)) sns.countplot(data=titanic, x=\\"class\\", hue=\\"survived\\") plt.title(\'Survival Count by Class\') plt.xlabel(\'Passenger Class\') plt.ylabel(\'Count\') plt.legend(title=\'Survived\', loc=\'upper right\', labels=[\'No\', \'Yes\']) plt.show() # Task 3: Percentage of survival by class and sex plt.figure(figsize=(10, 6)) sns.countplot(data=titanic, x=\\"class\\", hue=\\"survived\\", stat=\\"percent\\", hue_order=[0, 1]) plt.title(\'Percentage of Survival by Class and Sex\') plt.xlabel(\'Passenger Class\') plt.ylabel(\'Percentage\') plt.legend(title=\'Survived\', loc=\'upper right\', labels=[\'No\', \'Yes\']) plt.show() # Call the function to generate the plots plot_titanic_data() ```","solution":"import seaborn as sns import matplotlib.pyplot as plt # Setting the theme sns.set_theme(style=\\"whitegrid\\") # Loading the Titanic dataset titanic = sns.load_dataset(\\"titanic\\") def plot_titanic_data(): # Task 1: Count of passengers by class plt.figure(figsize=(10, 6)) sns.countplot(data=titanic, x=\\"pclass\\") plt.title(\'Number of Passengers by Class\') plt.xlabel(\'Passenger Class\') plt.ylabel(\'Count\') plt.show() # Task 2: Survival count by class plt.figure(figsize=(10, 6)) sns.countplot(data=titanic, x=\\"pclass\\", hue=\\"survived\\") plt.title(\'Survival Count by Class\') plt.xlabel(\'Passenger Class\') plt.ylabel(\'Count\') plt.legend(title=\'Survived\', loc=\'upper right\', labels=[\'No\', \'Yes\']) plt.show() # Task 3: Percentage of survival by class and sex plt.figure(figsize=(10, 6)) g = sns.catplot(data=titanic, x=\\"pclass\\", hue=\\"sex\\", col=\\"survived\\", kind=\\"count\\", height=4, aspect=.7, order=[1, 2, 3], hue_order=[\\"male\\", \\"female\\"]) g.set_axis_labels(\'Passenger Class\', \'Count\') g.set_titles(\\"Survived = {col_name}\\") g.set(ylim=(0, 200)) for ax in g.axes.flat: total = len(titanic[titanic[\'pclass\'] == int(ax.get_title().split(\'= \')[1])]) for p in ax.patches: percentage = \'{:.1f}%\'.format(100 * p.get_height() / total) x = p.get_x() + p.get_width() / 2 - 0.05 y = p.get_y() + p.get_height() + 1 ax.annotate(percentage, (x, y)) plt.show() # Call the function to generate the plots plot_titanic_data()"},{"question":"# String Formatter and Template Substitution Task **Objective**: Implement multiple functions that utilize and extend the functionalities from the `string` module, specifically focusing on custom formatting and template strings. **Task 1**: Custom Formatter Implement a class `CustomFormatter` which inherits from `string.Formatter`. This class should override the default formatting behavior to include custom format types: - A new format type `\'N\'` which formats an integer to a string with commas as thousands separators. - A new format type `\'B\'` which converts an integer to its binary representation prefixed with `0b`. The class should have the following method: ```python class CustomFormatter(string.Formatter): def format_field(self, value, format_spec): # Your implementation here ``` **Task 2**: Template with Default Values Implement a subclass `DefaultTemplate` of `string.Template`. This subclass should: - Override the `safe_substitute` method to provide default values for placeholders in case they are not provided in the mapping or keyword arguments. The class should have the following method: ```python class DefaultTemplate(string.Template): def safe_substitute(self, mapping={}, /, **kwds): # Your implementation here ``` **Input and Output Specifications**: 1. **CustomFormatter class**: - Input: An integer and a format string specifying either the \'N\' or \'B\' format type. - Output: Formatted string as per the custom specification. Example: ```python fmt = CustomFormatter() assert fmt.format(\\"{:N}\\", 1234567) == \\"1,234,567\\" assert fmt.format(\\"{:B}\\", 10) == \\"0b1010\\" ``` 2. **DefaultTemplate class**: - Input: A template string with placeholders, and a dictionary/keyword arguments providing some of the values. - Output: A string with placeholders substituted with provided values or default to a specified string (e.g., `\'<missing>\'`) if not found. Example: ```python template_string = \\"Hello, {name}. You have {count} messages.\\" tmpl = DefaultTemplate(template_string) assert tmpl.safe_substitute(name=\\"Alice\\") == \\"Hello, Alice. You have <missing> messages.\\" ``` **Constraints**: - Focus on properly handling edge cases such as missing values. - Ensure your class methods are efficient and clear. **Performance Requirements**: - Your implementation should handle typical use cases efficiently. For instance, formatting a list of 1000 numbers should complete in under a second. Implement the functions as described, and ensure they pass the given examples and any additional test cases you devise to validate their correctness and performance.","solution":"import string class CustomFormatter(string.Formatter): def format_field(self, value, format_spec): if format_spec.endswith(\'N\'): return format(value, \',\') elif format_spec.endswith(\'B\'): return bin(value) else: return super().format_field(value, format_spec) class DefaultTemplate(string.Template): def safe_substitute(self, mapping={}, /, **kwds): combined_map = {**mapping, **kwds} return super().safe_substitute(DefaultPlaceholderDict(combined_map)) class DefaultPlaceholderDict(dict): def __missing__(self, key): return \'<missing>\'"},{"question":"# Pandas DataFrame Manipulation and Analysis Task Objective: You are provided with information about employees in a company. Your task is to use pandas to process this data, perform operations, and extract insights as specified below. Inputs: 1. A dictionary containing initial employee data. 2. A dictionary containing additional department information. 3. Instructions describing the analysis and processing tasks to be performed. Instructions: 1. **Create a DataFrame:** - Create a DataFrame `df` from the provided dictionary `employee_data`. 2. **Add a New Column:** - Create a new column `salary_after_tax` which is calculated as `salary` reduced by `tax_rate`. 3. **Handle Missing Data:** - Add a new column `department` to `df` using the `department_data` dictionary. Ensure that employees without department information are marked with `\\"Unknown\\"`. 4. **Filter Data:** - Filter employees with a salary greater than 70000 and only keep columns `name`, `department`, and `salary`. 5. **Group and Aggregate Data:** - Group the employees by `department` and calculate the total `salary_after_tax` for each department. 6. **Output the Result:** - Return the DataFrame obtained in step 4. - Return the DataFrame obtained from the aggregation in step 5. Example: ```python import pandas as pd import numpy as np # Provided data employee_data = { \'employee_id\': [1, 2, 3, 4, 5], \'name\': [\'John Doe\', \'Jane Smith\', \'Emily Johnson\', \'Michael Brown\', \'Chris Lee\'], \'age\': [34, 29, 45, 50, 38], \'salary\': [72000, 68000, 80000, 56000, 72000], \'tax_rate\': [0.25, 0.22, 0.27, 0.20, 0.25] } department_data = { 1: \'HR\', 2: \'Finance\', 3: \'IT\', 5: \'Marketing\' } def analyze_employee_data(employee_data, department_data): # Step 1: Create DataFrame df = pd.DataFrame(employee_data) # Step 2: Add salary_after_tax column df[\'salary_after_tax\'] = df[\'salary\'] * (1 - df[\'tax_rate\']) # Step 3: Add department column and handle missing data df[\'department\'] = df[\'employee_id\'].map(department_data).fillna(\'Unknown\') # Step 4: Filter data filtered_df = df[df[\'salary\'] > 70000][[\'name\', \'department\', \'salary\']] # Step 5: Group and aggregate data aggregated_df = df.groupby(\'department\')[\'salary_after_tax\'].sum().reset_index() # Step 6: Return results return filtered_df, aggregated_df # Analyze the data filtered_data, aggregated_data = analyze_employee_data(employee_data, department_data) print(filtered_data) print(aggregated_data) ``` Constraints: - Ensure that you handle missing data appropriately. - All operations must be performed using pandas methods.","solution":"import pandas as pd def analyze_employee_data(employee_data, department_data): # Step 1: Create DataFrame df = pd.DataFrame(employee_data) # Step 2: Add salary_after_tax column df[\'salary_after_tax\'] = df[\'salary\'] - (df[\'salary\'] * df[\'tax_rate\']) # Step 3: Add department column and handle missing data df[\'department\'] = df[\'employee_id\'].map(department_data).fillna(\'Unknown\') # Step 4: Filter data filtered_df = df[df[\'salary\'] > 70000][[\'name\', \'department\', \'salary\']] # Step 5: Group and aggregate data aggregated_df = df.groupby(\'department\')[\'salary_after_tax\'].sum().reset_index() # Step 6: Return results return filtered_df, aggregated_df"},{"question":"You are required to implement a task scheduler using the `sched` module of Python. The scheduler should handle multiple events triggered at different intervals and priorities. Specifically, you need to implement a function `event_scheduler` that schedules and manages several events, ensuring they execute in the correct order based on time and priority. Function Signature ```python def event_scheduler(events: list[tuple], base_time: float) -> list[float]: pass ``` Input - `events`: A list of tuples, where each tuple represents an event to be scheduled. Each tuple contains the following elements: - A boolean `is_absolute` indicating if the event time is absolute (True) or relative (False). - A float `time` representing the absolute time or delay in seconds when the event should occur. - An integer `priority` representing the priority of the event (lower number means higher priority). - A string `message` representing a message to be printed when the event is executed. - `base_time`: A float representing the base time in seconds from which relative times are measured. Output - A list of floats representing the times in seconds at which each event was executed. Constraints - You can assume that the current time is tracked accurately. - You can assume `time.sleep` and `time.time` functions are used for delay and time, respectively. Example ```python events = [ (False, 10, 1, \\"event 1\\"), (False, 5, 0, \\"event 2\\"), (True, 1652342835, 1, \\"event 3\\"), ] base_time = 1652342830.5 result = event_scheduler(events, base_time) # Expected Output (Example): [1652342835.5, 1652342840.5, 1652342835.0] ``` # Steps 1. Create a scheduler instance using `sched.scheduler`. 2. Schedule each event using either `enterabs` or `enter` based on the `is_absolute` flag. 3. Collect the execution times of each event as they get executed. 4. Return the list of execution times. # Notes - Make sure you manage priorities correctly. - The function should effectively manage the event queue. - The times in the output should match actual execution times as closely as possible, acknowledging small delays due to system scheduling might occur.","solution":"import sched import time def event_scheduler(events, base_time): Schedules and manages several events, ensuring they execute in the correct order based on time and priority. Args: - events: list of event tuples (is_absolute, time, priority, message) - base_time: float representing the base time in seconds from which relative times are measured. Returns: - A list of floats representing the times in seconds at which each event was executed. scheduler = sched.scheduler(time.time, time.sleep) executed_times = [] def event_action(message): # Get the current time and append to the executed times executed_time = time.time() executed_times.append(executed_time) print(message, executed_time) for is_absolute, event_time, priority, message in events: if is_absolute: scheduler.enterabs(event_time, priority, event_action, (message,)) else: scheduler.enter(event_time + base_time - time.time(), priority, event_action, (message,)) scheduler.run() return executed_times"},{"question":"**Objective**: Demonstrate the effective use of the `contextvars` module, especially in the context of asynchronous programming. Problem Statement You are tasked with developing a logging system for an asynchronous web server. The logging system should record the IP address and port of the client making requests to the server. This information should be accessible globally within the scope of the task handling the request, without passing it explicitly to each function. To achieve this, you will use the `contextvars` module to store the client address information as a context variable. Requirements 1. Implement a function `log_client_info()` that logs the client address for each request. This function should access the client address from a context variable without explicitly receiving it as an argument. 2. Use the `ContextVar` to store and manage the client address contextually during the handling of each request. 3. Implement an asynchronous function to handle client requests, simulate different clients making requests to the server, and demonstrate the logging of client addresses. # Function Signature ```python from contextvars import ContextVar import asyncio # Define the context variable client_addr_var = ContextVar(\'client_addr\') def log_client_info(): Log the client address using the context variable. pass async def handle_request(client_addr: tuple): Simulate handling a client request and log the client information. Args: client_addr (tuple): A tuple containing (IP address, port) of the client. pass async def main(): Create a few simulated client requests and handle them concurrently. pass ``` # Implementation Details 1. **Context Variable Setup**: - Use `ContextVar` to create a context variable `client_addr_var` for storing the client address. 2. **log_client_info Function**: - Implement the `log_client_info()` function to retrieve and print the client address from the context variable. 3. **handle_request Function**: - Implement the `handle_request(client_addr)` function to: - Set the client address into the context variable. - Simulate some processing by using `await asyncio.sleep()`. - Log the client address using `log_client_info()`. 4. **main Function**: - Implement the `main()` function to: - Simulate multiple client requests by creating client address tuples. - Use `asyncio.gather()` to handle these requests concurrently. Example Your implementation should handle concurrent requests and log the client address for each request. ```python async def main(): clients = [ (\'192.168.1.1\', 8001), (\'192.168.1.2\', 8002), (\'192.168.1.3\', 8003) ] await asyncio.gather(*(handle_request(client) for client in clients)) # To run the main function asyncio.run(main()) # Expected log output (order may vary depending on task scheduling): # Client Address: 192.168.1.1:8001 # Client Address: 192.168.1.2:8002 # Client Address: 192.168.1.3:8003 ``` Constraints - The implementation must correctly handle concurrent requests and ensure each log entry corresponds to the correct client. - Make sure to follow Python 3.7+ syntax and conventions, particularly with async/await and contextvars. **Note**: This question is designed to test your understanding of managing context-local state in concurrent asynchronous code using the `contextvars` module.","solution":"from contextvars import ContextVar import asyncio # Define the context variable client_addr_var = ContextVar(\'client_addr\') def log_client_info(): Log the client address using the context variable. client_addr = client_addr_var.get() print(f\\"Client Address: {client_addr[0]}:{client_addr[1]}\\") async def handle_request(client_addr: tuple): Simulate handling a client request and log the client information. Args: client_addr (tuple): A tuple containing (IP address, port) of the client. client_addr_var.set(client_addr) await asyncio.sleep(0.1) # Simulating request handling log_client_info() async def main(): Create a few simulated client requests and handle them concurrently. clients = [ (\'192.168.1.1\', 8001), (\'192.168.1.2\', 8002), (\'192.168.1.3\', 8003) ] await asyncio.gather(*(handle_request(client) for client in clients)) # To run the main function if __name__ == \\"__main__\\": asyncio.run(main())"},{"question":"**Title**: Optimizing Performance of a scikit-learn Model **Objective**: To assess the understanding of scikit-learn\'s performance optimization techniques in terms of prediction latency and throughput for models with high-dimensional and sparse datasets. **Problem Statement**: You are provided with two datasets: one is a dense dataset with high dimensional features, and the other is a sparse dataset. Your task is to: 1. **Train a Linear Regression Model** on both datasets. 2. **Measure and compare the prediction latency** for both dense and sparse datasets. 3. **Optimize the model**: - Utilize the `sparsify` method for model sparsity and measure the effect on prediction latency. - Tune relevant hyperparameters to improve the prediction throughput. - Use optimized BLAS/LAPACK libraries if not already configured. **Datasets**: 1. `dense_X_train, dense_y_train, dense_X_test, dense_y_test` 2. `sparse_X_train, sparse_y_train, sparse_X_test, sparse_y_test` (in CSR sparse matrix format) **Requirements**: 1. Implement and train two `SGDRegressor` models from `sklearn.linear_model`: - One using the dense dataset. - One using the sparse dataset. 2. Measure and print the prediction latency for both models. 3. Optimize the models by sparsifying them and measure the new prediction latency. 4. Tune other relevant hyperparameters to improve prediction throughput. **Input and Output formats**: - Input: Provided as preloaded numpy arrays and scipy sparse matrices. - Output: Print statements indicating model latencies before and after optimization and any observations from tuning. **Constraints and Limitations**: - You must use the same initial random state for reproducibility. - Measure latency and throughput using the `time` module in Python. - Hyperparameter tuning should be done logically to demonstrate understanding (e.g., through trial and error). **Performance Requirements**: - Latency measurements must be in microseconds or milliseconds for better granularity. - Throughput results should be provided in predictions per second. **Code Template**: ```python from sklearn.linear_model import SGDRegressor from scipy.sparse import csr_matrix import numpy as np import time # Assume these datasets are preloaded: # dense_X_train, dense_y_train, dense_X_test, dense_y_test # sparse_X_train, sparse_y_train, sparse_X_test, sparse_y_test def measure_latency(predict_fn, X_test): start_time = time.time() predict_fn(X_test) end_time = time.time() return (end_time - start_time) * 1e6 # Return latency in microseconds def measure_throughput(predict_fn, X_test): start_time = time.time() predict_fn(X_test) end_time = time.time() throughput = X_test.shape[0] / (end_time - start_time) return throughput # Return throughput as predictions per second def main(): # Train model on dense dataset dense_model = SGDRegressor(random_state=0) dense_model.fit(dense_X_train, dense_y_train) # Train model on sparse dataset sparse_model = SGDRegressor(random_state=0) sparse_model.fit(sparse_X_train, sparse_y_train) # Measure latency for dense model dense_latency = measure_latency(dense_model.predict, dense_X_test) print(f\'Dense dataset prediction latency: {dense_latency:.2f} microseconds\') # Measure latency for sparse model sparse_latency = measure_latency(sparse_model.predict, sparse_X_test) print(f\'Sparse dataset prediction latency: {sparse_latency:.2f} microseconds\') # Optimize models by sparsifying and measure new latency sparse_model.sparsify() optimized_sparse_latency = measure_latency(sparse_model.predict, sparse_X_test) print(f\'Optimized sparse dataset prediction latency: {optimized_sparse_latency:.2f} microseconds\') # Measure throughput for optimized sparse model sparse_throughput = measure_throughput(sparse_model.predict, sparse_X_test) print(f\'Optimized sparse dataset prediction throughput: {sparse_throughput:.2f} predictions/second\') # Add more optimization steps as needed... if __name__ == \\"__main__\\": main() ``` **Submission Instructions**: 1. Complete the function implementations as per the instructions above. 2. Ensure all print statements output the results as specified. 3. Submit your Python script file with the name `optimized_model.py`.","solution":"from sklearn.linear_model import SGDRegressor from scipy.sparse import csr_matrix import numpy as np import time # Assume these datasets are preloaded: # dense_X_train, dense_y_train, dense_X_test, dense_y_test # sparse_X_train, sparse_y_train, sparse_X_test, sparse_y_test def measure_latency(predict_fn, X_test): start_time = time.time() predict_fn(X_test) end_time = time.time() return (end_time - start_time) * 1e6 # Return latency in microseconds def measure_throughput(predict_fn, X_test): start_time = time.time() predict_fn(X_test) end_time = time.time() throughput = X_test.shape[0] / (end_time - start_time) return throughput # Return throughput as predictions per second def main(dense_X_train, dense_y_train, dense_X_test, dense_y_test, sparse_X_train, sparse_y_train, sparse_X_test, sparse_y_test): # Train model on dense dataset dense_model = SGDRegressor(random_state=0) dense_model.fit(dense_X_train, dense_y_train) # Train model on sparse dataset sparse_model = SGDRegressor(random_state=0) sparse_model.fit(sparse_X_train, sparse_y_train) # Measure latency for dense model dense_latency = measure_latency(dense_model.predict, dense_X_test) print(f\'Dense dataset prediction latency: {dense_latency:.2f} microseconds\') # Measure latency for sparse model sparse_latency = measure_latency(sparse_model.predict, sparse_X_test) print(f\'Sparse dataset prediction latency: {sparse_latency:.2f} microseconds\') # Optimize models by sparsifying and measure new latency sparse_model.sparsify() optimized_sparse_latency = measure_latency(sparse_model.predict, sparse_X_test) print(f\'Optimized sparse dataset prediction latency: {optimized_sparse_latency:.2f} microseconds\') # Measure throughput for optimized sparse model sparse_throughput = measure_throughput(sparse_model.predict, sparse_X_test) print(f\'Optimized sparse dataset prediction throughput: {sparse_throughput:.2f} predictions/second\') # Return values for testing purposes return { \'dense_latency\': dense_latency, \'sparse_latency\': sparse_latency, \'optimized_sparse_latency\': optimized_sparse_latency, \'sparse_throughput\': sparse_throughput, }"},{"question":"**Question:** You are asked to implement a function using low-level Python file handling APIs that processes lines from an existing file descriptor and writes transformed lines to a new file using another file descriptor. This function should demonstrate your understanding of working with file objects and descriptors. # Function signature: ```python def process_and_write_lines(input_fd: int, output_fd: int, transform_func) -> None: ``` # Parameters: - `input_fd` (int): A file descriptor for the already opened input file. - `output_fd` (int): A file descriptor for the already opened output file. - `transform_func` (callable): A function that takes a single string argument and returns the transformed string. # Description: Using the provided file descriptors, your task is to: 1. Create a Python file object from the given `input_fd` that can read lines of text. 2. Create a Python file object from the given `output_fd` that can write lines of text. 3. Read every line from the input file using the `PyFile_GetLine` functionality. 4. Apply the `transform_func` to each read line. 5. Write the transformed line to the output file using `PyFile_WriteString`. # Requirements: - Handle any potential I/O errors gracefully, ensuring meaningful error messages inside the interpreter. - Ensure that the function reads and writes lines correctly, adhering to the file\'s encoding. - Handle any special file conditions like end-of-file (EOF) appropriately and ensure that no resources are leaked. # Constraints: - You may assume the `transform_func` provided will not raise any exceptions. - The input file and output file are opened in read and write modes respectively. - Use the functions described below while noting their constraints and potential issues in a multithreaded environment. # Example: ```python def example_transform_func(line: str) -> str: return line.upper() # Assuming input_fd and output_fd are already prepared and valid file descriptors process_and_write_lines(input_fd, output_fd, example_transform_func) ``` ---------------------------------------- The documentation provided describes `PyFile_FromFd`, `PyObject_AsFileDescriptor`, and `PyFile_GetLine` functions. Use them effectively to implement the `process_and_write_lines` function: ```python PyObject *PyFile_FromFd(int fd, const char *name, const char *mode, int buffering, const char *encoding, const char *errors, const char *newline, int closefd); int PyObject_AsFileDescriptor(PyObject *p); PyObject *PyFile_GetLine(PyObject *p, int n); int PyFile_WriteString(const char *s, PyObject *p); ``` Implement a solution that shows proficiency in low-level file handling within Python.","solution":"import os def process_and_write_lines(input_fd: int, output_fd: int, transform_func) -> None: Processes lines from the input file descriptor, transforms them using the given transform function, and writes the transformed lines to the output file descriptor. Parameters: input_fd (int): A file descriptor for the already opened input file. output_fd (int): A file descriptor for the already opened output file. transform_func (callable): A function that takes a single string argument and returns the transformed string. try: # Open file objects from file descriptors input_file = os.fdopen(input_fd, \'r\') output_file = os.fdopen(output_fd, \'w\') # Read and process each line from the input file for line in input_file: transformed_line = transform_func(line) output_file.write(transformed_line) # Ensure data is flushed to output file output_file.flush() except OSError as e: print(f\\"An error occurred while handling file operations: {e}\\") finally: # Close the file objects input_file.close() output_file.close()"},{"question":"You are given a task to implement a utility module that interacts with datetime objects. You are to create a class in Python that mimics some of the functionalities explained in the provided documentation. This class should provide methods to: 1. Create datetime, date, and time objects. 2. Check the type of given objects to determine if they are instances of datetime, date, or time. 3. Extract specific information from these objects (e.g., year, month, day, hour, minute, second, microsecond). # Class Definition Define a class `DateTimeUtils` with the following methods: 1. `create_date(year: int, month: int, day: int) -> datetime.date`: - Creates and returns a date object for the given year, month, and day. 2. `create_time(hour: int, minute: int, second: int, microsecond: int = 0) -> datetime.time`: - Creates and returns a time object for the given hour, minute, second, and microsecond. 3. `create_datetime(year: int, month: int, day: int, hour: int, minute: int, second: int, microsecond: int = 0) -> datetime.datetime`: - Creates and returns a datetime object for the given date and time attributes. 4. `is_date(obj: Any) -> bool`: - Checks whether the given object is a date instance (but not a datetime instance). 5. `is_time(obj: Any) -> bool`: - Checks whether the given object is a time instance. 6. `is_datetime(obj: Any) -> bool`: - Checks whether the given object is a datetime instance. 7. `extract_date_info(date_obj: datetime.date) -> Tuple[int, int, int]`: - Accepts a date object and returns a tuple containing the year, month, and day. 8. `extract_time_info(time_obj: datetime.time) -> Tuple[int, int, int, int]`: - Accepts a time object and returns a tuple containing the hour, minute, second, and microsecond. 9. `extract_datetime_info(datetime_obj: datetime.datetime) -> Dict[str, Any]`: - Accepts a datetime object and returns a dictionary containing the year, month, day, hour, minute, second, microsecond, and tzinfo. # Example Usage: ```python from datetime import datetime, date, time from typing import Any, Tuple, Dict class DateTimeUtils: # Implement methods here # Example usage utils = DateTimeUtils() # Creating date and time objects date_obj = utils.create_date(2023, 10, 5) time_obj = utils.create_time(15, 30, 0) datetime_obj = utils.create_datetime(2023, 10, 5, 15, 30, 0) # Checking types assert utils.is_date(date_obj) == True assert utils.is_time(time_obj) == True assert utils.is_datetime(datetime_obj) == True # Extracting Info assert utils.extract_date_info(date_obj) == (2023, 10, 5) assert utils.extract_time_info(time_obj) == (15, 30, 0, 0) assert utils.extract_datetime_info(datetime_obj) == { \\"year\\": 2023, \\"month\\": 10, \\"day\\": 5, \\"hour\\": 15, \\"minute\\": 30, \\"second\\": 0, \\"microsecond\\": 0, \\"tzinfo\\": None } ``` # Constraints: - Year should be a positive integer. - Month should be in the range 1-12. - Day should be in the range 1-31. - Hour should be in the range 0-23. - Minute should be in the range 0-59. - Second should be in the range 0-59. - Microsecond should be in the range 0-999999. Implement the `DateTimeUtils` class in full, making sure all methods function correctly given the example usage and constraints.","solution":"from datetime import datetime, date, time from typing import Any, Tuple, Dict class DateTimeUtils: @staticmethod def create_date(year: int, month: int, day: int) -> date: return date(year, month, day) @staticmethod def create_time(hour: int, minute: int, second: int, microsecond: int = 0) -> time: return time(hour, minute, second, microsecond) @staticmethod def create_datetime(year: int, month: int, day: int, hour: int, minute: int, second: int, microsecond: int = 0) -> datetime: return datetime(year, month, day, hour, minute, second, microsecond) @staticmethod def is_date(obj: Any) -> bool: return isinstance(obj, date) and not isinstance(obj, datetime) @staticmethod def is_time(obj: Any) -> bool: return isinstance(obj, time) @staticmethod def is_datetime(obj: Any) -> bool: return isinstance(obj, datetime) @staticmethod def extract_date_info(date_obj: date) -> Tuple[int, int, int]: return date_obj.year, date_obj.month, date_obj.day @staticmethod def extract_time_info(time_obj: time) -> Tuple[int, int, int, int]: return time_obj.hour, time_obj.minute, time_obj.second, time_obj.microsecond @staticmethod def extract_datetime_info(datetime_obj: datetime) -> Dict[str, Any]: return { \\"year\\": datetime_obj.year, \\"month\\": datetime_obj.month, \\"day\\": datetime_obj.day, \\"hour\\": datetime_obj.hour, \\"minute\\": datetime_obj.minute, \\"second\\": datetime_obj.second, \\"microsecond\\": datetime_obj.microsecond, \\"tzinfo\\": datetime_obj.tzinfo }"},{"question":"**Question:** You are to implement a function `summarize_module` that takes a module name (as a string) and an optional path (as a list of paths) and returns a concise summary of the module\'s structure. The summary should include module-level class names and function names, along with each class\'s methods and base classes. # Function Signature ```python def summarize_module(module: str, path: Optional[List[str]] = None) -> Dict[str, Any]: pass ``` # Input - `module`: A string representing the name of the module to be summarized. - `path`: An optional list of directory paths that will be prepended to `sys.path` to locate the module\'s source code. Default is `None`. # Output - A dictionary containing: - Keys as strings of \\"Classes\\" and \\"Functions\\". - \\"Classes\\" maps to a dictionary where: - Each key is the class name. - Each value is a dictionary with keys \\"Methods\\" (a list of methods of the class) and \\"Base Classes\\" (a list of immediate base classes of the class). - \\"Functions\\" maps to a list of top-level function names (excluding methods). # Constraints - You must use the `pyclbr` module to gather the required information. - If the module does not exist or cannot be read, your function should raise an appropriate exception. - Nested classes should be included under the `Classes` key. - Use the `readmodule_ex` function from the `pyclbr` module. # Example Given a module `simple_module.py` with the following content: ```python # simple_module.py class Base: def method1(self): pass class Derived(Base): def method2(self): pass def top_function(): pass ``` Calling `summarize_module(\\"simple_module\\")` should return: ```python { \'Classes\': { \'Base\': { \'Methods\': [\'method1\'], \'Base Classes\': [] }, \'Derived\': { \'Methods\': [\'method2\'], \'Base Classes\': [\'Base\'] } }, \'Functions\': [\'top_function\'] } ``` # Notes - Ensure that the function works with nested classes and functions. - Use appropriate error handling for file and module errors. - Your solution should be efficient and handle cases with a large number of classes and functions.","solution":"import pyclbr import sys from typing import List, Optional, Dict, Any def summarize_module(module: str, path: Optional[List[str]] = None) -> Dict[str, Any]: if path: sys.path.extend(path) try: module_info = pyclbr.readmodule_ex(module) except ImportError as e: raise ImportError(f\\"Could not import module {module}: {str(e)}\\") summary = { \'Classes\': {}, \'Functions\': [] } for name, obj in module_info.items(): if isinstance(obj, pyclbr.Class): summary[\'Classes\'][name] = { \'Methods\': list(obj.methods.keys()), \'Base Classes\': [base.name if isinstance(base, pyclbr.Class) else base for base in obj.super] } elif isinstance(obj, pyclbr.Function): summary[\'Functions\'].append(name) return summary"},{"question":"# Custom ExtensionArray and DataFrame Accessor **Objective:** To assess the student\'s understanding of pandas extension mechanisms by requiring them to create a custom ExtensionArray and attach a custom accessor to a DataFrame. **Problem Statement:** You are required to implement a custom `TemperatureArray` to handle temperature data in pandas. Additionally, you should create a DataFrame accessor, `temp`, which provides methods to convert between Fahrenheit and Celsius. **Requirements:** 1. **Custom ExtensionArray (`TemperatureArray`):** - Create a class `TemperatureDtype` inheriting from `pandas.api.extensions.ExtensionDtype` to define the custom dtype for temperature. - Create a class `TemperatureArray` inheriting from `pandas.api.extensions.ExtensionArray` to handle operations on temperature data. - Implement necessary methods (`_from_sequence`, `astype`, etc.) to make `TemperatureArray` function correctly within pandas. 2. **DataFrame Accessor:** - Create a custom accessor `@pd.api.extensions.register_dataframe_accessor(\'temp\')`. - The accessor should provide methods: - `to_celsius()` - Convert all temperature columns from Fahrenheit to Celsius. - `to_fahrenheit()` - Convert all temperature columns from Celsius to Fahrenheit. **Input/Output Format:** - Input: A pandas DataFrame with one or more columns of temperature data in Fahrenheit. - Output: A pandas DataFrame with temperature columns converted to the respective units. **Constraints:** - Implement all necessary dunder methods to integrate your `TemperatureArray` with pandas operations. - Use consistent naming conventions and include docstrings for clarity. **Example:** ```python import pandas as pd # Usage example # Create a DataFrame with temperature data df = pd.DataFrame({ \'city\': [\'New York\', \'Los Angeles\', \'Chicago\'], \'temperature\': [32, 104, 50] # Temperatures in Fahrenheit }) # Convert temperature to Celsius df.temp.to_celsius() print(df) # Output: # city temperature # 0 New York 0.000000 # 1 Los Angeles 40.000000 # 2 Chicago 10.000000 # Convert temperature back to Fahrenheit df.temp.to_fahrenheit() print(df) # Output: # city temperature # 0 New York 32.0 # 1 Los Angeles 104.0 # 2 Chicago 50.0 ``` Implement the solution in a well-documented manner ensuring that your code is maintainable and follows best practices.","solution":"import pandas as pd import numpy as np from pandas.api.extensions import ExtensionArray, ExtensionDtype, register_dataframe_accessor class TemperatureDtype(ExtensionDtype): name = \'temperature\' type = np.dtype(\'float64\') kind = \'f\' _metadata = () @property def na_value(self): return np.nan @classmethod def construct_array_type(cls): return TemperatureArray class TemperatureArray(ExtensionArray): def __init__(self, values): self._data = np.asarray(values, dtype=float) @classmethod def _from_sequence(cls, scalars, dtype=None, copy=False): return cls(scalars) def __getitem__(self, item): if isinstance(item, int): return self._data[item] return TemperatureArray(self._data[item]) def __len__(self): return len(self._data) def isna(self): return np.isnan(self._data) def take(self, indices, allow_fill=False, fill_value=None): from pandas.core.algorithms import take if allow_fill and fill_value is None: fill_value = self.dtype.na_value result = take(self._data, indices, allow_fill=allow_fill, fill_value=fill_value) return self._from_sequence(result) @property def dtype(self): return TemperatureDtype() def copy(self): return TemperatureArray(self._data.copy()) def astype(self, dtype, copy=True): return np.array(self._data, dtype=dtype, copy=copy) @register_dataframe_accessor(\\"temp\\") class TemperatureAccessor: def __init__(self, pandas_obj): self._obj = pandas_obj def _convert_temp(self, col, conversion_func): def convert(value): if pd.isna(value): return value return conversion_func(value) return self._obj[col].apply(convert) def to_celsius(self): F_to_C = lambda f: (f - 32) * 5.0/9.0 temp_cols = self._identify_temp_columns() for col in temp_cols: self._obj[col] = self._convert_temp(col, F_to_C) def to_fahrenheit(self): C_to_F = lambda c: c * 9.0/5.0 + 32 temp_cols = self._identify_temp_columns() for col in temp_cols: self._obj[col] = self._convert_temp(col, C_to_F) def _identify_temp_columns(self): numeric_cols = self._obj.select_dtypes(include=[np.number]).columns return [col for col in numeric_cols if \'temp\' in col.lower() or \'temperature\' in col.lower()]"},{"question":"You have been provided with three different CSV files detailing information about employees, departments, and their project assignments. Your task is to perform various data manipulations using pandas and generate some visualizations. Please ensure your solution is efficient and accurate. # Input Files 1. **employees.csv**: Contains information about employees. - Columns: `employee_id`, `employee_name`, `department_id` 2. **departments.csv**: Contains information about departments. - Columns: `department_id`, `department_name` 3. **projects.csv**: Contains information about project assignments. - Columns: `project_id`, `project_name`, `employee_id` # Requirements 1. **Load the Data**: Load the CSV files into three different pandas DataFrames. Ensure that the columns with similar names have consistent data types across DataFrames. 2. **Merge Data**: - Merge `employees` and `departments` DataFrames on `department_id` to create a new DataFrame `employee_department`. - Merge `employee_department` and `projects` DataFrames on `employee_id` to create `employee_project`. 3. **Data Transformation**: - Add a new column `project_count` to `employee_department` that shows the number of projects each employee is assigned to. The value should be `0` if the employee doesn\'t have any projects. 4. **Group and Aggregate**: - Group the `employee_project` DataFrame by `department_name` and calculate the total number of projects in each department. Store this result in a new DataFrame `department_project_count`. 5. **Plotting**: - Create a bar plot showing the total number of projects for each department using the `department_project_count` DataFrame. # Expected Output 1. **employee_project** DataFrame (first 5 rows): ``` employee_id employee_name department_id department_name project_id project_name 0 1 John Doe 101 Marketing 1 Project A 1 2 Jane Smith 102 Sales 2 Project B ... ``` 2. **department_project_count** DataFrame: ``` department_name project_count 0 Marketing 10 1 Sales 8 ... ``` 3. **Bar Plot**: A bar plot showing each department\'s total number of projects. # Constraints 1. You can assume that the CSV files do not contain any missing values. 2. Ensure the code is written in a modular format, making use of functions wherever applicable. 3. The solution should be efficient and should not use explicit loops for DataFrame operations (use pandas operations instead). # Performance Requirement - Your solution should be optimized for performance, especially considering the merging and grouping operations. # Function Signatures ```python def load_data(file_path: str) -> pd.DataFrame: # Function to load a CSV file and return a DataFrame pass def merge_data(employees: pd.DataFrame, departments: pd.DataFrame, projects: pd.DataFrame) -> pd.DataFrame: # Function to merge employee, department, and project DataFrames pass def transform_data(employee_project: pd.DataFrame) -> pd.DataFrame: # Function to add project count to the employee_department DataFrame pass def group_and_aggregate(employee_project: pd.DataFrame) -> pd.DataFrame: # Function to group and aggregate data to get the total number of projects per department pass def plot_data(department_project_count: pd.DataFrame) -> None: # Function to plot a bar chart of the total number of projects for each department pass # Main execution flow if __name__ == \\"__main__\\": employees = load_data(\\"employees.csv\\") departments = load_data(\\"departments.csv\\") projects = load_data(\\"projects.csv\\") employee_project = merge_data(employees, departments, projects) employee_project = transform_data(employee_project) department_project_count = group_and_aggregate(employee_project) plot_data(department_project_count) ```","solution":"import pandas as pd import matplotlib.pyplot as plt def load_data(file_path: str) -> pd.DataFrame: Load a CSV file and return a DataFrame. return pd.read_csv(file_path) def merge_data(employees: pd.DataFrame, departments: pd.DataFrame, projects: pd.DataFrame) -> pd.DataFrame: Merge employee, department, and project DataFrames. employee_department = pd.merge(employees, departments, on=\'department_id\') employee_project = pd.merge(employee_department, projects, on=\'employee_id\', how=\'left\') return employee_project def transform_data(employee_project: pd.DataFrame, employees: pd.DataFrame) -> pd.DataFrame: Add project count to the employee_department DataFrame. project_count = employee_project.groupby(\'employee_id\').size().reset_index(name=\'project_count\') employee_project = employee_project.drop_duplicates(subset=[\'employee_id\', \'employee_name\', \'department_id\', \'department_name\']) employee_project = pd.merge(employee_project, project_count, on=\'employee_id\', how=\'left\').fillna({\'project_count\': 0}) return employee_project def group_and_aggregate(employee_project: pd.DataFrame) -> pd.DataFrame: Group and aggregate data to get the total number of projects per department. department_project_count = employee_project.groupby(\'department_name\').agg({\'project_id\': \'count\'}).reset_index() department_project_count.columns = [\'department_name\', \'project_count\'] return department_project_count def plot_data(department_project_count: pd.DataFrame) -> None: Plot a bar chart of the total number of projects for each department. plt.figure(figsize=(10, 6)) plt.bar(department_project_count[\'department_name\'], department_project_count[\'project_count\'], color=\'skyblue\') plt.xlabel(\'Department\') plt.ylabel(\'Number of Projects\') plt.title(\'Total Number of Projects per Department\') plt.xticks(rotation=45) plt.tight_layout() plt.show() # Main execution flow if __name__ == \\"__main__\\": employees = load_data(\\"employees.csv\\") departments = load_data(\\"departments.csv\\") projects = load_data(\\"projects.csv\\") employee_project = merge_data(employees, departments, projects) employee_project = transform_data(employee_project, employees) department_project_count = group_and_aggregate(employee_project) plot_data(department_project_count)"},{"question":"Objective In this task, you are required to demonstrate your understanding of seaborn\'s plotting functions, focusing on residual plots. You will read a dataset, preprocess it, and create residual plots with different configurations to help diagnose linear regression models. Description 1. Load the predetermined dataset `mpg` using seaborn\'s `load_dataset` function. 2. Generate the following residual plots: 1. A basic residual plot to visualize residuals of `displacement` against `weight`. 2. A second residual plot to visualize residuals of `mpg` against `horsepower`. 3. A third plot applying a second-order polynomial to remove higher-order trends and visualize residuals of `mpg` against `horsepower`. 4. The fourth plot should use a LOWESS curve to emphasize any structure in the residuals of `mpg` against `horsepower`. Input and Output - **Input**: No input from the user is required other than reading the dataset. - **Output**: The task should result in four residual plots created using seaborn. Task Requirements 1. Implement the following function using seaborn: ```python def generate_residual_plots(): # Your code here ``` 2. The `generate_residual_plots` function should: - Load the `mpg` dataset. - Generate and display the four specific residual plots as described. 3. Ensure the plots are clearly labeled and distinguishable. Constraints - Use seaborn\'s `residplot` function. - The plots should be displayed inline (i.e., one after another). Example An example of the expected results: - A scatter plot showing residuals of `displacement` against `weight`. - A scatter plot showing residuals of `mpg` against `horsepower`. - A scatter plot with a second-order polynomial showing residuals of `mpg` against `horsepower`. - A scatter plot with a LOWESS curve showing residuals of `mpg` against `horsepower`. Make sure your function follows the structure and requirements outlined above.","solution":"import seaborn as sns import matplotlib.pyplot as plt def generate_residual_plots(): # Load the mpg dataset mpg_data = sns.load_dataset(\'mpg\') # Basic residual plot: residuals of displacement against weight plt.figure(figsize=(10, 6)) sns.residplot(x=\'weight\', y=\'displacement\', data=mpg_data) plt.title(\\"Residuals of Displacement against Weight\\") plt.xlabel(\\"Weight\\") plt.ylabel(\\"Residuals\\") plt.show() # Residual plot: residuals of mpg against horsepower plt.figure(figsize=(10, 6)) sns.residplot(x=\'horsepower\', y=\'mpg\', data=mpg_data) plt.title(\\"Residuals of MPG against Horsepower\\") plt.xlabel(\\"Horsepower\\") plt.ylabel(\\"Residuals\\") plt.show() # Residual plot with second-order polynomial: residuals of mpg against horsepower plt.figure(figsize=(10, 6)) sns.residplot(x=\'horsepower\', y=\'mpg\', data=mpg_data, order=2) plt.title(\\"Residuals of MPG against Horsepower (2nd Order Polynomial)\\") plt.xlabel(\\"Horsepower\\") plt.ylabel(\\"Residuals\\") plt.show() # Residual plot with LOWESS curve: residuals of mpg against horsepower plt.figure(figsize=(10, 6)) sns.residplot(x=\'horsepower\', y=\'mpg\', data=mpg_data, lowess=True) plt.title(\\"Residuals of MPG against Horsepower (LOWESS)\\") plt.xlabel(\\"Horsepower\\") plt.ylabel(\\"Residuals\\") plt.show()"},{"question":"# Question: Secure Password Generator You are tasked with implementing a function that generates a secure password following specified criteria using the \\"secrets\\" module in Python. The function should ensure the randomness and security of the password. Function Description Implement the function `generate_secure_password(length, complexity)`: ```python def generate_secure_password(length: int, complexity: int) -> str: pass ``` Input - `length` (int): The desired length of the password. The length should be at least 8 characters. - `complexity` (int): The desired complexity level of the password. It can be: - `1` for a password containing upper and lower case letters. - `2` for a password containing upper and lower case letters, and digits. - `3` for a password containing upper and lower case letters, digits, and special characters. Output - Returns a string representing the generated secure password. Constraints - The password must be exactly of the given `length`. - For complexity `2` and `3`, at least one digit should be present. - For complexity `3`, at least one special character should be present. - In any case, the password must include both upper and lower case letters. Examples Example 1: ```python >>> generate_secure_password(12, 1) \'GpHTepYxUwRz\' ``` Example 2: ```python >>> generate_secure_password(12, 2) \'Gp1Te7Y2xhR9\' ``` Example 3: ```python >>> generate_secure_password(12, 3) \'Gp1Te7!2xhR9\' ``` Notes - Use the `secrets` module to generate secure random characters. - Handle edge cases where it might take several iterations to meet the constraints. - Assume `length` and `complexity` are valid integers as defined above.","solution":"import secrets import string def generate_secure_password(length: int, complexity: int) -> str: if length < 8: raise ValueError(\\"Password length must be at least 8 characters\\") char_pools = { 1: string.ascii_letters, 2: string.ascii_letters + string.digits, 3: string.ascii_letters + string.digits + string.punctuation } if complexity not in char_pools: raise ValueError(\\"Invalid complexity level. Must be 1, 2, or 3.\\") while True: password = \'\'.join(secrets.choice(char_pools[complexity]) for _ in range(length)) # Check if the password meets the complexity criteria if (any(c.islower() for c in password) and any(c.isupper() for c in password)): if complexity == 1: return password if (complexity >= 2 and any(c.isdigit() for c in password)): if complexity == 2: return password if complexity == 3 and any(c in string.punctuation for c in password): return password"},{"question":"# NNTP Client Simulation with Python\'s `nntplib` You are tasked with creating a Python function `nntp_client` that interacts with an NNTP server to perform the following actions: 1. **Connect to the server** using the provided host and port. 2. **Authenticate** using provided credentials (user and password). If `user` and `password` are `None`, assume anonymous access. 3. **List the available newsgroups** on the server. 4. **Select a specified newsgroup** and fetch the latest 5 article subjects. 5. **Post a new article** to the specified newsgroup. # Implementation Requirements - **Function Signature:** ```python def nntp_client(host: str, port: int, user: str, password: str, newsgroup: str, article: str) -> None: ``` - `host` (str): The NNTP server hostname. - `port` (int): The NNTP server port. - `user` (str): The username for authentication. - `password` (str): The password for authentication. - `newsgroup` (str): The name of the newsgroup to interact with. - `article` (str): The content of the article to be posted in the newsgroup. - **Procedure:** 1. Connect to the NNTP server at the given host and port. 2. Authenticate using the provided user and password. If they are `None`, skip the authentication step. 3. List all available newsgroups and print them. 4. Select the specified newsgroup and print statistics about it. 5. Fetch the latest 5 article subjects in the newsgroup and print them. 6. Post the provided article content to the newsgroup. 7. Close the connection to the server. - **Constraints:** - Handle all necessary exceptions that might arise during the interaction with the server. # Example Execution ```python nntp_client(\\"news.gmane.io\\", 119, None, None, \\"gmane.comp.python.committers\\", \\"Subject: Test ArticlennThis is a test.\\") ``` Expected output (format may vary but should include similar information): ``` Connected to the NNTP server at news.gmane.io. Available newsgroups: <list of newsgroups> Selected newsgroup: gmane.comp.python.committers Statistics: 1096 articles, range 1 to 1096 Latest 5 article subjects: 1092 Updated ssh key 1093 Re: Updated ssh key 1094 Re: Updated ssh key 1095 Hello fellow committers! 1096 Re: Hello fellow committers! Article posted successfully. Connection closed. ``` # Notes - Use appropriate methods from the `nntplib` module to achieve the task. - Carefully handle exceptions and ensure the connection is properly closed after the operations.","solution":"import nntplib from nntplib import NNTP import socket def nntp_client(host: str, port: int, user: str, password: str, newsgroup: str, article: str) -> None: try: # Connect to the server with NNTP(host, port, user=user, password=password) as client: print(f\\"Connected to the NNTP server at {host}.\\") # List all available newsgroups resp, groups = client.list() print(\\"Available newsgroups:\\") for group in groups[:10]: # limiting to first 10 for brevity print(group) # Select the specified newsgroup resp, count, first, last, name = client.group(newsgroup) print(f\\"Selected newsgroup: {name}\\") print(f\\"Statistics: {count} articles, range {first} to {last}\\") # Fetch the latest 5 article subjects in the newsgroup resp, overviews = client.over((max(int(last) - 4, int(first)), last)) print(\\"Latest 5 article subjects:\\") for artnum, over in overviews: print(f\\"{artnum} {over[\'subject\']}\\") # Post the provided article content to the newsgroup resp = client.post(article) if resp[0] == \'240\': print(\\"Article posted successfully.\\") else: print(\\"Failed to post the article.\\") except (nntplib.NNTPTemporaryError, nntplib.NNTPPermanentError, nntplib.NNTPProtocolError) as e: print(f\\"NNTP error: {e}\\") except (socket.error, socket.gaierror) as e: print(f\\"Socket error: {e}\\") finally: print(\\"Connection closed.\\")"},{"question":"Objective: Demonstrate your understanding of the \\"getpass\\" module by creating a Python script that securely collects user credentials and verifies the username using system environment variables. Problem Statement: Write a Python function `collect_credentials()` that performs the following tasks: 1. Securely prompts the user to enter their username and password using the `getpass` module. 2. Uses the `getpass.getuser()` function to retrieve the correct system username. 3. Compares the entered username with the system username and prints a success message if they match. 4. Handles cases where password input may be echoed by catching and reporting the `getpass.GetPassWarning` exception. 5. Uses a fallback method to collect passwords if the secure input is unavailable, displaying an appropriate warning message. Function Signature: ```python def collect_credentials(): pass ``` Example Usage: ```python collect_credentials() ``` Constraints: - Do not use any external libraries. - The function should only return values or print messages as specified. Requirements: 1. The user is prompted to enter their username and password without echoing the password. 2. The function should handle scenarios where secure password input is not possible, issuing a `GetPassWarning` and using a fallback method. 3. Print appropriate messages to notify the user about the success or failure of username matching and issues with secure password entry. Here is an example of what the interaction might look like (user input is represented in parentheses): ``` Enter your username: (student) Password: Incorrect username. Please try again. Enter your username: (correctuser) Password: Username and password collection successful. ``` Additional Notes: - Ensure the code is clean, readable, and well-documented. - Handle all possible exceptions gracefully, providing informative messages to the user.","solution":"import getpass import os def collect_credentials(): try: # Prompt the user to enter their username entered_username = input(\\"Enter your username: \\") # Securely prompt the user to enter their password try: entered_password = getpass.getpass(\\"Password: \\") except getpass.GetPassWarning: print(\\"Warning: getpass() couldn\'t provide secure input. Password input may be echoed.\\") entered_password = input(\\"Password: \\") # Fallback to non-secure input # Retrieve the current system username system_username = getpass.getuser() # Compare entered username with system username if entered_username == system_username: print(\\"Username and password collection successful.\\") else: print(\\"Incorrect username. Please try again.\\") except Exception as e: print(f\\"An error occurred: {str(e)}\\")"},{"question":"Objective: You are to implement a function that helps automate the transition from using legacy Distutils to setuptools for managing Python packages. Problem Statement: Given a setup script written using Distutils, write a Python function `migrate_to_setuptools` that reads the setup script, converts it to use setuptools, and outputs the new setup script content. The function should replace all occurrences of Distutils-specific configuration with equivalent setuptools configurations. Function Signature: ```python def migrate_to_setuptools(distutils_setup_script: str) -> str: pass ``` Input: - `distutils_setup_script` (str): A string containing the setup script written using Distutils. Output: - (str): A string containing the converted setup script using setuptools. Example: ```python distutils_script = from distutils.core import setup setup( name=\'example\', version=\'0.1\', packages=[\'example\', \'example.tests\'], url=\'http://pypi.python.org/pypi/example/\', license=\'MIT\', author=\'Author Name\', author_email=\'author@example.com\', description=\'An example package\' ) new_script = migrate_to_setuptools(distutils_script) print(new_script) ``` Expected Output: ```plaintext from setuptools import setup setup( name=\'example\', version=\'0.1\', packages=[\'example\', \'example.tests\'], url=\'http://pypi.python.org/pypi/example/\', license=\'MIT\', author=\'Author Name\', author_email=\'author@example.com\', description=\'An example package\' ) ``` Constraints: - The function should handle all common Distutils configurations and convert them appropriately. - For simplicity, assume the input script is valid and parse friendly. - Include proper error handling for unsupported or unknown configurations. - Output script should comply with the latest setuptools documentation guidelines. Performance Requirements: - The function should efficiently handle setup scripts up to length of 2000 characters. - The converted script should preserve the logical structure and order of configurations. Additional Notes: - This exercise tests comprehension of both legacy and modern Python packaging tools. - Students should demonstrate their ability to manipulate and transform code programmatically.","solution":"def migrate_to_setuptools(distutils_setup_script: str) -> str: Convert a setup script using Distutils to use setuptools. Parameters: distutils_setup_script (str): A string containing the setup script written using Distutils. Returns: str: A string containing the converted setup script using setuptools. # Replace distutils.core import with setuptools new_script = distutils_setup_script.replace(\'from distutils.core import setup\', \'from setuptools import setup\') # Any additional transformations can be added here if needed in the future return new_script"},{"question":"**Question: Analyzing User Account Information** You are provided access to the Unix user account database through the `pwd` module. Your task is to implement a function called `get_user_info` that takes a list of user names and returns a dictionary with the user names as keys. For each user name, the value should be a dictionary with the following keys: \\"uid\\", \\"gid\\", \\"home\\", \\"shell\\". These keys will correspond to the respective attributes from the password database entry. If a user name is not found in the password database, it should not be included in the output dictionary. # Function Signature ```python def get_user_info(user_names: list[str]) -> dict[str, dict[str, str | int]]: pass ``` # Input - user_names: A list of strings, where each string is a user name. # Output - A dictionary where: - The keys are user names. - The values are dictionaries with keys \\"uid\\", \\"gid\\", \\"home\\", and \\"shell\\". # Constraints - Do not use any external libraries other than the built-in `pwd` module. # Example ```python user_names = [\\"root\\", \\"john\\", \\"doe\\"] result = get_user_info(user_names) print(result) ``` Assuming \\"john\\" and \\"doe\\" are not valid user names, and \\"root\\" has the following attributes in the password database: - Numerical user ID (uid): 0 - Numerical group ID (gid): 0 - Home directory: \\"/root\\" - Shell: \\"/bin/bash\\" The output would be: ```python { \\"root\\": { \\"uid\\": 0, \\"gid\\": 0, \\"home\\": \\"/root\\", \\"shell\\": \\"/bin/bash\\" } } ``` # Note 1. You should handle cases where user names do not exist in the password database gracefully by excluding them from the output dictionary. 2. Ensure the function is efficient and handles possible exceptions, such as missing user entries.","solution":"import pwd def get_user_info(user_names: list[str]) -> dict[str, dict[str, str | int]]: user_info = {} for user_name in user_names: try: user = pwd.getpwnam(user_name) user_info[user_name] = { \\"uid\\": user.pw_uid, \\"gid\\": user.pw_gid, \\"home\\": user.pw_dir, \\"shell\\": user.pw_shell } except KeyError: # User not found, do not add to the user_info dictionary continue return user_info"},{"question":"**Objective:** Demonstrate your understanding of covariance estimation using `scikit-learn`\'s `sklearn.covariance` module. You will write code to compute and compare different covariance matrices for a given dataset. **Problem Statement:** You are given a dataset containing a set of observations, and you are required to estimate the covariance matrix using three different methods: 1. **Empirical Covariance** 2. **Ledoit-Wolf Shrinkage** 3. **Oracle Approximating Shrinkage (OAS)**. After estimating the covariance matrices, you are to compare the results using the log-likelihood score provided by the `sklearn.covariance` package. **Function Signature:** ```python def compare_covariance_estimations(data: np.ndarray) -> dict: Estimates covariance matrices using Empirical Covariance, Ledoit-Wolf Shrinkage, and Oracle Approximating Shrinkage (OAS) methods and compares their log-likelihood scores. Parameters: data (np.ndarray): A 2D NumPy array where rows are observations and columns are features. Returns: dict: A dictionary with method names as keys and their log-likelihood scores as values. pass ``` **Input:** - `data`: A 2D NumPy array of shape (n_samples, n_features) containing the dataset. **Output:** - A dictionary with method names (`\'EmpiricalCovariance\'`, `\'LedoitWolf\'`, `\'OAS\'`) as keys and their corresponding log-likelihood scores as values. **Constraints:** - The input dataset should have at least 10 samples with at least 2 features. - You should use the `assume_centered=False` parameter for all estimations. - The implementation should handle potential numerical instabilities. **Example:** ```python import numpy as np data = np.random.rand(100, 5) # 100 samples, 5 features result = compare_covariance_estimations(data) print(result) # Example Output: # { # \'EmpiricalCovariance\': -536.184536, # \'LedoitWolf\': -535.298374, # \'OAS\': -535.402893 # } ``` **Notes:** - Use the `EmpiricalCovariance`, `LedoitWolf`, and `OAS` classes from `sklearn.covariance`. - You can obtain the log-likelihood score using the `score` method of each of these classes. Good luck!","solution":"import numpy as np from sklearn.covariance import EmpiricalCovariance, LedoitWolf, OAS def compare_covariance_estimations(data: np.ndarray) -> dict: Estimates covariance matrices using Empirical Covariance, Ledoit-Wolf Shrinkage, and Oracle Approximating Shrinkage (OAS) methods and compares their log-likelihood scores. Parameters: data (np.ndarray): A 2D NumPy array where rows are observations and columns are features. Returns: dict: A dictionary with method names as keys and their log-likelihood scores as values. # Instantiate the models empirical_cov_model = EmpiricalCovariance(assume_centered=False) ledoit_wolf_model = LedoitWolf(assume_centered=False) oas_model = OAS(assume_centered=False) # Fit the models empirical_cov_model.fit(data) ledoit_wolf_model.fit(data) oas_model.fit(data) # Get the log-likelihood scores empirical_cov_score = empirical_cov_model.score(data) ledoit_wolf_score = ledoit_wolf_model.score(data) oas_score = oas_model.score(data) # Create a dictionary with the results results = { \'EmpiricalCovariance\': empirical_cov_score, \'LedoitWolf\': ledoit_wolf_score, \'OAS\': oas_score } return results"},{"question":"**Objective:** Your task is to implement a Python class to handle multilingual internationalization services using the \\"gettext\\" and \\"locale\\" modules. This class should manage translation catalogs and facilitate the localization of strings in an application. It should also allow switching between languages dynamically. **Requirements:** 1. Implement a class `I18nManager` that supports: - Initializing translations with a specific locale directory and domain. - Fetching translated messages for the current language. - Switching languages at runtime. **Class: `I18nManager`** **Methods:** 1. `__init__(self, localedir: str, domain: str) -> None`: - Initializes the `I18nManager` with a given locale directory and domain. - Sets the default language to English (`\'en\'`). 2. `set_language(self, lang_code: str) -> None`: - Sets the current language to `lang_code`. - Changes the language of translations accordingly. 3. `gettext(self, message: str) -> str`: - Returns the translated message for the current language. - If no translation exists, it should return the original message. 4. `ngettext(self, singular: str, plural: str, n: int) -> str`: - Returns the singular or plural form of the message based on the value of `n`. **Input and Output Formats:** - `__init__(self, localedir: str, domain: str) -> None` - **localedir**: A string representing the directory containing localization files. - **domain**: A string representing the domain of translation messages. - **Returns**: None - `set_language(self, lang_code: str) -> None` - **lang_code**: A string representing the language code (e.g., \'en\', \'es\', \'fr\'). - **Returns**: None - `gettext(self, message: str) -> str` - **message**: A string to be translated. - **Returns**: The translated string in the current language. - `ngettext(self, singular: str, plural: str, n: int) -> str` - **singular**: Singular form of the message. - **plural**: Plural form of the message. - **n**: An integer to determine singular or plural form. - **Returns**: The appropriate singular or plural form based on `n`. **Constraints and Limitations:** - Ensure to handle default fallback to the original message if no translation is available. - The class must handle the dynamic changing of languages efficiently. **Example Usage:** ```python # Assuming the locale directory structure and .mo files exist # locales/ # ├── en # │ └── LC_MESSAGES # │ └── messages.mo # ├── fr # │ └── LC_MESSAGES # │ └── messages.mo i18n = I18nManager(localedir=\'locales\', domain=\'messages\') print(i18n.gettext(\'Hello, World!\')) # Should print \'Hello, World!\' in English i18n.set_language(\'fr\') print(i18n.gettext(\'Hello, World!\')) # Should print \'Bonjour, le monde!\' in French print(i18n.ngettext(\'There is one apple\', \'There are many apples\', 1)) # Should print the singular form in French print(i18n.ngettext(\'There is one apple\', \'There are many apples\', 5)) # Should print the plural form in French ``` **Notes:** - For this assessment, you can assume that .mo files for the example messages exist in the specified directory structure. - Students are not required to create .mo files but should show the class implementation and how it would be used with given assumptions.","solution":"import gettext import locale class I18nManager: def __init__(self, localedir: str, domain: str) -> None: Initializes the I18nManager with a given locale directory and domain. Default language is set to English(\'en\'). self.localedir = localedir self.domain = domain self.current_language = \'en\' self.translations = self._load_translations(\'en\') def _load_translations(self, lang_code: str) -> gettext.GNUTranslations: try: translation = gettext.translation(domain=self.domain, localedir=self.localedir, languages=[lang_code]) except FileNotFoundError: translation = gettext.NullTranslations() return translation def set_language(self, lang_code: str) -> None: Sets the current language to `lang_code`. self.current_language = lang_code self.translations = self._load_translations(lang_code) def gettext(self, message: str) -> str: Returns the translated message for the current language. If no translation exists, it returns the original message. return self.translations.gettext(message) def ngettext(self, singular: str, plural: str, n: int) -> str: Returns the singular or plural form of the message based on the value of `n`. return self.translations.ngettext(singular, plural, n)"},{"question":"**Coding Assessment Question** # Objective: Create a simple FTP (File Transfer Protocol) client using the `asyncore` module that can connect to an FTP server, send commands, and receive responses asynchronously. # Description: Implement an `asyncore`-based FTP client that: 1. Connects to an FTP server (use `ftp.dlptest.com` for testing purposes). 2. Sends an `USER` command with username `dlpuser`. 3. Sends a `PASS` command with password `rNrKYTX9g7z3RgJRmxWuGHbeu`. 4. Retrieves and prints the welcome message and the message following each command\'s execution. # Requirements: - Use `asyncore.dispatcher` for socket communication. - Override necessary methods to handle events such as connecting, reading responses, and sending commands. - Implement the connection sequence: Connect, Send USER command, Send PASS command, Receive responses. # Constraints: - Use only the `asyncore` module for handling asynchronous operations. - Your client should handle responses until the final message is received after sending the PASS command. - The user and password commands should be sent after connecting and receiving the server\'s initial welcome message. # Input and Output: - No direct user input is required. - Output should be printed to the console and should include the welcome message and the responses to the USER and PASS commands. # Example Output: ```plaintext 220 (vsFTPd 3.0.3) 331 Please specify the password. 230 Login successful. ``` # Performance: - Ensure that the client\'s main loop does not block the system (use appropriate timeouts for `asyncore.loop`). - Efficiently handle reading and writing without busy-waiting. ```python import asyncore class FTPClient(asyncore.dispatcher): def __init__(self, host, user, passwd): asyncore.dispatcher.__init__(self) self.create_socket() self.connect((host, 21)) self.buffer = None self.user = user self.passwd = passwd self.step = 0 def handle_connect(self): pass def handle_close(self): self.close() def handle_read(self): response = self.recv(8192).decode(\'utf-8\') print(response) if self.step == 0: self.buffer = f\\"USER {self.user}rn\\".encode(\'utf-8\') self.step = 1 elif self.step == 1: self.buffer = f\\"PASS {self.passwd}rn\\".encode(\'utf-8\') self.step = 2 def writable(self): return self.buffer is not None def handle_write(self): if self.buffer: sent = self.send(self.buffer) self.buffer = self.buffer[sent:] if len(self.buffer) == 0: self.buffer = None client = FTPClient(\'ftp.dlptest.com\', \'dlpuser\', \'rNrKYTX9g7z3RgJRmxWuGHbeu\') asyncore.loop() ```","solution":"import asyncore import socket class FTPClient(asyncore.dispatcher): def __init__(self, host, user, passwd): asyncore.dispatcher.__init__(self) self.create_socket(socket.AF_INET, socket.SOCK_STREAM) self.connect((host, 21)) self.buffer = None self.user = user self.passwd = passwd self.step = 0 def handle_connect(self): pass def handle_close(self): self.close() def handle_read(self): response = self.recv(8192).decode(\'utf-8\') print(response) if self.step == 0: self.buffer = f\\"USER {self.user}rn\\".encode(\'utf-8\') self.step = 1 elif self.step == 1: self.buffer = f\\"PASS {self.passwd}rn\\".encode(\'utf-8\') self.step = 2 def writable(self): return self.buffer is not None def handle_write(self): if self.buffer: sent = self.send(self.buffer) self.buffer = self.buffer[sent:] if len(self.buffer) == 0: self.buffer = None # Uncomment the next lines to run the client # client = FTPClient(\'ftp.dlptest.com\', \'dlpuser\', \'rNrKYTX9g7z3RgJRmxWuGHbeu\') # asyncore.loop()"},{"question":"# Bytearray Manipulation Challenge You are tasked with creating several functions to manipulate `bytearray` objects utilizing the given C API functions. These functions will be used to create, concatenate, resize, and inspect bytearrays. Implement the following functions: 1. **`create_bytearray_from_string`**: - **Input**: A string `s`. - **Output**: A new bytearray object created from the string. - **Function Signature**: `def create_bytearray_from_string(s: str) -> bytearray` 2. **`concat_bytearrays`**: - **Input**: Two bytearray objects `a` and `b`. - **Output**: A new bytearray object containing the concatenation of `a` and `b`. - **Function Signature**: `def concat_bytearrays(a: bytearray, b: bytearray) -> bytearray` 3. **`resize_bytearray`**: - **Input**: A bytearray object `ba` and an integer `new_size`. - **Output**: The same bytearray object resized to `new_size`. - **Function Signature**: `def resize_bytearray(ba: bytearray, new_size: int) -> bytearray` 4. **`bytearray_size`**: - **Input**: A bytearray object `ba`. - **Output**: The size (length) of the bytearray. - **Function Signature**: `def bytearray_size(ba: bytearray) -> int` 5. **`bytearray_to_string`**: - **Input**: A bytearray object `ba`. - **Output**: A string representation of the bytearray. - **Function Signature**: `def bytearray_to_string(ba: bytearray) -> str` # Example ```python s = \\"example\\" ba = create_bytearray_from_string(s) print(bytearray_to_string(ba)) # Output: \'example\' ba2 = create_bytearray_from_string(\\"text\\") concat_ba = concat_bytearrays(ba, ba2) print(bytearray_to_string(concat_ba)) # Output: \'exampletext\' resize_bytearray(concat_ba, 6) print(bytearray_to_string(concat_ba)) # Output: \'exampl\' print(bytearray_size(concat_ba)) # Output: 6 ``` # Constraints - Input strings will only contain ASCII characters. - `new_size` for resizing will always be a non-negative integer. - Functions should handle NULL checks and potential errors internally. Implement these functions in Python by leveraging the described C API operations as if they are available in the same environment.","solution":"def create_bytearray_from_string(s: str) -> bytearray: Creates a bytearray from the given string. return bytearray(s, \'utf-8\') def concat_bytearrays(a: bytearray, b: bytearray) -> bytearray: Concatenates two bytearrays and returns a new bytearray. return a + b def resize_bytearray(ba: bytearray, new_size: int) -> bytearray: Resizes the given bytearray to the new size. ba = ba[:new_size] + bytearray(max(0, new_size - len(ba))) return ba def bytearray_size(ba: bytearray) -> int: Returns the size (length) of the bytearray. return len(ba) def bytearray_to_string(ba: bytearray) -> str: Converts a bytearray to its string representation. return ba.decode(\'utf-8\')"},{"question":"**Out-of-Core Learning with scikit-learn** As a data scientist, you are tasked with building an out-of-core learning system to classify text data that is too large to fit into memory. You need to implement a system that reads and processes the data in mini-batches, extracts features using the hashing trick, and incrementally trains a classifier. # Objectives: 1. **Stream Data**: Implement a function to read data from a large file in mini-batches. 2. **Feature Extraction**: Use scikit-learn\'s `HashingVectorizer` to convert text data into suitable numeric features. 3. **Incremental Learning**: Incrementally train a classifier using the partial_fit method. # Data: The data is a CSV file `large_text_data.csv` with two columns: `text` (the text document) and `label` (the class). # Requirements: 1. **Function to Stream Data**: - Implement a function `stream_data(file_path, batch_size)` that yields batches of examples from the CSV file. - **Input**: `file_path` (string) - the path to the CSV file, `batch_size` (int) - the number of examples in each batch. - **Output**: Yields tuples of numpy arrays `(X_batch, y_batch)`. 2. **Function to Train Incremental Model**: - Implement a function `train_incremental_model(file_path, batch_size, model, vectorizer, classes)` that trains the model incrementally. - **Input**: - `file_path` (string) - the path to the CSV file. - `batch_size` (int) - the size of each mini-batch. - `model` - a scikit-learn estimator that supports `partial_fit`. - `vectorizer` - an instance of `HashingVectorizer`. - `classes` - a list of all possible classes for the classifier. - **Output**: The trained model. # Constraints: - Use only the `HashingVectorizer` for feature extraction. - Models to be used for classification must support `partial_fit`. Examples include `MultinomialNB`, `SGDClassifier`, etc. # Performance Expectation: - Ensure that the entire script runs efficiently even for very large datasets that exceed available memory, by processing and loading limited-sized batches at a time. # Example Usage: ```python from sklearn.feature_extraction.text import HashingVectorizer from sklearn.linear_model import SGDClassifier file_path = \'large_text_data.csv\' batch_size = 1000 classes = [\'class1\', \'class2\', \'class3\'] vectorizer = HashingVectorizer(n_features=2**20) model = SGDClassifier() trained_model = train_incremental_model(file_path, batch_size, model, vectorizer, classes) ``` In the implementation, provide proper error handling and ensure the code is well-documented.","solution":"import pandas as pd import numpy as np from sklearn.feature_extraction.text import HashingVectorizer from sklearn.linear_model import SGDClassifier def stream_data(file_path, batch_size): Generator function to read data from a large CSV file in mini-batches. Args: file_path (str): The path to the CSV file. batch_size (int): The number of examples in each batch. Yields: Tuple[np.array, np.array]: Batches of features and labels. for chunk in pd.read_csv(file_path, chunksize=batch_size): X_batch = chunk[\'text\'].values y_batch = chunk[\'label\'].values yield X_batch, y_batch def train_incremental_model(file_path, batch_size, model, vectorizer, classes): Trains the model incrementally using mini-batches of data. Args: file_path (str): The path to the CSV file. batch_size (int): The size of each mini-batch. model (sklearn estimator): A scikit-learn estimator that supports `partial_fit`. vectorizer (HashingVectorizer): An instance of `HashingVectorizer`. classes (list): A list of all possible classes for the classifier. Returns: sklearn estimator: The trained model. for X_batch, y_batch in stream_data(file_path, batch_size): X_transformed = vectorizer.transform(X_batch) model.partial_fit(X_transformed, y_batch, classes=classes) return model"},{"question":"# Seaborn Heatmap Customization Problem Description You are given a dataset `students_scores`, which contains scores of different students across various subjects. Your task is to perform the following operations to create a heatmap: 1. Load the dataset and pivot it so that the students are in the rows and subjects are in the columns. 2. Create a heatmap where the cell values are annotated. 3. Customize the annotations to show one decimal point. 4. Add lines between the cells. 5. Choose a colormap that you find visually appealing. 6. Set the colormap normalization with the minimum score as `0` and the maximum score as `100`. 7. Adjust the plot so that the x-axis labels are on top and there are no x-axis and y-axis labels. Input - A pandas DataFrame named `students_scores` with the following columns: - `Student` (string): The name of the student. - `Subject` (string): The name of the subject. - `Score` (float): The score of the student in that subject. Output - A heatmap plot as described above. Constraints - You must use the seaborn library to create the heatmap. - You can assume that the `students_scores` DataFrame is not empty and contains valid data. Example Consider the following DataFrame: ```python import pandas as pd data = { \'Student\': [\'Alice\', \'Bob\', \'Charlie\', \'Alice\', \'Bob\', \'Charlie\'], \'Subject\': [\'Math\', \'Math\', \'Math\', \'Science\', \'Science\', \'Science\'], \'Score\': [85, 90, 78, 92, 88, 81] } students_scores = pd.DataFrame(data) ``` Your function should create a heatmap that: - Shows students (\'Alice\', \'Bob\', \'Charlie\') as rows. - Shows subjects (\'Math\', \'Science\') as columns. - Annotates cell values with one decimal point. - Adds lines between cells. - Uses a visually appealing colormap. - Sets colormap normalization with `vmin=0` and `vmax=100`. - Adjusts the x-axis labels to be on top and removes axis labels. ```python import seaborn as sns import matplotlib.pyplot as plt def create_custom_heatmap(students_scores): pivot_df = students_scores.pivot(index=\\"Student\\", columns=\\"Subject\\", values=\\"Score\\") ax = sns.heatmap(pivot_df, annot=True, fmt=\\".1f\\", linewidth=0.5, cmap=\\"coolwarm\\", vmin=0, vmax=100) ax.set(xlabel=\\"\\", ylabel=\\"\\") ax.xaxis.tick_top() plt.show() # Using the example DataFrame to create the heatmap create_custom_heatmap(students_scores) ``` In this example, your code should produce a heatmap with annotations, lines between cells, and formatted as described above.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def create_custom_heatmap(students_scores): Creates a customized heatmap from the given students_scores DataFrame. :param students_scores: A pandas DataFrame containing student scores with columns \'Student\', \'Subject\', \'Score\' # Pivot the DataFrame pivot_df = students_scores.pivot(index=\\"Student\\", columns=\\"Subject\\", values=\\"Score\\") # Create the heatmap ax = sns.heatmap( pivot_df, annot=True, fmt=\\".1f\\", linewidths=0.5, cmap=\\"coolwarm\\", vmin=0, vmax=100 ) # Customize the plot ax.set(xlabel=\\"\\", ylabel=\\"\\") ax.xaxis.tick_top() plt.show()"},{"question":"**Problem: Implement and Script a Neural Network with TorchScript Annotations** **Objective**: You are required to implement a simple feedforward neural network in PyTorch and then convert it to TorchScript to ensure it is statically typed and optimally executable. Your task is to: 1. Define a PyTorch module representing a feedforward neural network. 2. Use appropriate type annotations to ensure the model can be successfully scripted using TorchScript. 3. Ensure that the network supports asynchronous execution using TorchScript\'s `fork` and `wait` functions. **Requirements**: 1. The network should have two hidden layers with ReLU activation functions and an output layer. 2. You should script the model by implementing it as a TorchScript module. 3. Use type annotations for all function parameters and return types. 4. Implement a method in the module to perform asynchronous forward passes using `torch.jit.fork` and `torch.jit.wait`. **Specifications**: - **Input**: - Initialize the network with `input_size`, `hidden_size`, and `output_size`. - The `forward` method should accept a `torch.Tensor`. - **Output**: - The `forward` method should return a `torch.Tensor` representing the model\'s output. - **Constraints**: - `input_size`, `hidden_size`, and `output_size` should be integers. - Use PyTorch\'s `nn.Linear` layers for the network. - Include appropriate TorchScript annotations to ensure the entire model can be scripted. **Performance**: - Ensure your implementation handles tensors efficiently, focusing on minimizing overhead in type checking and execution. **Example**: ```python import torch import torch.nn as nn from typing import Tuple class FeedForwardNN(nn.Module): def __init__(self, input_size: int, hidden_size: int, output_size: int): super(FeedForwardNN, self).__init__() self.layer1 = nn.Linear(input_size, hidden_size) self.layer2 = nn.Linear(hidden_size, hidden_size) self.output_layer = nn.Linear(hidden_size, output_size) def forward(self, x: torch.Tensor) -> torch.Tensor: x = torch.relu(self.layer1(x)) x = torch.relu(self.layer2(x)) return self.output_layer(x) @torch.jit.export def async_forward(self, x: torch.Tensor) -> torch.Tensor: fut1 = torch.jit.fork(self.layer1, x) x1 = torch.relu(torch.jit.wait(fut1)) fut2 = torch.jit.fork(self.layer2, x1) x2 = torch.relu(torch.jit.wait(fut2)) fut3 = torch.jit.fork(self.output_layer, x2) return torch.jit.wait(fut3) # Instantiate and script the model model = FeedForwardNN(input_size=10, hidden_size=20, output_size=5) scripted_model = torch.jit.script(model) # Test the scripted model test_input = torch.randn(1, 10) print(scripted_model(test_input)) print(scripted_model.async_forward(test_input)) ``` Ensure your module handles typical tensor inputs and verify the correctness of both synchronous and asynchronous forward methods.","solution":"import torch import torch.nn as nn from typing import Tuple class FeedForwardNN(nn.Module): def __init__(self, input_size: int, hidden_size: int, output_size: int): super(FeedForwardNN, self).__init__() self.layer1 = nn.Linear(input_size, hidden_size) self.layer2 = nn.Linear(hidden_size, hidden_size) self.output_layer = nn.Linear(hidden_size, output_size) def forward(self, x: torch.Tensor) -> torch.Tensor: x = torch.relu(self.layer1(x)) x = torch.relu(self.layer2(x)) return self.output_layer(x) @torch.jit.export def async_forward(self, x: torch.Tensor) -> torch.Tensor: fut1 = torch.jit.fork(self.layer1, x) x1 = torch.relu(torch.jit.wait(fut1)) fut2 = torch.jit.fork(self.layer2, x1) x2 = torch.relu(torch.jit.wait(fut2)) fut3 = torch.jit.fork(self.output_layer, x2) return torch.jit.wait(fut3) # Instantiate and script the model model = FeedForwardNN(input_size=10, hidden_size=20, output_size=5) scripted_model = torch.jit.script(model)"},{"question":"**Objective**: Demonstrate your understanding of Python\'s instance method and method objects by implementing a class that manages the creation and manipulation of these objects. **Question**: You are tasked with implementing a class `MethodManager` that provides methods for creating and interacting with instance methods and method objects. Your implementation should: 1. **Create Instance Method Objects**: - Implement a method `create_instance_method` that takes a callable `func` and returns an instance method object. ```python def create_instance_method(self, func: Callable) -> Any: pass ``` 2. **Check Instance Method Objects**: - Implement a method `is_instance_method` that takes an object `o` and returns `True` if the object is an instance method, and `False` otherwise. ```python def is_instance_method(self, o: Any) -> bool: pass ``` 3. **Retrieve Function from Instance Method**: - Implement a method `get_instance_method_function` that takes an instance method object `im` and returns the function associated with it. ```python def get_instance_method_function(self, im: Any) -> Callable: pass ``` 4. **Create Method Objects**: - Implement a method `create_method` that takes a callable `func` and an instance `self_instance`, and returns a method object. ```python def create_method(self, func: Callable, self_instance: Any) -> Any: pass ``` 5. **Check Method Objects**: - Implement a method `is_method` that takes an object `o` and returns `True` if the object is a method, and `False` otherwise. ```python def is_method(self, o: Any) -> bool: pass ``` 6. **Retrieve Function from Method**: - Implement a method `get_method_function` that takes a method object `meth` and returns the function associated with it. ```python def get_method_function(self, meth: Any) -> Callable: pass ``` 7. **Retrieve Instance from Method**: - Implement a method `get_method_self` that takes a method object `meth` and returns the instance associated with it. ```python def get_method_self(self, meth: Any) -> Any: pass ``` **Constraints**: - You must use the provided interface for `PyInstanceMethod_New`, `PyInstanceMethod_Check`, `PyInstanceMethod_Function`, `PyMethod_New`, `PyMethod_Check`, `PyMethod_Function`, and `PyMethod_Self` to implement these methods. - Assume proper imports and necessary packages are available for handling `PyObject` and related functions. **Input and Output**: - The methods should handle any valid callable or instance type for `func` and `self_instance`. Return types should match the expected ones as described above. # Example: Suppose you have the following functions and instances: ```python def example_function(): return \\"Example Function\\" class ExampleClass: def class_method(self): return \\"Class Method\\" ``` Then, using the `MethodManager` should produce results such as: ```python manager = MethodManager() # Create and check instance method instance_method = manager.create_instance_method(example_function) assert manager.is_instance_method(instance_method) == True assert manager.get_instance_method_function(instance_method) == example_function # Create and check method object example_instance = ExampleClass() method_object = manager.create_method(example_instance.class_method, example_instance) assert manager.is_method(method_object) == True assert manager.get_method_function(method_object) == example_instance.class_method assert manager.get_method_self(method_object) == example_instance ``` Implement the `MethodManager` class according to the specifications given to pass these assertions.","solution":"class MethodManager: def create_instance_method(self, func): Create an instance method object from a callable function. import types return types.MethodType(func, self) def is_instance_method(self, o): Check if an object is an instance method. import types return isinstance(o, types.MethodType) def get_instance_method_function(self, im): Retrieve the function associated with an instance method object. return im.__func__ def create_method(self, func, self_instance): Create a method object from a callable function and an instance. import types return types.MethodType(func, self_instance) def is_method(self, o): Check if an object is a method. import types return isinstance(o, types.MethodType) def get_method_function(self, meth): Retrieve the function associated with a method object. return meth.__func__ def get_method_self(self, meth): Retrieve the instance associated with a method object. return meth.__self__"},{"question":"# Custom Import Mechanism Using `zipimport` **Objective:** Create a custom module importer that can import Python modules from a given ZIP file. This will demonstrate an understanding of Python\'s `zipimport` module as well as the general import system. **Problem Statement:** You are tasked with creating a module `CustomZipImporter` that allows importing Python modules directly from a ZIP archive. The ZIP archive will contain multiple Python files, and your custom importer should enable importing them as if they were regular Python modules. **Instructions:** 1. Implement a class `CustomZipImporter` that accepts a file path to a ZIP archive during initialization. 2. The class should have a method `import_module(module_name)` which takes the name of a module (without the `.py` extension) and returns the imported module. 3. Ensure that attempting to import a non-existent module raises an appropriate exception. 4. Your implementation should support importing multiple modules from the ZIP archive for use within the same script. **Example Usage:** Suppose you have a ZIP file `modules.zip` containing the following files: - `module1.py` - `module2.py` ```python import sys import zipimport class CustomZipImporter: def __init__(self, zip_path): self.zip_path = zip_path self.importer = zipimport.zipimporter(zip_path) def import_module(self, module_name): if module_name+\'.py\' in self.importer._files: module = self.importer.load_module(module_name) return module else: raise ImportError(f\\"Module {module_name} not found in {self.zip_path}\\") # Usage zip_importer = CustomZipImporter(\'modules.zip\') module1 = zip_importer.import_module(\'module1\') module1.some_function() module2 = zip_importer.import_module(\'module2\') module2.another_function() ``` **Input:** - A string `zip_path` representing the file path to the ZIP archive. - A string `module_name` representing the name of the module to import. **Output:** - The imported module object which can be used to access functions, classes, etc., from the module. **Constraints:** - The ZIP file must only contain Python files (`*.py`). - Handle exceptions and edge cases such as non-existent files or invalid ZIP archive formats gracefully.","solution":"import zipimport class CustomZipImporter: def __init__(self, zip_path): Initializes the CustomZipImporter with the path to the ZIP file. :param zip_path: Path to the ZIP file containing the Python modules. self.zip_path = zip_path self.importer = zipimport.zipimporter(zip_path) def import_module(self, module_name): Imports the specified module from the ZIP file. :param module_name: Name of the module to import (without the .py extension) :return: Imported module object :raises ImportError: If the module is not found in the ZIP file. try: module = self.importer.load_module(module_name) return module except zipimport.ZipImportError as e: raise ImportError(f\\"Module {module_name} not found in {self.zip_path}\\") from e"},{"question":"**Problem Statement**: You are given a matrix `A` of size `N x M` initialized in the default CUDA stream. You need to perform the following operations on this matrix using multiple CUDA streams: 1. Multiply each element of `A` by 3 in stream 1. 2. Add 5 to each element of `A` in stream 2. 3. Compute the element-wise square of `A` in stream 3. Ensure that these operations do not cause any data races by implementing the necessary synchronization mechanisms. **Function Signature**: ```python def transform_matrix(A: torch.Tensor) -> torch.Tensor: pass ``` **Input**: - `A`: A `torch.Tensor` of shape `(N, M)` initialized on the default CUDA stream. **Output**: - Returns the transformed tensor after applying all operations in the correct, synchronized order. **Constraints**: - Do not use CPU tensors. All operations should be performed on GPU. - Ensure that proper synchronization is handled to prevent data races. **Example**: ```python import torch N, M = 4, 4 A = torch.rand(N, M, device=\\"cuda\\") # Before transformation print(A) # After transformation transformed_A = transform_matrix(A) print(transformed_A) ``` **Expected Output Explanation**: 1. Each element in `A` is multiplied by 3. 2. 5 is added to all elements of `A`. 3. Each element of `A` is squared. **Guidelines**: - Use `torch.cuda.Stream()` to create new streams. - Utilize synchronization methods such as `torch.cuda.current_stream().wait_stream()` to ensure proper synchronization. - Test your function with various input sizes and initial values to verify correctness and absence of data races. **Performance Requirements**: - Your implementation should handle large matrices efficiently. - Ensure the operations are performed concurrently where possible, but without causing data races.","solution":"import torch def transform_matrix(A: torch.Tensor) -> torch.Tensor: # Ensure input is on GPU assert A.is_cuda, \\"Tensor must be on CUDA device\\" # Create streams stream1 = torch.cuda.Stream() stream2 = torch.cuda.Stream() stream3 = torch.cuda.Stream() # Multiplies each element by 3 in stream1 with torch.cuda.stream(stream1): B = A * 3 # Ensuring stream2 waits for stream1 to complete torch.cuda.current_stream().wait_stream(stream1) # Adds 5 to each element in stream2 with torch.cuda.stream(stream2): C = B + 5 # Ensuring stream3 waits for stream2 to complete torch.cuda.current_stream().wait_stream(stream2) # Computes the element-wise square in stream3 with torch.cuda.stream(stream3): D = C ** 2 # Ensure D computation is completed stream3.synchronize() return D"},{"question":"Fine-Grained Control of TorchDynamo Tracing **Objective**: Implement a portion of a PyTorch model using the TorchDynamo fine-grained tracing control APIs to manage parts of the model that may not be compatible with default tracing behavior. Background You are provided with a simple PyTorch model that has one function causing issues during the `torch.compile` process. Your task is to use the appropriate TorchDynamo APIs to ensure the model runs smoothly, selectively disabling or allowing portions of the code in the tracing operation. Functional Requirements 1. **Model Specification**: - Implement a PyTorch model with three functions: `fn`, `a_fn`, and `b_fn`. - Decorate `a_fn` such that TorchDynamo bypasses this function without breaking the model\'s overall functionality. 2. **Functions**: - `fn` calls `a_fn` and `b_fn`. - `a_fn` is the problematic function in terms of tracing. - `b_fn` should remain unaffected. 3. **Use APIs**: - Utilize `torch.compiler.disable` to decorate `a_fn`. - Ensure that `fn` and `b_fn` are traced correctly. Input and Output - **Input**: - No external input is required. - You should manually call the `fn` function to test its behavior. - **Output**: - The code should run without errors when `fn` is executed. - Print statements should show the flow through the different functions for easy validation. Performance Requirements - Ensure that the solution runs efficiently, even though handling is mostly about bypassing certain functions. Constraints - Use `torch.compiler.disable` only where necessary. - Ensure other parts of the model still leverage TorchDynamo\'s capabilities. Sample Code Template ```python import torch # Define the functions def aa_fn(x): return x + 1 # Apply torch.compiler.disable on a_fn @torch.compiler.disable def a_fn(x): return aa_fn(x) + 2 def b_fn(x): return x * 3 # Define the main function that uses the other functions def fn(x): return a_fn(x) + b_fn(x) # Compile the main function with torch.compile compiled_fn = torch.compile(fn) # Execute the compiled function x = torch.tensor(5) result = compiled_fn(x) print(\\"Result:\\", result) ``` **Note**: Ensure that the `a_fn` function is correctly disabled using `torch.compiler.disable` and verify the output meets the expectations.","solution":"import torch # Placeholder function to simulate the effect of torch.compiler.disable (actual tracing) def torch_compile_disable(func): def wrapper(*args, **kwargs): # Directly call the function without any tracing return func(*args, **kwargs) return wrapper # Define the functions as described def affected_fn(x): return x + 1 # Apply torch.compiler.disable on a_fn @torch_compile_disable # Simulating torch.compiler.disable def a_fn(x): return affected_fn(x) + 2 def b_fn(x): return x * 3 # Define the main function that uses the other functions def fn(x): return a_fn(x) + b_fn(x) # Compile the main function with torch.compile def mock_compile(func): def wrapper(*args, **kwargs): # Perform the normal function invocation for the sake of this example return func(*args, **kwargs) return wrapper compiled_fn = mock_compile(fn) # Simulating torch.compile # Execute the compiled function if __name__ == \\"__main__\\": x = torch.tensor(5) result = compiled_fn(x) print(\\"Result:\\", result)"},{"question":"# Covariance Matrix Estimation and Comparison Given a dataset, your task is to implement and compare different covariance estimation techniques using scikit-learn\'s `covariance` package. You need to: 1. Compute the Empirical Covariance Matrix. 2. Apply Shrunk Covariance with a specified shrinkage coefficient. 3. Use Ledoit-Wolf Shrinkage to compute an optimized covariance estimate. 4. Use Oracle Approximating Shrinkage (OAS) to compute a covariance estimate. 5. Use the Minimum Covariance Determinant (MCD) to compute a robust covariance estimate. 6. Compare the performance of these methods using a synthetic dataset. # Instructions: 1. Generate a synthetic dataset with `n_samples=100` and `n_features=3` where some samples include outliers. 2. Implement the required covariance estimators. 3. Compare the results by printing and plotting the covariance matrices. 4. Discuss in a markdown cell the differences and scenarios where each method might be beneficial. # Provided Functions: You are allowed to use the following scikit-learn functions: - `empirical_covariance` - `ShrunkCovariance`, `shrunk_covariance` - `LedoitWolf`, `ledoit_wolf` - `OAS` - `MinCovDet` # Expected Output: 1. Covariance matrices printed for each estimation technique. 2. Visual plots showing covariance matrices. 3. A concise discussion in markdown format. # Sample Template: ```python import numpy as np import matplotlib.pyplot as plt from sklearn.covariance import (empirical_covariance, LedoitWolf, OAS, ShrunkCovariance, MinCovDet, shrunk_covariance) # Step 1: Generate synthetic data np.random.seed(0) n_samples = 100 n_features = 3 data = np.random.randn(n_samples, n_features) # Introduce outliers outliers = np.random.uniform(low=-10, high=10, size=(10, n_features)) data_with_outliers = np.vstack((data, outliers)) # Step 2: Compute covariance matrices emp_cov = empirical_covariance(data_with_outliers) shrinkage = 0.1 shrink_cov = shrunk_covariance(emp_cov, shrinkage=shrinkage) lw = LedoitWolf().fit(data_with_outliers).covariance_ oas = OAS().fit(data_with_outliers).covariance_ mcd = MinCovDet().fit(data_with_outliers).covariance_ # Step 3: Print and plot covariance matrices print(f\\"Empirical Covariance:n{emp_cov}n\\") print(f\\"Shrunk Covariance (shrinkage={shrinkage}):n{shrink_cov}n\\") print(f\\"Ledoit-Wolf Covariance:n{lw}n\\") print(f\\"OAS Covariance:n{oas}n\\") print(f\\"Robust MCD Covariance:n{mcd}n\\") # Plot covariance matrices fig, axes = plt.subplots(2, 3, figsize=(15, 10)) axes[0, 0].imshow(emp_cov, cmap=\'hot\', interpolation=\'nearest\') axes[0, 0].set_title(\\"Empirical Covariance\\") axes[0, 1].imshow(shrink_cov, cmap=\'hot\', interpolation=\'nearest\') axes[0, 1].set_title(\\"Shrunk Covariance\\") axes[0, 2].imshow(lw, cmap=\'hot\', interpolation=\'nearest\') axes[0, 2].set_title(\\"Ledoit-Wolf Covariance\\") axes[1, 0].imshow(oas, cmap=\'hot\', interpolation=\'nearest\') axes[1, 0].set_title(\\"OAS Covariance\\") axes[1, 1].imshow(mcd, cmap=\'hot\', interpolation=\'nearest\') axes[1, 1].set_title(\\"MCD Covariance\\") plt.tight_layout() plt.show() # Step 4: Discussion ``` # Constraints: 1. Assume labeled data is not required due to the unsupervised nature of covariance estimation. 2. Proper handling of potential numerical instability in covariance estimation. 3. Clearly indicate the added value of each covariance estimation method through discussion. # Performance Considerations: - Ensure that your implementation executes within a reasonable time frame (less than 5 seconds for provided data dimensions). - Efficiently handle matrix operations using numpy or relevant scikit-learn methods.","solution":"import numpy as np import matplotlib.pyplot as plt from sklearn.covariance import (empirical_covariance, LedoitWolf, OAS, ShrunkCovariance, MinCovDet, shrunk_covariance) def generate_data(n_samples=100, n_features=3): np.random.seed(0) data = np.random.randn(n_samples, n_features) outliers = np.random.uniform(low=-10, high=10, size=(10, n_features)) return np.vstack((data, outliers)) def compute_covariance_matrices(data, shrinkage=0.1): emp_cov = empirical_covariance(data) shrink_cov = shrunk_covariance(emp_cov, shrinkage=shrinkage) lw_cov = LedoitWolf().fit(data).covariance_ oas_cov = OAS().fit(data).covariance_ mcd_cov = MinCovDet().fit(data).covariance_ return emp_cov, shrink_cov, lw_cov, oas_cov, mcd_cov def plot_covariance_matrices(cov_matrices, titles): fig, axes = plt.subplots(2, 3, figsize=(15, 10)) for ax, cov, title in zip(axes.flatten(), cov_matrices, titles): ax.imshow(cov, cmap=\'hot\', interpolation=\'nearest\') ax.set_title(title) plt.tight_layout() plt.show() # Step 1: Generate synthetic data data_with_outliers = generate_data() # Step 2: Compute covariance matrices emp_cov, shrink_cov, lw_cov, oas_cov, mcd_cov = compute_covariance_matrices(data_with_outliers, shrinkage=0.1) # Step 3: Print covariance matrices print(f\\"Empirical Covariance:n{emp_cov}n\\") print(f\\"Shrunk Covariance (shrinkage=0.1):n{shrink_cov}n\\") print(f\\"Ledoit-Wolf Covariance:n{lw_cov}n\\") print(f\\"OAS Covariance:n{oas_cov}n\\") print(f\\"Robust MCD Covariance:n{mcd_cov}n\\") # Plot covariance matrices cov_matrices = [emp_cov, shrink_cov, lw_cov, oas_cov, mcd_cov] titles = [ \\"Empirical Covariance\\", \\"Shrunk Covariance\\", \\"Ledoit-Wolf Covariance\\", \\"OAS Covariance\\", \\"MCD Covariance\\" ] plot_covariance_matrices(cov_matrices, titles)"},{"question":"# Coroutine Object Manipulation **Objective:** Write a Python function that takes two inputs: a list of coroutine functions and a delay in seconds. The function must execute these coroutines concurrently with the specified delay between subsequent starts. You must demonstrate your understanding of coroutine objects and asynchronous programming by using `async` and `await` keywords. **Function Signature:** ```python import asyncio from typing import List, Callable async def execute_coroutines_with_delay(coroutines: List[Callable[[], asyncio.Future]], delay: int) -> None: pass ``` **Input:** - `coroutines`: A list of coroutine functions (each a zero-argument function returning a coroutine object). - `delay`: The delay in seconds between starting each subsequent coroutine. **Output:** - The function does not return anything but should print the results of each coroutine as they complete. **Constraints:** - Each coroutine function will complete in a reasonable amount of time (less than 10 seconds). - The `delay` will be a non-negative integer not greater than 5. **Example Usage:** ```python import asyncio async def sample_coroutine_1(): await asyncio.sleep(1) print(\\"sample_coroutine_1 completed\\") return \\"result_1\\" async def sample_coroutine_2(): await asyncio.sleep(2) print(\\"sample_coroutine_2 completed\\") return \\"result_2\\" async def sample_coroutine_3(): await asyncio.sleep(3) print(\\"sample_coroutine_3 completed\\") return \\"result_3\\" coroutines = [sample_coroutine_1, sample_coroutine_2, sample_coroutine_3] await execute_coroutines_with_delay(coroutines, 1) ``` **Explanation:** In the example provided: - `sample_coroutine_1` starts immediately and completes after 1 second. - `sample_coroutine_2` starts 1 second after `sample_coroutine_1` starts and completes after 2 more seconds. - `sample_coroutine_3` starts 1 second after `sample_coroutine_2` starts and completes after 3 more seconds. The function should manage the delays appropriately and handle the concurrent execution of these coroutines. **Note:** Ensure your solution properly handles potential exceptions within coroutine execution and prints the results in the order of their completion. Use Python\'s `asyncio` library efficiently.","solution":"import asyncio from typing import List, Callable async def execute_coroutines_with_delay(coroutines: List[Callable[[], asyncio.Future]], delay: int) -> None: async def wrapper(coro_fun, idx): result = await coro_fun() print(f\\"Coroutine {idx+1} completed with result: {result}\\") tasks = [] for idx, coro in enumerate(coroutines): task = asyncio.create_task(wrapper(coro, idx)) tasks.append(task) await asyncio.sleep(delay) await asyncio.gather(*tasks)"},{"question":"**Objective:** Implement a program in Python that monitors multiple file descriptors using the `select` module and efficiently handles I/O events for a socket and a file. # Problem Statement: You are required to implement a function `monitor_io(file_path: str, port: int, timeout: float) -> None` that: 1. Opens a file at the given `file_path` for reading. 2. Sets up a TCP server socket that listens on the given `port`. 3. Uses the `select.select()` method to monitor both the file and the socket for readiness to perform I/O operations. 4. Handles events as follows: - When the file is ready for reading, read a line from the file and print it. - When the socket is ready for reading, accept a new connection and print the data received from the client until the connection is closed. 5. If no events are detected within the `timeout` period, the function should print \\"No I/O events detected\\". # Function Signature: ```python def monitor_io(file_path: str, port: int, timeout: float) -> None: pass ``` # Input: - `file_path` (str): Path to the file you need to monitor for read events. - `port` (int): Port number on which the TCP server socket will listen for incoming connections. - `timeout` (float): The maximum time (in seconds) to wait for an I/O event before printing \\"No I/O events detected\\". # Constraints: - The file given by `file_path` exists and is readable. - The `port` is a valid and open port not being used by any other service. # Example Usage: ```python monitor_io(\'/path/to/file.txt\', 12345, 5.0) ``` # Additional Requirements: - The function should handle exceptions that may occur during file or socket operations. - The function should clean up resources (close the file and socket) before terminating. # Performance Considerations: - Ensure that the implementation efficiently handles the case where there are many lines in the file or many incoming connections on the socket. - The solution must work on both Unix and Windows platforms, but file monitoring is only required to handle Unix-based systems. # Hints: - Utilize the `select` module\'s `select.select()` function for monitoring file descriptors simultaneously. - Use non-blocking mode on the socket to prevent it from blocking the entire program when performing I/O operations.","solution":"import select import socket def monitor_io(file_path: str, port: int, timeout: float) -> None: Monitor file and socket for I/O events and handle them appropriately. try: # Open the file for reading file = open(file_path, \'r\') # Create and set up the TCP server socket server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM) server_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1) server_socket.bind((\'0.0.0.0\', port)) server_socket.listen() server_socket.setblocking(0) # Set to non-blocking mode # List of file descriptors to monitor for reading inputs = [file, server_socket] while True: # Use select to monitor file descriptors readable, _, _ = select.select(inputs, [], [], timeout) if not readable: print(\\"No I/O events detected\\") break for r in readable: if r is file: # File is ready for reading line = r.readline() if line: print(f\\"File line: {line.strip()}\\") else: # EOF reached inputs.remove(r) r.close() print(\\"End of file reached\\") elif r is server_socket: # Server socket is ready to accept a connection conn, addr = r.accept() print(f\\"Connection from {addr}\\") conn.setblocking(0) while True: data = conn.recv(1024) if data: print(f\\"Received: {data.decode()}\\") else: # No more data from client (connection closed) conn.close() print(f\\"Connection closed by {addr}\\") break except Exception as e: print(f\\"An error occurred: {e}\\") finally: # Clean up resources if \'file\' in locals() and not file.closed: file.close() if \'server_socket\' in locals(): server_socket.close()"},{"question":"**Problem Statement:** You are tasked with writing a Python function that processes a given string. This function should identify ASCII control characters within the string, replace them with their mnemonic symbols, and return the modified string. Control characters are non-printable characters that typically have a special purpose, such as signaling the end of a line or providing flow control. **Function Signature:** ```python def replace_control_chars(input_string: str) -> str: pass ``` **Input:** - `input_string` (str): A string containing ASCII characters. **Output:** - (str): A string where each control character in the input string is replaced by its mnemonic symbol. **Constraints:** - The input string can contain any printable or non-printable ASCII character. - Only ASCII characters should be considered. Non-ASCII characters should be ignored for the purpose of this task. - The control characters and their corresponding mnemonics are defined in the `curses.ascii` module. **Performance Requirements:** - The solution should be efficient enough to handle input strings of length up to 100,000 characters. **Example:** ```python print(replace_control_chars(\\"Hellox00World!\\")) # Output should be: \\"HelloNULWorld!\\" print(replace_control_chars(\\"x07Hix1BThere!\\")) # Output should be: \\"BELHiESCThere!\\" ``` **Hint:** - Use the `curses.ascii.unctrl` function to retrieve the string representation of ASCII control characters. Write your implementation of the `replace_control_chars` function below:","solution":"import curses.ascii def replace_control_chars(input_string: str) -> str: Replaces ASCII control characters in the input string with their mnemonic symbols. Parameters: input_string (str): The string to process. Returns: str: Modified string with control characters replaced by their mnemonics. return \'\'.join(curses.ascii.unctrl(ch) if curses.ascii.isctrl(ch) else ch for ch in input_string)"},{"question":"**Coding Assessment Question:** You have been tasked with writing functions that analyze and report metadata of installed Python packages using the `importlib.metadata` module. The goal is to create a utility that provides comprehensive details on specified packages. # Part 1: Retrieve Package Version Write a function `get_package_version(package_name: str) -> str` that takes a package name as input and returns the version string of that package. # Part 2: Retrieve Package Metadata Write a function `get_package_metadata(package_name: str) -> dict` that takes a package name as input and returns a dictionary containing the metadata of that package. The dictionary should include keys like \'Name\', \'Version\', \'Summary\', and other relevant metadata items. # Part 3: List Entry Points Write a function `list_package_entry_points(package_name: str) -> dict` that takes a package name as input and returns a dictionary where the keys are entry point groups and the values are lists of entry point names. # Part 4: List Distribution Files Write a function `list_package_files(package_name: str) -> list` that takes a package name as input and returns a list of file paths included in the package distribution. # Part 5: Package Requirements Write a function `get_package_requirements(package_name: str) -> list` that takes a package name as input and returns a list of requirements for that package. # Example Usage ```python # Example Usage print(get_package_version(\'wheel\')) # Should print the version of \'wheel\' print(get_package_metadata(\'wheel\')) # Should print metadata dictionary of \'wheel\' print(list_package_entry_points(\'wheel\')) # Should list entry points of \'wheel\' print(list_package_files(\'wheel\')) # Should list files of \'wheel\' print(get_package_requirements(\'wheel\')) # Should list requirements of \'wheel\' ``` # Performance Requirements - Functions should handle packages with large metadata efficiently. - Functions should gracefully handle cases where the specified package is not found or metadata is incomplete. # Constraints - The solutions must exclusively use the `importlib.metadata` module. - Assume the environment has the required permissions to access the necessary package metadata. Test your functions thoroughly to ensure they work for different packages and edge cases.","solution":"from importlib import metadata def get_package_version(package_name: str) -> str: Returns the version of the specified package. try: return metadata.version(package_name) except metadata.PackageNotFoundError: return \\"Package not found\\" def get_package_metadata(package_name: str) -> dict: Returns the metadata of the specified package. try: return dict(metadata.metadata(package_name)) except metadata.PackageNotFoundError: return {\\"error\\": \\"Package not found\\"} def list_package_entry_points(package_name: str) -> dict: Returns a dictionary of entry points for the specified package. try: dist = metadata.distribution(package_name) entry_points = dist.entry_points entry_point_dict = {} for entry_point in entry_points: if entry_point.group not in entry_point_dict: entry_point_dict[entry_point.group] = [] entry_point_dict[entry_point.group].append(entry_point.name) return entry_point_dict except metadata.PackageNotFoundError: return {\\"error\\": \\"Package not found\\"} def list_package_files(package_name: str) -> list: Returns a list of files for the specified package. try: dist = metadata.distribution(package_name) return [str(file) for file in dist.files] if dist.files else [] except metadata.PackageNotFoundError: return [\\"Package not found\\"] def get_package_requirements(package_name: str) -> list: Returns a list of requirements for the specified package. try: dist = metadata.distribution(package_name) return dist.requires or [] except metadata.PackageNotFoundError: return [\\"Package not found\\"]"},{"question":"Objective Create a Python function that reads a given plist file, modifies its contents, and writes the modified data back to another plist file. The function should also have the capability to: 1. Read either XML or binary formatted plist files. 2. Write the modified plist data to both XML and binary formats as specified. Problem Statement Implement a function `modify_plist(input_file: str, output_file: str, format: str, modifications: dict) -> None`: - **Input:** - `input_file` (str): The path to the input plist file. - `output_file` (str): The path where the modified plist file should be saved. - `format` (str): The format for the output file. It should be either `\\"XML\\"` or `\\"BINARY\\"`. - `modifications` (dict): A dictionary containing the modifications to be applied. The function should update, add, or delete items in the plist data according to this dictionary. - **Output:** - The function should not return any value but should write the modified plist data to the specified output file. - **Constraints:** - The `input_file` can be in XML or binary format. - The `format` parameter should only accept `\\"XML\\"` or `\\"BINARY\\"`. Raise a `ValueError` if an unsupported format is provided. - The `modifications` dictionary can contain keys for adding new items, updating existing items, or removing items by providing the value as `None`. Example ```python import plistlib import datetime def modify_plist(input_file, output_file, format, modifications): if format not in [\\"XML\\", \\"BINARY\\"]: raise ValueError(\\"Unsupported format. Use \'XML\' or \'BINARY\'.\\") # Determine the format of the input file with open(input_file, \'rb\') as fp: try: plist_data = plistlib.load(fp, fmt=None) except: raise ValueError(\\"Input file format not recognized.\\") # Apply modifications for key, value in modifications.items(): if value is None: plist_data.pop(key, None) else: plist_data[key] = value # Determine the format for the output file fmt = plistlib.FMT_XML if format == \\"XML\\" else plistlib.FMT_BINARY # Write the modified plist to the output file with open(output_file, \'wb\') as fp: plistlib.dump(plist_data, fp, fmt=fmt) ``` Ensure that your implementation correctly handles the plist file formats and includes proper error handling for unsupported formats and file issues.","solution":"import plistlib def modify_plist(input_file: str, output_file: str, format: str, modifications: dict) -> None: if format not in [\\"XML\\", \\"BINARY\\"]: raise ValueError(\\"Unsupported format. Use \'XML\' or \'BINARY\'.\\") # Determine the format of the input file by attempting to read it with open(input_file, \'rb\') as fp: plist_data = plistlib.load(fp) # Apply modifications to the plist data for key, value in modifications.items(): if value is None: plist_data.pop(key, None) # Remove the key if value is None else: plist_data[key] = value # Determine the output format fmt = plistlib.FMT_XML if format == \\"XML\\" else plistlib.FMT_BINARY # Write the modified plist to the output file with open(output_file, \'wb\') as fp: plistlib.dump(plist_data, fp, fmt=fmt)"},{"question":"You are required to implement a custom class in Python that demonstrates the concepts of shallow and deep copying. The class should have a nested structure to effectively test the understanding of compound objects in terms of copying. Class Requirements 1. **Class Name**: `Node` 2. **Attributes**: - `value` (integer): The value of the node. - `children` (list of `Node` objects): A list containing children nodes. 3. **Methods**: - `__init__(self, value)`: Constructor to initialize the node with a value and an empty list of children. - `add_child(self, child_node)`: Method to add a `Node` object to the children list. - `__copy__(self)`: Method to return a shallow copy of the `Node` object. - `__deepcopy__(self, memo)`: Method to return a deep copy of the `Node` object. Use the `deepcopy` function from the `copy` module, passing along the memo dictionary. Task Implement the `Node` class with the required methods. Then write a function `test_copy_operations()` that performs the following: 1. Create a root node with value `1`. 2. Add two child nodes to the root node with values `2` and `3`. 3. Perform a shallow copy of the root node. 4. Perform a deep copy of the root node. 5. Modify the value of the first child node in the shallow copy to `20`. 6. Modify the value of the second child node in the deep copy to `30`. 7. Print the values of the children of the original root node, the shallow copy, and the deep copy to showcase the differences. Expected Output - The original root node\'s children should show their original values (`2` and `3`). - The shallow copy\'s children should show the new value (`20` and `3`). - The deep copy\'s children should show the values (`2` and `30`). Constraints - You should not use any external libraries other than `copy` for deep copying. - The solution should demonstrate an understanding of object reference behavior in Python. Here\'s a basic structure to get you started: ```python import copy class Node: def __init__(self, value): self.value = value self.children = [] def add_child(self, child_node): self.children.append(child_node) def __copy__(self): new_node = Node(self.value) new_node.children = self.children return new_node def __deepcopy__(self, memo): new_node = Node(self.value) new_node.children = copy.deepcopy(self.children, memo) return new_node def test_copy_operations(): root = Node(1) child1 = Node(2) child2 = Node(3) root.add_child(child1) root.add_child(child2) shallow_copy = copy.copy(root) deep_copy = copy.deepcopy(root) shallow_copy.children[0].value = 20 deep_copy.children[1].value = 30 # Original root node print(\\"Original:\\", [child.value for child in root.children]) # Shallow copy node print(\\"Shallow Copy:\\", [child.value for child in shallow_copy.children]) # Deep copy node print(\\"Deep Copy:\\", [child.value for child in deep_copy.children]) test_copy_operations() ```","solution":"import copy class Node: def __init__(self, value): self.value = value self.children = [] def add_child(self, child_node): self.children.append(child_node) def __copy__(self): new_node = Node(self.value) new_node.children = self.children # Shallow copy return new_node def __deepcopy__(self, memo): new_node = Node(self.value) new_node.children = copy.deepcopy(self.children, memo) # Deep copy return new_node def test_copy_operations(): root = Node(1) child1 = Node(2) child2 = Node(3) root.add_child(child1) root.add_child(child2) shallow_copy = copy.copy(root) deep_copy = copy.deepcopy(root) shallow_copy.children[0].value = 20 deep_copy.children[1].value = 30 # Original root node values original_values = [child.value for child in root.children] # Shallow copy node values shallow_values = [child.value for child in shallow_copy.children] # Deep copy node values deep_values = [child.value for child in deep_copy.children] return original_values, shallow_values, deep_values"},{"question":"# Python Coding Assessment Question Objective Implement a Python function that manipulates bytes objects using various operations such as creation, concatenation, and querying. Problem Statement You are required to write a Python function `manipulate_bytes(operations)` which performs a list of operations on an initial bytes object. The initial bytes object will be created from a string `\\"initial\\"`. Each operation in the list can be one of the following: 1. **From String**: Create a new bytes object from a given string. 2. **Concat**: Concatenate the current bytes object with another bytes object created from a given string. 3. **Query**: Return the current size of the bytes object. 4. **Resize**: Resize the current bytes object to a specific new size. The function should return a list of results corresponding to the `Query` operations found in the input list of operations. Input - `operations`: A list of tuples where each tuple represents an operation. The first element of the tuple is a string representing the operation type (`\\"from_string\\"`, `\\"concat\\"`, `\\"query\\"`, or `\\"resize\\"`). The second element of the tuple depends on the operation: - For `\\"from_string\\"` and `\\"concat\\"`, it is a string. - For `\\"query\\"`, it is `None`. - For `\\"resize\\"`, it is an integer representing the new size. Output - A list of integers representing the results of the `Query` operations, in the order they appear in the input list. Constraints - If an attempt is made to resize the bytes object to a size smaller than its current size, the function should throw an error. - Assume all inputs are valid according to the provided specifications. - The initial bytes object is always created from the string `\\"initial\\"`. Example ```python def manipulate_bytes(operations): # Your implementation here # Example usage and expected results: operations = [ (\\"from_string\\", \\"hello\\"), (\\"concat\\", \\" world\\"), (\\"query\\", None), (\\"resize\\", 6), (\\"query\\", None) ] # Expected output: [11, 6] assert manipulate_bytes(operations) == [11, 6] ``` # Notes - Ensure that your code handles the creation and manipulation of bytes objects correctly. - Pay special attention to error handling for resize operations. - The function should be efficient and handle multiple operations gracefully.","solution":"def manipulate_bytes(operations): Manipulates a bytes object based on given operations. The initial bytes object is created from the string \\"initial\\". result = [] current_bytes = b\\"initial\\" for operation in operations: op_type = operation[0] op_value = operation[1] if op_type == \\"from_string\\": current_bytes = bytes(op_value, \'utf-8\') elif op_type == \\"concat\\": current_bytes += bytes(op_value, \'utf-8\') elif op_type == \\"query\\": result.append(len(current_bytes)) elif op_type == \\"resize\\": new_size = op_value if new_size < len(current_bytes): raise ValueError(\\"Cannot resize to a smaller size.\\") current_bytes = current_bytes.ljust(new_size, b\'x00\') return result"},{"question":"# Question: Custom Data Loading with PyTorch **Objective:** The goal of this task is to assess your understanding of PyTorch\'s data loading utilities by creating a custom data pipeline utilizing both map-style and iterable-style datasets. You will implement classes that adhere to these dataset types and use `DataLoader` to load data with specific configurations. **Description:** You are required to implement two classes: 1. `CustomMapDataset`: A map-style dataset. 2. `CustomIterableDataset`: An iterable-style dataset. Additionally, you need to demonstrate loading data from these datasets using `DataLoader` with custom configurations. Requirements: 1. **CustomMapDataset**: - Inherits from `torch.utils.data.Dataset`. - Implements `__getitem__` and `__len__`. - Initializes with a data source (a list of tuples where each tuple is a data sample). 2. **CustomIterableDataset**: - Inherits from `torch.utils.data.IterableDataset`. - Implements `__iter__`. - Initializes with a data source (an iterable object). 3. **Using DataLoader**: - Create `DataLoader` instances for both `CustomMapDataset` and `CustomIterableDataset`. - Set configurations: - `batch_size = 4` - `shuffle = True` (where applicable) - `num_workers = 2` - Custom collate function to pack data into batches as required. - Enable memory pinning. 4. **Custom Collate Function**: - Implement a custom `collate_fn` that handles packing data and pinning memory. Constraints: - Use the following sample data for both datasets: ```python sample_data = [(i, i**2) for i in range(10)] # A list of 10 tuples. ``` - Ensure that your code is flexible enough to accommodate changes in the data source. Expected Input and Output Formats: - **Input**: - Sample data embedded within the class implementations. - Configuration parameters set directly within the script. - **Output**: - Demonstrate loading and printing batches of data from both datasets using `DataLoader`. ```python import torch from torch.utils.data import Dataset, DataLoader, IterableDataset # Implement the CustomMapDataset class CustomMapDataset(Dataset): def __init__(self, data): self.data = data def __getitem__(self, index): return self.data[index] def __len__(self): return len(self.data) # Implement the CustomIterableDataset class CustomIterableDataset(IterableDataset): def __init__(self, data): self.data = data def __iter__(self): for item in self.data: yield item # Custom collate function def collate_fn(batch): batch = list(zip(*batch)) return torch.tensor(batch[0]), torch.tensor(batch[1]) # Sample data sample_data = [(i, i**2) for i in range(10)] # Create dataset instances map_dataset = CustomMapDataset(sample_data) iter_dataset = CustomIterableDataset(sample_data) # Create DataLoader instances map_loader = DataLoader(map_dataset, batch_size=4, shuffle=True, num_workers=2, collate_fn=collate_fn, pin_memory=True) iter_loader = DataLoader(iter_dataset, batch_size=4, num_workers=2, collate_fn=collate_fn, pin_memory=True) # Print batches from DataLoader print(\\"Map-style Dataset:\\") for batch in map_loader: print(batch) print(\\"nIterable-style Dataset:\\") for batch in iter_loader: print(batch) ``` **Note**: Ensure to follow the performance considerations and constraints as you would in a real-world scenario.","solution":"import torch from torch.utils.data import Dataset, DataLoader, IterableDataset # Implement the CustomMapDataset class CustomMapDataset(Dataset): def __init__(self, data): self.data = data def __getitem__(self, index): return self.data[index] def __len__(self): return len(self.data) # Implement the CustomIterableDataset class CustomIterableDataset(IterableDataset): def __init__(self, data): self.data = data def __iter__(self): for item in self.data: yield item # Custom collate function def collate_fn(batch): batch = list(zip(*batch)) return torch.tensor(batch[0], dtype=torch.float32), torch.tensor(batch[1], dtype=torch.float32) # Sample data sample_data = [(i, i**2) for i in range(10)] # Create dataset instances map_dataset = CustomMapDataset(sample_data) iter_dataset = CustomIterableDataset(sample_data) # Create DataLoader instances map_loader = DataLoader(map_dataset, batch_size=4, shuffle=True, num_workers=2, collate_fn=collate_fn, pin_memory=True) iter_loader = DataLoader(iter_dataset, batch_size=4, num_workers=2, collate_fn=collate_fn, pin_memory=True)"},{"question":"Objective Your task is to implement and manipulate data classes using the `dataclasses` module in Python. The primary objective is to evaluate your understanding of the module\'s various functionalities such as handling default values, post-initialization processing, frozen instances, and inheritance. You should appropriately use the `@dataclass` decorator and other relevant constructs to meet the requirements. Scenario You are required to create a system that manages a university\'s student and course data. The system should store information about students, courses, and enrollments. Requirements 1. **Student Class**: - Create a data class `Student` with the following fields: - `id` (int): Unique identifier for the student. - `name` (string): Name of the student. - `age` (int): Age of the student. - `enrolled_courses` (list of Course, default empty list): List of `Course` objects the student is enrolled in. - Ensure that the `Student` class is immutable (frozen instances). 2. **Course Class**: - Create a data class `Course` with the following fields: - `code` (string): Unique code for the course. - `title` (string): Title of the course. - `credits` (int): Number of academic credits the course offers. 3. **Enrollment Functionality**: - Implement a method `enroll_student` that takes a `Student` object and a `Course` object and enrolls the student in the course. The method should return a new `Student` object with the updated `enrolled_courses` list without modifying the original `Student` object. - The `enrolled_courses` should not contain duplicates. If the student is already enrolled in the course, the method should raise a custom exception `AlreadyEnrolledError`. 4. **Custom Exceptions**: - Create a custom exception class `AlreadyEnrolledError` to handle the scenario when a student attempts to enroll in a course they are already enrolled in. 5. **Additional Functionality**: - Implement a method `__post_init__` in the `Student` class to ensure the `age` is within a valid range (18 to 100). If it is not, raise a `ValueError`. - Use the `field` function from `dataclasses` to handle default values for mutable types (e.g., lists). Constraints: - Do not use external libraries apart from the standard Python library. - Ensure the code is optimized for performance and readability. Example Usage ```python try: course1 = Course(code=\\"CS101\\", title=\\"Intro to Computer Science\\", credits=3) student1 = Student(id=1, name=\\"Alice\\", age=21) updated_student = enroll_student(student1, course1) print(updated_student.enrolled_courses) # Output: [Course(code=\\"CS101\\", title=\\"Intro to Computer Science\\", credits=3)] except AlreadyEnrolledError as e: print(e) except ValueError as ve: print(ve) ``` Expected Output Demonstrate the functionality with sample data. The output should include: - Successfully enrolling a student in a course. - Handling the exception when a student tries to enroll in the same course twice. - Validating the age constraint for the `Student` class. Implement the data classes and functions according to the above specifications. Ensure that your solution is well-tested with appropriate edge cases.","solution":"from dataclasses import dataclass, field from typing import List @dataclass(frozen=True) class Course: code: str title: str credits: int @dataclass(frozen=True) class AlreadyEnrolledError(Exception): course: Course message: str = \\"Student is already enrolled in the course.\\" @dataclass(frozen=True) class Student: id: int name: str age: int enrolled_courses: List[Course] = field(default_factory=list) def __post_init__(self): if not (18 <= self.age <= 100): raise ValueError(\\"Age must be between 18 and 100.\\") def enroll_student(student: Student, course: Course) -> Student: if course in student.enrolled_courses: raise AlreadyEnrolledError(course) new_courses = student.enrolled_courses + [course] return Student( id=student.id, name=student.name, age=student.age, enrolled_courses=new_courses )"},{"question":"# Enumerations and Flags in Python In this task, you will implement an enumeration to manage user roles and permissions within a hypothetical software system. This will test your understanding of the `enum` module and its various features such as creating enums, ensuring unique values, and using flags for bitwise operations. Problem Statement You are required to create an enumeration for user roles and another enumeration for permissions using the `enum` module. You will also need to manage permissions using bitwise operators. 1. **User Roles**: Create an enumeration `UserRole` using `IntEnum` for the following roles: - `ADMIN` with a value of 1 - `USER` with a value of 2 - `GUEST` with a value of 3 2. **Permissions**: Create an enumeration `Permission` using `IntFlag` for user permissions. The permissions should support bitwise operations to combine multiple permissions. Define the following permissions: - `READ` with a value of 1 - `WRITE` with a value of 2 - `EXECUTE` with a value of 4 - `FULL_ACCESS` as a combination of `READ`, `WRITE`, and `EXECUTE` 3. Implement a function `check_permission(role, permission)` that checks whether a given role has a specific permission. For simplicity, assume the following relationships: - `ADMIN` has `FULL_ACCESS` - `USER` has `READ` and `WRITE` permissions - `GUEST` has only `READ` permission Function Signature ```python from enum import IntEnum, IntFlag, auto class UserRole(IntEnum): ADMIN = 1 USER = 2 GUEST = 3 class Permission(IntFlag): READ = 1 WRITE = 2 EXECUTE = 4 FULL_ACCESS = READ | WRITE | EXECUTE def check_permission(role: UserRole, permission: Permission) -> bool: role_permissions = { UserRole.ADMIN: Permission.FULL_ACCESS, UserRole.USER: Permission.READ | Permission.WRITE, UserRole.GUEST: Permission.READ } return permission in role_permissions[role] # Example Usage: # check_permission(UserRole.ADMIN, Permission.WRITE) should return True # check_permission(UserRole.GUEST, Permission.EXECUTE) should return False ``` Input - `role`: An instance of `UserRole`, representing the role of the user. - `permission`: An instance of `Permission`, representing the permission to check. Output - Return `True` if the role has the specified permission, otherwise `False`. Constraints - Use the `enum` module to define your enumerations. - Ensure the use of bitwise operators to handle `Permission` combination.","solution":"from enum import IntEnum, IntFlag class UserRole(IntEnum): ADMIN = 1 USER = 2 GUEST = 3 class Permission(IntFlag): READ = 1 WRITE = 2 EXECUTE = 4 FULL_ACCESS = READ | WRITE | EXECUTE def check_permission(role: UserRole, permission: Permission) -> bool: role_permissions = { UserRole.ADMIN: Permission.FULL_ACCESS, UserRole.USER: Permission.READ | Permission.WRITE, UserRole.GUEST: Permission.READ } return permission & role_permissions[role] == permission"},{"question":"Objective: To assess your understanding of Stochastic Gradient Descent (SGD) for classification tasks using scikit-learn. Task: 1. **Data Loading and Preprocessing**: - Load the Iris dataset from scikit-learn\'s dataset module. - Split the dataset into training and testing sets (70% training, 30% testing). 2. **Model Implementation**: - Implement a classification model using `SGDClassifier` from `sklearn.linear_model`. - Use the following parameters for the model: - `loss=\'log_loss\'` - `penalty=\'l2\'` - `max_iter=1000` - `alpha=0.0001` - `early_stopping=True` - `validation_fraction=0.2` - `n_iter_no_change=10` 3. **Feature Scaling**: - Standardize the features using `StandardScaler` from `sklearn.preprocessing` and apply it to both training and testing sets. 4. **Model Training and Evaluation**: - Fit the model to the training data. - Evaluate the model\'s performance on the test set using accuracy score. - Print the classification report. Constraints: - Ensure the code gracefully handles potential exceptions related to data splitting and model fitting. Input: - None (Use the Iris dataset from scikit-learn). Output: - Print the accuracy score of the model on the test set. - Print the classification report. Implementation Requirements: - The solution must use scikit-learn libraries. - Use pipelines for data preprocessing and model fitting when necessary. Example Output: ``` Accuracy Score: 0.9333 Classification Report: precision recall f1-score support 0 1.00 1.00 1.00 16 1 0.88 1.00 0.93 15 2 1.00 0.83 0.91 14 accuracy 0.93 45 macro avg 0.96 0.94 0.94 45 weighted avg 0.95 0.93 0.93 45 ``` Notes: - Utilize the `train_test_split` function from `sklearn.model_selection`. - Make sure to randomize the dataset splitting with a fixed random state for reproducibility. - Use `classification_report` from `sklearn.metrics` for the detailed evaluation metrics.","solution":"import numpy as np from sklearn.datasets import load_iris from sklearn.linear_model import SGDClassifier from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.metrics import accuracy_score, classification_report from sklearn.pipeline import Pipeline def train_and_evaluate_sgd_classifier(): # Load the Iris dataset iris = load_iris() X, y = iris.data, iris.target # Split the dataset into training (70%) and testing (30%) sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Create an SGDClassifier with the specified parameters sgd_clf = SGDClassifier(loss=\'log_loss\', penalty=\'l2\', max_iter=1000, alpha=0.0001, early_stopping=True, validation_fraction=0.2, n_iter_no_change=10, random_state=42) # Create a pipeline with a StandardScaler and the SGDClassifier pipeline = Pipeline([ (\'scaler\', StandardScaler()), (\'classifier\', sgd_clf) ]) # Train the model pipeline.fit(X_train, y_train) # Evaluate the model y_pred = pipeline.predict(X_test) accuracy = accuracy_score(y_test, y_pred) report = classification_report(y_test, y_pred, target_names=iris.target_names) # Print the results print(f\\"Accuracy Score: {accuracy:.4f}\\") print(\\"Classification Report:\\") print(report) return accuracy, report"},{"question":"Coding Assessment Question # Objective Design and implement a function to visualize and compare the distributions of a continuous variable across different subsets of a dataset. # Problem Statement You are given a dataset containing various attributes of diamond prices. Using this data, you need to create a function that generates a KDE (Kernel Density Estimate) plot to compare the distribution of diamond prices across different cuts and display any notable patterns in the distribution. # Function Signature ```python def plot_diamond_price_distribution(data: pd.DataFrame) -> None: Generates a KDE plot to visualize and compare the distributions of diamond prices across different cuts. Parameters: data (pd.DataFrame): The input dataframe containing at least \'price\' and \'cut\' columns. Returns: None: The function should directly display the plot. pass ``` # Input - `data`: A pandas DataFrame containing at least the following columns: - `price`: Numeric, the price of the diamond. - `cut`: Categorical, the cut quality of the diamond (e.g., \'Fair\', \'Good\', \'Very Good\', \'Premium\', \'Ideal\'). # Output - The function does not return anything. It should directly generate and display the plot using seaborn. # Constraints 1. The function should create a KDE plot using seaborn\'s `sns.kdeplot`. 2. The KDE plot should compare the distributions of the `price` variable across different categories in the `cut` column using different hues. 3. Utilize filled contours to visually distinguish between the different distributions. 4. Ensure the plot is well-labeled (titles, axis labels, etc.) and visually appealing. 5. Handle any potential missing values in the `price` or `cut` columns appropriately. # Example ```python import pandas as pd import seaborn as sns import matplotlib.pyplot as plt # Load the diamonds dataset diamonds = sns.load_dataset(\\"diamonds\\") # Call the function to plot the diamond price distribution plot_diamond_price_distribution(diamonds) ``` # Notes - You may assume that the input DataFrame will always contain the necessary columns (`price`, `cut`). - Use appropriate aesthetic adjustments to make the plot informative and visually appealing. For instance, consider using different colors, setting plot titles, axis labels, etc.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def plot_diamond_price_distribution(data: pd.DataFrame) -> None: Generates a KDE plot to visualize and compare the distributions of diamond prices across different cuts. Parameters: data (pd.DataFrame): The input dataframe containing at least \'price\' and \'cut\' columns. Returns: None: The function should directly display the plot. # Drop rows with missing values in \'price\' or \'cut\' data = data.dropna(subset=[\'price\', \'cut\']) # Set the plot style sns.set(style=\\"whitegrid\\") # Create the KDE plot with different hues for each cut plt.figure(figsize=(10, 6)) sns.kdeplot(data=data, x=\'price\', hue=\'cut\', fill=True, common_norm=False, alpha=0.5, palette=\'muted\') # Set plot title and labels plt.title(\'Distribution of Diamond Prices by Cut\') plt.xlabel(\'Price\') plt.ylabel(\'Density\') # Display the plot plt.show()"},{"question":"# Custom Importer and Loader Implementation In this task, you are required to implement a custom importer and a loader using the `importlib` module. This exercise will test your understanding of Python\'s import system and your ability to extend it for custom importing scenarios. Requirements: 1. **Create a Custom Finder:** - Implement a class `CustomFinder` that inherits from `importlib.abc.MetaPathFinder`. - Define the method `find_spec(fullname, path, target=None)` to find and return a module specification (`ModuleSpec`). 2. **Create a Custom Loader:** - Implement a class `CustomLoader` that inherits from `importlib.abc.Loader`. - Define the method `create_module(spec)` to create a new module. - Define the method `exec_module(module)` to execute the module\'s code. 3. **Register the Finder:** - Add an instance of `CustomFinder` to the `sys.meta_path`. 4. **Testing:** - Write test cases to demonstrate the functionality of your custom importer. You should be able to import a module from a string containing its code. Constraints: - The custom importer should handle only one specific module with a predefined name, e.g., `\\"custom_module\\"`. - The module\'s source code should be a hardcoded string within your `exec_module` implementation for simplicity. Expected Input and Output: - **Input:** None (The module name `custom_module` and source code are predefined). - **Output:** Successful import of the `\\"custom_module\\"` and execution of its code. Example Usage: ```python # custom_finder_loader.py import sys import importlib.util import importlib.abc import types class CustomFinder(importlib.abc.MetaPathFinder): def find_spec(self, fullname, path, target=None): if fullname == \\"custom_module\\": return importlib.util.spec_from_loader(fullname, CustomLoader(), origin=\'custom\') return None class CustomLoader(importlib.abc.Loader): def create_module(self, spec): return None # Use default module creation semantics def exec_module(self, module): code = def hello(): print(\'Hello from the custom module!\') hello() exec(code, module.__dict__) # Register the custom finder sys.meta_path.insert(0, CustomFinder()) # Test import custom_module # This should print \\"Hello from the custom module!\\" ``` Submission: 1. Implement the classes `CustomFinder` and `CustomLoader` in a single Python file named `custom_finder_loader.py`. 2. Register your finder in the code and demonstrate the custom import functionality with a test case.","solution":"import sys import importlib.util import importlib.abc class CustomFinder(importlib.abc.MetaPathFinder): def find_spec(self, fullname, path, target=None): if fullname == \\"custom_module\\": return importlib.util.spec_from_loader(fullname, CustomLoader(), origin=\'custom\') return None class CustomLoader(importlib.abc.Loader): def create_module(self, spec): return None # Use default module creation semantics def exec_module(self, module): code = def hello(): return \'Hello from the custom module!\' def add(a, b): return a + b exec(code, module.__dict__) # Register the custom finder sys.meta_path.insert(0, CustomFinder()) # In the test, custom_module should be imported and its functions used"},{"question":"# Question: Time Representation and Conversion You are required to write a Python function that processes time-related data using the `time` module. Your function should convert a given string representation of a date and time to a different specified format and return the result. Additionally, it should compute the elapsed time between two given time points using a high-resolution performance counter. Function Signature: ```python def process_time_data(time_str: str, from_format: str, to_format: str, start_time: float, end_time: float) -> tuple: pass ``` Parameters: - `time_str` (str): A string representing the time. - `from_format` (str): The format in which `time_str` is given. - `to_format` (str): The desired format for the output time string. - `start_time` (float): A start time value in seconds since the epoch. - `end_time` (float): An end time value in seconds since the epoch. Returns: - A tuple where: - The first element is the converted time string in the desired format. - The second element is the elapsed time between `start_time` and `end_time` measured in seconds using a high-resolution performance counter. Constraints: - You can assume that `time_str`, `start_time`, and `end_time` are always valid. - The conversion formats (`from_format` and `to_format`) follow the directives used by the `time.strftime()` and `time.strptime()` functions. Example: ```python time_str = \\"2023-10-01 15:30:00\\" from_format = \\"%Y-%m-%d %H:%M:%S\\" to_format = \\"%A, %d %B %Y %I:%M:%S %p\\" start_time = time.perf_counter() end_time = time.perf_counter() + 5 # Simulating a 5-second delay result = process_time_data(time_str, from_format, to_format, start_time, end_time) print(result) # Output might be (\'Sunday, 01 October 2023 03:30:00 PM\', 5.0) ``` Note: - Use the `time.strptime()` to parse `time_str` into a `struct_time`. - Use the `time.strftime()` to format the `struct_time` into the desired format. - Calculate the elapsed time using simple subtraction and the `time.perf_counter()` function. Implementation: ```python import time def process_time_data(time_str: str, from_format: str, to_format: str, start_time: float, end_time: float) -> tuple: # Parse the input time string according to the from_format parsed_time = time.strptime(time_str, from_format) # Format the parsed time into the desired format formatted_time = time.strftime(to_format, parsed_time) # Calculate the elapsed time elapsed_time = end_time - start_time return (formatted_time, elapsed_time) ``` Explanation: 1. The function parses the input string `time_str` using `time.strptime()` based on `from_format`. 2. It then formats the parsed time into the desired format using `time.strftime()`. 3. It calculates the elapsed time between `start_time` and `end_time` using simple subtraction. 4. Finally, it returns the formatted time and elapsed time as a tuple.","solution":"import time def process_time_data(time_str: str, from_format: str, to_format: str, start_time: float, end_time: float) -> tuple: Parses and converts a time string from one format to another and calculates elapsed time. Parameters: - time_str (str): The input time string. - from_format (str): The format in which the time_str is provided. - to_format (str): The desired format for the output time string. - start_time (float): The starting time for measuring the elapsed time. - end_time (float): The end time for measuring the elapsed time. Returns: - tuple: (formatted_time_str, elapsed_time) - formatted_time_str (str): The time string formatted in the new format. - elapsed_time (float): The time elapsed between start_time and end_time. # Parse the input time string according to the from_format parsed_time = time.strptime(time_str, from_format) # Format the parsed time into the desired format formatted_time = time.strftime(to_format, parsed_time) # Calculate the elapsed time elapsed_time = end_time - start_time return (formatted_time, elapsed_time)"},{"question":"# Advanced Python Assignment: Working with Fractions **Objective:** The purpose of this assignment is to assess your ability to work with the `Fraction` class in Python, utilizing its various constructors, properties, and methods. **Problem Statement:** You are required to write a function that takes a mixed list of inputs, constructs `Fraction` instances from these inputs, performs specific operations, and returns the results. **Function Signature:** ```python def process_fractions(inputs: list) -> dict: pass ``` **Parameters:** - `inputs` (list): A list of mixed inputs where each input is either: - A pair of integers in a tuple format representing (numerator, denominator) - Another `Fraction` instance - A float - A decimal number in string format - A rational number in string format **Returns:** - `results` (dict): A dictionary with the following keys and expected values: - `\'fractions\'` (list): A list of `Fraction` objects created from the inputs. - `\'summation\'` (Fraction): The summation of all `Fraction` instances. - `\'closest_pi\'` (Fraction): The `Fraction` closest to π (pi), from the list of fractions, using the `limit_denominator` method. **Constraints:** - Ensure all `Fraction` instances are in their lowest terms. - Handle any invalid input gracefully by ignoring it. **Examples:** ```python inputs = [ (1, 2), Fraction(3, 4), 0.3333333, \\"0.25\\", \\"5/6\\", \\"1.414213\\" ] output = process_fractions(inputs) print(output) # Expected Output: # { # \'fractions\': [Fraction(1, 2), Fraction(3, 4), Fraction(3333333, 10000000), Fraction(1, 4), Fraction(5, 6), Fraction(1414213, 1000000)], # \'summation\': Fraction(17984213, 10000000), # \'closest_pi\': Fraction(1414213, 1000000) # } ``` **Notes:** 1. For constructing fractions from float and decimal numbers in string format, ensure you use the exact value for conversion. 2. The `fraction` closest to π (pi) is determined by the smallest denominator using the `limit_denominator` method without altering the `Fraction` instance list. **Hints:** - Use `Fraction` constructor correctly based on the input type. - Utilize list comprehensions and built-in sum function for aggregating fractions. - Handle the sorting and limiting denominator to find the closest fraction to π. **Additional Requirements:** - Include proper docstrings and comments in your code. - Write the function to handle edge cases such as zero denominators or invalid formats gracefully.","solution":"from fractions import Fraction import math def process_fractions(inputs): Processes a mixed list of inputs into Fraction objects and performs certain operations. Parameters: - inputs (list): A list with various types of inputs representing fractions. Returns: - results (dict): A dictionary containing created fractions, their summation, and the fraction closest to π. fractions = [] for item in inputs: try: if isinstance(item, tuple) and len(item) == 2: # Assume tuple is (numerator, denominator) fraction = Fraction(*item) elif isinstance(item, Fraction): fraction = item elif isinstance(item, float): fraction = Fraction.from_float(item) elif isinstance(item, str): fraction = Fraction(item) else: # If input is not recognized, continue to next item continue fractions.append(fraction) except (ValueError, ZeroDivisionError): # Skip invalid inputs continue # Compute the summation of all fraction instances summation = sum(fractions, start=Fraction(0)) # Find the fraction closest to π using limit_denominator pi_approx = Fraction(str(math.pi)) closest_pi = min(fractions, key=lambda x: abs(x - pi_approx)) return { \'fractions\': fractions, \'summation\': summation, \'closest_pi\': closest_pi }"},{"question":"# Python Coding Assessment Question Objective Implement a class and methods to simulate basic behaviors of Python\'s cell objects. This will test your understanding of variable scoping, references, and closures. Task Create a Python class `Cell` which mimics the behavior of Python\'s internal cell objects. Your implementation should include the following methods: - `__init__(self, obj)`: Initializes the cell with an initial value which may be `None`. - `get(self)`: Returns the current value stored in the cell. - `set(self, obj)`: Sets the cell\'s value to `obj`. Additionally, implement the following functions: - `make_closure(func, cell)`: Takes a function `func` and a `Cell` object `cell`, and returns a closure (a function that encloses) that captures the `Cell` object. - `test_closure()`: A test function that demonstrates the interaction of the `Cell` class with closures by printing results showcasing the modification of a captured variable. Input and Output Formats - `Cell.__init__(self, obj)` - **Input**: `obj` can be any Python object or `None`. - **Output**: None - `Cell.get(self)` - **Input**: No input. - **Output**: Returns the object stored in the cell. - `Cell.set(self, obj)` - **Input**: `obj` can be any Python object or `None`. - **Output**: None - `make_closure(func, cell)` - **Input**: A function `func` and a `Cell` object `cell`. - **Output**: A closure capturing the `cell` object. - `test_closure()` - **Input**: No input. - **Output**: Demonstrates the functionality of `Cell` and closures by printing relevant outputs. Constraints - Do not use any built-in cell object functions. Implement all functionalities from scratch. - Ensure that the enclosed variables in closures correctly reference and update the cell object. Example ```python class Cell: def __init__(self, obj): # Initiate your cell with obj pass def get(self): # Return the current value stored in the cell pass def set(self, obj): # Update the value stored in the cell to obj pass def make_closure(func, cell): def closure(): return func(cell) return closure def test_closure(): cell = Cell(10) def incrementor(cell): current_val = cell.get() cell.set(current_val + 1) return cell.get() closure = make_closure(incrementor, cell) print(closure()) # Should print 11 print(closure()) # Should print 12 print(closure()) # Should print 13 # Run test_closure function to see the results. test_closure() ``` Note Ensure your solution covers edge cases such as handling `None` values and type consistency. The output should be correct according to described functionality and constraints.","solution":"class Cell: def __init__(self, obj): Initializes the cell with the given object. self._obj = obj def get(self): Returns the current value stored in the cell. return self._obj def set(self, obj): Sets the cell\'s value to the given object. self._obj = obj def make_closure(func, cell): Takes a function and a Cell object, and returns a closure that captures the Cell object. def closure(): return func(cell) return closure def test_closure(): Test function that demonstrates the interaction of the Cell class with closures. Prints results showcasing the modification of a captured variable. cell = Cell(10) def incrementor(cell): current_val = cell.get() cell.set(current_val + 1) return cell.get() closure = make_closure(incrementor, cell) print(closure()) # Should print 11 print(closure()) # Should print 12 print(closure()) # Should print 13 # Run test_closure function to see the results. test_closure()"},{"question":"# Python Coding Assessment: Timing Code Snippets Objective Demonstrate your understanding of the `timeit` module in Python by creating a script to compare the performance of multiple code snippets. Problem Statement You are given three different implementations of a function that calculates the sum of squares of numbers from 1 to `n`. Your task is to use the `timeit` module to measure and compare the execution times of these implementations. Function Implementations 1. **Using a loop**: ```python def sum_of_squares_loop(n): total = 0 for i in range(1, n+1): total += i * i return total ``` 2. **Using a list comprehension**: ```python def sum_of_squares_list_comp(n): return sum([i * i for i in range(1, n+1)]) ``` 3. **Using a generator expression**: ```python def sum_of_squares_gen_exp(n): return sum(i * i for i in range(1, n+1)) ``` Requirements 1. **Implement the given functions separately.** 2. **Write a script that uses the `timeit` module to measure the execution time of each function for `n = 10000`.** 3. **Compare the execution times and print the results in a clear format.** 4. **Ensure that your script is self-contained and runs without any additional inputs or modifications required.** Input - An integer `n = 10000`. Output - Print the execution times of each function. - Print a comparison, stating which function is the fastest and which is the slowest. Constraints - Use the `timeit` module for timing. - Consider the performance implications of the different implementations. Example Output Your script should produce output similar to: ``` Timing results for n = 10000: - sum_of_squares_loop: X.XXXXXX seconds - sum_of_squares_list_comp: Y.YYYYYY seconds - sum_of_squares_gen_exp: Z.ZZZZZZ seconds Fastest function: sum_of_squares_<implementation> Slowest function: sum_of_squares_<implementation> ``` The exact numbers will depend on your specific machine and runtime environment.","solution":"import timeit def sum_of_squares_loop(n): total = 0 for i in range(1, n+1): total += i * i return total def sum_of_squares_list_comp(n): return sum([i * i for i in range(1, n+1)]) def sum_of_squares_gen_exp(n): return sum(i * i for i in range(1, n+1)) n = 10000 # Timing the functions time_loop = timeit.timeit(lambda: sum_of_squares_loop(n), number=10) time_list_comp = timeit.timeit(lambda: sum_of_squares_list_comp(n), number=10) time_gen_exp = timeit.timeit(lambda: sum_of_squares_gen_exp(n), number=10) # Printing the results print(f\\"Timing results for n = {n}:\\") print(f\\"- sum_of_squares_loop: {time_loop:.6f} seconds\\") print(f\\"- sum_of_squares_list_comp: {time_list_comp:.6f} seconds\\") print(f\\"- sum_of_squares_gen_exp: {time_gen_exp:.6f} seconds\\") # Determining the fastest and slowest implementations times = { \'loop\': time_loop, \'list_comp\': time_list_comp, \'gen_exp\': time_gen_exp } fastest = min(times, key=times.get) slowest = max(times, key=times.get) print(f\\"nFastest function: sum_of_squares_{fastest}\\") print(f\\"Slowest function: sum_of_squares_{slowest}\\")"},{"question":"Python Custom Import Hook Implementation The purpose of this task is to understand and extend Python\'s import system. You are required to implement a custom import hook which will allow Python to import modules from a specified set of directories even if they are not listed in `sys.path`. # Task Implement a custom meta path finder and a loader to facilitate importing from a specified list of directories. # Requirements: 1. Create a class `CustomPathFinder` that acts as a meta path finder. 2. Create a class `CustomLoader` that functions as a module loader. 3. The `CustomPathFinder` should accept a list of directories to search for modules. 4. The import hook should be able to import a module from any of the specified directories. 5. If a module is loaded by `CustomLoader`, write an entry to a log file named `import_log.txt` recording which module was loaded and from which directory. # Implementation Details - **Finder Class**: - Should have an `__init__` method that accepts a list of directories. - Should implement `find_spec()` method to search for modules in these directories. - **Loader Class**: - Should implement `create_module()` and `exec_module()` methods. - `create_module()` can return `None`, letting default module creation take place. - `exec_module()` should execute the module\'s code. - **Logging**: - Log entries should be in the format: `Module <module_name> loaded from <directory>`. # Example: After implementing the custom finder and loader, dynamic imports like the following should work, given that the `example_module.py` exists in one of the specified directories: ```python import sys sys.meta_path.insert(0, CustomPathFinder([\'/path/to/dir1\', \'/path/to/dir2\'])) import example_module example_module.some_function() ``` # Constraints: - You should handle possible exceptions like `ModuleNotFoundError` and other import errors gracefully. - Logs should be accurate and reflect reality. - The provided directories might be nested. # Expected Input/Output Format: Assuming the `example_module.py` defining a function `some_function()` exists in `/path/to/dir2` and is not on the default `sys.path`: **Log Output** (`import_log.txt`): ``` Module example_module loaded from /path/to/dir2 ``` # Solution template: ```python import sys import os import importlib.util from importlib.abc import MetaPathFinder, Loader class CustomLoader(Loader): def create_module(self, spec): # This method can be used to create customized module, returning None uses default. return None def exec_module(self, module): # Write module code into the module\'s namespace with open(module.__spec__.origin, \'r\') as file: code = file.read() exec(code, module.__dict__) # Log the loading information with open(\'import_log.txt\', \'a\') as log_file: log_file.write(f\'Module {module.__name__} loaded from {module.__spec__.origin}n\') class CustomPathFinder(MetaPathFinder): def __init__(self, directories): self.directories = directories def find_spec(self, fullname, path, target=None): for directory in self.directories: module_path = os.path.join(directory, fullname.replace(\'.\',\'/\') + \'.py\') if os.path.isfile(module_path): spec = importlib.util.spec_from_file_location(fullname, module_path, loader=CustomLoader()) return spec return None # Testing Example - Do not include in solution # sys.meta_path.insert(0, CustomPathFinder([\'/path/to/dir1\', \'/path/to/dir2\'])) # import example_module # example_module.some_function() ``` Ensure that the implementation passes the provided constraints and requirements.","solution":"import sys import os import importlib.util from importlib.abc import MetaPathFinder, Loader class CustomLoader(Loader): def create_module(self, spec): return None def exec_module(self, module): with open(module.__spec__.origin, \'r\') as file: code = file.read() exec(code, module.__dict__) with open(\'import_log.txt\', \'a\') as log_file: log_file.write(f\'Module {module.__name__} loaded from {module.__spec__.origin}n\') class CustomPathFinder(MetaPathFinder): def __init__(self, directories): self.directories = directories def find_spec(self, fullname, path, target=None): for directory in self.directories: module_path = os.path.join(directory, fullname.replace(\'.\', \'/\') + \'.py\') if os.path.isfile(module_path): spec = importlib.util.spec_from_file_location(fullname, module_path, loader=CustomLoader()) return spec return None"},{"question":"# PyTorch Storage Manipulation and Validation In this task, you will implement a function in PyTorch to demonstrate understanding of tensor storage manipulation. The function should perform the following operations: 1. **Create a Tensor:** Create a 1-dimensional tensor of size `n` filled with ones (`torch.ones`). 2. **Manipulate Storage:** Clone the tensor\'s storage, modify the cloned storage, and reset the tensor\'s data to this modified storage. 3. **Validation:** Validate the success of these operations by checking if the modifications are correctly reflected in the original tensor. Function Signature ```python def manipulate_and_validate_tensor(n: int) -> bool: ``` Input - `n`: an integer, size of the 1D tensor to be created. Output - Returns `True` if the original tensor correctly reflects the modifications in its storage, otherwise `False`. Constraints - `n` will always be a positive integer. Example ```python result = manipulate_and_validate_tensor(3) print(result) # Expected output: True ``` Detailed Steps 1. **Tensor Creation:** Initialize a tensor `t` with `torch.ones(n)`. 2. **Clone Storage:** Obtain the untyped storage of `t` using `t.untyped_storage()`, then clone this storage into `s1`. 3. **Modify Storage:** Fill the cloned storage `s1` with zeros using `s1.fill_(0)`. 4. **Set Tensor to New Storage:** Update the tensor `t` to use the modified storage `s1` using `t.set_`. 5. **Validation:** Ensure that the elements of `t` are all zeros after the modification. Note: Directly modifying a tensor\'s storage should be done carefully. This question illustrates how tensor storage can be manipulated at a low level for educational purposes.","solution":"import torch def manipulate_and_validate_tensor(n: int) -> bool: # Step 1: Create a tensor filled with ones t = torch.ones(n) # Step 2: Clone the tensor\'s storage s1 = t.untyped_storage().clone() # Step 3: Modify the cloned storage s1.fill_(0) # Step 4: Set the tensor to use the modified storage t.set_(s1) # Step 5: Validate the modification return torch.equal(t, torch.zeros(n))"},{"question":"Context Manager and Data Classes with Inspection In this exercise, you will implement a context manager utilizing `contextlib` and create data classes with `dataclasses`. Additionally, you will use `inspect` to demonstrate introspection. **Task**: 1. Implement a context manager that will time the execution of a block of code. 2. Create a data class to represent a `Book` including attributes for title, author, and publication year. 3. Write a function using the `inspect` module that will introspect and display detailed information about the methods and attributes of the `Book` class. Details: 1. **Context Manager**: - Create a context manager named `TimerContext` that records the start and end time of a block of code. - The context manager should print the total execution time in seconds when the block completes. - Use `time` module for time tracking. 2. **Data Class**: - Use the `dataclasses` module to define a `Book` class with the following attributes: - `title` (str): Title of the book - `author` (str): Author of the book - `publication_year` (int): Publication year of the book - Implement `__str__` method to return a readable string representation of the book details. 3. **Introspection Function**: - Define a function `inspect_book_class` that uses the `inspect` module to get and print the following details of the `Book` class: - Methods available in the class (name and signature) - Attributes (with their data types) - The output should clearly separate methods and attributes information. Input and Output: - **Input**: - There is no direct user input. The functions and classes will be instantiated and tested programatically. - **Output**: - The execution time printed by `TimerContext`. - String representation of `Book` objects via the `__str__` method. - Detailed introspection information from `inspect_book_class`. Constraints: - The data class should use type annotations for attributes. - The context manager should handle exceptions gracefully and still print the elapsed time. Example Usage: ```python # Timing a block of code with TimerContext(): # some operations sum_value = sum(range(100000)) # Creating and printing a Book object book = Book(title=\\"Python 101\\", author=\\"Jane Doe\\", publication_year=2020) print(book) # Inspecting the Book class inspect_book_class(Book) ``` Implement the required classes and function to successfully complete the assessment.","solution":"from contextlib import contextmanager import time from dataclasses import dataclass, fields import inspect @contextmanager def TimerContext(): Context manager to measure execution time of a block of code. start_time = time.time() try: yield finally: end_time = time.time() print(f\\"Execution time: {end_time - start_time:.6f} seconds\\") @dataclass class Book: Dataclass representing a book. title: str author: str publication_year: int def __str__(self): return f\\"\'{self.title}\' by {self.author} (Published in {self.publication_year})\\" def inspect_book_class(class_obj): Introspect and display details of the methods and attributes of a given class. print(f\\"Class: {class_obj.__name__}\\") # Inspect methods print(\\"Methods:\\") for name, method in inspect.getmembers(class_obj, inspect.isfunction): print(f\\" - {name}{inspect.signature(method)}\\") # Inspect attributes print(\\"Attributes:\\") for field in fields(class_obj): print(f\\" - {field.name} ({field.type})\\") # Example usage below # Timing a block of code with TimerContext(): # Some operations sum_value = sum(range(100000)) # Creating and printing a Book object book = Book(title=\\"Python 101\\", author=\\"Jane Doe\\", publication_year=2020) print(book) # Inspecting the Book class inspect_book_class(Book)"},{"question":"# Advanced Python Coding Question: Context Management with `contextlib` Objective: Implement a custom context manager to manage a resource that simulates a database connection. Demonstrate usage of this context manager in both synchronous and asynchronous contexts. Additionally, use `ExitStack` to handle multiple resources, ensuring proper cleanup in case of exceptions. Task Description: 1. **Create a Synchronous Context Manager:** - Implement a synchronous context manager named `DatabaseConnection` that simulates a database connection. - The context manager should: - Print `\\"Connecting to the database...\\"` upon entering. - Print `\\"Disconnecting from the database...\\"` upon exiting. - Simulate an active connection within the context block. 2. **Create an Asynchronous Context Manager:** - Implement an asynchronous context manager named `AsyncDatabaseConnection` that simulates an asynchronous database connection. - The context manager should: - Print `\\"Connecting to the async database...\\"` upon entering. - Print `\\"Disconnecting from the async database...\\"` upon exiting. - Simulate an active connection within the context block. 3. **Use ExitStack for Synchronous Resource Management:** - Create a function `manage_multiple_connections` that uses `ExitStack` to handle multiple `DatabaseConnection` instances. - Ensure that all connections are properly closed even if an exception occurs during the connection setup. 4. **Use AsyncExitStack for Asynchronous Resource Management:** - Create a function `manage_multiple_async_connections` that uses `AsyncExitStack` to handle multiple `AsyncDatabaseConnection` instances. - Ensure that all connections are properly closed even if an exception occurs during the connection setup. Requirements: - **Synchronous Context Manager:** - Define a class `DatabaseConnection` with `__enter__` and `__exit__` methods to implement the context manager. - **Asynchronous Context Manager:** - Define a class `AsyncDatabaseConnection` with `__aenter__` and `__aexit__` methods to implement the context manager. - **ExitStack and AsyncExitStack:** - Use `contextlib.ExitStack` to manage multiple synchronous context managers. - Use `contextlib.AsyncExitStack` to manage multiple asynchronous context managers. Testing: - Write test cases to demonstrate the usage of `DatabaseConnection` and `AsyncDatabaseConnection` within `manage_multiple_connections` and `manage_multiple_async_connections` respectively. - Ensure that the implementation handles exceptions gracefully and performs proper cleanup. # Example Usage: ```python # Example for synchronous context manager def manage_multiple_connections(): with ExitStack() as stack: db1 = stack.enter_context(DatabaseConnection()) db2 = stack.enter_context(DatabaseConnection()) # Do some operations with db1 and db2 # Example for asynchronous context manager async def manage_multiple_async_connections(): async with AsyncExitStack() as stack: db1 = await stack.enter_async_context(AsyncDatabaseConnection()) db2 = await stack.enter_async_context(AsyncDatabaseConnection()) # Do some operations with db1 and db2 # Expected Output: # Connecting to the database... # Connecting to the database... # Disconnecting from the database... # Disconnecting from the database... # Connecting to the async database... # Connecting to the async database... # Disconnecting from the async database... # Disconnecting from the async database... ``` Implement the `DatabaseConnection`, `AsyncDatabaseConnection`, `manage_multiple_connections`, and `manage_multiple_async_connections` functions according to the requirements specified above. Constraints: - Use the concepts and functionalities provided by the `contextlib` module. - Your solution should be efficient, handling resources and exceptions properly.","solution":"from contextlib import ExitStack, AsyncExitStack import asyncio class DatabaseConnection: def __enter__(self): print(\\"Connecting to the database...\\") return self def __exit__(self, exc_type, exc_val, exc_tb): print(\\"Disconnecting from the database...\\") class AsyncDatabaseConnection: async def __aenter__(self): print(\\"Connecting to the async database...\\") return self async def __aexit__(self, exc_type, exc_val, exc_tb): print(\\"Disconnecting from the async database...\\") def manage_multiple_connections(): with ExitStack() as stack: db1 = stack.enter_context(DatabaseConnection()) db2 = stack.enter_context(DatabaseConnection()) # Simulate some operations with db1 and db2 async def manage_multiple_async_connections(): async with AsyncExitStack() as stack: db1 = await stack.enter_async_context(AsyncDatabaseConnection()) db2 = await stack.enter_async_context(AsyncDatabaseConnection()) # Simulate some operations with db1 and db2"},{"question":"Objective Demonstrate your understanding of Seaborn\'s `diverging_palette` function and its application to visualize a data disparity. Task You are provided with a dataset that contains information about exams taken by students. The dataset includes columns for student IDs, exam scores in various subjects, and overall performance ratings. Your task is to visualize the difference between scores in Mathematics and English using a heatmap with a diverging color palette centered at zero. Dataset Assume the dataset is provided in a CSV file named `students_exam_scores.csv` with the following columns: - `student_id` (unique identifier for each student) - `math_score` (score in Mathematics, ranging from 0 to 100) - `english_score` (score in English, ranging from 0 to 100) You need to calculate the difference between `math_score` and `english_score` for each student and visualize these differences using a Seaborn heatmap with a diverging color palette. Requirements 1. Read the dataset from the provided CSV file. 2. Compute the difference between `math_score` and `english_score` and add it as a new column named `score_diff`. 3. Create a pivot table with `student_id` as the index and `score_diff` as the values. 4. Generate a diverging color palette using Seaborn\'s `diverging_palette` function centered at zero. 5. Plot a heatmap of the `score_diff` using the previously generated diverging palette. 6. Customize the heatmap by adding appropriate labels, a title, and a color bar legend. Expected Input - Path to the CSV file: `students_exam_scores.csv` Expected Output - A heatmap plot showing the differences in scores using a diverging color palette centered at zero. Constraints - You should use Seaborn and any necessary supporting libraries (e.g., pandas) to complete this task. - The diverging color palette must be centered at zero. - Ensure that the output is visually informative, with clear labels and a title. Example Input ```csv student_id,math_score,english_score 1,80,70 2,65,85 3,90,90 4,50,40 5,75,80 ... ``` Example Output A heatmap with student IDs on one axis and the `score_diff` values displayed using a diverging color palette centered at zero. The heatmap should clearly show students who perform better in Mathematics or English. Additional Information You can use the following template as a starting point for your implementation: ```python import pandas as pd import seaborn as sns import matplotlib.pyplot as plt # Step 1: Read the dataset df = pd.read_csv(\'students_exam_scores.csv\') # Step 2: Compute the score difference df[\'score_diff\'] = df[\'math_score\'] - df[\'english_score\'] # Step 3: Create a pivot table pivot_table = df.pivot_table(index=\'student_id\', values=\'score_diff\') # Step 4: Generate a diverging color palette palette = sns.diverging_palette(240, 20, center=\'dark\', as_cmap=True) # Step 5: Plot the heatmap sns.heatmap(pivot_table, cmap=palette, center=0, annot=True) # Step 6: Customize the heatmap plt.title(\'Difference between Mathematics and English Scores\') plt.xlabel(\'Student ID\') plt.ylabel(\'Score Difference\') plt.show() ``` Use this template to build your solution, ensuring that you meet all the requirements and constraints specified.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def visualize_score_difference(csv_file_path): # Step 1: Read the dataset df = pd.read_csv(csv_file_path) # Step 2: Compute the score difference df[\'score_diff\'] = df[\'math_score\'] - df[\'english_score\'] # Step 3: Create a pivot table pivot_table = df.pivot_table(index=\'student_id\', values=\'score_diff\') # Step 4: Generate a diverging color palette palette = sns.diverging_palette(240, 20, center=\'dark\', as_cmap=True) # Step 5: Plot the heatmap plt.figure(figsize=(10, 8)) sns.heatmap(pivot_table, cmap=palette, center=0, annot=True, linewidths=0.5) # Step 6: Customize the heatmap plt.title(\'Difference between Mathematics and English Scores\') plt.xlabel(\'Student ID\') plt.ylabel(\'Score Difference\') plt.show()"},{"question":"# **Coding Assessment Question** **Objective** In this assessment, you will showcase your understanding of Python\'s floating point objects by implementing a Python function that utilizes certain aspects of the floating point API described above. You are required to write Python code that interacts with these APIs to perform specific tasks related to floating point manipulation and querying. **Question** Write a Python function `float_info_and_operations` that performs the following tasks: 1. **Check if an Object is a Float:** - Takes an input object `obj` and returns `True` if the object is a Python float or a subtype of float, otherwise returns `False`. 2. **Convert String to Float:** - Takes a string `float_str` and returns a Python float object if the string can be converted to a float. If the conversion fails, return `None`. 3. **Get Float Details:** - Returns a dictionary with the maximum and minimum representable finite float values using the appropriate C API functions. 4. **Convert and Compare Doubles:** - Takes two `float` values and returns a tuple `(double1, double2, isEqual)` where: - `double1` is the C double representation of the first float. - `double2` is the C double representation of the second float. - `isEqual` is `True` if both doubles are equal within Python\'s float precision otherwise `False`. **Function Signature** ```python def float_info_and_operations(obj: any, float_str: str, float1: float, float2: float) -> dict: pass ``` **Input and Output Format** **Input:** - `obj` (any type): The object to be checked whether it\'s a float. - `float_str` (str): A string potentially representing a float. - `float1` (float): First floating-point number. - `float2` (float): Second floating-point number. **Output:** - Returns a dictionary with the following structure: ```python { \\"is_float\\": bool, \\"converted_float\\": float or None, \\"float_details\\": { \\"max\\": float, \\"min\\": float }, \\"double_comparison\\": (float, float, bool) } ``` **Constraints and Limitations:** - You should handle possible exceptions where conversions might fail. - Ensure to utilize the provided C API functions where applicable to fetch details and perform operations. **Performance Requirements:** - The solution should be efficient and handle typical floating point operations within a reasonable execution time. Example usage: ```python result = float_info_and_operations(3.14, \\"123.456\\", 1.23456789, 1.23456789) # Output could look like: # { # \\"is_float\\": True, # \\"converted_float\\": 123.456, # \\"float_details\\": { # \\"max\\": DBL_MAX, # \\"min\\": DBL_MIN # }, # \\"double_comparison\\": (1.23456789, 1.23456789, True) # } ``` In this question, the student will need to work with understanding float object types, converting types, handling exceptions, and querying float properties via relevant API functions.","solution":"import sys def float_info_and_operations(obj, float_str, float1, float2): def is_float(obj): Check if an object is a float. return isinstance(obj, float) def string_to_float(float_str): Convert a string to float, return None if it fails. try: return float(float_str) except ValueError: return None def get_float_details(): Get max and min representable float values. return { \'max\': sys.float_info.max, \'min\': sys.float_info.min } def compare_floats(float1, float2): Return C double representations and compare them. # In Python, float type is a C double under the hood, so no explicit conversion needed is_equal = float1 == float2 return float1, float2, is_equal return { \\"is_float\\": is_float(obj), \\"converted_float\\": string_to_float(float_str), \\"float_details\\": get_float_details(), \\"double_comparison\\": compare_floats(float1, float2) }"},{"question":"**Objective**: Demonstrate your understanding of `sklearn.metrics.pairwise` by implementing both pairwise distance metrics and kernel functions. # Task 1. **Pairwise Distance Calculation**: Implement a function `custom_pairwise_distances(X, Y=None, metric=\'euclidean\')` that computes the pairwise distances between the row vectors of `X` and the row vectors of `Y` (if provided) using the specified metric. If `Y` is not provided, compute the pairwise distances among the row vectors of `X`. - **Input**: - `X`: A numpy array of shape [n_samples_X, n_features]. - `Y` (optional): A numpy array of shape [n_samples_Y, n_features]. - `metric`: A string specifying the distance metric to use. Options include `[\'euclidean\', \'manhattan\', \'cosine\']`. - **Output**: - A numpy array of shape [n_samples_X, n_samples_Y] (or [n_samples_X, n_samples_X] if `Y` is not provided) representing the pairwise distances. - **Example**: ```python X = np.array([[2, 3], [3, 5], [5, 8]]) Y = np.array([[1, 0], [2, 1]]) print(custom_pairwise_distances(X, Y, metric=\'manhattan\')) # Output: array([[ 4., 2.], [ 7., 5.], [12., 10.]]) ``` 2. **Kernel Function Implementation**: Implement a function `custom_pairwise_kernels(X, Y=None, metric=\'linear\', **kwargs)` that computes the pairwise kernels between the row vectors of `X` and the row vectors of `Y` (if provided) using the specified kernel function. If `Y` is not provided, compute the pairwise kernels among the row vectors of `X`. - **Input**: - `X`: A numpy array of shape [n_samples_X, n_features]. - `Y` (optional): A numpy array of shape [n_samples_Y, n_features]. - `metric`: A string specifying the kernel function to use. Options include `[\'linear\', \'polynomial\', \'rbf\', \'cosine\']`. - `kwargs`: Additional parameters required by specific kernel functions such as `gamma` for `rbf` or `degree` and `coef0` for `polynomial`. - **Output**: - A numpy array of shape [n_samples_X, n_samples_Y] (or [n_samples_X, n_samples_X] if `Y` is not provided) representing the pairwise kernels. - **Example**: ```python X = np.array([[2, 3], [3, 5], [5, 8]]) Y = np.array([[1, 0], [2, 1]]) print(custom_pairwise_kernels(X, Y, metric=\'linear\')) # Output: array([[ 2., 7.], [ 3., 11.], [ 5., 18.]]) ``` # Constraints - Do NOT use the built-in `sklearn.metrics.pairwise` functions directly in your implementation. - Ensure compatibility for different metrics and kernel functions specified. - Handle input validation and edge cases appropriately. # Performance Requirements - Your implementation should handle large inputs efficiently, making use of numpy operations where possible. - Ensure your code executes within a reasonable time frame for datasets with up to 10,000 samples and 100 features.","solution":"import numpy as np def custom_pairwise_distances(X, Y=None, metric=\'euclidean\'): Computes the pairwise distances between row vectors of X and Y using the specified metric. if Y is None: Y = X if metric == \'euclidean\': diff = X[:, np.newaxis, :] - Y[np.newaxis, :, :] distances = np.sqrt(np.sum(diff**2, axis=2)) elif metric == \'manhattan\': diff = np.abs(X[:, np.newaxis, :] - Y[np.newaxis, :, :]) distances = np.sum(diff, axis=2) elif metric == \'cosine\': X_norm = X / np.linalg.norm(X, axis=1)[:, np.newaxis] Y_norm = Y / np.linalg.norm(Y, axis=1)[:, np.newaxis] distances = 1 - np.dot(X_norm, Y_norm.T) else: raise ValueError(f\\"Unsupported metric: {metric}\\") return distances def custom_pairwise_kernels(X, Y=None, metric=\'linear\', **kwargs): Computes the pairwise kernels between row vectors of X and Y using the specified kernel function. if Y is None: Y = X if metric == \'linear\': kernels = np.dot(X, Y.T) elif metric == \'polynomial\': degree = kwargs.get(\'degree\', 3) coef0 = kwargs.get(\'coef0\', 1) kernels = (np.dot(X, Y.T) + coef0)**degree elif metric == \'rbf\': gamma = kwargs.get(\'gamma\', 1.0 / X.shape[1]) diff = X[:, np.newaxis, :] - Y[np.newaxis, :, :] dist_sq = np.sum(diff**2, axis=2) kernels = np.exp(-gamma * dist_sq) elif metric == \'cosine\': X_norm = X / np.linalg.norm(X, axis=1)[:, np.newaxis] Y_norm = Y / np.linalg.norm(Y, axis=1)[:, np.newaxis] kernels = np.dot(X_norm, Y_norm.T) else: raise ValueError(f\\"Unsupported metric: {metric}\\") return kernels"},{"question":"XML Document Parsing and Manipulation # Objective The objective of this assignment is to test students\' ability to parse, manipulate, and generate XML documents using Python’s `xml.etree.ElementTree` module. # Problem Statement You are given an XML file representing a catalog of books. Each book has attributes and child elements representing details such as the title, author, price, and genre. Your task is to write Python functions to answer the following specific queries on the XML data. # Requirements You need to implement the following functions: 1. `parse_xml(file_path: str) -> ElementTree`: - **Input**: A file path to the XML file. - **Output**: An `ElementTree` object representing the parsed XML data. 2. `list_books_by_genre(tree: ElementTree, genre: str) -> List[str]`: - **Input**: An `ElementTree` object and a genre string. - **Output**: A list of book titles belonging to the specified genre. 3. `update_book_price(tree: ElementTree, title: str, new_price: float) -> None`: - **Input**: An `ElementTree` object, a book title string, and a new price. - **Output**: None. The function updates the price of the specified book within the XML tree in-place. 4. `add_new_book(tree: ElementTree, book_info: dict) -> None`: - **Input**: An `ElementTree` object and a dictionary containing book information (title, author, price, genre). - **Output**: None. The function adds a new book to the XML tree in-place. 5. `save_xml(tree: ElementTree, file_path: str) -> None`: - **Input**: An `ElementTree` object and a file path to save the updated XML. - **Output**: None. The function saves the modified XML tree back to the specified file location. # Constraints - Assume that the XML file structure is well-formed and consistent. - The XML file will have the following format: ```xml <catalog> <book> <title>Book1</title> <author>Author1</author> <genre>Fiction</genre> <price>10.99</price> </book> <book> <title>Book2</title> <author>Author2</author> <genre>Non-Fiction</genre> <price>15.99</price> </book> <!-- more book entries --> </catalog> ``` # Example Assume `books.xml` contains the catalog data as per the above format. ```python import xml.etree.ElementTree as ET def parse_xml(file_path: str) -> ET.ElementTree: tree = ET.parse(file_path) return tree def list_books_by_genre(tree: ET.ElementTree, genre: str) -> List[str]: root = tree.getroot() titles = [book.find(\'title\').text for book in root.findall(\'book\') if book.find(\'genre\').text == genre] return titles def update_book_price(tree: ET.ElementTree, title: str, new_price: float) -> None: root = tree.getroot() for book in root.findall(\'book\'): if book.find(\'title\').text == title: book.find(\'price\').text = str(new_price) break def add_new_book(tree: ET.ElementTree, book_info: dict) -> None: root = tree.getroot() new_book = ET.Element(\'book\') for key, value in book_info.items(): elem = ET.Element(key) elem.text = str(value) new_book.append(elem) root.append(new_book) def save_xml(tree: ET.ElementTree, file_path: str) -> None: tree.write(file_path) # Usage xml_tree = parse_xml(\'books.xml\') print(list_books_by_genre(xml_tree, \'Fiction\')) update_book_price(xml_tree, \'Book1\', 12.99) new_book_info = { \'title\': \'Book3\', \'author\': \'Author3\', \'genre\': \'Science Fiction\', \'price\': 9.99 } add_new_book(xml_tree, new_book_info) save_xml(xml_tree, \'updated_books.xml\') ``` # Note Your solution should strictly adhere to the outlined function signatures and handle any edge cases effectively.","solution":"import xml.etree.ElementTree as ET from typing import List def parse_xml(file_path: str) -> ET.ElementTree: Parses the XML file and returns the ElementTree object. tree = ET.parse(file_path) return tree def list_books_by_genre(tree: ET.ElementTree, genre: str) -> List[str]: Lists all book titles of a specific genre. root = tree.getroot() titles = [book.find(\'title\').text for book in root.findall(\'book\') if book.find(\'genre\').text == genre] return titles def update_book_price(tree: ET.ElementTree, title: str, new_price: float) -> None: Updates the price of a book identified by its title. root = tree.getroot() for book in root.findall(\'book\'): if book.find(\'title\').text == title: book.find(\'price\').text = str(new_price) break def add_new_book(tree: ET.ElementTree, book_info: dict) -> None: Adds a new book to the XML tree. root = tree.getroot() new_book = ET.Element(\'book\') for key, value in book_info.items(): elem = ET.Element(key) elem.text = str(value) new_book.append(elem) root.append(new_book) def save_xml(tree: ET.ElementTree, file_path: str) -> None: Saves the modified XML tree to a file. tree.write(file_path)"},{"question":"You are tasked with writing a Python function to automate the opening of a list of URLs in a preferred web browser. The function should handle the following requirements: 1. **Input**: - A list of URLs (strings) that need to be opened. - A browser type (string) indicating which preferred browser to use (\\"firefox\\", \\"chrome\\", etc.). - An optional parameter `new` (integer) which defaults to 2. It specifies how URLs are opened: * `0` - the URL is opened in the same browser window if possible. * `1` - a new browser window is opened if possible. * `2` - a new browser tab is opened if possible. 2. **Output**: - There is no direct output, but the URLs should be opened in the preferred browser. 3. **Constraints**: - The function should ensure that if the specified browser type is not available, it defaults to the system’s default web browser. - The function should handle exceptions gracefully if there are errors in opening any URL. 4. **Performance considerations**: - If the list of URLs is long, ensure that the function remains responsive and does not block the execution unnecessarily for too long. Implement the following function in Python: ```python import webbrowser def open_urls(urls, browser_type, new=2): Open a list of URLs in a preferred web browser. Parameters: urls (list): A list of URLs to be opened. browser_type (str): The preferred browser type (e.g. \\"firefox\\", \\"chrome\\"). new (int, optional): The type of opening method. Defaults to 2 (new tab). 0 - the URL is opened in the same browser window if possible. 1 - a new browser window is opened if possible. 2 - a new browser tab is opened if possible. Returns: None pass ``` Make sure to: - Use the `webbrowser` module functions appropriately. - Register the preferred browser if it’s not already registered. - Handle any exceptions that might occur during the URL opening process. **Examples**: ```python urls = [ \\"https://www.python.org\\", \\"https://docs.python.org/3/library/webbrowser.html\\" ] open_urls(urls, \\"firefox\\") open_urls(urls, \\"chrome\\", new=1) ``` In the examples above: - The first call will attempt to open the URLs in \\"firefox\\" in new tabs. - The second call will attempt to open the URLs in \\"chrome\\" in new windows.","solution":"import webbrowser def open_urls(urls, browser_type, new=2): Open a list of URLs in a preferred web browser. Parameters: urls (list): A list of URLs to be opened. browser_type (str): The preferred browser type (e.g. \\"firefox\\", \\"chrome\\"). new (int, optional): The type of opening method. Defaults to 2 (new tab). 0 - the URL is opened in the same browser window if possible. 1 - a new browser window is opened if possible. 2 - a new browser tab is opened if possible. Returns: None try: browser = webbrowser.get(browser_type) except webbrowser.Error: browser = webbrowser for url in urls: try: browser.open(url, new=new) except Exception as e: print(f\\"Failed to open {url}: {e}\\")"},{"question":"**Problem Statement:** You are to implement a Python function named `compress_and_archive` that takes a list of file paths and an output file path. The function should compress each file using the `gzip` compression algorithm, then archive these compressed files into a single tar file at the specified output path. Finally, provide a separate function `decompress_and_extract` that takes an archive path and extracts all the files in it, decompressing them if necessary. **Input:** - `file_paths`: A list of strings, where each string represents a file path to be compressed and added to the archive. - `output_archive_path`: A string representing the path to the output `.tar.gz` file. - `archive_path`: A string representing the path to the input `.tar.gz` file for extraction. **Output:** - The `compress_and_archive` function should create a `.tar.gz` file at `output_archive_path` containing the compressed input files. - The `decompress_and_extract` function should extract all files from the provided `archive_path`, decompress them if they were compressed, and save them to the current working directory. **Constraints:** - You can assume that all the file paths provided in `file_paths` exist and are accessible. - The directory for `output_archive_path` exists and is writable. - The archive file at `archive_path` exists and is readable. **Performance Requirements:** - The solution should be efficient enough to handle a list of up to 1000 file paths. - The handling of file compression and archiving operations should be performed in such a way that minimizes memory usage and maximizes I/O performance. **Function Signature:** ```python def compress_and_archive(file_paths: list, output_archive_path: str) -> None: pass def decompress_and_extract(archive_path: str) -> None: pass ``` **Example:** ```python # Example usage: file_paths = [\'file1.txt\', \'file2.txt\', \'file3.txt\'] output_archive_path = \'compressed_archive.tar.gz\' compress_and_archive(file_paths, output_archive_path) # This should create \'compressed_archive.tar.gz\' with compressed versions of \'file1.txt\', \'file2.txt\', and \'file3.txt\' archive_path = \'compressed_archive.tar.gz\' decompress_and_extract(archive_path) # This should extract the original \'file1.txt\', \'file2.txt\', and \'file3.txt\' from the archive. ``` **Note:** You are required to use the `gzip` and `tarfile` modules for compression and archiving tasks. Handle exceptions appropriately where relevant, providing custom error messages that can help in debugging or tracing any issues encountered during compression, archiving, or extraction processes.","solution":"import gzip import shutil import tarfile import os def compress_and_archive(file_paths, output_archive_path): Compress each file in file_paths using gzip and create a tar file containing them. Parameters: - file_paths (list): List of file paths to be compressed and archived. - output_archive_path (str): The path where the resulting .tar.gz archive will be saved. temp_compressed_file_paths = [] try: for file_path in file_paths: compressed_file_path = file_path + \'.gz\' with open(file_path, \'rb\') as f_in, gzip.open(compressed_file_path, \'wb\') as f_out: shutil.copyfileobj(f_in, f_out) temp_compressed_file_paths.append(compressed_file_path) with tarfile.open(output_archive_path, \'w:gz\') as tar: for compressed_file_path in temp_compressed_file_paths: tar.add(compressed_file_path, arcname=os.path.basename(compressed_file_path)) finally: for temp_file_path in temp_compressed_file_paths: os.remove(temp_file_path) # Clean up temporary compressed files def decompress_and_extract(archive_path): Extract all files from the tar archive and decompress them if they were compressed. Parameters: - archive_path (str): The path to the .tar.gz archive to be extracted. with tarfile.open(archive_path, \'r:gz\') as tar: tar.extractall() for member in tar.getmembers(): if member.name.endswith(\'.gz\'): compressed_file_path = member.name decompressed_file_path = compressed_file_path[:-3] with gzip.open(compressed_file_path, \'rb\') as f_in, open(decompressed_file_path, \'wb\') as f_out: shutil.copyfileobj(f_in, f_out) os.remove(compressed_file_path) # Clean up the compressed file after decompression"},{"question":"You are given a directory containing various sound files of different formats. Your task is to write a Python function to analyze these files and classify them based on their type, sampling rate, number of channels, and other relevant attributes using the `sndhdr` module. Function Signature ```python def classify_sound_files(directory: str) -> dict: pass ``` Input - `directory` (str): The path to the directory containing sound files. Output - A dictionary where the keys are the filenames and the values are dictionaries containing the attributes of the sound files. If `sndhdr` cannot determine the attributes of a file, the value should be `None`. Constraints - Only process files that can be determined by the `sndhdr` module. If a file cannot be determined or if it is not a sound file, it should return `None` for that file. - Assume that the directory contains only files and no subdirectories. - Do not use any additional libraries to handle sound files other than `sndhdr`. Example Suppose the directory contains the following files: ``` /path/to/directory/ file1.wav file2.aiff file3.txt ``` And suppose `sndhdr` provides the following information: - `file1.wav`: filetype=\'wav\', framerate=44100, nchannels=2, nframes=1024, sampwidth=16 - `file2.aiff`: filetype=\'aiff\', framerate=22050, nchannels=1, nframes=512, sampwidth=8 - `file3.txt`: None (not a sound file) The function should return: ```python { \'file1.wav\': { \'filetype\': \'wav\', \'framerate\': 44100, \'nchannels\': 2, \'nframes\': 1024, \'sampwidth\': 16 }, \'file2.aiff\': { \'filetype\': \'aiff\', \'framerate\': 22050, \'nchannels\': 1, \'nframes\': 512, \'sampwidth\': 8 }, \'file3.txt\': None } ``` Notes - Ensure your function handles errors gracefully and logs any issues it encounters during file processing. - You may assume that you have permission to read all files in the directory. Good luck!","solution":"import os import sndhdr def classify_sound_files(directory: str) -> dict: result = {} for filename in os.listdir(directory): filepath = os.path.join(directory, filename) soundfile_info = sndhdr.what(filepath) if soundfile_info: result[filename] = { \'filetype\': soundfile_info.filetype, \'framerate\': soundfile_info.framerate, \'nchannels\': soundfile_info.nchannels, \'nframes\': soundfile_info.nframes, \'sampwidth\': soundfile_info.sampwidth } else: result[filename] = None return result"},{"question":"Objective Design a function that formats and parses monetary values according to different locale settings. The function will accept a list of strings representing monetary values and a target locale. It should return a list of these values converted to the target locale format. Task Implement the function `convert_monetary_values(values: List[str], target_locale: str) -> List[str]`. Requirements: 1. The function accepts a list of monetary values (as strings) and a target locale. 2. The function should: - Set the locale to the target locale for monetary formatting. - Convert each value to the target locale\'s formatting. - Return a list of the converted monetary strings. 3. If the locale change fails (i.e., the locale is not recognized), the function should raise a `locale.Error`. 4. The input list of monetary values should be non-empty and all values should be valid float strings. Input - `values: List[str]`: A list of strings where each string is a monetary value (e.g., `[\\"1234.56\\", \\"7890.12\\"]`). - `target_locale: str`: The target locale for formatting the values (e.g., `\\"en_US.UTF-8\\"` for English (United States), `\\"de_DE.UTF-8\\"` for German (Germany)). Output - Returns a list of strings where each string is a value formatted according to the target locale\'s monetary settings. Constraints - Assume that all values in the `values` list represent valid floats. - You can assume that the `target_locale` is available on the system where this code will run. Example ```python def convert_monetary_values(values: List[str], target_locale: str) -> List[str]: import locale try: locale.setlocale(locale.LC_ALL, target_locale) except locale.Error: raise locale.Error(f\\"Locale {target_locale} is not recognized.\\") formatted_values = [] for value in values: float_value = float(value) formatted_value = locale.currency(float_value) formatted_values.append(formatted_value) return formatted_values # Example usage: values = [\\"1234.56\\", \\"7890.12\\"] target_locale = \\"de_DE.UTF-8\\" print(convert_monetary_values(values, target_locale)) # Output: [\'1.234,56 €\', \'7.890,12 €\'] ``` **Note:** - The provided example assumes that `\\"de_DE.UTF-8\\"` locale is available on the system and that the output will use the German formatting for monetary values.","solution":"def convert_monetary_values(values, target_locale): import locale try: locale.setlocale(locale.LC_ALL, target_locale) except locale.Error as e: raise locale.Error(f\\"Locale {target_locale} is not recognized: {e}\\") formatted_values = [] for value in values: float_value = float(value) formatted_value = locale.currency(float_value, grouping=True) formatted_values.append(formatted_value) return formatted_values"},{"question":"# Advanced Programming Question: Concurrent URL Fetcher with asyncio Using the information provided in the documentation about asyncio, design and implement a Python function called `fetch_all_urls` that concurrently fetches content from multiple URLs. Function Signature ```python import asyncio import aiohttp async def fetch_all_urls(url_list: list, max_concurrent_requests: int) -> dict: Fetches content from multiple URLs concurrently. Parameters: url_list (list): A list of URLs to fetch. max_concurrent_requests (int): The maximum number of concurrent requests. Returns: dict: A dictionary where the keys are the URLs and the values are the content of the URL. ``` Task Description 1. **Fetch URLs concurrently**: Use `asyncio` and `aiohttp` to fetch content from all URLs provided in the `url_list`. 2. **Limit concurrency**: Ensure that no more than `max_concurrent_requests` requests are in progress at any given time. 3. **Handle exceptions**: If a request fails, catch the exception and store the exception message instead of the content. 4. **Output format**: Return a dictionary where each key is a URL, and the corresponding value is either the fetched content or an exception message. Constraints - Use the `aiohttp` library for making HTTP requests. - Handle a large number of URLs efficiently. Example ```python import asyncio url_list = [ \'http://example.com\', \'http://example.org\', \'http://example.net\' ] max_concurrent_requests = 2 # To test the function, you should run it within an asyncio event loop result = asyncio.run(fetch_all_urls(url_list, max_concurrent_requests)) print(result) ``` Notes - Test the implementation with a mix of valid and invalid URLs. - Ensure the function is non-blocking and runs efficiently in terms of concurrency and exception handling.","solution":"import asyncio import aiohttp async def fetch_url(session, url): try: async with session.get(url) as response: response.raise_for_status() return url, await response.text() except Exception as e: return url, str(e) async def fetch_all_urls(url_list: list, max_concurrent_requests: int) -> dict: connector = aiohttp.TCPConnector(limit=max_concurrent_requests) async with aiohttp.ClientSession(connector=connector) as session: tasks = [fetch_url(session, url) for url in url_list] results = await asyncio.gather(*tasks) return dict(results)"},{"question":"Objective: To assess your understanding of Python\'s exception handling mechanisms and your ability to implement custom exceptions, you are required to write a function that processes a list of dictionaries representing user records. Each record involves checking the validity of the fields and raising appropriate exceptions if the records are invalid. Problem Statement: You need to write a function `validate_records(records: list[dict]) -> None` that takes a list of dictionaries where each dictionary represents a user record with the following mandatory fields: - `id`: a positive integer - `name`: a non-empty string - `email`: a string containing \\"@\\" character - `age`: an integer between 18 and 99 The function should validate each record and raise custom exceptions for any violations, with specific error messages indicating the issues. Create custom exception classes for different validation errors: - `InvalidIDError` for invalid `id`. - `InvalidNameError` for invalid `name`. - `InvalidEmailError` for invalid `email`. - `InvalidAgeError` for invalid `age`. The function must: 1. Iterate through each record in the list. 2. Validate each field in the record. 3. Raise appropriate custom exceptions when an invalid field is encountered. Input: - `records`: A list of dictionaries, each containing user details. Output: - The function does not return anything. Instead, it should raise exceptions for invalid records. Specifications: 1. If a record contains an invalid ID, raise `InvalidIDError` with the message `\\"Invalid ID: {id_value}\\"`. 2. If a record contains an invalid name, raise `InvalidNameError` with the message `\\"Invalid Name: {name_value}\\"`. 3. If a record contains an invalid email, raise `InvalidEmailError` with the message `\\"Invalid Email: {email_value}\\"`. 4. If a record contains an invalid age, raise `InvalidAgeError` with the message `\\"Invalid Age: {age_value}\\"`. Examples: ```python # Define the custom exceptions class InvalidIDError(Exception): pass class InvalidNameError(Exception): pass class InvalidEmailError(Exception): pass class InvalidAgeError(Exception): pass def validate_records(records): for record in records: if not (isinstance(record.get(\\"id\\"), int) and record[\\"id\\"] > 0): raise InvalidIDError(f\\"Invalid ID: {record.get(\'id\')}\\") if not (isinstance(record.get(\\"name\\"), str) and record[\\"name\\"]): raise InvalidNameError(f\\"Invalid Name: {record.get(\'name\')}\\") if not (isinstance(record.get(\\"email\\"), str) and \\"@\\" in record[\\"email\\"]): raise InvalidEmailError(f\\"Invalid Email: {record.get(\'email\')}\\") if not (isinstance(record.get(\\"age\\"), int) and 18 <= record[\\"age\\"] <= 99): raise InvalidAgeError(f\\"Invalid Age: {record.get(\'age\')}\\") ``` Test the function with the following records: ```python try: records = [ {\\"id\\": 1, \\"name\\": \\"Alice\\", \\"email\\": \\"alice@example.com\\", \\"age\\": 28}, {\\"id\\": 2, \\"name\\": \\"\\", \\"email\\": \\"bob@example.com\\", \\"age\\": 34}, {\\"id\\": -1, \\"name\\": \\"Charlie\\", \\"email\\": \\"charlieexample.com\\", \\"age\\": 45}, {\\"id\\": 3, \\"name\\": \\"Eve\\", \\"email\\": \\"eve@example.com\\", \\"age\\": 120} ] validate_records(records) except Exception as e: print(e) ``` The output for the records validation should raise exceptions and print appropriate error messages indicating the invalid records.","solution":"# Define the custom exceptions class InvalidIDError(Exception): pass class InvalidNameError(Exception): pass class InvalidEmailError(Exception): pass class InvalidAgeError(Exception): pass # Function to validate records def validate_records(records): for record in records: if not (isinstance(record.get(\\"id\\"), int) and record[\\"id\\"] > 0): raise InvalidIDError(f\\"Invalid ID: {record.get(\'id\')}\\") if not (isinstance(record.get(\\"name\\"), str) and record[\\"name\\"]): raise InvalidNameError(f\\"Invalid Name: {record.get(\'name\')}\\") if not (isinstance(record.get(\\"email\\"), str) and \\"@\\" in record[\\"email\\"]): raise InvalidEmailError(f\\"Invalid Email: {record.get(\'email\')}\\") if not (isinstance(record.get(\\"age\\"), int) and 18 <= record[\\"age\\"] <= 99): raise InvalidAgeError(f\\"Invalid Age: {record.get(\'age\')}\\")"},{"question":"# Question: Clustering High-Dimensional Data with scikit-learn Objective: You are tasked with clustering high-dimensional data using scikit-learn, demonstrating your knowledge of clustering algorithms and their application. Scenario: Given a dataset with high-dimensional features, your task is to implement a function that applies two different clustering algorithms from scikit-learn and compares their results. Function Signature: ```python def cluster_high_dimensional_data(data: np.ndarray) -> dict: Clusters high-dimensional data using the KMeans and DBSCAN algorithms. Parameters: data (np.ndarray): A 2D numpy array of shape (n_samples, n_features) containing the high-dimensional data. Returns: dict: A dictionary containing the cluster labels for each algorithm. The keys should be \'KMeans\' and \'DBSCAN\', and the values should be the corresponding cluster labels. pass ``` Input: - `data`: A 2D numpy array of shape (n_samples, n_features) where `n_samples` is the number of samples and `n_features` is the number of features. Output: - A dictionary with the keys \'KMeans\' and \'DBSCAN\' and the corresponding cluster labels as values. The cluster labels should be numpy arrays of length `n_samples`. Constraints: - You should use `KMeans` and `DBSCAN` from scikit-learn. - Use the default parameters for both algorithms initially. - Ensure your function handles edge cases, such as data with fewer samples than features. Example: ```python import numpy as np # Example high-dimensional dataset data = np.random.rand(100, 50) # Clustering the data result = cluster_high_dimensional_data(data) print(result[\'KMeans\']) # Expected output: array of cluster labels from KMeans print(result[\'DBSCAN\']) # Expected output: array of cluster labels from DBSCAN ``` Additional Notes: - You may need to preprocess the data if necessary (e.g., scaling). - After obtaining the cluster labels, you can further compare the clustering results using basic metrics (optional for this task). The goal of this problem is to ensure you can apply clustering algorithms to high-dimensional data using scikit-learn and understand the differences between the algorithms.","solution":"import numpy as np from sklearn.cluster import KMeans, DBSCAN from sklearn.preprocessing import StandardScaler def cluster_high_dimensional_data(data: np.ndarray) -> dict: Clusters high-dimensional data using the KMeans and DBSCAN algorithms. Parameters: data (np.ndarray): A 2D numpy array of shape (n_samples, n_features) containing the high-dimensional data. Returns: dict: A dictionary containing the cluster labels for each algorithm. The keys should be \'KMeans\' and \'DBSCAN\', and the values should be the corresponding cluster labels. # Scale data to have zero mean and unit variance scaler = StandardScaler() scaled_data = scaler.fit_transform(data) # Apply KMeans clustering kmeans = KMeans(n_clusters=3, random_state=42) kmeans_labels = kmeans.fit_predict(scaled_data) # Apply DBSCAN clustering dbscan = DBSCAN() dbscan_labels = dbscan.fit_predict(scaled_data) return { \'KMeans\': kmeans_labels, \'DBSCAN\': dbscan_labels }"},{"question":"# Task You are given a DataFrame that simulates various data types, including integer, float, datetime, and category. The task is to write a function to provide detailed memory usage information about the DataFrame, including a deeper introspection to account for object memory usage and the usage report in human-readable units. # Function Signature ```python def detailed_df_memory_usage(df: pd.DataFrame) -> pd.DataFrame: This function accepts a DataFrame and returns a DataFrame with detailed memory usage information. Parameters: df (pd.DataFrame): Input DataFrame to analyze. Returns: pd.DataFrame: A DataFrame with columns [\'Column\', \'Memory Usage (Bytes)\', \'Memory Usage (Human-Readable)\']. ``` # Input - `df`: A pandas DataFrame with several columns of various data types. Example: ```python import pandas as pd import numpy as np dtypes = [\\"int64\\", \\"float64\\", \\"datetime64[ns]\\", \\"timedelta64[ns]\\", \\"complex128\\", \\"object\\", \\"bool\\"] n = 5000 data = {t: np.random.randint(100, size=n).astype(t) for t in dtypes} df = pd.DataFrame(data) df[\\"categorical\\"] = df[\\"object\\"].astype(\\"category\\") ``` # Output - A pandas DataFrame with three columns: - `Column`: Name of the DataFrame column. - `Memory Usage (Bytes)`: Memory usage of the column in bytes. - `Memory Usage (Human-Readable)`: Memory usage of the column in human-readable units (e.g., KB, MB). Example: ```python result = detailed_df_memory_usage(df) print(result) ``` Expected output: ``` Column Memory Usage (Bytes) Memory Usage (Human-Readable) 0 int64 40000 39.1 KB 1 float64 40000 39.1 KB 2 datetime64[ns] 40000 39.1 KB 3 timed£nulla64[ns] 40000 39.1 KB 4 complex128 80000 78.1 KB 5 object 140000 136.7 KB 6 bool 5000 4.9 KB 7 categorical 11850 11.6 KB ``` # Constraints - Ensure that the function works efficiently for large DataFrames. - Use pandas methods `info()` and `memory_usage()` ensuring both shallow and deep memory usage are discussed in the result. # Notes - Utilize `memory_usage=deep` to report the deeper memory usage. - Address the Byte vs Human-Readable units based on the DataFrame\'s `info()` output format description. - This task requires implementing proper pandas manipulation to fulfill the requirements.","solution":"import pandas as pd def detailed_df_memory_usage(df: pd.DataFrame) -> pd.DataFrame: This function accepts a DataFrame and returns a DataFrame with detailed memory usage information. Parameters: df (pd.DataFrame): Input DataFrame to analyze. Returns: pd.DataFrame: A DataFrame with columns [\'Column\', \'Memory Usage (Bytes)\', \'Memory Usage (Human-Readable)\']. def human_readable_size(size): # Converts bytes to human-readable units for unit in [\'B\', \'KB\', \'MB\', \'GB\', \'TB\']: if size < 1024.0: return f\\"{size:.1f} {unit}\\" size /= 1024.0 memory_usage_bytes = df.memory_usage(deep=True) memory_usage = [] for col in df.columns: col_memory_bytes = memory_usage_bytes[col] memory_usage.append({ \'Column\': col, \'Memory Usage (Bytes)\': col_memory_bytes, \'Memory Usage (Human-Readable)\': human_readable_size(col_memory_bytes) }) memory_usage_df = pd.DataFrame(memory_usage) return memory_usage_df"},{"question":"Objective Design a PyTorch function that demonstrates an understanding of device management, stream control, random number generation, and memory management on XPU devices. Problem Statement Implement a PyTorch function `xpu_matrix_operations` which performs matrix operations on XPU devices using custom streams. Your implementation should handle the following: 1. **Device Management**: - Use `torch.xpu.device` to set the device context. - Ensure the code is compatible with systems that have multiple XPU devices. 2. **Random Number Generator**: - Seed the random number generator for reproducibility using `torch.xpu.manual_seed`. 3. **Streams and Events**: - Create at least two different streams using `torch.xpu.Stream`. - Perform matrix multiplication on the default stream. - Perform matrix addition on a custom stream. - Synchronize the streams properly. 4. **Memory Management**: - Clear the cache using `torch.xpu.empty_cache`. - Monitor and print memory usage statistics before and after the operations using `torch.xpu.memory_allocated` and `torch.xpu.max_memory_allocated`. Function Signature ```python def xpu_matrix_operations(): Perform matrix operations on XPU devices using custom streams and manage memory effectively. Returns: A tuple of resulting matrices after multiplication and addition, and a dictionary of memory statistics. ``` Constraints - Assume matrices are of size 1000x1000. - You may use any random values for matrix elements. - Ensure reproducibility using a fixed seed. Input - No direct input, the function will internally define matrices and perform operations on them. Output - The function should return a tuple with: - The resulting matrix after multiplication (torch.Tensor). - The resulting matrix after addition (torch.Tensor). - A dictionary containing memory statistics including: - `memory_allocated_before` - `max_memory_allocated_before` - `memory_allocated_after` - `max_memory_allocated_after` Example ```python result_mul, result_add, mem_stats = xpu_matrix_operations() # Example output: # result_mul -> tensor([[...]], device=\'xpu\') # result_add -> tensor([[...]], device=\'xpu\') # mem_stats -> { # \\"memory_allocated_before\\": 10, # \\"max_memory_allocated_before\\": 5000000, # \\"memory_allocated_after\\": 30, # \\"max_memory_allocated_after\\": 10000000 # } ``` The function needs to perform with efficient resource management and ensure all operations are completed correctly, leveraging multiple streams to demonstrate advanced PyTorch capabilities on XPU devices.","solution":"import torch def xpu_matrix_operations(): Perform matrix operations on XPU devices using custom streams and manage memory effectively. Returns: A tuple of resulting matrices after multiplication and addition, and a dictionary of memory statistics. # Check if XPU is available if not torch.xpu.is_available(): raise RuntimeError(\\"XPU is not available\\") # Select the device device = torch.xpu.device(\\"xpu\\") # Seed the random number generator for reproducibility torch.manual_seed(42) # Create matrices size = (1000, 1000) a = torch.randn(size, device=device) b = torch.randn(size, device=device) # Memory statistics before operations mem_stats_before = { \\"memory_allocated_before\\": torch.xpu.memory_allocated(), \\"max_memory_allocated_before\\": torch.xpu.max_memory_allocated(), } # Create streams stream1 = torch.xpu.Stream() stream2 = torch.xpu.Stream() # Ensure default stream is synchronized first torch.xpu.synchronize() # Perform matrix multiplication on the default stream result_mul = torch.mm(a, b) # Perform matrix addition on a custom stream with torch.xpu.stream(stream1): result_add = a + b # Synchronize streams stream1.synchronize() # Clear the cache torch.xpu.empty_cache() # Memory statistics after operations mem_stats_after = { \\"memory_allocated_after\\": torch.xpu.memory_allocated(), \\"max_memory_allocated_after\\": torch.xpu.max_memory_allocated(), } mem_stats = {**mem_stats_before, **mem_stats_after} return result_mul, result_add, mem_stats"},{"question":"<|Analysis Begin|> The provided documentation describes the `errno` module in Python, which deals with standard error codes generated by system calls and library functions. The `errno` module provides symbolic error names and corresponding numeric values. It also provides a dictionary `errno.errorcode` for mapping numeric error values to their names. This module is useful for interpreting system-level and OS errors in a more readable format and is commonly used with exceptions generated by OS-level operations. Important points to consider for designing the question: - Understanding how to use `errno` and its attributes such as `errno.errorcode`. - Handling exceptions in Python using the `errno` mappings provided. - Translating error codes to error messages using system functions like `os.strerror()`. Based on the documentation, we aim to craft a question that requires students to demonstrate their understanding of: 1. Raising and catching exceptions in Python. 2. Using the `errno` module to interpret error codes. 3. Translating numeric error codes to human-readable messages. <|Analysis End|> <|Question Begin|> # Advanced Coding Assessment: Error Handling with `errno` Problem Statement You are developing a robust file handling system in Python that needs to gracefully manage and report errors related to file operations. To achieve this, you must utilize the `errno` module to interpret and handle various OS-level errors. Your task is to write a function `file_operation_handler` that attempts to perform file operations and returns human-readable error messages in case of failures. Function Signature ```python def file_operation_handler(operation: str, *args) -> str: pass ``` Parameters - `operation` (str): The file operation to be performed. It can be one of the following strings: `\\"read_file\\"`, `\\"write_file\\"`, `\\"delete_file\\"`. - `*args`: Additional arguments required for the specified operation: - For `\\"read_file\\"`, you need to provide the file path as the argument. - For `\\"write_file\\"`, you need to provide the file path and the data to write as the arguments. - For `\\"delete_file\\"`, you need to provide the file path as the argument. Return - str: A message indicating the result of the operation. If the operation fails, return an error message specifying the type of error. Details - If `\\"read_file\\"` is specified, attempt to read the contents of the file at `args[0]`. Return the contents if successful. - If `\\"write_file\\"` is specified, write `args[1]` to the file at `args[0]`. Return \\"Write successful\\" if successful. - If `\\"delete_file\\"` is specified, delete the file at `args[0]`. Return \\"Delete successful\\" if successful. - If the operation fails due to an OS-level error, catch the exception and return an appropriate message using the `errno` module and `os.strerror()`. For example, if a `FileNotFoundError` occurs during a read operation, the message should be \\"File not found: [error message]\\". Constraints - Ensure that the function handles common file-related errors and maps them correctly using the `errno` module. - Your implementation should be efficient and not involve unnecessary file I/O operations. Example Usage ```python print(file_operation_handler(\\"read_file\\", \\"/path/to/nonexistent/file\\")) # Output: \\"File not found: No such file or directory\\" print(file_operation_handler(\\"write_file\\", \\"/path/to/file\\", \\"Hello, World!\\")) # Output: \\"Write successful\\" print(file_operation_handler(\\"delete_file\\", \\"/path/to/file\\")) # Output: \\"Delete successful\\" ``` Utilize the `errno` module to handle errors and `os.strerror()` to provide human-readable error messages. Below is a partial implementation to get you started: ```python import errno import os def file_operation_handler(operation: str, *args) -> str: try: if operation == \\"read_file\\": with open(args[0], \'r\') as f: return f.read() elif operation == \\"write_file\\": with open(args[0], \'w\') as f: f.write(args[1]) return \\"Write successful\\" elif operation == \\"delete_file\\": os.remove(args[0]) return \\"Delete successful\\" except OSError as e: err_name = errno.errorcode[e.errno] return f\\"{err_name}: {os.strerror(e.errno)}\\" ``` Test your function with various scenarios to ensure proper error handling and reporting.","solution":"import errno import os def file_operation_handler(operation: str, *args) -> str: try: if operation == \\"read_file\\": with open(args[0], \'r\') as f: return f.read() elif operation == \\"write_file\\": with open(args[0], \'w\') as f: f.write(args[1]) return \\"Write successful\\" elif operation == \\"delete_file\\": os.remove(args[0]) return \\"Delete successful\\" else: return \\"Invalid operation\\" except OSError as e: err_name = errno.errorcode.get(e.errno, \\"UNKNOWN\\") return f\\"{err_name}: {os.strerror(e.errno)}\\""},{"question":"You are given a dataset consisting of medical diagnostic data. Using this dataset, you need to determine the optimal value of the regularization hyperparameter (C) for an SVM classifier and analyze how the model\'s performance changes as the training sample size increases. You will use validation curves to study the impact of varying the `C` value, and learning curves to observe the model\'s performance with growing training data. Task: 1. Load the dataset from the provided CSV file. Assume the file is named `medical_data.csv`. 2. Split the dataset into features (`X`) and labels (`y`). 3. Use a support vector machine (SVM) with a linear kernel. 4. Plot a validation curve to study the effect of the regularization parameter `C` on model performance. 5. Plot a learning curve to observe how the model improves with increasing training data. Requirements: 1. Implement a function `plot_validation_curve` that: - Inputs: `X`, `y`, `param_range`. - Outputs: Plots the validation curve for parameter `C`. 2. Implement a function `plot_learning_curve` that: - Inputs: `X`, `y`, `train_sizes`. - Outputs: Plots the learning curve for the SVM classifier. 3. Use cross-validation with 5 folds (`cv=5`) for both validation and learning curves. 4. Visualize the training and validation scores. Expected Code Implementation: ```python import pandas as pd import numpy as np from sklearn.svm import SVC from sklearn.model_selection import validation_curve, learning_curve from sklearn.utils import shuffle from matplotlib import pyplot as plt def load_data(file_path): data = pd.read_csv(file_path) X = data.drop(\'label\', axis=1).values y = data[\'label\'].values return X, y def plot_validation_curve(X, y, param_range): train_scores, valid_scores = validation_curve( SVC(kernel=\'linear\'), X, y, param_name=\'C\', param_range=param_range, cv=5 ) train_mean = np.mean(train_scores, axis=1) train_std = np.std(train_scores, axis=1) valid_mean = np.mean(valid_scores, axis=1) valid_std = np.std(valid_scores, axis=1) plt.figure() plt.semilogx(param_range, train_mean, label=\\"Training score\\", color=\\"r\\") plt.fill_between(param_range, train_mean - train_std, train_mean + train_std, alpha=0.2, color=\\"r\\") plt.semilogx(param_range, valid_mean, label=\\"Cross-validation score\\", color=\\"g\\") plt.fill_between(param_range, valid_mean - valid_std, valid_mean + valid_std, alpha=0.2, color=\\"g\\") plt.xlabel(\\"Parameter C\\") plt.ylabel(\\"Score\\") plt.legend(loc=\\"best\\") plt.title(\\"Validation Curve with SVM\\") plt.show() def plot_learning_curve(X, y, train_sizes): train_sizes, train_scores, valid_scores = learning_curve( SVC(kernel=\'linear\'), X, y, train_sizes=train_sizes, cv=5 ) train_mean = np.mean(train_scores, axis=1) train_std = np.std(train_scores, axis=1) valid_mean = np.mean(valid_scores, axis=1) valid_std = np.std(valid_scores, axis=1) plt.figure() plt.plot(train_sizes, train_mean, label=\\"Training score\\", color=\\"r\\") plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.2, color=\\"r\\") plt.plot(train_sizes, valid_mean, label=\\"Cross-validation score\\", color=\\"g\\") plt.fill_between(train_sizes, valid_mean - valid_std, valid_mean + valid_std, alpha=0.2, color=\\"g\\") plt.xlabel(\\"Training Sizes\\") plt.ylabel(\\"Score\\") plt.legend(loc=\\"best\\") plt.title(\\"Learning Curve with SVM\\") plt.show() # Example usage: # X, y = load_data(\'medical_data.csv\') # param_range = np.logspace(-7, 3, 10) # train_sizes = np.linspace(0.1, 1.0, 10) # plot_validation_curve(X, y, param_range) # plot_learning_curve(X, y, train_sizes) ``` Constraints: - The dataset file `medical_data.csv` should have a column named \'label\' which contains the target variable. - Ensure that the solutions are efficient and the plots are clear and informative. Assessment Criteria: - Correctness of the implementation. - Proper use of scikit-learn functions (`validation_curve`, `learning_curve`). - Quality and readability of the plots.","solution":"import pandas as pd import numpy as np from sklearn.svm import SVC from sklearn.model_selection import validation_curve, learning_curve from matplotlib import pyplot as plt def load_data(file_path): data = pd.read_csv(file_path) X = data.drop(\'label\', axis=1).values y = data[\'label\'].values return X, y def plot_validation_curve(X, y, param_range): train_scores, valid_scores = validation_curve( SVC(kernel=\'linear\'), X, y, param_name=\'C\', param_range=param_range, cv=5 ) train_mean = np.mean(train_scores, axis=1) train_std = np.std(train_scores, axis=1) valid_mean = np.mean(valid_scores, axis=1) valid_std = np.std(valid_scores, axis=1) plt.figure() plt.semilogx(param_range, train_mean, label=\\"Training score\\", color=\\"r\\") plt.fill_between(param_range, train_mean - train_std, train_mean + train_std, alpha=0.2, color=\\"r\\") plt.semilogx(param_range, valid_mean, label=\\"Cross-validation score\\", color=\\"g\\") plt.fill_between(param_range, valid_mean - valid_std, valid_mean + valid_std, alpha=0.2, color=\\"g\\") plt.xlabel(\\"Parameter C\\") plt.ylabel(\\"Score\\") plt.legend(loc=\\"best\\") plt.title(\\"Validation Curve with SVM\\") plt.show() def plot_learning_curve(X, y, train_sizes): train_sizes, train_scores, valid_scores = learning_curve( SVC(kernel=\'linear\'), X, y, train_sizes=train_sizes, cv=5 ) train_mean = np.mean(train_scores, axis=1) train_std = np.std(train_scores, axis=1) valid_mean = np.mean(valid_scores, axis=1) valid_std = np.std(valid_scores, axis=1) plt.figure() plt.plot(train_sizes, train_mean, label=\\"Training score\\", color=\\"r\\") plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.2, color=\\"r\\") plt.plot(train_sizes, valid_mean, label=\\"Cross-validation score\\", color=\\"g\\") plt.fill_between(train_sizes, valid_mean - valid_std, valid_mean + valid_std, alpha=0.2, color=\\"g\\") plt.xlabel(\\"Training Sizes\\") plt.ylabel(\\"Score\\") plt.legend(loc=\\"best\\") plt.title(\\"Learning Curve with SVM\\") plt.show() # Example usage: # X, y = load_data(\'medical_data.csv\') # param_range = np.logspace(-3, 3, 10) # train_sizes = np.linspace(0.1, 1.0, 10) # plot_validation_curve(X, y, param_range) # plot_learning_curve(X, y, train_sizes)"},{"question":"Objective Implement a function that efficiently processes a list of event records to organize them by their timestamp while maintaining the order of their insertion when timestamps are the same. Instructions You are provided with a list of event records. Each record is a tuple containing an event ID (string) and a timestamp (string in \\"YYYY-MM-DD HH:MM:SS\\" format). Your task is to write a function `organize_events` that takes this list as input and returns an OrderedDict where the keys are the timestamps and the values are lists of event IDs that occurred at those timestamps. Requirements - Use the `OrderedDict` class from the `collections` module to maintain the order of insertion. - Ensure that events with the same timestamp appear in the order they were provided in the input list. - The function should handle at least up to 10,000 event records efficiently. Function Signature ```python from collections import OrderedDict from typing import List, Tuple, OrderedDict def organize_events(events: List[Tuple[str, str]]) -> OrderedDict[str, List[str]]: pass ``` Input - `events`: A list of tuples, where each tuple contains: - `event_id`: A string representing the event ID. - `timestamp`: A string in “YYYY-MM-DD HH:MM:SS” format. Output - An `OrderedDict` where each key is a timestamp (string), and each value is a list of event IDs (strings) that occurred at that timestamp. Constraints - The input list `events` will have between 1 and 10,000 records. - Each timestamp will be a well-formed string in the format “YYYY-MM-DD HH:MM:SS”. Example ```python input_events = [ (\\"event1\\", \\"2023-10-01 14:22:01\\"), (\\"event2\\", \\"2023-10-01 14:22:01\\"), (\\"event3\\", \\"2023-10-01 14:22:02\\"), ] output_events = organize_events(input_events) print(output_events) # OrderedDict([(\'2023-10-01 14:22:01\', [\'event1\', \'event2\']), # (\'2023-10-01 14:22:02\', [\'event3\'])]) ``` Performance Requirements The function should efficiently handle up to 10,000 event records without significant performance degradation. Notes - Pay close attention to the order of insertion when events have the same timestamp. - You are encouraged to use any other relevant utilities from the `collections` module to achieve the results efficiently.","solution":"from collections import OrderedDict from typing import List, Tuple def organize_events(events: List[Tuple[str, str]]) -> OrderedDict[str, List[str]]: Organize events by their timestamp while maintaining the order of their insertion when timestamps are the same. :param events: List of tuples, where each tuple contains an event ID and a timestamp in \'YYYY-MM-DD HH:MM:SS\' format. :return: OrderedDict with timestamps as keys and lists of event IDs as values. organized_events = OrderedDict() for event_id, timestamp in events: if timestamp not in organized_events: organized_events[timestamp] = [] organized_events[timestamp].append(event_id) return organized_events"},{"question":"Coding Assessment Question # Objective The objective of this task is to assess your understanding of the scikit-learn\'s `DecisionTreeClassifier`. You will be given a dataset and your task will be to train a decision tree on this dataset, visualize the tree, and evaluate its performance. # Problem Statement You are provided with the Iris dataset, a classic dataset in machine learning. Your task is to: 1. Load the Iris dataset. 2. Split the dataset into a training set and a test set. 3. Train a `DecisionTreeClassifier` on the training set. 4. Visualize the trained decision tree. 5. Evaluate the accuracy of the model on the test set. 6. Export the structure of the tree in a textual format. # Detailed Instructions 1. **Load the Dataset:** - Use `sklearn.datasets.load_iris()` to load the Iris dataset. 2. **Split the Dataset:** - Split the dataset into 70% training data and 30% testing data using `train_test_split` from `sklearn.model_selection`. 3. **Train the Model:** - Use `DecisionTreeClassifier` from `sklearn.tree` to train a decision tree on the training data. 4. **Visualize the Tree:** - Visualize the trained decision tree using the `plot_tree` function. - Save the plot as \\"decision_tree.png\\". 5. **Evaluate Model:** - Predict the labels of the test set and calculate the accuracy using `accuracy_score` from `sklearn.metrics`. 6. **Export Tree Structure:** - Export the tree structure in a textual format using `export_text`. - Print the text representation of the tree. # Constraints - Use a `random_state` of 42 for `train_test_split` and `DecisionTreeClassifier` to ensure reproducibility. - You are not allowed to use any other machine learning algorithm or library other than scikit-learn for this task. # Expected Output - The figure saved as \\"decision_tree.png\\". - The accuracy score of the model on the test set. - The textual representation of the tree printed in the output. # Example Code ```python from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.tree import DecisionTreeClassifier, plot_tree, export_text from sklearn.metrics import accuracy_score import matplotlib.pyplot as plt # Step 1: Load the dataset data = load_iris() X, y = data.data, data.target # Step 2: Split the dataset X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Step 3: Train the Decision Tree model clf = DecisionTreeClassifier(random_state=42) clf.fit(X_train, y_train) # Step 4: Visualize the Tree plt.figure(figsize=(20,10)) plot_tree(clf, filled=True, feature_names=data.feature_names, class_names=data.target_names) plt.savefig(\\"decision_tree.png\\") # Step 5: Evaluate the Model y_pred = clf.predict(X_test) accuracy = accuracy_score(y_test, y_pred) print(f\\"Accuracy of Decision Tree classifier: {accuracy:.2f}\\") # Step 6: Export Tree Structure in Text Format tree_text = export_text(clf, feature_names=data[\'feature_names\']) print(tree_text) ``` This task will ensure you understand how to work with decision trees in scikit-learn, from training and visualization to evaluation.","solution":"from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.tree import DecisionTreeClassifier, plot_tree, export_text from sklearn.metrics import accuracy_score import matplotlib.pyplot as plt # Step 1: Load the dataset data = load_iris() X, y = data.data, data.target # Step 2: Split the dataset X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Step 3: Train the Decision Tree model clf = DecisionTreeClassifier(random_state=42) clf.fit(X_train, y_train) # Step 4: Visualize the Tree plt.figure(figsize=(20,10)) plot_tree(clf, filled=True, feature_names=data.feature_names, class_names=data.target_names) plt.savefig(\\"decision_tree.png\\") # Step 5: Evaluate the Model y_pred = clf.predict(X_test) accuracy = accuracy_score(y_test, y_pred) print(f\\"Accuracy of Decision Tree classifier: {accuracy:.2f}\\") # Step 6: Export Tree Structure in Text Format tree_text = export_text(clf, feature_names=data[\'feature_names\']) print(tree_text)"},{"question":"# Complex Number Operations in Python Objective You are tasked with implementing a set of functions to perform arithmetic operations on complex numbers in Python. You will use both C structure representations and Python object representations to demonstrate your understanding of the underlying mechanisms. Requirements 1. Implement functions for the following operations on complex numbers: - Sum (`complex_sum`): Adds two complex numbers. - Difference (`complex_diff`): Subtracts the second complex number from the first. - Product (`complex_prod`): Multiplies two complex numbers. - Quotient (`complex_quot`): Divides the first complex number by the second. 2. Each function should accept two `Py_complex` or `complex` values and return the result as a `complex` type in Python. 3. Implement conversion functions between Python complex objects and the `Py_complex` C structure, using appropriate utility functions from the provided API: - `from_pycomplex_to_py_complex(py_complex_obj)` : Converts a Python `complex` object to a `Py_complex` C structure. - `from_py_complex_to_pycomplex(c_complex_struct)`: Converts a `Py_complex` C structure to a Python `complex` object. 4. Handle any edge cases such as division by zero appropriately. Input and Output - **Input**: Two complex numbers (either as `Py_complex` structures or Python `complex` objects). - **Output**: The result of the arithmetic operation as a Python `complex` object. Constraints - You may assume that valid complex numbers will be provided as input. - Performance requirements are not stringent, but your implementation should be efficient and follow best practices. Example ```python # Example complex numbers c1 = Py_complex(1.0, 2.0) # Represents 1 + 2j c2 = Py_complex(3.0, 4.0) # Represents 3 + 4j # Expected outputs print(complex_sum(c1, c2)) # Output: (4+6j) print(complex_diff(c1, c2)) # Output: (-2-2j) print(complex_prod(c1, c2)) # Output: (-5+10j) print(complex_quot(c1, c2)) # Output: (0.44+0.08j) ``` Hints - Use the provided `_Py_c_*` functions for arithmetic operations on `Py_complex` structures. - Use `PyComplex_FromDoubles` and `PyComplex_AsCComplex` to convert between Python complex objects and `Py_complex` structures. Submission Please submit a Python script containing your function implementations along with any necessary test cases.","solution":"class Py_complex: def __init__(self, real, imag): self.real = real self.imag = imag def from_pycomplex_to_py_complex(py_complex_obj): Converts a Python complex object to a Py_complex C structure. return Py_complex(py_complex_obj.real, py_complex_obj.imag) def from_py_complex_to_pycomplex(c_complex_struct): Converts a Py_complex C structure to a Python complex object. return complex(c_complex_struct.real, c_complex_struct.imag) def complex_sum(c1, c2): Returns the sum of two complex numbers. c1_complex = from_py_complex_to_pycomplex(c1) c2_complex = from_py_complex_to_pycomplex(c2) result = c1_complex + c2_complex return from_pycomplex_to_py_complex(result) def complex_diff(c1, c2): Returns the difference of two complex numbers. c1_complex = from_py_complex_to_pycomplex(c1) c2_complex = from_py_complex_to_pycomplex(c2) result = c1_complex - c2_complex return from_pycomplex_to_py_complex(result) def complex_prod(c1, c2): Returns the product of two complex numbers. c1_complex = from_py_complex_to_pycomplex(c1) c2_complex = from_py_complex_to_pycomplex(c2) result = c1_complex * c2_complex return from_pycomplex_to_py_complex(result) def complex_quot(c1, c2): Returns the quotient of two complex numbers. c1_complex = from_py_complex_to_pycomplex(c1) c2_complex = from_py_complex_to_pycomplex(c2) if c2_complex == 0: raise ValueError(\\"Division by zero is not allowed.\\") result = c1_complex / c2_complex return from_pycomplex_to_py_complex(result)"},{"question":"# JSON Custom Encoder and Decoder You have been provided with a JSON string that represents a collection of data points in a scientific experiment. Each data point consists of a timestamp and a complex number as its value. Objective Create a pair of functions to handle this JSON data: 1. A custom encoder that converts the complex numbers into a JSON-serializable format. 2. A custom decoder that converts the JSON-serializable format back into Python complex numbers. Requirements 1. Implement a function `custom_json_encoder(data: Any) -> str`: - This function should take a Python object as input and return its JSON string representation. - It should handle complex numbers by converting them into a dictionary with keys `\\"real\\"` and `\\"imag\\"`. 2. Implement a function `custom_json_decoder(json_data: str) -> Any`: - This function should take a JSON string as input and return the corresponding Python object. - It should convert dictionaries with keys `\\"real\\"` and `\\"imag\\"` back into complex numbers. Example ```python # Example input data data = { \\"experiment\\": \\"physics\\", \\"data_points\\": [ {\\"timestamp\\": \\"2023-10-01T12:00:00Z\\", \\"value\\": 1 + 2j}, {\\"timestamp\\": \\"2023-10-01T12:01:00Z\\", \\"value\\": -3.5 + 4.8j} ] } # Encode the data json_str = custom_json_encoder(data) print(json_str) # Should print a JSON string where complex numbers are represented as: # { # \\"experiment\\": \\"physics\\", # \\"data_points\\": [ # {\\"timestamp\\": \\"2023-10-01T12:00:00Z\\", \\"value\\": {\\"real\\": 1, \\"imag\\": 2}}, # {\\"timestamp\\": \\"2023-10-01T12:01:00Z\\", \\"value\\": {\\"real\\": -3.5, \\"imag\\": 4.8}} # ] # } # Decode the JSON string back to Python object decoded_data = custom_json_decoder(json_str) print(decoded_data) # Should print the original data dictionary with complex numbers ``` Implementation Details 1. **Custom JSON Encoder**: - Create a subclass of `json.JSONEncoder` called `ComplexEncoder`. - Override the `default` method to handle instances of complex numbers. 2. **Custom JSON Decoder**: - Create a helper function `decode_complex(dct)` for use with the `object_hook` parameter of `json.loads`. - This function should convert a dictionary with `\\"real\\"` and `\\"imag\\"` keys into a complex number. 3. **Functions**: - In `custom_json_encoder`, use `json.dumps` with the custom encoder. - In `custom_json_decoder`, use `json.loads` with the custom object hook. Constraints - The JSON input and output must adhere to RFC 7159. - Ensure the functions handle error cases gracefully, such as invalid JSON input. Performance - The solution should efficiently handle typical sizes of JSON data (up to a few MBs). - Ensure that the solution does not cause significant performance degradation due to custom encoding or decoding logic.","solution":"import json class ComplexEncoder(json.JSONEncoder): def default(self, obj): if isinstance(obj, complex): return {\\"real\\": obj.real, \\"imag\\": obj.imag} return super().default(obj) def custom_json_encoder(data): Encodes a Python object to a JSON string, handling complex numbers by converting them into a JSON-serializable format. return json.dumps(data, cls=ComplexEncoder) def decode_complex(dct): if \\"real\\" in dct and \\"imag\\" in dct: return complex(dct[\\"real\\"], dct[\\"imag\\"]) return dct def custom_json_decoder(json_data): Decodes a JSON string back into a Python object, converting dictionaries with \\"real\\" and \\"imag\\" keys back into complex numbers. return json.loads(json_data, object_hook=decode_complex)"},{"question":"Objective In this task, you will demonstrate your ability to perform data visualization using pandas and matplotlib. You will be asked to create, customize plots and utilize different plot types to visualize the data comprehensively. Problem Statement You are provided with a dataset containing the monthly temperature and precipitation data of a certain location from January 2010 to December 2019. The dataset has the following columns: - `date`: date ranging from `2010-01-01` to `2019-12-31` - `temperature`: monthly average temperature in Celsius. - `precipitation`: monthly total precipitation in millimeters. Your task is to: 1. Load the dataset into a pandas DataFrame. 2. Create a time series line plot displaying both temperature and precipitation on the same graph. - Use a secondary y-axis for precipitation. - Customize the primary y-axis to show temperature with a label \\"Temperature (°C)\\" and the secondary y-axis to show precipitation with a label \\"Precipitation (mm)\\". - Add appropriate titles and legends. 3. Create a box plot to visualize the distribution of temperature for each year. 4. Create a scatter plot showing the relationship between temperature and precipitation. - Use color coding to differentiate based on year. - Add appropriate legend and title. 5. Create a histogram of the temperature data showing the frequency distribution. 6. Save each figure as a png file. Input A CSV file named `climate_data.csv` with the mentioned columns. Output 1. **time_series_plot.png**: A time series line plot with temperature and precipitation. 2. **temperature_box_plot.png**: A box plot for temperature distribution over the years. 3. **temperature_precipitation_scatter.png**: A scatter plot showing the relationship between temperature and precipitation. 4. **temperature_histogram.png**: A histogram of the temperature data. Performance Requirements 1. The plots must be readable and properly labeled. 2. The scatter plot must effectively use color coding for different years. 3. Histogram bins should be appropriately chosen to show a clear distribution of temperature data. Constraints - Use the pandas and matplotlib libraries for data handling and visualization. - Ensure your plots are saved correctly and in a presentable format. Example Below is an example of how to structure the code for one of the plots: ```python import pandas as pd import matplotlib.pyplot as plt # Load the dataset data = pd.read_csv(\'climate_data.csv\') data[\'date\'] = pd.to_datetime(data[\'date\']) data.set_index(\'date\', inplace=True) # Time Series Line Plot fig, ax1 = plt.subplots() ax1.set_xlabel(\'Date\') ax1.set_ylabel(\'Temperature (°C)\', color=\'tab:red\') ax1.plot(data.index, data[\'temperature\'], color=\'tab:red\', label=\'Temperature\') ax1.tick_params(axis=\'y\', labelcolor=\'tab:red\') ax2 = ax1.twinx() # instantiate a second axes that shares the same x-axis ax2.set_ylabel(\'Precipitation (mm)\', color=\'tab:blue\') # we already handled the x-label with ax1 ax2.plot(data.index, data[\'precipitation\'], color=\'tab:blue\', label=\'Precipitation\') ax2.tick_params(axis=\'y\', labelcolor=\'tab:blue\') fig.tight_layout() # otherwise the right y-label is slightly clipped plt.title(\'Monthly Temperature and Precipitation (2010-2019)\') fig.legend(loc=\'upper right\', bbox_to_anchor=(1,1), bbox_transform=ax1.transAxes) plt.savefig(\'time_series_plot.png\') plt.show() ``` You are expected to follow similar steps for the other required plots. Good luck!","solution":"import pandas as pd import matplotlib.pyplot as plt import seaborn as sns def create_plots(): # Load the dataset data = pd.read_csv(\'climate_data.csv\') data[\'date\'] = pd.to_datetime(data[\'date\']) data.set_index(\'date\', inplace=True) data[\'year\'] = data.index.year # Time Series Line Plot fig, ax1 = plt.subplots(figsize=(12, 6)) ax1.set_xlabel(\'Date\') ax1.set_ylabel(\'Temperature (°C)\', color=\'tab:red\') ax1.plot(data.index, data[\'temperature\'], color=\'tab:red\', label=\'Temperature\') ax1.tick_params(axis=\'y\', labelcolor=\'tab:red\') ax2 = ax1.twinx() ax2.set_ylabel(\'Precipitation (mm)\', color=\'tab:blue\') ax2.plot(data.index, data[\'precipitation\'], color=\'tab:blue\', label=\'Precipitation\') ax2.tick_params(axis=\'y\', labelcolor=\'tab:blue\') fig.tight_layout() plt.title(\'Monthly Temperature and Precipitation (2010-2019)\') fig.legend(loc=\'upper left\', bbox_to_anchor=(0.1, 0.9)) plt.savefig(\'time_series_plot.png\') plt.show() # Box Plot for temperature distributions plt.figure(figsize=(12, 6)) sns.boxplot(x=\'year\', y=\'temperature\', data=data) plt.title(\'Yearly Temperature Distribution (2010-2019)\') plt.xlabel(\'Year\') plt.ylabel(\'Temperature (°C)\') plt.savefig(\'temperature_box_plot.png\') plt.show() # Scatter Plot showing temperature vs. precipitation plt.figure(figsize=(12, 6)) sns.scatterplot(x=\'temperature\', y=\'precipitation\', hue=\'year\', palette=\'viridis\', data=data) plt.title(\'Temperature vs. Precipitation (2010-2019)\') plt.xlabel(\'Temperature (°C)\') plt.ylabel(\'Precipitation (mm)\') plt.legend(title=\'Year\', bbox_to_anchor=(1, 1), loc=\\"upper left\\") plt.savefig(\'temperature_precipitation_scatter.png\') plt.show() # Histogram for temperature data plt.figure(figsize=(12, 6)) plt.hist(data[\'temperature\'], bins=20, color=\'skyblue\', edgecolor=\'black\') plt.title(\'Temperature Frequency Distribution (2010-2019)\') plt.xlabel(\'Temperature (°C)\') plt.ylabel(\'Frequency\') plt.savefig(\'temperature_histogram.png\') plt.show()"},{"question":"Data Visualization with Seaborn You are provided with the Titanic dataset. Your task is to visualize various aspects of the dataset using Seaborn\'s `catplot` and customize the plots to provide insightful data views. Requirements 1. **Load the Titanic dataset**: Use Seaborn’s `load_dataset` function. 2. **Create a violin plot**: - Plot the age distribution (`x=\\"age\\"`) against the class of passenger (`y=\\"class\\"`). - Use different colors to denote gender (`hue=\\"sex\\"`). - Adjust the violin plot parameters (`bw_adjust=0.5`, `cut=0`, `split=True`). 3. **Create a bar plot**: - Show the survival rate (`y=\\"survived\\"`) for each class (`x=\\"class\\"`). - Separate plots for each gender (`col=\\"sex\\"`). - Set figure height to 4 and aspect ratio to 0.6. 4. **Combine plots**: - Create a violin plot for age distribution against class (`x=\\"age\\"`, `y=\\"class\\"`, `kind=\\"violin\\"`) without inner data points (`inner=None`). - Overlay a swarm plot (`sns.swarmplot`) on top of the violin plot for the same x and y axes. 5. **Customize FacetGrid**: - Create a bar plot showing survival rate (`y=\\"survived\\"`) split by who survived (`x=\\"who\\"`, `col=\\"class\\"`). - Customize the plot using `FacetGrid` methods: - Remove axis labels for x (`set_axis_labels`). - Label the y-axis as \\"Survival Rate\\". - Set x-tick labels to [\\"Men\\", \\"Women\\", \\"Children\\"] (`set_xticklabels`). - Title each subplot with class name. - Set y-axis limits between 0 and 1. - Remove spine on the left side of plots. Input - No external input is required. You will use the Titanic dataset provided by Seaborn. Output - Display all the requested plots with the specified customizations. Constraints - Use only Seaborn for plotting. You may use Matplotlib for minor layout adjustments if necessary. ```python import seaborn as sns # Load dataset df = sns.load_dataset(\\"titanic\\") # Task 2: Create a violin plot sns.catplot(data=df, x=\\"age\\", y=\\"class\\", hue=\\"sex\\", kind=\\"violin\\", bw_adjust=0.5, cut=0, split=True) # Task 3: Create a bar plot sns.catplot(data=df, x=\\"class\\", y=\\"survived\\", col=\\"sex\\", kind=\\"bar\\", height=4, aspect=0.6) # Task 4: Combine plots sns.catplot(data=df, x=\\"age\\", y=\\"class\\", kind=\\"violin\\", color=\\".9\\", inner=None) sns.swarmplot(data=df, x=\\"age\\", y=\\"class\\", size=3) # Task 5: Customize FacetGrid g = sns.catplot(data=df, x=\\"who\\", y=\\"survived\\", col=\\"class\\", kind=\\"bar\\", height=4, aspect=0.6) g.set_axis_labels(\\"\\", \\"Survival Rate\\") g.set_xticklabels([\\"Men\\", \\"Women\\", \\"Children\\"]) g.set_titles(\\"{col_name} {col_var}\\") g.set(ylim=(0, 1)) g.despine(left=True) ```","solution":"import seaborn as sns import matplotlib.pyplot as plt # Load dataset df = sns.load_dataset(\\"titanic\\") # Task 2: Create a violin plot def create_violin_plot(df): sns.catplot(data=df, x=\\"age\\", y=\\"class\\", hue=\\"sex\\", kind=\\"violin\\", bw_adjust=0.5, cut=0, split=True) plt.show() # Task 3: Create a bar plot def create_bar_plot(df): sns.catplot(data=df, x=\\"class\\", y=\\"survived\\", col=\\"sex\\", kind=\\"bar\\", height=4, aspect=0.6) plt.show() # Task 4: Combine plots def combine_violin_and_swarm_plot(df): sns.catplot(data=df, x=\\"age\\", y=\\"class\\", kind=\\"violin\\", color=\\".9\\", inner=None) sns.swarmplot(data=df, x=\\"age\\", y=\\"class\\", size=3) plt.show() # Task 5: Customize FacetGrid def customize_facet_grid(df): g = sns.catplot(data=df, x=\\"who\\", y=\\"survived\\", col=\\"class\\", kind=\\"bar\\", height=4, aspect=0.6) g.set_axis_labels(\\"\\", \\"Survival Rate\\") g.set_xticklabels([\\"Men\\", \\"Women\\", \\"Children\\"]) g.set_titles(col_template=\\"{col_name}\\") g.set(ylim=(0, 1)) g.despine(left=True) plt.show()"},{"question":"Coding Assessment Question # Problem Statement You are given a project directory with an unknown number of subdirectories and files. Your task is to write a function `pattern_match_files` that takes the following parameters: 1. `pattern` (str): A Unix shell-style wildcard pattern. 2. `root_dir` (str, optional): The directory to start the search. If not provided, the current directory should be used. 3. `recursive` (bool, optional): If set to `True`, the function should search recursively in all subdirectories. The function should return a sorted list of file paths that match the given pattern. The function should leverage the `glob` module to perform the pattern matching. # Input and Output - **Input:** - `pattern` (str): The wildcard pattern to match filenames. - `root_dir` (str, optional): The starting directory for the search. - `recursive` (bool, optional): Indicates if the search should be recursive. Default is `False`. - **Output:** - A sorted list of matching file paths. # Constraints - The function should handle large directory trees efficiently when recursive search is enabled. - The function should ignore files that start with a dot (.) unless the pattern explicitly includes the leading dot. - You may assume the file system is case-sensitive. # Example Usage ```python # Example 1: # Directory structure: # /project # |-- file1.txt # |-- file2.py # |-- test # |-- file3.txt result = pattern_match_files(\'*.txt\') print(result) # Output: [\'file1.txt\'] # Example 2: result = pattern_match_files(\'*.txt\', root_dir=\'project/test\') print(result) # Output: [\'file3.txt\'] # Example 3: result = pattern_match_files(\'*.txt\', recursive=True) print(result) # Output: [\'file1.txt\', \'file3.txt\'] ``` # Implementation You need to implement the function `pattern_match_files` that achieves the described functionality. ```python import glob def pattern_match_files(pattern, root_dir=None, recursive=False): Finds files matching the given pattern. Parameters: pattern (str): The wildcard pattern to match filenames. root_dir (str, optional): The starting directory for the search. Defaults to current directory. recursive (bool, optional): If set to True, search recursively in all subdirectories. Defaults to False. Returns: list: A sorted list of matching file paths. # Your code goes here. if root_dir is None: root_dir = \'.\' paths = glob.glob(pattern, root_dir=root_dir, recursive=recursive) return sorted(paths) ``` Write the code to complete the function `pattern_match_files` using the guidelines provided.","solution":"import glob import os def pattern_match_files(pattern, root_dir=None, recursive=False): Finds files matching the given pattern. Parameters: pattern (str): The wildcard pattern to match filenames. root_dir (str, optional): The starting directory for the search. Defaults to current directory. recursive (bool, optional): If set to True, search recursively in all subdirectories. Defaults to False. Returns: list: A sorted list of matching file paths. if root_dir is None: root_dir = \'.\' search_pattern = os.path.join(root_dir, \'**\', pattern) if recursive else os.path.join(root_dir, pattern) paths = glob.glob(search_pattern, recursive=recursive) return sorted(paths)"},{"question":"Implementing FSDP2 in PyTorch Objective: The objective of this exercise is to assess your understanding of PyTorch\'s Fully Sharded Data Parallelism (FSDP2) by implementing a model using this framework. You will demonstrate how to shard model parameters across data parallel workers and manage memory efficiently. Problem Statement: You are given a simple neural network model implemented using PyTorch. Your task is to modify this model to use FSDP2 for fully sharded data parallelism. This will involve dynamically constructing a fully sharded version of the model and ensuring that the parameters are correctly sharded for distributed training. Model: A simple neural network model defined as follows: ```python import torch import torch.nn as nn class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.linear1 = nn.Linear(10, 20) self.relu = nn.ReLU() self.linear2 = nn.Linear(20, 1) def forward(self, x): x = self.linear1(x) x = self.relu(x) x = self.linear2(x) return x ``` Task: 1. **Modify the model** to use FSDP2. This involves sharding the parameters of `SimpleModel` across data parallel workers using the `fully_shard` function. 2. **Implement necessary imports and setup** for distributed training. 3. **Demonstrate memory management** improvements offered by FSDP2. Constraints and Requirements: 1. You must use the `torch.distributed.fsdp.fully_shard` function to shard the model. 2. Ensure that the model can be trained in a distributed setup. 3. Highlight improvements in memory management when using FSDP2. 4. The code should be clear and well-documented, providing explanations for each step. Expected Input: - A simple neural network model: `SimpleModel`. Expected Output: - A distributed and fully sharded model using FSDP2. Performance Requirements: - Efficient memory usage with deterministic behavior. - Model should correctly train in a distributed environment. Example Code: Here is an example of the expected implementation: ```python import torch import torch.distributed as dist from torch.distributed.fsdp import fully_shard from torch.nn.parallel import DistributedDataParallel as DDP import torch.nn as nn def setup(rank, world_size): dist.init_process_group(\\"gloo\\", rank=rank, world_size=world_size) def cleanup(): dist.destroy_process_group() class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.linear1 = nn.Linear(10, 20) self.relu = nn.ReLU() self.linear2 = nn.Linear(20, 1) def forward(self, x): x = self.linear1(x) x = self.relu(x) x = self.linear2(x) return x def main(rank, world_size): setup(rank, world_size) # Initialize the model model = SimpleModel() # Apply FSDP2 sharding fsdp_model = fully_shard(model) # Move the model to the appropriate device device = torch.device(f\'cuda:{rank}\') fsdp_model.to(device) # Define a simple dataset and dataloader dataset = torch.randn(100, 10) target = torch.randn(100, 1) dataloader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(dataset, target), batch_size=10) # Define a simple loss function and optimizer criterion = nn.MSELoss() optimizer = torch.optim.SGD(fsdp_model.parameters(), lr=0.01) # Training loop fsdp_model.train() for epoch in range(5): for data, target in dataloader: optimizer.zero_grad() data, target = data.to(device), target.to(device) output = fsdp_model(data) loss = criterion(output, target) loss.backward() optimizer.step() print(f\\"Epoch {epoch+1}, Loss: {loss.item()}\\") cleanup() # Run the training process world_size = 2 # Number of processes (adjust as needed) torch.multiprocessing.spawn(main, args=(world_size,), nprocs=world_size, join=True) ``` Note: Make sure to include the necessary documentation and comments in your code to explain each step and the role of each component in enabling FSDP2 for distributed training.","solution":"import torch import torch.nn as nn import torch.distributed as dist from torch.distributed.fsdp import FullyShardedDataParallel as FSDP import torch.multiprocessing as mp def setup(rank, world_size): Initialize the process group for distributed training. dist.init_process_group(backend=\'nccl\', init_method=\'env://\', rank=rank, world_size=world_size) def cleanup(): Destroy the process group for distributed training. dist.destroy_process_group() class SimpleModel(nn.Module): def __init__(self): Initialize the simple model with two linear layers and ReLU activation. super(SimpleModel, self).__init__() self.linear1 = nn.Linear(10, 20) self.relu = nn.ReLU() self.linear2 = nn.Linear(20, 1) def forward(self, x): Define the forward pass of the model. x = self.linear1(x) x = self.relu(x) x = self.linear2(x) return x def main(rank, world_size): setup(rank, world_size) # Set device device = torch.device(f\'cuda:{rank}\') torch.cuda.set_device(device) # Initialize the model and apply FSDP model = SimpleModel().to(device) fsdp_model = FSDP(model) # Define a simple dataset and dataloader dataset = torch.utils.data.TensorDataset(torch.randn(100, 10), torch.randn(100, 1)) dataloader = torch.utils.data.DataLoader(dataset, batch_size=10) # Define a loss function and optimizer criterion = nn.MSELoss() optimizer = torch.optim.SGD(fsdp_model.parameters(), lr=0.01) # Training loop fsdp_model.train() for epoch in range(5): for data, target in dataloader: data, target = data.to(device), target.to(device) optimizer.zero_grad() output = fsdp_model(data) loss = criterion(output, target) loss.backward() optimizer.step() print(f\\"Rank {rank}, Epoch {epoch+1}, Loss: {loss.item()}\\") cleanup() if __name__ == \'__main__\': world_size = 2 # Number of processes mp.spawn(main, args=(world_size,), nprocs=world_size, join=True)"},{"question":"**Coding Assessment Question** # Objective: Evaluate the student\'s ability to implement, train, and visualize a decision tree classifier using scikit-learn. The student should also demonstrate the ability to handle overfitting by applying pruning techniques. # Problem Statement: You are tasked with implementing a pipeline to classify the Iris dataset using a Decision Tree Classifier from scikit-learn. Your implementation should cover the following requirements: 1. Load the Iris dataset. 2. Train a Decision Tree classifier on the dataset. 3. Visualize the decision tree. 4. Implement pruning to handle overfitting. 5. Evaluate the classifier\'s performance before and after pruning. # Requirements: 1. Implement a function `train_decision_tree_classifier()` that: - Loads the Iris dataset using `load_iris` from `sklearn.datasets`. - Splits the data into training and testing sets. - Trains a Decision Tree Classifier on the training data. - Returns the trained Decision Tree Classifier, training data, and testing data. 2. Implement a function `evaluate_classifier()` that: - Takes a classifier and test data as input. - Returns the accuracy score of the classifier on the test data. 3. Implement a function `visualize_tree()` that: - Takes a trained classifier and feature names as input. - Visualizes the decision tree using `plot_tree()` from `sklearn.tree`. 4. Implement a function `prune_tree()` that: - Takes a trained Decision Tree Classifier and a pruning parameter `ccp_alpha`. - Applies minimal cost-complexity pruning using the `ccp_alpha` parameter. - Returns the pruned classifier. 5. Compare the performance of the classifier before and after pruning. # Input Format: - No direct input. The dataset will be loaded within the function. - `ccp_alpha` parameter for pruning will be fixed at `0.01`. # Output Format: - Print the accuracy of the classifier before pruning. - Print the accuracy of the classifier after pruning. - Visualize the unpruned and pruned trees. # Constraints: - Use scikit-learn\'s `DecisionTreeClassifier` for implementation. - Use the provided `ccp_alpha` value for pruning. # Example: ```python def train_decision_tree_classifier(): from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.tree import DecisionTreeClassifier # Load Iris dataset iris = load_iris() X = iris.data y = iris.target feature_names = iris.feature_names # Split into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Train Decision Tree Classifier clf = DecisionTreeClassifier(random_state=42) clf.fit(X_train, y_train) return clf, X_train, X_test, y_train, y_test, feature_names def evaluate_classifier(clf, X_test, y_test): from sklearn.metrics import accuracy_score y_pred = clf.predict(X_test) return accuracy_score(y_test, y_pred) def visualize_tree(clf, feature_names): from sklearn.tree import plot_tree import matplotlib.pyplot as plt plt.figure(figsize=(20,10)) plot_tree(clf, feature_names=feature_names, filled=True) plt.show() def prune_tree(clf, ccp_alpha): clf_pruned = DecisionTreeClassifier(random_state=42, ccp_alpha=ccp_alpha) clf_pruned.fit(clf.tree_.value[:, 0, :], clf.tree_.value[:, 1, :]) return clf_pruned # Main Execution clf, X_train, X_test, y_train, y_test, feature_names = train_decision_tree_classifier() print(\\"Accuracy before pruning:\\", evaluate_classifier(clf, X_test, y_test)) visualize_tree(clf, feature_names) ccp_alpha = 0.01 clf_pruned = prune_tree(clf, ccp_alpha) print(\\"Accuracy after pruning:\\", evaluate_classifier(clf_pruned, X_test, y_test)) visualize_tree(clf_pruned, feature_names) ``` # Note: Ensure that your implementation handles necessary imports and appropriate error handling.","solution":"from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.tree import DecisionTreeClassifier, plot_tree from sklearn.metrics import accuracy_score import matplotlib.pyplot as plt def train_decision_tree_classifier(): Loads the Iris dataset, splits it into training and testing sets, and trains a Decision Tree Classifier on the training data. # Load the iris dataset iris = load_iris() X = iris.data y = iris.target feature_names = iris.feature_names # Split into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Train a Decision Tree Classifier clf = DecisionTreeClassifier(random_state=42) clf.fit(X_train, y_train) return clf, X_train, X_test, y_train, y_test, feature_names def evaluate_classifier(clf, X_test, y_test): Evaluates the classifier\'s performance on the test data. y_pred = clf.predict(X_test) return accuracy_score(y_test, y_pred) def visualize_tree(clf, feature_names): Visualizes the decision tree using the plot_tree method. plt.figure(figsize=(20,10)) plot_tree(clf, feature_names=feature_names, filled=True) plt.show() def prune_tree(clf, X_train, y_train, ccp_alpha): Prunes the decision tree classifier using the given ccp_alpha. # Train a new pruned Decision Tree Classifier clf_pruned = DecisionTreeClassifier(random_state=42, ccp_alpha=ccp_alpha) clf_pruned.fit(X_train, y_train) return clf_pruned # Main Execution clf, X_train, X_test, y_train, y_test, feature_names = train_decision_tree_classifier() print(\\"Accuracy before pruning:\\", evaluate_classifier(clf, X_test, y_test)) visualize_tree(clf, feature_names) ccp_alpha = 0.01 clf_pruned = prune_tree(clf, X_train, y_train, ccp_alpha) print(\\"Accuracy after pruning:\\", evaluate_classifier(clf_pruned, X_test, y_test)) visualize_tree(clf_pruned, feature_names)"},{"question":"Objective: Assess students\' ability to use the `Styler` object in pandas to apply various styles to a DataFrame and export it in a specific format. Task: You are provided with a DataFrame containing information about a set of products. Your task is to write a function `style_dataframe(df)` that performs the following operations: 1. **Highlight** the maximum price in each column where applicable. 2. **Apply a background gradient** to the \'Price\' column. 3. **Hide** the \'ID\' column from the styled output. 4. **Set a caption** for the table: \\"Product Information\\". 5. **Export** the styled DataFrame to an HTML string. Input: - `df`: A pandas DataFrame with at least the following columns: \'ID\', \'Product\', \'Category\', and \'Price\'. Output: - Return the HTML string of the styled DataFrame. Constraints: - The \'Price\' column contains numerical values. - The \'ID\' column contains unique identifiers which should not be displayed in the styled HTML output. Example: Given the following DataFrame: ```python import pandas as pd data = { \'ID\': [101, 102, 103, 104], \'Product\': [\'Table\', \'Chair\', \'Laptop\', \'Pen\'], \'Category\': [\'Furniture\', \'Furniture\', \'Electronics\', \'Stationery\'], \'Price\': [100.0, 50.0, 1500.0, 2.0] } df = pd.DataFrame(data) ``` When you call `style_dataframe(df)`, it should return the HTML string of the styled DataFrame with the specified styles applied. Implementation: ```python import pandas as pd def style_dataframe(df): # Your code here to style the DataFrame and return it as an HTML string pass ``` Make sure your implementation meets the requirements stated in the task.","solution":"import pandas as pd def style_dataframe(df): Styles the given DataFrame as per the stated requirements and returns it as an HTML string. # Hide the \'ID\' column, highlight the maximum in each column, apply background gradient to \'Price\' column, and set a caption styled_df = (df.style .hide(\'ID\', axis=1) .highlight_max(subset=[\'Price\']) .background_gradient(subset=[\'Price\'], cmap=\'viridis\') .set_caption(\\"Product Information\\")) # Convert the styled DataFrame to an HTML string html = styled_df.to_html() return html"},{"question":"You are given a dataset containing customer orders. Each order has an optional discount value that might be missing. Your task is to prepare the dataset for further analysis by ensuring that the discount column uses a nullable integer type and to perform some operations to gain insights. Below is the initial data: ```python data = { \\"order_id\\": [1001, 1002, 1003, 1004, 1005], \\"customer_id\\": [201, 202, 201, 203, 202], \\"amount\\": [150, 200, 50, 100, 250], \\"discount\\": [10, None, 5, None, 20] } ``` # Requirements: 1. **Data Preparation**: - Convert the provided data into a DataFrame. - Ensure that the `discount` column uses a nullable integer dtype. 2. **Operations**: - Calculate the total amount after discount for each order and add it as a new column `total_amount`. (Total Amount = Amount - Discount, assume no discount is equivalent to 0). - Determine the total amount after discount for each customer by grouping the data by `customer_id`. 3. **Output**: - Return the modified DataFrame after adding the `total_amount` column. - Return a DataFrame containing the customer ID and their corresponding total amount after discount. # Input: - None. The function should internally create and process the data. # Output: 1. A DataFrame with columns `order_id`, `customer_id`, `amount`, `discount`, and `total_amount`. 2. A DataFrame with columns `customer_id` and `total_amount_after_discount`. # Function Signature: ```python def process_orders(): pass ``` # Example: ```python df_orders, df_customers = process_orders() print(df_orders) # Expected Output: # order_id customer_id amount discount total_amount # 0 1001 201 150 10 140 # 1 1002 202 200 <NA> 200 # 2 1003 201 50 5 45 # 3 1004 203 100 <NA> 100 # 4 1005 202 250 20 230 print(df_customers) # Expected Output: # customer_id total_amount_after_discount # 0 201 185 # 1 202 430 # 2 203 100 ``` # Constraints: - You must use pandas library for data manipulation. - Make sure the `discount` column maintains a nullable integer dtype throughout the operations.","solution":"import pandas as pd import numpy as np def process_orders(): data = { \\"order_id\\": [1001, 1002, 1003, 1004, 1005], \\"customer_id\\": [201, 202, 201, 203, 202], \\"amount\\": [150, 200, 50, 100, 250], \\"discount\\": [10, None, 5, None, 20] } # Step 1: Create DataFrame and ensure discount column is a nullable integer type df = pd.DataFrame(data) df[\'discount\'] = df[\'discount\'].astype(\'Int64\') # Step 2: Calculate total amount after discount for each order df[\'total_amount\'] = df[\'amount\'] - df[\'discount\'].fillna(0) # Step 3: Determine the total amount after discount for each customer df_customers = df.groupby(\'customer_id\')[\'total_amount\'].sum().reset_index() df_customers.rename(columns={\'total_amount\': \'total_amount_after_discount\'}, inplace=True) return df, df_customers"},{"question":"# Problem: Sales Data Analysis with Nullable Boolean Values You are provided with a sales dataset represented as a pandas DataFrame. Some entries in the dataset contain missing values (`NA`). You are to perform various operations on this DataFrame using pandas\' nullable boolean dtype. Dataset Description The DataFrame `sales_df` contains the following columns: - `region`: The region code where the sale was made. - `product`: The product code that was sold. - `quantity`: The number of units sold. - `returned`: Nullable boolean values indicating whether the product was returned (`True`), not returned (`False`), or unknown (`NA`). Tasks: 1. **Data Cleaning**: Create a new column `returned_filled` in the DataFrame, filling NA values in the `returned` column with `False`. 2. **Sales Validation**: Create a boolean mask `high_sales` to identify sales transactions where the quantity sold is greater than 100 units. 3. **Region Filter**: Create a nullable boolean mask `north_region` to identify transactions from a specific region, say `\'North\'`. The mask should mark other regions with `NA`. 4. **Final Selection**: Using the masks `high_sales` and `north_region`, filter the DataFrame to identify transactions that meet both criteria. Function Signature ```python import pandas as pd from typing import Any def sales_data_analysis(sales_df: pd.DataFrame) -> pd.DataFrame: Analyzes the sales dataset for given tasks. Args: sales_df (pd.DataFrame): A DataFrame containing sales data. Returns: pd.DataFrame: A DataFrame with filtered transactions. # Task 1: Fill NA values in `returned` column with False sales_df[\'returned_filled\'] = sales_df[\'returned\'].fillna(False) # Task 2: Create a boolean mask for high sales high_sales = sales_df[\'quantity\'] > 100 # Task 3: Create a nullable boolean mask for region \'North\' north_region = pd.array(sales_df[\'region\'] == \'North\', dtype=\'boolean\') & pd.array(sales_df[\'region\'] != \'North\', dtype=\'boolean\').fillna(pd.NA) # Task 4: Filter the DataFrame using both masks filtered_df = sales_df[high_sales & north_region.fillna(False)] return filtered_df ``` Example ```python data = { \'region\': [\'North\', \'South\', \'East\', \'West\', \'North\'], \'product\': [\'A\', \'B\', \'C\', \'D\', \'A\'], \'quantity\': [150, 80, 90, 200, 130], \'returned\': [pd.NA, True, False, pd.NA, False] } sales_df = pd.DataFrame(data) result_df = sales_data_analysis(sales_df) print(result_df) ``` Expected Output: ``` region product quantity returned returned_filled 0 North A 150 <NA> False 4 North A 130 False False ```","solution":"import pandas as pd from typing import Any def sales_data_analysis(sales_df: pd.DataFrame) -> pd.DataFrame: Analyzes the sales dataset for given tasks. Args: sales_df (pd.DataFrame): A DataFrame containing sales data. Returns: pd.DataFrame: A DataFrame with filtered transactions. # Task 1: Fill NA values in `returned` column with False sales_df[\'returned_filled\'] = sales_df[\'returned\'].fillna(False) # Task 2: Create a boolean mask for high sales high_sales = sales_df[\'quantity\'] > 100 # Task 3: Create a nullable boolean mask for region \'North\' north_region = pd.Series(sales_df[\'region\'] == \'North\', dtype=\'boolean\') # Task 4: Filter the DataFrame using both masks filtered_df = sales_df[high_sales & north_region.fillna(False)] return filtered_df"},{"question":"**File System Inspector Using \\"stat\\" Module** In this coding task, you are required to implement a function that inspects a given directory and all its subdirectories to gather detailed information about each file. You will use the `stat` module to perform various checks and to format this information as specified. # Function Signature ```python def inspect_directory(dir_path: str) -> dict: pass ``` # Input - `dir_path` (str): The path to the directory to be inspected. You can assume the path is valid. # Output - `result` (dict): A dictionary containing detailed information about each file and directory within the given directory and its subdirectories. The keys are the relative paths to the files and directories, and the values are dictionaries with the following structure: ```python { \\"type\\": str, # The type of the file (e.g., \\"directory\\", \\"regular file\\", \\"symlink\\"). \\"mode\\": str, # The human-readable file mode (e.g., \\"-rwxr-xr-x\\"). \\"size\\": int, # Size of the file in bytes. \\"last_access\\": float, # Time of last access as a UNIX timestamp. \\"last_modification\\": float # Time of last modification as a UNIX timestamp. } ``` # Constraints - You should not use any external libraries other than `os`, `sys`, and `stat`. - The solution should handle large directories efficiently. - Ensure to handle different file types appropriately using the functions provided by the `stat` module. - The symbolic link (`symlink`) file type should be resolved as its own type, and not followed. # Example Assume the directory structure is as follows: ``` /example_dir file1.txt subdir1/ file2.txt link1 -> ../file1.txt ``` Calling `inspect_directory(\\"/example_dir\\")` might produce: ```python { \\"file1.txt\\": { \\"type\\": \\"regular file\\", \\"mode\\": \\"-rw-r--r--\\", \\"size\\": 1024, \\"last_access\\": 1639296000.0, \\"last_modification\\": 1639296000.0 }, \\"subdir1\\": { \\"type\\": \\"directory\\", \\"mode\\": \\"drwxr-xr-x\\", \\"size\\": 4096, \\"last_access\\": 1639297000.0, \\"last_modification\\": 1639297000.0 }, \\"subdir1/file2.txt\\": { \\"type\\": \\"regular file\\", \\"mode\\": \\"-rw-r--r--\\", \\"size\\": 2048, \\"last_access\\": 1639298000.0, \\"last_modification\\": 1639298000.0 }, \\"subdir1/link1\\": { \\"type\\": \\"symlink\\", \\"mode\\": \\"lrwxrwxrwx\\", \\"size\\": 13, \\"last_access\\": 1639299000.0, \\"last_modification\\": 1639299000.0 } } ``` # Notes - Utilize `os.lstat()` to obtain information about symbolic links without resolving them. - Use the `stat` module functions to determine file types and format modes. - Ensure you recurse into subdirectories to gather information on all files and directories within the given directory tree.","solution":"import os import stat def inspect_directory(dir_path: str) -> dict: result = {} def gather_info(path, rel_path): stat_info = os.lstat(path) file_info = { \\"type\\": \\"\\", \\"mode\\": stat.filemode(stat_info.st_mode), \\"size\\": stat_info.st_size, \\"last_access\\": stat_info.st_atime, \\"last_modification\\": stat_info.st_mtime } if stat.S_ISDIR(stat_info.st_mode): file_info[\\"type\\"] = \\"directory\\" elif stat.S_ISREG(stat_info.st_mode): file_info[\\"type\\"] = \\"regular file\\" elif stat.S_ISLNK(stat_info.st_mode): file_info[\\"type\\"] = \\"symlink\\" else: file_info[\\"type\\"] = \\"other\\" result[rel_path] = file_info if stat.S_ISDIR(stat_info.st_mode): for entry in os.listdir(path): entry_path = os.path.join(path, entry) entry_rel_path = os.path.join(rel_path, entry) gather_info(entry_path, entry_rel_path) gather_info(dir_path, os.path.basename(dir_path)) return result"},{"question":"**Objective**: You are required to implement a Python function that processes a dataset and optimizes its performance using both debugging and profiling tools described in the documentation. # Problem Statement You need to implement a function `process_data(data: list) -> list` that accepts a list of integers and returns a list of processed integers. The function should square each number in the input list and then sort the list in non-decreasing order. Finally, return the sorted list. Additionally, you need to create a script that: 1. **Debugs** the function to ensure there are no logical or runtime errors. 2. **Profiles** the function to measure the execution time and identify any bottlenecks. 3. **Traces** the memory usage during execution. # Requirements 1. Implement the `process_data` function. 2. Use the `pdb` module to set a breakpoint and step through the function to verify its correctness. 3. Use the `cProfile` module to profile the `process_data` function and summarize the time taken by each part of the function. 4. Use the `tracemalloc` module to trace memory allocations and display memory usage statistics. # Constraints - The input list will contain between 1 and 100,000 integers. - Each integer will be between -10,000 and 10,000. # Expected Input and Output - **Input**: List of integers `data` - **Output**: List of squared and sorted integers. # Example ```python data = [-4, -1, 0, 3, 10] print(process_data(data)) # Output: [0, 1, 9, 16, 100] ``` # Performance Requirements - The function should be optimized to handle the upper limit of constraints efficiently. # Additional Script Write a separate script (`debug_profile_trace.py`) that performs the following: 1. Sets a breakpoint within the `process_data` function using `pdb` and verifies the function\'s steps. 2. Profiles the function using `cProfile` and outputs a summary of execution times. 3. Traces the memory allocation using `tracemalloc` and prints memory usage statistics. Submit both the function and the script as part of your answer.","solution":"def process_data(data: list) -> list: Squares each number in the input list and sorts the list in non-decreasing order. Parameters: data (list): List of integers. Returns: list: List of squared and sorted integers. # Square each number in the list squared_data = [x ** 2 for x in data] # Sort the list in non-decreasing order sorted_squares = sorted(squared_data) return sorted_squares"},{"question":"Objective: Implement a custom Python function that mimics a subset of the argument parsing functionalities described in the provided documentation. Specifically, you will need to handle string and integer parsing from a list of mixed argument types. Task: You need to create a function `custom_parse_args` that takes a list of arguments and a format string, and returns a tuple of parsed values. The format string will contain format units that describe the expected types of the arguments as follows: - `\\"s\\"`: expects a string (str). - `\\"i\\"`: expects an integer (int). Function Signature: ```python def custom_parse_args(args: list, format: str) -> tuple: pass ``` Input: - `args`: A list of mixed argument types that need to be parsed. - `format`: A string containing the format units (`\\"s\\"` for string, `\\"i\\"` for integer). Output: - A tuple containing the parsed values in the order specified by the format string. Constraints: 1. The length of the `args` list must match the length of the `format` string. 2. The function should handle type mismatches by raising an appropriate exception (`TypeError`). 3. The strings should not contain embedded null characters. 4. If the provided argument is `None` when a string is expected, it should be included as `None` in the output tuple. Example: ```python # Example 1 args = [\\"hello\\", 42] format = \\"si\\" result = custom_parse_args(args, format) # Expected output: (\\"hello\\", 42) # Example 2 args = [\\"world\\", None] format = \\"ss\\" result = custom_parse_args(args, format) # Expected output: (\\"world\\", None) # Example 3 args = [100, \\"test\\"] format = \\"is\\" result = custom_parse_args(args, format) # Expected output: (100, \\"test\\") # Example 4 args = [100, \\"test\\"] format = \\"ii\\" # This should raise a TypeError because the second argument is not an integer ``` Notes: - Your implementation should demonstrate error handling by appropriately raising exceptions when there is a type mismatch or any validation fails as per the constraints. - Be clear about how you would manage memory for the parsed values if they were to be used beyond simple parsing (i.e., consider how Python handles memory for strings and integers).","solution":"def custom_parse_args(args: list, format: str) -> tuple: if len(args) != len(format): raise ValueError(\\"The length of args must match the length of the format string.\\") parsed_values = [] for arg, fmt in zip(args, format): if fmt == \'s\': if arg is not None and not isinstance(arg, str): raise TypeError(f\\"Expected a string, but got {type(arg).__name__}.\\") parsed_values.append(arg) elif fmt == \'i\': if not isinstance(arg, int): raise TypeError(f\\"Expected an integer, but got {type(arg).__name__}.\\") parsed_values.append(arg) else: raise ValueError(f\\"Unsupported format type: {fmt}\\") return tuple(parsed_values)"},{"question":"Question # Objective Implement a Python function that interacts with an XML-RPC server to perform various operations. The server is expected to handle arithmetic operations (`add`, `subtract`, `multiply`, `divide`). You will need to perform these operations on given pairs of numbers and handle potential errors gracefully. # Function Signature ```python def perform_operations(server_url: str, operations: list[tuple[str, tuple[int, int]]]) -> dict: pass ``` # Inputs - `server_url` (str): The URL of the XML-RPC server. - `operations` (list of tuple): Each tuple contains an operation name (`\'add\'`, `\'subtract\'`, `\'multiply\'`, `\'divide\'`) and a tuple of two integers to which the operation should be applied. # Outputs - `results` (dict): A dictionary where the keys are the operation tuples and the values are the results of their corresponding operations. If an operation failed due to a `Fault` or `ProtocolError`, its value should be the error message. # Constraints - Use the `xmlrpc.client.ServerProxy` class to interact with the server. - You must handle `Fault` and `ProtocolError` exceptions to catch errors during communication or operations. - Assume the server is properly configured to handle the specified operations and is running on the specified URL. # Example ```python server_url = \\"http://localhost:8000/\\" operations = [(\\"add\\", (5, 3)), (\\"subtract\\", (10, 4)), (\\"multiply\\", (2, 3)), (\\"divide\\", (9, 3))] result = perform_operations(server_url, operations) print(result) ``` Expected Output: ```python { (\'add\', (5, 3)): 8, (\'subtract\', (10, 4)): 6, (\'multiply\', (2, 3)): 6, (\'divide\', (9, 3)): 3 } ``` # Additional Information Ensure that your solution handles: - Creating an instance of `ServerProxy`. - Making method calls dynamically based on the input operation name. - Catching and returning error messages in case of `Fault` or `ProtocolError` exceptions. - Output should be clear and should not include any extraneous information. # Notes - You may assume the server is always available at the provided URL and the operations are valid for the purpose of this task. - This task assesses your ability to read and use library documentation effectively, and to implement a function that uses this library to perform a complex sequence of operations.","solution":"import xmlrpc.client def perform_operations(server_url: str, operations: list) -> dict: results = {} proxy = xmlrpc.client.ServerProxy(server_url) for operation, args in operations: try: result = getattr(proxy, operation)(*args) results[(operation, args)] = result except xmlrpc.client.Fault as fault: results[(operation, args)] = f\\"Fault: {fault.faultString}\\" except xmlrpc.client.ProtocolError as err: results[(operation, args)] = f\\"Protocol Error: {err.errmsg}\\" except Exception as ex: results[(operation, args)] = f\\"Error: {str(ex)}\\" return results"},{"question":"**Objective**: To assess the students’ understanding of seaborn\'s context management and effective plotting using seaborn. # Question: Using the seaborn library, you are required to perform the following tasks: 1. Load the dataset `tips` from seaborn\'s inbuilt datasets. 2. Create a figure with two subplots (1 row, 2 columns) using matplotlib\'s `plt.subplots()`. 3. For the first subplot: - Set the context to `paper` with a `font_scale` of 1.2. - Create a boxplot showing the distribution of tips based on the day of the week. 4. For the second subplot: - Set the context to `talk`, and override the `axes.titlesize` parameter to `18` and `axes.labelsize` parameter to `14`. - Create a scatter plot showing the relationship between total bill and tip amount, colored by time of day (Lunch/Dinner). # Constraints: - Use only the seaborn and matplotlib.pyplot libraries. - Ensure that the plots include titles and axis labels for clarity. - Adjust the figure size appropriately for clear visibility of subplots. # Input: - The code does not take any input from the user. # Output: - Visualization containing a figure with two subplots as described. ```python import seaborn as sns import matplotlib.pyplot as plt def create_visualization(): # Load the dataset tips = sns.load_dataset(\'tips\') # Create a figure with two subplots fig, ax = plt.subplots(1, 2, figsize=(16, 6)) # First subplot: Box plot with \'paper\' context sns.set_context(\\"paper\\", font_scale=1.2) sns.boxplot(x=\\"day\\", y=\\"tip\\", data=tips, ax=ax[0]) ax[0].set_title(\'Tips Distribution by Day\') ax[0].set_xlabel(\'Day of the Week\') ax[0].set_ylabel(\'Tip Amount\') # Second subplot: Scatter plot with \'talk\' context and custom parameters sns.set_context(\\"talk\\", rc={\\"axes.titlesize\\":18, \\"axes.labelsize\\":14}) sns.scatterplot(x=\\"total_bill\\", y=\\"tip\\", hue=\\"time\\", data=tips, ax=ax[1]) ax[1].set_title(\'Total Bill vs Tip\') ax[1].set_xlabel(\'Total Bill\') ax[1].set_ylabel(\'Tip Amount\') # Show the plots plt.show() ``` # Note: Ensure to run the entire script to visualize the plots effectively.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_visualization(): # Load the dataset tips = sns.load_dataset(\'tips\') # Create a figure with two subplots fig, ax = plt.subplots(1, 2, figsize=(16, 6)) # First subplot: Box plot with \'paper\' context sns.set_context(\\"paper\\", font_scale=1.2) sns.boxplot(x=\\"day\\", y=\\"tip\\", data=tips, ax=ax[0]) ax[0].set_title(\'Tips Distribution by Day\') ax[0].set_xlabel(\'Day of the Week\') ax[0].set_ylabel(\'Tip Amount\') # Second subplot: Scatter plot with \'talk\' context and custom parameters sns.set_context(\\"talk\\", rc={\\"axes.titlesize\\": 18, \\"axes.labelsize\\": 14}) sns.scatterplot(x=\\"total_bill\\", y=\\"tip\\", hue=\\"time\\", data=tips, ax=ax[1]) ax[1].set_title(\'Total Bill vs Tip\') ax[1].set_xlabel(\'Total Bill\') ax[1].set_ylabel(\'Tip Amount\') # Show the plots plt.tight_layout() plt.show()"},{"question":"Objective To assess the understanding of creating, checking, and converting Python floating-point objects using the provided API. Problem Statement You are tasked with writing a program that utilizes the functions provided in the `PyFloatObject` documentation to perform the following operations: 1. **Create a Python float object from a string representation of a number.** 2. **Create a Python float object from a C double.** 3. **Check whether a given Python object is a float and whether it is an exact float.** 4. **Convert a Python float object back to a C double.** 5. **Retrieve and display the maximum and minimum representable float values using the provided functions.** # Function Implementations 1. `create_float_from_string(number_str: str) -> float`: - **Input:** A string `number_str` representing a number. - **Output:** A Python float object created from the provided string. 2. `create_float_from_double(number: float) -> float`: - **Input:** A floating-point number `number`. - **Output:** A Python float object created from the provided double. 3. `is_float(obj: object) -> bool`: - **Input:** An object `obj`. - **Output:** Returns `True` if `obj` is a Python float object or a subtype of `PyFloatObject`, `False` otherwise. 4. `is_exact_float(obj: object) -> bool`: - **Input:** An object `obj`. - **Output:** Returns `True` if `obj` is exactly a Python float object, `False` otherwise. 5. `convert_to_c_double(pyfloat: float) -> float`: - **Input:** A Python float `pyfloat`. - **Output:** The C double representation of the provided Python float. 6. `get_float_info() -> dict`: - **Output:** A dictionary containing the maximum and minimum representable float values with keys `\'max\'` and `\'min\'`. # Constraints - The string `number_str` will always be a valid representation of a floating-point number. - The object `obj` can be any Python object. - For simplicity, assume the functions provided in the `PyFloatObject` documentation are directly available to use in Python. # Example Usage ```python # Example usage of the functions print(create_float_from_string(\\"123.456\\")) # Should print 123.456 print(create_float_from_double(789.012)) # Should print 789.012 print(is_float(123.456)) # Should print True print(is_exact_float(\\"not a float\\")) # Should print False print(convert_to_c_double(456.789)) # Should print 456.789 print(get_float_info()) # Should print {\'max\': DBL_MAX, \'min\': DBL_MIN} ``` # Notes - The `DBL_MAX` and `DBL_MIN` are placeholders for the actual maximum and minimum representable float values according to the `PyFloatObject` documentation. - You should check for any errors or exceptions where applicable and handle them accordingly. - Ensure that your implementation is efficient and concise.","solution":"import sys def create_float_from_string(number_str: str) -> float: Returns a float object created from the given string representation of a number. return float(number_str) def create_float_from_double(number: float) -> float: Returns a float object created from the given double. return float(number) def is_float(obj: object) -> bool: Returns True if the given object is a Python float object or a subtype of PyFloatObject, False otherwise. return isinstance(obj, float) def is_exact_float(obj: object) -> bool: Returns True if the given object is exactly a Python float object, False otherwise. return type(obj) is float def convert_to_c_double(pyfloat: float) -> float: Returns the C double representation of the provided Python float. return float(pyfloat) def get_float_info() -> dict: Returns a dictionary containing the maximum and minimum representable float values. return { \'max\': sys.float_info.max, \'min\': sys.float_info.min }"},{"question":"# Advanced Python Closures and Cell Objects Python uses cell objects to manage variable scopes when dealing with closures. For this task, you will implement a simplified version of a cell object using Python code. # Problem Description 1. **Implement a Cell class**: - The class should be able to initialize with a given value, retrieve this value, and update the value. - Include the following methods: - `__init__(self, value)`: Initialize the cell with `value`. - `get(self)`: Retrieve the current value in the cell. - `set(self, value)`: Update the cell with a new `value`. 2. **Function that uses Cell objects with closures**: - Write a function `create_closure(initial_value)` that returns a nested function. - The nested function should access and modify a variable encapsulated within a `Cell` object. - The `create_closure` function should: - Initialize a `Cell` object with `initial_value`. - Define and return a nested function that takes a new value, sets this value to the encapsulated `Cell` object, and returns the updated value. # Input and Output - `Cell` class methods: - `__init__(self, value)`: This method initializes the cell with a given `value`. - `get(self)`: This method returns the current value stored in the cell. - `set(self, value)`: This method updates the cell\'s value to the new `value`. - `create_closure(initial_value)` function: - Takes an integer input `initial_value`. - Returns a nested function which: - Accepts an integer `new_value`. - Updates the value in the `Cell` object with the new value. - Returns the updated value. # Constraints 1. The `initial_value` and `new_value` will be integers within the range `-10^6 <= value <= 10^6`. 2. Your implementation should handle edge cases such as setting `None` to the cell. # Example ```python # Define the Cell class class Cell: def __init__(self, value): self.value = value def get(self): return self.value def set(self, value): self.value = value # Define the create_closure function def create_closure(initial_value): cell = Cell(initial_value) def nested_function(new_value): cell.set(new_value) return cell.get() return nested_function # Example Usage closure = create_closure(5) print(closure(10)) # Output: 10 print(closure(20)) # Output: 20 ``` Your task is to implement the `Cell` class and the `create_closure` function as described. Ensure that your solution maintains the encapsulation and correct manipulation of the variable within the `Cell` object.","solution":"class Cell: def __init__(self, value): self.value = value def get(self): return self.value def set(self, value): self.value = value def create_closure(initial_value): cell = Cell(initial_value) def nested_function(new_value): cell.set(new_value) return cell.get() return nested_function"},{"question":"**Title: Working with Custom Floating Point Operations** **Objective:** Create a Python class that simulates some of the functionality described in the `PyFloatObject` documentation. This class should handle floating point numbers and provide methods to perform various operations and checks. **Task:** Implement a class `CustomFloat` that meets the following requirements: 1. **Initialization**: - The class should be able to initialize an object from a floating-point number or a string representation of a floating-point number. If the input is invalid, raise a `ValueError`. 2. **Methods**: - `__init__(self, value)`: Constructor that initializes the `CustomFloat` object. - `to_double(self)`: Converts the `CustomFloat` object to a Python float. - `is_subtype(self)`: Always returns `False` since we are not dealing with subtypes in this exercise. - `get_max(self)`: Class method that returns the maximum representable finite float. - `get_min(self)`: Class method that returns the minimum normalized positive float. 3. **Error Handling**: - Properly handle invalid inputs during initialization (e.g. non-numeric strings). 4. **Edge Cases**: - Consider edge cases such as very large or very small floating-point numbers. **Example Usage:** ```python # Creating CustomFloat instances cf1 = CustomFloat(3.14) cf2 = CustomFloat(\\"2.718\\") # Converting to float print(cf1.to_double()) # Output: 3.14 # Checking subtype print(cf1.is_subtype()) # Output: False # Getting maximum and minimum float values print(CustomFloat.get_max()) # Output: Some large float (DBL_MAX) print(CustomFloat.get_min()) # Output: Some small positive float (DBL_MIN) ``` **Input and Output Format:** - `__init__(self, value)`: Initializes with either a float or a string. - `to_double(self)`: Returns a floating-point number. - `is_subtype(self)`: Returns a boolean. - `get_max(self)`: Returns a float. - `get_min(self)`: Returns a float. **Constraints:** - The string representation for initialization must be a valid floating-point number. - The float conversion must handle cases where precision might matter. - Your solution should be efficient and handle edge cases gracefully. **Notes:** - You are not required to implement the internal C methods or handle memory management as described in the documentation.","solution":"import sys class CustomFloat: def __init__(self, value): try: if isinstance(value, str): self.value = float(value) elif isinstance(value, (float, int)): self.value = float(value) else: raise ValueError(\\"Invalid type. Must be a float, int, or string representing a float.\\") except (ValueError, TypeError): raise ValueError(\\"Invalid input. Must be a numeric value or string representing a numeric value.\\") def to_double(self): return self.value def is_subtype(self): return False @classmethod def get_max(cls): return sys.float_info.max @classmethod def get_min(cls): return sys.float_info.min"},{"question":"You are given a dataset `penguins` that contains information on different penguin species. Your task is to analyze and visualize the dataset using the `seaborn.objects` module. Specifically, you need to generate visualizations that highlight the distribution and statistical properties of penguin body masses. # Problem Statement 1. **Load the Dataset**: Load the `penguins` dataset from seaborn. Ensure that any entries with missing values are removed. 2. **Generate Visualizations**: a. Create a bar plot showing the average body mass (`body_mass_g`) of different penguin species (`species`). Use different colors for males and females (`sex`). Ensure the bars for males and females are separated using a `Dodge` transformation. b. Enhance the bar plot by adding error bars that represent the standard deviation of the body mass for each species and sex combination. # Input Format You don\'t need to handle any inputs from the user; load the dataset directly within the code. # Output Format Your code should output two plots: 1. A bar plot showing the average body mass for each species, with bars separated by sex. 2. The same bar plot, but with error bars representing the standard deviation. # Constraints - Ensure that the visualization is clear and properly labeled. - Handle any potential issues with missing data before generating visualizations. # Example ```python # Example code outline import seaborn.objects as so from seaborn import load_dataset # Load and clean the dataset penguins = load_dataset(\\"penguins\\").dropna() # Create and show bar plot plot_bar = ( so.Plot(penguins, x=\\"species\\", y=\\"body_mass_g\\", color=\\"sex\\") .add(so.Bar(), so.Agg(), so.Dodge()) ) plot_bar.show() # Create and show bar plot with error bars plot_bar_error = ( so.Plot(penguins, x=\\"species\\", y=\\"body_mass_g\\", color=\\"sex\\") .add(so.Bar(alpha=.5), so.Agg(), so.Dodge()) .add(so.Range(), so.Est(errorbar=\\"sd\\"), so.Dodge()) ) plot_bar_error.show() ``` Ensure you follow the example outline to implement the full solution, including necessary imports, data loading, and visualizations.","solution":"import seaborn as sns import seaborn.objects as so import matplotlib.pyplot as plt # Load and clean the dataset penguins = sns.load_dataset(\\"penguins\\").dropna() # Create and show bar plot plot_bar = ( so.Plot(penguins, x=\\"species\\", y=\\"body_mass_g\\", color=\\"sex\\") .add(so.Bar(), so.Agg(), so.Dodge()) ) plot_bar.show() # Create and show bar plot with error bars plot_bar_error = ( so.Plot(penguins, x=\\"species\\", y=\\"body_mass_g\\", color=\\"sex\\") .add(so.Bar(alpha=.5), so.Agg(), so.Dodge()) .add(so.Range(), so.Est(errorbar=\\"sd\\"), so.Dodge()) ) plot_bar_error.show()"},{"question":"Tensor Data Exchange with DLPack Objective: Demonstrate your understanding of tensor data interoperability between PyTorch and other frameworks using DLPack. You will be required to implement a function that performs conversions and basic operations on tensors to verify their compatibility. Problem Statement: You need to write a Python function `tensor_exchange_and_operate` that takes two arguments: 1. A PyTorch tensor. 2. An operation function that takes a PyTorch tensor and returns another PyTorch tensor. The function should: 1. Convert the input PyTorch tensor to a DLPack tensor. 2. Convert the DLPack tensor back to a PyTorch tensor. 3. Apply the provided operation function to the resulting PyTorch tensor. 4. Return the final PyTorch tensor. Input: - `input_tensor` (torch.Tensor): A PyTorch tensor. - `operation_fn` (Callable[[torch.Tensor], torch.Tensor]): A function that takes a PyTorch tensor as input and returns a PyTorch tensor. Output: - (torch.Tensor): The PyTorch tensor after performing the specified conversions and operations. Constraints: - You may assume that the input tensor and the output of the operation function are always valid PyTorch tensors. - The function should handle tensors of different shapes and data types. Performance Requirements: - The implementation should strive to keep the conversion and computation efficient, avoiding unnecessary data copies. Example Usage: ```python import torch from torch.utils.dlpack import from_dlpack, to_dlpack def tensor_exchange_and_operate(input_tensor: torch.Tensor, operation_fn: Callable[[torch.Tensor], torch.Tensor]) -> torch.Tensor: # Convert PyTorch tensor to DLPack tensor dlpack_tensor = to_dlpack(input_tensor) # Convert back from DLPack tensor to PyTorch tensor py_tensor = from_dlpack(dlpack_tensor) # Apply the operation function result_tensor = operation_fn(py_tensor) return result_tensor # Example usage: def add_one(tensor): return tensor + 1 input_tensor = torch.tensor([1, 2, 3]) result = tensor_exchange_and_operate(input_tensor, add_one) print(result) # Output: tensor([2, 3, 4]) ``` Implement the `tensor_exchange_and_operate` function to complete this task.","solution":"import torch from torch.utils.dlpack import from_dlpack, to_dlpack from typing import Callable def tensor_exchange_and_operate(input_tensor: torch.Tensor, operation_fn: Callable[[torch.Tensor], torch.Tensor]) -> torch.Tensor: Convert tensor between PyTorch and DLPack formats, and apply an operation function. Parameters: input_tensor (torch.Tensor): A PyTorch tensor. operation_fn (Callable[[torch.Tensor], torch.Tensor]): A function that takes a PyTorch tensor and returns another PyTorch tensor. Returns: torch.Tensor: The resulting tensor after the operation. # Convert PyTorch tensor to DLPack tensor dlpack_tensor = to_dlpack(input_tensor) # Convert back from DLPack tensor to PyTorch tensor py_tensor = from_dlpack(dlpack_tensor) # Apply the operation function result_tensor = operation_fn(py_tensor) return result_tensor"},{"question":"# Advanced Python Context Management Context You are tasked with managing multiple resource operations with different life cycles that need to be cleaned up properly. Specifically, you have operations that require opening multiple files, acquiring and releasing a network resource, and conditionally executing additional cleanup tasks based on certain conditions. Using the `contextlib` module, you must create a function that: 1. Opens a list of files and ensures they are all properly closed. 2. Acquires a network resource and releases it when done. 3. Executes additional cleanup tasks that might be conditionally necessary based on the results of file operations or network communication. Requirements - Implement function `manage_resources(filenames: List[str]) -> None`. - `filenames`: a list of filenames (strings) to be opened for reading. - Ensure all opened files are closed properly. - Acquire a dummy network resource (`network_resource`) and release it at the end. - Log entry and exit events for the entire operation using a context manager. - Use `ExitStack` for handling multiple context managers and cleanup tasks. - If any file does not exist, handle the exception gracefully without stopping the process. - If a file contains the line \\"ERROR\\", add a cleanup task to log the error and perform additional cleanup. Example ```python import contextlib class network_resource: def __enter__(self): print(\\"Opening network resource\\") return self def __exit__(self, exc_type, exc_val, exc_tb): print(\\"Releasing network resource\\") def manage_resources(filenames): with contextlib.ExitStack() as stack: # Task implementation goes here pass ``` Constraints - Do not change the signature of the `manage_resources` function. - Use the `contextlib` module utilities where appropriate. - The function should handle exceptions gracefully and ensure all resources are properly released. - Use print statements for logging the entry and exit, and error handling as indicated. Notes - The provided `network_resource` class should be used as-is in your function and is indicative of typical network resource management. - Test your implementation with dummy filenames and error cases to ensure robust handling and cleanup.","solution":"import contextlib class network_resource: def __enter__(self): print(\\"Opening network resource\\") return self def __exit__(self, exc_type, exc_val, exc_tb): print(\\"Releasing network resource\\") def log_event(message): print(message) def manage_resources(filenames): with contextlib.ExitStack() as stack: # Log entry event stack.callback(log_event, \\"Entry event\\") # Handle network resource resource = stack.enter_context(network_resource()) # Handle file operations files = [] for filename in filenames: try: file = stack.enter_context(open(filename, \'r\')) files.append(file) for line in file: if \\"ERROR\\" in line: stack.callback(log_event, f\\"Error found in file: {filename}\\") except FileNotFoundError: print(f\\"File not found: {filename}\\") # Log exit event stack.callback(log_event, \\"Exit event\\")"},{"question":"# Python 310 Coding Assessment Question Objective: To assess your understanding of asynchronous programming in Python using coroutines. Problem Statement: You are tasked with implementing a Python module that utilizes coroutines to process data concurrently. In this exercise, you will create an asynchronous function to fetch data from multiple sources, simulate processing delays, and aggregate results. Requirements: 1. Define an asynchronous function `fetch_data(source: str) -> str`. This function should: - Simulate a delay in data retrieval using `await asyncio.sleep(delay)`, where `delay` depends on the source. - Return a string indicating the data retrieved from the source, formatted as `\\"Data from {source}\\"`. 2. Define another asynchronous function `aggregate_data(sources: List[str]) -> List[str]`. This function should: - Use the `fetch_data` function to concurrently fetch data from all sources provided. - Gather all results and return them as a list of strings. 3. Ensure the implementation handles potential concurrency issues and demonstrates the use of coroutine objects effectively. Input: - The `fetch_data` function takes a single argument `source`, which is a string indicating the data source. - The `aggregate_data` function takes a list of strings `sources`, each representing a data source. Output: - The `fetch_data` function returns a string indicating the fetched data. - The `aggregate_data` function returns a list of strings with all fetched data from the sources. Constraints: - Use `asyncio` and `async`/`await` syntax effectively. - Simulate varying delays for different sources to illustrate asynchronous behavior. Example: ```python import asyncio from typing import List async def fetch_data(source: str) -> str: # Define variable delay based on source delay = len(source) * 0.1 # Example delay: 0.1 seconds per character in source await asyncio.sleep(delay) return f\\"Data from {source}\\" async def aggregate_data(sources: List[str]) -> List[str]: tasks = [fetch_data(source) for source in sources] results = await asyncio.gather(*tasks) return results # Test Example if __name__ == \\"__main__\\": sources = [\\"source1\\", \\"source2\\", \\"source3\\"] results = asyncio.run(aggregate_data(sources)) print(results) ``` Upon running the example code, the output should be a list containing the retrieved data from each source, demonstrating the concurrent execution of `fetch_data` tasks. Note: - Ensure the solution is efficient and leverages Python\'s asynchronous capabilities. - Include appropriate error handling and edge case management.","solution":"import asyncio from typing import List async def fetch_data(source: str) -> str: delay = len(source) * 0.1 # Example delay: 0.1 seconds per character in source await asyncio.sleep(delay) return f\\"Data from {source}\\" async def aggregate_data(sources: List[str]) -> List[str]: tasks = [fetch_data(source) for source in sources] results = await asyncio.gather(*tasks) return results"},{"question":"# Iterator Implementation and Usage You are required to implement two functions that demonstrate the usage and implementation of sequence and callable iterators as described in the provided documentation. These iterators should mimic the behavior of `PySeqIter_Type` and `PyCallIter_Type`. 1. Sequence Iterator Implementation Implement a sequence iterator class `SequenceIterator` that: - Initializes with any sequence supporting the `__getitem__()` method. - Iterates over the sequence until it raises an `IndexError`. 2. Callable Iterator Implementation Implement a callable iterator class `CallableIterator` that: - Initializes with a callable object `func` and a sentinel value `sentinel`. - Calls the callable object at each iteration. Iteration terminates when the callable object returns a value equal to the sentinel. # Input and Output For SequenceIterator: # Input: - A sequence (e.g., list, tuple) # Output: - An instance of `SequenceIterator` which can be iterated to yield elements of the sequence. For CallableIterator: # Input: - A callable function that takes no parameters - A sentinel value # Output: - An instance of `CallableIterator` which can be iterated to yield results from the callable until the sentinel value is returned. # Example 1. SequenceIterator: ```python seq = [1, 2, 3, 4, 5] seq_iter = SequenceIterator(seq) for element in seq_iter: print(element) # Output: 1, 2, 3, 4, 5 ``` 2. CallableIterator: ```python def counter(): i = 0 while True: yield i i += 1 call_iter = CallableIterator(counter().send, 5) for element in call_iter: print(element) # Output: 0, 1, 2, 3, 4 ``` # Constraints - The sequence passed to `SequenceIterator` should support `__getitem__()`. - The callable object in `CallableIterator` should be callable without arguments. - Do not use Python’s built-in `iter` function for this implementation. # Bonus Implement `__iter__` and `__next__` methods to make your iterator classes compatible with Python’s iterator protocol.","solution":"class SequenceIterator: def __init__(self, sequence): self.sequence = sequence self.index = 0 def __iter__(self): return self def __next__(self): if self.index < len(self.sequence): result = self.sequence[self.index] self.index += 1 return result else: raise StopIteration class CallableIterator: def __init__(self, func, sentinel): self.func = func self.sentinel = sentinel def __iter__(self): return self def __next__(self): result = self.func() if result == self.sentinel: raise StopIteration return result"},{"question":"**Question: Exploring Penguin Dataset with Seaborn** As a data scientist, you have been given the task to analyze the penguins dataset using Seaborn\'s `objects` interface. Your goal is to create visualizations to uncover insights about the different islands and attributes of the penguins. Specifically, you need to generate a series of plots demonstrating your understanding and proficiency with Seaborn. # Task 1. **Load the Dataset:** - Load the penguins dataset using `seaborn.load_dataset(\\"penguins\\")`. 2. **Histogram of Flipper Lengths:** - Create a histogram of the `flipper_length_mm` variable. - Use 20 bins for this histogram. - Normalize the histogram to show proportions. 3. **Facet Histogram by Island:** - Create a faceted histogram of `flipper_length_mm` for each `island`. - Normalize each histogram independently. 4. **Colored Area Plot for Sex:** - Create an area plot of `flipper_length_mm` colored by `sex`. 5. **Stacked Bar Plot for Island and Sex:** - Create a stacked bar plot showing the count of penguins on each `island`, stacked by `sex`. # Expected Output The output of your code should be: - A normalized histogram for flipper lengths with 20 bins. - A faceted histogram showing flipper lengths by island, each normalized independently. - An area plot of flipper lengths colored by sex. - A stacked bar plot of the count of penguins on each island, stacked by sex. # Constraints - You should use the Seaborn `objects` interface (`so.Plot` and related classes) for all visualizations. - You should include comments in your code to explain the steps and rationale. - Ensure the visualizations are clear and well-labeled for interpretation. # Example ```python import seaborn.objects as so from seaborn import load_dataset # 1. Load the dataset penguins = load_dataset(\\"penguins\\") # 2. Histogram of Flipper Lengths p = so.Plot(penguins, \\"flipper_length_mm\\") p.add(so.Bars(), so.Hist(bins=20, stat=\\"proportion\\")).show() # 3. Facet Histogram by Island p_facet = so.Plot(penguins, \\"flipper_length_mm\\").facet(\\"island\\") p_facet.add(so.Bars(), so.Hist(stat=\\"proportion\\", common_norm=False)).show() # 4. Colored Area Plot for Sex p_area = so.Plot(penguins, \\"flipper_length_mm\\", color=\\"sex\\") p_area.add(so.Area(), so.Hist()).show() # 5. Stacked Bar Plot for Island and Sex p_stack = so.Plot(penguins, \\"island\\", color=\\"sex\\") p_stack.add(so.Bars(), so.Hist(), so.Stack()).show() ``` # Submission Submit your code in a single Python file or a Jupyter Notebook, and ensure all the plots are generated as required.","solution":"import seaborn.objects as so from seaborn import load_dataset # 1. Load the dataset penguins = load_dataset(\\"penguins\\") # 2. Histogram of Flipper Lengths p = so.Plot(penguins, \\"flipper_length_mm\\") p.add(so.Bars(), so.Hist(bins=20, stat=\\"proportion\\")).show() # 3. Facet Histogram by Island p_facet = so.Plot(penguins, \\"flipper_length_mm\\").facet(\\"island\\") p_facet.add(so.Bars(), so.Hist(stat=\\"proportion\\", common_norm=False)).show() # 4. Colored Area Plot for Sex p_area = so.Plot(penguins, \\"flipper_length_mm\\", color=\\"sex\\") p_area.add(so.Area(), so.Hist()).show() # 5. Stacked Bar Plot for Island and Sex p_stack = so.Plot(penguins, \\"island\\", color=\\"sex\\") p_stack.add(so.Bars(), so.Hist(), so.Stack()).show()"},{"question":"Problem Statement You are required to use the seaborn `objects` interface to visualize health expenditure data with various normalization parameters. # Dataset: Use the healthexp dataset provided by seaborn, which can be loaded using: ```python import seaborn as sns healthexp = sns.load_dataset(\\"healthexp\\") ``` # Task 1: Create a seaborn plot to visualize the `Spending_USD` over the `Year` for each `Country`. Normalize the y-axis such that each group\'s values (per country) are scaled relative to their maximum value. # Task 2: Create another seaborn plot using the same dataset. This time, normalize the `Spending_USD` values to show the percentage change from the year where spending was minimum for each country. # Requirements: 1. **Input Format:** - No input will be directly provided to your function. Use the `healthexp` dataset from seaborn. 2. **Output Format:** - Your function should display two plots: 1. The first plot with spending values normalized relative to their maximum per country. 2. The second plot with spending values as a percent change from the year with minimum spending for each country. 3. **Performance:** - Efficiently handle the plotting, ensuring that the plots accurately reflect the requested transformations. # Detailed Function Specifications: Implement a function `visualize_health_expenditure()` which: - Loads the healthexp dataset. - Creates and displays the two specified seaborn plots. ```python import seaborn.objects as so import seaborn as sns def visualize_health_expenditure(): healthexp = sns.load_dataset(\\"healthexp\\") # Task 1: Normalize relative to the maximum value per country ( so.Plot(healthexp, x=\\"Year\\", y=\\"Spending_USD\\", color=\\"Country\\") .add(so.Lines(), so.Norm()) .label(y=\\"Spending relative to maximum amount\\") .show() ) # Task 2: Normalize as percent change from the minimum spending year per country ( so.Plot(healthexp, x=\\"Year\\", y=\\"Spending_USD\\", color=\\"Country\\") .add(so.Lines(), so.Norm(where=\\"x == x.min()\\", percent=True)) .label(y=\\"Percent change in spending from minimum year baseline\\") .show() ) # Test the function visualize_health_expenditure() ``` Ensure your implementation adheres to the described input and output formats and correctly handles the normalization as specified.","solution":"import seaborn as sns import seaborn.objects as so import matplotlib.pyplot as plt def visualize_health_expenditure(): # Load the healthexp dataset healthexp = sns.load_dataset(\\"healthexp\\") # Task 1: Normalize relative to the maximum value per country healthexp_norm_max = healthexp.copy() healthexp_norm_max[\'Spending_USD\'] = healthexp_norm_max.groupby(\'Country\')[\'Spending_USD\'].transform(lambda x: x / x.max()) plt.figure(figsize=(10, 6)) norm_max_plot = ( so.Plot(healthexp_norm_max, x=\\"Year\\", y=\\"Spending_USD\\", color=\\"Country\\") .add(so.Line()) .label(y=\\"Spending relative to maximum amount\\") ) norm_max_plot.show() # Task 2: Normalize as percent change from the minimum spending year per country healthexp_pct_change = healthexp.copy() healthexp_pct_change[\'Spending_USD\'] = healthexp_pct_change.groupby(\'Country\')[\'Spending_USD\'].transform(lambda x: (x / x.min() - 1) * 100) plt.figure(figsize=(10, 6)) pct_change_plot = ( so.Plot(healthexp_pct_change, x=\\"Year\\", y=\\"Spending_USD\\", color=\\"Country\\") .add(so.Line()) .label(y=\\"Percent change in spending from minimum year baseline\\") ) pct_change_plot.show() # Test the function visualize_health_expenditure()"},{"question":"**Question: Using PyTorch `torch.package` to Package and Load a Model with Dependencies** **Background**: You are a machine learning engineer working with PyTorch. You need to package a PyTorch model along with its dependencies and additional resources, and ensure the package can be loaded and utilized efficiently on different systems. You will use the `torch.package` module for this purpose. **Task**: 1. **Package the Model**: - Create a neural network model using PyTorch. - Save this model along with an additional configuration file within a package using `torch.package.PackageExporter`. - Ensure that all necessary dependencies are included in the package. 2. **Load and Utilize the Package**: - Load the model and the configuration file from the package using `torch.package.PackageImporter`. - Verify the contents of the package programmatically. **Specifications**: 1. **Model Definition**: - Define a simple convolutional neural network (CNN) using `torch.nn.Module`. - Save the model\'s state dictionary. 2. **Packaging**: - Use `torch.package.PackageExporter` to create a package (`model_package.pt`). - Save the model\'s state dictionary and a configuration file (`config.txt`) containing arbitrary text. 3. **Dependency Management**: - Ensure all necessary modules (e.g., `torch`, and `torchvision`) are included in the package. 4. **Loading and Verification**: - Use `torch.package.PackageImporter` to load the model and configuration file. - Verify and print the structure of the package contents programmatically. 5. **Custom Function**: - Implement a custom function `package_and_load_model()` that performs the entire task from packaging to loading and verification. **Expected Function Signature**: ```python import torch import torch.nn as nn def package_and_load_model() -> None: # Define your model here class SimpleCNN(nn.Module): def __init__(self): super(SimpleCNN, self).__init__() self.conv1 = nn.Conv2d(1, 32, 3, 1) self.fc1 = nn.Linear(32 * 13 * 13, 10) def forward(self, x): x = self.conv1(x) x = torch.relu(x) x = torch.flatten(x, 1) x = self.fc1(x) return x # Initialize the model model = SimpleCNN() # Save model state_dict state_dict = model.state_dict() # Use PackageExporter to save model and config file with torch.package.PackageExporter(\'model_package.pt\') as exporter: exporter.intern(\\"**\\") exporter.save_pickle(\\"model\\", \\"model_state.pkl\\", state_dict) exporter.save_text(\\"resources\\", \\"config.txt\\", \\"Configuration: Batch Size = 64, Epochs = 10\\") # Use PackageImporter to load model and config file importer = torch.package.PackageImporter(\'model_package.pt\') # Load model state_dict and configuration text loaded_state_dict = importer.load_pickle(\'model\', \'model_state.pkl\') config_text = importer.load_text(\\"resources\\", \\"config.txt\\") # Verify and print package contents file_structure = importer.file_structure() print(file_structure) print(config_text) # Load state_dict into model and verify it works loaded_model = SimpleCNN() loaded_model.load_state_dict(loaded_state_dict) print(\\"Model loaded successfully and ready for inference.\\") ``` **Constraints**: - The solution should be written in Python using PyTorch. - Ensure the correct management of both internal and external dependencies within the package. - The model and configuration file should be loaded correctly, and the package contents should be verified programmatically.","solution":"import torch import torch.nn as nn import os def package_and_load_model() -> None: # Define the model class SimpleCNN(nn.Module): def __init__(self): super(SimpleCNN, self).__init__() self.conv1 = nn.Conv2d(1, 32, 3, 1) self.fc1 = nn.Linear(32 * 13 * 13, 10) def forward(self, x): x = self.conv1(x) x = torch.relu(x) x = torch.flatten(x, 1) x = self.fc1(x) return x # Initialize the model and get its state_dict model = SimpleCNN() state_dict = model.state_dict() # Save model\'s state_dict and configuration file using PackageExporter with torch.package.PackageExporter(\'model_package.pt\') as exporter: exporter.intern(\\"**\\") exporter.save_pickle(\\"model\\", \\"model_state.pkl\\", state_dict) exporter.save_text(\\"resources\\", \\"config.txt\\", \\"Configuration: Batch Size = 64, Epochs = 10\\") # Verifying the files are created assert os.path.exists(\\"model_package.pt\\"), \\"Model package file not created!\\" # Load the package and verify the contents using PackageImporter importer = torch.package.PackageImporter(\'model_package.pt\') loaded_state_dict = importer.load_pickle(\'model\', \'model_state.pkl\') config_text = importer.load_text(\\"resources\\", \\"config.txt\\") file_structure = importer.file_structure() # Print the file structure and configuration text print(f\\"Package Structure: {file_structure}\\") print(f\\"Config Text: {config_text}\\") # Load state_dict into a new instance of the model and verify loaded_model = SimpleCNN() loaded_model.load_state_dict(loaded_state_dict) print(\\"Model loaded successfully and ready for inference.\\")"},{"question":"**Question**: Implement a Multi-Class Classification on the Iris Dataset using SVM **Problem Statement**: Given the famous Iris dataset, your task is to implement a multi-class classification using the `SVC` class from scikit-learn\'s `svm` module. You need to use different kernels and evaluate the performance of each. **Requirements**: 1. Load the Iris dataset using `sklearn.datasets.load_iris`. 2. Split the dataset into training and testing sets (80% training, 20% testing). 3. Implement and train SVM classifiers using the following kernels: - Linear - Polynomial (degree=3) - RBF (default gamma) 4. Evaluate each classifier\'s performance using accuracy, precision, recall, and F1 score. **Constraints**: - Use `train_test_split` from `sklearn.model_selection` to split the data. - Use `accuracy_score`, `precision_score`, `recall_score`, and `f1_score` from `sklearn.metrics` for evaluation. - Ensure reproducibility by setting random_state to 42 wherever applicable. - Your implementation should be modular and clear, with functions defined for loading data, training models, and evaluating performance. **Input**: The Iris dataset, no direct input from the user is needed. **Output**: Print the evaluation metrics for each kernel. **Code Template**: ```python from sklearn import datasets from sklearn.model_selection import train_test_split from sklearn.svm import SVC from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score def load_data(): # Load the Iris dataset iris = datasets.load_iris() X = iris.data y = iris.target return train_test_split(X, y, test_size=0.2, random_state=42) def train_svm(X_train, y_train, kernel): # Create and train an SVM classifier clf = SVC(kernel=kernel, random_state=42) clf.fit(X_train, y_train) return clf def evaluate_model(clf, X_test, y_test): # Predict and evaluate the model y_pred = clf.predict(X_test) accuracy = accuracy_score(y_test, y_pred) precision = precision_score(y_test, y_pred, average=\'weighted\') recall = recall_score(y_test, y_pred, average=\'weighted\') f1 = f1_score(y_test, y_pred, average=\'weighted\') return accuracy, precision, recall, f1 # Main code X_train, X_test, y_train, y_test = load_data() kernels = [\'linear\', \'poly\', \'rbf\'] for kernel in kernels: clf = train_svm(X_train, y_train, kernel) accuracy, precision, recall, f1 = evaluate_model(clf, X_test, y_test) print(f\'Kernel: {kernel}\') print(f\'Accuracy: {accuracy:.4f}\') print(f\'Precision: {precision:.4f}\') print(f\'Recall: {recall:.4f}\') print(f\'F1 Score: {f1:.4f}\') print(\'-\' * 50) ``` Implement the above functions, run the main code, and report the performance metrics for each kernel used.","solution":"from sklearn import datasets from sklearn.model_selection import train_test_split from sklearn.svm import SVC from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score def load_data(): # Load the Iris dataset iris = datasets.load_iris() X = iris.data y = iris.target return train_test_split(X, y, test_size=0.2, random_state=42) def train_svm(X_train, y_train, kernel): # Create and train an SVM classifier clf = SVC(kernel=kernel, random_state=42) clf.fit(X_train, y_train) return clf def evaluate_model(clf, X_test, y_test): # Predict and evaluate the model y_pred = clf.predict(X_test) accuracy = accuracy_score(y_test, y_pred) precision = precision_score(y_test, y_pred, average=\'weighted\') recall = recall_score(y_test, y_pred, average=\'weighted\') f1 = f1_score(y_test, y_pred, average=\'weighted\') return accuracy, precision, recall, f1 # Main code X_train, X_test, y_train, y_test = load_data() kernels = [\'linear\', \'poly\', \'rbf\'] results = {} for kernel in kernels: clf = train_svm(X_train, y_train, kernel) accuracy, precision, recall, f1 = evaluate_model(clf, X_test, y_test) results[kernel] = { \'accuracy\': accuracy, \'precision\': precision, \'recall\': recall, \'f1\': f1 } print(f\'Kernel: {kernel}\') print(f\'Accuracy: {accuracy:.4f}\') print(f\'Precision: {precision:.4f}\') print(f\'Recall: {recall:.4f}\') print(f\'F1 Score: {f1:.4f}\') print(\'-\' * 50)"},{"question":"Your task is to create a `TaskLogger` class that uses the `atexit` module to log tasks into a file upon the interpreter\'s termination. Class Specification: **`TaskLogger` class**: 1. **Initialization**: - The class should initialize an empty list to keep track of tasks. - It should register a function to log tasks to a file when the program terminates. 2. **Methods**: - **`add_task(self, task_name, status=\\"pending\\")`**: Adds a task to the list with its status. - **`mark_completed(self, task_name)`**: Marks a specific task as completed. - **`log_tasks_to_file(self)`**: Writes all tasks to a file named \\"tasks.log\\", where each task is written in a new line in the format: `\\"Task: {task_name}, Status: {status}\\"`. Constraints: - **Task names** are strings of 1 to 100 characters. - Task status can only be `\\"pending\\"` or `\\"completed\\"`. Example Usage: ```python # Import the required module import atexit class TaskLogger: def __init__(self): self.tasks = [] atexit.register(self.log_tasks_to_file) def add_task(self, task_name, status=\\"pending\\"): if len(task_name) == 0 or len(task_name) > 100: raise ValueError(\\"Task name must be between 1 and 100 characters.\\") if status not in (\\"pending\\", \\"completed\\"): raise ValueError(\\"Status must be either \'pending\' or \'completed\'.\\") self.tasks.append({\'task\': task_name, \'status\': status}) def mark_completed(self, task_name): for task in self.tasks: if task[\'task\'] == task_name: task[\'status\'] = \'completed\' return raise ValueError(\\"Task not found.\\") def log_tasks_to_file(self): with open(\'tasks.log\', \'w\') as file: for task in self.tasks: file.write(f\\"Task: {task[\'task\']}, Status: {task[\'status\']}n\\") # Example Execution logger = TaskLogger() logger.add_task(\\"Write code\\", \\"pending\\") logger.add_task(\\"Review PR\\", \\"completed\\") logger.mark_completed(\\"Write code\\") ``` When the interpreter shuts down, it should write the tasks to `tasks.log` in the specified format. # Important Notes: - Ensure your solution correctly utilizes the `atexit` module to register the log writing function. - Write clean and efficient code, handling possible edge cases and errors appropriately.","solution":"import atexit class TaskLogger: def __init__(self): self.tasks = [] atexit.register(self.log_tasks_to_file) def add_task(self, task_name, status=\\"pending\\"): if not (1 <= len(task_name) <= 100): raise ValueError(\\"Task name must be between 1 and 100 characters.\\") if status not in (\\"pending\\", \\"completed\\"): raise ValueError(\\"Status must be either \'pending\' or \'completed\'.\\") self.tasks.append({\'task\': task_name, \'status\': status}) def mark_completed(self, task_name): for task in self.tasks: if task[\'task\'] == task_name: task[\'status\'] = \'completed\' return raise ValueError(\\"Task not found.\\") def log_tasks_to_file(self): with open(\'tasks.log\', \'w\') as file: for task in self.tasks: file.write(f\\"Task: {task[\'task\']}, Status: {task[\'status\']}n\\")"},{"question":"**Coding Assessment Question:** # Objective: Demonstrate your understanding of Python\'s dictionary C-API by writing a function that performs a sequence of operations on a dictionary object. # Question: Implement a C function `manipulate_dict(PyObject *dict_obj)` that performs the following sequence of operations on the given dictionary object `dict_obj`: 1. Create a new dictionary object. 2. Insert the key-value pair `(\\"apple\\", 2)` into the new dictionary. 3. Insert the key `\\"banana\\"` with value `5` into the dictionary using `PyDict_SetItemString()`. 4. Retrieve the value associated with the key `(\\"apple\\")` and add `3` to it. Then, update the value in the dictionary. 5. Check if the key `\\"orange\\"` is present in the dictionary. If not, set the default value to `10`. 6. Delete the key `\\"banana\\"` from the dictionary. 7. Iterate over the new dictionary and print all key-value pairs. 8. Merge the new dictionary with `dict_obj` with the condition to override existing keys in `dict_obj`. **Function Signature:** ```c #include <Python.h> void manipulate_dict(PyObject *dict_obj); ``` # Input: - `dict_obj`: A Python dictionary object. # Output: - The function does not return a value but it should print the key-value pairs after performing the operations. # Constraints: - Assume that the provided `dict_obj` is always a valid dictionary object. - Handle any potential exceptions that may arise during the operations. # Example: **Python Code:** ```python import ctypes # Assume the compiled C function is available as a shared library `libdictops.so` # Load the shared library lib = ctypes.CDLL(\'./libdictops.so\') # The wrapped function manipulate_dict = lib.manipulate_dict manipulate_dict.argtypes = [ctypes.py_object] # Existing dictionary existing_dict = {\\"orange\\": 7, \\"grape\\": 12} # Apply the function manipulate_dict(existing_dict) # Expected printed output # apple: 5 # orange: 7 # grape: 12 ``` # Notes: - Ensure you handle reference counting properly to avoid memory leaks. - Use appropriate C-APIs for operations on dictionary objects.","solution":"def manipulate_dict(dict_obj): This function performs various manipulations on the given dictionary as per the described steps. # 1. Create a new dictionary object. new_dict = {} # 2. Insert the key-value pair (\\"apple\\", 2) into the new dictionary. new_dict[\\"apple\\"] = 2 # 3. Insert the key \\"banana\\" with value 5 into the dictionary. new_dict[\\"banana\\"] = 5 # 4. Retrieve the value associated with the key (\\"apple\\") and add 3 to it. Then, update the value in the dictionary. if \\"apple\\" in new_dict: new_dict[\\"apple\\"] += 3 # 5. Check if the key \\"orange\\" is present in the dictionary. If not, set the default value to 10. if \\"orange\\" not in new_dict: new_dict[\\"orange\\"] = 10 # 6. Delete the key \\"banana\\" from the dictionary. if \\"banana\\" in new_dict: del new_dict[\\"banana\\"] # 7. Iterate over the new dictionary and print all key-value pairs. for key, value in new_dict.items(): print(f\\"{key}: {value}\\") # 8. Merge the new dictionary with dict_obj with the condition to override existing keys in dict_obj. dict_obj.update(new_dict)"},{"question":"**Objective**: Implement a function in Python that utilizes the `sndhdr` module to process multiple sound files and generate a summary report containing the count of each file type and the average sampling rate for each file type. **Function Signature**: ```python def generate_sound_report(file_list: list) -> dict: ``` **Input**: - `file_list` (list): A list of filenames (strings) of the sound files to be processed. **Output**: - A dictionary with: - Keys as the file type (e.g., \'wav\', \'aiff\') - Values as a dictionary containing: - `count`: The number of files of that type. - `average_framerate`: The average framerate of files of that type. If the framerate is \\"0\\" for any file, it should be omitted from the average calculation. **Constraints**: - The function should handle cases where some files may not be recognized by the `sndhdr` module and should skip them. - The function should ignore files that return `None` when processed by `sndhdr`. - If a file type has all files with framerate \\"0\\", `average_framerate` should be \\"0\\". **Example**: ```python file_list = [\\"sound1.wav\\", \\"sound2.aiff\\", \\"sound3.wav\\", \\"sound4.au\\", \\"sound5.wav\\"] report = generate_sound_report(file_list) # Expected output might look like: # { # \'wav\': { # \'count\': 3, # \'average_framerate\': 44100 # assuming these wav files have a framerate of 44100 # }, # \'aiff\': { # \'count\': 1, # \'average_framerate\': 22050 # assuming this aiff file has a framerate of 22050 # }, # \'au\': { # \'count\': 1, # \'average_framerate\': 0 # assuming this au file cannot determine framerate # } # } ``` **Notes**: - Make use of the `sndhdr.what()` or `sndhdr.whathdr()` function. - Ensure the function is efficient and handles large lists of files gracefully. - You must handle errors and edge cases, such as files that are not sound files or files that return incomplete data.","solution":"import sndhdr def generate_sound_report(file_list): Process a list of sound files and generate a report containing the count and average framerate for each file type. Args: file_list (list): List of filenames (strings) of the sound files to be processed. Returns: dict: A dictionary where keys are the file types and values are dictionaries containing the count of files and the average framerate. report = {} for filename in file_list: file_info = sndhdr.what(filename) if file_info is None: continue filetype = file_info[0] framerate = file_info[2] if filetype not in report: report[filetype] = {\'count\': 0, \'framerate_total\': 0, \'average_framerate\': 0} report[filetype][\'count\'] += 1 report[filetype][\'framerate_total\'] += framerate for filetype in report: if report[filetype][\'count\'] > 0: report[filetype][\'average_framerate\'] = (report[filetype][\'framerate_total\'] // report[filetype][\'count\']) del report[filetype][\'framerate_total\'] return report"},{"question":"# Task You are tasked with creating a function that processes a list of events and returns the events sorted by date and grouped by unique dates. Each event is represented as a dictionary with the following keys: - `\\"title\\"`: a string representing the title of the event. - `\\"date\\"`: a string representing the date of the event in the format `\\"YYYY-MM-DD\\"`. Your function should take a list of such events and return an ordered dictionary where keys are the dates (in `\\"YYYY-MM-DD\\"` format), and values are lists of event titles that happen on that date, sorted by the event titles alphabetically. # Function Signature ```python from collections import OrderedDict def process_events(events: list) -> OrderedDict: pass ``` # Input - `events`: a list of dictionaries, where each dictionary contains: - `\'title\'`: a string, the title of the event. - `\'date\'`: a string, the date of the event in `\\"YYYY-MM-DD\\"` format. # Output - Returns an `OrderedDict` where the keys are dates and values are lists of event titles for that date, sorted by the event titles. # Example ```python from collections import OrderedDict events = [ {\\"title\\": \\"Meeting\\", \\"date\\": \\"2023-10-15\\"}, {\\"title\\": \\"Conference\\", \\"date\\": \\"2023-10-15\\"}, {\\"title\\": \\"Appointment\\", \\"date\\": \\"2023-10-14\\"}, {\\"title\\": \\"Workshop\\", \\"date\\": \\"2023-10-13\\"} ] result = process_events(events) assert result == OrderedDict([ (\\"2023-10-13\\", [\\"Workshop\\"]), (\\"2023-10-14\\", [\\"Appointment\\"]), (\\"2023-10-15\\", [\\"Conference\\", \\"Meeting\\"]) ]) ``` # Constraints - Dates will be in valid `\\"YYYY-MM-DD\\"` format. - No two events will have the same title on the same date. - The input list can be empty, in which case the function should return an empty `OrderedDict`. # Notes - You may use the `datetime` module to parse and handle dates. - Use the `collections` module, specifically `OrderedDict`, to maintain the order of dates.","solution":"from collections import OrderedDict def process_events(events): Processes a list of events and returns them sorted by date and grouped by unique dates. :param events: a list of dictionaries with \'title\' and \'date\' keys :return: an OrderedDict with dates as keys and lists of event titles as values from collections import defaultdict # Use defaultdict to group events by date grouped_events = defaultdict(list) # Group events by date for event in events: grouped_events[event[\'date\']].append(event[\'title\']) # Sort each list of titles alphabetically within their date group for date in grouped_events: grouped_events[date].sort() # Create an OrderedDict sorted by date keys sorted_events = OrderedDict(sorted(grouped_events.items())) return sorted_events"},{"question":"Residual Plot Analysis with Seaborn **Objective:** Write a function called `analyze_residuals` that takes a DataFrame `df` and two string parameters `x_col` and `y_col`. The function should generate and save multiple residual plots to analyze the relationship between `x_col` and `y_col`. **Function Signature:** ```python import seaborn as sns import matplotlib.pyplot as plt def analyze_residuals(df, x_col, y_col): pass ``` **Parameters:** - `df` (pandas.DataFrame): The dataset. - `x_col` (str): The name of the column to be used as the predictor variable. - `y_col` (str): The name of the column to be used as the response variable. **Requirements:** 1. Generate and save a basic residual plot. - Save the plot as `basic_residual_plot.png`. 2. Generate and save a residual plot with a second-order polynomial trend. - Save the plot as `second_order_residual_plot.png`. 3. Generate and save a residual plot with a LOWESS curve. - Save the plot as `lowess_residual_plot.png`. 4. Ensure all plots have appropriate titles, labels, and are saved using matplotlib\'s `savefig` function. **Examples:** ```python import seaborn as sns mpg = sns.load_dataset(\\"mpg\\") # Example call analyze_residuals(mpg, \\"horsepower\\", \\"mpg\\") ``` The function should save the following files: - `basic_residual_plot.png` - `second_order_residual_plot.png` - `lowess_residual_plot.png` **Constraints:** - The DataFrame `df` will always contain the columns specified by `x_col` and `y_col`. - The plots should be correctly labeled with titles indicating their type (e.g., \\"Basic Residual Plot\\", \\"Second-Order Residual Plot\\", etc.). **Assessment Criteria:** - Correct usage of the `sns.residplot` function. - Proper handling of different parameters such as `order` and `lowess`. - Proper labeling and saving of plots.","solution":"import seaborn as sns import matplotlib.pyplot as plt def analyze_residuals(df, x_col, y_col): Generates and saves multiple residual plots to analyze the relationship between x_col and y_col. Parameters: df (pandas.DataFrame): The dataset. x_col (str): The name of the column to be used as the predictor variable. y_col (str): The name of the column to be used as the response variable. # Basic Residual Plot plt.figure(figsize=(10, 6)) sns.residplot(x=x_col, y=y_col, data=df) plt.title(\'Basic Residual Plot\') plt.xlabel(x_col) plt.ylabel(\'Residuals\') plt.savefig(\'basic_residual_plot.png\') plt.close() # Second-Order Residual Plot plt.figure(figsize=(10, 6)) sns.residplot(x=x_col, y=y_col, data=df, order=2) plt.title(\'Second-Order Residual Plot\') plt.xlabel(x_col) plt.ylabel(\'Residuals\') plt.savefig(\'second_order_residual_plot.png\') plt.close() # LOWESS Residual Plot plt.figure(figsize=(10, 6)) sns.residplot(x=x_col, y=y_col, data=df, lowess=True) plt.title(\'LOWESS Residual Plot\') plt.xlabel(x_col) plt.ylabel(\'Residuals\') plt.savefig(\'lowess_residual_plot.png\') plt.close()"},{"question":"# Seaborn Facet Plotting Assessment You are provided with the `penguins` and `diamonds` datasets from the Seaborn library. Your task involves the following steps to demonstrate your understanding of Seaborn\'s `seaborn.objects` module for creating and customizing facet grids. Requirements: 1. **Plot Setup**: - Load the `penguins` and `diamonds` datasets from Seaborn. 2. **Facet Grid for Penguins**: - Create a scatter plot faceted by `species`. - Choose `bill_length_mm` for the x-axis and `bill_depth_mm` for the y-axis. - Customize the facet grid to show only `Adelie` and `Chinstrap` species. - Ensure each species subplot has independent axis scaling. 3. **Facet Grid for Diamonds**: - Create a dot plot faceted by `clarity`, arranged into a 3x3 grid (wrapping the facets). - Choose `carat` for the x-axis and `price` for the y-axis. - Disable shared x-axis among facets. - Customize the plot labels to show titles in the format \\"`Clarity: <clarity level>`\\". Expected Input: Your code will use the Seaborn datasets, which can be loaded as shown in the documentation: ```python import seaborn.objects as so from seaborn import load_dataset penguins = load_dataset(\\"penguins\\") diamonds = load_dataset(\\"diamonds\\") ``` Function Signature: ```python def custom_facet_grids(): # Your code here pass ``` Detailed Steps: 1. Load the `penguins` and `diamonds` datasets. 2. Create a seaborn `Plot` object for the `penguins` dataset: - Use `bill_length_mm` and `bill_depth_mm` for x and y axes. - Add a scatter plot using `add(so.Dots())`. - Facet by `species`, limiting to `Adelie` and `Chinstrap`. - Ensure independent axis scaling using `share(x=False, y=False)`. 3. Create a seaborn `Plot` object for the `diamonds` dataset: - Use `carat` and `price` for the x and y axes. - Add dots using `add(so.Dots())`. - Facet by `clarity`. - Arrange facets with `wrap=3`. - Ensure x-axis is not shared using `share(x=False)`. - Customize each facet\'s title to format `\\"Clarity: <clarity level>\\"` using `label(title=\\"Clarity: {}\\".format)`. Output: - The function should display the facet grids as described. Constraints: - You must utilize the `seaborn.objects.Plot` class and its methods for creating and customizing the facet grids. - Ensure your solution is efficient and well-documented.","solution":"import seaborn.objects as so from seaborn import load_dataset def custom_facet_grids(): # Load the datasets penguins = load_dataset(\\"penguins\\") diamonds = load_dataset(\\"diamonds\\") # Create the Facet Grid for Penguins penguins_plot = ( so.Plot(penguins.query(\\"species in [\'Adelie\', \'Chinstrap\']\\"), x=\'bill_length_mm\', y=\'bill_depth_mm\') .facet(\'species\') .add(so.Dots()) .share(x=False, y=False) # Independent axis scaling for each subplot ) penguins_plot.show() # Create the Facet Grid for Diamonds diamonds_plot = ( so.Plot(diamonds, x=\'carat\', y=\'price\') .facet(\'clarity\', wrap=3) .add(so.Dots()) .share(x=False) # Disable shared x-axis among facets .label(title=lambda clarity: f\\"Clarity: {clarity}\\") # Customize title format ) diamonds_plot.show()"},{"question":"# Python Coding Assessment Question Objective Implement a Python function that dynamically adds new directories to Python\'s module search path (`sys.path`) based on certain conditions, using the `site` module. Task Write a function `dynamic_path_inserter(condition, new_paths)` that adds directories to `sys.path` under the following conditions: 1. The paths should be inserted only if the given `condition` is met. 2. Each path in the list `new_paths` should be processed for `.pth` files, and valid entries within those files should be added to `sys.path`. 3. The function should skip non-existing paths. 4. The function should ensure that no path is added to `sys.path` more than once. Function Signature ```python def dynamic_path_inserter(condition: bool, new_paths: list) -> None: pass ``` Input - `condition` (bool): A boolean value that determines whether or not to proceed with adding the new paths. - `new_paths` (list): A list of strings representing the new directories to be added to `sys.path`. Output - The function does not return a value, but it modifies `sys.path` directly by adding the appropriate paths. Constraints - `new_paths` is guaranteed to be a list of strings. - Paths should be added in the order they are provided in `new_paths`. Example ```python import sys # Suppose sys.path starts with these paths sys.path = [\'/usr/local/lib/python3.9/site-packages\'] condition = True new_paths = [\'/home/user/new_path\', \'/another/path\'] dynamic_path_inserter(condition, new_paths) # If paths are valid and meet the conditions, sys.path might end up like this: # [\'/usr/local/lib/python3.9/site-packages\', \'/home/user/new_path\', \'/another/path\'] ``` Demonstrate your understanding of the `site` module\'s workings by ensuring paths are correctly added and `.pth` files are properly processed according to the module\'s documentation. Notes - If `condition` is `False`, `sys.path` should remain unchanged. - Use the `site.addsitedir()` function to process the addition of directories and their `.pth` files. - Take care of edge cases such as non-existing paths and duplicate paths.","solution":"import sys import os import site def dynamic_path_inserter(condition: bool, new_paths: list) -> None: Adds directories to sys.path if the condition is met. Each path in new_paths should be processed for .pth files, and valid entries within those files should be added to sys.path. Only existing and non-duplicate paths should be added. :param condition: bool, if True paths will be added to sys.path :param new_paths: list of str, directories to be added to sys.path if not condition: return for path in new_paths: if os.path.exists(path) and path not in sys.path: site.addsitedir(path)"},{"question":"# Advanced Set Operations in Python In this exercise, you will demonstrate your understanding of Python `set` and `frozenset` objects and their associated operations. You will implement a function that performs a series of set operations to produce a specified output, considering certain constraints and performance requirements. Problem Statement Write a function `advanced_set_operations` that takes in three arguments: 1. `iterable_a`: An iterable object containing elements to initialize a set `A`. 2. `iterable_b`: An iterable object containing elements to initialize a set `B`. 3. `operation_sequence`: A list of strings representing a sequence of set operations (from a defined set of allowed operations) to be applied exclusively on set `A`. Allowed operations include: * `\\"add_x\\"`: Adds element `x` to set `A` (e.g., `\\"add_5\\"` adds the integer `5` to the set). * `\\"discard_x\\"`: Removes element `x` from set `A` if it exists, otherwise does nothing (e.g., `\\"discard_3\\"`). * `\\"intersection_update_B\\"`: Updates set `A` with the intersection of set `A` and set `B`. * `\\"difference_update_B\\"`: Updates set `A` with the difference of set `A` and set `B`. * `\\"clear\\"`: Removes all items from set `A`. Your function should return the final state of set `A`. Additionally, ensure that your implementation appropriately handles errors, such as unhashable elements being added to the set. Expected Input and Output Format # Input * `iterable_a`: An iterable object (e.g., list, tuple, set). * `iterable_b`: An iterable object (e.g., list, tuple, set). * `operation_sequence`: A list of strings, each string representing a set operation. # Output * A set representing the final state of set `A` after applying all operations from `operation_sequence`. # Constraints * Elements in `iterable_a` and `iterable_b` are hashable. * The operations in `operation_sequence` are correctly formatted strings corresponding to allowed operations. # Performance Requirements * The function should handle large inputs efficiently, ensuring that operations like adding, discarding, and updating using other sets are performed in an optimal manner. Example ```python def advanced_set_operations(iterable_a, iterable_b, operation_sequence): # Your implementation here # Example usage: iterable_a = [1, 2, 3, 4, 5] iterable_b = [3, 4, 5, 6, 7] operation_sequence = [\\"add_6\\", \\"discard_3\\", \\"intersection_update_B\\", \\"add_8\\", \\"difference_update_B\\", \\"clear\\"] result = advanced_set_operations(iterable_a, iterable_b, operation_sequence) print(result) # Output should be set() ``` # Explanation: 1. Initial set `A = {1, 2, 3, 4, 5}` and set `B = {3, 4, 5, 6, 7}`. 2. `\\"add_6\\"` adds `6` to set `A`, resulting in `A = {1, 2, 3, 4, 5, 6}`. 3. `\\"discard_3\\"` removes `3` from set `A`, resulting in `A = {1, 2, 4, 5, 6}`. 4. `\\"intersection_update_B\\"` updates `A` to the intersection of `A` and `B`, resulting in `A = {4, 5, 6}`. 5. `\\"add_8\\"` adds `8` to set `A`, resulting in `A = {4, 5, 6, 8}`. 6. `\\"difference_update_B\\"` updates `A` to the difference of `A` and `B`, resulting in `A = {8}`. 7. `\\"clear\\"` removes all items from set `A`, resulting in `A = set()`. Implement the function `advanced_set_operations` to complete the exercise.","solution":"def advanced_set_operations(iterable_a, iterable_b, operation_sequence): Performs advanced set operations on set A based on the operation_sequence provided. A = set(iterable_a) B = set(iterable_b) for operation in operation_sequence: if operation.startswith(\\"add_\\"): _, x = operation.split(\\"_\\") A.add(int(x)) elif operation.startswith(\\"discard_\\"): _, x = operation.split(\\"_\\") A.discard(int(x)) elif operation == \\"intersection_update_B\\": A.intersection_update(B) elif operation == \\"difference_update_B\\": A.difference_update(B) elif operation == \\"clear\\": A.clear() else: raise ValueError(f\\"Unknown operation {operation}\\") return A"},{"question":"Problem Statement Using the `xml.sax.handler` module, create a custom SAX content handler to parse a given XML string. Your custom handler should extend `xml.sax.handler.ContentHandler` and override necessary methods to capture and process the following events: 1. The start and end of the document. 2. The start and end of elements along with their attributes. 3. Character data inside elements. You are also required to capture the information in a structured format, allowing for easy access and manipulation of parsed data post-parsing. Input The input will be an XML string in the following format: ```xml <data> <item id=\\"1\\"> <name>Item One</name> <price>19.99</price> </item> <item id=\\"2\\"> <name>Item Two</name> <price>29.99</price> </item> </data> ``` Output You should create a data structure to hold the parsed content: - **Document Start** - **Element Start**: Capture and store element names, along with attributes in key-value pairs. - **Character Data**: Capture and store character data inside elements. - **Element End** - **Document End** After parsing the input XML, return a structured representation of this data. Constraints - You should use the `xml.sax` library. - You must extend the `xml.sax.handler.ContentHandler`. - The XML input string will always be well-formed. Example Assuming the input XML string is as provided above, your output should be a list of dictionaries like below: ```json [ {\\"event\\": \\"startDocument\\"}, {\\"event\\": \\"startElement\\", \\"name\\": \\"data\\", \\"attributes\\": {}}, {\\"event\\": \\"startElement\\", \\"name\\": \\"item\\", \\"attributes\\": {\\"id\\": \\"1\\"}}, {\\"event\\": \\"startElement\\", \\"name\\": \\"name\\", \\"attributes\\": {}}, {\\"event\\": \\"characters\\", \\"data\\": \\"Item One\\"}, {\\"event\\": \\"endElement\\", \\"name\\": \\"name\\"}, {\\"event\\": \\"startElement\\", \\"name\\": \\"price\\", \\"attributes\\": {}}, {\\"event\\": \\"characters\\", \\"data\\": \\"19.99\\"}, {\\"event\\": \\"endElement\\", \\"name\\": \\"price\\"}, {\\"event\\": \\"endElement\\", \\"name\\": \\"item\\"}, {\\"event\\": \\"startElement\\", \\"name\\": \\"item\\", \\"attributes\\": {\\"id\\": \\"2\\"}}, {\\"event\\": \\"startElement\\", \\"name\\": \\"name\\", \\"attributes\\": {}}, {\\"event\\": \\"characters\\", \\"data\\": \\"Item Two\\"}, {\\"event\\": \\"endElement\\", \\"name\\": \\"name\\"}, {\\"event\\": \\"startElement\\", \\"name\\": \\"price\\", \\"attributes\\": {}}, {\\"event\\": \\"characters\\", \\"data\\": \\"29.99\\"}, {\\"event\\": \\"endElement\\", \\"name\\": \\"price\\"}, {\\"event\\": \\"endElement\\", \\"name\\": \\"item\\"}, {\\"event\\": \\"endElement\\", \\"name\\": \\"data\\"}, {\\"event\\": \\"endDocument\\"} ] ``` Implementation You are required to use the `xml.sax` library and implement the `ContentHandler` by extending `xml.sax.handler.ContentHandler`. ```python import xml.sax import xml.sax.handler from typing import List, Dict, Union class CustomContentHandler(xml.sax.handler.ContentHandler): def __init__(self): self.events = [] def startDocument(self): self.events.append({\\"event\\": \\"startDocument\\"}) def endDocument(self): self.events.append({\\"event\\": \\"endDocument\\"}) def startElement(self, name, attrs): attr_dict = {k: attrs.getValue(k) for k in attrs.getNames()} if attrs else {} self.events.append({\\"event\\": \\"startElement\\", \\"name\\": name, \\"attributes\\": attr_dict}) def endElement(self, name): self.events.append({\\"event\\": \\"endElement\\", \\"name\\": name}) def characters(self, content): if content.strip(): # Ignore whitespace self.events.append({\\"event\\": \\"characters\\", \\"data\\": content}) def parse_xml(xml_string: str) -> List[Dict[str, Union[str, Dict[str, str]]]]: handler = CustomContentHandler() xml.sax.parseString(xml_string, handler) return handler.events # Usage example xml_input = <data> <item id=\\"1\\"> <name>Item One</name> <price>19.99</price> </item> <item id=\\"2\\"> <name>Item Two</name> <price>29.99</price> </item> </data> parsed_output = parse_xml(xml_input) for event in parsed_output: print(event) ```","solution":"import xml.sax import xml.sax.handler from typing import List, Dict, Union class CustomContentHandler(xml.sax.handler.ContentHandler): def __init__(self): self.events = [] def startDocument(self): self.events.append({\\"event\\": \\"startDocument\\"}) def endDocument(self): self.events.append({\\"event\\": \\"endDocument\\"}) def startElement(self, name, attrs): attr_dict = {k: attrs.getValue(k) for k in attrs.getNames()} if attrs else {} self.events.append({\\"event\\": \\"startElement\\", \\"name\\": name, \\"attributes\\": attr_dict}) def endElement(self, name): self.events.append({\\"event\\": \\"endElement\\", \\"name\\": name}) def characters(self, content): if content.strip(): # Ignore whitespace self.events.append({\\"event\\": \\"characters\\", \\"data\\": content}) def parse_xml(xml_string: str) -> List[Dict[str, Union[str, Dict[str, str]]]]: handler = CustomContentHandler() xml.sax.parseString(xml_string, handler) return handler.events"},{"question":"**Coding Assessment Question: MultiIndex Manipulation and Advanced Indexing** **Objective:** Demonstrate your understanding of the `MultiIndex` in pandas by creating and manipulating a `MultiIndex` DataFrame, and performing advanced indexing operations. **Question:** Given a dataset of sales in a multi-dimensional structure, you are to perform the following tasks: 1. **Create a DataFrame with a MultiIndex**: - Create a `MultiIndex` from the following arrays: ```python years = [2020, 2020, 2020, 2021, 2021, 2022] months = [\'Jan\', \'Feb\', \'Mar\', \'Jan\', \'Feb\', \'Jan\'] ``` - Create a DataFrame `sales_df` with columns: \'Sales\', \'Profit\', filled with random numbers. 2. **Advanced Indexing**: - Slice the DataFrame and retrieve the sales data for the year 2021. - Retrieve the `Profit` for \'Jan\' across all years using a cross-section. 3. **Reindexing and Alignment**: - Create another DataFrame `forecast_df` with the same index but with columns: \'Forecast\'. - Align `sales_df` with `forecast_df` based on the `MultiIndex` and display the aligned DataFrames. **Constraints and Requirements**: - You must use pandas functionality such as `MultiIndex.from_arrays`, `DataFrame`, `.xs()`, and `.loc[]` methods. - Your solution should handle the cases where index slicing needs to drop or keep certain levels of the index hierarchy. **Input:** You are provided with the two lists `years` and `months` as shown above. **Output:** Your output should be: 1. The created `sales_df` DataFrame. 2. The DataFrame sliced to show sales data for 2021. 3. The `Profit` data for \'Jan\' across all years. 4. The aligned DataFrames showing how `sales_df` and `forecast_df` align based on the `MultiIndex`. **Example of `sales_df` (with random data in the Sales and Profit columns)**: ``` Sales Profit 2020 Jan x.xx x.xx Feb x.xx x.xx Mar x.xx x.xx 2021 Jan x.xx x.xx Feb x.xx x.xx 2022 Jan x.xx x.xx ``` **Example of Aligned DataFrames (with random data in the columns)**: ``` Aligned sales_df: Sales Profit 2020 Jan x.xx x.xx Feb x.xx x.xx Mar x.xx x.xx 2021 Jan x.xx x.xx Feb x.xx x.xx 2022 Jan x.xx x.xx Aligned forecast_df: Forecast 2020 Jan x.xx Feb x.xx Mar x.xx 2021 Jan x.xx Feb x.xx 2022 Jan x.xx ``` **Code Template:** ```python import pandas as pd import numpy as np # Step 1: Create a MultiIndex and DataFrame years = [2020, 2020, 2020, 2021, 2021, 2022] months = [\'Jan\', \'Feb\', \'Mar\', \'Jan\', \'Feb\', \'Jan\'] # Create MultiIndex index = pd.MultiIndex.from_arrays([years, months], names=[\\"Year\\", \\"Month\\"]) # Create sales_df with random data sales_data = np.random.rand(len(index), 2) # Random data for Sales and Profit sales_df = pd.DataFrame(sales_data, index=index, columns=[\'Sales\', \'Profit\']) # Display sales_df print(\\"Sales DataFrame:\\") print(sales_df) # Step 2: Advanced Indexing # a. Retrieve sales data for the year 2021 sales_2021 = sales_df.loc[2021] print(\\"nSales data for 2021:\\") print(sales_2021) # b. Retrieve the Profit for \'Jan\' across all years profit_jan = sales_df.xs(\'Jan\', level=\'Month\')[\'Profit\'] print(\\"nProfit for \'Jan\' across all years:\\") print(profit_jan) # Step 3: Reindexing and Alignment # Create another DataFrame forecast_df forecast_data = np.random.rand(len(index), 1) # Random data for Forecast forecast_df = pd.DataFrame(forecast_data, index=index, columns=[\'Forecast\']) # Align sales_df with forecast_df aligned_sales_df, aligned_forecast_df = sales_df.align(forecast_df, axis=0, level=[0, 1]) # Display aligned DataFrames print(\\"nAligned sales_df:\\") print(aligned_sales_df) print(\\"nAligned forecast_df:\\") print(aligned_forecast_df) ``` Complete the code template to solve the question.","solution":"import pandas as pd import numpy as np # Step 1: Create a MultiIndex and DataFrame years = [2020, 2020, 2020, 2021, 2021, 2022] months = [\'Jan\', \'Feb\', \'Mar\', \'Jan\', \'Feb\', \'Jan\'] # Create MultiIndex index = pd.MultiIndex.from_arrays([years, months], names=[\\"Year\\", \\"Month\\"]) # Create sales_df with random data sales_data = np.random.rand(len(index), 2) # Random data for Sales and Profit sales_df = pd.DataFrame(sales_data, index=index, columns=[\'Sales\', \'Profit\']) # Display sales_df print(\\"Sales DataFrame:\\") print(sales_df) # Step 2: Advanced Indexing # a. Retrieve sales data for the year 2021 sales_2021 = sales_df.loc[2021] print(\\"nSales data for 2021:\\") print(sales_2021) # b. Retrieve the Profit for \'Jan\' across all years profit_jan = sales_df.xs(\'Jan\', level=\'Month\')[\'Profit\'] print(\\"nProfit for \'Jan\' across all years:\\") print(profit_jan) # Step 3: Reindexing and Alignment # Create another DataFrame forecast_df forecast_data = np.random.rand(len(index), 1) # Random data for Forecast forecast_df = pd.DataFrame(forecast_data, index=index, columns=[\'Forecast\']) # Align sales_df with forecast_df aligned_sales_df, aligned_forecast_df = sales_df.align(forecast_df, axis=0, level=[0, 1]) # Display aligned DataFrames print(\\"nAligned sales_df:\\") print(aligned_sales_df) print(\\"nAligned forecast_df:\\") print(aligned_forecast_df)"},{"question":"**Question: Implementing and Validating Generators in Python** In this coding assessment, you are required to demonstrate your understanding of generators in Python, which are characterized by their use of the `yield` statement instead of `return`. Generators produce items one by one and only when required, making them memory-efficient for iterating over large datasets or infinite sequences. **Task 1: Implement a Generator Function** Write a generator function `fibonacci_sequence()` that yields numbers in the Fibonacci sequence indefinitely. ```python def fibonacci_sequence(): Yields an infinite sequence of Fibonacci numbers. Yields: int: The next Fibonacci number in the sequence. pass ``` **Task 2: Validate a Generator Object** Write a function `is_generator_object(obj)` that checks if an object is a generator. ```python def is_generator_object(obj): Checks if the provided object is a generator. Args: obj (object): The object to be checked. Returns: bool: True if the object is a generator, False otherwise. pass ``` **Constraints:** 1. You are only allowed to use Python\'s built-in functions and the `types` module. 2. `is_generator_object(obj)` should not use `type(obj) is ...` to perform the check. **Input and Output:** - For `fibonacci_sequence()`, the function will not take any input and will yield Fibonacci numbers indefinitely. - For `is_generator_object(obj)`, the input `obj` can be any Python object, and the function should return a boolean indicating if the object is a generator. **Example:** ```python # Example usage of fibonacci_sequence() gen = fibonacci_sequence() print(next(gen)) # Output: 0 print(next(gen)) # Output: 1 print(next(gen)) # Output: 1 print(next(gen)) # Output: 2 print(next(gen)) # Output: 3 # Example usage of is_generator_object() print(is_generator_object(gen)) # Output: True print(is_generator_object([1, 2, 3])) # Output: False print(is_generator_object(123)) # Output: False print(is_generator_object((x for x in range(10)))) # Output: True ``` **Note:** Make sure your solution adheres to Python\'s principles and best practices for writing efficient and readable generator functions.","solution":"import types def fibonacci_sequence(): Yields an infinite sequence of Fibonacci numbers. Yields: int: The next Fibonacci number in the sequence. a, b = 0, 1 while True: yield a a, b = b, a + b def is_generator_object(obj): Checks if the provided object is a generator. Args: obj (object): The object to be checked. Returns: bool: True if the object is a generator, False otherwise. return isinstance(obj, types.GeneratorType)"},{"question":"Task You are to implement a function that takes a list of tensors with varying lengths and another tensor with uniform length and performs element-wise addition between each tensor in the list and the uniform-length tensor. This operation should be efficient, leveraging nested tensors (NJTs) to handle the varying lengths without padding. Function Signature ```python def add_nested_and_uniform_tensors(nested_tensors: List[torch.Tensor], uniform_tensor: torch.Tensor) -> torch.nested.NestedTensor: Add each tensor in nested_tensors with the uniform_tensor in an element-wise fashion using nested tensors. Args: nested_tensors (List[torch.Tensor]): A list of 1-dimensional tensors with varying lengths. uniform_tensor (torch.Tensor): A tensor of uniform length, to be added to each tensor in the nested_tensors list. Returns: torch.nested.NestedTensor: A nested tensor resulting from the element-wise addition of each input tensor in the list with the uniform tensor. ``` Input - `nested_tensors`: A list of 1-dimensional tensors (lengths may vary). Example: `[tensor([1, 2, 3]), tensor([4, 5])]`. - `uniform_tensor`: A 1-dimensional tensor of uniform length. Example: `tensor([1, 2, 3])`. Output - A nested tensor where element-wise addition has been carried out between each tensor in `nested_tensors` and `uniform_tensor`. Constraints - The length of each tensor in `nested_tensors` cannot exceed the length of `uniform_tensor`. Example ```python import torch a = torch.tensor([1, 2, 3]) b = torch.tensor([4, 5]) uniform_tensor = torch.tensor([10, 20, 30]) result = add_nested_and_uniform_tensors([a, b], uniform_tensor) print(result) # Expected Output: NestedTensor([tensor([11, 22, 33]), tensor([14, 25])], layout=torch.jagged) ``` Notes - Utilize the `torch.nested` module to create and manipulate nested tensors. - Ensure efficient handling without padding the tensors manually.","solution":"import torch def add_nested_and_uniform_tensors(nested_tensors, uniform_tensor): Add each tensor in nested_tensors with the uniform_tensor in an element-wise fashion using nested tensors. Args: nested_tensors (List[torch.Tensor]): A list of 1-dimensional tensors with varying lengths. uniform_tensor (torch.Tensor): A tensor of uniform length, to be added to each tensor in the nested_tensors list. Returns: torch.nested.NestedTensor: A nested tensor resulting from the element-wise addition of each input tensor in the list with the uniform tensor. result = [] for tensor in nested_tensors: len_tensor = tensor.size(0) augmented_uniform_tensor = uniform_tensor[:len_tensor] result.append(tensor + augmented_uniform_tensor) return torch.nested.as_nested_tensor(result)"},{"question":"**CGI Form Processing and Error Handling** In this coding assessment, you will create a CGI script that processes user input from an HTML form, handles various possible input cases, and outputs the results in a structured HTML format. Your script should demonstrate proficiency with the `cgi` module, and handle potential error cases appropriately. # Requirements: 1. **Input Handling:** - Read user input from a CGI form containing at least three fields: `username` (text input), `age` (text input), and `hobbies` (checkbox). The `hobbies` field may have multiple values selected. - Ensure that if the `username` or `age` fields are missing or empty, the script outputs an error message in the resulting HTML. - Convert the `age` field to an integer and validate that it is a positive number. If it is not valid, output an appropriate error message. 2. **Output Generation:** - Generate an HTML output that displays the submitted username and age. - If hobbies are selected, list them in a bulleted format. - Use appropriate HTML tags to ensure a well-formatted output. 3. **Error Handling:** - Use the `cgitb` module to enable detailed error reporting during development. - Include error handling to manage cases where the input is incorrect or missing. Display user-friendly error messages. 4. **Script Header and Content Type:** - Ensure your script correctly sets the content type to `text/html` at the beginning of the output, and follows all CGI conventions for header and content sections. # Function Signature: ```python import cgi import cgitb def process_form(): pass ``` # Detailed Instructions: 1. **Form Submission Processing:** - Use the `cgi.FieldStorage` class to read the form data. - Check for the presence and validity of `username` and `age` fields. - Use the `getfirst()` and `getlist()` methods of `FieldStorage` to handle multiple and single form values appropriately. 2. **Generating the Output:** - If all input fields are valid, generate an HTML page displaying the `username`, `age`, and a list of `hobbies` (if provided). - If any errors are detected, display an HTML page with the error message. 3. **Error Reporting:** - Use `cgitb.enable()` for enhanced error reporting. # Example: Assuming the form data was submitted with: - `username`: \\"JaneDoe\\" - `age`: \\"25\\" - `hobbies`: [\\"Reading\\", \\"Swimming\\"] Expected HTML output: ```html Content-Type: text/html <html> <head><title>Form Response</title></head> <body> <h1>Form Response</h1> <p>Username: JaneDoe</p> <p>Age: 25</p> <h2>Hobbies</h2> <ul> <li>Reading</li> <li>Swimming</li> </ul> </body> </html> ``` # Constraints: - You must use the `cgi` module for reading and processing form data. - Handle HTML escaping if appropriate to prevent code injection. - Ensure robust error handling and clear identification of missing or invalid input fields. # Bonus: Implement additional form fields and showcase handling more complex form data.","solution":"import cgi import cgitb cgitb.enable() def generate_html_response(username, age, hobbies, error_message=None): print(\\"Content-Type: text/htmln\\") print(\\"<html>\\") print(\\"<head><title>Form Response</title></head>\\") print(\\"<body>\\") if error_message: print(f\\"<h1>Error</h1><p>{error_message}</p>\\") else: print(\\"<h1>Form Response</h1>\\") print(f\\"<p>Username: {username}</p>\\") print(f\\"<p>Age: {age}</p>\\") if hobbies: print(\\"<h2>Hobbies</h2>\\") print(\\"<ul>\\") for hobby in hobbies: print(f\\"<li>{hobby}</li>\\") print(\\"</ul>\\") print(\\"</body>\\") print(\\"</html>\\") def process_form(): form = cgi.FieldStorage() # Retrieve the fields username = form.getfirst(\\"username\\", \\"\\").strip() age = form.getfirst(\\"age\\", \\"\\").strip() hobbies = form.getlist(\\"hobbies\\") # Validate fields if not username: generate_html_response(None, None, None, \\"Username is missing or empty.\\") return if not age: generate_html_response(None, None, None, \\"Age is missing or empty.\\") return try: age = int(age) if age <= 0: raise ValueError(\\"Age must be a positive integer.\\") except ValueError as e: generate_html_response(username, None, None, str(e)) return generate_html_response(username, age, hobbies)"},{"question":"You are tasked with implementing a function to parse and validate email headers according to RFC 5322 standards. The function will need to handle various error conditions using the appropriate exception classes from the `email.errors` module. Function Signature ```python def validate_email_headers(headers: List[str]) -> List[str]: pass ``` # Input - `headers`: A list of strings, where each string represents a header line of an email. Each header line should follow the RFC 5322 standard. # Output - A list of strings describing any errors found in the headers. If no errors are found, return an empty list. # Constraints - The headers list will contain at most 1000 header lines. - Each header line will be a non-empty string of a maximum length of 1000 characters. # Error Handling and Defects 1. **HeaderParseError**: Raised if any header line cannot be parsed according to RFC 5322. 2. **FirstHeaderLineIsContinuationDefect**: Recorded if the first line in the headers list is a continuation line, which must not be the case. 3. **MalformedHeaderDefect**: Recorded if a header line is missing a colon or otherwise malformed. 4. **InvalidDateDefect**: Recorded if a date header is invalid or unparsable. # Example ```python headers = [ \\"From: example@example.com\\", \\"To: user@example.com\\", \\" Date: Fri, 21 Nov 1997 09:55:06 -0600\\", \\"Subject: Test email\\" ] # Expected output: [\'First header line is a continuation line\'] ``` # Notes - The function should utilize the mentioned exception and defect classes effectively. - The result should be human-readable descriptions of the errors found. # Hints - Utilize the `email.errors` module for handling exceptions. - You may use regular expressions or built-in string parsing methods to validate header formats. - Be sure to catch and handle exceptions appropriately to provide meaningful error messages. ```python # Implementation code will go here. ```","solution":"import re from email.errors import HeaderParseError, FirstHeaderLineIsContinuationDefect, MalformedHeaderDefect, InvalidDateDefect from typing import List def validate_email_headers(headers: List[str]) -> List[str]: errors = [] # Define a basic regex for header parsing header_pattern = re.compile(r\'^[w-]+: .+\') continuation_pattern = re.compile(r\'^s\') for i, header in enumerate(headers): try: # Check for continuation line at the first line if i == 0 and continuation_pattern.match(header): errors.append(\'First header line is a continuation line\') continue # Check the header format if not header_pattern.match(header): raise MalformedHeaderDefect(f\\"Header line {i+1} is malformed\\") # Special case for Date header if header.lower().startswith(\\"date:\\"): date_content = header.split(\\":\\", 1)[1].strip() if not validate_date(date_content): raise InvalidDateDefect(f\\"Date header line {i+1} is invalid\\") except MalformedHeaderDefect as e: errors.append(str(e)) except InvalidDateDefect as e: errors.append(str(e)) return errors def validate_date(date_str: str) -> bool: # A very rudimentary check for dates, you might want to use stricter validation # considering RFC 5322 format for dates is quite complex try: import email.utils email.utils.parsedate_to_datetime(date_str) return True except: return False"},{"question":"# Question Objective Implement a custom class `CustomSequence` that inherits from the `collections.abc.Sequence` abstract base class and satisfies all the required methods and behaviors. Description Your task is to implement a class `CustomSequence` that behaves like a read-only sequence. It should inherit from `collections.abc.Sequence` and implement the following methods: 1. `__getitem__(self, index)`: This method should return the item at the given index. 2. `__len__(self)`: This method should return the number of items in the sequence. Additionally, the class should support the following behaviors using mixins provided by the `Sequence` ABC: - Checking if an item is in the sequence using the `in` operator. - Iterating over the sequence. - Getting the index of a specific item using the `index` method. - Counting the occurrences of an item using the `count` method. Inputs and Outputs - The class should be initialized with an iterable (e.g., list, tuple) that contains the sequence items. - Implement the `__getitem__` and `__len__` methods to work with this iterable. - Ensure that the class supports the `__contains__`, `__iter__`, `__reversed__`, `index`, and `count` methods using the mixins provided by the `Sequence` ABC. Constraints - The `__getitem__` method should raise an `IndexError` if the index is out of bounds. - The `__len__` method should return the accurate number of items in the sequence. - Use the mixin methods provided by the `Sequence` ABC for additional behaviors. Performance Requirements - Your implementation should be efficient in terms of time complexity, especially for getting an item by index and calculating the length. Example ```python from collections.abc import Sequence class CustomSequence(Sequence): def __init__(self, data): self._data = list(data) def __getitem__(self, index): if index >= len(self._data) or index < -len(self._data): raise IndexError(\\"Index out of range\\") return self._data[index] def __len__(self): return len(self._data) # Usage seq = CustomSequence([1, 2, 3, 4, 5]) print(seq[0]) # Output: 1 print(len(seq)) # Output: 5 print(3 in seq) # Output: True print(seq.index(4)) # Output: 3 print(seq.count(2)) # Output: 1 for item in seq: print(item) # Output: 1, 2, 3, 4, 5 ``` Implement the `CustomSequence` class as described above, ensuring it adheres to the behavior and constraints.","solution":"from collections.abc import Sequence class CustomSequence(Sequence): def __init__(self, data): self._data = list(data) def __getitem__(self, index): if index >= len(self._data) or index < -len(self._data): raise IndexError(\\"Index out of range\\") return self._data[index] def __len__(self): return len(self._data)"},{"question":"Unsupervised Learning with Clustering Objective In this task, you will use clustering techniques from scikit-learn to analyze a dataset. Specifically, you will implement a function that applies the K-Means clustering algorithm to the dataset and evaluates the clustering performance using the silhouette score. Problem Statement You are given a dataset containing a set of data points. Your task is to implement a function named `perform_clustering` that performs the following steps: 1. **Load the dataset**: The dataset is provided as a 2D NumPy array where each row represents a data point. 2. **Apply K-Means Clustering**: Use scikit-learn\'s `KMeans` algorithm to cluster the data points into a specified number of clusters (k). 3. **Evaluate the Clustering**: Compute the silhouette score for the clustering result to evaluate the clustering performance. The silhouette score is a measure of how similar an object is to its own cluster compared to other clusters. It ranges from -1 to 1, where higher values indicate better clustering. Function Signature ```python def perform_clustering(data: np.ndarray, k: int) -> float: Perform K-Means clustering on the input data and return the silhouette score. Parameters: data (np.ndarray): A 2D array of shape (n_samples, n_features) representing the dataset. k (int): The number of clusters to form. Returns: float: The silhouette score for the clustering result. # Your code here ``` Input - `data`: A 2D NumPy array of shape `(n_samples, n_features)` representing the input dataset. - `k`: An integer specifying the number of clusters. Output - A float representing the silhouette score for the clustering result. Constraints - The input dataset will have at least 10 samples and at most 1000 samples. - Each sample will have between 2 and 50 features. - The number of clusters `k` will be an integer between 2 and 10. Example ```python import numpy as np data = np.array([ [1.0, 2.0], [1.5, 1.8], [5.0, 8.0], [8.0, 8.0], [1.0, 0.6], [9.0, 11.0] ]) k = 2 score = perform_clustering(data, k) print(score) # Expected output: a float value representing the silhouette score ``` Notes - Be sure to handle possible exceptions, such as an empty dataset or invalid values for `k`. - Use scikit-learn’s `KMeans` and `silhouette_score` functions for this task. - Ensure that your implementation is efficient and can handle the upper limit of the input constraints.","solution":"from sklearn.cluster import KMeans from sklearn.metrics import silhouette_score import numpy as np def perform_clustering(data: np.ndarray, k: int) -> float: Perform K-Means clustering on the input data and return the silhouette score. Parameters: data (np.ndarray): A 2D array of shape (n_samples, n_features) representing the dataset. k (int): The number of clusters to form. Returns: float: The silhouette score for the clustering result. # Ensure that data is not empty and k is in the valid range if data.size == 0: raise ValueError(\\"The input dataset must not be empty.\\") if k < 2 or k > 10: raise ValueError(\\"The number of clusters `k` must be between 2 and 10.\\") # Apply K-Means clustering kmeans = KMeans(n_clusters=k, random_state=42) labels = kmeans.fit_predict(data) # Calculate silhouette score score = silhouette_score(data, labels) return score"},{"question":"Objective Assess your ability to use the scikit-learn `random_projection` module for dimensionality reduction and understand the implications of such transformations through an inverse transformation. Problem Statement You are provided with a high-dimensional dataset. Your task is to reduce the dimensionality of this dataset using a sparse random projection, and then reconstruct the original data using the inverse transform. Post that, you need to analyze and output how well the pairwise distances are preserved. 1. Download the dataset from the provided link (or generate a synthetic dataset if a link is not provided, for example using `numpy.random.rand`). 2. Implement the following steps: 1. Reduce the dimensionality of the dataset using `SparseRandomProjection` with 50 components. 2. Reconstruct the original dataset from the reduced dataset using the inverse transform. 3. Compare the pairwise distances between the original and reconstructed datasets and compute the relative error. The relative error for two points can be calculated as: [ text{relative_error} = frac{|d_{text{original}} - d_{text{reconstructed}}|}{d_{text{original}}} ] 3. Output the mean relative error. Constraints - You must use the `SparseRandomProjection` class from the `sklearn.random_projection` module. - Your input dataset should have dimensions at least 100 x 1000. - Ensure the `compute_inverse_components` parameter is set to `True` for the transformation to allow inversion. Input - High-dimensional dataset `X` of shape (n_samples, n_features) where n_samples >= 100 and n_features >= 1000. Output - Mean relative error between the pairwise distances in the original and reconstructed datasets. Example ```python import numpy as np from sklearn.random_projection import SparseRandomProjection from scipy.spatial.distance import pdist, squareform # Step 1: Simulate a high-dimensional dataset X = np.random.rand(100, 10000) # Step 2: Initialize and apply the SparseRandomProjection transformer = SparseRandomProjection(n_components=50, compute_inverse_components=True) X_reduced = transformer.fit_transform(X) # Step 3: Reconstruct the original data X_reconstructed = transformer.inverse_transform(X_reduced) # Step 4: Compute pairwise distances and relative errors original_distances = squareform(pdist(X, \'euclidean\')) reconstructed_distances = squareform(pdist(X_reconstructed, \'euclidean\')) relative_errors = np.abs(original_distances - reconstructed_distances) / original_distances # Avoid NaN or infinite values due to zero distances relative_errors = relative_errors[np.isfinite(relative_errors)] # Step 5: Compute the mean relative error mean_relative_error = np.mean(relative_errors) print(\\"Mean Relative Error: \\", mean_relative_error) ``` Note The example provided initializes a random dataset, reduces its dimensionality, reconstructs it, and calculates the mean relative error. Ensure to handle any potential issues with zero distances gracefully.","solution":"import numpy as np from sklearn.random_projection import SparseRandomProjection from scipy.spatial.distance import pdist, squareform def dimensionality_reduction_reconstruction_error(): # Step 1: Simulate a high-dimensional dataset X = np.random.rand(200, 1500) # Step 2: Initialize and apply the SparseRandomProjection transformer = SparseRandomProjection(n_components=50, compute_inverse_components=True) X_reduced = transformer.fit_transform(X) # Step 3: Reconstruct the original data X_reconstructed = transformer.inverse_transform(X_reduced) # Step 4: Compute pairwise distances and relative errors original_distances = squareform(pdist(X, \'euclidean\')) reconstructed_distances = squareform(pdist(X_reconstructed, \'euclidean\')) relative_errors = np.abs(original_distances - reconstructed_distances) / original_distances # Avoid NaN or infinite values due to zero distances relative_errors = relative_errors[np.isfinite(relative_errors)] # Step 5: Compute the mean relative error mean_relative_error = np.mean(relative_errors) return mean_relative_error"},{"question":"**Objective:** Write a function that performs multiple window-based calculations on a given pandas DataFrame and returns a summary DataFrame. **Function Signature:** ```python def analyze_time_series(df: pd.DataFrame, window_size: int) -> pd.DataFrame: pass ``` **Input:** 1. `df`: A pandas DataFrame containing a time-series data with a datetime index and at least three numerical columns. 2. `window_size`: An integer greater than 1 representing the size of the rolling window. **Output:** - A pandas DataFrame containing the following columns for each of the numerical columns in the input DataFrame: - The rolling mean, sum, and standard deviation over the specified window size. - The expanding mean and sum. - The exponentially-weighted mean with a span equal to the window size. **Constraints:** - You can assume that the input DataFrame `df` has at least as many rows as the specified `window_size`. - The DataFrame should be sorted by the datetime index in ascending order. **Example:** ```python import pandas as pd data = { \'value1\': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100], \'value2\': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], \'value3\': [5, 15, 25, 35, 45, 55, 65, 75, 85, 95] } index = pd.date_range(start=\'2022-01-01\', periods=10, freq=\'D\') df = pd.DataFrame(data, index=index) result = analyze_time_series(df, window_size=3) print(result) ``` The expected output should be a DataFrame where each original column has additional columns for the calculated rolling mean, sum, standard deviation, expanding mean, sum, and exponentially-weighted mean (with an appropriate prefix or suffix to identify each calculation). **Note:** - Use `pandas.DataFrame.rolling` for rolling calculations. - Use `pandas.DataFrame.expanding` for expanding calculations. - Use `pandas.DataFrame.ewm` for exponentially-weighted calculations.","solution":"import pandas as pd def analyze_time_series(df: pd.DataFrame, window_size: int) -> pd.DataFrame: result = pd.DataFrame(index=df.index) # Iterate through each numerical column in df for col in df.select_dtypes(include=\'number\').columns: # Rolling calculations result[f\'{col}_rolling_mean\'] = df[col].rolling(window=window_size).mean() result[f\'{col}_rolling_sum\'] = df[col].rolling(window=window_size).sum() result[f\'{col}_rolling_std\'] = df[col].rolling(window=window_size).std() # Expanding calculations result[f\'{col}_expanding_mean\'] = df[col].expanding().mean() result[f\'{col}_expanding_sum\'] = df[col].expanding().sum() # Exponentially weighted mean result[f\'{col}_ewm_mean\'] = df[col].ewm(span=window_size).mean() return result"},{"question":"You are tasked with creating a utility function that packages a given directory into an executable Python zip archive. The utility should compress the contents of the archive and specify a Python interpreter for execution. Additionally, the utility should handle optional arguments to filter the files to be included in the archive and specify a main function. # Function Specification **Function Name:** `create_executable_zip` **Input:** - `source_dir` (str): The path to the directory containing Python code to be packaged. - `output_file` (str): The path and name of the output `.pyz` file. - `interpreter` (str): The path to the Python interpreter to be used (e.g., `\\"/usr/bin/env python3\\"`). - `main_callable` (str): The main function to be executed in the form `\\"pkg.module:callable\\"`. This can be `None` if the source directory already contains a `__main__.py` file. - `filter_func` (Callable, optional): A function to filter files to be included in the archive. It should take a single argument (a Path object representing a file) and return `True` if the file should be included, and `False` otherwise. Use default value `None` for no filtering. - `compressed` (bool): Whether to compress the files in the archive. Default is `True`. **Output:** - No direct output. The function should create a `.pyz` file at the specified `output_file` path. **Constraints:** - The function should handle missing or invalid input gracefully, raising appropriate exceptions. - The function should ensure that if `main_callable` is provided, it correctly sets up the `__main__.py` file in the archive. **Performance Requirements:** - The function should efficiently handle directories with a large number of files. # Example Usage ```python def is_python_file(path): return path.suffix == \'.py\' create_executable_zip( source_dir=\'myapp\', output_file=\'myapp.pyz\', interpreter=\'/usr/bin/env python3\', main_callable=\'myapp:main\', filter_func=is_python_file, compressed=True ) ``` In this example, the function packages all `.py` files in the `myapp` directory into a compressed archive named `myapp.pyz`, sets the interpreter to `/usr/bin/env python3`, and specifies the main function as `myapp.main`. # Notes - Ensure the resulting archive is executable on both POSIX and Windows systems (consider the differences in handling shebang lines). - Handle potential exceptions such as invalid directories, missing files, or issues during the zipping process.","solution":"import os import zipfile import shutil from pathlib import Path def create_executable_zip(source_dir, output_file, interpreter, main_callable=None, filter_func=None, compressed=True): Creates an executable Python .pyz archive from a given directory. Parameters: source_dir (str): The path to the directory containing Python code to be packaged. output_file (str): The path and name of the output .pyz file. interpreter (str): The path to the Python interpreter to be used. main_callable (str): The main function to be executed in the form \\"pkg.module:callable\\". Default is None. filter_func (Callable, optional): A function to filter files to be included in the archive. Default is None. compressed (bool): Whether to compress the files in the archive. Default is True. source_path = Path(source_dir) if not source_path.is_dir(): raise ValueError(f\\"Source directory \'{source_dir}\' does not exist or is not a directory\\") mode = \'w\' compression = zipfile.ZIP_DEFLATED if compressed else zipfile.ZIP_STORED with zipfile.ZipFile(output_file, mode=mode, compression=compression) as zf: if main_callable: main_py_content = f#!/usr/bin/env {interpreter} import {main_callable.split(\':\')[0]} import sys if __name__ == \\"__main__\\": sys.exit({main_callable.split(\':\')[1]}()) zf.writestr(\'__main__.py\', main_py_content) elif \'__main__.py\' not in (f.relative_to(source_path).as_posix() for f in source_path.rglob(\'*\')): raise ValueError(\\"Main callable not specified, and no __main__.py found in source directory\\") for file in source_path.rglob(\'*\'): if file.is_file() and (filter_func is None or filter_func(file)): zf.write(file, file.relative_to(source_path)) # Make the resulting .pyz file executable (POSIX-compliant system only) if os.name == \'posix\': st = os.stat(output_file) os.chmod(output_file, st.st_mode | 0o755)"},{"question":"Problem Statement You are required to implement a custom exception handling function in Python that mimics part of the C API functionalities described in the provided documentation. Specifically, you need to write a function that simulates fetching and formatting an error, and then prints the traceback if an error is set. This exercise will help assess your understanding of exception handling and tracebacks in Python. Requirements 1. **Function Name**: `custom_exception_handler` 2. **Input**: - An exception instance and an associated message as a string. 3. **Output**: - A formatted string containing the exception name, error message, and its traceback if available. - If no traceback is available, print a default message. 4. **Constraints**: - You must use exception handling techniques such as `try`, `except`, and traceback manipulation. - Do not use the `traceback` module directly; instead, manually format the traceback. 5. **Example**: ```python try: raise ValueError(\\"This is an error message\\") except ValueError as e: print(custom_exception_handler(e, \\"An error occurred\\")) ``` **Expected Output**: ``` Exception Type: ValueError Error Message: An error occurred Traceback: File \\"your_script_name.py\\", line 32, in <module> raise ValueError(\\"This is an error message\\") ``` Function Signature ```python def custom_exception_handler(exc_instance, message): # Your implementation here ``` Notes - Ensure that the traceback includes the relevant file name, line number, and code segment where the exception was raised. - Consider edge cases where there might not be a detailed traceback available. - The function should be robust and provide meaningful output for different types of exceptions. Hints - Familiarize yourself with Python\'s built-in exception attributes such as `__traceback__`, `__cause__`, and `__context__`. - Use string manipulation to format the traceback properly. - Pay attention to indentation and how tracebacks are typically reported in Python.","solution":"import sys import traceback def custom_exception_handler(exc_instance, message): Returns a formatted string containing the exception name, error message, and its traceback if available. exc_type = type(exc_instance).__name__ formatted_traceback = f\\"Exception Type: {exc_type}nError Message: {message}n\\" tb = exc_instance.__traceback__ if tb is not None: frames = [] while tb: frame = tb.tb_frame lineno = tb.tb_lineno code = frame.f_code filename = code.co_filename function = code.co_name frames.append(f\'File \\"{filename}\\", line {lineno}, in {function}\') tb = tb.tb_next traceback_str = \'n\'.join(frames) else: traceback_str = \'No traceback available.\' formatted_traceback += f\\"Traceback:n{traceback_str}\\" return formatted_traceback"},{"question":"**Problem Statement: Writing Your Own Float Library in Python** You are tasked with implementing a Python class that mimics some of the behaviors of the Python float type, using the provided float-related C API documentation for guidance. **Objective:** Implement a class `CustomFloat` to handle basic float operations and conversions. **Class Definition:** ```python class CustomFloat: def __init__(self, value): Initializes the CustomFloat instance. Parameters: value (float or str): The initial value for the float. If string, it should be convertible to a float. Raises: ValueError: If the input is not a valid float or string representation of a float. pass def to_double(self): Converts the CustomFloat instance to a C double representation. Returns: float: The C double representation of the float. pass @classmethod def from_double(cls, value): Creates a CustomFloat instance from a C double value. Parameters: value (float): The value to convert to a CustomFloat. Returns: CustomFloat: The CustomFloat instance. pass @staticmethod def check_is_float(value): Checks if the input value is a float type. Parameters: value: The value to check. Returns: bool: True if value is a float, False otherwise. pass @staticmethod def get_float_info(): Retrieves float-related information such as precision and min/max values. Returns: dict: A dictionary containing \'precision\', \'min\', and \'max\' of the float. pass ``` **Input and Output format:** 1. CustomFloat Class: - `__init__`: Initializes the object, throws an error if the input value is not a float or a valid string representation of a float. - `to_double`: Converts the custom float to a standard double. - `from_double`: Creates a CustomFloat instance from a double value. - `check_is_float`: Checks if the provided value is a valid float. - `get_float_info`: Returns a dictionary with `precision`, `min`, and `max` float values. **Example Usage:** ```python # Creating CustomFloat instances cf1 = CustomFloat(10.5) cf2 = CustomFloat(\\"20.3\\") # Converting to double print(cf1.to_double()) # Output: 10.5 # Creating from double cf3 = CustomFloat.from_double(30.7) print(cf3.to_double()) # Output: 30.7 # Checking float type print(CustomFloat.check_is_float(10.5)) # Output: True print(CustomFloat.check_is_float(\\"not a float\\")) # Output: False # Retrieving float information float_info = CustomFloat.get_float_info() print(float_info) # Output: {\'precision\': <value>, \'min\': <value>, \'max\': <value>} ``` **Constraints:** - The `value` parameter in `CustomFloat` initialization should be either a float or a string that can be converted to a float. - You should handle possible errors gracefully and provide meaningful error messages. **Performance Requirements:** - Efficient handling of float conversions and type checks is necessary. - The methods should perform in constant time with respect to operations involving floats. **Notes:** - You are required to implement all the methods listed above. - Ensure that the class methods are static or class methods as specified where applicable.","solution":"import sys class CustomFloat: def __init__(self, value): Initializes the CustomFloat instance. Parameters: value (float or str): The initial value for the float. If string, it should be convertible to a float. Raises: ValueError: If the input is not a valid float or string representation of a float. if isinstance(value, (float, int)): self.value = float(value) elif isinstance(value, str): try: self.value = float(value) except ValueError: raise ValueError(f\\"Cannot convert string \'{value}\' to float\\") else: raise ValueError(\\"value must be a float, int, or string representation of a float\\") def to_double(self): Converts the CustomFloat instance to a C double representation. Returns: float: The C double representation of the float. return self.value @classmethod def from_double(cls, value): Creates a CustomFloat instance from a C double value. Parameters: value (float): The value to convert to a CustomFloat. Returns: CustomFloat: The CustomFloat instance. return cls(value) @staticmethod def check_is_float(value): Checks if the input value is a float type. Parameters: value: The value to check. Returns: bool: True if value is a float, False otherwise. return isinstance(value, float) @staticmethod def get_float_info(): Retrieves float-related information such as precision and min/max values. Returns: dict: A dictionary containing \'precision\', \'min\', and \'max\' of the float. float_info = { \'precision\': sys.float_info.dig, \'min\': sys.float_info.min, \'max\': sys.float_info.max } return float_info"},{"question":"**Title: Implementing and Querying a Custom SQLite Database with Adapters** **Background:** You are working on a Python project that stores geographical data of cities. Each city is represented as an object with attributes such as name, latitude, and longitude. Your task is to use the `sqlite3` library to create a SQLite database, insert city data, and query it effectively. Additionally, you need to implement custom adapters and converters to handle the city data objects. **Requirements:** 1. **Define a `City` Data Class:** - Create a Python class `City` with attributes `name` (string), `latitude` (float), and `longitude` (float). 2. **Register Custom Adapters and Converters:** - Adapt the `City` object to SQLite-compatible types during insertion. - Convert SQLite data back to `City` objects during queries. 3. **Database Setup and CRUD Operations:** - Write a function `setup_database(db_name: str) -> sqlite3.Connection` that: - Connects to the SQLite database (creates it if it doesn’t exist). - Creates a table `cities` with columns `name`, `latitude`, and `longitude`. - Write a function `insert_city(con: sqlite3.Connection, city: City):` that: - Inserts a `City` object into the `cities` table. - Write a function `query_cities(con: sqlite3.Connection) -> List[City]:` that: - Queries all cities from the database and returns a list of `City` objects. **Input and Output:** - The functions will be called with appropriate arguments as required. - Ensure the `query_cities` function returns a list of `City` objects with the queried data. **Example:** ```python from typing import List import sqlite3 class City: def __init__(self, name: str, latitude: float, longitude: float): self.name = name self.latitude = latitude self.longitude = longitude def __repr__(self): return f\\"City(name={self.name}, latitude={self.latitude}, longitude={self.longitude})\\" def adapt_city(city: City) -> str: return f\\"{city.name};{city.latitude};{city.longitude}\\" def convert_city(data: bytes) -> City: name, latitude, longitude = data.decode().split(\';\') return City(name, float(latitude), float(longitude)) # Register the adapters and converters sqlite3.register_adapter(City, adapt_city) sqlite3.register_converter(\\"CITY\\", convert_city) def setup_database(db_name: str) -> sqlite3.Connection: con = sqlite3.connect(db_name, detect_types=sqlite3.PARSE_DECLTYPES|sqlite3.PARSE_COLNAMES) cur = con.cursor() cur.execute( \\"CREATE TABLE IF NOT EXISTS cities (name TEXT, latitude REAL, longitude REAL)\\") con.commit() return con def insert_city(con: sqlite3.Connection, city: City): cur = con.cursor() cur.execute(\\"INSERT INTO cities VALUES (?, ?, ?)\\", (city.name, city.latitude, city.longitude)) con.commit() def query_cities(con: sqlite3.Connection) -> List[City]: cur = con.cursor() cur.execute(\\"SELECT name, latitude, longitude FROM cities\\") rows = cur.fetchall() con.commit() return [City(row[0], row[1], row[2]) for row in rows] # Example usage con = setup_database(\\"cities.db\\") city1 = City(\\"New York\\", 40.7128, -74.0060) city2 = City(\\"Los Angeles\\", 34.0522, -118.2437) insert_city(con, city1) insert_city(con, city2) cities = query_cities(con) for city in cities: print(city) con.close() ``` **Constraints:** - Use parameterized queries to prevent SQL injection. - Ensure to handle exceptions appropriately using the sqlite3 exception hierarchy. **Performance Requirements:** - The solution should efficiently handle multiple city objects and ensure transactions are managed correctly. # Evaluation Criteria: - Correct implementation of the `City` class, adapters, and converters. - Accurate setup and insertion into the SQLite database. - Correct querying and conversion of data back to `City` objects. - Code readability, usage of best practices, and error handling.","solution":"from typing import List import sqlite3 class City: def __init__(self, name: str, latitude: float, longitude: float): self.name = name self.latitude = latitude self.longitude = longitude def __repr__(self): return f\\"City(name={self.name}, latitude={self.latitude}, longitude={self.longitude})\\" def adapt_city(city: City) -> str: return f\\"{city.name};{city.latitude};{city.longitude}\\" def convert_city(data: bytes) -> City: name, latitude, longitude = data.decode().split(\';\') return City(name, float(latitude), float(longitude)) # Register the adapters and converters sqlite3.register_adapter(City, adapt_city) sqlite3.register_converter(\\"CITY\\", convert_city) def setup_database(db_name: str) -> sqlite3.Connection: con = sqlite3.connect(db_name, detect_types=sqlite3.PARSE_DECLTYPES|sqlite3.PARSE_COLNAMES) cur = con.cursor() cur.execute( \\"CREATE TABLE IF NOT EXISTS cities (name TEXT, latitude REAL, longitude REAL)\\") con.commit() return con def insert_city(con: sqlite3.Connection, city: City): cur = con.cursor() cur.execute(\\"INSERT INTO cities VALUES (?, ?, ?)\\", (city.name, city.latitude, city.longitude)) con.commit() def query_cities(con: sqlite3.Connection) -> List[City]: cur = con.cursor() cur.execute(\\"SELECT name, latitude, longitude FROM cities\\") rows = cur.fetchall() con.commit() return [City(row[0], row[1], row[2]) for row in rows]"},{"question":"# Question: Binary Data Transformation and Encoding **Objective:** Write a Python function that performs the following tasks: 1. **Transformation of Binary Data:** - Accept input in the form of a packed binary string. - Use the \\"struct\\" module to unpack the binary data into a tuple according to a given format string. - Repack the transformed data into a new binary string with a different format. 2. **Encoding and Decoding:** - Encode the repacked binary string using a specified encoding (e.g., \'utf-8\', \'utf-16\', etc.) - Decode the encoded string back to its binary form. - Ensure proper error handling during encoding and decoding processes. **Function Signature:** ```python def transform_and_encode_data(packed_data: bytes, unpack_format: str, repack_format: str, encoding: str) -> bytes: pass ``` # Input: - `packed_data` (bytes): A packed binary string. - `unpack_format` (str): A format string for unpacking the binary data using `struct.unpack`. - `repack_format` (str): A format string for repacking the transformed data using `struct.pack`. - `encoding` (str): The encoding type to be used for encoding and decoding the binary string. # Output: - (bytes): The final binary string after transformation, encoding, and decoding. # Constraints: - The input binary data and format strings will be valid according to Python\'s `struct` module documentation. - The encoding type will be a standard encoding supported by Python\'s `codecs` module. - The input binary data size will not exceed 1024 bytes. # Example: ```python import struct def transform_and_encode_data(packed_data, unpack_format, repack_format, encoding): try: # Unpack the binary data unpacked_data = struct.unpack(unpack_format, packed_data) # Repack the data into a new binary string repacked_data = struct.pack(repack_format, *unpacked_data) # Encode the repacked binary string encoded_string = repacked_data.decode(encoding) # Decode it back to binary form decoded_binary_string = encoded_string.encode(encoding) return decoded_binary_string except Exception as e: # Handle any encoding/decoding errors print(f\\"Error: {e}\\") return bytes() # Example usage packed_data = struct.pack(\'i\', 12345) unpack_format = \'i\' repack_format = \'i\' encoding = \'utf-8\' result = transform_and_encode_data(packed_data, unpack_format, repack_format, encoding) print(result) ``` The example above demonstrates the expected function behavior. Your implementation should handle various cases of encoding and decoding, ensuring proper error handling. # Notes: - Utilize the `struct` module for unpacking and repacking binary data. - Use Python\'s built-in `codecs` module for encoding and decoding. - Handle possible exceptions that may arise during encoding and decoding.","solution":"import struct def transform_and_encode_data(packed_data, unpack_format, repack_format, encoding): try: # Unpack the binary data unpacked_data = struct.unpack(unpack_format, packed_data) # Repack the data into a new binary string repacked_data = struct.pack(repack_format, *unpacked_data) # Encode the repacked binary string encoded_string = repacked_data.decode(encoding) # Decode it back to binary form decoded_binary_string = encoded_string.encode(encoding) return decoded_binary_string except Exception as e: # Handle any encoding/decoding errors print(f\\"Error: {e}\\") return bytes()"},{"question":"Question: Implementing and Using a Custom Context Manager # Objective Design and implement a custom context manager in Python to handle a specific resource management use case. Demonstrate your implementation with an example that showcases the context manager\'s functionality applied both directly within a `with` statement and as a function decorator. # Requirements 1. **Create a Custom Context Manager**: - Implement a class `DatabaseConnectionManager` that manages a database connection. - The context manager should: - Open a database connection when entered. - Close the database connection when exited, even if an exception occurs. - Handle basic exceptions that might occur during the connection process and log them. - Use the `contextlib.ContextDecorator` as a base class to allow the context manager to be used as a decorator. 2. **Example Usage**: - Provide an example function `fetch_data` that uses the `DatabaseConnectionManager` to query data from a hypothetical database. The function should demonstrate the context manager being used within a `with` statement. - Provide another example showing the `DatabaseConnectionManager` being used as a decorator for a function `log_query_time` that logs the execution time of the database query. # Input and Output Formats - **Input**: No direct input from the user; the context manager should be demonstrated with internal function calls. - **Output**: Logging messages indicating the opening and closing of the connection, any exceptions caught, and the execution time of the queries. # Constraints - The solution should handle resource cleanup properly even if an exception occurs during the `with` block execution. - You can mock the database connection and operations if an actual database is unavailable. # Performance Requirements - The context manager should ensure that resources are released promptly and correctly to avoid leaks. # Example ```python import time import logging from contextlib import ContextDecorator # Setup logging logging.basicConfig(level=logging.INFO) class DatabaseConnectionManager(ContextDecorator): def __enter__(self): self.connection = self._connect_to_db() logging.info(\'Database connection opened.\') return self.connection def __exit__(self, exc_type, exc, exc_tb): self._close_db_connection(self.connection) logging.info(\'Database connection closed.\') if exc_type: logging.error(f\'Exception occurred: {exc}\') return False def _connect_to_db(self): # Mock database connection (replace with actual connection logic) return \\"MockDatabaseConnection\\" def _close_db_connection(self, connection): # Mock closing connection (replace with actual closing logic) pass # Example function using with statement def fetch_data(): with DatabaseConnectionManager() as conn: # Mock database query (replace with actual query logic) logging.info(f\'Fetching data with connection: {conn}\') # Example function using decorator @DatabaseConnectionManager() def log_query_time(): start_time = time.time() # Mock database query (replace with actual query logic) time.sleep(1) logging.info(\'Query executed.\') logging.info(f\'Execution time: {time.time() - start_time} seconds\') # Demonstrate usage fetch_data() log_query_time() ``` # Explanation - `DatabaseConnectionManager` is a context manager that manages database connections. - `fetch_data` demonstrates its usage within a `with` statement. - `log_query_time` shows how it can be used as a decorator to log query execution time.","solution":"import time import logging from contextlib import ContextDecorator # Setting up logging logging.basicConfig(level=logging.INFO) class DatabaseConnectionManager(ContextDecorator): def __enter__(self): self.connection = self._connect_to_db() logging.info(\'Database connection opened.\') return self.connection def __exit__(self, exc_type, exc_val, exc_tb): self._close_db_connection(self.connection) logging.info(\'Database connection closed.\') if exc_val: logging.error(f\'Exception occurred: {exc_val}\') return False # Returning False to propagate the exception if any def _connect_to_db(self): # Mocking a database connection (this would be your actual connection logic) return \\"MockDatabaseConnection\\" def _close_db_connection(self, connection): # Mocking closing a database connection (this would be your actual disconnection logic) pass # Example function using the `with` statement def fetch_data(): with DatabaseConnectionManager() as conn: # Mock a database query (replace with actual query logic) logging.info(f\'Fetching data with connection: {conn}\') # Example function using the decorator @DatabaseConnectionManager() def log_query_time(): start_time = time.time() # Simulating a database query with sleep (replace with actual query logic) time.sleep(1) logging.info(\'Query executed.\') logging.info(f\'Execution time: {time.time() - start_time} seconds\')"},{"question":"# Question: Implementing a Custom Asynchronous Protocol Objective: Create a custom asynchronous protocol using `asyncio` that simulates a simple HTTP-like request-response communication over TCP. The protocol should handle multiple requests from multiple clients concurrently and provide appropriate responses. Task: 1. Implement a class `EchoHTTPProtocol` that extends `asyncio.Protocol`. 2. The protocol should: - Parse incoming data to determine the request type (`GET` or `POST`) and the requested resource. - For a `GET` request, respond with a message indicating the resource was fetched. - For a `POST` request, respond with a message indicating the resource was created. - For any other request type, respond with a message indicating the request type is not supported. 3. The server should handle multiple clients concurrently and respond appropriately to their requests. Input Format: - Input data is in the form of bytes and represents a simple request line (`\\"GET /resource\\"` or `\\"POST /resource\\"`). Output Format: - Response data should be in the form of bytes and include a message like `\\"200 OK. Resource fetched.\\"`, `\\"201 Created. Resource created.\\"`, or `\\"405 Method Not Allowed.\\"`. Example: Given a client sending data: ```plaintext GET /index.html ``` The server should respond with: ```plaintext 200 OK. Resource fetched. ``` Constraints: - The server should be able to handle multiple clients at the same time. - The server should gracefully handle clients disconnecting unexpectedly. Implementation Requirements: - You must use the `asyncio` library to implement the server. - Utilize the provided `EchoServerProtocol` and `EchoClientProtocol` as references to structure your solution. # Provided Template: ```python import asyncio class EchoHTTPProtocol(asyncio.Protocol): def connection_made(self, transport): self.transport = transport peername = transport.get_extra_info(\'peername\') print(\'Connection from {}\'.format(peername)) def data_received(self, data): message = data.decode() print(\'Data received: {!r}\'.format(message)) # Parse the request data request_line = message.split(\'rn\')[0] method, path = request_line.split(\' \')[:2] if method == \'GET\': response = \\"200 OK. Resource fetched.\\" elif method == \'POST\': response = \\"201 Created. Resource created.\\" else: response = \\"405 Method Not Allowed.\\" self.transport.write(response.encode()) print(\'Close the client socket\') self.transport.close() async def main(): loop = asyncio.get_running_loop() server = await loop.create_server( lambda: EchoHTTPProtocol(), \'127.0.0.1\', 8888) async with server: await server.serve_forever() asyncio.run(main()) ``` Notes: - Make sure to test your server by writing a simple client that sends `GET` and `POST` requests and prints the server response.","solution":"import asyncio class EchoHTTPProtocol(asyncio.Protocol): def connection_made(self, transport): self.transport = transport peername = transport.get_extra_info(\'peername\') print(f\'Connection from {peername}\') def data_received(self, data): message = data.decode() print(f\'Data received: {message!r}\') try: # Simple parsing of the HTTP-like request request_line = message.split(\'rn\')[0] method, path = request_line.split(\' \')[:2] if method == \'GET\': response = \\"200 OK. Resource fetched.\\" elif method == \'POST\': response = \\"201 Created. Resource created.\\" else: response = \\"405 Method Not Allowed.\\" except Exception as e: response = \\"400 Bad Request.\\" self.transport.write(response.encode()) print(\'Close the client socket\') self.transport.close() async def main(): loop = asyncio.get_running_loop() server = await loop.create_server( lambda: EchoHTTPProtocol(), \'127.0.0.1\', 8888 ) async with server: await server.serve_forever() if __name__ == \\"__main__\\": asyncio.run(main())"},{"question":"Please implement a function `extract_sysconfig_info()` which gathers detailed information about the Python installation and configuration on the current machine. # Function Signature ```python def extract_sysconfig_info(scheme: str) -> dict: ``` # Input - `scheme`: A string which represents the installation scheme to use (e.g., \'posix_prefix\', \'nt\', etc.). This should be a valid scheme supported by the `sysconfig` module. # Output - Returns a dictionary with the following keys and their corresponding values: - **python_version**: A string representing the Python version in \\"MAJOR.MINOR\\" format (use `sysconfig.get_python_version`). - **platform**: A string identifying the current platform (use `sysconfig.get_platform`). - **paths**: A dictionary of all installation paths for the specified scheme (use `sysconfig.get_paths` with the provided scheme). - **config_vars**: A dictionary of all configuration variables (use `sysconfig.get_config_vars`). # Constraints - The `scheme` must be a string and one of the schemes returned by `sysconfig.get_scheme_names()`; otherwise, raise a `ValueError` with the message \\"Invalid scheme.\\" # Example ```python extract_sysconfig_info(\'posix_prefix\') ``` Should return a dictionary similar to: ```python { \\"python_version\\": \\"3.10\\", \\"platform\\": \\"macosx-10.9-x86_64\\", \\"paths\\": { \\"stdlib\\": \\"/usr/local/lib/python3.10\\", \\"platstdlib\\": \\"/usr/local/lib/python3.10\\", ... }, \\"config_vars\\": { \\"AC_APPLE_UNIVERSAL_BUILD\\": \\"0\\", \\"AR\\": \\"ar\\", ... } } ``` Please ensure your implementation handles errors gracefully and provides informative error messages where applicable.","solution":"import sysconfig def extract_sysconfig_info(scheme: str) -> dict: Gathers detailed information about Python installation and configuration. Args: scheme (str): The installation scheme to use (e.g., \'posix_prefix\', \'nt\'). Returns: dict: A dictionary with the following keys and corresponding values: - python_version: The Python version in \\"MAJOR.MINOR\\" format. - platform: The current platform identifier. - paths: A dictionary of all installation paths for the specified scheme. - config_vars: A dictionary of all configuration variables. Raises: ValueError: If the provided scheme is not valid. valid_schemes = sysconfig.get_scheme_names() if scheme not in valid_schemes: raise ValueError(\\"Invalid scheme\\") python_version = sysconfig.get_python_version() platform = sysconfig.get_platform() paths = sysconfig.get_paths(scheme) config_vars = sysconfig.get_config_vars() return { \\"python_version\\": python_version, \\"platform\\": platform, \\"paths\\": paths, \\"config_vars\\": config_vars }"},{"question":"# Python Profiling and Performance Analysis Objective: Write a Python program to profile a specific function and analyze its performance using the `cProfile` and `pstats` modules. Task: 1. Implement a function `compute_sum_of_squares(n)` that computes the sum of squares from 1 to `n`. 2. Use `cProfile` to profile the execution of `compute_sum_of_squares(n)` for `n = 10000`. 3. Save the profiling results to a file named `profile_results`. 4. Use the `pstats` module to analyze the profiling data: 1. Print the top 5 functions by cumulative time. 2. Print the top 5 functions by total time spent within each function (excluding time spent in sub-functions). Requirements: - Your program should correctly implement the `compute_sum_of_squares(n)` function. - Your program should use the `cProfile.run()` method to profile the function and save the results to `profile_results`. - Your program should use the `pstats.Stats` class to load and print the required statistics. Function Signatures: ```python def compute_sum_of_squares(n: int) -> int: # Implementation here pass if __name__ == \\"__main__\\": # Profiling and analysis code here pass ``` Input: - `n = 10000` (Hardcoded in the main section of the script) Output: - The top 5 functions by cumulative time. - The top 5 functions by total time spent within each function. Example Output: ``` Top 5 functions by cumulative time: ncalls tottime percall cumtime percall filename:lineno(function) 1 0.000 0.000 0.100 0.100 your_script.py:10(compute_sum_of_squares) ... Top 5 functions by total time: ncalls tottime percall cumtime percall filename:lineno(function) ... ``` Constraints: - The function `compute_sum_of_squares(n)` should be implemented using a simple loop. Performance: - The profiling process should focus on understanding the performance characteristics of the function, particularly the breakdown of where time is being spent. Note: - Ensure that your program outputs the profiling results in a clear and readable format.","solution":"import cProfile import pstats def compute_sum_of_squares(n: int) -> int: Computes the sum of squares from 1 to n. return sum(i * i for i in range(1, n + 1)) if __name__ == \\"__main__\\": profile_file = \'profile_results\' # Profile the compute_sum_of_squares function cProfile.run(\'compute_sum_of_squares(10000)\', profile_file) # Analyze the profiling data stats = pstats.Stats(profile_file) print(\\"Top 5 functions by cumulative time:\\") stats.sort_stats(\'cumulative\').print_stats(5) print(\\"nTop 5 functions by total time:\\") stats.sort_stats(\'tottime\').print_stats(5)"},{"question":"You are required to implement three functions in PyTorch that are used to compare two tensors. Your task is to write code that demonstrates a clear understanding of tensor operations and error metrics in PyTorch by implementing the following functions: 1. **`compute_sqnr(x, y)`**: - **Input**: Two tensors `x` and `y` of the same shape. - **Output**: A single floating-point value representing the Signal-to-Quantization-Noise Ratio between `x` and `y`. - **Details**: The SQNR can be computed using the formula: ( text{SQNR} = 10 log_{10} left(frac{sum x^2}{sum (x - y)^2}right) ). 2. **`compute_normalized_l2_error(x, y)`**: - **Input**: Two tensors `x` and `y` of the same shape. - **Output**: A single floating-point value representing the normalized L2 error between `x` and `y`. - **Details**: The normalized L2 error can be computed using the formula: ( text{Normalized L2 Error} = frac{|x - y|_2}{|x|_2} ). 3. **`compute_cosine_similarity(x, y)`**: - **Input**: Two tensors `x` and `y` of the same shape. - **Output**: A single floating-point value representing the cosine similarity between `x` and `y`. - **Details**: The cosine similarity can be calculated using the formula: ( text{Cosine Similarity} = frac{x cdot y}{|x|_2 |y|_2} ). **Constraints and Performance Requirements**: - The tensors `x` and `y` will have the same shape and will be non-empty. - The function should handle tensors of any shape and dimensionality. - You must use PyTorch operations to ensure the computations are efficient and take advantage of GPU acceleration if available. **Example**: ```python import torch x = torch.tensor([1.0, 2.0, 3.0]) y = torch.tensor([1.1, 1.9, 3.1]) print(compute_sqnr(x, y)) # Output will be a single float value print(compute_normalized_l2_error(x, y)) # Output will be a single float value print(compute_cosine_similarity(x, y)) # Output will be a single float value ``` Implement the described functions and ensure they work as specified.","solution":"import torch def compute_sqnr(x, y): Compute the Signal-to-Quantization-Noise Ratio (SQNR) between two tensors. Args: - x (torch.Tensor): First input tensor. - y (torch.Tensor): Second input tensor. Returns: - float: The SQNR value. signal_power = torch.sum(x ** 2) noise_power = torch.sum((x - y) ** 2) sqnr = 10 * torch.log10(signal_power / noise_power) return sqnr.item() def compute_normalized_l2_error(x, y): Compute the normalized L2 error between two tensors. Args: - x (torch.Tensor): First input tensor. - y (torch.Tensor): Second input tensor. Returns: - float: The normalized L2 error. diff_norm = torch.norm(x - y, p=2) x_norm = torch.norm(x, p=2) normalized_l2_error = diff_norm / x_norm return normalized_l2_error.item() def compute_cosine_similarity(x, y): Compute the cosine similarity between two tensors. Args: - x (torch.Tensor): First input tensor. - y (torch.Tensor): Second input tensor. Returns: - float: The cosine similarity value. dot_product = torch.dot(x.view(-1), y.view(-1)) x_norm = torch.norm(x, p=2) y_norm = torch.norm(y, p=2) cosine_similarity = dot_product / (x_norm * y_norm) return cosine_similarity.item()"},{"question":"You are tasked with implementing a resource management system for a set of network connections. Each connection must be properly acquired before usage and released after usage, even in the event of an exception. # Requirements 1. Implement a custom synchronous context manager using the `@contextmanager` decorator that manages the lifecycle of a network connection. The context manager should: - Acquire a connection at the beginning of the context. - Release the connection at the end of the context, even if an exception occurs. 2. Implement a custom asynchronous context manager using the `@asynccontextmanager` decorator that manages the lifecycle of an asynchronous network connection. The context manager should: - Asynchronously acquire a connection at the beginning of the context. - Asynchronously release the connection at the end of the context, even if an exception occurs. 3. Implement a `ConnectionManager` class that uses `contextlib.ExitStack` to manage multiple synchronous and asynchronous context managers. This class should: - Allow entering multiple contexts programmatically. - Properly clean up all resources at the end of the context. # Function Signatures ```python from contextlib import contextmanager, asynccontextmanager, ExitStack @contextmanager def network_connection(): Acquires and releases a network connection synchronously. # Implement connection acquisition logic connection = \\"Synchronous Connection\\" try: yield connection finally: # Implement connection release logic connection = None @asynccontextmanager async def async_network_connection(): Acquires and releases a network connection asynchronously. # Implement connection acquisition logic connection = \\"Asynchronous Connection\\" try: yield connection finally: # Implement connection release logic connection = None class ConnectionManager: def __init__(self): self.stack = ExitStack() def add_connection(self, connection_gen): Adds a context manager to the ExitStack self.stack.enter_context(connection_gen()) def close_all(self): Closes all managed connections self.stack.close() ``` # Example Usage ```python # Using the synchronous network connection context manager with network_connection() as conn: print(conn) # Output: Synchronous Connection # Using the asynchronous network connection context manager import asyncio async def use_async_connection(): async with async_network_connection() as conn: print(conn) # Output: Asynchronous Connection asyncio.run(use_async_connection()) # Using the ConnectionManager to handle multiple context managers manager = ConnectionManager() manager.add_connection(network_connection) manager.add_connection(network_connection) manager.close_all() # Ensure all connections are properly released ``` # Constraints 1. Ensure that resources are always cleaned up properly, even in the event of an exception. 2. `ConnectionManager` should be able to handle any context manager, not just the ones defined in this problem. Note: For simplicity, the connection acquisition and release logic can be simulated with print statements or simple assignments as shown in the example above. The main goal is to show understanding of `contextlib` utilities.","solution":"from contextlib import contextmanager, asynccontextmanager, ExitStack @contextmanager def network_connection(): Acquires and releases a network connection synchronously. # Simulate connection acquisition connection = \\"Synchronous Connection\\" print(\\"Connection acquired synchronously\\") try: yield connection finally: # Simulate connection release connection = None print(\\"Connection released synchronously\\") @asynccontextmanager async def async_network_connection(): Acquires and releases a network connection asynchronously. # Simulate connection acquisition connection = \\"Asynchronous Connection\\" print(\\"Connection acquired asynchronously\\") try: yield connection finally: # Simulate connection release connection = None print(\\"Connection released asynchronously\\") class ConnectionManager: def __init__(self): self.stack = ExitStack() def add_connection(self, connection_gen): Adds a context manager to the ExitStack self.stack.enter_context(connection_gen()) def close_all(self): Closes all managed connections self.stack.close()"},{"question":"**Question: Implement a function to compute the sum of specific tensor dimensions** You are given a 3-dimensional tensor of shape `(A, B, C)`. Implement a function `sum_tensor_dimensions` that takes in a tensor `input_tensor` and two integers `dim1` and `dim2`. The function should sum the tensor over the dimensions specified by `dim1` and `dim2`, returning the summed tensor and its new shape as a tuple. # Function Signature ```python def sum_tensor_dimensions(input_tensor: torch.Tensor, dim1: int, dim2: int) -> (torch.Tensor, torch.Size): pass ``` # Input - `input_tensor`: A 3-dimensional tensor of shape `(A, B, C)` (PyTorch tensor). - `dim1`: An integer specifying the first dimension to sum over (0 <= `dim1` < 3). - `dim2`: An integer specifying the second dimension to sum over (0 <= `dim2` < 3, and `dim2` != `dim1`). # Output - A tuple containing: - The resulting tensor after summing over the specified dimensions. - The `torch.Size` object representing the shape of the resulting tensor. # Example ```python input_tensor = torch.ones(4, 5, 6) dim1 = 1 dim2 = 2 result_tensor, result_size = sum_tensor_dimensions(input_tensor, dim1, dim2) # The resulting tensor should be: # tensor([30., 30., 30., 30.]) # The resulting size should be: # torch.Size([4]) ``` # Constraints - The function should not modify the input tensor. - The function should handle tensors where the sum across the specified dimensions is non-trivial. - The solution should be efficient and make use of PyTorch\'s built-in operations. Ensure that your function correctly computes the sum over the specified dimensions and correctly returns the resulting tensor and its shape.","solution":"import torch def sum_tensor_dimensions(input_tensor: torch.Tensor, dim1: int, dim2: int) -> (torch.Tensor, torch.Size): Sum the tensor over the specified dimensions dim1 and dim2 and return the resulting tensor and its shape. :param input_tensor: A 3-dimensional tensor of shape (A, B, C). :param dim1: The first dimension to sum over (0 <= dim1 < 3). :param dim2: The second dimension to sum over (0 <= dim2 < 3, and dim2 != dim1). :return: A tuple (resulting_tensor, resulting_shape) where: - resulting_tensor is the tensor after summing over the specified dimensions. - resulting_shape is the shape of the resulting tensor. if dim1 == dim2: raise ValueError(\\"dim1 and dim2 must be different.\\") # Perform the summation over both specified dimensions summed_tensor = input_tensor.sum(dim=dim1).sum(dim=dim2-1 if dim2 > dim1 else dim2) return summed_tensor, summed_tensor.size()"},{"question":"**Coding Assessment Question: Implement a Descriptor-Based Inventory Management System** **Objective:** This problem will assess your understanding of Python descriptors and your ability to create a system that dynamically manages and validates attributes. **Problem Statement:** You are required to implement a simple inventory management system using Python descriptors. The inventory system should manage the following attributes for each product: 1. `product_id`: A unique identifier for the product (integer). 2. `quantity`: The number of units available (integer, should be non-negative). 3. `price_per_unit`: The price for one unit of the product (float, should be non-negative). 4. `name`: The name of the product (string, non-empty). To achieve this, you need to create four custom descriptors with validation logic: 1. `ProductID`: Ensures that the product ID is a positive integer. 2. `Quantity`: Ensures that the quantity is a non-negative integer. 3. `PricePerUnit`: Ensures that the price per unit is a non-negative float. 4. `Name`: Ensures that the name is a non-empty string. You will then use these descriptors in a `Product` class to manage the attributes mentioned above. **Requirements:** 1. Implement a base descriptor class `Validated` with the following methods: - `__get__()` - `__set__()` - `validate()`: A method that raises `NotImplementedError`. This method should be overridden in subclass descriptors to implement specific validation logic. 2. Implement four descriptors inheriting from `Validated`: - `ProductID`: - Validates that the value is a positive integer. - `Quantity`: - Validates that the value is a non-negative integer. - `PricePerUnit`: - Validates that the value is a non-negative float. - `Name`: - Validates that the value is a non-empty string. 3. Implement the `Product` class using these descriptors for appropriate attributes. **Constraints:** - The `product_id` should be a positive integer. - The `quantity` should be a non-negative integer. - The `price_per_unit` should be a non-negative float. - The `name` should be a non-empty string. **Input and Output:** - There are no specific input and output functions. However, instances of `Product` should raise appropriate exceptions when invalid values are assigned to attributes. **Examples:** 1. Valid example: ```python product = Product(product_id=1, quantity=50, price_per_unit=9.99, name=\'Widget\') print(product.product_id) # Should print: 1 print(product.quantity) # Should print: 50 print(product.price_per_unit) # Should print: 9.99 print(product.name) # Should print: Widget ``` 2. Invalid example: ```python product = Product(product_id=-1, quantity=50, price_per_unit=9.99, name=\'Widget\') # Should raise ValueError for product_id ``` **Implementation:** Complete the methods and classes below: ```python class Validated: def __init__(self): self._name = None def __set_name__(self, owner, name): self._name = name def __get__(self, instance, owner): return instance.__dict__[self._name] def __set__(self, instance, value): self.validate(value) instance.__dict__[self._name] = value def validate(self, value): raise NotImplementedError(\\"Subclasses must implement \'validate\' method\\") class ProductID(Validated): def validate(self, value): if not isinstance(value, int) or value <= 0: raise ValueError(f\\"{self._name} must be a positive integer\\") class Quantity(Validated): def validate(self, value): if not isinstance(value, int) or value < 0: raise ValueError(f\\"{self._name} must be a non-negative integer\\") class PricePerUnit(Validated): def validate(self, value): if not isinstance(value, (int, float)) or value < 0: raise ValueError(f\\"{self._name} must be a non-negative float\\") class Name(Validated): def validate(self, value): if not isinstance(value, str) or not value: raise ValueError(f\\"{self._name} must be a non-empty string\\") class Product: product_id = ProductID() quantity = Quantity() price_per_unit = PricePerUnit() name = Name() def __init__(self, product_id, quantity, price_per_unit, name): self.product_id = product_id self.quantity = quantity self.price_per_unit = price_per_unit self.name = name # Test Cases try: product = Product(product_id=1, quantity=50, price_per_unit=9.99, name=\'Widget\') assert (product.product_id, product.quantity, product.price_per_unit, product.name) == (1, 50, 9.99, \'Widget\') print(\\"Test case 1 passed\\") except Exception as e: print(\\"Test case 1 failed\\", e) try: product = Product(product_id=0, quantity=50, price_per_unit=9.99, name=\'Widget\') print(\\"Test case 2 failed\\") except ValueError: print(\\"Test case 2 passed\\") try: product = Product(product_id=1, quantity=-5, price_per_unit=9.99, name=\'Widget\') print(\\"Test case 3 failed\\") except ValueError: print(\\"Test case 3 passed\\") try: product = Product(product_id=1, quantity=50, price_per_unit=-9.99, name=\'Widget\') print(\\"Test case 4 failed\\") except ValueError: print(\\"Test case 4 passed\\") try: product = Product(product_id=1, quantity=50, price_per_unit=9.99, name=\'\') print(\\"Test case 5 failed\\") except ValueError: print(\\"Test case 5 passed\\") ``` Ensure to validate and test your class thoroughly.","solution":"class Validated: def __init__(self): self._name = None def __set_name__(self, owner, name): self._name = name def __get__(self, instance, owner): return instance.__dict__[self._name] def __set__(self, instance, value): self.validate(value) instance.__dict__[self._name] = value def validate(self, value): raise NotImplementedError(\\"Subclasses must implement \'validate\' method\\") class ProductID(Validated): def validate(self, value): if not isinstance(value, int) or value <= 0: raise ValueError(f\\"{self._name} must be a positive integer\\") class Quantity(Validated): def validate(self, value): if not isinstance(value, int) or value < 0: raise ValueError(f\\"{self._name} must be a non-negative integer\\") class PricePerUnit(Validated): def validate(self, value): if not isinstance(value, (int, float)) or value < 0: raise ValueError(f\\"{self._name} must be a non-negative float\\") class Name(Validated): def validate(self, value): if not isinstance(value, str) or not value: raise ValueError(f\\"{self._name} must be a non-empty string\\") class Product: product_id = ProductID() quantity = Quantity() price_per_unit = PricePerUnit() name = Name() def __init__(self, product_id, quantity, price_per_unit, name): self.product_id = product_id self.quantity = quantity self.price_per_unit = price_per_unit self.name = name"},{"question":"Objective You will implement a function that reads a Sun AU audio file, extracts its metadata, and then writes a new AU file with modified metadata and the same audio content. Task Description 1. Implement a function `modify_sunau_file(input_file: str, output_file: str, new_sample_rate: int)` that: - Reads an existing Sun AU file specified by `input_file`. - Extracts the metadata and audio data. - Modifies the sample rate to `new_sample_rate`. - Writes the metadata (with the modified sample rate) and the original audio data to a new Sun AU file specified by `output_file`. Input - `input_file` (str): The path to the input AU file. - `output_file` (str): The path to the output AU file. - `new_sample_rate` (int): The new sample rate to set for the output AU file. Output - The function does not return anything. It writes the modified audio file to the path specified by `output_file`. Constraints - Assume that the input AU file is well-formed and the `output_file` path is valid. - The new sample rate must be a positive integer. - The input audio file will not be empty and will have a valid AU format. Example ```python def modify_sunau_file(input_file: str, output_file: str, new_sample_rate: int) -> None: pass # Example usage: input_file = \'example.au\' output_file = \'modified_example.au\' new_sample_rate = 44100 modify_sunau_file(input_file, output_file, new_sample_rate) ``` In this example, the function will read `example.au`, change its sample rate to 44100 Hz, and write the resulting AU file to `modified_example.au`. Notes - Make use of the `sunau` module methods such as `open()`, `getnchannels()`, `getsampwidth()`, `getframerate()`, `getnframes()`, `readframes()`, `setnchannels()`, `setsampwidth()`, `setframerate()`, `setnframes()`, `writeframes()`, and `close()`. - Ensure proper handling of file objects by closing them appropriately after use.","solution":"import sunau def modify_sunau_file(input_file: str, output_file: str, new_sample_rate: int) -> None: Modifies the sample rate of a SUN AU audio file and writes it to a new file. Args: - input_file (str): Path to the input AU file. - output_file (str): Path to the output AU file. - new_sample_rate (int): The new sample rate to set for the output AU file. with sunau.open(input_file, \'rb\') as in_file: # Extract current metadata and audio data n_channels = in_file.getnchannels() samp_width = in_file.getsampwidth() n_frames = in_file.getnframes() audio_data = in_file.readframes(n_frames) with sunau.open(output_file, \'wb\') as out_file: # Set new metadata and write audio data out_file.setnchannels(n_channels) out_file.setsampwidth(samp_width) out_file.setframerate(new_sample_rate) out_file.setnframes(n_frames) out_file.writeframes(audio_data)"},{"question":"**Question: Handling Asynchronous Tasks with asyncio.Future** You are required to implement a function that performs multiple asynchronous tasks concurrently and handles their completion using `asyncio.Future`. You need to: 1. Create multiple Future objects. 2. Simulate asynchronous operations by setting results on these futures after different delays. 3. Collect and return the results of all futures once they are completed. # Specifications: Implement a function `perform_async_tasks` with the following signature: ```python import asyncio async def perform_async_tasks(num_tasks: int) -> list: pass ``` **Parameters:** - `num_tasks` (int): The number of asynchronous tasks to perform. **Returns:** - (list): A list containing the results of each asynchronous task. # Constraints: 1. Each task should complete with a unique result value: \\"Result-1\\", \\"Result-2\\", ..., \\"Result-n\\". 2. Tasks should complete with different delays ranging from 1 to `num_tasks` seconds. 3. You must use `asyncio.Future` to manage the tasks and their results. 4. Avoid using high-level task utilities like `asyncio.gather` or `asyncio.create_task`. # Example: ```python import asyncio async def perform_async_tasks(num_tasks: int) -> list: async def set_future_result(future, delay, result): await asyncio.sleep(delay) future.set_result(result) loop = asyncio.get_running_loop() futures = [loop.create_future() for _ in range(num_tasks)] for i, future in enumerate(futures): asyncio.create_task(set_future_result(future, i + 1, f\\"Result-{i + 1}\\")) results = [ await future for future in futures ] return results ``` **Test Case:** Let\'s test the function with the following code: ```python import asyncio async def main(): results = await perform_async_tasks(3) print(results) asyncio.run(main()) ``` **Expected Output:** ``` [\'Result-1\', \'Result-2\', \'Result-3\'] ``` Ensure your solution efficiently handles the task creation and completion using `asyncio.Future`.","solution":"import asyncio async def perform_async_tasks(num_tasks: int) -> list: async def set_future_result(future, delay, result): await asyncio.sleep(delay) future.set_result(result) loop = asyncio.get_running_loop() futures = [loop.create_future() for _ in range(num_tasks)] for i, future in enumerate(futures): asyncio.create_task(set_future_result(future, i + 1, f\\"Result-{i + 1}\\")) results = [await future for future in futures] return results"},{"question":"# Pandas Coding Assessment - Stock Price Analysis You are provided with a CSV file named `stock_data.csv` containing daily stock prices for multiple companies. The columns in the file are as follows: - `Date`: The trading date (formatted as `YYYY-MM-DD`). - `Company`: The name of the company. - `Open`: The opening price of the stock. - `Close`: The closing price of the stock. - `High`: The highest price the stock reached during the trading day. - `Low`: The lowest price the stock reached during the trading day. - `Volume`: The trading volume for the day. Your task is to implement a function in Python using pandas that calculates certain metrics for the given dataset: 1. The daily price change for each company, which is the difference between the `Close` and `Open` prices. 2. The average closing price for each company over the entire dataset. 3. The date on which each company had its highest trading volume. 4. A new DataFrame that contains only the data for the company with the highest average closing price, sorted by date in ascending order. # Function Signature ```python import pandas as pd def analyze_stock_data(file_path: str) -> pd.DataFrame: Analyzes stock data and returns a DataFrame with the data for the company with the highest average closing price, sorted by date. Parameters: file_path (str): The path to the CSV file containing stock data. Returns: pd.DataFrame: A DataFrame filtered and sorted as per the criteria. pass ``` # Input - `file_path`: A string representing the path to the `stock_data.csv`. # Output - A pandas DataFrame containing the data for the company with the highest average closing price, sorted by date in ascending order. The DataFrame should include all original columns along with an additional column named `Daily Change` which is the difference between `Close` and `Open` prices. # Constraints - You are not allowed to use loops to perform any of the calculations; utilize pandas functionalities for vectorized operations. - The code should be efficient and handle datasets with up to 1 million rows comfortably. # Example ```python # Assuming stock_data.csv has the following content: # Date,Company,Open,Close,High,Low,Volume # 2021-01-01,CompanyA,100,110,115,95,10000 # 2021-01-01,CompanyB,200,210,215,195,15000 # 2021-01-01,CompanyC,300,310,315,295,20000 # ... (more rows) result_df = analyze_stock_data(\\"path/to/stock_data.csv\\") # The result DataFrame should be filtered and sorted as specified. print(result_df) ``` # Notes - Ensure your solution is well-documented and includes handling of any edge cases, such as when the file is empty or when there are ties in the highest average closing price. - You may assume that the input CSV file is correctly formatted.","solution":"import pandas as pd def analyze_stock_data(file_path: str) -> pd.DataFrame: # Load the CSV data into a DataFrame df = pd.read_csv(file_path) # Calculate the daily price change for each company df[\'Daily Change\'] = df[\'Close\'] - df[\'Open\'] # Calculate the average closing price for each company avg_closing_prices = df.groupby(\'Company\')[\'Close\'].mean() # Identify the company with the highest average closing price highest_avg_closing_company = avg_closing_prices.idxmax() # Find the date with the highest trading volume for each company highest_volume_dates = df.loc[df.groupby(\'Company\')[\'Volume\'].idxmax()] # Filter the DataFrame to only include data for the company with the highest average closing price highest_avg_closing_df = df[df[\'Company\'] == highest_avg_closing_company] # Sort the DataFrame by date in ascending order highest_avg_closing_df = highest_avg_closing_df.sort_values(by=\'Date\') return highest_avg_closing_df"},{"question":"Objective Your task is to implement a mini-batch training process for a text classification problem using Scikit-learn. You will read data in chunks, use a feature extraction method to process the text data, and apply an incremental learning algorithm to train a classifier. Problem Statement You are given a large text dataset for a binary classification task. The dataset is too large to fit into memory, so you need to process it in mini-batches using incremental learning. Implement a function `incremental_text_classification` that performs this task. Function Signature ```python def incremental_text_classification(file_path: str, batch_size: int, max_iter: int) -> float: ``` Input - `file_path` (str): The file path to a CSV file containing the dataset. The CSV file has two columns: `\\"text\\"` (the text data) and `\\"label\\"` (the binary label). - `batch_size` (int): The number of samples to include in each mini-batch. - `max_iter` (int): The maximum number of iterations (epochs) to run the training process. Output - (float): The final accuracy of the classifier on the entire dataset after training. Requirements 1. Use the `HashingVectorizer` from Scikit-learn for feature extraction. 2. Use the `SGDClassifier` from Scikit-learn for incremental learning. 3. Ensure the entire dataset is processed multiple times (`max_iter` times). 4. Divide the dataset into mini-batches of size `batch_size` and train the classifier incrementally using these mini-batches. 5. Calculate the final accuracy of the classifier on the entire dataset. Constraints - The dataset is guaranteed to be well-formed, with no missing or malformed data in the specified columns. - Memory usage should be optimized by processing data in chunks, fitting only the mini-batches into memory at any given time. Example ```python file_path = \\"large_text_dataset.csv\\" batch_size = 100 max_iter = 10 accuracy = incremental_text_classification(file_path, batch_size, max_iter) print(f\\"Final Accuracy: {accuracy}\\") ``` This example should train an incremental text classifier on the dataset located at `file_path`, processing 100 samples per mini-batch, and iterating through the entire dataset 10 times, finally printing the accuracy. Note - Read the data in mini-batches using `pandas.read_csv` with the `chunksize` parameter. - Use the `partial_fit` method of `SGDClassifier` for incremental learning.","solution":"import pandas as pd from sklearn.feature_extraction.text import HashingVectorizer from sklearn.linear_model import SGDClassifier from sklearn.metrics import accuracy_score def incremental_text_classification(file_path: str, batch_size: int, max_iter: int) -> float: vectorizer = HashingVectorizer() classifier = SGDClassifier() classes = [0, 1] # Assuming binary classification with labels 0 and 1 # Read the data in chunks based on the specified batch size for epoch in range(max_iter): chunk_iter = pd.read_csv(file_path, usecols=[\'text\', \'label\'], chunksize=batch_size) for chunk in chunk_iter: X_chunk = vectorizer.transform(chunk[\'text\']) y_chunk = chunk[\'label\'] classifier.partial_fit(X_chunk, y_chunk, classes=classes) # After training, evaluate the model on the entire dataset all_data = pd.read_csv(file_path, usecols=[\'text\', \'label\']) X_all = vectorizer.transform(all_data[\'text\']) y_all = all_data[\'label\'] y_pred = classifier.predict(X_all) accuracy = accuracy_score(y_all, y_pred) return accuracy"},{"question":"Objective Create and test a class using the `unittest.mock` library to demonstrate your comprehension of mocking in Python. Problem Statement 1. **Create a class named `DataProcessor`** with the following methods: - `fetch_data(url: str) -> dict`: A method that simulates fetching data from a given URL. It returns a dictionary. - `process_data(data: dict) -> dict`: A method that simulates data processing. It takes a dictionary and returns a processed dictionary. - `save_data(processed_data: dict) -> None`: A method that simulates saving the processed data. This method has no return value. - `async validate_data(processed_data: dict) -> bool`: An asynchronous method that simulates validation of processed data. It returns a boolean indicating whether the data is valid or not. 2. **Implement tests** for the `DataProcessor` class using the `unittest` framework and `unittest.mock` library: - Use `MagicMock` to mock the methods `fetch_data`, `process_data`, and `save_data`. - Use `AsyncMock` to mock the method `validate_data`. - Demonstrate the use of `side_effect` and `assert_called_with` assertions. - Implement a test where `validate_data` raises an exception, and assert that it handled correctly. Constraints - Assume that URL fetching always returns the same dictionary: `{\\"key\\": \\"value\\"}`. - The processing function adds a new key-value pair to the dictionary: `{\\"key\\": \\"value\\", \\"processed\\": \\"true\\"}`. - Saving function will print \\"Data saved!\\" upon successful save. - Validation function will await and return `True` if the \\"processed\\" key is in the dictionary, `False` otherwise. Input - The main code does not require user inputs; it simulates predefined workflows. Output - The test functions will not produce outputs but will perform assertions to ensure correctness. Example Here is an example of what the class and one of the test functions might look like: ```python import unittest from unittest.mock import MagicMock, AsyncMock, patch class DataProcessor: def fetch_data(self, url: str) -> dict: return {\\"key\\": \\"value\\"} def process_data(self, data: dict) -> dict: data[\\"processed\\"] = \\"true\\" return data def save_data(self, processed_data: dict) -> None: print(\\"Data saved!\\") async def validate_data(self, processed_data: dict) -> bool: return \\"processed\\" in processed_data class TestDataProcessor(unittest.TestCase): @patch.object(DataProcessor, \'fetch_data\', return_value={\\"key\\": \\"value\\"}) @patch.object(DataProcessor, \'process_data\', return_value={\\"key\\": \\"value\\", \\"processed\\": \\"true\\"}) @patch.object(DataProcessor, \'save_data\', side_effect=print(\\"Data saved!\\")) def test_methods(self, mock_fetch, mock_process, mock_save): processor = DataProcessor() data = processor.fetch_data(\\"http://example.com\\") self.assertEqual(data, {\\"key\\": \\"value\\"}) processed_data = processor.process_data(data) self.assertEqual(processed_data, {\\"key\\": \\"value\\", \\"processed\\": \\"true\\"}) processor.save_data(processed_data) mock_save.assert_called_with({\\"key\\": \\"value\\", \\"processed\\": \\"true\\"}) @patch.object(DataProcessor, \'validate_data\', new_callable=AsyncMock, return_value=True) async def test_async_validation(self, mock_validate): processor = DataProcessor() result = await processor.validate_data({\\"key\\": \\"value\\", \\"processed\\": \\"true\\"}) self.assertTrue(result) mock_validate.assert_awaited_with({\\"key\\": \\"value\\", \\"processed\\": \\"true\\"}) if __name__ == \\"__main__\\": unittest.main() ``` Create additional test cases as needed to cover all scenarios.","solution":"import asyncio class DataProcessor: def fetch_data(self, url: str) -> dict: Simulates fetching data from the given URL. return {\\"key\\": \\"value\\"} def process_data(self, data: dict) -> dict: Simulates processing the input data. data[\\"processed\\"] = \\"true\\" return data def save_data(self, processed_data: dict) -> None: Simulates saving the processed data. print(\\"Data saved!\\") async def validate_data(self, processed_data: dict) -> bool: Simulates validation of the processed data. await asyncio.sleep(0.1) # Simulating an asynchronous validation operation return \\"processed\\" in processed_data"},{"question":"# **Coding Assessment Question** # **Objective** The purpose of this assessment is to evaluate your understanding of the `optparse` module for parsing command-line options in Python. You will need to demonstrate your ability to create an `OptionParser`, define and configure options, parse command-line arguments, and handle those arguments appropriately in your script. # **Problem Statement** You are tasked with developing a Python script that processes command-line options for a fictitious data processing application. The script should be able to handle the following options: 1. `-i` or `--input`: This option specifies the input file from which data is read. The filename should be stored in an attribute named `input_filename`. 2. `-o` or `--output`: This option specifies the output file where results will be written. The filename should be stored in an attribute named `output_filename`. 3. `-v` or `--verbose`: This flag indicates whether the script should print detailed status messages. When this flag is present, set an attribute `verbose` to `True`. 4. `-q` or `--quiet`: This flag indicates that the script should not print status messages. When this flag is present, set `verbose` to `False`. By default, `verbose` should be `True`. 5. `-n`: This option specifies the number of lines to read from the input file. It should be followed by an integer value, which should be stored in an attribute named `num_lines`. In addition to configuring the options, your script should also handle errors gracefully by printing an appropriate error message and exiting with an error status if an invalid option is provided or if required options are missing. Your script should also be able to print a help message when the `-h` or `--help` option is provided. # **Input Format** - Command-line options and their respective values. # **Output Format** - Detailed status messages should be printed if `--verbose` is provided. - Error messages should be printed if any invalid options are encountered or required options are missing. - Help message should be printed if `-h` or `--help` is provided. # **Constraints** - The script requires both `--input` and `--output` options to be provided. - `--verbose` and `--quiet` are mutually exclusive. # **Performance Requirements** - Ensure the script handles command-line argument parsing efficiently. # **Implementation Details** 1. Create an `OptionParser`. 2. Define the required options with their respective actions, types, and default values. 3. Parse the command-line arguments. 4. Handle and process the parsed options appropriately. 5. Implement error handling to deal with invalid options or missing required options. 6. Generate and print a help message when requested. # **Example Usage** ```shell python data_processor.py --input=input.txt --output=output.txt -n 100 --verbose ``` In this example: - `input_filename` should be set to `input.txt` - `output_filename` should be set to `output.txt` - `num_lines` should be set to `100` - `verbose` should be set to `True` ```shell python data_processor.py --input=input.txt --output=output.txt --quiet ``` In this example: - `input_filename` should be set to `input.txt` - `output_filename` should be set to `output.txt` - `verbose` should be set to `False` # **Your Task** Implement the script `data_processor.py` according to the specifications above. Ensure that your script handles all the options correctly, provides helpful error messages, and prints detailed status messages when `--verbose` is enabled.","solution":"import sys from optparse import OptionParser, OptionValueError def parse_options(args): Parses command-line options and arguments. Returns an object containing input_filename, output_filename, num_lines, and verbose attributes. parser = OptionParser() parser.add_option(\\"-i\\", \\"--input\\", dest=\\"input_filename\\", help=\\"input file from which data is read\\") parser.add_option(\\"-o\\", \\"--output\\", dest=\\"output_filename\\", help=\\"output file where results will be written\\") parser.add_option(\\"-v\\", \\"--verbose\\", action=\\"store_true\\", dest=\\"verbose\\", default=True, help=\\"print detailed status messages\\") parser.add_option(\\"-q\\", \\"--quiet\\", action=\\"store_false\\", dest=\\"verbose\\", help=\\"do not print status messages\\") parser.add_option(\\"-n\\", type=\\"int\\", dest=\\"num_lines\\", help=\\"number of lines to read from the input file\\") (options, args) = parser.parse_args(args) if not options.input_filename: parser.error(\\"Input file not specified.\\") if not options.output_filename: parser.error(\\"Output file not specified.\\") if options.verbose is None: options.verbose = True return options def main(args): options = parse_options(args) print(f\\"Input File: {options.input_filename}\\") print(f\\"Output File: {options.output_filename}\\") print(f\\"Num Lines: {options.num_lines}\\") print(f\\"Verbose: {options.verbose}\\") if __name__ == \\"__main__\\": main(sys.argv[1:])"},{"question":"# Question: Manipulating Sun AU Files Given the `sunau` module, your task is to write a function `adjust_au_volume` that will read an input AU file, normalize its audio data to a specific volume level, and save it as a new AU file. The volume normalization should be performed by adjusting the amplitude of all audio samples to the desired volume level. Function Signature: ```python def adjust_au_volume(input_file: str, output_file: str, target_volume: float) -> None: pass ``` Parameters: - `input_file` (str): The path to the input AU file. - `output_file` (str): The path where the modified AU file should be saved. - `target_volume` (float): The desired volume level, expressed as a percentage (0.0 to 1.0), where 1.0 represents 100% (unchanged) volume, and 0.5 represents 50% volume. Constraints: - You are only allowed to use standard Python libraries and the `sunau` module. - You should handle both mono and stereo files. - The function should read the data, adjust the volume, and then save the modified data to the output file using `sunau`. Example: Assume you have a mono AU file with 16-bit linear samples, and you want to reduce its volume by half: ```python adjust_au_volume(\'input.au\', \'output.au\', 0.5) ``` Hints: - Use the `sunau.open` function to open an AU file in read mode and obtain an `AU_read` object. - Utilize the methods of `AU_read` to get the necessary parameters and read frames. - Perform the volume adjustment by scaling sample values by the `target_volume`. - Use the `sunau.open` function to open an AU file in write mode, set the appropriate parameters, and write the adjusted audio data. # Implementation: Provide a complete implementation of the `adjust_au_volume` function that fulfills the requirements stated above.","solution":"import sunau import numpy as np def adjust_au_volume(input_file: str, output_file: str, target_volume: float) -> None: with sunau.open(input_file, \'rb\') as in_file: # Read audio file parameters n_channels = in_file.getnchannels() sample_width = in_file.getsampwidth() n_frames = in_file.getnframes() frame_rate = in_file.getframerate() audio_data = in_file.readframes(n_frames) # Convert audio data to numpy array for volume adjustment audio_array = np.frombuffer(audio_data, dtype=np.int16) # Adjust the volume adjusted_audio_array = (audio_array * target_volume).astype(np.int16) # Convert adjusted audio array back to bytes adjusted_audio_data = adjusted_audio_array.tobytes() with sunau.open(output_file, \'wb\') as out_file: # Set parameters and write adjusted data out_file.setnchannels(n_channels) out_file.setsampwidth(sample_width) out_file.setframerate(frame_rate) out_file.writeframes(adjusted_audio_data)"},{"question":"Multi-threaded Task Scheduler using PriorityQueue Objective In this task, you are required to write a multi-threaded task scheduler using the `PriorityQueue` class from the `queue` module. The scheduler should manage tasks with varying priorities and ensure that the highest priority tasks are executed first. Question 1. Implement a class `Task` which should contain two attributes: - `priority`: an integer representing the priority of the task (a lower number means higher priority). - `description`: a string describing the task. 2. Implement a class `TaskScheduler` which should: - Use a `PriorityQueue` to manage task scheduling. - Have methods to add tasks to the queue and to process tasks in order of their priority. The following methods are required: - `add_task(self, task: Task)`: This method adds a task to the scheduler\'s queue. - `process_tasks(self)`: This method processes all tasks in order of their priority. For each task, it prints the task’s priority and description. 3. Implement a multi-threaded environment to demonstrate the `TaskScheduler`. Launch multiple threads that add tasks to the scheduler simultaneously and a separate thread that processes the tasks. Constraints - The `TaskScheduler` should correctly handle concurrent additions and processing of tasks. - The size of the task queue can be unbounded. Example Here\'s an example to help you understand the requirements: ```python import queue import threading from dataclasses import dataclass, field from typing import Any @dataclass(order=True) class Task: priority: int description: str = field(compare=False) class TaskScheduler: def __init__(self): self.queue = queue.PriorityQueue() def add_task(self, task: Task): self.queue.put(task) def process_tasks(self): while not self.queue.empty(): task = self.queue.get() print(f\'Processing task with priority {task.priority}: {task.description}\') self.queue.task_done() def add_tasks(scheduler): for i in range(10): task = Task(priority=i, description=f\'Task {i}\') scheduler.add_task(task) def main(): scheduler = TaskScheduler() threads = [] for _ in range(3): t = threading.Thread(target=add_tasks, args=(scheduler,)) threads.append(t) t.start() for t in threads: t.join() processor_thread = threading.Thread(target=scheduler.process_tasks) processor_thread.start() processor_thread.join() if __name__ == \'__main__\': main() ``` Explanation In this example: - `Task` is a simple dataclass with `priority` and `description`. - `TaskScheduler` uses a `PriorityQueue` to store and manage tasks. - Multiple threads add tasks to the queue simultaneously. - A separate thread processes the tasks in the order of their priority. Your implementation should be thread-safe and should accurately maintain the order of task processing based on priority.","solution":"import queue import threading from dataclasses import dataclass, field from typing import Any @dataclass(order=True) class Task: priority: int description: str = field(compare=False) class TaskScheduler: def __init__(self): self.queue = queue.PriorityQueue() def add_task(self, task: Task): self.queue.put(task) def process_tasks(self): while not self.queue.empty(): task = self.queue.get() print(f\'Processing task with priority {task.priority}: {task.description}\') self.queue.task_done() def add_tasks(scheduler, tasks): for task in tasks: scheduler.add_task(task) def main(): scheduler = TaskScheduler() tasks1 = [Task(priority=i, description=f\'Task {i}\') for i in range(5)] tasks2 = [Task(priority=i+5, description=f\'Task {i+5}\') for i in range(5)] tasks3 = [Task(priority=i+10, description=f\'Task {i+10}\') for i in range(5)] threads = [] threads.append(threading.Thread(target=add_tasks, args=(scheduler, tasks1))) threads.append(threading.Thread(target=add_tasks, args=(scheduler, tasks2))) threads.append(threading.Thread(target=add_tasks, args=(scheduler, tasks3))) for t in threads: t.start() for t in threads: t.join() processor_thread = threading.Thread(target=scheduler.process_tasks) processor_thread.start() processor_thread.join() if __name__ == \'__main__\': main()"},{"question":"Title: Advanced List and Dictionary Manipulations Objective: Implement a function that processes a list of dictionaries representing students\' grades and outputs a formatted report of students\' performance. Problem Statement: You are provided with a list of dictionaries, where each dictionary contains a student\'s name and their grades in various subjects. Your task is to implement a function `generate_report(students)` that processes this list and returns a summary report as a list of strings. The report should include: 1. A sorted list of students (by name) with their average grade. 2. A list of students who have failed (average grade below 60) along with their subjects and grades. 3. The highest average grade and the student who achieved it. Inputs: - `students`: A list of dictionaries, where each dictionary has the following structure: ```python [ {\\"name\\": \\"Alice\\", \\"grades\\": {\\"math\\": 85, \\"science\\": 92, \\"literature\\": 78}}, {\\"name\\": \\"Bob\\", \\"grades\\": {\\"math\\": 47, \\"science\\": 55, \\"literature\\": 60}}, ... ] ``` Outputs: - A list of strings representing the report, with the specific format and content as described above. Example: ```python [ \\"Student Averages:\\", \\"Alice: 85.0\\", \\"Bob: 54.0\\", ... \\"Failed Students:\\", \\"Robin: {\'math\': 47, \'science\': 55, \'literature\': 60}\\", ... \\"Highest Average Grade:\\", \\"Charlie: 91.0\\" ] ``` Constraints: - Each student\'s grades dictionary may contain different subjects. - The function should handle any number of students. - Your implementation should be efficient in terms of time and space complexity. Function Signature: ```python def generate_report(students: List[Dict[str, Any]]) -> List[str]: ``` Examples: ```python students = [ {\\"name\\": \\"Alice\\", \\"grades\\": {\\"math\\": 85, \\"science\\": 92, \\"literature\\": 78}}, {\\"name\\": \\"Bob\\", \\"grades\\": {\\"math\\": 47, \\"science\\": 55, \\"literature\\": 60}}, {\\"name\\": \\"Charlie\\", \\"grades\\": {\\"math\\": 88, \\"science\\": 90, \\"literature\\": 95}} ] print(generate_report(students)) # Output: # [ # \\"Student Averages:\\", # \\"Alice: 85.0\\", # \\"Bob: 54.0\\", # \\"Charlie: 91.0\\", # \\"Failed Students:\\", # \\"Bob: {\'math\': 47, \'science\': 55, \'literature\': 60}\\", # \\"Highest Average Grade:\\", # \\"Charlie: 91.0\\" # ] ```","solution":"def generate_report(students): # Calculate averages and organize students averages = [] failed_students = [] highest_avg_student = None highest_avg_grade = None for student in students: name = student[\\"name\\"] grades = student[\\"grades\\"] avg_grade = sum(grades.values()) / len(grades) averages.append((name, avg_grade)) if avg_grade < 60: failed_students.append((name, grades)) if highest_avg_grade is None or avg_grade > highest_avg_grade: highest_avg_grade = avg_grade highest_avg_student = name report = [] report.append(\\"Student Averages:\\") for name, avg in sorted(averages, key=lambda x: x[0]): report.append(f\\"{name}: {avg:.1f}\\") report.append(\\"Failed Students:\\") for name, grades in failed_students: report.append(f\\"{name}: {grades}\\") report.append(\\"Highest Average Grade:\\") report.append(f\\"{highest_avg_student}: {highest_avg_grade:.1f}\\") return report"},{"question":"**Coding Assessment Question: Visualization with Seaborn.objects** # Objective Demonstrate your understanding of the `seaborn.objects` module in Seaborn by creating a meaningful and informative visualization. # Task Given the `penguins` dataset from Seaborn, create a visualization that includes the following elements: 1. A scatter plot of `flipper_length_mm` versus `bill_length_mm`, colored by `species`. 2. Include error bars representing one standard deviation for each species. 3. Facet the plot by `island`, creating separate subplots for each island. 4. Customize plot features, ensuring that lines and markers are clearly distinguishable. # Input You will be provided with the `penguins` dataset pre-loaded using the following code: ```python import seaborn.objects as so from seaborn import load_dataset penguins = load_dataset(\\"penguins\\") ``` # Output A seaborn plot object meeting the described requirements. # Constraints - Use only the `seaborn.objects` module to create the visualization. - Ensure that the color coding for `species` is maintained across all facets. - The plot should not include unrelated customization that could clutter the visual. # Example Here is an example code snippet to guide you, which you need to extend to meet all requirements: ```python ( so.Plot(penguins, x=\\"flipper_length_mm\\", y=\\"bill_length_mm\\", color=\\"species\\") .add(so.Dots(), so.Agg(), so.Dodge()) .add(so.Range(), so.Est(errorbar=\\"sd\\"), so.Dodge()) .facet(\\"island\\") .add(so.Line(marker=\\"o\\"), so.Agg()) ) ``` Use this snippet as a starting point and extend it to create the complete visualization. # Additional Information - Make sure to handle any missing data appropriately. - Test your code to ensure that it produces the expected output without any errors.","solution":"import seaborn.objects as so from seaborn import load_dataset # Load dataset penguins = load_dataset(\\"penguins\\") # Create the plot plot = ( so.Plot(penguins, x=\\"flipper_length_mm\\", y=\\"bill_length_mm\\", color=\\"species\\") .add(so.Dots(), so.Dodge()) .add(so.Range(), so.Est(errorbar=\\"sd\\"), so.Dodge()) .facet(\\"island\\") .add(so.Line(marker=\\"o\\"), so.Agg()) ) # Display the plot plot.show()"},{"question":"# Advanced Assignment Handling in Python Problem Description You are required to implement a function called `process_and_reassign` that takes a dictionary as input. This dictionary contains various keys with corresponding values that could be int, float, list, tuple, or even another dictionary. Your function needs to process this dictionary and reassign its contents based on specific rules described below. Assignment Rules: 1. If a value is an integer, increment it by 1. 2. If a value is a float, multiply it by 2. 3. If a value is a string, append \\"-processed\\" to it. 4. If a value is a list, replace the first element with the list\'s length. 5. If a value is a tuple, convert it to a list. 6. If a value is a dictionary, interpret it as another instance requiring the same processing rules recursively. After processing the input dictionary, your function should return the processed dictionary. Input - A dictionary with keys as strings and values as integers, floats, strings, lists, tuples, or dictionaries. Output - A dictionary with processed values as per the rules specified. Constraints - The function should handle nested dictionaries gracefully. - The processing should be done in-place, modifying the original dictionary. Example ```python def process_and_reassign(data): # Your implementation here # Example Usage input_data = { \\"key1\\": 10, \\"key2\\": 2.5, \\"key3\\": \\"value\\", \\"key4\\": [1, 2, 3], \\"key5\\": (4, 5, 6), \\"key6\\": { \\"nestedKey1\\": \\"nestedValue\\", \\"nestedKey2\\": [10, 20, 30] } } expected_output = { \\"key1\\": 11, \\"key2\\": 5.0, \\"key3\\": \\"value-processed\\", \\"key4\\": [3, 2, 3], \\"key5\\": [4, 5, 6], \\"key6\\": { \\"nestedKey1\\": \\"nestedValue-processed\\", \\"nestedKey2\\": [3, 20, 30] } } assert process_and_reassign(input_data) == expected_output ``` Implement `process_and_reassign` in such a way that it meets the outlined specifications and passes the provided example.","solution":"def process_and_reassign(data): Processes a dictionary based on the rules specified and reassigns its contents. Args: data (dict): A dictionary with keys as strings and values as int, float, str, list, tuple, or dict. Returns: dict: A processed dictionary with values modified according to the rules. for key, value in data.items(): if isinstance(value, int): data[key] = value + 1 elif isinstance(value, float): data[key] = value * 2 elif isinstance(value, str): data[key] = value + \\"-processed\\" elif isinstance(value, list): if value: data[key] = [len(value)] + value[1:] elif isinstance(value, tuple): data[key] = list(value) elif isinstance(value, dict): data[key] = process_and_reassign(value) return data"},{"question":"Objective: Demonstrate your understanding and ability to use seaborn\'s `so` objects to create custom plots with jitter transformations. Problem: You are provided with a dataset `penguins` from the seaborn library. This dataset contains information about penguins, including species, body mass, and flipper length. Your task is to create a function that generates a series of plots with different visual transformations using the seaborn `so` objects and proper jitter transformations. Function Signature: ```python def create_penguin_plots(penguins): pass ``` Input: - `penguins`: A pandas DataFrame containing the penguins dataset. Expected Output: - The function does not return anything. Instead, it should generate and display three plots using the seaborn `so` interface: 1. A plot of penguin species vs. body mass with default jitter applied. 2. A plot of body mass vs. species with a `width` jitter of 0.5 applied. 3. A scatter plot of the rounded body mass vs. flipper length using custom jitter values of `x=200` and `y=5`. Constraints: - You must use the seaborn `so` interface for plotting and jitter transformation. - The generated plots should be displayed in the output of the function. Example: Here is an example outline of how the function should be structured. Implement the details as described: ```python import seaborn.objects as so from seaborn import load_dataset def create_penguin_plots(penguins): # Plot 1: Species vs. body mass with default jitter plot1 = ( so.Plot(penguins, \\"species\\", \\"body_mass_g\\") .add(so.Dots(), so.Jitter()) ) plot1.show() # Plot 2: Body mass vs. species with width jitter 0.5 plot2 = ( so.Plot(penguins, \\"body_mass_g\\", \\"species\\") .add(so.Dots(), so.Jitter(.5)) ) plot2.show() # Plot 3: Rounded body mass vs. flipper length with custom jitter x=200 y=5 plot3 = ( so.Plot( penguins[\\"body_mass_g\\"].round(-3), penguins[\\"flipper_length_mm\\"].round(-1) ) .add(so.Dots(), so.Jitter(x=200, y=5)) ) plot3.show() # Example usage penguins = load_dataset(\\"penguins\\") create_penguin_plots(penguins) ``` Note: Ensure seaborn and its dependencies are correctly installed in your environment before executing the function.","solution":"import seaborn as sns import seaborn.objects as so from seaborn import load_dataset def create_penguin_plots(penguins): Generates and displays three plots using seaborn\'s `so` interface with jitter transformations. 1. A plot of penguin species vs. body mass with default jitter applied. 2. A plot of body mass vs. species with a `width` jitter of 0.5 applied. 3. A scatter plot of the rounded body mass vs. flipper length using custom jitter values of `x=200` and `y=5`. Parameters: - penguins: A pandas DataFrame containing the penguins dataset. # Plot 1: Species vs. body mass with default jitter plot1 = ( so.Plot(penguins, x=\\"species\\", y=\\"body_mass_g\\") .add(so.Dots(), so.Jitter()) ) plot1.show() # Plot 2: Body mass vs. species with width jitter of 0.5 plot2 = ( so.Plot(penguins, x=\\"body_mass_g\\", y=\\"species\\") .add(so.Dots(), so.Jitter(0.5)) ) plot2.show() # Plot 3: Rounded body mass vs. flipper length with custom jitter (x=200, y=5) plot3 = ( so.Plot(penguins, x=penguins[\\"body_mass_g\\"].round(-2), y=penguins[\\"flipper_length_mm\\"].round(-1)) .add(so.Dots(), so.Jitter(x=200, y=5)) ) plot3.show() # Example usage penguins = load_dataset(\\"penguins\\") create_penguin_plots(penguins)"},{"question":"# CGI Form Processing with Python You have been tasked with creating a Python CGI script that processes data from an HTML form. The form allows users to submit their personal information, including uploading one or more files. Your script should: 1. Handle the submitted form data safely and effectively. 2. Process multiple input fields and file uploads. 3. Produce a well-structured HTML output displaying the form data and the number of lines in each uploaded file. Requirements: 1. **Input**: Assume the HTML form contains the following fields: - `username` (text) - `email` (text) - `bio` (textarea) - `file1`, `file2`, ..., `fileN` (file inputs) 2. **Output**: - An HTML response displaying the submitted data, including the username, email, and bio. - For each file uploaded, display the file name and the number of lines it contains. Constraints: - You must use the `cgi` module to handle the form data. - Ensure to handle cases where multiple files might not be uploaded. - Make use of the `getfirst()` and `getlist()` methods for form field processing to ensure safe handling of the input data. - Avoid storing the entire uploaded file contents in memory if possible. Performance Requirements: - The solution should handle a reasonable number of file uploads (up to 10 files, each with a size of up to 1 MB). Example: If a user submits the form with: - `username` = \\"John Doe\\" - `email` = \\"johndoe@example.com\\" - `bio` = \\"A long bio text...\\" - `file1` = A file with 10 lines of text - `file2` = A file with 20 lines of text The output should be an HTML page displaying: ```html <h1>Form Data Received</h1> <p>Username: John Doe</p> <p>Email: johndoe@example.com</p> <p>Bio: A long bio text...</p> <h2>Uploaded Files:</h2> <ul> <li>file1.txt: 10 lines</li> <li>file2.txt: 20 lines</li> </ul> ``` # Implementation: Write the CGI script that implements the above requirements. Your script should start by importing the necessary modules and enabling CGI traceback for easier debugging during development. ```python #!/usr/bin/env python3 import cgi import cgitb import os cgitb.enable() print(\\"Content-Type: text/html\\") print() form = cgi.FieldStorage() # Extracting form fields and handling file uploads username = form.getfirst(\\"username\\", \\"N/A\\") email = form.getfirst(\\"email\\", \\"N/A\\") bio = form.getfirst(\\"bio\\", \\"N/A\\") files = form.getlist(\\"file\\") file_info = [] for fileitem in files: if fileitem.file: filename = os.path.basename(fileitem.filename) line_count = 0 while True: line = fileitem.file.readline() if not line: break line_count += 1 file_info.append((filename, line_count)) # Generating HTML output print(\\"<html><head><title>Form Data Received</title></head><body>\\") print(\\"<h1>Form Data Received</h1>\\") print(f\\"<p>Username: {username}</p>\\") print(f\\"<p>Email: {email}</p>\\") print(f\\"<p>Bio: {bio}</p>\\") print(\\"<h2>Uploaded Files:</h2>\\") if file_info: print(\\"<ul>\\") for filename, line_count in file_info: print(f\\"<li>{filename}: {line_count} lines</li>\\") print(\\"</ul>\\") else: print(\\"<p>No files uploaded.</p>\\") print(\\"</body></html>\\") ``` Note: 1. Ensure the script has the correct shebang (`#!/usr/bin/env python3`) and permissions to execute as a CGI script. 2. Test the script on a server with CGI support and debug any issues as they arise.","solution":"#!/usr/bin/env python3 import cgi import cgitb import os cgitb.enable() print(\\"Content-Type: text/html\\") print() form = cgi.FieldStorage() # Extracting form fields and handling file uploads username = form.getfirst(\\"username\\", \\"N/A\\") email = form.getfirst(\\"email\\", \\"N/A\\") bio = form.getfirst(\\"bio\\", \\"N/A\\") files = form.getlist(\\"file\\") file_info = [] for fileitem in files: if fileitem.file: filename = os.path.basename(fileitem.filename) line_count = 0 while True: line = fileitem.file.readline() if not line: break line_count += 1 file_info.append((filename, line_count)) # Generating HTML output print(\\"<html><head><title>Form Data Received</title></head><body>\\") print(\\"<h1>Form Data Received</h1>\\") print(f\\"<p>Username: {username}</p>\\") print(f\\"<p>Email: {email}</p>\\") print(f\\"<p>Bio: {bio}</p>\\") print(\\"<h2>Uploaded Files:</h2>\\") if file_info: print(\\"<ul>\\") for filename, line_count in file_info: print(f\\"<li>{filename}: {line_count} lines</li>\\") print(\\"</ul>\\") else: print(\\"<p>No files uploaded.</p>\\") print(\\"</body></html>\\")"},{"question":"Coding Assessment Question # Objective In this exercise, you are required to demonstrate your understanding of the `http.cookies` module in Python by implementing functions that manage cookies. # Problem Statement You need to implement a class `CookieManager` that allows for the creation and manipulation of cookies. The class should use the `http.cookies` module to handle the cookies and their attributes. # Tasks 1. Implement the `CookieManager` class with the following methods: - `__init__(self, rawdata=None)`: Initializes the `CookieManager`. If `rawdata` is provided, it should be loaded into the internal `SimpleCookie` instance. - `add_cookie(self, name: str, value: str, path: str = None, expires: str = None, max_age: int = None, secure: bool = False, httponly: bool = False, samesite: str = None)`: Adds a cookie with the specified attributes to the cookie jar. - `delete_cookie(self, name: str)`: Deletes a cookie by name. - `get_cookie(self, name: str) -> str`: Retrieves the value of a cookie by name. - `output_cookies(self) -> str`: Returns a string suitable for HTTP headers that represent all cookies in the jar. # Input Specifications - `__init__(self, rawdata=None)`: `rawdata` (optional) can be a string representing HTTP cookie header or a dictionary with cookie names and values. # Output Specifications - `output_cookies(self) -> str` should return a single string containing HTTP header format representations of all current cookies. # Constraints - Cookie attribute `samesite` should be either \\"Strict\\" or \\"Lax\\" if provided. - The `delete_cookie` method should handle non-existing cookie names gracefully, without raising an error. # Example ```python # Initialize the CookieManager manager = CookieManager() # Adding cookies manager.add_cookie(name=\\"session_id\\", value=\\"abc1234\\") manager.add_cookie(name=\\"preferences\\", value=\\"dark_mode=true\\", path=\\"/\\", max_age=3600) # Retrieving cookie value print(manager.get_cookie(\\"session_id\\")) # Output: abc1234 # Deleting a cookie manager.delete_cookie(\\"session_id\\") # Outputting cookies as HTTP headers http_headers = manager.output_cookies() print(http_headers) ``` # Implementation Notes - Use `http.cookies.SimpleCookie` and `http.cookies.Morsel` to manage cookies within the `CookieManager` class. - Ensure that methods handle invalid input and edge cases appropriately, for example, adding a cookie with invalid `samesite` value or retrieving a non-existent cookie.","solution":"from http.cookies import SimpleCookie class CookieManager: def __init__(self, rawdata=None): self.cookie = SimpleCookie() if rawdata: if isinstance(rawdata, str): self.cookie.load(rawdata) elif isinstance(rawdata, dict): for key, value in rawdata.items(): self.cookie[key] = value def add_cookie(self, name: str, value: str, path: str = None, expires: str = None, max_age: int = None, secure: bool = False, httponly: bool = False, samesite: str = None): self.cookie[name] = value if path: self.cookie[name][\'path\'] = path if expires: self.cookie[name][\'expires\'] = expires if max_age: self.cookie[name][\'max-age\'] = max_age if secure: self.cookie[name][\'secure\'] = secure if httponly: self.cookie[name][\'httponly\'] = httponly if samesite and samesite in [\\"Strict\\", \\"Lax\\"]: self.cookie[name][\'samesite\'] = samesite elif samesite: raise ValueError(\\"Invalid samesite value. It should be either \'Strict\' or \'Lax\'\\") def delete_cookie(self, name: str): if name in self.cookie: del self.cookie[name] def get_cookie(self, name: str) -> str: if name in self.cookie: return self.cookie[name].value return None def output_cookies(self) -> str: return self.cookie.output(header=\'\', sep=\';\').strip()"},{"question":"# Semi-Supervised Learning with Scikit-Learn **Objective:** Implement a semi-supervised learning approach using scikit-learn\'s SelfTrainingClassifier to handle a dataset with a mix of labeled and unlabeled data. **Dataset:** You will be given a dataset with features and corresponding labels. Some of the labels will be `-1`, indicating that they are unlabeled. **Task:** 1. Load the dataset from the provided CSV file. 2. Split the dataset into features (`X`) and labels (`y`), treating `-1` as the identifier for unlabeled data. 3. Define a base classifier (e.g., a Logistic Regression model). 4. Implement the SelfTrainingClassifier with the base classifier. 5. Train the SelfTrainingClassifier using the provided dataset. 6. Make predictions on the test set and calculate the accuracy. **Input Format:** - A CSV file `data.csv` where the last column represents labels, and the rest are features. **Output Format:** - Print the accuracy of the model on the test set. **Constraints:** - Use `LogisticRegression` from scikit-learn as the base classifier. - The SelfTrainingClassifier should use the default parameters unless specified otherwise. **Performance Requirements:** - Ensure that the solution runs efficiently for a dataset with up to 10,000 samples. **Bonus Task (Optional):** - Compare the performance of the SelfTrainingClassifier against LabelPropagation and LabelSpreading models using the same dataset. **Example Code Structure:** ```python import pandas as pd import numpy as np from sklearn.linear_model import LogisticRegression from sklearn.semi_supervised import SelfTrainingClassifier from sklearn.model_selection import train_test_split # Additional imports for the bonus task from sklearn.semi_supervised import LabelPropagation, LabelSpreading from sklearn.metrics import accuracy_score # Load dataset data = pd.read_csv(\'data.csv\') X = data.iloc[:, :-1].values y = data.iloc[:, -1].values # Split dataset into training and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Define base classifier base_classifier = LogisticRegression(max_iter=1000) # Implement SelfTrainingClassifier self_training_model = SelfTrainingClassifier(base_classifier) self_training_model.fit(X_train, y_train) # Make predictions y_pred = self_training_model.predict(X_test) # Calculate accuracy accuracy = accuracy_score(y_test, y_pred) print(f\'Accuracy of the SelfTrainingClassifier: {accuracy:.2f}\') # Bonus Task: Compare performance with LabelPropagation and LabelSpreading label_prop_model = LabelPropagation() label_prop_model.fit(X_train, y_train) lp_pred = label_prop_model.predict(X_test) lp_accuracy = accuracy_score(y_test, lp_pred) print(f\'Accuracy of the LabelPropagation: {lp_accuracy:.2f}\') label_spread_model = LabelSpreading() label_spread_model.fit(X_train, y_train) ls_pred = label_spread_model.predict(X_test) ls_accuracy = accuracy_score(y_test, ls_pred) print(f\'Accuracy of the LabelSpreading: {ls_accuracy:.2f}\') ``` **Note:** Ensure to handle any potential issues such as missing labels or convergence warnings in LogisticRegression.","solution":"import pandas as pd import numpy as np from sklearn.linear_model import LogisticRegression from sklearn.semi_supervised import SelfTrainingClassifier from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score def semi_supervised_learning_with_self_training(file_path): # Load dataset data = pd.read_csv(file_path) X = data.iloc[:, :-1].values y = data.iloc[:, -1].values # Split dataset into training and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Define base classifier base_classifier = LogisticRegression(max_iter=1000) # Implement SelfTrainingClassifier self_training_model = SelfTrainingClassifier(base_classifier) self_training_model.fit(X_train, y_train) # Make predictions y_pred = self_training_model.predict(X_test) # Calculate accuracy accuracy = accuracy_score(y_test, y_pred) return accuracy"},{"question":"**Question: Inventory Management System** Design a Python function to manage an inventory of products for a small retail store. The function should allow adding new products, updating existing product information, retrieving product details, generating a report of all products, and deleting products from the inventory. # Specifications 1. **Function Name**: `manage_inventory` 2. **Inputs**: - `operation`: A string indicating the type of operation. This can be one of the following values: `\'add\'`, `\'update\'`, `\'retrieve\'`, `\'report\'`, `\'delete\'`. - `product_id`: An integer representing the unique identifier for the product (for \'add\', \'update\', \'retrieve\', and \'delete\' operations). - `product_info`: A dictionary containing the product details. It is required for \'add\' and \'update\' operations and can have the following keys: - `name` (string): The name of the product. - `quantity` (integer): The quantity of the product in stock. - `price` (float): The price of the product. 3. **Outputs**: - For `\'add\'` operation: Add a new product to the inventory. If a product with the same `product_id` already exists, return `\\"Product already exists.\\"`. - For `\'update\'` operation: Update the existing product\'s information. If the product does not exist, return `\\"Product not found.\\"`. - For `\'retrieve\'` operation: Return the product information as a dictionary. If the product does not exist, return `\\"Product not found.\\"`. - For `\'report\'` operation: Return a list of dictionaries containing all product details. - For `\'delete\'` operation: Remove the product from the inventory. If the product does not exist, return `\\"Product not found.\\"`. 4. **Constraints**: - The inventory should be managed efficiently using appropriate data structures. - Assume there will be no more than 1000 products in the inventory at any time. - The function must handle invalid operations by returning `\\"Invalid operation.\\"`. 5. **Performance**: - The solution should be optimized to handle typical inventory operations within a reasonable time frame. # Example Usage ```python inventory = {} def manage_inventory(operation, product_id=None, product_info=None): # Your implementation here # Adding a new product print(manage_inventory(\'add\', 1, {\'name\': \'Laptop\', \'quantity\': 10, \'price\': 999.99})) # Output: None print(manage_inventory(\'add\', 1, {\'name\': \'Laptop\', \'quantity\': 10, \'price\': 999.99})) # Output: \'Product already exists.\' # Retrieving product information print(manage_inventory(\'retrieve\', 1)) # Output: {\'name\': \'Laptop\', \'quantity\': 10, \'price\': 999.99} print(manage_inventory(\'retrieve\', 2)) # Output: \'Product not found.\' # Updating product information print(manage_inventory(\'update\', 1, {\'name\': \'Gaming Laptop\', \'quantity\': 5, \'price\': 1199.99})) # Output: None print(manage_inventory(\'update\', 2, {\'name\': \'Tablet\', \'quantity\': 15, \'price\': 299.99})) # Output: \'Product not found.\' # Generating a report of all products print(manage_inventory(\'report\')) # Output: [{\'name\': \'Gaming Laptop\', \'quantity\': 5, \'price\': 1199.99}] # Deleting a product print(manage_inventory(\'delete\', 1)) # Output: None print(manage_inventory(\'delete\', 2)) # Output: \'Product not found.\' ``` You are required to write the `manage_inventory` function that satisfies the above requirements.","solution":"inventory = {} def manage_inventory(operation, product_id=None, product_info=None): if operation == \'add\': if product_id in inventory: return \\"Product already exists.\\" inventory[product_id] = product_info elif operation == \'update\': if product_id not in inventory: return \\"Product not found.\\" inventory[product_id].update(product_info) elif operation == \'retrieve\': if product_id not in inventory: return \\"Product not found.\\" return inventory[product_id] elif operation == \'report\': return list(inventory.values()) elif operation == \'delete\': if product_id not in inventory: return \\"Product not found.\\" del inventory[product_id] else: return \\"Invalid operation.\\""},{"question":"# Persistent Data Storage and Retrieval using `pickle` and `shelve` Background: In Python, the `pickle` module is used for serializing and deserializing Python objects. Serialization is the process of converting a Python object into a byte stream, and deserialization is the reverse process. The `shelve` module can store these serialized objects persistently in a dictionary-like format. Problem Statement: You are asked to implement a small library, `PersistentDict`, that uses both `pickle` and `shelve` to store and retrieve Python objects. Your library should provide basic functionalities to add, retrieve, update, and delete objects while maintaining persistence across program restarts. Requirements: 1. Design a class `PersistentDict` that initializes with a file path to the shelve database. 2. Implement the following methods in the class: - `add(key: str, value: Any) -> None`: Adds a new item to the persistent dictionary. - `get(key: str) -> Any`: Retrieves the item associated with the given key. Raise a `KeyError` if the key does not exist. - `update(key: str, value: Any) -> None`: Updates the item associated with the given key. Raise a `KeyError` if the key does not exist. - `remove(key: str) -> None`: Removes the item associated with the given key. Raise a `KeyError` if the key does not exist. - `list_keys() -> list`: Returns a list of all the keys currently stored in the persistent dictionary. Input/Output Specifications: - The `add` method takes a key of type `str` and a value of any type and returns `None`. - The `get` method takes a key of type `str` and returns the corresponding value. - The `update` method takes a key of type `str` and a value of any type and returns `None`. - The `remove` method takes a key of type `str` and returns `None`. - The `list_keys` method returns a list of all keys currently stored. Constraints: 1. The file path provided during the initialization of the `PersistentDict` class must be used to locate the shelve database. 2. The methods should handle exceptions gracefully and ensure the integrity of the database. 3. You may assume that keys are unique and provided as strings. 4. Performance is not a primary concern, but efficiency and correct handling of data types and exceptions are important. Example: ```python # Example usage persist_dict = PersistentDict(\\"my_persistence.db\\") # Adding items persist_dict.add(\\"user1\\", {\\"name\\": \\"Alice\\", \\"age\\": 30}) persist_dict.add(\\"user2\\", {\\"name\\": \\"Bob\\", \\"age\\": 25}) # Retrieving items print(persist_dict.get(\\"user1\\")) # Output: {\'name\': \'Alice\', \'age\': 30} # Updating items persist_dict.update(\\"user1\\", {\\"name\\": \\"Alice\\", \\"age\\": 31}) # Listing keys print(persist_dict.list_keys()) # Output: [\'user1\', \'user2\'] # Removing items persist_dict.remove(\\"user2\\") # Attempting to get a non-existing key try: persist_dict.get(\\"user2\\") except KeyError: print(\\"Key not found\\") # Output: Key not found ``` Note: - Ensure your code handles opening and closing of the shelve database correctly to prevent data corruption. - Consider edge cases such as attempting to update or remove a key that does not exist.","solution":"import shelve class PersistentDict: def __init__(self, file_path: str): self.file_path = file_path def add(self, key: str, value: any) -> None: with shelve.open(self.file_path) as db: db[key] = value def get(self, key: str) -> any: with shelve.open(self.file_path) as db: if key not in db: raise KeyError(f\\"Key \'{key}\' not found\\") return db[key] def update(self, key: str, value: any) -> None: with shelve.open(self.file_path) as db: if key not in db: raise KeyError(f\\"Key \'{key}\' not found\\") db[key] = value def remove(self, key: str) -> None: with shelve.open(self.file_path) as db: if key not in db: raise KeyError(f\\"Key \'{key}\' not found\\") del db[key] def list_keys(self) -> list: with shelve.open(self.file_path) as db: return list(db.keys())"},{"question":"You are given a dataset in the form of a DataFrame `df` which contains daily weather data for multiple cities. The DataFrame has the following columns: - `Date`: The date of the recorded data (in the format \'YYYY-MM-DD\'). - `City`: The name of the city. - `Temperature`: The recorded temperature in Celsius. - `Humidity`: The recorded humidity percentage. - `Precipitation`: The recorded precipitation in mm. Your task is to implement a function `weather_analysis(df: pd.DataFrame) -> pd.DataFrame` that processes this data and returns a summary DataFrame. The summary DataFrame should contain: 1. For each city: - The average temperature. - The maximum temperature and the date it was recorded. - The minimum temperature and the date it was recorded. - The average humidity. - The total precipitation. The format of the summary DataFrame should be: - `City`: The name of the city. - `Average_Temperature`: The average temperature. - `Max_Temperature`: The maximum temperature recorded. - `Max_Temperature_Date`: The date when the maximum temperature was recorded. - `Min_Temperature`: The minimum temperature recorded. - `Min_Temperature_Date`: The date when the minimum temperature was recorded. - `Average_Humidity`: The average humidity. - `Total_Precipitation`: The total precipitation. # Constraints: - Assume that the DataFrame `df` contains at least one entry for every city. - The `Date` column is unique for each city\'s entry. # Example: Given the following input DataFrame: ``` data = { \'Date\': [\'2023-01-01\', \'2023-01-02\', \'2023-01-01\', \'2023-01-02\'], \'City\': [\'New York\', \'New York\', \'Los Angeles\', \'Los Angeles\'], \'Temperature\': [5, -1, 15, 14], \'Humidity\': [80, 85, 60, 59], \'Precipitation\': [0.5, 0.0, 1.0, 0.2] } df = pd.DataFrame(data) ``` Your function should return: ``` City Average_Temperature Max_Temperature Max_Temperature_Date Min_Temperature Min_Temperature_Date Average_Humidity Total_Precipitation 0 New York 2.0 5 2023-01-01 -1 2023-01-02 82.5 0.5 1 Los Angeles 14.5 15 2023-01-01 14 2023-01-02 59.5 1.2 ``` # Implementation: ```python def weather_analysis(df: pd.DataFrame) -> pd.DataFrame: # Write your implementation here pass ``` # Note: - Properly handle the date format to ensure the correct sorting and retrieval of min and max temperature dates. - Utilize pandas aggregation, transformation, and descriptive statistics functions as necessary.","solution":"import pandas as pd def weather_analysis(df: pd.DataFrame) -> pd.DataFrame: summary = df.groupby(\'City\').agg( Average_Temperature=(\'Temperature\', \'mean\'), Max_Temperature=(\'Temperature\', \'max\'), Max_Temperature_Date=(\'Date\', lambda x: x[df.loc[x.index, \'Temperature\'].idxmax()]), Min_Temperature=(\'Temperature\', \'min\'), Min_Temperature_Date=(\'Date\', lambda x: x[df.loc[x.index, \'Temperature\'].idxmin()]), Average_Humidity=(\'Humidity\', \'mean\'), Total_Precipitation=(\'Precipitation\', \'sum\') ).reset_index() return summary"},{"question":"You are provided with a neural network model written in PyTorch and a dataset of input features. Your task is to write a Python script that: 1. Checks if the MPS backend is available on the current device. 2. Moves the neural network model to the MPS device if it is available; otherwise, move the model to the CPU. 3. Creates input tensors using the provided dataset and moves them to the appropriate device (MPS or CPU). 4. Performs predictions using the neural network model on the input tensors. 5. Returns the predictions. Requirements: - You should define a function `run_inference(model, data)` where: - `model` is a PyTorch neural network model. - `data` is a list of input features. - The function returns the predictions as a tensor. - Ensure you handle the case where MPS is not available by falling back to the CPU. - Make sure to document any assumptions you make. Example Usage: ```python import torch import torch.nn as nn # Example neural network model class SimpleNet(nn.Module): def __init__(self): super(SimpleNet, self).__init__() self.linear = nn.Linear(10, 1) def forward(self, x): return self.linear(x) # Example data data = [[0.5] * 10, [0.1] * 10, [0.3] * 10] # Instantiate the model model = SimpleNet() # Implement the function def run_inference(model, data): # Check if MPS is available if torch.backends.mps.is_available(): device = torch.device(\\"mps\\") else: device = torch.device(\\"cpu\\") # Move model to appropriate device model.to(device) # Convert data to tensor and move to device input_tensor = torch.tensor(data, dtype=torch.float32).to(device) # Perform prediction model.eval() with torch.no_grad(): predictions = model(input_tensor) return predictions # Get predictions predictions = run_inference(model, data) print(predictions) ``` Make sure your solution properly handles the case where MPS is not available and uses the CPU instead.","solution":"import torch import torch.nn as nn def run_inference(model, data): Run inference using the given PyTorch model and dataset. Parameters: - model (nn.Module): A PyTorch neural network model. - data (list of list of floats): A dataset consisting of input features. Returns: - torch.Tensor: Predictions as a tensor. # Check if MPS is available if torch.backends.mps.is_available() and torch.device(\\"mps\\"): device = torch.device(\\"mps\\") else: device = torch.device(\\"cpu\\") # Move model to appropriate device model.to(device) # Convert data to tensor and move to device input_tensor = torch.tensor(data, dtype=torch.float32).to(device) # Perform prediction model.eval() with torch.no_grad(): predictions = model(input_tensor) return predictions # Example neural network model class SimpleNet(nn.Module): def __init__(self): super(SimpleNet, self).__init__() self.linear = nn.Linear(10, 1) def forward(self, x): return self.linear(x) # Example data data = [[0.5] * 10, [0.1] * 10, [0.3] * 10] # Instantiate the model model = SimpleNet() # Get predictions predictions = run_inference(model, data) print(predictions)"},{"question":"You are tasked with creating a Python function to analyze the Python configuration and environment settings for the current system. This function should extract specific pieces of information and present them in a formatted output. Problem Statement Implement a function `analyze_python_config()` that does the following: 1. Retrieve and display the current platform. 2. Retrieve and display the Python version in \\"MAJOR.MINOR\\" format. 3. Retrieve and display the current installation scheme for the platform. 4. Retrieve and display all supported schemes and their corresponding standard library paths (`stdlib`). 5. Retrieve and display a specific set of configuration variables: `AR`, `CXX`, `LIBDIR`. Input The function takes no input parameters. Output The function should return a dictionary containing: - `platform`: A string representing the current platform. - `python_version`: A string representing the Python version in \\"MAJOR.MINOR\\" format. - `current_scheme`: A string representing the current installation scheme. - `schemes_stdlib`: A dictionary where keys are scheme names and values are corresponding `stdlib` paths. - `config_vars`: A dictionary containing the specified configuration variables (`AR`, `CXX`, `LIBDIR`). Example output: ```python { \\"platform\\": \\"linux-x86_64\\", \\"python_version\\": \\"3.10\\", \\"current_scheme\\": \\"posix_prefix\\", \\"schemes_stdlib\\": { \\"posix_prefix\\": \\"/usr/local/lib/python3.10\\", \\"posix_home\\": \\"/home/user/.local/lib/python3.10\\", ... }, \\"config_vars\\": { \\"AR\\": \\"ar\\", \\"CXX\\": \\"g++\\", \\"LIBDIR\\": \\"/usr/local/lib\\" } } ``` Constraints - Ensure that your solution handles the possibility of missing configuration variables by setting their values to `None` if they are not found. - Use the `sysconfig` module for retrieving all information. Performance Requirements - The function should execute efficiently within reasonable limits given the typical performance capabilities of Python. # Function Signature ```python def analyze_python_config() -> dict: pass ```","solution":"import sys import sysconfig def analyze_python_config(): Analyze the Python configuration and environment settings. # Current platform current_platform = sys.platform # Python version in MAJOR.MINOR format python_version = f\\"{sys.version_info.major}.{sys.version_info.minor}\\" # Current installation scheme current_scheme = sysconfig.get_default_scheme() # All supported schemes and their corresponding standard library paths schemes_stdlib = {} for scheme in sysconfig.get_scheme_names(): schemes_stdlib[scheme] = sysconfig.get_path(\'stdlib\', scheme) # Specific configuration variables config_vars = {var: sysconfig.get_config_var(var) for var in [\'AR\', \'CXX\', \'LIBDIR\']} return { \\"platform\\": current_platform, \\"python_version\\": python_version, \\"current_scheme\\": current_scheme, \\"schemes_stdlib\\": schemes_stdlib, \\"config_vars\\": config_vars }"},{"question":"**Challenging Coding Assessment Question:** # Implementation of a Robust FTP Client with Error Handling and Directory Synchronization **Objective:** You need to implement a class `RobustFTPClient` that utilizes the `ftplib.FTP` or `ftplib.FTP_TLS` class to perform FTP operations. Your implementation should demonstrate understanding of connecting to an FTP server, navigating directories, uploading and downloading files, and handling specific errors. Additionally, the class should provide a method to synchronize a local directory with a remote directory. **Details:** 1. **Class Definition:** - Class Name: `RobustFTPClient` - Constructor Parameters: - `host`: FTP server address (string). - `user`: Username for login (string, default is \'anonymous\'). - `passwd`: Password for login (string, default is \'\'). - `secure`: Boolean indicating whether to use `FTP_TLS` (default is `False`). 2. **Methods:** - `connect()`: Connects to the FTP server and logs in. - `disconnect()`: Properly disconnects from the FTP server. - `list_files(remote_path)`: Lists files in the given remote directory. - `upload_file(local_file_path, remote_file_path)`: Uploads a file from the local path to the remote path. - `download_file(remote_file_path, local_file_path)`: Downloads a file from the remote path to the local path. - `synchronize_directory(local_dir, remote_dir)`: Synchronizes the files between the local directory and the remote directory. - `handle_errors(error)`: Custom error handler for FTP-specific errors. **Constraints:** - Handle all FTP-specific exceptions using a custom error handler. - Use context managers where applicable to ensure clean resource management. - Ensure that both text and binary file transfers are supported. - Maintain synchronization between the local and remote directories such that new and updated files in the local directory are uploaded to the remote directory. **Example Usage:** ```python ftp_client = RobustFTPClient(host=\'ftp.example.com\', user=\'user\', passwd=\'password\', secure=True) # Connect to the server ftp_client.connect() # List files in the remote directory print(ftp_client.list_files(\'/remote/directory\')) # Upload a file ftp_client.upload_file(\'/local/path/to/file.txt\', \'/remote/directory/file.txt\') # Download a file ftp_client.download_file(\'/remote/directory/file.txt\', \'/local/path/to/file.txt\') # Synchronize a local directory with a remote directory ftp_client.synchronize_directory(\'/local/directory\', \'/remote/directory\') # Disconnect from the server ftp_client.disconnect() ``` **Expected Output & Behavior:** - Proper connection and disconnection logs. - Accurate listing of files in the specified directory. - Correct upload and download of files with appropriate handling of both text and binary modes. - Synchronization should ensure that all files present locally are uploaded to the remote server with correct handling of updated files. - Appropriate error messages should be displayed if any FTP-specific error occurs using the `handle_errors` method. **Note:** You may use the `ftplib.FTP` documentation provided to understand the various methods and attributes available for implementation.","solution":"import ftplib import os class RobustFTPClient: def __init__(self, host, user=\'anonymous\', passwd=\'\', secure=False): self.host = host self.user = user self.passwd = passwd self.secure = secure self.ftp = ftplib.FTP_TLS() if secure else ftplib.FTP() def connect(self): try: self.ftp.connect(self.host) self.ftp.login(user=self.user, passwd=self.passwd) print(f\\"Connected to {self.host}\\") except ftplib.all_errors as e: self.handle_errors(e) def disconnect(self): try: self.ftp.quit() print(f\\"Disconnected from {self.host}\\") except ftplib.all_errors as e: self.handle_errors(e) def list_files(self, remote_path): try: return self.ftp.nlst(remote_path) except ftplib.all_errors as e: self.handle_errors(e) def upload_file(self, local_file_path, remote_file_path): try: with open(local_file_path, \'rb\') as file: self.ftp.storbinary(f\'STOR {remote_file_path}\', file) print(f\\"Uploaded {local_file_path} to {remote_file_path}\\") except ftplib.all_errors as e: self.handle_errors(e) except FileNotFoundError: print(f\\"Local file {local_file_path} not found.\\") def download_file(self, remote_file_path, local_file_path): try: with open(local_file_path, \'wb\') as file: self.ftp.retrbinary(f\'RETR {remote_file_path}\', file.write) print(f\\"Downloaded {remote_file_path} to {local_file_path}\\") except ftplib.all_errors as e: self.handle_errors(e) def synchronize_directory(self, local_dir, remote_dir): try: for root, _, files in os.walk(local_dir): for file in files: local_file_path = os.path.join(root, file) remote_file_path = os.path.join(remote_dir, os.path.relpath(local_file_path, local_dir)).replace(\'\', \'/\') self.upload_file(local_file_path, remote_file_path) print(f\\"Synchronized {local_dir} with {remote_dir}\\") except ftplib.all_errors as e: self.handle_errors(e) def handle_errors(self, error): print(f\\"FTP error: {error}\\")"},{"question":"**Python C API Challenge: Instance and Method Object Handling** In this challenge, you are required to create a Python class to simulate the behavior of instance methods and method objects using the Python C API. You\'ll need to utilize concepts provided in the documentation to bind functions to instance methods and class methods, and to retrieve associated functions and instances. # Problem Statement Implement a class `MethodManager` that manages instance methods and class methods. Your implementation should include the following: 1. **Initialization**: - A constructor that takes no arguments and initializes internal storage for methods. 2. **Add Instance Method (`add_instance_method`)**: - A method that takes two arguments: a callable `func` and an object `instance`. - It should create and store a new instance method object using `PyInstanceMethod_New`. 3. **Add Method (`add_method`)**: - A method that takes two arguments: a callable `func` and an object `self`. - It should create and store a new method object using `PyMethod_New`. 4. **Get Function (`get_function`)**: - A method that takes a method object (either instance or class method) and returns its associated callable. 5. **Get Instance (`get_instance`)**: - A method that takes a method object (either instance or class method) and returns the associated instance. # Constraints - You can assume that the methods will only be added once and not modified afterward. - The `func` parameters for `add_instance_method` and `add_method` will always be callable. - The method objects passed into `get_function` and `get_instance` will always be valid. # Example Usage ```python def example_func(): print(\\"This is an example function.\\") class ExampleClass: pass example_instance = ExampleClass() # Create MethodManager manager = MethodManager() # Add an instance method manager.add_instance_method(example_func, example_instance) # Add a class method manager.add_method(example_func, ExampleClass) # Retrieve the function func = manager.get_function(meth) print(func) # Should output a reference to example_func # Retrieve the instance of the method instance = manager.get_instance(meth) print(instance) # Should output a reference to example_instance ``` # Evaluation Criteria Your solution will be evaluated based on: - Correct implementation of the specified functionalities. - Proper use of Python C API functions as described in the documentation. - Handling of edge cases where necessary. Happy coding!","solution":"class MethodManager: def __init__(self): self.methods = {} def add_instance_method(self, func, instance): method_key = f\\"{id(func)}_{id(instance)}\\" self.methods[method_key] = {\\"func\\": func, \\"instance\\": instance, \\"type\\": \\"instance\\"} def add_method(self, func, self_instance): method_key = f\\"{id(func)}_{id(self_instance)}\\" self.methods[method_key] = {\\"func\\": func, \\"instance\\": self_instance, \\"type\\": \\"method\\"} def get_function(self, method): method_key = f\\"{id(method[\'func\'])}_{id(method[\'instance\'])}\\" return self.methods[method_key][\\"func\\"] def get_instance(self, method): method_key = f\\"{id(method[\'func\'])}_{id(method[\'instance\'])}\\" return self.methods[method_key][\\"instance\\"]"},{"question":"# Question: Managing Environment Variables with Subprocesses You are required to create a Python function that manages environment variables for a subprocess using the `os` module. Your function should: 1. Accept a dictionary of environment variables to be added or updated. 2. Launch a subprocess using a command provided, ensuring the environment variables are correctly set for this subprocess. 3. Return the output of the subprocess. Function Signature: ```python def manage_subprocess_env(env_updates: dict, command: str) -> str: ``` Input: - `env_updates` (dict): A dictionary of environment variables to be added or updated. Keys and values are strings. - `command` (str): A command to be executed by the subprocess. Output: - Returns a string: The output of the subprocess. Constraints: - The command passed to the subprocess will always be a valid shell command. - The environment variables should be correctly set for the subprocess only, not affecting the global environment variables of the running Python process. Example: ```python assert manage_subprocess_env({\'MY_VAR\': \'123\'}, \'echo MY_VAR\') == \'123n\' ``` Notes: - Make sure to use the `os` module to handle environment variables. - You may use the `subprocess` module to handle the spawning of the subprocess. - Remember to handle any exceptions that might occur during subprocess execution and provide an informative error message.","solution":"import subprocess import os def manage_subprocess_env(env_updates, command): Run a subprocess with updated environment variables. Args: env_updates (dict): A dictionary of environment variables to be added or updated. command (str): Command to be executed. Returns: str: The output of the subprocess. # Copy current environment variables and update with new values env = os.environ.copy() env.update(env_updates) try: # Run the subprocess with the updated environment result = subprocess.run(command, shell=True, env=env, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True) return result.stdout except subprocess.CalledProcessError as e: return f\\"Error: {e.stderr}\\""},{"question":"# Email Handling with the Python `email` Package **Objective:** Demonstrate your understanding of constructing and parsing email messages using the Python `email` package. **Problem Statement:** You need to implement a function `create_and_parse_email` that: 1. Constructs an email message given certain parameters. 2. Serializes this email message into a byte stream. 3. Parses the byte stream back into an `EmailMessage` object. 4. Returns the original email\'s subject and the parsed email\'s subject to verify the correctness. The function should work as follows: - **Input:** - `subject`: A string representing the subject of the email. - `sender`: A string representing the sender\'s email address. - `recipient`: A string representing the recipient\'s email address. - `body`: A string representing the body of the email. - **Output:** A tuple containing: - The subject of the originally created email. - The subject of the parsed email to ensure it matches the original. # Constraints: - Assume the input strings are all valid, and the function does not need to handle invalid formats. - Focus on using the `EmailMessage` object along with the parser and generator from the `email` package. # Example: ```python def create_and_parse_email(subject: str, sender: str, recipient: str, body: str) -> tuple: # Your implementation here # Sample Input subject = \\"Test Email\\" sender = \\"sender@example.com\\" recipient = \\"recipient@example.com\\" body = \\"This is a test email.\\" # Expected Output (\\"Test Email\\", \\"Test Email\\") ``` # Additional Notes: - Utilize the `email.message.EmailMessage` class to construct the email. - Serialize the constructed `EmailMessage` using the appropriate generator to convert it to a byte stream. - Parse the generated byte stream back into an `EmailMessage` using the provided parser. - Ensure that the email\'s subject remains consistent before and after serialization/parsing. Good luck!","solution":"from email.message import EmailMessage from email import policy from email.parser import BytesParser from email.generator import BytesGenerator from io import BytesIO def create_and_parse_email(subject: str, sender: str, recipient: str, body: str) -> tuple: # Create the EmailMessage object msg = EmailMessage() msg[\'Subject\'] = subject msg[\'From\'] = sender msg[\'To\'] = recipient msg.set_content(body) # Serialize the EmailMessage into a byte stream byte_stream = BytesIO() BytesGenerator(byte_stream, policy=policy.default).flatten(msg) # Get the byte stream content as bytes byte_stream_bytes = byte_stream.getvalue() # Parse the byte stream back into an EmailMessage object parsed_msg = BytesParser(policy=policy.default).parsebytes(byte_stream_bytes) # Return the original subject and the parsed subject return (msg[\'Subject\'], parsed_msg[\'Subject\'])"},{"question":"Objective Demonstrate your understanding of the `fcntl` module for file control and I/O control operations, including file locking mechanisms. Problem Statement Write a Python function `manage_file_locks` that performs the following operations on a specified file: 1. Opens the file with the specified mode (read, write, or append). 2. Applies a shared lock to the first `N` bytes of the file where `N` is provided as a function parameter. 3. Reads these `N` bytes and returns them. 4. Attempts to apply an exclusive lock to the entire file: - If the lock is acquired, writes a specified string to the file beginning at the end of the file. - If the lock is not acquired due to it being held by another process, an appropriate message should be returned. 5. Releases all locks on the file and closes it. Function Signature ```python def manage_file_locks(file_path: str, mode: str, n: int, write_data: str) -> str: pass ``` Input - `file_path`: A string representing the path to the file. - `mode`: A string representing the mode to open the file with. It can be \'r\' for read, \'w\' for write, and \'a\' for append. - `n`: An integer representing the number of bytes from the beginning of the file to lock and read. - `write_data`: A string containing the data to write to the file if an exclusive lock is acquired. Output - A string representing the `N` bytes read from the beginning of the file if the shared lock is successfully applied. - If the exclusive lock is not acquired, return a message indicating the failure due to another process holding the lock. Constraints - You may assume the file exists and the specified mode is valid. - Handle any exceptions that might occur and ensure the file is properly closed regardless of the outcome. - Use appropriate constants and functions from the `fcntl` module. Example ```python # Example usage result = manage_file_locks(\\"/path/to/file.txt\\", \\"r+\\", 100, \\"This is new data to be written.\\") print(result) # Output will vary based on the lock state of the file. ``` Notes - The function should be efficient and handle file operations correctly to prevent data corruption. - Consider edge cases such as attempting to lock more bytes than the size of the file.","solution":"import fcntl import os def manage_file_locks(file_path: str, mode: str, n: int, write_data: str) -> str: try: with open(file_path, mode) as f: # Apply shared lock to the first N bytes lock = fcntl.flock(f, fcntl.LOCK_SH) data = f.read(n) fcntl.flock(f, fcntl.LOCK_UN) # Attempt to apply exclusive lock to the entire file try: fcntl.flock(f, fcntl.LOCK_EX | fcntl.LOCK_NB) f.seek(0, os.SEEK_END) f.write(write_data) fcntl.flock(f, fcntl.LOCK_UN) return data except IOError: return \\"Failed to acquire exclusive lock; another process is holding the lock.\\" except Exception as e: return str(e)"},{"question":"# Question: In this assignment, you will demonstrate your understanding of Python\'s profiling tools by analyzing the performance of a given function and identifying the top performance bottlenecks. Task: 1. **Function Implementation**: - Implement a function named `analyze_performance` which takes two arguments: - `func`: A callable (the function to be profiled). - `args`: A tuple of arguments to be passed to `func`. ```python def analyze_performance(func, args): # Your code here ``` 2. **Profiling & Analysis**: - Use the `cProfile` module to profile the execution of `func` with the provided `args`. - Save the profiling data to a file named `\'profile_stats\'`. - Use the `pstats` module to read and analyze the saved profiling data. - Identify and return the top 5 function calls that consume the most cumulative time. 3. **Output Format**: - The function should return a list of tuples. Each tuple should contain: - The function name. - The total cumulative time spent in the function. - The list should be ordered by cumulative time in descending order. Example: ```python import time def example_function(x, y): time.sleep(x) time.sleep(y) return x + y result = analyze_performance(example_function, (1, 2)) print(result) # Expected output format: [(\'example_function\', <cumulative_time>), ...] ``` Constraints: - Ensure the `analyze_performance` function runs the profiling with minimal overhead. - Use the built-in `cProfile` methods effectively to gather profiling data and `pstats` to sort and filter the most time-consuming functions. - You may assume `func` is a well-defined function and `args` correctly match the function signature. Evaluation Criteria: - Correctness: The function correctly profiles the provided `func`. - Efficiency: Profiling should have minimal impact on the execution of `func`. - Clarity: The code should be well-written and documented. Note: This problem assumes familiarity with Python’s profiling tools (`cProfile` and `pstats`). Ensure that your solution not only correctly implements the profiling but also provides insights into the performance characteristics of the provided function.","solution":"import cProfile import pstats from io import StringIO def analyze_performance(func, args): Analyzes the performance of a given function with provided arguments. Parameters: func (callable): The function to be profiled. args (tuple): The arguments to be passed to the function. Returns: list of tuple: A list containing the top 5 function calls by cumulative time, each tuple contains the function name and cumulative time. # Profile the execution of the function profiler = cProfile.Profile() profiler.enable() # Run the function with the provided arguments func(*args) profiler.disable() # Save the profile data to a StringIO object instead of a file s = StringIO() ps = pstats.Stats(profiler, stream=s).sort_stats(pstats.SortKey.CUMULATIVE) ps.print_stats() # Parse the profiling data to get the top 5 function calls by cumulative time stats_lines = s.getvalue().splitlines() # Filter out the lines with function call stats (this is a more manual parsing approach) stats_data = [] for line in stats_lines[5:]: # Skip the header lines if \'function calls\' in line or line.strip() == \'\': continue parts = line.split() if len(parts) < 6: continue function_name = parts[-1] cumulative_time = float(parts[3]) stats_data.append((function_name, cumulative_time)) # Sort the collected data by cumulative time in descending order and get the top 5 entries top_stats = sorted(stats_data, key=lambda x: x[1], reverse=True)[:5] return top_stats"},{"question":"Objective: To assess your understanding and ability to implement various types of error bars using the seaborn library in Python as described in the provided documentation. Problem Statement: You are given a dataset that contains the weights of strawberries (in grams) from different farms over multiple years. You need to use seaborn to visualize the data with appropriate error bars to show both the spread of the data and the estimate uncertainty. Instructions: 1. Load the dataset from the given CSV file. 2. Create an aggregated plot that shows the mean weight of strawberries from each farm over the years with error bars representing standard deviation. 3. Create a second plot showing the confidence interval of the mean weight of strawberries from each farm over the years. 4. Create a third plot using regression lines to show the trend in weights over the years for each farm with a confidence interval band. Dataset: The dataset can be represented as follows (you will get the filename `strawberries.csv`): ``` Year,Farm,Weight 2018,Farm_1,30 2018,Farm_2,45 2019,Farm_1,50 ... and so on ``` Function Signature: ```python import pandas as pd def visualize_strawberry_weights(file_path: str): Given the file path of the CSV containing strawberry weights data, visualize the mean weight with standard deviation error bars and confidence intervals, and show trend lines with confidence intervals. :param file_path: str: The file path to the CSV file containing the strawberry weight data. :return: None pass ``` Requirements: - Ensure that you use seaborn for all visualizations. - The first plot should use error bars to show the standard deviation of weights. - The second plot should use error bars to show the 95% confidence interval of the mean weights. - The third plot should display regression lines with confidence interval bands for the yearly trend per farm. - Your plots should have appropriate labels and titles for clarity. - Use matplotlib.pyplot for any necessary customizations. Constraints: - You should use seaborn version 0.12 or later. - Handle any missing data appropriately within your visualizations. Example Output: There are no explicit assertions, but your visualizations should clearly show the different scales and error bars as described in the problem statement.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def visualize_strawberry_weights(file_path: str): Given the file path of the CSV containing strawberry weights data, visualize the mean weight with standard deviation error bars and confidence intervals, and show trend lines with confidence intervals. :param file_path: str: The file path to the CSV file containing the strawberry weight data. :return: None # Load the dataset data = pd.read_csv(file_path) # Set the seaborn theme sns.set_theme(style=\\"whitegrid\\") # Plot 1: Mean weight with standard deviation error bars plt.figure(figsize=(10, 6)) bar_plot = sns.barplot(data=data, x=\\"Year\\", y=\\"Weight\\", hue=\\"Farm\\", ci=\\"sd\\") bar_plot.set_title(\\"Mean Strawberry Weight with Standard Deviation Error Bars\\") bar_plot.set_ylabel(\\"Weight (grams)\\") plt.show() # Plot 2: Mean weight with 95% confidence interval error bars plt.figure(figsize=(10, 6)) bar_plot_ci = sns.barplot(data=data, x=\\"Year\\", y=\\"Weight\\", hue=\\"Farm\\", ci=95) bar_plot_ci.set_title(\\"Mean Strawberry Weight with 95% Confidence Interval Error Bars\\") bar_plot_ci.set_ylabel(\\"Weight (grams)\\") plt.show() # Plot 3: Regression lines with confidence interval band plt.figure(figsize=(10, 6)) reg_plot = sns.lmplot(data=data, x=\\"Year\\", y=\\"Weight\\", hue=\\"Farm\\", height=6, aspect=1.5, ci=95) reg_plot.set_axis_labels(\\"Year\\", \\"Weight (grams)\\") reg_plot.fig.suptitle(\\"Regression Trend of Strawberry Weights Over Years\\", y=1.03) plt.show()"},{"question":"**Priority Task Scheduler** You are tasked with implementing a priority task scheduler using the `heapq` module from Python. This scheduler will support adding tasks with priorities, removing tasks, updating task priorities, and retrieving the highest priority task. Tasks with the same priority should be handled in the order they were added. Implement a class `TaskScheduler` with the following methods: 1. `add_task(task: str, priority: int) -> None`: Adds a new task with the specified priority. If a task with the same name already exists, update its priority. 2. `remove_task(task: str) -> None`: Removes the task from the scheduler. If the task does not exist, raise a `KeyError`. 3. `pop_task() -> str`: Removes and returns the task with the highest priority from the scheduler. If the scheduler is empty, raise a `KeyError`. 4. `get_task() -> str`: Returns the task with the highest priority without removing it from the scheduler. If the scheduler is empty, raise a `KeyError`. Example: ```python scheduler = TaskScheduler() scheduler.add_task(\\"write code\\", 5) scheduler.add_task(\\"release product\\", 7) scheduler.add_task(\\"write spec\\", 1) scheduler.add_task(\\"create tests\\", 3) assert scheduler.pop_task() == \\"write spec\\" scheduler.remove_task(\\"create tests\\") scheduler.add_task(\\"create tests\\", 2) assert scheduler.get_task() == \\"write code\\" assert scheduler.pop_task() == \\"write code\\" ``` **Constraints:** - Task names are unique strings. - Priorities are integers. - The scheduler must maintain the heap property and the tasks must be returned in priority order. Your implementation should handle the above operations efficiently, with special attention to maintaining the heap invariant where necessary. **Requirements:** - You must use the `heapq` module for the heap operations. - Ensure your implementation uses dictionary lookups to handle task updates and removals efficiently. **Note:** Handle the situation where task priorities might need adjustment or tasks might need removal by using appropriate strategies that do not break the heap structure.","solution":"import heapq class TaskScheduler: def __init__(self): self.task_heap = [] self.task_map = {} self.entry_finder = {} self.REMOVED_TASK = \'<removed-task>\' self.counter = 0 def add_task(self, task: str, priority: int) -> None: if task in self.entry_finder: self.remove_task(task) count = self.counter entry = [priority, count, task] self.entry_finder[task] = entry heapq.heappush(self.task_heap, entry) self.counter += 1 def remove_task(self, task: str) -> None: entry = self.entry_finder.pop(task) entry[-1] = self.REMOVED_TASK def pop_task(self) -> str: while self.task_heap: priority, count, task = heapq.heappop(self.task_heap) if task != self.REMOVED_TASK: del self.entry_finder[task] return task raise KeyError(\\"pop from an empty priority queue\\") def get_task(self) -> str: while self.task_heap: priority, count, task = self.task_heap[0] if task != self.REMOVED_TASK: return task heapq.heappop(self.task_heap) raise KeyError(\\"get from an empty priority queue\\")"},{"question":"**Title:** Asynchronous Task Processing with Priority Queue **Problem Statement:** You are tasked with implementing an asynchronous task scheduler using `asyncio.PriorityQueue`. The scheduler will manage a set of tasks with varying priorities and ensure that higher priority tasks are processed before lower priority ones. Each task is represented as a tuple with a priority level and the time it takes to execute the task. **Requirements:** 1. Implement an asynchronous function `task_scheduler(tasks: List[Tuple[int, float]])` where `tasks` is a list of tuples. Each tuple contains two elements: - `priority`: An integer representing the priority of the task (lower values indicate higher priority). - `duration`: A float representing the time in seconds the task takes to execute. 2. The function should create a `PriorityQueue`, enqueue all tasks, and process them in order of their priority. 3. Use at least three worker coroutines to process the tasks concurrently. Each worker should: - Get a task from the queue. - \\"Execute\\" the task by using `await asyncio.sleep(duration)`. - Mark the task as done once completed. 4. The `task_scheduler` function should: - Ensure all tasks are processed before returning. - Measure the total time taken to process all tasks and print it. **Input:** - `tasks`: A list of tuples where each tuple contains: - `priority` (integer): The task priority (lower value means higher priority). - `duration` (float): The time in seconds the task will take to execute. **Output:** - Print the total time taken to process all tasks. **Example:** ```python import asyncio from typing import List, Tuple async def task_scheduler(tasks: List[Tuple[int, float]]): # Your implementation goes here # Example Usage tasks = [(1, 2.5), (3, 1.0), (2, 3.0), (4, 0.5)] asyncio.run(task_scheduler(tasks)) ``` Expected printed output (will vary based on task timings): ``` Total time taken to process all tasks: X.XX seconds ``` **Constraints:** - The number of tasks will be <= 100. - The maximum duration for any task will be <= 10 seconds. - Assume a maximum of 4 workers can be handling tasks concurrently. **Notes:** - Use the `asyncio.PriorityQueue` class. - Ensure proper handling of async methods and synchronization between tasks.","solution":"import asyncio from typing import List, Tuple import time async def worker(queue: asyncio.PriorityQueue): while True: priority, duration = await queue.get() await asyncio.sleep(duration) queue.task_done() async def task_scheduler(tasks: List[Tuple[int, float]]): queue = asyncio.PriorityQueue() for task in tasks: await queue.put(task) start_time = time.time() workers = [asyncio.create_task(worker(queue)) for _ in range(3)] await queue.join() for w in workers: w.cancel() total_time = time.time() - start_time print(f\\"Total time taken to process all tasks: {total_time:.2f} seconds\\")"},{"question":"**Objective:** Utilize the scikit-learn `neighbors` module to implement a custom nearest neighbor search and employ it to perform both classification and a novel dimensionality reduction technique. # Problem Statement You are provided with a dataset containing numerical features and corresponding labels. Your task is to write code that performs the following tasks: 1. Implement a customized nearest neighbor search using the KDTree algorithm. 2. Classify the data using k-nearest neighbors classification. 3. Perform supervised dimensionality reduction using Neighborhood Components Analysis (NCA). 4. Visualize the results of the dimensionality reduction. # Requirements 1. **Input Format:** - A 2D NumPy array `X` of shape `(n_samples, n_features)` representing the features. - A 1D NumPy array `y` of shape `(n_samples, )` representing the labels. - An integer `k` representing the number of neighbors. 2. **Implementation Steps:** - **Step 1:** Implement and fit a KDTree on the provided dataset `X`. - Function: `def create_kdtree(X):` - Input: 2D NumPy array `X` - Output: KDTree object - **Step 2:** Classify the data using the k-nearest neighbors classifier. - Function: `def knn_classification(X_train, y_train, X_test, k):` - Input: Training data `X_train`, training labels `y_train`, test data `X_test`, integer `k` - Output: Predictions on `X_test` - **Step 3:** Perform NCA for dimensionality reduction. - Function: `def apply_nca(X, y, n_components):` - Input: Data `X`, labels `y`, integer `n_components` - Output: Data reduced to `n_components` dimensions - **Step 4:** Visualize the 2D projection of the data obtained from NCA. - Function: `def plot_nca_2d(X_nca, y):` - Input: 2D projection of data `X_nca`, labels `y` - Output: Scatter plot 3. **Constraints:** - Use Euclidean distance for the KDTree. - Ensure that `k` is a positive integer less than or equal to the number of samples. - The output of the functions should match the expected format. # Performance Requirements - Your implementation should be efficient and leverage the KDTree structure for faster searches. - Verify your results by using the `accuracy_score` metric from `sklearn.metrics` for the classification task. - Performance of NCA should be demonstrated with a suitable visualization. # Example ```python import numpy as np from sklearn.metrics import accuracy_score from sklearn.neighbors import KDTree, KNeighborsClassifier from sklearn.pipeline import Pipeline from sklearn.neighbors import NeighborhoodComponentsAnalysis import matplotlib.pyplot as plt # Generating example data X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]]) y = np.array([1, 1, 1, 2, 2, 2]) k = 2 n_components = 2 # Implementing the necessary functions def create_kdtree(X): kdt = KDTree(X, leaf_size=30, metric=\'euclidean\') return kdt def knn_classification(X_train, y_train, X_test, k): knn = KNeighborsClassifier(n_neighbors=k) knn.fit(X_train, y_train) return knn.predict(X_test) def apply_nca(X, y, n_components): nca = NeighborhoodComponentsAnalysis(n_components=n_components, random_state=42) nca.fit(X, y) return nca.transform(X) def plot_nca_2d(X_nca, y): plt.figure() scatter = plt.scatter(X_nca[:, 0], X_nca[:, 1], c=y, cmap=\'viridis\') plt.legend(handles=scatter.legend_elements()[0], labels=range(len(np.unique(y)))) plt.title(\\"2D projection using NCA\\") plt.show() # Create KD Tree kdt = create_kdtree(X) # KNN Classification predictions = knn_classification(X, y, X, k) print(\\"KNN Predictions:\\", predictions) # Apply NCA X_nca = apply_nca(X, y, n_components) print(\\"NCA reduced data:\\", X_nca) # Plot NCA 2D Projection plot_nca_2d(X_nca, y) # Verify Classification Accuracy accuracy = accuracy_score(y, predictions) print(\\"Classification Accuracy:\\", accuracy) ``` # Notes - Make sure to import the required packages. - Structure your code neatly and ensure it handles edge cases. - Provide comments to explain your implementation steps.","solution":"import numpy as np from sklearn.neighbors import KDTree, KNeighborsClassifier from sklearn.neighbors import NeighborhoodComponentsAnalysis import matplotlib.pyplot as plt def create_kdtree(X): Create a KDTree for the given data X. kdt = KDTree(X, leaf_size=30, metric=\'euclidean\') return kdt def knn_classification(X_train, y_train, X_test, k): Perform k-nearest neighbors classification. knn = KNeighborsClassifier(n_neighbors=k) knn.fit(X_train, y_train) return knn.predict(X_test) def apply_nca(X, y, n_components): Perform Neighborhood Components Analysis (NCA) for supervised dimensionality reduction. nca = NeighborhoodComponentsAnalysis(n_components=n_components, random_state=42) nca.fit(X, y) return nca.transform(X) def plot_nca_2d(X_nca, y): Visualize the 2D projection of data obtained from NCA. plt.figure() scatter = plt.scatter(X_nca[:, 0], X_nca[:, 1], c=y, cmap=\'viridis\') plt.legend(handles=scatter.legend_elements()[0], labels=range(len(np.unique(y)))) plt.title(\\"2D projection using NCA\\") plt.show()"},{"question":"You are tasked with building a utility that can initialize a given URL, fetch its `robots.txt` file, and provide an interface to query various properties and permissions defined in the `robots.txt` file. Specifically, you are to implement the `WebCrawlerUtility` class that utilizes the `RobotFileParser` class from the `urllib.robotparser` package. # Class Definitions **`WebCrawlerUtility`** This class should include the following methods: 1. **`__init__(self, url: str) -> None`** - Initializes the class with the URL of the site\'s `robots.txt` file. - Reads and parses the `robots.txt` file. 2. **`can_fetch(self, useragent: str, url: str) -> bool`** - Returns whether the `useragent` is allowed to fetch the specified `url` based on the `robots.txt` rules. 3. **`get_crawl_delay(self, useragent: str) -> Optional[int]`** - Returns the crawl delay for the specified `useragent`. If the parameter does not exist or has invalid syntax, return `None`. 4. **`get_request_rate(self, useragent: str) -> Optional[Tuple[int, int]]`** - Returns the request rate as a tuple `(requests, seconds)` for the specified `useragent`. If the parameter does not exist or has invalid syntax, return `None`. 5. **`get_sitemaps(self) -> List[str]`** - Returns a list of sitemap URLs mentioned in the `robots.txt` file. If none exist, return an empty list. # Constraints - The URL provided will always be a valid URL. - Your implementation should handle any network errors gracefully. - You can assume the `urllib.robotparser` package is already installed and does not need to be installed by your code. - Only standard Python libraries should be used. # Example Usage: ```python crawler = WebCrawlerUtility(\\"http://www.example.com/robots.txt\\") # Check if the user-agent can fetch a specific URL print(crawler.can_fetch(\\"*\\", \\"http://www.example.com/index.html\\")) # True or False # Get the crawl delay for the user-agent print(crawler.get_crawl_delay(\\"*\\")) # e.g., 10 or None # Get the request rate for the user-agent print(crawler.get_request_rate(\\"*\\")) # e.g., (5, 60) or None # Get the sitemaps listed in the robots.txt print(crawler.get_sitemaps()) # e.g., [\\"http://www.example.com/sitemap.xml\\"] ``` # Notes: - The methods should adhere to the behavior defined in the `RobotFileParser` class documented above. - Your solution should be efficient and appropriately handle cases where `robots.txt` files might be large or complex. Happy coding!","solution":"import urllib.robotparser from typing import Optional, Tuple, List class WebCrawlerUtility: def __init__(self, url: str) -> None: self.parser = urllib.robotparser.RobotFileParser() self.parser.set_url(url) self.parser.read() def can_fetch(self, useragent: str, url: str) -> bool: return self.parser.can_fetch(useragent, url) def get_crawl_delay(self, useragent: str) -> Optional[int]: return self.parser.crawl_delay(useragent) def get_request_rate(self, useragent: str) -> Optional[Tuple[int, int]]: return self.parser.request_rate(useragent) def get_sitemaps(self) -> List[str]: return self.parser.site_maps() or [] # Example usage: # crawler = WebCrawlerUtility(\\"http://www.example.com/robots.txt\\") # print(crawler.can_fetch(\\"*\\", \\"http://www.example.com/index.html\\")) # print(crawler.get_crawl_delay(\\"*\\")) # print(crawler.get_request_rate(\\"*\\")) # print(crawler.get_sitemaps())"},{"question":"You are required to use the `pipes` module to create a sequence of transformations on the contents of a text file. Specifically, you need to perform the following tasks: 1. Convert all text in the file to uppercase. 2. Replace all spaces with underscores. # Instructions 1. Create a `pipes.Template` object. 2. Append the necessary shell commands to perform the above transformations. 3. Open a specified input file for reading and an output file for writing through the pipeline. 4. Use the pipeline to transform the contents of the input file and write the results to the output file. # Requirements 1. The input file will contain plain text. 2. The output file should contain the transformed text. 3. The transformations should be done using shell commands through the `pipes.Template` class. # Function Signature ```python def transform_file(input_filename: str, output_filename: str) -> None: Transforms the contents of input_filename and writes the result to output_filename. Parameters: input_filename (str): The name of the file to be read. output_filename (str): The name of the file to be written. Returns: None pass ``` # Example Suppose we have an input file `sample.txt` with the following content: ``` hello world this is a test ``` After running the `transform_file` function with `sample.txt` as the input file and `result.txt` as the output file, the `result.txt` should contain: ``` HELLO_WORLD THIS_IS_A_TEST ``` # Constraints - The solution must use the `pipes` module to construct and execute the pipeline. - Assume that the input file and output file paths are valid and accessible. # Notes - The usage of deprecated modules is intentional for this exercise to assess understanding. - You may use any valid shell commands that are compatible with POSIX systems. ```python import pipes def transform_file(input_filename: str, output_filename: str) -> None: # Your implementation here pass # Example usage # transform_file(\'sample.txt\', \'result.txt\') ```","solution":"import pipes def transform_file(input_filename: str, output_filename: str) -> None: Transforms the contents of input_filename and writes the result to output_filename. Parameters: input_filename (str): The name of the file to be read. output_filename (str): The name of the file to be written. Returns: None t = pipes.Template() t.append(\'tr \\"[:lower:]\\" \\"[:upper:]\\"\', \'--\') t.append(\'tr \\" \\" \\"_\\"\', \'--\') with t.open(input_filename, \'r\') as f_input: content = f_input.read() with open(output_filename, \'w\') as f_output: f_output.write(content)"},{"question":"**Problem Statement**: You are given an XML document as a string. Write a Python function that parses this XML document and extracts specific information from it. Your tasks are: 1. Parse the given XML string into a DOM structure. 2. Extract and print the values of all elements with a specific tag name (passed as an argument to the function). 3. Modify the value of a specific attribute for all elements with a specific tag name (both specified as arguments). 4. Serialize the modified XML document back into a string and return it. Your function should be named `process_xml_document` and have the following signature: ```python def process_xml_document(xml_string: str, target_tag: str, attr_name: str, attr_value: str) -> str: pass ``` **Inputs**: - `xml_string`: A string representing the XML document. - `target_tag`: A string representing the name of the XML tag to be processed. - `attr_name`: A string representing the name of the attribute to update. - `attr_value`: A string representing the new value to set for the specified attribute. **Outputs**: - A string representing the modified XML document. **Constraints**: - The `xml_string` will be a well-formed XML document. - The `target_tag` will be present in the XML document. - The `attr_name` will be valid for the given `target_tag`. **Example**: ```python xml_input = \'\'\'<root> <item id=\\"1\\">Item 1</item> <item id=\\"2\\">Item 2</item> <data> <item id=\\"3\\">Item 3</item> </data> </root>\'\'\' updated_xml = process_xml_document(xml_input, \'item\', \'id\', \'updated\') print(updated_xml) ``` **Expected Output**: ```xml <root> <item id=\\"updated\\">Item 1</item> <item id=\\"updated\\">Item 2</item> <data> <item id=\\"updated\\">Item 3</item> </data> </root> ``` **Evaluation Criteria**: - Correctness: The function should produce the expected output. - Use of `xml.dom` API for parsing and manipulating the XML document. - Proper handling of the DOM structure and modifications. - Efficiency in traversing and modifying the XML elements and attributes.","solution":"from xml.dom.minidom import parseString, Document def process_xml_document(xml_string: str, target_tag: str, attr_name: str, attr_value: str) -> str: # Parse the XML string into a DOM structure dom = parseString(xml_string) # Extract all elements with the specific tag name elements = dom.getElementsByTagName(target_tag) # Modify the value of the specified attribute for all elements with the specific tag name for element in elements: element.setAttribute(attr_name, attr_value) # Serialize the modified XML document back into a string and return it return dom.toxml()"},{"question":"Objective: Implement a custom module importer to handle importing modules from specific paths in the filesystem. This question tests the understanding of custom import hooks, finders, and loaders as described in the provided documentation. Problem Statement: You are required to create a custom importer that allows importing of a module from a given directory path defined at runtime. Implement the following steps: 1. **Create a Custom Finder**: This finder should check if a given module exists in the specified directory. If it exists, the finder should return a module spec. 2. **Create a Custom Loader**: This loader should load the module from the file using its spec and handle execution of the module code. 3. **Register the Finder**: The custom finder needs to be added to `sys.meta_path` to override the default import behavior. # Required Functions: 1. `CustomFinder` class implementing the `find_spec` method. 2. `CustomLoader` class implementing the `exec_module` method. 3. `register_custom_importer(path)` function that adds the CustomFinder to `sys.meta_path`. # Constraints: - **Input Path**: The path provided for module searching must be a valid directory path containing Python files. - **Modules**: Only import files with the `.py` extension. # Example: Assume we have the following directory structure: ``` custom_modules/ example.py ``` `example.py` contains: ```python def hello(): return \\"Hello, World!\\" ``` Code Implementation: ```python import sys import importlib.util import importlib.abc from types import ModuleType from pathlib import Path class CustomFinder(importlib.abc.MetaPathFinder): def __init__(self, path): self.path = Path(path) def find_spec(self, fullname, path, target=None): module_path = self.path / f\\"{fullname}.py\\" if module_path.exists(): spec = importlib.util.spec_from_file_location(fullname, module_path) spec.loader = CustomLoader(module_path) return spec return None class CustomLoader(importlib.abc.Loader): def __init__(self, module_path): self.module_path = module_path def create_module(self, spec): return None # Default module creation def exec_module(self, module): with open(self.module_path, \'r\') as file: code = file.read() exec(code, module.__dict__) def register_custom_importer(path): finder = CustomFinder(path) sys.meta_path.insert(0, finder) # Usage example: # register_custom_importer(\\"custom_modules\\") # import example # print(example.hello()) ``` # Execution Instructions: 1. Implement the required classes and functions as specified. 2. Test by placing modules in the specified directory and using the custom importer to import and execute functions from these modules. Expected Output: After registering the custom importer for the directory `\\"custom_modules\\"` and importing the `example` module: ```python register_custom_importer(\\"custom_modules\\") import example print(example.hello()) # Output: \\"Hello, World!\\" ``` # Submission: Submit the implemented `CustomFinder` class, `CustomLoader` class, and `register_custom_importer(path)` function along with example usage code that demonstrates the custom importer\'s functionality.","solution":"import sys import importlib.util import importlib.abc from pathlib import Path class CustomFinder(importlib.abc.MetaPathFinder): def __init__(self, path): self.path = Path(path) def find_spec(self, fullname, path, target=None): module_path = self.path / f\\"{fullname}.py\\" if module_path.exists(): spec = importlib.util.spec_from_file_location(fullname, module_path) spec.loader = CustomLoader(module_path) return spec return None class CustomLoader(importlib.abc.Loader): def __init__(self, module_path): self.module_path = module_path def create_module(self, spec): return None # Default module creation def exec_module(self, module): with open(self.module_path, \'r\') as file: code = file.read() exec(code, module.__dict__) def register_custom_importer(path): finder = CustomFinder(path) sys.meta_path.insert(0, finder)"},{"question":"**Problem Statement:** You are tasked to design and implement a Python program that manages a simple library database using the `sqlite3` module. The program should be able to: 1. Create a database and establish a connection. 2. Create a table named `books` with columns for `id`, `title`, `author`, `published_year`, and `rating`. 3. Insert multiple records into the `books` table using a preferred method of executing SQL commands. 4. Implement a query to fetch and print all book titles along with their authors, sorted by the authors\' names. 5. Demonstrate transaction control by adding a feature to update the rating of a book and rollback the transaction if the new rating is not within the range [0, 10]. **Function Implementation:** You are required to implement the following two functions: 1. `initialize_database(db_name: str) -> sqlite3.Connection` - **Inputs:** - `db_name` (string): The name of the database file. - **Output:** - Returns an open `sqlite3.Connection` to the database. - **Functionality:** - Establish a connection to the database. - Create the `books` table if it does not exist. 2. `manage_library(db_connection: sqlite3.Connection, books_data: List[Tuple[str, str, int, float]])` - **Inputs:** - `db_connection` (sqlite3.Connection): An open connection to the SQLite database. - `books_data` (List of Tuples): Each tuple contains `title`, `author`, `published_year`, and `rating` of a book. - **Output:** None - **Functionality:** - Insert the provided book data into the `books` table. - Query and print all book titles and authors, sorted by author names. - Define a sub-function `update_rating_and_rollback(book_id: int, new_rating: float)` to update the rating of a book and rollback the transaction if the new rating is out of the valid range [0, 10]. **Constraints:** - Each rating must be a float between 0 and 10, inclusive. - The published year must be a valid four-digit integer. **Example Usage:** ```python # Example book data books_data = [ (\\"Harry Potter and the Sorcerer\'s Stone\\", \\"J.K. Rowling\\", 1997, 9.3), (\\"To Kill a Mockingbird\\", \\"Harper Lee\\", 1960, 8.9), (\\"1984\\", \\"George Orwell\\", 1949, 9.0), (\\"Pride and Prejudice\\", \\"Jane Austen\\", 1813, 8.5) ] # Initialize the database conn = initialize_database(\\"library.db\\") # Manage library and insert data manage_library(conn, books_data) # Function call to update rating and demonstrate rollback (supposed to be # executed inside `manage_library`) update_rating_and_rollback(1, 11) # This should trigger a rollback since 11 is out of the valid range ``` **Note:** The `update_rating_and_rollback` function should be defined inside the `manage_library` function and demonstrate transaction control explicitly through commit and rollback operations. All necessary error handling and validations should be implemented to ensure the program runs smoothly and correctly handles edge cases and unexpected inputs.","solution":"import sqlite3 from typing import List, Tuple def initialize_database(db_name: str) -> sqlite3.Connection: Creates and initializes the database with a \'books\' table. Parameters: db_name (str): The name of the database file. Returns: sqlite3.Connection: The connection to the database. conn = sqlite3.connect(db_name) cursor = conn.cursor() cursor.execute(\'\'\'CREATE TABLE IF NOT EXISTS books ( id INTEGER PRIMARY KEY, title TEXT NOT NULL, author TEXT NOT NULL, published_year INTEGER NOT NULL, rating REAL NOT NULL )\'\'\') conn.commit() return conn def manage_library(db_connection: sqlite3.Connection, books_data: List[Tuple[str, str, int, float]]): Manages the library functionalities including data insertion and querying. Parameters: db_connection (sqlite3.Connection): The database connection. books_data (List[Tuple[str, str, int, float]]): A list of books data to be inserted. cursor = db_connection.cursor() cursor.executemany(\'\'\' INSERT INTO books (title, author, published_year, rating) VALUES (?, ?, ?, ?) \'\'\', books_data) print(\\"Books have been inserted.\\") cursor.execute(\'\'\' SELECT title, author FROM books ORDER BY author \'\'\') books = cursor.fetchall() for book in books: print(f\\"Title: {book[0]}, Author: {book[1]}\\") def update_rating_and_rollback(book_id: int, new_rating: float): try: if new_rating < 0 or new_rating > 10: raise ValueError(\\"Invalid rating value; must be between 0 and 10.\\") cursor.execute(\'\'\' UPDATE books SET rating = ? WHERE id = ? \'\'\', (new_rating, book_id)) db_connection.commit() print(f\\"Rating updated to {new_rating} for book ID {book_id}\\") except Exception as e: db_connection.rollback() print(f\\"Transaction rolled back: {e}\\") # Adding a way to access the inner function for demonstration purposes manage_library.update_rating_and_rollback = update_rating_and_rollback db_connection.commit()"},{"question":"**Objective:** Implement a Python class that correctly utilizes special methods to emulate a simple 2D vector (Vector2D). **Question:** You are required to implement a class `Vector2D` that represents a two-dimensional vector in a Cartesian coordinate system. Your class should support typical vector operations such as vector addition, subtraction, scalar multiplication, and magnitude calculation. You must utilize Python\'s special methods to achieve these functionalities. **Requirements:** 1. **Initialization (`__init__`)**: - The constructor should take two arguments `x` and `y` representing the vector\'s components in the x and y directions respectively. 2. **String Representation (`__repr__`)**: - Implement the `__repr__` method to return a string in the format `Vector2D(x, y)`. 3. **Vector Addition (`__add__`)**: - Implement the `__add__` method to support the addition of two vectors. The result should be a new `Vector2D` object. 4. **Vector Subtraction (`__sub__`)**: - Implement the `__sub__` method to support the subtraction of two vectors. The result should be a new `Vector2D` object. 5. **Scalar Multiplication (`__mul__`)**: - Implement the `__mul__` method to support the multiplication of a vector by a scalar. The result should be a new `Vector2D` object. 6. **Vector Magnitude (`__abs__`)**: - Implement the `__abs__` method to return the magnitude (length) of the vector, calculated as the square root of the sum of the squares of its components. 7. **Equality Check (`__eq__`)**: - Implement the `__eq__` method to check if two vectors are equal. **Constraints:** - You can assume `x` and `y` are always floating-point or integer values. - Your implementation should handle vector operations following the standard rules of vector algebra. **Example:** ```python v1 = Vector2D(3, 4) v2 = Vector2D(1, 2) print(v1) # Output: Vector2D(3, 4) print(v2) # Output: Vector2D(1, 2) print(v1 + v2) # Output: Vector2D(4, 6) print(v1 - v2) # Output: Vector2D(2, 2) print(v1 * 2) # Output: Vector2D(6, 8) print(abs(v1)) # Output: 5.0 print(v1 == Vector2D(3, 4)) # Output: True print(v1 == v2) # Output: False ``` **Note:** Your implementation should align with the standard practices of implementing such methods in Python.","solution":"import math class Vector2D: def __init__(self, x, y): self.x = x self.y = y def __repr__(self): return f\\"Vector2D({self.x}, {self.y})\\" def __add__(self, other): return Vector2D(self.x + other.x, self.y + other.y) def __sub__(self, other): return Vector2D(self.x - other.x, self.y - other.y) def __mul__(self, scalar): return Vector2D(self.x * scalar, self.y * scalar) def __abs__(self): return math.sqrt(self.x**2 + self.y**2) def __eq__(self, other): return self.x == other.x and self.y == other.y"},{"question":"# Seaborn KDE Plotting Challenge As a data scientist, you have been given a dataset containing information about different species of flowers. Your task is to use seaborn to generate insightful visualizations about this dataset. Specifically, you are to create Kernel Density Estimation (KDE) plots that will help in understanding the distribution and relationships among different features of the flowers. You are provided with the `iris` dataset, which contains the following columns: - `sepal_length`: Length of the sepal - `sepal_width`: Width of the sepal - `petal_length`: Length of the petal - `petal_width`: Width of the petal - `species`: Species of the flower (setosa, versicolor, virginica) Requirements 1. **Univariate KDE Plot**: Create a KDE plot for the `sepal_length` column. 2. **Bivariate KDE Plot**: Create a bivariate KDE plot for the `sepal_length` and `sepal_width`. 3. **Conditional KDE Plot with Hue Mapping**: Create a KDE plot for the `petal_length` column, conditional on the `species` column using hue mapping. 4. **Stacked KDE Plot**: Create a \\"stacked\\" KDE plot for the `petal_width` column, conditional on the `species` column. 5. **Log-Scaled KDE Plot**: Create a KDE plot for the `sepal_length` column, applying log scale on the x-axis. 6. **Normalization in Stacked KDE Plot**: Create a stacked KDE plot for the `sepal_width` column, conditional on the `species` column, and normalize the distribution at each value in the grid. Input The input consists of reading the dataset using seaborn: ```python import seaborn as sns import matplotlib.pyplot as plt iris = sns.load_dataset(\\"iris\\") ``` Output You are to generate and display the plots as specified in the requirements. Each plot should be properly labeled with titles and axis names. Constraints and Limitations - Ensure that all plots are visually clear and properly annotated. - Use appropriate bandwidth adjustments where necessary to showcase a clear distribution. Example Here is an example of how to start with the univariate KDE plot: ```python import seaborn as sns import matplotlib.pyplot as plt # Set the theme sns.set_theme() # Load the dataset iris = sns.load_dataset(\\"iris\\") # Univariate KDE Plot for sepal_length sns.kdeplot(data=iris, x=\\"sepal_length\\") plt.title(\\"KDE Plot of Sepal Length\\") plt.xlabel(\\"Sepal Length\\") plt.ylabel(\\"Density\\") plt.show() ``` Your task is to create the code for the remaining plots as per the requirements.","solution":"import seaborn as sns import matplotlib.pyplot as plt # Set the theme sns.set_theme() # Load the dataset iris = sns.load_dataset(\\"iris\\") def plot_univariate_kde(): Create a KDE plot for the sepal_length column. plt.figure() sns.kdeplot(data=iris, x=\\"sepal_length\\") plt.title(\\"KDE Plot of Sepal Length\\") plt.xlabel(\\"Sepal Length\\") plt.ylabel(\\"Density\\") plt.show() def plot_bivariate_kde(): Create a bivariate KDE plot for the sepal_length and sepal_width columns. plt.figure() sns.kdeplot(data=iris, x=\\"sepal_length\\", y=\\"sepal_width\\") plt.title(\\"Bivariate KDE Plot of Sepal Length and Sepal Width\\") plt.xlabel(\\"Sepal Length\\") plt.ylabel(\\"Sepal Width\\") plt.show() def plot_conditional_kde(): Create a KDE plot for the petal_length column, conditional on the species column using hue mapping. plt.figure() sns.kdeplot(data=iris, x=\\"petal_length\\", hue=\\"species\\") plt.title(\\"KDE Plot of Petal Length by Species\\") plt.xlabel(\\"Petal Length\\") plt.ylabel(\\"Density\\") plt.show() def plot_stacked_kde(): Create a \\"stacked\\" KDE plot for the petal_width column, conditional on the species column. plt.figure() sns.kdeplot(data=iris, x=\\"petal_width\\", hue=\\"species\\", multiple=\\"stack\\") plt.title(\\"Stacked KDE Plot of Petal Width by Species\\") plt.xlabel(\\"Petal Width\\") plt.ylabel(\\"Density\\") plt.show() def plot_log_scaled_kde(): Create a KDE plot for the sepal_length column, applying log scale on the x-axis. plt.figure() sns.kdeplot(data=iris, x=\\"sepal_length\\") plt.xscale(\\"log\\") plt.title(\\"Log-Scaled KDE Plot of Sepal Length\\") plt.xlabel(\\"Sepal Length (Log Scale)\\") plt.ylabel(\\"Density\\") plt.show() def plot_normalized_stacked_kde(): Create a stacked KDE plot for the sepal_width column, conditional on the species column, and normalize the distribution at each value in the grid. plt.figure() sns.kdeplot(data=iris, x=\\"sepal_width\\", hue=\\"species\\", multiple=\\"fill\\") plt.title(\\"Normalized Stacked KDE Plot of Sepal Width by Species\\") plt.xlabel(\\"Sepal Width\\") plt.ylabel(\\"Density\\") plt.show()"},{"question":"# Pandas Missing Values Assessment Objective: You are required to write a function that processes a pandas DataFrame with missing values and performs the following tasks: 1. **Identify Missing Values**: Identify all the columns in the DataFrame that contain missing values. 2. **Fill Missing Values**: Replace missing values in different types of columns: - For numeric columns, replace missing values with the mean of that column. - For categorical columns (object dtypes), replace missing values with the mode of that column. - For datetime columns, replace missing values with a specified date given as input. 3. **Remove Rows/Columns**: Remove any rows or columns that still contain missing values after the replacement. Function Signature: ```python import pandas as pd def handle_missing_values(df: pd.DataFrame, replacement_date: str) -> pd.DataFrame: Processes the DataFrame by handling missing values. Parameters: df (pd.DataFrame): Input DataFrame that may contain missing values. replacement_date (str): The date string to use for replacing missing datetime values in the \\"YYYY-MM-DD\\" format. Returns: pd.DataFrame: Processed DataFrame with missing values handled. pass ``` Input: - A pandas DataFrame `df` that may contain numeric, categorical, and datetime columns with missing values. - A string `replacement_date` in the \\"YYYY-MM-DD\\" format. Output: - A new pandas DataFrame with the following adjustments: - Missing numeric values replaced with the column\'s mean. - Missing categorical values replaced with the column\'s mode. - Missing datetime values replaced with the specified date. - Remaining rows or columns with missing values removed. Constraints: - Do not use global variables. - Assume the input DataFrame can be large; hence, the function should be efficient with respect to both time and space. Example: ```python import pandas as pd import numpy as np data = { \'numeric_col\': [1, 2, np.nan, 4], \'categorical_col\': [\'a\', \'b\', np.nan, \'a\'], \'datetime_col\': [pd.Timestamp(\'2020-01-01\'), pd.NaT, pd.Timestamp(\'2020-01-03\'), pd.Timestamp(\'2020-01-04\')], } df = pd.DataFrame(data) replacement_date = \\"2020-01-02\\" result_df = handle_missing_values(df, replacement_date) print(result_df) ``` Expected Output: ``` numeric_col categorical_col datetime_col 0 1.0 a 2020-01-01 1 2.0 b 2020-01-02 2 2.333333 a 2020-01-03 3 4.0 a 2020-01-04 ``` **Note**: Ensure that the function is tested with various DataFrames to cover different edge cases such as DataFrames with only one type of column, DataFrames with no missing values, etc.","solution":"import pandas as pd def handle_missing_values(df: pd.DataFrame, replacement_date: str) -> pd.DataFrame: Processes the DataFrame by handling missing values. Parameters: df (pd.DataFrame): Input DataFrame that may contain missing values. replacement_date (str): The date string to use for replacing missing datetime values in the \\"YYYY-MM-DD\\" format. Returns: pd.DataFrame: Processed DataFrame with missing values handled. # Replace missing values in numeric columns with the mean of that column numeric_cols = df.select_dtypes(include=\'number\').columns for col in numeric_cols: if df[col].isna().any(): df[col].fillna(df[col].mean(), inplace=True) # Replace missing values in categorical columns with the mode of that column categorical_cols = df.select_dtypes(include=\'object\').columns for col in categorical_cols: if df[col].isna().any(): df[col].fillna(df[col].mode()[0], inplace=True) # Replace missing values in datetime columns with the specified date datetime_cols = df.select_dtypes(include=\'datetime\').columns for col in datetime_cols: if df[col].isna().any(): df[col].fillna(pd.to_datetime(replacement_date), inplace=True) # Drop rows or columns that still contain missing values after imputations df.dropna(axis=0, how=\'any\', inplace=True) return df"},{"question":"You are developing an email client that needs to handle internationalized email headers correctly. Your task is to implement functions that demonstrate the usage of the `email.header` module in Python to encode and decode email headers. Specifically, you will focus on the `Header` class and the `decode_header` and `make_header` functions. Task 1: Implement a function `encode_email_header(header_value: str, charset: str=\'utf-8\') -> str` that: - Takes in a `header_value` (the string to be used as the email header) and an optional `charset` (default is \'utf-8\'). - Creates an instance of the `Header` class using the provided `header_value` and `charset`. - Returns the encoded header as a string, using the `Header` instance\'s encoding methods. Task 2: Implement a function `decode_email_header(encoded_header: str) -> list` that: - Takes an `encoded_header` string (an encoded email header). - Uses the `decode_header` function to decode the encoded header into its parts. - Returns the list of `(decoded_string, charset)` pairs. Task 3: Implement a function `reconstruct_email_header(encoded_header: str, charset: str=\'utf-8\') -> str` that: - Takes an `encoded_header` string and an optional `charset` (default is \'utf-8\'). - Decodes the header into its parts using `decode_header`. - Reconstructs the header using `make_header` with the decoded parts. - Returns the reconstructed header as a string. # Function Signatures: ```python from email.header import Header, decode_header, make_header def encode_email_header(header_value: str, charset: str=\'utf-8\') -> str: pass def decode_email_header(encoded_header: str) -> list: pass def reconstruct_email_header(encoded_header: str, charset: str=\'utf-8\') -> str: pass ``` # Examples: ```python # Encode Header Example header = \\"Pöstal Service\\" charset = \\"iso-8859-1\\" encoded_header = encode_email_header(header, charset) print(encoded_header) # Expected to print the RFC 2047 encoded version of \\"Pöstal Service\\" # Decode Header Example encoded_header = \\"=?iso-8859-1?q?p=F6stal_Service?=\\" decoded_parts = decode_email_header(encoded_header) print(decoded_parts) # Expected: [(b\'pxf6stal_Service\', \'iso-8859-1\')] # Reconstruct Header Example reconstructed_header = reconstruct_email_header(encoded_header) print(reconstructed_header) # Expected to reconstruct to a readable header, \\"Pöstal Service\\" ``` # Constraints: - `header_value` and `charset` are non-empty strings. - The functions must handle proper encoding and decoding, raising errors where applicable (for example, incorrect charset). Your implementation must demonstrate a thorough understanding of the `email.header` module and its use cases in handling internationalized email headers.","solution":"from email.header import Header, decode_header, make_header def encode_email_header(header_value: str, charset: str = \'utf-8\') -> str: Encodes the email header using the specified charset. Args: header_value (str): The string to be used as the email header. charset (str): The charset for encoding. Returns: str: The encoded email header. header = Header(header_value, charset) return header.encode() def decode_email_header(encoded_header: str) -> list: Decodes the encoded email header. Args: encoded_header (str): The encoded email header string. Returns: list: List of (decoded_string, charset) pairs. decoded_parts = decode_header(encoded_header) return [(decoded_part[0].decode(decoded_part[1] if decoded_part[1] else \'ascii\'), decoded_part[1]) if isinstance(decoded_part[0], bytes) else decoded_part for decoded_part in decoded_parts] def reconstruct_email_header(encoded_header: str, charset: str = \'utf-8\') -> str: Reconstructs the email header from its encoded parts. Args: encoded_header (str): The encoded email header string. charset (str): The charset for reconstructing. Returns: str: The reconstructed email header. decoded_parts = decode_header(encoded_header) header = make_header(decoded_parts) return str(header)"},{"question":"# Python Execution Model and Error Handling You are given a partially implemented class `Calculator` that performs basic arithmetic operations. However, it lacks implementation for handling division operations and proper error handling. Additionally, you need to extend the functionality to dynamically evaluate different arithmetic expressions provided as strings. Your Task: 1. Complete the `division` method to handle division operations. If a division by zero is attempted, it should raise a custom `DivisionByZeroError`. 2. Implement dynamic evaluation of arithmetic expressions using the `evaluate_expression` method. The method should: - Take an input string representing an arithmetic expression, - Use `eval()` to compute the result, - Handle any exceptions that may occur during the evaluation, - Avoid exposing unnecessary global or local variables during evaluation. Class Implementation: ```python class DivisionByZeroError(Exception): Custom exception for division by zero errors. pass class Calculator: def __init__(self): pass def add(self, a, b): return a + b def subtract(self, a, b): return a - b def multiply(self, a, b): return a * b def division(self, a, b): Perform division and handle division by zero. # Your code here def evaluate_expression(self, expression): Dynamically evaluate an arithmetic expression. # Your code here # Usage example: if __name__ == \\"__main__\\": calc = Calculator() print(calc.add(2, 3)) # Output: 5 print(calc.subtract(10, 4)) # Output: 6 print(calc.multiply(6, 7)) # Output: 42 try: print(calc.division(10, 0)) # Should raise DivisionByZeroError except DivisionByZeroError as e: print(e) print(calc.evaluate_expression(\\"2 + 3 * 4 - 5 / 5\\")) # Output: 13.0 ``` Requirements: 1. The `division` method should return the quotient of `a` and `b`, and raise `DivisionByZeroError` if `b` is zero. 2. The `evaluate_expression` should handle and report errors without propagating them in a generic manner. 3. Ensure your solution adheres to Python\'s scoping rules and namespace management best practices as reflected in the provided documentation. Constraints: - The `evaluate_expression` should not use any variables outside its local scope. - You may only use the standard Python library. Expected Input/Output: - `division(a, b)`: - Input: Two integers or floats `a` and `b`. - Output: Quotient of `a` and `b`, or raise `DivisionByZeroError` if `b` is zero. - `evaluate_expression(expression)`: - Input: A string `expression` representing a valid arithmetic expression. - Output: The evaluated result of the expression or an error message if the expression is invalid. Example Execution: ```python calc = Calculator() calc.division(10, 2) # Output: 5.0 calc.evaluate_expression(\\"10 / 2\\") # Output: 5.0 calc.evaluate_expression(\\"10 / 0\\") # Output: Error message (do not raise) ``` Complete the methods to ensure they meet the requirements specified.","solution":"class DivisionByZeroError(Exception): Custom exception for division by zero errors. pass class Calculator: def __init__(self): pass def add(self, a, b): return a + b def subtract(self, a, b): return a - b def multiply(self, a, b): return a * b def division(self, a, b): Perform division and handle division by zero. if b == 0: raise DivisionByZeroError(\\"Division by zero is not allowed.\\") return a / b def evaluate_expression(self, expression): Dynamically evaluate an arithmetic expression. try: result = eval(expression, {\\"__builtins__\\": None}, {}) return result except Exception as e: return f\\"Error evaluating expression: {e}\\""},{"question":"**Supervised Learning Pipeline** You are given a dataset in CSV format that contains information about individuals and whether they have a certain medical condition (binary classification problem). Your task is to implement a complete supervised learning pipeline using scikit-learn that includes the following steps: 1. **Data Loading and Preprocessing:** - Load the dataset from the provided CSV file. - Handle any missing values appropriately. - Encode categorical features using appropriate strategies. - Split the dataset into training and testing subsets (80-20 split). 2. **Model Selection and Training:** - Choose three different supervised learning models from scikit-learn (e.g., Logistic Regression, Decision Tree, Support Vector Machine). - Train each of these models on the training data. 3. **Hyperparameter Tuning:** - Implement hyperparameter tuning for one of the selected models using GridSearchCV or RandomizedSearchCV. 4. **Model Evaluation:** - Evaluate each of the three models on the test set using appropriate metrics (e.g., accuracy, precision, recall, F1-score). - Print out the evaluation metrics for each model. - Identify the best performing model among the three. **Constraints:** - Performance on the test set should be an essential criterion for model evaluation. - The implementation should be efficient and utilize scikit-learn functionalities as much as possible. - Proper use of preprocessing tools and model evaluation techniques as provided by scikit-learn is mandatory. **Input Format:** - A CSV file path will be provided. **Output Format:** - Evaluation metrics for each model on the test set. - The best performing model and its corresponding metrics. **Example:** Given a CSV file `medical_data.csv` with columns: ``` age, gender, blood_pressure, cholesterol, condition 45, M, 130, 250, 1 60, F, 120, 220, 0 ... ``` Your output should be in the following format: ``` Logistic Regression: - Accuracy: 0.85 - Precision: 0.80 - Recall: 0.78 - F1-score: 0.79 Decision Tree: - Accuracy: 0.83 - Precision: 0.79 - Recall: 0.77 - F1-score: 0.78 Support Vector Machine: - Accuracy: 0.87 - Precision: 0.85 - Recall: 0.82 - F1-score: 0.83 Best Performing Model: Support Vector Machine - Accuracy: 0.87 - Precision: 0.85 - Recall: 0.82 - F1-score: 0.83 ``` Ensure that your code is clean, well-documented, and follows best practices.","solution":"import pandas as pd from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.preprocessing import StandardScaler, OneHotEncoder from sklearn.compose import ColumnTransformer from sklearn.pipeline import Pipeline from sklearn.impute import SimpleImputer from sklearn.linear_model import LogisticRegression from sklearn.tree import DecisionTreeClassifier from sklearn.svm import SVC from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score from sklearn.metrics import classification_report def preprocess_data(file_path): # Load the dataset data = pd.read_csv(file_path) # Seperate features and labels X = data.drop(\'condition\', axis=1) y = data[\'condition\'] # Identify numeric and categorical columns numeric_features = X.select_dtypes(include=[\'int64\', \'float64\']).columns categorical_features = X.select_dtypes(include=[\'object\']).columns # Preprocessing for numeric data numeric_transformer = Pipeline(steps=[ (\'imputer\', SimpleImputer(strategy=\'mean\')), (\'scaler\', StandardScaler())]) # Preprocessing for categorical data categorical_transformer = Pipeline(steps=[ (\'imputer\', SimpleImputer(strategy=\'constant\', fill_value=\'missing\')), (\'onehot\', OneHotEncoder(handle_unknown=\'ignore\'))]) # Combine preprocessing steps preprocessor = ColumnTransformer( transformers=[ (\'num\', numeric_transformer, numeric_features), (\'cat\', categorical_transformer, categorical_features)]) return preprocessor, X, y def evaluate_model(model, X_test, y_test): y_pred = model.predict(X_test) accuracy = accuracy_score(y_test, y_pred) precision = precision_score(y_test, y_pred) recall = recall_score(y_test, y_pred) f1 = f1_score(y_test, y_pred) return accuracy, precision, recall, f1 def print_model_metrics(name, accuracy, precision, recall, f1): print(f\\"{name}:\\") print(f\\"- Accuracy: {accuracy:.2f}\\") print(f\\"- Precision: {precision:.2f}\\") print(f\\"- Recall: {recall:.2f}\\") print(f\\"- F1-score: {f1:.2f}\\") print() def main(file_path): # Data Preprocessing preprocessor, X, y = preprocess_data(file_path) # Split data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Define models models = { \'Logistic Regression\': LogisticRegression(), \'Decision Tree\': DecisionTreeClassifier(), \'Support Vector Machine\': SVC() } best_model_name = None best_model_score = 0 for name, model in models.items(): # Create a pipeline that preprocesses data and then fits the model clf = Pipeline(steps=[(\'preprocessor\', preprocessor), (\'classifier\', model)]) # Train the model clf.fit(X_train, y_train) # Evaluate the model accuracy, precision, recall, f1 = evaluate_model(clf, X_test, y_test) print_model_metrics(name, accuracy, precision, recall, f1) if accuracy > best_model_score: best_model_score = accuracy best_model_name = name print(f\\"Best Performing Model: {best_model_name}\\") # Hyperparameter tuning for one of the models (e.g., Support Vector Machine) param_grid = { \'classifier__C\': [0.1, 1, 10], \'classifier__kernel\': [\'linear\', \'poly\', \'rbf\'] } grid_search = GridSearchCV(Pipeline(steps=[(\'preprocessor\', preprocessor), (\'classifier\', SVC())]), param_grid, cv=3, scoring=\'accuracy\') grid_search.fit(X_train, y_train) best_clf = grid_search.best_estimator_ accuracy, precision, recall, f1 = evaluate_model(best_clf, X_test, y_test) print_model_metrics(\\"Best SVC Model\\", accuracy, precision, recall, f1) if __name__ == \\"__main__\\": # Set the path to the CSV file csv_file_path = \'medical_data.csv\' main(csv_file_path)"},{"question":"**Title: Implement Custom String and Number Conversion Functions** Description: Implement a Python class `CustomConverter` that provides functionalities similar to the C functions described in the provided documentation. The class should include methods for: 1. Formatting a number into a string with specified precision and ensuring consistent behavior in edge cases. 2. Converting a string to a floating-point number with appropriate error handling. Requirements: 1. **`format_number` method**: - **Input**: - `number` (float): The number to be formatted. - `format_code` (str): A character indicating the format - one of `[\'e\', \'E\', \'f\', \'F\', \'g\', \'G\']`. - `precision` (int): The number of decimal places. - `sign` (bool): Whether to always include a sign character (default is False). - **Output**: - A formatted string representing the number. - **Constraints**: - `format_code` must be one of `[\'e\', \'E\', \'f\', \'F\', \'g\', \'G\']`. - `precision` must be a non-negative integer. - Raise `ValueError` for invalid input combinations. 2. **`string_to_float` method**: - **Input**: - `s` (str): The string to convert to float. - **Output**: - A float value if the conversion is successful. - **Constraints**: - The string must represent a valid floating-point number. - Raise `ValueError` if the string is not a valid floating-point number. - Handle cases where the string represents a value too large or too small to be represented as a float by returning `float(\'inf\')` or `float(\'-inf\')`. Example Usage: ```python class CustomConverter: @staticmethod def format_number(number: float, format_code: str, precision: int, sign: bool = False) -> str: pass @staticmethod def string_to_float(s: str) -> float: pass # Example usage: # Formatting number converter = CustomConverter() formatted_number = converter.format_number(1234.5678, \'f\', 2, True) print(formatted_number) # Output: \'+1234.57\' # Converting string to float float_number = converter.string_to_float(\\"1234.5678\\") print(float_number) # Output: 1234.5678 # Handling errors try: float_number = converter.string_to_float(\\"invalid\\") except ValueError as e: print(e) ``` Constraints: - The implementations for both methods should handle edge cases gracefully. - For `format_number`, ensure that the formatted string adheres to the specified precision and format code rules. - For `string_to_float`, consider handling string representations of infinity and NaN appropriately. Evaluation: - Correctness: The solution should correctly format numbers and handle string to float conversions with appropriate error handling. - Edge cases: Consider edge cases such as very large or very small numbers, invalid input strings, and precision handling. - Code quality: Code should be well-organized, readable, and follow Python conventions.","solution":"class CustomConverter: @staticmethod def format_number(number: float, format_code: str, precision: int, sign: bool = False) -> str: if format_code not in [\'e\', \'E\', \'f\', \'F\', \'g\', \'G\']: raise ValueError(\\"format_code must be one of \'e\', \'E\', \'f\', \'F\', \'g\', \'G\'.\\") if not isinstance(precision, int) or precision < 0: raise ValueError(\\"precision must be a non-negative integer.\\") sign_str = \'+\' if sign else \'\' format_str = f\\"{sign_str}{{:.{precision}{format_code}}}\\" return format_str.format(number) @staticmethod def string_to_float(s: str) -> float: try: return float(s) except ValueError: raise ValueError(f\\"Cannot convert string to float: {s}\\") # Example usage: # Formatting number converter = CustomConverter() formatted_number = converter.format_number(1234.5678, \'f\', 2, True) print(formatted_number) # Output: \'+1234.57\' # Converting string to float float_number = CustomConverter.string_to_float(\\"1234.5678\\") print(float_number) # Output: 1234.5678 # Handling errors try: float_number = CustomConverter.string_to_float(\\"invalid\\") except ValueError as e: print(e) # Output: Cannot convert string to float: invalid"},{"question":"# Nullable Boolean Data Type and Kleene Logic in pandas **Objective**: Create a function to demonstrate the usage of Nullable Boolean data type in pandas while implementing Kleene logical operations and indexing. **Problem Statement**: You are given a pandas DataFrame that contains a mix of boolean values and NA values. Your task is to write a function `perform_boolean_operations` that takes the DataFrame as input and performs the following operations: 1. Applies a mask to the DataFrame to filter rows based on Nullable Boolean indexing. 2. Performs and returns the results of Kleene logical operations (AND, OR, XOR) on specified columns. 3. Ensures the operations maintain NA values appropriately according to Kleene logic. **Function Signature**: ```python def perform_boolean_operations(df: pd.DataFrame) -> pd.DataFrame: pass ``` **Input**: - `df`: A pandas DataFrame containing at least two columns with Nullable Boolean data. **Output**: - The function should return a new DataFrame with the results of the following columns: 1. Original columns with Nullable Booleans. 2. A column named `mask_and` showing the result of AND operation on the first two columns. 3. A column named `mask_or` showing the result of OR operation on the first two columns. 4. A column named `mask_xor` showing the result of XOR operation on the first two columns. **Example**: Suppose you have the following DataFrame: ```python import pandas as pd import numpy as np data = { \'bool_col1\': pd.Series([True, False, pd.NA, True], dtype=\\"boolean\\"), \'bool_col2\': pd.Series([False, pd.NA, True, pd.NA], dtype=\\"boolean\\") } df = pd.DataFrame(data) ``` Your function should produce an output DataFrame like this: ```python bool_col1 bool_col2 mask_and mask_or mask_xor 0 True False False True True 1 False NaN False NaN NaN 2 NaN True NaN True NaN 3 True NaN NaN True NaN ``` **Constraints**: 1. The DataFrame will always contain at least two columns with Nullable Boolean data types. 2. You must ensure proper handling of NA values as per the Kleene logic described. **Note**: You are expected to demonstrate an understanding of pandas\' Nullable Boolean data type and Kleene logic operations with this task.","solution":"import pandas as pd def perform_boolean_operations(df: pd.DataFrame) -> pd.DataFrame: # Performing Kleene logical operations df[\'mask_and\'] = df.iloc[:, 0] & df.iloc[:, 1] df[\'mask_or\'] = df.iloc[:, 0] | df.iloc[:, 1] df[\'mask_xor\'] = df.iloc[:, 0] ^ df.iloc[:, 1] return df"},{"question":"You are provided with a `penguins` dataset and tasked with creating a Seaborn plot that effectively visualizes and labels data. Specifically, you will construct a faceted plot based on the `species` attribute, using customized labels and titles. Follow the steps below to achieve this: Task 1. **Load the Dataset:** Use the Seaborn function to load the `penguins` dataset. 2. **Construct the Plot:** Create a faceted plot showing the relationship between `bill_length_mm` (x-axis) and `bill_depth_mm` (y-axis) with dependency on the `species` attribute: - Use dots (`Dot`) to represent individual points. - Color the dots by the `species`. 3. **Customize the Labels and Titles:** - Override the x-axis label to display \\"Bill Length (mm)\\". - Override the y-axis label to display \\"Bill Depth (mm)\\". - Set the title for the entire plot to \\"Penguin Species Comparison: Bill Dimensions\\". - Customize the title for each facet to display in uppercase. 4. **Enhance with a Legend:** - Add a legend to describe the colors representing different species. - Set the title for the legend to \\"Species\\". Input and Output - **Input:** ```python import seaborn.objects as so from seaborn import load_dataset ``` - **Output:** - A final faceted plot should be displayed with all the above customizations included. Constraints - Ensure you use the Seaborn `so.Plot` functionality for this task. - Ensure the legend and titles are appropriately customized as specified. Example Code Template You may start with the following template: ```python import seaborn.objects as so from seaborn import load_dataset # Load the dataset penguins = load_dataset(\\"penguins\\") # Construct the plot p = ( so.Plot(penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\") .add(so.Dot(), color=\\"species\\") ) # Customize the plot with labels and titles p.label(x=\\"Bill Length (mm)\\", y=\\"Bill Depth (mm)\\", legend=\\"Species\\") p.label(title=\\"Penguin Species Comparison: Bill Dimensions\\") # Facet by species and title each facet in uppercase p.facet(\\"species\\").label(title=str.upper) # Display the plot p.show() ``` Make sure your plot includes all the customizations as outlined.","solution":"import seaborn.objects as so from seaborn import load_dataset def create_penguin_plot(): # Load the dataset penguins = load_dataset(\\"penguins\\") # Construct the plot p = ( so.Plot(penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\") .add(so.Dot(), color=\\"species\\") .label(x=\\"Bill Length (mm)\\", y=\\"Bill Depth (mm)\\", legend=\\"Species\\") .label(title=\\"Penguin Species Comparison: Bill Dimensions\\") .facet(\\"species\\") .label(title=str.upper) ) # Display the plot p.show()"},{"question":"# Question: Advanced Seaborn Visualization Task Objective Create a complex and informative data visualization using the `seaborn` library to demonstrate your understanding of its advanced features. Dataset Use the built-in `penguins` dataset from seaborn. Task 1. Load the `penguins` dataset. 2. Create a faceted grid of KDE plots to show the distribution of `flipper_length_mm`. The plots should be faceted by `species` and colored by `sex`. 3. Customize the dimensions of the grid such that each subplot has a height of 5 and an aspect ratio of 0.7. 4. Ensure that each KDE plot has a marginal rug. 5. Add axis labels for each subplot: The x-axis should be labeled as \\"Flipper Length (mm)\\" and the y-axis should be labeled as \\"Density\\". 6. Set the titles for each facet to display the corresponding `species`. Constraints - Use only the functions and methods demonstrated in the provided seaborn documentation. - Ensure the final plot is clear, well-labeled, and visually appealing. Output The expected output is a complex plot with multiple facets, demonstrating the KDE distribution with proper labels and titles. Here is the detailed specification of the task: ```python import seaborn as sns import matplotlib.pyplot as plt # Task steps: # 1. Load the \'penguins\' dataset. # 2. Create a faceted grid of KDE plots faceted by \'species\' and colored by \'sex\'. # 3. Set individual plot dimensions with height=5 and aspect=0.7. # 4. Add a marginal rug to each KDE plot. # 5. Set axis labels for x: \\"Flipper Length (mm)\\" and y: \\"Density\\". # 6. Title each facet with the corresponding \'species\'. # Your Code Starts Here # Step 1: Load the penguins dataset penguins = sns.load_dataset(\\"penguins\\") # Step 2: Create the FacetGrid object with KDE plots g = sns.displot( data=penguins, x=\\"flipper_length_mm\\", hue=\\"sex\\", col=\\"species\\", kind=\\"kde\\", rug=True, height=5, aspect=0.7 ) # Step 5: Set the axis labels g.set_axis_labels(\\"Flipper Length (mm)\\", \\"Density\\") # Step 6: Set titles for each facet g.set_titles(\\"{col_name} penguins\\") # Your Code Ends Here plt.show() ``` Make sure your solution is efficient and adheres to the constraints provided.","solution":"import seaborn as sns import matplotlib.pyplot as plt def plot_penguins_kde(): Creates a complex seaborn plot showing the KDE distributions of flipper lengths in the penguins dataset, faceted by species and colored by sex. # Step 1: Load the penguins dataset penguins = sns.load_dataset(\\"penguins\\") # Step 2: Create the FacetGrid object with KDE plots g = sns.displot( data=penguins, x=\\"flipper_length_mm\\", hue=\\"sex\\", col=\\"species\\", kind=\\"kde\\", rug=True, height=5, aspect=0.7 ) # Step 5: Set the axis labels g.set_axis_labels(\\"Flipper Length (mm)\\", \\"Density\\") # Step 6: Set titles for each facet g.set_titles(\\"{col_name} penguins\\") plt.show()"},{"question":"Objective The goal of this assessment is to test your understanding of the `torch.cond` function, particularly in creating models with dynamic behavior based on tensor characteristics. Problem Statement You are tasked with creating a PyTorch model that dynamically changes its behavior based on the sum of all elements in the input tensor. Specifically, if the sum of all elements is greater than a given threshold, the model should compute the first derivative of the sine of the tensor. Otherwise, it should compute the first derivative of the cosine of the tensor. Requirements 1. Implement a model class `DynamicBehaviorModel` that inherits from `torch.nn.Module`. 2. In this model class: - Define two functions: `true_fn` and `false_fn`. The `true_fn` computes the first derivative of the sine of the input tensor, while the `false_fn` computes the first derivative of the cosine of the input tensor. - Use `torch.cond` to switch between these two functions based on whether the sum of the input tensor elements is greater than `threshold`. 3. Use the following signature for your `DynamicBehaviorModel` class: ```python import torch class DynamicBehaviorModel(torch.nn.Module): def __init__(self, threshold: float): super(DynamicBehaviorModel, self).__init__() self.threshold = threshold def forward(self, x: torch.Tensor) -> torch.Tensor: # Implement the conditional logic here ``` 4. The `forward` method should use `torch.cond` to apply the correct function based on the sum of the input tensor elements. Input Format - `threshold`: A float value that determines the switching point for the conditional logic. - `x`: A `torch.Tensor` input to the model. Output Format - Returns a `torch.Tensor` that is the result of either the first derivative of sine or cosine of the input tensor, depending on the sum of its elements. Constraints - You should not use explicit loops; instead, leverage `torch` functions for tensor operations. - Assume the input tensor has at least one element. Example Usage ```python import torch model = DynamicBehaviorModel(threshold=10.0) inp = torch.tensor([1.0, 2.0, 3.0]) # Sum is 6.0 <= 10.0 output = model(inp) # Should apply the first derivative of cosine to inp inp2 = torch.tensor([10.0, 20.0, 30.0]) # Sum is 60.0 > 10.0 output2 = model(inp2) # Should apply the first derivative of sine to inp2 ``` Implement the `DynamicBehaviorModel` class below: ```python import torch class DynamicBehaviorModel(torch.nn.Module): def __init__(self, threshold: float): super(DynamicBehaviorModel, self).__init__() self.threshold = threshold def forward(self, x: torch.Tensor) -> torch.Tensor: def true_fn(x): return torch.autograd.functional.jacobian(torch.sin, x) def false_fn(x): return torch.autograd.functional.jacobian(torch.cos, x) return torch.cond(x.sum() > self.threshold, true_fn, false_fn, (x,)) ``` # Evaluation Criteria - Correct implementation of conditional logic using `torch.cond`. - Correct computation of first derivatives using autograd. - Code readability and adherence to best practices. Good luck!","solution":"import torch class DynamicBehaviorModel(torch.nn.Module): def __init__(self, threshold: float): super(DynamicBehaviorModel, self).__init__() self.threshold = threshold def forward(self, x: torch.Tensor) -> torch.Tensor: if x.sum() > self.threshold: return torch.cos(x) else: return -torch.sin(x)"},{"question":"You are required to implement a function `analyze_robots_txt(url)`, which will utilize the `RobotFileParser` class to parse the \\"robots.txt\\" file of the provided URL and return specific information based on its contents. Your function should: 1. Parse the \\"robots.txt\\" file located at the given URL. 2. Return a dictionary containing: - If the user agent \'*\' can fetch the root URL of the site. - The crawl delay for the user agent \'*\'. - The request rate for the user agent \'*\', formatted as a tuple `(requests, seconds)`. - A list of sitemap URLs. # Function Signature ```python def analyze_robots_txt(url: str) -> dict: pass ``` # Input - `url` (str): The URL of the site whose \\"robots.txt\\" file you want to analyze. Ensure to construct the URL to \\"robots.txt\\" correctly. # Output - Returns a dictionary with the following keys: - `\'can_fetch_root\'`: A boolean indicating if the user agent \'*\' can fetch the root URL of the site. - `\'crawl_delay\'`: An integer or `None`, representing the crawl delay for the user agent \'*\'. - `\'request_rate\'`: A tuple `(requests, seconds)` or `None`, representing the request rate for the user agent \'*\'. - `\'sitemaps\'`: A list of URLs found in the \\"Sitemap\\" directive of the \\"robots.txt\\" file. # Example ```python result = analyze_robots_txt(\\"http://www.example.com\\") print(result) # Example output could be: # { # \'can_fetch_root\': True, # \'crawl_delay\': 10, # \'request_rate\': (5, 30), # \'sitemaps\': [\\"http://www.example.com/sitemap1.xml\\", \\"http://www.example.com/sitemap2.xml\\"] # } ``` # Constraints - Ensure to handle cases where certain parameters may not be present in the \\"robots.txt\\" file and return `None` in such cases. - The function should handle exceptions when the \\"robots.txt\\" file cannot be fetched or parsed. # Performance Requirements - Efficiently handle network operations to avoid long delays. - Ensure the function works correctly even for larger \\"robots.txt\\" files.","solution":"import urllib.robotparser import urllib.request def analyze_robots_txt(url: str) -> dict: Analyzes the robots.txt file of the provided URL and returns specific information based on its contents. Args: url (str): The URL of the site whose robots.txt file you want to analyze. Returns: dict: A dictionary containing information about user agent \'*\'. robots_url = urllib.parse.urljoin(url, \'/robots.txt\') rp = urllib.robotparser.RobotFileParser() try: rp.set_url(robots_url) rp.read() can_fetch_root = rp.can_fetch(\'*\', url) crawl_delay = rp.crawl_delay(\'*\') request_rate = rp.request_rate(\'*\') sitemaps = rp.site_maps() if request_rate is not None: request_rate = (request_rate.requests, request_rate.seconds) result = { \'can_fetch_root\': can_fetch_root, \'crawl_delay\': crawl_delay, \'request_rate\': request_rate, \'sitemaps\': sitemaps if sitemaps is not None else [] } except Exception as e: # In case of any exception, return default values result = { \'can_fetch_root\': False, \'crawl_delay\': None, \'request_rate\': None, \'sitemaps\': [] } return result"},{"question":"# Coding Task: Nested Functions and Exception Handling Objective: To assess your understanding of local and global variable scope, nested functions, and exception handling in Python. Problem Statement: Write a function `process_data` that takes a list of tuples as input. Each tuple contains two elements: an integer and a string. The function should process each tuple and return a dictionary where the keys are the integers and the values are the processed strings. The processing involves the following steps: 1. Concatenate the string with a suffix `\'_processed\'`. 2. If the integer is even, convert the string to uppercase. 3. If the integer is odd, convert the string to lowercase. Additionally, implement exception handling for the following cases: 1. Raise a `ValueError` if any tuple does not contain exactly two elements. 2. Raise a `TypeError` if the first element of any tuple is not an integer or the second element is not a string. Input: - A list of tuples, where each tuple contains an integer and a string. Output: - A dictionary with integers as keys and processed strings as values. Function Signature: ```python def process_data(data: List[Tuple[int, str]]) -> Dict[int, str]: ``` Constraints: - You can assume that the list will have at least one tuple. - Make sure to use nested functions where appropriate. - Do not use any external libraries. Example: ```python data = [(2, \'hello\'), (3, \'world\')] output = process_data(data) print(output) # Output: {2: \'HELLO_processed\', 3: \'world_processed\'} ``` Additional Information: - Use nested functions to break down the problem into smaller manageable tasks. - Ensure proper exception handling for invalid inputs. Now, implement the function `process_data` based on the problem statement above.","solution":"from typing import List, Tuple, Dict def process_data(data: List[Tuple[int, str]]) -> Dict[int, str]: def validate_tuple(tpl: Tuple[int, str]): if len(tpl) != 2: raise ValueError(\\"Each tuple must contain exactly two elements.\\") if not isinstance(tpl[0], int): raise TypeError(\\"The first element of each tuple must be an integer.\\") if not isinstance(tpl[1], str): raise TypeError(\\"The second element of each tuple must be a string.\\") def process_tuple(tpl: Tuple[int, str]) -> str: num, text = tpl suffix = \'_processed\' if num % 2 == 0: return text.upper() + suffix else: return text.lower() + suffix result = {} for tpl in data: validate_tuple(tpl) processed_string = process_tuple(tpl) result[tpl[0]] = processed_string return result"},{"question":"Objective: This question aims to test your understanding of using the `torch.fft` module for signal processing in PyTorch. You will implement functions to perform Fourier transforms and filtering operations on a given signal. Question: Write a function `apply_fft_filter` that performs the following steps: 1. Computes the Fourier Transform of a 1D real-valued signal. 2. Applies a high-pass filter to zero out all frequency components below a given threshold frequency. 3. Computes the inverse Fourier Transform to transform the filtered signal back to the time domain. Function Signature: ```python import torch def apply_fft_filter(signal: torch.Tensor, threshold: float) -> torch.Tensor: Applies a high-pass filter to the given 1D real-valued signal and returns the filtered signal. Parameters: signal (torch.Tensor): A 1D tensor containing the real-valued input signal. threshold (float): The threshold frequency for the high-pass filter. All frequency components below this value will be zeroed out. Returns: torch.Tensor: A 1D tensor containing the filtered signal in the time domain. ``` Input: - `signal`: A 1D tensor of shape `(N,)` containing the real-valued input signal. - `threshold`: A float representing the threshold frequency for the high-pass filter. Output: - A 1D tensor containing the filtered signal in the time domain. Constraints: - The input signal tensor will have a length N where 32 <= N <= 4096. - The threshold frequency will be a non-negative float value. Additional Information: - Use the `torch.fft.fft` and `torch.fft.ifft` functions for the Fourier and inverse Fourier transforms, respectively. - Use `torch.fft.fftfreq` to obtain the frequency bins corresponding to the FFT of the input signal. - Ensure that the final output is a real-valued tensor, as the input signal is real-valued. Example: ```python import torch # Example signal signal = torch.tensor([0.0, 0.707, 1.0, 0.707, 0.0, -0.707, -1.0, -0.707]) threshold = 0.2 # Function call filtered_signal = apply_fft_filter(signal, threshold) print(filtered_signal) ``` ```python # Expected Output (after filtering and inverse FFT): # This is a sample expected output and may not be exact due to floating-point operations. tensor([some_values]) ``` Note: The exact values in the output will depend on the specific implementation and signal processing performed.","solution":"import torch def apply_fft_filter(signal: torch.Tensor, threshold: float) -> torch.Tensor: Applies a high-pass filter to the given 1D real-valued signal and returns the filtered signal. Parameters: signal (torch.Tensor): A 1D tensor containing the real-valued input signal. threshold (float): The threshold frequency for the high-pass filter. All frequency components below this value will be zeroed out. Returns: torch.Tensor: A 1D tensor containing the filtered signal in the time domain. # Compute the FFT of the input signal fft_signal = torch.fft.fft(signal) # Get the frequency bins corresponding to the FFT freqs = torch.fft.fftfreq(signal.size(0)) # Apply the high-pass filter by zeroing out frequencies below the threshold mask = torch.abs(freqs) >= threshold fft_signal_filtered = fft_signal * mask # Compute the inverse FFT to transform back to the time domain filtered_signal = torch.fft.ifft(fft_signal_filtered) # Ensure the output is real-valued filtered_signal_real = filtered_signal.real return filtered_signal_real"},{"question":"# PyTorch Testing: Implement and Verify Model Output You are provided with an implementation of a simple feedforward neural network and some test cases. However, the test cases are incomplete. Your task is to implement additional test functions using the utilities from `torch.testing` to ensure the correctness and robustness of the model. # Instructions 1. **Define the Model**: Implement a simple feedforward neural network in PyTorch with: - 2 hidden layers with 64 neurons each. - ReLU activations after each layer. - Input dimension of 100, and an output dimension of 10. 2. **Generate Test Tensors**: - Use `torch.testing.make_tensor` to generate random input tensors of the appropriate shapes for testing. 3. **Implement Test Functions**: - Write a test function using `torch.testing.assert_close` to verify that the outputs of the model for two identical input tensors are the same. - Write a test function using `torch.testing.assert_allclose` to check that the model\'s output is within a specified tolerance of expected values. # Expected Input and Output Formats - Model definition as a `torch.nn.Module`. - Input tensors generated within the test functions. - Test functions should raise an assertion error if the conditions are not met. # Constraints - Do not use any external libraries other than PyTorch. - Ensure the tests are self-contained and executable independently. # Performance Requirements - Tests should run efficiently and complete within reasonable time for small batch sizes (e.g., 32). # Example ```python import torch import torch.nn as nn # Define your model class class SimpleFeedForward(nn.Module): def __init__(self, input_dim, hidden_dim, output_dim): super(SimpleFeedForward, self).__init__() self.fc1 = nn.Linear(input_dim, hidden_dim) self.fc2 = nn.Linear(hidden_dim, hidden_dim) self.fc3 = nn.Linear(hidden_dim, output_dim) def forward(self, x): x = torch.relu(self.fc1(x)) x = torch.relu(self.fc2(x)) x = self.fc3(x) return x # Define your tests def test_model_output_identity(): model = SimpleFeedForward(100, 64, 10) input_tensor = torch.testing.make_tensor((32, 100), dtype=torch.float32) output1 = model(input_tensor) output2 = model(input_tensor) torch.testing.assert_close(output1, output2) def test_model_output_within_tolerance(): model = SimpleFeedForward(100, 64, 10) input_tensor = torch.testing.make_tensor((32, 100), dtype=torch.float32) expected_output = torch.testing.make_tensor((32, 10), dtype=torch.float32) output = model(input_tensor) # Assume expected_output is precomputed or obtained from some source torch.testing.assert_allclose(output, expected_output, rtol=1e-3, atol=1e-5) # Example code to run tests if __name__ == \\"__main__\\": test_model_output_identity() test_model_output_within_tolerance() ``` Ensure your tests are comprehensive and cover various edge cases, including varying tensor shapes and values.","solution":"import torch import torch.nn as nn class SimpleFeedForward(nn.Module): def __init__(self, input_dim, hidden_dim, output_dim): super(SimpleFeedForward, self).__init__() self.fc1 = nn.Linear(input_dim, hidden_dim) self.fc2 = nn.Linear(hidden_dim, hidden_dim) self.fc3 = nn.Linear(hidden_dim, output_dim) def forward(self, x): x = torch.relu(self.fc1(x)) x = torch.relu(self.fc2(x)) x = self.fc3(x) return x"},{"question":"Implementing Custom Bytearray Operations Objective: Design and implement a Python class `CustomBytearray` that mimics certain functionalities of the Python built-in `bytearray` type, utilizing some of the core operations and methods typically performed on bytearrays. Your implementation will demonstrate your comprehension of bytearray manipulation, particularly through operations such as concatenation, resizing, and converting to/from strings. Background: Your task is to implement a class that partially mimics the behavior of Python bytearrays. This includes initializing from various types of input, concatenating two instances, retrieving the contents, resizing, and converting from and to strings. Requirements: 1. **Initialization**: Implement the constructor `__init__` to initialize your `CustomBytearray` instance from: - A string. - A list of integers. - Another `CustomBytearray` instance. 2. **Concatenation**: Implement a method `concat` that concatenates the current `CustomBytearray` instance with another `CustomBytearray` instance and returns the result. 3. **Size**: Implement a method `size` that returns the size (length) of the `CustomBytearray`. 4. **As String**: Implement a method `as_string` that returns the content of the `CustomBytearray` as a string. 5. **Resize**: Implement a method `resize` that resizes the internal buffer of the `CustomBytearray` to a specified length. The content should be truncated or extended with null bytes (0x00) as necessary. Input/Output Formats: 1. **Initialization**: ```python # from string b1 = CustomBytearray(\\"hello\\") # from list of integers b2 = CustomBytearray([104, 101, 108, 108, 111]) # from another CustomBytearray b3 = CustomBytearray(b1) ``` 2. **Concatenation**: ```python # Concatenates b1 and b2 b4 = b1.concat(b2) ``` 3. **Size**: ```python # Returns the size of b1 size = b1.size() # Output should be 5 ``` 4. **As String**: ```python # Returns the content of b1 as a string content = b1.as_string() # Output should be \'hello\' ``` 5. **Resize**: ```python # Resize b1 to length 3 b1.resize(3) resized_content = b1.as_string() # Output should be \'hel\' # Resize b1 to length 6 b1.resize(6) resized_content = b1.as_string() # Output should be \'helx00x00x00\' ``` Constraints: - You are not allowed to use the built-in `bytearray` type directly in your implementation. - Instead, you should manage the internal content using an appropriate data structure like a list of integers. - Assume the input strings consist of ASCII characters only. Class Template: ```python class CustomBytearray: def __init__(self, initial): # your code here def concat(self, other): # your code here def size(self): # your code here def as_string(self): # your code here def resize(self, new_size): # your code here ``` Implement all the methods to fulfill the given requirements and constraints.","solution":"class CustomBytearray: def __init__(self, initial): if isinstance(initial, str): self._content = [ord(c) for c in initial] elif isinstance(initial, list): if all(isinstance(i, int) and 0 <= i <= 255 for i in initial): self._content = initial else: raise ValueError(\\"List items must be integers in range 0-255.\\") elif isinstance(initial, CustomBytearray): self._content = initial._content.copy() else: raise TypeError(\\"Unsupported type for initialization.\\") def concat(self, other): if not isinstance(other, CustomBytearray): raise TypeError(\\"Can only concatenate with another CustomBytearray.\\") return CustomBytearray(self._content + other._content) def size(self): return len(self._content) def as_string(self): return \'\'.join(chr(c) for c in self._content) def resize(self, new_size): if new_size < 0: raise ValueError(\\"New size must be non-negative.\\") if new_size < len(self._content): self._content = self._content[:new_size] else: self._content.extend([0] * (new_size - len(self._content)))"},{"question":"Objective Design a concurrent system that processes text files in parallel and aggregates word counts across multiple files. Task You will implement two functions: `count_words_in_file` and `aggregate_word_counts`. 1. **`count_words_in_file(file_path: str) -> Dict[str, int]`** - This function accepts a file path as input, reads the file, and returns a dictionary with words as keys and their counts as values. - Words are defined as sequences of alphanumeric characters, delimited by non-alphanumeric characters. - Convert words to lowercase to ensure case insensitivity. 2. **`aggregate_word_counts(file_paths: List[str]) -> Dict[str, int]`** - This function accepts a list of file paths and processes each file concurrently using threads. - It should use the `ThreadPoolExecutor` from the `concurrent.futures` module to parallelize the word counting. - The function should aggregate the word counts from all files and return a single dictionary with the combined word counts. Input and Output Formats 1. **count_words_in_file** - **Input**: A string representing the file path. - **Output**: A dictionary where keys are words (str) and values are the counts of those words (int). Example: ```python count_words_in_file(\\"file1.txt\\") ``` Output: ```python {\'hello\': 3, \'world\': 2, \'example\': 1} ``` 2. **aggregate_word_counts** - **Input**: A list of strings, each representing a file path. - **Output**: A dictionary where keys are words (str) and values are the aggregated counts of those words (int) across all files. Example: ```python aggregate_word_counts([\\"file1.txt\\", \\"file2.txt\\"]) ``` Output: ```python {\'hello\': 5, \'world\': 3, \'example\': 2} ``` Constraints - Assume that each file fits into memory. - Handle any file-related exceptions gracefully. - Ensure the implementation efficiently uses threading for concurrent execution. Performance Requirements - The implementation must efficiently handle up to 100 files with an average size of 10MB each. - Aim for O(N) time complexity for counting words in a file where N is the number of words in the file. - The thread management and aggregation should not significantly increase the overall complexity. Example Given the contents of `file1.txt`: ``` Hello world! Hello Python. ``` And `file2.txt`: ``` Python is an example. World of Robotics. ``` The function call: ```python aggregate_word_counts([\\"file1.txt\\", \\"file2.txt\\"]) ``` Should produce the output: ```python {\'hello\': 2, \'world\': 2, \'python\': 2, \'is\': 1, \'an\': 1, \'example\': 1, \'of\': 1, \'robotics\': 1} ```","solution":"import re from collections import defaultdict, Counter from concurrent.futures import ThreadPoolExecutor from typing import Dict, List def count_words_in_file(file_path: str) -> Dict[str, int]: word_count = defaultdict(int) try: with open(file_path, \'r\') as file: for line in file: words = re.findall(r\'bw+b\', line.lower()) for word in words: word_count[word] += 1 except Exception as e: print(f\\"Error reading file {file_path}: {e}\\") return word_count def aggregate_word_counts(file_paths: List[str]) -> Dict[str, int]: aggregated_word_count = Counter() with ThreadPoolExecutor() as executor: results = executor.map(count_words_in_file, file_paths) for result in results: aggregated_word_count.update(result) return dict(aggregated_word_count)"},{"question":"# Question: Accurate Summation with Floating Point Numbers In this question, you are required to implement a function that sums a large list of floating-point numbers accurately. Due to the limitations of floating-point arithmetic, simply using Python\'s built-in `sum()` function may not yield an accurate result. Implement the function `accurate_sum()` which takes a list of floating-point numbers and returns their sum. The function should use the `math.fsum()` method to ensure the sum is as accurate as possible. Additionally, write a second function `fractional_representation()` which takes a floating-point number and returns its exact fractional representation as a tuple of two integers (numerator, denominator) using the `as_integer_ratio()` method. Function Signatures ```python import math def accurate_sum(numbers: list[float]) -> float: pass def fractional_representation(number: float) -> tuple[int, int]: pass ``` Input Format - `accurate_sum`: - A list of `n` floating-point numbers, where (1 leq n leq 10^6). - `fractional_representation`: - A single floating-point number. Output Format - `accurate_sum`: - A single floating-point number representing the accurate sum of the input list. - `fractional_representation`: - A tuple of two integers representing the exact numerator and denominator of the given floating-point number. Constraints 1. The floating-point numbers in the input list for `accurate_sum` are positive and can have up to 15 decimal places. 2. The floating-point number for `fractional_representation` is positive and can have up to 15 decimal places. Example ```python numbers = [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1] print(accurate_sum(numbers)) # Output: 1.0 number = 0.1 print(fractional_representation(number)) # Output: (3602879701896397, 36028797018963968) ``` Explanation For `accurate_sum`, the sum of ten 0.1 values should be exactly 1.0, but simply using the built-in `sum()` may not yield an accurate result due to floating-point precision errors. The `math.fsum()` function should handle this correctly. For `fractional_representation`, the floating-point number 0.1 is represented as a fraction (3602879701896397/36028797018963968), which is its precise binary representation. **Note**: Thoroughly test your functions with various edge cases to ensure accuracy.","solution":"import math def accurate_sum(numbers: list[float]) -> float: Returns the accurate sum of a list of floating-point numbers using math.fsum(). return math.fsum(numbers) def fractional_representation(number: float) -> tuple[int, int]: Returns the exact fractional representation of a floating-point number using as_integer_ratio(). return number.as_integer_ratio()"},{"question":"Objective Your task is to create a set of visualizations using the seaborn library that demonstrates your understanding of the fundamental and advanced features of this package. You will use a provided dataset and generate plots that showcase various customization options and techniques discussed in the seaborn documentation. Dataset You will use the `\\"tips\\"` dataset available in seaborn for this task. Tasks 1. **Scatter Plot:** - Create a scatter plot showing the relationship between the total bill and the tip amount. - Color the points based on whether the customer is a smoker or not using the hue semantic. - Use different marker styles for lunch and dinner times. 2. **Customized Scatter Plot:** - Create a scatter plot showing the relationship between the total bill and tip amount. - Change the point sizes based on the number of people (size). - Use a custom color palette for the hue semantic (smoker). 3. **Line Plot:** - Create a line plot showing the average tip amount over the total bill range. - Use different colors for each day of the week (day). - Add 95% confidence intervals around the mean tip amount. 4. **Faceted Scatter Plot:** - Create a faceted scatter plot showing the total bill vs. tip amount. - Facet the plot into columns based on the time (lunch or dinner) and rows based on smoker status. 5. **Advanced Line Plot:** - Create a line plot showing how the average tip amount varies with the total bill for each day of the week. - Use different line styles for each sex. - Disable the error bars to reduce computation time. Constraints - You should not use any loops to create these plots. - Each plot must be created using a single seaborn function call. - The plots should be clearly labeled and styled for effective communication. Expected Output You should submit the Python code that generates the plots as described. Ensure that your code is well-commented and follows best practices for readability.","solution":"import seaborn as sns import matplotlib.pyplot as plt # Load the \\"tips\\" dataset tips = sns.load_dataset(\\"tips\\") # 1. Scatter Plot def scatter_plot(): plt.figure(figsize=(8, 6)) sns.scatterplot(x=\'total_bill\', y=\'tip\', hue=\'smoker\', style=\'time\', data=tips) plt.title(\'Scatter Plot of Total Bill vs Tip by Smoker and Time\') plt.xlabel(\'Total Bill\') plt.ylabel(\'Tip\') plt.show() # 2. Customized Scatter Plot def customized_scatter_plot(): plt.figure(figsize=(8, 6)) sns.scatterplot(x=\'total_bill\', y=\'tip\', hue=\'smoker\', size=\'size\', palette=\'coolwarm\', data=tips) plt.title(\'Customized Scatter Plot of Total Bill vs Tip by Smoker with Size Variation\') plt.xlabel(\'Total Bill\') plt.ylabel(\'Tip\') plt.show() # 3. Line Plot def line_plot(): plt.figure(figsize=(8, 6)) sns.lineplot(x=\'total_bill\', y=\'tip\', hue=\'day\', ci=95, data=tips) plt.title(\'Line Plot of Average Tip Amount over Total Bill Range by Day of the Week\') plt.xlabel(\'Total Bill\') plt.ylabel(\'Tip\') plt.show() # 4. Faceted Scatter Plot def faceted_scatter_plot(): g = sns.FacetGrid(tips, col=\'time\', row=\'smoker\', height=4, aspect=1.5) g.map(sns.scatterplot, \'total_bill\', \'tip\') g.add_legend() g.set_axis_labels(\'Total Bill\', \'Tip\') g.set_titles(col_template=\'{col_name}\', row_template=\'{row_name}\') plt.show() # 5. Advanced Line Plot def advanced_line_plot(): plt.figure(figsize=(8, 6)) sns.lineplot(x=\'total_bill\', y=\'tip\', hue=\'day\', style=\'sex\', ci=None, data=tips) plt.title(\'Advanced Line Plot of Average Tip Amount over Total Bill Range by Day and Sex\') plt.xlabel(\'Total Bill\') plt.ylabel(\'Tip\') plt.show()"},{"question":"Objective You are required to demonstrate your understanding of the `pwd` module in Python by implementing a complex function that utilizes its capabilities. The function should extract and process user information effectively. Problem Statement Implement a function `user_info_summary(uid: int) -> str:` that retrieves and summarizes user information for a given numeric user ID (`uid`). Your function should: 1. Retrieve the user\'s password database entry using the `pwd` module. 2. Format the retrieved information into a readable string summary. 3. Handle errors appropriately (e.g., if the user ID does not exist, the function should raise a `ValueError` with an appropriate message). Input - `uid` (int): A numeric user ID. Output - A formatted string with the user\'s information. The string should contain the following information (each on a new line): - `Login Name: <pw_name>` - `User ID: <pw_uid>` - `Group ID: <pw_gid>` - `GECOS: <pw_gecos>` - `Home Directory: <pw_dir>` - `Shell: <pw_shell>` Constraints - You must use the `pwd` module to retrieve user information. - Your solution should handle exceptions and edge cases. - Assume the function is called on a Unix-based system. Example ```python # Example output format for uid = 0 (typically the root user) print(user_info_summary(0)) ``` ``` Login Name: root User ID: 0 Group ID: 0 GECOS: root Home Directory: /root Shell: /bin/bash ``` # Note: - The example output assumes specific values for illustrative purposes; actual values may vary depending on the system configuration.","solution":"import pwd def user_info_summary(uid: int) -> str: Retrieves and summarizes user information for a given numeric user ID (uid). Args: uid (int): A numeric user ID. Returns: str: A formatted string with the user\'s information. Raises: ValueError: If the user ID does not exist. try: user_info = pwd.getpwuid(uid) except KeyError: raise ValueError(f\\"No user with uid {uid} exists\\") summary = ( f\\"Login Name: {user_info.pw_name}n\\" f\\"User ID: {user_info.pw_uid}n\\" f\\"Group ID: {user_info.pw_gid}n\\" f\\"GECOS: {user_info.pw_gecos}n\\" f\\"Home Directory: {user_info.pw_dir}n\\" f\\"Shell: {user_info.pw_shell}\\" ) return summary"},{"question":"You are tasked with creating a task scheduler using the `graphlib` module\'s `TopologicalSorter` class. This scheduler must manage tasks, ensuring that all dependencies are respected, i.e., a task can only be executed once all its dependencies have been completed. You will also need to detect cycles in the task dependencies to prevent the scheduler from getting stuck in an infinite loop. # Function Signature ```python def task_scheduler(tasks: Dict[str, List[str]]) -> List[str]: ``` # Input - `tasks`: A dictionary where keys are task names (strings) and values are lists of names of tasks that must be completed before the key task can be started. # Output - A list of task names in the order they should be executed to respect all dependencies. If a cycle is detected, the function should return an empty list. # Constraints - All task names are unique. - The graph represented by the `tasks` dictionary is not guaranteed to be acyclic. - The number of tasks is at most 1000. - Task dependencies are well-formed, meaning that each dependency list contains only valid task names. # Example ```python tasks = { \\"D\\": [\\"B\\", \\"C\\"], \\"C\\": [\\"A\\"], \\"B\\": [\\"A\\"], \\"A\\": [] } assert task_scheduler(tasks) in [[\\"A\\", \\"B\\", \\"C\\", \\"D\\"], [\\"A\\", \\"C\\", \\"B\\", \\"D\\"]] tasks_with_cycle = { \\"A\\": [\\"B\\"], \\"B\\": [\\"C\\"], \\"C\\": [\\"A\\"] } assert task_scheduler(tasks_with_cycle) == [] ``` # Hint Use the `graphlib.TopologicalSorter` class to manage the dependencies and perform the topological sort. Utilize the functionality of detecting cycles to handle invalid input graphs gracefully.","solution":"from graphlib import TopologicalSorter, CycleError from typing import Dict, List def task_scheduler(tasks: Dict[str, List[str]]) -> List[str]: Schedule tasks based on their dependencies. Args: tasks (Dict[str, List[str]]): A dictionary where the keys are task names and values are lists of task names that need to be completed before the key task. Returns: List[str]: An ordered list of task names that respect the dependencies. If a cycle is detected, return an empty list. ts = TopologicalSorter(tasks) try: ordered_tasks = list(ts.static_order()) return ordered_tasks except CycleError: return []"},{"question":"Overview You are required to demonstrate your understanding of weak reference objects in Python by managing a list of objects using weak references, creating proxies for these objects, and handling their lifecycle during garbage collection. Task 1. Implement a class `TrackableObject` which contains: - An `id` attribute that identifies each object uniquely. - A `name` attribute to store the name of the object. - An appropriate `__init__` method. 2. Create a class `WeakRefManager`: - Initialize with a list of `TrackableObject` instances. - Create weak references for each `TrackableObject` and manage these references in a dictionary where keys are object `id`s. - Implement a method to create weak reference proxies for each stored object. - Implement a callback function that triggers upon the deletion of a `TrackableObject`, printing the object `id`. - Implement a method `clear_references` to clear all weak references. 3. Demonstrate the usage of `WeakRefManager` by: - Creating instances of `TrackableObject`. - Adding these instances to `WeakRefManager`. - Creating proxies for all stored objects. - Deleting some objects and observing the callbacks. - Clearing remaining weak references. Input and Output - **Input**: Your implementation will be tested by initializing `TrackableObject` instances, adding them to `WeakRefManager`, creating proxies, deleting objects, and calling `clear_references`. - **Output**: Appropriate print statements showing the management of weak references, objects being deleted, and proxies in action. Constraints - Use only the standard `weakref` library in Python. - Ensure the callback function correctly handles deletions and prints the `id`. Example: Consider the following use case: ```python # Create instances obj1 = TrackableObject(1, \\"Alpha\\") obj2 = TrackableObject(2, \\"Beta\\") obj3 = TrackableObject(3, \\"Gamma\\") # Manage with WeakRefManager manager = WeakRefManager([obj1, obj2, obj3]) manager.create_proxies() del obj1 manager.clear_references() ``` - Output should show that `obj1` is being deleted and other weak references are cleared successfully. Performance Your implementation should manage weak references efficiently without causing memory leaks or excessive slowdown due to reference management.","solution":"import weakref class TrackableObject: def __init__(self, id, name): self.id = id self.name = name class WeakRefManager: def __init__(self, objects): Initialize with a list of TrackableObject instances. self.refs = {} for obj in objects: self.refs[obj.id] = weakref.ref(obj, self.object_deleted) def object_deleted(self, weak_ref): Callback that triggers upon deletion of a TrackableObject. for obj_id, ref in self.refs.items(): if ref is weak_ref: print(f\\"Object with id {obj_id} has been deleted.\\") break def create_proxies(self): Create weak reference proxies for each stored object. self.proxies = {obj_id: weakref.proxy(ref()) for obj_id, ref in self.refs.items() if ref() is not None} def clear_references(self): Clear all weak references. self.refs.clear() print(\\"All weak references have been cleared.\\") # Example Usage: # Create instances obj1 = TrackableObject(1, \\"Alpha\\") obj2 = TrackableObject(2, \\"Beta\\") obj3 = TrackableObject(3, \\"Gamma\\") # Manage with WeakRefManager manager = WeakRefManager([obj1, obj2, obj3]) manager.create_proxies() del obj1 manager.clear_references()"},{"question":"**Objective:** Demonstrate your understanding of PyTorch\'s `torch.finfo` and `torch.iinfo` classes by implementing a function that prints specific numerical properties for various data types. **Task:** Create a function `print_type_properties` that accepts a list of PyTorch data type strings (e.g., `[\\"float32\\", \\"int64\\"]`) and prints out the properties for each type. Your function should handle both floating point and integer types correctly. **Function Signature:** ```python import torch def print_type_properties(dtypes: list): pass ``` **Input:** - `dtypes`: A list of strings representing PyTorch data types. Each string will be one of the following: `\\"float32\\"`, `\\"float64\\"`, `\\"float16\\"`, `\\"bfloat16\\"`, `\\"uint8\\"`, `\\"int8\\"`, `\\"int16\\"`, `\\"int32\\"`, `\\"int64\\"`. **Output:** - For each data type in the input list, print the numerical properties in the following format, separating properties by a comma: ``` Data type: {dtype} Bits: {bits} {Additional properties based on type} ``` - For floating point types (`finfo` attributes): ``` Eps: {eps} Max: {max} Min: {min} Tiny: {tiny} Resolution: {resolution} ``` - For integer types (`iinfo` attributes): ``` Max: {max} Min: {min} ``` **Example:** ```python dtypes = [\\"float32\\", \\"int64\\"] print_type_properties(dtypes) ``` Expected Output: ``` Data type: float32 Bits: 32 Eps: 1.1920928955078125e-07 Max: 3.4028234663852886e+38 Min: -3.4028234663852886e+38 Tiny: 1.1754943508222875e-38 Resolution: 1e-06 Data type: int64 Bits: 64 Max: 9223372036854775807 Min: -9223372036854775808 ``` **Constraints:** - You must verify the data type string before accessing the properties. If an invalid data type string is encountered, raise a `ValueError` with an appropriate message. - Use the corresponding PyTorch classes (`torch.finfo` and `torch.iinfo`) to obtain the properties. **Notes:** - Ensure your function is robust and can handle any combination of the provided data types. - Make sure the format of the printed properties matches the example output. **Assessment Criteria:** - Correctness: Proper implementation of the function and correct retrieval of data type properties. - Robustness: Proper handling of invalid data type strings. - Clarity: Clear and readable code with appropriate comments, if necessary.","solution":"import torch def print_type_properties(dtypes: list): Prints the numerical properties of the provided PyTorch data types. Args: dtypes (list): A list of strings representing PyTorch data types. for dtype in dtypes: try: if dtype.startswith(\\"float\\") or dtype == \\"bfloat16\\": torch_dtype = getattr(torch, dtype) info = torch.finfo(torch_dtype) print(f\\"Data type: {dtype}\\") print(f\\"Bits: {info.bits}\\") print(f\\"Eps: {info.eps}\\") print(f\\"Max: {info.max}\\") print(f\\"Min: {info.min}\\") print(f\\"Tiny: {info.tiny}\\") print(f\\"Resolution: {info.resolution}\\") elif dtype.startswith(\\"int\\") or dtype == \\"uint8\\" or dtype == \\"int8\\": torch_dtype = getattr(torch, dtype) info = torch.iinfo(torch_dtype) print(f\\"Data type: {dtype}\\") print(f\\"Bits: {info.bits}\\") print(f\\"Max: {info.max}\\") print(f\\"Min: {info.min}\\") else: raise ValueError(f\\"Invalid data type: {dtype}\\") except AttributeError: raise ValueError(f\\"Invalid data type: {dtype}\\") print() # Example usage: dtypes = [\\"float32\\", \\"int64\\"] print_type_properties(dtypes)"},{"question":"Background Optimizing the performance of machine learning models is crucial in real-world applications where prediction latency and throughput are important. Factors such as the number of features, input data sparsity, model complexity, and feature extraction latency can significantly affect the prediction performance. Task Write a Python function `evaluate_model_performance` that: 1. Accepts the following inputs: - `model`: A scikit-learn estimator (fitted). - `X`: A NumPy array or a sparse matrix representing the features. - `bulk`: A boolean indicating whether to perform bulk predictions or atomic predictions. - `assume_finite`: A boolean indicating whether to skip finiteness checks on the data. - `n_trials`: An integer specifying the number of times to repeat the prediction for averaging the results. 2. Measures and returns the average prediction latency and throughput per trial, considering: - Prediction latency as the time taken to make a prediction. - Throughput as the number of predictions made per second. Input - `model`: Fitted scikit-learn estimator. - `X`: NumPy array or sparse matrix of shape `(n_samples, n_features)`. - `bulk`: Boolean, default is `True`. - `assume_finite`: Boolean, default is `True`. - `n_trials`: Integer, default is `10`. Output - A dictionary with the following key-value pairs: - `average_latency`: The average time taken to make a single prediction in milliseconds. - `throughput`: The average number of predictions made per second. Constraints - You may use the `time` module to measure the prediction times. - Ensure that your solution works efficiently for large datasets (up to hundreds of thousands of samples). Example ```python from sklearn.linear_model import LogisticRegression from sklearn.datasets import make_classification from scipy.sparse import csr_matrix import numpy as np X, y = make_classification(n_samples=1000, n_features=20) sparse_X = csr_matrix(X) model = LogisticRegression().fit(X, y) result = evaluate_model_performance(model, sparse_X, bulk=True, assume_finite=True, n_trials=5) print(result) ``` Expected output format: ```python { \'average_latency\': 0.01, # average latency in milliseconds \'throughput\': 100000 # predictions per second } ``` Instructions 1. Implement the `evaluate_model_performance` function following the given specifications. 2. Test the function with various models and datasets, including dense and sparse representations. 3. Ensure the function handles both bulk and atomic prediction modes. 4. The function should use the appropriate scikit-learn configurations for finite checks if `assume_finite` is set to `True`.","solution":"import numpy as np import time def evaluate_model_performance(model, X, bulk=True, assume_finite=True, n_trials=10): Evaluates the performance of a scikit-learn model on the given dataset. Parameters: - model: Fitted scikit-learn estimator. - X: NumPy array or sparse matrix of shape (n_samples, n_features). - bulk: Boolean indicating whether to perform bulk predictions or atomic predictions. - assume_finite: Boolean indicating whether to skip finiteness checks on the data. - n_trials: Integer specifying the number of times to repeat the prediction for averaging the results. Returns: - A dictionary with the average prediction latency in milliseconds and throughput (predictions per second). from sklearn.utils.validation import check_is_fitted # Check if the model is already fitted check_is_fitted(model) total_latency = 0 # Repeat prediction to average the results for _ in range(n_trials): start_time = time.time() if bulk: if assume_finite: # Temporarily override scikit-learn checks with np.errstate(all=\'ignore\'): model.predict(X) else: model.predict(X) else: num_samples = X.shape[0] for i in range(num_samples): if assume_finite: with np.errstate(all=\'ignore\'): model.predict(X[i].reshape(1, -1)) else: model.predict(X[i].reshape(1, -1)) end_time = time.time() latency = (end_time - start_time) * 1000 # Convert to milliseconds total_latency += latency average_latency = total_latency / n_trials throughput = (X.shape[0] * n_trials) / (total_latency / 1000) # predictions per second return { \'average_latency\': average_latency, \'throughput\': throughput }"},{"question":"Objective In this coding assessment, you will demonstrate your understanding of the PyTorch distributed elastic events API. Specifically, you will implement a function to record events using the `torch.distributed.elastic.events.record` method. You will also need to define custom event objects and record them. Problem Statement 1. Implement a function `record_custom_event` that accepts the following parameters: - `event_name` (str): The name of the event. - `event_source` (str): The source of the event. - `event_value` (str): The value associated with the event. - `metadata` (dict): Optional metadata to be associated with the event (default is an empty dictionary). 2. The function should use the `torch.distributed.elastic.events.record` method to log this event. 3. The function should handle cases where the metadata is missing or is None by substituting it with an empty dictionary. 4. Create a custom event class `CustomEvent` which inherits from `torch.distributed.elastic.events.api.Event`. It should initialize with the following attributes: - `name` (str): The name of the event. - `source` (str): The source of the event. - `value` (str): The value associated with the event. - `metadata` (dict): Metadata associated with the event. 5. Ensure that your `record_custom_event` function can handle instances of `CustomEvent` as well as raw input parameters. Function Signature ```python import torch.distributed.elastic.events as events from torch.distributed.elastic.events.api import Event, EventSource, EventMetadataValue class CustomEvent(Event): def __init__(self, name: str, source: str, value: str, metadata: dict): self.name = name self.source = source self.value = value self.metadata = metadata def record_custom_event(event_name: str, event_source: str, event_value: str, metadata: dict = None): pass # Example usage: # event = CustomEvent(\\"test_event\\", \\"test_source\\", \\"test_value\\", {\\"key\\": \\"value\\"}) # record_custom_event(event.name, event.source, event.value, event.metadata) # or # record_custom_event(\\"test_event\\", \\"test_source\\", \\"test_value\\", {\\"key\\": \\"value\\"}) ``` Constraints - Use the `torch.distributed.elastic.events.record` method for logging the events. - Handle cases where metadata is not provided. - Ensure that `record_custom_event` can log both from raw input parameters and a `CustomEvent` instance. Test Cases 1. Record an event with all parameters provided. 2. Record an event without metadata. 3. Record an event using a `CustomEvent` instance. Write unit tests to evaluate the correctness of your implementation for the provided test cases. Performance Requirements - The function should handle event recording efficiently and correctly.","solution":"import torch.distributed.elastic.events as events from torch.distributed.elastic.events.api import Event class CustomEvent(Event): def __init__(self, name: str, source: str, value: str, metadata: dict): self.name = name self.source = source self.value = value self.metadata = metadata def record_custom_event(event_name: str, event_source: str, event_value: str, metadata: dict = None): if metadata is None: metadata = {} # Assuming events.record takes the event name, source, value, and metadata events.record(event_name, event_source, event_value, metadata)"},{"question":"# Question: Analyzing Ice Cream Sales Data with Seaborn You have been provided with a dataset named `ice_cream_sales.csv`, containing the following columns: - `temperature` (float): The temperature in degrees Celsius. - `sales` (int): The number of ice cream cones sold. - `day_of_week` (str): The day of the week (`Monday`, `Tuesday`, ..., `Sunday`). Your task is to create a series of visualizations to analyze the relationship between temperature, sales, and days of the week using the seaborn library. Specifically, you need to: 1. **Load and Inspect the Data**: - Load the dataset into a pandas DataFrame. - Print the first five rows of the DataFrame to inspect the data. 2. **Scatter Plot with Regression Line**: - Create a scatter plot showing the relationship between `temperature` and `sales`. - Add a linear regression line to this plot using seaborn\'s `lmplot`. 3. **Distribution Plot**: - Create a distribution plot for the `sales` variable and include a Kernel Density Estimate (KDE). 4. **Categorical Box Plot**: - Create a box plot showing the distribution of `sales` for each `day_of_week`. 5. **Customize the Plots**: - For all plots, set an appropriate theme using `sns.set_theme()`. - Add informative axis labels and a title to each plot. - Adjust font size and style for better readability. - Use matplotlib to perform any additional customization if needed. # Constraints: - Ensure that all visualizations are displayed correctly within a Jupyter notebook. - The dataset `ice_cream_sales.csv` file should be placed in the same directory as your notebook. # Expected Output: - The scatter plot with a regression line should show how `sales` is related to `temperature`. - The distribution plot should give insights into the distribution of `sales`. - The categorical box plot should illustrate the distribution of `sales` across different days of the week. # Example Code: ```python import pandas as pd import seaborn as sns import matplotlib.pyplot as plt # Load and inspect the data data = pd.read_csv(\'ice_cream_sales.csv\') print(data.head()) # Scatter plot with regression line sns.set_theme() sns.lmplot(data=data, x=\'temperature\', y=\'sales\') plt.xlabel(\'Temperature (°C)\') plt.ylabel(\'Ice Cream Sales\') plt.title(\'Relationship between Temperature and Ice Cream Sales\') plt.show() # Distribution plot sns.displot(data[\'sales\'], kde=True) plt.xlabel(\'Ice Cream Sales\') plt.title(\'Distribution of Ice Cream Sales\') plt.show() # Categorical box plot sns.catplot(data=data, kind=\'box\', x=\'day_of_week\', y=\'sales\') plt.xlabel(\'Day of the Week\') plt.ylabel(\'Ice Cream Sales\') plt.title(\'Ice Cream Sales per Day of the Week\') plt.show() ``` # Submission: Submit your Jupyter notebook file containing the required visualizations and any additional insights you gained from the plots.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def load_and_inspect_data(file_path): Load dataset from the given file path and print the first five rows. data = pd.read_csv(file_path) print(data.head()) return data def scatter_plot_with_regression(data): Create a scatter plot between temperature and sales with a regression line. sns.set_theme() sns.lmplot(data=data, x=\'temperature\', y=\'sales\') plt.xlabel(\'Temperature (°C)\') plt.ylabel(\'Ice Cream Sales\') plt.title(\'Relationship between Temperature and Ice Cream Sales\') plt.show() def distribution_plot(data): Create a distribution plot for the sales variable with KDE. sns.set_theme() sns.displot(data[\'sales\'], kde=True) plt.xlabel(\'Ice Cream Sales\') plt.title(\'Distribution of Ice Cream Sales\') plt.show() def categorical_box_plot(data): Create a box plot showing distribution of sales for each day of the week. sns.set_theme() sns.catplot(data=data, kind=\'box\', x=\'day_of_week\', y=\'sales\') plt.xlabel(\'Day of the Week\') plt.ylabel(\'Ice Cream Sales\') plt.title(\'Ice Cream Sales per Day of the Week\') plt.show()"},{"question":"**Objective**: This question is intended to assess your understanding of Seaborn\'s `Plot` functionality, particularly focusing on faceted plots and axis sharing capabilities. **Question**: You are provided with the \\"penguins\\" dataset. Your task is to create a faceted scatter plot using seaborn\'s `Plot` class to visualize the relationship between \\"flipper_length_mm\\" and \\"body_mass_g\\". Ensure that the plots are faceted by the \\"species\\" and \\"island\\" variables. Additionally, customize the axis sharing such that: - The X-axis is shared within columns. - The Y-axis is shared within rows. Write a function `create_faceted_scatter_plot` that accomplishes this. **Input**: The function should not take any input but should load the \\"penguins\\" dataset internally as part of its operation. **Output**: The function should return the Seaborn Plot object representing the customized faceted scatter plot. **Constraints**: - Use Seaborn\'s `Plot` class and related methods. - Correctly facet the data by \\"species\\" and \\"island\\". - Implement the specified axis sharing requirements. **Example Usage**: ```python import seaborn.objects as so from seaborn import load_dataset def create_faceted_scatter_plot(): penguins = load_dataset(\\"penguins\\") p = ( so.Plot(penguins, x=\\"flipper_length_mm\\", y=\\"body_mass_g\\") .facet(col=\\"species\\", row=\\"island\\") .add(so.Dots()) .share(x=\\"col\\", y=\\"row\\") ) return p # Example function call plot = create_faceted_scatter_plot() plot.show() ``` Your implementation should match the expected functionality as described above.","solution":"import seaborn.objects as so from seaborn import load_dataset def create_faceted_scatter_plot(): Creates a faceted scatter plot using Seaborn\'s Plot class to visualize the relationship between \'flipper_length_mm\' and \'body_mass_g\', faceted by \'species\' and \'island\'. It customizes the axis sharing to share the X-axis within columns and the Y-axis within rows. # Load the penguins dataset penguins = load_dataset(\\"penguins\\") # Create the Seaborn Plot object p = ( so.Plot(penguins, x=\\"flipper_length_mm\\", y=\\"body_mass_g\\") .facet(col=\\"species\\", row=\\"island\\") .add(so.Dots()) .share(x=\\"col\\", y=\\"row\\") ) return p"},{"question":"Coding Assessment Question # Objective Implement a custom timer server and client in PyTorch, which leverages inter-process communication. The custom timer should use a shared memory approach for communication between the server and client. # Description 1. **CustomTimerServer**: - Extend the `TimerServer` class. - Use a shared memory block to keep track of the timer state. - Implement methods for starting a timer and checking its expiry. 2. **CustomTimerClient**: - Extend the `TimerClient` class. - Use the same shared memory block to communicate with the server. - Implement methods to send timer start requests and query expiry status. # Requirements 1. Implement `CustomTimerServer` with: - Method `start_timer(duration: float)` to start a timer. - Method `is_expired() -> bool` to check if the timer has expired. 2. Implement `CustomTimerClient` with: - Method `send_start_request(duration: float)` to request the server to start the timer. - Method `check_expiry() -> bool` to check if the timer has expired. 3. Both server and client should communicate using a shared memory block. # Constraints - Use of Python\'s multiprocessing or shared memory libraries is recommended. - Ensure that your implementation correctly handles concurrency and potential race conditions. # Input and Output Formats CustomTimerServer - `start_timer(duration: float)`: Starts the timer for the specified duration in seconds. - `is_expired() -> bool`: Returns `True` if the timer has expired; `False` otherwise. CustomTimerClient - `send_start_request(duration: float)`: Sends a request to start the timer for the specified duration in seconds. - `check_expiry() -> bool`: Returns `True` if the timer has expired; `False` otherwise. # Example Usage ```python # Server implementation class CustomTimerServer(TimerServer): def __init__(self): super().__init__() # Initialization code here def start_timer(self, duration: float): # Logic to start timer def is_expired(self) -> bool: # Logic to check expiry # Client implementation class CustomTimerClient(TimerClient): def __init__(self): super().__init__() # Initialization code here def send_start_request(self, duration: float): # Logic to request timer start def check_expiry(self) -> bool: # Logic to check timer expiry if __name__ == \\"__main__\\": # Example to demonstrate the use of your timer server and client server = CustomTimerServer() client = CustomTimerClient() client.send_start_request(5) # Start timer for 5 seconds print(client.check_expiry()) # Should print False initially time.sleep(6) # Wait for the timer to expire print(client.check_expiry()) # Should print True after 6 seconds ``` # Performance Requirements - Server and client should be able to handle multiple timer requests without crashes or undefined behavior. - The timer validity should be accurately maintained across different processes. *Note*: Ensure your solution handles all edge cases and includes necessary imports. The accuracy of the timer in handling and checking expiry should be considered.","solution":"import time from multiprocessing import shared_memory class CustomTimerServer: def __init__(self): self.shm = shared_memory.SharedMemory(create=True, size=5) self.shm.buf[0] = 0 self.shm.buf[1:5] = int(time.time()).to_bytes(4, \'little\') def start_timer(self, duration: float): # Set the timer start time and duration curr_time = int(time.time()) self.shm.buf[0] = int(duration) self.shm.buf[1:5] = curr_time.to_bytes(4, \'little\') def is_expired(self) -> bool: start_time = int.from_bytes(self.shm.buf[1:5], \'little\') duration = self.shm.buf[0] return time.time() >= start_time + duration class CustomTimerClient: def __init__(self, name): self.shm = shared_memory.SharedMemory(name=name) def send_start_request(self, duration: float): curr_time = int(time.time()) self.shm.buf[0] = int(duration) self.shm.buf[1:5] = curr_time.to_bytes(4, \'little\') def check_expiry(self) -> bool: start_time = int.from_bytes(self.shm.buf[1:5], \'little\') duration = self.shm.buf[0] return time.time() >= start_time + duration"},{"question":"# Python C API Interaction **Objective:** Implement a Python function that uses the Python C API to compile and run Python code provided as a string. The function should handle exceptions and return the result of the code execution. Use the provided documentation on the Python C API to accomplish this task. **Requirements:** 1. Define a function `execute_python_code(code_str: str) -> Any`: - **Input:** `code_str`: a string containing valid Python code. - **Output:** The result of executing the code in `code_str`, or an error message if an exception occurs. 2. Use the Python C API functions such as `Py_CompileString`, `PyEval_EvalCode`, and any necessary structures to accomplish the task. 3. Ensure proper error handling: - If there is a syntax error in the code, return a suitable error message. - If an exception is raised during execution, return the exception details. 4. Example usage: ```python result = execute_python_code(\'2 + 2\') print(result) # Output should be 4 result = execute_python_code(\'print(\\"Hello, World!\\")\') # Output should be the print statement result (\'Hello, World!\') as a side effect, and None as the return value. result = execute_python_code(\'1 / 0\') print(result) # Output should be an error message indicating division by zero. ``` **Constraints:** 1. The function should use Python\'s C API functions to compile and run the code. 2. Assume that `code_str` will always be a string. 3. The implementation should handle interactive code inputs, snippets, and file-like inputs effectively. 4. Performance is not the primary concern, but the function should ensure handling of typical code sizes efficiently. **Performance Requirements:** The function should handle typical code lengths without significant performance degradation. It should be robust and handle errors gracefully, ensuring no disruption in the calling Python code.","solution":"import ctypes import builtins def execute_python_code(code_str): try: compiled_code = compile(code_str, \'<string>\', \'exec\') local_vars = {} exec(compiled_code, {}, local_vars) return local_vars except Exception as e: return str(e)"},{"question":"**Problem Statement:** You are a software developer tasked with processing sequences of data representing students\' scores in various subjects. You need to implement a function that performs the following operations: 1. **Filter** students who have passed in all subjects. A student is considered to have passed if their score in all subjects is 50 or above. 2. **Calculate** the average score for each filtered student. 3. **Label** each student based on their average score: - \\"Excellent\\" for average scores 90 and above. - \\"Good\\" for average scores between 70 and 89. - \\"Fair\\" for average scores between 50 and 69. 4. **Sort** the students by their labels in the order: Excellent, Good, Fair. 5. **Return** a list of tuples where each tuple consists of the student\'s name and their label. **Function Signature:** ```python def process_student_scores(students: list[tuple[str, list[int]]]) -> list[tuple[str, str]]: # Your code here ``` **Input:** - `students`: A list of tuples, where each tuple contains a student\'s name (a string) and a list of their scores (integers). **Output:** - A sorted list of tuples, where each tuple contains a student\'s name and their label. **Example:** ```python students = [ (\\"Alice\\", [95, 85, 92]), (\\"Bob\\", [75, 80, 78]), (\\"Charlie\\", [60, 65, 70]), (\\"David\\", [40, 85, 60]), # This student did not pass all subjects ] expected_output = [ (\\"Alice\\", \\"Excellent\\"), (\\"Bob\\", \\"Good\\"), (\\"Charlie\\", \\"Fair\\"), ] ``` **Constraints:** - The length of `students` list will be 1 <= len(students) <= 10^3. - Each student\'s scores list will have the same number of subjects and the length will be 1 <= len(scores) <= 10. - Each score will be an integer between 0 and 100. **Notes:** - Utilize built-in functions and sequence manipulation techniques as needed. - Ensure your solution is efficient and readable.","solution":"def process_student_scores(students): passed_students = [] for student in students: name, scores = student if all(score >= 50 for score in scores): average_score = sum(scores) / len(scores) if average_score >= 90: label = \\"Excellent\\" elif 70 <= average_score < 90: label = \\"Good\\" else: label = \\"Fair\\" passed_students.append((name, label)) # Sort according to the order: Excellent, Good, Fair sorted_students = sorted(passed_students, key=lambda x: (\\"Excellent\\", \\"Good\\", \\"Fair\\").index(x[1])) return sorted_students"},{"question":"# Custom Pairwise Metric Computations with scikit-learn Problem Statement You are provided with a dataset represented as a 2D NumPy array. Your task is to implement a function that computes pairwise distances and kernels using both built-in functions from the `sklearn.metrics.pairwise` module and a custom distance function. You need to write a Python function `compute_pairwise_metrics` which performs the following tasks: 1. **Compute Pairwise Euclidean and Manhattan Distances:** - Use the `pairwise_distances` function from `sklearn.metrics` to compute Euclidean and Manhattan distances between rows of the dataset. 2. **Compute Pairwise Linear, RBF, and Custom Kernels:** - Use the `pairwise_kernels` function to compute linear and RBF kernels between rows of the dataset. - Implement a custom kernel function that takes two input vectors and computes the kernel as: [ k(x, y) = left( x cdot y^T + 1 right)^3 ] and use this custom kernel to compute pairwise kernel values. 3. **Return Results as a Dictionary:** - Return a dictionary where keys are the metric names (`\'Euclidean\', \'Manhattan\', \'Linear Kernel\', \'RBF Kernel\', \'Custom Kernel\'`) and values are the corresponding pairwise distance/kernel matrices. Input - A 2D NumPy array `X` of shape `(n_samples, n_features)` representing the dataset. Output - A dictionary containing pairwise distance/kernel matrices for each metric. The structure of the dictionary should be: ```python { \'Euclidean\': euclidean_dist_matrix, \'Manhattan\': manhattan_dist_matrix, \'Linear Kernel\': linear_kernel_matrix, \'RBF Kernel\': rbf_kernel_matrix, \'Custom Kernel\': custom_kernel_matrix } ``` Constraints - The dataset `X` will have at most 500 rows and 100 columns. - You should use a gamma value of `0.1` for the RBF kernel calculation. Function Signature ```python import numpy as np from sklearn.metrics import pairwise_distances from sklearn.metrics.pairwise import pairwise_kernels def compute_pairwise_metrics(X: np.ndarray) -> dict: # Your code here ``` Example ```python X = np.array([[2, 3], [3, 5], [5, 8]]) result = compute_pairwise_metrics(X) print(result[\'Euclidean\']) # Expected Output (approximately): # array([[0. , 2.23606798, 5.83095189], # [2.23606798, 0. , 3.60555128], # [5.83095189, 3.60555128, 0. ]]) print(result[\'Manhattan\']) # Expected Output: # array([[0., 3., 8.], # [3., 0., 5.], # [8., 5., 0.]]) ```","solution":"import numpy as np from sklearn.metrics import pairwise_distances from sklearn.metrics.pairwise import pairwise_kernels def custom_kernel(x, y): return (np.dot(x, y.T) + 1) ** 3 def compute_pairwise_metrics(X: np.ndarray) -> dict: # Compute pairwise Euclidean and Manhattan distances euclidean_dist_matrix = pairwise_distances(X, metric=\'euclidean\') manhattan_dist_matrix = pairwise_distances(X, metric=\'manhattan\') # Compute pairwise Linear and RBF kernels linear_kernel_matrix = pairwise_kernels(X, metric=\'linear\') rbf_kernel_matrix = pairwise_kernels(X, metric=\'rbf\', gamma=0.1) # Compute pairwise custom kernel custom_kernel_matrix = pairwise_kernels(X, metric=custom_kernel) # Return results as a dictionary return { \'Euclidean\': euclidean_dist_matrix, \'Manhattan\': manhattan_dist_matrix, \'Linear Kernel\': linear_kernel_matrix, \'RBF Kernel\': rbf_kernel_matrix, \'Custom Kernel\': custom_kernel_matrix }"},{"question":"**Objective**: Implement a Python function that reads from multiple text files, processes each line to remove all vowels, and writes back the modified content to the original files. Provide an option to create backup files before modification. **Function Definition**: ```python def remove_vowels_from_files(files: list, create_backup: bool = False, backup_extension: str = \'.bak\', encoding: str = \'utf-8\'): Reads each line from a list of files, removes all vowels from each line, and writes the modified lines back to the same files. Parameters: - files (list): List of filenames to be processed. - create_backup (bool): If True, creates backup of each file with the specified backup_extension. - backup_extension (str): Extension for the backup files. Defaults to \'.bak\'. - encoding (str): Encoding to be used for reading from and writing to files. Defaults to \'utf-8\'. Returns: - None pass ``` # Input: - `files`: A list of strings, where each string is a filename. - `create_backup`: A boolean indicating whether to create a backup of each file. - `backup_extension`: A string representing the extension to use for backup files. - `encoding`: A string representing the encoding format to use. Default is \'utf-8\'. # Output: - The function does not return anything. It modifies the input files directly. # Example: Suppose there are two text files, `file1.txt` and `file2.txt` with the following contents: ``` file1.txt: Hello World Python Programming file2.txt: Goodbye World File Input Module ``` After calling: ```python remove_vowels_from_files([\'file1.txt\', \'file2.txt\'], create_backup=True, backup_extension=\'.backup\') ``` The content of the files should be: ``` file1.txt: Hll Wrld Pythn Prgrmmng file2.txt: Gdby Wrld Fl npt Mdl ``` And backup files will be created as `file1.txt.backup` and `file2.txt.backup` with original content. # Constraints: - Assume the input files exist and are readable. - The function should handle empty files gracefully. - If `create_backup` is False, the files should be modified directly without creating backup files. # Additional Requirements: - Utilize the `fileinput` module’s capabilities for iterating over lines and handling file input/output. - Ensure that the program gracefully handles potential I/O errors. - Use string manipulation techniques efficiently to remove vowels from each line. # Usage of `fileinput` Features: - Use `fileinput.input(files=files, inplace=True, backup=backup_extension, encoding=encoding)` to handle line reading and in-place editing with or without backups. - Ensure usage of appropriate file and line tracking functions if needed.","solution":"import fileinput def remove_vowels_from_files(files: list, create_backup: bool = False, backup_extension: str = \'.bak\', encoding: str = \'utf-8\'): Reads each line from a list of files, removes all vowels from each line, and writes the modified lines back to the same files. Parameters: - files (list): List of filenames to be processed. - create_backup (bool): If True, creates backup of each file with the specified backup_extension. - backup_extension (str): Extension for the backup files. Defaults to \'.bak\'. - encoding (str): Encoding to be used for reading from and writing to files. Defaults to \'utf-8\'. Returns: - None vowels = \\"aeiouAEIOU\\" # Use `fileinput` to handle file reading and writing for line in fileinput.input(files=files, inplace=True, backup=backup_extension if create_backup else None, encoding=encoding): # Remove vowels from the line new_line = \'\'.join(char for char in line if char not in vowels) # Print the new line which `fileinput` will write back to the file print(new_line, end=\'\')"},{"question":"**Objective**: Enhance your understanding and proficiency with PyTorch MPS device management and profiling tools. Problem Statement You\'re tasked with developing a PyTorch module that effectively utilizes MPS functionalities to perform tensor operations on MPS devices, manage the random number generator state, and profile the performance of operations. Your implementation should include functions that: 1. **Check and Return Device Information**: - `mps_device_status()`: - Returns a dictionary with the total number of MPS devices and the current and recommended memory allocations. - **Output**: ```python { \\"device_count\\": int, \\"current_allocated_memory\\": int, # in bytes \\"recommended_max_memory\\": int # in bytes } ``` 2. **Set and Get RNG State**: - `set_rng_state(seed: int) -> None`: - Sets the random number generator state on the MPS device using the provided seed. - `get_current_rng_state() -> Any`: - Returns the current random number generator state on the MPS device. 3. **Profile Tensor Operations**: - `profile_tensor_operations() -> dict`: - Profiles the time taken to perform a set of tensor operations (such as tensor addition, multiplication) on the MPS device. - **Output**: ```python { \\"add_time\\": float, # Time taken for addition in milliseconds \\"mul_time\\": float # Time taken for multiplication in milliseconds } ``` 4. **Release Cached Memory**: - `release_unused_memory() -> None`: - Frees any cached memory currently held by the MPS allocator. Constraints - Ensure the module works only if MPS devices are available. - The profiling should clearly differentiate between the timing of different tensor operations. - Comment your code for clarity and maintainability. Implementation Requirements - You should use the `torch.mps` module wherever necessary. - The solution should be efficient and handle exceptions gracefully. **Note**: You can use synthetic data (random tensors) for tensor operations. Example Usage ```python if __name__ == \\"__main__\\": status = mps_device_status() print(status) set_rng_state(42) rng_state = get_current_rng_state() print(rng_state) profile_results = profile_tensor_operations() print(profile_results) release_unused_memory() print(\\"Memory released.\\") ``` Ensure your implementation handles the functionality as per the requirements and validates the availability of MPS devices before performing the operations.","solution":"import torch import time def mps_device_status(): Returns a dictionary with the number of MPS devices and current memory allocations. if not torch.cuda.is_available(): return {\\"error\\": \\"MPS devices are not available.\\"} device_count = torch.cuda.device_count() current_allocated_memory = torch.cuda.memory_allocated(0) recommended_max_memory = torch.cuda.max_memory_allocated(0) return { \\"device_count\\": device_count, \\"current_allocated_memory\\": current_allocated_memory, \\"recommended_max_memory\\": recommended_max_memory } def set_rng_state(seed): Sets the random number generator state on the MPS device. torch.manual_seed(seed) def get_current_rng_state(): Returns the current random number generator state on the MPS device. return torch.random.get_rng_state() def profile_tensor_operations(): Profiles the time taken to perform a set of tensor operations on the MPS device. if not torch.cuda.is_available(): return {\\"error\\": \\"MPS devices are not available.\\"} device = torch.device(\\"cuda\\") a = torch.rand((1000, 1000), device=device) b = torch.rand((1000, 1000), device=device) start_add = time.time() c = a + b end_add = time.time() start_mul = time.time() d = a * b end_mul = time.time() return { \\"add_time\\": (end_add - start_add) * 1000, \\"mul_time\\": (end_mul - start_mul) * 1000 } def release_unused_memory(): Frees any cached memory currently held by the MPS allocator. if not torch.cuda.is_available(): return {\\"error\\": \\"MPS devices are not available.\\"} torch.cuda.empty_cache()"},{"question":"Coding Assessment Question # Objective Write a function `check_feature_availability` that, given the name of a feature, returns a tuple containing: 1. The version in which the feature was optionally introduced. 2. The version in which the feature became or will become mandatory. 3. The compiler flag associated with the feature. If the feature is not found, the function should return `None`. # Detailed Specifications 1. **Input**: A string `feature_name` representing the name of a feature (e.g., `\\"generators\\"`). 2. **Output**: A tuple of three elements: - The version when the feature was optionally introduced, as a 5-tuple (e.g., `(2, 2, 0, \'alpha\', 1)`). - The version when the feature became mandatory, as a 5-tuple (e.g., `(2, 3, 0, \'final\', 0)`). - The compiler flag associated with the feature, as an integer. - If the feature does not exist, return `None`. # Constraints 1. You must use the `from __future__ import …` statements to access the features. 2. Do not hard-code the versions and compiler flags. Extract this information programmatically from the `__future__` module. 3. Assume the input will always be a valid string. # Example ```python def check_feature_availability(feature_name: str): # Your code here # Example Usage result = check_feature_availability(\\"generators\\") print(result) # Output: ((2, 2, 0, \'alpha\', 1), (2, 3, 0, \'final\', 0), 0x1000) ``` # Hint - Utilize the `__future__` module and inspect its attributes dynamically to fetch the required information. - The document states that each feature is represented as an instance of the `_Feature` class which has the methods `getOptionalRelease()` and `getMandatoryRelease()` to get the version details, and `compiler_flag` for the compiler flag. # Note Ensure the code handles various edge cases, such as feature names that do not exist.","solution":"import __future__ def check_feature_availability(feature_name: str): feature = getattr(__future__, feature_name, None) if feature is None: return None return (feature.getOptionalRelease(), feature.getMandatoryRelease(), feature.compiler_flag)"},{"question":"Advanced Usage of Data Classes Objective: The goal of this exercise is to assess your understanding of Python\'s `dataclasses` module by implementing a set of data classes with specific requirements and behaviors. Problem Statement: You are required to design a set of data classes to model a library system. The system should manage books and members using data classes while adhering to the following specifications: 1. **Book Class**: - Define a `Book` class with the following attributes: - `title` (str): The title of the book. - `author` (str): The author of the book. - `isbn` (str): The unique ISBN number of the book. - `copies` (int): The number of copies available. - Ensure the `isbn` attribute is immutable (cannot be changed after initialization). - Implement a method `borrow_book()` that decreases the number of available copies by 1 if any copies are available, otherwise raises an Exception with the message \\"Out of copies.\\" 2. **Member Class**: - Define a `Member` class with the following attributes: - `full_name` (str): The full name of the member. - `member_id` (int): The unique ID assigned to the member. - `books_borrowed` (List[Book]): A list of books currently borrowed by the member. - Implement a method `borrow_book(book: Book)` that allows a member to borrow a book if it is available. This method should call the `borrow_book` method of the `Book` class and add the book to the `books_borrowed` list. 3. **Library Class**: - Define a `Library` class with the following attributes: - `books` (List[Book]): A list of books available in the library. - `members` (List[Member]): A list of registered members. - Implement a method `register_member(member: Member)` that adds a new member to the library. - Implement a method `add_book(book: Book)` that adds a new book to the library. - Implement a method `borrow_book(member_id: int, isbn: str)` that allows a member to borrow a book using their member ID and the book\'s ISBN. This method should handle all necessary logic and raise appropriate exceptions if the member or book is not found or if the book is out of copies. Input and Output Formats: - **Book Class:** ```python class Book: def __init__(self, title: str, author: str, isbn: str, copies: int): # initialization logic here def borrow_book(self): # method logic here ``` - **Member Class:** ```python class Member: def __init__(self, full_name: str, member_id: int): # initialization logic here def borrow_book(self, book: Book): # method logic here ``` - **Library Class:** ```python class Library: def __init__(self): # initialization logic here def register_member(self, member: Member): # method logic here def add_book(self, book: Book): # method logic here def borrow_book(self, member_id: int, isbn: str): # method logic here ``` Constraints: - ISBNs are unique and can be assumed to be valid strings. - Member IDs are unique and can be assumed to be valid integers. - The method `borrow_book` in the `Library` class should handle exceptions gracefully and provide meaningful error messages. Performance Requirements: - The solution should efficiently handle searches within the list of books and members. Using these guidelines, implement the required functionality and demonstrate the usage of the classes with appropriate examples.","solution":"from dataclasses import dataclass, field from typing import List, Optional @dataclass(frozen=True) class Book: title: str author: str isbn: str copies: int # Since the class is frozen, we need to use object.__setattr__ to manipulate attribute values def borrow_book(self): if self.copies > 0: object.__setattr__(self, \'copies\', self.copies - 1) else: raise Exception(\\"Out of copies\\") @dataclass class Member: full_name: str member_id: int books_borrowed: List[Book] = field(default_factory=list) def borrow_book(self, book: Book): book.borrow_book() self.books_borrowed.append(book) @dataclass class Library: books: List[Book] = field(default_factory=list) members: List[Member] = field(default_factory=list) def register_member(self, member: Member): self.members.append(member) def add_book(self, book: Book): self.books.append(book) def borrow_book(self, member_id: int, isbn: str): member = self._find_member(member_id) book = self._find_book(isbn) if member and book: member.borrow_book(book) def _find_member(self, member_id: int) -> Optional[Member]: for member in self.members: if member.member_id == member_id: return member raise Exception(\\"Member not found\\") def _find_book(self, isbn: str) -> Optional[Book]: for book in self.books: if book.isbn == isbn: return book raise Exception(\\"Book not found\\")"},{"question":"**Problem: Demonstrate Understanding of Copy-on-Write Mechanism in pandas** We are given a dataset that contains information about various products in a store including their product IDs, names, categories, and stock levels. This dataset will be used to demonstrate various operations while ensuring adherence to Copy-on-Write (CoW) principles. Your task is to implement a function `process_inventory` that performs the following operations: 1. Load the dataset into a DataFrame. 2. Apply the necessary operations to: - Add a new column `total_value` which is calculated as `price * stock`. - Update the stock level for a given product category (e.g., update stock for all `Electronics` by reducing it by 10 units). - Correctly handle chained assignment to ensure no unintended modifications. - Demonstrate the creation of a NumPy array from a column and properly handle the read/write permissions of the array. 3. Return the final DataFrame with the applied changes. # Input: - `data_path` (str): Path to the CSV file containing the dataset. - `category` (str): The product category for which the stock levels need to be updated. - `reduction` (int): The amount by which the stock levels for the specified category should be reduced. # Output: - `DataFrame`: The modified DataFrame after applying the operations described. # Dataset CSV Format: - `product_id` (int): Unique identifier for the product. - `product_name` (str): Name of the product. - `category` (str): Category the product belongs to. - `price` (float): Price per unit of the product. - `stock` (int): Current stock level of the product. # Constraints: - Perform all operations adhering to CoW to avoid unintended changes across DataFrame objects. - Ensure that chained assignments are handled without triggering any `ChainedAssignmentError`. # Example Usage: ```python data_path = \'inventory.csv\' category = \'Electronics\' reduction = 10 # Example inventory.csv: # product_id,product_name,category,price,stock # 1,Smartphone,Electronics,699,50 # 2,Laptop,Electronics,999,30 # 3,Headphones,Accessories,199,100 df = process_inventory(data_path, category, reduction) # Expected Output DataFrame: # product_id product_name category price stock total_value # 0 1 Smartphone Electronics 699 40 27960 # 1 2 Laptop Electronics 999 20 19980 # 2 3 Headphones Accessories 199 100 19900 ``` # Implementation Notes: - Use `loc` or equivalent techniques to perform updates in compliance with CoW principles. - Ensure that numpy array views created from pandas columns are handled properly regarding read/write permissions. Implement the function `process_inventory` in the code cell below: ```python import pandas as pd def process_inventory(data_path: str, category: str, reduction: int) -> pd.DataFrame: # Load the dataset into a DataFrame df = pd.read_csv(data_path) # Add a new column `total_value` df[\'total_value\'] = df[\'price\'] * df[\'stock\'] # Update the stock level for a given category df.loc[df[\'category\'] == category, \'stock\'] -= reduction # Example of handling chained assignment df.loc[df[\'category\'] == category, \'total_value\'] = df[\'price\'] * df[\'stock\'] # Accessing underlying NumPy array and making it writeable if necessary arr = df[\'stock\'].to_numpy() if not arr.flags.writeable: arr = arr.copy() return df ```","solution":"import pandas as pd def process_inventory(data_path: str, category: str, reduction: int) -> pd.DataFrame: # Load the dataset into a DataFrame df = pd.read_csv(data_path) # Add a new column `total_value` df[\'total_value\'] = df[\'price\'] * df[\'stock\'] # Update the stock level for the given category and recalculate total_value with pd.option_context(\'mode.chained_assignment\', None): df.loc[df[\'category\'] == category, \'stock\'] -= reduction df.loc[df[\'category\'] == category, \'total_value\'] = df[\'price\'] * df[\'stock\'] # Accessing underlying NumPy array and making it writable if necessary arr = df[\'stock\'].to_numpy() if not arr.flags.writeable: arr.setflags(write=True) return df"},{"question":"**Problem Statement:** You are tasked with implementing a function to set up a neural network model, move it to the MPS device, and perform a forward pass. The function should: 1. Check if the MPS backend is available. 2. If available, create a tensor of shape `(batch_size, input_size)` filled with ones on the MPS device. 3. Move a simple feed-forward neural network model to the MPS device. 4. Perform a forward pass of the model using the created tensor and return the result. You will implement the following function: ```python import torch import torch.nn as nn def mps_forward_pass(batch_size: int, input_size: int, hidden_size: int, output_size: int): Performs a forward pass of a simple feed-forward neural network on the MPS device. Parameters: - batch_size (int): The number of samples in the input tensor. - input_size (int): The size of each input sample. - hidden_size (int): The size of the hidden layer. - output_size (int): The size of the output layer. Returns: - torch.Tensor: The result of the forward pass. # Step 1: Check if MPS is available if not torch.backends.mps.is_available(): raise RuntimeError(\\"MPS backend not available\\") # Step 2: Create a tensor of shape (batch_size, input_size) filled with ones on the MPS device mps_device = torch.device(\\"mps\\") input_tensor = torch.ones((batch_size, input_size), device=mps_device) # Step 3: Define a simple feed-forward neural network class SimpleNet(nn.Module): def __init__(self, input_size, hidden_size, output_size): super(SimpleNet, self).__init__() self.fc1 = nn.Linear(input_size, hidden_size) self.relu = nn.ReLU() self.fc2 = nn.Linear(hidden_size, output_size) def forward(self, x): out = self.fc1(x) out = self.relu(out) out = self.fc2(out) return out # Instantiate the model and move it to the MPS device model = SimpleNet(input_size, hidden_size, output_size) model.to(mps_device) # Step 4: Perform a forward pass of the model using the created tensor output = model(input_tensor) return output ``` **Constraints:** - The function should raise a `RuntimeError` with the message \\"MPS backend not available\\" if the MPS backend is not available. - The forward pass should use only operations that are supported on the MPS device. **Example Usage:** ```python output = mps_forward_pass(batch_size=10, input_size=5, hidden_size=3, output_size=2) print(output) ``` In the example above, the function should return the result of the forward pass of the simple feed-forward neural network with the specified input tensor on the MPS device.","solution":"import torch import torch.nn as nn def mps_forward_pass(batch_size: int, input_size: int, hidden_size: int, output_size: int): Performs a forward pass of a simple feed-forward neural network on the MPS device. Parameters: - batch_size (int): The number of samples in the input tensor. - input_size (int): The size of each input sample. - hidden_size (int): The size of the hidden layer. - output_size (int): The size of the output layer. Returns: - torch.Tensor: The result of the forward pass. # Step 1: Check if MPS is available if not torch.backends.mps.is_available(): raise RuntimeError(\\"MPS backend not available\\") # Step 2: Create a tensor of shape (batch_size, input_size) filled with ones on the MPS device mps_device = torch.device(\\"mps\\") input_tensor = torch.ones((batch_size, input_size), device=mps_device) # Step 3: Define a simple feed-forward neural network class SimpleNet(nn.Module): def __init__(self, input_size, hidden_size, output_size): super(SimpleNet, self).__init__() self.fc1 = nn.Linear(input_size, hidden_size) self.relu = nn.ReLU() self.fc2 = nn.Linear(hidden_size, output_size) def forward(self, x): out = self.fc1(x) out = self.relu(out) out = self.fc2(out) return out # Instantiate the model and move it to the MPS device model = SimpleNet(input_size, hidden_size, output_size) model.to(mps_device) # Step 4: Perform a forward pass of the model using the created tensor output = model(input_tensor) return output"},{"question":"Coding Assessment Question # Task Using seaborn’s object-oriented interface, create a custom scatter plot with the following specific properties: 1. **Dataset**: Define a custom dataset with following fields: - `date` (20 records in daily intervals starting from \'2020-01-01\'). - `category` (Randomly assigned categories \'A\', \'B\', or \'C\'). - `value1` (Random float between 10 and 100). - `value2` (Random float between 100 and 300). 2. **Plot Requirements**: 1. **Coordinates**: `date` on the x-axis and `value1` on the y-axis. 2. **Color**: - Use a sequential palette (\'viridis\') to map the color of the points corresponding to `value2`. - Assign different edge colors to points (`value2`). 3. **Markers**: Different markers for each category. 4. **Point Size**: Proportional to `value2`. 5. **Transparency**: Apply different alpha levels based on `value1` (normalized between 0.4 and 0.9) 3. **Styling**: - Theme: Use \'white\' background. - Hide gridlines and spines except bottom spine. - Increase default font sizes for labels and ticks. - Ensure ticks are shown for `date` and `value1` axis. - Title should be \\"Custom Scatter Plot\\". 4. **Output**: The plot should include a legend indicating categories with different markers. Ensure the plot is clear to understand with all elements styled appropriately. # Implementation Write a Python function `create_custom_plot` that takes no parameters and produces the specified plot using seaborn. Make sure your function includes all necessary imports. # Example ```python import seaborn as sns import pandas as pd import numpy as np from seaborn import objects as so import matplotlib as mpl import matplotlib.pyplot as plt def create_custom_plot(): # Define dataset dates = pd.date_range(start=\'2020-01-01\', periods=20, freq=\'D\') categories = np.random.choice([\'A\', \'B\', \'C\'], 20) value1 = np.random.uniform(10, 100, 20) value2 = np.random.uniform(100, 300, 20) df = pd.DataFrame({\'date\': dates, \'category\': categories, \'value1\': value1, \'value2\': value2}) # Normalizing value1 for alpha transparency alpha = 0.4 + 0.5 * (value1 - value1.min())/(value1.max() - value1.min()) # Create plot p = ( so.Plot(df, x=\\"date\\", y=\\"value1\\", color=\\"value2\\") .add(so.Dot(marker=\\"category\\", pointsize=\\"value2\\", alpha=alpha)) .scale(y=so.Continuous(), color=\\"viridis\\", marker=mpl.markers.MarkerStyle.filled_markers) .layout(size=(10, 6)) .theme({ \\"axes.facecolor\\": \'white\', \\"axes.edgecolor\\": \'black\', \\"axes.grid\\": False, \\"axes.spines.right\\": False, \\"axes.spines.top\\": False, \\"xtick.labelsize\\": 14, \\"ytick.labelsize\\": 14, \\"axes.labelsize\\": 16, \\"axes.titlesize\\": 18, }) .label(title=\\"Custom Scatter Plot\\") ) # Plot p.plot() create_custom_plot() ``` Ensure that your solution handles the constraints effectively and the plot meets the styling and data representation requirements stated in the task.","solution":"import seaborn as sns import pandas as pd import numpy as np import matplotlib.pyplot as plt def create_custom_plot(): # Define dataset np.random.seed(0) dates = pd.date_range(start=\'2020-01-01\', periods=20, freq=\'D\') categories = np.random.choice([\'A\', \'B\', \'C\'], 20) value1 = np.random.uniform(10, 100, 20) value2 = np.random.uniform(100, 300, 20) df = pd.DataFrame({\'date\': dates, \'category\': categories, \'value1\': value1, \'value2\': value2}) # Normalize value1 for alpha transparency alpha = 0.4 + 0.5 * (value1 - value1.min()) / (value1.max() - value1.min()) # Set plot theme sns.set_theme(style=\'white\') fig, ax = plt.subplots(figsize=(12, 8)) # Create scatter plot for category in np.unique(categories): sub_df = df[df[\'category\'] == category] sc = ax.scatter( sub_df[\'date\'], sub_df[\'value1\'], c=sub_df[\'value2\'], s=sub_df[\'value2\'], alpha=alpha[sub_df.index], label=category, edgecolor=\'w\', cmap=\'viridis\', marker={\'A\': \'o\', \'B\': \'s\', \'C\': \'X\'}[category] ) # Customize axes and spines ax.spines[\'right\'].set_visible(False) ax.spines[\'top\'].set_visible(False) ax.spines[\'left\'].set_visible(True) ax.spines[\'bottom\'].set_visible(True) # Set labels and title ax.set_xlabel(\'Date\', fontsize=16) ax.set_ylabel(\'Value1\', fontsize=16) ax.set_title(\'Custom Scatter Plot\', fontsize=18) # Customize ticks plt.xticks(fontsize=14) plt.yticks(fontsize=14) # Add legend legend = ax.legend(title=\'Category\') plt.setp(legend.get_texts(), fontsize=14) plt.setp(legend.get_title(), fontsize=16) # Show plot plt.show()"},{"question":"# Seaborn Swarm Plot Coding Challenge You are provided a dataset `tips` which contains information about tips in a restaurant. The dataset has the following columns: - `total_bill`: Total bill amount (numeric). - `tip`: Tip amount (numeric). - `sex`: Gender of the bill payer (categorical: \\"Male\\" or \\"Female\\"). - `smoker`: Whether the bill payer is a smoker (categorical: \\"Yes\\" or \\"No\\"). - `day`: Day of the week (categorical: \\"Thur\\", \\"Fri\\", \\"Sat\\", \\"Sun\\"). - `time`: Time of the day (categorical: \\"Lunch\\", \\"Dinner\\"). - `size`: Size of the party (numeric). Using this dataset, you need to perform the following tasks: 1. **Univariate Swarm Plot**: Create a swarm plot for the `total_bill` column to visualize its univariate distribution. Ensure that no points overlap by setting a suitable point size. 2. **Categorical Swarm Plot**: Create a swarm plot to visualize the `total_bill` against `day`. Use different colors for different `sex` values. 3. **Faceted Swarm Plot**: Create a faceted swarm plot using `catplot` to show `total_bill` across different days (`col=\\"day\\"` facet), with `sex` as the hue, and split by `time` using subplots. Ensure that the plot is visually appealing by adjusting the aspect ratio. # Constraints: - Use Seaborn only. - Ensure code readability with appropriate comments. - Address any potential points overlapping issues in your swarm plots by adjusting point sizes. # Performance: - Your plots should be rendered within 1-2 seconds. - Be mindful of overlapping points and handle them appropriately. # Expected Output: - Three swarm plots that address the three tasks described above, customized and visually clear. **Example Function Signature** ```python import seaborn as sns import matplotlib.pyplot as plt # Load the dataset tips = sns.load_dataset(\\"tips\\") def create_swarm_plots(data): # Task 1: Univariate Swarm Plot plt.figure() sns.swarmplot(data=data, x=\\"total_bill\\", size=3) plt.savefig(\\"univariate_swarm_plot.png\\") # Task 2: Categorical Swarm Plot plt.figure() sns.swarmplot(data=data, x=\\"total_bill\\", y=\\"day\\", hue=\\"sex\\", dodge=True) plt.savefig(\\"categorical_swarm_plot.png\\") # Task 3: Faceted Swarm Plot g = sns.catplot(data=data, kind=\\"swarm\\", x=\\"time\\", y=\\"total_bill\\", hue=\\"sex\\", col=\\"day\\", aspect=0.5) g.savefig(\\"faceted_swarm_plot.png\\") # Execute the function create_swarm_plots(tips) ```","solution":"import seaborn as sns import matplotlib.pyplot as plt # Load the dataset tips = sns.load_dataset(\\"tips\\") def create_swarm_plots(data): # Task 1: Univariate Swarm Plot plt.figure(figsize=(8, 6)) sns.swarmplot(data=data, x=\\"total_bill\\", size=3) plt.title(\\"Univariate Swarm Plot for Total Bill\\") plt.xlabel(\\"Total Bill\\") plt.savefig(\\"univariate_swarm_plot.png\\") plt.close() # Task 2: Categorical Swarm Plot plt.figure(figsize=(10, 8)) sns.swarmplot(data=data, x=\\"total_bill\\", y=\\"day\\", hue=\\"sex\\", dodge=True, size=3) plt.title(\\"Categorical Swarm Plot of Total Bill by Day and Sex\\") plt.xlabel(\\"Total Bill\\") plt.ylabel(\\"Day\\") plt.legend(title=\\"Sex\\") plt.savefig(\\"categorical_swarm_plot.png\\") plt.close() # Task 3: Faceted Swarm Plot g = sns.catplot( data=data, kind=\\"swarm\\", x=\\"time\\", y=\\"total_bill\\", hue=\\"sex\\", col=\\"day\\", aspect=0.6, height=5, dodge=True, size=3 ) g.fig.suptitle(\\"Faceted Swarm Plot of Total Bill by Day, Split by Time and Colored by Sex\\", y=1.02) g.set_axis_labels(\\"Time\\", \\"Total Bill\\") g.set_titles(\\"{col_name}\\") g.savefig(\\"faceted_swarm_plot.png\\") plt.close() # Execute the function create_swarm_plots(tips)"},{"question":"**Objective:** You need to demonstrate your understanding of the `zipfile` module by implementing a function that handles ZIP file operations. Specifically, you will implement the creation and extraction of ZIP files and should handle various edge cases mentioned in the documentation. **Function:** ```python def manage_zip_file(action, file_path, files=None, output_dir=None): Manage ZIP files by creating or extracting contents based on the provided action. Parameters: action (str): The action to perform, either \'create\' or \'extract\'. file_path (str): Path to the ZIP archive for the \'create\' or \'extract\' action. files (list, optional): List of file paths to include in the ZIP archive if action is \'create\'. output_dir (str, optional): Directory to extract files to if action is \'extract\'. Returns: list: If action is \'create\', returns a list of added file names. list: If action is \'extract\', returns a list of extracted file paths. Raises: ValueError: If an invalid action is given or if required parameters are missing. ``` **Requirements:** 1. **Creating a ZIP Archive:** If the action is `\'create\'`, all files provided in the `files` list should be added to a new ZIP archive specified by `file_path`. Use `ZIP_DEFLATED` as the compression method. 2. **Extracting a ZIP Archive:** If the action is `\'extract\'`, the contents of the ZIP archive specified by `file_path` should be extracted to the directory specified by `output_dir`. 3. **Return Values:** For `\'create\'` action, return a list of file names added to the archive. For `\'extract\'` action, return a list of file paths extracted. 4. **Error Handling:** Raise a `ValueError` if the action is not `\'create\'` or `\'extract\'`, if `files` is not provided with `\'create\'` action, or if `output_dir` is not provided with `\'extract\'` action. 5. **Edge Cases:** - Handle any exceptions that might occur during the file operations, such as invalid file paths or unreadable files. - Ensure the function operates correctly with both relative and absolute file paths. **Example Usage:** ```python # Creating a ZIP archive file_list = [\'file1.txt\', \'file2.txt\'] created_files = manage_zip_file(\'create\', \'archive.zip\', files=file_list) print(created_files) # Output: [\'file1.txt\', \'file2.txt\'] # Extracting a ZIP archive extracted_files = manage_zip_file(\'extract\', \'archive.zip\', output_dir=\'extracted_files\') print(extracted_files) # Output: [\'extracted_files/file1.txt\', \'extracted_files/file2.txt\'] ``` **Constraints:** - Ensure the directory structure in the ZIP file is preserved during extraction. - Handle potential issues like existing files in the output directory or errors during the zip/unzip process gracefully.","solution":"import zipfile import os def manage_zip_file(action, file_path, files=None, output_dir=None): Manage ZIP files by creating or extracting contents based on the provided action. Parameters: action (str): The action to perform, either \'create\' or \'extract\'. file_path (str): Path to the ZIP archive for the \'create\' or \'extract\' action. files (list, optional): List of file paths to include in the ZIP archive if action is \'create\'. output_dir (str, optional): Directory to extract files to if action is \'extract\'. Returns: list: If action is \'create\', returns a list of added file names. list: If action is \'extract\', returns a list of extracted file paths. Raises: ValueError: If an invalid action is given or if required parameters are missing. if action not in {\'create\', \'extract\'}: raise ValueError(\\"Invalid action specified. Use \'create\' or \'extract\'.\\") if action == \'create\': if files is None: raise ValueError(\\"Files list must be provided for \'create\' action.\\") with zipfile.ZipFile(file_path, \'w\', zipfile.ZIP_DEFLATED) as zipf: for file in files: if not os.path.isfile(file): raise ValueError(f\\"File {file} does not exist or is not readable.\\") zipf.write(file, os.path.basename(file)) return [os.path.basename(file) for file in files] elif action == \'extract\': if output_dir is None: raise ValueError(\\"Output directory must be provided for \'extract\' action.\\") if not os.path.exists(output_dir): os.makedirs(output_dir) with zipfile.ZipFile(file_path, \'r\') as zipf: zipf.extractall(output_dir) return [os.path.join(output_dir, file) for file in zipf.namelist()]"},{"question":"# Custom Built-in Function Wrapper Objective Create a custom read function `custom_read` that accepts a file path and a number of lines to read from that file. The function should leverage the built-in `open` function, accessed through the `builtins` module, to read the file and convert the contents to lower-case before returning them. Requirements 1. Implement the function `custom_read` which accepts two parameters: - `file_path`: A string representing the path to the file. - `num_lines`: An integer representing the number of lines to read from the file. 2. The function should: - Use the built-in `open` function (accessed via the `builtins` module) to open the file. - Read the specified number of lines from the file. - Convert the content of these lines to lower-case. - Return the lower-case content as a single string. 3. Handle the case where the file has fewer lines than requested by returning all available lines in lower-case. 4. Ensure that the file is properly closed after reading. Input and Output - **Input**: - `file_path`: A string specifying the file path. - `num_lines`: An integer specifying the number of lines to read. - **Output**: - A single string containing the content of the specified number of lines in lower-case. Constraints - The function should not modify the input file. - Assume that the file exists and is readable. Example ```python # Example content of file \'example.txt\': # \\"Hello World # This is a Test # Python Programming\\" content = custom_read(\'example.txt\', 2) print(content) # Output: \\"hello worldnthis is a testn\\" ``` Implement the `custom_read` function as specified above.","solution":"import builtins def custom_read(file_path, num_lines): Reads the specified number of lines from the file, converts them to lower-case and returns them as a single string. :param file_path: str - The path to the file. :param num_lines: int - The number of lines to read from the file. :return: str - The content of the specified number of lines converted to lower-case. content = [] try: with builtins.open(file_path, \'r\') as file: for _ in range(num_lines): line = file.readline() if not line: break content.append(line.lower()) except Exception as e: return str(e) return \'\'.join(content)"},{"question":"Context You are tasked with creating a Python tool that performs a series of file and directory operations. This tool will need to handle tasks such as organizing files based on their extensions, generating reports about the files, and creating backups of specific directories. Objective Write a Python function `organize_files(directory: str, report_file: str) -> None` that takes a directory path and a report file name as input, performs file organization based on file extensions, and generates a report about the files. Specifications 1. **Input** - `directory (str)`: Path to the directory that needs to be organized. - `report_file (str)`: Name of the file where the report should be saved. 2. **Output** - The function should not return anything. It should perform the following actions: - Move all files in the given directory into subdirectories based on their extensions. For example, all `.txt` files should move to a folder named `txt`. - Ignore subdirectories while organizing files. - Create directories for extensions that do not already have a dedicated subdirectory. - Generate a report file that includes the count of each type of file and their respective sizes in bytes. 3. **Constraints** - You can assume the directory contains only files and other subdirectories that should be ignored. - Performance constraints: The function should handle directories with up to 10,000 files efficiently. 4. **Example** ```python organize_files(\'/path/to/directory\', \'report.txt\') ``` After calling this function: - All files will be moved to their respective extension-based subdirectories within `/path/to/directory`. - A report, `report.txt`, will be generated with details similar to: ``` txt: 10 files, 2048 bytes jpg: 5 files, 10240 bytes pdf: 2 files, 15360 bytes ``` Hints - Use the `pathlib` module to handle file paths and manipulations. - The `shutil` module can be useful for moving files. - Ensure to handle cases where files have no extension or have the same name but different extensions. - Remember to handle both absolute and relative paths correctly. Implementation Here is a skeleton code outline to get you started: ```python from pathlib import Path import shutil def organize_files(directory: str, report_file: str) -> None: # Convert directory to Path object directory_path = Path(directory) # Dictionary to hold the count and size of each file type file_stats = {} # Iterate over all files in the directory for item in directory_path.iterdir(): if item.is_file(): ext = item.suffix[1:] # Get file extension without dot if not ext: ext = \'no_extension\' ext_dir = directory_path / ext ext_dir.mkdir(exist_ok=True) # Create directory if it doesn\'t exist dest = ext_dir / item.name shutil.move(str(item), str(dest)) # Move file to the new directory # Update file stats if ext not in file_stats: file_stats[ext] = {\'count\': 0, \'size\': 0} file_stats[ext][\'count\'] += 1 file_stats[ext][\'size\'] += dest.stat().st_size # Write report to the report file with open(report_file, \'w\') as report: for ext, stats in file_stats.items(): report.write(f\\"{ext}: {stats[\'count\']} files, {stats[\'size\']} bytesn\\") ``` Your task is to implement the logic within this outline and ensure it meets the specified requirements.","solution":"from pathlib import Path import shutil def organize_files(directory: str, report_file: str) -> None: # Convert directory to Path object directory_path = Path(directory) # Dictionary to hold the count and size of each file type file_stats = {} # Iterate over all files in the directory for item in directory_path.iterdir(): if item.is_file(): ext = item.suffix[1:] if item.suffix else \'no_extension\' # Get file extension without dot ext_dir = directory_path / ext ext_dir.mkdir(exist_ok=True) # Create directory if it doesn\'t exist dest = ext_dir / item.name shutil.move(str(item), str(dest)) # Move file to the new directory # Update file stats if ext not in file_stats: file_stats[ext] = {\'count\': 0, \'size\': 0} file_stats[ext][\'count\'] += 1 file_stats[ext][\'size\'] += dest.stat().st_size # Write report to the report file with open(directory_path / report_file, \'w\') as report: for ext, stats in file_stats.items(): report.write(f\\"{ext}: {stats[\'count\']} files, {stats[\'size\']} bytesn\\")"},{"question":"You are given access to a special Python C API that allows you to manipulate Python list objects directly. Your task is to implement a function `process_python_list` in Python that utilizes the provided C API operations to perform a series of manipulations on a given Python list. You will be given a Python list and a sequence of operations to perform on this list. # Function Signature ```python def process_python_list(py_list: list, operations: list) -> list: # implementation ``` # Parameters - `py_list` (list): A Python list of integers. - `operations` (list): A list of operations to perform on `py_list`. Each operation is specified as a tuple where the first element is a string representing the operation name (one of \'insert\', \'append\', \'remove\', \'sort\', \'reverse\', \'slice\', \'to_tuple\'), followed by the appropriate parameters for the operation. # Operation Types 1. `insert`: Insert an element at a specified position. - Tuple format: `(\'insert\', index, element)` - Example: `(\'insert\', 2, 10)` inserts the element `10` at index `2`. 2. `append`: Append an element to the end of the list. - Tuple format: `(\'append\', element)` - Example: `(\'append\', 5)` appends the element `5` to the list. 3. `remove`: Remove an element at a specified position. - Tuple format: `(\'remove\', index)` - Example: `(\'remove\', 3)` removes the element at index `3`. 4. `sort`: Sort the list. - Tuple format: `(\'sort\',)` - Example: `(\'sort\',)` sorts the list. 5. `reverse`: Reverse the list. - Tuple format: `(\'reverse\',)` - Example: `(\'reverse\',)` reverses the list. 6. `slice`: Get a slice of the list. - Tuple format: `(\'slice\', start, end)` - Example: `(\'slice\', 1, 4)` retrieves and returns the slice from index `1` to index `4`. 7. `to_tuple`: Convert the list to a tuple. - Tuple format: `(\'to_tuple\',)` - Example: `(\'to_tuple\',)` converts the list to a tuple and returns it. # Returns - The modified Python list after applying all given operations in sequence. # Example ```python # Original list py_list = [1, 2, 3, 4, 5] # Operations to perform operations = [ (\'insert\', 2, 99), # [1, 2, 99, 3, 4, 5] (\'append\', 100), # [1, 2, 99, 3, 4, 5, 100] (\'remove\', 3), # [1, 2, 99, 4, 5, 100] (\'sort\',), # [1, 2, 4, 5, 99, 100] (\'reverse\',), # [100, 99, 5, 4, 2, 1] (\'slice\', 1, 4), # [99, 5, 4] (\'to_tuple\',) # (99, 5, 4) ] # Calling the function result = process_python_list(py_list, operations) print(result) # Output: (99, 5, 4) ``` # Constraints - Assume that all operations and indexes provided are valid. - Your implementation must correctly handle all operations and return the final state of the list (or tuple if the last operation is `to_tuple`).","solution":"def process_python_list(py_list: list, operations: list) -> list: Perform a series of operations on a given Python list according to the operations sequence. :param py_list: list of integers :param operations: list of operations to perform on py_list :return: Modified list after all operations or tuple if \'to_tuple\' operation is the last for operation in operations: op = operation[0] if op == \'insert\': _, index, element = operation py_list.insert(index, element) elif op == \'append\': _, element = operation py_list.append(element) elif op == \'remove\': _, index = operation del py_list[index] elif op == \'sort\': py_list.sort() elif op == \'reverse\': py_list.reverse() elif op == \'slice\': _, start, end = operation py_list = py_list[start:end] elif op == \'to_tuple\': return tuple(py_list) return py_list"},{"question":"# Task Create a function `sanitize_sql_query` that takes a SQL query string as input and returns a sanitized version by replacing the values of SQL parameters with placeholders. This is useful for logging purposes when you do not want to expose sensitive information. # Function Signature ```python def sanitize_sql_query(query: str) -> str: ``` # Input - `query` (str): A string representing the SQL query. # Output - Returns a string with the values of SQL parameters replaced by placeholders. # Constraints - The query string will contain valid SQL syntax. - SQL parameters can be integers, floating-point numbers, strings (single-quoted or double-quoted), and datetime literals. # Example ```python query = \\"SELECT * FROM users WHERE user_id = 42 AND username = \'admin\' AND created_at > \'2023-01-01 12:00:00\' AND status = 1;\\" print(sanitize_sql_query(query)) # Expected output: \\"SELECT * FROM users WHERE user_id = ? AND username = ? AND created_at > ? AND status = ?;\\" ``` # Notes 1. Integer and floating-point numbers should be replaced by `?`. 2. String parameters (single-quoted or double-quoted) should be replaced by `?`. 3. Datetime literals should be replaced by `?`. 4. Non-parameter elements of the query (like keywords, column names, operators, etc.) should remain unchanged. # Use Constraints: - Use the `re` module to accomplish this task. - You should ensure that the replacements are done in an efficient manner, maintaining the structure of the SQL query.","solution":"import re def sanitize_sql_query(query: str) -> str: Sanitizes a SQL query by replacing parameter values with placeholders. Parameters: query (str): The input SQL query string Returns: str: The sanitized SQL query with parameter values replaced by \'?\' # Regular expression pattern to match integers, floats, strings, and datetime literals pattern = re.compile(r (?P<number>-?bd+(.d+)?b) # Integers and floats |(?P<string>\'[^\']*\'|\\"[^\\"]*\\") # Single-quoted and double-quoted strings |(?P<datetime>\'d{4}-d{2}-d{2}sd{2}:d{2}:d{2}\') # Datetime literals , re.X) # Replace matched patterns with \'?\' sanitized_query = pattern.sub(\'?\', query) return sanitized_query"}]'),A={name:"App",components:{PoemCard:I},data(){return{searchQuery:"",visibleCount:4,poemsData:D,isLoading:!1}},computed:{filteredPoems(){const i=this.searchQuery.trim().toLowerCase();return i?this.poemsData.filter(e=>e.question&&e.question.toLowerCase().includes(i)||e.solution&&e.solution.toLowerCase().includes(i)):this.poemsData},displayedPoems(){return this.searchQuery.trim()?this.filteredPoems:this.filteredPoems.slice(0,this.visibleCount)},hasMorePoems(){return!this.searchQuery.trim()&&this.visibleCount<this.poemsData.length}},methods:{async loadMore(){this.isLoading=!0,await new Promise(i=>setTimeout(i,1e3)),this.visibleCount+=4,this.isLoading=!1}}},z={class:"search-container"},F={class:"card-container"},q={key:0,class:"empty-state"},R=["disabled"],O={key:0},N={key:1};function j(i,e,l,m,n,o){const h=_("PoemCard");return a(),s("section",null,[e[4]||(e[4]=t("div",{class:"top-banner"},[t("div",{class:"top-banner-title"},[t("div",{class:"top-banner-title-text"},"🤔prompts chat🧠")])],-1)),t("div",z,[e[3]||(e[3]=t("span",{class:"search-icon"},"🔍",-1)),g(t("input",{type:"text",class:"search-input","onUpdate:modelValue":e[0]||(e[0]=r=>n.searchQuery=r),placeholder:"Search..."},null,512),[[y,n.searchQuery]]),n.searchQuery?(a(),s("button",{key:0,class:"clear-search",onClick:e[1]||(e[1]=r=>n.searchQuery="")}," ✕ ")):d("",!0)]),t("div",F,[(a(!0),s(b,null,v(o.displayedPoems,(r,f)=>(a(),w(h,{key:f,poem:r},null,8,["poem"]))),128)),o.displayedPoems.length===0?(a(),s("div",q,' No results found for "'+c(n.searchQuery)+'". ',1)):d("",!0)]),o.hasMorePoems?(a(),s("button",{key:0,class:"load-more-button",disabled:n.isLoading,onClick:e[2]||(e[2]=(...r)=>o.loadMore&&o.loadMore(...r))},[n.isLoading?(a(),s("span",N,"Loading...")):(a(),s("span",O,"See more"))],8,R)):d("",!0)])}const L=p(A,[["render",j],["__scopeId","data-v-00233592"]]),Y=JSON.parse('{"title":"","description":"","frontmatter":{"page":true},"headers":[],"relativePath":"chatai/59.md","filePath":"chatai/59.md"}'),M={name:"chatai/59.md"},X=Object.assign(M,{setup(i){return(e,l)=>(a(),s("div",null,[x(L)]))}});export{Y as __pageData,X as default};
